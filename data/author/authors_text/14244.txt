Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 19?27,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Empirical lower bounds on alignment error rates in syntax-based machine
translation
Anders S?gaard?
Center for Language Technology
University of Copenhagen
soegaard@hum.ku.dk
Jonas Kuhn?
Dpt. of Linguistics
University of Potsdam
kuhn@ling.uni-potsdam.de
Abstract
The empirical adequacy of synchronous
context-free grammars of rank two (2-SCFGs)
(Satta and Peserico, 2005), used in syntax-
based machine translation systems such as
Wu (1997), Zhang et al (2006) and Chi-
ang (2007), in terms of what alignments they
induce, has been discussed in Wu (1997) and
Wellington et al (2006), but with a one-sided
focus on so-called ?inside-out alignments?.
Other alignment configurations that cannot be
induced by 2-SCFGs are identified in this pa-
per, and their frequencies across a wide col-
lection of hand-aligned parallel corpora are
examined. Empirical lower bounds on two
measures of alignment error rate, i.e. the one
introduced in Och and Ney (2000) and one
where only complete translation units are con-
sidered, are derived for 2-SCFGs and related
formalisms.
1 Introduction
Syntax-based approaches to machine translation
typically use synchronous grammars to recognize or
produce translation equivalents. The synchronous
?This work was done while the first author was a Senior
Researcher at the Dpt. of Linguistics, University of Potsdam,
supported by the German Research Foundation in the Emmy
Noether project Ptolemaios on grammar learning from paral-
lel corpora; and while he was a Postdoctoral Researcher at the
ISV Computational Linguistics Group, Copenhagen Business
School, supported by the Danish Research Foundation in the
project Efficient syntax- and semantics-based machine transla-
tion.
?The second author is supported by the German Research
Foundation in the Emmy Noether project Ptolemaios on gram-
mar learning from parallel corpora.
production rules are typically learned from align-
ment structures (Wu, 1997; Zhang and Gildea, 2004;
Chiang, 2007) or from alignment structures and
derivation trees for the source string (Yamada and
Knight, 2001; Zhang and Gildea, 2004). They are
also used for inducing alignments (Wu, 1997; Zhang
and Gildea, 2004).
It is for all three reasons, i.e. translation, in-
duction from alignment structures and induction of
alignment structures, important that the synchronous
grammars are expressive enough to induce all the
alignment structures found in hand-aligned gold
standard parallel corpora (Wellington et al, 2006).
Such alignments are supposed to reflect the structure
of translations, typically contain fewer errors and are
used to evaluate automatically induced alignments.
In this paper it is shown that the synchronous
grammars used in Wu (1997), Zhang et al (2006)
and Chiang (2007) are not expressive enough to do
that. The synchronous grammars used in these sys-
tems are, formally, synchronous context-free gram-
mars of rank two (2-SCFGs), or equivalently (nor-
mal form) inversion transduction grammars (ITGs).1
The notion of rank is defined as the maximum num-
ber of constituents aligned by a production rule,
i.e. the maximum number of distinct indeces. Our
results will be extended to slight extensions of 2-
SCFGs, incl. the extension of ITGs proposed by
Zens and Ney (2003) (xITGs), synchronous tree
substitution grammars of rank two (2-STSGs) (Eis-
ner, 2003; Shieber, 2007), i.e. where tree pairs in-
clude at most two linked pairs of nonterminals, and
synchronous tree-adjoining grammars of rank two
12-SCFGs allow distinct LHS nonterminals, while ITGs do
not; but for any 2-SCFG an equivalent ITG can be constructed
by creating a cross-product of nonterminals from two sides.
19
(2-STAGs) (Shieber and Schabes, 1990; Harbusch
and Poller, 1996; Nesson et al, 2008). The over-
all frequency of alignment structures that cannot
be induced by these approaches is examined across
a wide collection of hand-aligned parallel corpora.
Empirical lower bounds on the coverage of the sys-
tems are derived from our results.
Our notion of an alignment structure is standard.
Words can be aligned to multiple words. Unaligned
nodes are permitted. Maximally connected sub-
graphs are called translation units. There is one
more choice to make in the context of many-to-many
alignments, namely whether the alignment relation
is such that if wi|w?k and wi|w?l, resp., are aligned,
and wj |w?k are aligned too, then wj |w?l are also
aligned. If so, the alignment structure is divided into
complete translation units. Such alignment struc-
tures are therefore called complete; in Goutte et
al. (2004), alignment structures with this property
are said to be closed under transitivity. An align-
ment structure is simply written as a sequence of
alignments, e.g. ?wi|w?k, wi|w?l, wj |w?k, wj |w?k?, or,
alternatively, as sequences of (possibly discontinu-
ous) translation units, e.g. ?wiwj|w?kw?l?.
A translation unit induced by a synchronous
grammar is a set of terminals that are recognized
or generated simultaneously. Consequently, syn-
chronous grammars can only induce complete align-
ment structures (by transitivity of simultaneity).2
Syntax-based approaches to machine translations
are commonly evaluated in terms of their alignment
error rate (AER) on one or more parallel corpora
(Och and Ney, 2000; Zhang and Gildea, 2004). The
AER, in the case where all alignments are sure align-
ments, is
AER = 1? 2|SA ?GA||SA| + |GA|
where GA are the gold standard alignments, and SA
the alignments produced by the system.
AER has been criticized by Fraser and
Marcu (2007). They show that AER does not
penalize unequal precision and recall when a
distinction between sure and possible alignments is
2One of the hand-aligned parallel corpora used in our exper-
iments, the one also used in Pado? and Lapata (2006), includes
incomplete alignment structures.
made. Since no such distinction is assumed below,
the classical definition is used.
We introduce also the notion of translation unit
error rate (TUER), which is defined as
TUER = 1 ? 2|SU ?GU ||SU |+ |GU |
where GU are the translation units in the gold stan-
dard, and SU the translation units produced by the
system. In other words, what is measured is a sys-
tem?s ability to predict translation units relative to
the Gold standard, not just its ability to predict align-
ments. If the system only gets part of a translation
unit right, it is not rewarded.
In the context of many-to-many alignments, this
measure may tell us more about translation quality
than AER. Consider, for instance, the small chil-
dren?s book discourse in Danish:
(1) Mads
Mads
og
CONJ
Mette
Mette
l?gger
put.FIN.PRES
tal
number.PL
sammen.
together
?Mads and Mette add numbers.?
(2) Mads
Mads
og
CONJ
Mette
Mette
l?gger
put.FIN.PRES
tal
number.PL
sammen
together
hver
every
dag.
day
?Mads and Mette add numbers every day.?
(3) Mads
Mads
og
CONJ
Mette
Mette
kan
can.FIN.PRES
godt
good
lide
like.INF
at
to
addere.
add.INF
?Mads and Mette like to add.?
(4) Mette
Mette
sp?rger
ask.FIN.PRES
ofte:
often:
Skal
Shall.FIN.FUT/PRES
vi
PRON.PL.1
addere
add.INF
sammen?
together
?Mette often asks: Do you want to add together??
Say (1-4) and the English translations are a par-
allel corpus on which we would like to evaluate an
aligner or a statistical machine translation system.
Say also that the test corpus has been aligned. Let
the first three sentences be our training data and (4)
our test data.
20
Note that the words l?gger . . . sammen form a dis-
continuous translation unit (?add?). Say our aligner
aligned only sammen and add, but not l?gger and
add. This would mean that the alignments or trans-
lations of add would most likely be associated with
the following probabilities:
.66 (add, sammen)
.33 (add, addere)
which again means that our system is likely to
arrive at the wrong alignment or translation in (4).
Nevertheless these alignments are rewarded in AER.
TUER, on the other hand, reflects the intuition that
unless you get the entire translation unit it?s better to
get nothing at all.
The hand-aligned parallel corpora in our exper-
iments come from the Copenhagen Dependency
Treebank (Buch-Kromann, 2007), for five different
language pairs, the German-English parallel corpus
used in Pado? and Lapata (2006), and the six par-
allel corpora of the first 100 sentences of Europarl
(Koehn, 2005) for different language pairs docu-
mented in Graca et al (2008). Consequently, our
experiments include a total of 12 parallel corpora.
The biggest parallel corpus consists of 4,729 sen-
tence pairs; the smallest of 61 sentence pairs. The
average size is 541 sentence pairs. The six paral-
lel corpora documented in Graca et al (2008) use
sure and possible alignments; in our experiments, as
already mentioned, the two types of alignments are
treated alike.3
3The annotations of the parallel corpora differ in format and
consistency. In fact the empirical lower bounds obtained be-
low are lower bounds in two senses: (i) they are lower bounds
on TUERs because TUERs may be significantly higher than
the empirical lower bounds found here, and (ii) they are lower
bounds in the sense that there may be hidden instances of the
configurations in question in the parallel corpora. Most seri-
ously, our search algorithms only sort alignments, but not their
elements; instead they assume that their elements are listed in
chronological order. Sometimes, but rarely, this is not the case.
Consider, for instance, file 1497, line 12 in the Danish?Spanish
parallel corpus in the Copenhagen Dependency Treebank:
<align out=?a56? type=?? in=?b30+b32+b8? outsign=?af?
insign=?del de de?/>
This is a translation unit. The word in position 56 in the source
string is aligned to the words in positions 8, 30 and 32 in the
target string, but note that the target string words do not appear
in chronological order. In some cases our algorithms take care
of this; they do not, however, in general search all possible com-
binations of words and alignments, but rely on the linear order
Sect. 2 discusses the frequency of inside-out
alignments in our hand-aligned corpora, whereas
Sect. 3 is about complex translation units. Sect. 4
briefly introduces formalisms for syntax-based ma-
chine translation, but some prior knowledge is as-
sumed. Sect. 5 brings the three sections together
and presents lower bounds on the coverage of the
systems discussed in Sect. 4, obtained by inspection
of the results in Sect. 2 and 3. Sect. 6 compares
our results to related work, in particular Zens and
Ney (2003).
2 Inside-out alignments
Wu (1997) identified so-called inside-out align-
ments, two alignment configurations that cannot be
induced by binary synchronous context-free gram-
mars; these alignment configurations, while infre-
quent in language pairs such as English?French
(Cherry and Lin, 2006; Wellington et al, 2006),
have been argued to be frequent in other lan-
guage pairs, incl. English?Chinese (Wellington et
al., 2006) and English?Spanish (Lepage and De-
noual, 2005). While our main focus is on config-
urations that involve discontinuous translation units,
the frequencies of inside-out alignments in our par-
allel corpora are also reported. Recall that inside-out
alignments are of the form (or upside-down):
a b c d
e f g h
or
a b c d
e f g h
Our findings are summarized in Figure 1. Note
that there is some variation across the corpora. The
fact that there are no inside-out alignments in cor-
pora 2?4 may be because annotators of these corpora
have been very conservative, i.e. there are many un-
aligned nodes; the first corpus, which is also part of
the Danish Dependency Treebank, also has very few
inside-out alignments. It is not entirely clear to us if
this has to do with the languages in question or the
annotation guide lines (cf. Danish?Spanish).
In the Danish?Spanish corpus and in the English?
German corpus the number of inside-out alignments
is very high. This, to some extent, has to do with the
number of words that are aligned to multiple words.
of the annotation. This was necessary to do relatively efficient
queries. The effect, however, is that our results are lower than
the actual frequencies in the parallel corpora. They are in this
sense also lower bounds.
21
Snt. TUs IO IO-m IO-m/Snt.
Danish?English: 4,729 110,511 28 4 0.001
Danish?German: 61 1,026 0 0 0
Danish?Italian: 181 2,182 0 0 0
Danish?Russian: 61 618 0 0 0
Danish?Spanish: 710 6,110 2,562 158 0.223
English?German 987 68,760 191,490 1,178 1.194
English?French: 100 937 2,651 80 0.800
English-Portuguese: 100 941 3,856 66 0.660
English?Spanish: 100 950 2,287 67 0.670
Portuguese?French: 100 915 3,643 84 0.840
Portuguese?Spanish: 100 991 1,194 58 0.580
Spanish?French 100 975 1,390 61 0.610
Figure 1: Frequency of inside-out alignments.
Say, in the case of English?German, each inside-out
alignment is made out of eight two-word translation
units. There are 1,178 inside-out alignment modulo
translation units, i.e. when one or more inside-out
alignments over the same eight translation units only
count as one; this means that there would be 28 ?
1, 178 : 301, 568 inside-out alignments in total. The
actual number (191, 491) is smaller, but comparable.
The first example in the English?German corpus,
from sentence 2, illustrates this point. The sentences
are:
(5) Mr Jonckheer, I would like to thank you just as
warmly for your report on the seventh survey
on State aid in the European Union .
(6) Ebenso herzlich mo?chte ich Ihnen, Herr
Jonckheer, fu?r Ihren Bericht u?ber den
siebenten Bericht u?ber staatliche Beihilfen in
der Europa?ischen Union danken (24).
and the alignment structure is (commas count):
1 2 3 4 5 6 7 8 9
?1
3 4 5 7 8 9 24
The aligned translation units are:4
4Note that the alignment 3|5 is probably a mistake made by
the annotator. It should, it seems, be 3|6. Note also that this
alignment is not involved in any of the inside-out alignments.
?Mr|Herr? ?Jonckheer|Jonckheer?
?,|Ihnen . . . ,? ?I|ich?
?would like to|mo?chte? ?thank|danken?
?you|Ihnen?
Note that the following sets of alignments make
up distinct inside-out alignments modulo translation
units:
{?1|7, 4|4, 8|24, 9|5?, ?2|8, 4|4, 8|24, 9|5?,
?3|9, 4|4, 8|24, 9|5?, ?1|7, 5|3, 8|24, 9|5?,
?2|8, 5|3, 8|24, 9|5?, ?3|9, 5|3, 8|24, 9|5?}
The following sets of alignments in addition make
up distinct inside-out alignments, but the new align-
ments 6|3 and 7|3 are from the same translation unit
as 5|3:
{?1|7, 6|3, 8|24, 9|5?, ?2|8, 6|3, 8|24, 9|5?,
?3|9, 6|3, 8|24, 9|5?, ?1|7, 6|3, 8|24, 9|5?,
?2|8, 6|3, 8|24, 9|5?, ?3|9, 6|3, 8|24, 9|5?}
Consequently, the alignment of sentences (5) and
(6) in the English?German parallel corpus contains
12 inside-out alignments, but only six inside-out
alignments modulo translation units.
3 Cross-serial discontinuous translation
units
A discontinuous translation unit (DTU) is a transla-
tion unit where either the substring of source string
words or the substring of target string words that oc-
cur in it, is discontinuous, i.e. there is a gap in it.
Since translation units are induced by simulta-
neous recognition, it is necessary for synchronous
22
grammars to have rules that introduce multiple
source side terminals and/or multiple target side ter-
minals with at least one intervening nonterminal to
induce DTUs. A DTU with multiple gaps in the
same side is called a multigap DTU; it is easy to see
that binary grammars cannot induce multigap DTUs
with more than two gaps.
A sequence of DTUs is said to be cross-serial if it
is of the following form (or upside-down):
ai aj
bk bl bm bn
Call any sequence of cross-serial DTUs a cross-
serial DTU (CDTU). So a CDTU is an alignment
configuration such that the source-side, resp. target-
side, contains four tokens bk, bl, bm, bn such that (i)
bk ? bl ? bm ? bn, (ii) bk and bm belong to the
same translation unit T , and bl and bn belong to the
same translation unit T ?, and (iii) T and T ? are dis-
tinct translation units. The inability of ITGs, xITGs
and 2-STSGs to induce CDTUs follows from the ob-
servation that if bk and bm in the above are gener-
ated or recognized simultatenously in any of these
formalisms, bl and bn cannot be generated or recog-
nized simulaneously. This is a straight-forward con-
sequence of the context-freeness of the component
grammars.
The distinction between CDTUs and CDTUs
modulo translation units (CDTU-ms) is again im-
portant. The number of CDTU-ms is the number of
CDTUs such that all CDTUs differ by at most one
translation unit. The English?German parallel cor-
pus, for example, contains 15,717 CDTUs, but only
2,079 CDTU-ms. Since our evaluation measure is
TUER, we only systematically counted the occur-
rences of CDTU-ms. In a few cases, the number of
CDTUs was extracted too. In general, it was about
eight times higher than the number of CDTU-ms.
Our findings are summarized in Figure 2. There is
again variation, but the average ratio of CDTU-ms is
0.514, i.e. there is a CDTU-m in about every second
aligned sentence pair.
4 Syntax-based machine translation
Syntax-directed translation schemas (SDTSs) were
originally introduced by Culik (1966) and studied
formally by Aho and Ullman (1972), who stressed
the importance of using only binary SDTSs for effi-
ciency reasons,5 and later led to the development of
a number of near-equivalent theories, incl. 2-SCFGs
and (normal form) ITGs. Henceforth, we will refer
to this class of near-equivalent theories as ITGs (see
footnote 1). This also means that production rules
have at most one source-side and one target-side ter-
minal on the RHS (see below).
It is the ability of ITGs to induce alignments that
is our main focus. Related work includes Wu (1997),
Zens and Ney (2003) and Wellington et al (2006).
Our results will also be extended to xITGs, 2-STSGs
and 2-STAGs. O(|G|n6) time recognition algo-
rithms are known for ITGs, xITGs and 2-STSGs. 2-
STAGs (O(|G|n12)) are more complex.
The production rules in ITGs are of the follow-
ing form (Wu, 1997), with a notation similar to what
is typically used for SDTSs and SCFGs in the right
column:
A ? [BC] A ? ?B1C2, B1C2?
A ? ?BC? A ? ?B1C2, C2B1?
A ? e | f A ? ?e, f?
A ? e | ? A ? ?e, ??
A ? ? | f A ? ??, f?
It is important to note that RHSs of production
rules have at most one source-side and one target-
side terminal symbol. This prevents induction of
multiword translation units in any straight-forward
way. xITGs (Zens and Ney, 2003) in part solves this
problem. All production rules in ITGs can be pro-
duction rules in xITGs, but xITG production rules
can also be of the following form:
A ? [e/f1A?/f2] | ?e/f1A?/f2?
Note, however, that these production rules still do
not enable double-sided DTUs, i.e. DTUs that trans-
late into DTUs. Such, however, occur relatively fre-
quently in hand-aligned parallel corpora, e.g. 148
times in the Danish?Spanish corpus.
There is no room for detailed introductions of the
more complex formalisms, but briefly their differ-
ences can be summarized as follows:
The move from ITGs to 2-STSGs is relatively
simple. All production rules in ITGs characterize
5The hierarchy of SDTSs of rank k is non-collapsing, and
the recognition problem without a fixed rank is NP-hard (Aho
and Ullman, 1972; Rambow and Satta, 1994). See Zhang et
al. (2006) for an efficient binarization algorithm.
23
Snt. TUs DTUs DTUs/Snt. CDTU-ms CDTU-ms/Snt.
Danish?English: 4,729 110,511 1,801 0.381 6 0.001
Danish?German: 61 1,026 43 0.705 0 0
Danish?Italian: 181 2,182 63 0.348 1 0.006
Danish?Russian: 61 618 27 0.443 0 0
Danish?Spanish: 710 6,693 779 1.097 121 0.170
English?German 650 68,760 5,062 7.788 2,079 3.199
English?French: 100 937 95 0.950 38 0.380
English-Portuguese: 100 941 100 1.000 85 0.850
English?Spanish: 100 950 90 0.900 50 0.500
Portuguese?French: 100 915 77 0.770 27 0.270
Portuguese?Spanish: 100 991 80 0.800 55 0.550
Spanish?French 100 975 74 0.740 24 0.240
Figure 2: Frequency of cross-serial DTUs.
binary trees of depth 1. It is said that this is the do-
main of locality in ITGs. 2-STSGs extend the do-
main of locality to arbitrarily big trees. 2-STSGs
are collections of ordered pairs of aligned trees with
at most two pairs of linked nonterminals. The leaf
nodes in the trees may be decorated by terminals or
insertion slots where subtrees can be ?plugged in?.
This is exactly what is meant by tree substitution.
It is assumed that all terminals in a tree pair con-
stitute a translation unit. There exists a O(|G|n6)
time parsing algorithm for 2-STSGs. 2-STSGs in-
duce DTUs, double-sided DTUs and DTUs with at
most two gaps, but not inside-out alignments, CD-
TUs and multigap DTUs with more than two gaps.
The substitution operation on elementary trees is
supplied with an adjunction operation in 2-STAGs
(Shieber and Schabes, 1990; Harbusch and Poller,
1996; Nesson et al, 2008). In adjunction, auxil-
iary trees, i.e. elementary trees with a designated leaf
node labeled by a nonterminal identical to the non-
terminal that labels the root node, extend the derived
tree by expanding one of its nodes. If an auxiliary
tree t, with a root node and a leaf node both labeled
A, is adjoined at some node n also labeled A in a
derived tree t?, the subtree s? (of t?) rooted at n is re-
placed by t, and s? is then inserted at the leaf node of
t. In 2-STAGs, paired nodes across the source-side
and target-side trees are simultaneously expanded by
either substitution or adjunction. A O(|G|n12) pars-
ing algorithm can be deviced for 2-STAGs using the
techniques in Seki et al (1991). The following 2-
STAG translates Swiss-style cross-serial dependen-
cies {wambnxcmdny} into {w(ac)mx(bd)ny} and
thus induces cross-serial DTUs whenever m,n ? 1
(superscripts are pairings).
? Sbb""
w Z1 y
, Sbb""
w Z1 y
?? Z1
x
, Z1
x
?
? ZNAZZProceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 60?68,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
On the complexity of alignment problems in two synchronous grammar
formalisms
Anders S?gaard?
Center for Language Technology
University of Copenhagen
soegaard@hum.ku.dk
Abstract
The alignment problem for synchronous
grammars in its unrestricted form, i.e. whether
for a grammar and a string pair the grammar
induces an alignment of the two strings, re-
duces to the universal recognition problem,
but restrictions may be imposed on the align-
ment sought, e.g. alignments may be 1 : 1,
island-free or sure-possible sorted. The com-
plexities of 15 restricted alignment problems
in two very different synchronous grammar
formalisms of syntax-based machine transla-
tion, inversion transduction grammars (ITGs)
(Wu, 1997) and a restricted form of range
concatenation grammars ((2,2)-BRCGs) (S?-
gaard, 2008), are investigated. The universal
recognition problems, and therefore also the
unrestricted alignment problems, of both for-
malisms can be solved in time O(n6|G|). The
complexities of the restricted alignment prob-
lems differ significantly, however.
1 Introduction
The synchronous grammar formalisms used in
syntax-based machine translation typically induce
alignments by aligning all words that are recog-
nized simultaneously (Wu, 1997; Zhang and Gildea,
?This work was done while the first author was a Senior
Researcher at the Dpt. of Linguistics, University of Potsdam,
supported by the German Research Foundation in the Emmy
Noether project Ptolemaios on grammar learning from paral-
lel corpora; and while he was a Postdoctoral Researcher at the
ISV Computational Linguistics Group, Copenhagen Business
School, supported by the Danish Research Foundation in the
project Efficient syntax- and semantics-based machine transla-
tion.
2004). On a par with weak and strong generative ca-
pacity, it is thus possible to talk about the alignment
capacity of those formalisms. In this paper, two syn-
chronous grammar formalisms are discussed, inver-
sion transduction grammars (ITGs) (Wu, 1997) and
two-variable binary bottom-up non-erasing range
concatenation grammars ((2,2)-BRCGs) (S?gaard,
2008). It is known that ITGs do not induce the class
of inside-out alignments discussed in Wu (1997).
Another class that ITGs do not induce is that of
alignments with discontinuous translation units (S?-
gaard, 2008). S?gaard (2008), on the other hand,
shows that the alignments induced by (2,2)-BRCGs
are closed under union, i.e. (2,2)-BRCGs induce all
possible alignments.
The universal recognition problems of both ITGs
and (2,2)-BRCGs can be solved in time O(n6|G|).
This may come as a surprise, as ITGs restrict the
alignment search space considerably, while (2,2)-
BRCGs do not. In the context of the NP-hardness of
decoding in statistical machine translation (Knight,
1999; Udupa and Maji, 2006), it is natural to ask
why the universal recognition problem of (2,2)-
BRCGs isn?t NP-hard? How can (2,2)-BRCGs in-
duce all possible alignments and still avoid NP-
hardness? This paper bridges the gap between
these results and shows that when alignments are
restricted to be 1 : 1, island-free or sure-possible
sorted (see below), or all combinations thereof,
the alignment problem of (2,2)-BRCGs is NP-hard.
(2,2)-BRCGs in a sense avoid NP-hardness by giv-
ing up control over global properties of alignments,
e.g. any pair of words may be aligned multiple times
in a derivation.
60
The alignment structures induced by synchronous
grammars in syntax-based machine translation have
the following property: If an alignment structure in-
cludes alignments v|v?, v|w? and w|w?, it also in-
cludes the alignment w|v?, where w,w?, v, v? are
word instances.1 This follows from the fact that
only words that are recognized simultanously, are
aligned. Otherwise alignment structures are just a
binary symmetric relation on two strings, a source
and a target string, such that two words in the source,
resp. target string, cannot be aligned. Maximally
connected subgraphs (ignoring precedence edges)
are called translation units.
The alignment problem can be formulated this
way (with s, s? source and target sentence, resp.):
INSTANCE: G, ?s, s??.
QUESTION: Does G induce an alignment
on ?s, s???
The alignment problem in its unrestricted form
reduces to the universal recognition problem (Bar-
ton et al, 1987), i.e. whether for a grammar G and
a string pair ?s, s?? it holds that ?s, s?? ? L(G)?
Of course the alignment may in this case be empty
or partial. Both ITGs and (2,2)-BRCGs permit un-
aligned nodes.
This paper investigates the complexity of re-
stricted versions of the alignment problem for ITGs
and (2,2)-BRCGs. A simple example, which can
be solved in linear time for both formalisms, is the
alignment problem wrt. alignments that consist of a
single translation unit including all source and target
words. It may be formulated this way:
INSTANCE: G, ?s, s??.
QUESTION: Does G induce an alignment that
consists of a single translation unit
with no unaligned words on ?s, s???
This can be solved for ITGs by checking if there
is a production rule that introduces all the words in
the right order such that:2
1w|w? is our short hand notation for saying that w, a word
in the source string, and w?, a word in the target string, have
been aligned. In the formal definition of alignments below, it is
said that w ? Vs (w is a word in the source string), w? ? Vt
(w? is a word in the target string) and (w,w?) ? A, i.e. w is
aligned to w?, and vice versa. Alignments are bidirectional in
what follows.
2In fact in normal form ITGs, we can simply check if there
? The LHS nonterminal symbol (possibly suf-
fixed by the empty string ?) can be derived from
the start symbol.
? The empty string ? can be derived from all RHS
nonterminal symbols.
The only difference for (2,2)-BRCGs is that pro-
duction rules are typically referred to as clauses in
the range concatenation grammar literature.
This paper considers some more complex exam-
ples; namely, the alignment problems wrt. 1 : 1-
alignments, (source-side and/or target-side) island-
free alignments and sure-possible sorted alignments.
The formal definitions of the three properties are as
follows:
Definition 1.1. An alignment structure for a string
pair ?w1 . . . wn, v1 . . . vm? is a graph D = ?V,E?
where V = Vs : {w1, . . . , wn} ? Vt : {v1, . . . , vm}
and E = Es : {wi ? wj | i < j} ? Et : {vi ? vj |
i < j} ? A where A ? Vs ? Vt. If (wi, vj) ? A,
also written wi|vj , wi is said to be aligned to vj ,
and vice versa. An alignment structure is said to
be wellformed iff for all wi, wj , vi? , vj? it holds that
if wi|vi? , wi|vj? and wj|vi? are aligned then so are
wj |vj? . An alignment structure is said to be 1 : 1 iff
no word occurs in two distinct tuples inA. An align-
ment structure is said to be island-free iff all words
in V occur in some tuple inA; it is said to be source-
side, resp. target-side, island-free if all words in Vs,
resp. Vt, occur in some tuple in A. The set of align-
ments is divided into sure and possible alignments,
i.e. A = S ? P (in most cases P = ?). An align-
ment structure is said to be sure-possible sorted iff if
it holds that (wi, vj?) ? S then for all wj , vi? neither
(wi, vi?) ? P nor (wj , vj?) ? P holds; similarly, if
it holds that (wi, vj?) ? P then for all wj , vi? neither
(wi, vi?) ? S nor (wj , vj?) ? S holds.
The precedence relations in E are not important
for any of our definitions, but are important for
meaningful interpretation of alignment structures.
Note that synchronous grammars are guaranteed to
induce wellformed alignment structures. Some brief
motivation for the properties singled out:
is a production rule with the start symbol in the LHS that in-
troduces all the words in the right order, since all production
rules with nonterminal symbols in the RHS are branching and
contain no terminal symbols.
61
Result 1 : 1 IF(s) IF(t) SP ITGs (2,2)-BRCGs
(1) X O(n6|G|) NP-complete
(2) X O(n6|G|) NP-complete
(3) X O(n6|G|) NP-complete
(4) X O(n6|G|) NP-complete
(5) X X O(n6|G|) NP-complete
(6) X X O(n6|G|) NP-complete
(7) X X O(n6|G|) NP-complete
(8) X X O(n6|G|) NP-complete
(9) X X O(n6|G|) NP-complete
(10) X X O(n6|G|) NP-complete
(11) X X X O(n6|G|) NP-complete
(12) X X X O(n6|G|) NP-complete
(13) X X X O(n6|G|) NP-complete
(14) X X X O(n6|G|) NP-complete
(15) X X X X O(n6|G|) NP-complete
Figure 1: The complexity of restricted alignment problems for ITGs and (2,2)-BRCGs.
? 1 : 1-alignments have been argued to be ad-
equate by Melamed (1999) and elsewhere, and
it may therefore be useful to know if a grammar
extracted from a parallel corpus produces 1 : 1-
alignments for a finite set of sentence pairs.
? Island-free alignments are interesting to the ex-
tent that unaligned nodes increase the chance of
translation errors. An island threshold may for
instance be used to rule out risky translations.
? The notion of sure-possible sorted alignments
is more unusual, but can, for instance, be used
to check if the use of possible alignments is
consistently triggered by words that are hard to
align.
The results for all cross-classifications of the
four properties ? 1 : 1, source-side island-free
(IF(s)), target-side island-free (IF(t)) and sure-
possible sorted (SP) ? are presented in the table in
Figure 1.3 Note that all (24 ? 1 = 15) combina-
tions of the four properties lead to NP-hard align-
ment problems for (2,2)-BRCGs. Consequently,
3One of our reviewers remarks that the Figure 1 is ?artifi-
cially blown up?, since all combinations have the same com-
plexity. It cannot really be left out, however. The numbers in
the figure?s left-most column serves as a reference in the proofs
below. Since the 15 results derive from only four proofs, it is
convenient to have a short-hand notation for the decision prob-
lems.
while the unrestricted alignment problem for (2,2)-
BRCGs can be solved in O(n6|G|), the alignment
problem turns NP-hard as soon as restrictions are put
on the alignments sought. So the extra expressivity
of (2,2)-BRCGs in a way comes at the expense of
control over the kind of alignments obtained.
On the structure of the paper: Sect. 2 and 3 briefly
introduce, resp., ITGs and (2,2)-BRCGs. Sect. 4
presents three NP-hardness proofs from which the
15 results in Figure 1 can be derived. The three
proofs are based on reconstructions of the Hamilton
circuit problem, the 3SAT problem and the vertex
cover problem (Garey and Johnson, 1979).
2 Inversion transduction grammars
Inversion transduction grammars (ITGs) (Wu, 1997)
are a notational variant of binary syntax-directed
translation schemas (Aho and Ullman, 1972) and are
usually presented with a normal form:
A ? [BC]
A ? ?BC?
A ? e | f
A ? e | ?
A ? ? | f
where A,B,C ? N and e, f ? T . The
first production rule, intuitively, says that the sub-
tree [[]B []C ]A in the source language translates into
62
a subtree [[]B []C ]A, whereas the second produc-
tion rule inverts the order in the target language,
i.e. [[]C []B ]A. The universal recognition problem of
ITGs can be solved in time O(n6|G|) by a CYK-
style parsing algorithm with two charts.
Figure 1 tells us that all the restricted alignment
problems listed can be solved in time O(n6|G|).
The explanation is simple. It can be read off from
the syntactic form of the production rules in ITGs
whether they introduce 1 : 1-alignments, island-free
alignments or sure-possible sorted alignments. Note
that normal form ITGs only induce 1 : 1-alignments.
Consider, for example, the following grammar,
not in normal form for brevity:
(1) S ? ?ASB? | ?AB?
(2) A ? a | a
(3) A ? a | ?
(4) B ? b | b
Note that this grammar recognizes the transla-
tion {?anbn, bnam | n ? m}. To check if for a
string pair ?w1 . . . wn, v1 . . . vm? this grammar in-
duces an island-free alignment, simply remove pro-
duction rule (3). It holds that only strings in the sub-
language {?anbn, bnan | n ? 1} induce island-free
alignments. Similarly, to check if the grammar in-
duces source-side island-free alignments for string
pairs, no production rules will have to be removed.
3 Two-variable binary bottom-up
non-erasing range concatenation
grammars
(2,2)-BRCGs are positive RCGs (Boullier, 1998)
with binary start predicate names, i.e. ?(S) = 2. In
RCG, predicates can be negated (for complementa-
tion), and the start predicate name is typically unary.
The definition is changed only for aesthetic rea-
sons; a positive RCG with a binary start predicate
name S is turned into a positive RCG with a unary
start predicate name S? simply by adding a clause
S?(X1X2) ? S(X1,X2).
A positive RCG is a 5-tuple G = ?N,T, V, P, S?.
N is a finite set of predicate names with an arity
function ?: N ? N, T and V are finite sets of, resp.,
terminal and variables. P is a finite set of clauses of
the form ?0 ? ?1 . . . ?m, where each of the ?i, 0 ?
i ? m, is a predicate of the form A(?1, . . . , ??(A)).
Each ?j ? (T ?V )?, 1 ? j ? ?(A), is an argument.
S ? N is the start predicate name with ?(S) = 2.
Note that the order of RHS predicates in a clause
is of no importance. Three subclasses of RCGs are
introduced for further reference: An RCG G =
?N,T, V, P, S? is simple iff for all c ? P , it holds
that no variable X occurs more than once in the
LHS of c, and if X occurs in the LHS then it oc-
curs exactly once in the RHS, and each argument
in the RHS of c contains exactly one variable. An
RCGG = ?N,T, V, P, S? is a k-RCG iff for all A ?
N, ?(A) ? k. Finally, an RCG G = ?N,T, V, P, S?
is said to be bottom-up non-erasing iff for all c ? P
all variables that occur in the RHS of c also occur in
its LHS.
A positive RCG is a (2,2)-BRCG iff it is a 2-RCG,
if an argument of the LHS predicate contains at most
two variables, and if it is bottom-up non-erasing.
The language of a (2,2)-BRCG is based
on the notion of range. For a string pair
?w1 . . . wn, vn+2 . . . vn+1+m? a range is a pair
of indices ?i, j? with 0 ? i ? j ? n or
n < i ? j ? n + 1 + m, i.e. a string span,
which denotes a substring wi+1 . . . wj in the source
string or a substring vi+1 . . . vj in the target string.
Only consequtive ranges can be concatenated into
new ranges. Terminals, variables and arguments
in a clause are bound to ranges by a substitution
mechanism. An instantiated clause is a clause in
which variables and arguments are consistently
replaced by ranges; its components are instantiated
predicates. For example A(?g . . . h?, ?i . . . j?) ?
B(?g . . . h?, ?i + 1 . . . j ? 1?) is an instantiation
of the clause A(X1, aY1b) ? B(X1, Y1) if the
target string is such that vi+1 = a and vj = b.
A derive relation =? is defined on strings of
instantiated predicates. If an instantiated predicate
is the LHS of some instantiated clause, it can be
replaced by the RHS of that instantiated clause. The
language of a (2,2)-BRCG G = ?N,T, V, P, S? is
the set L(G) = {?w1 . . . wn, vn+2 . . . vn+1+m? |
S(?0, n?, ?n + 1, n + 1 + m?) ?=? ?}, i.e. an
input string pair ?w1 . . . wn, vn+2 . . . vn+1+m? is
recognized iff the empty string can be derived from
S(?0, n?, ?n + 1, n + 1 +m?).
It is not difficult to see that ITGs are also (2,2)-
BRCGs. The left column is ITG production rules;
63
the right column their translations in simple (2,2)-
BRCGs.
A? [BC] A(X1X2, Y1Y2) ? B(X1, Y1)C(X2, Y2)
A? ?BC? A(X1X2, Y1Y2) ? B(X1, Y2)C(X2, Y1)
A? e | f A(e, f) ? ?
A? e | ? A(e, ?) ? ?
A? ? | f A(?, f) ? ?
Consequently, (2,2)-BRCGs recognize all trans-
lations recognized by ITGs. In fact the inclusion is
strict, as shown in S?gaard (2008). The universal
recognition problem of (2,2)-BRCGs can be solved
in time O(n6|G|) by the CYK-style parsing algo-
rithm presented in S?gaard (2008).
Example 3.1. Consider the (2,2)-BRCG G =
?{Ss, S0, S?0, S1, S?1, A,B,C,D}, {a, b, c, d}, {X1 ,
X2, Y1, Y2}, P, Ss? with P the following set of
clauses:
(1) Ss(X1, Y1) ? S0(X1, Y1)S?0(X1, Y1)
(2) S0(X1X2, Y1) ? S1(X1, Y1)D(X2)
(3) S1(aX1c, abY1) ? S1(X1, Y1)
(4) S1(X1, Y1Y2) ? B(X1)C(Y1)D(Y2)
(5) S?0(X1X2, Y1) ? S?1(X2, Y1)A(X1)
(6) S?1(bX1d, Y1cd) ? S?1(X1, Y1)
(7) S?1(X1, Y1Y2) ? C(X1)A(Y1)B(Y2)
(8) A(aX1) ? A(X1)
(9) A(?) ? ?
(10) B(bX1) ? B(X1)
(11) B(?) ? ?
(12) C(cX1) ? C(X1)
(13) C(?) ? ?
(14) D(dX1) ? D(X1)
(15) D(?) ? ?
The string pair ?abbcdd, abcdcd? is derived:
Ss(?0, 6?, ?0, 6?)
=? S0(?0, 6?, ?0, 6?)S?0(?0, 6?, ?0, 6?) (1)=? S1(?0, 4?, ?0, 6?)D(?4, 6?) (2)
S?0(?0, 6?, ?0, 6?)=? S1(?0, 4?, ?0, 6?)S?0(?0, 6?, ?0, 6?) (14?15)=? S1(?1, 3?, ?2, 6?)S?0(?0, 6?, ?0, 6?) (3)=? B(?1, 3?)C(?2, 4?)D(?4, 6?) (4)
S?0(?0, 6?, ?0, 6?)=? S?0(?0, 6?, ?0, 6?) (10?15)=? S?1(?1, 6?, ?0, 6?)A(?0, 1?) (5)=? S?1(?1, 6?, ?0, 6?) (8?9)=? S?1(?2, 5?, ?0, 4?) (6)=? S?1(?3, 4?, ?0, 2?) (6)=? C(?3, 4?)A(?0, 1?)B(?1, 2?) (7)
=? ? (8?13)
Note that L(G) = {?anbmcndm, (ab)n(cd)m? |
m,n ? 0}.
4 Results
4.1 Checking island-freeness and sure-possible
sortedness
One possible way to check for island-freeness and
sure-possible sortedness in the context of (2,2)-
BRCGs is to augment the CYK-style algorithm with
feature structures (Boolean vectors); all there is
needed, e.g. to check sure-possible sortedness, is to
pair up the nonterminals inserted in the cells of the
chart with a flat feature structure of the form:
?
???
SURE1 val1
.
.
.
SUREn valn
?
???
where n is the length of the source, resp. tar-
get, string in the source, resp. target, chart, and
1 ? i ? n : val i ? {+,?}. When a clause ap-
plies that induces a sure alignment between a word
wi and some word in the target, resp. source, string,
the attribute SUREi is assigned the value +; if a pos-
sible alignment is induced between wi and another
word, the attribute is assigned the value -. This can
all be done in constant time. A copying clause now
checks if the appropriate nonterminals have been in-
serted in the cells in question, but also that the as-
sociated feature structures unify. This can be done
in linear time. Feature structures can be used the
same way to record what words have been aligned to
check island-freeness. Unfortunately, this technique
does not guarantee polynomial runtime. Note that
there can be 2n many distinct feature structures for
each nonterminal symbol in a chart. Consequently,
whereas the size of a cell in the standard CYK algo-
rithm is bounded by |N |, and in synchronous parsing
by |N |? (2n? 1),4 the cells are now of exponential
size in the worst case.
The following three sections provide three NP-
hardness proofs: The first shows that the alignment
4The indices used to check that two nonterminals are derived
simultaneously (S?gaard, 2008) mean that it may be necessary
within a cell in the source, resp. target, chart to keep track of
multiple tuples with the same nonterminals. In the worst case,
there is a nonterminal for each span in the target, resp. source,
chart, i.e. 2n? 1 many.
64
problem wrt. 1 : 1-alignments is NP-hard for (2,2)-
BRCGs and goes by reduction of the Hamilton cir-
cuit problem for directed connected graphs. The sec-
ond shows that the alignment problem wrt. source-
or target-side island-free and sure-possible sorted
alignments is NP-hard for (2,2)-BRCGs and goes
by 3SAT reduction. The third proof is more general
and goes by reduction of the vertex cover problem.
All three formal decision problems are discussed in
detail in Garey and Johnson (1979). All 15 results
in Figure 1 are derived from modifications of these
proofs.
4.2 NP-hardness of the 1 : 1 restriction for
(2,2)-BRCGs
Theorem 4.1. The alignment problem wrt. 1 : 1-
alignments is NP-hard for (2,2)-BRCGs.
Proof. An instance of the Hamilton circuit problem
for directed connected graphs is simply a directed
connected graph G = ?V,E? and the problem is
whether there is a path that visits each vertex exactly
once and returns to its starting point? Consider, for
instance, the directed connected graph:
1 2
3
4 5
It is easy to see that there is no path in this case
that visits each vertex exactly once and returns to its
starting point. The intuition behind our reconstruc-
tion of the Hamilton circuit problem for directed
connected graphs is to check this via alignments be-
tween a sequence of all the vertices in the graph and
itself. The grammar permits an alignment between
two wordsw|v if there is a directed edge between the
corresponding nodes in the graph, e.g. (w, v) ? E.
The alignment structures below depict the possible
alignments induced by the grammar obtained by the
translation described below for our example graph:
1 2 3 4 5
1 2 3 4 5
1 2 3 4 5
1 2 3 4 5
1 2 3 4 5
1 2 3 4 5
1 2 3 4 5
1 2 3 4 5
Since no alignment above is 1 : 1, there is no
solution to the corresponding circuit problem. The
translation goes as follows:
? Add a rule S(X1, Y1) ? {Svi(X1, Y1) |
?vi.?vj.(vi, vj) ? E}.
? For each edge (vi, vj) ? E add
a rule Svi(X1viX2, Y1vjY2) ?
?(X1)?(X2)?(X3)?(X4).5
? For all vi ? V add a rule ?(viX1) ? ?(X1).
? Add a rule ?(?) ? ?.
The grammar ensures source-side island-freeness,
and therefore if there exists a 1 : 1-alignment of any
linearization of V and itself, by connectivity of the
input graph, there is a solution to the Hamilton cir-
cuit problem for directed connected graphs.
4.3 NP-hardness of island-freeness and
sure-possible sortedness for (2,2)-BRCGs
Theorem 4.2. The alignment problem wrt. target-
side island-free and sure-possible sorted alignments
is NP-hard for (2,2)-BRCGs.
Proof. An instance of the 3SAT problem is a propo-
sitional logic formula ? that is a conjunction of
clauses of three literals connected by disjunctions,
and the problem whether this formula is satisfiable,
i.e. has a model? Say ? = p?q?r?p??q??r?. For our
reconstruction, we use the propositional variables
in ? as source string, and ? itself with ??s omitted
and conjuncts as words as the target string. One of
the representations of a solution constructed by the
translation described below is the following align-
ment structure:
p q r
p ? q ? r p? ? q? ? r?
Solid lines are sure alignments; dotted lines are
possible alignments. The intuition is to use sure
alignments to encode true assignments, and possi-
ble alignments as false assignments. The alignment
5? is an arbitrary predicate name chosen to reflect the fact
that all possible substrings over the vocabulary are recognized
by the ? predicates.
65
above thus corresponds to the model {p, r?}, which
clearly satisfies ?.
For the translation, assume that each 3SAT in-
stance, over a set of propositional variables PROP,
consists of a set of clauses c1 . . . cm that are sets of
literals of size 3. For any literal lj , if lj = p?j then
pos(lj) = pj and lit(lj) = ?; and if lj = pj then
pos(lj) = pj and lit(lj) = +. If lj is a literal in
ci, we write lj ? ci. First add the following four
clauses:
Ss(X1, Y1) ? Ss(X1, Y1) | Sp(X1, Y1)
Sp(X1, Y1) ? Ss(X1, Y1) | Sp(X1, Y1)
? If lj ? ci and lit(lj) = ?, add
Sp(X1pos(lj)X2, Y1ciY2) ? ?(X1)?(X2)
?(Y1)?(Y2) .
? If lj ? ci and lit(lj) = +, add
Ss(X1pos(lj)X2, Y1ciY2) ? ?(X1)?(X2)
?(Y1)?(Y2) .
? For all pj , add ?(pjX1) ? ?(X1).
? For all ci, add ?(ciX1) ? ?(X1).
? Add a rule ?(?) ? ?.
It is easy to see that the first rule adds at most
7m clauses, which for the largest non-redundant
formulas equals 7((2|PROP|)3). The second rule
adds at most 2|PROP| clauses; and the third at most
m ? (2|PROP|)3 clauses. It is also easy to see that
the grammar induces a target-side island-free, sure-
possible sorted alignment if and only if the 3SAT in-
stance is satisfiable. Note that the grammar does not
guarantee that all induced alignments are target-side
island-free. Nothing, in other words, corresponds
to conjunctions in our reconstruction. This is not
necessary as long as there is at least one target-side
island-free alignment that is induced.
Note that the proof also applies in the case where
it is the source side that is required to be island-free.
All needed is to make the source string the target
string, and vice versa. Note also that the proof can
be modified for the case where both sides are island-
free: Just add a dummy symbol to the clause side
and allow (or force) all propositional variables to
be aligned to this dummy symbol. Consequently, if
there is a target-side (clause-side) island-free align-
ment there is also an island-free alignment. Re-
versely, if there is an island-free alignment there is
also a target-side island-free alignment of the string
pair in question.
Note also that a more general proof can be ob-
tained by introducing a clause, similar to the clause
introduced in the first bullet point of the Hamil-
ton circuit reduction in the proof of Theorem 4.1:
S(X1, Y1) ? {Sci(X1, Y1) | 1 ? i ? m}. The
four rules used to change between sure and pos-
sible alignments then of course need to be copied
out for all Sci predicates, and the LHS predicates,
except ?, of the other clauses must be properly
subscripted. Now the grammar enforces target-
side island-freeness, and sure-possible sortedness is
the only restriction needed on alignments. Conse-
quently, this reduction proves (4) that the alignment
problem wrt. sure-possible sortedness is NP-hard for
(2,2)-BRCGs.
4.4 NP-hardness of island-freeness for
(2,2)-BRCGs
Theorem 4.3. The alignment problem wrt. island-
free alignments is NP-hard for (2,2)-BRCGs.
Proof. An instance of the vertex problem is a graph
D = ?V,E? and an integer k, and the prob-
lem whether there exists a vertex cover of D of
size k? Say D = ?V = {a, b, c, d}, E =
{(a, c), (b, c), (b, d), (c, d)}? and k = 2. The trans-
lation described below constructs a sentence pair
??1?2?3?4uu????, aaaabbbbccccdddd?
for this instance, and a (2,2)-BRCG with the
clauses in Figure 2. Note that there are four kinds
of clauses:
? A clause with an S predicate in the LHS. In
general, there will be one such clause in the
grammar constructed for any instance of the
vertex cover problem.
? 8 clauses with ?i predicates in the LHS. In gen-
eral, there will be 2|E| many clauses of this
form in the grammars.
? 8 clauses withU i predicates in the LHS. In gen-
eral, there will be |V |? (|V |?k) many clauses
of this form in the grammars.
66
? 16 clauses with ?1 predicates in the LHS. In
general, there will be (|E| ? |V | ? |E| ? |E| ?
(|V | ? k)) ? |V | many clauses of this form in
the grammars.
For an instance ?D = ?V,E?, k?, the translation
function in general constructs the following clauses:
S(X1, Y1) ? {?i(X1, Y1) | 1 ? i ? |E|}?
{U |V |?k(X1, Y1)}?
{?|E|?|V |?|E|?|E|?(|V |?k)(X1, Y1)}
and for all 1 ? i ? |E| iff ei ? E = (e, e?):
?i(X1?iX2, Y1eY2) ? ?(X1)?(X2)?(Y1)?(Y2)
?i(X1?iX2, Y1e?Y2) ? ?(X1)?(X2)?(Y1)?(Y2)
For all 2 ? i ? |V | ? k and for all v ? V :
U i(X1UX2, Y1v . . . vY2) ? U i?1(X1, Y1)
?(X2)?(Y2)
where |v . . . v| = |E|. For the case U1, add the
clauses for all v ? V :
U1(X1UX2, Y1v . . . vY2) ? ?(X1)?(Y1)
?(X2)?(Y2)
The string pair is constructed this way:
??1 . . . ?|E|U1 . . . U|V |?k
?1 . . . ?|E|?|V |?|E|?|E|?(|V |?k), ??
Finally, for all words w in this string pair, add:
?(wX1) ? ?(X1)
Since this translation is obviously polynomial, it
follows that the alignment problem wrt. island-free
alignments for (2,2)-BRCGs is NP-hard.
Note that the proof also applies if only the source,
resp. target, side is required to be island-free, since
the grammar restricts the alignments in a way such
that if one side is island-free then so is the other side.
This gives us results (2) and (3).
It is not difficult to see either that it is possible
to convert the grammar into a grammar that induces
1 : 1-alignments. This gives us results (5), (8) and
(11). Of course by the observation that all the gram-
mars only use sure alignments, it follows that the
alignment problems in (7), (9?10) and (12?15) are
also NP-hard.
5 Conclusion
The universal recognition problems of both ITGs
and (2,2)-BRCGs can be solved in time O(n6|G|).
This may come as a surprise, as ITGs restrict the
alignment space considerably, while (2,2)-BRCGs
induce all possible alignments. In the context of
the NP-hardness of decoding in statistical machine
translation (Knight, 1999; Udupa and Maji, 2006),
it is natural to ask why the universal recognition
problem of (2,2)-BRCGs isn?t NP-hard? This pa-
per bridges the gap between these results and shows
that when alignments are restricted to be 1 : 1,
island-free or sure-possible sorted, or all combi-
nations thereof, the alignment problem of (2,2)-
BRCGs is NP-hard. Consequently, while the un-
restricted alignment problem for (2,2)-BRCGs can
be solved in O(n6|G|), the alignment problem turns
NP-hard as soon as restrictions are put on the align-
ments sought. So the extra expressivity in a way
comes at the expense of control over the kind of
alignments obtained. Note also that an alignment
of two words may be enforced multiple times in a
(2,2)-BRCGs parse, since two derivation trees that
share leaves on both sides can align the same two
words.
Our results are not intended to be qualifications of
the usefulness of (2,2)-BRCGs (S?gaard, 2008), but
rather they are attempts to bridge a gap in our under-
standing of the synchronous grammar formalisms at
hand to us in syntax-based machine translation.
67
S(X1, Y1) ? ?1(X1, Y1)?2(X1, Y1)
?3(X1, Y1)?4(X1, Y1)
U2(X1, Y1)?4(X1, Y1)
?1(X1?1X2, Y1aY2) ? ?(X1)?(X2)?(Y1)?(Y2)
?1(X1?1X2, Y1cY2) ? ?(X1)?(X2)?(Y1)?(Y2)
. . .
U2(X1UX2, aaaaY1) ? U1(X1, Y1)?(X2)
U1(X1UX2, Y1bbbbY2) ? ?(X1)?(Y1)?(X2)?(Y2)
U2(X1UX2, Y1bbbbY2) ? U1(X1, Y1)?(X2)?(Y2)
. . .
?4(X1?X2, Y1aY2) ? ?3(X1, Y1)?(X2)?(Y2)
?4(X1?X2, Y1bY2) ? ?3(X1, Y1)?(X2)?(Y2)
. . .
Figure 2: A (2,2)-BRCG for the instance of the vertex cover problem ??{a, b, c, d}, {(a, c), (b, c), (b, d), (c, d)}?, 2?.
References
Alfred Aho and Jeffrey Ullman. 1972. The theory
of parsing, translation and compiling. Prentice-Hall,
London, England.
Edward Barton, Robert Berwick, and Erik Ristad. 1987.
Computational complexity and natural language. MIT
Press, Cambridge, Massachusetts.
Pierre Boullier. 1998. Proposal for a natural language
processing syntactic backbone. Technical report, IN-
RIA, Le Chesnay, France.
Michael Garey and David Johnson. 1979. Computers
and intractability. W. H. Freeman & Co., New York,
New York.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607?615.
Dan Melamed. 1999. Bitext maps and alignment
via pattern recognition. Computational Linguistics,
25(1):107?130.
Anders S?gaard. 2008. Range concatenation gram-
mars for translation. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics, Companion Volume, pages 103?106, Manchester,
England.
Raghavendra Udupa and Hemanta Maji. 2006. Compu-
tational complexity of statistical machine translation.
In Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 25?32, Trento, Italy.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Hao Zhang and Daniel Gildea. 2004. Syntax-based
alignment: supervised or unsupervised? In Proceed-
ings of the 20th International Conference on Compu-
tational Linguistics, pages 418?424, Geneva, Switzer-
land.
68
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 33?36,
Paris, October 2009. c?2009 Association for Computational Linguistics
Empirical lower bounds on translation unit error rate for the full class of
inversion transduction grammars
Anders S?gaard
Center for Language Technology
University of Copenhagen
soegaard@hum.ku.dk
Dekai Wu
Human Language Technology Center
Hong Kong Univ. of Science and Technology
dekai@cs.ust.hk
Abstract
Empirical lower bounds studies in which
the frequency of alignment configurations
that cannot be induced by a particular for-
malism is estimated, have been important
for the development of syntax-based ma-
chine translation formalisms. The for-
malism that has received most attention
has been inversion transduction grammars
(ITGs) (Wu, 1997). All previous work
on the coverage of ITGs, however, con-
cerns parse failure rates (PFRs) or sen-
tence level coverage, which is not di-
rectly related to any of the evaluation mea-
sures used in machine translation. S?gaard
and Kuhn (2009) induce lower bounds on
translation unit error rates (TUERs) for a
number of formalisms, incl. normal form
ITGs, but not for the full class of ITGs.
Many of the alignment configurations that
cannot be induced by normal form ITGs
can be induced by unrestricted ITGs, how-
ever. This paper estimates the difference
and shows that the average reduction in
lower bounds on TUER is 2.48 in absolute
difference (16.01 in average parse failure
rate).
1 Introduction
The first stage in training a machine translation
system is typically that of aligning bilingual text.
The quality of alignments is in that case of vi-
tal importance to the quality of the induced trans-
lation rules used by the system in subsequent
stages. In string-based statistical machine trans-
lation, the alignment space is typically restricted
by the n-grams considered in the underlying lan-
guage model, but in syntax-based machine trans-
lation the alignment space is restricted by very
different and less transparent structural contraints.
While it is easy to estimate the consequences of
restrictions to n-grams of limited size, it is less
trivial to estimate the consequences of the struc-
tural constraints imposed by syntax-based ma-
chine translation formalisms. Consequently, much
work has been devoted to this task (Wu, 1997;
Zens and Ney, 2003; Wellington et al, 2006;
Macken, 2007; S?gaard and Kuhn, 2009).
The task of estimating the consequences of
the structural constraints imposed by a particular
syntax-based formalism consists in finding what is
often called ?empirical lower bounds? on the cov-
erage of the formalism (Wellington et al, 2006;
S?gaard and Kuhn, 2009). Gold standard align-
ments are constructed and queried in some way
as to identify complex alignment configurations,
or they are parsed by an all-accepting grammar
such that a parse failure indicates that no align-
ment could be induced by the formalism.
The assumption in this and related work that en-
ables us to introduce a meaningful notion of align-
ment capacity is that simultaneously recognized
words are aligned (Wu, 1997; Zhang and Gildea,
2004; Wellington et al, 2006; S?gaard and Kuhn,
2009). As noted by S?gaard (2009), this defi-
nition of alignment has the advantageous conse-
quence that candidate alignments can be singled
out by mere inspection of the grammar rules. It
also has the consequence that alignments are tran-
sitive (Goutte et al, 2004), since simultaneity is
transitive.
While previous work (S?gaard and Kuhn, 2009)
has estimated empirical lower bounds for normal
form ITGs at the level of translation units (TUER),
or cepts (Goutte et al, 2004), defined as maxi-
mally connected subgraphs in alignments, nobody
has done this for the full class of ITGs. What
is important to understand is that while normal
form ITGs can induce the same class of transla-
tions as the full class of ITGs, they do not induce
the same class of alignments. They do not, for ex-
33
ample, induce discontinuous translation units (see
Sect. 3). Sect. 2 briefly presents some related re-
sults in the literature. Some knowledge about for-
malisms used in machine translation is assumed.
2 Related work
Aho and Ullman (1972) showed that 4-ary syn-
chronous context-free grammars (SCFGs) could
not be binarized, and Satta and Peserico (2005)
showed that the hiearchy of SCFGs beyond ternary
ones does not collapse; they also showed that the
complexity of the universal recognition problem
for SCFGs is NP-complete. ITGs on the other
hand has a O(|G|n6) solvable universal recog-
nition problem, which coincides with the unre-
stricted alignment problem (S?gaard, 2009). The
result extends to decoding in conjunction with a
bigram language model (Huang et al, 2005).
Wu (1997) introduced ITGs and normal form
ITGs. ITGs are a notational variant of the sub-
class of SCFGs such that all indexed nonterminals
in the source side of the RHS occur in the same
order or exactly in the inverse order in the target
side of the RHS. It turns out that this subclass of
SCFGs defines the same set of translations that can
be defined by binary SCFGs. The different forms
of production rules are listed below with the more
restricted normal form production rules in the right
column, with ? ? (N ?{e/f | e ? T ?, f ? T ?})?
(N nonterminals and T terminals, as usual). The
RHS operator [ ] preserves source language con-
stituent order in the target language, while ? ? re-
verses it.1
A ? [?] A ? [BC]
A ? ??? A ? ?BC?
A ? e/f
Several studies have adressed the alignment ca-
pacity of ITGs and normal form ITGs. Zens and
Ney (2003) induce lower bounds on PRFs for
normal form ITGs. Wellington et al (2006) in-
duce lower bounds on PRFs for ITGs. S?gaard
and Kuhn (2009) induce lower bounds on TUER
for normal form ITGs and more expressive for-
malisms for syntax-based machine translation. No
one has, however, to the best our knowledge in-
duced lower bounds on TUER for ITGs.
1One reviewer argues that our definition of full ITGs is
not equivalent to the definition in Wu (1997), which, in the
reviewer?s words, allows ?at most one lexical item from each
language?. Sect. 6 of Wu (1997), however, explicitly encour-
ages lexical elements in rules to have more than one lexical
item in many cases.
3 Experiments
As already mentioned empirical lower bounds
studies differ in four important respects, namely
wrt.: (i) whether they use hand-aligned or auto-
matically aligned gold standards, (ii) the level at
which they count failures, e.g. sentence, align-
ment or translation unit level, (iii) whether they
interpret translation units disjunctively or conjunc-
tively, and (iv) whether they induce the lower
bounds (a) by running an all-accepting grammar
on the gold standard data, (b) by logical charac-
terization of the structures that can be induced by
a formalism, or (c) by counting the frequency of
complex alignment configurations. The advantage
of (a) and (b) is that they are guaranteed to find the
highest possible lower bound on the gold standard
data, whereas (c) is more modular (formalism-
independent) and actually tells us what configu-
rations cause trouble.
(i) In this study we use hand-aligned gold stan-
dard data. It should be obvious why this is prefer-
able to automatically aligned data. The only rea-
son that some previous studies used automatically
aligned data is that hand-aligned data are hard to
come by. This study uses the data also used by
S?gaard and Kuhn (2009), which to the best of
our knowledge uses the largest collection of hand-
aligned parallel corpora used in any of these stud-
ies. (ii) Failures are counted at the level of trans-
lation units as argued for in the above, but sup-
plemented by parse failure rates for completeness.
(iii) Since we count failures at the level of transla-
tion units, it is natural to interpret them conjunc-
tively. Otherwise we would in reality count fail-
ures at the level of alignments. (iv) We use (c).
The conjunctive interpretation of translation
units was also adopted by Fox (2002) and is mo-
tivated by the importance of translation units and
discontinuous ones in particular to machine trans-
lation in general (Simard and colleagues, 2005;
Ayan and Dorr, 2006; Macken, 2007; Shieber,
2007). In brief,
TUER = 1 ? 2|SU ?GU ||SU |+ |GU |
where GU are the translation units in the gold stan-
dard, and SU the translation units produced by
the system. This evaluation measure is related to
consistent phrase error rate (CPER) introduced in
Ayan and Dorr (2006), except that it does not only
consider contiguous phrases.
34
3.1 Data
The characteristics of the hand-aligned gold stan-
dard parallel corpora used are presented in Fig-
ure 1. The Danish-Spanish text is part of
the Copenhagen Dependency Treebank (Parole),
English-German is from Pado and Lapata (2006)
(Europarl), and the six combinations of English,
French, Portuguese and Spanish are documented
in Graca et al (2008) (Europarl).
3.2 Alignment configurations
The full class of ITGs induces many alignment
configurations that normal form ITGs do not in-
duce, incl. discontinuous translation units (DTUs),
i.e. translation units with at least one gap, double-
sided DTUs, i.e. DTUs with both a gap in the
source side and a gap in the target side, and multi-
gap DTUs with arbitrarily many gaps (as long as
the contents in the gap are either respect the linear
order of the source side or the inverted order).
ITGs do not induce (i) inside-out alignments,
(ii) cross-serial DTUs, (iii) what is called the ?bon-
bon? configuration below, and (iv) multigap DTUs
with mixed order in the target side. The reader is
referred to Wu (1997) for discussion of inside-out
alignments. (ii) and (iii) are explained below.
3.2.1 Induced configurations
DTUs are easily induced by unrestricted ITG pro-
ductions, while they cannot be induced by pro-
ductions in normal form. The combination of the
production rules A ? [?/ne B nothing/pas] and
B ? [change/modifie], for example, induces a
DTU with a gap in the French side for the pair of
substrings ?change nothing, ne modifie pas?.
Multigap DTUs with up to three gaps are fre-
quent (S?gaard and Kuhn, 2009) and have shown
to be important for translation quality (Simard and
colleagues, 2005). While normal form ITGs do
not induce multigap DTUs, ITGs induce a partic-
ular subclass of multigap DTUs, namely those that
are constructed by linear or inverse interpolation.
3.2.2 Non-induced configurations
Inside-out alignments were first described by
Wu (1997), and their frequency has been a mat-
ter of some debate (Lepage and Denoual, 2005;
Wellington et al, 2006; S?gaard and Kuhn, 2009).
Cross-serial DTUs are made of two DTUs non-
contiguous to the same side such that both have
material in the gap of each other. Bonbons are
similar, except the DTUs are non-contiguous to
different sides, i.e. D has a gap in the source side
that contains at least one token in E, and E has
a gap in the target side that contains at least one
token in D. Here?s an example of a bonbon con-
figuration from Simard et al (2005):
Pierre ne mange pas
Pierre does not eat
Multigap DTUs with mixed transfer are, as al-
ready mentioned multigap DTUs with crossing
alignments from material in two distinct gaps.
3.3 Results
The lower bounds on TUER for the full class of
ITGs are obtained by summing the ratios of inside-
out alignments, cross-serial DTUs, bonbons and
mixed order multigap DTUs, subtracting any over-
lap between these classes of configurations. The
lower bounds on TUER for normal form ITGs
sum ratios of inside-out aligments and DTUs sub-
tracting any overlap. Figure 1 presents the ratio
(?100), and Figure 2 presents the induced lower
bounds on the full class of ITGs and normal form
ITGs. Any two configurations differ on all trans-
lation units in order to count as two distinct con-
figurations in these statistics. Otherwise a single
translation unit could be removed to simplify two
or more configurations.
4 Discussion
The usefulness of alignment error rate (AER) (Och
and Ney, 2000) has been questioned lately (Fraser
and Marcu, 2007); most importantly, AER does
not always seem to correlate with translation qual-
ity. TUER is likely to correlate better with transla-
tion quality, since it by definition correlates with
CPER (Ayan and Dorr, 2006). No large-scale
experiment has been done yet to estimate the
strength of this correlation.
Our study also relies on the assumption that
simulatenously recognized words are aligned in
bilingual parsing. The relationship between pars-
ing and alignment can of course be complicated in
ways that will alter the alignment capacity of ITG
and its normal form; on some definitions the two
formalisms may even become equally expressive.
5 Conclusion
It was shown that the absolute reduction in average
lower bound on TUER is 2.48 for the full class of
ITGs over its canonical normal form. For PRF, it
is 16.01.
35
Snts TUs IOAs DTUs CDTUs Bonbons MIX-DTUs
Da-Sp 926 6441 0.56 9.16 0.81 0.16 0.23
En-Fr 100 869 0.23 2.99 0.12 0.23 0.23
En-Ge 987 17354 1.75 5.55 0.45 0.05 0.79
En-Po 100 783 0.26 2.17 0.00 0.00 0.38
En-Sp 100 831 0.48 1.32 0.00 0.00 0.36
Po-Fr 100 862 0.23 3.13 0.58 0.00 0.46
Po-Sp 100 882 0.11 0.90 0.00 0.00 0.00
Sp-Fr 100 914 0.11 2.95 0.55 0.00 0.22
Figure 1: Characteristics of the parallel corpora and frequency of configurations ( nTUs ? 100).
ITGs NF-ITGs
LB-TUER LB-PFR Ovlp(TUs) Ovlp(Snts) LB-TUER PFR Ovlp(TUs) Ovlp(Snts)
Da-Sp 1.58 10.37 11 10 8.54 40.50 76 32
En-Fr 0.69 6.00 1 1 2.88 22.00 3 2
En-Ge 2.75 47.32 49 42 5.24 69.30 357 236
En-Po 0.64 5.00 0 0 2.43 19.00 0 0
En-Sp 0.84 7.00 0 0 1.80 15.00 0 0
Po-Fr 1.04 9.00 2 2 3.36 24.00 0 0
Po-Sp 0.11 1.00 1 1 0.90 8.00 1 1
Sp-Fr 0.77 7.00 1 1 3.06 23.00 0 0
AV 1.05 11.59 3.53 27.60
Figure 2: Induced lower bounds for ITGs and normal form ITGs (NF-ITGs). LB-TUER lists the lower bounds on TUER.
LB-PFR lists the lower bounds on parse failure rates. Finally, the third and fourth columns list configuration overlaps at the
level of translation units, resp. sentences.
References
Alfred Aho and Jeffrey Ullman. 1972. The theory of parsing,
translation and compiling. Prentice-Hall.
Necip Ayan and Bonnie Dorr. 2006. Going beyond AER. In
COLING-ACL?06, Sydney, Australia.
Heidi Fox. 2002. Phrasal cohesion and statistical machine
translation. In EMNLP?02, Philadelphia, PA.
Alexander Fraser and Daniel Marcu. 2007. Measuring word
alignment quality for statistical machine translation. Com-
putational Linguistics, 33(3):293?303.
Cyril Goutte, Kenji Yamada, and Eric Gaussier. 2004.
Aligning words using matrix factorisation. In ACL?04,
Barcelona, Spain.
Joao Graca, Joana Pardal, Lu??sa Coheur, and Diamantino
Caseiro. 2008. Building a golden collection of paral-
lel multi-language word alignments. In LREC?08, Mar-
rakech, Morocco.
Liang Huang, Hao Zhang, and Daniel Gildea. 2005. Ma-
chine translation as lexicalized parsing with hooks. In
IWPT?05, pages 65?73, Vancouver, BC.
Yves Lepage and Etienne Denoual. 2005. Purest ever
example-based machine translation. Machine Translation,
19(3?4):251?282.
Lieve Macken. 2007. Analysis of translational correspon-
dence in view of sub-sentential alignment. In METIS-II,
pages 9?18, Leuven, Belgium.
Franz Och and Hermann Ney. 2000. A comparison of align-
ment models for statistical machine translation. In COL-
ING?00, Saarbru?cken, Germany.
Sebastian Pado? and Mirella Lapata. 2006. Optimal con-
stituent alignment with edge covers for semantic projec-
tion. In ACL-COLING?06, Sydney, Australia.
Giorgio Satta and Enoch Peserico. 2005. Some compu-
tational complexity results for synchronous context-free
grammars. In HLT-EMNLP?05, Vancouver, BC.
Stuart Shieber. 2007. Probabilistic synchronous tree-
adjoining grammars for machine translation. In SSST?07,
pages 88?95, Rochester, NY.
Michel Simard and colleagues. 2005. Translating with non-
contiguous phrases. In HLT-EMNLP?05, Vancouver, BC.
Anders S?gaard and Jonas Kuhn. 2009. Empirical lower
bounds on alignment error rates in syntax-based machine
translation. In NAACL-HLT?09, SSST-3, Boulder, CO.
Anders S?gaard. 2009. On the complexity of alignment
problems in two synchronous grammar formalisms. In
NAACL-HLT?09, SSST-3, Boulder, CO.
Benjamin Wellington, Sonjia Waxmonsky, and Dan
Melamed. 2006. Empirical lower bounds on the com-
plexity of translational equivalence. In ACL?06, pages
977?984, Sydney, Australia.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Computa-
tional Linguistics, 23(3):377?403.
Richard Zens and Hermann Ney. 2003. A comparative study
on reordering constraints in statistical machine translation.
In ACL?03, Sapporo, Japan.
Hao Zhang and Daniel Gildea. 2004. Syntax-based align-
ment: supervised or unsupervised? In COLING?04, pages
418?424, Geneva, Switzerland.
36
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 206?209,
Paris, October 2009. c?2009 Association for Computational Linguistics
Using a maximum entropy-based tagger to improve a very fast vine parser
Anders S?gaard
Center for Language Technology
University of Copenhagen
soegaard@hum.ku.dk
Jonas Kuhn
Dpt. of Linguistics
University of Potsdam
kuhn@ling.uni-potsdam.de
Abstract
In this short paper, an off-the-shelf maxi-
mum entropy-based POS-tagger is used as
a partial parser to improve the accuracy of
an extremely fast linear time dependency
parser that provides state-of-the-art results
in multilingual unlabeled POS sequence
parsing.
1 Introduction
The dependency parsing literature has grown in all
directions the past 10 years or so. Dependency
parsing is used in a wide variety of applications,
and many different parsing techniques have been
proposed.
Two dependency parsers have become more
popular than the rest, namely MSTParser (Mc-
Donald et al, 2005) and MaltParser (Nivre et
al., 2007). MSTParser is slightly more accu-
rate than MaltParser on most languages, especially
when dependencies are long and non-projective,
but MaltParser is theoretically more efficient as it
runs in linear time. Both are relatively slow in
terms of training (hours, sometimes days), and rel-
atively big models are queried in parsing.
MSTParser and MaltParser can be optimized for
speed in various ways,1 but the many applications
of dependency parsers today may turn model size
into a serious problem. MSTParser typically takes
about a minute to parse a small standard test suite,
say 2?300 sentences; the stand-alone version of
MaltParser may take 5?8 minutes. Such parsing
times are problematic in, say, a machine transla-
tion system where for each sentence pair multiple
1Recent work has optimized MaltParser considerably for
speed. Goldberg and Elhadad (2008) speed up the MaltParser
by a factor of 30 by simplifying the decision function for the
classifiers. Parsing is still considerably slower than with our
vine parser, i.e. a test suite is parsed in about 15?20 seconds,
whereas our vine parser parses a test suite in less than two
seconds.
target sentences are parsed (Charniak et al, 2003;
Galley and Manning, 2009). Since training takes
hours or days, researchers are also more reluctant
to experiment with new features, and it is very
likely that the features typically used in parsing
are suboptimal in, say, machine translation.
Conceptually simpler dependency parsers are
also easier to understand, which makes debugging,
cross-domain adaption or cross-language adapta-
tion a lot easier. Finally, state-of-the-art depen-
dency parsers may in fact be outperformed by sim-
pler systems on non-standard test languages with,
say, richer morphology or more flexible word or-
der.
Vine parsing is a parsing strategy that guaran-
tees fast parsing and smaller models, but the ac-
curacy of dependency-based vine parsers has been
non-competitive (Eisner and Smith, 2005; Dreyer
et al, 2006).
This paper shows how the accuracy of
dependency-based vine parsers can be improved
by 1?5% across six very different languages with
a very small cost in training time and practically
no cost in parsing time.
The main idea in our experiments is to use
a maximum entropy-based part-of-speech (POS)
tagger to identify roots and tokens whose heads
are immediately left or right of them. These are
tasks that a tagger can solve. You simply read
off a tagged text from the training, resp. test, sec-
tion of a treebank and replace all tags of roots,
i.e. tokens whose syntactic head is an artificial root
node, with a new tag ROOT. You then train on
the training section and apply your tagger on the
test section. The decisions made by the tagger
are then, subsequently, used as hard constraints by
your parser. When the parser then tries to find root
nodes, for instance, it is forced to use the roots as-
signed by the tagger. This strategy is meaningful
if the tagger has better precision for roots than the
parser. If it has better recall than the parser, the
206
parser may be forced to select roots only from the
set of potential roots assigned by the tagger. In our
experiments, only the first strategy was used (since
the tagger?s precision was typically better than its
recall).
The dependency parser used in our experiments
is very simple. It is based on the Chu-Liu-
Edmonds algorithm (Edmonds, 1967), which is
also used in the MSTParser (McDonald et al,
2005), but it is informed only by a simple MLE
training procedure and omits cycle contraction in
parsing. This means that it produces cyclic graphs.
In the context of poor training, insisting on acyclic
output graphs often compromises accuracy by >
10%. On top of this parser, which is super fast but
often does not even outperform a simple structural
baseline, hard and soft constraints on dependency
length are learned discriminatively. The speed of
the parser allows us to repeatedly parse a tuning
section to optimize these constraints. In particular,
the tuning section (about 7500 tokens) is parsed
a fixed number of times for each POS/CPOS tag
to find the optimal dependency length constraint
when that tag is the tag of the head or dependent
word. In general, this discriminative training pro-
cedure takes about 10 minutes for an average-sized
treebank. The parser only produces unlabeled de-
pendency graphs and is still under development.
While accuracy is below state-of-the-art results,
our improved parser significantly outperforms a
default version of the MaltParser that is restricted
to POS tags only, on 5/6 languages (p ? 0.05),
and it significantly outperforms the baseline vine
parser on all languages.
2 Data
Our languages are chosen from different language
families. Arabic is a Semitic language, Czech is
Slavic, Dutch is Germanic, Italian is Romance,
Japanese is Japonic-Ryukyuan, and Turkish is
Uralic. All treebanks, except Italian, were also
used in the CONLL-X Shared Task (Buchholz and
Marsi, 2006). The Italian treebank is the law
section of the TUT Treebank used in the Evalita
2007 Dependency Parsing Challenge (Bosco et al,
2000).
3 Experiments
The Python/C++ implementation of the maximum
entropy-based part-of-speech (POS) tagger first
described in Ratnaparkhi (1998) that comes with
the maximum entropy library in Zhang (2004) was
used to identify arcs to the root node and to tokens
immediately left or right of the dependent. This
was done by first extracting a tagged text from
each treebank with dependents of the root node as-
signed a special tag ROOT. Similarly, tagged texts
were extracted in which dependents of their im-
mediate left, resp. right neighbors, were assigned a
special tag. Our tagger was trained on the texts ex-
tracted from the training sections of the treebanks
and evaluated on the texts extracted from the test
sections. The number of gold standard, resp. pre-
dicted, ROOT/LEFT/RIGHT tags are presented in
Figure 1. Precision and f-score are also computed.
Note that since our parser uses information from
our tagger as hard constraints, i.e. it disregards
arcs to the root node or immediate neighbors not
predicted by our tagger, precision is really what
is important, not f-score. Or more precisely, preci-
sion indicates if our tagger is of any help to us, and
f-score tells us to what extent it may be of help.
4 Results
The results in Figure 2 show that using a maxi-
mum entropy-based POS tagger to identify roots
(ROOT), tokens with immediate left heads (LEFT)
and tokens with immediate (RIGHT) heads im-
proves the accuracy of a baseline vine parser
across the board for all languages measured in
terms of unlabeled attachment score (ULA), or de-
creases are insignificant (Czech and Turkish). For
all six languages, there is a combination of ROOT,
LEFT and RIGHT that significantly outperforms
the vine parser baseline. In 4/6 cases, absolute im-
provements are ? 2%. The score for Dutch is im-
proved by > 4%. The extended vine parser is also
significantly better than the MaltParser restricted
to POS tags on 5/6 languages. MaltParser is prob-
ably better than the vine parser wrt. Japanese be-
cause average sentence length in this treebank is
very short (8.9); constraints on dependency length
do not really limit the search space.
In spite of the fact that our parser only uses POS
tags (except for the maximum entropy-based tag-
ger which considers both words and tags), scores
are now comparable to more mature dependency
parsers: ULA excl. punctuation for Arabic is
70.74 for Vine+ROOT+LEFT+RIGHT which is
better than six of the systems who participated in
the CONLL-X Shared Task and who had access to
all data in the treebank, i.e. tokens, lemmas, POS
207
Arabic Gold Predicted Precision F-score
ROOT 443 394 89.09 83.87
LEFT 3035 3180 84.28 86.24
RIGHT 313 196 82.14 63.26
Czech Gold Predicted Precision F-score
ROOT 737 649 85.36 79.94
LEFT 1485 1384 85.12 82.12
RIGHT 1288 1177 87.51 83.57
Dutch Gold Predicted Precision F-score
ROOT 522 360 74.44 60.77
LEFT 1734 1595 87.02 83.39
RIGHT 1300 1200 87.00 83.52
Italian Gold Predicted Precision F-score
ROOT 100 58 74.36 65.17
LEFT 1601 1640 90.30 91.39
RIGHT 192 129 84.87 74.14
Japanese Gold Predicted Precision F-score
ROOT 939 984 85.06 87.05
LEFT 1398 1382 97.76 97.19
RIGHT 2838 3016 92.27 95.08
Turkish Gold Predicted Precision F-score
ROOT 694 685 85.55 84.99
LEFT 750 699 91.70 88.47
RIGHT 3433 3416 84.19 83.98
Figure 1: Tag-specific evaluation of our tagger on the extracted texts.
Arabic Czech Dutch Italian Japanese Turkish
MaltParser 66.22 67.78 65.03 75.48 89.13 68.94
Vine 67.99 66.70 65.98 75.50 83.15 68.53
Vine+ROOT 68.68 66.65 66.21 78.06 83.82 68.45
Vine+ROOT+LEFT 69.68 68.14 68.05 77.14 84.64 68.37
Vine+RIGHT 68.50 67.38 68.18 78.55 84.17 69.87
Vine+ROOT+RIGHT 69.20 67.32 68.40 78.29 84.78 69.79
Vine+ROOT+LEFT+RIGHT 70.28 68.70 70.06 77.26 85.45 69.74
Figure 2: Labeled attachment scores (LASs) for MaltParser limited to POS tags, our baseline vine parser
(Vine) and our extensions of Vine. Best scores bold-faced.
208
tags, features and dependency relations; not just
the POS tags as in our case. In particular, our re-
sult is 2.28 better than Dreyer et al (2006) who
also use soft and hard constraints on dependency
lengths. They extend the parsing algorithm in Eis-
ner and Smith (2005) to labeled k-best parsing and
use a reranker to find the best parse according to
predefined global features. ULA excl. punctuation
for Turkish is 67.06 which is better than six of the
shared task participants, incl. Dreyer et al (2006)
(60.45).
The improvements come at an extremely low
cost. The POS tagger simply stores its decisions
in a very small table, typically 5?10 cells per sen-
tence, that is queried in no time in parsing. Pars-
ing a standard small test suite takes less than two
seconds, and the cost of the additional look-up is
too small to be measured. The training time of the
maximum entropy-based tagger is typically a mat-
ter of seconds or half a minute. Even running it on
the 1249k Prague Dependency Treebank (Czech)
is only a matter of minutes.
5 Conclusion and future work
Vine parsers are motivated by efficiency and ro-
bustness (Dreyer et al, 2006), which has become
more and more important over the last few years,
but none of the systems introduced in the liter-
ature provide competitive results in terms of ac-
curacy. Our experiments show how dependency-
based vine parsers can be significantly improved
by using a maximum entropy-based POS tagger
for initial partial parsing with almost no cost in
terms of training and parsing time.
Our choice of parser restricted us in a few re-
spects. Most importantly, our results are below
state-of-the-art results, and it is not clear if the
strategy scales to more accurate parsers. The strat-
egy of using a POS tagger to do partial parsing and
subsequently forward high precision decisions to
a parser only works on graph-based or constraint-
based dependency parsers where previous deci-
sions can be hardwired into candidate weight ma-
trices by setting weights to 0. It would be difficult
if at all possible to implement in history-based de-
pendency parsers such as MaltParser. Experiments
will be performed with the MSTParser soon.
Our parser also restricted us to considering un-
labeled dependency graphs. A POS tagger, how-
ever, can also be used to identify grammatical
functions (subjects, objects, . . . ), for example,
which may be used to hardwire dependency rela-
tions into candidate weight matrices. POS taggers
may also be used to identify other dependency re-
lations or more fine-grained features that can im-
prove the accuracy of dependency parsers.
References
Cristina Bosco, Vincenzo Lombardo, Daniela Vassallo,
and Leonardo Lesmo. 2000. Building a treebank for
Italian. In LREC, pages 99?105, Athens, Greece.
Sabine Buchholz and Erwin Marsi. 2006. CONLL-X
shared task on multilingual dependency parsing. In
CONLL-X, pages 149?164, New York City, NY.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for statistical
machine translation. In MT Summit IX, New Or-
leans, Louisiana.
Markus Dreyer, David A. Smith, and Noah A. Smith.
2006. Vine parsing and minimum risk reranking for
speed and precision. In CONLL-X, pages 201?205,
New York City, NY.
J. Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards,
71:233?240.
Jason Eisner and Noah A. Smith. 2005. Parsing with
soft and hard constraints on dependency length. In
IWPT?05, pages 30?41, Vancouver, Canada.
Michel Galley and Cristopher Manning. 2009.
Quadratic time dependency parsing for machine
translation. In ACL?09, Singapore, Singapore. To
appear.
Yoav Goldberg and Michael Elhadad. 2008.
splitSVM: fast, space-efficient, non-heuristic, poly-
nomial kernel computation for NLP applications. In
ACL?08, Short Papers, pages 237?240, Columbus,
Ohio.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In HLT-EMNLP
2005, pages 523?530, Vancouver, British Columbia.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CONLL 2007 shared task on
dependency parsing. In EMNLP-CONLL?07, pages
915?932, Prague, Czech Republic.
Adwait Ratnaparkhi. 1998. Maximum entropy mod-
els for natural language ambiguity resolution. Ph.D.
thesis, University of Pennsylvania.
Le Zhang. 2004. Maximum entropy mod-
eling toolkit for Python and C++. Uni-
versity of Edinburgh. Available at home-
pages.inf.ed.ac.uk/lzhang10/maxent toolkit.html.
209
Coling 2008: Companion volume ? Posters and Demonstrations, pages 99?102
Manchester, August 2008
On the weak generative capacity of weighted context-free grammars?
Anders S?gaard
University of Potsdam
soegaard@ling.uni-potsdam.de
Abstract
It is shown how weighted context-free
grammars can be used to recognize lan-
guages beyond their weak generative ca-
pacity by a one-step constant time exten-
sion of standard recognition algorithms.
1 Introduction
Weighted context-free grammars (WCFGs) are
used to disambiguate strings and thus filter out
subsets of the tree languages of the underlying
context-free grammars (CFGs). Weights can ei-
ther be used as probabilities, i.e. higher weights are
preferred, or as penalities, i.e. lower weights are
preferred. The first convention, also followed by
Smith and Johnson (2007), is followed here. The
subsets of the tree languages that consist of the
heaviest tree for each yield are called the Viterbi
tree languages. String languages are the yields of
tree languages, and Viterbi string languages are the
yields of Viterbi tree languages.
Infante-Lopez and de Rijke (2006) show that the
Viterbi tree languages strictly extend the tree lan-
guages.
The idea explored in this paper is simple. If
trees must have particular weights for their yields
to be recognized, weights can be used to en-
code non-local dependencies. Technically, the
{r
1
, . . . , r
n
}-language is defined as all the strings
for which the heaviest, i.e. most probable, tree
has weight r
i
? {r
1
, . . . , r
n
}. It is shown that
this class of languages includes common classes
?Thanks to Mark Hopkins, Daniel Quernheim and the
anonymous reviewers for helpful comments.
?c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
of context-sensitive languages. In other words,
standard Viterbi-style recognition algorithms for
WCFGs can be used to recognize these classes
by a one-step look-up that checks if the weight
of the heaviest tree is in {r
1
, . . . , r
n
}. We
say that {r
1
, . . . , r
n
}-languages are {r
1
, . . . , r
n
}-
recognized.
Sect. 1.1 presents formal preliminaries and a
Viterbi-style recognition algorithm for WCFGs.
Note that for simplicity we restrict weights to be
rational numbers.
Sect. 2 defines {r
1
, . . . , r
n
}-languages and
presents some examples of WCFGs that
{r
1
, . . . , r
n
}-recognize context-sensitive lan-
guages. Sect. 3 gives a rough characterization of
the class of languages that can be {r
1
, . . . , r
n
}-
recognized by WCFGs.
Cortes and Mohri (2000) introduced a simi-
lar idea in the context of weighted finite-state
automata (WFSAs) and showed that WFSAs
can be used to {r
1
, . . . , r
n
}-recognize context-
free languages. Their results are extended in
Sect. 4. It is shown that WFSAs can also be
used to {r
1
, . . . , r
n
}-recognize context-sensitive
languages. It is shown, however, that the non-
context-free languages that can be {r
1
, . . . , r
n
}-
recognized by WCFGs strictly extend the non-
context-free languages that can be {r
1
, . . . , r
n
}-
recognized by WFSAs.
Sect. 5 discusses a more exact characterization
of the weak generative capacity of WCFGs in this
view. Coprime WCFGs (CWCFGs), i.e. a subclass
of WCFGs where the weights can be partitioned
into reciprocal coprimes, are introduced. It is con-
jectured that the infinite hierarchy of k-CWCFGs
is non-collapsing, and the classes of languages that
can be {r
1
, . . . , r
n
}-recognized by k-CWCFGs are
characterized in terms of an untraditional modifi-
99
cation of indexed grammars.
1.1 Preliminaries
A CFG is a 4-tuple G = ?N,T, P, S? where N,T
are finite and disjoint sets of nonterminal and ter-
minal symbols, P a finite set of production rules of
the form A? ? where A ? N and ? ? (N ?T )?,
and S ? N is the start symbol. A WCFG is a
2-tuple G? = ?G,?? where G = ?N,T, P, S?
is a CFG and ? : P ? {m
n
| m ? Z
+
, n ?
Z
+
,m, n 6= 0} a (total) weight function.
A left-most derivation t(?) for some CFG G =
?N,T, P, S? is a sequence of production rules
?p
1
, . . . , p
m
? with 1 ? i ? m : p
i
? P such
that
S
p
1
=? ?
1
. . . ?
m?1
p
m
=? ?
? is called the yield of t(?). The tree language
T (G) is the set of all left-most derivations licensed
by the production rules of G. The string language
of G is the set of yields:
L(G) = {? | t(?) ? T (G)}
The accumulated weight of a derivation of a
string ? ?(t(?)) is the product of the weight of all
the productions in t(?). The Viterbi tree language
of a WCFG then is:
V (G) = {t(?) | t(?) ? arg max
t
?
(?)?T (G)
(?(t
?
(?)))}
A simple Viterbi recognition algorithm for
WCFGs is presented in Figure 1 for further ref-
erence.
2 Our extension
For a set of n many rational numbers {r
1
, . . . , r
n
},
the language that is {r
1
, . . . , r
n
}-recognized by
the WCFG G, L
{r
1
,...,r
n
}
(G), is defined:
L
{r
1
,...,r
n
}
(G) = {? | t(?) ? V (G), ?(t(?)) ?
{r
1
, . . . , r
n
}}
Call the class of all languages that can be
{r
1
, . . . , r
n
}-recognized by a WCFG for all fi-
nite and non-empty sets of rational numbers
{r
1
, . . . , r
n
} for balanced weighted context-free
languages (BWCFLs). In all our examples
{r
1
, . . . , r
n
} will be a singleton set.
Note that all there is needed to do to recognize
the BWCFLs is to change line 7 of the Viterbi al-
gorithm in Figure 1 to:
if (S, r
i
) ? t(0, n), r
i
? {r
1
, . . . , r
n
} then . . .
3 Bounds on weak generative capacity
The first result of this paper is the following:
Theorem 3.1. The BWCFLs strictly extend the
context-free languages.
Proof. It is not difficult to see that any context-free
language is a BWCFL. Simply construct a WCFG
G = ?G
?
, ?? for any CFG G? = ?N,T, P, S?
such that the weight associated with each produc-
tion rule in P is 1
1
. It then holds that L
{
1
1
}
(G) =
L(G
?
).
The other direction is not very difficult either.
It is shown that {anbncn | n ? 0}, which
is non-context-free by the Bar-Hillel lemma, is
a BWCFL. The language is, for instance, the
set of strings L
{
1
1
}
(G) for the WCFG G =
??{S, S
?
}, {a, b, c}, P, S?, ?? where P is the fol-
lowing set of production rules, and ? assigns the
weights in the left column to the items in the right
column:
1
2
: S ? Sc
2
1
: S ? S?
2
1
: S? ? aS?b
1
2
: S? ? ?
L
{
1
1
}
= {a
n
b
n
c
n
| n ? 0}. Some example
derivations are presented in Example 3.2.
Example 3.2. Consider the only and thus heaviest
tree for abc, resp. ab:
S
Q
Q


S
S?
b
b
"
"
a S?
?
b
c
S
S?
b
b
"
"
a S?
?
b
The weight of the left tree, whose yield is abc,
is 1
1
. The weight of the left tree is 2
1
.
Consider also the {1
1
}-language of G =
?{S,D, T, T
?
}, {a, b, c, d}, P, S? with production
rules P :
1
1
: S ? TD
1
2
: D ? dD
1
1
: D ? ?
1
1
: T ? aTc
1
1
: T ? T?
2
1
: T? ? bT?
1
1
: T? ? ?
100
BUILD(t, [w
1
. . . w
n
])
1 for j ? 1 to n
2 do t(j ? 1, j)? {(A,?) | A? w
j
? P, log(?(A? w
j
)) = ?}
3 for k ? (j ? 1) to 0
4 do t(k, j)? {(A,? + ?) | A? B ? P, log(?(A? B)) = ?,
(B,?) ? t(k, j), if (A,?
?
) ? t(k, j) then ? > ?
?
}
5 for i? (j ? 2) to 0
6 do t(i, j)? {(A,? + ? + ?) | A? BC ? P, log(?(A? BC)) = ?,
?k.(B,?) ? t(i, k), (C, ?) ? t(k, j), if (A,?
?
) ? t(i, j) then ? > ?
?
}
7 if (S, r
i
) ? t(0, n) then return success else failure
Figure 1: A Viterbi recognition algorithm for WCFGs
It should be relatively easy to see that L(G) =
{a
n
b
m
c
n
d
m
| n ? 0}.
It is not difficult to see that the BWCFLs are
a subset of the context-sensitive languages. This
follows from the fact that the left-most derivations
in the Viterbi tree languages of WCFGs are linear
in the length of the input string; in other words,
BWCFLs can be recognized in nondeterministic
linear space and thus by a linear bounded automa-
ton. Since any language that can be represented by
a linear bounded automaton is context-sensitive,
the BWCFLs must be a subset of the context-
sensitive ones.
The set of BWCFLs is also a subset of the range
concatenation languages (Boullier, 1998) by the
observation made in the introduction that they can
be recognized in polynomial (i.e. cubic) time by
standard algorithms and a one-step inspection of
the weight of the heaviest tree; and by the fact
that the range concatenation languages are exactly
the languages that can be recognized in polynomial
time (Boullier, 1998).
4 Weighted finite-state automata
Cortes and Mohri (2000) showed, in similar work,
that WFSAs can be used to recognize context-free,
i.e. non-regular, languages.
Example 4.1. The weighted finite-state automa-
ton T = ?{q
0
, q
1
}, {a, b}, ?, q
0
, {q
1
}? with the fol-
lowing ?-transitions {1
1
}-recognizes the language
L
{
1
1
}
(T ) = {a
n
b
n
| n ? 0}:
1
2
: ?(q
0
, a) = q
0
1
1
: ?(q
0
, ?) = q
1
2
1
: ?(q
1
, b) = q
1
It is not difficult to see that the strings
ab, aabb, . . . have derivations with weights 1
1
,
whereas the string aab, for example, only has a
derivation with weight 1
2
. Since 1
2
/? {
1
1
}, aab /?
L
{
1
1
}
(T ).
Cortes and Mohri (2000) also formulated an ex-
tension of WFSAs over cross-products of semi-
rings that recognized certain context-sensitive,
i.e. non-context-free languages, but their results
can be considerably extended. The automaton
in Example 4.2, for example, even recognizes a
language conjectured to be outside the linear in-
dexed languages, namely the MIX language (Gaz-
dar, 1988).
Example 4.2. The weighted finite-state automa-
ton T = ?{q
0
, q
1
, q
2
, q
3
}, {a, b, c}, ?, q
0
, {q
0
}?
with the following ?-transitions {1
1
}-recognizes
the MIX language:
1
8
: ?(q
0
, a) = q
1
1
8
: ?(q
1
, a) = q
2
1
8
: ?(q
2
, a) = q
3
1
125
: ?(q
0
, b) = q
1
1
125
: ?(q
1
, b) = q
2
1
125
: ?(q
2
, b) = q
3
1
729
: ?(q
0
, c) = q
1
1
729
: ?(q
1
, c) = q
2
1
729
: ?(q
2
, c) = q
3
90
3
1
: ?(q
3
, ?) = q
0
This example is a bit more complicated. Note
that 8 ? 125 ? 729 = 903. The strings
cab, bcabac, . . . have derivations with weights 1
1
,
since 903
8?125?729
=
1
1
, whereas the string cababa,
for instance, has no derivations with weight 1
1
. The
string cababa has exactly one derivation whose
weight is 903
8
2
?125
.
5 Coprime WCFGs
A 2-CWCFG is a WCFG over subsets of the ratio-
nal numbers C = { 1
n
| n ? ?} ? {
n
1
| n ? ?}
101
B. (2000) WCFGs
{a
n
1
. . . a
n
k
| n ? 0} X X
MIX X X
{a
n
b
m
c
n
d
m
| m,n ? 0} X X
{wcw | w ? {a, b}
?
} X X
Figure 2: Classes of languages {r
1
, . . . , r
n
}-
recognized by WCFGs and recognized by the ex-
tension in Boullier (2000).
where ? is an arbitrary set of coprimes (? ? N?)
such that there is a bijection from the production
rules onto themselves such that if a production rule
has weight 1
1
it is projected onto itself, and oth-
erwise, i.e. if it has weight 1
m
with m 6= 1 it is
projected onto a production rule with weight m
1
.
A k-CWCFG for k ? 1 is now the extension of
CWCFG where the sets of production rules the
product of whose weights is 1, can be of size at
most k, e.g. the WFSA in Example 4.2 is a 3-
CWCFG.
The infinite hierarchy of k-CWCFGs seems to
be non-collapsing. A k-CWCFG {r
1
, . . . , r
n
}-
recognizes the language {an
1
. . . a
n
2k
| n ? 0}, but
not {an
1
. . . a
n
2k+1
| n ? 0}. It has this property
in common with k-multiple context-free grammars
(Seki et al, 1991). 2-CWCFG can be shown to be
weakly equivalent with the extension of linear in-
dexed grammars (LIGs) (Gazdar, 1988) where the
stack is a multiset or a bag that is globally accessi-
ble and not just along spines. The universal recog-
nition problem for this extension of LIGs can be
shown to be NP-complete by reduction of the ver-
tex cover problem, similar to S?gaard et al (2007).
The generalization to k-CWCFG requires stacks of
stacks, but is otherwise relatively straight-forward.
6 Conclusions
It was shown how weighted context-free grammars
can be used to recognize languages beyond their
weak generative capacity by a one-step constant
time extension of standard recognition algorithms.
The class of languages that can be recognized this
way strictly extends the context-free languages,
but is included in the cubic time recognizable ones.
Boullier (2000) defines what he calls a ?cu-
bic time extension of CFG? that recognizes gen-
eralizations of the copy language that are beyond
WCFG. It remains to be seen if the set of BWCFLs
is a strict subset of the set of languages that can be
recognized by this formalism. They all recognize
the classes of languages in Figure 2.
References
Boullier, Pierre. 1998. Proposal for a natural language
processing syntactic backbone. Technical report, IN-
RIA, Le Chesnay, France.
Boullier, Pierre. 2000. A cubic time extension of
context-free grammars. Grammars, 3(2?3):111?
131.
Cortes, Corinna and Mehryar Mohri. 2000. Context-
free recognition with weighted automata. Gram-
mars, 3(2?3):133?150.
Gazdar, Gerald. 1988. Applicability of indexed gram-
mars to natural languages. In Reyle, Uwe and Chris-
tian Rohrer, editors, Natural language parsing and
linguistic theories, pages 69?94. Reidel, Dordrecht,
the Netherlands.
Infante-Lopez, Gabriel and Maarten de Rijke. 2006. A
note on the expressive power of probabilistic context
free grammars. Journal of Logic, Language and In-
formation, 15(3):219?231.
Seki, Hiroyuki, Takashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On multiple context-
free grammars. Theoretical Computer Science,
88(2):191?229.
Smith, Noah and Mark Johnson. 2007. Weighted
and probabilistic context-free grammars are equally
expressive. Computational Linguistics, 33(4):477?
491.
S?gaard, Anders, Timm Lichte, and Wolfgang Maier.
2007. On the complexity of linguistically motivated
extensions of tree-adjoining grammar. In Proceed-
ings of Recent Advances in Natural Language Pro-
cessing 2007, Borovets, Bulgaria.
102
Coling 2008: Companion volume ? Posters and Demonstrations, pages 103?106
Manchester, August 2008
Range concatenation grammars for translation
Anders S?gaard
University of Potsdam
soegaard@ling.uni-potsdam.de
Abstract
Positive and bottom-up non-erasing bi-
nary range concatenation grammars (Boul-
lier, 1998) with at most binary predicates
((2,2)-BRCGs) is a O(|G|n6) time strict
extension of inversion transduction gram-
mars (Wu, 1997) (ITGs). It is shown
that (2,2)-BRCGs induce inside-out align-
ments (Wu, 1997) and cross-serial discon-
tinuous translation units (CDTUs); both
phenomena can be shown to occur fre-
quently in many hand-aligned parallel cor-
pora. A CYK-style parsing algorithm is
introduced, and induction from aligment
structures is briefly discussed.
Range concatenation grammars (RCG) (Boul-
lier, 1998) mainly attracted attention in the for-
mal language community, since they recognize ex-
actly the polynomial time recognizable languages,
but recently they have been argued to be useful
for data-driven parsing too (Maier and S?gaard,
2008). Bertsch and Nederhof (2001) present the
only work to our knowledge on using RCGs for
translation. Both Bertsch and Nederhof (2001)
and Maier and S?gaard (2008), however, only
make use of so-called simple RCGs, known to be
equivalent to linear context-free rewrite systems
(LCFRSs) (Weir, 1988; Boullier, 1998). Our strict
extension of ITGs, on the other hand, makes use
of the ability to copy substrings in RCG deriva-
tions; one of the things that makes RCGs strictly
more expressive than LCFRSs. Copying enables
us to recognize the intersection of any two transla-
tions that we can recognize and induce the union
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
of any two alignment structures that we can in-
duce. Our extension of ITGs in fact introduces
two things: (i) A clause may introduce any num-
ber of terminals. This enables us to induce mul-
tiword translation units. (ii) A clause may copy a
substring, i.e. a clause can associate two or more
nonterminals A
1
, . . . A
n
with the same substring
and thereby check if the substring is in the inter-
section of the languages of the subgrammars with
start predicate names A
1
, . . . A
n
.
The first point is motivated by studies such
as Zens and Ney (2003) and simply reflects
that in order to induce multiword translation
units in this kind of synchronous grammars, it
is useful to be able to introduce multiple ter-
minals simultaneously. The second point gives
us a handle on context-sensitivity. It means
that (2,2)-BRCGs can define translations such as
{?a
n
b
m
c
n
d
m
, a
n
b
m
d
m
c
n
? | m,n ? 0}, i.e. a
translation of cross-serial dependencies into nested
ones; but it also means that (2,2)-BRCGs induce
a larger class of alignment structures. In fact the
set of alignment structures that can be induced is
closed under union, i.e. any alignment structure
can be induced. The last point is of practical in-
terest. It is shown below that phenomena such as
inside-out alignments and CDTUs, which cannot
be induced by ITGs, but by (2,2)-BRCGs, occur
frequently in many hand-aligned parallel corpora.
1 (2,2)-BRCGs and ITGs
(2,2)-BRCGs are positive RCGs (Boullier, 1998)
with binary start predicate names, i.e. ?(S) = 2. In
RCG, predicates can be negated (for complemen-
tation), and the start predicate name is typically
unary. The definition is changed only for aesthetic
reasons; a positive RCG with a binary start predi-
cate name S is turned into a positive RCG with a
103
unary start predicate name S? simply by adding a
clause S?(X
1
X
2
) ? S(X
1
,X
2
).
Definition 1.1 (Positive RCGs). A positive RCG
is a 5-tuple G = ?N,T, V, P, S?. N is a finite
set of predicate names with an arity function ?:
N ? Z
?
, T and V are finite sets of, resp., ter-
minal and variables. P is a finite set of clauses
of the form ?
0
? ?
1
. . . ?
m
, where and each
of the ?
i
, 0 ? i ? m, is a predicate of the
form A(?
1
, . . . , ?
?(A)
). Each ?
j
? (T ? V )
?
,
1 ? j ? ?(A), is an argument. S ? N is the
start predicate name with ?(S) = 2.
Note that the order of RHS predicates in a clause
is of no importance. Three subclasses of RCGs are
introduced for further reference: An RCG G =
?N,T, V, P, S? is simple iff for all c ? P , it holds
that no variable X occurs more than once in the
LHS of c, and if X occurs in the LHS then it
occurs exactly once in the RHS, and each argu-
ment in the RHS of c contains exactly one vari-
able. An RCG G = ?N,T, V, P, S? is a k-RCG
iff for all A ? N, ?(A) ? k. Finally, an RCG
G = ?N,T, V, P, S? is said to be bottom-up non-
erasing iff for all c ? P all variables that occur in
the RHS of c also occur in its LHS.
A positive RCG is a (2,2)-BRCG iff it is a 2-
RCG, if an argument of the LHS predicate contains
at most two variables, and if it is bottom-up non-
erasing.
The language of a (2,2)-BRCG is based
on the notion of range. For a string pair
?w
1
. . . w
n
, v
n+2
. . . v
n+1+m
? a range is a pair of
indices ?i, j? with 0 ? i ? j ? n or n < i ?
j ? n + 1 + m, i.e. a string span, which de-
notes a substring w
i+1
. . . w
j
in the source string
or a substring v
i+1
. . . v
j
in the target string. Only
consequtive ranges can be concatenated into new
ranges. Terminals, variables and arguments in
a clause are bound to ranges by a substitution
mechanism. An instantiated clause is a clause in
which variables and arguments are consistently re-
placed by ranges; its components are instantiated
predicates. For example A(?g . . . h?, ?i . . . j?) ?
B(?g . . . h?, ?i+1 . . . j ? 1?) is an instantiation of
the clause A(X
1
, aY
1
b) ? B(X
1
, Y
1
) if the tar-
get string is such that v
i+1
= a and v
j
= b. A
derive relation =? is defined on strings of instan-
tiated predicates. If an instantiated predicate is the
LHS of some instantiated clause, it can be replaced
by the RHS of that instantiated clause. The lan-
guage of a (2,2)-BRCG G = ?N,T, V, P, S? is
the set L(G) = {?w
1
. . . w
n
, v
n+2
. . . v
n+1+m
? |
S(?0, n?, ?n + 1, n + 1 + m?)
?
=? ?}, i.e. an
input string pair ?w
1
. . . w
n
, v
n+2
. . . v
n+1+m
? is
recognized iff the empty string can be derived from
S(?0, n?, ?n + 1, n+ 1 +m?).
Theorem 1.2 ((Boullier, 2000)). The recognition
problem of bottom-up non-erasing k-RCG can be
solved in time O(|G|nd) where d = max
c
j
?P
(k
j
+
v
j
) where c
j
is the jth clause in P , k
j
is the arity of
its LHS predicate, and v
j
is the number of different
variables in that LHS predicate.
It follows immediately that the recognition
problem of (2,2)-BRCG can be solved in time
O(|G|n
6
), since k
j
can be at most 2, and v
j
can
be at most 4.
Example 1.3. Consider the (2,2)-BRCG G =
?{S
0
, S
1
, S
2
}, {a, b, c, d, e, f, g, h}, {X
1
,X
2
, Y
1
,
Y
2
}, P, S
0
? with P the following set of clauses:
S
0
(X
1
, Y
1
) ? S
1
(X
1
, Y
1
)S
2
(X
1
, Y
1
)
S
1
(X
1
d, Y
1
Y
2
) ? A
0
(X
1
, Y
2
)E(Y
1
)
A
0
(X
1
c, Y
1
h) ? A
1
(X
1
, Y
1
)
A
1
(aX
1
, g) ? B(X
1
)
S
2
(aX
1
, Y
1
Y
2
) ? T
0
(X
1
, Y
1
)G(Y
2
)
T
0
(X
1
d, Y
1
f) ? T
1
(X
1
, Y
1
)
T
1
(bX
1
, e) ? C(X
1
)
B(b) ? ? C(c) ? ?
E(ef) ? ? G(gh) ? ?
which when words that are recognized simulta-
neously are aligned, induces the alignment:
a b c d
e f g h
by inducing the alignments in the, resp., S
1
and
S
2
derivations:
a b c d
e f g h
a b c d
e f g h
Example 1.4. Consider the (2,2)-BRCG G =
?{S
s
, S
0
, S
?
0
, S
1
, S
?
1
, A,B,C,D}, {a, b, c, d}, {X
1
,
X
2
, Y
1
, Y
2
}, P, S
s
? with P the following set of
clauses:
S
s
(X
1
, Y
1
) ? S
0
(X
1
, Y
1
)S
?
0
(X
1
, Y
1
)
S
0
(X
1
X
2
, Y
1
) ? S
1
(X
1
, Y
1
)D(X
2
)
S
1
(aX
1
c, abY
1
) ? S
1
(X
1
, Y
1
)
S
1
(X
1
, Y
1
Y
2
) ? B(X
1
)C(Y
1
)D(Y
2
)
S
?
0
(X
1
X
2
, Y
1
) ? S
?
1
(X
2
, Y
1
)A(X
1
)
S
?
1
(bX
1
d, Y
1
cd) ? S
?
1
(X
1
, Y
1
)
S
?
1
(X
1
, Y
1
Y
2
) ? C(X
1
)A(Y
1
)B(Y
2
)
A(aX
1
) ? A(X
1
) A(?) ? ?
B(bX
1
) ? B(X
1
) B(?) ? ?
C(cX
1
) ? C(X
1
) C(?) ? ?
D(dX
1
) ? D(X
1
) D(?) ? ?
Note that L(G) = {?anbmcndm, (ab)n(cd)m? |
m,n ? 0}.
104
Since the component grammars in ITGs are
context-free, Example 1.4 shows that there is at
least one translation not recognizable by ITGs that
is recognized by a (2,2)-BRCG; {anbmcndm |
m,n ? 0} is known to be non-context-free. ITGs
translate into simple (2,2)-BRCGs in the follow-
ing way; see Wu (1997) for a definition of ITGs.
The left column is ITG production rules; the right
column their translations in simple (2,2)-BRCGs.
A? [BC] A(X
1
X
2
, Y
1
Y
2
)? B(X
1
, Y
1
)C(X
2
, Y
2
)
A? ?BC? A(X
1
X
2
, Y
1
Y
2
)? B(X
1
, Y
2
)C(X
2
, Y
1
)
A? e | f A(e, f)? ?
A? e | ? A(e, ?)? ?
A? ? | f A(?, f)? ?
It follows immediately that
Theorem 1.5. (2,2)-BRCGs are strictly more ex-
pressive than ITGs.
2 Alignment capacity
Zens and Ney (2003) identify a class of alignment
structures that cannot be induced by ITGs, but
that can be induced by a number of similar syn-
chronous grammar formalisms, e.g. synchronous
tree substitution grammar (STSG) (Eisner, 2003).
Inside-out alignments (Wu, 1997), such as the
one in Example 1.3, cannot be induced by any of
these theories; in fact, there seems to be no useful
synchronous grammar formalisms available that
handle inside-out alignments, with the possible
exceptions of synchronous tree-adjoining gram-
mars (Shieber and Schabes, 1990), Bertsch and
Nederhof (2001) and generalized multitext gram-
mars (Melamed et al, 2004), which are all way
more complex than ITG, STSG and (2,2)-BRCG.
Nevertheless, Wellington et al (2006) report that
5% of the sentence pairs in an aligned paral-
lel Chinese?English corpus contained inside-out
alignments. Example 1.3 shows that (2,2)-BRCGs
induce inside-out alignments.
An even stronger motivation for using (2,2)-
BRCG for translation is the existence of cross-
serial DTUs (CDTUs). Informally, a CDTU is a
DTU such that there is a part of another DTU in its
gap. Here?s a simple example:
a b c d
e f
Neither ITGs nor STSGs can induce CDTUs;
ITGs cannot induce DTUs with multiple gaps
(MDTUs) either. Our experiments are summarized
in Figure 1. Overall the results show that handling
CDTUs is important for alignment error rates.
3 Parsing and induction from alignments
A CYK-style algorithm is presented for (2,2)-
BRCG in Figure 2; it is assumed, w.l.o.g, that if
the same variable occurs twice in the LHS of a
clause, the clause is of the form A
0
(X
1
, Y
1
) ?
A
1
(X
1
, Y
1
)A
2
(X
1
, Y
1
). It modifies the original
CYK algorithm (Younger, 1967) in four ways: (i)
It uses two charts; one for the source string (s) and
one for the target string (t). (ii) Pairs of nontermi-
nals and integers (A, ?), rather than just nontermi-
nals, are stored in the cells of the chart (l. 2,4,6,7).
Integers represent derivation steps at which non-
terminals are inserted. (iii) Multiple terminals are
allowed (l. 2,6,7). (iv) If a clause is copying, the
same two cells in the chart are visited twice (l. 4).
Note that the variable ? in insertion, e.g. in l. 4/1, is
the current derivation step, but ?
i
in look-up, e.g. in
l. 4/2, is the derivation step in which the associated
nonterminal was added to the chart.
The overall runtime of this algorithm is in
O(|G|n
6
), since it has, for branching clauses, six
embedded loops that iterate over the string, i.e. the
four for loops and the two ?s in Figure 2.
The induction problem from alignments can be
reduced to the induction problem for ITGs by sim-
ply unravelling the alignment structures. The sim-
plest algorithm for doing this assumes that align-
ments are sequences of translation units, and con-
siders each at a time. If a gap is found, the trans-
lation unit is a DTU and is moved to a new align-
ment structure. The complexity of the algorithm
is quadratic in the length of the input sentences,
i.e. linear in the size of the alignment structure,
and for a sentence pair ?w
1
. . . w
n
, v
1
. . . v
m
? the
ITG induction algorithm has to consider at most
min(n+m)
2
aligment structures.
4 Conclusion
A new class of grammars for syntax-based ma-
chine translation was presented; while its recogni-
tion problem remains solvable in time O(|G|n6),
the grammars induce frequently occurring align-
ment configurations that cannot be induced by
comparable classes of grammars in the literature.
A parsing and an induction algorithm were pre-
sented.
105
Sent. TUs DTUs CDTUs MDTUs CDTUs/Sent.
English?French: 100 937 95 36 11 36%
English-Portuguese: 100 939 100 52 3 52%
English?Spanish: 100 950 90 26 7 26%
Portuguese?French: 100 915 77 19 3 19%
Portuguese?Spanish: 100 991 80 40 3 40%
Spanish?French: 100 975 74 24 8 24%
Figure 1: Statistics for six 100-sentence hand-aligned Europarl bitexts (Graca et al, 2008).
BUILD(s, [w
1
. . . w
n
]), (t, [v
1
. . . v
m
])
1 for j ? 1 to n, for j? ? 1 tom
2 do s(i? 1, j), t(i? ? 1, j?)? {(A, ?) | A(w
i
. . . w
j
, v
i
?
. . . v
j
?
)? ? ? P}
3 for k? (j ? 1) to 0, for k? ? (j? ? 1) to 0
4 do s(k, j), t(k?, j?)? {(A, ?) | A(X
1
, Y
1
)? B(X
1
, Y
1
)C(X
1
, Y
1
) ? P,
(B, ?
1
), (C, ?
2
) ? s(k, j), (B, ?
1
), (C, ?
2
) ? t(k
?
, j
?
)}
5 for l? (j ? 2) to 0, for l? ? (j? ? 2) to 0
6 do s(l, j), t(l?, j?)? {(A, ?) | A(?
1
X
1
?
2
X
2
?
3
, ?
1
Y
1
?
2
Y
2
?
3
)? B(X
1
, Y
1
)C(X
2
, Y
2
) ? P,
?i.(B, ?
1
) ? s(l + |?
1
|, i), (C, ?
2
) ? s(i+ |?
2
|, j ? |?
3
|), ?
1
= w
l+1
. . . w
l+|?
1
|
,
?
2
= w
i+1
. . . w
i+|?
2
|
, ?
3
= w
j?|?
3
|
. . . w
j
,
?i
?
.(B, ?
1
) ? t(l
?
+ |?
1
|, i
?
), (C, ?
2
) ? t(i
?
+ |?
2
|, j
?
? |?
3
|), ?
1
= v
l
?
+1
. . . v
l
?
+|?
1
|
,
?
2
= v
i
?
+1
. . . v
i
?
+|?
2
|
, ?
3
= v
j
?
?|?
3
|
. . . v
j
?
}
7 do s(l, j), t(l?, j?)? {(A, ?) | A(?
1
X
1
?
2
X
2
?
3
, ?
1
Y
1
?
2
Y
2
?
3
)? B(X
1
, Y
1
)C(X
2
, Y
2
) ? P,
?i.(B, ?
1
) ? s(l + |?
1
|, i), (C, ?
2
) ? s(i+ |?
2
|, j ? |?
3
|), ?
1
= w
l+1
. . . w
l+|?
1
|
,
?
2
= w
i+1
. . . w
i+|?
2
|
, ?
3
= w
j?|?
3
|
. . . w
j
,
?i
?
.(C, ?
2
) ? t(l
?
+ |?
1
|, i
?
), (B, ?
1
) ? t(i
?
+ |?
2
|, j
?
? |?
3
|), ?
1
= v
l
?
+1
. . . v
l
?
+|?
1
|
,
?
2
= v
i
?
+1
. . . v
i
?
+|?
2
|
, ?
3
= v
j
?
?|?
3
|
. . . v
j
?
}
8 if (S, ?
1
) ? s(0, n), (S, ?
1
) ? t(0, m) then return success else failure
Figure 2: CYK-style parsing algorithm for (2,2)-BRCG.
References
Bertsch, Eberhard and Mark-Jan Nederhof. 2001. On the
complexity of some extensions of RCG parsing. In Pro-
ceedings of the 7th International Workshop on Parsing
Technologies, pages 66?77, Beijing, China.
Boullier, Pierre. 1998. Proposal for a natural language pro-
cessing syntactic backbone. Technical report, INRIA, Le
Chesnay, France.
Boullier, Pierre. 2000. A cubic time extension of context-free
grammars. Grammars, 3(2?3):111?131.
Eisner, Jason. 2003. Learning non-isomorphic tree mappings
for machine translation. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics,
pages 205?208, Sapporo, Japan.
Graca, Joao, Joana Pardal, Lu?sa Coheur, and Diamantino Ca-
seiro. 2008. Building a golden collection of parallel multi-
language word alignments. In Proceedings of the 6th In-
ternational Conference on Language Resources and Eval-
uation, Marrakech, Morocco.
Maier, Wolfgang and Anders S?gaard. 2008. Treebanks and
mild context-sensitivity. In Proceedings of the 13th Con-
ference on Formal Grammar, Hamburg, Germany.
Melamed, Dan, Giorgio Satta, and Benjamin Wellington.
2004. Generalized multitext grammars. In Proceedings
of the 42nd Annual Meeting of the Association for Compu-
tational Linguistics, pages 661?668, Barcelona, Spain.
Shieber, Stuart and Yves Schabes. 1990. Synchronous tree-
adjoining grammars. In Proceedings of the 13th Con-
ference on Computational Linguistics, pages 253?258,
Helsinki, Finland.
Weir, David. 1988. Characterizing mildly context-sensitive
grammar formalisms. Ph.D. thesis, University of Pennsyl-
vania, Philadelphia, Pennsylvania.
Wellington, Benjamin, Sonjia Waxmonsky, and Dan
Melamed. 2006. Empirical lower bounds on the complex-
ity of translational equivalence. In Proceedings of the 44th
Annual Conference of the Association for Computational
Linguistics, pages 977?984, Sydney, Australia.
Wu, Dekai. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Computa-
tional Linguistics, 23(3):377?403.
Younger, Daniel. 1967. Recognition and parsing of context-
free languages in time n3. Information and Control,
10(2):189?208.
Zens, Richard and Hermann Ney. 2003. A comparative study
on reordering constraints in statistical machine translation.
In Proceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 144?151, Sapporo,
Japan.
106
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 239?242,
New York, June 2006. c?2006 Association for Computational Linguistics
239
240
241
242
Proceedings of the 3rd Workshop on Scalable Natural Language Understanding, pages 65?72,
New York City, June 2006. c?2006 Association for Computational Linguistics
 
	Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1065?1073,
Beijing, August 2010
Semi-supervised dependency parsing using generalized tri-training
Anders S?gaard and Christian Rish?j
Center for Language Technology
University of Copenhagen
{soegaard|crjensen}@hum.ku.dk
Abstract
Martins et al (2008) presented what to
the best of our knowledge still ranks as
the best overall result on the CONLL-
X Shared Task datasets. The paper
shows how triads of stacked dependency
parsers described in Martins et al (2008)
can label unlabeled data for each other in
a way similar to co-training and produce
end parsers that are significantly better
than any of the stacked input parsers.
We evaluate our system on five datasets
from the CONLL-X Shared Task and ob-
tain 10?20% error reductions, incl. the
best reported results on four of them.
We compare our approach to other semi-
supervised learning algorithms.
1 Introduction
Semi-supervised learning of structured variables
is a difficult problem that has received consid-
erable attention recently, but most results have
been negative (Abney, 2008). This paper uses
stacked learning (Wolpert, 1992) to reduce struc-
tured variables, i.e. dependency graphs, to multi-
nomial variables, i.e. attachment and labeling
decisions, which are easier to manage in semi-
supervised learning scenarios, and which can
later be combined into dependency trees using
parsing algorithms for arc-factored dependency
parsing. Our approach thus combines ensemble-
based methods and semi-supervised learning.
Ensemble-based methods such as stacked
learning are used to reduce the instability of clas-
sifiers, to average out their errors and to com-
bine the strengths of diverse learning algorithms.
Ensemble-based methods have attracted a lot of
attention in dependency parsing recently (Sagae
and Lavie, 2006; Hall et al, 2007; Nivre and
McDonald, 2008; Martins et al, 2008; Fishel
and Nivre, 2009; Surdeanu and Manning, 2010).
Nivre and McDonald (2008) were first to intro-
duce stacking in the context of dependency pars-
ing.
Semi-supervised learning is typically moti-
vated by data sparseness. For many classifi-
cation tasks in natural language processing, la-
beled data can be in short supply but unla-
beled data is more readily available. Semi-
supervised methods exploit unlabeled data in ad-
dition to labeled data to improve performance
on classification tasks. If the predictions of a
learner l on unlabeled data are used to improve
a learner l? in semi-supervised learning, the ro-
bustness of learning will depend on the stabil-
ity of l. Combining ensemble-based and semi-
supervised methods may thus lead to more ro-
bust semi-supervised learning.
Ensemble-based and semi-supervised meth-
ods are some of the areas that receive most at-
tention in machine learning today, but relatively
little attention has been given to combining these
methods (Zhou, 2009). Semi-supervised learn-
ing algorithms can be categorized with respect
to the number of views, i.e. the number of fea-
ture sets, and the number of learners used to in-
form each other (Hady and Schwenker, 2008).
Self-training and expectation maximization are
perhaps the best known semi-supervised learn-
ing algorithms (Abney, 2008). They are both
single-view and single-learner algorithms. Since
there is thus only a single perspective on data,
1065
selecting unlabeled data points with predictions
is a difficult task. There is an imminent danger
that the learner amplifies its previous mistakes,
and while several techniques such as balancing
and throttling have been developed to avoid such
caveats, using single-view and single-learner al-
gorithms often requires both caution and experi-
ence with the modeling task at hand.
Algorithms with multiple views on data are
known to be more robust. This insight led to the
development of co-training (Blum and Mitchell,
1998), a two-view method where views inform
each other, but it also paved the way for the inte-
gration of ensemble-based and semi-supervised
methods, i.e. for methods with multiple learners.
It was mentioned that relatively little work has
been devoted to this topic, but there are notable
exceptions:
Bennett et al (2003) generalized boosting to
semi-supervised learning in a seminal paper,
where the idea of iterative or recursive ensembles
was also introduced. Li and Zhou (2005) intro-
duce tri-training, a form of co-training that trains
an ensemble of three learners on labeled data and
runs them on unlabeled data. If two learners
agree on their labeling of a data point, the data
point is added to the labeled data of the third
learner with the prediction of the first two. Di-
daci and Roli (2006) extend self-training and co-
training to multiple learners. Li and Zhou (2007)
generalize tri-training to larger ensembles of ran-
dom trees. The technique is also known as co-
forests. Hady and Schwenker (2008) general-
ize existing ensemble-based methods for semi-
supervised learning scenarios; in particular they
embed ensembles in a form of co-training that is
shown to maintain the diversity of the ensemble
over time. Milidiu and Duarte (2009) generalize
boosting at start to semi-supervised learning.
This paper applies a generalization of tri-
training to two classification problems, attach-
ment and labeling. The attachment classifier?s
weights are used for arc-factored dependency
parsing, and the labeling classifier?s weights are
then used to label the dependency tree delivered
by the parser.
Semi-supervised dependency parsing has at-
tracted a lot of attention recently (Koo et al,
2008; Wang et al, 2008; Suzuki et al, 2009),
but there has, to the best of our knowledge, been
no previous attempts to apply tri-training or re-
lated combinations of ensemble-based and semi-
supervised methods to any of these tasks, ex-
cept for the work of Sagae and Tsujii (2007)
discussed in Sect. 2.6. However, tri-training
has been applied to Chinese chunking (Chen et
al., 2006), question classification (Nguyen et al,
2008) and POS tagging (S?gaard, 2010).
We compare generalized tri-training to other
semi-supervised learning algorithms, incl. self-
training, the original tri-training algorithm based
on bootstrap samples (Li and Zhou, 2005),
co-forests (Li and Zhou, 2007) and semi-
supervised support vector machines (Sindhwani
and Keerthi, 2006).
Sect. 2 introduces dependency parsing and
stacked learning. Stacked learning is general-
ized to dependency parsing, and previous work is
briefly surveyed. We then describe how stacked
dependency parsers can be further stacked as in-
put for two end classifiers that can be combined
to produce dependency structures. These two
classifiers will learn multinomial variables (at-
tachment and labeling) from a combination of
labeled data and unlabeled data using a gener-
alization of tri-training. Sect. 3 describes our ex-
periments. We describe the data sets, and how
the unlabeled data was prepared. Sect. 4 presents
our results. Sect. 5 presents an error analysis and
discusses the results in light of other results in
the literature, and Sect. 6 concludes the paper.
2 Background and related work
2.1 Dependency parsing
Dependency parsing models a sentence as a tree
where words are vertices and grammatical func-
tions are directed edges (dependencies). Each
word thus has a single incoming edge, except
one called the root of the tree. Dependency pars-
ing is thus a structured prediction problem with
trees as structured variables. Each sentence has
exponentially many possible dependency trees.
Our observed variables are sentences with words
labeled with part-of-speech tags. The task for
1066
each sentence is to find the dependency tree that
maximizes an objective function which in our
case is learned from a combination of labeled
and unlabeled data.
More formally, a dependency tree for a
sentence x = w1, . . . , wn is a tree T =
?{0, 1, . . . , n}, A? with A ? V ? V the set of
dependency arcs. Each vertex corresponds to
a word in the sentence, except 0 which is the
root vertex, i.e. for any i ? n ?i, 0? 6? A.
Since a dependency tree is a tree it is acyclic.
A tree is projective if every vertex has a continu-
ous projection, i.e. if and only if for every arc
?i, j? ? A and node k ? V , if i < k < j
or j < k < i then there is a subset of arcs
{?i, i1?, ?i1, i2?, . . . , ?ik?1, ik?} ? A such that
ik = k.
In this paper we use a maximum spanning tree
algorithm, the so-called Chu-Liu-Edmonds algo-
rithm (CLE) (Edmonds, 1967) to turn the pre-
dictions of our semi-supervised classifiers into a
dependency tree.
2.2 Stacked learning
Stacked generalization, or simply stacking, was
first proposed by Wolpert (1992). Stacking is an
ensemble-based learning method where multiple
weak classifiers are combined in a strong end
classifier. The idea is to train the end classifier
directly on the predictions of the input classifiers.
Say each input classifier ci with 1 ? i ?
n receives an input x and outputs a prediction
ci(x). The end classifier then takes as input
?x, c1(x), . . . , cn(x)? and outputs a final predic-
tion c0(?x, c1(x), . . . , cn(x)?). Training is done
by cross-validation. In sum, stacking is training
a classifier on the output of classifiers.
2.3 Stacked dependency parsing
Stacked learning can be generalized to structured
prediction tasks such as dependency parsing. Ar-
chitectures for stacking dependency parsers typi-
cally only use one input parser, but otherwise the
intuition is the same: the input parser is used to
augment the dependency structures that the end
parser is trained and evaluated on.
Nivre and McDonald (2008) first showed how
the MSTParser (McDonald et al, 2005) and the
MaltParser (Nivre et al, 2007) could be im-
proved by stacking each parser on the predic-
tions of the other. Martins et al (2008) general-
ized their work, considering more combinations
of parsers, and stacking the end parsers on non-
local features from the predictions of the input
parser, e.g. siblings and grand-parents. In this
work we use three stacked dependency parsers
for each language: mst2 (p1), malt/mst2 (p2) and
malt/mst1 (p3).
The notation ?malt/mst2? means that the
second-order MSTParser has been stacked on the
MaltParser. The capital letters refer to feature
configurations. Configuration D stacks a level 1
parser on several (non-local) features of the pre-
dictions of the level 0 parser (along with the in-
put features): the predicted edge, siblings, grand
parents and predicted head of candidate modifier
if predicted edge is 0. Configuration E stacks
a level 1 parser on the features in configuration
D and all the predicted children of the candi-
date head. The chosen parser configurations are
those that performed best in Martins et al (2008)
across the different datasets.
2.4 Stacking stacked dependency parsing
The input features of the input classifiers in
stacked learning x can of course be removed
from the input of the end classifier. It is also
possible to stack stacked classifiers. This leaves
us with four strategies for recursive stacking;
namely to constantly augment the feature set,
with level n classifiers trained on the predictions
of the classifiers at all n? 1 lower levels with or
without the input features x, or simply to train a
level n classifier on the predictions of the level
n? 1 classifiers with or without x.
In this work we stack stacked dependency
parsers by training classifiers on the output of
three stacked dependency parsers and POS tags.
Consequently, we use one of the features from x.
Note that we train classifiers and not parsers on
this new level 2.
The reduction is done the following way: First
we train a classifier on the relative distance from
a word to its head to induce attachments. For
example, we may obtain the following features
from the predictions of our level 1 parsers:
1067
label p1 p2 p3 POS
1 1 -1 1 NNP
0 0 0 0 VBD
In the second row all input parsers, p1?3 in
columnsaa 2?4, agree that the verb is the root of
the sentence. Column 1 tells us that this is cor-
rect. In the first row, two out of three parsers
agree on attaching the noun to the verb, which
again is correct. We train level 2 classifiers on
feature vectors produced this way. Note that or-
acle performance of the ensemble is no upper
bound on the accuracy of a classifier trained on
level 1 predictions this way, since a classifier
may learn the right decision from three wrong
predictions and a POS tag.
Second we train a classifier to predict depen-
dency relations. Our feature vectors are similar
to the ones just described, but now contain de-
pendency label predictions, e.g.:
label p1 p2 p3 POS
SBJ SBJ SBJ SBJ NN
ROOT ROOT ROOT COORD VBN
2.5 Generalized tri-training
Tri-training was originally introduced in Li and
Zhou (2005). The method involves three learners
that inform each other.
Let L denote the labeled data and U the
unlabeled data. Assume that three classifiers
c1, c2, c3 have been trained on L. In the origi-
nal algorithm, the three classifiers are obtained
by applying the same learning algorithm to three
bootstrap samples of the labeled data; but in gen-
eralized algorithms, three different learning al-
gorithms are used. An unlabeled datapoint in
U is labeled for a classifier, say c1, if the other
two classifiers agree on its label, i.e. c2 and c3.
Two classifiers inform the third. If the two clas-
sifiers agree on a labeling, we assume there is a
good chance that they are right. In the original
algorithm, learning stops when the classifiers no
longer change; in generalized tri-training, a fixed
stopping criterion is used. The three classifiers
are combined by voting. Li and Zhou (2005)
show that under certain conditions the increase
in classification noise rate is compensated by the
amount of newly labeled data points.
The most important condition is that the
three classifiers are diverse. If the three clas-
1: for i ? {1..3} do
2: ci ? train classifier (li, L)
3: end for
4: repeat
5: for i ? {1..3} do
6: for x ? U do
7: Li ? ?
8: if cj(x) = ck(x)(j, k 6= i) then
9: Li ? Li ? {(x, cj(x)}
10: end if
11: end for
12: ci ? train classifier(li, L ? Li)
13: end for
14: until stopping criterion is met
15: apply c1
Figure 1: Generalized tri-training.
sifiers are identical, tri-training degenerates to
self-training. As already mentioned, Li and
Zhou (2005) obtain this diversity by training
classifiers on bootstrap samples. In their exper-
iments, they consider classifiers based on deci-
sion trees, BP neural networks and na??ve Bayes
inference.
In this paper we generalize the tri-training al-
gorithm and use three different learning algo-
rithms rather than bootstrap samples to create
diversity: a na??ve Bayes algorithm (no smooth-
ing), random forests (Breiman, 2001) (with 100
unpruned decision trees) and an algorithm that
induces unpruned decision trees. The overall al-
gorithm is sketched in Figure 1 with li a learning
algorithm.
Our weights are those of the random forest
classifier after a fixed number of rounds. The
attachment classifier iterates once over the unla-
beled data, while the dependency relations clas-
sifier uses three iterations. The optimal number
of iterations could of course be estimated on de-
velopment data instead. Given the weights for an
input sentence we use CLE to find its most likely
dependency tree.
2.6 Related work
This paper uses stacking rather than voting to
construct ensembles, but voting has been more
1068
widely used in dependency parsing than stack-
ing. Voting was first introduced in dependency
parsing in Zeman and Zabokrtsky (2005). Sagae
and Lavie (2006) later used weighted voting and
reparsing, i.e. using CLE to find the dependency
tree that reflects the maximum number of votes.
They also showed that binning the vote over
part-of-speech tags led to further improvements.
This set-up was adopted by Hall et al (2007) in
the best performing system in the CONLL 2007
Shared Task. Fishel and Nivre (2009) later ex-
perimented with binning the vote on other fea-
tures with modest improvements.
Semi-supervised dependency parsing has only
recently been explored, and failures have been
more frequent than successes. There are,
however, noteable exceptions such as Koo et
al. (2008), Wang et al (2008), Suzuki et
al. (2009) and Sagae and Gordon (2009).
The semi-supervised methods employed in
these experiments are very different from more
traditional scenarios such as self-training and co-
training. Two approaches (Koo et al, 2008;
Sagae and Gordon, 2009) use clusters obtained
from large amounts of unlabeled data to augment
their labeled data by introducing new features,
and two approaches (Wang et al, 2008; Suzuki et
al., 2009) combine probability distributions ob-
tained from labeled data with probability distri-
butions obtained from unlabeled data.
Successes with self-training and co-training
are rare, and several authors report negative re-
sults, e.g. Spreyer and Kuhn (2009). A note-
able exception in constituent-based parsing is the
work of McClosky et al (2006) who show that
self-training is possible if a reranker is used to
inform the underlying parser.
Sagae and Tsujii (2007) participated in (and
won) the CONLL 2007 Shared Task on do-
main adaptation. They first trained a max-
imum entropy-based transition-based depen-
dency parser on the out-of-domain labeled data
and an SVM-based transition-based dependency
parser on the reversed out-of-domain labeled
data. The two parsers parse the in-domain la-
beled data (reversed, in the case of the SVM-
based parser). Identical analyses are added to the
original training set. The first parser is retrained
and used to parse the test data. In sum, the au-
thors do one round of co-training with the fol-
lowing selection criterion: If the two parsers pro-
duce the same dependency structures for a sen-
tence, the dependency structure is added to the
labeled data. This criterion is also the selection
criterion in tri-training.
3 Experiments
3.1 Data
We use five datasets from the CONLL-X Shared
Task (Buchholz and Marsi, 2006).1 Lemmas and
morphological features (FEATS) are ignored,
since we only add POS and CPOS tags to un-
labeled data. For German and Swedish, we
use 100,000 sentences from the Leipzig Corpora
Collection (Biemann et al, 2007) as unlabeled
data. For Danish, Dutch, and Portuguese we
use 100,000 sentences from the Europarl cor-
pus (Koehn, 2005). The data characteristics are
provided in Figure 2. The unlabeled data were
POS tagged using the freely available SVMTool
(Gimenez and Marquez, 2004) (model 4, left-
right-left).
3.2 Algorithm
Once our data has been prepared, we train the
stacked dependency parsers and use them to la-
bel training data for our classifiers (?4,000 to-
kens), our test data and our unlabeled data. This
gives us three sets of predictions for each of the
three data sets. Using the features described in
Sect. 2.4 we then construct data for training our
two triads of classifiers (for attachment and de-
pendency relations). The entire architecture can
be depicted as in Figure 3.
We first stack three dependency parsers as
described in Martins et al (2008). We then
stack three classifiers on top of these dependency
parsers (and POS tags): a na??ve Bayes classifier,
a random forest, and a decision tree. Finally,
1The CONLL-X Shared Task consists of 12 datasets,
but we did not have consistently tokenized unlabeled data
for Arabic, Chinese, Japanese, Slovene and Turkish. Mar-
tins et al (2008) ignore Czech. Our experiment with the
Spanish dataset crashed unexpectedly. We will post results
on the website as soon as possible.
1069
tokens sents tokens/sents POSs DEPRELs
Danish train 94,386 5,190 18.2 24 52
unl (Europarl) 2,422,144 100,000 24.2 - -
test 5,852 322 18.2 - -
Dutch train 195,069 13,349 14.6 13 26
unl (Europarl) 2,336,176 100,000 23.4 - -
test 5,585 386 14.5 - -
German train 699,610 39,216 17.8 52 46
unl (LCC) 1,763,281 100,000 17.6 - -
test 5,694 357 15.9 - -
Portuguese train 206,678 9,071 22.3 21 55
unl (Europarl) 2,882,967 100,000 28.8 - -
test 5,867 288 22.8 - -
Swedish train 191,467 11,042 17.4 37 56
unl (LCC) 1,727,068 100,000 17.3 - -
test 5,656 389 14.5 - -
Figure 2: Characteristics of the data sets.
tri-training
.
.
.
nb forests tree
stacking
mst2/mst2 malt/mst2 malt/mst1
stacking
mst2 malt mst1
Figure 3: Tri-training stacked classifiers.
we tri-train these three stacked classifiers and for
each test sentence output the weights provided
by the random forest classifier. These weights
are used to find the best possible dependency tree
using CLE.
3.3 Baselines
The best of the stacked input parsers is of course
our natural baseline.
Since we have generalized tri-training, we
also compare generalized tri-training to the orig-
inal tri-training algorithm based on bootstrap
samples. The original tri-training algorithm
is run with the same decomposition and the
same features as our generalized tri-training al-
gorithm. We use the learning algorithm orig-
inally used in Li and Zhou (2005), namely
C4.5. We also compare our results to self-
training (no pool, no growth rate) and co-forests
(Li and Zhou, 2007). Finally, we compare our
results to semi-supervised support vector ma-
chines (S3VMs) (Sindhwani and Keerthi, 2006).
Since S3VMs produce binary classifiers, and
one-vs.-many combination would be very time-
consuming, we train a binary classifier that pro-
duces a probability that any candidate arc is cor-
rect and do greedy head selection. We optimized
the feature set and included a total of seven fea-
tures (head POS, dependent POS, dependent left
neighbor POS, distance+direction, predictions of
the three classifiers).
4 Results
Our results are presented in Figure 4. Labeled
(LAS) and unlabeled attachment scores (UAS)
and labeling accuracy (LA) are defined as usual
and include punctuation signs unless otherwise
noted. Difference (?) in LAS, error reduction
and p-value compare our results to the best input
stacked parser (malt/mst2, excerpt for Swedish).
Generalized tri-training (tri-training-CLE),
i.e. using CLE to find the best well-formed de-
pendency trees given the weights provided by
our tri-trained random forest classifier, leads to
highly significant improvements on all data sets
(p < 0.001) with an average error reduction of
14,9%. The results for the other semi-supervised
learning algorithms are presented in Figure 5.
We only used 10% of the unlabeled data (10k
sentences) in this experiment and only did un-
labeled parsing, but it is quite evident that these
learning strategies seem less promising than gen-
1070
Danish LAS(%) UAS(%) LA(%) EM(%) ? LAS err.red(%) p-value
mst2 84.64 89.11 91.35 24.84
malt/mst2 86.36 90.50 92.09 27.64
malt/mst1 86.11 90.23 91.87 25.78
tri-training-CLE 87.76 92.11 92.87 27.95 1.40 10.26 <0.0001
tri-training-CLE (excl. pnc.) 87.54 92.61 91.68
CONLL-X best (excl. pnc.) 84.79 90.58 89.22
Martins et al (excl. pnc.) 86.79 - -
Dutch
mst2 80.27 84.32 84.96 23.32
malt/mst2 81.00 84.58 85.46 24.35
malt/mst1 80.72 84.17 85.34 26.17
tri-training-CLE 83.42 88.18 87.82 28.00 2.42 12.74 <0.0001
tri-training-CLE (excl. pnc.) 81.73 86.97 86.61
CONLL-X best (excl. pnc.) 79.19 83.57 83.89
Martins et al (excl. pnc.) 81.61 - -
German
mst2 87.32 89.88 93.05 35.85
malt/mst2 88.06 90.53 93.52 40.06
malt/mst1 88.04 90.50 93.48 38.10
tri-training-CLE 90.41 93.22 94.61 43.14 2.35 19.68 <0.0001
tri-training-CLE (excl. pnc.) 90.30 93.49 93.87
CONLL-X best (excl. pnc.) 87.34 90.38 92.11
Martins et al (excl. pnc.) 88.66 - -
Portuguese
mst2 84.83 88.44 92.04 25.69
malt/mst2 85.39 88.80 92.59 28.13
malt/mst1 85.00 88.39 92.23 25.69
tri-training-CLE 88.03 91.89 93.54 29.86 2.64 18.07 <0.0001
tri-training-CLE (excl. pnc.) 89.18 93.69 92.43
CONLL-X best (excl. pnc.) 87.60 91.36 91.54
Martins et al (excl. pnc.) 88.46 - -
Swedish
mst2 81.82 87.36 87.29 27.76
malt/mst2 84.42 89.57 88.68 31.62
malt/mst1 84.74 89.83 89.07 31.11
tri-training-CLE 86.83 92.04 90.65 32.65 2.09 13.70 <0.0001
tri-training-CLE (excl. pnc.) 86.66 92.45 89.58
CONLL-X best (excl. pnc.) 84.58 89.50 87.39
Martins et al (excl. pnc.) 85.16 - -
AV 2.18 14.89
Figure 4: Results on CONLL-X datasets. Scores are including punctuation unless otherwise noted.
? and p-value is difference with respect to best input parser.
UAS malt-mst2 S3VMs self-training orig-tri-training co-forests tri-training tri-training[full]
Danish 90.50 90.47 89.68 89.66 88.79 90.60 92.21
Dutch 84.58 85.34 84.06 83.83 83.97 86.07 88.06
German 90.53 90.15 89.83 89.92 88.47 90.81 93.20
Portuguese 88.80 65.64 87.60 87.62 87.06 89.16 91.87
Swedish 89.83 81.46 89.09 89.20 88.65 90.22 92.24
AV 88.80 82.61 88.05 88.05 87.44 89.37 91.52
Figure 5: Comparison of different semi-supervised learning algorithms (10% of unlabeled data)
using 2-fold CV and no reparsing, UAS including punctuation.
1071
eralized tri-training.
5 Error analysis and discussion
Error reductions are higher with dependencies
to the root node and long distance dependencies
than with local dependencies. The table below
lists the labeled attachment F1-scores for the five
datasets binned on dependency length. The av-
erage error reduction is the same for root depen-
dencies and long distance dependencies (length
>7), but significantly lower for local dependen-
cies. This seems to indicate that large amounts of
data are necessary for the parser to recover long
distance dependencies.
root 1 2 4?7 >7
Da(F1) 98.45 96.21 92.09 88.17 90.93
? err.red 41.34 10.69 13.92 15.75 21.92
Du(F1) 83.65 94.47 88.60 82.40 81.54
? err.red 28.39 16.74 20.72 17.00 31.88
Ge(F1) 97.33 96.47 94.28 92.42 93.94
? err.red 26.65 19.77 17.46 25.25 38.97
Po(F1) 96.23 97.05 95.17 84.80 87.11
? err.red 22.47 19.56 24.86 22.56 26.97
Sw(F1) 96.37 95.67 93.46 88.42 89.57
? err.red 32.85 14.10 15.04 25.97 31.50
AV err.red 30.34 16.17 18.40 21.31 30.25
Our results for Danish, Dutch, German and
Portuguese are to the best of our knowledge the
best reported results in the literature. Zhang and
Chan (2009) obtain a LAS of 87.20 for Swedish
with transition-based parsing based on reinforce-
ment learning. They evaluate their system on
a subset of the CONLL-X datasets and obtain
their (by far) best improvement on the Swedish
dataset. They speculate that ?the reason might
be that [long distance dependencies] are not pop-
ular in Swedish?. Since our parser is particu-
larly good at long distance dependencies, this
may also explain why a supervised parser outper-
forms our system on this dataset. Interestingly,
our unlabeled attachment score is a lot better
than the one reported by Zhang and Chan (2009),
namely 92.45 compared to 91.84.
Generally, our UASs are better than our LASs.
Since we separate attachment and labeling out
in two independent steps, improvements in UAS
and improvements in LA do not necessarily lead
to improvements in LAS. While our average er-
ror reduction in LAS is 14.9%, our average error
reductions in UAS is 23.6%. The average error
reduction in LA is 14.0%. In two-stage depen-
dency parsers or dependency parsers with joint
models, improvements in UAS are typically fol-
lowed by comparable improvements in LAS.
6 Conclusion
This paper showed how the stacked depen-
dency parsers introduced in Martins et al (2008)
can be improved by inference from unlabeled
data. Briefly put, we stack three diverse clas-
sifiers on triads of stacked dependency parsers
and let them label unlabeled data for each
other in a co-training-like architecture. Our
average error reductions in LAS over the best
of our stacked input parsers is 14.9%; in
UAS, it is 23.6%. The code is available at
http://cst.dk/anders/tridep.html.
References
Abney, Steven. 2008. Semi-supervised learning for
computational linguistics. Chapman & Hall.
Bennett, Kristin, Ayhan Demiriz, and Richard
Maclin. 2003. Exploiting unlabeled data in en-
semble methods. In KDD.
Biemann, Chris, G. Heyer, U. Quasthoff, and
M. Richter. 2007. The Leipzig corpora collection.
In Corpus Linguistics.
Blum, Avrim and Tom Mitchell. 1998. Combining
labeled and unlabeled with-co-training. In COLT.
Breiman, Leo. 2001. Random forests. Machine
Learning, 45:5?32.
Buchholz, Sabine and Erwin Marsi. 2006. CONLL-
X shared task on multilingual dependency parsing.
In CONLL.
Chen, Wenliang, Yujie Zhang, and Hitoshi Isahara.
2006. Chinese chunking with tri-training learning.
In Computer processing of oriental languages,
pages 466?473. Springer, Berlin, Germany.
Didaci, Luca and Fabio Roli. 2006. Using co-
training and self-training in semi-supervised mul-
tiple classifier systems. In SSPR& SPR, pages
522?530. Springer, Berlin, Germany.
Edmonds, J. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards,
71:233?240.
1072
Fishel, Mark and Joakim Nivre. 2009. Voting and
stacking in data-driven dependency parsing. In
NODALIDA.
Gimenez, Jesus and Lluis Marquez. 2004. SVM-
Tool: a general POS tagger generator based on
support vector machines. In LREC.
Hady, Mohamed and Friedhelm Schwenker. 2008.
Co-training by committee. International Journal
of Software and Informatics, 2:95?124.
Hall, Johan, Jens Nilsson, Joakim Nivre, Gulsen
Eryigit, Beata Megyesi, Mattias Nilsson, and
Markus Saers. 2007. Single malt or blended? In
CONLL.
Koehn, Philipp. 2005. Europarl: a parallel corpus for
statistical machine translation. In MT-Summit.
Koo, Terry, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency pars-
ing. In ACL.
Li, Ming and Zhi-Hua Zhou. 2005. Tri-training:
exploiting unlabeled data using three classifiers.
IEEE Transactions on Knowledge and Data En-
gineering, 17(11):1529?1541.
Li, Ming and Zhi-Hua Zhou. 2007. Improve
computer-aided diagnosis with machine learning
techniques using undiagnosed samples. IEEE
Transactions on Systems, Man and Cybernetics,
37(6):1088?1098.
Martins, Andre?, Dipanjan Das, Noah Smith, and Eric
Xing. 2008. Stacking dependency parsers. In
EMNLP.
McClosky, David, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
HLT-NAACL.
McDonald, Ryan, Fernando Pereira, Kiril Ribarov,
and Jan Hajic?. 2005. Non-projective dependency
parsing using spanning tree algorithms. In HLT-
EMNLP.
Milidiu, Ruy and Julio Duarte. 2009. Improv-
ing BAS committee performance with a semi-
supervised approach. In European Symposium on
Artificial Neural Networks.
Nguyen, Tri, Le Nguyen, and Akira Shimazu. 2008.
Using semi-supervised learning for question clas-
sification. Journal of Natural Language Process-
ing, 15:3?21.
Nivre, Joakim and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In ACL-HLT.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser.
Natural Language Engineering, 13(2):95?135.
Sagae, Kenji and Andrew Gordon. 2009. Cluster-
ing words by syntactic similarity improves depen-
dency parsing of predicate-argument structures. In
IWPT.
Sagae, Kenji and Alon Lavie. 2006. Parser combina-
tion by reparsing. In HLT-NAACL.
Sagae, Kenji and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with lr models and
parser ensembles. In EMNLP-CONLL.
Sindhwani, Vikas and Sathiya Keerthi. 2006. Large
scale semi-supervised linear SVMs. In ACM SI-
GIR.
S?gaard, Anders. 2010. Simple semi-supervised
training of part-of-speech taggers. In ACL.
Spreyer, Kathrin and Jonas Kuhn. 2009. Data-driven
dependency parsing of new languages using in-
complete and noisy training data. In CONLL.
Surdeanu, Mihai and Christopher Manning. 2010.
Ensemble models for dependency parsing: cheap
and good? In NAACL.
Suzuki, Jun, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. Semi-supervised convex
training for dependency parsing. In EMNLP.
Wang, Qin, Dekang Lin, and Dale Schuurmans.
2008. Semi-supervised convex training for depen-
dency parsing. In ACL.
Wolpert, David. 1992. Stacked generalization. Neu-
ral Networks, 5:241?259.
Zeman, Daniel and Zdene?k ?Zabokrtsky?. 2005. Im-
proving parsing accuracy by combining diverse
dependency parsers. In IWPT.
Zhang, Lidan and Kwok Chan. 2009. Dependency
parsing with energy-based reinforcement learning.
In IWPT.
Zhou, Zhi-Hua. 2009. When semi-supervised learn-
ing meets ensemble learning. In MCS.
1073
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 803?807,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
With blinkers on: robust prediction of eye movements across readers
Franz Matties and Anders S?gaard
University of Copenhagen
Njalsgade 142
DK-2300 Copenhagen S
Email: soegaard@hum.ku.dk
Abstract
Nilsson and Nivre (2009) introduced a tree-
based model of persons? eye movements in
reading. The individual variation between
readers reportedly made application across
readers impossible. While a tree-based model
seems plausible for eye movements, we show
that competitive results can be obtained with
a linear CRF model. Increasing the inductive
bias also makes learning across readers pos-
sible. In fact we observe next-to-no perfor-
mance drop when evaluating models trained
on gaze records of multiple readers on new
readers.
1 Introduction
When we read a text, our gaze does not move
smoothly and continuously along its lines. Rather,
our eyes fixate at a word, then skip a few words,
to jump to a new fixation point. Such rapid eye
movements are called saccades. Sometimes we even
jump backwards. Backward saccades are called re-
gressions. Gaze can be recorded using eye track-
ing devices (Starr and Rayner, 2001). Since eye
movements in reading give us important information
about what readers find complicated in a text, and
what readers find completely predictable, predicting
eye movements on new texts has many practical ap-
plications in text-to-text generation and human com-
puter interaction, for example.
The problem of predicting eye movements in
reading is, for a reader ri and a given sequence of
word tokens w1 . . . wn, to predict a set of fixation
points F ? {w1, . . . , wn}, i.e., the fixation points of
ri?s gaze. For each token wj , the reader ri may skip
wj or fixate at wj . Models are evaluated on record-
ings of human reading obtained using eye tracking
devices. The supervised prediction problem that we
consider in this paper, also uses eye tracking data for
learning models of eye movement.
Nilsson and Nivre (2009) first introduced this su-
pervised learning task and used the Dundee corpus
to train and evaluate a tree-based model, essentially
treating the problem of predicting eye movements in
reading as transition-based dependency parsing.
We follow Hara et al (2012) in modeling only
forward saccades and not regressions and refix-
ations. While Nilsson and Nivre (2009) try to
model a subset of regressions and refixations, they
do not evaluate this part of their model focusing
only on fixation accuracy and distribution accuracy,
i.e., they evaluate how well they predict a set
of fixation points rather than a sequence of points in
order. This enables us to model eye movements in
reading as a sequential problem of determining the
length of forward saccades, increasing the inductive
bias of our learning algorithm in a motivated way.
Note that because we work with visual input, we
do not tokenize our input in our experiments, i.e.,
punctuation does not count as input tokens.
Example Figure 1 presents an example sentence
and gaze records from the Dundee corpus. The
Dundee corpus contains gaze records of 10 readers
in total. Note that there is little consensus on what
words are skipped. 5/10 readers skip the first word.
Generally, closed class items (prepositions, copulae,
quantifiers) seem to be skipped more open, but we
do see a lot of individual variation. While others for
this reason have refrained from evaluation across
readers (Nilsson and Nivre, 2009; Hara et al, 2012),
803
Sentence
Are tourists enticed by these attractions threathening their very existence?
r1 Fixate Fixate Fixate Skip Fixate Fixate Fixate Skip Fixate Fixate
r2 Fixate Fixate Fixate Fixate Fixate Fixate Fixate Fixate Fixate Fixate
r3 Fixate Fixate Fixate Fixate Fixate Fixate Fixate Fixate Skip Fixate
r4 Skip Fixate Fixate Skip Fixate Fixate Fixate Fixate Fixate Fixate
r5 Skip Fixate Fixate Skip Fixate Fixate Fixate Skip Fixate Fixate
r6 Skip Fixate Fixate Skip Fixate Fixate Fixate Fixate Skip Fixate
r7 Skip Fixate Fixate Fixate Fixate Fixate Fixate Fixate Fixate Fixate
r8 Fixate Fixate Fixate Fixate Fixate Fixate Fixate Fixate Fixate Fixate
r9 Fixate Fixate Fixate Fixate Fixate Fixate Fixate Fixate Fixate Fixate
r10 Skip Fixate Fixate Fixate Fixate Fixate Fixate Fixate Skip Fixate
# skips 5 0 0 4 0 0 0 2 3 0
Figure 1: The gaze records of the three first readers for the first sentence in the Dundee corpus.
we show that our model predicts gaze better across
readers than a previously proposed model (Nilsson
and Nivre, 2009) does training and evaluating on the
same readers. A final observation is that fixations
are very frequent at the word level ? in fact, even
skilled readers make 94 fixations per 100 words
(Starr and Rayner, 2001) ? which motivates using
F1-score of skips as metric. We follow Nilsson and
Nivre (2009) in reporting word-level accuracy, but
find it particularly interesting that the simple model
proposed here outperforms previous models by a
large margin in F1-score over skips.
Related work Below we use a sequential model
rather than a tree-based model to bias our model
toward predicting forward saccades. Nilsson and
Nivre (2009), in contrast, present a more expressive
tree-based model for modeling eye movements, with
some constraints on the search space. The transition-
based model uses consecutive classification rather
than structured prediction. The features used in their
model are very simple. In particular, they use use
word lengths and frequencies, like us, as well as
distances between tokens (important in a transition-
based model), and, finally, the history of previous
decisions.
Hara et al (2012) use a linear CRF model for the
same problem, like us, but they consider a slightly
different problem, namely that of predicting eye
movement when reading text on a specific screen.
They therefore use screen position as a feature. In
addition, they use word forms, POS, various mea-
sures of surprise of word length, as well as per-
plexity of bi- and trigrams. The features relating to
screen position were the most predictive ones.
2 Our approach
We use linear CRFs to model eye movements in
reading. We follow Hara et al (2012) in using small
window sizes (at most five words) for extracting fea-
tures. Rather than using word forms, POS, etc.,
we use only word length and the log probability of
words ? both known to correlate well with likeli-
hood of fixation, as well as fixation times (McDon-
ald and Shillcock, 2012; Kliegl et al, 2004; Rein-
gold et al, 2012). The model thus reflects a hy-
pothesis that eye movements are largely unaffected
by semantic content, that eye movements depend on
the physical properties and frequency of words, and
that there is a sequential dependence between fixa-
tion times. Tabel 1 gives the complete set of fea-
tures. We also evaluated using word forms and POS
on held-out data, but this did not lead to improve-
ments. There is evidence for the impact of mor-
phology on eye movements (Liversedge and Blythe,
2007; Bertram, 2011), but we did not incorporate
this into our model. Finally, we did not incorporate
predictability of tokens, although this is also known
to correlate with fixation times (Kliegl et al, 2004).
Hara et al (2012) use perplexity features to capture
this.
We use a publicly available implementation of lin-
ear CRFs1 with default parameters (L2-regularized,
C = 1).
1https://code.google.com/p/crfpp/
804
3 Predicting a reader?s eye movements
In this experiment we consider exactly the same set-
up as Nilsson and Nivre (2009) considered. In the
Dundee corpus, we have gaze data for 10 persons.
The corpus consists of 2,379 sentences, 56,212 to-
kens and 9,776 types. The gaza data was recorded
using a Dr. Bouis Oculometer Eyetracker, sampling
the position of the right eye every millisecond. We
use texts 1?16 (1911 sentences) for training, 17?
18 (237 sentences) for development and 19?20 (231
sentences) for testing.
Results are presented in Table 2 and are slightly
better than Nilsson and Nivre (2009), mainly be-
cause of better predictions of skips. Our error re-
duction over their model in terms of F1 over skips
is 9.4%. The baseline model used in Nilsson and
Nivre (2009), the E-Z Reader (Reichle et al, 1998),
obtained a fixation accuracy of 57.7%.
4 Predicting across readers
Hara et al (2012) consider the problem of learning
from the concatenation of the gaze data from the 10
persons in the Dundee corpus, but they also evalu-
ate on data from these persons. In our second ex-
periment, we consider the more difficult problem of
learning from one person?s gaze data, but evaluat-
ing on gaze data from another test person. This is a
more realistic scenario if we want to use our model
to predict eye movements in reading on anyone but
our test persons. This has been argued to be impossi-
ble in previous work (Nilsson and Nivre, 2009; Hara
et al, 2012).
Our results are presented in Table 3. Interestingly,
results are very robust across reader pairs. In fact,
only in 4/10 cases do we get the best results training
on gaze data from the reader we evaluate on. Note
also that the readers seem to form two groups ? (a, b,
h, i, j) and (c, d, e, f, g) ? that provide good training
material for each other. Training on concatenated
data from all members in each group may be benefi-
cial.
5 Learning from multiple readers
In our final experiment, we learn from the gaze
records of nine readers and evaluate on the tenth.
This is a realistic evaluation of our ability to predict
fixations for new, previously unobserved readers. In-
terestingly we can predict the fixations of new read-
ers better than Nilsson and Nivre (2009) predict fix-
ations when the training and test data are produced
by the same reader. The results are presented in Ta-
ble 4. In fact our skip F1 score is actually better than
in our first experiments. As already mentioned, this
result can probably be improved by using a subset of
readers or by weighting training examples, e.g., by
importance weighting (Shimodaira, 2000). For now,
this is left for future work.
6 Discussion
Our contributions in this paper are: (i) a model for
predicting a reader?s eye movements that is compet-
itive to state-of-the-art, but simpler, with a smaller
search space than Nilsson and Nivre (2009) and a
smaller feature model than Hara et al (2012), (ii)
showing that the simpler model is robust enough to
model eye movements across readers, and finally,
(iii) showing that even better models can be obtained
training on records from multiple readers.
It is interesting that a model without lexical infor-
mation is more robust across readers. This suggests
that deep processing has little impact on eye move-
ments. See Starr and Rayner (2001) for discussion.
The features used in this study are well-motivated
and account as well for the phenomena as previously
proposed models. It would be interesting to incor-
porate morphological features and perplexity-based
features, but we leave this for future work.
7 Conclusion
This study is, to the best of our knowledge, the first
to consider the problem of learning to predict eye
movements in reading across readers. We present
a very simple model of eye movements in read-
ing that performs a little better than Nilsson and
Nivre (2009) in terms of fixation accuracy, evaluated
on one reader at a time, but predicts skips signifi-
cantly better. The true merit of the approach, how-
ever, is its ability to predict eye movements across
readers. In fact, it predicts the eye movements of
new readers better than Nilsson and Nivre (2009) do
when the training and test data are produced by the
same reader.
805
References
Raymond Bertram. 2011. Eye movements and morpho-
logical processing in reading. The Mental Lexicon,
6:83?109.
Tadayoshi Hara, Daichi Mochihashi, Yoshinobu Kano,
and Akiko Aizawa. 2012. Predicting word fixation
in text with a CRF model for capturing general read-
ing strategies among readers. In Workshop on Eye-
tracking and NLP, COLING.
Reinhold Kliegl, Ellen Grabner, Martin Rolfs, and Ralf
Engbert. 2004. Length, frequency, and predictability
effects of words on eye movements in reading. Euro-
pean Journal of Cognitive Psychology, 16:262?284.
Simon Liversedge and Hazel Blythe. 2007. Lexical and
sublexical influences on eye movements during read-
ing. Language and Linguistic Compass, 1:17?31.
Scott McDonald and Richard Shillcock. 2012. Eye
movements reveal the on-line computation of lexical
probabilities during reading. Psychological Science,
14:648?652.
Matthias Nilsson and Joakim Nivre. 2009. Learning
where to look: Modeling eye movements in reading.
In CoNLL.
Erik Reichle, Alexander Pollatsek, Donald Fisher, and
Keith Rayner. 1998. Toward a model of eye
movement control in reading. Psychological Review,
105:125?157.
Eyal Reingold, Erik Reichle, Mackenzie Glaholt, and
Heather Sheridan. 2012. Direct lexical control of
eye movements in reading. Cognitive Psychology,
65:177?206.
Hidetoshi Shimodaira. 2000. Improving predictive in-
ference under covariate shift by weighting the log-
likelihood function. Journal of Statistical Planning
and Inference, 90:227?244.
Matthew Starr and Keith Rayner. 2001. Eye movements
during reading: some current controversies. Trends in
Cognitive Science, 5:156?163.
806
Feature Description
WordLength {L?2, L?1, L0, L1, L2} The number of letters for a token
WordProbability {P?1, P0, P1} The log probability of a word (rounded) as
given in the Dundee data
Table 1: Feature template
Fixation Accuracy Fixations (F1) Skips (F1)
Reader N&N Model N&N Model N&N Model
a 70.0 70.2 71.8 70.0 67.4 70.3
b 66.5 66.2 74.1 71.2 75.0 58.8
c 70.9 70.4 77.3 74.7 59.4 64.4
d 78.9 76.5 84.7 81.3 65.9 68.5
e 71.8 70.5 73.5 69.9 69.9 71.0
f 67.9 66.4 76.8 72.8 47.7 55.8
g 56.6 65.1 61.7 61.8 49.9 67.8
h 66.9 67.7 72.7 70.3 58.2 64.6
i 69.1 71.5 74.1 73.9 60.7 68.8
j 76.3 74.6 82.0 77.3 65.2 71.1
average 69.5 69.9 75.2 72.3 62.6 66.1
Table 2: Comparison between NN09 and our model.
train/test a b c d e f g h i j
a - 67.2 67.6 71.5 69.7 63.4 64.9 66.9 70.7 72.6
b 67.7 - 70.1 76.9 68.0 65.7 62.9 67.1 69.1 72.8
c 69.3 67.3 - 76.5 69.7 65.1 64.3 67.4 71.0 74.2
d 69.0 67.2 70.0 - 69.1 65.1 63.9 67.3 70.1 73.9
e 70.1 66.6 67.5 71.2 - 63.8 64.7 66.9 70.9 72.6
f 66.5 65.9 69.1 76.7 66.5 - 62.4 66.8 68.6 71.4
g 69.7 67.1 67.2 69.5 69.6 61.6 - 67.8 70.3 70.3
h 70.5 67.5 69.3 74.7 70.5 64.2 64.5 - 70.8 74.2
i 70.9 68.1 69.6 74.4 70.7 64.0 64.6 68.0 - 74.2
j 70.7 68.0 69.5 74.7 70.4 64.1 64.7 68.2 71.5 -
Table 3: Results learning across readers. Bold-faced numbers better than when training on same reader
Fixation Accuracy Fixations (F1) Skips (F1)
Reader N&N Model N&N Model N&N Model
a 70.0 70.3 71.8 72.1 67.4 68.2
b 66.5 67.9 74.1 70.6 75.0 64.6
c 70.9 69.8 77.3 73.1 59.4 65.6
d 78.9 75.5 84.7 79.5 65.9 69.5
e 71.8 70.6 73.5 72.0 69.9 69.0
f 67.9 64.5 76.8 68.6 47.7 59.2
g 56.6 64.7 61.7 65.0 49.9 64.5
h 66.9 68.1 72.7 70.9 58.2 64.8
i 69.1 71.3 74.6 74.1 60.7 67.9
j 76.3 74.2 82.0 77.2 65.2 70.4
average 69.5 69.7 75.2 72.3 62.6 66.4
Table 4: Comparison of NN09 and our cross-reader model trained on nine readers
807
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1476?1480,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Using crowdsourcing to get representations based on regular expressions
Anders S?gaard and Hector Martinez and Jakob Elming and Anders Johannsen
Center for Language Technology
University of Copenhagen
DK-2300 Copenhagen S
{soegaard|alonso|zmk867|ajohannsen}@hum.ku.dk
Abstract
Often the bottleneck in document classifica-
tion is finding good representations that zoom
in on the most important aspects of the doc-
uments. Most research uses n-gram repre-
sentations, but relevant features often occur
discontinuously, e.g., not. . . good in sentiment
analysis. In this paper we present experi-
ments getting experts to provide regular ex-
pressions, as well as crowdsourced annota-
tion tasks from which regular expressions can
be derived. Somewhat surprisingly, it turns
out that these crowdsourced feature combina-
tions outperform automatic feature combina-
tion methods, as well as expert features, by a
very large margin and reduce error by 24-41%
over n-gram representations.
1 Introduction
Finding good representations of classification prob-
lems is often glossed over in the literature. Sev-
eral authors have emphasized the need to pay more
attention to finding such representations (Wagstaff,
2012; Domingos, 2012), but in document classifica-
tion most research still uses n-gram representations.
This paper considers two document classification
problems where such representations seem inade-
quate. The problems are answer scoring (Burstein
et al, 1998), on data from stackoverflow.com, and
multi-attribute sentiment analysis (McAuley et al,
2012). We argue that in order to adequately repre-
sent such problems we need discontinuous features,
i.e., regular expressions.
The problem with using regular expressions as
features is of course that even with a finite vocab-
ulary we can generate infinitely many regular ex-
pressions that match our documents. We suggest to
use expert knowledge or crowdsourcing in the loop.
In particular we present experiments where standard
representations are augmented with features from a
few hours of manual work, by machine learning ex-
perts or by turkers.
Somewhat surprisingly, we find that features de-
rived from crowdsourced annotation tasks lead to the
best results across the three datasets. While crowd-
sourcing of annotation tasks has become increasing
popular in NLP, this is, to the best of our knowledge,
the first attempt to crowdsource the problem of find-
ing good representations.
1.1 Related work
Musat et al (2012) design a collaborative two-player
game for sentiment annotation and collecting a sen-
timent lexicon. One player guesses the sentiment of
a text and picks a word from it that is representative
of its sentiment. The other player also provides a
guess observing only this word. If the two guesses
agree, both players get a point. The idea of gam-
ifying the problem of finding good representations
goes beyond crowdsourcing, but is not considered
here. Boyd-Graber et al (2012) crowdsource the
feature weighting problem, but using standard rep-
resentations. The work most similar to ours is prob-
ably Tamuz et al (2011), who learn a ?crowd kernel?
by asking annotators to rate examples by similarity,
providing an embedding that promotes feature com-
binations deemed relative when measuring similar-
ity.
1476
BoW Exp AMT
n P (1) m ?x m ?x m ?x
STACKOVERFLOW 97,519 0.5013 30,716 0.00131 1,156 0.1380 172,691 0.00331
TASTE 152,390 0.5003 38,227 0.00095 666 0.10631 114,588 0.00285
APPEARANCE 152,331 0.5009 37,901 0.00097 650 0.14629 102,734 0.00289
Table 1: Characteristics of the n?m data sets
2 Experiments
Data The three datasets used in our experi-
ments come from two sources, namely stackover-
flow.com and ratebeer.com. The two beer review
datasets (TASTE and APPEARANCE) are described
in McAuley et al (2012) and available for down-
load.1 Each input example is an unstructured review
text, and the associated label is the score assigned to
taste or appearance by the reviewer. We randomly
sample about 152k data points, as well as 500 exam-
ples for experiments with experts and turks.
We extracted the STACKOVERFLOW dataset from
a publicly available data dump,2, and we briefly de-
scribe our sampling process here. We select pairs of
answers, where one is ranked higher than the other
by stackoverflow.com users. Obviously the answers
submitted first have a better chance of being ranked
highly, so we also require that the highest ranked
answer was submitted last. From this set of answer
pairs, we randomly sample 97,519 pairs, as well as
500 examples for our experiments with experts and
turks.
Our experiments are classification experiments
using the same learning algorithm in all experi-
ments, namely L1-regularized logistic regression.
We don?t set any parameters The only differences
between our systems are in the feature sets. Results
are from 5-fold cross-validation. The four feature
sets are described below: BoW, HI, Exp and AMT.
For motivating using regular expressions, con-
sider the following sentence from a review of John
Harvard?s Grand Cru:
(1) Could have been more flavorful.
The only word carrying direct sentiment in this
sentence is flavorful, which is positive, but the sen-
tence is a negative evaluation of the Grand Cru?s
1http://snap.stanford.edu/data/web-RateBeer.html
2http://www.clearbits.net/torrents/2076-aug-2012
taste. The trigram been more flavorful seems neg-
ative at first, but in the context of negation or in a
comparative, it can become positive again. How-
ever, note that this trigram may occur discontinu-
ously, e.g., in been less watery and more flavorful.
In order to match such occurrences, we need simple
regular expressions, e.g.,:
been.*more.*flavorful
This is exactly the kind of regular expressions we
asked experts to submit, and that we derived from
the crowdsourced annotation tasks. Note that the
sentence says nothing about the beer?s appearance,
so this feature is only relevant in TASTE, not in
APPEARANCE.
BoW and BoW+HI Our most simple baseline ap-
proach is a bag-of-words model of unigram features
(BoW). We lower-case our data, but leave in stop
words. We also introduce a semantically enriched
unigram model (BoW)+HI, where in addition to
representing what words occur in a text, we also
represent what Harvard Inquirer (HI)3 word classes
occur in it. The HI classes are used to generate
features from the crowdsourced annotation tasks,
so the semantically enriched unigram model is an
important baseline in our experiments below.
BoW+Exp In order to collect regular expressions
from experts, we set up a web interface for query-
ing held-out portions of the datasets with regular ex-
pressions that reports how occurrences of the sub-
mitted regular expressions correlate with class. We
used the Python re syntax for regular expressions
after augmenting word forms with POS and seman-
tic classes from the HI. Few of the experts made use
of the POS tags, but many regular expressions in-
cluded references to HI classes.
3http://www.wjh.harvard.edu/ inquirer/homecat.htm
1477
Regular expressions submitted by participants
were visible to other participants during the exper-
iment, and participants were allowed to work to-
gether. Participants had 15 minutes to familiarize
themselves with the syntax used in the experiments.
Each query was executed in 2-30 seconds.
Seven researchers and graduate students spent
five effective hours querying the datasets with
regular expressions. In particular, they spent three
hours on the Stack Exchange dataset, and one hour
on each of the two RateBeer datasets. One had to
leave an hour early. So, in total, we spent 20 person
hours on Stack Exchange, and seven person hours
on each of the RateBeer datasets. In the five hours,
we collected 1,156 regular expressions for the
STACKOVERFLOW dataset, and about 650 regular
expressions for each of the two RateBeer datasets.
Exp refers to these sets of regular expressions. In
our experiments below we concatenate these with
the BoW features to form BoW+Exp.
BoW+AMT For each dataset, we also had 500 held-
out examples annotated by three turkers each, using
Amazon Mechanical Turk,4 obtaining 1,500 HITs
for each dataset. The annotators were presented with
each text, a review or an answer, twice: once as run-
ning text, once word-by-word with bullets to tick off
words. The annotators were instructed to tick off
words or phrases that they found predictive of the
text?s sentiment or answer quality. They were not in-
formed about the class of the text. We chose this an-
notation task, because it is relatively easy for annota-
tors to mark spans of text with a particular attribute.
This set-up has been used in other applications, in-
cluding NER (Finin et al, 2010) and error detection
(Dahlmeier et al, 2013). The annotators were con-
strained to tick off at least three words, including
one closed class item (closed class items were col-
ored differently). Finally, we only used annotators
with a track record of providing high-quality anno-
tations in previous tasks. It was clear from the aver-
age time spent by annotators that annotating STACK-
OVERFLOW was harder than annotating the Rate-
beer datasets. The average time spent on a Rate-
beer HIT was 44s, while for STACKOVERFLOW it
was 3m:8s. The mean number of words ticked off
4www.mturk.com
BoW HI Exp AMT
STACKOVERF 0.655 0.654 0.683 0.739
TASTE 0.798 0.797 0.798 0.867
APPEARANCE 0.758 0.760 0.761 0.859
Table 2: Results using all features
was between 5.6 and 7, with more words ticked off
in STACKOVERFLOW. The maximum number of
words ticked off by an annotator was 41. We spent
$292.5 on the annotations, including a trial round.
This was supposed to match, roughly, the cost of the
experts consulted for BoW+Exp.
The features generated from the annotations were
constructed as follows: We use a sliding window of
size 3 to extract trigrams over the possibly discon-
tinuous words ticked off by the annotators. These
trigrams were converted into regular expressions by
placing Kleene stars between the words. This gives
us a manually selected subset of skip trigrams. For
each skip trigram, we add copies with one or more
words replaced by one of their HI classes.
Feature combinations This subsection introduces
some harder baselines for our experiments, consid-
ered in Experiment #2. The simplest possible way
of combining unigram features is by considering n-
gram models. An n-gram extracts features from a
sliding window (of size n) over the text. We call this
model BoW(N = n). Our BoW(N = 1) model
takes word forms as features, and there are obvi-
ously more advanced ways of automatically combin-
ing such features.
Kernel representations We experimented with ap-
plying an approximate feature map for the addi-
tive ?2-kernel. We used two sample steps, result-
ing in 4N + 1 features. See Vedaldi and Zimmer-
man (2011) for details.
Deep features We also ran denoising autoen-
coders (Pascal et al, 2008), previously applied
to a wide range of NLP tasks (Ranganath et al,
2009; Socher et al, 2011; Chen et al, 2012), with
2N nodes in the middle layer to obtain a deep
representation of our datasets from ?2-BoW input.
The network was trained for 15 epochs. We set the
drop-out rate to 0.0 and 0.3.
Summary of feature sets The feature sets ? BoW,
1478
10
2
10
3
N
60
62
64
66
68
70
72
74
A
c
c
u
r
a
c
y
StackOverflow
?
2
?BoW
?
2
?BoW+HI
?
2
?BoW+Exp
?
2
?BoW+AMT
10
2
10
3
N
70
75
80
85
90
A
c
c
u
r
a
c
y
Taste
?
2
?BoW
?
2
?BoW+HI
?
2
?BoW+Exp
?
2
?BoW+AMT
10
2
10
3
N
70
75
80
85
90
A
c
c
u
r
a
c
y
Appearance
?
2
?BoW
?
2
?BoW+HI
?
2
?BoW+Exp
?
2
?BoW+AMT
Figure 1: Results selecting N features using ?2 (left to right): STACKOVERFLOW, TASTE, and APPEARANCE. The
x-axis is logarithmic scale.
10
2
10
3
N
60
62
64
66
68
70
72
74
A
c
c
u
r
a
c
y
?
2
?BoW(N=1)
?
2
?BoW(N=2)
?
2
?kernel
?
2
?dauto
?
2
?BoW+AMT
10
2
10
3
N
70
75
80
85
90
A
c
c
u
r
a
c
y
?
2
?BoW(N=1)
?
2
?BoW(N=2)
?
2
?kernel
?
2
?dauto
?
2
?BoW+AMT
10
2
10
3
N
70
72
74
76
78
80
82
84
A
c
c
u
r
a
c
y
?
2
?BoW(N=1)
?
2
?BoW(N=2)
?
2
?kernel
?
2
?dauto
?
2
?BoW+AMT
Figure 2: Results using different feature combination techniques (left to right): STACKOVERFLOW, TASTE, and
APPEARANCE. The x-axis is logarithmic scale.
Exp and AMT ? are very different. Their character-
istics are presented in Table 1. P (1) is the class dis-
tribution, e.g., the prior probability of positive class.
n is the number of data points, m the number of
features. Finally, ?x is the average density of data
points. One observation is of course that the expert
feature set Exp is much smaller than BoW and AMT,
but note also that the expert features fire about 150
times more often on average than the BoW features.
HI is only a small set of additional features.
3 Results
Experiment #1: BoW vs. Exp and AMT We present
results using all features, as well as results obtained
after selecting k features as ranked by a simple ?2
test. The results using all collected features are pre-
sented in Table 2. The error reduction on STACK-
OVERFLOW when adding crowdsourced features to
our baseline model (BoW+AMT), is 24.3%. On
TASTE, it is 34.2%. On APPEARANCE, it is 41.0%.
The BoW+AMT feature set is bigger than those of
the other models. We therefore report results using
the top-k features as ranked by a simple ?2 test.
The result curves are presented in the three plots in
Fig. 1. With +500 features, BoW+AMT outperforms
the other models by a large margin.
Experiment #2: AMT vs. more baselines The
BoW baseline uses a standard representation that,
while widely used, is usually thought of as a weak
baseline. BoW+HIT did not provide a stronger base-
line. We also show that bigram features, kernel-
based decomposition and deep features do not pro-
vide much stronger baselines either. The result
curves are presented in the three plots in Fig. 2.
BoW+AMT is still significantly better than all other
models with +500 features. Since autoencoders
are consistently worse than denoising autoencoders
(drop-out 0.3), we only plot denoising autoencoders.
4 Conclusion
We presented a new method for deriving feature
representations from crowdsourced annotation tasks
and showed how it leads to 24%-41% error reduc-
tions on answer scoring and multi-aspect sentiment
analysis problems. We saw no significant improve-
ments using features contributed by experts, kernel
representations or learned deep representations.
1479
References
Jordan Boyd-Graber, Brianna Satinoff, He He, and Hal
Daume. 2012. Besting the quiz master: Crowdsourc-
ing incremental classification games. In NAACL.
Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu,
Martin Chodorow, Lisa Braden-Harder, and Mary Dee
Harris. 1998. Automated scoring using a hybrid fea-
ture identification technique. In ACL.
Minmin Chen, Zhixiang Xu, Kilian Weinberger, and Fei
Sha. 2012. Marginalized denoising autoencoders for
domain adaptation. In ICML.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
English. In Workshop on Innovative Use of NLP for
Building Educational Applications, NAACL.
Pedro Domingos. 2012. A few useful things to know
about machine learning. In CACM.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in Twitter data with crowd-
sourcing. In NAACL Workshop on Creating Speech
and Language Data with Amazon?s Mechanical Turk.
Julian McAuley, Jure Leskovec, and Dan Jurafsky. 2012.
Learning attitudes and attributes from multi-aspect re-
views. In ICDM.
Claudiu-Christian Musat, Alireza Ghasemi, and Boi Falt-
ings. 2012. Sentiment analysis using a novel human
computation game. In Workshop on the People?s Web
Meets NLP, ACL.
Vincent Pascal, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and com-
posing robust features with denoising autoencoders. In
ICML.
Rajesh Ranganath, Dan Jurafsky, and Dan McFarland.
2009. It?s not you, it?s me: detecting flirting and its
misperception in speed-dates. In NAACL.
Richard Socher, Eric Huan, Jeffrey Pennington, Andrew
Ng, and Christopher Manning. 2011. Dynamic pool-
ing and unfolding recursive autoencoders for para-
phrase detection. In NIPS.
Omer Tamuz, Ce Liu, Serge Belongie, Ohad Shamir, and
Adam Tauman Kalai. 2011. Adaptively learning the
crowd kernel. In ICML.
Andrea Vedaldi and Andrew Zisserman. 2011. Efficient
additive kernels via explicit feature maps. In CVPR.
Kiri Wagstaff. 2012. Machine learning that matters. In
ICML.
1480
Proceedings of NAACL-HLT 2013, pages 607?611,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Estimating effect size across datasets
Anders S?gaard
Center for Language Technology
University of Copenhagen
soegaard@hum.ku.dk
Abstract
Most NLP tools are applied to text that is dif-
ferent from the kind of text they were eval-
uated on. Common evaluation practice pre-
scribes significance testing across data points
in available test data, but typically we only
have a single test sample. This short paper
argues that in order to assess the robustness
of NLP tools we need to evaluate them on
diverse samples, and we consider the prob-
lem of finding the most appropriate way to es-
timate the true effect size across datasets of
our systems over their baselines. We apply
meta-analysis and show experimentally ? by
comparing estimated error reduction over ob-
served error reduction on held-out datasets ?
that this method is significantly more predic-
tive of success than the usual practice of using
macro- or micro-averages. Finally, we present
a new parametric meta-analysis based on non-
standard assumptions that seems superior to
standard parametric meta-analysis.
1 Introduction
NLP tools and online services such as the Stanford
Parser or Google Translate are used for a wide va-
riety of purposes and therefore also on very differ-
ent kinds of data. Some use the Stanford Parser
to parse literature (van Cranenburgh, 2012), while
others use it for processing social media content
(Brown, 2011). The parser, however, was not neces-
sarily evaluated on literature or social media content
during development. Still, users typically expect
reasonable performance on any natural language in-
put. This paper asks what we as developers can do
to estimate the effect of a change to our system ? not
on the labeled test data that happens to be available
to us, but on future, still unseen datasets provided by
our end users.
The usual practice in NLP is to evaluate a sys-
tem on a small sample of held-out labeled data.
The observed effect size on this sample is then val-
idated by significance testing across data points,
testing whether the observed difference in perfor-
mance means is likely to be due to mere chance.
The preferred significance test is probably the non-
parametric paired bootstrap (Efron and Tibshirani,
1993; Berg-Kirkpatrick et al, 2012), but many re-
searchers also resort to Student?s t-test for depen-
dent means relying on the assumption that their met-
ric scores are normally distributed.
Such significance tests tell us nothing about how
likely our change to our system is to lead to improve-
ments on new datasets. The significance tests all rely
on the assumption that our datapoints are sampled
i.i.d. at random. The significance tests only tell us
how likely it is that the observed difference in per-
formance means would change if we sampled a big-
ger test sample the same way we sampled the one
we have available to us right now.
In standard machine learning papers a similar sit-
uation arises. If we are developing a new percep-
tron learning algorithm, for example, we are inter-
ested in how likely the new learning algorithm is to
perform better than other perceptron learning algo-
rithms across datasets, and we may for that reason
evaluate it on a large set of repository datasets.
Demsar (2006) presents motivation for using non-
parametric methods such as the Wilcoxon signed
607
rank test to estimate significance across datasets.
The t-test is based on means, and typically results
across datasets are not commensurable. The t-
test is also extremely sentitive to outliers. Notice
also that typically we do not have enough datasets
to do paired bootstrapping (van den Noortgate and
Onghena, 2005).
In this paper we will assume that the Wilcoxon
signed rank test provides a reasonable estimate of
the significance of an observed difference in perfor-
mance means across datasets, or of the significance
of observed error reductions, but note that this still
depends on the assumption that datasets are sampled
i.i.d. at random. More importantly, a non-parametric
test across data sets does not provide an actual esti-
mate of the effect size. Estimating effect size is im-
portant, e.g. when there is a trade-off between per-
formance gains and computational efficiency.
In evaluations across datasets in NLP we typically
use the macro-average as an estimate of effect size,
but in other fields such as psychology or medicine it
is more common to use a weighted mean obtained
using what is known as the fixed effects model or
the random effects model for meta-analysis.
The experiments reported on in this paper fo-
cus on estimating error reduction and show that
meta-analysis is generally superior to macro- and
micro-average in terms of predicting future error re-
ductions. Parametric meta-analysis, however, over-
parameterizes the distribution of error reductions,
leading to some instability. While meta-analysis is
generally superior to macro-average, it is sometimes
off by a large margin. We therefore introduce a new
parametric meta-analysis that seems better suited to
predicting error reductions. In our experiments test
set sizes are balanced, so micro-averages will be
near-identical to macro-averages.
2 Meta-analysis
Meta-analysis is the statistical analysis of the ef-
fect sizes of several studies and is very popular
in fields such as psychology or medicine. Meta-
analysis has not been applied very often to NLP.
In NLP most people work on applying new meth-
ods to old datasets, and meta-analysis is designed
to analyze series of studies applying old methods to
new datasets, e.g. running the same experiments on
new subjects. However, meta-analysis is applicable
to experiments with multiple datasets.
In psychology or medicine you often see stud-
ies running similar experiments on different sam-
ples with very different results. Meta-analysis stems
from the observation that if we want to estimate an
effect from a large set of studies, the average ef-
fect across all the studies will put too much weight
on results obtained on small datasets in which you
typically see more variance. The most popular ap-
proaches to meta-analysis are the fixed effects and
the random effects model. The fixed effects model is
applicable when you assume a true effect size (esti-
mated by the individual studies). If you cannot make
that assumption because the studies may differ in
various aspects, leading the within-study estimates
to be estimates of slightly different effect sizes, you
need to use the random effects model. Both ap-
proaches to meta-analysis are parametric and rely on
the effect sizes to be normally distributed.
2.1 Fixed effects model
In the fixed effects model we weight the effect sizes
T1, . . . , TM ? or error reductions, in our case ? by
the inverse of the variance vi in the study, i.e. wi =
1
vi . The combined effect size T is then:
T? =
?Mi?1wiTi
?M
i?1 wi
The variance of the combined effect is now:
v = 1
?M
i?1 wi
and the 95% confidence interval is then T? ?
1.96?v.
2.2 Random effects model
In the random effects model we replace the variance
vi with the variance plus between-studies variance
?2:
?2 =
?k
i?1 wiT 2i ?
(
?k
i?1 wiTi)2
?k
i?1 wi
? df
?k
i?1 wi ?
?k
i?1 w2i
?k
i?1 wi
(1)
with df = N ? 1, except all negative values are
replaced by 0.
608
?comp rec sci talk other
sys others sport vehicles politics religion
(a) (b) (c) (d) (e) (f) (g) (h)
Figure 1: Hierarchical structure of 20 Newsgroups. (a) IBM, MAC, (b) GRAPHICS, MS-WINDOWS, X-WINDOWS,
(c) BASEBALL, HOCKEY, (d) AUTOS, MOTORCYCLES, (e) CRYPTOGRAPHY, ELECTRONICS, MEDICINE, SPACE,
(f) GUNS, MIDEAST, MISCELLANEOUS, (g) ATHEISM, CHRISTIANITY, MISCELLANEOUS, (h) FORSALE
macro-av fixed random gumbel
k = 5
err. -0.1656 -0.0350 -0.0428 -0.0400
p-value - < 0.001 < 0.001 < 0.001
k = 10
err. -0.1402 -0.0329 -0.0413 -0.0359
p-value - < 0.001 < 0.001 < 0.001
k = 15
err. -0.0809 -0.0799 -0.0804 -0.0704
p-value - < 0.001 < 0.001 < 0.001
Figure 2: Using macro-average and meta-analysis to pre-
dict error reductions on document classification datasets
based on k observations. The scores are averages across
20 experiments. The p-values were computed using
Wilcoxon signed rank tests.
The random effects model is obviously more con-
servative in its confidence intervals, and often we
will not be able to obtain significance across datasets
using a random effects model. If, for example, we
apply a fixed effects model to test whether Bernoulli
naive Bayes (NB) fairs better than a perceptron (P)
model on 25 randomly extracted cross-domain doc-
ument classification problem instances from the 20
Newsgroups dataset (see Sect. 4), the 95% confi-
dence interval is [3.9%, 5.2%]. The weighted mean
is 4.6% (macro-average 3.9%). Using a random
effects model on the same 25 datasets, the 95%
confidence interval becomes [?6.5%, 6.6%]. The
weighted mean estimate is also slighly different
from that of a fixed effects model. The first question
we ask is which of these models provides the best es-
timate of effect size as observed on future datasets?
2.3 The error reductions distribution
Both the fixed effects and the random effects model
assume that effect sizes are normally distributed. We
can apply Darling-Anderson tests to test whether er-
ror reductions in 20 Newsgroups are in fact normally
distributed. Even a small sample of ten 20 News-
groups datasets provides enough evidence to reject
the hypothesis that error reductions (of NB over
P) are normally distributed. The Darling-Anderson
tests consistently tell us that the chance that our sam-
ple distribtutions of error reductions are normally
distributed is below 1%. The over-paramaterization
means that the estimates we get are unstable. While
both models are superior to macro-average esti-
mates, they may provide ?far-off? estimates.
Using Darling-Anderson tests we could also re-
ject the hypothesis that error reductions were lo-
gistically distributed, but we did not find evidence
for rejecting the hypothesis that error reductions are
Gumbel-distributed.1 Gumbel distributions are used
to model error distributions in the latent variable for-
mulation of multinomial logit regression. A para-
metric meta-analysis model based on the assumption
that error reductions are Gumbel distributed is an in-
teresting alternative to non-parametric meta-analysis
(Hedges and Olkin, 1984; van den Noortgate and
Onghena, 2005), since there seems to be little con-
sensus in the literature about the best way to ap-
proach non-parametric meta-analysis.
Gumbel distributions take the following form:
1
? e
z?e?z
where z = x??? with ? the location, and ? the
scale. We fit a Gumbel distribution to our weighted
error reductions (wiTi) and compute the combined
1Abidin et al (2012) has shown that Darling-Anderson is
superior to other goodness-of-fit tests for Gumbel distributions.
609
macro-av fixed random gumbel
k = 5
err. 0.0531 0.0525 0.0526 0.0489
p-value - ? 0.98 ? 0.98 ? 0.79
k = 7
err. 0.0928 0.0852 0.0852 0.0858
p-value - < 0.001 < 0.001 < 0.001
k = 9
err. 0.0587 0.05743 0.05743 0.0532
p-value - ? 0.68 ? 0.68 ? 0.13
Figure 3: Using macro-average and meta-analysis to pre-
dict error reductions in cross-lingual dependency parsing.
See text for details.
effect
T? = ? + 0.57721?1
M
?M
i?1 wi
where 0.57721 is the Euler-Mascheroni constant,
and the variance of the combined effect v = pi26 ?2.
3 Experiments in document classification
and dependency parsing
Our first experiment makes use of the 20 News-
groups document classification dataset.2 The top-
ics in 20 Newsgroups are hierarchically structured,
which enables us to extract a large set of binary
classification problems with considerable bias be-
tween source and target data (Chen et al, 2009;
Sun et al, 2011). See the hierarchy in Figure 1.
We extract 20 high-level binary classification prob-
lems by considering all pairs of top-level cate-
gories, e.g. COMPUTERS-RECREATIVE (comp-rec).
For each of these 20 problems, we have differ-
ent possible datasets, e.g. IBM-BASEBALL, MAC-
MOTORCYCLES, etc. A problem instance takes
training and test data from two different datasets be-
long to the same high-level problem. For exam-
ple a problem instance could be learning to dis-
tinguish articles about Macintosh and motorcycles
MAC-MOTORCYCLES (evaluated on the 20 News-
groups test section) using labeled data from IBM-
BASEBALL (the training section). In total we have
288 available problem instances in the 20 News-
groups dataset.
In our first experiment we are interested in pre-
dicting the error reductions of a naive Bayes learner
2http://people.csail.mit.edu/jrennie/20Newsgroups/
over a perceptron model. We use publicly available
implementations with default parameters.3 In each
experiment we randomly select k datasets and es-
timate the true effect size using macro-average, a
fixed effects model, a random effects model, and
a corrected random effects model. In order to es-
timate the within-study variance we take 50 paired
bootstrap samples of the system outputs. We evalu-
ate our estimates against the observed average effect
across 5 new randomly extracted datasets. For each
k we repeat the experiment 20 times and report aver-
age error. We vary k to see how many observations
are needed for our estimates to be reliable.
The results are presented in Figure 2. We note that
meta-analysis provides much better estimates than
macro-averages across the board. Our parametric
meta-analysis based on the assumption that error re-
ductions are Gumbel distributed performs best with
more observations.
Our second experiment repeats the same proce-
dure using available data from cross-lingual depen-
dency parsing. We use the submitted results by par-
ticipants in the CoNLL-X shared task (Buchholz and
Marsi, 2006) and try to predict the error reduction of
one system over another given k many observations.
Given that we only have 12 submissions per system
we use k ? {5, 7, 9} randomly extracted datasets
for observations and test on another five randomly
extracted datasets. While results (Figure 3) are only
statistically significant with k = 7, we see that meta-
analysis estimates effect size across data sets better
than macro-average in all cases.
4 Conclusions
We have argued that evaluation across datasets is
important for developing robust NLP tools, and
that meta-analysis can provide better estimates
of effect size across datasets than macro-average.
We also noted that parametric meta-analysis over-
parameterizes error reduction distributions and sug-
gested a new parametric method for estimating ef-
fect size across datasets.
Acknowledgements
Anders S?gaard is funded by the ERC Starting Grant
LOWLANDS No. 313695.
3http://scikit-learn.org/stable/
610
References
Nahdiya Abidin, Mohd Adam, and Habshah Midi. 2012.
The goodness-of-fit test for Gumbel distribution: a
comparative study. MATEMATIKA, 28(1):35?48.
Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein.
2012. An empirical investigation of statistical signifi-
cance in nlp. In EMNLP.
Gregory Brown. 2011. An error analysis of relation ex-
traction in social media documents. In ACL.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing. In
CoNLL.
Bo Chen, Wai Lam, Ivor Tsang, and Tak-Lam Wong.
2009. Extracting discriminative concepts for domain
adaptation in text mining. In KDD.
Janez Demsar. 2006. Statistical comparisons of clas-
sifiers over multiple data sets. Journal of Machine
Learning Research, 7:1?30.
Bradley Efron and Robert Tibshirani. 1993. An introduc-
tion to the bootstrap. Chapman & Hall, Boca Raton,
FL.
Larry Hedges and Ingram Olkin. 1984. Nonparametric
estimators of effect size in meta-analysis. Psychologi-
cal Bulletin, 96:573?580.
Qian Sun, Rita Chattopadhyay, Sethuraman Pan-
chanathan, and Jieping Ye. 2011. Two-stage weight-
ing framework for multi-source domain adaptation. In
NIPS.
Andreas van Cranenburgh. 2012. Literary author-
ship attribution with phrase-structure fragments. In
Workshop on Computational Linguistics for Litera-
ture, NAACL.
Wim van den Noortgate and Patrick Onghena. 2005.
Parametric and nonparametric bootstrap methods for
meta-analysis. Behavior Research Methods, 37:11?
22.
611
Proceedings of NAACL-HLT 2013, pages 617?626,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Down-stream effects of tree-to-dependency conversions
Jakob Elming, Anders Johannsen, Sigrid Klerke, Emanuele Lapponi?,
Hector Martinez, Anders S?gaard
Center for Language Technology, University of Copenhagen
?Institute for Informatics, University of Oslo
Abstract
Dependency analysis relies on morphosyntac-
tic evidence, as well as semantic evidence.
In some cases, however, morphosyntactic ev-
idence seems to be in conflict with seman-
tic evidence. For this reason dependency
grammar theories, annotation guidelines and
tree-to-dependency conversion schemes often
differ in how they analyze various syntactic
constructions. Most experiments for which
constituent-based treebanks such as the Penn
Treebank are converted into dependency tree-
banks rely blindly on one of four-five widely
used tree-to-dependency conversion schemes.
This paper evaluates the down-stream effect of
choice of conversion scheme, showing that it
has dramatic impact on end results.
1 Introduction
Annotation guidelines used in modern depen-
dency treebanks and tree-to-dependency conversion
schemes for converting constituent-based treebanks
into dependency treebanks are typically based on
a specific dependency grammar theory, such as the
Prague School?s Functional Generative Description,
Meaning-Text Theory, or Hudson?s Word Grammar.
In practice most parsers constrain dependency struc-
tures to be tree-like structures such that each word
has a single syntactic head, limiting diversity be-
tween annotation a bit; but while many dependency
treebanks taking this format agree on how to an-
alyze many syntactic constructions, there are still
many constructions these treebanks analyze differ-
ently. See Figure 1 for a standard overview of clear
and more difficult cases.
The difficult cases in Figure 1 are difficult for
the following reason. In the easy cases morphosyn-
tactic and semantic evidence cohere. Verbs gov-
ern subjects morpho-syntactically and seem seman-
tically more important. In the difficult cases, how-
ever, morpho-syntactic evidence is in conflict with
the semantic evidence. While auxiliary verbs have
the same distribution as finite verbs in head position
and share morpho-syntactic properties with them,
and govern the infinite main verbs, main verbs seem
semantically superior, expressing the main predi-
cate. There may be distributional evidence that com-
plementizers head verbs syntactically, but the verbs
seem more important from a semantic point of view.
Tree-to-dependency conversion schemes used
to convert constituent-based treebanks into
dependency-based ones also take different stands on
the difficult cases. In this paper we consider four dif-
ferent conversion schemes: the Yamada-Matsumoto
conversion scheme yamada,1 the CoNLL 2007
format conll07,2 the conversion scheme ewt used in
the English Web Treebank (Petrov and McDonald,
2012),3 and the lth conversion scheme (Johansson
1The Yamada-Matsumoto scheme can be
replicated by running penn2malt.jar available at
http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html. We
used Malt dependency labels (see website). The Yamada-
Matsumoto scheme is an elaboration of the Collins scheme
(Collins, 1999), which is not included in our experiments.
2The CoNLL 2007 conversion scheme can be
obtained by running pennconverter.jar available at
http://nlp.cs.lth.se/software/treebank converter/with the
?conll07? flag set.
3The EWT conversion scheme can be repli-
cated using the Stanford converter available at
http://nlp.stanford.edu/software/stanford-dependencies.shtml
617
Clear cases Difficult cases
Head Dependent ? ?
Verb Subject Auxiliary Main verb
Verb Object Complementizer Verb
Noun Attribute Coordinator Conjuncts
Verb Adverbial Preposition Nominal
Punctuation
Figure 1: Clear and difficult cases in dependency annotation.
and Nugues, 2007).4 We list the differences in
Figure 2. An example of differences in analysis is
presented in Figure 3.
In order to access the impact of these conversion
schemes on down-stream performance, we need ex-
trinsic rather than intrinsic evaluation. In general
it is important to remember that while researchers
developing learning algorithms for part-of-speech
(POS) tagging and dependency parsing seem ob-
sessed with accuracies, POS sequences or depen-
dency structures have no interest on their own. The
accuracies reported in the literature are only inter-
esting insofar they correlate with the usefulness of
the structures predicted by our systems. Fortunately,
POS sequences and dependency structures are use-
ful in many applications. When we consider tree-to-
dependency conversion schemes, down-stream eval-
uation becomes particularly important since some
schemes are more fine-grained than others, leading
to lower performance as measured by intrinsic eval-
uation metrics.
Approach in this work
In our experiments below we apply a state-of-the-art
parser to five different natural language processing
(NLP) tasks where syntactic features are known to
be effective: negation resolution, semantic role la-
beling (SRL), statistical machine translation (SMT),
sentence compression and perspective classification.
In all five tasks we use the four tree-to-dependency
conversion schemes mentioned above and evaluate
them in terms of down-stream performance. We also
compare our systems to baseline systems not rely-
4The LTH conversion scheme can be ob-
tained by running pennconverter.jar available at
http://nlp.cs.lth.se/software/treebank converter/ with the
?oldLTH? flag set.
ing on syntactic features, when possible, and to re-
sults in the literature, when comparable results exist.
Note that negation resolution and SRL are not end
applications. It is not easy to generalize across five
very different tasks, but the tasks will serve to show
that the choice of conversion scheme has significant
impact on down-stream performance.
We used the most recent release of the Mate parser
first described in Bohnet (2010),5 trained on Sec-
tions 2?21 of the Wall Street Journal section of the
English Treebank (Marcus et al, 1993). The graph-
based parser is similar to, except much faster, and
performs slightly better than the MSTParser (Mc-
Donald et al, 2005), which is known to perform
well on long-distance dependencies often important
for down-stream applications (McDonald and Nivre,
2007; Galley and Manning, 2009; Bender et al,
2011). This choice may of course have an effect on
what conversion schemes seem superior (Johansson
and Nugues, 2007). Sentence splitting was done us-
ing splitta,6, and the sentences were then tokenized
using PTB-style tokenization7 and tagged using the
in-built Mate POS tagger.
Previous work
There has been considerable work on down-stream
evaluation of syntactic parsers in the literature, but
most previous work has focused on evaluating pars-
ing models rather than linguistic theories. No one
has, to the best of our knowledge, compared the
impact of choice of tree-to-dependency conversion
scheme across several NLP tasks.
Johansson and Nugues (2007) compare the im-
pact of yamada and lth on semantic role labeling
5http://code.google.com/p/mate-tools/
6http://code.google.com/p/splitta/
7http://www.cis.upenn.edu/?treebank/tokenizer.sed
618
FORM1 FORM2 yamada conll07 ewt lth
Auxiliary Main verb 1 1 2 2
Complementizer Verb 1 2 2 2
Coordinator Conjuncts 2 1 2 2
Preposition Nominal 1 1 1 2
Figure 2: Head decisions in conversions. Note: yamada also differ from CoNLL 2007 in proper names.
Figure 3: CoNLL 2007 (blue) and LTH (red) dependency conversions.
performance, showing that lth leads to superior per-
formance.
Miyao et al (2008) measure the impact of syntac-
tic parsers in an information extraction system iden-
tifying protein-protein interactions in biomedical re-
search articles. They evaluate dependency parsers,
constituent-based parsers and deep parsers.
Miwa et al (2010) evaluate down-stream per-
formance of linguistic representations and parsing
models in biomedical event extraction, but do not
evaluate linguistic representations directly, evaluat-
ing representations and models jointly.
Bender et al (2011) compare several parsers
across linguistic representations on a carefully de-
signed evaluation set of hard, but relatively frequent
syntactic constructions. They compare dependency
parsers, constituent-based parsers and deep parsers.
The authors argue in favor of evaluating parsers on
diverse and richly annotated data. Others have dis-
cussed various ways of evaluating across annotation
guidelines or translating structures to a common for-
mat (Schwartz et al, 2011; Tsarfaty et al, 2012).
Hall et al (2011) discuss optimizing parsers for
specific down-stream applications, but consider only
a single annotation scheme.
Yuret et al (2012) present an overview of the
SemEval-2010 Evaluation Exercises on Semantic
Evaluation track on recognition textual entailment
using dependency parsing. They also compare sev-
eral parsers using the heuristics of the winning sys-
tem for inference. While the shared task is an
example of down-stream evaluation of dependency
parsers, the evaluation examples only cover a subset
of the textual entailments relevant for practical ap-
plications, and the heuristics used in the experiments
assume a fixed set of dependency labels (ewt labels).
Finally, Schwartz et al (2012) compare the
above conversion schemes and several combinations
thereof in terms of learnability. This is very different
from what is done here. While learnability may be
a theoretically motivated parameter, our results indi-
cate that learnability and downstream performance
do not correlate well.
2 Applications
Dependency parsing has proven useful for a wide
range of NLP applications, including statistical ma-
chine translation (Galley and Manning, 2009; Xu et
al., 2009; Elming and Haulrich, 2011) and sentiment
analysis (Joshi and Penstein-Rose, 2009; Johansson
and Moschitti, 2010). This section describes the ap-
plications and experimental set-ups included in this
study.
In the five applications considered below we
619
use syntactic features in slightly different ways.
While our statistical machine translation and sen-
tence compression systems use dependency rela-
tions as additional information about words and on
a par with POS, our negation resolution system uses
dependency paths, conditioning decisions on both
dependency arcs and labels. In perspective classifi-
cation, we use dependency triples (e.g. SUBJ(John,
snore)) as features, while the semantic role labeling
system conditions on a lot of information, including
the word form of the head, the dependent and the ar-
gument candidates, the concatenation of the depen-
dency labels of the predicate, and the labeled depen-
dency relations between predicate and its head, its
arguments, dependents or siblings.
2.1 Negation resolution
Negation resolution (NR) is the task of finding nega-
tion cues, e.g. the word not, and determining their
scope, i.e. the tokens they affect. NR has recently
seen considerable interest in the NLP community
(Morante and Sporleder, 2012; Velldal et al, 2012)
and was the topic of the 2012 *SEM shared task
(Morante and Blanco, 2012).
The data set used in this work, the Conan Doyle
corpus (CD),8 was released in conjunction with the
*SEM shared task. The annotations in CD extend
on cues and scopes by introducing annotations for
in-scope events that are negated in factual contexts.
The following is an example from the corpus show-
ing the annotations for cues (bold), scopes (under-
lined) and negated events (italicized):
(1) Since we have been so
unfortunate as to miss him [. . . ]
CD-style scopes can be discontinuous and overlap-
ping. Events are a portion of the scope that is se-
mantically negated, with its truth value reversed by
the negation cue.
The NR system used in this work (Lapponi et al,
2012), one of the best performing systems in the
*SEM shared task, is a CRF model for scope resolu-
tion that relies heavily on features extracted from de-
pendency graphs. The feature model contains token
distance, direction, n-grams of word forms, lemmas,
POS and combinations thereof, as well as the syntac-
tic features presented in Figure 4. The results in our
8http://www.clips.ua.ac.be/sem2012-st-neg/data.html
Syntactic
constituent
dependency relation
parent head POS
grand parent head POS
word form+dependency relation
POS+dependency relation
Cue-dependent
directed dependency distance
bidirectional dependency distance
dependency path
lexicalized dependency path
Figure 4: Features used to train the conditional random
field models
experiments are obtained from configurations that
differ only in terms of tree-to-dependency conver-
sions, and are trained on the training set and tested
on the development set of CD. Since the negation
cue classification component of the system does not
rely on dependency features at all, the models are
tested using gold cues.
Table 1 shows F1 scores for scopes, events and
full negations, where a true positive correctly as-
signs both scope tokens and events to the rightful
cue. The scores are produced using the evaluation
script provided by the *SEM organizers.
2.2 Semantic role labeling
Semantic role labeling (SRL) is the attempt to de-
termine semantic predicates in running text and la-
bel their arguments with semantic roles. In our
experiments we have reproduced the second best-
performing system in the CoNLL 2008 shared task
in syntactic and semantic parsing (Johansson and
Nugues, 2008).9
The English training data for the CoNLL 2008
shared task were obtained from PropBank and
NomBank. For licensing reasons, we used
OntoNotes 4.0, which includes PropBank, but not
NomBank. This means that our system is only
trained to classify verbal predicates. We used
the Clearparser conversion tool10 to convert the
OntoNotes 4.0 and subsequently supplied syntac-
tic dependency trees using our different conversion
schemes. We rely on gold standard argument identi-
fication and focus solely on the performance metric
semantic labeled F1.
9http://nlp.cs.lth.se/software/semantic parsing: propbank
nombank frames
10http://code.google.com/p/clearparser/
620
2.3 Statistical machine translation
The effect of the different conversion schemes was
also evaluated on SMT. We used the reordering
by parsing framework described by Elming and
Haulrich (2011). This approach integrates a syn-
tactically informed reordering model into a phrase-
based SMT system. The model learns to predict the
word order of the translation based on source sen-
tence information such as syntactic dependency re-
lations. Syntax-informed SMT is known to be use-
ful for translating between languages with different
word orders (Galley and Manning, 2009; Xu et al,
2009), e.g. English and German.
The baseline SMT system is created as described
in the guidelines from the original shared task.11
Only modifications are that we use truecasing in-
stead of lowercasing and recasing, and allow train-
ing sentences of up to 80 words. We used data
from the English-German restricted task: ?3M par-
allel words of news, ?46M parallel words of Eu-
roparl, and ?309M words of monolingual Europarl
and news. We use newstest2008 for tuning, new-
stest2009 for development, and newstest2010 for
testing. Distortion limit was set to 10, which is
also where the baseline system performed best. The
phrase table and the lexical reordering model is
trained on the union of all parallel data with a max
phrase length of 7, and the 5-gram language model
is trained on the entire monolingual data set.
We test four different experimental systems that
only differ with the baseline in the addition of a syn-
tactically informed reordering model. The baseline
system was one of the tied best performing system
in the WMT 2011 shared task on this dataset. The
four experimental systems have reordering models
that are trained on the first 25,000 sentences of the
parallel news data that have been parsed with each
of the tree-to-dependency conversion schemes. The
reordering models condition reordering on the word
forms, POS, and syntactic dependency relations of
the words to be reordered, as described in Elming
and Haulrich (2011). The paper shows that while
reordering by parsing leads to significant improve-
ments in standard metrics such as BLEU (Papineni
et al, 2002) and METEOR (Lavie and Agarwal,
2007), improvements are more spelled out with hu-
11 http://www.statmt.org/wmt11/translation-task.html
man judgements. All SMT results reported below
are averages based on 5 MERT runs following Clark
et al (2011).
2.4 Sentence compression
Sentence compression is a restricted form of sen-
tence simplification with numerous usages, includ-
ing text simplification, summarization and recogniz-
ing textual entailment. The most commonly used
dataset in the literature is the Ziff-Davis corpus.12 A
widely used baseline for sentence compression ex-
periments is Knight and Marcu (2002), who intro-
duce two models: the noisy-channel model and a de-
cision tree-based model. Both are tree-based meth-
ods that find the most likely compressed syntactic
tree and outputs the yield of this tree. McDonald et
al. (2006) instead use syntactic features to directly
find the most likely compressed sentence.
Here we learn a discriminative HMM model
(Collins, 2002) of sentence compression using
MIRA (Crammer and Singer, 2003), comparable to
previously explored models of noun phrase chunk-
ing. Our model is thus neither tree-based nor
sentence-based. Instead we think of sentence com-
pression as a sequence labeling problem. We com-
pare a model informed by word forms and predicted
POS with models also informed by predicted depen-
dency labels. The baseline feature model conditions
emission probabilities on word forms and POS us-
ing a ?2 window and combinations thereoff. The
augmented syntactic feature model simply adds de-
pendency labels within the same window.
2.5 Perspective classification
Finally, we include a document classification dataset
from Lin and Hauptmann (2006).13 The dataset con-
sists of blog posts posted at bitterlemons.org by Is-
raelis and Palestinians. The bitterlemons.org web-
site is set up to ?contribute to mutual understanding
through the open exchange of ideas.? In the dataset,
each blog post is labeled as either Israeli or Pales-
tinian. Our baseline model is just a standard bag-
of-words model, and the system adds dependency
triplets to the bag-of-words model in a way similar
to Joshi and Penstein-Rose (2009). We do not re-
move stop words, since perspective classification is
12LDC Catalog No.: LDC93T3A.
13https://sites.google.com/site/weihaolinatcmu/data
621
bl yamada conll07 ewt lth
DEPRELS - 12 21 47 41
PTB-23 (LAS) - 88.99 88.52 81.36? 87.52
PTB-23 (UAS) - 90.21 90.12 84.22? 90.29
Neg: scope F1 - 81.27 80.43 78.70 79.57
Neg: event F1 - 76.19 72.90 73.15 76.24
Neg: full negation F1 - 67.94 63.24 61.60 64.31
SentComp F1 68.47 72.07 64.29 71.56 71.56
SMT-dev-Meteor 35.80 36.06 36.06 36.16 36.08
SMT-test-Meteor 37.25 37.48 37.50 37.58 37.51
SMT-dev-BLEU 13.66 14.14 14.09 14.04 14.06
SMT-test-BLEU 14.67 15.04 15.04 14.96 15.11
SRL-22-gold - 81.35 83.22 84.72 84.01
SRL-23-gold - 79.09 80.85 80.39 82.01
SRL-22-pred - 74.41 76.22 78.29 66.32
SRL-23-pred - 73.42 74.34 75.80 64.06
bitterlemons.org 96.08 97.06 95.58 96.08 96.57
Table 1: Results. ?: Low parsing results on PTB-23 using ewt are explained by changes between the PTB-III and the
Ontonotes 4.0 release of the English Treebank.
similar to authorship attribution, where stop words
are known to be informative. We evaluate perfor-
mance doing cross-validation over the official train-
ing data, setting the parameters of our learning algo-
rithm for each fold doing cross-validation over the
actual training data. We used soft-margin support
vector machine learning (Cortes and Vapnik, 1995),
tuning the kernel (linear or polynomial with degree
3) and C = {0.1, 1, 5, 10}.
3 Results and discussion
Our results are presented in Table 1. The parsing
results are obtained relying on predicted POS rather
than, as often done in the dependency parsing liter-
ature, relying on gold-standard POS. Note that they
comply with the result in Schwartz et al (2012) that
Yamada-Matsumoto-style annotation is more easily
learnable.
The negation resolution results are significantly
better using syntactic features in yamada annota-
tion. It is not surprising that a syntactically ori-
ented conversion scheme performs well in this task.
Since Lapponi et al (2012) used Maltparser (Nivre
et al, 2007) with the freely available pre-trained
parsing model for English,14 we decided to also
run that parser with the gold-standard cues, in ad-
14http://www.maltparser.org/mco/english parser/engmalt.html
dition to Mate. The pre-trained model was trained
on Sections 2?21 of the Wall Street Journal sec-
tion of the English Treebank (Marcus et al, 1993),
augmented with 4000 sentences from the Question-
Bank,15 which was converted using the Stanford
converter and thus similar to the ewt annotations
used here. The results were better than using ewt
with Mate trained on Sections 2?21 alone, but worse
than the results obtained here with yamada conver-
sion scheme. F1 score on full negation was 66.92%.
The case-sensitive BLEU evaluation of the
SMT systems indicates that choice of conversion
scheme has no significant impact on overall perfor-
mance. The difference to the baseline system is
significant (p < 0.01), showing that the reorder-
ing model leads to improvement using any of the
schemes. However, the conversion schemes lead to
very different translations. This can be seen, for
example, by the fact that the relative tree edit dis-
tance between translations of different syntactically
informed SMT systems is 12% higher than within
each system (across different MERT optimizations).
The reordering approach puts a lot of weight on
the syntactic dependency relations. As a conse-
quence, the number of relation types used in the
conversion schemes proves important. Consider the
15http://www.computing.dcu.ie/?jjudge/qtreebank/
622
REFERENCE: Zum Glu?ck kam ich beim Strassenbahnfahren an die richtige Stelle .
SOURCE: Luckily , on the way to the tram , I found the right place .
yamada: Glu?cklicherweise hat auf dem Weg zur S-Bahn , stellte ich fest , dass der richtige Ort .
conll07: Glu?cklicherweise hat auf dem Weg zur S-Bahn , stellte ich fest , dass der richtige Ort .
ewt: Zum Glu?ck fand ich auf dem Weg zur S-Bahn , am richtigen Platz .
lth: Zum Glu?ck fand ich auf dem Weg zur S-Bahn , am richtigen Platz .
BASELINE: Zum Glu?ck hat auf dem Weg zur S-Bahn , ich fand den richtigen Platz .
Figure 5: Examples of SMT output.
ORIGINAL: * 68000 sweden ab of uppsala , sweden , introduced the teleserve , an integrated answering
machine and voice-message handler that links a macintosh to touch-tone phones .
BASELINE: 68000 sweden ab introduced the teleserve an integrated answering
machine and voice-message handler .
yamada 68000 sweden ab introduced the teleserve integrated answering
machine and voice-message handler .
conll07 68000 sweden ab sweden introduced the teleserve integrated answering
machine and voice-message handler .
ewt 68000 sweden ab introduced the teleserve integrated answering
machine and voice-message handler .
lth 68000 sweden ab introduced the teleserve an integrated answering
machine and voice-message handler .
HUMAN: 68000 sweden ab introduced the teleserve integrated answering
machine and voice-message handler .
Figure 6: Examples of sentence compression output.
example in Figure 5. German requires the verb in
second position, which is obeyed in the much bet-
ter translations produced by the ewt and lth sys-
tems. Interestingly, the four schemes produce virtu-
ally identical structures for the source sentence, but
they differ in their labeling. Where conll07 and ya-
mada use the same relation for the first two con-
stituents (ADV and vMOD, respectively), ewt and
lth distinguish between them (ADVMOD/PREP and
ADV/LOC). This distinction may be what enables
the better translation, since the model may learn to
move the verb after the sentence adverbial. In the
other schemes, sentence adverbials are not distin-
guished from locational adverbials. Generally, ewt
and lth have more than twice as many relation types
as the other schemes.
The schemes ewt and lth lead to better SRL
performance than conll07 and yamada when re-
lying on gold-standard syntactic dependency trees.
This supports the claims put forward in Johansson
and Nugues (2007). These annotations also hap-
pen to use a larger set of dependency labels, how-
ever, and syntactic structures may be harder to re-
construct, as reflected by labeled attachment scores
(LAS) in syntactic parsing. The biggest drop in
SRL performance going from gold-standard to pre-
dicted syntactic trees is clearly for the lth scheme,
at an average 17.8% absolute loss (yamada 5.8%;
conll07 6.8%; ewt 5.5%; lth 17.8%).
The ewt scheme resembles lth in most respects,
but in preposition-noun dependencies it marks the
preposition as the head rather than the noun. This
is an important difference for SRL, because seman-
tic arguments are often nouns embedded in preposi-
tional phrases, like agents in passive constructions.
It may also be that the difference in performance is
simply explained by the syntactic analysis of prepo-
sitional phrases being easier to reconstruct.
The sentence compression results are generally
much better than the models proposed in Knight and
Marcu (2002). Their noisy channel model obtains
an F1 compression score of 14.58%, whereas the
decision tree-based model obtains an F1 compres-
sion score of 31.71%. While F1 scores should be
complemented by human judgements, as there are
typically many good sentence compressions of any
source sentence, we believe that error reductions of
more than 50% indicate that the models used here
623
ADV AMOD CC COORDDEP EXP GAP IOBJ LGS NMOD OBJ P PMOD PRD PRN PRT ROOT SBJ VC VMOD
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
L
a
b
e
l
s
srl
neg
Figure 7: Distributions of dependency labels in the
Yamada-Matsumoto scheme
(though previously unexplored in the literature) are
fully competitive with state-of-the-art models.
We also see that the models using syntactic fea-
tures perform better than our baseline model, except
for the model using conll07 dependency annotation.
This may be surprising to some, since distributional
information is often considered important in sen-
tence compression (Knight and Marcu, 2002). Some
output examples are presented in Figure 6. Un-
surprisingly, it is seen that the baseline model pro-
duces grammatically incorrect output, and that most
of our syntactic models correct the error leading to
ungrammaticality. The model using ewt annotation
is an exception. We also see that conll07 introduces
another error. We believe that this is due to the way
the conll07 tree-to-dependency conversion scheme
handles coordination. While the word Sweden is not
coordinated, it occurs in a context, surrounded by
commas, that is very similar to coordinated items.
In perspective classification we see that syntactic
features based on yamada and lth annotations lead
to improvements, with yamada leading to slightly
better results than lth. The fact that a syntactically
oriented conversion scheme leads to the best results
may reflect that perspective classification, like au-
thorship attribution, is less about content than stylis-
tics.
While lth seems to lead to the overall best re-
sults, we stress the fact that the five tasks considered
here are incommensurable. What is more interest-
ing is that, task to task, results are so different. The
semantically oriented conversion schemes, ewt and
lth, lead to the best results in SRL, but with a signif-
icant drop for lth when relying on predicted parses,
while the yamada scheme is competitive in the other
four tasks. This may be because distributional infor-
mation is more important in these tasks than in SRL.
The distribution of dependency labels seems rel-
atively stable across applications, but differences in
data may of course also affect the usefulness of dif-
ferent annotations. Note that conll07 leads to very
good results for negation resolution, but bad results
for SRL. See Figure 7 for the distribution of labels
in the conll07 conversion scheme on the SRL and
negation scope resolution data. Many differences
relate to differences in sentence length. The nega-
tion resolution data is literary text with shorter sen-
tences, which therefore uses more punctuation and
has more root dependencies than newspaper articles.
On the other hand we do see very few predicate de-
pendencies in the SRL data. This may affect down-
stream results when classifying verbal predicates in
SRL. We also note that the number of dependency
labels have less impact on results in general than we
would have expected. The number of dependency
labels and the lack of support for some of them may
explain the drop with predicted syntactic parses in
our SRL results, but generally we obtain our best re-
sults with yamada and lth annotations, which have
12 and 41 dependency labels, respectively.
4 Conclusions
We evaluated four different tree-to-dependency con-
version schemes, putting more or less emphasis on
syntactic or semantic evidence, in five down-stream
applications, including SMT and negation resolu-
tion. Our results show why it is important to be
precise about exactly what tree-to-dependency con-
version scheme is used. Tools like pennconverter.jar
gives us a wide range of options when converting
constituent-based treebanks, and even small differ-
ences may have significant impact on down-stream
performance. The small differences are also impor-
tant for more linguistic comparisons that also tend to
gloss over exactly what conversion scheme is used,
e.g. Ivanova et al (2012).
Acknowledgements
Hector Martinez is funded by the ERC grant
CLARA No. 238405, and Anders S?gaard is
funded by the ERC Starting Grant LOWLANDS
No. 313695.
624
References
Emily Bender, Dan Flickinger, Stephan Oepen, and
Yi Zhang. 2011. Parser evaluation over local and non-
local dependencies in a large corpus. In EMNLP.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In COLING.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: controlling for optimizer instabil-
ity. In ACL.
Mike Collins. 1999. Head-driven statistical models for
natural language parsing. Ph.D. thesis, University of
Pennsylvania.
Michael Collins. 2002. Discriminative training methods
for Hidden Markov Models. In EMNLP.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?297.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive algorithms for multiclass problems. In JMLR.
Jakob Elming and Martin Haulrich. 2011. Reordering
by parsing. In Proceedings of International Workshop
on Using Linguistic Information for Hybrid Machine
Translation (LIHMT-2011).
Michel Galley and Christopher Manning. 2009.
Quadratic-time dependency parsing for machine trans-
lation. In ACL.
Keith Hall, Ryan McDonald, Jason Katz-Brown, and
Michael Ringgaard. 2011. Training dependency
parsers by jointly optimizing multiple objectives. In
EMNLP.
Angelina Ivanova, Stephan Oepen, Lilja ?vrelid, and
Dan Flickinger. 2012. Who did what to whom? a con-
trastive study of syntactico-semantic dependencies. In
LAW.
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion expres-
sion detection. In CoNLL.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
NODALIDA.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic-semantic analysis
with propbank and nombank. In CoNLL.
Mahesh Joshi and Carolyn Penstein-Rose. 2009. Gen-
eralizing dependency features for opinion mining. In
ACL.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91?107.
Emanuele Lapponi, Erik Velldal, Lilja ?vrelid, and
Jonathon Read. 2012. UiO2: Sequence-labeling nega-
tion using dependency features. In *SEM.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an au-
tomatic metric for mt evaluation with high levels of
correlation with human judgments. In WMT.
Wei-Hao Lin and Alexander Hauptmann. 2006. Are
these documents written from different perspectives?
In COLING-ACL.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Lin-
guistics, 19(2):313?330.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsers. In
EMNLP-CoNLL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing
2005, pages 523?530, Vancouver, British Columbia.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In EACL.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010. Evaluating dependency repre-
sentation for event extraction. In COLING.
Yusuke Miyao, Rune S? tre, Kenji Sagae, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
ACL.
Roser Morante and Eduardo Blanco. 2012. *sem 2012
shared task: Resolving the scope and focus of nega-
tion. In *SEM.
Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special issue.
Computational linguistics, 38(2):223?260.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: a language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In ACL.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 Shared Task on Parsing the Web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Roy Schwartz, and Omri Abend, Roi Reichart, and Ari
Rappoport. 2011. Neutralizing linguistically prob-
lematic annotations in unsupervised dependency pars-
ing evaluation. In ACL.
Roy Schwartz, Omri Abend, and Ari Rappoport. 2012.
Learnability-based syntactic annotation design. In
COLING.
625
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Cross-framework evaluation for statistical pars-
ing. In EACL.
Erik Velldal, Lilja ?vrelid, Jonathon Read, and Stephan
Oepen. 2012. Speculation and negation: Rules,
rankers, and the role of synta. Computational linguis-
tics, 38(2):369?410.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In NAACL-
HLT, Boulder, Colorado.
Deniz Yuret, Laura Rimell, and Aydin Han. 2012. Parser
evaluation using textual entailments. Language Re-
sources and Evaluation, Published online 31 October
2012.
626
Proceedings of NAACL-HLT 2013, pages 668?672,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Zipfian corruptions for robust POS tagging
Anders S?gaard
Center for Language Technology
University of Copenhagen
soegaard@hum.ku.dk
Abstract
Inspired by robust generalization and adver-
sarial learning we describe a novel approach
to learning structured perceptrons for part-of-
speech (POS) tagging that is less sensitive to
domain shifts. The objective of our method is
to minimize average loss under random distri-
bution shifts. We restrict the possible target
distributions to mixtures of the source distri-
bution and random Zipfian distributions. Our
algorithm is used for POS tagging and eval-
uated on the English Web Treebank and the
Danish Dependency Treebank with an average
4.4% error reduction in tagging accuracy.
1 Introduction
Supervised learning approaches have advanced the
state of the art on a variety of tasks in natural lan-
guage processing, often resulting in systems ap-
proaching the level of inter-annotator agreement on
in-domain data, e.g. in POS tagging, where Shen
et al (2007) report a tagging accuracy of 97.3%.
However, performance of state-of-the-art supervised
systems is known to drop considerably on out-of-
domain data. State-of-the-art POS taggers trained
on the Penn Treebank (Marcus et al, 1993) mapped
to Google?s universal tag set (Petrov et al, 2011)
achieve tagging accuracies in the range of 89?91%
on Web 2.0 data (Petrov and McDonald, 2012) .
To bridge this gap we may consider using semi-
supervised or transfer learning methods to adjust to
new target domains (Blitzer et al, 2006; Daume III,
2007), pooling unlabeled data from those domains.
However, in many applications this is not possible.
If we want to provide an online service or design a
piece of software with many potential users covering
a wide range of use cases, we do not know the target
domain in advance. This is the usual problem of ro-
bust learning, but in this paper we describe a novel
learning algorithm that goes beyond robust learning
by making various assumptions about the difference
between the source domain and the (unknown) target
domain. Under these assumptions we can minimize
average loss under (all possible or a representative
sample of) domain shifts. We evaluate our approach
on two recently introduced cross-domain POS tag-
ging datasets.
Our approach is inspired by work in robust gen-
eralization (Ben-Tal and Nemirovski, 1998; Trafalis
and Gilbert, 2007) and adversarial learning (Glober-
son and Roweis, 2006; Dekel and Shamir, 2008;
S?gaard and Johannsen, 2012). Our approach also
bears similarities to feature bagging (Sutton et al,
2006). Sutton et al (2006) noted that in learning of
linear models useful features are often swamped by
correlating, but more indicative features. If the more
indicative features are absent in the target domain
due to out-of-vocabulary (OOV) effects, we are left
with the swamped features which were not updated
properly. This is, indirectly, the problem solved in
adversarial learning with corrupted data points. Ad-
versarial learning can also be seen as a way of av-
eraging exponentially many models (Hinton et al,
2012).
Adversarial learning techniques have been devel-
oped for security-related learning tasks, e.g. where
systems need to be robust to failing sensors. We also
show how we can do better than straight-forward ap-
668
plication of adversarial learning techniques by mak-
ing a second assumption about our data, namely that
domains are mixtures of Zipfian distributions over
our features. Similar assumptions have been made
before in computational linguistics, e.g. by Goldberg
and Elhadad (2008).
2 Approach overview
In this paper we consider the structured perceptron
(Collins, 2002) ? with POS tagging as our practical
application. The structured perceptron is prone to
feature swamping (Sutton et al, 2006), and we want
to prevent that using a technique inspired by adver-
sarial learning (Globerson and Roweis, 2006; Dekel
and Shamir, 2008). The modification presented here
to the structured perceptron only affects a single line
of code in a publicly available implementation (see
below), but the consequences are significant.
Online adversarial learning (S?gaard and Jo-
hannsen, 2012), briefly, works by sampling random
corruptions of our data, or random feature deletions,
in the learning phase. A discriminative learner see-
ing corrupted data points with missing features will
not update part of the model and will thus try to
find a decision boundary classifying the training data
correctly relying on the remaining features. This de-
cision boundary may be very different from the deci-
sion boundary found otherwise by the discriminative
learner. If we sample enough corruptions, the model
learned from the corrupted data will converge on the
model minimizing average loss over all corruptions
(Dekel and Shamir, 2008).
Example Consider the plot in Figure 1. The solid
line with no stars (2d-fit) is the SVM fit in two
dimensions, while the dashed line is what that fit
amounts to if the feature x is missing in the tar-
get. The solid line with stars (1d-fit) is our fit if we
could predict the missing feature, training an SVM
only with the y feature. The 1d-fit decision bound-
ary only misclassifies a single data point compared
to the original fit which misclassifies more than 15
negatives with the x feature missing.
The plot thus shows that the best fit in m dimen-
sions is often not the best in < m dimensions. Con-
sequently, if we think there is a risk that features will
be missing in the target, finding the best fit in m di-
mensions is not necessarily the best we can do. Of
0 2 4 6 8 10 12
?14
?12
?10
?8
?6
?4
?2
0
2
4
2d-fit
2d-fit with missing feature
1d-fit
Figure 1: The best fit in m dimensions is often not the
best in < m dimensions.
course we do not know what features will be miss-
ing in advance. The intuition in adversarial learning
is that we may obtain more robust decision bound-
aries by minimizing loss over a set of possible fea-
ture deletions. We extend this idea below, modeling
not only OOV effects, but a broader class of distri-
butional shifts.
3 Structured perceptron
The structured perceptron (Collins, 2002) models
sequences as Markov chains of unobserved variables
(POS), each emitting an observed variable (a word
form). The structured perceptron is similar to the av-
eraged perceptron (Freund and Schapire, 1999), ex-
cept data points are sequences of vectors rather than
just vectors. Consequently, the structured percep-
tron does not predict a class label but a sequence of
labels (using Viterbi decoding). In learning we up-
date the features at the positions where the predicted
labels are different from the true labels. We do this
by adding weight to features present in the correct
solution and subtracting weight from features only
present in the predicted solution. The generic aver-
aged perceptron learning algorithm is presented in
Figure 2. A publicly available and easy-to-modify
Python reimplementation of the structured percep-
tron can be found in the LXMLS toolkit.1 We use
the LXMLS toolkit as our baseline with the default
feature model, but use the PTB tagset rather than the
Google tagset (Petrov et al, 2011) used by default
in the LXMLS toolkit.
1https://github.com/gracaninja/lxmls-toolkit
669
1: X = {?yi,xi?}Ni=1
2: w0 = 0,v = 0, i = 0
3: for k ? K do
4: for n ? N do
5: if sign(w ? x) 6= yn then
6: wi+1 ? update(wi)
7: i? i+ 1
8: end if
9: v? v + wi
10: end for
11: end for
12: return w = v/(N ?K)
Figure 2: Generic averaged perceptron
4 Minimizing loss under OOV effects
We will think of domain shifts as data point corrup-
tions. S?gaard and Johannsen (2012) model domain
shifts using binary vectors of length m where m is
the size of of our feature representation. Each vector
then represents an expected OOV effect by encoding
what features are (predicted to be) missing in the tar-
get data, i.e. the ith feature will be missing if the ith
element of the binary vector is 0. However, since
we are minimizing average loss under OOV effects
it makes sense to restrict the class of vectors to en-
code OOV effects that we are likely to observe. This
could, for example, involve fixing an expected rate
of missing features or bounding it by some interval,
or it could involve distinguishing between features
that are likely to be missing in the target and fea-
tures that are not. Here is what we do in this paper:
Rather than thinking of domain shifts as some-
thing that deletes features, we propose to see do-
main shifts as something making certain features
less likely to occur in our data. We will in other
words simulate soft OOV effects, rather than hard
OOV effects. One way to think of this is as an im-
portance weighting of our features. This section pro-
vides some intuition for using inverse Zipfian distri-
butions as weight functions.
Say we are interested in making a model ?D1
learned from a known distribution D1 robust against
the distributional differences betweenD1 and an un-
known distribution D2. These two distributions are
somehow related to a distributionD0 (the underlying
language distribution from which the domain distri-
butions are sampled).
It is common to assume that linguistic distribu-
1: X = {?yi,xi?}Ni=1
2: w0 = 0,v = 0, i = 0
3: for k ? K do
4: for n ? N do
5: ? ? random.zipf(3,M)
6: if sign(w ? x ? ?) 6= yn then
7: wi+1 ? update(wi)
8: i? i+ 1
9: end if
10: v? v + wi
11: end for
12: end for
13: return w = v/(N ?K)
Figure 3: Z3SP
tions follow power laws (Zipf, 1935; Goldberg and
Elhadad, 2008). We will assume thatD1 = D0?Z1
whereZ1 is some Zipfian distribution. SayD0 ? Z0
is the master Zipfian distribution of language L0. If
we assume that (otherwise independent) domainsL1
and L2 follow products of Zipfians Z0 ? Z1 and
Z0 ?Z2, we derive the following:
Say w = ?Z0?Z1 is the model learned from the
source data. The ideal model is w? = ?Z0?Z2 , but
both Zipfians Z1 and Z2 are unknown. Since Z2
is unknown (and in many applications, we want to
model several Zi), the overall best model we can
hope for is w? = ?Z0 . Z0 is also unknown, but we
can observe a finite sample Z0 ?Z1. Since the den-
sity of Z1 is directly related to the weights in w, a
crude estimate of ?Z0 would be w
? ? w 1Z1 . Since
we cannot observe Z1, we instead try to minimize
average loss under all hypotheses about Z1.
In practice, we implement the idea of reweight-
ing by random inverse Zipfian distributitons (instead
of binary vectors) in the following way: Passing
through the data in averaged perceptron learning
(Figure 2), we consider one data point at a time. In
order to minize loss in all possible domains, we need
to consider all possible inverse Zipfian reweightings.
This would be possible if we provided a convex
formulation of the minimization problem along the
lines of Dekel and Shamir (2008), but instead we
randomly sample from a Zipfian and factor its in-
verse into our dataset. The parameter of the Zipfians
is set (to 3) on development data (the EWT-email de-
velopment data). The modified learning algorithm,
Z3SP, is presented in Figure 3.
670
5 POS tagging
POS tagging is the problem of assigning syntactic
categories or POS to tokenized word forms in run-
ning text. Most approaches to POS tagging use su-
pervised learning to learn sequence labeling models
from annotated ressources. The major ressource for
English is the Wall Street Journal (WSJ) sections of
the English Treebank (Marcus et al, 1993). POS
taggers are usually trained on Sect. 0?18 and eval-
uated on Sect. 22?24. In this paper we are not in-
terested in in-domain performance on WSJ data, but
rather in developing a robust POS tagger that is less
sensitive to domain shifts than current state-of-the-
art POS taggers and use the splits from a recent pars-
ing shared task rather than the standard POS tagging
ones.
6 Experiments
We train our tagger on Sections 2?21 of the WSJ
sections of the English Treebank, in the Ontotes
4.0 release. This was also the training data used
in the experiments in the Parsing the Web (PTW)
shared task at NAACL 2012.2 In the shared task
they used the coarse-grained Google tagset (Petrov
et al, 2011). We believe this tagset is too coarse-
grained for most purposes (Manning, 2011) and do
experiments with the original PTB tagset instead.
Our evaluation data comes from the English Web
Treebank (EWT),3 which was also used in the PTW
shared task. The EWT contains development and
evaluation data for five domains: answers (from Ya-
hoo!), emails (from the Enron corpus), BBC news-
groups, Amazon reviews, and weblogs. In order not
to optimize on in-domain data, we tune on the Email
development data and evaluate on the remaining do-
mains (the test sections).
The Web 2.0 data used for evaluation contains a
lot of non-canonical language use. An example is
the sentence you r retarded. from the Email section.
The POS tagger finds no support for r as a verb in the
training data, but needs to infer this from the context.
We also include experiments on the Danish De-
pendency Treebank (DDT) (Buch-Kromann, 2003),
which comes with meta-data enabling us to single
out four domains: newspaper, law, literature and
2https://sites.google.com/site/sancl2012/home/shared-task
3LDC Catalog No.: LDC2012T13.
SP BSP Z3SP
EWT-answers 85.22 85.45 85.59
EWT-newsgroups 86.82 86.94 87.42
EWT-reviews 84.92 85.14 85.67
EWT-weblogs 87.00 87.06 87.39
DDT-law 92.38 92.80 93.35
DDT-lit 93.61 93.80 93.85
DDT-mag 94.71 94.44 94.68
Table 1: Results. BSP samples binary vectors with prob-
abilities {0 : 0.1, 1 : 0.9}
magazines. We train our tagger on the newspaper
data and evaluate on the remaining three sections.
6.1 Results
The results are presented in Table 1. We first note
that improvements over the structured perceptron
are statistically significant with p < 0.01 across all
domains, except DDT-mag. We also note that us-
ing inverse Zipfian reweightings is better than using
binary vectors in almost all cases. We believe that
these are strong results given that we are assuming
no knowledge of the target domain, and our mod-
ification of the learning algorithm does not affect
computational efficiency at training or test time. The
average error reduction of Z3SP over the structured
perceptron (SP) is 8%. Since using inverse Zipfian
reweightings seems more motivated for node poten-
tials than for edge potentials, we also tried using
BSP for edge potentials and Z3SP for node poten-
tials. This mixed model acchieved 93.70, 93.91 and
94.35 on the DDT data, which on average is slightly
better than Z3SP.
7 Conclusions
Inspired by robust generalization and adversarial
learning we introduced a novel approach to learning
structured perceptrons for sequential labeling, which
is less sensitive to OOV effects. We evaluated our
approach on POS tagging data from the EWT and
the DDT with an average 4.4% error reduction over
the structured perceptron.
Acknowledgements
Anders S?gaard is funded by the ERC Starting Grant
LOWLANDS No. 313695.
671
References
Aharon Ben-Tal and Arkadi Nemirovski. 1998. Robust
convex optimization. Mathematics of Operations Re-
search, 23(4).
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
Matthias Buch-Kromann. 2003. The Danish Depen-
dency Treebank and the DTAG Treebank Tool. In
TLT.
Michael Collins. 2002. Discriminative training methods
for Hidden Markov Models. In EMNLP.
Hal Daume III. 2007. Frustratingly easy domain adapta-
tion. In ACL.
Ofer Dekel and Ohad Shamir. 2008. Learning to classify
with missing and corrupted features. In ICML.
Yoav Freund and Robert Schapire. 1999. Large margin
classification using the perceptron algorithm. Machine
Learning, 37:277?296.
Amir Globerson and Sam Roweis. 2006. Nightmare
at test time: robust learning by feature deletion. In
ICML.
Yoav Goldberg and Michael Elhadad. 2008. splitSVM:
fast, space-efficient, non-heuristic, polynomial kernel
computation for NLP applications. In ACL.
Geoffrey Hinton, N. Srivastava, A. Krizhevsky,
I. Sutskever, and R. Salakhutdinov. 2012. Im-
proving neural networks by preventing co-adaptation
of feature detectors. http://arxiv.org/abs/1207.0580.
Chris Manning. 2011. Part-of-speech tagging from
97%?to 100%: Is it time for some linguistics? In CI-
CLing.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Lin-
guistics, 19(2):313?330.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 Shared Task on Parsing the Web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Slav Petrov, Dipanjan Das, and Ryan McDonald.
2011. A universal part-of-speech tagset. CoRR
abs/1104.2086.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In ACL.
Anders S?gaard and Anders Johannsen. 2012. Robust
learning in random subspaces: equipping NLP against
OOV effects. In COLING.
Charles Sutton, Michael Sindelar, and Andrew McCal-
lum. 2006. Reducing weight undertraining in struc-
tured discriminative learning. In NAACL.
T Trafalis and R Gilbert. 2007. Robust support vector
machines for classification and computational issues.
Optimization Methods and Software, 22:187?198.
George Zipf. 1935. The psycho-biology of language.
Houghton Mifflin.
672
Proceedings of the ACL 2010 Conference Short Papers, pages 205?208,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Simple semi-supervised training of part-of-speech taggers
Anders S?gaard
Center for Language Technology
University of Copenhagen
soegaard@hum.ku.dk
Abstract
Most attempts to train part-of-speech tag-
gers on a mixture of labeled and unlabeled
data have failed. In this work stacked
learning is used to reduce tagging to a
classification task. This simplifies semi-
supervised training considerably. Our
prefered semi-supervised method com-
bines tri-training (Li and Zhou, 2005) and
disagreement-based co-training. On the
Wall Street Journal, we obtain an error re-
duction of 4.2% with SVMTool (Gimenez
and Marquez, 2004).
1 Introduction
Semi-supervised part-of-speech (POS) tagging is
relatively rare, and the main reason seems to be
that results have mostly been negative. Meri-
aldo (1994), in a now famous negative result, at-
tempted to improve HMM POS tagging by expec-
tation maximization with unlabeled data. Clark
et al (2003) reported positive results with little
labeled training data but negative results when
the amount of labeled training data increased; the
same seems to be the case in Wang et al (2007)
who use co-training of two diverse POS taggers.
Huang et al (2009) present positive results for
self-training a simple bigram POS tagger, but re-
sults are considerably below state-of-the-art.
Recently researchers have explored alternative
methods. Suzuki and Isozaki (2008) introduce
a semi-supervised extension of conditional ran-
dom fields that combines supervised and unsuper-
vised probability models by so-called MDF pa-
rameter estimation, which reduces error on Wall
Street Journal (WSJ) standard splits by about 7%
relative to their supervised baseline. Spoustova
et al (2009) use a new pool of unlabeled data
tagged by an ensemble of state-of-the-art taggers
in every training step of an averaged perceptron
POS tagger with 4?5% error reduction. Finally,
S?gaard (2009) stacks a POS tagger on an un-
supervised clustering algorithm trained on large
amounts of unlabeled data with mixed results.
This work combines a new semi-supervised
learning method to POS tagging, namely tri-
training (Li and Zhou, 2005), with stacking on un-
supervised clustering. It is shown that this method
can be used to improve a state-of-the-art POS tag-
ger, SVMTool (Gimenez and Marquez, 2004). Fi-
nally, we introduce a variant of tri-training called
tri-training with disagreement, which seems to
perform equally well, but which imports much less
unlabeled data and is therefore more efficient.
2 Tagging as classification
This section describes our dataset and our input
tagger. We also describe how stacking is used to
reduce POS tagging to a classification task. Fi-
nally, we introduce the supervised learning algo-
rithms used in our experiments.
2.1 Data
We use the POS-tagged WSJ from the Penn Tree-
bank Release 3 (Marcus et al, 1993) with the
standard split: Sect. 0?18 is used for training,
Sect. 19?21 for development, and Sect. 22?24 for
testing. Since we need to train our classifiers on
material distinct from the training material for our
input POS tagger, we save Sect. 19 for training our
classifiers. Finally, we use the (untagged) Brown
corpus as our unlabeled data. The number of to-
kens we use for training, developing and testing
the classifiers, and the amount of unlabeled data
available to it, are thus:
tokens
train 44,472
development 103,686
test 129,281
unlabeled 1,170,811
205
The amount of unlabeled data available to our
classifiers is thus a bit more than 25 times the
amount of labeled data.
2.2 Input tagger
In our experiments we use SVMTool (Gimenez
and Marquez, 2004) with model type 4 run incre-
mentally in both directions. SVMTool has an ac-
curacy of 97.15% on WSJ Sect. 22-24 with this
parameter setting. Gimenez and Marquez (2004)
report that SVMTool has an accuracy of 97.16%
with an optimized parameter setting.
2.3 Classifier input
The way classifiers are constructed in our experi-
ments is very simple. We train SVMTool and an
unsupervised tagger, Unsupos (Biemann, 2006),
on our training sections and apply them to the de-
velopment, test and unlabeled sections. The re-
sults are combined in tables that will be the input
of our classifiers. Here is an excerpt:1
Gold standard SVMTool Unsupos
DT DT 17
NNP NNP 27
NNP NNS 17*
NNP NNP 17
VBD VBD 26
Each row represents a word and lists the gold
standard POS tag, the predicted POS tag and the
word cluster selected by Unsupos. For example,
the first word is labeled ?DT?, which SVMTool
correctly predicts, and it belongs to cluster 17 of
about 500 word clusters. The first column is blank
in the table for the unlabeled section.
Generally, the idea is that a classifier will learn
to trust SVMTool in some cases, but that it may
also learn that if SVMTool predicts a certain tag
for some word cluster the correct label is another
tag. This way of combining taggers into a single
end classifier can be seen as a form of stacking
(Wolpert, 1992). It has the advantage that it re-
duces POS tagging to a classification task. This
may simplify semi-supervised learning consider-
ably.
2.4 Learning algorithms
We assume some knowledge of supervised learn-
ing algorithms. Most of our experiments are im-
plementations of wrapper methods that call off-
1The numbers provided by Unsupos refer to clusters; ?*?
marks out-of-vocabulary words.
the-shelf implementations of supervised learning
algorithms. Specifically we have experimented
with support vector machines (SVMs), decision
trees, bagging and random forests. Tri-training,
explained below, is a semi-supervised learning
method which requires large amounts of data.
Consequently, we only used very fast learning al-
gorithms in the context of tri-training. On the de-
velopment section, decisions trees performed bet-
ter than bagging and random forests. The de-
cision tree algorithm is the C4.5 algorithm first
introduced in Quinlan (1993). We used SVMs
with polynomial kernels of degree 2 to provide a
stronger stacking-only baseline.
3 Tri-training
This section first presents the tri-training algo-
rithm originally proposed by Li and Zhou (2005)
and then considers a novel variant: tri-training
with disagreement.
Let L denote the labeled data and U the unla-
beled data. Assume that three classifiers c1, c2, c3
(same learning algorithm) have been trained on
three bootstrap samples of L. In tri-training, an
unlabeled datapoint in U is now labeled for a clas-
sifier, say c1, if the other two classifiers agree on
its label, i.e. c2 and c3. Two classifiers inform
the third. If the two classifiers agree on a label-
ing, there is a good chance that they are right.
The algorithm stops when the classifiers no longer
change. The three classifiers are combined by ma-
jority voting. Li and Zhou (2005) show that un-
der certain conditions the increase in classification
noise rate is compensated by the amount of newly
labeled data points.
The most important condition is that the three
classifiers are diverse. If the three classifiers are
identical, tri-training degenerates to self-training.
Diversity is obtained in Li and Zhou (2005) by
training classifiers on bootstrap samples. In their
experiments, they consider classifiers based on the
C4.5 algorithm, BP neural networks and naive
Bayes classifiers. The algorithm is sketched
in a simplified form in Figure 1; see Li and
Zhou (2005) for all the details.
Tri-training has to the best of our knowledge not
been applied to POS tagging before, but it has been
applied to other NLP classification tasks, incl. Chi-
nese chunking (Chen et al, 2006) and question
classification (Nguyen et al, 2008).
206
1: for i ? {1..3} do
2: Si ? bootstrap sample(L)
3: ci ? train classifier(Si)
4: end for
5: repeat
6: for i ? {1..3} do
7: for x ? U do
8: Li ? ?
9: if cj(x) = ck(x)(j, k 6= i) then
10: Li ? Li ? {(x, cj(x)}
11: end if
12: end for
13: ci ? train classifier(L ? Li)
14: end for
15: until none of ci changes
16: apply majority vote over ci
Figure 1: Tri-training (Li and Zhou, 2005).
3.1 Tri-training with disagreement
We introduce a possible improvement of the tri-
training algorithm: If we change lines 9?10 in the
algorithm in Figure 1 with the lines:
if cj(x) = ck(x) 6= ci(x)(j, k 6= i) then
Li ? Li ? {(x, cj(x)}
end if
two classifiers, say c1 and c2, only label a data-
point for the third classifier, c3, if c1 and c2 agree
on its label, but c3 disagrees. The intuition is
that we only want to strengthen a classifier in its
weak points, and we want to avoid skewing our
labeled data by easy data points. Finally, since tri-
training with disagreement imports less unlabeled
data, it is much more efficient than tri-training. No
one has to the best of our knowledge applied tri-
training with disagreement to real-life classifica-
tion tasks before.
4 Results
Our results are presented in Figure 2. The stacking
result was obtained by training a SVM on top of
the predictions of SVMTool and the word clusters
of Unsupos. SVMs performed better than deci-
sion trees, bagging and random forests on our de-
velopment section, but improvements on test data
were modest. Tri-training refers to the original al-
gorithm sketched in Figure 1 with C4.5 as learn-
ing algorithm. Since tri-training degenerates to
self-training if the three classifiers are trained on
the same sample, we used our implementation of
tri-training to obtain self-training results and vali-
dated our results by a simpler implementation. We
varied poolsize to optimize self-training. Finally,
we list results for a technique called co-forests (Li
and Zhou, 2007), which is a recent alternative to
tri-training presented by the same authors, and for
tri-training with disagreement (tri-disagr). The p-
values are computed using 10,000 stratified shuf-
fles.
Tri-training and tri-training with disagreement
gave the best results. Note that since tri-training
leads to much better results than stacking alone,
it is unlabeled data that gives us most of the im-
provement, not the stacking itself. The differ-
ence between tri-training and self-training is near-
significant (p <0.0150). It seems that tri-training
with disagreement is a competitive technique in
terms of accuracy. The main advantage of tri-
training with disagreement compared to ordinary
tri-training, however, is that it is very efficient.
This is reflected by the average number of tokens
in Li over the three learners in the worst round of
learning:
av. tokens in Li
tri-training 1,170,811
tri-disagr 173
Note also that self-training gave very good re-
sults. Self-training was, again, much slower than
tri-training with disagreement since we had to
train on a large pool of unlabeled data (but only
once). Of course this is not a standard self-training
set-up, but self-training informed by unsupervised
word clusters.
4.1 Follow-up experiments
SVMTool is one of the most accurate POS tag-
gers available. This means that the predictions
that are added to the labeled data are of very
high quality. To test if our semi-supervised learn-
ing methods were sensitive to the quality of the
input taggers we repeated the self-training and
tri-training experiments with a less competitive
POS tagger, namely the maximum entropy-based
POS tagger first described in (Ratnaparkhi, 1998)
that comes with the maximum entropy library in
(Zhang, 2004). Results are presented as the sec-
ond line in Figure 2. Note that error reduction is
much lower in this case.
207
BL stacking tri-tr. self-tr. co-forests tri-disagr error red. p-value
SVMTool 97.15% 97.19% 97.27% 97.26% 97.13% 97.27% 4.21% <0.0001
MaxEnt 96.31% - 96.36% 96.36% 96.28% 96.36% 1.36% <0.0001
Figure 2: Results on Wall Street Journal Sect. 22-24 with different semi-supervised methods.
5 Conclusion
This paper first shows how stacking can be used to
reduce POS tagging to a classification task. This
reduction seems to enable robust semi-supervised
learning. The technique was used to improve the
accuracy of a state-of-the-art POS tagger, namely
SVMTool. Four semi-supervised learning meth-
ods were tested, incl. self-training, tri-training, co-
forests and tri-training with disagreement. All
methods increased the accuracy of SVMTool sig-
nificantly. Error reduction on Wall Street Jour-
nal Sect. 22-24 was 4.2%, which is comparable
to related work in the literature, e.g. Suzuki and
Isozaki (2008) (7%) and Spoustova et al (2009)
(4?5%).
References
Chris Biemann. 2006. Unsupervised part-of-speech
tagging employing efficient graph clustering. In
COLING-ACL Student Session, Sydney, Australia.
Wenliang Chen, Yujie Zhang, and Hitoshi Isahara.
2006. Chinese chunking with tri-training learn-
ing. In Computer processing of oriental languages,
pages 466?473. Springer, Berlin, Germany.
Stephen Clark, James Curran, and Mike Osborne.
2003. Bootstrapping POS taggers using unlabeled
data. In CONLL, Edmonton, Canada.
Jesus Gimenez and Lluis Marquez. 2004. SVMTool: a
general POS tagger generator based on support vec-
tor machines. In LREC, Lisbon, Portugal.
Zhongqiang Huang, Vladimir Eidelman, and Mary
Harper. 2009. Improving a simple bigram HMM
part-of-speech tagger by latent annotation and self-
training. In NAACL-HLT, Boulder, CO.
Ming Li and Zhi-Hua Zhou. 2005. Tri-training: ex-
ploiting unlabeled data using three classifiers. IEEE
Transactions on Knowledge and Data Engineering,
17(11):1529?1541.
Ming Li and Zhi-Hua Zhou. 2007. Improve computer-
aided diagnosis with machine learning techniques
using undiagnosed samples. IEEE Transactions on
Systems, Man and Cybernetics, 37(6):1088?1098.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155?171.
Tri Nguyen, Le Nguyen, and Akira Shimazu. 2008.
Using semi-supervised learning for question classi-
fication. Journal of Natural Language Processing,
15:3?21.
Ross Quinlan. 1993. Programs for machine learning.
Morgan Kaufmann.
Adwait Ratnaparkhi. 1998. Maximum entropy mod-
els for natural language ambiguity resolution. Ph.D.
thesis, University of Pennsylvania.
Anders S?gaard. 2009. Ensemble-based POS tagging
of italian. In IAAI-EVALITA, Reggio Emilia, Italy.
Drahomira Spoustova, Jan Hajic, Jan Raab, and
Miroslav Spousta. 2009. Semi-supervised training
for the averaged perceptron POS tagger. In EACL,
Athens, Greece.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In ACL, pages 665?673,
Columbus, Ohio.
Wen Wang, Zhongqiang Huang, and Mary Harper.
2007. Semi-supervised learning for part-of-speech
tagging of Mandarin transcribed speech. In ICASSP,
Hawaii.
David Wolpert. 1992. Stacked generalization. Neural
Networks, 5:241?259.
Le Zhang. 2004. Maximum entropy modeling toolkit
for Python and C++. University of Edinburgh.
208
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 48?52,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Semisupervised condensed nearest neighbor for part-of-speech tagging
Anders S?gaard
Center for Language Technology
University of Copenhagen
Njalsgade 142, DK-2300 Copenhagen S
soegaard@hum.ku.dk
Abstract
This paper introduces a new training set con-
densation technique designed for mixtures
of labeled and unlabeled data. It finds a
condensed set of labeled and unlabeled data
points, typically smaller than what is obtained
using condensed nearest neighbor on the la-
beled data only, and improves classification
accuracy. We evaluate the algorithm on semi-
supervised part-of-speech tagging and present
the best published result on the Wall Street
Journal data set.
1 Introduction
Labeled data for natural language processing tasks
such as part-of-speech tagging is often in short sup-
ply. Semi-supervised learning algorithms are de-
signed to learn from a mixture of labeled and un-
labeled data. Many different semi-supervised algo-
rithms have been applied to natural language pro-
cessing tasks, but the simplest algorithm, namely
self-training, is the one that has attracted most atten-
tion, together with expectation maximization (Ab-
ney, 2008). The idea behind self-training is simply
to let a model trained on the labeled data label the
unlabeled data points and then to retrain the model
on the mixture of the original labeled data and the
newly labeled data.
The nearest neighbor algorithm (Cover and Hart,
1967) is a memory-based or so-called lazy learn-
ing algorithm. It is one of the most extensively
used nonparametric classification algorithms, sim-
ple to implement yet powerful, owing to its theo-
retical properties guaranteeing that for all distribu-
tions, its probability of error is bound by twice the
Bayes probability of error (Cover and Hart, 1967).
Memory-based learning has been applied to a wide
range of natural language processing tasks including
part-of-speech tagging (Daelemans et al, 1996), de-
pendency parsing (Nivre, 2003) and word sense dis-
ambiguation (Ku?bler and Zhekova, 2009). Memory-
based learning algorithms are said to be lazy be-
cause no model is learned from the labeled data
points. The labeled data points are the model. Con-
sequently, classification time is proportional to the
number of labeled data points. This is of course im-
practical. Many algorithms have been proposed to
make memory-based learning more efficient. The
intuition behind many of them is that the set of la-
beled data points can be reduced or condensed, since
many labeled data points are more or less redundant.
The algorithms try to extract a subset of the overall
training set that correctly classifies all the discarded
data points through the nearest neighbor rule. Intu-
itively, the model finds good representatives of clus-
ters in the data or discards the data points that are far
from the decision boundaries. Such algorithms are
called training set condensation algorithms.
The need for training set condensation is partic-
ularly important in semi-supervised learning where
we rely on a mixture of labeled and unlabeled data
points. While the number of labeled data points
is typically limited, the number of unlabeled data
points is typically high. In this paper, we intro-
duce a new semi-supervised learning algorithm that
combines self-training and condensation to produce
small subsets of labeled and unlabeled data points
that are highly relevant for determining good deci-
48
sion boundaries.
2 Semi-supervised condensed nearest
neighbor
The nearest neighbor (NN) algorithm (Cover and
Hart, 1967) is conceptually simple, yet very pow-
erful. Given a set of labeled data points T , label any
new data point (feature vector) x with y where x?
is the data point in T most similar to x and ?x?, y?.
Similarity is usually measured in terms of Euclidean
distance. The generalization of the nearest neighbor
algorithm, k nearest neighbor, finds the k most simi-
lar data points Tk to x and assigns x the label y? such
that:
y? = arg max
y???Y
??x?,y???TkE(x,x
?)||y? = y??||
with E(?, ?) Euclidean distance and || ? || = 1 if the
argument is true (else 0). In other words, the k most
similar points take a weighted vote on the class of x.
Naive implementations of the algorithm store all
the labeled data points and compare each of them to
the data point that is to be classified. Several strate-
gies have been proposed to make nearest neighbor
classification more efficient (Angiulli, 2005). In
particular, training set condensation techniques have
been much studied.
The condensed nearest neighbor (CNN) algorithm
was first introduced in Hart (1968). Finding a sub-
set of the labeled data points may lead to faster
and more accurate classification, but finding the best
subset is an intractable problem (Wilfong, 1992).
CNN can be seen as a simple technique for approxi-
mating such a subset of labeled data points.
The CNN algorithm is defined in Figure 1 with T
the set of labeled data points and T (t) is label pre-
dicted for t by a nearest neighbor classifier ?trained?
on T .
Essentially we discard all labeled data points
whose label we can already predict with the cur-
rent subset of labeled data points. Note that we
have simplified the CNN algorithm a bit compared
to Hart (1968), as suggested, for example, in Alpay-
din (1997), iterating only once over data rather than
waiting for convergence. This will give us a smaller
set of labeled data points, and therefore classifica-
tion requires less space and time. Note that while
the NN rule is stable, and cannot be improved by
T = {?x1, y1?, . . . , ?xn, yn?}, C = ?
for ?xi, yi? ? T do
if C(xi) 6= yi then
C = C ? {?xi, yi?}
end if
end for
return C
Figure 1: CONDENSED NEAREST NEIGHBOR.
T = {?x1, y1?, . . . , ?xn, yn?}, C = ?
for ?xi, yi? ? T do
if C(xi) 6= yi or PC(?xi, yi?|xi) < 0.55 then
C = C ? {?xi, yi?}
end if
end for
return C
Figure 2: WEAKENED CONDENSED NEAREST NEIGH-
BOR.
techniques such as bagging (Breiman, 1996), CNN
is unstable (Alpaydin, 1997).
We also introduce a weakened version of the al-
gorithm which not only includes misclassified data
points in the classifier C , but also correctly classi-
fied data points which were labeled with relatively
low confidence. So C includes all data points that
were misclassified and those whose correct label
was predicted with low confidence. The weakened
condensed nearest neighbor (WCNN) algorithm is
sketched in Figure 2.
C inspects k nearest neighbors when labeling
new data points, where k is estimated by cross-
validation. CNN was first generalized to k-NN in
Gates (1972).
Two related condensation techniques, namely re-
moving typical elements and removing elements by
class prediction strength, were argued not to be
useful for most problems in natural language pro-
cessing in Daelemans et al (1999), but our experi-
ments showed that CNN often perform about as well
as NN, and our semi-supervised CNN algorithm
leads to substantial improvements. The condensa-
tion techniques are also very different: While re-
moving typical elements and removing elements by
class prediction strength are methods for removing
data points close to decision boundaries, CNN ide-
49
Figure 3: Unlabeled data may help find better representa-
tives in condensed training sets.
ally only removes elements close to decision bound-
aries when the classifier has no use of them.
Intuitively, with relatively simple problems,
e.g. mixtures of Gaussians, CNN and WCNN try to
find the best possible representatives for each clus-
ter in the distribution of data, i.e. finding the points
closest to the center of each cluster. Ideally, CNN
returns one point for each cluster, namely the cen-
ter of each cluster. However, a sample of labeled
data may not include data points that are near the
center of a cluster. Consequently, CNN sometimes
needs several points to stabilize the representation of
a cluster; e.g. the two positives in Figure 3.
When a large number of unlabeled data points
that are labeled according to nearest neighbors pop-
ulates the clusters, chances increase that we find data
points near the centers of our clusters, e.g. the ?good
representative? in Figure 3. Of course the centers of
our clusters may move, but the positive results ob-
tained experimentally below suggest that it is more
likely that labeling unlabeled data by nearest neigh-
bors will enable us to do better training set conden-
sation.
This is exactly what semi-supervised condensed
nearest neighbor (SCNN) does. We first run a
WCNN C and obtain a condensed set of labeled data
points. To this set of labeled data points we add a
large number of unlabeled data points labeled by a
NN classifier T on the original data set. We use a
simple selection criterion and include all data points
1: T = {?x1, y1?, . . . , ?xn, yn?}, C = ?, C ? = ?
2: U = {?x?1?, . . . , ?x?m?} # unlabeled data
3: for ?xi, yi? ? T do
4: if C(xi) 6= yi or PC(?xi, yi?|xi) < 0.55
then
5: C = C ? {?xi, yi?}
6: end if
7: end for
8: for ?x?i? ? U do
9: if PT (?x?i, T (x?i)?|wi) > 0.90 then
10: C = C ? {?x?i, T (x?i)?}
11: end if
12: end for
13: for ?xi, yi? ? C do
14: if C ?(xi) 6= yi then
15: C ? = C ? ? {?xi, yi?}
16: end if
17: end for
18: return C ?
Figure 4: SEMI-SUPERVISED CONDENSED NEAREST
NEIGHBOR.
that are labeled with confidence greater than 90%.
We then obtain a new WCNN C ? from the new data
set which is a mixture of labeled and unlabeled data
points. See Figure 4 for details.
3 Part-of-speech tagging
Our part-of-speech tagging data set is the standard
data set from Wall Street Journal included in Penn-
III (Marcus et al, 1993). We use the standard splits
and construct our data set in the following way, fol-
lowing S?gaard (2010): Each word in the data wi
is associated with a feature vector xi = ?x1i , x2i ?
where x1i is the prediction on wi of a supervised part-
of-speech tagger, in our case SVMTool1 (Gimenez
and Marquez, 2004) trained on Sect. 0?18, and x2i
is a prediction on wi from an unsupervised part-of-
speech tagger (a cluster label), in our case Unsu-
pos (Biemann, 2006) trained on the British National
Corpus.2 We train a semi-supervised condensed
nearest neighbor classifier on Sect. 19 of the devel-
opment data and unlabeled data from the Brown cor-
pus and apply it to Sect. 22?24. The labeled data
1http://www.lsi.upc.es/?nlp/SVMTool/
2http://wortschatz.uni-leipzig.de/?cbiemann/software/
50
points are thus of the form (one data point or word
per line):
JJ JJ 17*
NNS NNS 1
IN IN 428
DT DT 425
where the first column is the class labels or the
gold tags, the second column the predicted tags and
the third column is the ?tags? provided by the unsu-
pervised tagger. Words marked by ?*? are out-of-
vocabulary words, i.e. words that did not occur in
the British National Corpus. The unsupervised tag-
ger is used to cluster tokens in a meaningful way.
Intuitively, we try to learn part-of-speech tagging by
learning when to rely on SVMTool.
The best reported results in the literature on Wall
Street Journal Sect. 22?24 are 97.40% in Suzuki et
al. (2009) and 97.44% in Spoustova et al (2009);
both systems use semi-supervised learning tech-
niques. Our semi-supervised condensed nearest
neighbor classifier achieves an accuracy of 97.50%.
Equally importantly it condensates the available data
points, from Sect. 19 and the Brown corpus, that
is more than 1.2M data points, to only 2249 data
points, making the classifier very fast. CNN alone is
a lot worse than the input tagger, with an accuracy
of 95.79%. Our approach is also significantly better
than S?gaard (2010) who apply tri-training (Li and
Zhou, 2005) to the output of SVMTool and Unsu-
pos.
acc (%) data points err.red
CNN 95.79 3,811
SCNN 97.50 2,249 40.6%
SVMTool 97.15 -
S?gaard 97.27 -
Suzuki et al 97.40 -
Spoustova et al 97.44 -
In our second experiment, where we vary the
amount of unlabeled data points, we only train our
ensemble on the first 5000 words in Sect. 19 and
evaluate on the first 5000 words in Sect. 22?24.
The derived learning curve for the semi-supervised
learner is depicted in Figure 5. The immediate drop
in the red scatter plot illustrates the condensation ef-
fect of semi-supervised learning: when we begin to
add unlabeled data, accuracy increases by more than
1.5% and the data set becomes more condensed.
Semi-supervised learning means that we populate
Figure 5: Normalized accuracy (range: 92.62?94.82) and
condensation (range: 310?512 data points).
clusters in the data, making it easier to identify rep-
resentative data points. Since we can easier identify
representative data points, training set condensation
becomes more effective.
4 Implementation
The implementation used in the experiments builds
on Orange 2.0b for Mac OS X (Python and C++).
In particular, we made use of the implementations
of Euclidean distance and random sampling in their
package. Our code is available at:
cst.dk/anders/sccn/
5 Conclusions
We have introduced a new learning algorithm that
simultaneously condensates labeled data and learns
from a mixture of labeled and unlabeled data. We
have compared the algorithm to condensed nearest
neighbor (Hart, 1968; Alpaydin, 1997) and showed
that the algorithm leads to more condensed models,
and that it performs significantly better than con-
densed nearest neighbor. For part-of-speech tag-
ging, the error reduction over condensed nearest
neighbor is more than 40%, and our model is 40%
smaller than the one induced by condensed nearest
neighbor. While we have provided no theory for
semi-supervised condensed nearest neighbor, we be-
lieve that these results demonstrate the potential of
the proposed method.
51
References
Steven Abney. 2008. Semi-supervised learning for com-
putational linguistics. Chapman & Hall.
Ethem Alpaydin. 1997. Voting over multiple con-
densed nearest neighbors. Artificial Intelligence Re-
view, 11:115?132.
Fabrizio Angiulli. 2005. Fast condensed nearest neigh-
bor rule. In Proceedings of the 22nd International
Conference on Machine Learning.
Chris Biemann. 2006. Unsupervised part-of-speech
tagging employing efficient graph clustering. In
COLING-ACL Student Session.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
T. Cover and P. Hart. 1967. Nearest neighbor pattern
classification. IEEE Transactions on Information The-
ory, 13(1):21?27.
Walter Daelemans, Jakub Zavrel, Peter Berck, and Steven
Gillis. 1996. MBT: a memory-based part-of-speech
tagger generator. In Proceedings of the 4th Workshop
on Very Large Corpora.
Walter Daelemans, Antal Van Den Bosch, and Jakub Za-
vrel. 1999. Forgetting exceptions is harmful in lan-
guage learning. Machine Learning, 34(1?3):11?41.
W Gates. 1972. The reduced nearest neighbor rule.
IEEE Transactions on Information Theory, 18(3):431?
433.
Jesus Gimenez and Lluis Marquez. 2004. SVMTool: a
general POS tagger generator based on support vector
machines. In LREC.
Peter Hart. 1968. The condensed nearest neighbor rule.
IEEE Transactions on Information Theory, 14:515?
516.
Sandra Ku?bler and Desislava Zhekova. 2009. Semi-
supervised learning for word-sense disambiguation:
quality vs. quantity. In RANLP.
Ming Li and Zhi-Hua Zhou. 2005. Tri-training: ex-
ploiting unlabeled data using three classifiers. IEEE
Transactions on Knowledge and Data Engineering,
17(11):1529?1541.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Lin-
guistics, 19(2):313?330.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies, pages
149?160.
Anders S?gaard. 2010. Simple semi-supervised training
of part-of-speech taggers. In ACL.
Drahomira Spoustova, Jan Hajic, Jan Raab, and Miroslav
Spousta. 2009. Semi-supervised training for the aver-
aged perceptron POS tagger. In EACL.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael
Collins. 2009. An empirical study of semi-supervised
structured conditional models for dependency parsing.
In EMNLP.
G. Wilfong. 1992. Nearest neighbor problems. Interna-
tional Journal of Computational Geometry and Appli-
cations, 2(4):383?416.
52
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 682?686,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Data point selection for cross-language adaptation of dependency parsers
Anders S?gaard
Center for Language Technology
University of Copenhagen
Njalsgade 142, DK-2300 Copenhagen S
soegaard@hum.ku.dk
Abstract
We consider a very simple, yet effective, ap-
proach to cross language adaptation of depen-
dency parsers. We first remove lexical items
from the treebanks and map part-of-speech
tags into a common tagset. We then train a
language model on tag sequences in otherwise
unlabeled target data and rank labeled source
data by perplexity per word of tag sequences
from less similar to most similar to the target.
We then train our target language parser on
the most similar data points in the source la-
beled data. The strategy achieves much better
results than a non-adapted baseline and state-
of-the-art unsupervised dependency parsing,
and results are comparable to more complex
projection-based cross language adaptation al-
gorithms.
1 Introduction
While unsupervised dependency parsing has seen
rapid progress in recent years, results are still far
from the results that can be achieved with supervised
parsers and not yet good enough to solve real-world
problems. In this paper, we will be interested in an
alternative strategy, namely cross-language adapta-
tion of dependency parsers. The idea is, briefly put,
to learn how to parse Arabic, for example, from, say,
a Danish treebank, comparing unlabeled data from
both languages. This is similar to, but more diffi-
cult than most domain adaptation or transfer learn-
ing scenarios, where differences between source and
target distributions are smaller.
Most previous work in cross-language adapta-
tion has used parallel corpora to project dependency
structures across translations using word alignments
(Smith and Eisner, 2009; Spreyer and Kuhn, 2009;
Ganchev et al, 2009), but in this paper we show
that similar results can be achieved by much simpler
means. Specifically, we build on the cross-language
adaptation algorithm for closely related languages
developed by Zeman and Resnik (2008) and extend
it to much less related languages.
1.1 Related work
Zeman and Resnik (2008) simply mapped part-of-
speech tags of source and target language treebanks
into a common tagset, delexicalized them (removed
all words), trained a parser on the source language
treebank and applied it to the target language. The
intuition is that, at least for relatively similar lan-
guages, features based on part-of-speech tags are
enough to do reasonably well, and languages are rel-
atively similar at this level of abstraction. Of course
annotations differ, but nouns are likely to be depen-
dents of verbs, prepositions are likely to be depen-
dents of nouns, and so on.
Specifically, Zeman and Resnik (2008) trained a
constituent-based parser on the training section of
the Danish treebank and evaluated it on sentences
of up to 40 words in the test section of the Swedish
treebank and obtained an F1-score of 66.40%. Dan-
ish and Swedish are of course very similar languages
with almost identical syntax, so in a way this result is
not very surprising. In this paper, we present similar
results (50-75%) on full length sentences for very
different languages from different language fami-
lies. Since less related languages differ more in their
syntax, we use data point selection to find syntactic
682
constructions in the source language that are likely
to be similar to constructions in the target language.
Smith and Eisner (2009) think of cross-language
adaptation as unsupervised projection using word
aligned parallel text to construct training material for
the target language. They show that hard projection
of dependencies using word alignments performs
better than the unsupervised dependency parsing
approach described in Klein and Manning (2004),
based on EM with clever initialization, and that
a quasi-synchronous model using word alignments
to reestimate parameters in EM performs even bet-
ter. The authors report good results (65%-70%) for
somewhat related languages, training on English and
testing on German and Spanish, but they modified
the annotation in the German data making the treat-
ment of certain syntactic constructions more similar
to the English annotations.
Spreyer and Kuhn (2009) use a similar approach
to parse Dutch using labeled data from German and
obtain good results, but again these are very simi-
lar languages. They later extended their results to
English and Italian (Spreyer et al, 2010), but also
modified annotation considerably in order to do so.
Finally, Ganchev et al (2009) report results of a
similar approach for Bulgarian and Spanish; they re-
port results with and without hand-written language-
specific rules that complete the projected partial de-
pendency trees.
We will compare our results to the plain approach
of Zeman and Resnik (2008), Ganchev et al (2009)
without hand-written rules and two recent contribu-
tions to unsupervised dependency parsing, Gillen-
water et al (2010) and Naseem et al (2010). Gillen-
water et al (2010) is a fully unsupervised exten-
sion of the approach described in Klein and Man-
ning (2004), whereas Naseem et al (2010) rely on
hand-written cross-lingual rules.
2 Data
We use four treebanks from the CoNLL 2006 Shared
Task with standard splits. We use the tagset map-
pings also used by Zeman and Resnik (2008) to ob-
tain a common tagset.12 They define tagset map-
1https://wiki.ufal.ms.mff.cuni.cz/user:zeman:interset
2We use the first letter in the common tag as coarse-grained
part-of-speech, and the first three as fine-grained part-of-speech.
pings for Arabic, Bulgarian, Czech, Danish, Por-
tuguese and Swedish. We only use four of these tree-
banks, since Bulgarian and Czech as well as Danish
and Swedish are very similar languages.
The four treebanks used in our experiments are
thus those for Arabic, Bulgarian, Danish and Por-
tuguese. Arabic is a Semitic VSO language with
relatively free word order and rich morphology. Bul-
garian is a Slavic language with relatively free word
order and rich morphology. Danish is a Germanic
V2 language with relatively poor morphology. Fi-
nally, Portuguese is a Roman language with rela-
tively free word order and rich morphology. In sum,
we consider four languages that are less related than
the language pairs studied in earlier papers on cross-
language adaptation of dependency parsers.
3 Experiments
3.1 Data point selection
The key idea in our experiments is that we can use a
simple form of instance weighting, similar to what is
often used for correcting sample selection bias or for
domain adaptation, to improve the approach in Ze-
man and Resnik (2008) by selecting only sentences
in the source data that are similar to our target do-
main or language, considering their perplexity per
word in a language model trained on target data. The
idea is that we order the labeled source data from
most similar to least similar to our target data, using
perplexity per word as metric, and use only a portion
of the source data that is similar to our target data.
In cross-language adaptation, the sample selec-
tion bias is primarily a bias in marginal distribu-
tion P (x). This is the covariate shift assumption
(Shimodaira, 2000). Consequently, each sentence
should be weighted by Pt(x)Ps(x) where Pt is the target
distribution, and Ps the source distribution.
To see this let x ? X in lowercase denote a spe-
cific value of the input variable, an unlabeled exam-
ple. y ? Y in lowercase denotes a class value, and
?x, y? is a labeled example. P (?x, y?) is the joint
probability of the labeled example, and P? (?x, y?) its
empirical distribution.
In supervised learning with N labeled data points,
we minimize the empirical risk to find a good model
?? for a loss function l : X ? Y ??:
683
?? = argmin
???
?
?x,y??X?Y
P? (?x, y?)l(x, y, ?)
= argmin
???
N
?
i=1
l(xi, yi, ?)
In domain adaptation, we can rewrite this as:
?? = argmin
???
?
?x,y??X?Y
Pt(?x, y?)
Ps(?x, y?)
P?s(?x, y?)l(x, y, ?)
= argmin
???
Ns
?
i=1
Pt(?xsi , ysi ?)
Ps(?xsi , ysi ?)
l(xsi , y
s
i , ?)
Under the covariate shift assumption Pt(?x,y?)Ps(?x,y?) for a
pair ?x, y? can be replaced with Pt(x)Ps(x) . We simplify
this function further assuming that
Pt(x)
Ps(x)
=
{
0 if Pt(x) is low
1 if Pt(x) is high
}
We use perplexity per word of the source lan-
guage POS sequences relative to a model trained
on target language POS sequences to guess whether
Pt(x) is high or low.
The treebanks are first delexicalized and all fea-
tures except part-of-speech tags removed. The
part-of-speech tags are mapped into a common
tagset using the technique described in Zeman and
Resnik (2008). For our main results, which are pre-
sented in Figure 1, we use the remaining three tree-
banks as training material for each language. The
test section of the language in question is used for
testing, while the POS sequences in the target train-
ing section is used for training the unsmoothed lan-
guage model. We use an unsmoothed trigram lan-
guage model rather than a smoothed language model
since modified Knesser-Ney smoothing is not de-
fined for sequences of part-of-speech tags.3
In our experiments we use a graph-based second-
order non-projective dependency parser that induces
models using MIRA (McDonald et al, 2005).4 We
do not optimize parameters on the different lan-
guages, but use default parameters across the board.
3http://www-speech.sri.com/projects/srilm/
4http://sourceforge.net/projects/mstparser/
We present two results and a baseline for each lan-
guage in Figure 1. Our baseline is the accuracy of
our dependency parser trained on three languages
and evaluated on the fourth language, where tree-
banks have been delexicalized, and part-of-speech
tags mapped into a common format. This is the pro-
posal by Zeman and Resnik (2008). We then present
results using the 90% most similar data points and
results where the amount of labeled data used is se-
lected using 100 sentences sampled from the train-
ing data as held-out data. It can be seen that using
90% of the labeled data seems to be a good strat-
egy if using held-out data is not an option. Since we
consider the unsupervised scenario where no labeled
data is available for the target language, we consider
the results obtained using the 90% most similar sen-
tences in the labeled data as our primary results.
That we obtain good results training on all the
three remaining treebanks for each language illus-
trates the robustness of our approach. However, it
may in some cases be better to train on data from
a single resource only. The results presented in
Figure 2 are the best results obtained with varying
amounts of source language data (10%, 20%, . . . , or
100%). The results are only explorative. In all cases,
we obtain slightly results with training material from
only one language that are better than or as good as
our main results, but differences are marginal. We
obtain the best results for Arabic training using la-
beled data from the Bulgarian treebank, and the best
results for Bulgarian training on Portuguese only.
The best results for Danish were, somewhat surpris-
ingly, obtained using the Arabic treebank,5 and the
best results for Portuguese were obtained training
only on Bulgarian data.
4 Error analysis
Consider our analysis of the Arabic sentence in Fig-
ure 3, using the three remaining treebanks as source
data. First note that our dependency labels are all
wrong; we did not map the dependency labels of
the source and target treebanks into a common set
of labels. Otherwise we only make mistakes about
punctuation. Our labels seem meaningful, but come
5Arabic and Danish have in common that definiteness is ex-
pressed by inflectional morphology, though, and both languages
frequently use VSO constructions.
684
Arabic Bulgarian Danish Portuguese
? 10 ? ? 10 ? ? 10 ? ? 10 ?
Ganchev et al (2009) - - 67.8 - - - - -
Gillenwater et al (2010) - - 54.3 - 47.2 - 59.8 -
Naseem et al (2010) - - - - 51.9 - 71.5 -
100% (baseline) - 45.5 - 44.5 - 51.7 - 37.1
90% 48.3 48.4 77.1 70.2 59.4 51.9 83.1 75.1
Held-out % - 49.2 - 70.3 - 52.8 - 75.1
Figure 1: Main results.
source/target Arabic Bulgarian Danish Portuguese
Arabic - 45.8 56.5 37.8
Bulgarian 50.2 - 50.8 76.9
Danish 46.9 60.4 - 63.5
Portuguese 50.1 70.3 52.2 -
Figure 2: Best results obtained with different combinations of source and target languages.
Figure 3: A predicted analysis for an Arabic sentence and
its correct analysis.
from different treebanks, e.g. ?pnct? from the Danish
treebank and ?PUNC? from the Portuguese one.
If we consider the case where we train on all re-
maining treebanks and use the 90% data points most
similar to the target language, and compare it to our
100% baseline, our error reductions are distributed
as follows, relative to dependency length: For Ara-
bic, the error reduction in F1 scores decreases with
dependency length, and more errors are made at-
taching to the root, but for Portuguese, where the
improvements are more dramatic, we see the biggest
improvements with attachments to the roots and
long dependencies:
Portuguese bl (F1) 90% (F1) err.red
root 0.627 0.913 76.7%
1 0.720 0.894 62.1%
2 0.292 0.768 67.2%
3?6 0.328 0.570 36.0%
7? 0.240 0.561 42.3%
For Danish, we see a similar pattern, but for Bul-
garian, error reductions are equally distributed.
Generally, it is interesting that cross-language
adaptation and data point selection were less effec-
tive for Danish. One explantation may be differ-
ences in annotation, however. The Danish depen-
dency treebank is annotated very differently from
most other dependency treebanks; for example, the
treebank adopts a DP-analysis of noun phrases.
Finally, we note that all languages benefit from re-
moving the least similar 10% of the labeled source
data, but results are less influenced by how much of
the remaining data we use. For example, for Bulgar-
ian our baseline result using 100% of the source data
is 44.5%, and the result obtained using 90% of the
source data is 70.2%. Using held-out data, we only
use 80% of the source data, which is slightly better
(70.3%), but even if we only use 10% of the source
data, our accuracy is still significantly better than the
baseline (66.9%).
5 Conclusions
This paper presented a simple data point selection
strategy for semi-supervised cross language adapta-
tion where no labeled target data is available. This
problem is difficult, but we have presented very pos-
itive results. Since our strategy is a parameter-free
wrapper method it can easily be applied to other
dependency parsers and other problems in natural
language processing, incl. part-of-speech tagging,
named entity recognition, and machine translation.
685
References
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext pro-
jection constraints. In ACL.
Jennifer Gillenwater, Kuzman Ganchev, Joao Graca, Fer-
nando Pereira, and Ben Taskar. 2010. Sparsity in de-
pendency grammar induction. In ACL.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In ACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In ACL.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowledge
to guide grammar induction. In EMNLP.
Hidetoshi Shimodaira. 2000. Improving predictive in-
ference under covariate shift by weighting the log-
likelihood function. Journal of Statistical Planning
and Inference, 90:227?244.
David Smith and Jason Eisner. 2009. Parser adaptation
and projection with quasi-synchronous grammar fea-
tures. In EMNLP.
Kathrin Spreyer and Jonas Kuhn. 2009. Data-driven de-
pendency parsing of new languages using incomplete
and noisy training data. In CoNLL.
Kathrin Spreyer, Lilja ?vrelid, and Jonas Kuhn. 2010.
Training parsers on partial trees: a cross-language
comparison. In LREC.
Daniel Zeman and Philip Resnik. 2008. Cross-language
parser adaptation between related languages. In IJC-
NLP.
686
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 640?644,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Part-of-speech tagging with antagonistic adversaries
Anders S?gaard
Center for Language Technology
University of Copenhagen
DK-2300 Copenhagen S
soegaard@hum.ku.dk
Abstract
Supervised NLP tools and on-line services
are often used on data that is very dif-
ferent from the manually annotated data
used during development. The perfor-
mance loss observed in such cross-domain
applications is often attributed to covari-
ate shifts, with out-of-vocabulary effects
as an important subclass. Many discrim-
inative learning algorithms are sensitive to
such shifts because highly indicative fea-
tures may swamp other indicative features.
Regularized and adversarial learning algo-
rithms have been proposed to be more ro-
bust against covariate shifts. We present
a new perceptron learning algorithm us-
ing antagonistic adversaries and compare
it to previous proposals on 12 multilin-
gual cross-domain part-of-speech tagging
datasets. While previous approaches do
not improve on our supervised baseline,
our approach is better across the board
with an average 4% error reduction.
1 Introduction
Most learning algorithms assume that training and
test data are governed by identical distributions;
and more specifically, in the case of part-of-speech
(POS) tagging, that training and test sentences
were sampled at random and that they are identi-
cally and independently distributed. Significance
is usually tested across data points in standard
NLP test sets. Such datasets typically contain run-
ning text rather than independently sampled sen-
tences, thereby violating the assumption that data
points are independently distributed and sampled
at random. More importantly, significance across
data points only says something about the likely-
hood of observing the same effect on more data
sampled the same way, but says nothing about
likely performance on sentences sampled from
different sources or different domains.
This paper considers the POS tagging problem,
i.e. where we have training and test data consist-
ing of sentences in which all words are assigned
a label y chosen from a finite set of class labels
{NOUN, VERB, DET,. . . }. We assume that we
are interested in performance across data sets or
domains rather than just performance across data
points, but that we do not know the target domain
in advance. This is often the case when we develop
NLP tools and on-line services. We will do cross-
domain experiments using several target domains
in order to compute significance across domains,
enabling us to say something about likely perfor-
mance on new domains.
Several authors have noted how POS tagging
performance is sensitive to cross-domain shifts
(Blitzer et al, 2006; Daume III, 2007; Jiang and
Zhai, 2007), and while most authors have as-
sumed known target distributions and pool unla-
beled target data in order to automatically correct
cross-domain bias (Jiang and Zhai, 2007; Fos-
ter et al, 2010), methods such as feature bag-
ging (Sutton et al, 2006), learning with random
adversaries (Globerson and Roweis, 2006) and
L?-regularization (Dekel and Shamir, 2008) have
been proposed to improve performance on un-
known target distributions. These methods ex-
plicitly or implicitly try to minimize average or
worst-case expected error across a set of possi-
ble test distributions in various ways. These al-
gorithms are related because of the intimate rela-
tionship between adversarial corruption and reg-
ularization (Ghaoui and Lebret, 1997; Xu et al,
640
2009; Hinton et al, 2012). This paper presents a
new method based on learning with antagonistic
adversaries.
Outline. Section 2 introduces previous work on
robust perceptron learning, as well as the meth-
ods dicussed in the paper. Section 3 motivates
and introduces learning with antagonistic adver-
saries. Section 4 presents experiments on POS tag-
ging and discusses how to evaluate cross-domain
performance. Learning with antagonistic adver-
saries is superior to the other approaches across
10/12 datasets with an average error reduction of
4% over a supervised baseline.
Motivating example. The problem with
out-of-vocabulary effects can be illus-
trated using a small labeled data set:
{x1 = ?1, ?0, 1, 0??,x2 = ?1, ?0, 1, 1??,x3 =
?0, ?0, 0, 0??,x4 = ?1, ?0, 0, 1??}. Say we train
our model on x1?3 and evaluate it on the fourth
data point. Most discriminate learning algorithms
only update parameters when training examples
are misclassified. In this example, a model
initialized by zero weights would misclassify x1,
update the parameter associated with feature x2
at a fixed rate ?, and the returned model would
then classify all data points correctly. Hence
the parameter associated with feature x3 would
never be updated, although this feature is also
correlated with class. If x2 is missing in our test
data (out-of-vocabulary), we end up classifying
all data points as negative. In this case, we would
wrongly predict that x4 is negative.
2 Robust perceptron learning
Our framework will be averaged perceptron learn-
ing (Freund and Schapire, 1999; Collins, 2002).
We use an additive update algorithm and aver-
age parameters to prevent over-fitting. In adver-
sarial learning, adversaries corrupt the data point
by applying transformations to data points. An-
tagonistic adversaries choose transformations in-
formed by the current model parameters w, but
random adversaries randomly select transforma-
tions from a predefined set of possible transforma-
tions, e.g. deletions of at most k features (Glober-
son and Roweis, 2006).
Feature bagging. In feature bagging (Sutton et al,
2006), the data is represented by different bags of
features or different views, and the models learned
using different feature bags are combined by aver-
aging. We can reformulate feature bagging as an
adversarial learning problem. For each pass, the
adversary chooses a deleting transformation cor-
responding to one of the feature bags. In Sut-
ton et al (2006), the feature bags simply divide
the features into two or more representations. In
an online setting feature bagging can be modelled
as a game between a learner and an adversary, in
which (a) the adversary can only choose between
deleting transformations, (b) the adversary cannot
see model parameters when choosing a transfor-
mation, and in which (c) the adversary only moves
in between passes over the data.1
Learning with random adversaries
(LRA). Globerson and Roweis (2006) let an
adversary corrupt labeled data during training
to learn better models of test data with missing
features. They assume that missing features
are randomly distributed and show that the
optimization problem is a second-order cone
program. LRA is an adversarial game in which
the two players are unaware of the other player?s
current move, and in particular, where the ad-
versary does not see model parameters and only
randomly corrupts the data points. Globerson
and Roweis (2006) formulate LRA as a batch
learning problem of minimizing worst case loss
under deleting transformations deleting at most
k features. This is related to regularization in the
following way: If model parameters are chosen
to minimize expected error in the absence of any
k features, we explicitly prevent under-weighting
more than n ? k features, i.e. the model must be
able to classify data well in the absence of any k
features. The sparsest possible model would thus
assign weights to k + 1 parameters.
L?-regularization hedges its bets even more than
adversarial learning by minimizing expected er-
ror with max ||w|| < C. In the online setting,
this corresponds to playing against an adversary
that clips any weight above a certain threshold C,
whether positive or negative (Dekel and Shamir,
2008). In geometric terms the weights are pro-
jected back onto the hyper-cube C. A related
approach, which is not explored in the experi-
ments below, is to regularize linear models toward
weights with low variance (Bergsma et al, 2010).
1Note that the batch version of feature bagging is an in-
stance of group L1 regularization (Jacob et al, 2009; Schmidt
and Murphy, 2010; Martins et al, 2011). Often group regu-
larization is about finding sparser models rather than robust
models. Sparse models can be obtained by grouping corre-
lated features; non-sparse models can be obtained by using
independent, exhaustive views.
641
1: X = {?yi,xi?}Ni=1, ? deletion rate
2: w0 = 0,v = 0, i = 0
3: for k ? K do
4: for n ? N do
5: ?1 ? random.sample(P (1) = 1? ?)
6: ?2 ? ||w|| < ?||w|| + ?||w||
7: ? ? (?1 + ?2)(0,1)
8: if sign(w ? xn ? ?) 6= yn then
9: wi+1 ? update(wi)
10: i? i+ 1
11: end if
12: v? v + wi
13: end for
14: end for
15: return w = v/(N ?K)
Figure 1: Learning with antagonistic adversaries
3 Learning with antagonistic adversaries
The intuition behind learning with antagonistic ad-
versaries is that the adversary should focus on the
most predictive features. In the prediction game,
this would allow the adversary to inflict more dam-
age, corrupting data points by removing good fea-
tures (rather than random ones). If the adversary
focuses on the most predictive features, she is im-
plicitly regularizing the model to obtain a more
equal distribution of weights.
We draw random binary vectors with P (1) =
1 ? ? as in adversarial learning, but deletions are
only effective if ?j = 0 and the weight wj is more
than a standard deviation (?||w||) from the mean
of the current absolute weight distribution (?||w||).
In other words, we only delete the predictive fea-
tures, with predictivity being relative to the current
mean weight.
The algorithm is presented in Figure 1. For each
data point, we draw a random binary vector ?1
with ? chance of zeros. ?2 is a vector with the
ith scalar zero if and only if the absolute value of
the weight wi in w is more than a standard devia-
tion higher than the current mean. The ith scalar
in ? is only zero if the ith scalars in both ?1 and ?2
are zero. The corresponding features are a random
subset of the predictive features.2
2The approach taken is similar in spirit to confidence-
weighted learning (Dredze et al, 2008). The intuition behind
confidence-weighted learning is to more agressively update
rare features or features that we are less confident about. In
learning with antagonistic adversaries the adversaries delete
predictive features; that is, features that we are confident
about. When these features are deleted, we do not update
the corresponding weights. In relative terms, we therefore
update rare features more aggressively than common ones.
Note also that by doing so we regularize toward weights with
low variance (Bergsma et al, 2010).
4 Experiments
We consider part-of-speech (POS) tagging, i.e. the
problem of assigning syntactic categories to word
tokens in running text. POS tagging accuracy is
known to be very sensitive to domain shifts. Fos-
ter et al (2011) report a POS tagging accuracy on
social media data of 84% using a tagger that ac-
chieves an accuracy of about 97% on newspaper
data. In the case of social media data, many errors
occur due to different spelling and capitalization
conventions. The main source of error, though, is
the increased out-of-vocabulary rate, i.e. the many
unknown words. While POS taggers can often re-
cover the part of speech of a previously unseen
word from the context it occurs in, this is harder
than for previously seen words.
We use the LXMLS toolkit3 as our baseline
with the default feature model, but use the PTB
tagset rather than the Google tagset (Petrov et
al., 2011) used by default in the LXMLS toolkit.
We use four groups of datasets. The first group
comes from the English Web Treebank (EWT),4
also used in the Parsing the Web shared task
(Petrov and McDonald, 2012). We train our tag-
ger on Sections 2?21 of the WSJ data in the Penn-
III Treebank (PTB), Ontonotes 4.0 release. The
EWT contains development and test data for five
domains: answers (from Yahoo!), emails (from
the Enron corpus), BBC newsgroups, Amazon re-
views, and weblogs. We use the emails develop-
ment section for development and test on the re-
maining four test sets. We also do experiments
with additional data from PTB. For these experi-
ments we use the 0th even split of the biomedical
section (PTB-biomedical) as development data,
the 9th split and the chemistry section (PTB-
chemistry) as test data, and the remaining biomed-
ical data (splits 1?8) as training data. This data
was also used for developing and testing in the
CoNLL 2007 Shared Task (Nivre et al, 2007).
Our third group of datasets also comes from
Ontonotes 4.0.5 We use the Chinese Ontonotes
(CHO) data, covering five different domains. We
use newswire for training data and randomly sam-
pled broadcasted news for development. Finally
we do experiments with the Danish section of the
Copenhagen Dependency Treebank (CDT). For
CDT we rely on the treebank meta-data and sin-
3https://github.com/gracaninja/lxmls-toolkit
4LDC Catalog No.: LDC2012T13.
5LDC Catalog No.: LDC2011T03.
642
SP Our L? LRA
EWT-answers 86.04 86.06 85.90 86.06
EWT-newsgroups 87.70 87.92 87.78 87.66
EWT-reviews 85.96 86.10 85.80 86.00
EWT-weblogs 87.59 87.89 87.60 87.54
PTB-biomedical 95.05 95.26 95.46 94.43
PTB-chemistry 90.32 90.60 90.56 90.58
CHO-broadcast 78.38 78.42 78.27 78.28
CHO-magazines 78.50 78.57 76.80 78.29
CHO-weblogs 79.64 79.76 79.24 79.37
CDT-law 93.96 95.64 93.91 94.25
CDT-literature 93.93 94.19 94.15 94.15
CDT-magazines 94.95 95.06 94.71 95.04
Wilcoxon p <0.01
macro-av. err.red 4.0 -1.2 -0.2
Table 1: Results (in %).
gle out the newspaper section as training data and
use held-out newspaper data for development.
We observe two characteristics about our
datasets: (a) The class distributions are relatively
stable across domains. For CDT, for example,
we see almost identical distributions of parts of
speech, except literature has more prepositions.
(b) The OOV rate is significantly higher across do-
mains than within domains. This holds even for
the PTB datasets, where the OOV rate is 14.6% on
the biomedical test data, but 43.3% on the chem-
istry test data. These two observations confirm
that cross-domain data is primarily biased by co-
variate shifts.
All learning algorithms do the same number of
passes over each training data set. The number
of iterations was set optimizing baseline system
performance on development data. For EWT and
CHO, we do 10 passes over the data. For PTB,
we do 15 passes over the data, and for CDT, we
do 25 passes over the data. The deletion rate in
adversarial learning was fixed to 0.1% (optimized
on the EWT emails data; not optimized on PTB,
CHO or CDT). In L?-regularization, the parame-
ter C was optimized the same way and set to 20.
Results are averages over five runs.
4.1 Results
The results are presented in Table 1. Learn-
ing with antagonistic adversaries performs signifi-
cantly better than structured perceptron (SP) learn-
ing, L?-regularization and LRA across the board.
We follow Demsar (2006) in computing signif-
icance across datasets using a Wilcoxon signed
rank test. This is a strong result given that our al-
gorithm is as computationally efficient as SP and
does not pool unlabeled data to adapt to a spe-
cific target distribution. What we see is that let-
ting an antagonistic adversary corrupt our labeled
data - somewhat surprisingly, maybe - leads to bet-
ter cross-domain performance. L?-regularization
leads to worse performance, and LRA performs
very similar to SP on average. Improvements
to LRA have also been explored in Trafalis and
Gilbert (2007) and Dekel and Shamir (2008).
We note that on the in-domain dataset (PTB-
biomedical), L?-regularization performs best, but
our approach also performs better than the struc-
tured perceptron baseline on this dataset.
4.2 Analysis
The number of zero weights or very small weights
is significantly lower for learning with antagonis-
tic adversaries than for the baseline structured per-
ceptron. So our models become less sparse. On
the other hand, we have more parameters with av-
erage weights in our models. Weights are in other
words better distributed. We also observe that pa-
rameters are updated slightly more with antago-
nistic adversaries. In our PTB experiments, for
example, the mean weight is 14.2 in structured
perceptron learning, but 14.5 with antagonistic ad-
versaries. On the other hand, weight variance is
slightly lower; recall the connection to variance
regularization (Bergsma et al, 2010). Note that
L?-regularization with C = 20 corresponds to
clipping all weights above 20, i.e. roughly a third
of the weights in this case. To validate our intu-
itions about what is going on, we also tried to in-
crease the deletion rate. If ? is increased to 1%,
the mean weight goes up to 19.2. The adversarial
model is less sparse than the baseline model.
A last observation is that the structured percep-
tron baseline model expectedly fits the training
data better than the robust models. On CDT, the
structured perceptron has an accuracy of 98.26%
on held-out training data, whereas our model has
an accuracy of only 97.85%. The L?-regularized
has an accuracy of 97.82%, whereas LRA has an
accuracy of 98.18%.
5 Conclusion
We presented a discriminative learning algorithms
for cross-domain structured prediction that seems
more robust to covariate shifts than previous ap-
proaches. Our approach was superior to previous
approaches across 12 multilingual cross-domain
POS tagging datasets, with an average error reduc-
tion of 4% over a structured perceptron baseline.
643
References
Shane Bergsma, Dekang Lin, and Dale Schuurmans.
2010. Improved natural language learning via
variance-regularization support vector machines. In
CoNLL.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models. In EMNLP.
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In ACL.
Ofer Dekel and Ohad Shamir. 2008. Learning to clas-
sify with missing and corrupted features. In ICML.
Janez Demsar. 2006. Statistical comparisons of clas-
sifiers over multiple data sets. Journal of Machine
Learning Research, 7:1?30.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. In
ICML.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In EMNLP.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Josef Le Roux, Joakim Nivre, Deirde Hogan, and
Josef van Genabith. 2011. From news to comments:
Resources and benchmarks for parsing the language
of Web 2.0. In IJCNLP.
Yoav Freund and Robert Schapire. 1999. Large margin
classification using the perceptron algorithm. Ma-
chine Learning, 37:277?296.
Laurent El Ghaoui and Herve Lebret. 1997. Robust
solutions to least-squares problems with uncertain
data. In SIAM Journal of Matrix Analysis and Ap-
plications.
Amir Globerson and Sam Roweis. 2006. Nightmare
at test time: robust learning by feature deletion. In
ICML.
Geoffrey Hinton, N. Srivastava, A. Krizhevsky,
I. Sutskever, and R. Salakhutdinov. 2012. Improv-
ing neural networks by preventing co-adaptation of
feature detectors. http://arxiv.org/abs/1207.0580.
Laurent Jacob, Guillaume Obozinski, and Jean-
Philippe Vert. 2009. Group lasso with overlap and
graph lasso. In ICML.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In ACL.
Andre Martins, Noah Smith, Pedro Aguiar, and Mario
Figueiredo. 2011. Structured sparsity in structured
prediction. In EMNLP.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task on De-
pendency Parsing. In EMNLP-CoNLL.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 Shared Task on Parsing the Web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL).
Slav Petrov, Dipanjan Das, and Ryan McDonald.
2011. A universal part-of-speech tagset. CoRR
abs/1104.2086.
Mark Schmidt and Kevin Murphy. 2010. Convex
structure learning in log-linear models: beyond pair-
wise potentials. In AISTATS.
Charles Sutton, Michael Sindelar, and Andrew McCal-
lum. 2006. Reducing weight undertraining in struc-
tured discriminative learning. In NAACL.
T Trafalis and R Gilbert. 2007. Robust support vec-
tor machines for classification and computational is-
sues. Optimization Methods and Software, 22:187?
198.
Huan Xu, Constantine Caramanis, and Shie Mannor.
2009. Robustness and regularization of support vec-
tor machines. In JMLR.
644
Proceedings of the ACL Student Research Workshop, pages 142?149,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Simple, readable sub-sentences
Sigrid Klerke
Centre for Language Technology
University of Copenhagen
sigridklerke@gmail.com
Anders S?gaard
Centre for Language Technology
University of Copenhagen
soegaard@hum.ku.dk
Abstract
We present experiments using a new unsu-
pervised approach to automatic text sim-
plification, which builds on sampling and
ranking via a loss function informed by
readability research. The main idea is
that a loss function can distinguish good
simplification candidates among randomly
sampled sub-sentences of the input sen-
tence. Our approach is rated as equally
grammatical and beginner reader appro-
priate as a supervised SMT-based baseline
system by native speakers, but our setup
performs more radical changes that better
resembles the variation observed in human
generated simplifications.
1 Introduction
As a field of research in NLP, text simplification
(TS) has gained increasing attention recently, pri-
marily for English text, but also for Brazilian Por-
tuguese (Specia, 2010; Alu?sio et al, 2008), Dutch
(Daelemans et al, 2004), Spanish (Drndarevic
and Saggion, 2012), Danish (Klerke and S?gaard,
2012), French (Seretan, 2012) and Swedish (Ry-
bing and Smith, 2009; Decker, 2003). Our experi-
ments use Danish text which is similar to English
in that it has a deep orthography making it hard
to map between letters and sounds. Danish has a
relatively free word order and sparse morfology.
TS can help readers with below average reading
skills access information and may supply relevant
training material, which is crucial for developing
reading skills. However, manual TS is as expen-
sive as translation, which is a key limiting factor
on the availability of easy-to-read material. One of
the persistent chalenges of TS is that different in-
terventions are called for depending on the target
reader population. Automatic TS is an effective
way to counter these limitations.
2 Approach
Definitions of TS typically reflect varying target
reader populations and the methods studied. For
our purposes we define TS to include any oper-
ation on the linguistic structure and content of a
text, intended to produce new text, which
1. has semantic content similar to (a part of) the
original text
2. requires less cognitive effort to decode and
understand by a target reader, compared to
the original text.
Operations on linguistic content may include
deletion, reordering and insertion of content,
paraphrasing concepts, resolving references, etc.,
while typography and layout are excluded as non-
linguistic properties.
We cast the problem of generating a more read-
able sentence from an input as a problem of choos-
ing a reasonable sub-sentence from the words
present in the original. The corpus-example below
illustrates how a simplified sentence can be em-
bedded as scattered parts of a non-simplified sen-
tence. The words in bold are the common parts
which make up almost the entire human generated
simplification and constitutes a suitable simplifi-
cation on its own.
Original : Der er m?lt hvad der bliver betegnet som abnormt store
m?ngder af radioaktivt materiale i havvand n?r det jordsk?lvsramte
atomkraftv?rk i Japan .
What has been termed an abnormally large amount of radioactivity
has been measured in sea water near the nuclear power plant that
was hit by earthquakes in Japan
Simplified : Der er m?lt en stor m?ngde radioaktivt materiale i havet
n?r atom-kraftv?rket Fukushima i Japan .
A large amount of radioactivity has been measured in the sea near
the nuclear power plant Fukushima in Japan
To generate candidate sub-sentences we use a
random deletion procedure in combination with
142
general dependency-based heuristics for conserv-
ing main sentence constituents, and then introduce
a loss-function for choosing between candidates.
Since we avoid relying on a specialized parallel
corpus or a simplification grammar, which can be
expensive to create, the method is especially rel-
evant for under-resourced languages and organi-
zations. Although we limit rewriting to deletions,
the space of possible candidates grows exponen-
tially with the length of the input sentence, pro-
hibiting exhaustive candidate generation, which is
why we chose to sample the deletions randomly.
However, to increase the chance of sampling good
candidates, we restrict the search space under
the assumption that some general patterns apply,
namely, that the main verb and subject should al-
ways be kept, negations should be kept and that if
something is kept that originally had objects, those
objects should also be kept. Another way in which
we restrict the candidate space is by splitting long
sentences. Some clauses are simple to identify
and extract, like relative clauses, and doing so can
dramatically reduce sentence length. Both sim-
ple deletions and extraction of clauses can be ob-
served in professionally simplified text. (Medero,
2011; Klerke, 2012)
The next section positions this research in the
context of related work. Section 4 presents the ex-
perimental setup including generation and evalu-
ation. In Section 5, the results are presented and
discussed and, finally, concluding remarks and fu-
ture perspectives are presented in the last section.
3 Related work
Approaches for automatic TS traditionally focus
on lexical substitution (De Belder and Moens,
2012; Specia et al, 2012; Yatskar et al, 2010), on
identifying re-write rules at sentence level either
manually (Chandrasekar et al, 1996; Carroll et al,
1999; Canning et al, 2000; Siddharthan, 2010;
Siddharthan, 2011; Seretan, 2012) or automati-
cally from parallel corpora (Woodsend and Lap-
ata, 2011; Coster and Kauchak, 2011; Zhu et al,
2010) and possibly learning cues for when to ap-
ply such changes (Petersen and Ostendorf, 2007;
Medero, 2011; Bott et al, 2012).
Chandrasekar et al (1996) propose a structural
approach, which uses syntactic cues to recover rel-
ative clauses and appositives. Sentence level syn-
tactic re-writing has since seen a variety of man-
ually constructed general sentence splitting rules,
designed to operate both on dependencies and
phrase structure trees, and typically including lex-
ical cues (Siddharthan, 2011; Heilman and Smith,
2010; Canning et al, 2000). Similar rules have
been created from direct inspection of simplifica-
tion corpora (Decker, 2003; Seretan, 2012) and
discovered automatically from large scale aligned
corpora (Woodsend and Lapata, 2011; Zhu et al,
2010).
In our experiment we apply few basic sentence
splitting rules as a pre-processing technique be-
fore using an over-generating random deletion ap-
proach.
Carroll et al (1999) perform lexical substitution
from frequency counts and eliminate anaphora by
resolving and replacing the referring expressions
with the entity referred to. Their system further
include compound sentence splitting and rewrit-
ing of passive sentences to active ones (Canning
et al, 2000). Research into lexical simplification
remains an active topic. De Belder and Moens
(2012; Specia et al (2012) are both recent pub-
lications of new resources for evaluating lexical
simplification in English consisting of lists of syn-
onyms ranked by human judges. Another type
of resource is graded word-lists as described in
Brooke et al (2012). Annotator agreement and
comparisons so far shows that it is easy to over-
fit to reflect individual annotator and domain dif-
ferences that are not of relevance to generalized
systems.
In a minimally supervised setup, our TS ap-
proach can be modified to include lexical simpli-
fications as part of the random generation process.
This would require a broad coverage list of words
and simpler synonyms, which could for instance
be extracted from a parallel corpus like the DSim
corpus.
For the majority of research in automatic TS
the question of what constitutes cognitive load is
not discussed. An exception is Siddharthan and
Katsos (2012), who seek to isolate the psycho-
linguistically motivated notions of sentence com-
prehension from sentence acceptability by actually
measuring the effect of TS on cognition on a small
scale.
Readability research is a line of research that is
more directly concerned with the nature of cogni-
tive load in reading building on insights from psy-
cholinguistics. One goal is to develop techniques
and metrics for assessing the readability of unseen
143
text. Such metrics are used as a tool for teachers
and publishers, but existing standard metrics (like
Flesch-Kincaid (Flesch, 1948) and LIX (Bjorns-
son, 1983)) were designed and optimized for easy
manual application to human written text, requir-
ing thehuman reader to assess that the text is
congruent and coherent. More recent methods
promise to be applicable to unassessed text. Lan-
guage modeling in particular has shown to be a
robust and informative component of systems for
assessing text readability (Schwarm and Osten-
dorf, 2005; Vajjala and Meurers, 2012) as it is bet-
ter suited to evaluate grammaticality than standard
metrics. We use language modeling alongside tra-
ditional metrics for selecting good simplification
candidates.
4 Experiments
4.1 Baseline Systems
We used the original input text and the human sim-
plified text from the sentence aligned DSim corpus
which consist of 48k original and manually sim-
plified sentences of Danish news wire text (Klerke
and S?gaard, 2012) as reference in the evaluations.
In addition we trained a statistical machine trans-
lation (SMT) simplification system, in effect trans-
lating from normal news wire text to simplified
news. To train an SMT system, a large resource
of aligned parallel text and a language model of
the target language are needed. We combined the
25 million words Danish Korpus 20001 with the
entire 1.75 million words unaligned DSim cor-
pus (Klerke and S?gaard, 2012) to build the lan-
guage model2. Including both corpora gives bet-
ter coverage and assigns lower average ppl and a
simlar difference in average ppl between the two
sides of a held out part of the DSim corpus com-
pared to using only the simplified part of DSim
for the language model. Following Coster and
Kauchak (2011), we used the phrase-based SMT
Moses (Koehn et al, 2007), with GIZA++ word-
alignment (Och and Ney, 2000) and phrase tables
learned from the sentence aligned portion of the
DSim corpus.
1http://korpus.dsl.dk/korpus2000/
engelsk_hovedside
2The LM was a 5-gram Knesser-Ney smoothed lowercase
model, built using IRSTLM (Federico et al, 2008)
4.2 Experimental setup
Three system variants were set up to generate
simplified output from the original news wire of
the development and test partitions of the DSim
corpus. The texts were dependency-parsed us-
ing Bohnet?s parser (Bohnet, 2010) trained on the
Danish Treebank3 (Kromann, 2003) with default
settings4.
1. Split only performed simple sentence split-
ting.
2. Sample over-generated candidates by sam-
pling the heuristically restricted space of ran-
dom lexical deletions and ranking candidates
with a loss function.
3. Combined is a combination of the two, ap-
plying the sampling procedure of Sample to
the split sentences from Split.
Sentence Splitting We implemented sentence
splitting to extract relative clauses, as marked by
the dependency relation rel, coordinated clauses,
coord, and conjuncts, conj, when at least a verb
and a noun is left in each part of the split. Only
splits resulting in sentences of more than three
words were considered. Where applicable, re-
ferred entities were included in the extracted sen-
tence by using the dependency analysis to extract
the subtree of the former head of the new sen-
tence5. In case of more than one possibility, the
split resulting in the most balanced division of the
sentence was chosen and the rules were re-applied
if a new sentence was still longer than ten tokens.
Structural Heuristics To preserve nodes from
later deletion we applied heuristics using simple
structural cues from the dependency structures.
We favored nodes headed by a subject relation,
subj, and object relations, *obj, and negating
modifiers (the Danish word ikke) under the as-
sumption that these were most likely to be impor-
tant for preserving semantics and generating well-
formed candidates under the sampling procedure
described below. The heuristics were applied both
to trees, acting by preserving entire subtrees and
applied to words, only preserving single tokens.
3http://ilk.uvt.nl/conll/post_task_
data.html
4Performance of the parser on the treebank test set La-
beled attatchment score (LAS) = 85.65 and Unlabeled at-
tatchment score (UAS) = 90.29
5For a formal description see (Klerke, 2012)
144
This serves as a way of avoiding relying heavily
on possibly faulty dependency analyses and also
avoid the risk of insisting on keeping long, com-
plex or superfluous modifiers.
Sampling Candidates for scoring were over-
generated by randomly selecting parts of a (pos-
sibly split) input sentence. Either the selected
nodes with their full sub-tree or the single tokens
from the flat list of tokens were eliminated, unless
they were previously selected for preservation by
a heuristic. Some additional interaction between
heuristics and sampling happened when the dele-
tions were performed on trees: deletion of subtrees
allow non-continuous deletions when the parses
are non-projective, and nodes that were otherwise
selected for keeping may nevertheless be removed
if they are part of a subtree of a node selected for
deletion. After pruning, all nodes that used to have
outgoing obj-relations had the first child node of
these relations restored.
4.3 Scoring
We rank candidates according to a loss function
incorporating both readability score (the lower,
the more readable) and language model perplexity
(the lower, the less perplexing) as described below.
The loss function assigns values to the candidates
such that the best simplification candidate receives
the lowest score.
The loss function is a weighted combination of
three scores: perplexity (PPL), LIX and word-
class distribution (WCD). The PPL scores were
obtained from a 5-gram language model of Dan-
ish6 We used the standard readability metric for
Danish, LIX (Bjornsson, 1983)7. Finally, the
WCD measured the variation in universal pos-
tag-distribution 8 compared to the observed tag-
variation in the entire simplified corpus. For PPL
and LIX we calculated the difference between the
score of the input sentence and the candidate.
Development data was used for tuning the
weights of the loss function. Because the
candidate-generation is free to produce extremely
short candidates, we have to deal with candidates
6The LM was Knesser-Ney smoothed, using the same cor-
pora as the baseline system, without punctuation and built us-
ing SRILM (Stolcke, 2002).
7LIX is similar to the English Flesch-Kincaid grade level
in favoring short sentences with short words. The formula
is LIX = average sentence length + % long words , with
long words being of more than 6 characters. (Anderson,
1983) calculated a conversion from LIX to grade levels.
8suggested by(Petrov et al, 2011)
receiving extremely low scores. Those scores
never arise in the professionally simplified text,
so we eliminate extreme candidates by introduc-
ing filters on all scores. The lower limit was tuned
experimentally and fixed approximately two times
below the average difference observed between
the two parts of the aligned DSim corpus, thus lim-
iting the reduction in PPL and LIX to 60% of the
input?s PPL and LIX. The upper limit was fixed
at the input-level plus 20% to allow more varied
candidates through the filters. The WCD-filter ac-
cepted all candidates with a tag-variance that fell
below the 75-percentile observed variance in the
simplified training part of the DSim corpus. The
resulting loss was calculated as the sum of three
weighted scores.
Below is the loss function we minimized over
the filtered candidates t ? Ts for each input sen-
tence, s. The notation var() denotes the range al-
lowed through a hard filter. Using development
data we set the values of the term weights to
? = 1, ? = 6 and ? = 2.
t? = argmin
t?Ts
loss(s, t)
loss(s, t) = ? ?LIX(s, t)var(LIX(s)) + ?
?PPL(s, t)
var(PPL(s))
+ ??WCD(.75, t)WCD(.75)
If no candidates passed through the filters, the
input sentence was kept.
4.4 Evaluation
Evaluation was performed by a group of proficient
Danish speaking volunteers who received written
instructions and responded anonymously via an
online form. 240 sentences were evaluated: six
versions of each of 40 test set sentences. 48
sentences were evaluated by four judges, and
the remaining by one judge each. The judges
were asked to rate each sentence in terms of
grammaticality and in terms of perceived beginner
reader appropriateness, both on a 5-point scale,
with one signifying very good and five signifying
very bad. The evaluators had to rate six versions
of each sentence: original news wire, a human
simplified version, the baseline system, a split
sentence version (Split), a sampled only version
(Sample), and a version combining the Split and
Sample techniques (Combined). The presentation
was randomized. Below are example outputs
145
for the baseline and the other three automatic
systems:
BL: Der er hvad der bliver betegnet som abnormt store m?ngder
radioaktivt materiale i havvand n?r frygter atomkraftv?rk .
Split : Der er m?lt hvad. Hvad bliver betegnet som abnormt
store m?ngder af radioaktivt materiale i havvand n?r det
jordsk?lvsramte atomkraftv?rk i Japan .
Sample: Der er m?lt hvad der bliver betegnet som store m?ngder
af radioaktivt materiale i havvand japan .
Comb.: Der er m?lt hvad. Hvad bliver betegnet som store m?ngder
af radioaktivt materiale det atomkraftv?rk i japan .
5 Results
The ranking of the systems in terms of begin-
ner reader appropriateness and grammaticality, are
shown in Figure 1. From the test set of the DSim
corpus, 15 news wire texts were arbitrarily se-
lected for evaluation. For these texts we calcu-
lated median LIX and PPL. The results are shown
in Table 1. The sentences for human evaluation
were drawn arbitrarily from this collection. As
expected, the filtering of candidates and the loss
function force the systems Sample and Combined
to choose simplifications with LIX and PPL scores
close to the ones observed in the human simpli-
fied version. Split sentences only reduce LIX as
a result of shorter sentences, however PPL is the
highest, indicating a loss of grammaticality. Most
often this was caused by tagger and parser errors.
The baseline reduces PPL slightly, while LIX is
unchanged. This reflects the importance of the
language model in the SMT system.
In the analyses below, the rating were collapsed
to three levels. For texts ranked by more than
one judge, we calculated agreement as Krippen-
dorff?s ?. The results are shown in Table 2. In
addition to sentence-wise agreement, the system-
wise evaluation agreement was calculated as all
judges were evaluating the same 6 systems 8 times
each. We calculated ? of the most frequent score
(mode) assigned by each judge to each system.
As shown in Table 2 this system score agreement
was only about half of the single sentence agree-
ment, which reflect a notable instability in output
quality of all computer generated systems. The
same tendency is visible in both histograms in Fig-
ure 1a and 1b. While grammaticality is mostly
agreed upon when the scores are collapsed into
three bins (? = 0.650), proficient speakers do not
agree to the same extent on what constitutes be-
ginner reader appropriate text (? = 0.338). The
average, mean and most frequent assigned ranks
are recorded in Table 3. Significant differences at
p < 0.05 are reported in Table 4.
1 very good
2 3 4 5 very poor
0
10
20
30
40
50
60
70
Beginning reader appropriatenessvotes for each system
OriginalSimplifiedBaselineSplitSampleCombined
(a) Sentence ? Beginner
1 very good
2 3 4 5 very poor
0
10
20
30
40
50
60
70
Grammaticalityvotes for each system
OriginalSimplifiedBaselineSplitSampleCombined
(b) Sentence ? Grammar.
Figure 1: Distribution of all rankings on systems
before collapsing rankings.
Orig. Simpl. Base Split Sample Comb.
PPL 222 174 214 234 164 177
LIX 45 (10) 39 (8) 45 (10) 41(9) 36 (8) 32 (7)
Table 1: LIX and PPL scores for reference texts
and system generated output. Medians are re-
ported, because distributions are very skewed,
which makes the mean a bad estimator of central
tendency. LIX grade levels in parenthesis.
Reflecting the fair agreement on grammatical-
ity, all comparisons come out significant except
the human generated versions that are judged as
equally grammatical and the Combined and Base-
line systems that are indistinguishable in gram-
maticality. Beginner reader appropriateness is sig-
nificantly better in the human simplified version
146
Systems Sentences
Beginner reader 0.168 0.338
Grammaticality 0.354 0.650
Table 2: Krippendorff?s ? agreement for full-text
and sentence evaluation. Agreement on system
ranks was calculated from the most frequent score
per judge per system.
compared to all other versions, and the original
version is significantly better than the Sample and
Split systems. The remaining observed differences
are not significant due to the great variation in
quality as expressed in Figure 1a.
We found that our Combined system produced
sentences that were as grammatical as the base-
line and also frequently judged to be appropriate
for beginner readers. The main source of error
affecting both Combined and Split is faulty sen-
tence splitting as a result of errors in tagging and
parsing. One way to avoid this in future develop-
ment is to propagate several split variants to the
final sampling and scoring. In addition, the sys-
tems Combined and Sample are prone to omitting
important information that is perceived as missing
when compared directly to the original, although
those two systems are the ones that score the clos-
est to the human generated simplifications. As can
be expected in a system operating exclusively at
sentence level, coherence across sentence bound-
aries remains a weak point.
Another important point is that while the base-
line system performs well in the evaluation, this
is likely due to its conservativeness: choosing
simplifications resembling the original input very
closely. This is evident both in our automatic mea-
sures (see Table 1) and from manual inspection.
Our systems Sample and Combine, on the other
hand, have been tuned to perform much more radi-
cal changes and in this respect more closely model
the changes we see in the human simplification.
Combined is thus evaluated to be at level with
the baseline in grammaticality and beginner reader
appropriateness, despite the fact that the baseline
system is supervised.
Conclusion and perspectives
We have shown promising results for simplifica-
tion of Danish sentences. We have also shown
that using restricted over-generation and scoring
can be a feasible way for simplifying text with-
out relying directly on large scale parallel corpora,
Sent. ? Beginner Sent. ? Grammar
x? x? mode x? x? mode
Human Simp. 1.44 1 1 1.29 1 1
Orig. 2.14 1 1 1.32 1 1
Base 2.58 3 1 1.88 2 1
Split 3.31 3 5 2.44 3 3
Sample 3.22 3 5 2.39 3 3
Comb. 2.72 1 1 1.93 2 1
Table 3: Human evaluation. Mean (x?), median (x?)
and most frequent (mode) of assigned ranks by be-
ginner reader appropriateness and grammaticality
as assessed by proficient Danish speakers.
Comb. Sample Split Base Orig.
Human Simp. b, g b, g b, g b, g b
Orig. g b, g b, g g
Base g g
Split g
Sample g
Table 4: Significant differences between systems
in experiment b: Beginner reader appropriate-
ness and g: Grammaticality. Bonferroni-corrected
Mann-Whitney?s U for 15 comparisons, two-tailed
test. A letter indicate significant difference at cor-
rected p < 0.05 level.
which for many languages do not exist. To inte-
grate language modeling and readability metrics in
scoring is a first step towards applying results from
readability research to the simplification frame-
work. Our error analysis showed that many errors
come from pre-processing and thus more robust
NLP-tools for Danish are needed. Future perspec-
tives include combining supervised and unsuper-
vised methods to exploit the radical unsupervised
deletion approach and the knowledge obtainable
from observable structural changes and potential
lexical simplifications. We plan to focus on refin-
ing the reliability of sentence splitting in the pres-
ence of parser errors as well as on developing a
loss function that incorporates more of the insights
from readability research, and to apply machine
learning techniques to the weighting of features.
Specifically we would like to investigate the use-
fulness of discourse features and transition proba-
bilities (Pitler and Nenkova, 2008) for performing
and evaluating full-text simplifications.
Acknowledgements
Thanks to Mirella Lapata and Kristian Woodsend
for their feedback and comments early in the pro-
cess of this work and to the Emnlp@Cph group
and reviewers for their helpful comments.
147
References
S.M. Alu?sio, Lucia Specia, T.A.S. Pardo, E.G.
Maziero, H.M. Caseli, and R.P.M. Fortes. 2008. A
corpus analysis of simple account texts and the pro-
posal of simplification strategies: first steps towards
text simplification systems. In Proceedings of the
26th annual ACM international conference on De-
sign of communication, pages 15?22. ACM.
Jonathan Anderson. 1983. LIX and RIX: Variations on
a little-known readability index. Journal of Reading,
26(6):490?496.
C. H. Bjornsson. 1983. Readability of Newspapers
in 11 Languages. Reading Research Quarterly,
18(4):480?497.
B Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, pages 89?97. Association for
Computational Linguistics.
S. Bott, H. Saggion, and D. Figueroa. 2012. A hy-
brid system for spanish text simplification. In Third
Workshop on Speech and Language Processing for
Assistive Technologies (SLPAT), Montreal, Canada.
Julian Brooke, Vivian Tsang, David Jacob, Fraser
Shein, and Graeme Hirst. 2012. Building Read-
ability Lexicons with Unannotated Corpora. In Pro-
ceedings of the First Workshop on Predicting and
Improving Text Readability for target reader popula-
tions, pages 33?39, Montr{?}al, Canada, June. As-
sociation for Computational Linguistics.
Y. Canning, J. Tait, J. Archibald, and R. Crawley.
2000. Cohesive generation of syntactically simpli-
fied newspaper text. Springer.
John Carroll, G. Minnen, D. Pearce, Yvonne Canning,
S. Devlin, and J. Tait. 1999. Simplifying text
for language-impaired readers. In Proceedings of
EACL, volume 99, pages 269?270. Citeseer.
R. Chandrasekar, Christine Doran, and B Srinivas.
1996. Motivations and methods for text simplifica-
tion. In Proceedings of the 16th conference on Com-
putational linguistics-Volume 2, pages 1041?1044.
Association for Computational Linguistics.
William Coster and David Kauchak. 2011. Simple En-
glish Wikipedia: a new text simplification task. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers-Volume 2, vol-
ume 2, pages 665?669. Association for Computa-
tional Linguistics.
W. Daelemans, A. H?thker, and E.T.K. Sang. 2004.
Automatic sentence simplification for subtitling in
dutch and english. In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation, pages 1045?1048.
A. Davison and R.N. Kantor. 1982. On the failure of
readability formulas to define readable texts: A case
study from adaptations. Reading Research Quar-
terly, pages 187?209.
J. De Belder and M.F. Moens. 2012. A dataset for the
evaluation of lexical simplification. Computational
Linguistics and Intelligent Text Processing, pages
426?437.
Anna Decker. 2003. Towards automatic grammati-
cal simplification of Swedish text. Master?s thesis,
Stockholm University.
Biljana Drndarevic and Horacio Saggion. 2012. To-
wards Automatic Lexical Simplification in Spanish:
An Empirical Study. In Proceedings of the First
Workshop on Predicting and Improving Text Read-
ability for target reader populations, pages 8?16,
Montr{?}al, Canada, June. Association for Compu-
tational Linguistics.
M Federico, N Bertoldi, and M Cettolo. 2008.
IRSTLM: an open source toolkit for handling large
scale language models. In Ninth Annual Conference
of the International Speech Communication Associ-
ation.
Rudolph Flesch. 1948. A new readability yardstick.
Journal of applied psychology, 32(3):221.
Michael Heilman and Noah A Smith. 2010. Extract-
ing simplified statements for factual question gen-
eration. In Proceedings of the Third Workshop on
Question Generation.
Sigrid Klerke and Anders S?gaard. 2012. DSim , a
Danish Parallel Corpus for Text Simplification. In
Proceedings of Language Resources and Evaluation
(LREC 2012), pages 4015?4018.
Sigrid Klerke. 2012. Automatic text simplification in
danish. sampling a restricted space of rewrites to op-
timize readability using lexical substitutions and de-
pendency analyses. Master?s thesis, University of
Copenhagen.
P Koehn, H Hoang, A Birch, C Callison-Burch, M Fed-
erico, N Bertoldi, B Cowan, W Shen, C Moran,
R Zens, and Others. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
M T Kromann. 2003. The Danish Dependency Tree-
bank and the DTAG treebank tool. In Proceedings
of the Second Workshop on Treebanks and Linguistic
Theories (TLT), page 217.
Julie Medero. 2011. Identifying Targets for Syntactic
Simplification. In Proceedings of Speech and Lan-
guage Technology in Education.
148
F.J. Och and H. Ney. 2000. A comparison of alignment
models for statistical machine translation. In Pro-
ceedings of the 18th conference on Computational
linguistics-Volume 2, pages 1086?1090. Association
for Computational Linguistics.
S.E. E Petersen and Mari Ostendorf. 2007. Text sim-
plification for language learners: a corpus analy-
sis. In the Proceedings of the Speech and Language
Technology for Education Workshop, pages 69?72.
Citeseer.
S. Petrov, D. Das, and R. McDonald. 2011. A
universal part-of-speech tagset. Arxiv preprint
ArXiv:1104.2086.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
Jonas Rybing and Christian Smith. 2009. CogFLUX
Grunden till ett automatiskt textf?renklingssystem
f?r svenska. Master?s thesis, Link?pings Univer-
sitet.
Sarah E Schwarm and Mari Ostendorf. 2005. Reading
Level Assessment Using Support Vector Machines
and Statistical Language Models. In Proceedings
of the 43rd Annual Meeting of the ACL, pages 523?
530.
V. Seretan. 2012. Acquisition of syntactic simplifica-
tion rules for french. In Proceedings of Language
Resources and Evaluation (LREC 2012).
Advaith Siddharthan and Napoleon Katsos. 2012.
Offline Sentence Processing Measures for testing
Readability with Users. In Proceedings of the First
Workshop on Predicting and Improving Text Read-
ability for target reader populations, pages 17?24,
Montr{?}al, Canada, June. Association for Compu-
tational Linguistics.
Advaith Siddharthan. 2010. Complex lexico-syntactic
reformulation of sentences using typed dependency
representations. Proceedings of the 6th Interna-
tional Natural Language Generation Conference.
Advaith Siddharthan. 2011. Text Simplification us-
ing Typed Dependencies: A Comparison of the Ro-
bustness of Different Generation Strategies. In Pro-
ceedings of the 13th European Workshop on Natural
Language Generation, pages 2?11.
L. Specia, S.K. Jauhar, and R. Mihalcea. 2012.
Semeval-2012 task 1: English lexical simplification.
In Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012), pages 347?
355.
L. Specia. 2010. Translating from complex to simpli-
fied sentences. In Proceedings of the 9th interna-
tional conference on Computational Processing of
the Portuguese Language, pages 30?39.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the Sev-
enth International Conference on Spoken Language
Processing.
S. Vajjala and D. Meurers. 2012. On improving the
accuracy of readability classification using insights
from second language acquisition. In Proceedings
of the 7th Workshop on Innovative Use of NLP for
Building Educational Applications (BEA7), pages
163?173.
Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to Simplify Sentences with Quasi-Synchronous
Grammar and Integer Programming. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing (2011), pages 409?
420.
Mark Yatskar, Bo Pang, C. Danescu-Niculescu-Mizil,
and Lillian Lee. 2010. For the sake of simplic-
ity: Unsupervised extraction of lexical simplifica-
tions from Wikipedia. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 365?368. Association for
Computational Linguistics.
Zhemin Zhu, Delphine Bernhard, and I. Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of The
23rd International Conference on Computational
Linguistics, pages 1353?1361. Association for Com-
putational Linguistics.
149
Proceedings of the TextGraphs-6 Workshop, pages 60?68,
Portland, Oregon, USA, 19-24 June 2011. c?2011 Association for Computational Linguistics
From ranked words to dependency trees: two-stage unsupervised
non-projective dependency parsing
Anders S?gaard
Center for Language Technology
University of Copenhagen
soegaard@hum.ku.dk
Abstract
Usually unsupervised dependency parsing
tries to optimize the probability of a corpus
by modifying the dependency model that was
presumably used to generate the corpus. In
this article we explore a different view in
which a dependency structure is among other
things a partial order on the nodes in terms of
centrality or saliency. Under this assumption
we model the partial order directly and derive
dependency trees from this order. The result is
an approach to unsupervised dependency pars-
ing that is very different from standard ones in
that it requires no training data. Each sentence
induces a model from which the parse is read
off. Our approach is evaluated on data from 12
different languages. Two scenarios are consid-
ered: a scenario in which information about
part-of-speech is available, and a scenario in
which parsing relies only on word forms and
distributional clusters. Our approach is com-
petitive to state-of-the-art in both scenarios.
1 Introduction
Unsupervised dependency parsers do not achieve
the same quality as supervised or semi-supervised
parsers, but in some situations precision may be less
important compared to the cost of producing manu-
ally annotated data. Moreover, unsupervised depen-
dency parsing is attractive from a theoretical point
of view as it does not rely on a particular style of an-
notation and may potentially provide insights about
the difficulties of human language learning.
Unsupervised dependency parsing has seen rapid
progress recently, with error reductions on English
(Marcus et al, 1993) of about 15% in six years
(Klein and Manning, 2004; Spitkovsky et al, 2010),
and better and better results for other languages
(Gillenwater et al, 2010; Naseem et al, 2010),
but results are still far from what can be achieved
with small seeds, language-specific rules (Druck et
al., 2009) or using cross-language adaptation (Smith
and Eisner, 2009; Spreyer et al, 2010).
The standard method in unsupervised dependency
parsing is to optimize the overall probability of the
corpus by assigning trees to its sentences that cap-
ture general patterns in the distribution of part-of-
speech (POS). This happens in several iterations
over the corpus. This method requires clever initial-
ization, which can be seen as a kind of minimal su-
pervision. State-of-the-art unsupervised dependency
parsers, except Seginer (2007), also rely on manu-
ally annotated text or text processed by supervised
POS taggers. Since there is an intimate relationship
between POS tagging and dependency parsing, the
POS tags can also be seen as a seed or as partial an-
notation. Inducing a model from the corpus is typi-
cally a very slow process.
This paper presents a new and very different ap-
proach to unsupervised dependency parsing. The
parser does not induce a model from a big corpus,
but with a few exceptions only considers the sen-
tence in question. It does use a larger corpus to
induce distributional clusters and a ranking of key
words in terms of frequency and centrality, but this
is computationally efficient and is only indirectly re-
lated to the subsequent assignment of dependency
structures to sentences. The obvious advantage of
not relying on training data is that we do not have to
60
worry about whether the test data reflects the same
distribution as the target data (domain adaptation),
and since our models are much smaller, parsing will
be very fast.
The parser assigns a dependency structure to a se-
quence of words in two stages. It first decorates
the n nodes of what will become our dependency
structure with word forms and distributional clus-
ters, constructs a directed acyclic graph from the
nodes in O(n2), and ranks the nodes using iterative
graph-based ranking (Page and Brin, 1998). Subse-
quently, it constructs a tree from the ranked list of
words using a simple O(n log n) parsing algorithm.
Our parser is evaluated on the selection of 12
dependency treebanks also used in Gillenwater et
al. (2010). We consider two cases: parsing raw text
and parsing text with information about POS.
Strictly unsupervised dependency parsing is of
course a more difficult problem than unsupervised
dependency parsing of manually annotated POS se-
quences. Nevertheless our strictly unsupervised
parser, which only sees word forms, performs signif-
icantly better than structural baselines, and it outper-
forms the standard POS-informed DMV-EM model
(Klein and Manning, 2004) on 3/12 languages. The
full parser, which sees manually annotated text, is
competitive to state-of-the-art models such as E-
DMV PR AS 140 (Gillenwater et al, 2010).1
1.1 Preliminaries
The observed variables in unsupervised dependency
parsing are a corpus of sentences s = s1, . . . , sn
where each word wj in si is associated with a POS
tag pj . The hidden variables are dependency struc-
tures t = t1, . . . , tn where si labels the vertices of
ti. Each vertex has a single incoming edge, possibly
except one called the root of the tree. In this work
and in most other work in dependency parsing, we
introduce an artificial root node so that all vertices
decorated by word forms have an incoming edge.
A dependency structure such as the one in Fig-
ure 1 is thus a tree decorated with labels and aug-
mented with a linear order on the nodes. Each edge
(i, j) is referred to as a dependency between a head
word wi and a dependent word wj and sometimes
1Naseem et al (2010) obtain slightly better results, but only
evaluate on six languages. They made their code public, though:
http://groups.csail.mit.edu/rbg/code/dependency/
written wi ? wj . Let w0 be the artificial root of
the dependency structure. We use?+ to denote the
transitive closure on the set of edges. Both nodes
and edges are typically labeled. Since a dependency
structure is a tree, it satisfies the following three
constraints: A dependency structure over a sentence
s : w1, . . . , wn is connected, i.e.:
?wi ? s.w0 ?+ wi
A dependency structure is also acyclic, i.e.:
??wi ? s.wi ?+ wi
Finally, a dependency structure is single-headed,
i.e.:
?wi.?wj.(w0 ? wi ? w0 ? wj)? wi = wj
If we also require that each vertex other than the
artificial root node has an incoming edge we have a
complete characterization of dependency structures.
In sum, a dependency structure is a tree with a lin-
ear order on the leaves where the root of the tree
for practical reasons is attached to an artificial root
node. The artificial root node makes it easier to im-
plement parsing algorithms.
Finally, we define projectivity, i.e. whether the
linear order is projective wrt. the dependency tree,
as the property of dependency trees that if wi ? wj
it also holds that all words in between wi and wj
are dominated by wi, i.e. wi ?+ wk. Intuitively,
a projective dependency structure contains no cross-
ing edges. Projectivity is not a necessary property
of dependency structures. Some dependency struc-
tures are projective, others are not. Most if not
all previous work in unsupervised dependency pars-
ing has focused on projective dependency parsing,
building on work in context-free parsing, but our
parser is guaranteed to produce well-formed non-
projective dependency trees. Non-projective pars-
ing algorithms for supervised dependency parsing
have, for example, been presented in McDonald et
al. (2005) and Nivre (2009).
1.2 Related work
Dependency Model with Valence (DMV) by Klein
and Manning (2004) was the first unsupervised de-
pendency parser to achieve an accuracy for manually
61
POS-tagged English above a right-branching base-
line.
DMV is a generative model in which the sentence
root is generated and then each head recursively gen-
erates its left and right dependents. For each si ? s,
ti is assumed to have been built the following way:
The arguments of a head h in direction d are gen-
erated one after another with the probability that no
more arguments of h should be generated in direc-
tion d conditioned on h, d and whether this would be
the first argument of h in direction d. The POS tag of
the argument of h is generated given h and d. Klein
and Manning (2004) use expectation maximization
(EM) to estimate probabilities with manually tuned
linguistically-biased priors.
Smith and Eisner (2005) use contrastive es-
timation instead of EM, while Smith and Eis-
ner (2006) use structural annealing which penal-
izes long-distance dependencies initially, gradually
weakening the penalty during training. Cohen et
al. (2008) use Bayesian priors (Dirichlet and Logis-
tic Normal) with DMV. All of the above approaches
to unsupervised dependency parsing build on the
linguistically-biased priors introduced by Klein and
Manning (2004).
In a similar way Gillenwater et al (2010) try
to penalize models with a large number of dis-
tinct dependency types by using sparse posteriors.
They evaluate their system on 11 treebanks from the
CoNLL 2006 Shared Task and the Penn-III treebank
and achieve state-of-the-art performance.
An exception to using linguistically-biased priors
is Spitkovsky et al (2009) who use predictions on
sentences of length n to initialize search on sen-
tences of length n+ 1. In other words, their method
requires no manual tuning and bootstraps itself on
increasingly longer sentences.
A very different, but interesting, approach is taken
in Brody (2010) who use methods from unsuper-
vised word alignment for unsupervised dependency
parsing. In particular, he sees dependency parsing
as directional alignment from a sentence (possible
dependents) to itself (possible heads) with the mod-
ification that words cannot align to themselves; fol-
lowing Klein and Manning (2004) and the subse-
quent papers mentioned above, Brody (2010) con-
siders sequences of POS tags rather than raw text.
Results are below state-of-the-art, but in some cases
better than the DMV model.
2 Ranking dependency tree nodes
The main intuition behind our approach to unsuper-
vised dependency parsing is that the nodes near the
root in a dependency structure are in some sense the
most important ones. Semantically, the nodes near
the root typically express the main predicate and
its arguments. Iterative graph-based ranking (Page
and Brin, 1998) was first used to rank webpages
according to their centrality, but the technique has
found wide application in natural language process-
ing. Variations of the algorithm presented in Page
and Brin (1998) have been used in keyword extrac-
tion and extractive summarization (Mihalcea and Ta-
rau, 2004), word sense disambiguation (Agirre and
Soroa, 2009), and abstractive summarization (Gane-
san et al, 2010). In this paper, we use it as the first
step in a two-step unsupervised dependency parsing
procedure.
The parser assigns a dependency structure to a se-
quence of words in two stages. It first decorates
the n nodes of what will become our dependency
structure with word forms and distributional clus-
ters, constructs a directed acyclic graph from the
nodes in O(n2), and ranks the nodes using iterative
graph-based ranking. Subsequently, it constructs
a tree from the ranked list of words using a sim-
ple O(n log n) parsing algorithm. This section de-
scribes the graph construction step in some detail
and briefly describes the iterative graph-based rank-
ing algorithm used.
The first step, however, is assigning distributional
clusters to the words in the sentence. We use a hi-
erarchical clustering algorithm to induce 500 clus-
ters from the treebanks using publicly available soft-
ware.2 This procedure is quadratic in the number of
clusters, but linear in the size of the corpus. The
cluster names are bitvectors (see Figure 1).
2.1 Edges
The text graph is now constructed by adding dif-
ferent kinds of directed edges between nodes. The
edges are not weighted, but multiple edges between
nodes will make transitions between these nodes in
2http://www.cs.berkeley.edu/?pliang/software/brown-
cluster-1.2.zip
62
iterative graph-based ranking more likely. The dif-
ferent kinds of edges play the same role in our model
as the rule templates in the DMV model, and they
are motivated below.
Some of the edge assignments discussed below
may seem rather heuristic. The edge template was
developed on development data from the English
Penn-III treebank (Marcus et al, 1993). Our edge
selection was incremental considering first an ex-
tended set of candidate edges with arbitrary param-
eters and then considering each edge type at a time.
If the edge type was helpful, we optimized any pos-
sible parameters (say context windows) and went on
to the next edge type: otherwise we disregarded it.3
Following data set et al (2010), we apply the best
setting for English to all other languages.
Vine edges. Eisner and Smith (2005) motivate a vine
parsing approach to supervised dependency parsing
arguing that language users have a strong prefer-
ence for short dependencies. Reflecting preference
for short dependencies, we first add links between
all words and their neighbors and neighbors? neigh-
bors. This also guarantees that the final graph is con-
nected.
Keywords and closed class words. We use a key-
word extraction algorithm without stop word lists to
extract non-content words and the most important
content words, typically nouns. The algorithm is a
crude simplification of TextRank (Mihalcea and Ta-
rau, 2004) that does not rely on linguistic resources,
so that we can easily apply it to low-resource lan-
guages. Since we do not use stop word lists, highly
ranked words will typically be non-content words,
followed by what is more commonly thought of as
keywords. Immediate neighbors to top-100 words
are linked to these words. The idea is that non-
content words may take neighboring words as ar-
guments, but dependencies are typically very local.
The genuine keywords, ranked 100?1000, may be
heads of dependents further away, and we therefore
add edges between these words wi and their neigh-
boring words wj if |i? j| ? 4.
Head-initial/head-final. It is standard in unsuper-
vised dependency parsing to compare against a
3The search was simplified considerably. For example, we
only considered symmetric context windows, where left context
length equals length of right context, and we binned this length
considering only values 1, 2, 4, 8 and all.
structural baseline; either left-attach, i.e. all words
attach to their left neighbor, or right-attach. Which
structural baseline is used depends on the language
in question. It is thus assumed that we know enough
about the language to know what structural baseline
performs best. It is therefore safe to incorporate this
knowledge in our unsupervised parsers; our parsers
are still as ?unsupervised? as our baselines. If a lan-
guage has a strong left-attach baseline, like Bulgar-
ian, the first word in the sentence is likely to be very
central for reasons of economy of processing. The
language is likely to be head-initial. On the other
hand, if a language has a strong right-attach base-
line, like Turkish, the last word is likely to be cen-
tral. The language is likely to be head-final. Some
languages like Slovene have strong (< 20%) left-
attach and right-attach baselines, however. We in-
corporate the knowledge that a language has a strong
left-attach or right-attach baseline if more than one
third of the dependencies are attachments to a im-
mediate left, resp. right, neighbor. Specifically, we
add edges from all nodes to the first element in the
sentence if a language has a strong left-attach base-
line; and from all edges to the last (non-punctuation)
element in the sentence if a language has a strong
right-attach baseline.
Word inequality. An edge is added between two
words if they have different word forms. It is not
very likely that a dependent and a head have the
same word form.
Cluster equality. An edge is added between two
words if they are neighbors or neighbors? neighbors
and belong to the same clusters. If so, the two words
may be conjoined.
Morphological inequality. If two words wi, wj in
the same context (|i ? j| ? 4) share prefix or suf-
fix, i.e. the first or last three letters, we add an edge
between them.
2.2 Edges using POS
Verb edges. All words are attached to all words with
a POS tag beginning with ?V. . . ?.
Finally, when we have access to POS information,
we do not rely on vine edges besides left-attach, and
we do not rely on keyword edges or suffix edges ei-
ther.
63
2.3 Ranking
Given the constructed graph we rank the nodes using
the algorithm in Page and Brin (1998), also known
as PageRank. The input to this algorithm is any di-
rected graph G = ?E,V ? and the output is an as-
signment PR : V ? R of a score, also referred to
as PageRank, to each vertex in the graph such that all
scores sum to 1. A simplified version of PageRank
can be defined recursively as:
PR(v) = ?w?Bv
PR(w)
L(w)
where Bv is the set of vertices such that (w, v) ?
E, and L(w) is the number of outgoing links from
w, i.e. |{(u, u?)|(u, u?) ? E, u = w}|. In addi-
tion to this, Page and Brin (1998) introduces a so-
called damping factor to reflect that fact that Internet
users do not continue crawling web sites forever, but
restart, returning to random web sites. This influ-
ences centrality judgments and therefore should be
reflected in the probability assignment. Since there
is no obvious analogue of this in our case, we sim-
plify the PageRank algorithm and do not incorpo-
rate damping (or equivalent, set the damping factor
to 1.0).
Note, by the way, that although our graphs are
non-weighted and directed, like a graph of web
pages and hyperlinks (and unlike the text graphs in
Mihalcea and Tarau (2004), for example), several
pairs of nodes may be connected by multiple edges,
making a transition between them more probable.
Multiple edges provide a coarse weighting of the un-
derlying minimal graph.
2.4 Example
In Figure 1, we see an example graph of word nodes,
represented as a matrix, and a derived dependency
structure.4 We see that there are four edges from
The to market and six from The to crumbles, for
example. We then compute the PageRank of each
node using the algorithm described in Page and
Brin (1998); see also Figure 1. The PageRank val-
ues rank the nodes or the words. In Sect. 3, we de-
scribe a method for building a dependency tree from
4The dependency structure in Figure 1 contains dependency
labels such as ?SBJ? and ?ROOT?. These are just included for
readability. We follow the literature on unsupervised depen-
dency parsing and focus only on unlabeled dependency parsing.
from/to The market crumbled .
The 0 4 6 3
market 4 0 5 3
crumbled 4 4 0 4
. 3 4 6 0
PR(%) 22.8 24.1 30.3 22.7
Figure 1: Graph, pagerank (PR) and predicted depen-
dency structure for sentence 5, PTB-III Sect. 23.
a ranking of the nodes. This method will produce
the correct analysis of this sentence; see Figure 1.
This is because the PageRank scores reflect syntac-
tic superiority; the root of the sentence typically has
the highest rank, and the least important nodes are
ranked lowly.
3 From ranking of nodes to dependency
trees
Consider the example in Figure 1 again. Once we
have ranked the nodes in our dependency structure,
we build a dependency structure from it using the
parsing algorithm in Figure 2. The input of the
graph is a list of ranked words pi = ?n1, . . . , nm?,
where each node ni corresponds to a sentence posi-
tion npr2ind(i) decorated by a word form wpr2ind(i),
where pr2ind : {1, . . . ,m} ? {1, . . . ,m} is a
mapping from rank to sentence position.
The interesting step in the algorithm is the head
selection step. Each word is assigned a head taken
from all the previously used heads and the word to
which a head was just assigned. Of these words,
we simply select the closest head. If two possible
heads are equally close, we select the one with high-
est PageRank.
Our parsing algorithm runs in O(n log n), since
it runs over the ranked words in a single pass con-
sidering only previously stored words as possible
heads, and guarantees connectivity, acyclicity and
single-headedness, and thus produces well-formed
non-projective dependency trees. To see this, re-
member that wellformed dependency trees are such
that all nodes but the artificial root nodes have a sin-
gle incoming edge. This follows immediately from
64
1: pi = ?n1, . . . , nm? # the ranking of nodes
2: H = ?n0? # possible heads
3: D = ? # dependency structure
4: pr2ind : {1, . . . ,m} ? {1, . . . ,m} # a mapping from rank to sentence position
5: for ni ? pi do
6: if |H|=1 then
7: c = 0 # used to ensure single-headedness
8: else
9: c = 1
10: end if
11: nj? = argminnj?H[c:] |pr2ind(i)? pr2ind(j)| # select head of wj
12: H = ni ?H # make ni a possible head
13: D = {(wpr2ind (i) ? wpr2ind(j?))} ?D # add new edge to D
14: end for
15: return D
Figure 2: Parsing algorithm.
the fact that each node is assigned a head (line 11).
Furthermore, the dependency tree must be acyclic.
This follows immediately from the fact that a word
can only attach to a word with higher rank than it-
self. Connectivity follows from the fact that there
is an artificial root node and that all words attach to
this node or to nodes dominated by the root node.
Finally, we ensure single-headedness by explicitly
disregarding the root node once we have attached the
node with highest rank to it (line 6?7). Our parsing
algorithm does not guarantee projectivity, since the
iterative graph-based ranking of nodes can permute
the nodes in any order.
4 Experiments
We use exactly the same experimental set-up as
Gillenwater et al (2010). The edge model was
developed on development data from the English
Penn-III treebank (Marcus et al, 1993), and we eval-
uate on Sect. 23 of the English treebanks and the test
sections of the remaining 11 treebanks, which were
all used in the CoNLL-X Shared Task (Buchholz and
Marsi, 2006). Gillenwater et al (2010) for some
reason did not evaluate on the Arabic and Chinese
treebanks also used in the shared task. We also fol-
low Gillenwater et al (2010) in only evaluating our
parser on sentences of at most 10 non-punctuation
words and in reporting unlabeled attachment scores
excluding punctuation.
4.1 Strictly unsupervised dependency parsing
We first evaluate the strictly unsupervised parsing
model that has no access to POS information. Since
we are not aware of other work in strictly unsuper-
vised multi-lingual dependency parsing, so we com-
pare against the best structural baseline (left-attach
or right-attach) and the standard DMV-EM model
of Klein and Manning (2004). The latter, however,
has access to POS information and should not be
thought of as a baseline. Results are presented in
Figure 3.
It is interesting that we actually outperform DMV-
EM on some languages. On average our scores are
significantly better (p < 0.01) than the best struc-
tural baselines (3.8%), but DMV-EM with POS tags
is still 3.0% better than our strictly unsupervised
model. For English, our system performs a lot worse
than Seginer (2007).
4.2 Unsupervised dependency parsing
(standard)
We then evaluate our unsupervised dependency
parser in the more standard scenario of parsing sen-
tences annotated with POS. We now compare our-
selves to two state-of-the-art models, namely DMV
PR-AS 140 and E-DMV PR-AS 140 (Gillenwater et
al., 2010). Finally, we also report results of the IBM
model 3 proposed by Brody (2010) for unsupervised
dependency parsing, since this is the only recent pro-
65
baseline EM ours
Bulgarian 37.7 37.8 41.9
Czech 32.5 29.6 28.7
Danish 43.7 47.2 43.7
Dutch 38.7 37.1 33.1
English 33.9 45.8 36.1
German 27.2 35.7 36.9
Japanese 44.7 52.8 56.5
Portuguese 35.5 35.7 35.2
Slovene 25.5 42.3 30.0
Spanish 27.0 45.8 38.4
Swedish 30.6 39.4 34.5
Turkish 36.6 46.8 45.9
AV 34.5 41.3 38.3
Figure 3: Unlabeled attachment scores (in %) on raw text.
(EM baseline has access to POS.)
posal we are aware of that departs significantly from
the DMV model. The results are presented in Fig-
ure 4.
Our results are on average significantly better than
DMV PR-AS 140 (2.5%), and better than DMV PR-
AS 140 on 8/12 languages. E-DMV PR-AS 140 is
slightly better than our model on average (1.3%),
but we still obtain better results on 6/12 languages.
Our results are a lot better than IBM-M3. Naseem
et al (2010) report better results than ours on Por-
tuguese, Slovene, Spanish and Swedish, but worse
on Danish.
5 Error analysis
In our error analysis, we focus on the results for
German and Turkish. We first compare the results
of the strictly unsupervised model on German with
the results on German text annotated with POS. The
main difference between the two models is that more
links to verbs are added to the sentence graph prior
to ranking nodes when parsing text annotated with
POS. For this reason, the latter model improves con-
siderably in attaching verbs compared to the strictly
unsupervised model:
acc strict-unsup unsup
NN 43% 48%
NE 41% 39%
VVFIN 31% 100%
VAFIN 9% 86%
VVPP 13% 53%
While the strictly unsupervised model is about as
Figure 5: Predicted dependency structures for sentence 4
in the German test section; strictly unsupervised (above)
and standard (below) approach. Red arcs show wrong
decisions.
good at attaching nouns as the model with POS, it
is much worse attaching verbs. Since more links
to verbs are added, verbs receive higher rank, and
this improves f-scores for attachments to the artifi-
cial root node:
f-score strict-unsup unsup
to root 39.5% 74.0%
1 62.3% 69.6%
2 7.4% 24.4%
3?6 0 22.4%
7 0 0
This is also what helps the model with POS when
parsing the example sentence in Figure 5. The POS-
informed parser also predicts longer dependencies.
The same pattern is observed in the Turkish data,
but perhaps less dramatically so:
acc strict-unsup unsup
Noun 43% 42%
Verb 41% 51%
The increase in accuracy is again higher with
verbs than with nouns, but the error reduction was
higher for German.
f-score strict-unsup unsup
to root 57.4% 90.4%
1 65.7% 69.6%
2 32.1% 26.5%
3?6 11.6% 24.7%
7 0 12.5%
The parsers predict more long dependencies for
Turkish than for German; precision is generally
good, but recall is very low.
6 Conclusion
We have presented a new approach to unsupervised
dependency parsing. The key idea is that a depen-
66
DMV PR-AS 140 E-DMV PR-AS 140 ours IBM-M3
Bulgarian 54.0 59.8 52.5
Czech 32.0 54.6 42.8
Danish 42.4 47.2 55.2 41.9
Dutch 37.9 46.6 49.4 35.3
English 61.9 64.4 50.2 39.3
German 39.6 35.7 50.4
Japanese 60.2 59.4 58.3
Portuguese 47.8 49.5 52.8
Slovene 50.3 51.2 44.1
Spanish 62.4 57.9 52.1
Swedish 38.7 41.4 45.5
Turkish 53.4 56.9 57.9
AV 48.4 52.2 50.9
Figure 4: Unlabeled attachment scores (in %) on text annotated with POS.
dency structure also expresses centrality or saliency,
so by modeling centrality directly, we obtain infor-
mation that we can use to build dependency struc-
tures. Our unsupervised dependency parser thus
works in two stages; it first uses iterative graph-
based ranking to rank words in terms of central-
ity and then constructs a dependency tree from the
ranking. Our parser was shown to be competitive to
state-of-the-art unsupervised dependency parsers.
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In EACL.
Samuel Brody. 2010. It depends on the translation: un-
supervised dependency parsing via word alignment. In
EMNLP.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
CoNLL.
Shay Cohen, Kevin Gimpel, and Noah Smith. 2008. Un-
supervised bayesian parameter estimation for depen-
dency parsing. In NIPS.
Gregory Druck, Gideon Mann, and Andrew McCal-
lum. 2009. Semi-supervised learning of dependency
parsers using generalized expectation criteria. In ACL-
IJCNLP.
Jason Eisner and Noah A. Smith. 2005. Parsing with soft
and hard constraints on dependency length. In IWPT.
K Ganesan, C Zhai, and J Han. 2010. Opinosis: a graph-
based approach to abstractive summarization of highly
redudant opinions. In COLING.
Jennifer Gillenwater, Kuzman Ganchev, Joao Graca, Fer-
nando Pereira, and Ben Taskar. 2010. Sparsity in de-
pendency grammar induction. In ACL.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In ACL.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Lin-
guistics, 19(2):313?330.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency parsing
using spanning tree algorithms. In HLT-EMNLP.
Rada Mihalcea and Paul Tarau. 2004. Textrank: bringing
order into texts. In EMNLP.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowledge
to guide grammar induction. In EMNLP.
Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In ACL-IJCNLP.
Larry Page and Sergey Brin. 1998. The anatomy of a
large-scale hypertextual web search engine. In Inter-
national Web Conference.
Yoav Seginer. 2007. Fast unsupervised incremental pars-
ing. In ACL.
Noah Smith and Jason Eisner. 2005. Contrastive estima-
tion: training log-linear models on unlabeled data. In
ACL.
Noah Smith and Jason Eisner. 2006. Annealing struc-
tural bias in multilingual weighted grammar induction.
In COLING-ACL.
David Smith and Jason Eisner. 2009. Parser adaptation
and projection with quasi-synchronous grammar fea-
tures. In EMNLP.
Valentin Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2009. Baby steps: how ?less is more? in unsu-
pervised dependency parsing. In NIPS Workshop on
Grammar Induction, Representation of Language and
Language Learning.
67
Valentin Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,
and Christopher Manning. 2010. Viterbi training im-
proves unsupervised dependency parsing. In CoNNL.
Kathrin Spreyer, Lilja ?vrelid, and Jonas Kuhn. 2010.
Training parsers on partial trees: a cross-language
comparison. In LREC.
68
Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 29?32,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Shared task system description:
Frustratingly hard compositionality prediction
Anders Johannsen, Hector Martinez Alonso, Christian Rish?j and Anders S?gaard
Center for Language Technology
University of Copenhagen
{ajohannsen|alonso|crjensen|soegaard}@hum.ku.dk
Abstract
We considered a wide range of features for
the DiSCo 2011 shared task about composi-
tionality prediction for word pairs, including
COALS-based endocentricity scores, compo-
sitionality scores based on distributional clus-
ters, statistics about wordnet-induced para-
phrases, hyphenation, and the likelihood of
long translation equivalents in other lan-
guages. Many of the features we considered
correlated significantly with human compo-
sitionality scores, but in support vector re-
gression experiments we obtained the best re-
sults using only COALS-based endocentric-
ity scores. Our system was nevertheless the
best performing system in the shared task, and
average error reductions over a simple base-
line in cross-validation were 13.7% for En-
glish and 50.1% for German.
1 Introduction
The challenge in the DiSCo 2011 shared task is to
estimate and predict the semantic compositionality
of word pairs. Specifically, the data set consists of
adjective-noun, subject-verb and object-verb pairs
in English and German. The organizers also pro-
vided the Wacky corpora for English and German
with lowercased lemmas.1 In addition, we also ex-
perimented with wordnets and using Europarl cor-
pora for the two languages (Koehn, 2005), but none
of the features based on these resources were used
in the final submission.
Semantic compositionality is an ambiguous term
in the linguistics litterature. It may refer to the po-
sition that the meaning of sentences is built from
1http://wacky.sslmit.unibo.it/
the meaning of its parts through very general prin-
ciples of application, as for example in type-logical
grammars. It may also just refer to a typically not
very well defined measure of semantic transparency
of expressions or syntactic constructions, best illus-
trated by examples:
(1) pull the plug
(2) educate people
The verb-object word pair in example (1) is in
the training data rated as much less compositional
than example (2). The intuition is that the mean-
ing of the whole is less related to the meaning of
the parts. The compositionality relation is not de-
fined more precisely, however, and this may in part
explain why compositionality prediction seems frus-
tratingly hard.
2 Features
Many of our features were evaluated with different
amounts of slop. The slop parameter permits non-
exact matches without resorting to language-specific
shallow patterns. The words in the compounds are
allowed to move around in the sentence one position
at a time. The value of the parameter is the maxi-
mum number of steps. Set to zero, it is equivalent
to an exact match. Below are a couple of example
configurations. Note that in order for w1 and w2 to
swap positions, we must have slop > 1 since slop=1
would place them on top of each other.
x x w1 w2 x x (slop=0)
x x w1 x w2 x (slop=1)
x x w1 x x w2 (slop=2)
x x w2 w1 x x (slop=2)
29
2.1 LEFT-ENDOC, RIGHT-ENDOC and
DISTR-DIFF
These features measure the endocentricity of a word
pair w1 w2. The distribution of w1 is likely to be
similar to the distribution of ?w1 w2? if w1 is the
syntactic head of ?w1 w2?. The same is to be ex-
pected for w2, when w2 is the head.
Syntactic endocentricity is related to composi-
tionality, but the implication is one-way only. A
highly compositional compound is endocentric, but
an endocentric compound need not be highly com-
positional. For example, the distribution of ?olive
oil?, which is endocentric and highly compositional,
is very similar to the distribution of ?oil?, the head
word. On the other hand, ?golden age? which is
ranked as highly non-compositional in the training
data, is certainly endocentric. The distribution of
?golden age? is not very different from that of ?age?.
We used COALS (Rohde et al, 2009) to cal-
culate word distributions. The COALS algorithm
builds a word-to-word semantic space from a cor-
pus. We used the implementation by Jurgens and
Stevens (2010), generating the semantic space from
the Wacky corpora for English and German with du-
plicate sentences removed and low-frequency words
substituted by dummy symbols. The word pairs
have been fed to COALS as compounds that have to
be treated as single tokens, and the semantic space
has been generated and reduced using singular value
decompositon. The vectors for w1, w2 and ?w1 w2?
are calculated, and we compute the cosine distance
between the semantic space vectors for the word
pair and its parts, and between the parts themselves,
namely for ?w1 w2? and w1, for ?w1 w2? and w2,
and for w1 and w2, say for ?olive oil? and ?olive?,
for ?olive oil? and ?oil?, and for ?olive? and ?oil?.
LEFT-ENDOC is the cosine distance between the left
word and the compound. RIGHT-ENDOC is the co-
sine distance between the right word and the com-
pound. Finally, DISTR-DIFF is the cosine distance
between the two words, w1 and w2.
2.2 BR-COMP
To accommodate for the weaknesses of syntactic en-
docentricity features, we also tried introducing com-
positionality scores based on hierarchical distribu-
tional clusters that would model semantic composi-
tionality more directly. The scores are referred to
below as BR-COMP (compositionality scores based
on Brown clusters), and the intuition behind these
scores is that a word pair ?w1 w2?, e.g. ?hot dog?, is
non-compositional if w1 and w2 have high colloca-
tional strength, but if w1 is replaced with a different
word w?1 with similar distribution, e.g. ?warm?, then
?w?1 w2? is less collocational. Similarly, if w2 is re-
placed with a different word w?2 with similar distri-
bution, e.g. ?terrier?, then ?w1 w?2? is also much less
collocational than ?w1 w2?.
We first induce a hierarchical clustering of the
words in the Wacky corpora cl : W ? 2W with
W the set of words in our corpora, using publicly
available software.2 Let the collocational strength of
the two words w1 and w2 be G2(w1, w2). We then
compute the average collocational strength of distri-
butional clusters, BR-CS (collocational strength of
Brown clusters):
BR-CS(w1, w2) =
?Nx?cl(w1),x??cl(w2)G
2(x, x?)
N
with N = |cl(w1)| ? |cl(w2)|. We now let
BR-COMP(w1, w2) =
BR-CS(w1,w2)
G2(w1,w2)
.
The Brown clusters were built with C = 1000
and a cut-off frequency of 1000. With these settings
the number of word types per cluster is quite high,
which of course has a detrimental effect on the se-
mantic coherence of the cluster. To counter this we
choose to restrict cl(w) and cl(w?) to include only
the 50 most frequently occurring terms.
2.3 PARAPHR
These features have to do with alternative phrasings
using synonyms from Princeton WordNet 3 and Ger-
maNet4. One word in the compound is held con-
stant while the other is replaced with its synonyms.
The intuition is again that non-compositional com-
pounds are much more frequent than any compound
that results from replacing one of the constituent
words with one of its synonyms. For ?hot dog? we
thus generate ?hot terrier? and ?warm dog?, but not
?warm terrier?. Specifically, PARAPHR?100 means
2http://www.cs.berkeley.edu/?pliang/software/
3http://wordnet.princeton.edu/
4GermaNet Copyright c? 1996, 2008 by University of
Tu?bingen.
30
that at least one of the alternative compounds has
a document count of more than 100 in the cor-
pus. PARAPHRav is the average count for all para-
phrases, PARAPHRsum is the sum of these counts,
and PARAPHRrel is the average count for all para-
phrases over the count of the word pair in question.
2.4 HYPH
The HYPH features were inspired by Bergsma et
al. (2010). It was only used for English. Specif-
ically, we used the relative frequency of hyphen-
ated forms as features. For adjective-noun pairs
we counted the number of hyphenated occurrences,
e.g. ?front-page?, and divided that number by the
number of non-hyphenated occurrences, e.g. ?front
page?. For subject-verb and object-verb pairs, we
add -ing to the verb, e.g. ?information-collecting?,
and divided the number of such forms with non-
hyphenated equivalents, e.g. ?information collect-
ing?.
2.5 TRANS-LEN
The intuition behind our bilingual features is that
non-compositional words typically translate into a
single word or must be paraphrased using multiple
words (circumlocution or periphrasis). TRANS-LEN
is the probability that the phrase?s translation, possi-
bly with intervening articles and markers, is longer
than lmin and shorter than lmax , i.e.:
TRANS-LEN(w1, w2, lmin , lmax ) =
???trans(w1 w2),l1?|? |?l2P (?|w1 w2)
???trans(w1 w2)P (?|w1 w2)
We use English and German Europarl (Koehn,
2005) to train our translation models. In particular,
we use the phrase tables of the Moses PB-SMT sys-
tem5 trained on a lemmatized version of the WMT11
parallel corpora for English and German. Below
TRANS-LEN-n will be the probability of the trans-
lation of a word pair being n or more words. We
also experimented with average translation length as
a feature, but this did not correlate well with seman-
tic compositionality.
5http://statmt.org
feat ?
English German
rel-type = ADJ NN 0.0750 *0.1711
rel-type = V SUBJ 0.0151 **0.2883
rel-type = V OBJ 0.0880 0.0825
LEFT-ENDOC **0.3257 *0.1637
RIGHT-ENDOC **0.3896 0.1379
DISTR-DIFF *0.1885 0.1128
HYPH (5) 0.1367 -
HYPH (5) reversed *0.1829 -
G2 0.1155 0.0535
BR-CS *0.1592 0.0242
BR-COMP 0.0292 0.0024
Count (5) 0.0795 *0.1523
PARAPHR?|w1 w?2| 0.1123 0.1242
PARAPHRrel (5) 0.0906 0.0013
PARAPHRav (1) 0.1080 0.0743
PARAPHRav (5) 0.1313 0.0707
PARAPHRsum (1) 0.0496 0.0225
PARAPHR?100 (1) **0.2434 0.0050
PARAPHR?100 (5) **0.2277 0.0198
TRANS-LEN-1 0.0797 0.0509
TRANS-LEN-2 0.1109 0.0158
TRANS-LEN-3 0.0935 0.0489
TRANS-LEN-5 0.0240 0.0632
Figure 1: Correlations. Coefficients marked with * are
significant (p < 0.05), and coefficients marked with **
are highly significant (p < 0.01). We omit features with
different slop values if they perform significantly worse
than similar features.
3 Correlations
We have introduced five different kinds of features,
four of which are supposed to model semantic com-
positionality directly. For feature selection, we
therefore compute the correlation of features with
compositionality scores and select features that cor-
relate significantly with compositionality. The fea-
tures are then used for regression experiments.
4 Regression experiments
For our regression experiments, we use support vec-
tor regression with a high (7) degree kernel. Other-
wise we use default parameters of publicly available
software.6 In our experiments, however, we were
not able to produce substantially better results than
what can be obtained using only the features LEFT-
ENDOC and RIGHT-ENDOC. In fact, for German
using only LEFT-ENDOC gave slightly better results
than using both. These features are also those that
correlate best with human compositionality scores
according to Figure 1. Consequently, we only use
6http://www.csie.ntu.edu.tw/?cjlin/libsvm/
31
these features in our official runs. Our evaluations
below are cross-validation results on training and de-
velopment data using leave-one-out. We compare
using only LEFT-ENDOC and RIGHT-ENDOC (for
English) with using all significant features that seem
relatively independent. For English, we used LEFT-
ENDOC, RIGHT-ENDOC, DISTR-DIFF, HYPH (5)
reversed, BR-CS, PARAPHR?100 (1). For German,
we used rel-type = ADJ NN, rel-type=V SUBJ and
RIGHT-ENDOC. We only optimized on numeric
scores. The submitted coarse-grained scores were
obtained using average +/- average deviation.7
English German
dev test dev test
BL 18.395 47.123
all sign. indep. 19.22 23.02
L-END+R-END 15.89 16.19 23.51 24.03
err.red (L+R) 0.137 0.501
5 Discussion
Our experiments have shown that the DiSCo 2011
shared task about compositionality prediction was a
tough challenge. This may be because of the fine-
grained compositionality metric or because of in-
consistencies in annotation, but note also that the
syntactically oriented features seem to perform a
lot better than those trying to single out semantic
compositionality from syntactic endocentricity and
collocational strength. For example, LEFT-ENDOC,
RIGHT-ENDOC and BR-CS correlate with compo-
sitionality scores, whereas BR-COMP does not, al-
though it is supposed to model compositionality
more directly. Could it perhaps be that annotations
reflect syntactic endocentricity or distributional sim-
ilarity to a high degree, rather than what is typically
thought of as semantic compositionality?
Consider a couple of examples of adjective-noun
pairs in English in Figure 2 for illustration. These
examples are taken from the training data, but we
have added our subjective judgments about semantic
and syntactic markedness and collocational strength
(peaking at G2 scores). It seems that semantic
markedness is less important for scores than syntac-
7These thresholds were poorly chosen, by the way. Had we
chosen less balanced cut-offs, say 0 and 72, our improved accu-
racy on coarse-grained scores (59.4) would have been compara-
ble to and slightly better than the best submitted coarse-grained
scores (58.5).
sem syn coll score
floppy disk X 61
free kick X 77
happy birthday X X 47
large scale X X 55
old school X X X 37
open source X X 49
real life X 69
small group 91
Figure 2: Subjective judgments about semantic and syn-
tactic markedness and collocational strength.
tic markedness and collocational strength. In partic-
ular, the combination of syntactic markedness and
collocational strength makes annotators rank word
pairs such as happy birthday and open source as
non-compositional, although they seem to be fully
compositional from a semantic perspective. This
may explain why our COALS-features are so predic-
tive of human compositionality scores, and why G2
correlates better with these scores than BR-COMP.
6 Conclusions
In our experiments for the DiSCo 2011 shared task
we have considered a wide range of features and
showed that some of them correlate significantly and
sometimes highly significantly with human compo-
sitionality scores. In our regression experiments,
however, our best results were obtained with only
one or two COALS-based endocentricity features.
We report error reductions of 13.7% for English and
50.1% for German.
References
Shane Bergsma, Aditya Bhargava, Hua He, and Grzegorz
Kondrak. 2010. Predicting the semantic composition-
ality of prefix verbs. In EMNLP.
David Jurgens and Keith Stevens. 2010. The S-Space
package: an open source package for word space mod-
els. In ACL.
Philipp Koehn. 2005. Europarl: a parallel corpus for
statistical machine translation. In MT-Summit.
Douglas Rohde, Laura Gonnerman, and David Plaut.
2009. An improved model of semantic similarity
based on lexical co-occurrence. In Cognitive Science.
32
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 447?451,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Factored Translation with Unsupervised Word Clusters
Christian Rish?j
Center for Language Technology
University of Copenhagen
crjensen@hum.ku.dk
Anders S?gaard
Center for Language Technology
University of Copenhagen
soegaard@hum.ku.dk
Abstract
Unsupervised word clustering algorithms ?
which form word clusters based on a measure of
distributional similarity ? have proven to be
useful in providing beneficial features for var-
ious natural language processing tasks involv-
ing supervised learning. This work explores the
utility of such word clusters as factors in sta-
tistical machine translation.
Although some of the language pairs in this
work clearly benefit from the factor augmen-
tation, there is no consistent improvement in
translation accuracy across the board. For all
language pairs, the word clusters clearly im-
prove translation for some proportion of the
sentences in the test set, but has a weak or
even detrimental effect on the rest.
It is shown that if one could determine whether
or not to use a factor when translating a given
sentence, rather substantial improvements in
precision could be achieved for all of the lan-
guage pairs evaluated. While such an ?oracle?
method is not identified, evaluations indicate
that unsupervised word cluster are most bene-
ficial in sentences without unknown words.
1 Factored translation
One can go far in terms of translation quality with
plenty of bilingual text and a translation model that
maps small chunks of tokens as they appear in the
surface form, that is, the usual phrase-based sta-
tistical machine translation model. Yet even with
a large parallel corpus, data sparsity is still an is-
sue. Factored translation models are an extension of
phrase-based models which allow integration of addi-
tional word-level annotation into the model. Operat-
ing on more general representations, such as lemmas
or some kind of stems, translation model can draw
on richer statistics and to some degree offset the data
sparsity problem.
4.1.1 The Brown algorithm
In this thesis, we use the bottom-up agglomerative word clustering algorithm of
(Brown et al, 1992) to derive a hierarchical clustering of words. The input to the
algorithm is a text, which is a sequence of words w1, . . . , wn. The output from the
clustering algorithm is a binary tree, in which the leaves of the tree are the words.
We interpret each internal node as a cluster containing the words in that subtree.
Initially, the algorithm starts with each word in its own cluster. As long as there
are at least two clusters left, the algorithm merges the two clusters that maximizes
the quality of the resulting clustering (quality will be defined later).1 Note that the
algorithm generates a hard clustering?each word belongs to exactly one cluster.
To define the quality of a clustering, we view the clustering in the context of a class-
based bigram la guage model. Given a clustering C that maps each word to a cluster,
the class-based language model assigns a probability to the input text w1, . . . , wn,
where the maximum-likelihood estimate of the model parameters (estimated with
empirical counts) are used. We define the quality of the clustering C to be the
logarithm of this probability (see Figure 4-1 and Equation 4.1) normalized by the
length of the text.
...
...
c1 c2 c3 ci cn
w1 w2 w3 wi wn
P (ci|ci?1)
P (wi|ci) ci = C(wi)
Figure 4-1: The class-based bigram language model, which defines the quality of a
clustering, represented as a Bayesian network.
1We use the term clustering to refer to a set of clusters.
44
Figure 1: Bayesian network illustrating the class-
based language model that is used to define th qual-
ity of a clustering in the Brown algorithm [Liang,
2005]
2 Unsupervised word clusters
Unsupervised word clusters owe their appeal perhaps
mostly to the relative ease of obtaining them. Ob-
taining regular morphological, syntactic or seman-
tic analyses for tokens in a text relies on some sort
of tagger, either based on manually crafted rules or
trainable on an annotated corpus. Both rule-crafting
and corpus annotation are time-consuming and ex-
pensive processes, and might not be feasible for a
small or resource-scarce language.
For unsupervised word clusters, on the other hand,
one merely needs a large amount of raw (unanno-
tated) text and some processing power. Such cluster-
ing is thus particularly interesting for resource-scarce
languages, and especially so if the clusters enable the
training of more generalized translation models with-
out more bilingual text.
The independence of annotated corpora or hand-
crafted rules make unsupervised clusters interesting
for languages rich in NLP resources too. They of-
fer a way to exploit vast amounts of raw, unanno-
tated, monolingual text, in a manner akin to the way
language models profitably may be trained on vast
amounts of raw monolingual text.
With the broad coverage achievable from vast
amounts of monolingual text, word clusters might
help alleviate the problem of unknown words in
translation. It is imaginable that a word form oth-
erwise unknown to the translation model belongs to
447
a known cluster. Appropriate use of word clusters,
coupled with a broad-coverage language model, could
make it be possible for the translation model to ar-
rive at the intended translation.
In this work we use two unsupervised clustering al-
gorithms: Brown and Unsupos. Other clustering al-
gorithms were on the drawing board as well, namely
embeddings from the Neural Language Model of Col-
lobert and Weston [2008] and word representations
from random indexing (RI)1. These, however, were
abandoned due to time constraints.
2.1 The Brown algorithm
The bottom-up agglomerative algorithm of Brown
et al [1992] processes a sequence of tokens and pro-
duces a binary tree with tokens as leaf nodes. Each
internal node in the tree can be interpreted as a clus-
ter containing the tokens on the leaf nodes of that
subtree. The clustering produced is thus a hierarchi-
cal clustering.
Very briefly, the algorithm proceeds by first as-
signing every token to its own cluster, and then iter-
atively merges the two clusters that maximises the
quality of the resulting clustering, where the quality
of a clustering is defined in terms of a class-based
language model (figure 1).
Note that this algorithm produces a hard clus-
tering, in the sense that it assigns each token to a
single cluster. From a semantic perspective, there
are homographic words whose underlying senses are
conceptually and possibly syntactically distinct, and
whose cluster-tag intuitively should depend on their
use in running text. The clustering obtained from the
Brown algorithm does not accommodate this wish.
We use the implementation2 of Liang [2005].
2.2 jUnsupos
Contrary to the hard clustering of the Brown algo-
rithm, the jUnsupos algorithm of Biemann [2006]
emits a Viterbi tagger which is sensitive to the con-
text of a token in running text. Thus, word forms can
belong to more than a single cluster, and such word
forms ? which are considered ambiguous by the al-
gorithm ? will be assigned to a cluster depending
on their context.
In a coarse outline, the algorithm works by first
inducing a distributional clustering for unambiguous
high-frequency tokens, as well as a co-occurrence-
based clustering for less common tokens. The two
partly overlapping clusterings are then combined to
1https://github.com/turian/random-indexing-
wordrepresentations
2Available at http://www.cs.berkeley.edu/~pliang/software/
100001001 immediate urgent ongoing absolute ex-
traordinary exceptional ideological un-
precedented appalling overwhelming al-
leged automatic [...]
11111100111111110 worried concerned skeptical
unhappy uneasy reticent unsure per-
plexed excited apprehensive legion un-
concerned [...]
111111100010001 cover include involve exclude
confuse encompass designate preclude
transcend duplicate defy precede [...]
1111111000000 encourage promote protect defend
safeguard restore assist preserve coordi-
nate convince destroy integrate [...]
0111000 china russia iran israel turkey ukraine in-
dia japan pakistan georgia serbia europol
[...]
1000110010 waste water drugs land fish material
meat profit alcohol forest blood chemi-
cals [...]
Figure 2: Exemplars of word clusters obtained using
the Brown algorithm (C=1000), showing the 12 most
frequent tokens per cluster
produce a lexicon with derived syntactic categories
and word forms.
2.3 Cluster count and complexion
A reasonable question when faced with the task of
inducing word clusters in an unsupervised manner
is: How many clusters to produce? This question is
presumably closely intertwined with the question of
what sort of beast a cluster obtained in this man-
ner can be expected to be. Would a clustering with
around 30-90 clusters correspond somewhat closely
to an ordinary part-of-speech tag-set for the given
language?
Looking at the handful of exemplar clusters shown
in figure 2, which were obtained with the Brown algo-
rithm (using a cluster count of 1000), we cautiously
note some apparent patterns.
? The clusters appear to be subsets of the cluster-
ing implied by conventional part-of-speech tags:
The first two consist of adjectives (including the
rather ambiguous form legion), the next two
(transitive) verbs and the final two nouns.
? Syntactically, members of the two apparent verb
448
clusters seem to consist of verbs in their infini-
tive (or plurally inflected) form.
? From a quasi-semantic perspective, the last clus-
ter appears to consist of nouns for corporeal
goods (as apposed to immaterial things).
? While most exemplars from the second-last clus-
ter are countries, all of the shown forms can be
said to be proper nouns.
Note that only the 12 most frequent forms from each
cluster are displayed, the apparent patterns should
be taken with a pinch of salt. Although the qualities
suggested can be expected to relate to distributional
properties that the clusters reflect, exceptional mem-
bers are perhaps to be expected.
In the present work, we went with the pre-trained
models for jUnsupos3, which have the following
characteristics4:
Lang Corpus # Sents # Tags
cs LCC 4 M 539
de Wortschatz 40 M 396
en Medline 2004 34 M 480
es LCC 4.5 M 415
fr LCC 3 M 359
For the Brown algorithm, we are contrasting clus-
ter count choices of 320 and 1000, based on reports
of other successful applications [Turian et al, 2010]5,
with clustering models trained on monolingual data
from the Europarl corpus and the News Commentary
corpus.
3 Experimental setup
The baseline systems were set up in accordance with
the guidelines on the shared task website. That is,
they were trained with grow-diag-final-and word
alignment heuristics and msd-bidirectional-fe re-
ordering.
Translation models were trained on a concatena-
tion of the Europarl and News Commentary corpora,
which were first tokenized, then filtered to sentence
lengths of up to 40 tokens, and finally lowercased.
5-gram language models were built using
ngram-count on a concatenation of the Eu-
roparl corpora and the News Commentary corpora.
3As available at http://wortschatz.uni-
leipzig.de/~cbiemann/software/unsupos.html
4LCC refers to the Leipzig Corpora, available at
http://corpora.uni-leipzig.de/. Wortschatz refers to
http://www.wortschatz.uni-leipzig.de/. Medline is avail-
able at http://www.nlm.nih.gov/mesh/filelist.html.
5A planned evaluation of a cluster count of 3200 was aban-
doned due to time constraints
For the unsupervised word clusters, 5-gram language
models were used as well, built from tagged versions
of the same corpora. All language models were
binarised and loaded using KenLM [Heafield, 2011].
Minimum error rate training (MERT) was used to
optimise parameters on both baseline and factored
models against the 2008 news test set, as suggested
on the shared task website6.
All phrase tables were filtered and binarised for
the development and testing corpora during tuning
and testing, respectively.
Seeing that the preparation of the raw corpora,
word clustering models, factored corpora, language
models, as well as training, optimization and eval-
uation of the various models was a rather involved,
yet repetitive process, we took a stab at making a
GNU Makefile-based approach for automated han-
dling (and parallelisation) of the whole dependency
graph of subtasks. The ongoing effort, which shares
some aspirations and abilities with the recently an-
nounced Experiment Management System (EMS), is
publicly available7.
4 Results
Table 1a lists BLEU scores for adding jUnsupos tags
(uPOS), Brown clusters with 320 clusters (C320) or
Brown clusters with 1000 clusters (C1000) as either
an alignment factor, a two-sided translation factor or
a source-sided translation factor.
Although using Brown clusters (C1000) as a two-
sided translation factor improves BLEU scores for
some language pairs, most notably en-cs, en-de and
cs-en, no clear across-the-board benefit is seen.
4.1 Oracle scores
Based on the hypothesis that the factorisations are
beneficial when translation some sentences, and not
when translating others, we completed an oracle-
based evaluation, in which we assume to know a pri-
ori whether to use the factored model for translating
a given sentence, or just go with the baseline, unfac-
tored model. In reality, we don?t have such an or-
acle method for arbitrary sentences, but when deal-
ing with the shared task test set (or other corpora
for which we have reference translations), it was easy
enough to check per-sentence BLEU scores for each
model and make the decision based on a comparison.
Table 1b lists BLEU scores obtainable with each
factor configuration given such an oracle method. In
this scenario, most factored models beat the baseline,
indicating that the factorisations are beneficial for
certain sentences, and detrimental for others.
6http://www.statmt.org/wmt11/translation-task.html
7At https://gibhub.com/crishoj/factored
449
Pair Baseline Alignment factor Two-sided translation Source-sided transl. BestC1000 C320 uPOS C1000 C320 uPOS C1000 C320 uPOS ? %
cs-en 18.18 17.77 17.19 13.54 18.59 18.36 17.50 18.19 18.19 17.59 0.41 2.3%
de-en 18.45 17.94 17.57 16.36 18.56 18.42 17.93 18.12 18.12 17.86 0.11 0.6%
en-cs 11.85 11.82 11.61 9.75 12.73 12.28 10.94 11.92 11.92 11.85 0.88 7.4%
en-de 13.27 12.90 12.83 11.98 13.81 13.84 13.19 12.94 12.94 12.92 0.57 4.3%
en-es 28.08 27.10 26.52 24.90 28.40 28.16 27.50 27.31 27.31 27.19 0.32 1.1%
en-fr 25.90 24.60 23.98 21.85 25.89 20.59 24.16 24.89 24.89 24.74 ? ?
es-en 26.70 24.87 24.71 23.92 25.76 25.96 25.40 24.92 24.92 24.92 ? ?
fr-en 24.73 23.18 23.13 21.76 24.01 22.86 23.23 23.37 23.37 23.04 ? ?
(a) BLEU scores for factor configurations in comparison to the unfactored baseline
Pair Baseline Alignment factor Two-sided translation Source-sided transl. BestC1000 C320 uPOS C1000 C320 uPOS C1000 C320 uPOS ? %
cs-en 18.18 19.93 19.81 19.19 20.01 20.00 19.83 19.58 19.58 19.63 1.83 10.1%
de-en 18.45 20.06 20.00 19.75 20.28 20.26 20.15 19.84 19.84 19.90 1.83 9.9%
en-cs 11.85 13.18 13.14 12.81 13.77 13.58 12.98 12.83 12.83 12.93 1.92 16.2%
en-de 13.27 14.56 14.60 14.36 14.98 15.10 14.81 14.21 14.21 14.28 1.83 13.8%
en-es 28.08 29.70 29.50 29.17 30.33 30.2 30.00 29.54 29.54 29.56 2.25 8.0%
en-fr 25.90 27.34 27.22 26.90 27.84 26.98 27.32 27.15 27.15 27.16 1.94 7.5%
es-en 26.70 27.83 27.81 27.74 28.16 28.20 28.06 27.64 27.64 27.73 1.50 5.6%
fr-en 24.73 25.86 25.95 25.83 26.16 26.31 26.05 25.66 25.66 25.69 1.58 6.4%
(b) BLEU scores with an oracle-directed, per-sentence selective usage of either the baseline or the factored model
Table 1: BLEU scores when using Brown Clusters with granularity 1000 (C1000), granularity 320 (C320)
and unsupervised part-of-speech tags (uPOS) as either an added alignment factor, a two-sided translation
factor or a source-sided translation factor
Pair Baseline Oracle Abs. ? Rel. %
cs-en 18.18 22.60 4.42 24.3%
de-en 18.45 22.42 3.97 21.5%
en-cs 11.85 15.89 4.04 34.1%
en-de 13.27 17.16 3.89 29.3%
en-es 28.08 32.52 4.44 15.8%
en-fr 25.90 30.07 4.17 16.1%
es-en 26.70 30.22 3.52 13.2%
fr-en 24.73 28.67 3.94 15.9%
Table 2: BLEU scores under the assumption of an
oracle function indicating the optimal factor config-
uration for each sentence
4.2 Combined oracle scores
Imagine another oracle function, which would not
simply determine whether to prefer a given factored
model over the baseline for a given sentence, but
instead indicate which of several possible factored
models to use when translating a given sentence.
BLEU scores obtainable under the assumption of
such a combined oracle function are listed in table 2.
As was the case for the individual factored models
(table 1a), en-cs, en-de and cs-en see the largest ben-
efits over the baselines.
These oracle scores are obviously an idealised case.
They indicate an upper bound that one could seek to
approximate by constructing an appropriate oracle
function.
4.3 Unknown words
In section 2 it was hypothesised that word clus-
ters are potentially beneficial in translating sentences
with unknown words ? that is, word forms which
were not seen in any aligned sentences (but which
may belong to a word cluster known by the transla-
tion model).
With this hypothesis in mind, we would like to
450
Pair Sentences Baseline C1000 Rel. %
cs-en 1955 65% 17.63 17.70 0.4%
de-en 1925 64% 17.84 17.56 -1.6%
en-cs 1583 53% 11.85 12.63 6.6%
en-de 1395 46% 13.65 13.47 -1.3%
en-es 1327 44% 27.77 27.97 0.7%
en-fr 1369 46% 25.43 25.11 -1.3%
es-en 1316 44% 26.43 25.41 -3.9%
fr-en 1423 47% 24.20 23.56 -2.6%
Avg. 1537 51% 20.60 20.43 -0.4%
(a) BLEU scores for sentences with unknown words
Pair Sentences Baseline C1000 Rel. %
cs-en 1048 35% 19.63 20.77 5.8%
de-en 1078 36% 20.03 21.24 6.0%
en-cs 1420 47% 11.85 12.90 8.9%
en-de 1608 54% 12.97 14.22 9.6%
en-es 1676 56% 28.41 28.88 1.7%
en-fr 1634 54% 26.46 26.81 1.3%
es-en 1687 56% 27.01 26.15 -3.2%
fr-en 1580 53% 25.40 24.58 -3.2%
Avg. 1466 49% 21.47 21.94 3.4%
(b) BLEU scores for sentences with no unknown words
Table 3: BLEU scores for the best overall factorisa-
tion, Brown clusters (C=1000) as a two-sided trans-
lation factor, on sentences with (table 3a) and with-
out (table 3b) unknown words
see how the factored models fare in comparison to
the unfactored baselines, specifically for those sen-
tences containing unknown words, and for the rest
(sentences without unknown words). This targeted
evaluation was done using the best overall factor con-
figuration: Brown clusters (C=1000) as a two-sided
translation factor.
The results are shown in tables 3a and 3b. On
average (across language paris), 51% test set sen-
tences contain at least 1 unknown word. Contrary
to what might be expected, the factorisation seems
to be most beneficial for sentences with all known
words (3.4% improvement in BLEU score on aver-
age). For sentences with unknown words, the effect
is weak or detrimental (except for en-cs), averaging
a slight decrease (-0.4%) in BLEU score across the
language pairs.
The lack of benefit for sentences with unknown
words is likely due to the fact that no additional
monolingual data was used to make the Brown clus-
ters for this experiment. In other words, there is
no chance of knowing the Brown cluster for an un-
known word. Furthermore, we assume that gains for
sentences with unknown words are more likely with
a factorisation that includes an alternative decoding
path for word clusters8.
5 Conclusions and future work
In this work we have explored the utility of three un-
supervised word clusterings as either an alignment
factor, a two-sided translation factor or a source-
sided translation factor.
Although no across-the-board benefit was seen, it
was evident that the factorisations help in translating
some proportion of the test set sentences. Being able
to determine for which sentences to use a factored
model is clearly desirable.
Overall, the single most beneficial of the factor
configurations explored was Brown clusters with a
granularity of 1000, as a two-sided translation factor.
A more detailed evaluation of the effects of different
cluster sizes, as well as using clusters induced from
more text, would be interesting in a follow-up study.
Using clusters in some more interesting factor
configurations, particularly in alternative decoding
paths, is still pending.
References
C. Biemann. Unsupervised part-of-speech tagging em-
ploying efficient graph clustering. In Proceedings of
the 21st International Conference on computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics: Student Research
Workshop, pages 7?12, 2006.
P. F Brown, V. J.D Pietra, P. V deSouza, J. C Lai, and
R. L Mercer. Class-based n-gram models of natural
language. Computational linguistics, 18(4):467?479,
1992.
R. Collobert and J. Weston. A unified architecture for
natural language processing: Deep neural networks
with multitask learning. In Proceedings of the 25th
international conference on Machine learning, page
160?167, 2008.
K. Heafield. KenLM: faster and smaller language model
queries. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, Edinburgh, UK, July
2011. Association for Computational Linguistics.
P. Liang. Semi-supervised learning for natural language.
PhD thesis, Massachusetts Institute of Technology,
2005.
J. Turian, L. Ratinov, and Y. Bengio. Word repre-
sentations: A simple and general method for semi-
supervised learning. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, page 384?394, 2010.
8Evaluation of factor configurations with alternative de-
coding paths were abandoned due to limited computational
resources and initially discouraging results
451
