Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 75?83,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Mining the Web for Reciprocal Relationships
Michael Paul, Roxana Girju, and Chen Li
Linguistics and Computer Science Departments and Beckman Institute,
University of Illinois at Urbana-Champaign
{mjpaul2, girju, chenli}@illinois.edu
Abstract
In this paper we address the problem of
identifying reciprocal relationships in English.
In particular we introduce an algorithm that
semi-automatically discovers patterns encod-
ing reciprocity based on a set of simple but
effective pronoun templates. Using a set of
most frequently occurring patterns, we extract
pairs of reciprocal pattern instances by search-
ing the web. Then we apply two unsuper-
vised clustering procedures to form meaning-
ful clusters of such reciprocal instances. The
pattern discovery procedure yields an accu-
racy of 97%, while the clustering procedures
indicate accuracies of 91% and 82%. More-
over, the resulting set of 10,882 reciprocal in-
stances represent a broad-coverage resource.
1 Introduction
Reciprocity is a pervasive concept which has been
studied a lot in a wide variety of fields from ethics
to game theory where it is analyzed as a highly ef-
fective ?tit for tat? strategy. The ethic of reciprocity
(also known as the golden rule), for example, is a
moral code born from social interaction: ?Do onto
others as you would wish them do onto you?. The
golden rule appears in most religions and cultures as
a standard used to resolve conflicts.
According to sociologists and philosophers, the
concept of reciprocity lies at the foundation of social
organization. It strengthens and maintains social re-
lations among people, beyond the basic exchange of
useful goods. Thus, the way people conceptualize
reciprocity and the way it is expressed in language
play an important role in governing people?s behav-
ior, judgments, and thus their social interactions.
In this paper we present an analysis of the concept
of reciprocity as expressed in English and present a
way to model it. In particular we introduce an al-
gorithm that semi-automatically discovers patterns
encoding reciprocity based on a set of simple but ef-
fective pronoun templates. We then rank the identi-
fied patterns according to a scoring function and se-
lect the most frequent ones. Using these patterns we
query the web and run two unsupervised clustering
procedures to form meaningful clusters of reciprocal
pattern instances. The pattern discovery procedure
yields an accuracy of 97%, while the clustering pro-
cedures indicate accuracies of 91% and 82%. More-
over, the resulting set of 10,882 reciprocal instances
represent a broad-coverage resource.
Next we define the concept of reciprocity as ex-
pressed in English.
Reciprocity in language
The Oxford English Dictionary Online1 defines
reciprocity as ?a state or relationship in which there
is mutual action, influence, giving and taking, cor-
respondence, etc., between two parties?, while in
WordNet the verb to reciprocate means ?to act, feel,
or give mutually or in return?.
Reciprocity is defined as a relation between two
eventualities eo (original eventuality) and er (recip-
rocated eventuality), which can occur in various re-
ciprocal constructions. Each eventuality is an event2
or a state between two participants. Thus, the rela-
1http://www.oed.com/
2We use the term ?event? to denote all those actions or ac-
tivities performed by people.
75
tion of reciprocity <(eo(X, Y), er(Z, W)) describes
a situation where the eventuality er is performed ?in
return? for eo. Thus, reciprocity can be seen as a
special type of causal relation.
The two arguments of each eventuality represent
the subject and the object (direct or indirect), in this
order, and they might not all be explicitely stated
in the sentence, but can be inferred. Moreover, the
participants of the two eventualities might or might
not be the same. A few such examples are presented
below with the corresponding reciprocity relations:
(1) Mary argued with Paul at the station.
<(argue with(Mary, Paul), argue with(Paul Mary)) &
<(argue with(Paul, Mary), argue with(Mary, Paul))
(2) Paul and Mary hate each other.
<(hate(Paul, Mary), hate(Mary, Paul)) &
<(hate(Mary, Paul), hate(Paul, Mary))
(3) Mary likes Paul and he likes her, too.
<(like(Mary, Paul), like(Paul, Mary)) &
<(like(Paul, Mary), like(Mary, Paul))
(4) Mary likes Paul for helping her sister.
<(help(Paul, Mary?s sister), like(Mary,Paul))3
As shown in the examples above, in English
there are two basic types of reciprocal construc-
tions: mono-clausal reciprocals (involving words
such as (to) hug, to agree/argue with, partner of, mu-
tual(ly), together, each other ? examples (1) and (2))
or sentence-level reciprocals (involving two consec-
utive clauses ? examples (3) and (4)). Most of the
sentence-level reciprocals are paraphrased by coor-
dinations or subordinations of two clauses with the
same or different predicate and most of the time in-
verted arguments. They might also manifest various
markers as shown in bold in the examples.
In this paper we focus only on sentence-level con-
structions when the eventualities occur in different
consecutive clauses, and when the subject ? object
arguments of each eventuality are personal pronoun
pairs which occur in reverse order in each eventual-
ity. One such example is ?She likes him for help-
ing her?. Here the two eventualities are like(she,
he) and help(he, she). In this example, although the
subject of the second verb is not explicitely stated,
it is easily inferred. These simplifying assumptions
3We assume here that the subject of the verb help has been
recovered and the coreference solved.
will prove very useful in the semi-supervised pat-
tern discovery procedure to ensure the accuracy of
the discovered patterns and their matched instances.
Such a resource of reciprocal event pairs can
be very useful in a number of applications, rang-
ing from question answering and textual entailment
(since reciprocal event pairs encode a type of causal
relation), to behavior analysis of social groups (to
monitor cooperation, trustworthiness and personal-
ity), and behavior prediction in negotiations.
The paper is organized as follows. In the next sec-
tion we present relevant previous work. In Section
3 we detail a semi-supervised approach of extract-
ing patterns which encode reciprocity in English. In
section 4 we extract pairs of reciprocal instances and
cluster them in meaningful clusters. In section 5 we
present the experimental data and results. Discus-
sions and conclusion are presented in Section 6.
2 Previous work
Although the concept of reciprocity has been studied
a lot in different disciplines such as social sciences
(Gergen et al, 1980), anthropology (Sahlins, 1972),
economics (Fehr and Gachter, 2000), and philoso-
phy (Becker, 1990), linguists have started to look
deeper into this problem only more recently. More-
over, to the best of our knowledge, in computational
linguistics the problem is novel.
In linguistics, most of the work on reciprocity fo-
cuses on mono-clausal reciprocal constructions, in
particular on the quantifiers each other and one an-
other (Dalrymple et al, 1998; Heim, 1991; Ko?nig,
2005). Most of this work has been done by lan-
guage typologists (Maslova and Nedjalkov, 2005;
Haspelmath, 2007) who are interested in how recip-
rocal constructions of these types vary from one lan-
guage to another and they do this through compara-
tive studies of large sets of world?s languages.
In computational linguistics, our pattern discov-
ery procedure extends over previous approaches
that use surface patterns as indicators of semantic
relations between nouns or verbs ((Hearst, 1998;
Chklovski and Pantel, 2004; Etzioni et al, 2004;
Turney, 2006; Davidov and Rappoport, 2008) inter
alia). We extend over these approaches in two ways:
(i) our patterns indicate a new type of relation be-
tween verbs, (ii) instead of seed or hook words we
76
use a set of simple but effective pronoun templates
which ensure the validity of the patterns extracted.
To the best of our knowledge, the rest of our
reciprocity model is novel. In particular, we use a
novel procedure which extracts pairs of reciprocal
instances and present two novel unsupervised clus-
tering methods which group the instance pairs in
meaningful ways. We also present some interesting
observations on the data thus obtained and suggest
future research directions.
3 Pattern discovery procedure
Our algorithm first discovers clusters of patterns in-
dicating reciprocity in English, and then merges the
resulting clusters to identify the final set of recipro-
cal constructions. In this section we detail the algo-
rithm and evaluate it in subsection 5.2.
3.1 Pronoun templates
In this paper we focus on reciprocal eventualities
which occur in two consecutive clauses and have
two arguments: a subject and an object. One way
to do this is to fully parse each sentence of a corpus
and identify coordinations or subordinations of two
clauses. Then identify the subject and object argu-
ments of each verb in each clause with the help of
a PropBank-style grammatical or semantic role la-
beler (Kingsbury et al, 2002) and make sure they
represent people named entities (as indicated by
proper names, personal pronouns, etc.). Since our
focus is on reciprocal constructions, we also have to
keep in mind that the verbs have to have the same
set of arguments (subject-object) in reverse order.
Thus, noun and pronoun coreference should also be
resolved at this point.
Instead of starting with such a complex and error-
prone preprocessing procedure, our algorithm con-
siders a set of pronoun templates, where personal
pronouns are anchor words (they have to be matched
as such). Each template consists of four personal
pronouns corresponding to a subject - object pair in
one clause, and a subject - object pair in the other
clause. Two such examples are
?[Part1] I [Part2] him [Part3] he [Part4] me [Part5]? and
?[Part1] they [Part2] us [Part3] we [Part4] them [Part5]?,
where [Part1] - [Part5] are partitions identifying
any sequence of words. This is an elegant proce-
dure since in English, pronouns have different cases
such as nominative and accusative4 which identify
the subject, and respectively the object of an event.
This saves us the trouble of parsing a sentence to
find the grammatical roles of each verb. In English,
there are 30 possible arrangements of nominative -
accusative case personal pronoun pairs. Thus we
built 30 pronoun templates.
This approach is similar to that of seed words
(e.g., (Hearst, 1998)) or hook words (e.g., (Davidov
and Rappoport, 2008)) in previous work. However,
in our case they are fixed and rich in grammatical in-
formation in the sense that they have to correspond
to subject - object pairs in consecutive clauses.
Since the first two pronouns in each pronoun tem-
plate belong to the first clause (C1), and the last two
to the second clause (C2), the templates can be re-
stated as [Part1] C1 [Part3] C2 [Part5], with the re-
striction that partition 3 should not contain any of
the four pronouns in the template. C1 denotes ?Pro-
noun1 [Part2] Pronoun2? and C2 denotes ?Pronoun3
[Part4] Pronoun4?. Partitions 2 and 4 contain the
verb phrases (and thus the eventualities) we would
like to extract. For speed and memory reasons, we
limit their size to no more than 5 words.
Moreover, since the two clauses are consecutive,
we hypothesize that they should be very close to
each other. Thus, we restrict the size of each par-
tition 1, 3, and 5 to no more than 5 words. We then
consider all possible variations of the pattern where
the size of each partition varies from 0 to 5. This re-
sults in 216 possible combinations (63). Moreover,
to ensure the accuracy of the procedure, partitions 1
and 5 should be bounded to the left and respectively
to the right by punctuation marks, parentheses, or
paragraph boundaries. An example of an instance
matched by one such pattern is ?, I cooked dinner
for her and she loves me for that .?
3.2 Scoring function
One way to compute the prominence of the discov-
ered patterns would be to consider the frequency of
each of the five partitions. However, as our pre-
liminary experiments suggest, although individual
4In English, the pronouns you has the same form in nomina-
tive and accusative.
77
patterns within each partition do often repeat, rank-
ing patterns spanning all three partitions (PART1,
PART3, and PART5) is problematic. Patterns with
relatively long partitions (more than 2 words each)
seldomly occur more than once in the entire corpus.
Thus frequency would produce very little differenti-
ation in ranking the patterns.
Thus we developed an alternative scoring system
in lieu of frequencies. A sequence of size n (seq(n))
is an instance of a pronoun template and a subse-
quence of size k (seq(k)) is simply a substring of the
sequence with k < n. For example, for the instance
?I love her and she loves me , too? of length 9, there
will be two subsequences of length 8: ?love her and
she loves me , too? and ?I love her and she loves me
,?. Taking into account the frequencies of the subse-
quences occurring within instances of each partition,
we use the following recursive scoring function (n is
the length of each subsequence of size n):
Score(seq(n)) =
8
><
>:
Disc(freq(seq(n)))+
P
seq(n?1) Disc(Score(seq(n ? 1))), if n> 1
freq(seq(n)), if n= 1
(1)
In addition, in order to ensure a valid ranking
over the extracted templates with different lengths
for each partition, we need to normalize the scores
obtained for PART1, PART3, and PART5. In other
words, we need to scale the scores obtained for each
partition to discount the scores of longer partitions,
so that the maximum possible score would remain
the same regardless of how long the partition is.
So we use the following formula to compute the
discount for each of PART1, PART3, and PART5,
where n is the length of the subsequence:
Disc(Score(seq(n))) =
{
(1.0? fraction) ? fractionm?nm?n+1 , if n> 1
fractionm?n
m?n+1 , if n= 1
(2)
Fraction is an empirically predetermined parame-
ter - here set to 0.5. The variable m is the length of
the entire PART1, PART3, or PART5 in question.
This allows not only the frequency of the exact
pattern to contribute to the score, but also occur-
rences of similar patterns, although to a lesser ex-
tent. And since partitions 1, 3, and 5 constitute the
salient parts of the pattern as the environment for the
two reciprocal clauses C1 and C2, we take the score
to be ranked as Score(PART1)?Score(PART3)?
Score(PART5).
We searched the 30 pronoun templates with var-
ious partition sizes on a 20 million word English
corpus obtained from Project Gutenberg, the largest
single collection of free electronic books (over
27,000) (http://www.gutenberg.org) and British Na-
tional Corpus (BNC), an 100 million word collec-
tion of English from spoken and written sources.
There were 2,750 instances matched which were
ranked by the scoring function. There were 1,613
distinct types of patterns which generated 1,866 dis-
tinct pattern instances. Thus, we selected the top
15 patterns, after manual validation. These patterns
represent 56% of the data (Table 1). All the other
patterns were discarded as having very low frequen-
cies and being very specific.
The manual validation was necessary in order to
collapse some of the identified instances into more
general classes. For example, the patterns ?C1 and
C2 to? (e.g., ?He could not hurt me and I would not
wish him to.?), ?C1 and C2 in? (e.g., ?I give you and
you take me in.?), and ?C1 and C2 fast said Aunt
Jane? (e.g., ?He will come to her and she can hold
him fast said Aunt Jane.?) were collapsed into ?C1
and C2?. This procedure can be partially solved by
identifying complex verbs such as ?take in?. How-
ever, we leave this improvement for future work.
Patterns Examples
C1 [, |; |.] C2 I help him; he helps me.
C1 and C2 He understands her and she understands
him.
C1 and C2 [right] back I kissed him and and he kissed me back.
C1 and C2 for that They helped us and we appreciate them
for that.
C1 and C2, too I love her and she loves me, too.
C1 when C2 He ignores her when she scolds him.
C1 whenever C2 He is there for her whenever she needs
him.
C1 because C2 They tolerate us because we helped them.
C1 as much as C2 He loves her as much as she loves him.
C1 for C2 (vb-ing) He thanked her for being patient with him.
C1 but C2 I loved her but she dumped me.
C1 for what C2 They will punish him for what he did to
them.
C1 and thus C2 She rejected him and thus he killed her.
when C1, C2 When he confronted them, they arrested
him.
C1 as long as C2 She will stay with him as long as
he doesn?t hurt her.
Table 1: The top 15 reciprocal patterns along with examples.
78
4 Clustering of Reciprocal Eventualities
It seems reasonable to expect that certain reciproc-
ities could be grouped together. For example, the
language used in convincing a person of some-
thing could be characterized by verbs such as
eo = {convince, promise, assure, beg} and er =
{believe, trust, choose, forgive}.
There are many potential uses for this sort of
grouping. Having a single group label for multiple
reciprocal eventuality pairs would allow us to iden-
tify certain language patterns as a particular speech
act. Also, such clusters could be useful if one wants
to perform a macro-level analysis of reciprocity in a
specific domain. For example, examining reciprocal
language could be useful in analyzing the nature of
a social community or the theme of a literary work.
Generalizing over many similar instances, will give
us better insight into how people communicate ? as
reactions (effects) to other people?s actions (causes).
Thus, in this section we present a model for clus-
tering the eventualities we extract through the pro-
cess described in the previous sections. Experimen-
tal results are presented in Section 5.
4.1 Representing the data
After obtaining these patterns, we must extract pairs
of eventualities of the form (eo, er). This involves
both reducing the clauses into a form that is seman-
tically representative of some eventuality, as well as
determining the order of the two eventualities (i.e.,
if they are asymmetric).
As shown in the previous sections, each pat-
tern contains two clauses of the form ?Pronouni
[Part2/4] Pronounj?, where the first pronouns is
the subject and the second is the object. From
each clause we extract only the non-auxiliary verb,
as it carries the most meaning. We first stem the
verb and then negate it if it is preceded by not or
n?t. For example, ?They do not like him because
he snubbed them? is represented as the eventualities
(eo, er) = (snub,?like).
Certainly, we are missing important information
by excluding phrases and ignoring modality. How-
ever, these features can be difficult to capture accu-
rately, and since inaccurate input could degrade the
clustering accuracy, in this research we stick with
the important and easily-obtainable features.
4.2 Ordering the eventualities
Most patterns entail a particular ordering of the two
eventualities, corresponding to symmetric (e.g., ?He
loves her and she loves him?) or asymmetric eventu-
alities (e.g., ?He ignores her when she scolds him?).
In ambiguous situations (e.g., He loves her and she
loves him? and ?He cheated on her and she still
loves him!?), we determine the order through clues
such as the relative temporal ordering of the verbs as
determined by their tense (e.g., past or present tense
happens before future tense) and whether the verbs
denote an action (e.g., ?to chase?) or a state (e.g.,
?to love?). For this we rely on our previous work
(Girju, 2009) where we identified the order of even-
tualities based on a set of such features employed in
a semi-supervised model whose accuracy is 90.2%.
4.3 Modeling the relationships
The extracted eventuality pairs can be represented
as a bipartite graph with a node for all eo values
in one partition, a node for all er values in another
partition, and an edge between these nodes for each
(eo, er) pair. An intuitive way to cluster these even-
tualities is to find groups of nodes such that each
node in one partition has an edge to every node in
the other partition and vice versa. This is a form of
hard-clustering, as membership in a cluster is strictly
yes or no. The goal is that one could randomly pull
an eo and an er from a given cluster and the reci-
procity would be valid. For example, ?help? and
?give? could both be reciprocated by either ?thank?
or ?like?. Thus, given a cluster, not only is there a
reciprocal relationship between verbs in the eo group
with the verbs in the er group, but there is often
a kind of similarity relationship between the verbs
within each eo or er group.
This approach gives precise and concrete relations
between verbs, but while it could be well-suited
to some applications (such as knowledge base con-
struction or automatic verb classification (Joanis et
al., 2008)5) it has disadvantages in the context of
grouping these verbs together. The clusters are small
and sparse, and the results are difficult to interpret,
as there are many overlapping clusters.
5These verb classes correspond to some extent to the Verb-
Net (Kipper et al, 2000) or FrameNet-style (Baker et al, 1998)
verb classes such as admire, judgment.
79
..
.
.
.
.
.
.
.
.
.
.
cheat
hurt
forgive
despise
hate
betray
Figure 1: A sample of our data as a bipartite graph. Some edges have
been omitted for readability. The nodes {eo=?betray?, eo=?cheat?,
er=?despise?, er=?hate?} form a cluster with our hard-clustering ap-
proach.
We instead adopt a probabilistic framework,
which allows us to relax the restrictiveness of
the clusters while retaining information about the
strength of the pairwise relations. Thus, we design
a bimodal mixture model in which we assume that
each pair of eventualities (eo, er) belongs to a latent
class z, and each class is associated with two distinct
multinomial distributions from which the two even-
tualities are independently drawn. Thus, the proba-
bility of generating a particular pair is:
P (eo, er) =
|Z|?
k
P (eo|z = k)P (er|z = k) (3)
Each class can be thought of as a general type of
reciprocity, such as an action followed by apprecia-
tion, or an attack followed by retaliation. We should
be clear that each class is characterized not by a dis-
tribution of specific pairs, but by a distribution of
eo verbs and a distribution of er verbs. This allows
for the classification of (eo, er) pairs that do not ap-
pear in the corpus. For example, if we have not seen
the pair (slap, punch), but we know that (slap, hit)
and (kick, punch) belong to the same class, then it
is likely that (slap, punch) is in the same group.
This model can be used in a fully supervised as
well as a semi-/unsupervised setting. If some or
all of the class labels are unknown, we can learn
the model parameters using an estimator such as
Expectation-Maximization (EM) (Dempster et al,
1977). For each eventuality pair ci in a collection
C, we update P (z = k|ci) with the following equa-
tion, which represents the E-step:
P (z|ci) ? P (z)P (e(ci)o |z)P (e(ci)r |z) (4)
In the M-step, we use the following update equa-
tions:
P (z = k) ? ? +
|C|?
i
P (z = k|ci) (5)
P (eo = j|z) = ? +
?|C|
i I(e(ci)o = j)P (z|ci)
|Eo|? +?j?
?
i I(e(ci)o = j?)P (z|ci)(6)
where I is a binary indicator function. The equa-
tion for P (er = j|z) is identical to that for eo, but
with er instead6.
? and ? are the hyperparameters of the uniform
Dirichlet priors of P (z) and P (e?|z). They can
be tuned to control the level of smoothing; a value
of 1.0 is equivalent to the commonly-used Laplace
smoothing (Nigam et al, 2000).
4.4 Identifying polarity words
Since we are interested in analyzing how people in-
teract, we would also like to identify the polarity
(affective value) associated with each eventuality.
Thus, we automatically identify polarity words in
both clauses. For this we consider the standard po-
larity values: Good, Bad, and Neutral.
In the next section we present in detail the results
of the evaluation.
5 Experimental data and results
5.1 Data collection
While the Gutenberg and BNC collections are use-
ful in obtaining the frequent patterns, they do not
contain a very large number of eventuality pairs
to do meaningful clustering. We thus query the
web through Google to easily obtain thousands of
examples. We queried each of the top 15 pat-
terns and all pronoun combinations thereof (e.g.
?they * us because we * them?) and took the top
500 results for each pattern/pronoun combination
(15*30*500)7. We then extracted the clauses from
the result snippets using the procedure outlined in
the previous section and ended up with 10,882 pairs
6We sometimes use the shorthand P (z) to represent P (z =
k), which is updated for each particular value of z.
7This is because Google limits traffic. However, in the future
we can acquire more instances.
80
(4,403 unique pairs) since some of the queries had
less than 500 matched instances8.
5.2 Pattern discovery procedure
Since we wanted to see to what extent the 15 most
frequently occurring patterns encode reciprocity, we
selected a sample of 10 pattern instances matched
by each pattern in the text collection obtained from
the web. We presented the resulting 130 sentences
(a few patterns were not frequent on the web, so we
obtained a few less than 10 instances) to 2 judges
who evaluated them as encoding reciprocity (?yes?)
or not (?no?). The judges agreed 97% of the time.
Moreover, only 2.3% of the 130 pattern instances
did not encode reciprocity as agreed by both judges.
These statistics show that these patterns are highly
accurate indicators of reciprocity in English.
5.3 Unsupervised clustering
We can capture pattern instance clusters with no
prior labeling by initializing the EM parameters ran-
domly. In our experiments we used ? = 1.0 and
? = 0.01, with varying numbers of clusters (which
we denote as k). EM is sensitive to the initial pa-
rameters and can perform poorly due to many local
maxima. We thus ran the algorithm several times,
and saved the output with the best log-likelihood.
Results from clustering with k = 6 are shown
in Table 2. The examples shown correspond to a
random sample of 10 pairs within the top 10% of
P (eo, er|cluster) within each cluster. We find that
with larger values of k such as 30 or 50, some of the
clusters become noisier, but we can capture finer-
grained clusters such as eo = {libel, defame} and
er = {sue,?sue}.
Upon a close look at the clusters in Table 2, one
can see that each one seems to have a central theme.
Cluster 1 seems to contain mostly positive actions
reciprocated by verbs describing gratitude and ap-
preciation. Cluster 2 has to do with cognition; Clus-
ter 3 has to do with the way people communicate and
interact. Cluster 4 captures relationships of need and
desire. Cluster 5 is about love and adoration, while
Cluster 6 is about hate and other negative events, and
how they are reciprocated.
8The reciprocity dataset is available for download at
http://apfel.ai.uiuc.edu/resources.html.
Accuracy
No. instances 6 clusters 9 clusters
Top 20 90.8% 82.2%
20/100 71.7% 66.1%
20/All 34.2% 26.1%
Table 3: Cluster membership accuracy for 6 and 9 clusters.
Cluster membership is defined as argmaxc
P (eo|c) P (er|c). We took three samples of pairs:
(1) the top 20 pairs with the highest P (eo, er|c) val-
ues, (2) a random 20 of the top 10%, and (3) a ran-
dom 20 of all pairs assigned to each cluster. We pre-
sented the pairs to two judges who were asked to
identify each pair as belonging to the cluster or not
based on coherence; that is, all pairs labeled ?yes?
appear to be related in some way.
Because we fix the number of clusters, we are
making the assumption that each reciprocal pair
could be put into one of k groups, which is obviously
an assumption that will not hold true. However, if a
pair does not fit well into any of the clusters, this
should be reflected by a low probability. Thus we
can achieve decently high accuracy if we consider
only the highest-ranked pairs. The accuracy when
considering all pairs is only 34% which means that
34% of reciprocal pairs can be meaningfully placed
into only 6 groups, which is actually fairly high.
A big source of inter-annotator disagreement
comes from the ambiguity of certain verbs, which
is a weakness of our limited representation. For ex-
ample, without additional information it is not clear
how a pair like (know, ask) might relate to others.
5.4 Polarity word identification
For this procedure we used the Subjectivity Clues
(Wilson et al, 2005) which provides 8,220 entries.
From all the 10,882 eventuality pairs, 40.1% of the
total number of words were in the subjectivity lexi-
con, while 36.9% of the pairs had both words in the
subjectivity lexicon.
Table 4 shows all possible combinations of pairs
of affective values and their associated probabilities
in the corpus. These values are computed for those
pairs where both words have known polarity.
As one might expect, each polarity class is most
likely to be reciprocated by itself: Good for Good
(altruism) and Bad for Bad (retaliation). Further-
more, it is more likely that Good follows Bad (?turn
81
eo er eo er eo er eo er eo er eo er
help thank know respect call tell need need love love hate hate
allow thank trust know ask give need trust adore love attack hate
invite thank tell trust tell help want need understand love attack forgive
rescue thank tell know tell tell want trust love adore slap hate
join thank know know contact tell want want teach love hurt attack
inform thank know trust meet hear help need protect love betray punish
join admire know follow follow see offer need feed love kill hate
send thank give let watch send help help challenge love hit curse
support thank let like tell ignore help trust need love treat dislike
teach owe help marry confront tell love need give love ruin shoot
Table 2: The clusters induced after running our unsupervised algorithm with k = 6 clusters. The pairs correspond to a sample of the top 10% of
pairs with the highest value of P (eo, er|cluster) for each cluster.
Good Bad Neutral Total
Good 0.90 0.18 0.29 0.63
Bad 0.09 0.82 0.08 0.29
Neutral 0.01 0.002 0.63 0.09
Table 4: All possible combinations of pairs of affective values and
their associated probabilities as found in the corpus. The numbers in the
table correspond to conditional probabilities P(rowi|colj ). The Total
column indicates the probability of each affective class (P(rowi)).
the other cheek?) than that Bad follows Good.
We experimented with incorporating polarity into
our clustering process. We defined 9 clusters for
each combination of polarity pairs, and initialized
the model by labeling the eventuality pairs where
the polarity of both words was known. We then
ran the EM process on all of the pairs, and since
the model parameters were initialized with these 9
groups, their pairs were more likely to fit into clus-
ters that matched their polarity. We found, how-
ever, that it had trouble clustering the less-common
classes ? essentially, everything but (Good, Good)
and (Bad, Bad). For example, the cluster that was
initialized as (Bad, Good) ended up being dominated
by er = thanks and mostly positive-polarity words
as eo. This seems to be due to the fact that many of
these pairs included er = thanks (often in sarcasm,
as in ?he thanked them for embarrassing him?). But
there are many more words associated with thanks
that are Good, thus those pairs were put into the
same group, and the Good verbs eventually overtook
the cluster. Problems such as this could perhaps be
avoided with more varied labeled data.
We selected a sample of the top 20 pair instances
for each of the 9 clusters of polarity pairs and gave
them to 2 judges who agreed 82% of the time.
6 Discussion and Conclusions
In this paper we presented an analysis of the concept
of reciprocity as expressed in English and a way to
model it. The experimental results provided nice in-
sights into the problem, but can be further improved.
We noticed that the identification of polarity
words is not always enough to capture the affect of
each eventuality. Thus, the text needs to be further
processed to identify speech acts corresponding to
each clause in the reciprocal patterns. For exam-
ple, words such as ?sorry? can be classified as neg-
ative, while the entire clause ?I am sorry? captures
the speech act of APOLOGY which is associated with
good intentions. As future work, we will recluster
the reciprocity pairs.
Another observation concerns the reciprocity
property of magnitude (cf. (Jackendoff, 2005))
or equivalence of value between two eventualities.
Most of the time reciprocal eventualities have the
same or similar magnitude, as the patterns identified
indicate a more or less equivalence of value ? i.e.,
hugs for kisses, thanks for help. And most of these
constructions do not focus so much on the magni-
tude, but on the order in which one eventuality (the
effect) is a reaction to the other (the cause). How-
ever, a closer look at our data shows that there are
also constructions which indicate this property more
precisely. One such example is ?C1 as much as C2?
where even a negation in C1 or C2 might destroy the
magnitude balance (e.g., ?She does not love him as
much as he loves her.?).
We would like to study this property in more de-
tail as well. This kind of study is very important
in the analysis of people?s behavior, judgments, and
thus their social interactions.
82
References
C. Baker, Ch. Fillmore, and J. Lowe. 1998. The Berkeley
FrameNet Project. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics (COLING-ACL 1998), pages 86?90,
Montreal, Canada.
L. Becker, editor. 1990. Reciprocity. University of
Chicago Press, Chicago.
T. Chklovski and P. Pantel. 2004. Verbocean: Mining
the web for fine-grained semantic verb relations. In
Proceedings of the Empirical Methods in Natural Lan-
guage Processing (EMNLP) Conference.
M. Dalrymple, M. Kazanawa, Y. Kim, S. Mchombo,
and S. Peters. 1998. Reciprocal expressions and the
concept of reciprocity. Linguistics and Philosophy,
21:159?210.
D. Davidov and A. Rappoport. 2008. Unsupervised
discovery of generic relationships using pattern clus-
ters and its evaluation by automaticaly generated sat
analogy questions. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics (ACL).
A. P. Dempster, N.M. Laird, and D. B. Rdin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39:1?38.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates. 2004.
Methods for domain-independent information extrac-
tion from the web: An experimental comparison. In
Proceedings of the National Conference on Artificial
Intelligence (AAAI) Conference.
E. Fehr and S. Gachter. 2000. Cooperation and Punish-
ment in Public Goods Experiments. American Eco-
nomic Review, 90:980?994.
K. Gergen, M. Greenberg, and R. Willis, editors. 1980.
Social Exchange: Advances in Theory and Research.
New York: Plenum.
R. Girju. 2009. Reciprocity in language. In Technical
Report. University of Illinois at Urbana-Champaign.
M. Haspelmath. 2007. Further remarks on reciprocal
constructions. In Vladimir P. Nedjalkov, editor, Re-
ciprocal Constructions, pages 2087?2115.
M. Hearst. 1998. Automated Discovery of WordNet Re-
lations. In Christiane Fellbaum, editor, An Electronic
Lexical Database and Some of its Applications, pages
131?151. MIT Press, Cambridge, MA.
I. Heim. 1991. Reciprocity and plurality. Linguistic In-
quiry, 22:63?101.
R. Jackendoff. 2005. The peculiar logic of value. Jour-
nal of Cognition and Culture, 6:375?407.
E. Joanis, S. Stevenson, and D. James. 2008. A general
feature space for automatic verb classification. Natu-
ral Language Engineering, 14(3).
P. Kingsbury, M. Palmer, and M. Marcus. 2002. Adding
Semantic Annotation to the Penn Treebank. In Pro-
ceedings of the 2nd Human Language Technology
Conference (HLT 2002), pages 252?256, San Diego,
California.
K. Kipper, H. Trang Dang, and M. Palmer. 2000. Class-
based construction of a verb lexicon. In Proceedings
of the National Conference on Artificial Intelligence
(AAAI), pages 691?696, Austin, TX.
E. Ko?nig. 2005. Reciprocity in language: Cultural con-
cepts and patterns of encoding. Uhlenbeck Lecture,
23.
E. Maslova and V. Nedjalkov. 2005. Reciprocal con-
structions. In M. Haspelmath, M. Dryer, D. Gill,
and B. Comrie, editors, The World Atlas of Language
Structures, pages 430?433. New York: Oxford Univer-
sity Press.
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell.
2000. Text classification from labeled and unlabeled
documents using EM. Machine Learning, 39:103?
134.
M. Sahlins, editor. 1972. Stone Age Economics.
Chicago: Aldine-Atherton.
P. Turney. 2006. Similarity of semantic relations. Com-
putational Linguistics, 32(3):379?416.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing contextual polarity in phrase-level sentiment anal-
ysis. In Proceedings of the Human Language Technol-
ogy (HLT/EMNLP) Conference.
83
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 490?500,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Document Summarization via Guided Sentence Compression
Chen Li1, Fei Liu2, Fuliang Weng2, Yang Liu1
1 Computer Science Department, The University of Texas at Dallas
Richardson, Texas 75080, USA
2 Research and Technology Center, Robert Bosch LLC
Palo Alto, California 94304, USA
{chenli,yangl@hlt.utdallas.edu}
{fei.liu, fuliang.weng@us.bosch.com}
Abstract
Joint compression and summarization has
been used recently to generate high quality
summaries. However, such word-based joint
optimization is computationally expensive. In
this paper we adopt the ?sentence compression
+ sentence selection? pipeline approach for
compressive summarization, but propose to
perform summary guided compression, rather
than generic sentence-based compression. To
create an annotated corpus, the human anno-
tators were asked to compress sentences while
explicitly given the important summary words
in the sentences. Using this corpus, we train
a supervised sentence compression model us-
ing a set of word-, syntax-, and document-
level features. During summarization, we use
multiple compressed sentences in the inte-
ger linear programming framework to select
salient summary sentences. Our results on the
TAC 2008 and 2011 summarization data sets
show that by incorporating the guided sen-
tence compression model, our summarization
system can yield significant performance gain
as compared to the state-of-the-art.
1 Introduction
Automatic summarization can be broadly divided
into two categories: extractive and abstractive sum-
marization. Extractive summarization focuses on
selecting the salient sentences from the document
collection and concatenating them to form a sum-
mary; while abstractive summarization is generally
considered more difficult, involving sophisticated
techniques for meaning representation, content plan-
ning, surface realization, etc., and the ?true abstrac-
tive summarization remains a researcher?s dream?
(Radev et al, 2002).
There has been a surge of interest in recent
years on generating compressed document sum-
maries as a viable step towards abstractive sum-
marization. These compressive summaries often
contain more information than sentence-based ex-
tractive summaries since they can remove insignif-
icant sentence constituents and make space for more
salient information that is otherwise dropped due to
the summary length constraint. Two general strate-
gies have been used for compressive summarization.
One is a pipeline approach, where sentence-based
extractive summarization is followed or proceeded
by sentence compression (Knight and Marcu, 2000;
Lin, 2003; Zajic et al, 2007; Wang et al, 2013).
Another line of work uses joint compression and
summarization. They have been shown to achieve
promising performance (Daume?, 2006; Martins and
Smith, 2009; Berg-Kirkpatrick et al, 2011; Chali
and Hasan, 2012; Almeida and Martins, 2013; Qian
and Liu, 2013). One popular approach for such joint
compression and summarization is via integer lin-
ear programming (ILP). However, since words are
the units in the optimization framework, solving this
ILP problem can be expensive.
In this study, we use the pipeline compression
and summarization method because of its compu-
tational efficiency. Prior work using such pipeline
methods simply uses generic sentence-based com-
pression for each sentence in the documents, no mat-
ter whether compression is done before or after sum-
mary sentence extraction. We propose to use sum-
490
mary guided compression combined with ILP-based
sentence selection for summarization in this paper.
We create a compression corpus for this purpose.
Using human summaries for a set of documents, we
identify salient words in the sentences. During anno-
tation, the human annotators are given these salient
words and asked to generate compressed sentences.
We expect such ?guided? sentence compression is
beneficial for the pipeline compression and summa-
rization task. In addition, previous research on joint
modeling for compression and summarization sug-
gested that the labeled extraction and compression
data sets would be helpful for learning a better joint
model (Daume?, 2006; Martins and Smith, 2009).
We hope that our work on this guided compression
will also be of benefit to the future joint modeling
studies.
Using our created compression data, we train
a supervised compression model using a variety
of word-, sentence-, and document-level features.
During summarization, we generate multiple com-
pression candidates for each sentence, and use the
ILP framework to select compressed summary sen-
tences. In addition, we also propose to apply a pre-
selection step to select some important sentences,
which can both speed up the summarization system
and improve performance. We evaluate our pro-
posed summarization approach on the TAC 2008
and 2011 data sets using the standard ROUGE met-
ric (Lin, 2004). Our results show that by incorporat-
ing a guided sentence compression model, our sum-
marization system can yield significant performance
gain as compared to the state-of-the-art reported re-
sults.
2 Related Work
Summarization research has seen great development
over the last fifty years (Nenkova and McKeown,
2011). Compared to the abstractive counterpart, ex-
tractive summarization has received considerable at-
tention due to its clear problem formulation ? to ex-
tract a set of salient and non-redundant sentences
from the given document set. Both unsupervised and
supervised approaches have been explored for sen-
tence selection. The supervised approaches include
the Bayesian classifier (Kupiec et al, 1995), max-
imum entropy (Osborne, 2002), skip-chain condi-
tional random fields (CRF) (Galley, 2006), discrim-
inative reranking (Aker et al, 2010), among others.
The extractive summary sentence selection prob-
lem can also be formulated in an optimization
framework. Previous approaches include the inte-
ger linear programming (ILP) and submodular func-
tions, which are used to solve the optimization prob-
lem. In particular, Gillick et al (2009) proposed
a concept-based ILP approach for summarization.
Li et al (2013) improved it by using supervised
stragety to estimate concept weight in ILP frame-
work. In (Lin and Bilmes, 2010), the authors model
the sentence selection problem as maximizing a sub-
modular function under a budget constraint. A
greedy algorithm is proposed to efficiently approxi-
mate the solution to this NP-hard problem.
Compressive summarization receives increasing
attention in recent years, since it offers a viable
step towards abstractive summarization. The com-
pressed summaries can be generated through a joint
model of the sentence selection and compression
processes, or through a pipeline approach that in-
tegrates a generic sentence compression model with
a summary sentence pre-selection or post-selection
step.
Many studies explore the joint sentence compres-
sion and selection setting. Martins and Smith (2009)
jointly perform sentence extraction and compression
by solving an ILP problem; Berg-Kirkpatrick et al
(2011) propose an approach to score the candidate
summaries according to a combined linear model
of extractive sentence selection and compression.
They train the model using a margin-based objec-
tive whose loss captures the final summary qual-
ity. Woodsend and Lapata (2012) present a method
where the summary?s informativeness, succinctness,
and grammaticality are learned separately from data
but optimized jointly using an ILP setup; Yoshikawa
et al (2012) incorporate semantic role information
in the ILP model; Chali and Hasan (2012) investi-
gate three strategies in compressive summarization:
compression before extraction, after extraction, or
joint compression and extraction in one global op-
timization framework. These joint models offer a
promise for high quality summaries, but they often
have high computational cost. Qian and Liu (2013)
propose a graph-cut based method that improves the
speed of joint compression and summarization.
491
The pipeline approach, where sentence-based ex-
tractive summarization is followed or proceeded by
sentence compression, is also popular. Knight and
Marcu (2000) utilize the noisy channel and deci-
sion tree method to perform sentence compression;
Lin (2003) shows that pure syntactic-based com-
pression may not improve the system performance;
Zajic et al (2007) compare two sentence compres-
sion approaches for multi-document summarization,
including a ?parse-and-trim? and a noisy-channel ap-
proach; Galanis and Androutsopoulos (2010) use
the maximum entropy model to generate the candi-
date compressions by removing the branches from
the source sentences; Liu and Liu (2013) couple
the sentence compression and extraction approaches
for summarizing the spoken documents; Wang et al
(2013) design a series of learning-based compres-
sion models built on parse trees, and integrate them
in query-focused multi-document summarization.
Prior studies often rely heavily on the generic sen-
tence compression approaches (McDonald, 2006;
Nomoto, 2007; Clarke and Lapata, 2008; Thadani
and McKeown, 2013) for compressing the sentences
in the documents, yet a generic compression system
may not be the best fit for the summarization pur-
pose.
In this paper, we adopt the pipeline-based com-
pressive summarization framework, but propose a
novel guided compression method that is catered to
the summarization task. We expect this approach
to take advantage of the efficient pipeline process-
ing while producing satisfying results as the joint
models. We train a supervised guided compression
model to produce n-best compressions for each sen-
tence, and use an ILP formulation to select the best
set of summary sentences. In addition, we pro-
pose to apply a sentence pre-selection step to fur-
ther accelerate the processing and enhance the per-
formance.
3 Guided Compression Corpus
The goal of guided sentence compression is to create
compressed sentences that are grammatically cor-
rect and contain the important information that we
would like to preserve in the final summary. Fol-
lowing the compression literature (Clarke and Lap-
ata, 2008), the compression task is defined as a word
Original Sentence:
The gas leak was contained Monday afternoon , nearly 18
hours after it was reported , Statoil spokesman Oeivind
Reinertsen said .
Compression A:
The gas leak was contained
Compression B:
The gas leak was contained Monday afternoon
Compression C:
The gas leak was contained nearly 18 hours after it was
reported
Table 1: Example sentence and three compressions.
deletion problem, that is, the human annotators (and
also automatic compression systems) are allowed to
only remove words from the original sentence to
form a compression. The key difference between
our proposed guided compression with generic sen-
tence compression is that, we provide guidance to
the human compression process by specifying a set
of ?important words? that we wish to keep for each
sentence. We expect this kind of summary oriented
compression would benefit the ultimate summariza-
tion task. Take the sentence shown in Table 1 as an
example. For generic sentence compression, there
may be multiple ?good? human compressions for this
sentence, such as those listed in the table. Without
guidance, a human annotator (or automatic system)
is likely to use option A or B; however, if ?18 hours?
appears in the summary, then we want to provide this
guidance in the compression process, hence option
C may be the best compression choice. This guided
compression therefore avoids removing the salient
words that are important to the final summary.
To generate the guided compression corpus, we
use the TAC 2010 data set1 that was used for
the multi-document summarization task. There are
46 topics. Each has 10 news documents, and
also four human-created abstractive reference sum-
maries. Since annotating all the sentences in this
data set is time consuming and some sentences are
not very important for the summarization task, we
choose a set of sentences that are highly related to
the human abstracts for annotation. We compare
each sentence with the four human abstracts using
the ROUGE-2 metric (Lin, 2004), and the sentences
1http://www.nist.gov/tac/2010/
492
Original Sentence:
He said Vietnam veterans are presumed to have been ex-
posed to Agent Orange and veterans with any of the 10 dis-
eases is presumed to have contracted it from the exposure ,
without individual proof .
Guided Compression:
Vietnam veterans are presumed to have been exposed to
Agent Orange.
Original Sentence:
The province has limited the number of trees to be chopped
down in the forest area in northwest Yunnan and has stopped
building sugar factories in the Xishuangbanna region to
preserve the only tropical rain forest in the country located
there .
Guided Compression:
province has stopped building sugar factories in the
Xishuangbanna region to preserve tropical rain forest.
Table 2: Example original sentences and their guided
compressions. The ?guiding words? are italicized and
marked in red.
with the highest scores are selected.
In annotation, human annotators are provided
with important ?guiding words? (highlighted in the
annotation interface) that we want to preserve in the
sentences. We calculate the word overlap between a
sentence and each of those sentences in the human
abstracts, and use a set of heuristic rules to deter-
mine the ?guiding words? in a sentence: the longest
consecutive word overlaps (greater than 2 words) in
each sentence pair are first selected; the rest overlaps
that contain 2 or more words (excluding the stop-
words) are also selected. We suggest the human an-
notators to use their best judgment to keep the guid-
ing words as many as possible while compressing
the sentence.
We use the Amazon Mechanical Turk (AMT) for
data annotation2. In total, we select 1,150 sentences
from the TAC news documents. They are grouped
into about 230 human intelligence tasks (HITs) with
5 sentences in each HIT. A sentence was compressed
by 3 human annotatorsand we select the shortest
candidate as the goldstandard compression for each
sentence. In Table 2, we show two example sen-
tences, their guiding words (bold), and the human
compressions. The first example shows that giving
up some guiding words is acceptable, since more
2http://www.mturk.com
unnecessary words will be included in order to ac-
commodate all the guiding words; the second ex-
ample shows that the guided compression can lead
to more aggressive word deletions since the con-
stituents that are not important to the summary will
be deleted even though they contain salient informa-
tion by themselves.
For our compression corpus, which contains
1,150 sentences and their guided compressions, the
average compression rate, as measured by the per-
centage of dropped words, is about 50%. This com-
pression ratio is higher compared to other generic
sentence compression corpora, in which the word
deletion rate ranges from 24% to 34% depending
on different text genres and annotation guidelines
(Clarke and Lapata, 2008; Liu and Liu, 2009). This
suggests that the annotators can remove words more
aggressively when they are provided with a limited
set of guiding words.
4 Summarization System
Our summarization system consists of three key
components: we train a supervised guided compres-
sion model using our created compression data, with
a variety of features.then we use this model to gener-
ate n-best compressions for each sentence; we feed
the multiple compressed sentences to the ILP frame-
work to select the best summary sentences. In ad-
dition, we propose a sentence pre-selection step that
can both speed up the summarization system and im-
prove the performance.
4.1 Guided Sentence Compression
Sentence compression has been explored in previous
studies using both supervised and unsupervised ap-
proaches, including the noisy-channel and decision
tree model (Knight and Marcu, 2000; Turner and
Charniak, 2005), discriminative learning (McDon-
ald, 2006), integer linear programming (Clarke and
Lapata, 2008; Thadani and McKeown, 2013), con-
ditional random fields (CRF) (Nomoto, 2007; Liu
and Liu, 2013), etc. In this paper, we employ the
CRF-based compression approach due to its proved
performance and its flexibility to integrate differ-
ent levels of discriminative features. Under this
framework, sentence compression is formulated as
a sequence labeling problem, where each word is
493
labeled as either ?0? (retained) or ?1? (removed).
We develop different levels of features to capture
word-specific characteristics, sentence related infor-
mation, and document level importance. Most of the
features are extracted based only on the sentence to
be compressed. However, we introduce a few doc-
ument level features. These are designed to cap-
ture the word and sentence significance within the
given document collection and are thus expected to
be more summary related.
Word and sentence features:
? Word n-grams: identity of the current word
and two words before and after, as well as all
the bigrams and trigrams that can be formed by
the adjacent words and the current word.
? POS n-grams: same as the word n-grams, but
use the part-of-speech tags instead.
? Named entity tags: binary features represent-
ing whether the current word is a person, loca-
tion, or temporal expression. We use the Stan-
ford CoreNLP tools3 for named entity tagging.
? Stopwords: whether the current word is a stop-
word or not.
? Conjunction features: (1) conjunction of the
current word with its relative position in the
sentence; (2) conjunction of the NER tag with
its relative position.
? Syntactic features: We obtain the syntactic
parsing tree using the Berkeley Parser (Petrov
and Klein, 2007), then obtain the following fea-
tures: (1) the last sentence constituent tag in
the path from the root to the word; (2) depth:
length of the path starting from the root node
to the word; (3) normalized depth: depth di-
vided by the longest path in the parsing tree;
(4) whether the word is under an SBAR node;
(5) depth and normalized depth of the SBAR
node if the word is under an SBAR node;
? Dependency features: We employ the
Penn2Malt toolkit 4 to convert the parse re-
sult from the Berkeley parser to the depen-
dency parsing tree, and use these dependency
3http://nlp.stanford.edu/software/corenlp.shtml
4http://stp.lingfil.uu.se/?nivre/research/Penn2Malt.html
features: (1) dependency relations such as
?AMOD? (adjective modifier), ?NMOD? (noun
modifier), etc. (2) whether the word has a child,
left child, or right child in the dependency tree.
Document-level features:
? Sentence salience score: We use a simple re-
gression model to estimate a salience score for
each sentence (more details in Section 4.3),
which represents the importance of the sen-
tence in the document. This score is discretized
into four binary features according to the aver-
age sentence salience.
? Unigram document frequency: this is the
current word?s document frequency based on
the 10 documents associated with each topic.
? Bigram document frequency: document fre-
quency for the two bigrams, the current word
and its previous or next word.
Some of the above features were employed in re-
lated sentence compression studies (Nomoto, 2007;
Liu and Liu, 2013). In addition to these features, we
explored other related features, including the abso-
lute position of the current word, whether the word
appears in the corresponding topic title and descrip-
tions, conjunction of the syntactic tag with the tree
depth, etc.; however, these features did not lead to
improved performance. We train the CRF model
with the Pocket CRF toolkit5 using the guided com-
pression corpus collected in Section 3. During sum-
marization, we apply the model to a given sentence
to generate its n-best guided compressions and use
them in the following summarization step.
4.2 Summary Sentence Selection
The sentence selection process is similar to the stan-
dard sentence-based extractive summarization, ex-
cept that the input to the selection module is a list
of compressed sentences in our work. Many extrac-
tive summarization approaches can be applied for
this purpose. In this work, we choose the integer
linear programming (ILP) method, specifically, the
concept-based ILP framework introduced in (Gillick
5http://sourceforge.net/projects/pocket-crf-1/
494
et al, 2009), mainly because it yields best perfor-
mance in the TAC evaluation tasks. This ILP ap-
proach aims to extract sentences that can cover as
many important concepts as possible, while ensuring
the summary length is within a given constraint. We
follow the study in (Gillick et al, 2009) to use word
bi-grams as concepts, and assign a weight to each
bi-gram using its document frequency in the given
document collection for a test topic. Two differences
are between our ILP setup and that in (Gillick et al,
2009). First, since we use multiple compressions
for one sentence, we need to introduce an additional
constraint: for each sentence, only one of the n-best
compressions may be included in the summary. Sec-
ond, we optimize a joint score of the concept cover-
age and the sentence salience. The formal ILP for-
mulation is shown below:
max
?
i
wici +
?
j
vj
?
k
sjk (1)
s.t.
?
k
sjk ? 1?j (2)
sjkOcci jk ? ci (3)
?
jk
sjkOcci jk ? ci (4)
?
jk
ljksjk ? L (5)
ci ? {0, 1} ?i (6)
sjk ? {0, 1} ?j, k (7)
where ci and sjk are binary variables indicating the
presence of a concept and a sentence respectively;
sjk denotes the kth candidate compression of the
jth sentence; wi represents the weight of the con-
cept; vj is the sentence salience score of the jth
sentence, predicted using a regression model (Sec-
tion 4.3), and all of its compressed candidates share
this value. (1) is the new objective function we use
that combines the coverage of the concepts and the
sentence salience scores. (2) represents our addi-
tional constraint, which requires that for each sen-
tence j, only one candidate compression will be cho-
sen. Occi jk represents the occurrence of concept i
in the sentence sjk. Inequalities (3) and (4) associate
the sentences and the concepts. Constraint (5) con-
trols the summary length, as measured by the total
number of words in the summary. We use an open
source ILP solver6.
4.3 Sentence Pre-selection
The above ILP method can offer an exact solution
to the defined objective function. However, ILP is
computationally expensive when the formulation in-
volves large quantities of variables, i.e, when we
have many sentences and a large number of candi-
date compressions for each sentence. We therefore
propose to apply a sentence pre-selection step be-
fore the compression. This kind of selection step
has been used in previous ILP-based summarization
systems (Berg-Kirkpatrick et al, 2011; Gillick et al,
2009). In this work, we propose to use a simple su-
pervised support vector regression (SVR) model (Ng
et al, 2012) to predict a salience score for each sen-
tence and select the top ranked sentences for further
processing (compression and summarization).
To train the SVR model, the target value for each
sentence is the ROUGE-2 score between the sen-
tence and the four human abstracts (this same value
is used for sentence selection in corpus annotation
(Section 3)). We employ three commonly used fea-
tures: (1) sentence position in the document; (2) sen-
tence length as indicated by a binary feature: it takes
the value of 0 if the number of words in the sentence
is greater than 50 or less than 10, otherwise the fea-
ture value is 1; (3) interpolated n-gram document
frequency as introduced in (Ng et al, 2012), which
is a weighted linear combination of the document
frequency of the unigrams and bigrams contained in
the sentence:
f(s) =
?
?
wu?S
DF (wu) + (1? ?)
?
wb?S
DF (wb)
|S|
where wu and wb represent the unigrams and bi-
grams contained in the sentence S; ? is a balancing
factor; |S| denotes the number of words in the sen-
tence.
The SVR model was trained using the SVMlight
toolkit7. Using this model, we can predict a salience
score (Vj in Eq 1) for each sentence and only select
the top n sentences and supply them to the compres-
sion and summarization steps. In practice, using a
fixed n may not be a good choice since the number
6http://www.gnu.org/software/glpk/
7http://svmlight.joachims.org/
495
of sentences varies greatly for different topics. We
therefore set n heuristically based on the total num-
ber of sentencesm for each topic: n=15 ifm > 150;
n=10 if m < 100; n=0.1 ?m otherwise.
5 Experimental Results
5.1 Experimental Setup
For our experiments, we use the standard TAC data
sets8, which have been used in the NIST competi-
tions and in other summarization studies. In par-
ticular, we used the TAC 2010 data set for creating
the guided compression corpus and training the SVR
pre-selection model, the TAC 2009 data set as devel-
opment set for parameter tuning, and the TAC 2008
and 2011 data sets as the test set for reporting the
final summarization results.
We compare our pipeline summarization sys-
tem against three recent studies, which have re-
ported some of the highest published results on this
task. Berg-Kirkpatrick et al (2011) introduce a
joint model for sentence extraction and compres-
sion. The model is trained using a margin-based ob-
jective whose loss captures the end summary qual-
ity; Woodsend and Lapata (2012) learn individ-
ual summary aspects from data, e.g., informative-
ness, succinctness, grammaticality, stylistic writ-
ing conventions, and jointly optimize the outcome
in an integer linear programming framework. Ng
et al (2012) exploit category-specific information
for multi-document summarization. In addition to
the three previous studies, we also report the best
achieved results in the TAC competitions.
5.2 Summarization Results
In Table 3 and Table 4, we present the results of our
system and the aforementioned summarization stud-
ies. We use the ROUGE evaluation metrics (Lin,
2004), with R-2 measuring the bigram overlap be-
tween the system and reference summaries and R-
SU4 measuring the skip-bigram with the maximum
gap length of 4. ?Our System? uses the pipeline
setting including the three components described in
Section 4. We use the SVR-based approach to pre-
select a set of sentences from the document set; these
sentences are further fed to the guided compression
module that produces n-best compressions for each
8http://www.nist.gov/tac/data/index.html
System R-2 R-SU4 CompR
TAC?08 Best System 11.03 13.96 n/a
(Berg-Kirkpatrick et al, 2011) 11.70 14.38 n/a
(Woodsend et al, 2012) 11.37 14.47 n/a
Our System 12.35? 15.27? 43.06%
Our System w/o Pre-selection 12.02 14.98 55.69%
Our System w/ Generic Comp 10.88 13.79 30.90%
Table 3: Results on the TAC 2008 data set. ?Our Sys-
tem? uses the SVR-based sentence pre-selection + guided
compression + ILP-based summary sentence selection.
?Our System w/ Generic Comp? uses the pre-selection +
generic compression + ILP summary sentence selection
setting. ?CompR? represents the compression ratio, i.e.,
percentage of dropped words. ? represents our system
outperforms the best previous result at the 95% signifi-
cance level.
System R-2 R-SU4 CompR
TAC?11 Best System 13.44 16.51 n/a
(Ng et al, 2012) 13.93 16.83 n/a
Our System 14.40 16.89 39.90%
Our System w/o Pre-selection 13.74 16.5 53.81%
Our System w/ Generic Comp 13.08 16.23 30.10%
Table 4: Results on the TAC 2011 data set. The systems
use the same settings as for the TAC 2008 data set.
sentence; the ILP-based framework is then used to
select the summary sentences from these compres-
sions.
We can see from the table that in general, our sys-
tem achieves considerably better results compared to
the state-of-the-art on both the TAC 2008 and 2011
data sets. On the TAC 2008 data set, our system out-
performs the best reported result at the 95% signifi-
cance level; on the TAC 2011 data set, our system
also yields considerable performance gain though
not exceed the 95% significance level. In the fol-
lowing, we show more detailed analysis to study the
effect of different system parameters.
With or without sentence pre-selection. First
we evaluate the impact of sentence pre-selection
step. In Table 3 and Table 4, we include the
results when this step is not used (?Our System
w/o Pre-selection?). That is, all of the sentences
in the documents (excluding those containing less
than 5 words) are compressed and used in the ILP-
496
based summary sentence selection module. We can
see that although sentence pre-selection removes
some sentences from consideration in the later sum-
marization step, it actually significantly improves
system performance. In the TAC 2008 data set,
each topic contains averagely 210 sentences; while
the pre-selection step chooses 13 sentences among
them. These numbers are 185 and 12 for the TAC
2011 data set. Table 5 shows the average running
time of each topic in TAC 2011 data for the two sys-
tems, with or without the pre-selection step. Here
we fix the number of compressions to 100 in both
cases for fair comparison. We can see the selec-
tion step greatly accelerates the system processing.
When applying the pre-selection step, fewer sen-
tences are used in the compression and summariza-
tion, this means we are able to use more compres-
sion candidates for each sentence (considering the
complexity of ILP module). Using the TAC 2009
as development set, we tuned the number of can-
didate compressions generated for each sentence.
Without pre-selection, we used the 100-best candi-
dates generated from the compression model; with
pre-selection, we are able to increase the number
to 200-best candidate compressions and still main-
tain reasonable computational cost. These are the
numbers used in the results in Table 3 and 4. Us-
ing more compressions helps improve summariza-
tion performance. We also notice that the compres-
sion ratios are quite different when using sentence
pre-selection vs. not. This suggests that in the im-
portant sentences (those are kept after pre-selection),
there is more summary related information and thus
the compression model keeps more words in them
(lower compression ratio).
System
Compressed Number of Running
Sentences Compressions Time (sec)
w/o Pre-selection 185 100 3.9
w/ Pre-selection 12 100 0.85
Table 5: Average running time of our system, w/ or w/o
the sentence pre-selection step. Experiments conducted
on the TAC 2011 data set. Running time refers only to
the execution time of the ILP module for each topic.
Number of compression candidates. This pa-
rameter (denoted as n) also impacts system perfor-
mance. Figure 1 shows the R-2 scores of the two
systems (with and without the sentence pre-selection
step) when using different number of compressions
for each sentence. In general, we find that the R-2
scores do not change much when n is large enough.
For example, the ?with pre-selection? system can
achieve relatively stable R-2 scores on the TAC 2008
data set (ranging from 12.2 to 12.4) when m is
greater than 140; similarly, the R-2 scores on the
TAC 2011 data is over 14.2 when m is greater than
100. Without the pre-selection step, the scores are
less stable in regard to the changing of the m value,
since the large amount of sentences plus a high vol-
ume of the compression candidates may incur huge
computational cost to the ILP solver. This is also the
reason that in Figure 1, for the system without pre-
selection, we only vary n from 1 to 100. In general,
we also notice that given more compression candi-
dates, the R-2 score is still improving, as indicated
by Figure 1. The improved performance of ?with
pre-selection? over ?without pre-selection? is partly
because fewer sentences are used and thus we are
able to increase the number of compression candi-
dates for these sentences in the ILP sentence extrac-
tion module.
Quality of sentence compression training data.
In order to illustrate the contribution of our
summary-guided sentence compression component,
we train a generic sentence compression model
and use this in our compression and summariza-
tion pipeline. The generic compression model was
trained using the Edinburgh sentence compression
corpus (Clarke and Lapata, 2008), which contains
1370 sentences collected from news articles. This
data set has been widely used in other summariza-
tion studies (Martins and Smith, 2009). Each sen-
tence has 3 compressions and we choose the short-
est compression as the reference. The average com-
pression rate of this corpus is about 28%, lower than
that in our summary guided compression data. Note
that in generic sentence compression, we only use
those word and sentence features described in Sec-
tion 4.1, not the document-level features since they
are not available for the Edinburgh data set. Results
of our system using the generic compression model
(with sentence pre-selection) are shown in the last
row of Table 3 and Table 4. We can see that the sys-
tem with this generic compression model performs
497
 11
 11.5
 12
 12.5
 13
 13.5
 14
 14.5
 15
 0  20  40  60  80  100
RO
UG
E-
2
# Compression Candidates
TAC 2011
TAC 2008
 11
 11.5
 12
 12.5
 13
 13.5
 14
 14.5
 15
 0  50  100  150  200  250
RO
UG
E-
2
# Compression Candidates
TAC 2011
TAC 2008
 11
 11.5
 12
 12.5
 13
 13.5
 14
 14.5
 15
 0  20  40  60  80  100
RO
UG
E-
2
# Compression Candidates
TAC 2011
TAC 2008
 11
 11.5
 12
 12.5
 13
 13.5
 14
 14.5
 15
 0  50  100  150  200  250
RO
UG
E-
2
# Compression Candidates
TAC 2011
TAC 2008
Figure 1: R-2 scores of the two systems (without and
with the sentence pre-selection step) when using differ-
ent number of compressions for each sentence.
worse than ours, and is also inferior to the TAC best
performing system on both data sets, which signi-
fies the importance of our proposed summary guided
sentence compression approach. We can also see
there is a difference in the compression ratio in the
system generated compressions when using differ-
ent compression corpora to train the compression
models. The resulting compression ratio patterns are
consistent with those in the training data, that is, us-
ing our guided compression corpus our system com-
pressed sentences more aggressively.
Learning curve of guided compression. Since
we use a supervised compression model, we further
consider the relationship between the summarization
performance and the number of sentence pairs used
for training the guided compression model. In to-
tal, there are 1150 training sentence pairs in our cor-
pus. We incrementally add 100 sentence pairs each
time and plot the learning curve in Figure 2. In
the compression step, we generate only the 1-best
compression candidate in order to remove the im-
pact caused by the downstream summary sentence
selection module. As seen from Figure 2, increasing
the compression training data generally improves
summarization performance, although there are also
fluctuations. When adding more training sentence
pairs, the system performance is likely to further in-
crease.
 10.5
 11
 11.5
 12
 12.5
 200  400  600  800 1000 1200
RO
UG
E-
2
# Sentence Pairs in the Training Set
TAC 2011
TAC 2008
Figure 2: ROUGE-2 scores when using different number
of sentences to train the guided compression model.
6 Conclusion and Future Work
In this paper, we propose a pipeline summariza-
tion approach that combines a novel guided com-
pression model with ILP-based summary sentence
selection. We create a guided compression cor-
pus, where the human annotators were explicitly in-
formed about the important summary words during
the compression annotation. We then train a super-
vised compression model to capture the guided com-
pression process using a set of word-, sentence-, and
document-level features. We conduct experiments
on the TAC 2008 and 2011 summarization data sets
and show that by incorporating the guided sentence
compression model, our summarization system can
yield significant performance gain as compared to
the state-of-the-art. In future, we would like to
further explore the reinforcement relationship be-
tween keywords and summaries (Wan et al, 2007),
improve the readability of the sentences generated
from the guided compression system, and report re-
sults using multiple evaluation metrics (Nenkova et
al., 2007; Louis and Nenkova, 2012) as well as per-
forming human evaluations.
498
Acknowledgments
Part of this work was done during the first au-
thor?s internship in Bosch Research and Technol-
ogy Center. The work is also partially supported
by NSF award IIS-0845484 and DARPA Contract
No. FA8750-13-2-0041. Any opinions, findings,
and conclusions or recommendations expressed are
those of the author and do not necessarily reflect the
views of the funding agencies.
References
Ahmet Aker, Trevor Cohn, and Robert Gaizauskas. 2010.
Multi-document summarization using a* search and
discriminative training. In Proceedings of EMNLP.
Miguel B. Almeida and Andre F. T. Martins. 2013. Fast
and robust compressive summarization with dual de-
composition and multi-task learning. In Proceedings
of ACL.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL.
Yllias Chali and Sadid A. Hasan. 2012. On the effective-
ness of using sentence compression models for query-
focused multi-document summarization. In Proceed-
ings of COLING.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression an integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research.
Hal Daume?. 2006. Practical structured learning tech-
niques for natural language processing. Ph.D. thesis,
University of Southern California.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance. In
Proceedings of EMNLP.
Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, Berndt
Bohnet, Yang Liu, and Shasha Xie. 2009. The icsi/utd
summarization system at tac 2009. In Proceedings of
TAC.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization - step one: Sentence compression.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings of
SIGIR.
Chen Li, Xian Qian, and Yang Liu. 2013. Using super-
vised bigram-based ilp for extractive summarization.
In Proceedings of ACL.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submodular
functions. In Proceedings of NAACL.
Chin-Yew Lin. 2003. Improving summarization perfor-
mance by sentence compression - A pilot study. In
Proceeding of the Sixth International Workshop on In-
formation Retrieval with Asian Language.
Chin-Yew Lin. 2004. Rouge: a package for automatic
evaluation of summaries. In Proceedings of ACL.
Fei Liu and Yang Liu. 2009. From extractive to abstrac-
tive meeting summaries: Can it be done by sentence
compression? In Proceedings of ACL-IJCNLP.
Fei Liu and Yang Liu. 2013. Towards abstractive speech
summarization: Exploring unsupervised and super-
vised approaches for spoken utterance compression.
IEEE Transactions on Audio, Speech, and Language
Processing.
Annie Louis and Ani Nenkova. 2012. Automati-
cally assessing machine summary content with a gold-
standard. Computational Linguistics.
Andre F. T. Martins and Noah A. Smith. 2009. Summa-
rization with a joint model for sentence extraction and
compression. In Proceedings of the ACL Workshop
on Integer Linear Programming for Natural Language
Processing.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL.
Ani Nenkova and Kathleen McKeown. 2011. Automatic
summarization. Foundations and Trends in Informa-
tion Retrieval.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The pyramid method: Incorporating hu-
man content selection variation in summarization eval-
uation. ACM Transactions on Speech and Language
Processing.
Jun-Ping Ng, Praveen Bysani, Ziheng Lin, Min-Yen Kan,
and Chew-Lim Tan. 2012. Exploiting category-
specific information for multi-document summariza-
tion. In Proceedings of COLING.
Tadashi Nomoto. 2007. Discriminative sentence com-
pression with conditional random fields. Information
Processing and Management.
Miles Osborne. 2002. Using maximum entropy for sen-
tence extraction. In Proceedings of the ACL-02 Work-
shop on Automatic Summarization.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL.
499
Xian Qian and Yang Liu. 2013. Fast joint compression
and summarization via graph cuts. In Proceedings of
EMNLP.
Dragomir R. Radev, Eduard Hovy, and Kathleen McKe-
own. 2002. Introduction to the special issue on sum-
marization. In Computational Linguistics.
Kapil Thadani and Kathleen McKeown. 2013. Sentence
compression with joint structural inference. In Pro-
ceedings of CoNLL.
Jenine Turner and Eugene Charniak. 2005. Supervised
and unsupervised learning for sentence compression.
In Proceedings of ACL.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. To-
wards an iterative reinforcement approach for simulta-
neous document summarization and keyword extrac-
tion. In Proceedings of ACL.
Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-
rian, and Claire Cardie. 2013. A sentence com-
pression based framework to query-focused multi-
document summarization. In Proceedings of ACL.
Kristian Woodsend and Mirella Lapata. 2012. Multiple
aspect summarization using integer linear program-
ming. In Proceedings of EMNLP-CoNLL.
Katsumasa Yoshikawa, Tsutomu Hirao, Ryu Iida, and
Manabu Okumura. 2012. Sentence compression with
semantic role constraints. In Proceedings of ACL.
David Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization
tasks. In Information Processing and Management.
500
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 691?701,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Improving Multi-documents Summarization by Sentence Compression
based on Expanded Constituent Parse Trees
Chen Li1, Yang Liu1, Fei Liu2, Lin Zhao3, Fuliang Weng3
1 Computer Science Department, The University of Texas at Dallas
Richardson, TX 75080, USA
2 School of Computer Science, Carnegie Mellon University
Pittsburgh, PA 15213, USA
3 Research and Technology Center, Robert Bosch LLC
Palo Alto, California 94304, USA
{chenli,yangl@hlt.utdallas.edu}
{feiliu@cs.cmu.edu}
{lin.zhao,fuliang.weng@us.bosch.com}
Abstract
In this paper, we focus on the problem
of using sentence compression techniques
to improve multi-document summariza-
tion. We propose an innovative sentence
compression method by considering every
node in the constituent parse tree and de-
ciding its status ? remove or retain. In-
teger liner programming with discrimina-
tive training is used to solve the problem.
Under this model, we incorporate various
constraints to improve the linguistic qual-
ity of the compressed sentences. Then we
utilize a pipeline summarization frame-
work where sentences are first compressed
by our proposed compression model to ob-
tain top-n candidates and then a sentence
selection module is used to generate the
final summary. Compared with state-of-
the-art algorithms, our model has simi-
lar ROUGE-2 scores but better linguistic
quality on TAC data.
1 Introduction
Automatic summarization can be broadly divided
into two categories: extractive and abstractive
summarization. Extractive summarization focuses
on selecting salient sentences from the document
collection and concatenating them to form a sum-
mary; while abstractive summarization is gener-
ally considered more difficult, involving sophisti-
cated techniques for meaning representation, con-
tent planning, surface realization, etc.
There has been a surge of interest in recent years
on generating compressed document summaries as
a viable step towards abstractive summarization.
These compressive summaries often contain more
information than sentence-based extractive sum-
maries since they can remove insignificant sen-
tence constituents and make space for more salient
information that is otherwise dropped due to the
summary length constraint. Two general strate-
gies have been used for compressive summariza-
tion. One is a pipeline approach, where sentence-
based extractive summarization is followed or pro-
ceeded by sentence compression (Lin, 2003; Zajic
et al., 2007; Vanderwende et al., 2007; Wang et al.,
2013). Another line of work uses joint compres-
sion and summarization. Such methods have been
shown to achieve promising performance (Daume?,
2006; Chali and Hasan, 2012; Almeida and Mar-
tins, 2013; Qian and Liu, 2013), but they are typi-
cally computationally expensive.
In this study, we propose an innovative sen-
tence compression model based on expanded con-
stituent parse trees. Our model uses integer lin-
ear programming (ILP) to search the entire space
of compression, and is discriminatively trained.
It is built based on the discriminative sentence
compression model from (McDonald, 2006) and
(Clarke and Lapata, 2008), but our method uses
an expanded constituent parse tree rather than only
the leaf nodes in previous work. Therefore we
can extract rich features for every node in the con-
stituent parser tree. This is an advantage of tree-
based compression technique (Knight and Marcu,
2000; Galley and McKeown, 2007; Wang et al.,
2013). Similar to (Li et al., 2013a), we use a
pipeline summarization framework where multi-
ple compression candidates are generated for each
pre-selected important sentence, and then an ILP-
691
based summarization model is used to select the
final compressed sentences. We evaluate our pro-
posed method on the TAC 2008 and 2011 data
sets using the standard ROUGE metric (Lin, 2004)
and human evaluation of the linguistic quality.
Our results show that using our proposed sentence
compression model in the summarization system
can yield significant performance gain in linguis-
tic quality, without losing much performance on
the ROUGE metric.
2 Related Work
Summarization research has seen great develop-
ment over the last fifty years (Nenkova and McKe-
own, 2011). Compared to the abstractive counter-
part, extractive summarization has received con-
siderable attention due to its clear problem for-
mulation: to extract a set of salient and non-
redundant sentences from the given document
set. Both unsupervised and supervised approaches
have been explored for sentence selection. Su-
pervised approaches include the Bayesian classi-
fier (Kupiec et al., 1995), maximum entropy (Os-
borne, 2002), skip-chain CRF (Galley, 2006), dis-
criminative reranking (Aker et al., 2010), among
others. The extractive summary sentence selec-
tion problem can also be formulated in an opti-
mization framework. Previous methods include
using integer linear programming (ILP) and sub-
modular functions to solve the optimization prob-
lem (Gillick et al., 2009; Li et al., 2013b; Lin and
Bilmes, 2010).
Compressive summarization receives increas-
ing attention in recent years, since it offers a vi-
able step towards abstractive summarization. The
compressed summaries can be generated through a
joint model of the sentence selection and compres-
sion processes, or through a pipeline approach that
integrates a sentence compression model with a
summary sentence pre-selection or post-selection
step.
Many studies have explored the joint sentence
compression and selection setting. Martins and
Smith (2009) jointly performed sentence extrac-
tion and compression by solving an ILP prob-
lem. Berg-Kirkpatrick et al. (2011) proposed an
approach to score the candidate summaries ac-
cording to a combined linear model of extrac-
tive sentence selection and compression. They
trained the model using a margin-based objec-
tive whose loss captures the final summary qual-
ity. Woodsend and Lapata (2012) presented an-
other method where the summary?s informative-
ness, succinctness, and grammaticality are learned
separately from data but optimized jointly using an
ILP setup. Yoshikawa et al. (2012) incorporated
semantic role information in the ILP model.
Our work is closely related with the pipeline
approach, where sentence-based extractive sum-
marization is followed or proceeded by sentence
compression. There have been many studies on
sentence compression, independent of the summa-
rization task. McDonald (2006) firstly introduced
a discriminative sentence compression model to
directly optimize the quality of the compressed
sentences produced. Clarke and Lapata (2008)
improved the above discriminative model by us-
ing ILP in decoding, making it convenient to
add constraints to preserve grammatical structure.
Nomoto (2007) treated the compression task as
a sequence labeling problem and used CRF for
it. Thadani and McKeown (2013) presented an
approach for discriminative sentence compression
that jointly produces sequential and syntactic rep-
resentations for output text. Filippova and Altun
(2013) presented a method to automatically build
a sentence compression corpus with hundreds of
thousands of instances on which deletion-based
compression algorithms can be trained.
In addition to the work on sentence compres-
sion as a stand-alone task, prior studies have also
investigated compression for the summarization
task. Knight and Marcu (2000) utilized the noisy
channel and decision tree method to perform sen-
tence compression in the summarization task. Lin
(2003) showed that pure syntactic-based compres-
sion may not significantly improve the summariza-
tion performance. Zajic et al. (2007) compared
two sentence compression approaches for multi-
document summarization, including a ?parse-and-
trim? and a noisy-channel approach. Galanis and
Androutsopoulos (2010) used the maximum en-
tropy model to generate the candidate compres-
sions by removing branches from the source sen-
tences. Woodsend and Lapata (2010) presented a
joint content selection and compression model for
single-document summarization. They operated
over a phrase-based representation of the source
document which they obtained by merging infor-
mation from PCFG parse trees and dependency
graphs. Liu and Liu (2013) adopted the CRF-
based sentence compression approach for summa-
692
rizing spoken documents. Unlike the word-based
operation, some of these models e.g (Knight and
Marcu, 2000; Siddharthan et al., 2004; Turner
and Charniak, 2005; Galanis and Androutsopou-
los, 2010; Wang et al., 2013), are tree-based ap-
proaches that operate on the parse trees and thus
the compression decision can be made for a con-
stituent, instead of a single word.
3 Sentence Compression Method
Sentence compression is a task of producing a
summary for a single sentence. The compressed
sentence should be shorter, contain important con-
tent from the original sentence, and be grammat-
ical. In some sense, sentence compression can
be described as a ?scaled down version of the
text summarization problem? (Knight and Marcu,
2002). Here similar to much previous work on
sentence compression, we just focus on how to re-
move/select words in the original sentence without
using operation like rewriting sentence.
3.1 Discriminative Compression Model by
ILP
McDonald (2006) presented a discriminative com-
pression model, and Clarke and Lapata (2008) im-
proved it by using ILP for decoding. Since our
proposed method is based upon this model, in
the following we briefly describe it first. Details
can be found in (Clarke and Lapata, 2008). In
this model, the following score function is used
to evaluate each compression candidate:
s(x, y) =
|y|
?
j=2
s(x, L(y
j?1
), L(y
j
)) (1)
where x = x
1
x
2
, ..., x
n
represents an original sen-
tence and y = y
1
y
2
, ..., y
m
denotes a compressed
sentence. Because the sentence compression prob-
lem is defined as a word deletion task, y
j
must oc-
cur in x. Function L(y
i
) ? [1...n] maps word y
i
in
the compression to the word index in the original
sentence x. Note that L(y
i
) < L(y
i+1
) is required,
that is, each word in x can only occur at most
once in compression y. In this model, a first or-
der Markov assumption is used for the score func-
tion. Decoding this model is to find the combina-
tion of bigrams that maximizes the score function
in Eq (1). Clarke and Lapata (2008) introduced the
following variables and used ILP to solve it:
?
i
=
{
1 if x
i
is in the compression
0 otherwise
?i ? [1..n]
?
i
=
{
1 if x
i
starts the compression
0 otherwise
?i ? [1..n]
?
i
=
{
1 if x
i
ends the compression
0 otherwise
?i ? [1..n]
?
ij
=
{
1 if x
i
, x
j
are in the compression
0 otherwise
?i ? [1..n ? 1]?j ? [i + 1..n]
Using these variables, the objective function can
be defined as:
max z =
n
?
i=1
?
i
? s(x, 0, i)
+
n?1
?
i=1
n
?
j=i+1
?
ij
? s(x, i, j)
+
n
?
i=1
?
i
? s(x, i, n + 1) (2)
The following four basic constraints are used to
make the compressed result reasonable:
n
?
i=1
?
i
= 1 (3)
?
j
? ?
j
?
j
?
i=1
?
ij
= 0 ?j ? [1..n] (4)
?
i
?
n
?
j=i+1
?
ij
? ?
i
= 0 ?i ? [1..n] (5)
n
?
i=1
?
i
= 1 (6)
Formula (3) and (6) denote that exactly one
word can begin or end a sentence. Formula (4)
means if a word is in the compressed sentence, it
must either start the compression or follow another
word; formula (5) represents if a word is in the
693
compressed sentence, it must either end the sen-
tence or be followed by another word.
Furthermore, discriminative models are used for
the score function:
s(x, y) =
|y|
?
j=2
w ? f(x, L(y
j?1
), L(y
j
)) (7)
High dimensional features are used and their cor-
responding weights are trained discriminatively.
Above is the basic supervised ILP formula-
tion for sentence compression. Linguistically and
semantically motivated constraints can be added
in the ILP model to ensure the correct grammar
structure in the compressed sentence. For exam-
ple, Clarke and Lapata (2008) forced the introduc-
ing term of prepositional phrases and subordinate
clauses to be included in the compression if any
word from within that syntactic constituent is also
included, and vice versa.
3.2 Compression Model based on Expanded
Constituent Parse Tree
In the above ILP model, variables are defined for
each word in the sentence, and the task is to pre-
dict each word?s status. In this paper, we propose
to adopt the above ILP framework, but operate di-
rectly on the nodes in the constituent parse tree,
rather than just the words (leaf nodes in the tree).
This way we can remove or retain a chunk of the
sentence rather than isolated words, which we ex-
pect can improve the readability and grammar cor-
rectness of the compressed sentences.
The top part of Fig1 is a standard constituent
parse tree. For some levels of the tree, the nodes
at that same level can not represent a sentence. We
extend the parse tree by duplicating non-POS con-
stituents so that leaf nodes (words and their corre-
sponding POS tags) are aligned at the bottom level
as shown in bottom of as Fig1. In the example tree,
the solid lines represent relationship of nodes from
the original parse tree, the long dot lines denote the
extension of the duplication nodes from the up-
per level to the lower level, and the nodes at the
same level are connected (arrowed lines) to repre-
sent that is a sequence. Based on this expanded
constituent parse tree, we can consider every level
as a ?sentence? and the tokens are POS tags and
parse tree labels. We apply the above compression
model in Section 3.1 on every level to decide every
node?s status in the final compressed sentence. In
order to make the compressed parsed tree reason-
able, we model the relationship of nodes between
PRP/
I 
VBP/ 
am 
DT/  
a 
NN/ 
worker 
IN/ 
from 
NNP/
USA
NP IN 
PRP 
PRP 
PRP 
DT/ 
the
 
NNP/
USA
 
IN/ 
from 
NN/ 
worker 
PP 
NP 
PRP/ 
I 
DT/ 
the
S 
VP NP 
VBP NP 
NP PP 
DT NN 
VBP 
VBP 
S 
VP NP 
VBP/ 
am 
NP 
NP 
DT/ 
a 
Figure 1: A regular constituent parse tree and its
Expanded constituent tree.
adjacent levels as following: if the parent node is
labeled as removed, all of its children will be re-
moved; one node will retain if at least one of its
children is kept.
Therefore, the objective function in the new ILP
formulation is:
max z =
height
?
l=1
(
n
l
?
i=1
?
l
i
? s(x, 0, l
i
)
+
n
l
?1
?
i=1
n
l
?
j=i+1
?
l
ij
? s(x, l
i
, l
j
)
+
n
l
?
i=1
?
l
i
? s(x, l
i
, n
l
+ 1) ) (8)
where height is the depth for a parse tree (starting
from level 1 for the tree), and n
l
means the length
of level l (for example, n
5
= 6 in the example
in Fig1). Then every level will have a set of pa-
rameters ?l
i
, ?
l
i
, ?
l
i
, and ?l
ij
, and the corresponding
constraints as shown in Formula (3) to (6). The re-
lationship between nodes from adjacent levels can
be expressed as:
?
l
i
? ?
(l+1)
j
(9)
?
l
i
?
?
?
(l+1)
j
(10)
in which node j at level (l+1) is the child of node
694
i at level l. In addition, 1 ? l ? height ? 1,
1 ? i ? n
l
and 1 ? j ? n
l+1
.
3.3 Linguistically Motivated Constraints
In our proposed model, we can jointly decide the
status of every node in the constituent parse tree
at the same time. One advantage is that we can
add constraints based on internal nodes or rela-
tionship in the parse tree, rather than only using
the relationship based on words. In addition to
the constraints proposed in (Clarke and Lapata,
2008), we introduce more linguistically motivated
constraints to keep the compressed sentence more
grammatically correct. The following describes
the constraints we used based on the constituent
parse tree.
? If a node?s label is ?SBAR?, its parent?s label
is ?NP? and its first child?s label is ?WHNP? or
?WHPP? or ?IN?, then if we can find a noun
in the left siblings of ?SBAR?, this subordi-
nate clause could be an attributive clause or
appositive clause. Therefore the found noun
node should be included in the compression
if the ?SBAR? is also included, because the
node ?SBAR? decorates the noun. For exam-
ple, the top part of Fig 2 is part of expanded
constituent parse tree of sentence ?Those who
knew David were all dead.? The nodes in el-
lipse should share the same status.
? If a node?s label is ?SBAR?, its parent?s label
is ?VP? and its first child?s label is ?WHNP?,
then if we can find a verb in the left siblings
of ?SBAR?, this subordinate clause could be
an objective clause. Therefore, the found
verb node should be included in the compres-
sion if the ?SBAR? node is also included, be-
cause the node ?SBAR? is the object of that
verb. An example is shown in the bottom part
of Fig 2. The nodes in ellipse should share the
same status.
? If a node?s label is ?SBAR?, its parent?s
label is ?VP? and its first child?s label is
?WHADVP?, then if the first leaf for this node
is a wh-word (e.g., ?where, when, why?) or
?how?, this clause may be an objective clause
(when the word is ?why, how, where?) or at-
tributive clause (when the word is ?where?) or
adverbial clause (when the word is ?when?).
Therefore, similar to above, if a verb or noun
is found in the left siblings of ?SBAR?, the
VBD/ 
knew 
NNP/ 
David 
NP 
DT 
DT 
DT 
VP 
WP 
WP/ 
who 
DT/ 
Those 
VBD 
S 
NP 
PRP/ 
he 
PRP/ 
    I 
VBP/ 
believe  
PRP VBP 
 WP/ 
what 
WHNP S PRP 
 
VBP 
VBD/ 
said 
SBAR 
VP NP 
NP VP WP PRP 
 
VBP 





















NP 
SBAR 
WHNP 
WHNP 
 
S 
Figure 2: Expanded constituent parse tree for ex-
amples.
found verb or noun node should be included
in the compression if the ?SBAR? node is also
included.
? If a node?s label is ?SBAR? and its parent?s la-
bel is ?ADJP?, then if we can find a ?JJ?, ?JJR?,
or ?JJS? in the left siblings of ?SBAR?, the
?SBAR? node should be included in the com-
pression if the found ?JJ?, ?JJR? or ?JJS? node
is also included because the node ?SBAR? is
decorated by the adjective.
? The node with a label of ?PRN? can be re-
moved without other constraints.
We also include some other constraints based on
the Stanford dependency parse tree. Table 1 lists
the dependency relations we considered.
? For type I relations, the parent and child node
with those relationships should have the same
value in the compressed result (both are kept
or removed).
? For type II relations, if the child node in
those relations is retained in the compressed
sentence, the parent node should be also re-
tained.
695
Dependency Relation Example
prt: phrase verb particle They shut down the station. prt(shut,down)
prep: prepositional modifier He lives in a small village. prep(lives,in)
I pobj: object of a preposition I sat on the chair. pobj(on,chair)
nsubj: nominal subject The boy is cute. nsubj(cute,boy)
cop: copula Bill is big. cop(big,is)
partmod: participial modifier Truffles picked during the spring are tasty. partmod(truffles,picked)
II nn: noun compound modifier Oil price futures. nn(futures,oil)
acomp: adjectival complement She looks very beautiful. acomp(looks,beautiful)
pcomp: prepositional complement He felt sad after learning that tragedy. pcomp(after,learning)
III ccomp: clausal complement I am certain that he did it. ccomp(certain,did)
tmod: temporal modifier Last night I swam in the pool. tmod(swam,night)
Table 1: Some dependency relations used for extra constraints. All the examples are from (Marneffe and
Manning, 2002)
? For type III relations, if the parent node in
these relations is retained, the child node
should be kept as well.
3.4 Features
So far we have defined the decoding process
and related constraints used in decoding. These
all rely on the score function s(x, y) = w ?
f(x, L(y
j?1
), L(y
j
)) for every level in the con-
stituent parse tree. We included all the features in-
troduced in (Clarke and Lapata, 2008) (those fea-
tures are designed for leaves). Table 2 lists the
additional features we used in our system.
General Features for Every Node
1. individual node label and concatenation of a pair of
nodes
2. distance of two nodes at the same level
3. is the node at beginning or end at that level?
4. do the two nodes have the same parent?
5. if two nodes do not have the same parent, then is the left
node the rightmost child of its parent? is the right node the
leftmost child of its parent?
6. combination of parent label if the node pair are not
under the same parent
7. number of node?s children: 1/0/>1
8. depth of nodes in the parse tree
Extra Features for Leaf nodes
1. word itself and concatenation of two words
2. POS and concatenation of two words? POS
3. whether the word is a stopword
4. node?s named entity tag
5. dependency relationship between two leaves
Table 2: Features used in our system besides those
used in (Clarke and Lapata, 2008).
3.5 Learning
To learn the feature weights during training, we
perform ILP decoding on every sentence in the
training set, to find the best hypothesis for each
node in the expanded constituent parse tree. If
the hypothesis is incorrect, we update the feature
weights using the structured perceptron learning
strategy (Collins, 2002). The reference label for
every node in the expanded constituent parse tree
is obtained automatically from the bottom to the
top of the tree. Since every leaf node (word) is
human annotated (removed or retain), we annotate
the internal nodes as removed if all of its children
are removed. Otherwise, the node is annotated as
retained.
During perceptron training, a fixed learning rate
is used and parameters are averaged to prevent
overfitting. In our experiment, we observe sta-
ble convergence using the held-out development
corpus, with best performance usually obtained
around 10-20 epochs.
4 Summarization System
Similar to (Li et al., 2013a), our summarization
system is , which consists of three key compo-
nents: an initial sentence pre-selection module
to select some important sentence candidates; the
above compression model to generate n-best com-
pressions for each sentence; and then an ILP sum-
marization method to select the best summary sen-
tences from the multiple compressed sentences.
The sentence pre-selection model is a simple su-
pervised support vector regression (SVR) model
that predicts a salience score for each sentence and
selects the top ranked sentences for further pro-
cessing (compression and summarization). The
target value for each sentence during training is
the ROUGE-2 score between the sentence and the
human written abstracts. We use three common
features: (1) sentence position in the document;
(2) sentence length; and (3) interpolated n-gram
document frequency as introduced in (Ng et al.,
2012).
The final sentence selection process follows the
696
ILP method introduced in (Gillick et al., 2009).
Word bi-grams are used as concepts, and their doc-
ument frequency is used as weights. Since we use
multiple compressions for one sentence, an addi-
tional constraint is used: for each sentence, only
one of its n-best compressions may be included in
the summary.
For the compression module, using the ILP
method described above only finds the best com-
pression result for a given sentence. To generate
n-best compression candidates, we use an iterative
approach ? we add one more constraints to prevent
it from generating the same answer every time af-
ter getting one solution.
5 Experimental Results
5.1 Experimental Setup
Summarization Data For summarization experi-
ments, we use the standard TAC data sets1, which
have been used in the NIST competitions. In par-
ticular, we used the TAC 2010 data set as train-
ing data for the SVR sentence pre-selection model,
TAC 2009 data set as development set for parame-
ter tuning, and the TAC 2008 and 2011 data as the
test set for reporting the final summarization re-
sults. The training data for the sentence compres-
sion module in the summarization system is sum-
mary guided compression corpus annotated by (Li
et al., 2013a) using TAC2010 data. In the com-
pression module, for each word we also used its
document level feature.2
Compression Data We also evaluate our com-
pression model using the data set from (Clarke
and Lapata, 2008). It includes 82 newswire arti-
cles with manually produced compression for each
sentence. We use the same partitions as (Martins
and Smith, 2009), i.e., 1,188 sentences for training
and 441 for testing.
Data Processing We use Stanford CoreNLP
toolkit3 to tokenize the sentences, extract name en-
tity tags, and generate the dependency parse tree.
Berkeley Parser (Petrov et al., 2006) is adopted
to obtain the constituent parse tree for every sen-
tence and POS tag for every token. We use Pocket
1http://www.nist.gov/tac/data/index.html
2Document level features for a word include information
such as the word?s document frequency in a topic. These
features cannot be extracted from a single sentence, as in the
standard sentence compression task, and are related to the
document summarization task.
3http://nlp.stanford.edu/software/corenlp.shtml
CRF4 to implement the CRF sentence compres-
sion model. SVMlight5 is used for the summary
sentence pre-selection model. Gurobi ILP solver6
does all ILP decoding.
5.2 Summarization Results
We compare our summarization system against
four recent studies, which have reported some of
the highest published results on this task. Berg-
Kirkpatrick et al. (2011) introduced a joint model
for sentence extraction and compression. Wood-
send and Lapata (2012) learned individual sum-
mary aspects from data, e.g., informativeness, suc-
cinctness, grammaticalness, stylistic writing con-
ventions, and jointly optimized the outcome in
an ILP framework. Ng et al. (2012) exploited
category-specific information for multi-document
summarization. Almeida and Martins (2013) pro-
posed compressive summarization method by dual
decomposition and multi-task learning. Our sum-
marization framework is the same as (Li et al.,
2013a), except they used a CRF-based compres-
sion model. In addition to the four previous stud-
ies, we also report the best achieved results in the
TAC competitions.
Table 3 shows the summarization results of our
method and others. The top part contains the re-
sults for TAC 2008 data and bottom part is for
TAC 2011 data. We use the ROUGE evaluation
metrics (Lin, 2004), with R-2 measuring the bi-
gram overlap between the system and reference
summaries and R-SU4 measuring the skip-bigram
with the maximum gap length of 4. In addition,
we evaluate the linguistic quality (LQ) of the sum-
maries for our system and (Li et al., 2013a).7 The
linguistic quality consists of two parts. One eval-
uates the grammar quality within a sentence. For
this, annotators marked if a compressed sentence
is grammatically correct. Typical grammar errors
include lack of verb or subordinate clause. The
other evaluates the coherence between sentences,
including the order of sentences and irrelevant sen-
tences. We invited 3 English native speakers to do
this evaluation. They gave every compressed sen-
tence a grammar score and a coherence score for
4http://sourceforge.net/projects/pocket-crf-1/
5http://svmlight.joachims.org/
6http://www.gurobi.com
7We chose to evaluate the linguistic quality for this system
because of two reasons: one is that we have an implementa-
tion of that method; the other more important one is that it
has the highest reported ROUGE results among the compared
methods.
697
System R-2 R-SU4 Gram Cohere
TAC?08 Best System 11.03 13.96 n/a n/a
(Berg-Kirk et al., 2011) 11.70 14.38 n/a n/a
(Woodsend et al., 2012) 11.37 14.47 n/a n/a
(Almeida et al.,2013) 12.30 15.18 n/a n/a
(Li et al., 2013a) 12.35 15.27 3.81 3.41
Our System 12.23 15.47 4.29 4.11
TAC?11 Best System 13.44 16.51 n/a n/a
(Ng et al., 2012) 13.93 16.83 n/a n/a
(Li et al., 2013a) 14.40 16.89 3.67 3.32
Our System 14.04 16.67 4.18 4.07
Table 3: Summarization results on the TAC 2008
and 2011 data sets.
each topic. The score is scaled and ranges from 1
(bad) to 5 (good). Therefore, in table 3, the gram-
mar score is the average score for each sentence
and coherence score is the average for each topic.
We measure annotators? agreement in the follow-
ing way: we consider the scores from each anno-
tator as a distribution and we find that these three
distributions are not statistically significantly dif-
ferent each other (p > 0.05 based on paired t-test).
We can see from the table that in general, our
system achieves better ROUGE results than most
previous work except (Li et al., 2013a) on both
TAC 2008 and TAC 2011 data. However, our
system?s linguistic quality is better than (Li et
al., 2013a). The CRF-based compression model
used in (Li et al., 2013a) can not well model the
grammar. Particularly, our results (ROUGE-2) are
statistically significantly (p < 0.05) higher than
TAC08 Best system, but are not statistically signif-
icant compared with (Li et al., 2013a) (p > 0.05).
The pattern is similar in TAC 2011 data. Our result
(R-2) is statistically significantly (p < 0.05) better
than TAC11 Best system, but not statistically (p >
0.05) significantly different from (Li et al., 2013a).
However, for the grammar and coherence score,
our results are statistically significantly (p < 0.05)
than (Li et al., 2013a). All the above statistics are
based on paired t-test.
5.3 Compression Results
The results above show that our summarization
system is competitive. In this section we focus
on the evaluation of our proposed compression
method. We compare our compression system
against four other models. HedgeTrimmer in Dorr
et al. (2003) applied a variety of linguistically-
motivated heuristics to guide the sentences com-
System C Rate (%) Uni-F1 Rel-F1
HedgeTrimmer 57.64 0.64 0.50
McDonald (2006) 70.95 0.77 0.55
Martins (2009) 71.35 0.77 0.56
Wang (2013) 68.06 0.79 0.59
Our System 71.19 0.77 0.58
Table 4: Sentence compression results. The hu-
man compression rate of the test set is 69%.
pression; McDonald (2006) used the output of two
parsers as features in a discriminative model that
decomposes over pairs of consecutive words; Mar-
tins and Smith (2009) built the compression model
in the dependency parse and utilized the relation-
ship between the head and modifier to preserve the
grammar relationship; Wang et al. (2013) devel-
oped a novel beam search decoder using the tree-
based compression model on the constituent parse
tree, which could find the most probable compres-
sion efficiently.
Table 4 shows the compression results of vari-
ous systems, along with the compression ratio (C
Rate) of the system output. We adopt the com-
pression metrics as used in (Martins and Smith,
2009) that measures the macro F-measure for the
retained unigrams (Uni-F1), and the one used
in (Clarke and Lapata, 2008) that calculates the
F1 score of the grammatical relations labeled by
(Briscoe and Carroll, 2002) (Rel-F1). We can see
that our proposed compression method performs
well, similar to the state-of-the-art systems.
To evaluate the power of using the expanded
parse tree in our model, we conducted another ex-
periment where we only consider the bottom level
of the constituent parse tree. In some sense, this
could be considered as the system in (Clarke and
Lapata, 2008). Furthermore, we use two differ-
ent setups: one uses the lexical features (about the
words) and the other does not. Table 5 shows the
results using the data in (Clarke and Lapata, 2008).
For a comparison, we also include the results us-
ing the CRF-based compression model (the one
used in (Nomoto, 2007; Li et al., 2013a)). We
report results using both the automatically calcu-
lated compression metrics and the linguistic qual-
ity score. Three English native speaker annotators
were asked to judge two aspects of the compressed
sentence compared with the gold result: one is the
content that looks at whether the important words
are kept and the other is the grammar score which
evaluates the sentence?s readability. Each of these
698
two scores ranges from 1(bad) to 5(good).
Table 5 shows that when using lexical features,
our system has statistically significantly (p < 0.05)
higher Grammar value and content importance
value than the CRF and the leaves only system.
When no lexical features are used, default system
can achieve statistically significantly (p < 0.01)
higher results than the CRF and the leaves only
system.
We can see that using the expanded parse tree
performs better than using the leaves only, espe-
cially when lexical features are not used. In ad-
dition, we observe that our proposed compression
method is more generalizable than the CRF-based
model. When our system does not use lexical
features in the leaves, it achieves better perfor-
mance than the CRF-based model. This is impor-
tant since such a model is more robust and may be
used in multiple domains, whereas a model rely-
ing on lexical information may suffer more from
domain mismatch. From the table we can see our
proposed tree based compression method consis-
tently has better linguistic quality. On the other
hand, the CRF compression model is the most
computationally efficient one among these three
compression methods. It is about 200 times faster
than our model using the expanded parse tree. Ta-
ble 6 shows some examples using different meth-
ods.
System C Rate(%) Uni-F1 Rel-F1 Gram Imp
Using lexical features
CRF 79.98 0.80 0.51 3.9 4.0
ILP(I) 80.54 0.79 0.57 4.0 4.2
ILP(II) 79.90 0.80 0.57 4.2 4.4
No lexical features
CRF 77.75 0.78 0.51 3.35 3.5
ILP(I) 77.77 0.78 0.56 3.7 3.9
ILP(II) 77.78 0.80 0.58 4.1 4.2
Table 5: Sentence compression results: effect of
lexical features and expanded parse tree. ILP(I)
represents the system using only bottom nodes in
constituent parse tree. ILP(II) is our system. Imp
means the content importance value.
6 Conclusion
In this paper, we propose a discriminative ILP sen-
tence compression model based on the expanded
constituent parse tree, which aims to improve the
linguistic quality of the compressed sentences in
the summarization task. Linguistically motivated
constraints are incorporated to improve the sen-
tence quality. We conduct experiments on the TAC
Using lexical features
Source:
Apart from drugs, detectives believe money is laun-
dered from a variety of black market deals involving
arms and high technology.
Human compress:
detectives believe money is laundered from a variety of
black market deals.
CRF result :
Apart from drugs detectives believe money is laundered
from a black market deals involving arms and technol-
ogy.
ILP(I) Result:
detectives believe money is laundered from a variety of
black deals involving arms.
ILP(II) Result:
detectives believe money is laundered from black mar-
ket deals.
No lexical features
Source:
Mrs Allan?s son disappeared in May 1989, after a party
during his back packing trip across North America.
Human compress:
Mrs Allan?s son disappeared in 1989, after a party dur-
ing his trip across North America.
CRF result :
Mrs Allan?s son disappeared May 1989, after during his
packing trip across North America.
ILP(I) Result:
Mrs Allan?s son disappeared in May, 1989, after a party
during his packing trip across North America .
ILP(II) Result:
Mrs Allan?s son disappeared in May 1989, after a party
during his trip.
Table 6: Examples of original sentences and their
compressed sentences from different systems.
2008 and 2011 summarization data sets and show
that by incorporating this sentence compression
model, our summarization system can yield signif-
icant performance gain in linguistic quality with-
out losing much ROUGE results. The analysis
of the compression module also demonstrates its
competitiveness, in particular the better linguistic
quality and less reliance on lexical cues.
Acknowledgments
We thank the anonymous reviewers for their de-
tailed and insightful comments on earlier drafts
of this paper. The work is also partially sup-
ported by NSF award IIS-0845484 and DARPA
Contract No. FA8750-13-2-0041. Any opinions,
findings, and conclusions or recommendations ex-
pressed are those of the authors and do not neces-
sarily reflect the views of the funding agencies.
699
References
Ahmet Aker, Trevor Cohn, and Robert Gaizauskas.
2010. Multi-document summarization using a*
search and discriminative training. In Proceedings
of EMNLP.
Miguel B. Almeida and Andre F. T. Martins. 2013.
Fast and robust compressive summarization with
dual decomposition and multi-task learning. In Pro-
ceedings of ACL.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of ACL.
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of LREC.
Yllias Chali and Sadid A. Hasan. 2012. On the effec-
tiveness of using sentence compression models for
query-focused multi-document summarization. In
Proceedings of COLING.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression an integer linear
programming approach. Journal of Artificial Intelli-
gence Research.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP.
Hal Daume?. 2006. Practical structured learning tech-
niques for natural language processing. Ph.D. the-
sis, University of Southern California.
Bonnie Dorr, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach
to headline generation. In Proceedings of NAACL.
Katja Filippova and Yasemin Altun. 2013. Overcom-
ing the lack of parallel data in sentence compression.
In Proceedings of EMNLP.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Proceedings of NAACL.
Michel Galley and Kathleen McKeown. 2007. Lexi-
calized markov grammars for sentence compression.
In Processings of NAACL.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proceedings of EMNLP.
Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, Berndt
Bohnet, Yang Liu, and Shasha Xie. 2009. The
icsi/utd summarization system at tac 2009. In Pro-
ceedings of TAC.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of AAAI.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139(1):91?107.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings
of SIGIR.
Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013a.
Document summarization via guided sentence com-
pression. In Proceedings of the EMNLP.
Chen Li, Xian Qian, and Yang Liu. 2013b. Using su-
pervised bigram-based ilp for extractive summariza-
tion. In Proceedings of ACL.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submodu-
lar functions. In Proceedings of NAACL.
Chin-Yew Lin. 2003. Improving summarization per-
formance by sentence compression - A pilot study.
In Proceeding of the Sixth International Workshop
on Information Retrieval with Asian Language.
Chin-Yew Lin. 2004. Rouge: a package for automatic
evaluation of summaries. In Proceedings of ACL.
Fei Liu and Yang Liu. 2013. Towards abstractive
speech summarization: Exploring unsupervised and
supervised approaches for spoken utterance com-
pression. IEEE Transactions on Audio, Speech, and
Language Processing.
Marie-Catherine de Marneffe and Christopher D Man-
ning. 2002. Stanford typed dependencies manual.
Andre F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extrac-
tion and compression. In Proceedings of the ACL
Workshop on Integer Linear Programming for Natu-
ral Language Processing.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceed-
ings of EACL.
Ani Nenkova and Kathleen McKeown. 2011. Auto-
matic summarization. Foundations and Trends in
Information Retrieval.
Jun-Ping Ng, Praveen Bysani, Ziheng Lin, Min-Yen
Kan, and Chew-Lim Tan. 2012. Exploiting
category-specific information for multi-document
summarization. In Proceedings of COLING.
Tadashi Nomoto. 2007. Discriminative sentence com-
pression with conditional random fields. Informa-
tion Processing and Management.
Miles Osborne. 2002. Using maximum entropy for
sentence extraction. In Proceedings of the ACL-02
Workshop on Automatic Summarization.
700
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
COLING-ACL.
Xian Qian and Yang Liu. 2013. Fast joint compression
and summarization via graph cuts. In Proceedings
of EMNLP.
Advaith Siddharthan, Ani Nenkova, and Kathleen
McKeown. 2004. Syntactic simplification for im-
proving content selection in multi-document sum-
marization. In Proceedings of Coling.
Kapil Thadani and Kathleen McKeown. 2013. Sen-
tence compression with joint structural inference. In
Proceedings of CoNLL.
Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of ACL.
Lucy Vanderwende, Hisami Suzuki, Chris Brockett,
and Ani Nenkova. 2007. Beyond sumbasic: Task-
focused summarization with sentence simplification
and lexical expansion. Information Processing &
Management.
Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-
rian, and Claire Cardie. 2013. A sentence com-
pression based framework to query-focused multi-
document summarization. In Proceedings of ACL.
Kristian Woodsend and Mirella Lapata. 2010. Auto-
matic generation of story highlights. In Proceedings
of ACL.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of EMNLP-CoNLL.
Katsumasa Yoshikawa, Tsutomu Hirao, Ryu Iida, and
Manabu Okumura. 2012. Sentence compression
with semantic role constraints. In Proceedings of
ACL.
David Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sen-
tence compression as a tool for document summa-
rization tasks. In Information Processing and Man-
agement.
701
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1004?1013,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Using Supervised Bigram-based ILP for Extractive Summarization
Chen Li, Xian Qian, and Yang Liu
The University of Texas at Dallas
Computer Science Department
chenli,qx,yangl@hlt.utdallas.edu
Abstract
In this paper, we propose a bigram based
supervised method for extractive docu-
ment summarization in the integer linear
programming (ILP) framework. For each
bigram, a regression model is used to es-
timate its frequency in the reference sum-
mary. The regression model uses a vari-
ety of indicative features and is trained dis-
criminatively to minimize the distance be-
tween the estimated and the ground truth
bigram frequency in the reference sum-
mary. During testing, the sentence selec-
tion problem is formulated as an ILP prob-
lem to maximize the bigram gains. We
demonstrate that our system consistently
outperforms the previous ILP method on
different TAC data sets, and performs
competitively compared to the best results
in the TAC evaluations. We also con-
ducted various analysis to show the im-
pact of bigram selection, weight estima-
tion, and ILP setup.
1 Introduction
Extractive summarization is a sentence selection
problem: identifying important summary sen-
tences from one or multiple documents. Many
methods have been developed for this problem, in-
cluding supervised approaches that use classifiers
to predict summary sentences, graph based ap-
proaches to rank the sentences, and recent global
optimization methods such as integer linear pro-
gramming (ILP) and submodular methods. These
global optimization methods have been shown to
be quite powerful for extractive summarization,
because they try to select important sentences and
remove redundancy at the same time under the
length constraint.
Gillick and Favre (Gillick and Favre, 2009) in-
troduced the concept-based ILP for summariza-
tion. Their system achieved the best result in the
TAC 09 summarization task based on the ROUGE
evaluation metric. In this approach the goal is
to maximize the sum of the weights of the lan-
guage concepts that appear in the summary. They
used bigrams as such language concepts. The as-
sociation between the language concepts and sen-
tences serves as the constraints. This ILP method
is formally represented as below (see (Gillick and
Favre, 2009) for more details):
max ?i wici (1)
s.t. sjOccij ? ci (2)?
j sjOccij ? ci (3)?
j ljsj ? L (4)
ci ? {0, 1} ?i (5)
sj ? {0, 1} ?j (6)
ci and sj are binary variables (shown in (5) and
(6)) that indicate the presence of a concept and
a sentence respectively. wi is a concept?s weight
and Occij means the occurrence of concept i in
sentence j. Inequalities (2)(3) associate the sen-
tences and concepts. They ensure that selecting a
sentence leads to the selection of all the concepts
it contains, and selecting a concept only happens
when it is present in at least one of the selected
sentences.
There are two important components in this
concept-based ILP: one is how to select the con-
cepts (ci); the second is how to set up their weights
(wi). Gillick and Favre (Gillick and Favre, 2009)
used bigrams as concepts, which are selected from
a subset of the sentences, and their document fre-
quency as the weight in the objective function.
In this paper, we propose to find a candidate
summary such that the language concepts (e.g., bi-
grams) in this candidate summary and the refer-
ence summary can have the same frequency. We
expect this restriction is more consistent with the
1004
ROUGE evaluation metric used for summarization
(Lin, 2004). In addition, in the previous concept-
based ILP method, the constraints are with respect
to the appearance of language concepts, hence it
cannot distinguish the importance of different lan-
guage concepts in the reference summary. Our
method can decide not only which language con-
cepts to use in ILP, but also the frequency of these
language concepts in the candidate summary. To
estimate the bigram frequency in the summary,
we propose to use a supervised regression model
that is discriminatively trained using a variety of
features. Our experiments on several TAC sum-
marization data sets demonstrate this proposed
method outperforms the previous ILP system and
often the best performing TAC system.
2 Proposed Method
2.1 Bigram Gain Maximization by ILP
We choose bigrams as the language concepts in
our proposed method since they have been suc-
cessfully used in previous work. In addition, we
expect that the bigram oriented ILP is consistent
with the ROUGE-2 measure widely used for sum-
marization evaluation.
We start the description of our approach for the
scenario where a human abstractive summary is
provided, and the task is to select sentences to
form an extractive summary. Then Our goal is
to make the bigram frequency in this system sum-
mary as close as possible to that in the reference.
For each bigram b, we define its gain:
Gain(b, sum) = min{nb,ref , nb,sum} (7)
where nb,ref is the frequency of b in the reference
summary, and nb,sum is the frequency of b in the
automatic summary. The gain of a bigram is no
more than its frequency in the reference summary,
hence adding redundant bigrams will not increase
the gain.
The total gain of an extractive summary is de-
fined as the sum of every bigram gain in the sum-
mary:
Gain(sum) =
?
b
Gain(b, sum)
=
?
b
min{nb,ref ,
?
s
z(s) ? nb,s} (8)
where s is a sentence in the document, nb,s is
the frequency of b in sentence s, z(s) is a binary
variable, indicating whether s is selected in the
summary. The goal is to find z that maximizes
Gain(sum) (formula (8)) under the length con-
straint L.
This problem can be casted as an ILP problem.
First, using the fact that
min{a, x} = 0.5(?|x ? a| + x + a), x, a ? 0
we have
?
b
min{nb,ref ,
?
s
z(s) ? nb,s} =
?
b
0.5 ? (?|nb,ref ?
?
s
z(s) ? nb,s|+
nb,ref +
?
s
z(s) ? nb,s)
Now the problem is equivalent to:
max
z
?
b
(?|nb,ref ?
?
s
z(s) ? nb,s| +
nb,ref +
?
s
z(s) ? nb,s)
s.t.
?
s
z(s) ? |S| ? L; z(s) ? {0, 1}
This is equivalent to the ILP:
max
?
b
(
?
s
z(s) ? nb,s ?Cb) (9)
s.t.
?
s
z(s) ? |S| ? L (10)
z(s) ? {0, 1} (11)
?Cb ? nb,ref ?
?
s
z(s) ? nb,s ? Cb
(12)
where Cb is an auxiliary variable we introduce that
is equal to |nb,ref ?
?
s z(s) ? nb,s|, and nb,ref is
a constant that can be dropped from the objective
function.
2.2 Regression Model for Bigram Frequency
Estimation
In the previous section, we assume that nb,ref is
at hand (reference abstractive summary is given)
and propose a bigram-based optimization frame-
work for extractive summarization. However, for
the summarization task, the bigram frequency is
unknown, and thus our first goal is to estimate such
frequency. We propose to use a regression model
for this.
Since a bigram?s frequency depends on the sum-
mary length (L), we use a normalized frequency
1005
in our method. Let nb,ref = Nb,ref ? L, where
Nb,ref = n(b,ref)?
b n(b,ref)
is the normalized frequency
in the summary. Now the problem is to automati-
cally estimate Nb,ref .
Since the normalized frequency Nb,ref is a real
number, we choose to use a logistic regression
model to predict it:
Nb,ref =
exp{w?f(b)}?
j exp{w?f(bj)}
(13)
where f(bj) is the feature vector of bigram bj and
w? is the corresponding feature weight. Since even
for identical bigrams bi = bj , their feature vectors
may be different (f(bi) 6= f(bj)) due to their dif-
ferent contexts, we sum up frequencies for identi-
cal bigrams {bi|bi = b}:
Nb,ref =
?
i,bi=b
Nbi,ref
=
?
i,bi=b exp{w?f(bi)}?
j exp{w?f(bj)}
(14)
To train this regression model using the given
reference abstractive summaries, rather than trying
to minimize the squared error as typically done,
we propose a new objective function. Since the
normalized frequency satisfies the probability con-
straint
?
b Nb,ref = 1, we propose to use KL di-
vergence to measure the distance between the es-
timated frequencies and the ground truth values.
The objective function for training is thus to mini-
mize the KL distance:
min
?
b
N?b,ref log
N?b,ref
Nb,ref
(15)
where N?b,ref is the true normalized frequency of
bigram b in reference summaries.
Finally, we replace Nb,ref in Formula (15) with
Eq (14) and get the objective function below:
max
?
b
N?b,ref log
?
i,bi=b exp{w?f(bi)}?
j exp{w?f(bj)}
(16)
This shares the same form as the contrastive es-
timation proposed by (Smith and Eisner, 2005).
We use gradient decent method for parameter esti-
mation, initial w is set with zero.
2.3 Features
Each bigram is represented using a set of features
in the above regression model. We use two types
of features: word level and sentence level features.
Some of these features have been used in previous
work (Aker and Gaizauskas, 2009; Brandow et al,
1995; Edmundson, 1969; Radev, 2001):
? Word Level:
? 1. Term frequency1: The frequency of
this bigram in the given topic.
? 2. Term frequency2: The frequency of
this bigram in the selected sentences1 .
? 3. Stop word ratio: Ratio of stop words
in this bigram. The value can be {0, 0.5,
1}.
? 4. Similarity with topic title: The
number of common tokens in these two
strings, divided by the length of the
longer string.
? 5. Similarity with description of the
topic: Similarity of the bigram with
topic description (see next data section
about the given topics in the summariza-
tion task).
? Sentence Level: (information of sentence
containing the bigram)
? 6. Sentence ratio: Number of sentences
that include this bigram, divided by the
total number of the selected sentences.
? 7. Sentence similarity: Sentence sim-
ilarity with topic?s query, which is the
concatenation of topic title and descrip-
tion.
? 8. Sentence position: Sentence posi-
tion in the document.
? 9. Sentence length: The number of
words in the sentence.
? 10. Paragraph starter: Binary feature
indicating whether this sentence is the
beginning of a paragraph.
3 Experiments
3.1 Data
We evaluate our method using several recent TAC
data sets, from 2008 to 2011. The TAC summa-
rization task is to generate at most 100 words sum-
maries from 10 documents for a given topic query
(with a title and more detailed description). For
model training, we also included two years? DUC
data (2006 and 2007). When evaluating on one
TAC data set, we use the other years of the TAC
data plus the two DUC data sets as the training
data.
1See next section about the sentence selection step
1006
3.2 Summarization System
We use the same system pipeline described in
(Gillick et al, 2008; McDonald, 2007). The key
modules in the ICSI ILP system (Gillick et al,
2008) are briefly described below.
? Step 1: Clean documents, split text into sen-
tences.
? Step 2: Extract bigrams from all the sen-
tences, then select those bigrams with doc-
ument frequency equal to more than 3. We
call this subset as initial bigram set in the fol-
lowing.
? Step 3: Select relevant sentences that contain
at least one bigram from the initial bigram
set.
? Step 4: Feed the ILP with sentences and the
bigram set to get the result.
? Step 5: Order sentences identified by ILP as
the final result of summary.
The difference between the ICSI and our system
is in the 4th step. In our method, we first extract all
the bigrams from the selected sentences and then
estimate each bigram?s Nb,ref using the regression
model. Then we use the top-n bigrams with their
Nb,ref and all the selected sentences in our pro-
posed ILP module for summary sentence selec-
tion. When training our bigram regression model,
we use each of the 4 reference summaries sepa-
rately, i.e., the bigram frequency is obtained from
one reference summary. The same pre-selection of
sentences described above is also applied in train-
ing, that is, the bigram instances used in training
are from these selected sentences and the reference
summary.
4 Experiment and Analysis
4.1 Experimental Results
Table 1 shows the ROUGE-2 results of our pro-
posed system, the ICSI system, and also the best
performing system in the NIST TAC evaluation.
We can see that our proposed system consistently
outperforms ICSI ILP system (the gain is statis-
tically significant based on ROUGE?s 95% confi-
dence internal results). Compared to the best re-
ported TAC result, our method has better perfor-
mance on three data sets, except 2011 data. Note
that the best performing system for the 2009 data
is the ICSI ILP system, with an additional com-
pression step. Our ILP method is purely extrac-
tive. Even without using compression, our ap-
proach performs better than the full ICSI system.
The best performing system for the 2011 data also
has some compression module. We expect that af-
ter applying sentence compression and merging,
we will have even better performance, however,
our focus in this paper is on the bigram-based ex-
tractive summarization.
ICSI Proposed TAC Rank1
ILP System System
2008 0.1023 0.1076 0.1038
2009 0.1160 0.1246 0.1216
2010 0.1003 0.1067 0.0957
2011 0.1271 0.1327 0.1344
Table 1: ROUGE-2 summarization results.
There are several differences between the ICSI
system and our proposed method. First is the
bigrams (concepts) used. We use the top 100
bigrams from our bigram estimation module;
whereas the ICSI system just used the initial bi-
gram set described in Section 3.2. Second, the
weights for those bigrams differ. We used the es-
timated value from the regression model; the ICSI
system just uses the bigram?s document frequency
in the original text as weight. Finally, two systems
use different ILP setups. To analyze which fac-
tors (or all of them) explain the performance dif-
ference, we conducted various controlled experi-
ments for these three factors (bigrams, weights,
ILP). All of the following experiments use the
TAC 2009 data as the test set.
4.2 Effect of Bigram Weights
In this experiment, we vary the weighting methods
for the two systems: our proposed method and the
ICSI system. We use three weighting setups: the
estimated bigram frequency value in our method,
document frequency, or term frequency from the
original text. Table 2 and 3 show the results using
the top 100 bigrams from our system and the ini-
tial bigram set from the ICSI system respectively.
We also evaluate using the two different ILP con-
figurations in these experiments.
First of all, we can see that for both ILP sys-
tems, our estimated bigram weights outperform
the other frequency-based weights. For the ICSI
ILP system, using bigram document frequency
achieves better performance than term frequency
(which verified why document frequency is used
in their system). In contrast, for our ILP method,
1007
# Weight ILP ROUGE-2
1 Estimated value Proposed 0.12462 ICSI 0.1178
3 Document freq Proposed 0.11094 ICSI 0.1132
5 Term freq Proposed 0.11166 ICSI 0.1080
Table 2: Results using different weighting meth-
ods on the top 100 bigrams generated from our
proposed system.
# Weight ILP ROUGE-2
1 Estimated value Proposed 0.11572 ICSI 0.1161
3 Document freq Proposed 0.11014 ICSI 0.1160
5 Term freq Proposed 0.11096 ICSI 0.1072
Table 3: Results using different weighting meth-
ods based on the initial bigram sets. The average
number of bigrams is around 80 for each topic.
the bigram?s term frequency is slightly more use-
ful than its document frequency. This indicates
that our estimated value is more related to bi-
gram?s term frequency in the original text. When
the weight is document frequency, the ICSI?s re-
sult is better than our proposed ILP; whereas when
using term frequency as the weights, our ILP has
better results, again suggesting term frequency fits
our ILP system better. When the weight is esti-
mated value, the results depend on the bigram set
used. The ICSI?s ILP performs slightly better than
ours when it is equipped with the initial bigram,
but our proposed ILP has much better results us-
ing our selected top100 bigrams. This shows that
the size and quality of the bigrams has an impact
on the ILP modules.
4.3 The Effect of Bigram Set?s size
In our proposed system, we use 100 top bigrams.
There are about 80 bigrams used in the ICSI ILP
system. A natural question to ask is the impact
of the number of bigrams and their quality on the
summarization system. Table 4 shows some statis-
tics of the bigrams. We can see that about one
third of bigrams in the reference summary are in
the original text (127.3 out of 321.93), verifying
that people do use different words/bigram when
writing abstractive summaries. We mentioned that
we only use the top-N (n is 100 in previous ex-
periments) bigrams in our summarization system.
On one hand, this is to save computational cost for
the ILP module. On the other hand, we see from
the table that only 127 of these more than 2K bi-
grams are in the reference summary and are thus
expected to help the summary responsiveness. In-
cluding all the bigrams would lead to huge noise.
# bigrams in ref summary 321.93
# bigrams in text and ref summary 127.3
# bigrams used in our regression model 2140.7
(i.e., in selected sentences)
Table 4: Bigram statistics. The numbers are the
average ones for each topic.
Fig 1 shows the bigram coverage (number of bi-
grams used in the system that are also in reference
summaries) when we vary N selected bigrams. As
expected, we can see that as n increases, there
are more reference summary bigrams included in
the system. There are 25 summary bigrams in the
top-50 bigrams and about 38 in top-100 bigrams.
Compared with the ICSI system that has around 80
bigrams in the initial bigram set and 29 in the ref-
erence summary, our estimation module has better
coverage.
0
10
20
30
40
50
60
70
80
90
100
110
120
130
50 500 950 1400 1850 2300 2750 3200
Number of Selected Bigram
N
um
be
r
of
B
ig
ra
m
bo
th
in
Se
le
ct
ed
an
d
R
ef
er
en
ce
Figure 1: Coverage of bigrams (number of bi-
grams in reference summary) when varying the
number of bigrams used in the ILP systems.
Increasing the number of bigrams used in the
system will lead to better coverage, however, the
incorrect bigrams also increase and have a nega-
tive impact on the system performance. To exam-
ine the best tradeoff, we conduct the experiments
by choosing the different top-N bigram set for the
two ILP systems, as shown in Fig 2. For both the
ILP systems, we used the estimated weight value
for the bigrams.
1008
We can see that the ICSI ILP system performs
better when the input bigrams have less noise
(those bigrams that are not in summary). However,
our proposed method is slightly more robust to this
kind of noise, possibly because of the weights we
use in our system ? the noisy bigrams have lower
weights and thus less impact on the final system
performance. Overall the two systems have sim-
ilar trends: performance increases at the begin-
ning when using more bigrams, and after certain
points starts degrading with too many bigrams.
The optimal number of bigrams differs for the two
systems, with a larger number of bigrams in our
method. We also notice that the ICSI ILP system
achieved a ROUGE-2 of 0.1218 when using top
60 bigrams, which is better than using the initial
bigram set in their method (0.1160).
0.109
0.111
0.113
0.115
0.117
0.119
0.121
0.123
0.125
40 50 60 70 80 90 100 110 120 130
Number of selected bigram
R
ou
ge
-2
Proposed ILP
ICSI
Figure 2: Summarization performance when vary-
ing the number of bigrams for two systems.
4.4 Oracle Experiments
Based on the above analysis, we can see the impact
of the bigram set and their weights. The following
experiments are designed to demonstrate the best
system performance we can achieve if we have ac-
cess to good quality bigrams and weights. Here we
use the information from the reference summary.
The first is an oracle experiment, where we use
all the bigrams from the reference summaries that
are also in the original text. In the ICSI ILP
system, the weights are the document frequency
from the multiple reference summaries. In our ILP
module, we use the term frequency of the bigram.
The oracle results are shown in Table 5. We can
see these are significantly better than the automatic
systems.
From Table 5, we notice that ICSI?s ILP per-
forms marginally better than our proposed ILP. We
hypothesize that one reason may be that many bi-
grams in the summary reference only appear once.
Table 6 shows the frequency of the bigrams in the
summary. Indeed 85% of bigram only appear once
ILP System ROUGE-2
Our ILP 0.2124
ICSI ILP 0.2128
Table 5: Oracle experiment: using bigrams and
their frequencies in the reference summary as
weights.
and no bigrams appear more than 9 times. For the
majority of the bigrams, our method and the ICSI
ILP are the same. For the others, our system has
slight disadvantage when using the reference term
frequency. We expect the high term frequency
may need to be properly smoothed/normalized.
Freq 1 2 3 4 5 6 7 8 9
Ave# 277 32 7.5 3.2 1.1 0.3 0.1 0.1 0.04
Table 6: Average number of bigrams for each term
frequency in one topic?s reference summary.
We also treat the oracle results as the gold stan-
dard for extractive summarization and compared
how the two automatic summarization systems
differ at the sentence level. This is different from
the results in Table 1, which are the ROUGE re-
sults comparing to human written abstractive sum-
maries at the n-gram level. We found that among
the 188 sentences in this gold standard, our system
hits 31 and ICSI only has 23. This again shows
that our system has better performance, not just
at the word level based on ROUGE measures, but
also at the sentence level. There are on average
3 different sentences per topic between these two
results.
In the second experiment, after we obtain the
estimated Nb,ref for every bigram in the selected
sentences from our regression model, we only
keep those bigrams that are in the reference sum-
mary, and use the estimated weights for both ILP
modules. Table 7 shows the results. We can
consider these as the upper bound the system
can achieve if we use the automatically estimated
weights for the correct bigrams. In this experi-
ment ICSI ILP?s performance still performs better
than ours. This might be attributed to the fact there
is less noise (all the bigrams are the correct ones)
and thus the ICSI ILP system performs well. We
can see that these results are worse than the pre-
vious oracle experiments, but are better than using
the automatically generated bigrams, again show-
ing the bigram and weight estimation is critical for
1009
summarization.
# Weight ILP ROUGE-2
1 Estimated value Proposed 0.18882 ICSI 0.1942
Table 7: Summarization results when using the es-
timated weights and only keeping the bigrams that
are in the reference summary.
4.5 Effect of Training Set
Since our method uses supervised learning, we
conduct the experiment to show the impact of
training size. In TAC?s data, each topic has two
sets of documents. For set A, the task is a standard
summarization, and there are 4 reference sum-
maries, each 100 words long; for set B, it is an up-
date summarization task ? the summary includes
information not mentioned in the summary from
set A. There are also 4 reference summaries, with
400 words in total. Table 8 shows the results on
2009 data when using the data from different years
and different sets for training. We notice that when
the training data only contains set A, the perfor-
mance is always better than using set B or the com-
bined set A and B. This is not surprising because
of the different task definition. Therefore, for the
rest of the study on data size impact, we only use
data set A from the TAC data and the DUC data as
the training set. In total there are about 233 topics
from the two years? DUC data (06, 07) and three
years? TAC data (08, 10, 11). We incrementally
add 20 topics every time (from DUC06 to TAC11)
and plot the learning curve, as shown in Fig 3. As
expected, more training data results in better per-
formance.
Training Set # Topics ROUGE-2
08 Corpus (A) 48 0.1192
08 Corpus( B) 48 0.1178
08 Corpus (A+B) 96 0.1188
10 Corpus (A) 46 0.1174
10 Corpus (B) 46 0.1167
10 Corpus (A+B) 92 0.1170
11 Corpus (A) 44 0.1157
11 Corpus (B) 44 0.1130
11 Corpus (A+B) 88 0.1140
Table 8: Summarization performance when using
different training corpora.
0.112
0.113
0.114
0.115
0.116
0.117
0.118
0.119
0.12
0.121
0.122
0.123
0.124
0.125
20 40 60 80 100 120 140 160 180 200 220 240
Number of trainning topics
R
ou
ge
-2
Figure 3: Learning curve
4.6 Summary of Analysis
The previous experiments have shown the impact
of the three factors: the quality of the bigrams
themselves, the weights used for these bigrams,
and the ILP module. We found that the bigrams
and their weights are critical for both the ILP se-
tups. However, there is negligible difference be-
tween the two ILP methods.
An important part of our system is the super-
vised method for bigram and weight estimation.
We have already seen for the previous ILP method,
when using our bigrams together with the weights,
better performance can be achieved. Therefore we
ask the question whether this is simply because
we use supervised learning, or whether our pro-
posed regression model is the key. To answer this,
we trained a simple supervised binary classifier
for bigram prediction (positive means that a bi-
gram appears in the summary) using the same set
of features as used in our bigram weight estima-
tion module, and then used their document fre-
quency in the ICSI ILP system. The result for
this method is 0.1128 on the TAC 2009 data. This
is much lower than our result. We originally ex-
pected that using the supervised method may out-
perform the unsupervised bigram selection which
only uses term frequency information. Further ex-
periments are needed to investigate this. From this
we can see that it is not just the supervised meth-
ods or using annotated data that yields the over-
all improved system performance, but rather our
proposed regression setup for bigrams is the main
reason.
5 Related Work
We briefly describe some prior work on summa-
rization in this section. Unsupervised methods
have been widely used. In particular, recently sev-
eral optimization approaches have demonstrated
1010
competitive performance for extractive summa-
rization task. Maximum marginal relevance
(MMR) (Carbonell and Goldstein, 1998) uses a
greedy algorithm to find summary sentences. (Mc-
Donald, 2007) improved the MMR algorithm to
dynamic programming. They used a modified ob-
jective function in order to consider whether the
selected sentence is globally optimal. Sentence-
level ILP was also first introduced in (McDon-
ald, 2007), but (Gillick and Favre, 2009) revised
it to concept-based ILP. (Woodsend and Lapata,
2012) utilized ILP to jointly optimize different as-
pects including content selection, surface realiza-
tion, and rewrite rules in summarization. (Gala-
nis et al, 2012) uses ILP to jointly maximize the
importance of the sentences and their diversity
in the summary. (Berg-Kirkpatrick et al, 2011)
applied a similar idea to conduct the sentence
compression and extraction for multiple document
summarization. (Jin et al, 2010) made a com-
parative study on sentence/concept selection and
pairwise and list ranking algorithms, and con-
cluded ILP performed better than MMR and the
diversity penalty strategy in sentence/concept se-
lection. Other global optimization methods in-
clude submodularity (Lin and Bilmes, 2010) and
graph-based approaches (Erkan and Radev, 2004;
Leskovec et al, 2005; Mihalcea and Tarau, 2004).
Various unsupervised probabilistic topic models
have also been investigated for summarization and
shown promising. For example, (Celikyilmaz and
Hakkani-Tu?r, 2011) used it to model the hidden
abstract concepts across documents as well as the
correlation between these concepts to generate
topically coherent and non-redundant summaries.
(Darling and Song, 2011) applied it to separate
the semantically important words from the low-
content function words.
In contrast to these unsupervised approaches,
there are also various efforts on supervised learn-
ing for summarization where a model is trained to
predict whether a sentence is in the summary or
not. Different features and classifiers have been
explored for this task, such as Bayesian method
(Kupiec et al, 1995), maximum entropy (Osborne,
2002), CRF (Galley, 2006), and recently reinforce-
ment learning (Ryang and Abekawa, 2012). (Aker
et al, 2010) used discriminative reranking on mul-
tiple candidates generated by A* search. Recently,
research has also been performed to address some
issues in the supervised setup, such as the class
data imbalance problem (Xie and Liu, 2010).
In this paper, we propose to incorporate the
supervised method into the concept-based ILP
framework. Unlike previous work using sentence-
based supervised learning, we use a regression
model to estimate the bigrams and their weights,
and use these to guide sentence selection. Com-
pared to the direct sentence-based classification or
regression methods mentioned above, our method
has an advantage. When abstractive summaries
are given, one needs to use that information to au-
tomatically generate reference labels (a sentence
is in the summary or not) for extractive summa-
rization. Most researchers have used the similarity
between a sentence in the document and the ab-
stractive summary for labeling. This is not a per-
fect process. In our method, we do not need to
generate this extra label for model training since
ours is based on bigrams ? it is straightforward to
obtain the reference frequency for bigrams by sim-
ply looking at the reference summary. We expect
our approach also paves an easy way for future au-
tomatic abstractive summarization. One previous
study that is most related to ours is (Conroy et al,
2011), which utilized a Naive Bayes classifier to
predict the probability of a bigram, and applied
ILP for the final sentence selection. They used
more features than ours, whereas we use a discrim-
inatively trained regression model and a modified
ILP framework. Our proposed method performs
better than their reported results in TAC 2011 data.
Another study closely related to ours is (Davis et
al., 2012), which leveraged Latent Semantic Anal-
ysis (LSA) to produce term weights and selected
summary sentences by computing an approximate
solution to the Budgeted Maximal Coverage prob-
lem.
6 Conclusion and Future Work
In this paper, we leverage the ILP method as a core
component in our summarization system. Dif-
ferent from the previous ILP summarization ap-
proach, we propose a supervised learning method
(a discriminatively trained regression model) to
determine the importance of the bigrams fed to
the ILP module. In addition, we revise the ILP to
maximize the bigram gain (which is expected to
be highly correlated with ROUGE-2 scores) rather
than the concept/bigram coverage. Our proposed
method yielded better results than the previous
state-of-the-art ILP system on different TAC data
1011
sets. From a series of experiments, we found that
there is little difference between the two ILP mod-
ules, and that the improved system performance is
attributed to the fact that our proposed supervised
bigram estimation module can successfully gather
the important bigram and assign them appropriate
weights. There are several directions that warrant
further research. We plan to consider the context
of bigrams to better predict whether a bigram is in
the reference summary. We will also investigate
the relationship between concepts and sentences,
which may help move towards abstractive summa-
rization.
Acknowledgments
This work is partly supported by DARPA under
Contract No. HR0011-12-C-0016 and FA8750-
13-2-0041, and NSF IIS-0845484. Any opinions
expressed in this material are those of the authors
and do not necessarily reflect the views of DARPA
or NSF.
References
Ahmet Aker and Robert Gaizauskas. 2009. Summary
generation for toponym-referenced images using ob-
ject type language models. In Proceedings of the
International Conference RANLP.
Ahmet Aker, Trevor Cohn, and Robert Gaizauskas.
2010. Multi-document summarization using a*
search and discriminative training. In Proceedings
of the EMNLP.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the ACL.
Ronald Brandow, Karl Mitze, and Lisa F. Rau. 1995.
Automatic condensation of electronic publications
by sentence selection. Inf. Process. Manage.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
the SIGIR.
Asli Celikyilmaz and Dilek Hakkani-Tu?r. 2011. Dis-
covery of topically coherent sentences for extractive
summarization. In Proceedings of the ACL.
John M. Conroy, Judith D. Schlesinger, Jeff Kubina,
Peter A. Rankel, and Dianne P. O?Leary. 2011.
Classy 2011 at tac: Guided and multi-lingual sum-
maries and evaluation metrics. In Proceedings of the
TAC.
William M. Darling and Fei Song. 2011. Probabilistic
document modeling for syntax removal in text sum-
marization. In Proceedings of the ACL.
Sashka T. Davis, John M. Conroy, and Judith D.
Schlesinger. 2012. Occams - an optimal combinato-
rial covering algorithm for multi-document summa-
rization. In Proceedings of the ICDM.
H. P. Edmundson. 1969. New methods in automatic
extracting. J. ACM.
Gu?nes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text
summarization. J. Artif. Int. Res.
Dimitrios Galanis, Gerasimos Lampouras, and Ion An-
droutsopoulos. 2012. Extractive multi-document
summarization with integer linear programming and
support vector regression. In Proceedings of the
COLING.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proceedings of the EMNLP.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the
Workshop on Integer Linear Programming for Natu-
ral Langauge Processing on NAACL.
Dan Gillick, Benoit Favre, and Dilek Hakkani-Tu?r.
2008. In The ICSI Summarization System at TAC
2008.
Feng Jin, Minlie Huang, and Xiaoyan Zhu. 2010. A
comparative study on ranking and selection strate-
gies for multi-document summarization. In Pro-
ceedings of the COLING.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings
of the SIGIR.
Jure Leskovec, Natasa Milic-Frayling, and Marko Gro-
belnik. 2005. Impact of linguistic analysis on the
semantic graph coverage and learning of document
extracts. In Proceedings of the AAAI.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submodu-
lar functions. In Proceedings of the NAACL.
Chin-Yew Lin. 2004. Rouge: a package for auto-
matic evaluation of summaries. In Proceedings of
the ACL.
Ryan McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. In Pro-
ceedings of the European conference on IR research.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into text. In Proceedings of the
EMNLP.
Miles Osborne. 2002. Using maximum entropy for
sentence extraction. In Proceedings of the ACL-02
Workshop on Automatic Summarization.
1012
Dragomir R. Radev. 2001. Experiments in single and
multidocument summarization using mead. In In
First Document Understanding Conference.
Seonggi Ryang and Takeshi Abekawa. 2012. Frame-
work of automatic text summarization using rein-
forcement learning. In Proceedings of the EMNLP.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: training log-linear models on unlabeled
data. In Proceedings of the ACL.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of the EMNLP.
Shasha Xie and Yang Liu. 2010. Improving supervised
learning for meeting summarization using sampling
and regression. Comput. Speech Lang.
1013
Proceedings of the ACL 2014 Student Research Workshop, pages 86?93,
Baltimore, Maryland USA, June 22-27 2014. c?2014 Association for Computational Linguistics
Improving Text Normalization via Unsupervised Model and
Discriminative Reranking
Chen Li and Yang Liu
The University of Texas at Dallas
Computer Science Department
chenli,yangl@hlt.utdallas.edu
Abstract
Various models have been developed for
normalizing informal text. In this paper,
we propose two methods to improve nor-
malization performance. First is an unsu-
pervised approach that automatically iden-
tifies pairs of a non-standard token and
proper word from a large unlabeled cor-
pus. We use semantic similarity based on
continuous word vector representation, to-
gether with other surface similarity mea-
surement. Second we propose a reranking
strategy to combine the results from differ-
ent systems. This allows us to incorporate
information that is hard to model in indi-
vidual systems as well as consider multi-
ple systems to generate a final rank for a
test case. Both word- and sentence-level
optimization schemes are explored in this
study. We evaluate our approach on data
sets used in prior studies, and demonstrate
that our proposed methods perform better
than the state-of-the-art systems.
1 Introduction
There has been a lot of research efforts recently
on analysis of social media text (e.g., from Twit-
ter and Facebook) (Ritter et al., 2011; Owoputi et
al., 2013; Liu et al., 2012b). One challenge in
processing social media text is how to deal with
the frequently occurring non-standard words, such
as bday (meaning birthday), snd (meaning sound)
and gl (meaning girl) . Normalizing informal text
(changing non-standard words to standard ones)
will ease subsequent language processing mod-
ules.
Text normalization has been an important topic
for the text-to-speech field. See (Sproat et al.,
2001) for a good report of this problem. Recently,
much research on normalization has been done
for social text domain, which has many abbrevi-
ations or non-standard tokens. A simple approach
for normalization would be applying traditional
spell checking model, which is usually based on
edit distance (Damerau, 1964; Levenshtein, 1966).
However, this model can not well handle the non-
standard words in social media text due to the large
variation in generating them.
Another line of work in normalization adopts
a noisy channel model. For a non-standard to-
ken A, this method finds the most possible stan-
dard word ?S based on the Bayes rule: ?S =
argmaxP (S|A) = argmaxP (A|S) ? P (S).
Different methods have been used to compute
P (A|S). Pennell and Liu (2010) used a CRF se-
quence modeling approach for deletion-based ab-
breviations. Liu et al. (2011) further extended this
work by considering more types of non-standard
words without explicit pre-categorization for non-
standard tokens.
In addition, the noisy channel model has also
been utilized on the sentence level. Choudhury et
al. (2007) used a hidden Markov model to sim-
ulate SMS message generation, considering the
non-standard tokens in the input sentence as emis-
sion states in HMM and labeling results as pos-
sible candidates. Cook and Stevenson (2009) ex-
tended work by adding several more subsystems
in this error model according to the most common
non-standard token?s formation process.
Machine translation (MT) is another commonly
chosen method for text normalization. It is also
used on both the token and the sentence level. Aw
et al. (2006) treated SMS as another language, and
used MT methods to translate this ?foreign lan-
guage? to regular English. Contractor et al. (2010)
used an MT model as well but the focus of their
work is to utilize an unsupervised method to clean
noisy text. Pennell and Liu (2011) firstly intro-
duced an MT method at the token level which
translates an unnormalized token to a possible cor-
86
rect word.
Recently, a new line of work surges relying on
the analysis of huge amount of twitter data, of-
ten in an unsupervised fashion. By using con-
text information from a large corpus, Han et al.
(2012) generated possible variant and normaliza-
tion pairs, and constructed a dictionary of lexical
variants of known words, which are further ranked
by string similarity. This dictionary can facilitate
lexical normalization via simple string substitu-
tion. Hassan and Menezes (2013) proposed an ap-
proach based on the random walk algorithm on a
contextual similarity bipartite graph, constructed
from n-gram sequences on a large unlabeled text
corpus. Yang and Eisenstein (2013) presented a
unified unsupervised statistical model for text nor-
malization.
2 Previous Normalization Methods Used
in Reranking
In this work we adopt several normalization meth-
ods developed in previous studies. The following
briefly describes these previous approaches. Next
section will introduce our proposed methods using
unsupervised learning and discriminative rerank-
ing for system combination.
2.1 Character-block level MT
Pennell and Liu (2011) proposed to use a
character-level MT model for text normalization.
The idea is similar to traditional translation,
except that the translation unit is characters,
not words. Formally, for a non-standard word
A = a
1
a
2
...a
n
, the MT method finds the
most likely standard word S = s
1
s
2
...s
m
(a
i
and s
i
are the characters in the words): S =
argmaxP (S|A) = argmaxP (A|S)P (S) =
argmaxP (a
1
a
2
...a
n
|s
1
s
2
...s
m
)P (s
1
s
2
...s
m
)
where P (a
1
a
2
...a
n
|s
1
s
2
...s
m
) is from a character-
level translation model, and P (s
1
s
2
...s
m
) is from
a character-level language model. (Li and Liu,
2012a) modified this approach to perform the
translation at the character-block level in order
to generate better alignment between characters
(analogous to the word vs. phrase based alignment
in traditional MT). This system generates one
ranked list of word candidates.
2.2 Character-level Two-step MT
Li and Liu (2012b) extended the character-level
MT model by incorporating the pronunciation in-
formation. They first translate non-standard words
to possible pronunciations, which are then trans-
lated to standard words in the second step. This
method has been shown to yield high coverage
(high accuracy in its n-best hypotheses). There are
two candidate lists generated by this two-step MT
method. The first one is based on the pronuncia-
tion list produced in the first step (some phonetic
sequences directly correspond to standard words).
The second list is generated from the second trans-
lation step.
2.3 Character-Block level Sequence Labeling
Pennell and Liu (2010) used sequence labeling
model (CRF) for normalizing deletion-based ab-
breviation at the character-level. The model labels
every character in a standard word as ?Y? or ?N?
to represent whether it appears or not in a possible
abbreviation token. The features used for the clas-
sification task represent the character?s position,
pronunciation and context information. Using the
sequence labeling model, a standard word can
generate many possible non-standard words. A re-
verse look-up table is used to store the correspond-
ing possible standard words for the non-standard
words for reverse lookup during testing. Liu et al.
(2011) extended the above model to handle other
types of non-standard words. (Li and Liu, 2012a)
used character-blocks (same ones as that in the
character-block MT method above) as the units in
this sequence labeling framework. There is one
list of word candidates from this method.
2.4 Spell Checker
The forth normalization subsystem is the Jazzy
Spell Checker1, which is based on edit distance
and integrates a phonetic matching algorithm as
well. This provides one list of hypotheses.
3 Proposed Method
All the above models except the Spell Checker are
supervised methods that need labeled data con-
sisting of pairs of non-standard words and proper
words. In this paper we propose an unsupervised
method to create the lookup table of the non-
standard words and their corresponding proper
words offline. We further propose to use differ-
ent discriminative reranking approaches to com-
bine multiple individual systems.
1http://jazzy.sourceforge.net
87
3.1 Unsupervised Corpus-based Similarity
for Normalization
Previous work has shown that unlabeled text can
be used to induce unsupervised word clusters
that can improve performance of many supervised
NLP tasks (Koo et al., 2008; Turian et al., 2010;
Ta?ckstro?m et al., 2012). We investigate using a
large unlabeled Twitter corpus to automatically
identify pairs of non-standard words and their cor-
responding standard words.
We use the Edinburgh Twitter corpus (Petro-
vic et al., 2010), and a dictionary obtained
from http://ciba.iciba.com/ to identify all the in-
vocabulary and out-of-vocabulary (OOV) words in
the corpus. The task is then to automatically find
the corresponding OOV words (if any) for each
dictionary word, and the likelihood of each pair.
The key question is how to compute this likelihood
or similarity.
We propose to use an unsupervised method
based on the large corpus to induce dense real-
valued low-dimension word embedding and then
use the inner product as a measure of semantic
similarity. We use the continuous bag-of-words
model that is similar to the feedforward neural
network language model to compute vector rep-
resentations of words. This model was first in-
troduced by (Mikolov et al., 2013). We use the
tool word2vec2 to implement this model. Two
constraints are used in order to eliminate unlikely
word pairs: (I) OOV words need to begin with the
same letter as the dictionary standard word; (II)
OOV words can only consist of English letter and
digits.
In addition to considering the above semantic
similarity, for the normalization task, we use other
information including the surface character level
similarity based on longest common sequence be-
tween the two tokens, and the frequency of the to-
ken. The final score between a dictionary word w
and an OOV word t is:
sim(w, t) =
longest common string(w, t)
length(t)
? log(TermFreq(t))
? inner product(vec(w), vec(t))
?
longest common seq(w, t)
length(t)
(1)
The first and second term share the same property
of visual prime value used in (Liu et al., 2012a).
2https://code.google.com/p/word2vec/
The third term is the vector-based semantic simi-
larity of the two words, calculated by our proposed
model. The last term is the length of longest com-
mon sequence between the two words divided by
the length of the OOV word.
Using this method, we can identify all the pos-
sible OOV words for each dictionary word based
on an unlabeled large corpus. Each pair has a
similarity score. Then a reverse lookup table is
created to store the corresponding possible stan-
dard words for each non-standard word, which is
used during testing. This framework is similar to
the sequence labeling method described in Sec-
tion 2.3 in the sense of creating the mapping ta-
ble between the OOV and dictionary words. How-
ever, the difference is that this is an unsupervised
method whereas the sequence labeling uses super-
vised learning to generate possible candidates.
3.2 Reranking for System Combination
3.2.1 Word Level Reranking
Each of the above systems has its own strength and
weakness. The MT model and the sequence la-
beling models have better precision, the two-step
MT model has a broader coverage of candidates,
and the spell checker has a high confidence for
simple non-standard words. Therefore combining
these systems is expected to yield better overall
results. We propose to use a supervised maximum
entropy reranking model to combine our proposed
unsupervised method with those described in Sec-
tion 2 (4 systems that have 5 candidate lists). The
features we used in the normalization reranking
model are shown in Table 1. This maxent rerank-
ing method has shown success in many previous
work such as (Charniak and Johnson, 2005; Ji et
al., 2006).
Features:
1.Boolean value to indicate whether a candidate is on the
list of each system. There are 6 lists and thus 6 such fea-
tures.
2.A concatenation of the 6 boolean features above.
3.The position of this candidate in each candidate list. If
this candidate is not on a list, the value of this feature is -1
for that list.
4.The unigram language model probability of the candi-
date.
5.Boolean value to indicate whether the first character of
the candidate and non-standard word is the same.
6.Boolean value to indicate whether the last character of
the candidate and non-standard word is the same.
Table 1: Features for Reranking.
The first three features are related to the indi-
88
vidual systems, and the last three features com-
pare the candidate with the non-standard word. It
is computationally expensive to include informa-
tion represented in the last three features in the in-
dividual systems since they need to consider more
candidates in the normalization step; whereas in
reranking, only a small set of word candidates
are evaluated, thus it is more feasible to use such
global features in the reranking model. We also
tried some other lexical features such as the length
difference of the non-standard word and the can-
didate, whether non-standard word contains num-
bers, etc. But they did not obtain performance
gain. Another advantage of the reranker is that we
can use information about multiple systems, such
as the first three features.
3.2.2 Sentence Level Reranking and
Decoding
In the above reranking method, we only use infor-
mation about the individual words. When contex-
tual words are available (in sentences or Tweets),
we can use that information. If a sentence con-
taining OOV words is given during testing, we
can perform standard sentence level Viterbi decod-
ing to combine information from the normaliza-
tion candidates and language model scores.
Furthermore, if sentences are available during
training (not just isolated word pairs as used in all
the previous supervised individual systems and the
Maxent reranking above), we can also use contex-
tual information for training the reranker. This can
be achieved in two different ways. First, we add
the Language Model score from context words as
features in the reranker. In this work, in addition to
the features in Table 1, we add a trigram probabil-
ity to represent the context information. For every
candidate of a non-standard word, we use trigram
probability from the language model. The trigram
consists of this candidate, and the previous and the
following token of the non-standard word. If the
previous/following word is also a non-standard to-
ken, then we calculate the trigram using all of their
candidates and then take the average. After adding
the additional LM probability feature, the same
Maxent reranking method as above is used, which
optimizes the word level accuracy.
The second method is to change the training ob-
jective and perform the optimization at the sen-
tence level. The feature set can be the same as the
word level reranker, or with the additional contex-
tual LM score features. To train the model (feature
weights), we perform sentence level Viterbi de-
coding on the training set to find the best hypoth-
esis for each non-standard word. If the hypothe-
sis is incorrect, we update the feature weight us-
ing structured perceptron strategy (Collins, 2002).
We will explore these different feature and train-
ing configurations for reranking in the following
experiments.
4 Experiments
4.1 Experimental Setup
The following data sets are used in our experi-
ments. We use Data 1 and Data 2 as test data, and
Data 3 as training data for all the supervised mod-
els.
? Data 1: 558 pairs of non-standard tokens and
standard words collected from 549 tweets in
2010 by (Han and Baldwin, 2011).
? Data 2: 3,962 pairs of non-standard tokens
and standard words collected from 6,160
tweets between 2009 and 2010 by (Liu et al.,
2011).
? Data 3: 2,333 unique pairs of non-standard
tokens and standard words, collected from
2,577 Twitter messages (selected from the
Edinburgh Twitter corpus) used in (Pennell
and Liu, 2011). We made some changes on
this data, removing the pairs that have more
than one proper words, and sentences that
only contain such pairs.3
? Data 4: About 10 million twitter messages
selected from the the Edinburgh Twitter cor-
pus mentioned above, consisting of 3 million
unique tokens. This data is used by the un-
supervised method to create the mapping ta-
ble, and also for building the word-based lan-
guage model needed in sentence level nor-
malization.
The dictionary we used is obtained from
http://ciba.iciba.com/, which includes 75,262 En-
glish word entries and their corresponding pho-
netic symbols (IPA symbols). This is used in var-
ious modules in the normalization systems. The
number of the final standard words used to create
the look-up table is 10,105 because we only use
the words that have the same number of character-
block segments and phones. These 10,105 words
3http://www.hlt.utdallas.edu/?chenli/normalization
89
cover 90.77% and 93.74% standard words in Data
set 1 and Data set 2 respectively. For the non-
standard words created in the CRF model, they
cover 80.47% and 86.47% non-standard words in
Data set1 and Data set 2. This coverage using the
non-standard words identified by the new unsuper-
vised model is 91.99% and 92.32% for the two
data sets, higher than that by the CRF model.
During experiments, we use CRF++ toolkit 4
for our sequence labeling model, SRILM toolkit
(Stolcke, 2002) to build all the language models,
Giza++ (Och and Ney, 2003) for automatic word
alignment, and Moses (Koehn et al., 2007) for
translation decoding in three MT systems.
4.2 Isolated Word Normalization
Experiments
Table 2 shows the isolated word normalization re-
sults on the two test data sets for various systems.
The performance metrics include the accuracy for
the top-1 candidate and other top-N candidates.
Coverage means how many test cases correct an-
swers can be obtained in the final list regardless
of its positions. The top part presents the results
on Data Set 1 and the bottom shows the results on
Data Set 2. We can see that our proposed unsu-
pervised corpus similarity model achieves better
top-1 accuracy than the other individual systems
described in Section 2. Its top-n coverage is not
always the best ? the 2-step MT method has advan-
tages in its coverage. The results in the table also
show that reranking improves system performance
over any of the used individual systems, which is
expected. After reranking, on Data set 1, our sys-
tem yields better performance than previously re-
ported ones. On Data set 2, it has better top-1 ac-
curacy than (Liu et al., 2012a), but slightly worse
top-N coverage. However, the method in (Liu et
al., 2012a) has higher computational cost because
of the calculation of the prime visual values for
each non-standard word on the fly during testing.
In addition, they also used more training data than
ours.
4.3 Sentence Level Normalization Results
We have already seen that after reranking we ob-
tain better word-level normalization performance,
for both top-1 and other top-N candidates. One
follow-up question is whether this improved per-
formance carries over to sentence level normaliza-
4http://crfpp.googlecode.com/
System Accuracy %Top1 Top3 Top10 Top20 Cover
Data 1
MT 61.81 73.53 78.50 79.57 80.00
MT21 39.61 52.93 63.59 65.36 65.72
MT22 53.64 68.56 77.44 80.46 88.10
SL 53.29 61.99 69.09 71.92 75.85
SC 50.27 56.31 56.84 57.02 57.02
UCS 61.81 69.98 74.60 76.55 82.17
Rerank 77.14 86.96 93.04 94.82 95.90
Sys1 75.69 n/a n/a n/a n/a
Sys2 73 81.9 86.7 89.2 94.2
Data 2
MT 55.02 63.3 66.99 67.77 68.00
MT21 35.64 47.65 54.67 56.01 56.4
MT22 49.02 62.49 70.99 74.86 80.07
SL 46.52 55.05 61.21 62.97 66.21
SC 51.16 55.48 55.88 55.88 55.88
UCS 57.29 65.75 70.55 72.64 80.84
Rerank 74.44 84.57 90.25 92.37 93.5
Sys1 69.81 82.51 92.24 93.79 95.71
Sys2 62.6 75.1 84 87.5 90.7
Sys3 73.04 n/a n/a n/a n/a
Table 2: MT: Character-block Level MT;
MT21&MT22: First&Second step in Character-
level Two-step MT; SL: Sequence Labeling sys-
tem; SC: Spell Checker; UCS: Unsupervised Cor-
pus Similarity Model; Sys1 is from (Liu et al.,
2012a); Sys2 is from (Li and Liu, 2012a); Sys3
is from (Yang and Eisenstein, 2013).
tion when context information is used via the in-
corporation of a language model. Since detecting
which tokens need normalization in the first place
is a hard task itself in social media text and is an
open question currently, similar to some previous
work, we assume that we already know the non-
standard words that need to be normalized for a
given sentence. Then the sentence-level normal-
ization task is just to find which candidate from
the n-best lists for each of those already ?detected?
non-standard words is the best one. We use the
tweets in the Data set 1 described above because
Data set 2 only has token pairs but not sentences.
Table 3 shows the sentence level normaliza-
tion results using different reranking configura-
tions with respect to the features used in the
reranker and the training process. Regarding fea-
tures, reranker 1 and 3 use the features described
90
in Section 3.2.1, i.e., features based on the words
only, without the additional trigram LM probabil-
ity feature; reranker 2 and 4 use the additional LM
probability feature. About training, reranker 1 and
2 use the Maxent reranking that is trained and op-
timized for the word level; reranker 3 and 4 use
structure perceptron training at the sentence level.
Note that all of the systems perform Viterbi decod-
ing during testing to determine the final top one
candidate for each non-standard word in the sen-
tence. The scores from the reranked normalization
output and the LM probabilities are combined in
decoding. From the results, we can see that adding
contextual information (LM probabilities) as fea-
tures in the reranker is useful. When this feature
is not used, using sentence-level training objec-
tive benefits (reranker 3 outperforms 1); however,
when this feature is used, performing sentence-
level training via structure perceptron is not useful
(reranker 2 outperforms 4), partly because the con-
textual information is incorporated in the features
already and using it in sentence-level decoding for
training is redundant and does not bring additional
gain. Finally compared to the previously report
results, our system performs the best.
System Acc % System Acc %
Reranker1 84.30 Reranker2 86.91
Reranker3 85.03 Reranker4 85.37
Sys1 84.13 Sys2 82.23
Table 3: Sentence level normalization results on
Data Set 1 using different reranking setups. Sys1
is from (Liu et al., 2012a); Sys2 is from (Yang and
Eisenstein, 2013). Acc % is the top one accuracy.
4.4 Impact of Unsupervised Corpus
Similarity Model
Our last question is regarding unsupervised model
importance in the reranking system and contribu-
tions of its different similarity measure compo-
nents. We conduct the following two experiments:
First, we removed the new model and just use the
other remaining models in reranking (five candi-
date lists). Second, we kept this new model but
changed the corpus similarity measure (removed
the third item in Eq(1) that represents the seman-
tic similarity). This way we can evaluate the im-
pact of the semantic similarity measure based on
the continuous word vector representation.
Table 4 shows the word level and sentence re-
sults on Data set 1 and 2 using these different
setups. Because of space limit, we only present
the top one accuracy. The other top-n results
have similar patterns. Sentence level normaliza-
tion uses the Reranker 2 described above. We can
see that there is a degradation in both of the new
setups, suggesting that the unsupervised method
itself is beneficial, and in particular the word vec-
tor based semantic similarity component is crucial
to the system performance.
System Word Level Sent LevelData1 Data2 Data1
system-A 73.75 70.33 84.51
system-B 74.77 70.83 86.22
system-C 77.14 74.44 86.91
Table 4: Word level and Sentence level normaliza-
tion results (top-1 accuracy in %) after reranking
on Data Set 1 and 2. System-A is without using
the unsupervised model, system-B is without its
semantic similarity measure, and system-C is our
proposed system.
5 Conclusions
In this paper, we proposed a novel normalization
system by using unsupervised methods in a large
corpus to identify non-standard words and their
corresponding proper words. We further combine
it with several previously developed normalization
systems by a reranking strategy. In addition, we
explored different sentence level reranking meth-
ods to evaluate the impact of context information.
Our experiments show that the reranking system
not only significantly improves the word level nor-
malization accuracy, but also helps the sentence
level decoding. In the future work, we plan to ex-
plore more useful features and also leverage pair-
wise and link reranking strategy.
Acknowledgments
We thank the NSF for travel and conference sup-
port for this paper. The work is also partially sup-
ported by DARPA Contract No. FA8750-13-2-
0041. Any opinions, findings, and conclusions or
recommendations expressed are those of the au-
thor and do not necessarily reflect the views of the
funding agencies.
91
References
Aiti Aw, Min Zhang, Juan Xiao, Jian Su, and Jian Su.
2006. A phrase-based statistical model for sms text
normalization. In Processing of COLING/ACL.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd ACL.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure
of texting language. IJDAR.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP.
Danish Contractor, Tanveer A. Faruquie, L. Venkata
Subramaniam, and L. Venkata Subramaniam. 2010.
Unsupervised cleansing of noisy text. In Proceed-
ings of COLING.
Paul Cook and Suzanne Stevenson. 2009. An unsu-
pervised model for text message normalization. In
Proceedings of NAACL.
Fred J Damerau. 1964. A technique for computer de-
tection and correction of spelling errors. Communi-
cations of the ACM, 7(3):171?176.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twitter.
In Proceeding of 49th ACL.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary
for microblogs. In Proceedings of the 2012 EMNLP.
Hany Hassan and Arul Menezes. 2013. Social text
normalization using contextual graph random walks.
In Proceedings of ACL.
Heng Ji, Cynthia Rudin, and Ralph Grishman. 2006.
Re-ranking algorithms for name tagging. In Pro-
ceedings of the Workshop on Computationally Hard
Problems and Joint Inference in Speech and Lan-
guage Processing.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, Evan Herbst, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of ACL.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL.
Vladimir I Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. In
Soviet physics doklady, volume 10, page 707.
Chen Li and Yang Liu. 2012a. Improving text nor-
malization using character-blocks based models and
system combination. In Proceedings of COLING
2012.
Chen Li and Yang Liu. 2012b. Normalization of text
messages using character- and phone-based machine
translation approaches. In Proceedings of 13th In-
terspeech.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011. Insertion, deletion, or substitution?: normal-
izing text messages without pre-categorization nor
supervision. In Proceedings of the 49th ACL: short
papers.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012a. A
broad-coverage normalization system for social me-
dia language. In Proceedings of the 50th ACL.
Xiaohua Liu, Ming Zhou, Xiangyang Zhou,
Zhongyang Fu, and Furu Wei. 2012b. Joint
inference of named entity recognition and normal-
ization for tweets. In Proceedings of ACL.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. Proceedings of Workshop at
ICLR.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL.
Deana Pennell and Yang Liu. 2010. Normalization of
text messages for text-to-speech. In ICASSP.
Deana Pennell and Yang Liu. 2011. A character-level
machine translation approach for normalization of
sms abbreviations. In Proceedings of 5th IJCNLP.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. The edinburgh twitter corpus. In Proceedings
of NAACL.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of EMNLP.
Richard Sproat, Alan W. Black, Stanley F. Chen,
Shankar Kumar, Mari Ostendorf, and Christopher
Richards. 2001. Normalization of non-standard
words. Computer Speech & Language, 15(3):287?
333.
Andreas Stolcke. 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing.
92
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
NAACL.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of ACL.
Yi Yang and Jacob Eisenstein. 2013. A log-linear
model for unsupervised text normalization. In Pro-
ceedings of EMNLP.
93
