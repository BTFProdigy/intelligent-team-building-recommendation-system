Regularisation Techniques for Conditional
Random Fields: Parameterised Versus
Parameter-Free
Andrew Smith and Miles Osborne
School of Informatics, University of Edinburgh, United Kingdom
a.p.smith-2@sms.ed.ac.uk, miles@inf.ed.ac.uk
Abstract. Recent work on Conditional Random Fields (CRFs) has
demonstrated the need for regularisation when applying these models
to real-world NLP data sets. Conventional approaches to regularising
CRFs has focused on using a Gaussian prior over the model parameters.
In this paper we explore other possibilities for CRF regularisation. We
examine alternative choices of prior distribution and we relax the usual
simplifying assumptions made with the use of a prior, such as constant
hyperparameter values across features. In addition, we contrast the effec-
tiveness of priors with an alternative, parameter-free approach. Specifi-
cally, we employ logarithmic opinion pools (LOPs). Our results show
that a LOP of CRFs can outperform a standard unregularised CRF and
attain a performance level close to that of a regularised CRF, without
the need for intensive hyperparameter search.
1 Introduction
Recent work on Conditional Random Fields (CRFs) has demonstrated the need
for regularisation when applying these models to real-world NLP data sets ([8],
[9]). Standard approaches to regularising CRFs, and log-linear models in general,
has focused on the use of a Gaussian prior. Typically, for simplicity, this prior is
assumed to have zero mean and constant variance across model parameters. To
date, there has been little work exploring other possibilities. One exception is
Peng & McCallum [8]. They investigated feature-dependent variance for a Gaus-
sian prior, and explored different families of feature sets. They also compared
different priors for CRFs on an information extraction task.
In the first part of this paper, we compare priors for CRFs on standard
sequence labelling tasks in NLP: NER and POS tagging. Peng & McCallum
used variable hyperparameter values only for a Gaussian prior, based on feature
counts in the training data. We use an alternative Bayesian approach to mea-
sure confidence in empirical expected feature counts, and apply this to all the
priors we test. We also look at varying the Gaussian prior mean. Our results
show that: (1) considerable search is required to identify good hyperparameter
values for all priors (2) for optimal hyperparameter values, the priors we tested
perform roughly equally well (3) in some cases performance can be improved
using feature-dependent hyperparameter values.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 896?907, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Regularisation Techniques for Conditional Random Fields 897
As can be seen, a significant short-coming of using priors for CRF regular-
isation is the requirement for intensive search of hyperparameter space. In the
second part of the paper we contrast this parameterised prior approach with
an alternative, parameter-free method. We factor the CRF distribution into a
weighted product of individual expert CRF distributions, each focusing on a
particular subset of the distribution. We call this model a logarithmic opinion
pool (LOP) of CRFs (LOP-CRFs).
Our results show that LOP-CRFs, which are unregularised, can outperform
the unregularised standard CRF and attain a performance level that rivals that
of the standard CRF regularised with a prior. This performance may be achieved
with a considerably lower time for training by avoiding the need for intensive
hyperparameter search.
2 Conditional Random Fields
A linear chain CRF defines the conditional probability of a label sequence s
given an observed sequence o via:
p(s | o) = 1
Z(o)
exp
(
T+1
?
t=1
?
k
?kfk(st?1, st,o, t)
)
(1)
where T is the length of both sequences, ?k are parameters of the model and Z(o)
is the partition function that ensures (1) represents a probability distribution.
The functions fk are feature functions representing the occurrence of different
events in the sequences s and o.
The parameters ?k can be estimated by maximising the conditional log-
likelihood of a set of labelled training sequences. The log-likelihood is given by:
LL(?) =
?
o
p?(o)
?
s
p?(s|o)
[
T+1
?
t=1
? ? f(s,o, t)
]
?
?
o
p?(o) log Z(o; ?)
where p?(s|O) and p?(o) are empirical distributions defined by the training set. At
the maximum likelihood solution the model satisfies a set of feature constraints,
whereby the expected count of each feature under the model is equal to its
empirical count on the training data:
Ep?(o,s)[fk] ? Ep(s|o)[fk] = 0, ?k
In general this cannot be solved for the ?k in closed form so numerical routines
must be used. Malouf [6] and Sha & Pereira [9] show that gradient-based algo-
rithms, particularly limited memory variable metric (LMVM), require much less
time to reach convergence, for some NLP tasks, than the iterative scaling meth-
ods previously used for log-linear optimisation problems. In all our experiments
we use the LMVM method to train the CRFs.
For CRFs with general graphical structure, calculation of Ep(s|o)[fk] is in-
tractable, but for the linear chain case Lafferty et al [5] describe an efficient
898 A. Smith and M. Osborne
dynamic programming procedure for inference, similar in nature to the forward-
backward algorithm in hidden Markov models.
Given a trained CRF model defined as in (1), the most probable labelling
under the model for a new observed sequence o is given by argmaxsp(s|o). This
can be recovered efficiently using the Viterbi algorithm.
3 Parameterised Regularisation: Priors for CRFs
Most approaches to CRF regularisation have focused on the use of a prior distri-
bution over the model parameters. A prior distribution encodes prior knowledge
about the nature of different models. However, prior knowledge can be difficult
to encode reliably and the optimal choice of prior family may vary from task to
task. In this paper we investigate the use of three prior families for the CRF.
3.1 Gaussian Prior
The most common prior used for CRF regularisation has been the Gaussian. Use
of the Gaussian prior assumes that each model parameter is drawn independently
from a Gaussian distribution. Ignoring terms that do not affect the parameters,
the regularised log-likelihood with a Gaussian prior becomes:
LL(?) ? 1
2
?
k
(
?k ? ?k
?k
)2
where ?k is the mean and ?k the variance for parameter ?k. At the optimal
point, for each ?k, the model satisfies:
Ep?(o,s)[fk] ? Ep(s|o)[fk] =
?k ? ?k
?2k
(2)
Usually, for simplicity, each ?k is assumed zero and ?k is held constant across
the parameters. In this paper we investigate other possibilities. In particular,
we allow the means to take on non-zero values, and the variances to be feature-
dependent. This is described in more detail later. In each case values for means
and variances may be optimised on a development set.
We can see from (2) that use of a Gaussian prior enforces the constraint that
the expected count of a feature under the model is discounted with respect to the
count of that feature on the training data. As discussed in [1], this corresponds
to a form of logarithmic discounting in feature count space and is similar in
nature to discounting schemes employed in language modelling.
3.2 Laplacian Prior
Use of the Laplacian prior assumes that each model parameter is drawn inde-
pendently from the Laplacian distribution. Ignoring terms that do not affect the
parameters, the regularised log-likelihood with a Laplacian prior becomes:
LL(?) ?
?
k
|?k|
?k
Regularisation Techniques for Conditional Random Fields 899
where ?k is a hyperparameter, and at the optimal point the model satisfies:
Ep?(o,s)[fk] ? Ep(s|o)[fk] =
sign(?k)
?k
, ?k = 0 (3)
Peng & McCallum [8] note that the exponential prior (a one-sided version of the
Laplacian prior here) represents applying an absolute discount to the empirical
feature count. They fix the ?k across features and set it using an expression for
the discount used in absolute discounting for language modelling. By contrast we
allow the ?k to vary with feature and optimise values using a development set.
The derivative of the penalty term above with respect to a parameter ?k is
discontinuous at ?k = 0. To tackle this problem we use an approach described
by Williams, who shows how the discontinuity may be handled algorithmically
[13]. His method leads to sparse solutions, where, at convergence, a substantial
proportion of the model parameters are zero. The result of this pruning effect is
different, however, to feature induction, where features are included in the model
based on their effect on log-likelihood.
3.3 Hyperbolic Prior
Use of the hyperbolic prior assumes that each model parameter is drawn inde-
pendently from the hyperbolic distribution. Ignoring constant terms that do not
involve the parameters, the regularised log-likelihood becomes:
LL(?) ?
?
k
log
(
e?k?k + e??k?k
2
)
where ?k is a hyperparameter, and at the optimal point the model satisfies:
Ep?(o,s)[fk] ? Ep(s|o)[fk] = ?k
(
e?k?k ? e??k?k
e?k?k + e??k?k
)
(4)
3.4 Feature-Dependent Regularisation
For simplicity it is usual when using a prior to assume constant hyperparameter
values across all features. However, as a hyperparameter value determines the
amount of regularisation applied to a feature, we may not want to assume equal
values. We may have seen some features more frequently than others and so
be more confident that their empirical expected counts are closer to the true
expected counts in the underlying distribution.
Peng & McCallum [8] explore feature-dependent variance for the Gaussian
prior. They use different schemes to determine the variance for a feature based
on its observed count in the training data. In this paper we take an alternative,
Bayesian approach motivated more directly by our confidence in the reliability
of a feature?s empirical expected count.
In equations (2), (3) and (4) the level of regularisation applied to a feature
takes the form of a discount to the expected count of the feature on the training
900 A. Smith and M. Osborne
data. It is natural, therefore, that the size of this discount, controlled through
a hyperparameter, is related to our confidence in the reliability of the empiri-
cal expected count. We formulate a measure of this confidence. We follow the
approach of Kazama & Tsujii [4], extending it to CRFs.
The empirical expected count, Ep?(o,s)[fk], of a feature fk is given by:
?
o,s
p?(o, s)
?
t
fk(st?1, st,o, t)=
?
o
p?(o)
?
s
p?(s|o)
?
t
fk(st?1, st,o, t)
=
?
o
p?(o)
?
t,s?,s??
p?(st?1 = s?, st = s??|o)fk(s?, s??,o, t)
Now, our CRF features have the following form:
fk(st?1, st,o, t) =
{
1 if st?1 = s1, st = s2 and hk(o, t) = 1
0 otherwise
where s1 and s2 are the labels associated with feature fk and hk(o, t) is a binary-
valued predicate defined on observation sequence o at position t. With this
feature definition, and contracting notation for the empirical probability to save
space, Ep?(o,s)[fk] becomes:
?
o
p?(o)
?
t,s?,s??
p?(s?, s??|o)?(s?, s1)?(s??, s2)hk(o, t) =
?
o
p?(o)
?
t
p?(s1, s2|o)hk(o, t)
=
?
o
p?(o)
?
t:hk(o,t)=1
p?(s1, s2|o)
Contributions to the inner sum are only made at positions t in sequence o where
the hk(o, t) = 1. Suppose that we make the assumption that at these positions
p?(s?, s??|o) ? p?(s?, s??|hk(o, t) = 1). Then:
Ep?(o,s)[fk] =
?
o
p?(o)
?
t:hk(o,t)=1
p?(s1, s2|hk(o, t) = 1)
Now, if we assume that we can get a reasonable estimate of p?(o) from the training
data then the only source of uncertainty in the expression for Ep?(o,s)[fk] is the term
p?(st?1 = s1, st = s2|hk(o, t) = 1). Assuming this term is independent of sequence
o and position t, we can model it as the parameter ? of a Bernoulli random variable
that takes the value 1 when feature fk is active and 0 when the feature is not active
but hk(o, t) = 1. Suppose there are a and b instances of these two events, respec-
tively. We endow the Bernoulli parameter with a uniform prior Beta distribution
Be(1,1) and, having observed the training data, we calculate the variance of the
posterior distribution, Be(1 + a, 1 + b). The variance is given by:
var[?] = V =
(1 + a)(1 + b)
(a + b + 2)2(a + b + 3)
The variance of Ep?(o,s)[fk] therefore given by:
var
[
Ep?(o,s)[fk]
]
= V
?
?
?
o
?
t:hk(o,t)=1
p?(o)2
?
?
Regularisation Techniques for Conditional Random Fields 901
We use this variance as a measure of the confidence we have in Ep?(o,s)[fk] as an
estimate of the true expected count of feature fk. We therefore adjust hyper-
parameters in the different priors according to this confidence for each feature.
Note that this value for each feature can be calculated off-line.
4 Parameter-Free Regularisation: Logarithmic Opinion
Pools
So far we have considered CRF regularisation through the use of a prior. As
we have seen, most prior distributions are parameterised by a hyperparameter,
which may be used to tune the level of regularisation. In this paper we also
consider a parameter-free method. Specifically, we explore the use of logarithmic
opinion pools [3].
Given a set of CRF model experts with conditional distributions p?(s|o)
and a set of non-negative weights w? with
?
? w? = 1, a logarithmic opinion
pool is defined as the distribution:
p?(s|o) = 1
Z?(o)
?
?
[p?(s|o)]w? , with Z?(o) =
?
s
?
?
[p?(s|o)]w?
Suppose that there is a ?true? conditional distribution q(s|o) which each
p?(s|o) is attempting to model. In [3] Heskes shows that the KL divergence
between q(s|o) and the LOP can be decomposed as follows:
K (q, p?) =
?
?
w?K (q, p?) ?
?
?
w?K (p?, p?) = E ? A (5)
This explicitly tells us that the closeness of the LOP model to q(s|o) is governed
by a trade-off between two terms: an E term, which represents the closeness of
the individual experts to q(s|o), and an A term, which represents the closeness of
the individual experts to the LOP, and therefore indirectly to each other. Hence
for the LOP to model q well, we desire models p? which are individually good
models of q (having low E) and are also diverse (having large A).
Training LOPs for CRFs. The weights w? may be defined a priori or may be
found by optimising an objective criterion. In this paper we combine pre-trained
expert CRF models under a LOP and train the weights w? to maximise the
likelihood of the training data under the LOP. See [10] for details.
Decoding LOPs for CRFs. Because of the log-linear form of a CRF, a
weighted product of expert CRF distributions corresponds to a single CRF distri-
bution with log potentials given by a linear combination (with the same weights)
of the corresponding log potentials of the experts. Consequently, it is easy to form
the LOP given a set of weights and expert models, and decoding with the LOP
is no more complex than decoding with a standard CRF. Hence LOP decoding
can be achieved efficiently using the Viterbi algorithm.
902 A. Smith and M. Osborne
5 The Tasks
In this paper we compare parametric and LOP-based regularisation techniques
for CRFs on two sequence labelling tasks in NLP: named entity recognition
(NER) and part-of-speech tagging (POS tagging).
5.1 Named Entity Recognition
All our results for NER are reported on the CoNLL-2003 shared task dataset
[12]. For this dataset the entity types are: persons (PER), locations (LOC),
organisations (ORG) and miscellaneous (MISC). The training set consists of
14, 987 sentences and 204, 567 tokens, the development set consists of 3, 466
sentences and 51, 578 tokens and the test set consists of 3, 684 sentences and
46, 666 tokens.
5.2 Part-of-Speech Tagging
For our experiments we use the CoNLL-2000 shared task dataset [11]. This has
48 different POS tags. In order to make training time manageable, we collapse
the number of POS tags from 48 to 5 following the procedure used in [7]. In
summary: (1) All types of noun collapse to category N. (2) All types of verb
collapse to category V. (3) All types of adjective collapse to category J. (4)
All types of adverb collapse to category R. (5) All other POS tags collapse to
category O. The training set consists of 7, 300 sentences and 173, 542 tokens, the
development set consists of 1, 636 sentences and 38, 185 tokens and the test set
consists of 2, 012 sentences and 47, 377 tokens.
5.3 Experts and Expert Sets
As we have seen, our parameter-free LOP models require us to define and train
a number of expert models. For each task we define a single, complex CRF,
which we call a monolithic CRF, and a range of expert sets. The monolithic
CRF for NER comprises a number of word and POS features in a window of five
words around the current word, along with a set of orthographic features defined
on the current word. The monolithic CRF for NER has 450, 345 features. The
monolithic CRF for POS tagging comprises word and POS features similar to
those in the NER monolithic model, but over a smaller number of orthographic
features. The monolithic model for POS tagging has 188, 488 features.
Each of our expert sets consists of a number of CRF experts. Usually these
experts are designed to focus on modelling a particular aspect or subset of the
distribution. The experts from a particular expert set are combined under a
LOP-CRF with the unregularised monolithic CRF.
We define our expert sets as follows: (1) Simple consists of the monolithic
CRF and a single expert comprising a reduced subset of the features in the
monolithic CRF. This reduced CRF models the entire distribution rather than
focusing on a particular aspect or subset, but is much less expressive than the
Regularisation Techniques for Conditional Random Fields 903
monolithic model. The reduced model comprises 24, 818 features for NER and
47, 420 features for POS tagging. (2) Positional consists of the monolithic CRF
and a partition of the features in the monolithic CRF into three experts, each
consisting only of features that involve events either behind, at or ahead of
the current sequence position. (3) Label consists of the monolithic CRF and a
partition of the features in the monolithic CRF into five experts, one for each
label. For NER an expert corresponding to label X consists only of features that
involve labels B-X or I-X at the current or previous positions, while for POS
tagging an expert corresponding to label X consists only of features that involve
label X at the current or previous positions. These experts therefore focus on
trying to model the distribution of a particular label. (4) Random consists of
the monolithic CRF and a random partition of the features in the monolithic
CRF into four experts. This acts as a baseline to ascertain the performance
that can be expected from an expert set that is not defined via any linguistic
intuition.
6 Experimental Results
For each task our baseline model is the monolithic model, as defined earlier.
All the smoothing approaches that we investigate are applied to this model. For
NER we report F-scores on the development and test sets, while for POS tagging
we report accuracies on the development and test sets.
6.1 Priors
Feature-Independent Hyperparameters. Tables 1 and 2 give results on the
two tasks for different priors with feature-independent hyperparameters. In the
case of the Gaussian prior, the mean was fixed at zero with the variance being the
adjustable hyperparameter. In each case hyperparameter values were optimised
on the development set. In order to obtain the results shown, extensive search
of the hyperparameter space was required. The results show that: (1) For each
prior there is a performance improvement over the unregularised model. (2) Each
of the priors gives roughly the same optimal performance.
These results are contrary to the conclusions of Peng & McCallum in [8]. On
an information extraction task they found that the Gaussian prior performed
Table 1. F-scores for priors on NER
Prior Development Test
Unreg. monolithic 88.33 81.87
Gaussian 89.84 83.98
Laplacian 89.56 83.43
Hyperbolic 89.84 83.90
Table 2. Accuracies for priors on POS
tagging
Prior Development Test
Unreg. monolithic 97.92 97.65
Gaussian 98.02 97.84
Laplacian 98.05 97.78
Hyperbolic 98.00 97.85
904 A. Smith and M. Osborne
significantly better than alternative priors. Indeed they appeared to report per-
formance figures for the hyperbolic and Laplacian priors that were lower than
those of the unregularised model. There are several possible reasons for these
differences. Firstly, for the hyperbolic prior, Peng & McCallum appeared not
to use an adjustable hyperparameter. In that case the discount applied to each
empirical expected feature count was dependent only on the current value of the
respective model parameter and corresponds in our case to using a fixed value
of 1 for the ? hyperparameter. Our results for this value of the hyperparameter
are similarly poor. The second reason is that for the Laplacian prior, they again
used a fixed value for the hyperparameter, calculated via an absolute discount-
ing method used language modelling [1]. Having achieved poor results with this
value they experimented with other values but obtained even worse performance.
By contrast, we find that, with some search of the hyperparameter space, we can
achieve performance close to that of the other two priors.
Feature-Dependent Hyperparameters. Tables 3 and 4 give results for dif-
ferent priors with feature-dependent hyperparameters. Again, for the Gaussian
prior the mean was held at 0. We see here that trends differ between the two
tasks. For POS tagging we see performance improvements with all the priors over
the corresponding feature-independent hyperparameter case. Using McNemar?s
matched-pairs test [2] on point-wise labelling errors, and testing at a significance
level of 5% level, all values in Table 4 represent a significant improvement over
the corresponding model with feature-independent hyperparameter values, ex-
cept the one marked with ?. However, for NER the opposite is true. There is a
performance degradation over the corresponding feature-independent hyperpa-
rameter case. Values marked with ? are significantly worse at the 5% level. The
hyperbolic prior performs particularly badly, giving no improvement over the
unregularised monolithic. The reasons for these results are not clear. One pos-
sibility is that defining the degree of regularisation on a feature specific basis is
too dependent on the sporadic properties of the training data. A better idea may
be to use an approach part-way between feature-independent hyperparameters
and feature-specific hyperparameters. For example, features could be clustered
based on confidence in their empirical expected counts, with a single confidence
being associated with each cluster.
Varying the Gaussian Mean. When using a Gaussian prior it is usual to fix
the mean at zero because there is usually no prior information to suggest penal-
ising large positive values of model parameters any more or less than large mag-
Table 3. F-scores for priors on NER
Prior Development Test
Gaussian 89.43 83.27?
Laplacian 89.28 83.37
Hyperbolic 88.34? 81.63?
Table 4. Accuracies for priors on POS
tagging
Prior Development Test
Gaussian 98.12 97.88?
Laplacian 98.12 97.92
Hyperbolic 98.15 97.92
Regularisation Techniques for Conditional Random Fields 905
nitude negative values. It also simplifies the hyperparameter search, requiring
the need to optimise only the variance hyperparameter. However, it is unlikely
that optimal performance is always achieved for a mean value of zero.
To investigate this we fix the Gaussian variance at the optimal value found
earlier on the development set, with a mean of zero, and allow the mean to
vary away from zero. For both tasks we found that we could achieve significant
performance improvements for non-zero mean values. On NER a model with
mean 0.7 (and variance 40) achieved an F-score of 90.56% on the development set
and 84.71% on the test set, a significant improvement over the best model with
mean 0. We observe a similar pattern for POS tagging. These results suggest
that considerable benefit may be gained from a well structured search of the
joint mean and variance hyperparameter space when using a Gaussian prior
for regularisation. There is of course a trade-off here, however, between finding
better hyperparameters values and suffering increased search complexity.
6.2 LOP-CRFs
Tables 5 and 6 show the performance of LOP-CRFs for the NER and POS
tagging experts respectively. The results demonstrate that: (1) In every case
the LOPs significantly outperform the unregularised monolithic. (2) In most
cases the performance of LOPs is comparable to that obtained using the different
priors on each task. In fact, values marked with ? show a significant improvement
over the performance obtained with the Gaussian prior with feature-independent
hyperparameter values. Only the value marked with ? in Table 6 significantly
under performs that model.
Table 5. LOP F-scores on NER
Expert set Development set Test set
Unreg. monolithic 88.33 81.87
Simple 90.26 84.22?
Positional 90.35 84.71?
Label 89.30 83.27
Random 88.84 83.06
Table 6. LOP accuracies on POS tagging
Expert set Development set Test set
Unreg. monolithic 97.92 97.65
Simple 98.31? 98.12?
Positional 98.03 97.81
Label 97.99 97.77
Random 97.99 97.76?
We can see that the performance of the LOP-CRFs varies with the choice of
expert set. For example, on NER the LOP-CRFs for the simple and positional
expert sets perform better than those for the label and random sets. Looking
back to equation 5, we conjecture that the simple and positional expert sets
achieve good performance in the LOP-CRF because they consist of experts that
are diverse while simultaneously being reasonable models of the data. The label
expert set exhibits greater diversity between the experts, because each expert
focuses on modelling a particular label only, but each expert is a relatively poor
model of the entire distribution. Similarly, the random experts are in general
better models of the entire distribution but tend to be less diverse because they
906 A. Smith and M. Osborne
do not focus on any one aspect or subset of it. Intuitively, then, we want to
devise experts that are simultaneously diverse and accurate.
The advantage of the LOP-CRF approach over the use of a prior is that it
is ?parameter-free? in the sense that each expert in the LOP-CRF is unregu-
larised. Consequently, we are not required to search a hyperparameter space.
For example, to carefully tune the hyperbolic hyperparameter in order to obtain
the optimal value we report here, we ran models for 20 different hyperparameter
values. In addition, in most cases the expert CRFs comprising the expert sets
are small, compact models that train more quickly than the monolithic with a
prior, and can be trained in parallel.
7 Conclusion
In this paper we compare parameterised and parameter-free approaches to
smoothing CRFs on two standard sequence labelling tasks in NLP. For the
parameterised methods, we compare different priors. We use both feature-
independent and feature-dependent hyperparameters in the prior distributions.
In the latter case we derive hyperparameter values using a Bayesian approach
to measuring our confidence in empirical expected feature counts. We find that:
(1) considerable search is required to identify good hyperparameter values for
all priors (2) for optimal hyperparameter values, the priors we tested perform
roughly equally well (3) in some cases performance can be improved using
feature-dependent hyperparameter values.
We contrast the use of priors to an alternative, parameter-free method using
logarithmic opinion pools. Our results show that a LOP of CRFs, which contains
unregularised models, can outperform the unregularised standard CRF and at-
tain a performance level that rivals that of the standard CRF regularised with a
prior. The important point, however, is that this performance may be achieved
with a considerably lower time for training by avoiding the need for intensive
hyperparameter search.
References
1. Chen, S. and Rosenfeld, R.: A Survey of Smoothing Techniques for ME Models.
IEEE Transactions on Speech and Audio Processing (2000) 8(1) 37?50
2. Gillick, L., Cox, S.: Some statistical issues in the comparison of speech recognition
algorithms. ICASSP (1989) 1 532?535
3. Heskes, T.: Selecting weighting factors in logarithmic opinion pools. NIPS (1998)
4. Kazama, J. and Tsujii, J.: Evaluation and Extension of Maximum Entropy Models
with Inequality Constraints. EMNLP (2003)
5. Lafferty, J. and McCallum, A. and Pereira, F.: Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Sequence Data. ICML (2001)
6. Malouf, R.: A comparison of algorithms for maximum entropy parameter estima-
tion. CoNLL (2002)
7. McCallum, A., Rohanimanesh, K. Sutton, C.: Dynamic Conditional Random Fields
for Jointly Labeling Multiple Sequences. NIPS Workshop on Syntax, Semantics,
Statistics (2003)
Regularisation Techniques for Conditional Random Fields 907
8. Peng, F. and McCallum, A.: Accurate Information Extraction from Research Pa-
pers using Conditional Random Fields. HLT-NAACL (2004)
9. Sha, F. and Pereira, F.: Shallow Parsing with Conditional Random Fields. HLT-
NAACL (2003)
10. Smith, A., Cohn, T., Osborne, M.: Logarithmic Opinion Pools for Conditional
Random Fields. ACL (2005)
11. Tjong Kim Sang, E. F. and Buchholz, S.: Introduction to the CoNLL-2000 shared
task: Chunking. CoNLL (2000)
12. Tjong Kim Sang, E. F. and De Meulder, F.: Introduction to the CoNLL-2003
Shared Task: Language-Independent Named Entity Recognition. CoNLL (2003)
13. Williams, P.: Bayesian Regularisation and Pruning using a Laplace Prior. Neural
Computation (1995) 7(1) 117?143
Proceedings of the 43rd Annual Meeting of the ACL, pages 10?17,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Scaling Conditional Random Fields Using Error-Correcting Codes
Trevor Cohn
Department of Computer Science
and Software Engineering
University of Melbourne, Australia
tacohn@csse.unimelb.edu.au
Andrew Smith
Division of Informatics
University of Edinburgh
United Kingdom
a.p.smith-2@sms.ed.ac.uk
Miles Osborne
Division of Informatics
University of Edinburgh
United Kingdom
miles@inf.ed.ac.uk
Abstract
Conditional Random Fields (CRFs) have
been applied with considerable success to
a number of natural language processing
tasks. However, these tasks have mostly
involved very small label sets. When
deployed on tasks with larger label
sets, the requirements for computational
resources mean that training becomes
intractable.
This paper describes a method for train-
ing CRFs on such tasks, using error cor-
recting output codes (ECOC). A number
of CRFs are independently trained on the
separate binary labelling tasks of distin-
guishing between a subset of the labels
and its complement. During decoding,
these models are combined to produce a
predicted label sequence which is resilient
to errors by individual models.
Error-correcting CRF training is much
less resource intensive and has a much
faster training time than a standardly
formulated CRF, while decoding
performance remains quite comparable.
This allows us to scale CRFs to previously
impossible tasks, as demonstrated by our
experiments with large label sets.
1 Introduction
Conditional random fields (CRFs) (Lafferty et
al., 2001) are probabilistic models for labelling
sequential data. CRFs are undirected graphical
models that define a conditional distribution over
label sequences given an observation sequence.
They allow the use of arbitrary, overlapping,
non-independent features as a result of their global
conditioning. This allows us to avoid making
unwarranted independence assumptions over the
observation sequence, such as those required by
typical generative models.
Efficient inference and training methods exist
when the graphical structure of the model forms
a chain, where each position in a sequence is
connected to its adjacent positions. CRFs have been
applied with impressive empirical results to the
tasks of named entity recognition (McCallum and
Li, 2003), simplified part-of-speech (POS) tagging
(Lafferty et al, 2001), noun phrase chunking (Sha
and Pereira, 2003) and extraction of tabular data
(Pinto et al, 2003), among other tasks.
CRFs are usually estimated using gradient-based
methods such as limited memory variable metric
(LMVM). However, even with these efficient
methods, training can be slow. Consequently, most
of the tasks to which CRFs have been applied are
relatively small scale, having only a small number
of training examples and small label sets. For
much larger tasks, with hundreds of labels and
millions of examples, current training methods
prove intractable. Although training can potentially
be parallelised and thus run more quickly on large
clusters of computers, this in itself is not a solution
to the problem: tasks can reasonably be expected
to increase in size and complexity much faster
than any increase in computing power. In order to
provide scalability, the factors which most affect the
resource usage and runtime of the training method
10
must be addressed directly ? ideally the dependence
on the number of labels should be reduced.
This paper presents an approach which enables
CRFs to be used on larger tasks, with a significant
reduction in the time and resources needed for
training. This reduction does not come at the cost
of performance ? the results obtained on benchmark
natural language problems compare favourably,
and sometimes exceed, the results produced from
regular CRF training. Error correcting output
codes (ECOC) (Dietterich and Bakiri, 1995) are
used to train a community of CRFs on binary
tasks, with each discriminating between a subset
of the labels and its complement. Inference is
performed by applying these ?weak? models to an
unknown example, with each component model
removing some ambiguity when predicting the label
sequence. Given a sufficient number of binary
models predicting suitably diverse label subsets, the
label sequence can be inferred while being robust
to a number of individual errors from the weak
models. As each of these weak models are binary,
individually they can be efficiently trained, even
on large problems. The number of weak learners
required to achieve good performance is shown to
be relatively small on practical tasks, such that the
overall complexity of error-correcting CRF training
is found to be much less than that of regular CRF
training methods.
We have evaluated the error-correcting CRF on
the CoNLL 2003 named entity recognition (NER)
task (Sang and Meulder, 2003), where we show
that the method yields similar generalisation perfor-
mance to standardly formulated CRFs, while requir-
ing only a fraction of the resources, and no increase
in training time. We have also shown how the error-
correcting CRF scales when applied to the larger
task of POS tagging the Penn Treebank and also
the even larger task of simultaneously noun phrase
chunking (NPC) and POS tagging using the CoNLL
2000 data-set (Sang and Buchholz, 2000).
2 Conditional random fields
CRFs are undirected graphical models used to spec-
ify the conditional probability of an assignment of
output labels given a set of input observations. We
consider only the case where the output labels of the
model are connected by edges to form a linear chain.
The joint distribution of the label sequence, y, given
the input observation sequence, x, is given by
p(y|x) = 1Z(x) exp
T+1
?
t=1
?
k
?kfk(t,yt?1,yt,x)
where T is the length of both sequences and ?k are
the parameters of the model. The functions fk are
feature functions which map properties of the obser-
vation and the labelling into a scalar value. Z(x)
is the partition function which ensures that p is a
probability distribution.
A number of algorithms can be used to find the
optimal parameter values by maximising the log-
likelihood of the training data. Assuming that the
training sequences are drawn IID from the popula-
tion, the conditional log likelihood L is given by
L =
?
i
log p(y(i)|x(i))
=
?
i
?
?
?
T (i)+1
?
t=1
?
k
?kfk(t,y(i)t?1,y
(i)
t ,x(i))
? log Z(x(i))
}
where x(i) and y(i) are the ith observation and label
sequence. Note that a prior is often included in the
L formulation; it has been excluded here for clar-
ity of exposition. CRF estimation methods include
generalised iterative scaling (GIS), improved itera-
tive scaling (IIS) and a variety of gradient based
methods. In recent empirical studies on maximum
entropy models and CRFs, limited memory variable
metric (LMVM) has proven to be the most efficient
method (Malouf, 2002; Wallach, 2002); accord-
ingly, we have used LMVM for CRF estimation.
Every iteration of LMVM training requires the
computation of the log-likelihood and its deriva-
tive with respect to each parameter. The partition
function Z(x) can be calculated efficiently using
dynamic programming with the forward algorithm.
Z(x) is given by?y ?T (y) where ? are the forward
values, defined recursively as
?t+1(y) =
?
y?
?t(y?) exp
?
k
?kfk(t + 1, y?, y,x)
11
The derivative of the log-likelihood is given by
?L
??k
=
?
i
?
?
?
T (i)+1
?
t=1
fk(t,y(i)t?1,y
(i)
t ,x(i))
?
?
y
p(y|x(i))
T (i)+1
?
t=1
fk(t,yt?1,yt,x(i))
?
?
?
The first term is the empirical count of feature k,
and the second is the expected count of the feature
under the model. When the derivative equals zero ?
at convergence ? these two terms are equal. Evalu-
ating the first term of the derivative is quite simple.
However, the sum over all possible labellings in the
second term poses more difficulties. This term can
be factorised, yielding
?
t
?
y?,y
p(Yt?1 = y?, Yt = y|x(i))fk(t, y?, y,x(i))
This term uses the marginal distribution over pairs of
labels, which can be efficiently computed from the
forward and backward values as
?t?1(y?) exp
?
k ?kfk(t, y?, y,x(i))?t(y)
Z(x(i))
The backward probabilities ? are defined by the
recursive relation
?t(y) =
?
y?
?t+1(y?) exp
?
k
?kfk(t + 1, y, y?,x)
Typically CRF training using LMVM requires
many hundreds or thousands of iterations, each of
which involves calculating of the log-likelihood
and its derivative. The time complexity of a single
iteration is O(L2NTF ) where L is the number
of labels, N is the number of sequences, T is
the average length of the sequences, and F is
the average number of activated features of each
labelled clique. It is not currently possible to state
precise bounds on the number of iterations required
for certain problems; however, problems with a
large number of sequences often require many more
iterations to converge than problems with fewer
sequences. Note that efficient CRF implementations
cache the feature values for every possible clique
labelling of the training data, which leads to a
memory requirement with the same complexity of
O(L2NTF ) ? quite demanding even for current
computer hardware.
3 Error Correcting Output Codes
Since the time and space complexity of CRF
estimation is dominated by the square of the number
of labels, it follows that reducing the number
of labels will significantly reduce the complexity.
Error-correcting coding is an approach which recasts
multiple label problems into a set of binary label
problems, each of which is of lesser complexity than
the full multiclass problem. Interestingly, training a
set of binary CRF classifiers is overall much more
efficient than training a full multi-label model. This
is because error-correcting CRF training reduces
the L2 complexity term to a constant. Decoding
proceeds by predicting these binary labels and then
recovering the encoded actual label.
Error-correcting output codes have been used for
text classification, as in Berger (1999), on which the
following is based. Begin by assigning to each of the
m labels a unique n-bit string Ci, which we will call
the code for this label. Now train n binary classi-
fiers, one for each column of the coding matrix (con-
structed by taking the labels? codes as rows). The j th
classifier, ?j , takes as positive instances those with
label i where Cij = 1. In this way, each classifier
learns a different concept, discriminating between
different subsets of the labels.
We denote the set of binary classifiers as
? = {?1, ?2, . . . , ?n}, which can be used for
prediction as follows. Classify a novel instance x
with each of the binary classifiers, yielding a n-bit
vector ?(x) = {?1(x), ?2(x), . . . , ?n(x)}. Now
compare this vector to the codes for each label. The
vector may not exactly match any of the labels due
to errors in the individual classifiers, and thus we
chose the actual label which minimises the distance
argmini?(?(x), Ci). Typically the Hamming
distance is used, which simply measures the number
of differing bit positions. In this manner, prediction
is resilient to a number of prediction errors by the
binary classifiers, provided the codes for the labels
are sufficiently diverse.
3.1 Error-correcting CRF training
Error-correcting codes can also be applied to
sequence labellers, such as CRFs, which are capable
of multiclass labelling. ECOCs can be used with
CRFs in a similar manner to that given above for
12
classifiers. A series of CRFs are trained, each
on a relabelled variant of the training data. The
relabelling for each binary CRF maps the labels
into binary space using the relevant column of the
coding matrix, such that label i is taken as a positive
for the jth model example if Cij = 1.
Training with a binary label set reduces the time
and space complexity for each training iteration to
O(NTF ); the L2 term is now a constant. Pro-
vided the code is relatively short (i.e. there are
few binary models, or weak learners), this translates
into considerable time and space savings. Coding
theory doesn?t offer any insights into the optimal
code length (i.e. the number of weak learners).
When using a very short code, the error-correcting
CRF will not adequately model the decision bound-
aries between all classes. However, using a long
code will lead to a higher degree of dependency
between pairs of classifiers, where both model simi-
lar concepts. The generalisation performance should
improve quickly as the number of weak learners
(code length) increases, but these gains will diminish
as the inter-classifier dependence increases.
3.2 Error-correcting CRF decoding
While training of error-correcting CRFs is simply
a logical extension of the ECOC classifier method
to sequence labellers, decoding is a different mat-
ter. We have applied three decoding different strate-
gies. The Standalone method requires each binary
CRF to find the Viterbi path for a given sequence,
yielding a string of 0s and 1s for each model. For
each position t in the sequence, the tth bit from
each model is taken, and the resultant bit string
compared to each of the label codes. The label
with the minimum Hamming distance is then cho-
sen as the predicted label for that site. This method
allows for error correction to occur at each site, how-
ever it discards information about the uncertainty of
each weak learner, instead only considering the most
probable paths.
The Marginals method of decoding uses the
marginal probability distribution at each position
in the sequence instead of the Viterbi paths. This
distribution is easily computed using the forward
backward algorithm. The decoding proceeds as
before, however instead of a bit string we have a
vector of probabilities. This vector is compared
to each of the label codes using the L1 distance,
and the closest label is chosen. While this method
incorporates the uncertainty of the binary models, it
does so at the expense of the path information in the
sequence.
Neither of these decoding methods allow the
models to interact, although each individual weak
learner may benefit from the predictions of the
other weak learners. The Product decoding method
addresses this problem. It treats each weak model
as an independent predictor of the label sequence,
such that the probability of the label sequence given
the observations can be re-expressed as the product
of the probabilities assigned by each weak model.
A given labelling y is projected into a bit string for
each weak learner, such that the ith entry in the
string is Ckj for the jth weak learner, where k is
the index of label yi. The weak learners can then
estimate the probability of the bit string; these are
then combined into a global product to give the
probability of the label sequence
p(y|x) = 1Z ?(x)
?
j
pj(bj(y)|x)
where pj(q|x) is the predicted probability of q given
x by the jth weak learner, bj(y) is the bit string
representing y for the jth weak learner and Z ?(x)
is the partition function. The log probability is
?
j
{Fj(bj(y), x) ? ?j ? log Zj(x)} ? log Z ?(x)
where Fj(y, x) = ?T+1t=1 fj(t,yt?1,yt,x). This log
probability can then be maximised using the Viterbi
algorithm as before, noting that the two log terms are
constant with respect to y and thus need not be eval-
uated. Note that this decoding is an equivalent for-
mulation to a uniformly weighted logarithmic opin-
ion pool, as described in Smith et al (2005).
Of the three decoding methods, Standalone
has the lowest complexity, requiring only a binary
Viterbi decoding for each weak learner. Marginals
is slightly more complex, requiring the forward
and backward values. Product, however, requires
Viterbi decoding with the full label set, and many
features ? the union of the features of each weak
learner ? which can be quite computationally
demanding.
13
3.3 Choice of code
The accuracy of ECOC methods are highly depen-
dent on the quality of the code. The ideal code
has diverse rows, yielding a high error-correcting
capability, and diverse columns such that the weak
learners model highly independent concepts. When
the number of labels, k, is small, an exhaustive
code with every unique column is reasonable, given
there are 2k?1 ? 1 unique columns. With larger
label sets, columns must be selected with care to
maximise the inter-row and inter-column separation.
This can be done by randomly sampling the column
space, in which case the probability of poor separa-
tion diminishes quickly as the number of columns
increases (Berger, 1999). Algebraic codes, such as
BCH codes, are an alternative coding scheme which
can provide near-optimal error-correcting capabil-
ity (MacWilliams and Sloane, 1977), however these
codes provide no guarantee of good column separa-
tion.
4 Experiments
Our experiments show that error-correcting CRFs
are highly accurate on benchmark problems with
small label sets, as well as on larger problems with
many more labels, which would be otherwise prove
intractable for traditional CRFs. Moreover, with a
good code, the time and resources required for train-
ing and decoding can be much less than that of the
standardly formulated CRF.
4.1 Named entity recognition
CRFs have been used with strong results on the
CoNLL 2003 NER task (McCallum, 2003) and thus
this task is included here as a benchmark. This data
set consists of a 14,987 training sentences (204,567
tokens) drawn from news articles, tagged for per-
son, location, organisation and miscellaneous enti-
ties. There are 8 IOB-2 style labels.
A multiclass (standardly formulated) CRF was
trained on these data using features covering word
identity, word prefix and suffix, orthographic tests
for digits, case and internal punctuation, word
length, POS tag and POS tag bigrams before and
after the current word. Only features seen at least
once in the training data were included in the model,
resulting in 450,345 binary features. The model was
Model Decoding MLE Regularised
Multiclass 88.04 89.78
Coded standalone 88.23? 88.67?
marginals 88.23? 89.19
product 88.69? 89.69
Table 1: F1 scores on NER task.
trained without regularisation and with a Gaussian
prior. An exhaustive code was created with all
127 unique columns. All of the weak learners
were trained with the same feature set, each having
around 315,000 features. The performance of the
standard and error-correcting models are shown in
Table 1. We tested for statistical significance using
the matched pairs test (Gillick and Cox, 1989) at
p < 0.001. Those results which are significantly
better than the corresponding multiclass MLE or
regularised model are flagged with a ?, and those
which are significantly worse with a ?.
These results show that error-correcting CRF
training achieves quite similar performance to the
multiclass CRF on the task (which incidentally
exceeds McCallum (2003)?s result of 89.0 using
feature induction). Product decoding was the
better of the three methods, giving the best
performance both with and without regularisation,
although this difference was only statistically
significant between the regularised standalone and
the regularised product decoding. The unregularised
error-correcting CRF significantly outperformed
the multiclass CRF with all decoding strategies,
suggesting that the method already provides some
regularisation, or corrects some inherent bias in the
model.
Using such a large number of weak learners is
costly, in this case taking roughly ten times longer
to train than the multiclass CRF. However, much
shorter codes can also achieve similar results. The
simplest code, where each weak learner predicts
only a single label (a.k.a. one-vs-all), achieved an
F score of 89.56, while only requiring 8 weak learn-
ers and less than half the training time as the multi-
class CRF. This code has no error correcting capa-
bility, suggesting that the code?s column separation
(and thus interdependence between weak learners)
is more important than its row separation.
14
An exhaustive code was used in this experiment
simply for illustrative purposes: many columns
in this code were unnecessary, yielding only a
slight gain in performance over much simpler
codes while incurring a very large increase in
training time. Therefore, by selecting a good subset
of the exhaustive code, it should be possible to
reduce the training time while preserving the strong
generalisation performance. One approach is to
incorporate skew in the label distribution in our
choice of code ? the code should minimise the
confusability of commonly occurring labels more
so than that of rare labels. Assuming that errors
made by the weak learners are independent, the
probability of a single error, q, as a function of the
code length n can be bounded by
q(n) ? 1 ?
?
l
p(l)
bhl?12 c
?
i=0
(
n
i
)
p?i(1 ? p?)n?i
where p(l) is the marginal probability of the label l,
hl is the minimum Hamming distance between l and
any other label, and p? is the maximum probability
of an error by a weak learner. The performance
achieved by selecting the code with the minimum
loss bound from a large random sample of codes
is shown in Figure 1, using standalone decoding,
where p? was estimated on the development set. For
comparison, randomly sampled codes and a greedy
oracle are shown. The two random sampled codes
show those samples where no column is repeated,
and where duplicate columns are permitted (random
with replacement). The oracle repeatedly adds to the
code the column which most improves its F1 score.
The minimum loss bound method allows the per-
formance plateau to be reached more quickly than
random sampling; i.e. shorter codes can be used,
thus allowing more efficient training and decoding.
Note also that multiclass CRF training required
830Mb of memory, while error-correcting training
required only 380Mb. Decoding of the test set
(51,362 tokens) with the error-correcting model
(exhaustive, MLE) took between 150 seconds for
standalone decoding and 173 seconds for integrated
decoding. The multiclass CRF was much faster,
taking only 31 seconds, however this time difference
could be reduced with suitable optimisations.
 83
 84
 85
 86
 87
 88
 89
 90
 10  15  20  25  30  35  40  45  50
F1
 s
co
re
code length
random
random with replacement
minimum loss bound
oracle
MLE multiclass CRF
Regularised multiclass CRF
Figure 1: NER F1 scores for standalone decoding
with random codes, a minimum loss code and a
greedy oracle.
Coding Decoding MLE Regularised
Multiclass 95.69 95.78
Coded - 200 standalone 95.63 96.03
marginals 95.68 96.03
One-vs-all product 94.90 96.57
Table 2: POS tagging accuracy.
4.2 Part-of-speech Tagging
CRFs have been applied to POS tagging, however
only with a very simple feature set and small training
sample (Lafferty et al, 2001). We used the Penn
Treebank Wall Street Journal articles, training on
sections 2?21 and testing on section 24. In this
task there are 45,110 training sentences, a total of
1,023,863 tokens and 45 labels.
The features used included word identity, prefix
and suffix, whether the word contains a number,
uppercase letter or a hyphen, and the words one
and two positions before and after the current word.
A random code of 200 columns was used for this
task. These results are shown in Table 2, along with
those of a multiclass CRF and an alternative one-vs-
all coding. As for the NER experiment, the decod-
ing performance levelled off after 100 bits, beyond
which the improvements from longer codes were
only very slight. This is a very encouraging char-
acteristic, as only a small number of weak learners
are required for good performance.
15
The random code of 200 bits required 1,300Mb
of RAM, taking a total of 293 hours to train and
3 hours to decode (54,397 tokens) on similar
machines to those used before. We do not have
figures regarding the resources used by Lafferty et
al.?s CRF for the POS tagging task and our attempts
to train a multiclass CRF for full-scale POS tagging
were thwarted due to lack of sufficient available
computing resources. Instead we trained on a
10,000 sentence subset of the training data, which
required approximately 17Gb of RAM and 208
hours to train.
Our best result on the task was achieved using
a one-vs-all code, which reduced the training
time to 25 hours, as it only required training 45
binary models. This result exceeds Lafferty et al?s
accuracy of 95.73% using a CRF but falls short of
Toutanova et al (2003)?s state-of-the-art 97.24%.
This is most probably due to our only using a
first-order Markov model and a fairly simple feature
set, where Tuotanova et al include a richer set of
features in a third order model.
4.3 Part-of-speech Tagging and Noun Phrase
Segmentation
The joint task of simultaneously POS tagging and
noun phrase chunking (NPC) was included in order
to demonstrate the scalability of error-correcting
CRFs. The data was taken from the CoNLL 2000
NPC shared task, with the model predicting both the
chunk tags and the POS tags. The training corpus
consisted of 8,936 sentences, with 47,377 tokens
and 118 labels.
A 200-bit random code was used, with the follow-
ing features: word identity within a window, pre-
fix and suffix of the current word and the presence
of a digit, hyphen or upper case letter in the cur-
rent word. This resulted in about 420,000 features
for each weak learner. A joint tagging accuracy of
90.78% was achieved using MLE training and stan-
dalone decoding. Despite the large increase in the
number of labels in comparison to the earlier tasks,
the performance also began to plateau at around 100
bits. This task required 220Mb of RAM and took a
total of 30 minutes to train each of the 200 binary
CRFs, this time on Pentium 4 machines with 1Gb
RAM. Decoding of the 47,377 test tokens took 9,748
seconds and 9,870 seconds for the standalone and
marginals methods respectively.
Sutton et al (2004) applied a variant of the CRF,
the dynamic CRF (DCRF), to the same task, mod-
elling the data with two interconnected chains where
one chain predicted NPC tags and the other POS
tags. They achieved better performance and train-
ing times than our model; however, this is not a
fair comparison, as the two approaches are orthogo-
nal. Indeed, applying the error-correcting CRF algo-
rithms to DCRF models could feasibly decrease the
complexity of the DCRF, allowing the method to be
applied to larger tasks with richer graphical struc-
tures and larger label sets.
In all three experiments, error-correcting CRFs
have achieved consistently good generalisation per-
formance. The number of weak learners required
to achieve these results was shown to be relatively
small, even for tasks with large label sets. The time
and space requirements were lower than those of a
traditional CRF for the larger tasks and, most impor-
tantly, did not increase substantially when the num-
ber of labels was increased.
5 Related work
Most recent work on improving CRF performance
has focused on feature selection. McCallum (2003)
describes a technique for greedily adding those
feature conjuncts to a CRF which significantly
improve the model?s log-likelihood. His experi-
mental results show that feature induction yields a
large increase in performance, however our results
show that standardly formulated CRFs can perform
well above their reported 73.3%, casting doubt
on the magnitude of the possible improvement.
Roark et al (2004) have also employed feature
selection to the huge task of language modelling
with a CRF, by partially training a voted perceptron
then removing all features that the are ignored
by the perceptron. The act of automatic feature
selection can be quite time consuming in itself,
while the performance and runtime gains are often
modest. Even with a reduced number of features,
tasks with a very large label space are likely to
remain intractable.
16
6 Conclusion
Standard training methods for CRFs suffer greatly
from their dependency on the number of labels,
making tasks with large label sets either difficult
or impossible. As CRFs are deployed more widely
to tasks with larger label sets this problem will
become more evident. The current ?solutions? to
these scaling problems ? namely feature selection,
and the use of large clusters ? don?t address the
heart of the problem: the dependence on the square
of number of labels.
Error-correcting CRF training allows CRFs to be
applied to larger problems and those with larger
label sets than were previously possible, without
requiring computationally demanding methods such
as feature selection. On standard tasks we have
shown that error-correcting CRFs provide compa-
rable or better performance than the standardly for-
mulated CRF, while requiring less time and space to
train. Only a small number of weak learners were
required to obtain good performance on the tasks
with large label sets, demonstrating that the method
provides efficient scalability to the CRF framework.
Error-correction codes could be applied to
other sequence labelling methods, such as the
voted perceptron (Roark et al, 2004). This may
yield an increase in performance and efficiency
of the method, as its runtime is also heavily
dependent on the number of labels. We plan to
apply error-correcting coding to dynamic CRFs,
which should result in better modelling of naturally
layered tasks, while increasing the efficiency and
scalability of the method. We also plan to develop
higher order CRFs, using error-correcting codes to
curb the increase in complexity.
7 Acknowledgements
This work was supported in part by a PORES travel-
ling scholarship from the University of Melbourne,
allowing Trevor Cohn to travel to Edinburgh.
References
Adam Berger. 1999. Error-correcting output coding for
text classification. In Proceedings of IJCAI: Workshop on
machine learning for information filtering.
Thomas G. Dietterich and Ghulum Bakiri. 1995. Solving mul-
ticlass learning problems via error-correcting output codes.
Journal of Artificial Intelligence Reseach, 2:263?286.
L. Gillick and Stephen Cox. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In Pro-
ceedings of the IEEE Conference on Acoustics, Speech and
Signal Processing, pages 532?535, Glasgow, Scotland.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labelling sequence data. In Proceedings of
ICML 2001, pages 282?289.
Florence MacWilliams and Neil Sloane. 1977. The theory of
error-correcting codes. North Holland, Amsterdam.
Robert Malouf. 2002. A comparison of algorithms for max-
imum entropy parameter estimation. In Proceedings of
CoNLL 2002, pages 49?55.
Andrew McCallum and Wei Li. 2003. Early results for named
entity recognition with conditional random fields, feature
induction and web-enhanced lexicons. In Proceedings of
CoNLL 2003, pages 188?191.
Andrew McCallum. 2003. Efficiently inducing features of
conditional random fields. In Proceedings of UAI 2003,
pages 403?410.
David Pinto, Andrew McCallum, Xing Wei, and Bruce Croft.
2003. Table extraction using conditional random fields.
In Proceedings of the Annual International ACM SIGIR
Conference on Research and Development in Information
Retrieval, pages 235?242.
Brian Roark, Murat Saraclar, Michael Collins, and Mark John-
son. 2004. Discriminative language modeling with condi-
tional random fields and the perceptron algorithm. In Pro-
ceedings of ACL 2004, pages 48?55.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduc-
tion to the CoNLL-2000 shared task: Chunking. In Proceed-
ings of CoNLL 2000 and LLL 2000, pages 127?132.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduc-
tion to the CoNLL-2003 shared task: Language-independent
named entity recognition. In Proceedings of CoNLL 2003,
pages 142?147, Edmonton, Canada.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with
conditional random fields. In Proceedings of HLT-NAACL
2003, pages 213?220.
Andrew Smith, Trevor Cohn, and Miles Osborne. 2005. Loga-
rithmic opinion pools for conditional random fields. In Pro-
ceedings of ACL 2005.
Charles Sutton, Khashayar Rohanimanesh, and Andrew McCal-
lum. 2004. Dynamic conditional random fields: Factorized
probabilistic models for labelling and segmenting sequence
data. In Proceedings of the ICML 2004.
Kristina Toutanova, Dan Klein, Christopher Manning, and
Yoram Singer. 2003. Feature rich part-of-speech tagging
with a cyclic dependency network. In Proceedings of HLT-
NAACL 2003, pages 252?259.
Hanna Wallach. 2002. Efficient training of conditional random
fields. Master?s thesis, University of Edinburgh.
17
Proceedings of the 43rd Annual Meeting of the ACL, pages 18?25,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Logarithmic Opinion Pools for Conditional Random Fields
Andrew Smith
Division of Informatics
University of Edinburgh
United Kingdom
a.p.smith-2@sms.ed.ac.uk
Trevor Cohn
Department of Computer Science
and Software Engineering
University of Melbourne, Australia
tacohn@csse.unimelb.edu.au
Miles Osborne
Division of Informatics
University of Edinburgh
United Kingdom
miles@inf.ed.ac.uk
Abstract
Recent work on Conditional Random
Fields (CRFs) has demonstrated the need
for regularisation to counter the tendency
of these models to overfit. The standard
approach to regularising CRFs involves a
prior distribution over the model parame-
ters, typically requiring search over a hy-
perparameter space. In this paper we ad-
dress the overfitting problem from a dif-
ferent perspective, by factoring the CRF
distribution into a weighted product of in-
dividual ?expert? CRF distributions. We
call this model a logarithmic opinion
pool (LOP) of CRFs (LOP-CRFs). We ap-
ply the LOP-CRF to two sequencing tasks.
Our results show that unregularised expert
CRFs with an unregularised CRF under
a LOP can outperform the unregularised
CRF, and attain a performance level close
to the regularised CRF. LOP-CRFs there-
fore provide a viable alternative to CRF
regularisation without the need for hyper-
parameter search.
1 Introduction
In recent years, conditional random fields (CRFs)
(Lafferty et al, 2001) have shown success on a num-
ber of natural language processing (NLP) tasks, in-
cluding shallow parsing (Sha and Pereira, 2003),
named entity recognition (McCallum and Li, 2003)
and information extraction from research papers
(Peng and McCallum, 2004). In general, this work
has demonstrated the susceptibility of CRFs to over-
fit the training data during parameter estimation. As
a consequence, it is now standard to use some form
of overfitting reduction in CRF training.
Recently, there have been a number of sophisti-
cated approaches to reducing overfitting in CRFs,
including automatic feature induction (McCallum,
2003) and a full Bayesian approach to training and
inference (Qi et al, 2005). These advanced meth-
ods tend to be difficult to implement and are of-
ten computationally expensive. Consequently, due
to its ease of implementation, the current standard
approach to reducing overfitting in CRFs is the use
of a prior distribution over the model parameters,
typically a Gaussian. The disadvantage with this
method, however, is that it requires adjusting the
value of one or more of the distribution?s hyper-
parameters. This usually involves manual or auto-
matic tuning on a development set, and can be an ex-
pensive process as the CRF must be retrained many
times for different hyperparameter values.
In this paper we address the overfitting problem
in CRFs from a different perspective. We factor the
CRF distribution into a weighted product of indi-
vidual expert CRF distributions, each focusing on
a particular subset of the distribution. We call this
model a logarithmic opinion pool (LOP) of CRFs
(LOP-CRFs), and provide a procedure for learning
the weight of each expert in the product. The LOP-
CRF framework is ?parameter-free? in the sense that
it does not involve the requirement to adjust hyper-
parameter values.
LOP-CRFs are theoretically advantageous in that
their Kullback-Leibler divergence with a given dis-
tribution can be explicitly represented as a function
of the KL-divergence with each of their expert dis-
tributions. This provides a well-founded framework
for designing new overfitting reduction schemes:
18
look to factorise a CRF distribution as a set of di-
verse experts.
We apply LOP-CRFs to two sequencing tasks in
NLP: named entity recognition and part-of-speech
tagging. Our results show that combination of un-
regularised expert CRFs with an unregularised stan-
dard CRF under a LOP can outperform the unreg-
ularised standard CRF, and attain a performance
level that rivals that of the regularised standard CRF.
LOP-CRFs therefore provide a viable alternative to
CRF regularisation without the need for hyperpa-
rameter search.
2 Conditional Random Fields
A linear chain CRF defines the conditional probabil-
ity of a state or label sequence s given an observed
sequence o via1:
p(s |o) = 1
Z(o) exp
(T+1
?
t=1
?
k
?k fk(st?1,st ,o, t)
)
(1)
where T is the length of both sequences, ?k are pa-
rameters of the model and Z(o) is the partition func-
tion that ensures (1) represents a probability distri-
bution. The functions fk are feature functions rep-
resenting the occurrence of different events in the
sequences s and o.
The parameters ?k can be estimated by maximis-
ing the conditional log-likelihood of a set of labelled
training sequences. The log-likelihood is given by:
L (? ) = ?
o,s
p?(o,s) log p(s |o;? )
= ?
o,s
p?(o,s)
[T+1
?
t=1
? ? f(s,o, t)
]
? ?
o
p?(o) logZ(o;? )
where p?(o,s) and p?(o) are empirical distributions
defined by the training set. At the maximum like-
lihood solution the model satisfies a set of feature
constraints, whereby the expected count of each fea-
ture under the model is equal to its empirical count
on the training data:
1In this paper we assume there is a one-to-one mapping be-
tween states and labels, though this need not be the case.
Ep?(o,s)[ fk]?Ep(s|o)[ fk] = 0, ?k
In general this cannot be solved for the ?k in
closed form so numerical routines must be used.
Malouf (2002) and Sha and Pereira (2003) show
that gradient-based algorithms, particularly limited
memory variable metric (LMVM), require much
less time to reach convergence, for some NLP tasks,
than the iterative scaling methods (Della Pietra et
al., 1997) previously used for log-linear optimisa-
tion problems. In all our experiments we use the
LMVM method to train the CRFs.
For CRFs with general graphical structure, calcu-
lation of Ep(s|o)[ fk] is intractable, but for the linear
chain case Lafferty et al (2001) describe an efficient
dynamic programming procedure for inference, sim-
ilar in nature to the forward-backward algorithm in
hidden Markov models.
3 Logarithmic Opinion Pools
In this paper an expert model refers a probabilistic
model that focuses on modelling a specific subset of
some probability distribution. The concept of com-
bining the distributions of a set of expert models via
a weighted product has previously been used in a
range of different application areas, including eco-
nomics and management science (Bordley, 1982),
and NLP (Osborne and Baldridge, 2004).
In this paper we restrict ourselves to sequence
models. Given a set of sequence model experts, in-
dexed by ? , with conditional distributions p?(s |o)
and a set of non-negative normalised weights w? , a
logarithmic opinion pool 2 is defined as the distri-
bution:
pLOP(s |o) = 1ZLOP(o) ?? [p?(s |o)]
w? (2)
with w? ? 0 and ?? w? = 1, and where ZLOP(o) is
the normalisation constant:
ZLOP(o) = ?
s
?? [p?(s |o)]
w? (3)
2Hinton (1999) introduced a variant of the LOP idea called
Product of Experts, in which expert distributions are multiplied
under a uniform weight distribution.
19
The weight w? encodes our confidence in the opin-
ion of expert ? .
Suppose that there is a ?true? conditional distri-
bution q(s | o) which each p?(s | o) is attempting to
model. Heskes (1998) shows that the KL divergence
between q(s | o) and the LOP, can be decomposed
into two terms:
K(q, pLOP) = E ?A (4)
= ?
?
w?K (q, p?)??
?
w?K (pLOP, p?)
This tells us that the closeness of the LOP model
to q(s | o) is governed by a trade-off between two
terms: an E term, which represents the closeness
of the individual experts to q(s | o), and an A term,
which represents the closeness of the individual
experts to the LOP, and therefore indirectly to each
other. Hence for the LOP to model q well, we desire
models p? which are individually good models of q
(having low E) and are also diverse (having large A).
3.1 LOPs for CRFs
Because CRFs are log-linear models, we can see
from equation (2) that CRF experts are particularly
well suited to combination under a LOP. Indeed, the
resulting LOP is itself a CRF, the LOP-CRF, with
potential functions given by a log-linear combina-
tion of the potential functions of the experts, with
weights w? . As a consequence of this, the nor-
malisation constant for the LOP-CRF can be calcu-
lated efficiently via the usual forward-backward al-
gorithm for CRFs. Note that there is a distinction be-
tween normalisation constant for the LOP-CRF, ZLOP
as given in equation (3), and the partition function of
the LOP-CRF, Z. The two are related as follows:
pLOP(s |o) = 1ZLOP(o) ?? [p?(s |o)]
w?
= 1
ZLOP(o) ??
[
U?(s |o)
Z?(o)
]w?
= ?? [U?(s |o)]
w?
ZLOP(o)?? [Z?(o)]w?
where U? = exp?T+1t=1 ?k ??k f?k(st?1,st ,o, t) and so
logZ(o) = logZLOP(o)+?
?
w? logZ?(o)
This relationship will be useful below, when we de-
scribe how to train the weights w? of a LOP-CRF.
In this paper we will use the term LOP-CRF
weights to refer to the weights w? in the weighted
product of the LOP-CRF distribution and the term
parameters to refer to the parameters ??k of each
expert CRF ? .
3.2 Training LOP-CRFs
In our LOP-CRF training procedure we first train
the expert CRFs unregularised on the training data.
Then, treating the experts as static pre-trained mod-
els, we train the LOP-CRF weights w? to maximise
the log-likelihood of the training data. This training
process is ?parameter-free? in that neither stage in-
volves the use of a prior distribution over expert CRF
parameters or LOP-CRF weights, and so avoids the
requirement to adjust hyperparameter values.
The likelihood of a data set under a LOP-CRF, as
a function of the LOP-CRF weights, is given by:
L(w) = ?
o,s
pLOP(s |o;w) p?(o,s)
= ?
o,s
[ 1
ZLOP(o;w) ?? p?(s |o)
w?
]p?(o,s)
After taking logs and rearranging, the log-
likelihood can be expressed as:
L (w) = ?
o,s
p?(o,s)?
?
w? log p?(s |o)
? ?
o
p?(o) logZLOP(o;w)
= ?
?
w? ?
o,s
p?(o,s) log p?(s |o)
+ ?
?
w? ?
o
p?(o) logZ?(o)
? ?
o
p?(o) logZ(o;w)
For the first two terms, the quantities that are mul-
tiplied by w? inside the (outer) sums are indepen-
dent of the weights, and can be evaluated once at the
20
beginning of training. The third term involves the
partition function for the LOP-CRF and so is a func-
tion of the weights. It can be evaluated efficiently as
usual for a standard CRF.
Taking derivatives with respect to w? and rear-
ranging, we obtain:
?L (w)
?w? = ?o,s p?(o,s) log p? (s |o)
+ ?
o
p?(o) logZ? (o)
? ?
o
p?(o)EpLOP(s|o)
[
?
t
logU? t(o,s)
]
where U? t(o,s) is the value of the potential function
for expert ? on clique t under the labelling s for ob-
servation o. In a way similar to the representation
of the expected feature count in a standard CRF, the
third term may be re-written as:
??
o
?
t
?
s?,s??
pLOP(st?1 = s?,st = s??,o) logU? t(s?,s??,o)
Hence the derivative is tractable because we can use
dynamic programming to efficiently calculate the
pairwise marginal distribution for the LOP-CRF.
Using these expressions we can efficiently train
the LOP-CRF weights to maximise the log-
likelihood of the data set.3 We make use of the
LMVM method mentioned earlier to do this. We
will refer to a LOP-CRF with weights trained using
this procedure as an unregularised LOP-CRF.
3.2.1 Regularisation
The ?parameter-free? aspect of the training pro-
cedure we introduced in the previous section relies
on the fact that we do not use regularisation when
training the LOP-CRF weights w? . However, there
is a possibility that this may lead to overfitting of
the training data. In order to investigate this, we
develop a regularised version of the training proce-
dure and compare the results obtained with each. We
3We must ensure that the weights are non-negative and nor-
malised. We achieve this by parameterising the weights as func-
tions of a set of unconstrained variables via a softmax transfor-
mation. The values of the log-likelihood and its derivatives with
respect to the unconstrained variables can be derived from the
corresponding values for the weights w? .
use a prior distribution over the LOP-CRF weights.
As the weights are non-negative and normalised we
use a Dirichlet distribution, whose density function
is given by:
p(w) = ?(?? ??)?? ?(??) ?? w
???1?
where the ?? are hyperparameters.
Under this distribution, ignoring terms that are
independent of the weights, the regularised log-
likelihood involves an additional term:
?
?
(?? ?1) logw?
We assume a single value ? across all weights. The
derivative of the regularised log-likelihood with
respect to weight w? then involves an additional
term 1w? (? ? 1). In our experiments we use thedevelopment set to optimise the value of ? . We will
refer to a LOP-CRF with weights trained using this
procedure as a regularised LOP-CRF.
4 The Tasks
In this paper we apply LOP-CRFs to two sequence
labelling tasks in NLP: named entity recognition
(NER) and part-of-speech tagging (POS tagging).
4.1 Named Entity Recognition
NER involves the identification of the location and
type of pre-defined entities within a sentence and is
often used as a sub-process in information extrac-
tion systems. With NER the CRF is presented with
a set of sentences and must label each word so as to
indicate whether the word appears outside an entity
(O), at the beginning of an entity of type X (B-X) or
within the continuation of an entity of type X (I-X).
All our results for NER are reported on the
CoNLL-2003 shared task dataset (Tjong Kim Sang
and De Meulder, 2003). For this dataset the en-
tity types are: persons (PER), locations (LOC),
organisations (ORG) and miscellaneous (MISC).
The training set consists of 14,987 sentences and
204,567 tokens, the development set consists of
3,466 sentences and 51,578 tokens and the test set
consists of 3,684 sentences and 46,666 tokens.
21
4.2 Part-of-Speech Tagging
POS tagging involves labelling each word in a sen-
tence with its part-of-speech, for example noun,
verb, adjective, etc. For our experiments we use the
CoNLL-2000 shared task dataset (Tjong Kim Sang
and Buchholz, 2000). This has 48 different POS
tags. In order to make training time manageable4,
we collapse the number of POS tags from 48 to 5
following the procedure used in (McCallum et al,
2003). In summary:
? All types of noun collapse to category N.
? All types of verb collapse to category V.
? All types of adjective collapse to category J.
? All types of adverb collapse to category R.
? All other POS tags collapse to category O.
The training set consists of 7,300 sentences and
173,542 tokens, the development set consists of
1,636 sentences and 38,185 tokens and the test set
consists of 2,012 sentences and 47,377 tokens.
4.3 Expert sets
For each task we compare the performance of the
LOP-CRF to that of the standard CRF by defining
a single, complex CRF, which we call a monolithic
CRF, and a range of expert sets.
The monolithic CRF for NER comprises a num-
ber of word and POS tag features in a window of
five words around the current word, along with a
set of orthographic features defined on the current
word. These are based on those found in (Curran and
Clark, 2003). Examples include whether the cur-
rent word is capitalised, is an initial, contains a digit,
contains punctuation, etc. The monolithic CRF for
NER has 450,345 features.
The monolithic CRF for POS tagging comprises
word and POS features similar to those in the NER
monolithic model, but over a smaller number of or-
thographic features. The monolithic model for POS
tagging has 188,448 features.
Each of our expert sets consists of a number of
CRF experts. Usually these experts are designed to
4See (Cohn et al, 2005) for a scaling method allowing the
full POS tagging task with CRFs.
focus on modelling a particular aspect or subset of
the distribution. As we saw earlier, the aim here is
to define experts that model parts of the distribution
well while retaining mutual diversity. The experts
from a particular expert set are combined under a
LOP-CRF and the weights are trained as described
previously.
We define our range of expert sets as follows:
? Simple consists of the monolithic CRF and a
single expert comprising a reduced subset of
the features in the monolithic CRF. This re-
duced CRF models the entire distribution rather
than focusing on a particular aspect or subset,
but is much less expressive than the monolithic
model. The reduced model comprises 24,818
features for NER and 47,420 features for POS
tagging.
? Positional consists of the monolithic CRF and
a partition of the features in the monolithic
CRF into three experts, each consisting only of
features that involve events either behind, at or
ahead of the current sequence position.
? Label consists of the monolithic CRF and a
partition of the features in the monolithic CRF
into five experts, one for each label. For NER
an expert corresponding to label X consists
only of features that involve labels B-X or I-
X at the current or previous positions, while for
POS tagging an expert corresponding to label
X consists only of features that involve label
X at the current or previous positions. These
experts therefore focus on trying to model the
distribution of a particular label.
? Random consists of the monolithic CRF and a
random partition of the features in the mono-
lithic CRF into four experts. This acts as a
baseline to ascertain the performance that can
be expected from an expert set that is not de-
fined via any linguistic intuition.
5 Experiments
To compare the performance of LOP-CRFs trained
using the procedure we described previously to that
of a standard CRF regularised with a Gaussian prior,
we do the following for both NER and POS tagging:
22
? Train a monolithic CRF with regularisation us-
ing a Gaussian prior. We use the development
set to optimise the value of the variance hyper-
parameter.
? Train every expert CRF in each expert set with-
out regularisation (each expert set includes the
monolithic CRF, which clearly need only be
trained once).
? For each expert set, create a LOP-CRF from
the expert CRFs and train the weights of the
LOP-CRF without regularisation. We compare
its performance to that of the unregularised and
regularised monolithic CRFs.
? To investigate whether training the LOP-CRF
weights contributes significantly to the LOP-
CRF?s performance, for each expert set we cre-
ate a LOP-CRF with uniform weights and com-
pare its performance to that of the LOP-CRF
with trained weights.
? To investigate whether unregularised training
of the LOP-CRF weights leads to overfitting,
for each expert set we train the weights of the
LOP-CRF with regularisation using a Dirich-
let prior. We optimise the hyperparameter in
the Dirichlet distribution on the development
set. We then compare the performance of the
LOP-CRF with regularised weights to that of
the LOP-CRF with unregularised weights.
6 Results
6.1 Experts
Before presenting results for the LOP-CRFs, we
briefly give performance figures for the monolithic
CRFs and expert CRFs in isolation. For illustration,
we do this for NER models only. Table 1 shows F
scores on the development set for the NER CRFs.
We see that, as expected, the expert CRFs in iso-
lation model the data relatively poorly compared to
the monolithic CRFs. Some of the label experts, for
example, attain relatively low F scores as they focus
only on modelling one particular label. Similar be-
haviour was observed for the POS tagging models.
Expert F score
Monolithic unreg. 88.33
Monolithic reg. 89.84
Reduced 79.62
Positional 1 86.96
Positional 2 73.11
Positional 3 73.08
Label LOC 41.96
Label MISC 22.03
Label ORG 29.13
Label PER 40.49
Label O 60.44
Random 1 70.34
Random 2 67.76
Random 3 67.97
Random 4 70.17
Table 1: Development set F scores for NER experts
6.2 LOP-CRFs with unregularised weights
In this section we present results for LOP-CRFs with
unregularised weights. Table 2 gives F scores for
NER LOP-CRFs while Table 3 gives accuracies for
the POS tagging LOP-CRFs. The monolithic CRF
scores are included for comparison. Both tables il-
lustrate the following points:
? In every case the LOP-CRFs outperform the
unregularised monolithic CRF
? In most cases the performance of LOP-CRFs
rivals that of the regularised monolithic CRF,
and in some cases exceeds it.
We use McNemar?s matched-pairs test (Gillick
and Cox, 1989) on point-wise labelling errors to ex-
amine the statistical significance of these results. We
test significance at the 5% level. At this threshold,
all the LOP-CRFs significantly outperform the cor-
responding unregularised monolithic CRF. In addi-
tion, those marked with ? show a significant im-
provement over the regularised monolithic CRF.
Only the value marked with ? in Table 3 significantly
under performs the regularised monolithic. All other
values a do not differ significantly from those of the
regularised monolithic CRF at the 5% level.
These results show that LOP-CRFs with unreg-
ularised weights can lead to performance improve-
ments that equal or exceed those achieved from a
conventional regularisation approach using a Gaus-
sian prior. The important difference, however, is that
the LOP-CRF approach is ?parameter-free? in the
23
Expert set Development set Test set
Monolithic unreg. 88.33 81.87
Monolithic reg. 89.84 83.98
Simple 90.26 84.22?
Positional 90.35 84.71?
Label 89.30 83.27
Random 88.84 83.06
Table 2: F scores for NER unregularised LOP-CRFs
Expert set Development set Test set
Monolithic unreg. 97.92 97.65
Monolithic reg. 98.02 97.84
Simple 98.31? 98.12?
Positional 98.03 97.81
Label 97.99 97.77
Random 97.99 97.76?
Table 3: Accuracies for POS tagging unregularised
LOP-CRFs
sense that each expert CRF in the LOP-CRF is un-
regularised and the LOP weight training is also un-
regularised. We are therefore not required to search
a hyperparameter space. As an illustration, to ob-
tain our best results for the POS tagging regularised
monolithic model, we re-trained using 15 different
values of the Gaussian prior variance. With the
LOP-CRF we trained each expert CRF and the LOP
weights only once.
As an illustration of a typical weight distribution
resulting from the training procedure, the positional
LOP-CRF for POS tagging attaches weight 0.45 to
the monolithic model and roughly equal weights to
the other three experts.
6.3 LOP-CRFs with uniform weights
By training LOP-CRF weights using the procedure
we introduce in this paper, we allow the weights to
take on non-uniform values. This corresponds to
letting the opinion of some experts take precedence
over others in the LOP-CRF?s decision making. An
alternative, simpler, approach would be to com-
bine the experts under a LOP with uniform weights,
thereby avoiding the weight training stage. We
would like to ascertain whether this approach will
significantly reduce the LOP-CRF?s performance.
As an illustration, Table 4 gives accuracies for LOP-
CRFs with uniform weights for POS tagging. A sim-
ilar pattern is observed for NER. Comparing these
values to those in Tables 2 and 3, we can see that in
Expert set Development set Test set
Simple 98.30 98.12
Positional 97.97 97.79
Label 97.85 97.73
Random 97.82 97.74
Table 4: Accuracies for POS tagging uniform LOP-
CRFs
general LOP-CRFs with uniform weights, although
still performing significantly better than the unreg-
ularised monolithic CRF, generally under perform
LOP-CRFs with trained weights. This suggests that
the choice of weights can be important, and justifies
the weight training stage.
6.4 LOP-CRFs with regularised weights
To investigate whether unregularised training of the
LOP-CRF weights leads to overfitting, we train
the LOP-CRF with regularisation using a Dirich-
let prior. The results we obtain show that in most
cases a LOP-CRF with regularised weights achieves
an almost identical performance to that with unreg-
ularised weights, and suggests there is little to be
gained by weight regularisation. This is probably
due to the fact that in our LOP-CRFs the number
of experts, and therefore weights, is generally small
and so there is little capacity for overfitting. We con-
jecture that although other choices of expert set may
comprise many more experts than in our examples,
the numbers are likely to be relatively small in com-
parison to, for example, the number of parameters in
the individual experts. We therefore suggest that any
overfitting effect is likely to be limited.
6.5 Choice of Expert Sets
We can see from Tables 2 and 3 that the performance
of a LOP-CRF varies with the choice of expert set.
For example, in our tasks the simple and positional
expert sets perform better than those for the label
and random sets. For an explanation here, we re-
fer back to our discussion of equation (5). We con-
jecture that the simple and positional expert sets
achieve good performance in the LOP-CRF because
they consist of experts that are diverse while simulta-
neously being reasonable models of the data. The la-
bel expert set exhibits greater diversity between the
experts, because each expert focuses on modelling a
particular label only, but each expert is a relatively
24
poor model of the entire distribution and the corre-
sponding LOP-CRF performs worse. Similarly, the
random experts are in general better models of the
entire distribution but tend to be less diverse because
they do not focus on any one aspect or subset of it.
Intuitively, then, we want to devise experts that pro-
vide diverse but accurate views on the data.
The expert sets we present in this paper were
motivated by linguistic intuition, but clearly many
choices exist. It remains an important open question
as to how to automatically construct expert sets for
good performance on a given task, and we intend to
pursue this avenue in future research.
7 Conclusion and future work
In this paper we have introduced the logarithmic
opinion pool of CRFs as a way to address overfit-
ting in CRF models. Our results show that a LOP-
CRF can provide a competitive alternative to con-
ventional regularisation with a prior while avoiding
the requirement to search a hyperparameter space.
We have seen that, for a variety of types of expert,
combination of expert CRFs with an unregularised
standard CRF under a LOP with optimised weights
can outperform the unregularised standard CRF and
rival the performance of a regularised standard CRF.
We have shown how these advantages a LOP-
CRF provides have a firm theoretical foundation in
terms of the decomposition of the KL-divergence
between a LOP-CRF and a target distribution, and
how this provides a framework for designing new
overfitting reduction schemes in terms of construct-
ing diverse experts.
In this work we have considered training the
weights of a LOP-CRF using pre-trained, static ex-
perts. In future we intend to investigate cooperative
training of LOP-CRF weights and the parameters of
each expert in an expert set.
Acknowledgements
We wish to thank Stephen Clark, our colleagues in
Edinburgh and the anonymous reviewers for many
useful comments.
References
R. F. Bordley. 1982. A multiplicative formula for aggregating
probability assessments. Management Science, (28):1137?
1148.
T. Cohn, A. Smith, and M. Osborne. 2005. Scaling conditional
random fields using error-correcting codes. In Proc. ACL
2005.
J. Curran and S. Clark. 2003. Language independent NER
using a maximum entropy tagger. In Proc. CoNLL-2003.
S. Della Pietra, Della Pietra V., and J. Lafferty. 1997. Induc-
ing features of random fields. In IEEE PAMI, volume 19(4),
pages 380?393.
L. Gillick and S. Cox. 1989. Some statistical issues in the
comparison of speech recognition algorithms. In Interna-
tional Conference on Acoustics, Speech and Signal Process-
ing, volume 1, pages 532?535.
T. Heskes. 1998. Selecting weighting factors in logarithmic
opinion pools. In Advances in Neural Information Process-
ing Systems 10.
G. E. Hinton. 1999. Product of experts. In ICANN, volume 1,
pages 1?6.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proc. ICML 2001.
R. Malouf. 2002. A comparison of algorithms for maximum
entropy parameter estimation. In Proc. CoNLL-2002.
A. McCallum and W. Li. 2003. Early results for named entity
recognition with conditional random fields, feature induction
and web-enhanced lexicons. In Proc. CoNLL-2003.
A. McCallum, K. Rohanimanesh, and C. Sutton. 2003. Dy-
namic conditional random fields for jointly labeling multiple
sequences. In NIPS-2003 Workshop on Syntax, Semantics
and Statistics.
A. McCallum. 2003. Efficiently inducing features of condi-
tional random fields. In Proc. UAI 2003.
M. Osborne and J. Baldridge. 2004. Ensemble-based active
learning for parse selection. In Proc. NAACL 2004.
F. Peng and A. McCallum. 2004. Accurate information extrac-
tion from research papers using conditional random fields.
In Proc. HLT-NAACL 2004.
Y. Qi, M. Szummer, and T. P. Minka. 2005. Bayesian condi-
tional random fields. In Proc. AISTATS 2005.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. HLT-NAACL 2003.
E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduction to
the CoNLL-2000 shared task: Chunking. In Proc. CoNLL-
2000.
E. F. Tjong Kim Sang and F. De Meulder. 2003. Introduction to
the CoNLL-2003 shared task: Language-independent named
entity recognition. In Proc. CoNLL-2003.
25
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 133?140, New York City, June 2006. c?2006 Association for Computational Linguistics
Using Gazetteers in Discriminative Information Extraction
Andrew Smith
Division of Informatics
University of Edinburgh
United Kingdom
a.p.smith-2@sms.ed.ac.uk
Miles Osborne
Division of Informatics
University of Edinburgh
United Kingdom
miles@inf.ed.ac.uk
Abstract
Much work on information extraction has
successfully used gazetteers to recognise
uncommon entities that cannot be reliably
identified from local context alone. Ap-
proaches to such tasks often involve the
use of maximum entropy-style models,
where gazetteers usually appear as highly
informative features in the model. Al-
though such features can improve model
accuracy, they can also introduce hidden
negative effects. In this paper we de-
scribe and analyse these effects and sug-
gest ways in which they may be overcome.
In particular, we show that by quarantin-
ing gazetteer features and training them
in a separate model, then decoding using
a logarithmic opinion pool (Smith et al,
2005), we may achieve much higher accu-
racy. Finally, we suggest ways in which
other features with gazetteer feature-like
behaviour may be identified.
1 Introduction
In recent years discriminative probabilistic models
have been successfully applied to a number of infor-
mation extraction tasks in natural language process-
ing (NLP), such as named entity recognition (NER)
(McCallum and Li, 2003), noun phrase chunking
(Sha and Pereira, 2003) and information extraction
from research papers (Peng and McCallum, 2004).
Discriminative models offer a significant advantage
over their generative counterparts by allowing the
specification of powerful, possibly non-independent
features which would be difficult to tractably encode
in a generative model.
In a task such as NER, one sometimes encoun-
ters an entity which is difficult to identify using lo-
cal contextual cues alone because the entity has not
be seen before. In these cases, a gazetteer or dic-
tionary of possible entity identifiers is often useful.
Such identifiers could be names of people, places,
companies or other organisations. Using gazetteers
one may define additional features in the model that
represent the dependencies between a word?s NER
label and its presence in a particular gazetteer. Such
gazetteer features are often highly informative, and
their inclusion in the model should in principle re-
sult in higher model accuracy. However, these fea-
tures can also introduce hidden negative effects tak-
ing the form of labelling errors that the model makes
at places where a model without the gazetteer fea-
tures would have labelled correctly. Consequently,
ensuring optimal usage of gazetteers can be difficult.
In this paper we describe and analyse the labelling
errors made by a model, and show that they gen-
erally result from the model?s over-dependence on
the gazetteer features for making labelling decisions.
By including gazetteer features in the model we
may, in some cases, transfer too much explanatory
dependency to the gazetteer features from the non-
gazetteer features. In order to avoid this problem, a
more careful treatment of these features is required
during training. We demonstrate that a traditional
regularisation approach, where different features are
regularised to different degrees, does not offer a sat-
133
isfactory solution. Instead, we show that by training
gazetteer features in a separate model to the other
features, and decoding using a logarithmic opinion
pool (LOP) (Smith et al, 2005), much greater ac-
curacy can be obtained. Finally, we identify other
features with gazetteer feature-like properties and
show that similar results may be obtained using our
method with these features.
We take as our model a linear chain conditional
random field (CRF), and apply it to NER in English.
2 Conditional Random Fields
A linear chain conditional random field (CRF) (Laf-
ferty et al, 2001) defines the conditional probability
of a label sequence s given an observed sequence o
via:
p
 
s  o 
1
Z
 
o 
exp

T  1
?
t  1
?
k
?k fk
 
st  1 	 st 	 o 	 t 
 (1)
where T is the length of both sequences, ?k are pa-rameters of the model and Z   o  is a partition func-
tion that ensures that (1) represents a probability dis-
tribution. The functions fk are feature functions rep-resenting the occurrence of different events in the
sequences s and o.
The parameters ?k can be estimated by maximis-ing the conditional log-likelihood of a set of labelled
training sequences. At the maximum likelihood so-
lution the model satisfies a set of feature constraints,
whereby the expected count of each feature under
the model is equal to its empirical count on the train-
ing data:
E p?  o  s  fk  Ep  s  o  fk   0 	 k
In general this cannot be solved for the ?k in closedform, so numerical optimisation must be used. For
our experiments we use the limited memory variable
metric (LMVM) (Sha and Pereira, 2003) routine,
which has become the standard algorithm for CRF
training with a likelihood-based objective function.
To avoid overfitting, a prior distribution over the
model parameters is typically used. A common ex-
ample of this is the Gaussian prior. Use of a prior
involves adding extra terms to the objective and its
derivative. In the case of a Gaussian prior, these ad-
ditional terms involve the mean and variance of the
distribution.
3 Previous Use of Gazetteers
Gazetteers have been widely used in a variety of in-
formation extraction systems, including both rule-
based systems and statistical models. In addition to
lists of people names, locations, etc., recent work
in the biomedical domain has utilised gazetteers of
biological and genetic entities such as gene names
(Finkel et al, 2005; McDonald and Pereira, 2005).
In general gazetteers are thought to provide a useful
source of external knowledge that is helpful when
an entity cannot be identified from knowledge con-
tained solely within the data set used for training.
However, some research has questioned the useful-
ness of gazetteers (Krupka and Hausman, 1998).
Other work has supported the use of gazetteers in
general but has found that lists of only moderate
size are sufficient to provide most of the benefit
(Mikheev et al, 1999). Therefore, to date the ef-
fective use of gazetteers for information extraction
has in general been regarded as a ?black art?. In this
paper we explain some of the likely reasons for these
findings, and propose ways to more effectively han-
dle gazetteers when they are used by maxent-style
models.
In work developed independently and in parallel
to the work presented here, Sutton et al (2006) iden-
tify general problems with gazetteer features and
propose a solution similar to ours. They present re-
sults on NP-chunking in addition to NER, and pro-
vide a slightly more general approach. By contrast,
we motivate the problem more thoroughly through
analysis of the actual errors observed and through
consideration of the success of other candidate solu-
tions, such as traditional regularisation over feature
subsets.
4 Our Experiments
In this section we describe our experimental setup,
and provide results for the baseline models.
4.1 Task and Dataset
Named entity recognition (NER) involves the iden-
tification of the location and type of pre-defined en-
tities within a sentence. The CRF is presented with
a set of sentences and must label each word so as
to indicate whether the word appears outside an en-
tity, at the beginning of an entity of a certain type or
134
within the continuation of an entity of a certain type.
Our results are reported on the CoNLL-2003
shared task English dataset (Sang and Meulder,
2003). For this dataset the entity types are: per-
sons (PER), locations (LOC), organisations (ORG)
and miscellaneous (MISC). The training set consists
of 14
	
987 sentences and 204
	
567 tokens, the devel-
opment set consists of 3
	
466 sentences and 51
	
578
tokens and the test set consists of 3
	
684 sentences
and 46
	
666 tokens.
4.2 Gazetteers
We employ a total of seven gazetteers for our ex-
periments. These cover names of people, places
and organisations. Specifically, we have gazetteers
containing surnames (88
	
799 entries), female first
names (4
	
275 entries), male first names (1
	
219 en-
tries), names of places (27
	
635 entries), names of
companies (20
	
638 and 279
	
195 entries) and names
of other organisations (425 entries).
4.3 Feature set
Our experiments are centred around two CRF mod-
els, one with and one without gazetteer features.
The model without gazetteer features, which we call
standard, comprises features defined in a window
of five words around the current word. These in-
clude features encoding n-grams of words and POS
tags, and features encoding orthographic properties
of the current word. The orthographic features are
based on those found in (Curran and Clark, 2003).
Examples include whether the current word is capi-
talised, is an initial, contains a digit, contains punc-
tuation, etc. In total there are 450
	
345 features in the
standard model.
We call the second model, with gazetteer features,
standard+g. This includes all the features contained
in the standard model as well as 8
	
329 gazetteer
features. Our gazetteer features are a typical way
to represent gazetteer information in maxent-style
models. They are divided into two categories: un-
lexicalised and lexicalised. The unlexicalised fea-
tures model the dependency between a word?s pres-
ence in a gazetteer and its NER label, irrespective
of the word?s identity. The lexicalised features, on
the other hand, include the word?s identity and so
provide more refined word-specific modelling of the
Model Development TestUnreg. Reg. Unreg. Reg.
standard 88.21 89.86 81.60 83.97
standard+g 89.19 90.40 83.10 84.70
Table 1: Model F scores
standard+g 
7
sta
nda
rd   44,945 160
7 228 1,333
Table 2: Test set errors
gazetteer-NER label dependency.1 There are 35 un-
lexicalised gazetteer features and 8
	
294 lexicalised
gazetteer features, giving a total of 458
	
675 features
in the standard+g model.
4.4 Baseline Results
Table 1 gives F scores for the standard and stan-
dard+g models. Development set scores are in-
cluded for completeness, and are referred to later in
the paper. We show results for both unregularised
and regularised models. The regularised models are
trained with a zero-mean Gaussian prior, with the
variance set using the development data.
We see that, as expected, the presence of the
gazetteer features allows standard+g to outperform
standard, for both the unregularised and regularised
models. To test significance, we use McNemar?s
matched-pairs test (Gillick and Cox, 1989) on point-
wise labelling errors. In each case, the standard+g
model outperforms the standard model at a signif-
icance level of p  0  02. However, these results
camouflage the fact that the gazetteer features intro-
duce some negative effects, which we explore in the
next section. As such, the real benefit of including
the gazetteer features in standard+g is not fully re-
alised.
5 Problems with Gazetteer Features
We identify problems with the use of gazetteer fea-
tures by considering test set labelling errors for
both standard and standard+g. We use regularised
models here as an illustration. Table 2 shows the
1Many gazetteer entries involve strings of words where the
individual words in the string do not appear in the gazetteer in
isolation. For this reason the lexicalised gazetteer features are
not simply determined by the word identity features.
135
number of sites (a site being a particular word at a
particular position in a sentence) where labellings
have improved, worsened or remained unchanged
with respect to the gold-standard labelling with the
addition of the gazetteer features. For example, the
value in the top-left cell is the number of sites where
both the standard and standard+g label words cor-
rectly.
The most interesting cell in the table is the top-
right one, which represents sites where standard is
correctly labelling words but, with the addition of
the gazetteer features, standard+g mislabels them.
At these sites, the addition of the gazetteer features
actually worsens things. How well, then, could
the standard+g model do if it could somehow re-
duce the number of errors in the top-right cell? In
fact, if it had correctly labelled those sites, a signifi-
cantly higher test set F score of 90  36% would have
been obtained. This potential upside suggests much
could be gained from investigating ways of correct-
ing the errors in the top-right cell. It is not clear
whether there exists any approach that could correct
all the errors in the top-right cell while simultane-
ously maintaining the state in the other cells, but ap-
proaches that are able to correct at least some of the
errors should prove worthwhile.
On inspection of the sites where errors in the top-
right cell occur, we observe that some of the er-
rors occur in sequences where no words are in any
gazetteer, so no gazetteer features are active for any
possible labelling of these sequences. In other cases,
the errors occur at sites where some of the gazetteer
features appear to have dictated the label, but have
made an incorrect decision. As a result of these ob-
servations, we classify the errors from the top-right
cell of Table 2 into two types: type A and type B.
5.1 Type A Errors
We call type A errors those errors that occur at sites
where gazetteer features seem to have been directly
responsible for the mislabelling. In these cases the
gazetteer features effectively ?over-rule? the other
features in the model causing a mislabelling where
the standard model, without the gazetteer features,
correctly labels the word.
An example of a type A error is given in the sen-
tence extract below:
about/O Healy/I-LOC
This is the labelling given by standard+g. The cor-
rect label for Healy here is I-PER. The standard
model is able to decode this correctly as Healy
appears in the training data with the I-PER label.
The reason for the mislabelling by the standard+g
model is that Healy appears in both the gazetteer of
place names and the gazetteer of person surnames.
The feature encoding the gazetteer of place names
with the I-LOC label has a ? value of 4  20, while
the feature encoding the gazetteer of surnames with
the I-PER label has a ? value of 1  96, and the fea-
ture encoding the word Healy with the I-PER la-
bel has a ? value of 0  25. Although other features
both at the word Healy and at other sites in the sen-
tence contribute to the labelling of Healy, the influ-
ence of the first feature above dominates. So in this
case the addition of the gazetteer features has con-
fused things.
5.2 Type B Errors
We call type B errors those errors that occur at
sites where the gazetteer features seem to have been
only indirectly responsible for the mislabelling. In
these cases the mislabelling appears to be more at-
tributable to the non-gazetteer features, which are in
some sense less expressive after being trained with
the gazetteer features. Consequently, they are less
able to decode words that they could previously la-
bel correctly.
An example of a type B error is given in the sen-
tence extract below:
Chanderpaul/O was/O
This is the labelling given by standard+g. The
correct labelling, given by standard, is I-PER for
Chanderpaul. In this case no words in the sen-
tence (including the part not shown) are present in
any of the gazetteers so no gazetteer features are ac-
tive for any labelling of the sentence. Consequently,
the gazetteer features do not contribute at all to the
labelling decision. Non-gazetteer features in stan-
dard+g are, however, unable to find the correct la-
belling for Chanderpaul when they previously
could in the standard model.
For both type A and type B errors it is clear that
the gazetteer features in standard+g are in some
136
sense too ?powerful? while the non-gazetteers fea-
tures have become too ?weak?. The question, then,
is: can we train all the features in the model in a
more sophisticated way so as to correct for these ef-
fects?
6 Feature Dependent Regularisation
One interpretation of the findings of our error analy-
sis above is that the addition of the gazetteer features
to the model is having an implicit over-regularising
effect on the other features. Therefore, is it possible
to adjust for this effect through more careful explicit
regularisation using a prior? Can we directly reg-
ularise the gazetteer features more heavily and the
non-gazetteer features less? We investigate this pos-
sibility in this section.
The standard+g model is regularised by fitting
a single Gaussian variance hyperparameter across
all features. The optimal value for this single hy-
perparameter is 45. We now relax this single con-
straint by allocating a separate variance hyperparam-
eter to different feature subsets, one for the gazetteer
features (?gaz) and one for the non-gazetteer fea-tures (?non-gaz). The hope is that the differing sub-sets of features are best regularised using different
prior hyperparameters. This is a natural approach
within most standardly formulated priors for log-
linear models. Clearly, by doing this we increase
the search space significantly. In order to make the
search manageable, we constrain ourselves to three
scenarios: (1) Hold ?non-gaz at 45, and regularise thegazetteer features a little more by reducing ?gaz. (2)Hold ?gaz at 45, and regularise the non-gazetteer fea-tures a little less by increasing ?non-gaz. (3) Simulta-neously regularise the gazetteer features a little more
than at the single variance optimum, and regularise
the non-gazetteer features a little less.
Table 3 gives representative development set F
scores for each of these three scenarios, with each
scenario separated by a horizontal dividing line. We
see that in general the results do not differ signifi-
cantly from that of the single variance optimum. We
conjecture that the reason for this is that the regu-
larising effect of the gazetteer features on the non-
gazetteer features is due to relatively subtle inter-
actions during training that relate to the dependen-
cies the features encode and how these dependen-
?gaz ?non   gaz F score42 45 90.40
40 45 90.30
45 46 90.39
45 50 90.38
44.8 45.2 90.41
43 47 90.35
Table 3: FDR development set F scores
cies overlap. Regularising different feature subsets
by different amounts with a Gaussian prior does not
directly address these interactions but instead just
rather crudely penalises the magnitude of the pa-
rameter values of different feature sets to different
degrees. Indeed this is true for any standardly for-
mulated prior. It seems therefore that any solution to
the regularising problem should come through more
explicit restricting or removing of the interactions
between gazetteer and non-gazetteer features during
training.
7 Combining Separately Trained Models
We may remove interactions between gazetteer and
non-gazetteer features entirely by quarantining the
gazetteer features and training them in a separate
model. This allows the non-gazetteer features to
be protected from the over-regularising effect of the
gazetteer features. In order to decode taking advan-
tage of the information contained in both models, we
must combine the models in some way. To do this
we use a logarithmic opinion pool (LOP) (Smith
et al, 2005). This is similar to a mixture model,
but uses a weighted multiplicative combination of
models rather than a weighted additive combination.
Given models p? and per-model weights w? , theLOP distribution is defined by:
pLOP
 
s  o 
1
ZLOP
 
o  ??  p?
 
s  o 

w? (2)
with w?  0 and ?? w?  1, and where ZLOP   o  isa normalising function. The weight w? encodes thedependence of the LOP on model ? . In the case of a
CRF, the LOP itself is a CRF and so decoding is no
more complex than for standard CRF decoding.
In order to use a LOP for decoding we must set
the weights w? in the weighted product. In (Smith et
137
Feature Subset Feature Type
s1 simple structural features
s2 advanced structural features
n n-grams of words and POS tags
o simple orthographic features
a advanced orthographic features
g gazetteer features
Table 4: standard+g feature subsets
al., 2005) a procedure is described whereby the (nor-
malised) weights are explicitly trained. In this paper,
however, we only construct LOPs consisting of two
models in each case, one model with gazetteer fea-
tures and one without. We therefore do not require
the weight training procedure as we can easily fit the
two weights (only one of which is free) using the de-
velopment set.
To construct models for the gazetteer and non-
gazetteer features we first partition the feature set of
the standard+g model into the subsets outlined in
Table 4. The simple structural features model label-
label and label-word dependencies, while the ad-
vanced structural features include these features as
well as those modelling label-label-word conjunc-
tions. The simple orthographic features measure
properties of a word such as capitalisation, presence
of a digit, etc., while the advanced orthographic
properties model the occurrence of prefixes and suf-
fixes of varying length.
We create and train different models for the
gazetteer features by adding different feature sub-
sets to the gazetteer features. We regularise these
models in the usual way using a Gaussian prior. In
each case we then combine these models with the
standard model and decode under a LOP.
Table 5 gives results for LOP decoding for the
different model pairs. Results for the standard+g
model are included in the first row for comparison.
For each LOP the hyphen separates the two models
comprising the LOP. So, for example, in the second
row of the table we combine the gazetteer features
with simple structural features in a model, train and
decode with the standard model using a LOP. The
simple structural features are included so as to pro-
vide some basic support to the gazetteer features.
We see from Table 5 that the first two LOPs sig-
nificantly outperform the regularised standard+g
LOP Dev Set Test Set
standard+g 90.40 84.70
s1g-standard 91.34 85.98
s2g-standard 91.32 85.59
s2ng-standard 90.66 84.59
s2nog-standard 90.47 84.92
s2noag-standard 90.56 84.78
Table 5: Reg. LOP F scores
LOP LOP Weights
s1g-standard [0.39, 0.61]
s2g-standard [0.29, 0.71]
s2ng-standard [0.43, 0.57]
s2nog-standard [0.33, 0.67]
s2noag-standard [0.39, 0.61]
Table 6: Reg. LOP weights
model (at a significance level of p  0  01, on both
the test and development sets). By training the
gazetteer features separately we have avoided their
over-regularising effect on the non-gazetteer fea-
tures. This relies on training the gazetteer features
with a relatively small set of other features. This is
illustrated as we read down the table, below the top
two rows. As more features are added to the model
containing the gazetteer features we obtain decreas-
ing test set F scores because the advantage created
from separate training of the features is increasingly
lost.
Table 6 gives the corresponding weights for the
LOPs in Table 5, which are set using the develop-
ment data. We see that in every case the LOP al-
locates a smaller weight to the gazetteer features
model than the non-gazetteer features model and in
doing so restricts the influence that the gazetteer fea-
tures have in the LOP?s labelling decisions.
Table 7, similar to Table 2 earlier, shows test set
labelling errors for the standard model and one of
the LOPs. We take the s2g-standard LOP here for
illustration. We see from the table that the number
of errors in the top-right cell shows a reduction of
29% over the corresponding value in Table 2. We
have therefore reduced the number errors of the type
we were targeting with our approach. The approach
has also had the effect of reducing the number of er-
rors in the bottom-right cell, which further improves
model accuracy.
All the LOPs in Table 5 contain regularised mod-
138
s2g-standard LOP
 
7
sta
nda
rd   44,991 114
7 305 1,256
Table 7: Test set errors
LOP Dev Set Test Set
s1g-standard 90.58 84.87
s2g-standard 90.70 84.28
s2ng-standard 89.70 84.01
s2nog-standard 89.48 83.99
s2noag-standard 89.40 83.70
Table 8: Unreg. LOP F scores
els. Table 8 gives test set F scores for the cor-
responding LOPs constructed from unregularised
models. As we would expect, the scores are lower
than those in Table 5. However, it is interesting to
note that the s1g-standard LOP still outperforms
the regularised standard+g model.
In summary, by training the gazetteer features
and non-gazetteer features in separate models and
decoding using a LOP, we are able to overcome
the problems described in earlier sections and can
achieve much higher accuracy. This shows that
successfully deploying gazetteer features within
maxent-style models should involve careful consid-
eration of restrictions on how features interact with
each other, rather than simply considering the abso-
lute values of feature parameters.
8 Gazetteer-Like Features
So far our discussion has focused on gazetteer fea-
tures. However, we would expect that the problems
we have described and dealt with in the last sec-
tion also occur with other types of features that have
similar properties to gazetteer features. By applying
similar treatment to these features during training we
may be able harness their usefulness to a greater de-
gree than is currently the case when training in a sin-
gle model. So how can we identify these features?
The task of identifying the optimal partitioning
for creation of models in the previous section is in
general a hard problem as it relies on clustering the
features based on their explanatory power relative to
all other clusters. It may be possible, however, to de-
vise some heuristics that approximately correspond
to the salient properties of gazetteer features (with
respect to the clustering) and which can then be used
to identify other features that have these properties.
In this section we consider three such heuristics. All
of these heuristics are motivated by the observation
that gazetteer features are both highly discriminative
and generally very sparse.
Family Singleton Features We define a feature
family as a set of features that have the same con-
junction of predicates defined on the observations.
Hence they differ from each other only in the NER
label that they encode. Family singleton features
are features that have a count of 1 in the training
data when all other members of that feature family
have zero counts. These features have a flavour of
gazetteer features in that they represent the fact that
the conjunction of observation predicates they en-
code is highly predictive of the corresponding NER
label, and that they are also very sparse.
Family n-ton Features These are features that
have a count of n (greater than 1) in the training
data when all other members of that feature family
have zero counts. They are similar to family sin-
gleton features, but exhibit gazetteer-like properties
less and less as the value of n is increased because a
larger value of n represents less sparsity.
Loner Features These are features which occur
with a low mean number of other features in the
training data. They are similar to gazetteer features
in that, at the points where they occur, they are in
some sense being relied upon more than most fea-
tures to explain the data. To create loner feature sets
we rank all features in the standard+g model based
on the mean number of other features they are ob-
served with in the training data, then we take subsets
of increasing size. We present results for subsets of
size 500, 1000, 5000 and 10000.
For each of these categories of features we add
simple structural features (the s1 set from earlier),
to provide basic structural support, and then train a
regularised model. We also train a regularised model
consisting of all features in standard+g except the
features from the category in question. We decode
these model pairs under a LOP as described earlier.
Table 9 gives test set F scores for LOPs cre-
ated from each of the categories of features above
139
LOP Test Set
FSF 85.79
FnF 84.78
LF 500 85.80
LF 1000 85.70
LF 5000 85.77
LF 10000 85.62
Table 9: Reg. LOP F scores
(with abbreviated names derived from the category
names). The results show that for the family single-
ton features and each of the loner feature sets we
obtain LOPs that significantly outperform the reg-
ularised standard+g model (p  0  0002 in every
case). The family n-ton features? LOP does not do
as well, but that is probably due to the fact that some
of the features in this set have a large value of n and
so behave much less like gazetteer features.
In summary, we obtain the same pattern of results
using our quarantined training and LOP decoding
method with these categories of features that we do
with the gazetteer features. We conclude that the
problems with gazetteer features that we have iden-
tified in this paper are exhibited by general discrim-
inative features with gazetteer feature-like proper-
ties, and our method is also successful with these
more general features. Clearly, the heuristics that
we have devised in this section are very simple, and
it is likely that with more careful engineering better
feature partitions can be found.
9 Conclusion and future work
In this paper we have identified and analysed nega-
tive effects that can be introduced to maxent-style
models by the inclusion of highly discriminative
gazetteer features. We have shown that such ef-
fects manifest themselves through errors that gen-
erally result from the model?s over-dependence on
the gazetteer features for decision making. To over-
come this problem a more careful treatment of these
features is required during training. We have pro-
posed a solution that involves quarantining the fea-
tures and training them separately to the other fea-
tures in the model, then decoding the separate mod-
els with a logarithmic opinion pool. In fact, the LOP
provides a natural way to handle the problem, with
different constituent models for the different fea-
ture types. The method leads to much greater ac-
curacy, and allows the power of gazetteer features
to be more effectively harnessed. Finally, we have
identified other feature sets with gazetteer feature-
like properties and shown that similar results may be
obtained using our method with these feature sets.
In this paper we defined intuitively-motivated fea-
ture partitions (gazetteer feature-based or otherwise)
using heuristics. In future work we will focus on au-
tomatically determining such partitions.
References
James Curran and Stephen Clark. 2003. Language independent
NER using a maximum entropy tagger. In Proc. CoNLL-
2003.
Jenny Finkel, Shipra Dingare, Christopher D. Manning, Malv-
ina Nissim, Beatrice Alex, and Claire Grover. 2005. Ex-
ploring the boundaries: gene and protein identification in
biomedical text. BMC Bioinformatics, (6).
L. Gillick and Stephen Cox. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In Inter-
national Conference on Acoustics, Speech and Signal Pro-
cessing, volume 1, pages 532?535.
George R. Krupka and Kevin Hausman. 1998. Isoquest Inc:
Description of the NetOwl (TM) extractor system as used
for MUC-7. In Proc. MUC-7.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ICML 2001.
Andrew McCallum and Wei Li. 2003. Early results for named
entity recognition with conditional random fields, feature in-
duction and web-enhanced lexicons. In Proc. CoNLL-2003.
Ryan McDonald and Fernando Pereira. 2005. Identifying gene
and protein mentions in text using conditional random fields.
BMC Bioinformatics, (6).
Andrei Mikheev, Marc Moens, and Claire Grover. 1999.
Named entity recognition without gazetteers.
Fuchun Peng and Andrew McCallum. 2004. Accurate informa-
tion extraction from research papers using conditional ran-
dom fields. In Proc. HLT-NAACL 2004.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduc-
tion to the CoNLL-2003 shared task: Language-independent
named entity recognition. In Proc. CoNLL-2003.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with
conditional random fields. In Proc. HLT-NAACL 2003.
Andrew Smith, Trevor Cohn, and Miles Osborne. 2005. Loga-
rithmic opinion pools for conditional random fields. In Proc.
ACL 2005.
Charles Sutton, Michael Sindelar, and Andrew McCallum.
2006. Reducing weight undertraining in struxctured dis-
criminative learning. In Proc. HLT/NAACL 2006.
140
Automatic Extraction of Semantic Networks from Text using Leximancer
Andrew E. Smith.
Key Centre for Human Factors and Applied Cognitive Psychology,
The University of Queensland,
Queensland, Australia, 4072.
asmith@humanfactors.uq.edu.au
Abstract
Leximancer is a software system for perform-
ing conceptual analysis of text data in a largely
language independent manner. The system is
modelled on Content Analysis and provides
unsupervised and supervised analysis using
seeded concept classifiers. Unsupervised on-
tology discovery is a key component.
1 Method
The strategy used for conceptual mapping of text in-
volves abstracting families of words to thesaurus con-
cepts. These concepts are then used to classify text at
a resolution of several sentences. The resulting concept
tags are indexed to provide a document exploration en-
vironment for the user. A smaller number of simple
concepts can index many more complex relationships
by recording co-occurrences, and complex systems ap-
proaches can be applied to these systems of agents.
To achieve this, several novel algorithms were de-
veloped: a learning optimiser for automatically select-
ing, learning, and adapting a concept from the word us-
age within the text, and an asymmetric scaling process
for generating a cluster map of concepts based on co-
occurrence in the text.
Extensive evaluation has been performed on real doc-
ument collections in collaboration with domain experts.
The method adopted has been to perform parallel analy-
ses with these experts and compare the results.
An outline of the algorithms (Smith, 2000) follows:
1. Text preparation: Standard techniques are em-
ployed, including name and term preservation, to-
kenisation, and the application of a stop-list.
2. Unsupervised and supervised ontology discovery:
Concepts can be seeded by a domain expert to suit
user requirements, or they can be chosen automat-
ically using a ranking algorithm for finding seed
words which reflect the themes present in the data.
This process looks for words near the centre of local
maxima in the lexical co-occurrence network.
3. Filling the thesaurus: A machine learning algorithm
is used to find the relevant thesaurus words from the
text data. This iterative optimiser, derived from a
word disambiguation technique (Yarowsky, 1995),
finds the nearest local maximum in the lexical co-
occurrence network from each concept seed. Early
results show that this lexical network can be reduced
to a Scale-free and Small-world network1.
4. Classification: Text is tagged with multiple concepts
using the thesaurus, to a sentence resolution.
5. Mapping: The concepts and their relative co-
occurrence frequencies now form a semantic net-
work. This is scaled using an asymmetric scaling
algorithm, and made into a lattice by ranking con-
cepts by their connectedness, or centrality.
6. User interface: A browser is used for exploring the
classification system in depth. The semantic lat-
tice browser enables semantic characterisation of the
data and discovery of indirect association. Con-
cept co-occurrence spectra and themed text segment
browsing are also provided.
2 Analysis of the PNAS Data Set
The data set presented here consisted of text and meta-
data from Proceedings of the National Academy of Sci-
ence, 1997 to 2002. These examples are extracted from
the abstract data. Firstly, Leximancer was configured to
map the document set in unsupervised mode. A screen
image of this interactive map is shown in figure 1. This
1Following (Steyvers and Tenenbaum, 2003).
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 23-24
                                                         Proceedings of HLT-NAACL 2003
shows the semantic lattice (left), with the co-occurrence
links from the concept ?brain? highlighted (left and right).
Figure 1: Unsupervised map of PNAS abstracts.
Figure 2 shows the top of the thesaurus entry for the
concept ?brain?. This concept was seeded with just the
word ?brain? and then the learning system found a larger
family of words and names which are strongly relevant
to ?brain? in the these abstracts. In the figure, terms in
square brackets are identified proper names, and numeri-
cal values are the relevancy weights.
Figure 2: Thesaurus entry for ?brain? (excerpt).
It is also of interest to discover which concepts tend
to be unique to each year of the PNAS proceedings, and
so identify trends. This usually requires a different form
of analysis, since concepts which characterise the whole
data set may not be good for discriminating parts. By
placing the data for each year in a folder, Leximancer can
tag each text sentence with the relevant year, and place
each year as a prior concept on the map. The result-
ing map contains the prior concepts plus other concepts
which are relevant to at least one of the priors, and shows
trending from early years to later years (figure 3).
Figure 3: Temporal map of PNAS abstracts.
3 Conclusion
The Leximancer system has demonstrated several major
strengths for text data analysis:
? Large amounts of text can be analysed rapidly in a
quantitative manner. Text is quickly re-classified us-
ing different ontologies when needs change.
? The unsupervised analysis generates concepts which
are well-defined ? they have signifiers which com-
municate the meaning of each concept to the user.
? Machine Learning removes much of the need to re-
vise thesauri as the domain vocabulary evolves.
References
Andrew E. Smith. 2000. Machine mapping of document
collections: the leximancer system. In Proceedings
of the Fifth Australasian Document Computing Sym-
posium, Sunshine Coast, Australia, December. DSTC.
http://www.leximancer.com/technology.html.
Mark Steyvers and Joshua B. Tenenbaum. 2003. The
large-scale structure of semantic networks: Statistical
analyses and a model of semantic growth. Submitted
to Cognitive Science. http://www-psych.stanford.edu/
?msteyver.
David Yarowsky. 1995. Unsupervised word-sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association for
Computational Linguistics (ACL-95), pages 189?196,
Cambridge, MA. http://www.cs.jhu.edu/?yarowsky/
pubs.html.
