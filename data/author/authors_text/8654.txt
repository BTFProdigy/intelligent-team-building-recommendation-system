Generating Overview Summaries of Ongoing Email Thread Discussions 
Stephen Wan 
Department of Computing 
Macquarie University 
Sydney NSW 2109 
 swan@ics.mq.edu.au 
Kathy McKeown 
Columbia University 
Department of Computer Science 
1214 Amsterdam Avenue 
NY - 10027-7003, USA 
kathy@cs.columbia.edu 
 
Abstract 
The tedious task of responding to a backlog of 
email is one which is familiar to many researchers.  
As a subset of email management, we address the 
problem of constructing a summary of email 
discussions.  Specifically, we examine ongoing 
discussions which will ultimately culminate in a 
consensus in a decision-making process.  Our 
summary provides a snapshot of the current state-
of-affairs of the discussion and facilitates a speedy 
response from the user, who might be the 
bottleneck in some matter being resolved.  We 
present a method which uses the structure of the 
thread dialogue and word vector techniques to 
determine which sentence in the thread should be 
extracted as the main issue.   Our solution 
successfully identifies the sentence containing the 
issue of the thread being discussed, potentially 
more informative than subject line.   
1 Introduction 
Imagine the chore of sifting through your 
overflowing email inbox after an extended period 
away from the office.  The discovery that some of 
these emails form part of a larger decision-making 
discussion only heightens the sense of urgency and 
stress.  Such a discussion may require an urgent 
response and a user?s lack of contribution may be a 
bottleneck in some matter being resolved.  Such a 
scenario is seems quite familiar and intuitively, one 
would expect that better solutions to presenting the 
contents of the email inbox might be useful in 
facilitating a timely reply to a missed email 
discussion. 
One such solution might be a summary of that 
very email discussion.  However, it would be much 
more useful if the summary did not just tell the 
user what the thread is about.  Such information 
might be easily obtained from the subject line, or if 
not, a conventional off-the-shelf summarizer might 
provide the gist of the thread quite easily.     
However, in contrast to a conventional sentence 
extraction summary in Figure 1, the ideal summary 
ought to provide sufficient information about the 
current state-of-affairs of the discussion, in order to 
minimize any further delay in the matter being 
resolved.    Specifically, this might include a 
description of the matter being discussed and the 
responses received so far.  An example of such a 
summary is presented in Figure 2.  In this example, 
it is not sufficient to know that a plaque is being 
designed.  Crucially, the wording of the plaque is 
under discussion and requires feedback from the 
thread participants.  It is not difficult to appreciate 
the usefulness of such a summary to avoid writing 
responses to older, and hence irrelevant, emails.  
Accordingly, we envisage that the resulting 
summary to be not just indicative of the thread 
content but informative Borko (1975).   
 
1. Here's the plaque info. 
2. http://www.affordableawards.com/plaques/o
rdecon.htm  
3. I like the plaque, and aside for exchanging 
Dana's name for "Sally Slater" and ACM for 
"Ladies Auxiliary", the wording is nice. 
4. We just need to contact the plaque folks and 
ask what format they need for the logo. 
 
Figure 1. Example summary from a conventional 
sentence extraction summarizer 
Issue: Let me know if you agree or disagree 
w/choice of plaque and (especially) wording. 
 
Response 1: I like the plaque, and aside for 
exchanging Dana's name for "Sally Slater" 
and ACM for "Ladies Auxiliary", the 
wording is nice.  
Response 2: I prefer Christy's wording to the 
plaque original.  
Figure 2. Example summary from our system 
We present a novel approach which identifies 
the main issue within the email and finds the 
responses to that issue within subsequent emails. 
Our approach uses a combination of traditional 
vector space techniques and Singular Value 
Decomposition (SVD).  We rely on the premise 
that the participants of the discussion have 
implicitly determined which sentence from the 
initiating email of the thread is most important and 
that we can see evidence of this inherent in the 
content of their respective replies.   
In the remainder of the paper, we provide 
background on email usage and our observations 
of discussion thread structure in Section 2 to 
support our basic premise.  Section 3 provides a 
description of related work in the area.   To date, 
use of dialogue structure has mostly been limited 
to finding question-answer pairs in order to extract 
the pairing as a whole for the sake of coherence.  
We present a more formal description of the 
problem we are addressing and our algorithms for 
issue in Section 4.  Section 5 outlines our handling 
of response extraction.  In Section 6, we present a 
preliminary evaluation we have conducted along 
with the results.  Finally we end with concluding 
remarks in Section 7.
2 Background: Email Threads 
2.1 Email Discussions supporting a Decision-
Making Process 
The focus of this paper is on email discussions 
supporting a group decision-making process.  In 
contrast to studies on individual email usage (for 
an overview see: Ducheneaut and Bellotti, 2001), 
this research area has been less explored.  
Occasionally, such discussions end with an online 
vote.   However, Ducheneaut and Belotti do note 
that voting is relatively infrequent and our own 
experience with our email corpora tends to support 
this.   
In general, we expect that these threads contain 
supporting discussions, and the actual decision 
might occur outside of the email medium, for 
example in a board meeting.  What we hope to 
observe is that, for some issue discussed, candidate 
solutions and responses highlighting the pros and 
cons of a solution are introduced via email.  
Decision-making discussion threads occur 
frequently enough in environments which depend 
on professional usage of email.  In the corpus we 
examined, 40% of the threads were decision-
making discussions.   
2.2 Constraints on and Choice of a Corpus of 
Email Discussions 
To collect a corpus of these threads, we placed a 
few constraints on the mailing list archives we 
found online.   
To begin with, we focused on threads from 
mailing lists that were set up to support 
organization activities as these often involve 
decision-making processes.  As we are also 
interested in examining the role of dialogue, we 
required access to the email thread structure from 
which we can infer a basic dialogue structure.      
We chose to use the archives of the Columbia 
University ACM Student Chapter Committee as 
this group has organized several events and used 
email as their primary mode of communication 
outside of meetings.  For practical reasons, it was 
relatively straightforward to obtain the necessary 
permissions to use the data, something that might 
be more difficult for other archives. Possible 
alternative corpora might be the mailing lists of 
organizing committees, for example that of a 
conference organizing committee or a steering 
group.  Project-based mailing lists might also be 
potentially used, especially if the group 
participants have sufficient shared background to 
engage in discussions.   
2.3 Observations on Thread Structure  
The Columbia University ACM Student Chapter 
Committee was made up of about 10 people. Upon 
initial examination of the data, we found that we 
could classify the threads of email according to its 
purpose.  The set of group tasks facilitated by the 
email correspondence were: decision-making, 
information provision, requests for action and 
social conversation. 
However, it is natural for the group to engage in 
multiple tasks.  Thus, we use the term ?task shift? 
to refer to adjacent segments of the thread 
(comprised of emails) which reflect distinct group 
goals.  In the corpus we use, we observe that these 
tasks usually occur sequentially.  In some cases, a 
single email proposes more than one issue for 
discussion, and subsequent responses address each 
of these in turn. 
Intuitively, it makes sense to create a summary 
for a single task.  Accordingly, we have designed 
our algorithm to accept only dialogue structures 
addressing a single group task.  If discussions 
invoke short clarification questions, these should 
not be treated differently if the task remains the 
same.  One supporting reason for this is the 
syntactic variation with which participants express 
disagreement.  We have observed that 
disagreement is often expressed as a clarification 
question, or as a question which offers an 
alternative suggestion. 
3 Related Work 
To date, email thread summarization has not 
been explored in any great depth within the Natural 
Language Processing (NLP) research community.   
Research on thread summarization has included 
some work on using dialogue structure for email 
summarization.  Nenkova et al (2003) advocate 
the use of overview sentences similar to ours.  
They extract sentences based on the presence of 
subject line key words.  However, should the 
subject line not reflect the content of the thread, 
our method has the potential to extract the true 
discussion issue since it based on the responses of 
other participants.  
Lam et al (2002) use the context of the 
preceding thread to provide background 
information for email summaries.  However, they 
note that even after appropriate preprocessing of 
email text, simply concatenating preceding context 
can lead to long summaries.  In contrast, instead of 
extracting email texts verbatim, we extract single 
sentences from particular emails in the thread. As a 
result, our summaries tend to be much shorter.     
Murakoshi et al (1999) describe an approach 
which extracts question-answer pairs from an 
email thread.  Extraction is based on the use of 
pattern-based information extraction methods.  The 
summary thus provides the question-answer pair 
intact, thereby improving the coherence.  Question-
answer summaries would presumably be suited to 
discussions which support an information 
provision task, a complementary task to the one we 
examine. 
Rambow et al (2004) apply sentence extraction 
techniques to the thread to construct a generic 
summary.  Though not specifically using dialogue 
structure, one feature used marks if a sentence is a 
question or not. 
Work has also been done on more accurately 
constructing the dialogue structure.  Newman and 
Blitzer (2003) focus on clustering related 
newsgroup messages into dialogue segments.  The 
segments are then linked using email header 
information to form a hierarchical structure.  Their 
summary is simply the first sentence from each 
segment.  We envisage dialogue structure 
summaries showing an overview of topics would 
be combined with approaches such as ours which 
provide summaries of segments.   
We also note the existing work that explores the 
summarization of speech transcripts.  Speech is a 
very different mode of communication.  An 
overview of the differences between asynchronous 
and synchronous modes of communication is 
provided by Clark (1991) and Simpson-Young et 
al. (2000).  Alexandersson et al (2000) note that in 
speech there is a tendency not to repeat shared 
conversation context.  They use the preceding 
dialogue structure, modeled using Dialogue 
Representation Theory, to provide additional 
ellipsed information.  It is unclear how such an 
approach might apply to an email corpus which has 
the potential to cover a broader set of domains.   
More recently, Zechner and Lavie (2001) 
identify question-answer dialogue segments in 
order to extract the pair as a whole.     
Hillard et al (2003) have also produced a system 
which generates summaries of speech discussions 
supporting a decision-making process.  Their work 
differs from ours in that they focus on categorizing 
the polarity of responses in order to summarize 
consensus. 
4 Issue Detection 
To make the problem more manageable we 
make the following assumptions about the types of 
threads that our algorithm will handle.  To begin 
with, we assume that the threads have been 
correctly constructed and classified as discussions 
supporting decision-making.  Needless to say, the 
first assumption is a little unrealistic given that 
thread construction is a difficult problem.  For 
example, it is not uncommon to receive emails 
with recycled subject lines simply because replying 
to an email is often more convenient than typing in 
an address.   
The other assumptions we make have to do with 
the dialogue structure of the threads.  The first is 
that the issue being discussed (usually a statement 
describing the matter to be decided) is to be found 
in the first email.  The second is that the email 
thread doesn?t shift task, nor does it contain 
multiple issues.   
The first assumption is based on what we have 
observed to be normal behavior.  Exceptions to this 
rule are broken threads and cases where the 
participants have responded to a forwarded email.  
In the first case, this can be seen as an error in 
thread construction and identification.  In such 
cases however, even in such a thread, the first 
email usually contains a reference to the issue at 
hand, although it may be an impoverished 
paraphrase.  Our algorithm extracts these 
paraphrases in lieu of the original wording.  Cases 
where participants have responded to a forwarded 
email are not common.  For such threads, we 
attempt to extract the sentence participants respond 
to.  However, again, this may not be the best 
formulation of the issue. 
Secondly, we assume that a text segmentation 
algorithm (for examples see Hearst?s ?Text-Tiling? 
algorithm 1997, Choi et al 2000) has already 
segmented the threads according to shifts in task.  
Operationally, our detection of shifts in task would 
then be based on corresponding changes in 
vocabulary used. 
4.1 The Algorithm 
Our summarization approach is to extract a set of 
sentences consisting of one issue, and the 
corresponding responses ? one per participant.  
Our sentence extraction mechanisms borrow from 
information retrieval methods which represent text 
as weighted term frequency vectors (for an 
overview see: Salton and McGill, 1983).   
In Figure 3, we present the general framework of 
the algorithm.  In this framework we divide the 
thread into two parts, the initiating email and the 
replies.  We create a comparison vector that 
represents what the replies are about.  We can 
construct variations of this framework by changing 
the way we build our comparison vector.  The aim 
is to compare each sentence to the comparison 
vector for the replies.  Thus, we build separate 
vector representations, called candidate vectors, for 
each sentence in the first email.  Using the cosine 
similarity metric to compare candidate vectors with 
the comparison vector, we rank the sentences of 
the first email.  Conceptually, the highest ranked 
sentence will be the one that is closest in content to 
the replies and this is extracted as the issue of the 
discussion. 
 
1. Separate thread into issue_email  and replies 
2. Create ?comparison vector? V representing replies 
3. For each sentence s in issue_email 
3.1 Construct vector representation S for sentence s 
3.2   Compare V and S using cosine similarity 
4. Rank sentences according to their cosine similarity 
scores 
5. Extract top ranking sentence 
Figure 3. Framework for extracting discussion 
Issues. 
We now discuss the four methods for building 
the comparison vector.  These are:  
1. The Centroid method 
2. The SVD Centroid method.   
3. The SVD Key Sentence method  
4. Combinations of methods: Oracles 
4.1.1 The Centroid Method 
In the Centroid method, we first build a term by 
sentence (t ? s) matrix, A, from the reply emails.  
In this matrix, rows represent sentences and 
columns represent unique words found in the 
thread.  Thus, the cells of a row store the term 
frequencies of words in a particular sentence.  
From this matrix, we form a centroid to represent 
the content of the replies.  This is a matter of 
summing each row vector and normalizing by the 
number of rows.  This centroid is then what we use 
as our comparison vector. 
4.1.2 The SVD Centroid Method 
Our interpretation of the SVD results is based on 
that of Gong and Liu (1999) and Hoffman (1999).  
Gong and Liu use SVD for text segmentation and 
summarization purposes.  Hoffman describes the 
results of SVD within a probabilistic framework.  
For a more complete summary of our interpretation 
of the SVD analysis see Wan et al (2003).     
To begin with, we construct the matrix A as in 
the Centroid Method.  The matrix A provides a 
representation of each sentence in w 
dimensionality, where w is the size of the 
vocabulary of the thread.  The SVD analysis1 is the 
product of three matrices U, S and V transpose.  In 
the following equation, dimensionality is indicated 
by the subscripts.  
 
SVD(At ? s) = Ut ? r Sr ? r(Vs ? r) tr 
 
Conceptually, the analysis essentially maps the 
sentences into a smaller dimensionality r, which 
we interpret as the main ?concepts? that are 
discussed in the sentences.  These dimensions, or 
concepts, are automatically identified by the SVD 
analysis on the basis of similarities of co-
occurrences.  The rows of V matrix represent the 
sentences of the first email, and each row vector 
describes how a given sentence relates to the 
discovered concepts.  Importantly, the number of 
discovered concepts is less than or equal to the 
vocabulary of the thread in question.  If it is less 
than the vocabulary size, then the SVD analysis 
has been able to combine several related terms into 
a single concept.  Conceptually, this corresponds to 
finding word associations between synonyms 
though in general, this association may not 
conserve part-of-speech.  In contrast to the values 
of the A matrix which are always positive (since 
they are based on frequencies), the values of each 
cell in the V matrix can be negative.  This 
represents the degree to which the sentence relates 
to a particular concept.  We build a centroid from 
the V matrix to form our comparison vector. 
4.1.3 The SVD Key Sentence Method 
The SVD Key Sentence Method is similar to the 
preceding method.  We build the matrix A, apply 
the SVD analysis and obtain the matrix V.   Instead 
of constructing a vector which represents all of the 
replies, we choose one sentence from the replies 
that is most representative of the thread content.  
This is done by selecting the most important 
concept and finding the sentence that contains the 
most words related to it.  The SVD analysis by 
default sorts the concepts according to degree to 
which sentences are associated with it.   By this 
definition, the most important sentence is 
                                                     
1
 We use the SVD function in the JAMA Java Matrix 
Package (http://math.nist.gov/javanumerics/jama/) to 
compute the analysis. 
represented by the values in the first column of the 
matrix V.  We then take the maximum of this 
column vector and note its row index, r, which 
denotes a sentence.   We use the rth  row vector of 
the V matrix as the comparison vector. 
In both the SVD Centroid method and the SVD 
Key Sentence method, the comparison vector has a 
different dimensionality than the candidate vectors.  
To perform the comparison, we must map the 
candidate vectors into this new dimensionality.  
This is done by pre-multiplying each candidate 
vector with the result of the matrix multiplication:  
Utranspose ? S.  Both of the matrices involved are 
obtained from the SVD analysis. 
4.1.4 Combinations of methods: Oracles 
Since we have three alternatives for constructing 
the comparison vector we consider the possibility 
of combining the approaches.  In Wan et al (2003) 
we showed that using a combination of traditional 
TF?IDF approaches and SVD approaches was 
useful given that SVD provided additional 
information about word associations.  Similarly, 
our two SVD methods provide complementary 
information.  The vector computed by the SVD 
centroid method provides information about the 
replies and accounts for word associations such as 
synonyms.  However, like the centroid method, 
this vector will include all topics discussed in the 
replies, even small digressions.  In contrast, the 
SVD Key sentence is potentially better at ignoring 
these digressions by focusing on a single concept.  
We present three heuristic oracles which 
essentially re-rank the candidate issue sentences 
identified by each of the three methods.  Re-
ranking is based on a voting mechanism.  The rules 
for three oracles are presented in Figures 4 and 5.   
 
1. If a majority exists return it 
2. If tie then: 
retrieve the lowest index number i,  
where i    1 
3. If all methods return different answers, then 
choose Centroid Method?s answer 
Figure 4.  Oracle 1 heuristic rules 
The oracle in Figure 4 attempts to choose the 
best sentence, retrieving a single sentence.  Rule 2 
attempts to encode the intuition that the issue 
sentence is likely to occur early in the email, 
however, not usually at the top of the email.  
Finally, we use the Centroid Method as a default 
because it is less prone to errors arising from low 
vocabulary sizes found in shorter threads.  For 
such threads, we found that SVD approaches tend 
not to perform so well.   
The second oracle again relies on a majority 
vote.  However, it relaxes the constraint of just 
returning a single sentence if the majority is the 
first sentence of the email.  Since we tend not to 
find issue sentences in at the very top of emails, we 
return all possible issue sentences in rule 1. 
 
1. If a majority exists then return it;  
UNLESS i = 1 in which case, return all 
choices  
2. If tie then retrieve the lowest index number i,  
where i    1 
3. If all methods return different answers, then 
choose Centroid Method?s answer 
Figure 5.  Oracle 2 heuristic rules 
Finally, as a baseline, the third oracle returns all 
the possible issue sentences identified by all of the 
contributing methods.  
5 Extracting the Responses to the Issue 
To extract the responses to the issue, we simply 
take the first sentence of the replies of each 
responding participant.  We make sure to only 
extract one response per participant.   
An alternative solution analogous to that of issue 
detection was also considered.  In this solution, we 
applied the issue detection algorithm to the reply 
email in question.  However, it turns out that most 
of the tagged responses occurred at the start of 
each reply email and a more complex approach 
was unnecessary and potentially introduced more 
errors. 
6 Evaluation of Issue Detection Algorithms 
6.1 The Test Data 
The test data used was a portion of the Columbia 
ACM Student Chapter corpus.  This corpus 
included a total of 300 threads which were 
constructed using message-ID information found 
in the header.  On average, there were 190 words 
per thread and 6.9 sentences in the first email.  
Threads longer than two emails2 were 
categorized manually.  We identified discussions 
that supported a decision-making process.  For 
these, we manually annotated the issue of the 
thread and the responses to the issue.  Although we 
do not currently use this information, we also 
classified the responses as being either in 
agreement or disagreement.  According to the 
assumptions listed in Section 4, we discarded those 
threads in which the issue was not found in the first 
email.  In total, we identified 37 discussion 
                                                     
2
 Longer threads offered a great chance of identifying 
a discussion. 
threads, each of which forms a test case.  A manual 
annotation of the discussion issues was done by 
following the instruction: Select the sentence from 
the first email that subsequent emails are 
responding to.?  These annotated issue sentences 
formed our gold standard. 
Our approach was designed to operate on the 
new textual contributions of each participant.  
Thus, the emails underwent a limited 
preprocessing stage.  Email headers, automatically 
embedded ?reply context? text and static signatures 
were ignored.   
6.2 Evaluation Framework and Results 
The evaluation was designed to test if our 
methods which use dialogue structure improve 
sentence extraction results.  We used the recall-
precision metric to compare the results of a system 
with the manually annotated gold standard.  In 
total, we tested 6 variations of our issue detection 
algorithms.  These included the Centroid method, 
the SVD Centroid method and the SVD Key 
Sentence method and the 3 oracles. 
For each test case, the approach being tested was 
used to extract one or more sentences 
corresponding to the issue of the discussion, which 
was then compared to the gold standard.  The 
baseline used was the first n sentences of the first 
email as a summary, where n ranged from 1 to 3 
sentences.   
The recall-precision results of the evaluation are 
presented in Table 1.  On average, the chance of 
correctly choosing the correct sentence randomly 
in a test set was 21.9%.   
We used an ANOVA to test whether there was 
an overall effect between the various methods for 
recall and precision.   We rejected the null 
hypothesis, that is, the choice of method does 
affect recall and precision (?=0.05, dfnumerator= 8, 
dfdenoinator= 324). 
To determine if our techniques were statistically 
significant compared to the baselines, we ran pair-
wise two-tailed student t-tests to compare the three 
methods and the first oracle to the n=1 baseline 
since these all returned a single sentence.  The 
results are presented in Table 2.   Similarly, Table 
3 shows the t-test comparisons for the oracle and 
oracle baseline against the n=3 baseline.   
Except for the SVD Key Sentence method, all 
the methods were significantly better than the n=1 
baseline.  However, a useful recall score was only 
obtained using the oracle methods.  When 
comparing the oracle methods which returned 
more than one sentence against the n=3 baseline, 
we found no significant difference in recall.  
However, when comparing precision performance 
we found that the difference between the precision 
of Centroid method and the three oracles were 
significantly different compared to the baseline. 
 
Method Ave.Recall % Ave. Prec. & 
Centroid 62.2 62.2 
SVD Centroid 48.6 48.6 
SVD Key Sent 37.8 37.8 
Oracle 1 62.2 62.2 
Oracle 2 70.3 62.7 
Oracle Baseline 83.8 45.1 
Baseline n=1 24.3 24.3 
Baseline n=2 48.6 24.3 
Baseline n=3 64.0 21.6 
Table 1. Average recall and precision values for 
each method.   
Method Prob(Recall)  Prob(Prec.) 
Centroid 0.0016 0.0016 
SVD Centroid 0.0187 0.0187 
SVD Key Sent 0.1601 0.1601 
Oracle 1 0.0004 0.0004 
Table 2.  Pair-wise t-test scores comparing each 
method to the n=1 baseline (df = 36).  The values 
show the probability of the obtained t value. 
Method Prob(Recall)  Prob(Prec.) 
Oracle 2 0.5998 0.0001 
Oracle Baseline 0.1686 0.0108 
Table 3. Pair-wise t-test scores comparing each 
method to the n=3 baseline (df = 36).  The values 
show the probability of the obtained t value. 
The recall and precision statistics for the 
Centroid method was the most impressive of the 
three methods proposed, far outperforming the 
baseline.  The results of comparisons involving the 
oracles, which combine the three methods, showed 
improved performance, suggesting that such 
techniques might potentially be useful in an email 
thread summary.  Whilst there was little difference 
between the recall values of the three oracles and 
the baselines, the benefit of using a more involved 
approach such as ours is demonstrated clearly by 
the gain in precision performance which will 
impact the usefulness of such a summary.   It is 
also interesting to note that the performance of the 
oracles was achieved by simply using simple rules 
without any corpus training. 
7 Conclusion and Future Work 
The methods described in this paper would form 
part of a larger email thread summarizer able to 
identify task boundaries and then initiate the 
appropriate summarization strategy for that task.  
We have addressed the sub-problem of 
summarizing the decision-making processes which 
have been supported by discussions over email. 
Despite the preliminary nature of our investigation, 
our findings are encouraging and lend support to 
the view that a combination of simple word vector 
approaches with singular value decomposition 
approaches do well at extracting discussion issues.  
Such methods, even with only a simple notion 
dialogue structure achieve a useful level of recall 
and precision.   We would like to conduct extrinsic 
experiments to test our assumptions about the 
usefulness of these summaries. Further 
investigations will also focus on examine issues of 
scalability, with regard to group size, and domain 
independence.   We would also like to investigate 
how issue detection might be integrated with a 
more complete solution to email thread 
summarization. 
8 Acknowledgements 
The research described in this paper was 
partially supported by a grant provided through 
NSF's Knowledge Discovery and Dissemination 
Program. We would like to thank the NLP groups 
of Columbia University, Macquarie University and 
CSIRO for feedback received on this work.   
References  
J. Alexandersson, P. Poller, M. Kipp, and R. Engel. 
2000. Multilingual Summary Generation in a 
Speech-To-Speech Translation System for 
Multilingual Dialogues. In Proc. of INLG-2000, 
Mitzpe Ramon, Israel.
B. Simpson-Young, N. Ozkan, C.Paris, C. Chung, 
J. Brook, K. Yap. 2000 Video Messaging: 
Addressing the Characteristics of the Medium. In 
the Proceedings of the Euromedia 2000 
Conference. Antwerp, May 2000. 
Borko, H., and Bernier, C. 1975 Abstracting 
Concepts and Methods. New York: Academic 
Press. 
F.Y.Y. Choi. 2000 Advances in domain 
independent linear text segmentation. In the 
Proc. of the North American Chapter of the 
Association. for Comp. Linguistics, pp. 26-33. 
Clark, H.H. and S.E Brennan. 1991. Grounding in 
Communication? In Readings in Groupware and 
Computer-Supported Collaborative Work, R.M. 
Baeker, ed. Morgan Kaufmann, California, 222-
223. 
Nicolas Ducheneaut, Victoria Bellotti 2001. E-mail 
as habitat: an exploration of embedded personal 
information management, interactions. in 
Communications of the ACM v.8 n.5, p.30-38. 
Dustin Hillard, Mari Ostendorf, and Elizabeth 
Shriberg 2003. Detection Of Agreement vs. 
Disagreement In Meetings: Training With 
Unlabeled Data. in the Proc. HLT-NAACL 
Conference, Edmonton, Canada, May 2003. 
Gong Y., and Liu, X. 2001. Generic Text 
Summarization Using Relevance Measure and 
Latent Semantic Analysis. In the Proceedings 
SIGIR 2001: pages 19-25. 
M. Hearst. 1994. Multi-paragraph segmentation of 
expository text. In the Proc. of the 2nd Annual 
Meeting of the Association for Computational 
Linguistics, Las Cruces, NM. 
Hiroyuki Murakoshi, Akira Shimazu, and Koichiro 
Ochimizu. 1999. Construction of Deliberation 
Structure in Email Communication. In 
Proceedings of the Pacific Association for 
Computational Linguistics (PACLING'99), pages 
16--28, Aug. 
T. Hofmann. 1999. Probabilistic latent semantic 
analysis, in the Proceedings of the Fifteenth 
Conference on Uncertainty in Artificial 
Intelligence, Morgan Kaufmann Publishers, San 
Francisco, CA, pp. 289-296. 
Lam, Derek and Rohall, Steven L. and Schmandt, 
Chris and Stern, Mia K. 2002. Exploiting E-mail 
Structure to Improve Summarization.  Technical 
Paper at IBM Watson Research Center #20-02 
Ani Nenkova and Amit Bagga. 2003. Facilitating 
email thread access by extractive summary 
generation. In Proceedings of RANLP, Bulgaria. 
Newman and Blitzer, Paula Newman and John 
Blitzer. 2002. Summarizing Archived 
Discussions: a Beginning. In the Proceeding of 
Intelligent User Interfaces. 
G. Salton and M. J. McGill. 1983. Introduction to 
modern information retrieval, McGraw-Hill, 
New York. 
Owen Rambow, Lokesh Shrestha, John Chen and 
Christy Laurdisen. 2004. Summarizing Email 
Threads. In the Proc. of HLT-NAACL 2004: 
Short Papers. 
Stephen Wan, Mark Dras, C?cile Paris, Robert 
Dale. 2003. Using Thematic Information in 
Statistical Headline Generation. In the 
Proceedings of the Workshop on Multilingual 
Summarization and Question Answering at ACL 
2003, July 11, Sapporo, Japan 
K. Zechner and A. Lavie. 2001 Increasing the 
coherence of spoken dialogue summaries by 
cross-speaker information linking. In 
Proceedings of the NAACL-01 Workshop on 
Automatic Summarization, Pittsburgh, PA, June, 
2001. 
Using Thematic Information in Statistical Headline Generation 
Stephen Wan 
Center for Language 
Technology 
Macquarie University 
Sydney, Australia 
swan@ics.mq.edu.au 
Mark Dras 
Center for Language 
Technology 
Macquarie University 
Sydney, Australia 
madras@ics.mq.edu.au 
C?cile Paris 
CSIRO Mathematical 
and Information 
Sciences 
Locked Bag 17 
North Ryde 1670 
Sydney, Australia 
Cecile.Paris@csiro.au 
Robert Dale 
Center for Language 
Technology 
Macquarie University 
Sydney, Australia 
rdale@ics.mq.edu.au 
 
Abstract  
We explore the problem of single 
sentence summarisation.  In the news 
domain, such a summary might 
resemble a headline.  The headline 
generation system we present uses 
Singular Value Decomposition (SVD) to 
guide the generation of a headline 
towards the theme that best represents 
the document to be summarised.   In 
doing so, the intuition is that the 
generated summary will more accurately 
reflect the content of the source 
document.  This paper presents SVD as 
an alternative method to determine if a 
word is a suitable candidate for 
inclusion in the headline.  The results of 
a recall based evaluation comparing 
three different strategies to word 
selection, indicate that thematic 
information does help improve recall. 
1 Introduction 
Ours is an age where many documents are 
archived electronically and are available 
whenever needed. In the midst of this plethora of 
information, the successful completion of a 
research task is affected by the ease with which 
users can quickly identify the relevant electronic 
documents that satisfy their information needs.  
To do so, a researcher often relies on generated 
summaries that reflect the contents of the 
original document.    
We explore the problem of single sentence 
summarisation, the primary focus of this paper.  
Instead of identifying and extracting the most 
important sentence, we generate a new sentence 
from scratch.   The resulting sentence summary 
may not occur verbatim in the source document 
but may instead be a paraphrase combining key 
words and phrases from the text.   
As a precursor to single sentence summarisation, 
we first explore the particular case of headline 
generation in the news domain, specifically 
English news.  Although headlines are often 
constructed to be sensationalist, we regard 
headline generation as an approximation to 
single sentence summarisation, given that a 
corpus of single sentence summaries does not 
exist.   
Our system re-uses words from the news article 
to generate a single sentence summary that 
resembles a headline.  This is done by selecting 
and then appending words from the source 
article. This approach has been explored by a 
number of researchers (eg. see Witbrock and 
Mittal, 1999; Jin and Hauptmann, 2002) and we 
will describe their work further in the next 
section.  In existing approaches, a word is 
selected on the basis of two criteria: how well it 
acts as a summary word, and how grammatical it 
will be given the preceding summary words that 
have already been chosen. 
The purpose of this paper is to present work 
which investigates the use of Singular Value 
Decomposition (SVD) as a means of 
determining if a word is a good candidate for 
inclusion in the headline. 
To introduce the notion of using SVD for single 
sentence summarisation in this paper, we 
examine the simplest summarisation scenario.  
Thus, presently we are only concerned with 
single document summarisation.  In addition, we 
limit the focus of our discussion to the 
generation of generic summaries.  
In the remainder of this paper, we describe our 
motivation for using SVD by describing 
difficulties in generating headlines in Section 2.  
In Section 3, as motivation for our approach, we 
illustrate how words can be used out of context, 
resulting in factually incorrect statements.  
Section 4 provides an overview of related work. 
In Section 5, we give a detailed description of 
how we generate the sentence summary 
statistically and how we use SVD to guide the 
generation process.  In Section 6, we present our 
experimental design in which we evaluated our 
approach, along with the results and 
corresponding discussion.  Finally, in Section 7, 
we present our conclusions and future work. 
2 The Veracity of Generated Summaries 
Berger and Mittal (2000) describe limitations to 
the generation of headlines by recycling words 
from the article.  One such limitation is that the 
proposition expressed by the generated summary 
is not guaranteed to reflect the information in the 
source text.  As an example, they present two 
sentences of differing meaning which uses the 
same words.   We present their example in 
Example 1, which illustrates the case in which 
the subject and object are swapped.   
The dog bit the postman 
The postman bit the dog. 
Example 1. An example of different propositions 
presented in two sentences which use the same 
words. 
However, we believe that the veracity of the 
generated sentence, with respect to the original 
document, is affected by a more basic problem 
than variation in word order.   Because words 
from any part of a source document can be 
combined probabilistically, there is a possibility 
that words can be used together out of context.  
We refer to this as Out-of-Context error.   Figure 
1 presents an example of a generated headline in 
which the adverb wrongly reports stock price 
movement.  It also presents the actual context in 
which that adverb was used. 
Generated headline 
?singapore stocks shares rebound?? 
 
Actual headline: 
?Singapore shares fall, seen higher after 
holidays.? 
 
Original context of use of ?rebound?: 
?Singapore shares closed down below the 
2,200 level on Tuesday but were expected to 
rebound immediately after Chinese Lunar 
New Year and Muslim Eid Al-Fitr holidays, 
dealers said.?
 
Figure 1.  An error in the generated headline due 
to a word being re-used out of context. 
Out-of-Context errors arise due to limitations in 
the two criteria for selecting words mentioned in 
Section 1.  While, for selection purposes, a word 
is scored according to its goodness as candidate 
summary word, word order is determined by a 
notion of grammaticality, modelled 
probabilistically using ngrams of lexemes.  
However, the semantic relationship implied by 
probabilistically placing two words next to each 
other, for example an adjective and a noun, 
might be suspect.  As the name ?Out-of-
Context? suggests, this is especially true if the 
words were originally used in non-contiguous 
and unrelated contexts.  This limitation in the 
word selection criteria can be characterized as 
being due to a lack of long distance relationship 
information. 
3 Our Approach to ?Encouraging Truth? 
In response to this limitation, we explore the use 
of a matrix operation, Singular Value 
Decomposition (SVD) to guide the selection of 
words.  Although our approach still does not 
guarantee factual correctness with respect to the 
source document, it has the potential to alleviate 
the Out-of-Context problem by improving the 
selection criteria of words for inclusion in the 
generated sentence, by considering the original 
contexts in which words were used.  With this 
improved criteria, we hope to "encourage truth" 
by incorporating long distance relationships 
between words.  Conceptually, SVD provides an 
analysis of the data which describes the 
relationship between the distribution of words 
and sentences.  This analysis includes a 
grouping of sentences based on similar word 
distributions, which correspond to what we will  
refer to here as the main themes of the 
document.1  By incorporating this information 
into the word selection criteria, the generated 
sentence will "gravitate" towards a single theme.  
That is, it will tend to use words from that 
theme, reducing the chance that words are 
placed together out of context.   
By reflecting the content of the main theme, the 
summary may be informative (Borko, 1975).  
That is, the primary piece of information within 
the source document might be included within 
the summary. However, it would remiss of us to 
claim that this quality of the summary is 
guaranteed.  In general, the generated summaries 
are at least useful to gauge what the source text 
is about, a characteristic described by Borko as 
being indicative.   
Figure 2 presents the generated summary using 
SVD for the same test article presented in Figure 
1.  In this case, the summary is informative as 
not only are we told that the article is about a 
stock market, but the movement in price in this 
example is correctly determined. 
Generated headline using SVD: 
?singapore shares fall? 
Figure 2. The headline generated using an SVD-
based word selection criterion.  The movement 
in share price is correct. 
4 Related Work 
As the focus of this paper is on statistical single-
sentence summarisation we will not focus on 
preceding work which generates summaries 
greater in length than a sentence.  We direct the 
reader to Paice (1990) for an overview of 
summarisation based on sentence extraction.  
Examples of recent systems include Kupiec et 
al. (1995) and Brandow et al (1995).    For 
examples of work in producing abstract-like 
summaries, see Radev and McKeown (1998), 
which combines work in information extraction 
                                                 
1
 Theme is a term that is used in many ways by many 
researchers, and generally without any kind of formal 
definition.  Our use of the term here is akin to the 
notion that underlies work on text segmentation, 
where sentences naturally cluster in terms of their 
?aboutness?. 
and natural language processing.  Hybrid 
methods for abstract-like summarisation which 
combine statistical and symbolic approaches 
have also been explored; see, for example, 
McKeown et al (1999), Jing and McKeown 
(1999), and Hovy and Lin (1997). 
Statistical single sentence summarisation has 
been explored by a number of researchers (see 
for example, Witbrock and Mittal, 1999; Zajic et 
al., 2002).  We build on the approach employed 
by Witbrock and Mittal (1999) which we will 
describe in more detail in Section 3.   
Interestingly, in the work of Witbrock and Mittal 
(1999), the selection of words for inclusion in 
the headline is decided solely on the basis of 
corpus statistics and does not use statistical 
information about the distribution of words in 
the document itself.  Our work differs in that we 
utilise an SVD analysis to provide information 
about the document to be summarized, 
specifically its main theme.    
Discourse segmentation for sentence extraction 
summarisation has been studied in work such as 
Boguraev and Neff (2000) and Gong and Liu 
(2001).  The motivation behind discovering 
segments in a text is that a sentence extraction 
summary should choose the most representative 
sentence for each segment, resulting in a 
comprehensive summary.  In the view of Gong 
and Liu (2001), segments form the main themes 
of a document.  They present a theme 
interpretation of the SVD analysis, as it is used 
for discourse segmentation, upon which our use 
of the technique is based.  However, Gong and 
Liu use SVD for creating sentence extraction 
summaries, not for generating a single sentence 
summary by re-using words. 
In subsequent work to Witbrock and Mittal 
(1999), Banko et al (2000) describe the use of 
information about the position of words within 
four quarters of the source document.  The 
headline candidacy score of a word is weighted 
by its position in one of quarters.  We interpret 
this use of position information as a means of 
guiding the generation of a headline towards the 
central theme of the document, which for news 
articles typically occurs in the first quarter.  
SVD potentially offers a more general 
mechanism for handling the discovery of the 
central themes and their positions within the 
document.   
Jin et al (2002) have also examined a statistical 
model for headlines in the context of an 
information retrieval application.  Jin and 
Hauptmann (2001) provide a comparison of a 
variety of learning approaches used by 
researchers for modelling the content of 
headlines including the Iterative Expectation-
Maximisation approach, the K-Nearest 
neighbours approach, a term vector approach 
and the approach of Witbrock and Mittal (1999).  
In this comparison, the approach of Witbrock 
and Mittal (1999) fares favourably, ranking 
second after the term vector approach to title 
word retrieval (see Jin and Hauptmann, 2001, 
for details).   However, while it performs well, 
the term vector approach Jin et al (2002) 
advocate doesn't explicitly try to model the way 
a headline will usually discuss the main theme 
and may thus be subject to the Out-of-Context 
problem. 
Finally, for completeness, we mention the work 
of Knight and Marcu (2000), who examine 
single sentence compression.  Like Witbrock 
and Mittal (1999), they couch summarisation as 
a noisy channel problem.  Under this framework, 
the summary is a noise-less source of 
information and the full text is the noisy result.  
However, in contrast to our approach, Knight 
and Marcu (2000) handle parse trees instead of 
the raw text.  Their system learns how to 
simplify parse trees of sentences extracted from 
the document to be summarized, to uncover the 
original noise-less forms. 
5 Generating a Single Sentence Summary 
In this section, we describe our approach to 
single sentence summarisation.  As mentioned 
earlier, our approach is based on that of 
Witbrock and Mittal (1999).  It differs in the 
way we score words for inclusion in the 
headline.  Section 5.1 presents our re-
implementation of Witbrock and Mittal?s (1999) 
framework and introduces the Content Selection 
strategy they employ.  Section 5.2 describes our 
extension using SVD resulting in two alternative 
Content Selection strategies.  
5.1 Searching for a Probable Headline 
We re-implemented the work described in 
Witbrock and Mittal (1999) to provide a single 
sentence summarisation mechanism.  For full 
details of their approach, we direct the reader to 
their paper (Witbrock and Mittal, 1999).  A brief 
overview of our implementation of their 
algorithm is presented here. 
Conceptually, the task is twofold.  First, the 
system must select n words from a news article 
that best reflect its content.  Second, the best 
(grammatical) word ordering of these n words 
must be determined.  Witbrock and Mittal 
(1999) label these two tasks as Content Selection 
and Realisation.  Each of these criteria are 
scored probabilistically, whereby the probability 
is estimated by prior collection of corpus 
statistics.   
To estimate Content Selection probability for 
each word, we use the Maximum Likelihood 
Estimate (MLE).  In an offline training stage, the 
system counts the number of times a word is 
used in a headline, with the condition that it 
occurs in the corresponding news article.  To 
form the probability, this frequency data is 
normalised by the number of times the word is 
used in articles across the whole corpus.  This 
particular strategy of content selection, we refer 
to this as the Conditional probability.   
The Realisation criterion is determined simply 
by the use of bigram statistics, which are again 
collected over a training corpus during the 
training stage.  The MLE of the probability of 
word sequences is calculated using these bigram 
statistics.  Bigrams model the grammaticality of 
a word given the preceding word that has 
already been chosen. 
It should be noted that both the Content 
Selection and Realisation criteria influence 
whether a word is selected for inclusion in the 
headline.  For example, a preposition might 
poorly reflect the content of a news article and 
score a low Content Selection probability.  
However, given the context of the preceding 
word, it may be the only likely choice. 
In both the training stage and the headline 
generation stage, the system employs the same 
preprocessing.  The preprocessing, which 
mirrors that used by Witbrock and Mittal (1999), 
replaces XML markup tags and punctuation 
(except apostrophes) with whitespace.  In 
addition, the remaining text is transformed into 
lower case to make string matching case 
insensitive.  The system performs tokenisation 
by using whitespace as a word delimiter. 
In Witbrock and Mittal?s approach (1999), the 
headline generation problem reduces to finding 
the most probable path through a bag of words 
provided by the source document, essentially a 
search problem.  They use the beam search 
variety of the Viterbi algorithm (Forney, 1973) 
to efficiently search for the headline.  In our 
implementation, we provided the path length as 
a parameter to this search mechanism.  In 
addition, we used a beam size of 20.   
To use the Viterbi algorithm to search for a path, 
the probability of adding a new word to an 
existing path is computed by combining the 
Content selection probability, the Realisation 
probability and the probability of the existing 
path, which is recursively defined. Combining 
each component probability is done by finding 
the logs of the probabilities and adding them 
together.  The Viterbi algorithm sorts the paths 
according to the path probabilities, directing the 
search towards the more probable word 
sequences first.  The use of repeated words in 
the path is not permitted. 
5.2 Using Singular Value Decomposition for 
Content Selection 
As an alternative to the Conditional probability, 
we examine the use of SVD in determining the 
Content Selection probability.  Before we 
outline the procedure for basing this probability 
on SVD, we will first outline our interpretation 
of the SVD analysis, based on that of Gong and 
Liu (2001).  Our description is not intended to 
be a comprehensive explanation of SVD, and we 
direct the reader to Manning and Sch?tze (2000) 
for a description of how SVD is used in 
information retrieval. 
Conceptually, when used to analyse documents, 
SVD can discover relationships between word 
co-occurrences in a collection of text.  For 
example, in the context of information retrieval, 
this provides one way to retrieve additional 
documents that contain synonyms of query 
terms, where synonymy is defined by similarity 
of word co-occurrences.  By discovering 
patterns in word co-occurrences, SVD also 
provides information that can be used to cluster 
documents based on similarity of themes. 
In the context of single document 
summarisation, we require SVD to cluster 
sentences based on similarities of themes.   The 
SVD analysis provides a number of related 
pieces of information relating to how words and 
sentences relate to these themes.  One such piece 
of information is a matrix of scores, indicating 
how representative the sentence is of each 
theme.  Thus, for a sentence extraction 
summary, Gong and Liu (2001) would pick the 
top n themes, and for each of these themes, use 
this matrix to choose the sentence that best 
represents it.   
For single sentence summarisation, we assume 
that the theme of the generated headline will 
match the most important theme of the article.  
The SVD analysis orders its presentation of 
themes starting with the one that accounts for 
the greatest variation between sentences.  The 
SVD analysis provides another matrix which 
scores how well each word relates to each 
theme.  Given a theme, scores for each word, 
contained in a column vector of the matrix, can 
then normalised to form a probability.  The 
remainder of this section provides a more 
technical description of how this is done. 
To begin with, we segment a text into sentences.  
Our sentence segmentation preprocessing is 
quite simple and based on the heuristics found in 
Manning and Sch?tze (2000).  After removing 
stopwords, we then form a terms by sentences 
matrix, A.  Each column of A represents a 
sentence.  Each row represents the usage of a 
word in various sentences. Thus the frequency 
of word t in sentence s is stored in the cell  Ats.  
This gives us an t * s matrix, where t ? s.  That 
is, we expect the lexicon size of a particular 
news article to exceed the number of sentences.   
For such a matrix, the SVD of A is a process 
that provides the right hand side of the following 
equation: 
A = U.S. Vtranspose  
where U is  a t * r matrix, S is an r * r matrix, 
and V is an s * r matrix.  The dimension size r is 
the rank of A, and is less than or equal to the 
number of columns of A, in this case, s.    The 
matrix S is a diagonal matrix with interesting 
properties, the most important of which is that 
the diagonal is sorted by size.  The diagonal 
values indicate the variation across sentences for 
a particular theme, where each theme is 
represented by a separate diagonal element.  The 
matrix V indicates how representative a sentence 
is of a score.  Similarly the matrix U indicates 
how related to the themes each word is.  A 
diagram of this is presented in Figure 3. 
Before describing how we use each of these 
matrices, it is useful to outline what SVD is 
doing geometrically.  Each sentence, a column 
in the matrix A, can be thought of as an object in 
t dimensional space.  SVD uncovers the 
relations between dimensions. For example, in 
the case of text analysis, it would discover 
relationships between words such as synonyms.  
In a trivial extreme of this case where two 
sentences differ only by a synonym, SVD would 
ideally discover that the two synonyms have 
very similar word co-occurrences.  In the 
analysis matrices of U, S and V, the redundant 
dimensions corresponding to these highly 
similar words might be removed, resulting in a 
reduced number of dimensions, r, required to 
represent the sentences.   
 
Figure 3.  A diagram of our interpretation of the 
SVD matrices as it relates to single sentence 
summarisation. 
Of the resulting matrices, V is an indication of 
how each sentence relates to each theme, 
indicated by a score.  Thus, following Gong and 
Liu (2001), a plausible candidate for the most 
important sentence is found by taking the first 
column vector of V (which has s elements), and 
finding the element with the highest value.  This 
sentence will be the one which is most 
representative of the theme.  The index of that 
element is the index of the sentence to extract.   
However, our aim is not to extract a sentence but 
to utilise the theme information.  The U matrix 
of the analysis provides information about how 
well words correspond to a particular theme.  
We examine the first column of the U matrix, 
sum the elements and then normalize each 
element by the sum to form a probability.  This 
probability, which we refer to as the SVD 
probability, is then used as the Content Selection 
probability in the Viterbi search algorithm. 
As an alternative to using the SVD probability 
and the Conditional Probability in isolation, a 
Combined Probability is calculated using the 
harmonic mean of the two.  The harmonic mean 
was used in case the two component 
probabilities differed consistently in their 
respective orders of magnitude.  Intuitively, 
when calculating a combined probability, this 
evens the importance of each component 
probability. 
To summarize, we end up with three alternative 
strategies in estimating the Content Selection 
Probability: the Conditional Probability, the 
SVD Probability and the Combined Probability. 
6 Experiments  
6.1 Data 
In our experiments, we attempted to match the 
experimental conditions of Witbrock and Mittal 
(1999).  We used news articles from the first six 
months of the Reuters 1997 corpus (Jan 1997 to 
June 1997).  Specifically, we only examined 
news articles from the general Reuters category 
(GCAT) which covers primarily politics, sport 
and economics.   This category was chosen not 
because of any particular domain coverage but 
because other categories exhibited frequent use 
of tabular presentation.  The GCAT category 
contains in excess of 65,000 articles.  Following 
Witbrock and Mittal (1999), we randomly 
selected 25,000 articles for training and a further 
1000 articles for testing, ensuring that there was 
no overlap between the two data sets.  During 
the training stage, we collected bigrams from the 
headline data, and the frequency of words 
occurring in headlines. 
6.2 Experiment Design 
We conducted an evaluation experiment to 
compare the performance of the three Content 
Selection strategies that we identified in Section 
5: the Conditional probability, the SVD 
probability, and the Combined probability.  We 
measure performance in terms of recall, i.e. how 
many of the words in the actual headline match 
words in the generated headline.2  The recall 
metric is normalised to form a percentage by 
dividing the word overlap by the number of 
words in the actual headline.   
For each test article, we generated headlines 
using each of the three strategies.  For each 
strategy, we generated headlines of varying 
lengths, ranging from length 1 to 13, where the 
latter is the length of the longest headline found 
in the test set.  We then compared the different 
strategies for generated headlines of equal 
length.   
To determine if differences in recall scores were 
significant, we used the Wilcoxon Matched Pairs 
Signed Ranks (WMPSR) test (Seigel and 
Castellan, 1988).  In our case, for a particular 
pair of Content Selection strategies, the alternate 
hypothesis was that the choice of Content 
Selection strategy affects recall performance.  
The null hypothesis held that there was no 
difference between the two content selection 
strategies.  Our use of the non-parametric test 
was motivated by the observation that recall 
scores were not normally distributed.  In fact, 
our results showed a positive skew for recall 
scores.  To begin with, we compared the recall 
scores of the SVD strategy and the Conditional 
strategy in one evaluation.  The strategy that was 
found to perform better was then compared with 
the Combined strategy. 
                                                 
2
 Word overlap, whilst the easiest way to evaluate the 
summaries quantitatively, is an imprecise measure 
and must be interpreted with the knowledge that non-
recall words in the generated headline might still 
indicate clearly what the source document is about. 
In addition to the recall tests, we conducted an 
analysis to determine the extent to which the 
SVD strategy and the Conditional probability 
strategy were in agreement about which words 
to select for inclusion in the generated headline.  
For this analysis, we ignored the bigram 
probability of the Realisation component and 
just measured the agreement between the top n 
ranking words selected by each content selection 
strategy.  Over the test set, we counted how 
many words were selected by both strategies, 
just one strategy, and no strategies.  By 
normalising scores by the number of test cases, 
we determine the average agreement across the 
test set.  We ran this experiment for a range of 
different values of N, ranging from 1 to 13, the 
length of the longest headline in the test set.   
6.3 Results 
6.3.1 Recall Comparison 
The results for the comparison of recall scores 
are presented in Table 1 and Table 2.  Table 1 
shows results of the WMPSR test when 
comparing the SVD strategy with the 
Conditional strategy.3  Since the Conditional 
strategy was found to perform better, we then 
compared this with the Combined strategy, as 
shown in Table 2.  From Table 1, it is clear that, 
for all sentence lengths, there is a significant 
difference between the SVD strategy and the 
Conditional strategy, and so we reject the null 
hypothesis.  Similarly, Table 2 shows that there 
is a significant difference between the 
Conditional strategy and the Combined strategy, 
and again we reject the null hypothesis. We 
conclude that SVD probability alone is 
outperformed by the Conditional probability; 
however, using both probabilities together leads 
to a better performance. 
 
 
 
 
                                                 
3
 The performance of our Conditional strategy is 
roughly comparable to the results obtained by Banko, 
Mittal and Witbrock (2000), in which they report 
recall scores between 20% to 25%, depending on the 
length of the generated headline.   
Sentence 
Length 
Average 
Recall : 
SVD 
Average 
Recall : 
Cond. Probability 
Reject  
H0 
1 03.68% 03.98% p ? 0.0 yes 
2 07.02% 06.97% p ?  0.5 yes 
3 10.05% 11.44% p ? 0.0 yes 
4 12.39% 13.90% p ? 0.0 yes 
5 14.21% 15.73% p ?0.0 yes 
6 15.57% 17.84% p ?1.1e-05 yes 
7 16.59% 19.14% p ? 1.8e-07 yes 
8 17.74% 20.30% p ? 1.3e-07 yes 
9 18.74% 21.33% p ? 1.3e-06 yes 
10 19.73% 22.44% p ? 1.0e-06 yes 
11 20.19% 23.50% p ? 2.2e-10 yes 
12 20.85% 24.54% p ? 4.4e-13 yes 
13 21.13% 25.13% p ? 1.4e-12 yes 
Table 1. A comparison of recall scores for the 
SVD strategy and the Conditional strategy. 
Sentence 
Length 
Average 
Recall : 
Cond 
Average 
Recall :  
Combined Probability 
Reject  
H0 
1 03.98% 04.05% p ? 0.1305 yes 
2 06.97% 08.60% p ? 2.8e-13 yes 
3 11.44% 12.34% p ? 0.0007 yes 
4 13.90% 15.44% p ? 8.5e-09 yes 
5 15.73% 17.33% p ? 1.9e-09 yes 
6 17.84% 18.72% p ? 0.0003 yes 
7 19.14% 20.34% p ? 1.3e-05 yes 
8 20.30% 21.48% p ? 2.9e-06 yes 
9 21.33% 22.60% p ? 4.0e-06 yes 
10 22.44% 23.82% p ? 1.2e-06 yes 
11 23.50% 24.56% p ? 0.0003 yes 
12 24.54% 25.44% p ? 0.0008 yes 
13 25.13% 26.37% p ? 8.6e-06 yes 
Table 2. A comparison of recall scores for the 
Conditional strategy and the Combined strategy.   
6.3.2 Agreement between Strategies 
The agreement between strategies is presented in 
Table 3.  Interestingly, of the words recalled, the 
majority have only been selected by one content 
selection strategy.  That is, the set of words 
recalled by one content selection strategy do not 
necessarily subsume the set recalled by the 
other.  This supports the results obtained in the 
recall comparison in which a combined strategy 
leads to higher recall.  Interestingly, the last 
column in the table shows that the potential 
combined recall is greater than the recall 
achieved by the combined strategy; we will 
return to this point in Section 6.4. 
 
 
 
 
Sentence 
Length 
Selected 
by neither 
method 
Selected by 
only 1 
method 
Selected 
by both 
methods 
Total 
Recall 
1 91.6% 8.0% 0.3% 8.3% 
2 84.7% 14.1% 1.0% 15.1% 
3 79.9% 17.5% 2.5% 20.0% 
4 76.6% 19.3% 3.9% 23.2% 
5 73.8% 21.0% 5.1% 26.1% 
6 71.4% 22.1% 6.4% 28.5% 
7 69.6% 22.4% 7.8% 30.2% 
8 67.9% 22.9% 9.1% 32.0% 
9 66.4% 23.2% 12.3% 35.5% 
10 65.0% 23.5% 11.3% 34.8% 
11 63.9% 23.6% 12.3% 35.9% 
12 63.0% 23.6% 13.2% 36.8% 
13 62.1% 23.5% 14.3% 37.8% 
Table 3.  Agreement of words chosen between 
the SVD strategy and the Conditional 
probability strategy to content selection 
6.4 Discussion 
The SVD strategy ultimately did not perform as 
well ass we might have hoped.  There are a 
number of possible reasons for this. 
1. Whilst using the Combined probability did 
lead to a significantly improved result, this 
increase in recall was only small.  Indeed, 
the analysis of the agreement between the 
Conditional strategy and the SVD strategy 
indicates that the current method of 
combining the two probabilities is not 
optimal and that there is still considerable 
margin for improvement. 
2. Even though the recall of the SVD strategy 
was poorer by a only a few percent, the lack 
of improvement in recall is perplexing, 
given that we expected the thematic 
information to ensure words were used in 
correct contexts. There are several possible 
explanations, each warranting further 
investigation.  It may be the case that the 
themes identified by the SVD analysis were 
quite narrow, each encompassing only small 
number of sentences.  If this is the case, 
certain words occurring in sentences outside 
the theme would be given a lower 
probability even if they were good headline 
word candidates.  Further investigation is 
necessary to determine if this is a short-
coming of our SVD strategy or an artefact of 
the domain.  For example, it might be the 
case that the sentences of news articles are 
already thematically quite dissimilar.   
3. One might also question our experimental 
design.  Perhaps the kind of improvement 
brought about when using the SVD 
probability cannot be measured by simply 
counting recall.  Instead, it may be the case 
that an evaluation involving a panel of 
judges is required to determine if the 
generated text is qualitatively better in terms 
of how faithful the summary is to the 
information in the source document.  For 
example, a summary that is more accurate 
may not necessarily result in better recall.  
Finally, it is conceivable that the SVD 
strategy might be more sensitive to 
preprocessing stages such as sentence 
delimitation and stopword lists, which are 
not necessary when using the Conditional 
strategy.  
Despite these outstanding questions, there are 
pragmatic benefits when using SVD.  The 
conditional strategy requires a paired training set 
of summaries and source documents.  In our 
case, this was easily obtained by using headlines 
in lieu of single sentence summaries.  However, 
in cases where a paired corpus is not available 
for training, the SVD strategy might be more 
appropriate, given that the performance does not 
differ considerably.  In such a situation, a 
collection of documents is only necessary for 
collecting bigram statistics. 
7 Conclusion 
Combining both the SVD probability and 
Conditional probability marginally improves 
recall, lending support to the intuition that 
thematic information may help generate better 
single sentence summaries.  However, there are 
still many unanswered questions.  In future 
work, we intend to investigate these techniques 
in a domain other than news text so that we can 
draw conclusions as to how well these strategies 
generalise to other genres.  We also intend to 
conduct user evaluations to gauge the quality of 
the generated summaries for both the 
Conditional and the SVD strategies.  Indeed, a 
user-based evaluation would be extremely 
helpful in determining if the thematic 
information provided by the SVD strategy does 
help improve the veracity of the generated 
summaries.    
References  
Banko M., Mittal V., and Witbrock M. (2000) 
Headline generation based on statistical translation. 
In Proceedings of the 38th Annual Meeting of the 
Association for Computational Linguistics. 
Boguraev B., and Neff M. (2000) Discourse 
segmentation in aid of document summarization. In 
Proceedings of the Hawaii International 
Conference on System Sciences (HICSS- 33), 
Minitrack on Digital Documents Understanding. 
Maui, Hawaii: IEEE. 
Borko, H., and Bernier, C. (1975) Abstracting 
Concepts and Methods. New York: Academic 
Press. 
Brandow, R., Mitze, K., and Rau, L. (1995) 
Automatic condensation of electronic publications 
by sentence selection. In Information Processing 
and Management, 31(5), pages 675-685. 
Forney G. D. (1973) The Viterbi Algorithm.  In the 
Proceedings of the IEEE, pages 268-278. 
Gong Y., and Liu, X. (2001) Generic Text 
Summarization Using Relevance Measure and 
Latent Semantic Analysis. In the Proceedings 
SIGIR 2001: pages 19-25. 
Hovy, E. and Lin, C. (1997) Automated text 
summarization in SUMMARIST.  In the 
Proceedings of ACL-EACL?97 Workshop on 
Intelligent Scalable Text Summarization, pages 18-
24. 
Jin, R., and Hauptmann, A. (2001) Learning to Select 
Good Title Words: An New Approach based on 
Reversed Information Retrieval.  In the 
Proceedings of the Eighteen International 
Conference on Machine Learning (ICML 2001), 
Williams College,MA, June 28-July 1. 
Jin, R., Zhai, C., and Hauptmann, A. (2002) Title 
language model for information retrieval. In the 
Proceedings of the 25th Annual International ACM 
SIGIR Conference on Research and Development 
in Information Retrieval (SIGIR 2002), Tampere, 
Finland, August 11-15. 
Jing, H., and McKeown, K. (1999) The 
decomposition of human-written summary 
sentences. In the Proceedings of the 22nd 
Conference on Research and Development in 
Information Retrieval (SIGIR--99).  
Knight, K. and Marcu, D. (2000) Statistics-based 
summarization---Step one: Sentence compression. 
In Proceedings of AAAI-2000. 
Kupiec, J., Pedersen, J., and Chen, F. (1995) A 
Trainable Document Summarizer. In Proceedings 
of the 18th Annual International ACM SIGIR 
Conference on Research and Development in 
Information Retrieval. Fox, E., Ingwersen, P., and 
Fidel, R. (Editors), pages 68?73. 
Manning C. and Sch?tze H. (2000) Foundations of 
Statistical Natural Language Processing.  MIT 
Press: Cambridge MA. 
Marcu, D. (2000) The Theory and Practice of 
Discourse Parsing and Summarization. 
Cambridge: The MIT Press. 
McKeown, K., Klavans, J., Hatzivassiloglou, V., 
Barzilay, R., and Eskin, E. (1999) Towards 
multidocument summarization by reformulation: 
Progress and prospects. In the Proceedings of the 
Sixteenth National Conference on Artificial 
Intelligence (AAAI--99). 
Paice, C. (1990) Constructing Literature Abstracts by 
Computers: Techniques and Prospects.  In 
Information Processing and Management, Vol. 26, 
No. 1, pages 171?186. 
Radev, D. and McKeown, K. (1998) Generating 
natural language summaries from multiple on-line 
sources. Computational Linguistics, 24(3):469-500, 
September.  
Siegel, Sidney and Castellan, Jr. N. John. (1988) 
Nonparametric Statistics For The Behavioral 
Sciences. McGraw-Hill, Inc., second edition. 
Witbrock, M., and Mittal, V. (1999) 
Ultrasummarization: A statistical approach to 
generating highly condensed non-extractive 
summaries. In the Proceedings of the 22nd 
International Conference on Research and 
Development in Information Retrieval (SIGIR '99). 
Zajic D., Door B., and Schwartz R. (2002) Automatic 
Headline Generation for Newspaper Stories. In the 
Proceedings of the Document Understanding 
Conference (DUC 2002). 
Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 45?53,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Designing a Citation-Sensitive Research Tool:
An Initial Study of Browsing-Specific Information Needs
Stephen Wan?, Ce?cile Paris?,
? ICT Centre,
CSIRO, Australia
Firstname.Lastname@csiro.au
Michael Muthukrishna?, Robert Dale?
?Centre for Language Technology
Faculty of Science
Macquarie University, Australia
rdale@science.mq.edu.au
Abstract
Practitioners and researchers need to stay
up-to-date with the latest advances in
their fields, but the constant growth in
the amount of literature available makes
this task increasingly difficult. We in-
vestigated the literature browsing task via
a user requirements analysis, and identi-
fied the information needs that biomed-
ical researchers commonly encounter in
this application scenario. Our analysis re-
veals that a number of literature-based re-
search tasks are preformed which can be
served by both generic and contextually
tailored preview summaries. Based on this
study, we describe the design of an im-
plemented literature browsing support tool
which helps readers of scientific literature
decide whether or not to pursue and read a
cited document. We present findings from
a preliminary user evaluation, suggesting
that our prototype helps users make rele-
vance judgements about cited documents.
1 Introduction
Practitioners and researchers in all fields face
a great challenge in attempting to keep up-to-
date with the literature relevant to their work.
In this context, search engines provide a useful
tool for information discovery; but search is just
one modality for gathering information. We also
regularly read through documents and expect to
find additional relevant information in referenced
(cited or hyperlinked) documents. This results in
a browsing-based activity, where we explore con-
nections through related documents.
This browsing behaviour is increasingly sup-
ported today as publishers of scientific material
deliver hyperlinked documents via a variety of
media including Adobe?s Portable Document For-
mat (PDF) as well as the more conventional web
hypertext format. Given appropriate document
databases and knowledge of referencing conven-
tions, it is relatively straightforward to support
the automatic downloading of cited documents:
such functionality already exists within reference
managers such as JabRef 1 and Sente2. This
?blind downloading?, however, does not address
the question of the relevancy of the linked docu-
ment for the reader at the time of reading. Apart
from the publication details of the reference and
the citation context, readers are provided with very
little information on the basis of which to de-
termine whether the cited document is worth ex-
ploring more thoroughly. Given the potentially
large number of citations that may be encountered,
this results in the following browsing-specific sce-
nario: how can we help a user quickly determine
whether the cited document is indeed worth down-
loading, perhaps paying for, and reading?
In the study presented here, we focussed on the
needs of biomedical researchers, who are often
time-poor and yet apparently spend 18% of their
time gathering and reviewing information (Hersh,
2008). They regularly search through reposito-
ries of online scholarly literature to update their
expert knowledge; in this domain, the penalty for
not staying up-to-date with the latest advances can
be severe, potentially affecting medical experi-
ments. In our work, we found that two thirds of re-
searchers regularly engaged in browsing scientific
literature. Given the prevalent use of the browsing
modality, we believe that novel research tools are
needed to help readers make decisions about the
relevance of cited material.
To better understand the user?s information
needs that arise when reading and browsing
through academic literature, and to ascertain what
NLP techniques we might be able to use to
help support them, we conducted a user require-
1jabref.sourceforge.net
2www.thirdstreetsoftware.com
45
ments analysis. It revealed a number of common
problems faced by readers of scientific literature.
These served to focus our efforts in designing and
implementing a browsing support tool for scien-
tific literature, referred to here as CSIBS.
CSIBS helps readers decide which cited docu-
ments to read by providing them with information
which is useful at the point when citations are en-
countered. The application provides information
about the cited document and identifies important
sentences in that document, based on the user?s
current reading context. The key observation here
is that the reading context can indicate why the
reader might be interested in the cited document.
In addition to meta-data about the cited document,
and its abstract, a contextualised preview is shown
within the same browser in which the citing docu-
ment is being viewed (for example, Adobe Acro-
bat Reader or a web browser), thus avoiding an
interruption to the user?s primary reading activ-
ity. This contextualised preview contains impor-
tant sentences from the cited document that are re-
lated to the reading context.
We present related work on understanding in-
formation needs in Section 2; we outline our user
requirements analysis in the domain of scientific
literature in Section 3; and the results of the analy-
sis and our understanding of the browsing-specific
information needs are presented in Section 4. In
Section 5, we describe a tool developed to meet
the most pressing of these information needs. Sec-
tion 6 presents a feedback from an initial evalua-
tion. We conclude by discussing our overall find-
ings in Section 7.
2 Related Work
2.1 Information Needs
Existing work on information needs, beginning
with Taylor (1962), typically focuses on mapping
from a particular query to the underlying inter-
est of the user. In a recent example of such
work, Henrich and Luedecke (2007) describes
methods for constructing lists of domain-specific
key words which may correspond well to user
interests. However, we are interested in relat-
ing information needs to user tasks in scenarios
in which there is no explicit query, as in Bystrm
et al (1995); in particular, our work focuses on
browsing scenarios. Toms (2000) presents a study
of browsing behaviour over electronic texts and
examines the differences between searching and
browsing. In that work, browsing is performed
across multiple news articles where the links be-
tween articles are inferred based on topic simi-
larity. In contrast, we consider explicit hyper-
text links which are linguistically embedded in the
document as citations, where the embedding text
serves as link anchors.
2.2 Information Needs in Biomedicine
Ely et al (2000) present an overview of the infor-
mation needs of practicing clinicians, deriving a
set of commonly asked questions. Although we
are interested in doctors as users, the type of in-
formation needs presented in this paper relate to
the activity of conducting scientific investigation,
rather than that of treating a patient.
Task-based analyses of the biomedical domain
have been studied by Bartlett and Neugebauer
(2008) and Tran et al (2004). Their analyses, like
ours, are task-based and use qualitative studies to
uncover the underlying uses of information. How-
ever, the tasks outlined in these related works are
focused on a specific set of information needs in a
research area: for example, the determination of a
functional analysis of gene sequences. Our work
differs in that we wish to take a more general view
in order to elicit information needs to do with sci-
entific research, at least at the level of biomedical
sciences.
The information needs and tasks of academic
users have been studied previously by Belkin
(1994), who focuses on scholarly publications in
the humanities domain. We perform an investi-
gation along similar lines, but with a focus on
academic literature used to conduct scientific re-
search.
2.3 Using Scientific Literature
The genre of academic literature, and the devel-
opment of technologies to support researchers as
users, has been studied by several groups work-
ing in automatic text summarisation. Teufel and
Moens (2002) describe a summarisation approach
that extracts text from documents and highlights
the rhetorical role that an extract plays within
the originating document (for example, stating the
Aim of an experiment). Qazvinian and Radev
(2008) present an approach to summarising aca-
demic documents based on finding citation con-
texts in the entire set of published literature for the
document in question. Both approaches, however,
treat the cited document in isolation of the read-
46
ing context and do not actively support the reading
task.
3 Understanding How Researchers
Browse through Scientific Literature
To determine what readers of scientific literature
want to know about cited documents, we con-
ducted a user requirements analysis. Our method
is based on Grounded Theory (Glaser and Strauss,
1967), a commonly used approach in Human
Computer Interaction (Corbin and Strauss, 2008).
We began by interviewing subjects from an appro-
priate user demographic and recording their verbal
descriptions about a real scenario situated in their
day-to-day activities. Following this, we designed
a questionnaire for wider participation which pre-
sented scenario-based questions attempting to un-
cover their information needs and tasks. Partic-
ipants were asked to provide free text answers.
The responses were then collated and analysed for
commonalities, bringing to the fore those issues
that were salient across the participants. We report
on the questionnaire design and responses in this
paper.
Beginning with such a study can reduce the
risk of building tools that have only limited util-
ity. This is particularly true of new and less un-
derstood application scenarios, such as the one ex-
plored here.
3.1 Questionnaire Design
An online questionnaire was used to reach par-
ticipants who actively read academic literature.3
To encourage participation, the questionnaire was
limited to 10 questions, which were formulated in-
dependently of any particular scientific domain.
We were explicit about the aims of the question-
naire by providing an initial brief, stating that the
feedback from participants would be used to de-
velop new tools for browsing through scientific lit-
erature. Within the questionnaire, to prepare par-
ticipants for our scenario-based questions, the first
few questions were basic and concerned the gen-
eral usage of scientific literature. For example,
we asked about the high-level reasons for which
they used scientific literature (e.g., ?To learn about
a new topic?; ?To update your knowledge on a
particular topic?). Participants could also specify
3The online questionnaire tool, SurveyMonkey
(www.surveymonkey.com), was used to implement
the questionnaire as an online interactive form.
their own reasons. In addition, we also asked them
about the frequency of their literature browsing ac-
tivity.
The main section of the questionnaire consisted
of a series of questions, corresponding to the is-
sues we wanted to explore:
1. What information needs do researchers have
of a cited document, and what specific tasks
does this information serve?
2. What makes it difficult for researchers to find
the answers to their questions about cited
documents?
3. What tasks are potential targets for automa-
tion?
Questions were to be answered with free text
responses, focussed by presenting a scenario in
which the researcher encounters a citation whilst
reading a scientific publication. The first question
above aims to better understand the researchers?
information needs and tasks; the second and third
are concerned with ideas for potential applications
which could benefit from NLP and IR research.
To address the first research issue, participants
were asked to recall a recent experience in which,
while reading a publication, they had encountered
a citation. Within this context, participants were
asked to describe what questions they may have
had of the cited document. To clarify how these
questions relate to a specific context of use, re-
spondents were then asked to relate the questions
they identified back to some task undertaken as
part of their research work.
Responses regarding the difficulties encoun-
tered in satisfying information needs were col-
lected with respect to the participants earlier re-
sponses. So as to not bias the participant, the
question was phrased neutrally. We asked what as-
pects of scientific literature and current technology
made it easy or hard to find answers to the partic-
ipants? personal research questions. We examined
responses with the aim of determining how tech-
nology might reduce the burden of knowledge dis-
covery. Responses were again focused by using
the same scenario as in the previous question.
The third research issue was explored via two
separate questions. The first presented the partici-
pants with a scenario in which they had access to
a non-expert human assistant who could perform
one or more simple tasks identified in their ear-
lier responses; they were then asked what kinds
47
of tasks they would delegate to such an assistant.
A second, more direct, question was presented re-
quiring participants to describe which tools they
would like to use, or to suggest new tools that
would help them in the future, when it came to
browsing through scientific literature.
Finally, optional questions about the partici-
pants? research backgrounds were presented at the
end of the questionnaire. These were deliberately
placed last to reduce barriers to completion.
4 Questionnaire Data Analysis
4.1 Analysing the Results
We recruited users with a background in biomed-
ical life sciences since we had access to an ex-
tensive corpus of documents in this domain with
which to build some kind of application. Note,
however, that our questions were not specific to
this domain, and the questionnaire could poten-
tially be re-run with participants from a different
scientific background.
We contacted 36 users who might be interested
in life sciences publications. Of these, 24 partici-
pants started the questionnaire, and 18 completed
it. Of the 24 participants, two thirds indicated that
they browsed through academic literature at least
once a week.
The written responses were separately analysed
by three of the authors. Responses to each ques-
tion were examined, checking for repeated terms
and concepts that could form the basis of clus-
tering. Salient information needs were matched
to corresponding tasks, and commonly mentioned
areas of difficulty and suggestions for delega-
tion were grouped. Once each author had per-
formed his or her own analysis, the salient group-
ings for each question were collaboratively deter-
mined, consolidating the three analyses performed
in isolation. The most salient groupings were then
examined for potential tasks that might be auto-
mated.
4.2 Questionnaire Data
We now present the results of the analysis. These
are organised with respect to each of the three re-
search issues.
4.2.1 Questions of the Cited Document
Figure 1 presents the most frequently indicated in-
formation needs and the most frequent tasks that
were identified. The information needs can be
Information Needs Freq
[md] About accessing the full text 9
[co] Article details (Definition, Methods, Results) 7
[md] About the authors 6
[md] About the publication date 5
[co] About relevance to own work 4
[md] The abstract 3
[co] The references 3
Participant Task Freq
Deciding whether to believe the citation 4
Finding baselines for experiments 3
Comparing own ideas to article 3
Finding information to justify the citation 3
Finding information about methods 2
Finding additional references 2
Updating clinical knowledge 2
Conducting a survey of the literature 2
Identifying key researchers in the field 2
Updating research knowledge 2
Figure 1: Principal information needs and tasks of
participants with regard to citations. In the first
table, information needs are prefixed by ?md? for
meta-data and ?co? for content-oriented. ?Freq? in-
dicates the number of occurrences in the results.
grouped into two main categories. The first, which
we refer to as meta-data needs, refers to informa-
tion about the document external to the document
content itself. These needs could be met by a se-
ries of database queries about the document, in-
volving, for example, the author information and
the citation counts for the document. We note
that, often, the abstract can also be retrieved via
a database query (and thus does not require any
in-depth text analysis of the cited document), al-
though technically this is not meta-data. In terms
of the underlying task, this kind of generic infor-
mation may be used in deciding whether to trust
the cited source.
The second category of information needs,
which we refer to as being content-oriented, can
be met by providing information sourced from
within the cited document. This type of informa-
tion facilitates multiple tasks. For example, these
might include understanding why a document was
cited, or finding new baselines to design new ex-
periments. We refer to these tasks in general as
citation-focused, as some underlying information
need is triggered by the text that the participant has
just read, whether this is for advancing one?s un-
derstanding of a topic, or pursuing a specific line
of scientific inquiry.
48
4.2.2 Difficulties in Finding Answers
This question required participants to voluntarily
reflect on their own research practices, a process
that is influenced partially by their expertise in
research and their exposure to different research
tools. Some responses described features of soft-
ware that were appealing, while others related to
the difficulties faced by researchers in finding rel-
evant information. In this paper, we present only
the subset of responses that concern the difficulties
encountered, since this will influence the function-
ality of new research tools. These responses are
presented in Figure 2.
Difficulties Freq
Finding the exact text to justify the citation 3
Poor writing 2
Comparing documents 1
Resolving references to the same object 1
Figure 2: Difficulties in finding information.
In general, the difficulties concerned some kind
of analysis of text. We note that these tasks
are largely citation-focused, requiring content-
oriented information. Examples of comments re-
garding this task are presented in Figure 3. For ex-
ample, participants wanted to know how the cited
document compared the citing document from the
perspective of experimental design. However, the
citation-focused task that was most commonly
mentioned as difficult was that of justifying cita-
tions. Participants mentioned that reading through
the entire cited document for this purpose was a
tedious task, particularly when looking for infor-
mation in poorly written documents.
4.2.3 Tasks for Automation
Our analysis of responses to the task automation
questions revealed two interesting outcomes: del-
egation occurred often with the use of key words,
and participants expressed the need for tools to
express relationships between domain concepts.
These are presented in Figure 4.
Responses to the question regarding task del-
egation revealed that for research-oriented tasks,
participants felt the need to direct assistants
through the use of key words. This is consistent
to responses to earlier questions detailing what
aspects of current technology were attractive, in-
cluding user interface conventions such as key
word highlighting. Otherwise, the other reported
Citation usually does not include the position of the informa-
tion in the cited article . . . it might be necessary to read all of
the article to find it in another reference and so on.
If the first report was only citing the second report for a small
piece of information, that information may be hard to locate
in the second report.
The original reference may have just cited a very small com-
ponent of the second report, either just a comment made in
the discussion or a supplemental figure . . . It may take a while
to locate and justify the citation if it isn?t the major finding of
the report.
If I see a citation in a report that I am interested in, I gen-
erally want to know if the cited report actually supports the
statement in the original report. Very often ? way too often ?
citations do not. For all important citations I track down the
original cited work and verify that it actually says what it is
supposed to.
Figure 3: Some sample responses from users with
regard to justifying citations; emphases added.
Automation Possibilities Freq
Search cited document for key words 4
Search for further publications using key words 3
Refine search using related concepts 6
Figure 4: Potential candidates for a new research
tool.
delegated task was that of simple database entry of
publication records. We interpret these responses
as indicating that participants are not overly will-
ing to hand over responsibility for complex tasks
to assistants. If delegation of more research-
oriented activities occurs, participants want to
understand how and why results were obtained.
While responses were made assuming delegation
to human assistants, we believe that such issues
are even more crucial for results obtained via au-
tomated means.
Suggested novel features centered upon a bet-
ter representation of relationships between do-
main concepts to be used for query refinement.
Responses included expressions such as ?refined
search?, a handling of user-specified ?mind maps?
(for repeated searches), and the use of ?trails? ex-
plaining how results connected to search terms,
key words and the author.
5 Prototype Requirements
As a result of these findings, we chose to build a
tool that meets the two types of information needs
revealed in the initial user requirements study. The
49
purpose of the resulting tool, CSIBS, is to help
readers prioritise which cited documents are worth
spending time to download and read further. In
this way, CSIBS helps readers to browse and nav-
igate through a dense network of cited documents.
To facilitate this task in accordance with the
elicited user requirements, CSIBS produces an
alternate version of a published article that has
been prepared with pop-up previews of cited doc-
uments. Each preview contains meta-data, the ab-
stract and content-oriented information. It is pro-
vided to the user to help perform research tasks
that arise as a consequence of encountering a cita-
tion and needing to investigate further. The pre-
view is not intended to serve as a surrogate for
the cited document. Rather, it is aimed at help-
ing readers make relevance judgements about ci-
tations.
The meta-data helps the user to appraise the ci-
tation and to make a value judgement about the
work cited. The abstract provides a generic sum-
mary of the cited document, indicating the scope
of the work cited. The content-oriented informa-
tion supports any citation-focused tasks, for exam-
ple citation justification, through the provision of
detailed information sourced from within the cited
document. We refer to this as a Contextualised
Preview. It is constructed using automatic text
summarisation techniques that tailor the resulting
summary to the user?s current interests, here ap-
proximately represented by the citation context:
that is, the sentence in which the citation is lin-
guistically embedded. We briefly describe CSIBS,
in this section; for a full description, see Wan et al
(2009).
Each preview appears in a pop-up text box ac-
tivated by moving the mouse over the citation.
The specific interaction (a double click versus a
?mouse-over?) depends on whether the article is
displayed via a web browser or as a PDF docu-
ment. Figure 5 shows the resulting pop-up for the
PDF display.
5.1 A Meta-Data Summary and Abstract
Participants often wanted a generic summary out-
lining the overall scope and contributions of the
cited work. This is typically available via the ab-
stract. Additionally, CSIBS presents a variety of
meta-data returned from queries to an online pub-
lications database:4
4www.embase.com
? The full reference: This provides readers
with the date of publication and the journal
title, amongst other things.
? Author Information: CSIBS can include data
to help the reader establish a level of trust
in the citation, primarily focusing on infor-
mation about the authors? affiliations and the
number of related citations in the research
area.
? The citation count for the cited document:
Participants indicated that this was useful in
appraising the cited article.
These pieces of information were commonly iden-
tified as useful in helping readers make value
judgements about the cited work. This is perhaps
an artifact of the biomedical domain, where re-
search has a critical nature and concerns health
and medical issues.
5.2 A Contextualised Preview
To generate the contextualised preview of the cited
document, the system finds the set of sentences
that relate to the citation context, employing ap-
proaches for summarising documents that exploit
anchor text (Wan and Paris, 2008). Following
Spark Jones (1998), we specify the purpose of the
contextualised summary along particular dimen-
sions, indicated here in italics:
? The situation is tied to a particular context of
use: an in-browser summary triggered by a
citation and its citing context.
? An audience of expert researchers is as-
sumed.
? The intended usage of the summary is one of
preview. We assume that the reader is making
a relevance judgement as to whether or not to
download (and, if necessary, buy) the cited
document. Specifically, the information pre-
sented should help the reader determine the
level of trust to place in the document, un-
derstand why the article is cited, and decide
whether or not to read it.
? The summary is intended only to provide
a partial coverage of the whole document,
specifically focused on content that directly
relates to the citation context.
? The style of the summary is intended to be
indicative. That is, it should present specific
50
Figure 5: A sample pop-up with an automatically generated summary, triggered by a mouse action over
the citation. Extracted sentences are grouped together by section titles. Words that match with the
citation context are coloured and emboldened.
details to facilitate a relevance judgement, al-
lowing the user to determine if the cited docu-
ment can be used to source more information
on a topic, as opposed to just mentioning it in
passing.
To create the preview summary, the cited docu-
ment is downloaded from a publisher?s database5
in its XML form and then segmented into sec-
tions, paragraphs and sentences. Each sentence in
the cited document is compared with the citation
context in order to find the best justification sen-
tences for that particular citation. Due to the lim-
ited space available in the pop-up, the number of
extracted sentences is capped at a predefined limit,
currently set to four. Using vector space methods
(Salton and McGill, 1983) weighted with term fre-
quency (and omitting stop words), the best match-
ing sentence is defined as the one scoring the high-
est on the cosine similarity metric with the citation
context. The attractiveness of this approach lies
in its simplicity, resulting in a fast computation of
5www.sciencedirect.com
a preview (? 0.03 seconds), making the process
amenable to batch processing of multiple docu-
ments or, in the future, live generation of previews
at runtime. To help with the readability of the re-
sulting preview, the system also extracts structural
information from the cited document. In particu-
lar, for each extracted sentence, the system identi-
fies the section in which it belongs; the extracted
sentences are then grouped by section, and pre-
sented with their section headings, as illustrated in
Figure 5.
CSIBS focuses on returning precise results, so
that the system does not exacerbate any existing
information overload problems by burdening the
reader with poorly matching sentences. To achieve
this, we currently use exact matches to words in
the citation context; in on-going work, we are ex-
ploring methods to relax this constraint without
hurting performance. In line with our user require-
ments analysis, we have designed the tool so that
the user is able to easily see how the summary was
constructed. Matching tokens are highlighted, al-
lowing the reader to understand why specific sen-
51
tences were extracted.
6 Initial Feedback
6.1 Evaluation Overview
We built a prototype version of CSIBS and con-
ducted a preliminary qualitative evaluation. The
goal was to examine how participants would react
to the pop-up previews. The feedback allows us to
further clarify our analysis and subsequent devel-
opment.
We asked participants to view a number of pop-
up previews in order to answer the question: Is
the Citation Justified? This was one of the more
difficult questions that researchers found challeng-
ing when making a relevancy judgement. The ac-
tual judgements are not important in this evalua-
tion. Instead, we gauged the reported utility of the
prototype based on the participants? self-reported
confidence when performing the task. To capture
this information, participants were asked to score
their confidence on a 3-point Likert scale.
Three biomedical researchers, all of whom had
taken part in our original user requirements analy-
sis, participated in the evaluation. Each participant
was shown nine different passages containing a ci-
tation context, each situated in a different FEBS
Letters6 publication (which was also presented in
full to the participants). At each viewing of a ci-
tation context, two supporting texts were provided
with which the participant was asked to answer the
citation justification question. For all participants,
the first supporting text was produced by a base-
line system that simply provided the full reference
of the citation. The second was either the abstract
or the contextualised preview, which in this eval-
uation was limited to three sentences. Meta-data
was not presented for this study as we specifically
wanted feedback on the citation justification task.
The small sample size does not permit hypoth-
esis testing. However, we are encouraged by the
comparable positive gains in self-reported confi-
dence scores (Abstract: +1.2 versus CSIBS: +2.2)
compared to simply showing the full reference.
Since both preview types were positive, we as-
sume that these types of information facilitated the
relevance judgements. Participants also reported
that, for the contextualised preview, 2 out of 3 sen-
tences were found to be useful on average.
6The journal of the Federation of Europeans Biochemical
Societies.
The qualitative feedback also supported CSIBS.
One participant made some particularly interest-
ing observations regarding selected sentences and
the structure of the cited document. Specifically,
useful sentences tended to be located deeper in the
cited document, for example in the methods sec-
tions This participant suggested that, for an expert
user, showing sentences from the earlier sections
of a publication was not useful; for the same rea-
son, the abstract might be too general and not help-
ful in justifying a citation. Finally, this participant
remarked that, in those situations where each doc-
ument downloaded from a proprietary repository
incurs a fee, the citation-sensitive previews would
be very useful in deciding whether to download
the document.
7 Conclusions
In this paper, we presented an analysis of
browsing-specific information needs in the do-
main of scientific literature. In this context, users
have information needs that are not realised as
search queries; rather these remain implicit in the
minds of users as they browse through hyperlinked
documents. Our analysis sheds light on these in-
formation needs, and the tasks being performed in
their pursuit, using a set of scenario-based ques-
tions.
The analysis revealed two tasks often performed
by participants: the appraisal task and the citation-
focused task. CSIBS was designed to support the
underlying needs by providing meta-data informa-
tion, the abstract, and a contextualised preview for
each citation. The user requirement of search re-
finement was not directly addressed in this work,
but could be met by techniques of query refine-
ment in IR, synonym-based expansion in sum-
marisation, and of course, additional user speci-
fied key terms. In future work, we will explore
these possibilities. Our results to date are encour-
aging for the use of NLP techniques to support
readers prioritise which cited documents to read
when browsing through scientific literature.
Acknowledgments
We would like to thank all the participants who
took part in our study. We would also like to thank
Julien Blondeau and Ilya Anisimoff, who helped
to implement the prototype.
52
References
Joan C. Bartlett and Tomasz Neugebauer. 2008. A
task-based information retrieval interface to support
bioinformatics analysis. In IIiX ?08: Proceedings of
the second international symposium on Information
interaction in context, pages 97?101, New York, NY,
USA. ACM.
Nicholas J. Belkin. 1994. Design principles for
electronic textual resources: Investigating users and
uses of scholarly information. In Current Issues in
Computational Linguistics: In Honour of Donald
Walker.Kluwer, pages 1?18. Kluwer.
Katriina Bystrm, Katriina Murtonen, Kalervo Jrvelin,
Kalervo Jrvelin, and Kalervo Jrvelin. 1995. Task
complexity affects information seeking and use.
In Information Processing and Management, pages
191?213.
Juliet Corbin and Anselm L. Strauss. 2008. Basics of
qualitative research : techniques and procedures for
developing grounded theory. Sage, 3rd edition.
John W Ely, Jerome A Osheroff, Paul N Gorman,
Mark H Ebell, M Lee Chambliss, Eric A Pifer, and
P Zoe Stavri. 2000. A taxonomy of generic clini-
cal questions: classification study. British Medical
Journal, 321:429?432.
Barney G. Glaser and Anselm L. Strauss. 1967. The
Discovery of Grounded Theory: Strategies for Qual-
itative Research. Aldine de Gruyter, New York.
Andreas Henrich and Volker Luedecke. 2007. Char-
acteristics of geographic information needs. In GIR
?07: Proceedings of the 4th ACM workshop on Ge-
ographical information retrieval, pages 1?6, New
York, NY, USA. ACM.
W. R. Hersh. 2008. Information Retrieval. Springer.
Information Retrieval for biomedical researchers.
Vahed Qazvinian and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation summary
networks. In The 22nd International Conference on
Computational Linguistics (COLING 2008), Mach-
ester, UK, August.
G. Salton and M. J. McGill. 1983. Introduction to
modern information retrieval. McGraw-Hill, New
York.
Karen Spark Jones. 1998. Automatic summarizing:
factors and directions. In I. Mani and M. Maybury,
editors, Advances in Automatic Text Summarisation.
MIT Press, Cambridge MA.
Robert S Taylor. 1962. Process of asking questions.
American Documentation, 13:391?396, October.
Simone Teufel and Marc Moens. 2002. Summa-
rizing scientific articles: experiments with rele-
vance and rhetorical status. Computional Linguis-
tics, 28(4):409?445.
Elaine G. Toms. 2000. Understanding and facilitating
the browsing of electronic text. International Jour-
nal of Human-Computing Studies, 52(3):423?452.
D Tran, C Dubay, P Gorman, and W. Hersh. 2004. Ap-
plying task analysis to describe and facilitate bioin-
formatics tasks. Studies in Health Technology and
Informatics, 107107(Pt 2):818?22.
Stephen Wan and Ce?cile Paris. 2008. In-browser sum-
marisation: Generating elaborative summaries bi-
ased towards the reading context. In The 46th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: Short
Paper, Columbus, Ohio, June.
Stephen Wan, Ce?cile Paris, and Robert Dale. 2009.
Whetting the appetite of scientists: Producing sum-
maries tailored to the citation context. In Proceed-
ings of the Joint Conference on Digital Libraries.
53
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 129?132,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
In-Browser Summarisation: Generating Elaborative Summaries Biased
Towards the Reading Context
Stephen Wan and Ce?cile Paris
ICT Centre?
CSIRO
Locked Bag 17, North Ryde, Sydney
NSW 1670, Australia
Firstname.Lastname@csiro.au
Abstract
We investigate elaborative summarisation,
where the aim is to identify supplementary in-
formation that expands upon a key fact. We
envisage such summaries being useful when
browsing certain kinds of (hyper-)linked doc-
ument sets, such as Wikipedia articles or
repositories of publications linked by cita-
tions. For these collections, an elaborative
summary is intended to provide additional in-
formation on the linking anchor text. Our con-
tribution in this paper focuses on identifying
and exploring a real task in which summarisa-
tion is situated, realised as an In-Browser tool.
We also introduce a neighbourhood scoring
heuristic as a means of scoring matches to rel-
evant passages of the document. In a prelim-
inary evaluation using this method, our sum-
marisation system scores above our baselines
and achieves a recall of 57% annotated gold
standard sentences.
1 Introduction
It has long been held that a summary is useful, par-
ticularly if it supports the underlying task of the user
? for an overview of summarisation scenarios see
Spark Jones (1998). For example, generic (that is,
not query-specific) summaries, which are often in-
dicative, providing just the gist of a document, are
only useful if they happen to address the underlying
need of the user.
In a push to make summaries more responsive
to user needs, the field of summarisation has ex-
plored the overlap with complex question-answering
?Information and Communication Technologies Centre
research to produce query-focused summaries. Such
work includes the recent DUC challenges on query-
focused summarisation,1 in which the user needs are
represented by short paragraphs of text written by
human judges. These are then used as input to the
summarisation process. However, modelling user
needs is a difficult task. DUC descriptions of in-
formation needs are only an artificial stipulation of a
user?s interest.
In this work, we propose a tool built into an inter-
net browser that makes use of a very simple heuris-
tic for determining user interest.2 The basic premise
of the heuristic is that the text currently being read
provides an approximation of the current user inter-
est. Specifically, as a user reads a sentence, it po-
tentially represents a fine-grained information need.
We identify the sentence of interest without com-
plex methods, relying instead on the user to move
the mouse over the anchor text link to request a sum-
mary of the linked document, thus identifying to the
browser plug-in which sentence is now in focus.
To generate the summary, the whole document,
specifically the linking sentence that contains the an-
chor text, serves as the reading context, a potential
indicator of the user interest. An example of the cur-
rent output on Wikipedia text is presented in Figure
1. It shows an elaborative summary of a document
about the Space Shuttle Discovery expanding on the
content of the linking sentence. In this case, it gives
further information about a space walk in which the
shuttle was repaired inflight.
Our summarisation tool, the In-Browser Elabora-
1http://duc.nist.gov/guidelines/2006.html
2We currently work with the Firefox browser.
129
Figure 1: A summary generated when moving the mouse
over the link ?Discovery?s? (mouse pointer omitted).
tive Summariser (IBES), complements generic sum-
maries in providing additional information about a
particular aspect of a page.3 Generic summaries
themselves are easy to generate due to rules enforced
by the Wikipedia style-guide, which dictates that all
titles be noun phrases describing an entity, thus serv-
ing as a short generic summary. Furthermore, the
first sentence of the article should contain the title
in subject position, which tends to create sentences
that define the main entity of the article.
For the elaborative summarisation scenario de-
scribed, we are interested in exploring ways in
which the reading context can be leveraged to pro-
duce the elaborative summary. One method ex-
plored in this paper attempts to map the content of
the linked document into the semantic space of the
reading context, as defined in vector-space. We use
Singular Value Decomposition (SVD), the underly-
ing method behind Latent Semantic Analysis (Deer-
wester et al, 1990), as a means of identifying latent
topics in the reading context, against which we com-
pare the linked document. We present our system
and the results from our preliminary investigation in
the remainder of this paper.
3http://www.ict.csiro.au/staff/stephen.wan/ibes/
2 Related Work
Using link text for summarisation has been explored
previously by Amitay and Paris (2000). They identi-
fied situations when it was possible to generate sum-
maries of web-pages by recycling human-authored
descriptions of links from anchor text. In our work,
we use the anchor text as the reading context to pro-
vide an elaborative summary for the linked docu-
ment.
Our work is similar in domain to that of the 2007
CLEF WiQA shared task.4 However, in contrast to
our application scenario, the end goal of the shared
task focuses on suggesting editing updates for a
particular document and not on elaborating on the
user?s reading context.
A related task was explored at the Document Un-
derstanding Conference (DUC) in 2007.5 Here the
goal was to find new information with respect to a
previously seen set of documents. This is similar to
the elaborative goal of our summary in the sense that
one could answer the question: ?What else can I say
about topic X (that hasn?t already been mentioned
in the reading context)?. However, whereas DUC
focused on unlinked news wire text, we explore a
different genre of text.
3 Algorithm
Our approach is designed to select justification sen-
tences and expand upon them by finding elaborative
material. The first stage identifies those sentences
in the linked document that support the semantic
content of the anchor text. We call those sentences
justification material. The second stage finds mate-
rial that is supplementary yet relevant for the user.
In this paper, we report on the first of these tasks,
though ultimately both are required for elaborative
summaries.
To locate justification material, we implemented
two known summarisation techniques. The first
compares word overlap between the anchor text and
the linked document. The second approach attempts
to discover a semantic space, as defined by the read-
ing context. The linked document is then mapped
into this semantic space. These are referred to as the
Simple Link method and the SVD method, where
4http://ilps.science.uva.nl/WiQA/
5http://duc.nist.gov/guidelines/2007.html
130
the latter divides further into two variants: SVD-
Link and SVD-topic.
3.1 Simple Link Method
The first strategy, Simple Link, makes use of stan-
dard vector space approaches from Information Re-
trieval. A vector of word frequencies, omitting stop-
words, is used to represent each sentence in the read-
ing context and in the linked document. The vec-
tor for the anchor sentence is compared with vectors
for each linked document sentence, using the cosine
similarity metric. The highest scoring sentences are
then retrieved as the summary.
3.2 Two Singular Value Decomposition (SVD)
Methods
In these approaches, the semantic space of the linked
document is mapped into that of the reading context.
Intuitively, only those sentences that map well into
the reading context space and are similar to the link-
ing sentence would be good justification material.
To begin with, the reading context document is
represented as a term-by-sentence matrix, A, where
stop words are omitted and frequencies are weighted
using inverse document frequency. A Singular Value
Decomposition (SVD) analysis is performed (using
the JAMA package6) on this matrix which provides
three resulting matrices: A = USV tr .
The S-matrix defines the themes of the reading
context. The U-matrix relates the reading context
vocabulary to the discovered themes. Finally, the
V-matrix relates the original sentences to each of the
themes. The point of the SVD analysis is to discover
these themes based on co-variance between the word
frequencies. If words occur together, they are se-
mantically related and the co-variance is marked as
a theme, allowing one to capture fuzzy matches be-
tween related words. Crucially, each sentence can
now be represented with a vector of membership
scores to each theme.
The first of the semantic space mapping methods,
SVD-link, finds the theme that the anchor text be-
longs to best. This is done by consulting the V-
matrix of the SVD analysis to find the highest scor-
ing theme for that sentence, which we call the link-
ing theme. Each sentence in the linked document,
6http://math.nist.gov/javanumerics/jama/
after mapping it to the SVD-derived vector space, is
then examined. The highest scoring sentences that
belong to the linking theme are then extracted.
The second method, SVD-topic, makes a differ-
ent assumption about the nature of the reading con-
text. Instead of taking the anchor text as an indicator
of the user?s information need, it assumes that the
top n themes of the reading context document rep-
resent the user?s interest. Of the linked document
sentences, for each of those top n reading context
themes, the best scoring sentence is extracted.
4 Evaluation
In lieu of a user-centered experiment, our prelimi-
nary experiments evaluated the effectiveness of the
tool in terms of finding justification material for an
elaborative summary. We evaluated the three sys-
tems described in Section 3. Each system selected
5 sentences. We tested against two baselines. The
first simply returns the first 5 sentences. The second
produces a generic summary based on Gong and Liu
(2001), independently of the reading context.
4.1 Data
The data used is a collection of Wikipedia articles
obtained automatically from the web. The snap-
shot of the corpus was collected in 2007. Of these,
links from about 600 randomly chosen documents
were filtered with a heuristic that enforced a sen-
tence length of at least 10 words such that the link in
the anchor text occurred after this minimum length.
This heuristic was used as an approximate means
of filtering out sentences where the linking sentence
was simply a definition of the entity linked. In these
cases, the justification material is usually trivially
identified as the first sentence of the linked docu-
ment. This leaves us with links that potentially re-
quire more complicated summarisation methods.
Of these cases, 125 cases were randomly selected
and the linked documents annotated for varying de-
grees of relevancy. This resulted in 50 relevant doc-
ument links, which we further annotated, selecting
sentences supporting the anchor sentence, with a
Cohen?s Kappa of 0.55. The intersection of the se-
lected sentences was then used as a gold standard for
each test case.
131
System Recall Precision
generic 0.13 0.05
SVD-topic 0.14 0.06
SVD-link 0.22 0.09
simple-link 0.28 0.11
Table 1: Recall and Precision figures for all summarisers
without the first 5 sentences.
4.2 Results
It is difficult to beat the first-5 baseline, which attains
the best recall of 0.52 and a precision of 0.2, with all
other strategies falling behind. However, we believe
that this may be due to the presence of some types
of Wikipedia articles that are narrow in scope and
centered on specific events. For such articles, we
would naturally advocate using the first N sentences
as a summary.
To examine the performance of the summarisa-
tion strategies on sentences beyond the top-N , we
filtered the gold standard sets to remove sentences
occurring in positions 1-5 in the linked document,
and tested recall and precision on the remaining
sentences. This reduces our test set by 10 cases.
Since documents may be lengthy (more than 100
sentences), selecting justification material is a dif-
ficult task. The results are shown in Table 1 and in-
dicate that systems using reading context do better
than a generic summariser.
Thinking ahead to the second expansion step in
which we find elaborative material, good candidates
for such sentences may be found in the immedi-
ate vicinity of justification sentences. If so, near
matches for justification sentences may still be use-
ful in indicating that, at least, the right portion of
the document was identified. Thus, to test for near
matches, we scored a match if the gold sentence
occurred on either side of the system-selected sen-
tence. We refer to this as the neighbourhood heuris-
tic.
Table 2 shows the effect on recall and preci-
sion if we treat each selected sentence as defining a
neighbourhood of relevance in the linked document.
Again, performance on the first 5 sentences were ig-
nored. Recall improved by up to 10% with only a
small drop in precision (6%). When the neighbour-
hood heuristic is run on the original gold sentence
System Recall Precision
generic 0.27 0.04
SVD-topic 0.27 0.04
SVD-link 0.30 0.05
simple-link 0.38 0.06
Table 2: Recall and Precision figures using the neigh-
bourhood heuristic (without the first 5 sentences).
set (with the first 5 sentences), recall reaches 0.57,
which lies above an amended 0.55 baseline.
5 Future Work and Conclusions
We introduced the concept of a user-biased elabo-
rative summarisation, using the reading context as
an indicator of the information need. Our paper
presents a scenario in which elaborative summari-
sation may be useful and explored simple summari-
sation strategies to perform this role. Results are
encouraging and our preliminary evaluation shows
that reading context is helpful, achieving a recall
of 57% when identifying sentences that justify con-
tent in the linking sentence of the reading context.
In future work, we intend to explore other latent
topic methods to improve recall and precision per-
formance. Further development of elaborative sum-
marisation strategies and a user-centered evaluation
are also planned.
References
Einat Amitay and Ce?cile Paris. 2000. Automatically
summarising web sites: is there a way around it? In
Proceedings of the 9th international conference on In-
formation and knowledge management, NY, USA.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Yihong Gong and Xin Liu. 2001. Generic text summa-
rization using relevance measure and latent semantic
analysis. In Proceedings of the 24th ACM SIGIR con-
ference. New Orleans, USA.
Karen Spark Jones. 1998. Automatic summarizing:
factors and directions. In I. Mani and M. May-
bury (ed.), Advances in Automatic Text Summarisa-
tion. MIT Press, Cambridge MA.
132
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 543?552,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Seed and Grow: Augmenting Statistically Generated Summary Sentences
using Schematic Word Patterns
Stephen Wan?? Robert Dale? Mark Dras?
?Centre for Language Technology
Department of Computing
Macquarie University
Sydney, NSW 2113
swan,madras,rdale@ics.mq.edu.au
Ce?cile Paris?
?ICT Centre
CSIRO
Sydney, Australia
Cecile.Paris@csiro.au
Abstract
We examine the problem of content selection
in statistical novel sentence generation. Our
approach models the processes performed by
professional editors when incorporating ma-
terial from additional sentences to support
some initially chosen key summary sentence,
a process we refer to as Sentence Augmen-
tation. We propose and evaluate a method
called ?Seed and Grow? for selecting such
auxiliary information. Additionally, we argue
that this can be performed using schemata, as
represented by word-pair co-occurrences, and
demonstrate its use in statistical summary sen-
tence generation. Evaluation results are sup-
portive, indicating that a schemata model sig-
nificantly improves over the baseline.
1 Introduction
In the context of automatic text summarisation, we
examine the problem of statistical novel sentence
generation, with the aim of moving from the current
state-of-the-art of sentence extraction to abstract-
like summaries. In particular, we focus on the task of
selecting content to include within a generated sen-
tence.
Our approach to novel sentence generation is to
model the processes underlying summarisation as
performed by professional editors and abstractors.
An example of the target output of this kind of gen-
eration is presented in Figure 1. In this example, the
human authored summary sentence was taken verba-
tim from the executive summary of a United Nations
proposal for the provision of aid addressing a partic-
ular humanitarian crisis. Such documents typically
exceed a hundred pages.
Human-Authored Summary Sentence:
Repeated [poor seasonal rains]1 [in 2004]2, culminating
in [food insecurity]3, indicate [another year]4 of crisis,
the scale of which is larger than last year?s and is further
[exacerbated by diminishing coping assets]5 [in both
rural and urban areas]6.
Key Source Sentence:
The consequences of [another year]4 of [poor rains]1 on
[food security]3 are severe.
Auxiliary Source Sentence(s):
However in addition to the needs of economic recovery
activities for IDPs, [food insecurity]3 [over the major-
ity of 2004]2 [has created great stress]5 on the poorest
families in the country, [both within the urban and rural
settings]6.
Figure 1: Alignment of a summary sentence to sentences
in the full document. Phrases of similar meaning are co-
indexed.
To write such summaries, we assume that the hu-
man abstractor begins by choosing key sentences
from the full document. Then, for each key sen-
tence, a set of auxiliary material is identified. The
key sentence is revised incorporating these auxil-
iary sentences to produce the eventual summary sen-
tence.
To study this phenomenon, a corpus of UN docu-
ments was collected and analysed.1 Each document
was divided into two parts comprising its executive
summary, and the remainder, referred to here as the
source. We manually aligned each executive sum-
mary sentence with one or more sentences from the
source, by choosing a key sentence that provided
1This corpus is described in detail in Section 5.1.
543
evidence for the content of the summary sentence
along with additional sentences that provided sup-
porting material.
We refer to the resulting corpus as the UN Con-
solidated Appeals Process (UN CAP) corpus. It is
a collection of sentence alignments, each referred to
as an aligned sentence tuple, which consists of:
1. A human authored summary sentence from the
executive summary;
2. A key sentence from the source;
3. Zero or more auxiliary sentences from the
source.
The key and any auxiliary sentences are referred to
collectively as the aligned source sentences.
We argue that some process that combines infor-
mation from multiple sentences is required if we are
to generate summary sentences similar to that por-
trayed in Figure 1. This is supported by our analysis
of the UN CAP corpus. Of the 580 aligned sentence
tuples, the majority, 61% of cases, appear to be ex-
amples of such a process.
Furthermore, the auxiliary sentences are clearly
necessary. We found that only 30% of the open-class
words in the summary are found in the key sentence.
If one selects all the open-class words from aligned
source sentences, recall increases to an upper limit
of 45% without yet accounting for stemming. This
upper bound is consistent with the upper limit of
50% found by Daume? III and Marcu (2005) which
takes into account stemming differences.
This demonstrates that the auxiliary material is
a valuable source of content which should be inte-
grated into the summary sentence, allowing an im-
provement in recall of up to 15% prior to account-
ing for morphological, synonym and paraphrase dif-
ferences. Of course, the trick is to improve recall
without hurting precision. A naive addition of all
words in the aligned source sentences incurs a drop
in precision from 30% to 23%. The problem thus is
one of selecting the relevant auxiliary content words
without introducing unimportant content. We refer
to this problem of incorporating material from aux-
iliary sentences to supplement a key sentence as Sen-
tence Augmentation.
In this paper, sentence augmentation is modelled
as a noisy channel process and has two facets: con-
tent selection and language modelling. This paper
focuses on the former, in which the system must
rank text segments?in this case, words?for inclu-
sion in the generated sentence. Given a ranked se-
lection of words, a language model would then order
them appropriately, as described in work on sentence
regeneration (for example, see Soricut and Marcu
(2005); Wan et al (2005)).
Provided with an aligned sentence tuple, the prob-
lem lies in effectively selecting words from the aux-
iliary sentences to bolster those taken from the key
sentence. Given that there are on average 2.7 aux-
iliary sentences per aligned sentence tuple, this ad-
ditional influx of words poses a considerable chal-
lenge.
We begin with the premise that, for documents
of a homogeneous type (in this case, the genre is
a funding proposal, and the domain is humanitarian
aid), it may be possible to identify patterns in the or-
ganisation of information in summaries. For exam-
ple, Figure 2 presents three summary sentences from
our corpus that share the same patterned juxtapo-
sition of two concepts DisplacedPersons and Host-
ingCommunities. Documents may exhibit common
patterns since they have a similar goal: namely, to
convince donors to give financial support. In the
above example, the juxtaposition highlights the fact
that those in need are not just those people from the
?epicenter? of the crisis but also those that look after
them.
We propose and evaluate a method called ?Seed
and Grow? for selecting content from auxiliary sen-
tences. That is, we first select the core meaning of
the summary, given here by the key sentence, and
then we find those pieces of additional information
that are conventionally juxtaposed with it.
Such patterns are reminiscent of Schemata, the or-
ganisations of propositional content introduced by
McKeown (1985). Schemata typically involve a
symbolic representation of each proposition?s se-
mantics. However, in our case, a text-to-text gener-
ation scenario, we are without such representations
and so must find other means to encode these pat-
terns.
To alleviate the situation, we turn to word-pair co-
occurrences to approximate schematic patterns. Fig-
544
Sentence 1:
The increased number of [internally displaced persons]1
and the continued presence of refugees have fur-
ther strained the scarce natural resources of [host
communities]2, stretching their capacity to the limit.
Sentence 2:
100,000 people, a significant portion of the population,
remain [displaced]1, burdening the already precarious
living conditions of [host families]2 in Dili and the
Districts.
Sentence 3:
The current humanitarian situation in Timor-Leste is
characterised by: An estimated [100,000 displaced
people]1 (10% of the population) living in camps and
with [host families]2 in the districts; A total or partial de-
struction of over 3,000 homes in Dili affecting at least
14,000 IDPs
Figure 2: Examples of the pattern ?DisplacedPersons[1],
HostingCommunities[2]?.
ure 2 showed that mentions of the plight of interna-
tionally displaced persons are often followed by de-
scriptions of the impact on the host communities that
look after them. In this particular example, this is
realised lexically in the co-occurrences of the words
displaced and host.
Corpus-based methods inspired by the notion of
schemata have been explored in the past by Lap-
ata (2003) and Barzilay and Lee (2004) for order-
ing sentences extracted in a multi-document sum-
marisation application. However, to our knowledge,
using word co-occurrences in this manner to repre-
sent schematic knowledge for the purposes of select-
ing content in a statistically-generated summary sen-
tence has not previously been explored.
This paper seeks to determine whether or not such
patterns exist in homogeneous data; and further-
more, whether such patterns can be used to better
select words from auxiliary sentences. In particular,
we propose the ?Seed and Grow? approach for this
task. The results show that even simple modelling
approaches are able to model this schematic infor-
mation.
In the remainder of this paper, we contrast our ap-
proach to related text-to-text research in Section 2.
The Content Selection model is presented in Section
3. Section 4 describes how a binary classification
model is used in a statistical text generation system.
Section 5 describes our evaluation of the model for a
summary generation task. We conclude, in Section
6, that domain-specific schematic patterns can be ac-
quired and applied to content selection for statistical
sentence generation.
2 Related Work
2.1 Content Selection in Text-to-Text Systems
Statistical text-to-text summarisation applications
have borrowed much from the related field of statis-
tical machine translation. In one of the first works to
present summarisation as a noisy channel approach,
Witbrock and Mittal (1999) presented a conditional
model for learning the suitability of words from a
news article for inclusion in headlines, or ?ultra-
summaries?. Inspired by this approach, and with
the intention of designing a robust statistical gener-
ation system, our work is also based on the noisy
channel model. Into this, we incorporate our con-
tent selection model, which includes Witbrock and
Mittal?s model supplemented with schema-based in-
formation.
Roughly, text-to-text transformations fall into
three categories: those in which information is com-
pressed, conserved, and augmented. We use these
distinctions to organise this overview of the litera-
ture.
In Sentence Compression work, a single sentence
undergoes pruning to shorten its length. Previ-
ous approaches have focused on statistical syntactic
transformations (Knight and Marcu, 2002). For con-
tent selection, discourse-level considerations were
proposed by Daume? III and Marcu (2002), who ex-
plored the use of Rhetorical Structure Theory (Mann
and Thompson, 1988). More recently, Clarke and
Lapata (2007) use Centering Theory (Grosz et al,
1995) and Lexical Chains (Morris and Hirst, 1991)
to identify which information to prune. Our work is
similar in incorporating discourse-level phenomena
for content selection. However, we look at schema-
like information as opposed to chains of references
and focus on the sentence augmentation task.
The work of Barzilay and McKeown (2005) on
Sentence Fusion introduced the problem of convert-
ing multiple sentences into a single summary sen-
545
tence. Each sentence set ideally tightly clusters
around a single news event. Thus, there is one gen-
eral proposition to be realised in the summary sen-
tence, identified by finding the common elements in
the input sentences. We see this as an example of
conservation. In our work, this general proposition
is equivalent to the core information for the sum-
mary sentence before the incorporation of supple-
mentary material.
In contrast to both compression and conservation
work, we focus on augmenting the information in
a key sentence. The closest work is that of Jing
and McKeown (1999) and Daume? III and Marcu
(2005), in which multiple sentences are processed,
with fragments within them being recycled to gener-
ate the novel generated text.
In both works, recyclable fragments are identified
by automatic means. Jing and McKeown (1999) use
models that are based on ?copy-and-paste? opera-
tions learnt from the behaviour of human abstrac-
tors as found in a corpus. Daume? III and Marcu
(2005) propose a model that encodes how likely it
is that different sized spans of text are skipped to
reach words and phrases to recycle.
While similar in task, our models differ substan-
tially in the nature of the phenomenon modelled. In
this work, we focus on content-based considerations
that model which words can be combined to build
up a new sentence.
2.2 Schemata and Text Generation
There exists related work from Natural Language
Generation (NLG) in finding material to build up
sentences. As mentioned above, our content selec-
tion model is inspired by work on schemata from
NLG (McKeown, 1985). Barzilay and Lee (2004)
showed that it is possible to obtain schema-like
knowledge automatically from a corpus for the pur-
poses of extracting sentences and ordering them.
However, their work represents patterns at the sen-
tence level, and is thus not directly comparable to
our work, given our focus on sentence generation.
In our system, what is required is a means to rank
words for use in generation. Thus, we focus on com-
monly occurring word co-occurrences, with the aim
of encoding conventions in the texts we are trying to
generate. In this respect, this is similar to work by
Lapata (2003), who builds a conditional model of
words across adjacent sentences, focusing on words
in particular semantic roles. Like Barzilay and Lee
(2004), this model was used to order extracted sen-
tences in summaries. In contrast, our work focuses
on word patterns found within a summary sentence,
not between sentences. Additionally, our tasks dif-
fer as we examine the statistical sentence generation
instead of sentence ordering.
3 Linguistic Intuitions behind Word
Selection
The ?Seed and Grow? approach proposed in this pa-
per divides the word-level content selection prob-
lem into two underlying subproblems. We address
these with two separate models, called the salience
and schematic models. The salience model chooses
the key content for the summary sentence while the
schematic model attempts to identify what else is
typically mentioned given those salient pieces of in-
formation.
3.1 A Salience Model: Learning ?Buzzwords?
There are a variety of methods for determining the
salient information in a text, and these underpin
most work in automatic text summarisation. As an
example of a salience model trained on corpus data,
Witbrock and Mittal (1999) introduced a method for
scoring summary words for inclusion within news
headlines. In their model, headlines were treated as
?ultra-summaries?. Their model learns which words
are typically used in headlines and encodes, at least
to some degree, which words are attention grabbing.
In the domain of funding proposals, key words
that grab attention may amount to domain-specific
buzzwords. Intuitively, a reader, perhaps someone
in charge of allocating donations, tends to look for
certain types of key information matching donation
criteria, and so human abstract authors will target
their summaries for this purpose.
We thus adapt the Witbrock and Mittal (1999)
model to identify such domain specific buzzwords
(BWM, for ?buzzword model?). For an aligned sen-
tence tuple, the probability that a word is selected
based on the salience of a word with respect to the
domain is defined as:
probbwm(select = 1|w) =
|summaryw|
|sourcew|
(1)
546
where summaryw is the set of aligned sentence tu-
ples that contain the word w in the summary sen-
tence and in the source sentences. The denomina-
tor, sourcew, is the set of aligned sentence tuples that
have the word w in either the key or an auxiliary sen-
tence.
As is implicit in this equation, we could just use
this buzzword model to select content not only from
the key sentence, but from the auxiliary sentences
as well. While it is intended ultimately to find the
key content of the summary, it can also serve as an
alternative baseline for auxiliary content selection to
compare against the ?Seed and Grow? model.
3.2 A Schema Model: Approximation via
Word co-Occurrences
To restate the problem at hand: the task is one
of finding elements of secondary importance that
schematically elaborate on the key information. We
do this by examining sample summary sentences for
conventional juxtapositions of concepts. As men-
tioned in Section 1, schemata are approximated here
with patterns of word-pair co-occurrences. Using a
corpus of human-authored summaries in the domain
of our application, it is thus possible to learn what
those common combinations of words are.
Roughly, the process is as follows. To begin with,
a seed set of words is chosen. The purpose of the
seed set is to represent the core proposition of the
summary sentence.
In this work, this core proposition is given by the
key sentence and so the non-stopwords belonging to
it are used to populate the seed set. In the ?Seed and
Grow? approach, we check to see which words from
auxiliary sentences pair well with words in the seed
set.
3.2.1 Collecting Word-level Patterns
Each training case in the corpus contains a single
human-authored summary sentence that can be used
to learn which pairs of words conventionally occur
in a summary. For each summary sentence, stop-
words are removed. Then, each pairing of words in
the sentence is used to update a pair-wise word co-
occurrence frequency table. When looking up and
storing a frequency, the order of words is ignored.
3.2.2 Scoring Word-Pair Co-occurrence
Strength
For any two words, w1 from the seed set and w2 from
an auxiliary sentence, the word-pair co-occurrence
probability is defined as follows:
probco-oc(w1,w2)
= freq(w1,w2)
freq(w1)+ freq(w2)? freq(w1,w2)
(2)
where f req(w1,w2) is a lookup in the word-pair co-
occurrence frequency table. This table stores co-
occurrence word pairs occurring in the summary
sentence.
3.2.3 Combining a Set of Co-occurrence Scores
Each auxiliary word now has a series of scores,
one for each comparison with a seed word. To rank
each auxiliary word, these need to be combined into
a single score for sorting.
When combining the set of co-occurrence scores,
one might want to account for the fact that each pair-
ing of a seed word with an auxiliary word might
not contribute equally to the overall selection of that
auxiliary word. Intuitively, a word in the seed set,
derived from the key sentence, may only make a
minor contribution to the core meaning of the sum-
mary sentence. For example, words that are part of
an adjunct phrase in the key sentence might not be
good candidates to elaborate upon. Thus, one might
want to weight these seed words lower, to reduce
their influence on triggering schematically associ-
ated words.
To allow for this, a seed weight vector is main-
tained, storing a weight per seed word. Different
weighting schemes are possible. For example, a
scheme might indicate the salience of a word. In
addition to the buzzword model (BWM) described
earlier, one might employ a standard vector space
approach (Salton and McGill, 1983) from Informa-
tion Retrieval, which uses term frequency scores
weighted with an inverse document frequency fac-
tor, or tf-idf. We also implement the case in which all
seed words are treated equally using binary weights,
where 1 indicates the presence of a seed word, and
0 indicates its absence. In the evaluations described
in Section 5, we refer to these three seed weighting
schemes as bwm and tf-idf, and binary respectively.
547
To find the probability of selecting an auxiliary
word using the schematic word-pair co-occurrence
model (WCM), an averaged probability is found
by normalising the sum of the weighted probabili-
ties, where weights are provided by one of the three
schemes above:
probwcm(wi) =
1
Z
?
|seed|
?
k=0
weightsk ?probco-oc(wi,wk) (3)
where seed is the set of seed words and wk is the kth
word in that set. The vector, weights, stores the seed
weights. The normalisation factor for the weighted
average, Z, is the number of auxiliary words.
Finally, since the WCM model only serves to se-
lect words from the auxiliary sentences, words from
the key sentence must be given scores as well. For
these words, the scoring is as follows:
probwcm(w) =
1
Z
(
1
|seed| + probwcm(w)
)
(4)
where Z is a normalisation across the set of seed
words.
4 Combining Buzzwords and Word-Pair
Co-Occurrence Models for Generation
As mentioned above, the noisy channel approach
is used for producing the augmented sentence. Al-
though the focus of this paper is on Content Selec-
tion, an overview of the end-to-end generation pro-
cess is presented for completeness.
Sentence augmentation is essentially a text-to-text
process: A key sentence and auxiliary material are
transformed into a single summary sentence. Fol-
lowing Witbrock and Mittal (1999), the task is to
search for the string of words that maximises the
probability prob(summary|source). Standardly re-
formulating this probability using Bayes? rule re-
sults in the following:
probcm(source|summary)?problm(summary) (5)
In this paper, we are concerned with the first
factor, probcm(source|summary), referred to as the
channel model (CM), which combines both the
buzzword (BWM) and word-pair co-occurrence
(WCM) models. An examination of differences be-
tween the two approaches revealed only a 20% word
overlap on the Jaccard metric.
In order to combine multiple models, we intend
to use machine learning approaches to combine the
information in each model in a similar manner to
Berger et al (1996). We are currently exploring the
use of logistic regression methods to learn a func-
tion that would treat, as features, the probabilities
defined by the salience and schematic content selec-
tion models. Although generation is possible using
each content selection model in isolation, evalua-
tions of the combined model are on-going and are
not presented in this paper.
5 Evaluation
In this evaluation, the task is to select n words from
the aligned source sentences for inclusion in a sum-
mary. As a gold-standard for comparison, we sim-
ply examine what words were actually chosen in the
summary sentence of the aligned sentence tuple. We
are specifically interested in open-class words, and
so a stopword list of closed-class words is used to
filter the sentences in each test case.
We evaluate against the set of open-class words
in the human-authored summary sentence using re-
call and precision metrics. Recall is the size of
the intersection of the selected and gold-standard
sets, normalised by the length of the gold-standard
sentence (in words). This recall metric is similar
to the ROUGE-1 metric, the unigram version of
the ROUGE metric (Lin and Hovy, 2003) used in
the Document Understanding Conferences2 (DUC).
Precision is the size of the intersection normalised
by the number of words selected. We also report the
F-measure, which is the harmonic mean of the recall
and precision scores.
Recall, precision and F-measure are measured at
various values of n ranging from 1 to the number of
open-class words in the gold-standard summary sen-
tence for a particular test case. For the purposes of
evaluation, differences in tokens due to morphology
were explored crudely via the use of Porter?s stem-
ming algorithm. However, the results from stem-
ming are not that different from exact token matches
when examining performance on the entire data set
2http://duc.nist.gov
548
Number of training cases 530
Average words in summary sentence 27.0
Average stopwords in summary sentence 10.3
Average number of auxiliary sentences 2.75
Word count: summary sentences 4630
Word count: source sentences 21356
Word type count in corpus 3800
Table 1: Statistics for the UN CAP training set
and so, for simplicity, these are omitted in this dis-
cussion.
5.1 The Data
The corpus is made up of a number of humanitar-
ian aid proposals called Consolidated Appeals Pro-
cess (UN CAP) documents, which are archived at
the United Nations website.3 135 documents from
the period 2002 to 2007 were downloaded by the au-
thors. A preprocessing stage extracted text from the
PDF files and segmented the documents into execu-
tive summary and source sections. These were then
automatically segmented further into sentences.
Executive summary sentences were manually
aligned by the authors to source key and auxiliary
sentences, producing a corpus of 580 aligned sen-
tence tuples referred to here as the UN CAP cor-
pus. Of these, 230 tuples were paraphrase cases (i.e.
without aligned auxiliary sentences). The remaining
550 cases were instances of sentence augmentation
(with at least one auxiliary sentence).
Of the 580 cases, 50 cases were set aside for test-
ing. The remaining 530 cases were used for train-
ing. Statistics for the training portion of the sentence
augmentation set are provided in Table 1.
In this paper, aligned sentence tuples are obtained
via manual annotation. Automatic construction
of these sentence-level alignments is possible and
has been explored by Jing and McKeown (1999).
We also envisage using tools for scoring sentence
similarity (for example, see Hatzivassiloglou et al
(2001)) for automatically constructing them; this is
the focus of work by Wan and Paris (2008).
3http://ochaonline3.un.org/humanitarianappeal/index.htm
5.2 The Baselines
Three baselines were used in this work: the random,
tf-idf and position baselines. A random word selec-
tor shows what performance might be achieved in
the absence of any linguistic knowledge.
We also sorted all words in the aligned source sen-
tences by their weighted tf-idf scores. This baseline
selects words in order until the desired word limit
is reached. This baseline is referred to as the tf-idf
baseline.
Finally, we selected words based on their sen-
tence order, choosing first those words from the key
sentence. When these are exhausted, auxiliary sen-
tences are sorted by their sentence positions in the
original document. Words from the first auxiliary
sentence are then chosen. This continues until ei-
ther the desired number of words have been chosen,
or no words remain. This baseline is known as the
position baseline.
5.3 Content Selection Results
We compare the three baselines to the two mod-
els presented in Section 3. These are the buzzword
salience model (BWM) and the schematic word-pair
co-occurrence model (WCM).
We begin by presenting recall, precision and F-
measure graphs when selecting from the aligned
source sentences, comprising the key and auxiliary
sentences. Figure 3 shows the results for the two
models against the three baselines. The two mod-
els, the positional, and the tf-idf baselines perform
better than the random baseline, as measured by a
two-tailed Wilcoxon Matched Pairs Signed Ranks
test (? = 0.05).
The WCM consistently out-performs the BWM
on all metrics, and the differences are statistically
significant. In fact, the BWM also generally per-
forms worse than the position and tf-idf baselines.
WCM and the position baseline both significantly
outperform the tf-idf baseline on all metrics for
longer sentence lengths.
That the position baseline and WCM should per-
form similarly is not really surprising since, in ef-
fect, the position baseline first chooses words from
the key sentence and then selects auxiliary words.
The difference essentially lies in how the auxiliary
words are chosen.
549
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0  5  10  15  20  25  30
R
ec
al
l
Number of Open-class Words Selected
WCM
BWM
Position
tf.idf
Random
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0  5  10  15  20  25  30
Pr
ec
is
io
n
Number of Open-class Words Selected
WCM
BWM
Position
tf.idf
Random
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0  5  10  15  20  25  30
Fm
ea
su
re
Number of Open-class Words Selected
WCM
BWM
Position
tf.idf
Random
Figure 3: Recall, Precision and F-measure performance
for open-class words from the entire input set (key and
auxiliary). Models presented are the Buzzword Model
(BWM), the Word-Pair Co-occurrence Model (WCM)
and position, tf-idf and random baselines.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0  5  10  15  20  25  30
Fm
ea
su
re
Number of Open-class Words Selected
WCM
position
Figure 4: F-measure scores for content selection on just
the auxiliary sentences. Models presented are the Word-
Pair Co-occurrence model (WCM) and the position base-
line.
The results of Figure 3 weakly support the
hypothesis that using schematic word-pair co-
occurrences helps improve performance over mod-
els without discourse-related features. The graphs
show that WCM edges above the position base-
line when the number of selected open-class words
ranges from 10 to 15. Note that the average num-
ber of open-class words in a human authored sum-
mary sentence is 16. The only significant difference
found was in the F-measure and precision scores for
19 selected open-class words. Nevertheless, a gen-
eral trend can be observed in which WCM performs
better than the position baseline.
Ultimately, however, what we want to do is select
auxiliary content to supplement the key sentence.
To examine the effect of two best performing ap-
proaches, WCM and the position baseline, on this
task, were both modified so that the key sentence
words were explicitly given a zero probability. Thus,
the recall, precision and F-measure scores obtained
are based solely on the ability of either to select aux-
iliary words. The F-measure scores are presented
Figure 4. WCM consistently outperforms the po-
sition baseline for the selection of auxiliary words.
Differences are significant for 6 or more selected
open-class words.
The results show that even when considering only
exact token matches, we can improve on the re-
call of open-class words, and do so without penalty
in precision. Our working hypothesis is that such
gains are possible because the corpus has a homo-
550
geneous quality and key patterns are sufficiently re-
peated even when the overall data set is of the or-
der of hundreds of cases. The benefit of using a
model encoding some schematic information is fur-
ther shown by the performance of WCM over the
position baseline when selecting words from auxil-
iary sentences.
This is an interesting finding given that do-
main independent methods are increasingly used
on domain-specific corpora such as financial and
biomedical texts, for which we may have access to
only a limited amount of data. We anticipate that as
we introduce methods to account for paraphrase and
synonym differences, performance might rise fur-
ther still.
5.4 Testing Seed Weighting Schemes
We can also weight seed words in the ?Seed and
Grow? approach in a variety of ways. To test
whether weighting schemes have any effect on con-
tent selection performance, we examined the use
of three schemes. We were particularly interested
in those schemes that indicate the contribution of
a seed word to the core meaning of a sentence.
These are the binary, tf-idf and buzzword weight-
ing schemes described in Section 3. We present
the F-measure graph for these three variants of the
schematic word-pair co-occurrence model (WCM)
in Figure 5.
The graphs show that there is no discernible dif-
ference between the seed weighting schemes. No
scheme significantly outperforms another. Thus, we
conclude that the choice of these particular seed
weighting schemes has no effect on performance. In
future work, we intend to examine whether weight-
ing schemes encoding syntactic information might
fare better, since such information might more accu-
rately represent the contribution of a substring to the
main clause of the sentence.
6 Conclusions and Future Work
In this paper, we argued a case for sentence augmen-
tation, a component that facilitates abstract-like text
summarisation. We showed that such a process can
account for summary sentences as authored by pro-
fessional editors. We proposed the use of schemata,
as approximated with a word-pair co-occurrence
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0  5  10  15  20  25  30
Fm
ea
su
re
Number of Open-class Words Selected
binary
tfidf
BWM
Figure 5: F-measure performance for open-class words
from the entire input set (key and auxiliary). Models
presented are variants of the Word-Pair Co-occurrence
Model (WCM) that differ in the seed weighting schemes.
model, and advocated a new schema-based ?Seed
and Grow? content selection model used for statisti-
cal sentence generation.
We also showed that domain-specific patterns,
schematic word-pair co-occurrences in this case, can
be acquired from a limited amount of data as indi-
cated by modest performance gains for content se-
lection using schemata information. We postulate
that this is particularly true when dealing with ho-
mogeneous data.
In future work, we intend to explore other string
matches corresponding to variations due to para-
phrases and synonymy. We would also like to study
the effects of corpus size when learning schematic
patterns. Finally, we are currently investigating the
use of machine learning methods to combine the
best of the Salience and Schemata models in order
to provide a single model for use in decoding.
7 Acknowledgments
We would like to thank the reviewers for their in-
sightful comments. This work was funded by the
CSIRO ICT Centre and Centre for Language Tech-
nology at Macquarie University.
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Daniel Marcu Su-
san Dumais and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings, pages 113?120, Boston,
551
Massachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
Regina Barzilay and Kathleen R. McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297?328.
Adam L. Berger, Stephen Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
James Clarke and Mirella Lapata. 2007. Modelling com-
pression with discourse constraints. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
1?11.
Hal Daume? III and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL ? 2002), pages 449 ? 456,
Philadelphia, PA, July 6 ? 12.
Hal Daume? III and Daniel Marcu. 2005. Induction
of word and phrase alignments for automatic doc-
ument summarization. Computational Linguistics,
31(4):505?530, December.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
V. Hatzivassiloglou, J. Klavans, M. Holcombe, R. Barzi-
lay, M. Kan, and K. McKeown. 2001. Simfinder: A
flexible clustering tool for summarization. pages 41?
49. Association for Computational Linguistics.
Hongyan Jing and Kathleen McKeown. 1999. The de-
composition of human-written summary sentences. In
Research and Development in Information Retrieval,
pages 129?136.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139(1):91?107.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the 41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 545?552, Sapporo, Japan.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In NAACL ?03: Proceedings of the 2003 Confer-
ence of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 71?78, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
W. C. Mann and S. A. Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Kathleen R McKeown. 1985. Text Generation: Using
Discourse Strategies and Focus Constraints to Gen-
erate Natural Language Text. Cambridge University
Press.
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indicator
of the structure of text. Computational Linguistics,
17(1):21?48.
G. Salton and M. J. McGill. 1983. Introduction to mod-
ern information retrieval. McGraw-Hill, New York.
Radu Soricut and Daniel Marcu. 2005. Towards de-
veloping generation algorithms for text-to-text appli-
cations. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 66?74, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Stephen Wan and Ce?cile Paris. 2008. In-browser sum-
marisation: Generating elaborative summaries biased
towards the reading context. In Proceedings of ACL-
08: HLT, Short Papers, pages 129?132, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Stephen Wan, Robert Dale Mark Dras, and Ce?cile Paris.
2005. Towards statistical paraphrase generation: pre-
liminary evaluations of grammaticality. In Proceed-
ings of The 3rd International Workshop on Paraphras-
ing (IWP2005), pages 88?95, Jeju Island, South Korea.
Michael J. Witbrock and Vibhu O. Mittal. 1999. Ultra-
summarization (poster abstract): a statistical approach
to generating highly condensed non-extractive sum-
maries. In SIGIR ?99: Proceedings of the 22nd annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 315?316,
New York, NY, USA. ACM Press.
552
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 852?860,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Improving Grammaticality in Statistical Sentence Generation:
Introducing a Dependency Spanning Tree Algorithm with an Argument
Satisfaction Model
Stephen Wan?? Mark Dras? Robert Dale?
?Centre for Language Technology
Department of Computing
Macquarie University
Sydney, NSW 2113
swan,madras,rdale@ics.mq.edu.au
Ce?cile Paris?
?ICT Centre
CSIRO
Sydney, Australia
Cecile.Paris@csiro.au
Abstract
Abstract-like text summarisation requires
a means of producing novel summary sen-
tences. In order to improve the grammati-
cality of the generated sentence, we model
a global (sentence) level syntactic struc-
ture. We couch statistical sentence genera-
tion as a spanning tree problem in order to
search for the best dependency tree span-
ning a set of chosen words. We also intro-
duce a new search algorithm for this task
that models argument satisfaction to im-
prove the linguistic validity of the gener-
ated tree. We treat the allocation of modi-
fiers to heads as a weighted bipartite graph
matching (or assignment) problem, a well
studied problem in graph theory. Using
BLEU to measure performance on a string
regeneration task, we found an improve-
ment, illustrating the benefit of the span-
ning tree approach armed with an argu-
ment satisfaction model.
1 Introduction
Research in statistical novel sentence generation
has the potential to extend the current capabili-
ties of automatic text summarisation technology,
moving from sentence extraction to abstract-like
summarisation. In this paper, we describe a new
algorithm that improves upon the grammaticality
of statistically generated sentences, evaluated on a
string regeneration task, which was first proposed
as a surrogate for a grammaticality test by Ban-
galore et al (2000). In this task, a system must
regenerate the original sentence which has had its
word order scrambled.
As an evaluation task, string regeneration re-
flects the issues that challenge the sentence gen-
eration components of machine translation, para-
phrase generation, and summarisation systems
(Soricut and Marcu, 2005). Our research in sum-
marisation utilises the statistical generation algo-
rithms described in this paper to generate novel
summary sentences.
The goal of the string regeneration task is to re-
cover a sentence once its words have been ran-
domly ordered. Similarly, for a text-to-text gen-
eration scenario, the goal is to generate a sen-
tence given an unordered list of words, typically
using an n-gram language model to select the best
word ordering. N-gram language models appear
to do well at a local level when examining word
sequences smaller than n. However, beyond this
window size, the sequence is often ungrammati-
cal. This is not surprising as these methods are un-
able to model grammaticality at the sentence level,
unless the size of n is sufficiently large. In prac-
tice, the lack of sufficient training data means that
n is often smaller than the average sentence length.
Even if data exists, increasing the size of n corre-
sponds to a higher degree polynomial complexity
search for the best word sequence.
In response, we introduce an algorithm for
searching for the best word sequence in a way
that attempts to model grammaticality at the sen-
tence level. Mirroring the use of spanning tree al-
gorithms in parsing (McDonald et al, 2005), we
present an approach to statistical sentence genera-
tion. Given a set of scrambled words, the approach
searches for the most probable dependency tree, as
defined by some corpus, such that it contains each
word of the input set. The tree is then traversed to
obtain the final word ordering.
In particular, we present two spanning tree al-
gorithms. We first adapt the Chu-Liu-Edmonds
(CLE) algorithm (see Chu and Liu (1965) and Ed-
monds (1967)), used in McDonald et al (2005),
to include a basic argument model, added to keep
track of linear precedence between heads and
modifiers. While our adapted version of the CLE
algorithm finds an optimal spanning tree, this does
852
not always correspond with a linguistically valid
dependency tree, primarily because it does not at-
tempt to ensure that words in the tree have plausi-
ble numbers of arguments.
We propose an alternative dependency-
spanning tree algorithm which uses a more
fine-grained argument model representing argu-
ment positions. To find the best modifiers for
argument positions, we treat the attachment of
edges to the spanning tree as a weighted bipartite
graph matching problem (or the assignment
problem), a standard problem in graph theory.
The remainder of this paper is as follows. Sec-
tion 2 outlines the graph representation of the
spanning tree problem. We describe a standard
spanning tree algorithm in Section 3. Section 4 de-
fines a finer-grained argument model and presents
a new dependency spanning tree search algorithm.
We experiment to determine whether a global de-
pendency structure, as found by our algorithm,
improves performance on the string regeneration
problem, presenting results in Section 5. Related
work is presented in Section 6. Section 7 con-
cludes that an argument model improves the lin-
guistic plausibility of the generated trees, thus im-
proving grammaticality in text generation.
2 A Graph Representation of
Dependencies
In couching statistical generation as a spanning
tree problem, this work is the generation analog
of the parsing work by McDonald et al (2005).
Given a bag of words with no additional con-
straints, the aim is to produce a dependency tree
containing the given words. Informally, as all de-
pendency relations between each pair of words are
possible, the set of all possible dependencies can
be represented as a graph, as noted by McDon-
ald et al (2005). Our goal is to find the subset of
these edges corresponding to a tree with maximum
probability such that each vertex in the graph is
visited once, thus including each word once. The
resulting tree is a spanning tree, an acyclic graph
which spans all vertices. The best tree is the one
with an optimal overall score. We use negative log
probabilities so that edge weights will correspond
to costs. The overall score is the sum of the costs
of the edges in the spanning tree, which we want
to minimise. Hence, our problem is the minimum
spanning tree (MST) problem.
We define a directed graph (digraph) in a stan-
dard way, G = (V,E) where V is a set of vertices
and E ? {(u, v)|u, v ? V } is a set of directed
edges. For each sentence w = w1 . . . wn, we de-
fine the digraph Gw = (Vw, Ew) where Vw =
{w0, w1, . . . , wn}, with w0 a dummy root vertex,
and Ew = {(u, v)|u ? Vw, v ? Vw \ {w0}}.
The graph is fully connected (except for the root
vertex w0 which is only fully connected outwards)
and is a representation of possible dependencies.
For an edge (u, v), we refer to u as the head and v
as the modifier.
We extend the original formulation of McDon-
ald et al (2005) by adding a notion of argument
positions for a word, providing points to attach
modifiers. Adopting an approach similar to John-
son (2007), we look at the direction (left or right)
of the head with respect to the modifier; we con-
sequently define a set D = {l, r} to represent
this. Set D represents the linear precedence of the
words in the dependency relation; consequently,
it partially approximates the distinction between
syntactic roles like subject and object.
Each edge has a pair of associated weights, one
for each direction, defined by the function s :
E?D ? R, based on a probabilistic model of de-
pendency relations. To calculate the edge weights,
we adapt the definition of Collins (1996) to use di-
rection rather than relation type (represented in the
original as triples of non-terminals). Given a cor-
pus, for some edge e = (u, v) ? E and direction
d ? D, we calculate the edge weight as:
s((u, v), d) = ?log probdep(u, v, d) (1)
We define the set of part-of-speech (PoS) tags P
and a function pos : V ? P , which maps vertices
(representing words) to their PoS, to calculate the
probability of a dependency relation, defined as:
probdep(u, v, d)
= cnt((u, pos(u)), (v, pos(v)), d)
co-occurs((u, pos(u)), (v, pos(v))) (2)
where cnt((u, pos(u)), (v, pos(v)), d) is the num-
ber of times where (v, pos(v)) and (u, pos(u))
are seen in a sentence in the training data, and
(v, pos(v)) modifies (u, pos(u)) in direction d.
The function co-occurs((u, pos(u)), (v, pos(v)))
returns the number of times that (v, pos(v)) and
(u, pos(u)) are seen in a sentence in the training
data. We adopt the same smoothing strategy as
Collins (1996), which backs off to PoS for unseen
dependency events.
853
3 Generation via Spanning Trees
3.1 The Chu-Liu Edmonds Algorithm
Given the graph Gw = (Vw, Ew), the Chu-Liu
Edmonds (CLE) algorithm finds a rooted directed
spanning tree, specified by Tw, which is an acyclic
set of edges in Ew minimising
?
e?Tw,d?D s(e, d).
The algorithm is presented as Algorithm 1.1
There are two stages to the algorithm. The first
stage finds the best edge for each vertex, connect-
ing it to another vertex. To do so, all outgoing
edges of v, that is edges where v is a modifier, are
considered, and the one with the best edge weight
is chosen, where best is defined as the smallest
cost. This minimisation step is used to ensure that
each modifier has only one head.
If the chosen edges Tw produce a strongly con-
nected subgraph Gmw = (Vw, Tw), then this is the
MST. If not, a cycle amongst some subset of Vw
must be handled in the second stage. Essentially,
one edge in the cycle is removed to produce a sub-
tree. This is done by finding the best edge to join
some vertex in the cycle to the main tree. This has
the effect of finding an alternative head for some
word in the cycle. The edge to the original head
is discarded (to maintain one head per modifier),
turning the cycle into a subtree. When all cycles
have been handled, applying a greedy edge selec-
tion once more will then yield the MST.
3.2 Generating a Word Sequence
Once the tree has been generated, all that remains
is to obtain an ordering of words based upon it.
Because dependency relations in the tree are either
of leftward or rightward direction, it becomes rel-
atively trivial to order child vertices with respect
to a parent vertex. The only difficulty lies in find-
ing a relative ordering for the leftward (to the par-
ent) children, and similarly for the rightward (to
the parent) children.
We traverse Gmw using a greedy algorithm to or-
der the siblings using an n-gram language model.
Algorithm 2 describes the traversal in pseudo-
code. The generated sentence is obtained by call-
ing the algorithm with w0 and Tw as parameters.
The algorithm operates recursively if called on an
1Adapted from (McDonald et al, 2005) and
http://www.ce.rit.edu/? sjyeec/dmst.html . The dif-
ference concerns the direction of the edge and the edge
weight function. We have also folded the function ?contract?
in McDonald et al (2005) into the main algorithm. Again
following that work, we treat the function s as a data
structure permitting storage of updated edge weights.
/* initialisation */
Discard the edges exiting the w0 if any.1
/* Chu-Liu/Edmonds Algorithm */
begin2
Tw ? (u, v) ? E : ?v?V,d?Darg min
(u,v)
s((u, v), d)
3
if Mw = (Vw, Tw) has no cycles then return Mw4
forall C ? Tw : C is a cycle in Mw do5
(e, d)? arg min
e?,d?
s(e?, d?) : e ? C
6
forall c = (vh, vm, ) ? C and dc ? D do7
forall e? = (vi, vm) ? E and d? ? D do8
s(e?, d?)? s(e?, d?)? s(c, dc)? s(e, d)9
end10
end11
s(e, d)? s(e, d) + 112
end13
Tw ? (u, v) ? E : ?v?V,d?Darg min
(u,v)
s((u, v), d)
14
return Mw15
end16
Algorithm 1: The pseudo-code for the Chu-Liu
Edmonds algorithm with our adaptation to in-
clude linear precedence.
inner node. If a vertex v is a leaf in the dependency
tree, its string realisation realise(v) is returned.
We keep track of ordered siblings with two lists,
one for each direction. If the sibling set is left-
wards, the ordered list, Rl, is initialised to be the
singleton set containing a dummy start token with
an empty realisation. If the sibling set is right-
wards then the ordered list, Rr is initialised to be
the realisation of the parent.
For some sibling set C ? Vw to be ordered, the
algorithm chooses the next vertex, v ? C, to insert
into the appropriate ordered list, Rx, x ? D, by
maximising the probability of the string of words
that would result if the realisation, realise(v), were
concatenated with Rx.
The probability of the concatenation is calcu-
lated based on a window of words around the join.
This window length is defined to be 2?floor(n/2),
for some n, in this case, 4.
If the siblings are leftwards, the window con-
sists of the last min(n ? 1, |Rl|) previously cho-
sen words concatenated with the first min(n ?
1, |realise(v)|). If the siblings are rightwards, the
window consists of the last min(n?1, |realise(v)|)
previously chosen words concatenated with the
first min(n ? 1, |Rr|). The probability of a win-
dow of words, w0 . . . wj , of length j+1 is defined
by the following equation:
probLMO(w0 . . . wj)
=
j?k?1
?
i=0
probMLE(wi+k|wi . . . wi+k?1)
(3)
854
/* LMO Algorithm */
input : v, Tw where v ? Vw
output: R ? Vw
begin1
if isLeaf(v) then2
return {realise(v)}3
end4
else5
Cl ? getLeftChildren(v, Tw)6
Cr ? getRightChildren(v, Tw)7
Rl ? {start}8
Rr ? {realise(v)}9
while Cl 6= {} do10
c? arg max
c?Cl
probngram(LMO(c, Tw) ? Rl)11
Rl ? realise(c, Tw) ? Rl12
Cl ? Cl \ {c}13
end14
while Cr 6= {} do15
c? arg max
c?Cr
probngram(Rr ? LMO(c, Tw))16
Rr ? Rr ? realise(c, Tw)17
Cr ? Cr \ {c}18
end19
return Rl ? Rr20
end21
end22
Algorithm 2: The Language Model Ordering al-
gorithm for linearising an Tw.
where k = min(n? 1, j ? 1), and,
probMLE(wi+k|wi . . . wi+k?1)
= cnt(wi . . . wi+k)
cnt(wi . . . wi+k?1)
(4)
where probMLE(wi+k|wi . . . wi+k?1) is the max-
imum likelihood estimate n-gram probability. We
refer to this tree linearisation method as the Lan-
guage Model Ordering (LMO).
4 Using an Argument Satisfaction Model
4.1 Assigning Words to Argument Positions
One limitation of using the CLE algorithm for
generation is that the resulting tree, though max-
imal in probability, may not conform to basic lin-
guistic properties of a dependency tree. In partic-
ular, it may not have the correct number of argu-
ments for each head word. That is, a word may
have too few or too many modifiers.
To address this problem, we can take into ac-
count the argument position when assigning a
weight to an edge. When attaching an edge con-
necting a modifier to a head to the spanning tree,
we count how many modifiers the head already
has. An edge is penalised if it is improbable that
the head takes on yet another modifier, say in the
example of an attachment to a preposition whose
argument position has already been filled.
However, accounting for argument positions
makes an edge weight dynamic and dependent on
surrounding tree context. This makes the search
for an optimal tree an NP-hard problem (McDon-
ald and Satta, 2007) as all possible trees must be
considered to find an optimal solution.
Consequently, we must choose a heuristic
search algorithm for finding the locally optimum
spanning tree. By representing argument positions
that can be filled only once, we allow modifiers
to compete for argument positions and vice versa.
The CLE algorithm only considers this competi-
tion in one direction. In line 3 of Algorithm 1,
only heads compete for modifiers, and thus the so-
lution will be sub-optimal. In Wan et al (2007),
we showed that introducing a model of argument
positions into a greedy spanning tree algorithm
had little effect on performance. Thus, to consider
both directions of competition, we design a new
algorithm for constructing (dependency) spanning
trees that casts edge selection as a weighted bipar-
tite graph matching (or assignment) problem.
This problem is to find a weighted alignments
between objects of two distinct sets, where an ob-
ject from one set is uniquely aligned to some ob-
ject in the other set. The optimal alignment is one
where the sum of alignment costs is minimal. The
graph of all possible assignments is a weighted bi-
partite graph. Here, to discuss bipartite graphs, we
will extend our notation in a fairly standard way,
to write Gp = (U, V,Ep), where U, V are the dis-
joint sets of vertices and Ep the set of edges.
In our paper, we treat the assignment between
attachment positions and words as an assignment
problem. The standard polynomial-time solution
to the assignment problem is the Kuhn-Munkres
(or Hungarian) algorithm (Kuhn, 1955).2
4.2 A Dependency-Spanning Tree Algorithm
Our alternative dependency-spanning tree algo-
rithm, presented as Algorithm 3, incrementally
adds vertices to a growing spanning tree. At
each iteration, the Kuhn-Munkres method assigns
words that are as yet unattached to argument posi-
tions already available in the tree. We focus on the
bipartite graph in Section 4.3.
Let the sentence w have the dependency graph
Gw = (Vw, Ew). At some arbitrary iteration of the
algorithm (see Figure 1), we have the following:
? Tw ? Ew, the set of edges in the spanning
tree constructed so far;
2GPL code: http://sites.google.com/site/garybaker/
hungarian-algorithm/assignment
855
Partially determined spanning tree:
w0
made
john
? l0
? r1 cups
of
? l0
? l1
for
? l0
? l3
johnl0 mader1 ofl0 cupsl1 forl0 madel3
Hw1 Hw2 Hw3 Hw4 Hw5 Hw6
Mw1 Mw2 Mw3 Mw4 Mw5 Mw6
coffee everyone yesterday ?1 ?2 ?3
Figure 1: A snapshot of the generation process.
Each word in the tree has argument positions to
which we can assign remaining words. Padding
Mw with ? is described in Section 4.3.
? Hw = {u, v | (u, v) ? Tw}, the set of ver-
tices in Tw, or ?attached vertices?, and there-
fore potential heads; and
? Mw = Vw\Hw, the set of ?unattached ver-
tices?, and therefore potential modifiers.
For the potential heads, we want to define the set
of possible attachment positions available in the
spanning tree where the potential modifiers can at-
tach. To talk about these attachment positions, we
define the set of labels L = {(d, j)|d ? D, j ?
N}, an element (d, j) representing an attachment
point in direction d, position j. Valid attachment
positions must be in sequential order and not miss-
ing any intermediate positions (e.g. if position 2
on the right is specified, position 1 must be also):
so we define for some i ? N, 0 ? i < N , a set
Ai ? L such that if the label (d, j) ? Ai then the
label (d, k) ? Ai for 0 ? k < j. Collecting these,
we define A = {Ai | 0 ? i < N}.
To map a potential head onto the set of attach-
ment positions, we define a function q : Hw ? A.
So, given some v ? Hw, q(v) = Ai for some
0 ? i < N . In talking about an individual attach-
ment point (d, j) ? q(v) for potential head v, we
/* initialisation */
Hw ? {w0}1
Mw ? V ?2
Uw ? {w0R1}3
U ?w ? {}4
Tw ? {}5
/* The Assignment-based Algorithm */
begin6
while Mw 6= {} and U ?w 6= Uw do7
U ?w ? Uw8
foreach ?u, (d, j)), v? ? Kuhn-Munkres(Gpw =9
(Uw,M?w, E
p
w)) do
Tw ? Tw ? {(u, v)}10
if u ? Hw then11
Uw ? Uw \ {u}12
end13
Uw ? Uw ? next(q(u))14
Uw ? Uw ? next(q(m))15
q(m)? q(m) \ next(q(m))16
q(h)? q(h) \ next(q(h))17
Mw ?Mw \ {m}18
Hw ? Hw ? {m}19
end20
end21
end22
Algorithm 3: The Assignment-based Depen-
dency Tree Building algorithm.
use the notation vdj . For example, when referring
to the second argument position on the right with
respect to v, we use vr2.
For the implementation of the algorithm, we
have defined q, to specify attachment points, as
follows, given some v ? Hw:
q(v) =
?
?
?
?
?
?
?
{vr1} if v = w0, the root
{vl1} if pos(v) is a preposition
L if pos(v) is a verb
{vlj |j ? N} otherwise
Defining q allows one to optionally incorporate
linguistic information if desired.
We define the function next : q(v) ? A, v ?
Hw that returns the position (d, j) with the small-
est value of j for direction d. Finally, we write the
set of available attachment positions in the span-
ning tree as U ? {(v, l) | v ? Hw, l ? q(v)}.
4.3 Finding an Assignment
To construct the bipartite graph used for the as-
signment problem at line 9 of Algorithm 3, given
our original dependency graph Gw = (Vw, Ew),
and the variables defined from it above in Sec-
tion 4.2, we do the following. The first set of
vertices, of possible heads and their attachment
points, is the set Uw. The second set of ver-
tices is the set of possible modifiers augmented
by dummy vertices ?i (indicating no modifica-
tion) such that this set is at least as large as Uw :
M ?w = Mw?{?0, . . . , ?max(0,|Uw|?|Mw|)}. The bi-
856
partite graph is then Gpw = (Uw,M ?w, Epw), where
Epw = {(u, v) |u ? Uw, v ? M ?w}.
The weights on the edges of this graph incor-
porate a model of argument counts. The weight
function is of the form sap : Ep ? R. We
consider some e ? Epw: e = (v?, v) for some
v? ? Uw, v ? M ?w; and v? = (u, (d, j)) for some
u ? Vw, d ? D, j ? N. s(u,M ?w) is defined to re-
turn the maximum cost so that the dummy leaves
are only attached as a last resort. We then define:
sap(e)
= ?log(probdep(u, v, d) ? probarg(u, d, j))
(5)
where probdep(u, v, d) is as in equation 2, using
the original dependency graph defined in Section
2; and probarg(u, d, j), an estimate of the prob-
ability that a word u with i arguments assigned
already can take on more arguments, is defined as:
probarg(u, d, j)
=
??
i=j+1 cntarg(u, d, i)
cnt(u, d) (6)
where cntarg(u, d, i) is the number of times word
u has been seen with i arguments in direction
d; and cnt(u, d) = ?i?N cntarg(u, d, i). As the
probability of argument positions beyond a certain
value for i in a given direction will be extremely
small, we approximate this sum by calculating the
probability density up to a fixed maximum, in this
case 7 argument positions, and assume zero prob-
ability beyond that.
5 Evaluation
5.1 String Generation Task
The best-performing word ordering algorithm is
one that makes fewest grammatical errors. As a
surrogate measurement of grammaticality, we use
the string regeneration task. Beginning with a
human-authored sentence with its word order ran-
domised, the goal is to regenerate the original sen-
tence. Success is indicated by the proportion of the
original sentence regenerated, as measured by any
string comparison method: in our case, using the
BLEU metric (Papineni et al, 2002). One benefit
to this evaluation is that content selection, as a fac-
tor, is held constant. Specifically, the probability
of word selection is uniform for all words.
The string comparison task and its associated
metrics like BLEU are not perfect.3 The evalu-
ation can be seen as being overly strict. It as-
sumes that the only grammatical order is that of the
original human authored sentence, referred to as
the ?gold standard? sentence. Should an approach
chance upon an alternative grammatical ordering,
it would penalised. However, all algorithms and
baselines compared would suffer equally in this
respect, and so this will be less problematic when
averaging across multiple test cases.
5.2 Data Sets and Training Procedures
The Penn Treebank corpus (PTB) was used to pro-
vide a model of dependency relations and argu-
ment counts. It contains about 3 million words
of text from the Wall Street Journal (WSJ) with
human annotations of syntactic structures. Depen-
dency events were sourced from the events file of
the Collins parser package, which contains the de-
pendency events found in training sections 2-22 of
the corpus. Development was done on section 00
and testing was performed on section 23.
A 4-gram language model (LM) was also ob-
tained from the PTB training data, referred to as
PTB-LM. Additionally, a 4-gram language model
was obtained from a subsection of the BLLIP?99
Corpus (LDC number: LDC2000T43) containing
three years of WSJ data from 1987 to 1989 (Char-
niak et al, 1999). As in Collins et al (2004),
the 1987 portion of the BLLIP corpus containing
20 million words was also used to create a lan-
guage model, referred to here as BLLIP-LM. N-
gram models were smoothed using Katz?s method,
backing off to smaller values of n.
For this evaluation, tokenisation was based on
that provided by the PTB data set. This data
set alo delimits base noun phrases (noun phrases
without nested constituents). Base noun phrases
were treated as single tokens, and the rightmost
word assumed to be the head. For the algorithms
tested, the input set for any test case consisted of
the single tokens identified by the PTB tokenisa-
tion. Additionally, the heads of base noun phrases
were included in this input set. That is, we do not
regenerate the base noun phrases.4
3Alternative grammaticality measures have been devel-
oped recently (Mutton et al, 2007). We are currently explor-
ing the use of this and other metrics.
4This would correspond to the use of a chunking algo-
rithm or a named-entity recogniser to find noun phrases that
could be re-used for sentence generation.
857
Algorithms PTB-LM BLLIP-LM
Viterbi baseline 14.9 18.0
LMO baseline 24.3 26.0
CLE 26.4 26.8
AB 33.6 33.7
Figure 2: String regeneration as measured in
BLEU points (maximum 100)
5.3 Algorithms and Baselines
We compare the baselines against the Chu-Liu
Edmonds (CLE) algorithm to see if spanning
tree algorithms do indeed improve upon conven-
tional language modelling. We also compare
the Assignment-based (AB) algorithm against the
baselines and CLE to see if, additionally, mod-
elling argument assignments improves the re-
sulting tree and thus the generated word se-
quence. Two baseline generators based on n-
gram language-models were used, representing
approaches that optimise word sequences based on
the local context of the n-grams.
The first baseline re-uses the LMO greedy se-
quence algorithm on the same set of input words
presented to the CLE and AB algorithms. We ap-
ply LMO in a rightward manner beginning with
a start-of-sentence token. Note that this baseline
generator, like the two spanning tree algorithms,
will score favourably using BLEU since, mini-
mally, the word order of the base noun phrases will
be correct when each is reinserted.
Since the LMO baseline reduces to bigram gen-
eration when concatenating single words, we test
a second language model baseline which always
uses a 4-gram window size. A Viterbi-like gen-
erator with a 4-gram model and a beam of 100 is
used to generate a sequence. For this baseline, re-
ferred to as the Viterbi baseline, base noun phrases
were separated into their constituent words and in-
cluded in the input word set.
5.4 Results
The results are presented in Table 2. Significance
was measured using the sign test and the sampling
method outlined in (Collins et al, 2005). We will
examine the results in the PTB-LM column first.
The gain of 10 BLEU points by the LMO baseline
over the Viterbi baseline shows the performance
improvement that can be gained when reinserting
the base noun phrases.
AB: the dow at this point was down about 35 points
CLE: was down about this point 35 points the dow at
LMO: was this point about at down the down 35 points
Viterbi: the down 35 points at was about this point down
Original: at this point, the dow was down about 35 points
Figure 3: Example generated sentences using the
BLLIP-LM.
The CLE algorithm significantly out-performed
the LMO baseline by 2 BLEU points, from which
we conclude that incorporating a model for global
syntactic structure and treating the search for a
dependency tree as a spanning problem helps for
novel sentence generation. However, the real im-
provement can be seen in the performance of the
AB system which significantly out-performs all
other methods, beating the CLE algorithm by 7
BLEU points, illustrating the benefits of a model
for argument counts and of couching tree building
as an iterative set of argument assignments.
One might reasonably ask if more n-gram data
would narrow the gap between the tree algorithms
and the baselines, which encode global and lo-
cal information respectively. Examining results in
the BLLIP-LM column, all approaches improve
with the better language model. Unsurprisingly,
the improvements are most evident in the base-
lines which rely heavily on the language model.
The margin narrows between the CLE algorithm
and the LMO baseline. However, the AB algo-
rithm still out-performs all other approaches by
7 BLEU points, highlighting the benefit in mod-
elling dependency relations. Even with a language
model that is one order of magnitude larger than
the PTB-LM, the AB still maintains a sizeable lead
in performance. Figure 3 presents sample gener-
ated strings.
6 Related Work
6.1 Statistical Surface Realisers
The work in this paper is similar to research in
statistical surface realisation (for example, Langk-
ilde and Knight (1998); Bangalore and Rambow
(2000); Filippova and Strube (2008)). These start
with a semantic representation for which a specific
rendering, an ordering of words, must be deter-
mined, often using language models to govern tree
traversal. The task in this paper is different as it is
a text-to-text scenario and does not begin with a
representation of semantics.
858
The dependency model and the LMO lineari-
sation algorithm are based heavily on word order
statistics. As such, the utility of this approach is
limited to human languages with minimal use of
inflections, such as English. Approaches for other
language types, for example German, have been
explored (Filippova and Strube, 2007).
6.2 Text-to-Text Generation
As a text-to-text approach, our work is more sim-
ilar to work on Information Fusion (Barzilay et
al., 1999), a sub-problem in multi-document sum-
marisation. In this work, sentences presenting the
same information, for example multiple news arti-
cles describing the same event, are merged to form
a single summary by aligning repeated words and
phrases across sentences.
Other text-to-text approaches for generating
novel sentences also aim to recycle sentence frag-
ments where possible, as we do. Work on phrase-
based statistical machine translation has been
applied to paraphrase generation (Bannard and
Callison-Burch, 2005) and multi-sentence align-
ment in summarisation (Daume? III and Marcu,
2004). These approaches typically use n-gram
models to find the best word sequence.
The WIDL formalism (Soricut and Marcu,
2005) was proposed to efficiently encode con-
straints that restricted possible word sequences,
for example dependency information. Though
similar, our work here does not explicitly repre-
sent the word lattice.
For these text-to-text systems, the order of ele-
ments in the generated sentence is heavily based
on the original order of words and phrases in the
input sentences from which lattices are built. Our
approach has the benefit of considering all possi-
ble orderings of words, corresponding to a wider
range of paraphrases, provided with a suitable de-
pendency model is available.
6.3 Parsing and Semantic Role Labelling
This paper presents work closely related to parsing
work by McDonald et al (2005) which searches
for the best parse tree. Our work can be thought of
as generating projective dependency trees (that is,
without crossing dependencies).
The key difference between parsing and gener-
ation is that, in parsing, the word order is fixed,
whereas for generation, this must be determined.
In this paper, we search across all possible tree
structures whilst searching for the best word or-
dering. As a result, an argument model is needed
to identify linguistically plausible spanning trees.
We treated the alignment of modifiers to head
words as a bipartite graph matching problem. This
is similar to work in semantic role labelling by
Pado? and Lapata (2006). The alignment of an-
swers to question types as a semantic role labelling
task using similar methods was explored by Shen
and Lapata (2007).
Our work is also strongly related to that of
Wong and Mooney (2007) which constructs sym-
bolic semantic structures via an assignment pro-
cess in order to provide surface realisers with in-
put. Our approach differs in that we do not be-
gin with a fixed set of semantic labels. Addition-
ally, our end goal is a dependency tree that encodes
word precedence order, bypassing the surface re-
alisation stage.
7 Conclusions
In this paper, we presented a new use of spanning
tree algorithms for generating sentences from an
input set of words, a task common to many text-
to-text scenarios. The algorithm finds the best de-
pendency trees in order to ensure that the result-
ing string has grammaticality modelled at a global
(sentence) level. Our algorithm incorporates a
model of argument satisfaction which is treated as
an assignment problem, using the Kuhn-Munkres
assignment algorithm. We found a significant im-
provement using BLEU to measure improvements
on the string regeneration task. We conclude that
our new algorithm based on the assignment prob-
lem and an argument model finds trees that are lin-
guistically more plausible, thereby improving the
grammaticality of the generated word sequence.
References
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a probabilistic hierarchical model for gen-
eration. In Proceedings of the 18th Conference on
Computational Linguistics, Saarbru?cken, Germany.
Srinivas Bangalore, Owen Rambow, and Steve Whit-
taker. 2000. Evaluation metrics for generation.
In Proceedings of the first international conference
on Natural language generation, Morristown, NJ,
USA.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting of the Asso-
859
ciation for Computational Linguistics, Ann Arbor,
Michigan.
Regina Barzilay, Kathleen R. McKeown, and Michael
Elhadad. 1999. Information fusion in the context
of multi-document summarization. In Proceedings
of the 37th conference on Association for Computa-
tional Linguistics, Morristown, NJ, USA.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson. 1999. Bllip 1987-89
wsj corpus release 1. Technical report, Linguistic
Data Consortium.
Y. J. Chu and T. H. Liu. 1965. On the shortest
arborescence of a directed graph. Science Sinica,
v.14:1396?1400.
Christopher Collins, Bob Carpenter, and Gerald Penn.
2004. Head-driven parsing for word lattices. In Pro-
ceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, Morristown, NJ,
USA.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, Morristown, NJ, USA.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proceed-
ings of the Thirty-Fourth Annual Meeting of the As-
sociation for Computational Linguistics, San Fran-
cisco.
Hal Daume? III and Daniel Marcu. 2004. A phrase-
based hmm approach to document/abstract align-
ment. In Proceedings of EMNLP 2004, Barcelona,
Spain..
J. Edmonds. 1967. Optimum branchings. J. Research
of the National Bureau of Standards, 71B:233?240.
Katja Filippova and Michael Strube. 2007. Generating
constituent order in german clauses. In Proceedings
of the 45th Annual Meeting on Association for Com-
putational Linguistics. Prague, Czech Republic.
Katja Filippova and Michael Strube. 2008. Sentence
fusion via dependency graph compression. In Con-
ference on Empirical Methods in Natural Language
Processing, Waikiki, Honolulu, Hawaii.
Mark Johnson. 2007. Transforming projective bilex-
ical dependency grammars into efficiently-parsable
cfgs with unfold-fold. In Proceedings of the 45th
Annual Meeting on Association for Computational
Linguistics. Prague, Czech Republic.
H.W. Kuhn. 1955. The hungarian method for the as-
signment problem. Naval Research Logistics Quar-
terly, 219552:83?97 83?97.
Irene Langkilde and Kevin Knight. 1998. The practi-
cal value of N-grams in derivation. In Proceedings
of the Ninth International Workshop on Natural Lan-
guage Generation, New Brunswick, New Jersey.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency
parsing. In Proceedings of the Tenth International
Conference on Parsing Technologies, Prague, Czech
Republic.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, Morristown, NJ, USA.
Andrew Mutton, Mark Dras, Stephen Wan, and Robert
Dale. 2007. Gleu: Automatic evaluation of
sentence-level fluency. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, Prague, Czech Republic.
Sebastian Pado? and Mirella Lapata. 2006. Optimal
constituent alignment with edge covers for seman-
tic projection. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Com-
putational Linguistics, Morristown, NJ, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, Philadelphia, July.
Dan Shen and Mirella Lapata. 2007. Using seman-
tic roles to improve question answering. In Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, Prague,
Czech Republic.
Radu Soricut and Daniel Marcu. 2005. Towards devel-
oping generation algorithms for text-to-text applica-
tions. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, Ann
Arbor, Michigan.
Stephen Wan, Robert Dale, Mark Dras, and Ce?cile
Paris. 2007. Global revision in summarisation:
Generating novel sentences with prim?s algorithm.
In Proceedings of 10th Conference of the Pacific As-
sociation for Computational Linguistic, Melbourne,
Australia.
Yuk Wah Wong and Raymond Mooney. 2007. Genera-
tion by inverting a semantic parser that uses statisti-
cal machine translation. In Human Language Tech-
nologies 2007: The Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, Rochester, New York.
860
Towards Statistical Paraphrase Generation: Preliminary Evaluations of
Grammaticality
Stephen Wan
 
Mark Dras
 
Robert Dale
 
 
Center for Language Technology
Div of Information Communication Sciences
Macquarie University
Sydney, NSW 2113
swan,madras,rdale@ics.mq.edu.au
Ce?cile Paris


Information and Communication
Technologies
CSIRO
Sydney, Australia
Cecile.Paris@csiro.au
Abstract
Summary sentences are often para-
phrases of existing sentences. They
may be made up of recycled fragments
of text taken from important sentences
in an input document. We investigate
the use of a statistical sentence gener-
ation technique that recombines words
probabilistically in order to create new
sentences. Given a set of event-related
sentences, we use an extended version
of the Viterbi algorithm which employs
dependency relation and bigram proba-
bilities to find the most probable sum-
mary sentence. Using precision and
recall metrics for verb arguments as a
measure of grammaticality, we find that
our system performs better than a bi-
gram baseline, producing fewer spuri-
ous verb arguments.
1 Introduction
Human authored summaries are more than just
a list of extracted sentences. Often the sum-
mary sentence is a paraphrase of a sentence in the
source text, or else a combination of phrases and
words from important sentences that have been
pieced together to form a new sentence. These
sentences, referred to as Non-Verbatim Sentences,
can replace extracted text to improve readability
and coherence in the summary.
Consider the example in Figure 1 which
presents an alignment between a human authored
summary sentence and a source sentence. The
Summary Sentence:
Every province in the country, except one, endured sporadic fighting, looting
or armed banditry in 2003.
Source Sentence:
However, as the year unfolded, every province has been subjected to fighting,
looting or armed banditry, with the exception of just one province (Kirundo,
in northern Burundi).
Figure 1: An aligned summary and source sen-
tence.
text is taken from a corpus of Humanitarian Aid
Proposals1 produced by the United Nations for
the purpose of convincing donors to support a re-
lief effort.
The example illustrates that sentence extraction
alone cannot account for the breadth of human au-
thored summary sentences. This is supported by
evidence presented in (Jing and McKeown, 1999)
and (Daume? III and Marcu, 2004).
Moving towards the goal of abstract-like auto-
matic summary generation challenges us to con-
sider mechanisms for generating non-verbatim
sentences. Such a mechanism can usefully be
considered as automatically generating a para-
phrase.2 We treat the problem as one in which a
new and previously unseen summary sentence is
to be automatically produced given some closely
related sentences extracted from a source text.
Following on from (Witbrock and Mittal,
1999), we use and extend the Viterbi algorithm
(Forney, 1973) for the purposes of generating
non-verbatim sentences. This approach treats
1These are available publically at
http://www.reliefweb.com.
2Paraphrase here includes sentences generated in an In-
formation Fusion task (Barzilay et al, 1999).
88
sentence generation as a search problem. Given
a set of words (taken from some set of sentences
to paraphrase), we search for the most likely se-
quence given some language model. Intuitively,
we want the generated string to be grammatical
and to accurately reflect the content of the source
text.
Within the Viterbi search process, each time we
append a word to the partially generated sentence,
we consider how well it attaches to a dependency
structure. The focus of this paper is to evaluate
whether or not a series of iterative considerations
of dependency structure results in a grammatical
generated sentence. Previous preliminary evalu-
ations (Wan et al, 2005) indicate that the gen-
erated sequences contain less fragmented text as
measured by an off-the-shelf dependency parser;
more fragments would indicate a grammatically
problematic sentence.
However, while encouraging, such an evalu-
ation says little about what the actual sentence
looks like. For example, such generated text
might only be useful if it contains complete
clauses. Thus, in this paper, we use the precision
and recall metric to measure how many generated
verb arguments, as extracted from dependency re-
lations, are correct.
The remainder of this paper is structured as fol-
lows. Section 2 provides an overview introducing
our approach. In Section 3, we briefly illustrate
our algorithm with examples. A brief survey of
related work is presented in Section 4. We present
our grammaticality experiments in Section 5. We
conclude with further work in Section 6.
2 An Overview of our Approach to
Statistical Sentence Generation
One could characterise the search space as being
a series of nested sets. The outer most set would
contain all possible word sequences. Within this,
a smaller set of strings exhibiting some semblance
of grammaticality might be found, though many
of these might be gibberish. Further nested sets
are those that are grammatical, and within those,
the set of paraphrases that are entailed by the in-
put text.
However, given that we limit ourselves to sta-
tistical techniques and avoid symbolic logic, we
cannot make any claim of strict entailment. We
Original Text
A military transporter was scheduled to take off in the afternoon from Yokota
air base on the outskirts of Tokyo and fly to Osaka with 37,000 blankets .
Mondale said the United States, which has been flying in blankets and is
sending a team of quake relief experts, was prepared to do more if Japan
requested .
United States forces based in Japan will take blankets to help earthquake
survivors Thursday, in the U.S. military?s first disaster relief operation in
Japan since it set up bases here.
Our approach with Dependencies
6: united states forces based in blankets
8: united states which has been flying in blankets
11: a military transporter was prepared to osaka with 37,000 blankets
18: mondale said the afternoon from yokota air base on the united states which
has been flying in blankets
20: mondale said the outskirts of tokyo and is sending a military transporter
was prepared to osaka with 37,000 blankets
23: united states forces based in the afternoon from yokota air base on the
outskirts of tokyo and fly to osaka with 37,000 blankets
27: mondale said the afternoon from yokota air base on the outskirts of tokyo
and is sending a military transporter was prepared to osaka with 37,000 blan-
kets
29: united states which has been flying in the afternoon from yokota air base
on the outskirts of tokyo and is sending a team of quake relief operation in
blankets
31: united states which has been flying in the afternoon from yokota air base
on the outskirts of tokyo and is sending a military transporter was prepared to
osaka with 37,000 blankets
34: mondale said the afternoon from yokota air base on the united states which
has been flying in the outskirts of tokyo and is sending a military transporter
was prepared to osaka with 37,000 blankets
36: united states which has been flying in japan will take off in the after-
noon from yokota air base on the outskirts of tokyo and is sending a military
transporter was prepared to osaka with 37,000 blankets
Figure 2: A selection of example output. Sen-
tences are prefixed by their length.
thus propose an intermediate set of sentences
which conserve the content of the source text
without necessarily being entailed. These are re-
ferred to as the set of verisimilitudes, of which
properly entailed sentences are a subset. The aim
of our choice of features and our algorithm exten-
sion is to reduce the search space from gibberish
strings to that of verisimilitudes. While generat-
ing verisimilitudes is our end goal, in this paper,
we are concerned principally with the generating
of grammatical sentences.
To do so, the extension adds an extra feature
propagation mechanism to the Viterbi algorithm
such that features are passed along a word se-
quence path in the search space whenever a new
word is appended to it. Propagated features are
used to influence the choice of subsequent words
suitable for appending to a partially generated
sentence. In our case, our feature is a depen-
dency structure of the word sequence correspond-
ing to the search path. Our present dependency
representation is based on that of (Kittredge and
89
Mel?cuk, 1983). However, it contains only the
head and modifier of a relation, ignoring relation-
ship labels for the present.
Algorithmically, after appending a word to a
path, a dependency structure of the partially gen-
erated string is obtained probabilistically. Along
with bigram information, the long-distance con-
text of dependency head information of the pre-
ceding word sequence will be useful in generat-
ing better sentences by filtering out all words that
might, at a particular position in the string, lead
to a spurious dependency relation in the final sen-
tence. Example output is presented in Figure 2.
As the dependency ?parsing? mechanism is lin-
ear3 and is embedded within the Viterbi algo-
rithm, the result is an O( 

) algorithm.
By examining surface-syntactic dependency
structure at each step in the search, resulting sen-
tences are likely to be more grammatical. This
marraige of models has been tested in other fields
such as speech recognition (Chelba and Jelinek,
1998) with success. Although it is an impover-
ished representation of semantics, considering de-
pendency features in our application context may
also serendipitously assist verisimilitude genera-
tion.
3 The Extended Viterbi Algorithm:
Propagating Dependency Structure
In this section, we present an overview of the
main features of our algorithm extension. We di-
rect the interested reader to our technical paper
(Wan et al, 2005) for full details.
The Viterbi algorithm (for a comprehensive
overview, see (Manning and Schu?tze, 1999)) is
used to search for the best path across a network
of nodes, where each node represents a word in
the vocabulary. The best sentence is a string of
words, each one emitted by the corresponding vis-
ited node on the path.
Arcs between nodes are weighted using a com-
bination of two pieces of information: a bigram
probability corresponding to that pair of words;
and a probability corresponding to the likelihood
of a dependency relation between that pair of
words. Specifically, the transition probability
3The parse is thus not necessarily optimal, in the sense of
guaranteeing the most likely parse.
defining these weights is the average of the depen-
dency transition probability and the bigram prob-
ability.
To simplify matters in this evaluation, we
assume that the emission probability is always
one. The emission probability is interpreted
as being a Content Selection mechanism that
chooses words that are likely to be in a summary.
Thus, in this paper, each word has an equally
likely chance of being selected for the sentence.
Transition Probability is defined as:
	
		 
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 344?351,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
GLEU: Automatic Evaluation of Sentence-Level Fluency
Andrew Mutton? Mark Dras? Stephen Wan?,? Robert Dale?
?Centre for Language Technology ?Information and Communication Technologies
Macquarie University CSIRO
NSW 2109 Australia NSW 2109 Australia
madras@ics.mq.edu.au
Abstract
In evaluating the output of language tech-
nology applications?MT, natural language
generation, summarisation?automatic eval-
uation techniques generally conflate mea-
surement of faithfulness to source content
with fluency of the resulting text. In this
paper we develop an automatic evaluation
metric to estimate fluency alone, by examin-
ing the use of parser outputs as metrics, and
show that they correlate with human judge-
ments of generated text fluency. We then de-
velop a machine learner based on these, and
show that this performs better than the indi-
vidual parser metrics, approaching a lower
bound on human performance. We finally
look at different language models for gener-
ating sentences, and show that while individ-
ual parser metrics can be ?fooled? depending
on generation method, the machine learner
provides a consistent estimator of fluency.
1 Introduction
Intrinsic evaluation of the output of many language
technologies can be characterised as having at least
two aspects: how well the generated text reflects
the source data, whether it be text in another lan-
guage for machine translation (MT), a natural lan-
guage generation (NLG) input representation, a doc-
ument to be summarised, and so on; and how well it
conforms to normal human language usage. These
two aspects are often made explicit in approaches
to creating the text. For example, in statistical MT
the translation model and the language model are
treated separately, characterised as faithfulness and
fluency respectively (as in the treatment in Jurafsky
and Martin (2000)). Similarly, the ultrasummarisa-
tion model of Witbrock and Mittal (1999) consists
of a content model, modelling the probability that a
word in the source text will be in the summary, and
a language model.
Evaluation methods can be said to fall into two cate-
gories: a comparison to gold reference, or an appeal
to human judgements. Automatic evaluation meth-
ods carrying out a comparison to gold reference tend
to conflate the two aspects of faithfulness and flu-
ency in giving a goodness score for generated out-
put. BLEU (Papineni et al, 2002) is a canonical ex-
ample: in matching n-grams in a candidate transla-
tion text with those in a reference text, the metric
measures faithfulness by counting the matches, and
fluency by implicitly using the reference n-grams as
a language model. Often we are interested in know-
ing the quality of the two aspects separately; many
human judgement frameworks ask specifically for
separate judgements on elements of the task that cor-
respond to faithfulness and to fluency. In addition,
the need for reference texts for an evaluation metric
can be problematic, and intuitively seems unneces-
sary for characterising an aspect of text quality that
is not related to its content source but to the use of
language itself. It is a goal of this paper to provide
an automatic evaluation method for fluency alone,
without the use of a reference text.
One might consider using a metric based on lan-
guage model probabilities for sentences: in eval-
344
uating a language model on (already existing) test
data, a higher probability for a sentence (and lower
perplexity over a whole test corpus) indicates bet-
ter language modelling; perhaps a higher probability
might indicate a better sentence. However, here we
are looking at generated sentences, which have been
generated using their own language model, rather
than human-authored sentences already existing in
a test corpus; and so it is not obvious what language
model would be an objective assessment of sentence
naturalness. In the case of evaluating a single sys-
tem, using the language model that generated the
sentence will only confirm that the sentence does
fit the language model; in situations such as com-
paring two systems which each generate text using
a different language model, it is not obvious that
there is a principled way of deciding on a fair lan-
guage model. Quite a different idea was suggested
in Wan et al (2005), of using the grammatical judge-
ment of a parser to assess fluency, giving a measure
independent of the language model used to gener-
ate the text. The idea is that, assuming the parser
has been trained on an appropriate corpus, the poor
performance of the parser on one sentence relative
to another might be an indicator of some degree of
ungrammaticality and possibly disfluency. In that
work, however, correlation with human judgements
was left uninvestigated.
The goal of this paper is to take this idea and de-
velop it. In Section 2 we look at some related work
on metrics, in particular for NLG. In Section 3, we
verify whether parser outputs can be used as esti-
mators of generated sentence fluency by correlating
them with human judgements. In Section 4, we pro-
pose an SVM-based metric using parser outputs as
features, and compare its correlation against human
judgements with that of the individual parsers. In
Section 5, we investigate the effects on the various
metrics from different types of language model for
the generated text. Then in Section 6 we conclude.
2 Related Work
In terms of human evaluation, there is no uniform
view on what constitutes the notion of fluency, or its
relationship to grammaticality or similar concepts.
We mention a few examples here to illustrate the
range of usage. In MT, the 2005 NIST MT Evalu-
ation Plan uses guidelines1 for judges to assess ?ad-
equacy? and ?fluency? on 5 point scales, where they
are asked to provide intuitive reactions rather than
pondering their decisions; for fluency, the scale de-
scriptions are fairly vague (5: flawless English; 4:
good English; 3: non-native English; 2: disfluent
English; 1: incomprehensible) and instructions are
short, with some examples provided in appendices.
Zajic et al (2002) use similar scales for summari-
sation. By contrast, Pan and Shaw (2004), for their
NLG system SEGUE tied the notion of fluency more
tightly to grammaticality, giving two human evalu-
ators three grade options: good, minor grammatical
error, major grammatical/pragmatic error. As a fur-
ther contrast, the analysis of Coch (1996) was very
comprehensive and fine-grained, in a comparison of
three text-production techniques: he used 14 human
judges, each judging 60 letters (20 per generation
system), and required them to assess the letters for
correct spelling, good grammar, rhythm and flow,
appropriateness of tone, and several other specific
characteristics of good text.
In terms of automatic evaluation, we are not aware
of any technique that measures only fluency or sim-
ilar characteristics, ignoring content, apart from that
of Wan et al (2005). Even in NLG, where, given the
variability of the input representations (and hence
difficulty in verifying faithfulness), it might be ex-
pected that such measures would be available, the
available metrics still conflate content and form.
For example, the metrics proposed in Bangalore et
al. (2000), such as Simple Accuracy and Generation
Accuracy, measure changes with respect to a refer-
ence string based on the idea of string-edit distance.
Similarly, BLEU has been used in NLG, for example
by Langkilde-Geary (2002).
3 Parsers as Evaluators
There are three parts to verifying the usefulness of
parsers as evaluators: choosing the parsers and the
metrics derived from them; generating some texts
for human and parser evaluation; and, the key part,
getting human judgements on these texts and corre-
lating them with parser metrics.
1http://projects.ldc.upenn.edu/TIDES/
Translation/TranAssessSpec.pdf
345
3.1 The Parsers
In testing the idea of using parsers to judge fluency,
we use three parsers, from which we derive four
parser metrics, to investigate the general applicabil-
ity of the idea. Those chosen were the Connexor
parser,2 the Collins parser (Collins, 1999), and the
Link Grammar parser (Grinberg et al, 1995). Each
produces output that can be taken as representing
degree of ungrammaticality, although this output is
quite different for each.
Connexor is a commercially available dependency
parser that returns head?dependant relations as well
as stemming information, part of speech, and so on.
In the case of an ungrammatical sentence, Connexor
returns tree fragments, where these fragments are
defined by transitive head?dependant relations: for
example, for the sentence Everybody likes big cakes
do it returns fragments for Everybody likes big cakes
and for do. We expect that the number of fragments
should correlate inversely with the quality of a sen-
tence. For a metric, we normalise this number by
the largest number of fragments for a given data set.
(Normalisation matters most for the machine learner
in Section 4.)
The Collins parser is a statistical chart parser that
aims to maximise the probability of a parse using dy-
namic programming. The parse tree produced is an-
notated with log probabilities, including one for the
whole tree. In the case of ungrammatical sentences,
the parser will assign a low probability to any parse,
including the most likely one. We expect that the
log probability (becoming more negative as the sen-
tence is less likely) should correlate positively with
the quality of a sentence. For a metric, we normalise
this by the most negative value for a given data set.
Like Connexor, the Link Grammar parser returns in-
formation about word relationships, forming links,
with the proviso that links cannot cross and that in
a grammatical sentence all links are indirectly con-
nected. For an ungrammatical sentence, the parser
will delete words until it can produce a parse; the
number it deletes is called the ?null count?. We ex-
pect that this should correlate inversely with sen-
tence quality. For a metric, we normalise this by
the sentence length. In addition, the parser produces
2http://www.connexor.com
another variable possibly of interest. In generating
a parse, the parser produces many candidates and
rules some out by a posteriori constraints on valid
parses. In its output the parser returns the number of
invalid parses. For an ungrammatical sentence, this
number may be higher; however, there may also be
more parses. For a metric, we normalise this by the
total number of parses found for the sentence. There
is no strong intuition about the direction of correla-
tion here, but we investigate it in any case.
3.2 Text Generation Method
To test whether these parsers are able to discriminate
sentence-length texts of varying degrees of fluency,
we need first to generate texts that we expect will be
discriminable in fluency quality ranging from good
to very poor. Below we describe our method for gen-
erating text, and then our preliminary check on the
discriminability of the data before giving them to hu-
man judges.
Our approach to generating ?sentences? of a fixed
length is to take word sequences of different lengths
from a corpus and glue them together probabilisti-
cally: the intuition is that a few longer sequences
glued together will be more fluent than many shorter
sequences. More precisely, to generate a sentence of
length n, we take sequences of length l (such that l
divides n), with sequence i of the form wi,1 . . . wi,l,
where wi, is a word or punctuation mark. We start
by selecting sequence 1, first by randomly choos-
ing its first word according to the unigram probabil-
ity P (w1,1), and then the sequence uniformly ran-
domly over all sequences of length l starting with
w1,1; we select subsequent sequences j (2 ? j ?
n/l) randomly according to the bigram probability
P (wj,1 |wj?1,l). Taking as our corpus the Reuters
corpus,3 for length n = 24, we generate sentences
for sequence sizes l = 1, 2, 4, 8, 24 as in Figure 1.
So, for instance, the sequence-size 8 example was
constructed by stringing together the three consecu-
tive sequences of length 8 (There . . . to; be . . . have;
to . . . .) taken from the corpus.
These examples, and others generated, appear to
be of variable quality in accordance with our intu-
ition. However, to confirm this prior to testing them
3http://trec.nist.gov/data/reuters/
reuters.html
346
Extracted (Sequence-size 24)
Ginebra face Formula Shell in a sudden-death playoff on Sun-
day to decide who will face Alaska in a best-of-seven series for
the title.
Sequence-size 8
There is some thinking in the government to be nearly as dra-
matic as some people have to be slaughtered to eradicate the
epidemic.
Sequence-size 4
Most of Banharn?s move comes after it can still be averted the
crash if it should again become a police statement said.
Sequence-size 2
Massey said in line with losses, Nordbanken is well-placed to
benefit abuse was loaded with Czech prime minister Andris
Shkele, said.
Sequence-size 1
The war we?re here in a spokesman Jeff Sluman 86 percent jump
that Spain to what was booked, express also said.
Figure 1: Sample sentences from the first trial
Description Correlation
Small 0.10 to 0.29
Medium 0.30 to 0.49
Large 0.50 to 1.00
Table 1: Correlation coefficient interpretation
out for discriminability in a human trial, we wanted
see whether they are discriminable by some method
other than our own judgement. We used the parsers
described in Section 3.1, in the hope of finding a
non-zero correlation between the parser outputs and
the sequence lengths.
Regarding the interpretation of the absolute value of
(Pearson?s) correlation coefficients, both here and in
the rest of the paper, we adopt Cohen?s scale (Co-
hen, 1988) for use in human judgements, given in
Table 1; we use this as most of this work is to do with
human judgements of fluency. For data, we gener-
ated 1000 sentences of length 24 for each sequence
length l = 1, 2, 3, 4, 6, 8, 24, giving 7000 sentences
in total. The correlations with the four parser out-
puts are as in Table 2, with the medium correlations
for Collins and Link Grammar (nulled tokens) indi-
cating that the sentences are indeed discriminable to
some extent, and hence the approach is likely to be
useful for generating sentences for human trials.
3.3 Human Judgements
The next step is then to obtain a set of human judge-
ments for this data. Human judges can only be ex-
pected to judge a reasonably sized amount of data,
Metric Corr.
Collins Parser 0.3101
Connexor -0.2332
Link Grammar Nulled Tokens -0.3204
Link Grammar Invalid Parses 0.1776
GLEU 0.4144
Table 2: Parser vs sequence size for original data set
so we first reduced the set of sequence sizes to be
judged. To do this we determined for the 7000
generated sentences the scores according to the (ar-
bitrarily chosen) Collins parser, and calculated the
means for each sequence size and the 95% confi-
dence intervals around these means. We then chose
a subset of sequence sizes such that the confidence
intervals did not overlap: 1, 2, 4, 8, 24; the idea was
that this would be likely to give maximally discrim-
inable sentences. For each of these sequences sizes,
we chose randomly 10 sentences from the initial set,
giving a set for human judgement of size 50.
The judges consisted of twenty volunteers, all native
English speakers without explicit linguistic training.
We gave them general guidelines about what consti-
tuted fluency, mentioning that they should consider
grammaticality but deliberately not giving detailed
instructions on the manner for doing this, as we were
interested in the level of agreement of intuitive un-
derstanding of fluency. We instructed them also that
they should evaluate the sentence without consider-
ing its content, using Colourless green ideas sleep
furiously as an example of a nonsensical but per-
fectly fluent sentence. The judges were then pre-
sented with the 50 sentences in random order, and
asked to score the sentences according to their own
scale, as in magnitude estimation (Bard et al, 1996);
these scores were then normalised in the range [0,1].
Some judges noted that the task was difficult be-
cause of its subjectivity. Notwithstanding this sub-
jectivity and variation in their approach to the task,
the pairwise correlations between judges were high,
as indicated by the maximum, minimum and mean
values in Table 3, indicating that our assumption
that humans had an intuitive notion of fluency
and needed only minimal instruction was justified.
Looking at mean scores for each sequence size,
judges generally also ranked sentences by sequence
size; see Figure 2. Comparing human judgement
347
Statistic Corr.
Maximum correlation 0.8749
Minimum correlation 0.4710
Mean correlation 0.7040
Standard deviation 0.0813
Table 3: Data on correlation between humans
Figure 2: Mean scores for human judges
correlations against sequence size with the same cor-
relations for the parser metrics (as for Table 2, but on
the human trial data) gives Table 4, indicating that
humans can also discriminate the different generated
sentence types, in fact (not surprisingly) better than
the automatic metrics.
Now, having both human judgement scores of some
reliability for sentences, and scoring metrics from
three parsers, we give correlations in Table 5. Given
Cohen?s interpretation, the Collins and Link Gram-
mar (nulled tokens) metrics show moderate correla-
tion, the Connexor metric almost so; the Link Gram-
mar (invalid parses) metric correlation is by far the
weakest. The consistency and magnitude of the first
three parser metrics, however, lends support to the
idea of Wan et al (2005) to use something like these
as indicators of generated sentence fluency. The aim
of the next section is to build a better predictor than
the individual parser metrics alone.
Metric Corr.
Humans 0.6529
Collins Parser 0.4057
Connexor -0.3804
Link Grammar Nulled Tokens -0.3310
Link Grammar Invalid Parses 0.1619
GLEU 0.4606
Table 4: Correlation with sequence size for human
trial data set
Metric Corr.
Collins Parser 0.3057
Connexor -0.3445
Link-Grammar Nulled Tokens -0.2939
Link Grammar Invalid Parses 0.1854
GLEU 0.4014
Table 5: Correlation between metrics and human
evaluators
4 An SVM-Based Metric
In MT, one problem with most metrics like BLEU
is that they are intended to apply only to document-
length texts, and any application to individual sen-
tences is inaccurate and correlates poorly with
human judgements. A neat solution to poor
sentence-level evaluation proposed by Kulesza and
Shieber (2004) is to use a Support Vector Machine,
using features such as word error rate, to estimate
sentence-level translation quality. The two main in-
sights in applying SVMs here are, first, noting that
human translations are generally good and machine
translations poor, that binary training data can be
created by taking the human translations as posi-
tive training instances and machine translations as
negative ones; and second, that a non-binary metric
of translation goodness can be derived by the dis-
tance from a test instance to the support vectors. In
an empirical evaluation, Kulesza and Shieber found
that their SVM gave a correlation of 0.37, which
was an improvement of around half the gap between
the BLEU correlations with the human judgements
(0.25) and the lowest pairwise human inter-judge
correlation (0.46) (Turian et al, 2003).
We take a similar approach here, using as features
the four parser metrics described in Section 3. We
trained an SVM,4 taking as positive training data
the 1000 instances of sentences of sequence length
24 (i.e. sentences extracted from the corpus) and
as negative training data the 1000 sentences of se-
quence length 1. We call this learner GLEU.5
As a check on the ability of the GLEU SVM to dis-
tinguish these ?positive? sentences from ?negative?
ones, we evaluated its classification accuracy on a
(new) test set of size 300, split evenly between sen-
tences of sequence length 24 and sequence length 1.
4We used the package SVM-light (Joachims, 1999).
5For GrammaticaLity Evaluation Utility.
348
This gave 81%, against a random baseline of 50%,
indicating that the SVM can classify satisfactorily.
We now move from looking at classification accu-
racy to the main purpose of the SVM, using distance
from support vector as a metric. Results are given
for correlation of GLEU against sequence sizes for
all data (Table 2) and for the human trial data set
(Table 4); and also for correlation of GLEU against
the human judges? scores (Table 5). This last indi-
cates that GLEU correlates better with human judge-
ments than any of the parsers individually, and is
well within the ?moderate? range for correlation in-
terpretation. In particular, for the GLEU?human cor-
relation, the score of 0.4014 is approaching the min-
imum pairwise human correlation of 0.4710.
5 Different Text Generation Methods
The method used to generate text in Section 3.2 is
a variation of the standard n-gram language model.
A question that arises is: Are any of the metrics de-
fined above strongly influenced by the type of lan-
guage model used to generate the text? It may be the
case, for example, that a parser implementation uses
its own language model that predisposes it to favour
a similar model in the text generation process. This
is a phenomenon seen in MT, where BLEU seems to
favour text that has been produced using a similar
statistical n-gram language model over other sym-
bolic models (Callison-Burch et al, 2006).
Our previous approach used only sequences of
words concatenated together. To define some new
methods for generating text, we introduced varying
amounts of structure into the generation process.
5.1 Structural Generation Methods
PoStag In the first of these, we constructed a
rough approximation of typical sentence grammar
structure by taking bigrams over part-of-speech
tags.6 Then, given a string of PoS tags of length
n, t1 . . . tn, we start by assigning the probabilities
for the word in position 1, w1, according to the con-
ditional probability P (w1 | t1). Then, for position j
(2 ? j ? n), we assign to candidate words the value
P (wj | tj)?P (wj |wj?1) to score word sequences.
6We used the supertagger of Bangalore and Joshi (1999).
So, for example, we might generate the PoS tag tem-
plate Det NN Adj Adv, take all the words corre-
sponding to each of these parts of speech, and com-
bine bigram word sequence probability with the con-
ditional probability of words with respect to these
parts of speech. We then use a Viterbi-style algo-
rithm to find the most likely word sequence.
In this model we violate the Markov assumption of
independence in much the same way as Witbrock
and Mittal (1999) in their combination of content
and language model probabilities, by backtracking
at every state in order to discourage repeated words
and avoid loops.
Supertag This is a variant of the approach above,
but using supertags (Bangalore and Joshi, 1999) in-
stead of PoS tags. The idea is that the supertags
might give a more fine-grained definition of struc-
ture, using partial trees rather than parts of speech.
CFG We extracted a CFG from the ?10% of the
Penn Treebank found in the NLTK-lite corpora.7
This CFG was then augmented with productions de-
rived from the PoS-tagged data used above. We then
generated a template of length n pre-terminal cate-
gories using this CFG. To avoid loops we biased the
selection towards terminals over non-terminals.
5.2 Human Judgements
We generated sentences according to a mix of the
initial method of Section 3.2, for calibration, and
the new methods above. We again used a sentence
length of 24, and sequence lengths for the initial
method of l = 1, 8, 24. A sample of sentences gen-
erated for each of these six types is in Figure 3.
For our data, we generated 1000 sentences per gen-
eration method, giving a corpus of 6000 sentences.
For the human judgements we also again took 10
sentences per generation method, giving 60 sen-
tences in total. The same judges were given the same
instructions as previously.
Before correlating the human judges? scores and
the parser outputs, it is interesting to look at how
each parser treats the sentence generation methods,
and how this compares with human ratings (Ta-
ble 6). In particular, note that the Collins parser rates
the PoStag- and Supertag-generated sentences more
7http://nltk.sourceforge.net
349
Extracted (Sequence-size 24)
After a near three-hour meeting and last-minute talks with Pres-
ident Lennart Meri, the Reform Party council voted overwhelm-
ingly to leave the government.
Sequence-size 8
If Denmark is closely linked to the Euro Disney reported a net
profit of 85 million note: the figures were rounded off.
Sequence-size 1
Israelis there would seek approval for all-party peace now com-
plain that this year, which shows demand following year and 56
billion pounds.
POS-tag, Viterbi-mapped
He said earlier the 9 years and holding company?s government,
including 69.62 points as a number of last year but market.
Supertag, Viterbi-mapped
That 97 saying he said in its shares of the market 74.53 percent,
adding to allow foreign exchange: I think people.
Context-Free Grammar
The production moderated Chernomyrdin which leveled gov-
ernment back near own 52 over every a current at from the said
by later the other.
Figure 3: Sample sentences from the second trial
sent. type s-24 s-8 s-1 PoS sup. CFG
Collins 0.52 0.48 0.41 0.60 0.57 0.36
Connexor 0.12 0.16 0.24 0.26 0.25 0.43
LG (null) 0.02 0.06 0.10 0.09 0.11 0.18
LG (invalid) 0.78 0.67 0.56 0.62 0.66 0.53
GLEU 1.07 0.32 -0.96 0.28 -0.06 -2.48
Human 0.93 0.67 0.44 0.39 0.44 0.31
Table 6: Mean normalised scores per sentence type
highly even than real sentences (in bold). These
are the two methods that use the Viterbi-style algo-
rithm, suggesting that this probability maximisation
has fooled the Collins parser. The pairwise correla-
tion between judges was around the same on average
as in Section 3.3, but with wider variation (Table 7).
The main results, determining the correlation of the
various parser metrics plus GLEU against the new
data, are in Table 8. This confirms the very vari-
able performance of the Collins parser, which has
dropped significantly. GLEU performs quite consis-
tently here, this time a little behind the Link Gram-
mar (nulled tokens) result, but still with a better
correlation with human judgement than at least two
Statistic Corr.
Maximum correlation 0.9048
Minimum correlation 0.3318
Mean correlation 0.7250
Standard deviation 0.0980
Table 7: Data on correlation between humans
Metric Corr.
Collins Parser 0.1898
Connexor -0.3632
Link-Grammar Nulled Tokens -0.4803
Link Grammar Invalid Parses 0.1774
GLEU 0.4738
Table 8: Correlation between parsers and human
evaluators on new human trial data
Metric Corr.
Collins Parser 0.2313
Connexor -0.2042
Link-Grammar Nulled Tokens -0.1289
Link Grammar Invalid Parses -0.0084
GLEU 0.4312
Table 9: Correlation between parsers and human
evaluators on all human trial data
judges with each other. (Note also that the GLEU
SVM was not retrained on the new sentence types.)
Looking at all the data together, however, is where
GLEU particularly displays its consistency. Aggre-
gating the old human trial data (Section 3.3) and the
new data, and determining correlations against the
metrics, we get the data in Table 9. Again the SVM?s
performance is consistent, but is now almost twice
as high as its nearest alternative, Collins.
5.3 Discussion
In general, there is at least one parser that correlates
quite well with the human judges for each sentence
type. With well-structured sentences, the probabilis-
tic Collins parser performs best; on sentences that
are generated by a poor probabilistic model lead-
ing to poor structure, Link Grammar (nulled tokens)
performs best. This supports the use of a machine
learner taking as features outputs from several parser
types; empirically this is confirmed by the large ad-
vantage GLEU has on overall data (Table 9).
The generated text itself from the Viterbi-based gen-
erators as implemented here is quite disappoint-
ing, given an expectation that introducing structure
would make sentences more natural and hence lead
to a range of sentence qualities. In hindsight, this
is not so surprising; in generating the structure tem-
plate, only sequences (over tags) of size 1 were used,
which is perhaps why the human judges deemed
them fairly close to sentences generated by the origi-
350
nal method using sequence size 1, the poorest of that
initial data set.
6 Conclusion
In this paper we have investigated a new approach to
evaluating the fluency of individual generated sen-
tences. The notion of what constitutes fluency is
an imprecise one, but trials with human judges have
shown that even if it cannot be exactly defined, or
even articulated by the judges, there is a high level
of agreement about what is fluent and what is not.
Given this data, metrics derived from parser out-
puts have been found useful for measuring fluency,
correlating up to moderately well with these human
judgements. A better approach is to combine these
in a machine learner, as in our SVM GLEU, which
outperforms individual parser metrics. Interestingly,
we have found that the parser metrics can be fooled
by the method of sentence generation; GLEU, how-
ever, gives a consistent estimate of fluency regard-
less of generation type; and, across all types of gen-
erated sentences examined in this paper, is superior
to individual parser metrics by a large margin.
This all suggests that the approach has promise, but
it needs to be developed further for pratical use. The
SVM presented in this paper has only four features;
more features, and in particular a wider range of
parsers, should raise correlations. In terms of the
data, we looked only at sentences generated with
several parameters fixed, such as sentence length,
due to our limited pool of judges. In future we would
like to examine the space of sentence types more
fully. In particular, we will look at predicting the flu-
ency of near-human quality sentences. More gener-
ally, we would like to look also at how the approach
of this paper would relate to a perplexity-based met-
ric; how it compares against BLEU or similar mea-
sures as a predictor of fluency in a context where ref-
erence sentences are available; and whether GLEU
might be useful in applications such as reranking of
candidate sentences in MT.
Acknowledgements
We thank Ben Hutchinson and Mirella Lapata for discussions,
and Srinivas Bangalore for the TAG supertagger. The sec-
ond author acknowledges the support of ARC Discovery Grant
DP0558852.
References
Srinivas Bangalore and Aravind Joshi. 1999. Supertagging:
An approach to almost parsing. Computational Linguistics,
25(2):237?265.
Srinivas Bangalore, Owen Rambow, and Steve Whittaker.
2000. Evaluation metrics for generation. In Proceedings of the
First International Natural Language Generation Conference
(INLG2000), Mitzpe Ramon, Israel.
E. Bard, D. Robertson, and A. Sorace. 1996. Magnitude esti-
mation and linguistic acceptability. Language, 72(1):32?68.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn.
2006. Re-evaluating the Role of Bleu in Machine Translation
Research. In Proceedings of EACL, pages 249?256.
Jose? Coch. 1996. Evaluating and comparing three text-
production strategies. In Proceedings of the 16th International
Conference on Computational Linguistics (COLING?96), pages
249?254.
J. Cohen. 1988. Statistical power analysis for the behavioral
sciences. Erlbaum, Hillsdale, NJ, US.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of Penn-
sylvania.
Dennis Grinberg, John Lafferty, and Daniel Sleator. 1995. A
robus parsing algorithm for link grammars. In Proceedings of
the Fourth International Workshop on Parsing Technologies.
Thorsten Joachims. 1999. Making Large-Scale SVM Learning
Practical. MIT Press.
Daniel Jurafsky and James Martin. 2000. Speech and Lan-
guage Processing: An Introduction to Natural Languge Pro-
cessing, Computational Linguistics, and Speech Recognition.
Prentice-Hall.
Alex Kulesza and Stuart Shieber. 2004. A learning approach to
improving sentence-level MT evaluation. In Proceedings of the
10th International Conference on Theoretical and Methodolog-
ical Issues in Machine Translation, Baltimore, MD, US.
Irene Langkilde-Geary. 2002. An empirical verification of cov-
erage and correctness for a general-purpose sentence generator.
In Proceedings of the International Natural Language Genera-
tion Conference (INLG) 2002, pages 17?24.
Shimei Pan and James Shaw. 2004. Segue: A hybrid case-
based surface natural language generator. In Proceedings of
the International Conference on Natural Language Generation
(INLG) 2004, pages 130?140.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. Technical Report RC22176, IBM.
Joseph Turian, Luke Shen, and I. Dan Melamed. 2003. Evalua-
tion of Machine Translation and its evaluation. In Proceedings
of MT Summit IX, pages 23?28.
Stephen Wan, Robert Dale, Mark Dras, and Ce?cile Paris. 2005.
Searching for grammaticality: Propagating dependencies in the
Viterbi algorithm. In Proceedings of the 10th European Natural
Language Processing Wworkshop, Aberdeen, UK.
Michael Witbrock and Vibhu Mittal. 1999. Ultra-
summarization: A statistical approach to generating highly con-
densed non-executive summaries. In Proceedings of the 22nd
International Conference on Research and Development in In-
formation Retrieval (SIGIR?99).
David Zajic, Bonnie Dorr, and Richard Schwartz. 2002. Au-
tomatic headline generation for newspaper stories. In Pro-
ceedings of the ACL-2002 Workshop on Text Summarization
(DUC2002), pages 78?85.
351
Searching for Grammaticality: Propagating Dependencies in the Viterbi
Algorithm
Stephen Wan12 Robert Dale1 Mark Dras1
1Centre for Language Technology
Div. of Information Communication Sciences
Macquarie University
Sydney, Australia
swan,rdale,madras@ics.mq.edu.au
Ce?cile Paris2
2Information and Communication
Technologies
CSIRO
Sydney, Australia
Cecile.Paris@csiro.au
Abstract
In many text-to-text generation scenarios (for in-
stance, summarisation), we encounter human-
authored sentences that could be composed by re-
cycling portions of related sentences to form new
sentences. In this paper, we couch the generation
of such sentences as a search problem. We in-
vestigate a statistical sentence generation method
which recombines words to form new sentences.
We propose an extension to the Viterbi algorithm
designed to improve the grammaticality of gener-
ated sentences. Within a statistical framework, the
extension favours those partially generated strings
with a probable dependency tree structure. Our
preliminary evaluations show that our approach
generates less fragmented text than a bigram base-
line.
1 Introduction
In abstract-like automatic summary generation, we often re-
quire the generation of new and previously unseen summary
sentences given some closely related sentences from a source
text. We refer to these as Non-Verbatim Sentences. These
sentences are used instead of extracted sentences for a variety
of reasons including improved conciseness and coherence.
The need for a mechanism to generate such sentences is
supported by recent work showing that sentence extraction is
not sufficient to account for the scope of written human sum-
maries. Jing and McKeown [1999] found that only 42% of
sentences from summaries of news text were extracted sen-
tences. This is also supported by the work of Knight and
Marcu [2002] (cited by [Daume? III and Marcu, 2004]), which
finds that only 2.7% of human summary sentences are ex-
tracts. In our own work on United Nations Humanitarian
Aid Proposals, we noticed that only 30% of sentences are
extracted from the source document, either verbatim or with
trivial string replacements.
While the figures do vary, it shows that additional mecha-
nisms beyond sentence extraction are needed. In response to
this, our general research problem is one in which given a set
of related sentences, a single summary sentence is produced
by recycling words from the input sentence set.
In this paper, we adopt a statistical technique to allow easy
portability across domains. The Viterbi algorithm [Forney,
1973] is used to search for the best traversal of a network of
words, effectively searching through the sea of possible word
sequences. We modify the algorithm so that it narrows the
search space not only to those sequences with a semblance of
grammaticality (via n-grams), but further still to those that are
grammatical sentences preserving the dependency structure
found in the source material.
Consider the following ungrammatical word sequence typ-
ical of that produced by an n-gram generator, ?The quick
brown fox jumped over the lazy dog slept by the log ?. One
diagnosis of the problem is that the word dog is also used
as the subject of the second verb slept. Ideally, we want to
avoid such sequences since they introduce text fragments, in
this case ?slept by the log?. We could, for example, record
the fact that dog is already governed by the verb jumped, and
thus avoid appending a second governing word slept.
To do so, our extension propagates dependency features
during the search and uses these features to influence the
choice of words suitable for appending to a partially gener-
ated sentence. Dependency relations are derived from shal-
low syntactic dependency structure [Kittredge and Mel?cuk,
1983]. Specifically, we use representations of relations be-
tween a head and modifier, ignoring relationship labels for
the present.
Within the search process, we constrain the choice of fu-
ture words by preferring words that are likely to attach to
the dependency structure of the partially generated sentence.
Thus, sequences with plausible structures are ranked higher.
The remainder of the paper is structured as follows. In Sec-
tion 2, we describe the problem in detail and our approach.
We outline our use of the Viterbi algorithm in Section 3. In
Section 4, we describe how this is extended to cater for de-
pendency features. We compare related research in Section 5.
A preliminary evaluation is presented and discussed in Sec-
tion 6. Finally, we conclude with future work in Section 7.
2 Narrowing the Search Space: A Description
of the Statistical Sentence Generation
Problem
In this work, sentence generation is essentially a search for
the most probable sequence of words, given some source text.
However, this constitutes an enormous space which requires
efficient searching. Whilst reducing a vocabulary to a suit-
able subset narrows this space somewhat, we can use statis-
tical models, representing properties of language, to prune
the search space of word sequences further to those strings
that reflect real language usage. For example, n-gram models
limit the word sequences examined to those that seem gram-
matically correct, at least for small windows of text.
However, n-grams alone often result in sentences that,
whilst near-grammatical, are often just gibberish. When com-
bined with a (word) content selection model, we narrow the
search space even further to those sentences that appear to
make sense. Accordingly, approaches such as Witbrock and
Mittal [1999] and Wan et al [2003] have investigated models
that improve the choice of words in the sentence. Witbrock
and Mittal?s content model chooses words that make good
headlines, whilst that of Wan et al attempts to ensure that,
given a short document like a news article, only words from
sentences of the same subtopic are combined to form a new
sentences. In this paper, we narrow the search space to those
sequences that conserve dependency structures from within
the input text.
Our algorithm extension essentially passes along the long-
distance context of dependency head information of the pre-
ceding word sequence, in order to influence the choice of the
next word appended to the sentence. This dependency struc-
ture is constructed statistically by an O(n) algorithm, which
is folded into the Viterbi algorithm. Thus, the extension is
in an O(n4) algorithm. The use of dependency relations fur-
ther constrains the search space. Competing paths through
the search space are ranked taking into account the proposed
dependency structures of the partially generated word se-
quences. Sentences with probable dependency structures are
ranked higher. To model the probability of a dependency re-
lation, we use the statistical dependency models inspired by
those described in Collins [1996].
3 Using The Viterbi Algorithm for Sentence
Generation
We assume that the reader is familiar with the Viterbi al-
gorithm. The interested reader is referred to Manning and
Schutze [1999] for a more complete description. Here, we
summarise our re-implementation (described in [Wan et al,
2003]) of the Viterbi algorithm for summary sentence gener-
ation, as first introduced by Witbrock and Mittal [1999].
In this work, we begin with a Hidden Markov Model
(HMM) where the nodes (ie, states) of the graph are uniquely
labelled with words from a relevant vocabulary. To obtain a
suitable subset of the vocabulary, words are taken from a set
of related sentences, such as those that might occur in a news
article (as is the case for the original work by Witbrock and
Mittal). In this work, we use the clusters of event related sen-
tences from the Information Fusion work by Barzilay et al
[1999]. The edges between nodes in the HMM are typically
weighted using bigram probabilities extracted from a related
corpus.
The three probabilities of the unmodified Viterbi algorithm
are defined as follows:
Transition Probability (using the Maximum Likelihood Esti-
mate to model bigram probabilities)1:
ptrngram (wi+1|wi) =
count(wi, wi+1)
count(wi)
Emission Probability: (For the purposes of testing the new
transition probability function described in Section 4, this is
set to 1 in this paper):
pem(w) = 1
Path Probability is defined recursively as:
ppath(w0, . . . , wi+1) =
ptrngram (wi+1|wi)? pem(w)? ppath(w0 . . . wi)
The unmodified Viterbi algorithm as outlined here would
generate word sequences just using a bigram model. As noted
above, such sequences will often be ungrammatical.
4 A Mechanism for Propagating Dependency
Features in the Extended Viterbi Algorithm
In our extension, we modify the definition of the Transition
Probability such that not only do we consider bigram prob-
abilities but also dependency-based transition probabilities.
Examining the dependency head of the preceding string then
allows us to consider long-distance context when append-
ing a new word. The algorithm ranks highly those words
with a plausible dependency relation to the preceding string,
with respect to the source text being generated from (or sum-
marised).
However, instead of considering just the likelihood of a
dependency relation between adjacent pairs of words, we can
consider the likelihood of a word attaching to the dependency
tree structure of the partially generated sentence. Specifically,
it is the rightmost root-to-leaf branch that can still be modified
or governed by the appending of a new word to the string.
This rightmost branch is stored as a stack. It is updated and
propagated to the end of the path each time we add a word.
Thus, our extension has two components: Dependency
Transition and Head Stack Reduction. Aside from these mod-
ifications, the Viterbi algorithm remains the same.
In the remaining subsections, we describe in detail how the
dependency relations are computed and how the stack is re-
duced. In Figure 3, we present pseudo-code for the extended
Viterbi algorithm.
4.1 Scoring a Dependency Transition
Dependency Parse Preprocessing of Source Text
The Dependency Transition is simply an additional weight on
the HMM edge. The transition probability is the average of
the two transition weights based on bigrams and dependen-
cies:
ptr(wi+1|w1) =
average(ptrngram (wi+1|w1), ptrdep (wi+1|w1))
Before we begin the generation process, we first use a depen-
dency parser to parse all the sentences from the source text to
1Here the subscripts refer to the fact that this is a transition prob-
ability based on n-grams. We will later propose an alternative using
dependency transitions.
obtain dependency trees. A traversal of each dependency tree
yields all parent-child relationships, and we update an adja-
cency matrix of connectivity accordingly. Because the status
of a word as a head or modifier depends on the word order in
English, we consider relative word positions to determine if
a relation has a forward or backward2 direction. Forward and
backward directional relations are stored in separate matri-
ces. The Forward matrix stores relations in which the head is
to the right of modifier in the sentence. Conversely, the Back-
ward matrix stores relations in the head to left of the modifier.
This distinction is required later in the stack reduction step.
As an example, given the two strings (using characters
in lieu of words) ?d b e a c? and ?b e d c a? and the
corresponding dependency trees:
a
b
d e
c
c
d
b
e
a
we obtain the following adjacency matrices:
Forward (or Right-Direction) Adjacency Matrix
?
?
?
?
0 a b c d e
a 0 1 0 0 0
b 0 0 0 1 0
c 0 0 0 1 0
d 0 1 0 0 0
e 0 0 0 0 0
?
?
?
?
Backward (or Left-Direction) Adjacency Matrix
?
?
?
?
0 a b c d e
a 0 0 1 0 0
b 0 0 0 0 2
c 1 0 0 0 0
d 0 0 0 0 0
e 0 0 0 0 0
?
?
?
?
We refer to the matrices as Adjright and Adjleft respectively.The cell value in each matrix indicates the number of times
word i (that is, the row index) governs word j (that is, the
column index).
Computing the Dependency Transition Probability
We define the Dependency Transition weight as:
ptrdep (wi+1|wi) =
p(Depsym(wi+1, headStack(wi))
where Depsym is the symmetric relation stating that some
dependency relation occurs between a word and any of the
words in the stack, irrespective of which is the head. Intu-
itively, the stack is a compressed representation of the depen-
dency tree corresponding to the preceding words. The prob-
ability indicates how likely it is that the new word can attach
itself to this incrementally built dependency tree, either as a
modifier or a governer. Since the stack is cumulatively passed
on at each point, we need only consider the stack stored at the
preceding word.
This is estimated as follows:
p(Depsym(wi+1, headStack(wi))) =
max
h?headStack(wi)
p(Depsym(wi+1, h))
2These are defined analogously to similar concepts in Combina-
tory Categorial Grammar [Steedman, 2000].
the quick brown fox jumps
jumps over the lazy dog .
[
the
]
[
quick
the
]
[
brown
quick
the
]
[
fox
]
[
jumps
]
[
over
jumps
]
[
the
over
jumps
]
[ lazy
the
over
jumps
]
[
dog
over
jumps
]
Figure 1: A path through a lattice. Although separated on
two lines, it represents a single sequence of words. The stack
(oriented upwards) grows and shrinks as we add words. Note
that the modifiers to dog are popped off before it is pushed on.
Note also that modifiers of existing items on the stack, such
as over are merely pushed on. Words with no connection to
previously seen stack items are also pushed on (eg. quick) in
the hope that a head will be found later.
Here, we assume that a word can only attach to the tree
once at a single node; hence, we find the node that max-
imises the probability of node attachment. The relation-
ship Depsym(a, b) is modelled using a simplified version of
Collins? [1996] dependency model.
Because of the status of word as the head relies on thepreservation of word order, we keep track of the direction-
ality of a relation. For two words a and b where a precedes b
in the generated string,
p(Depsym(a, b)) ?
Adjright(a, b) + Adjleft(b, a)
cnt(co-occur(a, b))
where Adjright and Adjleft are the right and left adjacencymatrices. Recall that row indices are heads and column in-
dices are modifiers.
4.2 Head Stack Reduction
Once we decide that a newly considered path is better than
any other previously considered one, we update the head
stack to represent the extended path. At any point in time,
the stack represents the rightmost root-to-leaf branch of the
dependency tree (for the generated sentence) that can still
be modified or governed by concatenating new words to the
string.3 Within the stack, older words may be modified by
newer words. Our rules for modifying the stack are designed
to cater for a projective4 dependency grammar.
There are three possible alternative outcomes of the reduc-
tion. The first is that the proposed top-of-stack (ToS) has
no dependency relation to any of the existing stack items, in
which case the stack remains unchanged. For the second and
third cases, we check each item on the stack and keep a record
3Note that we can scan through the stack as well as push onto
and pop from the top; this is thus the same type of stack as used in,
for example, Nested Stack Automata.
4That is, if wi depends on wj , all words in between wi and wj
are also dependent on wj .
reduceHeadStack(aNode, aStack) returns aStack
Nodenew ?aNode
Stack ?aStack # duplicate
Nodemax ?NULL
Edgeprob ?0
# Find best chunk
While notEmpty(aStack)
Head ?pop(aStack)
if p(depsym (Nodenew, Head)) > Edgeprob
Nodemax ?Head
Edgeprob ?depsym(Nodenew, Head)
# Keep only best chunk
While top(aStack) 6= Nodemax
pop(aStack)
# Determine new head of existing string
if isReduced(Nodenew,Nodemax)
pop(aStack)
else
push(Nodenew, aStack)
Figure 2: Pseudocode for the Head Stack Reduction operation
only of the best probable dependency between the proposed
ToS and the appropriate stack item. The second outcome,
then, is that the proposed ToS is the head of some item on
the stack. All items up to and including that stack item are
popped off and the proposed ToS is pushed on. The third out-
come is that it modifies some item on the stack. All stackitems up to (but not including) the stack item are popped off
and the proposed ToS is pushed on. The pseudocode is pre-
sented in Figure 2. An example of stack manipulation is pre-
sented in Figure 1. We rely on two external functions. The
first function, depsym/2, has already been presented above.
The second function, isReduced/2, relies on an auxiliary
function returning the probability of one word being governed
by the other, given the relative order of the words. This is in
essence our parsing step, determining which word governs
the other. The function is defined as follows:
isReduced(w1, w2) =
p(isHeadRight(w1, w2)) > p(isHeadLeft(w1, w2))
where w1 precedes w2, and:
p(isHeadRight(w1, w2))
? Adjright(w1, w2)
cnt(hasRelation(w1, w2, wherei(w1) < i(w2)))
and similarly,
p(isHeadLeft(w1, w2))
?
Adjleft(w2, w1)
cnt(hasRelation(w1, w2, wherei(w1) < i(w2)))
where hasRelation/2 is the number of times we see
the two words in a dependency relation, and where i(wi)
returns a word position in the corpus sentence. The
function isReduced/2 makes calls to p(isHeadRight/2)
andp(isHeadLeft/2). It returns true if the first parameter
viterbiSearch(maxLength, stateGraph) returns bestPath
numStates ?getNumStates(stateGraph)
viterbi ?a matrix[numStates+2,maxLength+2]
viterbi[0,0].score ?1.0
for each time step t from 0 to maxLength do
# Termination Condition
if ((viterbi[endState, t].score 6= 0)
AND isAcceptable(endState.headStack))
# Backtrace from endState and return path
# Continue appending words
for each state s from 0 to numStates do
for each transition s? from s
newScore ?
viterbi[s,t].score ? ptr(s?|s) ? pem(s?)
if ((viterbi[s?,t+1].score = 0) OR
(newScore > viterbi[s?, t+1]))
viterbi[s?,t+1].score ?newScore
viterbi[s?,t+1].headStack ?
reduceHeadStack(s?,viterbi[s,t].headStack)
backPointer[s?,t+1] ?s
Backtrace from viterbi[endState,t] and return path
Figure 3: Extended Viterbi Algorithm
is the head of the second, and false otherwise. In the com-
parison, the denominator is constant. We thus need only the
numerator in these auxiliary functions.
Collins? distance heuristics [1996] weight the probability
of a dependency relation between two words based on the
distance between them. We could implement a similar strat-
egy by favouring small reductions in the head stack. Thus a
reduction with a more recent stack item which is closer to the
proposed ToS would be less penalised than an older one.
5 Related Work
There is a wealth of relevant research related to sentence gen-
eration. We focus here on a discussion of related work from
statistical sentence generation and from summarisation.
In recent years, there has been a steady stream of research
in statistical text generation. We focus here on work which
generates sentences from some sentential semantic represen-
tation via a statistical method. For examples of related sta-
tistical sentence generators see Langkilde and Knight [1998]
and Bangalore and Rambow [2000]. These approaches be-
gin with a representation of sentence semantics that closely
resembles that of a dependency tree. This semantic represen-
tation is turned into a word lattice. By ranking all traversals
of this lattice using an n-gram model, the best surface realisa-
tion of the semantic representation is chosen. The system then
searches for the best path through this lattice. Our approach
differs in that we do not start with a semantic representation.
Instead, we paraphrase the original text. We search for the
best word sequence and dependency tree structure concur-
rently.
Research in summarisation has also addressed the prob-
lem of generating non-verbatim sentences; see [Jing and
McKeown, 1999], [Barzilay et al, 1999] and more recently
[Daume? III and Marcu, 2004]. Jing presented a HMM for
learning alignments between summary and source sentences
trained using examples of summary sentences generated by
humans. Daume III also provides a mechanism for sub-
sentential alignment but allows for alignments between multi-
ple sentences. Both these approaches provide models for later
recombining sentence fragments. Our work differs primarily
in granularity. Using words as a basic unit potentially offers
greater flexibility in pseudo-paraphrase generation since we
able to modify the word sequence within the phrase.
It should be noted, however, that a successful execution of
our algorithm is likely to conserve constituent structure (ie. a
coarser granularity) via the use of dependencies, whilst still
making available a flexibility at the word level. Addition-
ally, our use of dependencies allows us to generate not only a
string but a dependency tree for that sentence.
6 Evaluation
In this section, we outline our preliminary evaluation of gram-
maticality in which we compare our dependency based gener-
ation method against a baseline. To study any improvements
in grammaticality, we compare our dependency based gener-
ation method against a baseline consisting of sentences gen-
erated using bigram model.
In the evaluation, we do not use any smoothing algorithms
for dependency counts. For both our approach and the base-
line, Katz?s Back-off smoothing algorithm is used for bigram
probabilities.
For our evaluation cases, we use the Information Fusion
data collected by [Barzilay et al, 1999]. This data is made
up of news articles that have been first grouped by topic,
and then component sentences further clustered by similar-
ity of events. There are 100 sentence clusters and on average
there are 4 sentences per cluster. Each sentence in the cluster
is initially passed through the Connexor dependency parser
(www.connexor.com) to obtain dependency relations. Each
sentence cluster forms an evaluation case in which we gener-
ate a single sentence. Example output and the original text of
the cluster is presented in Figure 4.
To give both our approach and the baseline the greatest
chance of generating a sentence, we obtain our bigrams from
our evaluation cases.5 Aside from this preprocessing to col-
lect input sentence bigrams and dependencies, there is no
training as such. For each evaluation case, both our system
and the baseline method generates a set of answer strings,
from 3 to 40 words in length.
For each generated output of a given sentence length, we
count the number of times the Connexor parser resorts to re-
turning partial parses. This count, albeit a noisy one, is used
as our measure of ungrammaticality. We calculate the aver-
age ungrammaticality score across evaluation cases for each
sentence length.
5Note that this is permissible in this case because we are not
making any claims about the coverage of our model.
Original Text
A military transporter was scheduled to take off in the afternoon from Yokota air base
on the outskirts of Tokyo and fly to Osaka with 37,000 blankets .
Mondale said the United States, which has been flying in blankets and is sending a
team of quake relief experts, was prepared to do more if Japan requested .
United States forces based in Japan will take blankets to help earthquake survivors
Thursday, in the U.S. military?s first disaster relief operation in Japan since it set up
bases here.
Our approach with Dependencies and End of Sentence Check
6: united states forces based in blankets
8: united states which has been flying in blankets
11: a military transporter was prepared to osaka with 37,000 blankets
18: mondale said the afternoon from yokota air base on the united states which has been
flying in blankets
20: mondale said the outskirts of tokyo and is sending a military transporter was pre-
pared to osaka with 37,000 blankets
23: united states forces based in the afternoon from yokota air base on the outskirts of
tokyo and fly to osaka with 37,000 blankets
27: mondale said the afternoon from yokota air base on the outskirts of tokyo and is
sending a military transporter was prepared to osaka with 37,000 blankets
29: united states which has been flying in the afternoon from yokota air base on the
outskirts of tokyo and is sending a team of quake relief operation in blankets
31: united states which has been flying in the afternoon from yokota air base on the out-
skirts of tokyo and is sending a military transporter was prepared to osaka with 37,000
blankets
34: mondale said the afternoon from yokota air base on the united states which has
been flying in the outskirts of tokyo and is sending a military transporter was prepared
to osaka with 37,000 blankets
36: united states which has been flying in japan will take off in the afternoon from
yokota air base on the outskirts of tokyo and is sending a military transporter was pre-
pared to osaka with 37,000 blankets
Figure 4: A cluster of related sentences and sample generated
output from our system. Leftmost numbers indicate sentence
length.
 1
 1.5
 2
 2.5
 3
 3.5
 4
 4.5
 5
 3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38
Un
gr
am
m
at
ica
lity
 S
co
re
Sentence Length
Ungrammaticality Errors across Sentence Lengths
Baseline
System
Figure 5: Ungrammaticality scores for generated output.
Higher scores indicates worse performance.
The results are presented in Figure 5. Our approach almost
always performs better than the baseline, producing less er-
rors per sentence length. Using the Wilcoxon Signed Rank
Text (alpha = 0.5), we found that for sentences of length
greater than 12, the differences were usually significant.
7 Conclusion and Future Work
In this paper, we presented an extension to the Viterbi al-
gorithm that statistically determines dependency structure of
partially generated sentences and selects of words that are
likely to attach to this structure. The resulting sentence is
more grammatical than that generated using a bigram base-
line. In future work, we intend to conduct experiments to see
whether the smoothing approaches chosen are successful in
parsing without introducing spurious dependency relations.
We would also like to re-integrate the emission probability
(that is, the word content selection model). We are also in
the process of developing a measure of consistency. Finally,
we intend to provide a comparison evaluation with Barzilay?s
Information Fusion work.
8 Acknowledgements
This work was funded by the Centre for Language Technol-
ogy at Macquarie University and the CSIRO Information and
Communication Technology Centre. We would like to thank
the research groups of both organisations for useful com-
ments and feedback.
References
[Bangalore and Rambow, 2000] Srinivas Bangalore and
Owen Rambow. Exploiting a probabilistic hierarchical
model for generation. In Proceedings of the 18th Con-
ference on Computational Linguistics (COLING?2000),
July 31 - August 4 2000, Universita?t des Saarlandes,
Saarbru?cken, Germany, 2000.
[Barzilay et al, 1999] Regina Barzilay, Kathleen R. McKe-
own, and Michael Elhadad. Information fusion in the con-
text of multi-document summarization. In Proceedings of
the 37th conference on Association for Computational Lin-
guistics, pages 550?557, Morristown, NJ, USA, 1999. As-
sociation for Computational Linguistics.
[Collins, 1996] Michael John Collins. A new statistical
parser based on bigram lexical dependencies. In Arivind
Joshi and Martha Palmer, editors, Proceedings of the
Thirty-Fourth Annual Meeting of the Association for Com-
putational Linguistics, pages 184?191, San Francisco,
1996. Morgan Kaufmann Publishers.
[Daume? III and Marcu, 2004] Hal Daume? III and Daniel
Marcu. A phrase-based hmm approach to docu-
ment/abstract alignment. In Dekang Lin and Dekai Wu,
editors, Proceedings of EMNLP 2004, pages 119?126,
Barcelona, Spain, July 2004. Association for Computa-
tional Linguistics.
[Forney, 1973] G. David Forney. The viterbi algorithm. Pro-
ceedings of The IEEE, 61(3):268?278, 1973.
[Jing and McKeown, 1999] Hongyan Jing and Kathleen
McKeown. The decomposition of human-written sum-
mary sentences. In Research and Development in Infor-
mation Retrieval, pages 129?136, 1999.
[Kittredge and Mel?cuk, 1983] Richard I. Kittredge and Igor
Mel?cuk. Towards a computable model of meaning-text
relations within a natural sublanguage. In IJCAI, pages
657?659, 1983.
[Knight and Marcu, 2002] Kevin Knight and Daniel Marcu.
Summarization beyond sentence extraction: a probabilis-
tic approach to sentence compression. Artif. Intell.,
139(1):91?107, 2002.
[Langkilde and Knight, 1998] Irene Langkilde and Kevin
Knight. The practical value of N-grams in derivation. In
Eduard Hovy, editor, Proceedings of the Ninth Interna-
tional Workshop on Natural Language Generation, pages
248?255, New Brunswick, New Jersey, 1998. Association
for Computational Linguistics.
[Manning and Schu?tze, 1999] Christopher D. Manning and
Hinrich Schu?tze. Foundations of Statistical Natural Lan-
guage Processing. The MIT Press, Cambridge, Mas-
sachusetts, 1999.
[Steedman, 2000] Mark Steedman. The syntactic process.
MIT Press, Cambridge, MA, USA, 2000.
[Wan et al, 2003] Stephen Wan, Mark Dras, Cecile Paris,
and Robert Dale. Using thematic information in statistical
headline generation. In The Proceedings of the Workshop
on Multilingual Summarization and Question Answering
at ACL 2003, Sapporo, Japan, July 2003.
[Witbrock and Mittal, 1999] Michael J. Witbrock and
Vibhu O. Mittal. Ultra-summarization (poster abstract):
a statistical approach to generating highly condensed
non-extractive summaries. In SIGIR ?99: Proceedings of
the 22nd annual international ACM SIGIR conference on
Research and development in information retrieval, pages
315?316, New York, NY, USA, 1999. ACM Press.
