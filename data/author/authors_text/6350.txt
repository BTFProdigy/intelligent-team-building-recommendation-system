A Simple Approach to Building Ensembles of Naive Bayesian 
Classifiers for Word Sense Disambiguation 
Ted Pedersen  
Depar tment  of Computer  Sc ience 
Un ivers i ty  of M innesota  Du luth  
Du luth ,  MN 55812 USA 
tpederse?d, umn. edu 
Abst ract  
This paper presents a corpus-based approach to 
word sense disambiguation that builds an ensemble 
of Naive Bayesian classifiers, each of which is based 
on lexical features that represent co-occurring words 
in varying sized windows of context. Despite the 
simplicity of this approach, empirical results disam- 
biguating the widely studied nouns line and interest 
show that such an ensemble achieves accuracy rival- 
ing the best previously published results. 
1 Int roduct ion 
Word sense disambiguation is often cast as a prob- 
lem in supervised learning, where a disambiguator is 
induced from a corpus of manually sense-tagged text 
using methods from statistics or machine learning. 
These approaches typically represent the context in 
which each sense-tagged instance of a word occurs 
with a set of linguistically motivated features. A 
learning algorithm induces a representative model 
from these features which is employed as a classifier 
to perform disambiguation. 
This paper presents a corpus-based approach that 
results in high accuracy by combining a number of 
very simple classifiers into an ensemble that per- 
forms disambiguation via a majority vote. This is 
motivated by the observation that enhancing the fea- 
ture set or learning algorithm used in a corpus-based 
approach does not usually improve disambiguation 
accuracy beyond what can be attained with shallow 
lexical features and a simple supervised learning al- 
gorithm. 
For example, a Naive Bayesian classifier (Duda 
and Hart, 1973) is based on a blanket assumption 
about the interactions among features in a sense- 
tagged corpus and does not learn a representative 
model. Despite making such an assumption, this 
proves to be among the most accurate techniques 
in comparative studies of corpus-based word sense 
disambiguation methodologies (e.g., (Leacock et al, 
1993), (Mooney, 1996), (Ng and Lee, 1996), (Pealer- 
sen and Bruce, 1997)). These studies represent the 
context in which an ambiguous word occurs with a 
wide variety of features. However, when the con- 
tribution of each type of feature to overall accuracy 
is analyzed (eg. (Ng and Lee, 1996)), shallow lexi- 
cal features uch as co-occurrences and collocations 
prove to be stronger contributors to accuracy than 
do deeper, linguistically motivated features uch as 
part-of-speech and verb-object relationships. 
It has also been shown that the combined accuracy 
of an ensemble of multiple classifiers is often signifi- 
cantly greater than that of any of the individual clas- 
sifiers that make up the ensemble (e.g., (Dietterich, 
1997)). In natural language processing, ensemble 
techniques have been successfully applied to part-  
of-speech tagging (e.g., (Brill and Wu, 1998)) and 
parsing (e.g., (Henderson and Brill, 1999)). When 
combined with a history of disambiguation success 
using shallow lexical features and Naive Bayesian 
classifiers, these findings suggest hat word sense dis- 
ambiguation might best be improved by combining 
the output of a number of such classifiers into an 
ensemble. 
This paper begins with an introduction to the 
Naive Bayesian classifier. The features used to rep- 
resent the context in which ambiguous words occur 
are presented, followed by the method for selecting 
the classifiers to include in the ensemble. Then, the 
line and interest data is described. Experimental re- 
sults disambiguating these words with an ensemble 
of Naive Bayesian classifiers are shown to rival pre- 
viously published results. This paper closes with a 
discussion of the choices made in formulating this 
methodology and plans for future work. 
2 Na ive  Bayes ian  C lass i f ie rs  
A Naive Bayesian classifier assumes that all the fea- 
ture variables representing a problem are condition- 
ally independent given the value of a classification 
variable. In word sense disambiguation, the context 
in which an ambiguous word occurs is represented by 
the feature variables (F1, F2, . . . ,  F~) and the sense 
of the ambiguous word is represented by the classi- 
fication variable (S). In this paper, all feature vari- 
ables Fi are binary and represent whether or not a 
particular word occurs within some number of words 
to the left or right of an ambiguous word, i.e., a win- 
63
dow of context. For a Naive Bayesian classifier, the 
joint probability of observing a certain combination 
of contextual features with a particular sense is ex- 
pressed as: 
n 
p(F~, F~,..., Fn, S) = p(S) H p(FilS) 
i=1  
The parameters of this model are p(S) and 
p(Fi\]S). The sufficient statistics, i.e., the summaries 
of the data needed for parameter estimation, are the 
frequency counts of the events described by the in- 
terdependent variables (Fi, S). In this paper, these 
counts are the number of sentences in the sense- 
tagged text where the word represented by Fi oc- 
curs within some specified window of context of the 
ambiguous word when it is used in sense S. 
Any parameter that has a value of zero indicates 
that the associated word never occurs with the spec- 
ified sense value. These zero values are smoothed 
by assigning them a very small default probability. 
Once all the parameters have been estimated, the 
model has been trained and can be used as a clas- 
sifier to perform disambiguation by determining the 
most probable sense for an ambiguous word, given 
the context in which it occurs. 
2.1 Representation of Context 
The contextual features used in this paper are bi- 
nary and indicate if a given word occurs within some 
number of words to the left or right of the ambigu- 
ous word. No additional positional information is 
contained in these features; they simply indicate if 
the word occurs within some number of surrounding 
words. 
Punctuation and capitalization are removed from 
the windows of context. All other lexical items are 
included in their original form; no stemming is per- 
formed and non-content words remain. 
This representation of context is a variation on 
the bag-of-words feature set, where a single window 
of context includes words that occur to both the left 
and right of the ambiguous word. An early use of 
this representation is described in (Gale et al, 1992), 
where word sense disambiguation is performed with 
a Naive Bayesian classifier. The work in this pa- 
per differs in that there are two windows of context, 
one representing words that occur to the left of the 
ambiguous word and another for those to the right. 
2.2 Ensembles  of  Naive Bayes ian  Classif iers 
The left and right windows of context have nine dif- 
ferent sizes; 0, 1, 2, 3, 4, 5, 10, 25, and 50 words. 
The first step in the ensemble approach is to train a 
separate Naive Bayesian classifier for each of the 81 
possible combination of left and right window sizes. 
Naive_Bayes (1,r) represents a classifier where the 
model parameters have been estimated based on Ire- 
quency counts of shallow lexical features from two 
windows of context; one including I words to the left 
of the ambiguous word and the other including r 
words to the right. Note that Naive_Bayes (0,0) in- 
cludes no words to the left or right; this classifier acts 
as a majority classifier that assigns every instance of 
an ambiguous word to the most frequent sense in 
the training data. Once the individual classifiers are 
trained they are evaluated using previously held-out 
test data. 
The crucial step in building an ensemble is select- 
ing the classifiers to include as members. The ap- 
proach here is to group the 81 Naive Bayesian clas- 
sifiers into general categories representing the sizes 
of the windows of context. There are three such 
ranges; narrow corresponds to windows 0, 1 and 2 
words wide, medium to windows 3, 4, and 5 words 
wide, and wide to windows 10, 25, and 50 words 
wide. There are nine possible range categories since 
there are separate left and right windows. For exam- 
ple, Naive_Bayes(1,3) belongs to the range category 
(narrow, medium) since it is based on a one word 
window to the left and a three word window to the 
right. The most accurate classifier in each of the 
nine range categories i  selected for inclusion in the 
ensemble. Each of the nine member classifiers votes 
for the most probable sense given the particular con- 
text represented by that classifier; the ensemble dis- 
ambiguates by assigning the sense that receives a 
majority of the votes. 
3 Exper imenta l  Data  
The line data was created by (Leacock et al, 1993) 
by tagging every occurrence of line in the ACL/DCI 
Wall Street Journal corpus and the American Print- 
ing House for the Blind corpus with one of six pos- 
sible WordNet senses. These senses and their fre- 
quency distribution are shown in Table 1. This data 
has since been used in studies by (Mooney, 1996), 
(Towell and Voorhees, 1998), and (Leacock et al, 
1998). In that work, as well as in this paper, a subset 
of the corpus is utilized such that each sense is uni- 
formly distributed; this reduces the accuracy of the 
majority classifier to 17%. The uniform distribution 
is created by randomly sampling 349 sense-tagged 
examples from each sense, resulting in a training cor- 
pus of 2094 sense-tagged sentences. 
The interest data was created by (Bruce and 
Wiebe, 1994) by tagging all occurrences of interest 
in the ACL/DCI Wall Street Journal corpus with 
senses from the Longman Dictionary of Contempo- 
rary English. This data set was subsequently used 
for word sense disambiguation experiments by (Ng 
and Lee, 1996), (Pedersen et al, 1997), and (Peder- 
sen and Bruce, 1997). The previous tudies and this 
paper use the entire 2,368 sense-tagged sentence cor- 
pus in their experiments. The senses and their ire- 
64 
sense count 
product 2218 
written or spoken text 405 
telephone connection 429 
formation of people or things; queue 349 
an artificial division; boundary 376 
a thin, flexible object; cord 371 
total 4148 
Table 1: Distribution of senses for line - the exper- 
iments in this paper and previous work use a uni- 
formly distributed subset of this corpus, where each 
sense occurs 349 times. 
sense count 
money paid for the use of money 1252 
a share in a company or business 500 
readiness to give attention 361 
advantage, advancement or favor 178 
activity that one gives attention to 66 
causing attention to be given to 11 
total 2368 
Table 2: Distribution of senses for interest - the ex- 
periments in this paper and previous work use the 
entire corpus, where each sense occurs the number 
of times shown above. 
quency distribution are shown in Table 2. Unlike 
line, the sense distribution is skewed; the majority 
sense occurs in 53% of the sentences, while the small- 
est minority sense occurs in less than 1%. 
4 Experimental  Resu l ts  
Eighty-one Naive Bayesian classifiers were trained 
and tested with the line and interest data. Five- 
fold cross validation was employed; all of the sense- 
tagged examples for a word were randomly shuffled 
and divided into five equal folds. Four folds were 
used to train the Naive Bayesian classifier while the 
remaining fold was randomly divided into two equal 
sized test sets. The first, devtes t ,  was used to eval- 
uate the individual classifiers for inclusion in the en- 
semble. The second, tes t ,  was used to evaluate the 
accuracy of the ensemble. Thus the training data 
for each word consists of 80% of the available sense- 
tagged text, while each of the test sets contains 10%. 
This process is repeated five times so that each 
fold serves as the source of the test data once. The 
average accuracy of the individual Naive Bayesian 
classifiers across the five folds is reported in Tables 
3 and 4. The standard deviations were between .01 
and .025 and are not shown given their relative con- 
sistency. 
65 
Each classifier is based upon a distinct representa- 
tion of context since each employs a different com- 
bination of right and left window sizes. The size 
and range of the left window of context is indicated 
along the horizontal margin in Tables 3 and 4 while 
the right window size and range is shown along the 
vertical margin. Thus, the boxes that subdivide ach 
table correspond to a particular ange category. The 
classifier that achieves the highest accuracy in each 
range category is included as a member of the ensem- 
ble. In case of a tie, the classifier with the smallest 
total window of context is included in the ensemble. 
The most accurate single classifier for line is 
Naive_Bayes (4,25), which attains accuracy of 84% 
The accuracy of the ensemble created from the most 
accurate classifier in each of the range categories is 
88%. The single most accurate classifier for interest 
is Naive._Bayes(4,1), which attains accuracy of 86% 
while the ensemble approach reaches 89%. The in- 
crease in accuracy achieved by both ensembles over 
the best individual classifier is statistically signifi- 
cant, as judged by McNemar's test with p = .01. 
4.1 Compar i son  to  P rev ious  Resu l ts  
These experiments use the same sense-tagged cor- 
pora for interest and line as previous tudies. Sum- 
maries of previous results in Tables 5 and 6 show 
that the accuracy of the Naive Bayesian ensemble 
is comparable to that of any other approach. How- 
ever, due to variations in experimental methodolo- 
gies, it can not be concluded that the differences 
among the most accurate methods are statistically 
significant. For example, in this work five-fold cross 
validation is employed to assess accuracy while (Ng 
and Lee, 1996) train and test using 100 randomly 
sampled sets of data. Similar differences in train- 
ing and testing methodology exist among the other 
studies. Still, the results in this paper are encourag- 
ing due to the simplicity of the approach. 
4.1.1 In teres t  
The interest data was first studied by (Bruce and 
Wiebe, 1994). They employ a representation of 
context that includes the part-of-speech of the two 
words surrounding interest, a morphological feature 
indicating whether or not interest is singular or plu- 
ral, and the three most statistically significant co- 
occurring words in the sentence with interest, as de- 
termined by a test of independence. These features 
are abbreviated as p-o-s, morph, and co-occur in 
Table 5. A decomposable probabilistic model is in- 
duced from the sense-tagged corpora using a back- 
ward sequential search where candidate models are 
evaluated with the log-likelihood ratio test. The se- 
lected model was used as a probabilistic lassifier on 
a held-out set of test data and achieved accuracy of 
78%. 
The interest data was included in a study by (Ng 
wide 
medium 
narrow 
50 .63 
25 .63 
10 .62 
5 .61 
4 .60 
3 .58 
2 .53 
1 .42 
0 .14 
0 
.73 .80 
.74 .80 
.75 .81 
.75 .80 
.73 .80 
.73 .79 
.71 .79 
.68 .78 
.58 .73 
.82 .83 
.82 .s4 
.82 .83 
.81 .82 
.82 .82 
.82 .83 
.81 .82 
.79 .80 
.77 .79 
.83 .83 
.83 .83 
.83 .83 
.82 .82 
.82 .82 
.83 .82 
.82 .81 
.79 .80 
.79 .79 
.83 .83 
.83 .83 
.83 .84 
.82 .83 
.82 .82 
.81 .82 
.81 .81 
.81 .81 
.79 .80 
1 2 3 4 5 10 25 50 
narrow medium wide 
Table 3: Accuracy of Naive Bayesian classifiers for line evaluated with the devtes t  data. The italicized 
accuracies are associated with the classifiers included in the ensemble, which attained accuracy of 88% when 
evaluated with the tes t  data. 
wide 
medium 
narrow 
50 .74 
25 .73 
10 .75 
5 .73 
4 .72 
3 .70 
2 .66 
1 .63 
0 .53 
0 
.80 .82 
.80 .82 
.82 .84 
.83 .85 
.83 .85 
.84 .86 
.83 .85 
.82 .85 
.72 .77 
1 2 
nar row 
.83 .83 .83 
.83 .83 .83 
? 84 .84 .84 
.86 .85 .85 
.85 .84 .84 
.86 .86 .85 
.82 .80 .81 
.81 .80 .80 
.82 .81 .81 
.83 .81 .81 
.83 .81 .8O 
.83 .81 .80 
.86 .86 .84 .83 .80 .80 
.85 .86 .85 .82 .81 .80 
.78 .79 .77 .77 .76 .75 
3 4 5 10 25 50 
medium wide 
Table 4: Accuracy of Naive Bayesian classifiers for interest evaluated with the devtes t  data. The italicized 
accuracies are associated with the classifiers included in the ensemble, which attained accuracy of 89% when 
evaluated with the tes t  data. 
and Lee, 1996), who represent he context of an 
ambiguous word with the part-of-speech of three 
words to the left and right of interest, a morpho- 
logical feature indicating if interest is singular or 
plural, an unordered set of frequently occurring 
keywords that surround interest, local collocations 
that include interest, and verb-object syntactic re- 
lationships. These features are abbreviated p-o-s, 
morph, co-occur, collocates, and verb-obj in Table 
5. A nearest-neighbor classifier was employed and 
achieved an average accuracy of 87% over repeated 
trials using randomly drawn training and test sets. 
(Pedersen et al, 1997) and (Pedersen and Bruce, 
1997) present studies that utilize the original Bruce 
and Wiebe feature set and include the interest data. 
The first compares a range of probabilistic model 
selection methodologies and finds that none outper- 
form the Naive Bayesian classifier, which attains ac- 
curacy of 74%. The second compares a range of ma- 
chine learning algorithms and finds that a decision 
tree learner (78%) and a Naive Bayesian classifier 
(74%) are most accurate. 
66 
4.1.2 Line 
The line data was first studied by (Leacock et al, 
1993). They evaluate the disambiguation accuracy 
of a Naive Bayesian classifier, a content vector, and 
a neural network. The context of an ambiguous 
word is represented by a bag-of-words where the 
window of context is two sentences wide. This fea- 
ture set is abbreviated as 2 sentence b-o-w in Table 
6. When the Naive Bayesian classifier is evaluated 
words are not stemmed and capitalization remains. 
However, with the content vector and the neural net- 
work words are stemmed and words from a stop-list 
are removed. They report no significant differences 
in accuracy among the three approaches; the Naive 
Bayesian classifier achieved 71% accuracy, the con- 
tent vector 72%, and the neural network 76%. 
The line data was studied again by (Mooney, 
1996), where seven different machine learning 
methodologies are compared. All learning algo- 
rithms represent the context of an ambiguous word 
using the bag-of-words with a two sentence window 
of context. In these experiments words from a stop- 
Naive Bayesian Ensemble 
Ng ~: Lee, 1996 
Bruce & Wiebe, 1994 
Pedersen & Bruce, 1997 
accuracy 
89% 
87% 
78% 
78% 
74% 
method 
ensemble of 9 
nearest neighbor 
model selection 
decision tree 
naive bayes 
feature set 
varying left & right b-o-w 
p-o-s, morph, co-occur 
collocates, verb-obj 
p-o-s, morph, co-occur 
p-o-s, morph, co-occur 
Table 5: Comparison to previous results for interest 
Naive Bayesian Ensemble 
Towell & Voorhess, 1998 
Leacock, Chodor0w, & Miller, 1998 
Leacock, Towell, & Voorhees, 1993 
Mooney, 1996 
accuracy 
88% 
87% 
84% 
76% 
72% 
71% 
72% 
71% 
method 
ensemble oi ~ 9 
feature set 
varying left & right bSo-w 
neural net local ~z topical b-o-w, p-o-s 
naive bayes local & topical b-o-w, p.-o-s 
neural net 2 sentence b-o-w 
content vector 
naive bayes 
naive bayes 2 sentence b-o-w 
perceptron 
Table 6: Comparison to previous results for line 
list are removed, capitalization is ignored, and words 
are stemmed. The two most accurate methods in 
this study proved to be a Naive Bayesian classifier 
(72%) and a perceptron (71%). 
The line data was recently revisited by both (Tow- 
ell and Voorhees, 1998) and (Leacock et al, 1998). 
The former take an ensemble approach where the 
output from two neural networks is combined; one 
network is based on a representation of local con- 
text while the other represents topical context. The 
latter utilize a Naive Bayesian classifier. In both 
cases context is represented by a set of topical and 
local features. The topical features correspond to 
the open-class words that occur in a two sentence 
window of context. The local features occur within a 
window of context hree words to the left and right 
of the ambiguous word and include co-occurrence 
features as well as the part-of-speech of words in 
this window. These features are represented as lo- 
cal & topical b-o-w and p-o-s in Table 6. (Towell 
and Voorhees, 1998) report accuracy of 87% while 
(Leacock et al, 1998) report accuracy of 84%. 
5 D iscuss ion  
The word sense disambiguation e sembles in this pa- 
per have the following characteristics: 
? The members of the ensemble are Naive 
Bayesian classifiers, 
? the context in which an ambiguous word oc- 
curs is represented by co-occurrence f atures 
A7 
extracted from varying sized windows of sur- 
rounding words, 
? member classifiers are selected for the ensembles 
based on their performance relative to others 
with comparable window sizes, and 
? a majority vote of the member classifiers deter- 
mines the outcome of the ensemble. 
Each point is discussed below. 
5.1 Naive Bayesian classifiers 
The Naive Bayesian classifier has emerged as a con- 
sistently strong performer in a wide range of com- 
parative studies of machine learning methodologies. 
A recent survey of such results, as well as pos- 
sible explanations for its success, is presented in 
(Domingos and Pazzani, 1997). A similar finding 
has emerged in word sense disambiguation, where 
a number of comparative studies have all reported 
that no method achieves ignificantly greater accu- 
racy than the Naive Bayesian classifier (e.g., (Lea- 
cock et al, 1993), (Mooney, 1996), (Ng and Lee, 
1996), (Pedersen and Bruce, 1997)). 
In many ensemble approaches the member classi- 
tiers are learned with different algorithms that are 
trained with the same data. For example, an en- 
semble could consist of a decision tree, a neural net- 
work, and a nearest neighbor classifier, all of which 
are learned from exactly the same set of training 
data. This paper takes a different approach, where 
the learning algorithm is the same for all classifiers 
67
but the training data is different. This is motivated 
by the belief that there is more to be gained by vary- 
ing the representation f context han there is from 
using many different learning algorithms on the same 
data. This is especially true in this domain since the 
Naive Bayesian classifier has a history of success and 
since there is no generally agreed upon set of features 
that have been shown to be optimal for word sense 
disambiguation. 
5.2 Co-occur rence  features  
Shallow lexical features uch as co-occurrences and 
collocations are recognized as potent sources of dis- 
ambiguation information. While many other con- 
textual features are often employed, it isn't clear 
that they offer substantial advantages. For exam- 
ple, (Ng and Lee, 1996) report that local collocations 
alone achieve 80% accuracy disambiguating interest, 
while their full set of features result in 87%. Prelim- 
inary experiments for this paper used feature sets 
that included collocates, co-occurrences, part-of-  
speech and grammatical information for surrounding 
words. However, it was clear that no combination of 
features resulted in disambiguation accuracy signifi- 
cantly higher than that achieved with co-occurrence 
features. 
5.3 Select ing ensemble  members  
The most accurate classifier from each of nine pos- 
sible category ranges is selected as a member of 
the ensemble. This is based on preliminary experi- 
ments that showed that member classifiers with sim- 
ilar sized windows of context often result in little or 
no overall improvement in disambiguation accuracy. 
This was expected since slight differences in window 
sizes lead to roughly equivalent representations of 
context and classifiers that have little opportunity 
for collective improvement. For example, an ensem- 
ble was created for interest using the nine classifiers 
in the range category (medium, medium). The ac- 
curacy of this ensemble was 84%, slightly less than 
the most accurate individual classifiers in that range 
which achieved accuracy of 86%. 
Early experiments also revealed that an ensemble 
based on a majority vote of all 81 classifiers per- 
formed rather poorly. The accuracy for interest was 
approximately 81% and line was disambiguated with 
slightly less than 80% accuracy. The lesson taken 
from these results was that an ensemble should con- 
sist of classifiers that represent as differently sized 
windows of context as possible; this reduces the im- 
pact of redundant errors made by classifiers that 
represent very similarly sized windows of context. 
The ultimate success of an ensemble depends on the 
ability to select classifiers that make complementary 
errors. This is discussed in the context of combin- 
ing part-of-speech taggers in (Brill and Wu, 1998). 
They provide a measure for assessing the comple- 
mentarity of errors between two taggers that could 
be adapted for use with larger ensembles such as the 
one discussed here, which has nine members. 
5.4 D isambiguat ion  by major i ty  vote  
In this paper ensemble disambiguation is based on a 
simple majority vote of the nine member classifiers. 
An alternative strategy is to weight each vote by 
the estimated joint probability found by the Naive 
Bayesian classifier. However, a preliminary study 
found that the accuracy of a Naive Bayesian ensem- 
ble using a weighted vote was poor. For interest, 
it resulted in accuracy of 83% while for line it was 
82%. The simple majority vote resulted in accuracy 
of 89% for interest and 88% for line. 
6 Future Work 
A number of issues have arisen in the course of this 
work that merit further investigation. 
The simplicity of the contextual representation 
can lead to large numbers of parameters in the Naive 
Bayesian model when using wide windows of con- 
text. Some combination of stop-lists and stemming 
could reduce the numbers of parameters and thus 
improve the overall quality of the parameter esti- 
mates made from the training data. 
In addition to simple co-occurrence f atures, the 
use of collocation features eems promising. These 
are distinct from co-occurrences in that they are 
words that occur in close proximity to the ambiguous 
word and do so to a degree that is judged statisti- 
cally significant. 
One limitation of the majority vote in this paper 
is that there is no mechanism for dealing with out- 
comes where no sense gets a majority of the votes. 
This did not arise in this study but will certainly 
occur as Naive Bayesian ensembles are applied to 
larger sets of data. 
Finally, further experimentation with the size of 
the windows of context seems warranted. The cur- 
rent formulation is based on a combination of intu- 
ition and empirical study. An algorithm to deter- 
mine optimal windows sizes is currently under de- 
velopment. 
7 Conc lus ions  
This paper shows that word sense disambiguation 
accuracy can be improved by combining a number 
of simple classifiers into an ensemble. A methodol- 
ogy for formulating an ensemble of Naive Bayesian 
classifiers is presented, where each member classifier 
is based on co-occurrence features extracted from 
a different sized window of context. This approach 
was evaluated using the widely studied nouns line 
and interest, which are disambiguated with accuracy 
of 88% and 89%, which rivals the best previously 
published results. 
68
8 Acknowledgments  
This work extends ideas that began in collabora- 
tion with Rebecca Bruce and Janyce Wiebe. Clau- 
dia Leacock and Raymond Mooney provided valu- 
able assistance with the line data. I am indebted to 
an anonymous reviewer who pointed out the impor- 
tance of separate tes t  and devtest data sets. 
A preliminary version of this paper appears in 
(Pedersen, 2000). 
Re ferences  
E. Brill and J. Wu. 1998. Classifier combination for 
improved lexical disambiguation. In Proceedings 
of the 36th Annual Meeting of the Association for 
Computational Linguistics, Montreal. 
R. Bruce and J. Wiebe. 1994. Word-sense disam- 
biguation using decomposable models. In Proceed- 
ings of the 32nd Annual Meeting of the Associ- 
ation for Computational Linguistics, pages 139- 
146. 
T. Dietterich. 1997. Machine--learning research: 
Four current directions. AI magazine, 18(4):97- 
136. 
P. Domingos and M. Pazzani. 1997. On the optimal- 
ity of the simple Bayesian classifier under zero-one 
loss. Machine Learning, 29:103-130. 
R. Duda and P. Hart. 1973. Pattern Classification 
and Scene Analysis. Wiley, New York, NY. 
W. Gale, K. Church, and D. Yarowsky. 1992. A 
method for disambiguating word senses in a large 
corpus. Computers and the Humanities, 26:415- 
439. 
J. Henderson and E. Brill. 1999. Exploiting diver- 
sity in natural anguage processing: Combining 
parsers. In Proceedings of the Fourth Conference 
on Empirical Methods in Natural Language Pro- 
cessing, College Park, MD, June. 
C. Leacock, G. Towell, and E. Voorhees. 1993. 
Corpus-based statistical sense resolution. In Pro- 
ceedings of the ARPA Workshop on Human Lan- 
guage Technology, pages 260-265, March. 
C. Leacock, M. Chodorow, and G. Miller. 1998. Us- 
ing corpus statistics and WordNet relations for 
sense identification. Computational Linguistics, 
24(1):147-165, March. 
R. Mooney. 1996. Comparative experiments on dis- 
ambiguating word senses: An illustration of the 
role of bias in machine learning. In Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing, pages 82-91, May. 
H.T. Ng and H.B. Lee. 1996. Integrating multiple 
knowledge sources to disambiguate word sense: 
An exemplar-based approach. In Proceedings of 
the 34th Annual Meeting of the Society for Com- 
putational Linguistics, pages 40-47. 
T. Pedersen and R. Bruce. 1997. A new supervised 
69 
learning algorithm for word sense disambiguation. 
In Proceedings of the Fourteenth National Con- 
ference on Artificial Intelligence, pages 604-609, 
Providence, RI, July. 
T. Pedersen, R. Bruce, and J. Wiebe. 1997. Sequen- 
tial model selection for word sense disambigua- 
tion. In Proceedings of the Fifth Conference on 
Applied Natural Language Processing, pages 388- 
395, Washington, DC, April. 
T. Pedersen. 2000. An ensemble approach to 
corpus-based word sense disambiguation. I  Pro- 
ceedings of the Conference on Intelligent Text 
Processing and Computational Linguistics, pages 
205-218, Mexico City, February. 
G. Towell and E. Voorhees. 1998. Disambiguating 
highly ambiguous words. Computational Linguis- 
tics, 24(1):125-146, March. 
Selecting the ?Right? Number of Senses
Based on Clustering Criterion Functions
Ted Pedersen and Anagha Kulkarni
Department of Computer Science
University of Minnesota, Duluth
Duluth, MN 55812 USA
{tpederse,kulka020}@d.umn.edu
http://senseclusters.sourceforge.net
Abstract
This paper describes an unsupervised
knowledge?lean methodology for auto-
matically determining the number of
senses in which an ambiguous word is
used in a large corpus. It is based on the
use of global criterion functions that assess
the quality of a clustering solution.
1 Introduction
The goal of word sense discrimination is to cluster
the occurrences of a word in context based on its
underlying meaning. This is often approached as a
problem in unsupervised learning, where the only
information available is a large corpus of text (e.g.,
(Pedersen and Bruce, 1997), (Schu?tze, 1998), (Pu-
randare and Pedersen, 2004)). These methods usu-
ally require that the number of clusters to be dis-
covered (k) be specified ahead of time. However,
in most realistic settings, the value of k is unknown
to the user.
Word sense discrimination seeks to cluster N
contexts, each of which contain a particular tar-
get word, into k clusters, where we would like
the value of k to be automatically selected. Each
context consists of approximately a paragraph of
surrounding text, where the word to be discrimi-
nated (the target word) is found approximately in
the middle of the context. We present a methodol-
ogy that automatically selects an appropriate value
for k. Our strategy is to perform clustering for suc-
cessive values of k, and evaluate the resulting solu-
tions with a criterion function. We select the value
of k that is immediately prior to the point at which
clustering does not improve significantly.
Clustering methods are typically either parti-
tional or agglomerative. The main difference is
that agglomerative methods start with 1 or N clus-
ters and then iteratively arrive at a pre?specified
number (k) of clusters, while partitional methods
start by randomly dividing the contexts into k clus-
ters and then iteratively rearranging the members
of the k clusters until the selected criterion func-
tion is maximized. In this work we have used K-
means clustering, which is a partitional method,
and the H2 criterion function, which is the ratio
of within cluster similarity to between cluster sim-
ilarity. However, our approach can be used with
any clustering algorithm and global criterion func-
tion, meaning that the criterion function should ar-
rive at a single value that assesses the quality of the
clustering for each value of k under consideration.
2 Methodology
In word sense discrimination, the number of con-
texts (N) to cluster is usually very large, and con-
sidering all possible values of k from 1...N would
be inefficient. As the value of k increases, the cri-
terion function will reach a plateau, indicating that
dividing the contexts into more and more clusters
does not improve the quality of the solution. Thus,
we identify an upper bound to k that we refer to as
deltaK by finding the point at which the criterion
function only changes to a small degree as k in-
creases.
According to the H2 criterion function, the
higher its ratio of within cluster similarity to be-
tween cluster similarity, the better the clustering.
A large value indicates that the clusters have high
internal similarity, and are clearly separated from
each other. Intuitively then, one solution to select-
ing k might be to examine the trend of H2 scores,
and look for the smallest k that results in a nearly
maximum H2 value.
However, a graph of H2 values for a clustering
111
of the 4 sense verb serve as shown in Figure 1 (top)
reveals the difficulties of such an approach. There
is a gradual curve in this graph and the maximum
value (plateau) is not reached until k values greater
than 100.
We have developed three methods that take as
input the H2 values generated from 1...deltaK
and automatically determine the ?right? value of
k, based on finding when the changes in H2 as k
increases are no longer significant.
2.1 PK1
The PK1 measure is based on (Mojena, 1977),
which finds clustering solutions for all values of
k from 1..N , and then determines the mean and
standard deviation of the criterion function. Then,
a score is computed for each value of k by sub-
tracting the mean from the criterion function, and
dividing by the standard deviation. We adapt this
technique by using the H2 criterion function, and
limit k from 1...deltaK:
PK1(k) = H2(k)?mean(H2[1...deltaK])std(H2[1...deltaK])
(1)
To select a value of k, a threshold must be set.
Then, as soon as PK1(k) exceeds this threshold,
k-1 is selected as the appropriate number of clus-
ters. We have considered setting this threshold us-
ing the normal distribution based on interpreting
PK1 as a z-score, although Mojena makes it clear
that he views this method as an ?operational rule?
that is not based on any distributional assumptions.
He suggests values of 2.75 to 3.50, but also states
they would need to be adjusted for different data
sets. We have arrived at an empirically determined
value of -0.70, which coincides with the point in
the standard normal distribution where 75% of the
probability mass is associated with values greater
than this.
We observe that the distribution of PK1 scores
tends to change with different data sets, making it
hard to apply a single threshold. The graph of the
PK1 scores shown in Figure 1 illustrates the dif-
ficulty - the slope of these scores is nearly linear,
and as such the threshold (as shown by the hori-
zontal line) is a somewhat arbitrary cutoff.
2.2 PK2
PK2 is similar to (Hartigan, 1975), in that both
take the ratio of a criterion function at k and k-1,
0.001
0.002
0.003
0.004
0.005
0.006
0.007
0.008
0.009
0 50 100 150 200
H2 vs k
s
4r
-2.000
-1.500
-1.000
-0.500
0.000
0.500
1.000
1.500
2 3 4 5 6 7 8 9 1011121314151617
PK1 vs k
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r r
24
0.900
1.000
1.100
1.200
1.300
1.400
1.500
1.600
1.700
1.800
1.900
2 3 4 5 6 7 8 9 1011121314 151617
PK2 vs kr
r
r
r
r
r
r r r r
r r r r r
r
2
4
0.990
0.995
1.000
1.005
1.010
1.015
1.020
1.025
1.030
1.035
1.040
2 3 4 5 6 7 8 9 1011121314 151617
PK3 vs k
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
2
4
Figure 1: Graphs of H2 (top) and PK 1-3 for
serve: Actual number of senses (4) shown as trian-
gle (all), predicted number as square (PK1-3), and
deltaK (17) shown as dot (H2) and upper limit of
k (PK1-3).
112
in order to assess the relative improvement when
increasing the number of clusters.
PK2(k) = H2(k)H2(k ? 1) (2)
When this ratio approaches 1, the clustering has
reached a plateau, and increasing k will have no
benefit. If PK2 is greater than 1, then an addi-
tional cluster improves the solution and we should
increase k. We compute the standard deviation of
PK2 and use that to establish a boundary as to
what it means to be ?close enough? to 1 to consider
that we have reached a plateau. Thus, PK2 will
select k where PK2(k) is the closest to (but not
less than) 1 + standard deviation(PK2[1...deltaK]).
The graph of PK2 in Figure 1 shows an el-
bow that is near the actual number of senses. The
critical region defined by the standard deviation is
shaded, and note that PK2 selected the value of
k that was outside of (but closest to) that region.
This is interpreted as being the last value of k that
resulted in a significant improvement in cluster-
ing quality. Note that here PK2 predicts 3 senses
(square) while in fact there are 4 actual senses (tri-
angle). It is significant that the graph of PK2 pro-
vides a clearer representation of the plateau than
does that of H2.
2.3 PK3
PK3 utilizes three k values, in an attempt to find
a point at which the criterion function increases
and then suddenly decreases. Thus, for a given
value of k we compare its criterion function to the
preceding and following value of k:
PK3(k) = 2?H2(k)H2(k ? 1) + H2(k + 1) (3)
PK3 is close to 1 if the three H2 values form
a line, meaning that they are either ascending, or
they are on the plateau. However, our use of
deltaK eliminates the plateau, so in our case values
of 1 show that k is resulting in consistent improve-
ments to clustering quality, and that we should
continue. When PK3 rises significantly above 1,
we know that k+1 is not climbing as quickly, and
we have reached a point where additional clus-
tering may not be helpful. To select k we chose
the largest value of PK3(k) that is closest to (but
still greater than) the critical region defined by the
standard deviation of PK3. This is the last point
where a significant increase in H2 was observed.
Note that the graph of PK3 in Figure 1 shows the
value of PK3 rising and falling dramatically in
the critical region, suggesting a need for additional
points to make it less localized.
PK3 is similar in spirit to (Salvador and Chan,
2004), which introduces the L measure. This tries
to find the point of maximum curvature in the cri-
terion function graph, by fitting a pair of lines to
the curve (where the intersection of these lines rep-
resents the selected k).
3 Experimental Results
We conducted experiments with words that have 2,
3, 4, and 6 actual senses. We used three words that
had been manually sense tagged, including the 3
sense adjective hard, the 4 sense verb serve, and
the 6 sense noun line. We also created 19 name
conflations where sets of 2, 3, 4, and 6 names of
persons, places, or organizations that are included
in the English GigaWord corpus (and that are typ-
ically unambiguous) are replaced with a single
name to create pseudo or false ambiguities. For
example, we replaced all mentions of Bill Clinton
and Tony Blair with a single name that can refer
to either of them. In general the names we used
in these sets are fairly well known and occur hun-
dreds or even thousands of times.
We clustered each word or name using four dif-
ferent configurations of our clustering approach,
in order to determine how consistent the selected
value of k is in the face of changing feature sets
and context representations. The four configura-
tions are first order feature vectors made up of un-
igrams that occurred 5 or more times, with and
without singular value decomposition, and then
second order feature vectors based on bigrams that
occurred 5 or more times and had a log?likelihood
score of 3.841 or greater, with and without sin-
gular value decomposition. Details on these ap-
proaches can be found in (Purandare and Peder-
sen, 2004).
Thus, in total there are 22 words to be discrim-
inated, 7 with 2 senses, 6 words with 3 senses, 6
with 4 senses, and 3 words with 6 senses. Four
different configurations of clustering are run for
each word, leading to a total of 88 experiments.
The results are shown in Tables 1, 2, and 3. In
these tables, the actual numbers of senses are in
the columns, and the predicted number of senses
are in the rows.
We see that the predicted value of PK1 agreed
113
Table 1: k Predicted by PK1 vs Actual k
2 3 4 6
1 6 6 3 3 18
2 5 5 1 3 14
3 4 1 7 2 14
4 6 5 7 1 19
5 4 2 1 7
6 2 3 3 2 10
7 1 1 2
8 1 1
9 1 1 2
11 1 1
28 24 24 12 88
Table 2: k Predicted by PK2 vs Actual k
2 3 4 6
1 3 1 4
2 8 5 7 6 26
3 8 10 8 2 30
4 4 2 3 9
5 1 3 2 6
6 1 2 1 4
7 2 2
9 1 1 2
10 1 2 3
11 1 1
12 1 1
17 2 2
28 24 24 12 88
with the actual value in 15 cases, whereas PK3
agreed in 17 cases, and PK2 agreed in 22 cases.
We observe that PK1 and PK3 also experienced
considerable confusion, in that their predictions
were in many cases several clusters off of the cor-
rect value. While PK2 made various mistakes,
it was generally closer to the correct values, and
had fewer spurious responses (very large or very
small predictions). We note that the distribution
of PK2?s predictions were most like those of the
actual senses.
4 Conclusions
This paper shows how to use clustering criterion
functions as a means of automatically selecting the
number of senses k in an ambiguous word. We
have found that PK2, a ratio of the criterion func-
tions for the current and previous value of k, is
Table 3: k Predicted by PK3 vs Actual k
2 3 4 6
1 3 4 1 1 9
2 13 9 12 4 38
3 4 3 4 4 15
4 2 2 1 1 6
5 2 1 1 1 5
6 1 2 3 6
7 1 1 1 3
9 1 1
10 1 1
11 2 2
12 1 1
13 1 1
28 24 24 12 88
most effective, although there are many opportu-
nities for future improvements to these techniques.
5 Acknowledgments
This research is supported by a National Science
Foundation Faculty Early CAREER Development
Award (#0092784). All of the experiments in
this paper were carried out with the SenseClusters
package, which is freely available from the URL
on the title page.
References
J. Hartigan. 1975. Clustering Algorithms. Wiley, New
York.
R. Mojena. 1977. Hierarchical grouping methods and
stopping rules: An evaluation. The Computer Jour-
nal, 20(4):359?363.
T. Pedersen and R. Bruce. 1997. Distinguishing word
senses in untagged text. In Proceedings of the Sec-
ond Conference on Empirical Methods in Natural
Language Processing, pages 197?207, Providence,
RI, August.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proceedings of the Conference on
Computational Natural Language Learning, pages
41?48, Boston, MA.
S. Salvador and P. Chan. 2004. Determining the
number of clusters/segments in hierarchical cluster-
ing/segmentation algorithms. In Proceedings of the
16th IEEE International Conference on Tools with
AI, pages 576?584.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
114
Last Words
Empiricism Is Not a Matter of Faith
Ted Pedersen?
University of Minnesota, Duluth
1. The Sad Tale of the Zigglebottom Tagger
?Hurrah, this is it!? you exclaim as you set down the most recent issue of Computational
Linguistics. ?This Zigglebottom Tagger is exactly what I need!? A gleeful smile crosses
your face as you imagine how your system will improve once you replace your tagger
from graduate school with the clearly superior Zigglebottom method. You rub your
hands together and page through the article looking for a way to obtain the tagger,
but nothing is mentioned. That doesn?t dampen your enthusiasm, so you search the
Web, but still nothing turns up. You persist though; those 17 pages of statistically
significant results really are impressive. So you e-mail Zigglebottom asking for the
tagger.
Some days, or perhaps weeks, later, you get a hesitant reply saying: ?We?re planning
to release a demo version soon, stay tuned . . . ? Or perhaps: ?We don?t normally do this,
but we can send you a copy (informally) once we clean it up a bit . . . ? Or maybe: ?We
can?t actually give you the tagger, but you should be able to re-implement it from the
article. Just let us know if you have any questions . . . ?
Still having faith, and lacking any better alternative, you decide to re-implement the
Zigglebottom Tagger. Despite three months of on-and-off effort, the end result provides
just the same accuracy as your old tagger, which is nowhere near that reported in the
article. Feeling sheepish, you conclude you must have misunderstood something, or
maybe there?s a small detail missing from the article. So you contact Zigglebottom again
and explain your predicament. He eventually responds: ?We?ll look into this right away
and get back to you . . . ?
A year passes. You have the good fortune to bump into Zigglebottom at the Annual
Meeting of the Association for Computational Linguistics (ACL). You angle for a seat
next to him during a night out, and you buy him a few beers before you politely
resume your quest for the tagger. Finally, he confesses rather glumly: ?My student
Pifflewhap was the one who did the implementation and ran the experiments, and if
he?d only respond to my e-mail I could ask him to tell you how to get it working, but
he?s graduated now and is apparently too busy to reply.?
After a fewmore beers, Zigglebottom finally agrees to give you the tagger: ?I?ll send
you the version of the code I have, no promises though!? And true to his word, what he
sends is incomplete and undocumented. It doesn?t compile easily, and it?s engineered
so that a jumble of programs must be run in an undisclosed kabalistic sequence known
only to (perhaps) the elusive Pifflewhap. You try your best to make it work every now
? Department of Computer Science, 1114 Kirby Drive, University of Minnesota, Duluth, MN 55812, USA.
E-mail: tpederse@d.umn.edu.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 3
and then for a few months, but eventually you give up, and go back to using the same
old tagger you used before.
2. The Paradox of Faith-Based Empiricism
The tale of the Zigglebottom Tagger is one of disappointment, not just for you but also
for Zigglebottom himself. While his work achieved publication, it must gnaw at his
scientific conscience that he can?t reproduce his own results. The fact that you can?t
reproduce those results either raises questions, but those are resolved with a shrug of
your shoulders and by giving the benefit of the doubt to Zigglebottom. He?s not a fraud;
there?s just some crucial detail that is neither recorded in the article nor in the software,
which can?t be installed and run in any case.
The problem here is not the Zigglebottom article; as a community we accept that
our publications don?t provide enough space to describe our elaborate 21st century
empirical methods in sufficient detail to allow for re-implementation and reproduction
of results. This is true despite the generous page allowances in Computational Linguistics
and even more so in our much more constrained conference proceedings.
What?s really missing is the software that produced the results that convinced the
reviewers the article should be published. This is particularly troubling given the highly
empirical nature of the work reported in so many of our publications. We publish
page after page of experimental results where apparently small differences determine
the perceived value of the work. In this climate, convenient reproduction of results
establishes a vital connection between authors and readers.
Our community expects published papers to be rigorously reviewed and made
available via open access as soon as possible (e.g., via the ACL Anthology1). We expect
the supporting corpora and lexical resources will be made available even if at some cost
(e.g., via the Linguistic Data Consortium2). Yet, we do not have the same expectations
regarding our software. While we have table after table of results to pore over, we
usually don?t have access to the software that would allow us to reproduce those results.
This cuts to the core of whether we are engaged in science, engineering, or theology:
Scientists reproduce results; engineers build impressive and enduring artifacts; and
theologians muse about what they believe but can?t see or prove.
Before you judge the analogy with theology as being too harsh, conduct the follow-
ing experiment. Randomly select one of your own publications from a year or two ago
and think about what would be involved in reproducing the results. How long would
it take, assuming you would be able to do it? If you can?t reproduce those results, why
do you believe them? Why should your readers?
Our inability to reproduce results leads to a debilitating paradox, where we as
reviewers and readers accept highly empirical results on faith. We do this routinely,
to the point where we seem to have given up on the idea of being able to reproduce
results. This is the natural consequence of faith-based empiricism, and the only way to
fight that movement is with a little bit of heresy. Let?s not accept large tables of empirical
results on faith, let?s insist that we be able to reproduce them exactly and conveniently.
Let?s insist that we are scientists first and foremost, and agree that this means that we
must be able to reproduce each other?s results.
1 www.aclweb.org/anthology/.
2 www.ldc.upenn.edu/.
466
Pedersen Empiricism Is Not a Matter of Faith
3. A Heretic?s Guide to Reproducibility
In many cases the failure to release software that allows results to be reproduced is
not a conscious decision, but rather unintentional fallout from how we manage projects
and set priorities in guiding our careers. What follows are a few simple ideas that any
researcher can adopt to make it much easier (and more likely) to produce software that
can not only be released but that will allow users to reproduce results with minimal
effort. As more of us use and release such software, our expectations as a community
will rise, and we?ll eventually see software releases as a natural part of the publication
process, much as we now view data sharing.
3.1 Release Early, Release Often
The single greatest barrier to releasing software is that we don?t think about doing it
early enough. It?s only when we get that first e-mail asking for the implementation of a
method discussed in Computational Linguistics that the issue arises, and by then it?s too
late. At that point the task of converting our code into a well-documented and easy to
use package is often nearly impossible.
Beyond difficulties caused by poor documentation, the passage of time, and
turnover in project members, there can even be legal concerns. When projects do not
plan to release software, it?s often the case that system development will include stages
based on helter-skelter cutting and pasting of code from other sources. The effect of this
is to erase all traces of the origin of that code and the terms under which it was made
available. Once you have gone down this route, it?s very hard to consider releasing the
resulting software.
However, if you plan from the start to distribute your software, you will inevitably
be guided by considerations that are important to your potential audience. You will
choose licenses, hardware platforms, and programming languages that avoid any obvi-
ous barriers to distribution and use. You will develop an infrastructure of Web services,
software repositories, andmailing lists that will evolve with your project. Youwill avoid
haphazard development methodologies that lead to disorganized and impossible-to-
maintain code. The prospect of having actual external users of your software will
inspire a discipline and orderliness on your development and deployment processes
that will likely result in much better software than if you developed it for internal use
only.
It is true that releasing software that is both usable and reliable requires a strong
hand to guide system development, and that?s a skill that many researchers don?t think
they have. However, it?s really quite simple to develop. All you must do is play the part
of a demanding yet naive client from time to time from the very start of the project.
Insist that the code be easy to install and use and that the results that come from it
be easy to understand and absolutely reproducible. If the project is too large for you
to play this role yourself, assign it to one or more members of your team, and make
sure they play the part as if they are a new user encountering the system for the first
time.
If you do this from the beginning of a project it takes surprisingly little time, and
you end up with much better documentation and software, and a system that can be
easily and conveniently used to reproduce results both by outside users and by yourself
after the passage of some time.
467
Computational Linguistics Volume 34, Number 3
3.2 Measure Your Career in Downloads and Users
Researchers sometimes fall into the trap of seeing software and reproduction of results
as frills, and not essential components in their career development: ?Asmuch as I would
like to, I don?t have the time to produce distributable code. Besides, my promotion will
be based on publications and grants, not software releases . . . ?
This suggests that you can either spend your time creating and releasing software,
or you can spend it writing grant proposals and papers, but not both. This overlooks
a very happy side-effect that comes from creating releasable code?you will be more
efficient in producing newwork of your own since you can easily reproduce and extend
your own results.
There is also a danger that this attitudewill evolve over time into a self-perpetuating
cycle: ?I?ve worked on this for X years, why would I just give it away?? This ignores the
fact that ?giving it away? will make it easier for others to use your work, because if you
don?t make your code available, who is really going to spend years re-implementing
what you did?
Webber (2007) draws attention to the amount of time our community wastes in
writing and reviewing papers that are rejected and eventually abandoned. In a similar
vein, we should all think about the time we cost our community when we don?t release
software and make anyone who is interested in using or validating our work do their
own implementation.
If software is released publicly under one of the standard licenses that protects your
copyright (e.g., the GNU General Public License3 or the Mozilla Public License4) then
there is little danger of your work being misappropriated, and you will build a reservoir
of good will within our community. Most users don?t want to steal from you; they
simply want to use your code to build their own system while giving you all the credit
that is your due. As your software acquires a following, you can use that as a foundation
for offering tutorials andworkshops and othermeans of dissemination that will increase
your visibility in the research community, thereby enhancing the credibility and impact
of the work you have done.
3.3 Ensure Project Survivability By Releasing Software
Released software can allow your project to sustain itself despite turnover in personnel
and the passage of time. There is no greater satisfaction than opening up a software
release that has not been used for a few years and immediately being able to start
producing meaningful results, without having to reverse engineer it or trace through
code line by line. The more time passes, the more you become just like every other
potential user of your software; so, as you are creating it, remember that in a few years
your memory of all the details that now seem so obvious will have faded, and you will
be grateful for a job well done, and that will translate into time saved as you begin to
use that software again.
Imagine meeting with a new project member and being able to say: ?Go download
this software, read the documentation, install it, run the script that reproduces our ACL
experiments, and thenwe can start talking tomorrow about how you are going to extend
that work . . . ? This lowers the bar for entry to your project for new colleagues, and saves
3 www.gnu.org/copyleft/gpl.html.
4 www.mozilla.org/MPL/.
468
Pedersen Empiricism Is Not a Matter of Faith
your existing team considerable time when introducing a new member to the work of
your group.
Although youwon?t spendmuch time thinking about it at the start of a project, your
students will graduate, post-docs will move on, employees will resign, and you might
even find a better job somewhere. Having publicly released software helps clarify what
rights former project members have once they have left a project. This is a painfully
murky area, and it can lead to many misunderstandings and bad feelings that take time
and energy to deal with as they arise.
That confusion can also cause former colleagues to distance themselves from a
project simply because they feel they don?t have the right to participate, and in fact
in some cases they may not even have access to or copies of the very system they spent
all those months or years working on. This difficult situation is absolutely avoided if
you release the software: Your former colleagues will have exactly the same rights as
anyone else. They can remain a part of the community of users, testers, and developers,
and can often provide valuable continuity in a project even if they have moved to a new
project or organization. The same is true for you. Suppose you move from the academic
world to a position in industry: If your project code has already been released prior to
this move, then you can safely continue to use it without fear of losing control of it to
your new employer.
3.4 Make The World A Better Place
Finally, although this viewpoint may seem quaint or naive, a great deal of our research is
funded by public tax dollars, by people who make ten dollars an hour waiting tables or
standing behind a counter in a convenience store for 12 hours at a time. We are fortunate
to do what we do: even if it takes many hours and causes great personal stress, in the
end the work is challenging and satisfying, and compared to how most people in the
world live and work, we are leading charmed and privileged lives.
Although most taxpayers won?t have much interest in reading our papers and run-
ning our code, they ought to have that opportunity. And who knows, maybe when their
children take a Computational Linguistics or Artificial Intelligence class they will run
across a piece of our publicly available code that will cause them to pause and think, and
maybe inspire them to try something new or different, maybe even make them think
about becoming one of our community. It?s not the most likely scenario, but it seems
like we really ought to try to give back as much as we can to the greater public good.
4. What should Computational Linguistics Do?
We seem as a community to have accepted a very curious state of affairs. As reviewers
and readers of Computational Linguistics and the proceedings of ACL conferences, we in-
sist upon extensive, rigorous, and fine-grained evaluations, where the difference in per-
formance between competing methods is sometimes rather small. However, we don?t
expect to be able to reproduce these results or modify these experiments in any way.
With the rise of search engines as a source of linguistic data, we may have even
reached a point where we don?t expect our data to be reproducible due to the arbitrary
results they provide. Kilgarriff (2007) argues, ?Googleology is bad science,? to which
we would simply add ?because it is not reproducible.?
But instead of insisting upon reproducibility, we tell ourselves to think about
the bigger picture, to focus on the ideas and not the software, as those are just ?im-
plementation issues.? This is a debilitating paradox, because results must be supported
469
Computational Linguistics Volume 34, Number 3
experimentally with great precision and detail and are judged according to harsh em-
pirical standards, but we as readers and reviewers are asked to accept that these results
are accurate and reproducible on faith.
If we believe in empirical methods and the value of comparisons and experimental
studies, then we must also believe in having access to the software that produced those
results as a necessary and essential part of the evidentiary process. Without that we are
asked to re-implement methods that are often too complicated and underspecified for
this to be possible, or to accept the reported results as a matter of faith.
There are two courses of action open to us. One is to back away from the very
stringent standards that focus on evaluation and comparisons of empirical results; to
approach things more with a focus on bigger ideas, and less on statistically significant
empirical results. This is not necessarily a bad thing, and might address concerns such
as those raised by Chuch (2005) about very conservative reviewing in our field and the
resulting tendency to prefer incremental improvements.
However, the other path is to accept (and in fact insist) that highly detailed empirical
studies must be reproducible to be credible, and that it is unreasonable to expect that
reproducibility be possible based on the description provided in a publication. Thus,
releasing software that makes it easy to reproduce and modify experiments should be
an essential part of the publication process, to the point where we might one day only
accept for publication articles that are accompanied by working software that allows for
immediate and reliable reproduction of results.
Acknowledgments
I would like to thank Robert Dale for
suggesting this topic, and for his many
helpful comments and suggestions.
References
Chuch, Kenneth. 2005. Reviewing the
reviewers. Computational Linguistics,
31(4):575?578.
Kilgarriff, Adam. 2007. Googleology is bad
science. Computational Linguistics,
33(1):147?151.
Webber, Bonnie. 2007. Breaking news:
Changing attitudes and practices.
Computational Linguistics, 33(4):607?611.
470
A Decision Tree of Bigrams
is an Accurate Predictor of Word Sense
Ted Pedersen
Department of Computer Science
University of Minnesota Duluth
Duluth, MN 55812 USA
tpederse@d.umn.edu
Abstract
This paper presents a corpus-based approach to
word sense disambiguation where a decision tree as-
signs a sense to an ambiguous word based on the
bigrams that occur nearby. This approach is evalu-
ated using the sense-tagged corpora from the 1998
SENSEVAL word sense disambiguation exercise. It
is more accurate than the average results reported
for 30 of 36 words, and is more accurate than the
best results for 19 of 36 words.
1 Introduction
Word sense disambiguation is the process of selecting
the most appropriate meaning for a word, based on
the context in which it occurs. For our purposes it is
assumed that the set of possible meanings, i.e., the
sense inventory, has already been determined. For
example, suppose bill has the following set of possi-
ble meanings: a piece of currency, pending legisla-
tion, or a bird jaw. When used in the context of The
Senate bill is under consideration, a human reader
immediately understands that bill is being used in
the legislative sense. However, a computer program
attempting to perform the same task faces a di?cult
problem since it does not have the benet of innate
common{sense or linguistic knowledge.
Rather than attempting to provide computer pro-
grams with real{world knowledge comparable to
that of humans, natural language processing has
turned to corpus{based methods. These approaches
use techniques from statistics and machine learn-
ing to induce models of language usage from large
samples of text. These models are trained to per-
form particular tasks, usually via supervised learn-
ing. This paper describes an approach where a deci-
sion tree is learned from some number of sentences
where each instance of an ambiguous word has been
manually annotated with a sense{tag that denotes
the most appropriate sense for that context.
Prior to learning, the sense{tagged corpus must be
converted into a more regular form suitable for auto-
matic processing. Each sense{tagged occurrence of
an ambiguous word is converted into a feature vec-
tor, where each feature represents some property of
the surrounding text that is considered to be relevant
to the disambiguation process. Given the exibility
and complexity of human language, there is poten-
tially an innite set of features that could be utilized.
However, in corpus{based approaches features usu-
ally consist of information that can be readily iden-
tied in the text, without relying on extensive exter-
nal knowledge sources. These typically include the
part{of{speech of surrounding words, the presence
of certain key words within some window of context,
and various syntactic properties of the sentence and
the ambiguous word.
The approach in this paper relies upon a feature
set made up of bigrams, two word sequences that
occur in a text. The context in which an ambiguous
word occurs is represented by some number of binary
features that indicate whether or not a particular
bigram has occurred within approximately 50 words
to the left or right of the word being disambiguated.
We take this approach since surface lexical fea-
tures like bigrams, collocations, and co{occurrences
often contribute a great deal to disambiguation ac-
curacy. It is not clear how much disambiguation ac-
curacy is improved through the use of features that
are identied by more complex pre{processing such
as part{of{speech tagging, parsing, or anaphora res-
olution. One of our objectives is to establish a clear
upper bounds on the accuracy of disambiguation us-
ing feature sets that do not impose substantial pre{
processing requirements.
This paper continues with a discussion of our
methods for identifying the bigrams that should be
included in the feature set for learning. Then the
decision tree learning algorithm is described, as are
some benchmark learning algorithms that are in-
cluded for purposes of comparison. The experimen-
tal data is discussed, and then the empirical results
are presented. We close with an analysis of our nd-
ings and a discussion of related work.
2 Building a Feature Set of Bigrams
We have developed an approach to word sense dis-
ambiguation that represents text entirely in terms of
the occurrence of bigrams, which we dene to be two
cat :cat totals
big n
11
= 10 n
12
= 20 n
1+
= 30
:big n
21
= 40 n
22
= 930 n
2+
= 970
totals n
+1
=50 n
+2
=950 n
++
=1000
Figure 1: Representation of Bigram Counts
consecutive words that occur in a text. The distri-
butional characteristics of bigrams are fairly consis-
tent across corpora; a majority of them only occur
one time. Given the sparse and skewed nature of
this data, the statistical methods used to select in-
teresting bigrams must be carefully chosen. We ex-
plore two alternatives, the power divergence family
of goodness of t statistics and the Dice Coe?cient,
an information theoretic measure related to point-
wise Mutual Information.
Figure 1 summarizes the notation for word and
bigram counts used in this paper by way of a 2  2
contingency table. The value of n
11
shows how many
times the bigram big cat occurs in the corpus. The
value of n
12
shows how often bigrams occur where
big is the rst word and cat is not the second. The
counts in n
+1
and n
1+
indicate how often words big
and cat occur as the rst and second words of any
bigram in the corpus. The total number of bigrams
in the corpus is represented by n
++
.
2.1 The Power Divergence Family
(Cressie and Read, 1984) introduce the power diver-
gence family of goodness of t statistics. A number
of well known statistics belong to this family, includ-
ing the likelihood ratio statisticG
2
and Pearson's X
2
statistic.
These measure the divergence of the observed
(n
ij
) and expected (m
ij
) bigram counts, where m
ij
is estimated based on the assumption that the com-
ponent words in the bigram occur together strictly
by chance:
m
ij
=
n
i+
 n
+j
n
++
Given this value, G
2
and X
2
are calculated as:
G
2
= 2
X
i;j
n
ij
 log
n
ij
m
ij
X
2
=
X
i;j
(n
ij
 m
ij
)
2
m
ij
(Dunning, 1993) argues in favor of G
2
over X
2
, es-
pecially when dealing with very sparse and skewed
data distributions. However, (Cressie and Read,
1984) suggest that there are cases where Pearson's
statistic is more reliable than the likelihood ratio and
that one test should not always be preferred over
the other. In light of this, (Pedersen, 1996) presents
Fisher's exact test as an alternative since it does not
rely on the distributional assumptions that underly
both Pearson's test and the likelihood ratio.
Unfortunately it is usually not clear which test
is most appropriate for a particular sample of data.
We take the following approach, based on the obser-
vation that all tests should assign approximately the
same measure of statistical signicance when the bi-
gram counts in the contingency table do not violate
any of the distributional assumptions that underly
the goodness of t statistics. We perform tests us-
ing X
2
, G
2
, and Fisher's exact test for each bigram.
If the resulting measures of statistical signicance
dier, then the distribution of the bigram counts is
causing at least one of the tests to become unreli-
able. When this occurs we rely upon the value from
Fisher's exact test since it makes fewer assumptions
about the underlying distribution of data.
For the experiments in this paper, we identied
the top 100 ranked bigrams that occur more than 5
times in the training corpus associated with a word.
There were no cases where rankings produced by
G
2
, X
2
, and Fisher's exact test disagreed, which is
not altogether surprising given that low frequency
bigrams were excluded. Since all of these statistics
produced the same rankings, hereafter we make no
distinction among them and simply refer to them
generically as the power divergence statistic.
2.2 Dice Coe?cient
The Dice Coe?cient is a descriptive statistic that
provides a measure of association among two words
in a corpus. It is similar to pointwise Mutual Infor-
mation, a widely used measure that was rst intro-
duced for identifying lexical relationships in (Church
and Hanks, 1990). Pointwise Mutual Information
can be dened as follows:
MI(w
1
; w
2
) = log
2
n
11
 n
++
n
+1
 n
1+
where w
1
and w
2
represent the two words that make
up the bigram.
Pointwise Mutual Information quanties how of-
ten two words occur together in a bigram (the nu-
merator) relative to how often they occur overall in
the corpus (the denominator). However, there is a
curious limitation to pointwise Mutual Information.
A bigram w
1
w
2
that occurs n
11
times in the corpus,
and whose component words w
1
and w
2
only occur
as a part of that bigram, will result in increasingly
strong measures of association as the value of n
11
decreases. Thus, the maximum pointwise Mutual
Information in a given corpus will be assigned to bi-
grams that occur one time, and whose component
words never occur outside that bigram. These are
usually not the bigrams that prove most useful for
disambiguation, yet they will dominate a ranked list
as determined by pointwise Mutual Information.
The Dice Coe?cient overcomes this limitation,
and can be dened as follows:
Dice(w
1
; w
2
) =
2  n
11
n
+1
+ n
1+
When n
11
= n
1+
= n
+1
the value of Dice(w
1
; w
2
)
will be 1 for all values n
11
. When the value of n
11
is less than either of the marginal totals (the more
typical case) the rankings produced by the Dice Co-
e?cient are similar to those of Mutual Information.
The relationship between pointwise Mutual Infor-
mation and the Dice Coe?cient is also discussed in
(Smadja et al, 1996).
We have developed the Bigram Statistics Package
to produce ranked lists of bigrams using a range of
tests. This software is written in Perl and is freely
available from www.d.umn.edu/~tpederse.
3 Learning Decision Trees
Decision trees are among the most widely used ma-
chine learning algorithms. They perform a general
to specic search of a feature space, adding the most
informative features to a tree structure as the search
proceeds. The objective is to select a minimal set of
features that e?ciently partitions the feature space
into classes of observations and assemble them into
a tree. In our case, the observations are manually
sense{tagged examples of an ambiguous word in con-
text and the partitions correspond to the dierent
possible senses.
Each feature selected during the search process is
represented by a node in the learned decision tree.
Each node represents a choice point between a num-
ber of dierent possible values for a feature. Learn-
ing continues until all the training examples are ac-
counted for by the decision tree. In general, such
a tree will be overly specic to the training data
and not generalize well to new examples. Therefore
learning is followed by a pruning step where some
nodes are eliminated or reorganized to produce a
tree that can generalize to new circumstances.
Test instances are disambiguated by nding a path
through the learned decision tree from the root to a
leaf node that corresponds with the observed fea-
tures. An instance of an ambiguous word is dis-
ambiguated by passing it through a series of tests,
where each test asks if a particular bigram occurs in
the available window of context.
We also include three benchmark learning algo-
rithms in this study: the majority classier, the de-
cision stump, and the Naive Bayesian classier.
The majority classier assigns the most common
sense in the training data to every instance in the
test data. A decision stump is a one node decision
tree(Holte, 1993) that is created by stopping the de-
cision tree learner after the single most informative
feature is added to the tree.
The Naive Bayesian classier (Duda and Hart,
1973) is based on certain blanket assumptions about
the interactions among features in a corpus. There
is no search of the feature space performed to build
a representative model as is the case with decision
trees. Instead, all features are included in the classi-
er and assumed to be relevant to the task at hand.
There is a further assumption that each feature is
conditionally independent of all other features, given
the sense of the ambiguous word. It is most often
used with a bag of words feature set, where every
word in the training sample is represented by a bi-
nary feature that indicates whether or not it occurs
in the window of context surrounding the ambiguous
word.
We use the Weka (Witten and Frank, 2000) imple-
mentations of the C4.5 decision tree learner (known
as J48), the decision stump, and the Naive Bayesian
classier. Weka is written in Java and is freely avail-
able from www.cs.waikato.ac.nz/~ml.
4 Experimental Data
Our empirical study utilizes the training and test
data from the 1998 SENSEVAL evaluation of word
sense disambiguation systems. Ten teams partic-
ipated in the supervised learning portion of this
event. Additional details about the exercise, in-
cluding the data and results referred to in this
paper, can be found at the SENSEVAL web site
(www.itri.bton.ac.uk/events/senseval/) and in (Kil-
garri and Palmer, 2000).
We included all 36 tasks from SENSEVAL for
which training and test data were provided. Each
task requires that the occurrences of a particular
word in the test data be disambiguated based on
a model learned from the sense{tagged instances in
the training data. Some words were used in multiple
tasks as dierent parts of speech. For example, there
were two tasks associated with bet, one for its use as
a noun and the other as a verb. Thus, there are
36 tasks involving the disambiguation of 29 dierent
words.
The words and part of speech associated with each
task are shown in Table 1 in column 1. Note that
the parts of speech are encoded as n for noun, a
for adjective, v for verb, and p for words where the
part of speech was not provided. The number of
test and training instances for each task are shown
in columns 2 and 4. Each instance consists of the
sentence in which the ambiguous word occurs as well
as one or two surrounding sentences. In general the
total context available for each ambiguous word is
less than 100 surrounding words. The number of
distinct senses in the test data for each task is shown
in column 3.
5 Experimental Method
The following process is repeated for each task. Cap-
italization and punctuation are removed from the
training and test data. Two feature sets are selected
from the training data based on the top 100 ranked
bigrams according to the power divergence statistic
and the Dice Coe?cient. The bigram must have oc-
curred 5 or more times to be included as a feature.
This step lters out a large number of possible bi-
grams and allows the decision tree learner to focus
on a small number of candidate bigrams that are
likely to be helpful in the disambiguation process.
The training and test data are converted to fea-
ture vectors where each feature represents the occur-
rence of one of the bigrams that belong in the feature
set. This representation of the training data is the
actual input to the learning algorithms. Decision
tree and decision stump learning is performed twice,
once using the feature set determined by the power
divergence statistic and again using the feature set
identied by the Dice Coe?cient. The majority clas-
sier simply determines the most frequent sense in
the training data and assigns that to all instances
in the test data. The Naive Bayesian classier is
based on a feature set where every word that occurs
5 or more times in the training data is included as a
feature.
All of these learned models are used to disam-
biguate the test data. The test data is kept separate
until this stage. We employ a ne grained scoring
method, where a word is counted as correctly disam-
biguated only when the assigned sense tag exactly
matches the true sense tag. No partial credit is as-
signed for near misses.
6 Experimental Results
The accuracy attained by each of the learning algo-
rithms is shown in Table 1. Column 5 reports the
accuracy of the majority classier, columns 6 and 7
show the best and average accuracy reported by the
10 participating SENSEVAL teams. The evaluation
at SENSEVAL was based on precision and recall, so
we converted those scores to accuracy by taking their
product. However, the best precision and recall may
have come from dierent teams, so the best accuracy
shown in column 6 may actually be higher than that
of any single participating SENSEVAL system. The
average accuracy in column 7 is the product of the
average precision and recall reported for the par-
ticipating SENSEVAL teams. Column 8 shows the
accuracy of the decision tree using the J48 learning
algorithm and the features identied by a power di-
vergence statistic. Column 10 shows the accuracy
of the decision tree when the Dice Coe?cient selects
the features. Columns 9 and 11 show the accuracy
of the decision stump based on the power divergence
statistic and the Dice Coe?cient respectively. Fi-
nally, column 13 shows the accuracy of the Naive
Bayesian classier based on a bag of words feature
set.
The most accurate method is the decision tree
based on a feature set determined by the power di-
vergence statistic. The last line of Table 1 shows
the win-tie-loss score of the decision tree/power di-
vergence method relative to every other method. A
win shows it was more accurate than the method in
the column, a loss means it was less accurate, and
a tie means it was equally accurate. The decision
tree/power divergence method was more accurate
than the best reported SENSEVAL results for 19
of the 36 tasks, and more accurate for 30 of the 36
tasks when compared to the average reported accu-
racy. The decision stumps also fared well, proving to
be more accurate than the best SENSEVAL results
for 14 of the 36 tasks.
In general the feature sets selected by the power
divergence statistic result in more accurate decision
trees than those selected by the Dice Coe?cient.
The power divergence tests prove to be more reliable
since they account for all possible events surround-
ing two words w
1
and w
2
; when they occur as bigram
w
1
w
2
, when w
1
or w
2
occurs in a bigram without the
other, and when a bigram consists of neither. The
Dice Coe?cient is based strictly on the event where
w
1
and w
2
occur together in a bigram.
There are 6 tasks where the decision tree / power
divergence approach is less accurate than the SEN-
SEVAL average; promise-n, scrap-n, shirt-n, amaze-
v, bitter-p, and sanction-p. The most dramatic dif-
ference occurred with amaze-v, where the SENSE-
VAL average was 92.4% and the decision tree accu-
racy was 58.6%. However, this was an unusual task
where every instance in the test data belonged to a
single sense that was a minority sense in the training
data.
7 Analysis of Experimental Results
The characteristics of the decision trees and deci-
sion stumps learned for each word are shown in
Table 2. Column 1 shows the word and part of
speech. Columns 2, 3, and 4 are based on the feature
set selected by the power divergence statistic while
columns 5, 6, and 7 are based on the Dice Coe?-
cient. Columns 2 and 5 show the node selected to
serve as the decision stump. Columns 3 and 6 show
the number of leaf nodes in the learned decision tree
relative to the number of total nodes. Columns 4
and 7 show the number of bigram features selected
Table 1: Experimental Results
(1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12)
senses j48 stump j48 stump naive
word-pos test in test train maj best avg pow pow dice dice bayes
accident-n 267 8 227 75.3 87.1 79.6 85.0 77.2 83.9 77.2 83.1
behaviour-n 279 3 994 94.3 92.9 90.2 95.7 95.7 95.7 95.7 93.2
bet-n 274 15 106 18.2 50.7 39.6 41.8 34.5 41.8 34.5 39.3
excess-n 186 8 251 1.1 75.9 63.7 65.1 38.7 60.8 38.7 64.5
oat-n 75 12 61 45.3 66.1 45.0 52.0 50.7 52.0 50.7 56.0
giant-n 118 7 355 49.2 67.6 56.6 68.6 59.3 66.1 59.3 70.3
knee-n 251 22 435 48.2 67.4 56.0 71.3 60.2 70.5 60.2 64.1
onion-n 214 4 26 82.7 84.8 75.7 82.7 82.7 82.7 82.7 82.2
promise-n 113 8 845 62.8 75.2 56.9 48.7 63.7 55.8 62.8 78.0
sack-n 82 7 97 50.0 77.1 59.3 80.5 58.5 80.5 58.5 74.4
scrap-n 156 14 27 41.7 51.6 35.1 26.3 16.7 26.3 16.7 26.7
shirt-n 184 8 533 43.5 77.4 59.8 46.7 43.5 51.1 43.5 60.9
amaze-v 70 1 316 0.0 100.0 92.4 58.6 12.9 60.0 12.9 71.4
bet-v 117 9 60 43.2 60.5 44.0 50.8 58.5 52.5 50.8 58.5
bother-v 209 8 294 75.0 59.2 50.7 69.9 55.0 64.6 55.0 62.2
bury-v 201 14 272 38.3 32.7 22.9 48.8 38.3 44.8 38.3 42.3
calculate-v 218 5 249 83.9 85.0 75.5 90.8 88.5 89.9 88.5 80.7
consume-v 186 6 67 39.8 25.2 20.2 36.0 34.9 39.8 34.9 31.7
derive-v 217 6 259 47.9 44.1 36.0 82.5 52.1 82.5 52.1 72.4
oat-v 229 16 183 33.2 30.8 22.5 30.1 22.7 30.1 22.7 56.3
invade-v 207 6 64 40.1 30.9 25.5 28.0 40.1 28.0 40.1 31.0
promise-v 224 6 1160 85.7 82.1 74.6 85.7 84.4 81.7 81.3 85.3
sack-v 178 3 185 97.8 95.6 95.6 97.8 97.8 97.8 97.8 97.2
scrap-v 186 3 30 85.5 80.6 68.6 85.5 85.5 85.5 85.5 82.3
seize-v 259 11 291 21.2 51.0 42.1 52.9 25.1 49.4 25.1 51.7
brilliant-a 229 10 442 45.9 31.7 26.5 55.9 45.9 51.1 45.9 58.1
oating-a 47 5 41 57.4 49.3 27.4 57.4 57.4 57.4 57.4 55.3
generous-a 227 6 307 28.2 37.5 30.9 44.9 32.6 46.3 32.6 48.9
giant-a 97 5 302 94.8 98.0 93.5 95.9 95.9 94.8 94.8 94.8
modest-a 270 9 374 61.5 49.6 44.9 72.2 64.4 73.0 64.4 68.1
slight-a 218 6 385 91.3 92.7 81.4 91.3 91.3 91.3 91.3 91.3
wooden-a 196 4 362 93.9 81.7 71.3 96.9 96.9 96.9 96.9 93.9
band-p 302 29 1326 77.2 81.7 75.9 86.1 84.4 79.8 77.2 83.1
bitter-p 373 14 144 27.0 44.6 39.8 36.4 31.3 36.4 31.3 32.6
sanction-p 431 7 96 57.5 74.8 62.4 57.5 57.5 57.1 57.5 56.8
shake-p 356 36 963 23.6 56.7 47.1 52.2 23.6 50.0 23.6 46.6
win-tie-loss (j48-pow vs. X) 23-7-6 19-0-17 30-0-6 28-9-3 14-15-7 28-9-3 24-1-11
to represent the training data.
This table shows that there is little dierence in
the decision stump nodes selected from feature sets
determined by the power divergence statistics versus
the Dice Coe?cient. This is to be expected since the
top ranked bigrams for each measure are consistent,
and the decision stump node is generally chosen from
among those.
However, there are dierences between the feature
sets selected by the power divergence statistics and
the Dice Coe?cient. These are reected in the dif-
ferent sized trees that are learned based on these
feature sets. The number of leaf nodes and the total
number of nodes for each learned tree is shown in
columns 3 and 6. The number of internal nodes is
simply the dierence between the total nodes and
the leaf nodes. Each leaf node represents the end
of a path through the decision tree that makes a
sense distinction. Since a bigram feature can only
appear once in the decision tree, the number of inter-
Table 2: Decision Tree and Stump Characteristics
power divergence dice coe?cient
(1) (2) (3) (4) (5) (6) (7)
word-pos stump node leaf/total features stump node leaf/total features
accident-n by accident 8/15 101 by accident 12/23 112
behaviour-n best behaviour 2/3 100 best behaviour 2/3 104
bet-n betting shop 20/39 50 betting shop 20/39 50
excess-n in excess 13/25 104 in excess 11/21 102
oat-n the oat 7/13 13 the oat 7/13 13
giant-n the giants 16/31 103 the giants 14/27 78
knee-n knee injury 23/45 102 knee injury 20/39 104
onion-n in the 1/1 7 in the 1/1 7
promise-n promise of 95/189 100 a promising 49/97 107
sack-n the sack 5/9 31 the sack 5/9 31
scrap-n scrap of 7/13 8 scrap of 7/13 8
shirt-n shirt and 38/75 101 shirt and 55/109 101
amaze-v amazed at 11/21 102 amazed at 11/21 102
bet-v i bet 4/7 10 i bet 4/7 10
bother-v be bothered 19/37 101 be bothered 20/39 106
bury-v buried in 28/55 103 buried in 32/63 103
calculate-v calculated to 5/9 103 calculated to 5/9 103
consume-v on the 4/7 20 on the 4/7 20
derive-v derived from 10/19 104 derived from 10/19 104
oat-v oated on 24/47 80 oated on 24/47 80
invade-v to invade 55/109 107 to invade 66/127 108
promise-v promise to 3/5 100 promise you 5/9 106
sack-v return to 1/1 91 return to 1/1 91
scrap-v of the 1/1 7 of the 1/1 7
seize-v to seize 26/51 104 to seize 57/113 104
brilliant-a a brilliant 26/51 101 a brilliant 42/83 103
oating-a in the 7/13 10 in the 7/13 10
generous-a a generous 57/113 103 a generous 56/111 102
giant-a the giant 2/3 102 a giant 1/1 101
modest-a a modest 14/27 101 a modest 10/19 105
slight-a the slightest 2/3 105 the slightest 2/3 105
wooden-a wooden spoon 2/3 104 wooden spoon 2/3 101
band-p band of 14/27 100 the band 21/41 117
bitter-p a bitter 22/43 54 a bitter 22/43 54
sanction-p south africa 12/23 52 south africa 12/23 52
shake-p his head 90/179 100 his head 81/161 105
nal nodes represents the number of bigram features
selected by the decision tree learner.
One of our original hypotheses was that accurate
decision trees of bigrams will include a relatively
small number of features. This was motivated by
the success of decision stumps in performing disam-
biguation based on a single bigram feature. In these
experiments, there were no decision trees that used
all of the bigram features identied by the ltering
step, and for many words the decision tree learner
went on to eliminate most of the candidate features.
This can be seen by comparing the number of inter-
nal nodes with the number of candidate features as
shown in columns 4 or 7.
1
It is also noteworthy that the bigrams ultimately
selected by the decision tree learner for inclusion in
the tree do not always include those bigrams ranked
most highly by the power divergence statistic or the
Dice Coe?cient. This is to be expected, since the
selection of the bigrams from raw text is only mea-
1
For most words the 100 top ranked bigrams form the set
of candidate features presented to the decision tree learner. If
there are ties in the top 100 rankings then there may be more
than 100 features, and if the there were fewer than 100 bi-
grams that occurred more than 5 times then all such bigrams
are included in the feature set.
suring the association between two words, while the
decision tree seeks bigrams that partition instances
of the ambiguous word into into distinct senses. In
particular, the decision tree learner makes decisions
as to what bigram to include as nodes in the tree
using the gain ratio, a measure based on the over-
all Mutual Information between the bigram and a
particular word sense.
Finally, note that the smallest decision trees are
functionally equivalent to our benchmark methods.
A decision tree with 1 leaf node and no internal
nodes (1/1) acts as a majority classier. A deci-
sion tree with 2 leaf nodes and 1 internal node (2/3)
has the structure of a decision stump.
8 Discussion
One of our long-term objectives is to identify a core
set of features that will be useful for disambiguat-
ing a wide class of words using both supervised and
unsupervised methodologies.
We have presented an ensemble approach to word
sense disambiguation (Pedersen, 2000) where mul-
tiple Naive Bayesian classiers, each based on co{
occurrence features from varying sized windows of
context, is shown to perform well on the widely stud-
ied nouns interest and line. While the accuracy of
this approach was as good as any previously pub-
lished results, the learned models were complex and
di?cult to interpret, in eect acting as very accurate
black boxes.
Our experience has been that variations in learn-
ing algorithms are far less signicant contributors
to disambiguation accuracy than are variations in
the feature set. In other words, an informative fea-
ture set will result in accurate disambiguation when
used with a wide range of learning algorithms, but
there is no learning algorithm that can perform well
given an uninformative or misleading set of features.
Therefore, our focus is on developing and discover-
ing feature sets that make distinctions among word
senses. Our learning algorithms must not only pro-
duce accurate models, but they should also shed new
light on the relationships among features and allow
us to continue rening and understanding our fea-
ture sets.
We believe that decision trees meet these criteria.
A wide range of implementations are available, and
they are known to be robust and accurate across a
range of domains. Most important, their structure
is easy to interpret and may provide insights into
the relationships that exist among features and more
general rules of disambiguation.
9 Related Work
Bigrams have been used as features for word sense
disambiguation, particularly in the form of colloca-
tions where the ambiguous word is one component
of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng
and Lee, 1996), (Yarowsky, 1995)). While some of
the bigrams we identify are collocations that include
the word being disambiguated, there is no require-
ment that this be the case.
Decision trees have been used in supervised learn-
ing approaches to word sense disambiguation, and
have fared well in a number of comparative studies
(e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)).
In the former they were used with the bag of word
feature sets and in the latter they were used with a
mixed feature set that included the part-of-speech of
neighboring words, three collocations, and the mor-
phology of the ambiguous word. We believe that
the approach in this paper is the rst time that de-
cision trees based strictly on bigram features have
been employed.
The decision list is a closely related approach that
has also been applied to word sense disambigua-
tion (e.g., (Yarowsky, 1994), (Wilks and Stevenson,
1998), (Yarowsky, 2000)). Rather than building and
traversing a tree to perform disambiguation, a list is
employed. In the general case a decision list may suf-
fer from less fragmentation during learning than de-
cision trees; as a practical matter this means that the
decision list is less likely to be over{trained. How-
ever, we believe that fragmentation also reects on
the feature set used for learning. Ours consists of
at most approximately 100 binary features. This re-
sults in a relatively small feature space that is not
as likely to suer from fragmentation as are larger
spaces.
10 Future Work
There are a number of immediate extensions to this
work. The rst is to ease the requirement that bi-
grams be made up of two consecutive words. Rather,
we will search for bigrams where the component
words may be separated by other words in the text.
The second is to eliminate the ltering step by which
candidate bigrams are selected by a power diver-
gence statistic. Instead, the decision tree learner
would consider all possible bigrams. Despite increas-
ing the danger of fragmentation, this is an interest-
ing issue since the bigrams judged most informative
by the decision tree learner are not always ranked
highly in the ltering step. In particular, we will
determine if the ltering process ever eliminates bi-
grams that could be signicant sources of disam-
biguation information.
In the longer term, we hope to adapt this approach
to unsupervised learning, where disambiguation is
performed without the benet of sense tagged text.
We are optimistic that this is viable, since bigram
features are easy to identify in raw text.
11 Conclusion
This paper shows that the combination of a simple
feature set made up of bigrams and a standard deci-
sion tree learning algorithm results in accurate word
sense disambiguation. The results of this approach
are compared with those from the 1998 SENSEVAL
word sense disambiguation exercise and show that
the bigram based decision tree approach is more ac-
curate than the best SENSEVAL results for 19 of 36
words.
12 Acknowledgments
The Bigram Statistics Package has been imple-
mented by Satanjeev Banerjee, who is supported by
a Grant{in{Aid of Research, Artistry and Scholar-
ship from the O?ce of the Vice President for Re-
search and the Dean of the Graduate School of the
University of Minnesota. We would like to thank
the SENSEVAL organizers for making the data and
results from the 1998 event freely available. The
comments of three anonymous reviewers were very
helpful in preparing the nal version of this paper.
A preliminary version of this paper appears in (Ped-
ersen, 2001).
References
R. Bruce and J. Wiebe. 1994. Word-sense disam-
biguation using decomposable models. In Proceed-
ings of the 32nd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 139{
146.
K. Church and P. Hanks. 1990. Word association
norms, mutual information and lexicography. In
Proceedings of the 28th Annual Meeting of the
Association for Computational Linguistics, pages
76{83.
N. Cressie and T. Read. 1984. Multinomial good-
ness of t tests. Journal of the Royal Statistics
Society Series B, 46:440{464.
R. Duda and P. Hart. 1973. Pattern Classication
and Scene Analysis. Wiley, New York, NY.
T. Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61{74.
R. Holte. 1993. Very simple classication rules per-
form well on most commonly used datasets. Ma-
chine Learning, 11:63{91.
A. Kilgarri and M. Palmer. 2000. Special issue on
SENSEVAL: Evaluating word sense disambigua-
tion programs. Computers and the Humanities,
34(1{2).
R. Mooney. 1996. Comparative experiments on dis-
ambiguating word senses: An illustration of the
role of bias in machine learning. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 82{91, May.
H.T. Ng and H.B. Lee. 1996. Integrating multiple
knowledge sources to disambiguate word sense:
An exemplar-based approach. In Proceedings of
the 34th Annual Meeting of the Association for
Computational Linguistics, pages 40{47.
T. Pedersen and R. Bruce. 1997. A new supervised
learning algorithm for word sense disambiguation.
In Proceedings of the Fourteenth National Con-
ference on Articial Intelligence, pages 604{609,
Providence, RI, July.
T. Pedersen. 1996. Fishing for exactness. In Pro-
ceedings of the South Central SAS User's Group
(SCSUG-96) Conference, pages 188{200, Austin,
TX, October.
T. Pedersen. 2000. A simple approach to building
ensembles of naive bayesian classiers for word
sense disambiguation. In Proceedings of the First
Annual Meeting of the North American Chapter
of the Association for Computational Linguistics,
pages 63{69, Seattle, WA, May.
T. Pedersen. 2001. Lexical semantic ambiguity res-
olution with bigram{based decision trees. In Pro-
ceedings of the Second International Conference
on Intelligent Text Processing and Computational
Linguistics, pages 157{168, Mexico City, Febru-
ary.
F. Smadja, K. McKeown, and V. Hatzivassiloglou.
1996. Translating collocations for bilingual lexi-
cons: A statistical approach. Computational Lin-
guistics, 22(1):1{38.
Y. Wilks and M. Stevenson. 1998. Word
sense disambiguation using optimised combina-
tions of knowledge sources. In Proceedings of
COLING/ACL-98.
I. Witten and E. Frank. 2000. Data Mining - Practi-
cal Machine Learning Tools and Techniques with
Java Implementations. Morgan{Kaufmann, San
Francisco, CA.
D. Yarowsky. 1994. Decision lists for lexical amgi-
guity resolution: Application to accent resotration
in Spanish and French. In Proceedings of the 32nd
Annual Meeting of the Association for Computa-
tional Linguistics.
D. Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 189{
196, Cambridge, MA.
D. Yarowsky. 2000. Hierarchical decision lists for
word sense disambiguation. Computers and the
Humanities, 34(1{2).
SenseClusters - Finding Clusters that Represent Word Senses
Amruta Purandare and Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
{pura0010,tpederse}@d.umn.edu
http://senseclusters.sourceforge.net
Abstract
SenseClusters is a freely available word sense
discrimination system that takes a purely unsu-
pervised clustering approach. It uses no knowl-
edge other than what is available in a raw un-
structured corpus, and clusters instances of a
given target word based only on their mutual
contextual similarities. It is a complete sys-
tem that provides support for feature selec-
tion from large corpora, several different con-
text representation schemes, various clustering
algorithms, and evaluation of the discovered
clusters.
1 Introduction
Most words in natural language have multiple possible
meanings that can only be determined by considering the
context in which they occur. Given instances of a tar-
get word used in a number of different contexts, word
sense discrimination is the process of grouping these in-
stances into clusters that refer to the same word mean-
ing. Approaches to this problem are often based on
the strong contextual hypothesis of (Miller and Charles,
1991), which states that two words are semantically re-
lated to the extent that their contextual representations
are similar. Hence the problem of word sense discrimi-
nation reduces to that of determining which contexts of a
given target word are related or similar.
SenseClusters creates clusters made up of the contexts
in which a given target word occurs. All the instances in
a cluster are contextually similar to each other, making it
more likely that the given target word has been used with
the same meaning in all of those instances. Each instance
normally includes 2 or 3 sentences, one of which contains
the given occurrence of the target word.
SenseClusters was originally intended to discriminate
among word senses. However, the methodology of clus-
tering contextually (and hence semantically) similar in-
stances of text can be used in a variety of natural language
processing tasks such as synonymy identification, text
summarization and document classification. SenseClus-
ters has also been used for applications such as email sort-
ing and automatic ontology construction.
In the sections that follow we will describe the basic
functionality supported by SenseClusters. In general pro-
cessing starts by selecting features from a corpus of text.
Then these features are used to create an appropriate rep-
resentation of the contexts that are to be clustered. There-
after the actual clustering takes place, followed by an op-
tional evaluation stage that compares the discovered clus-
ters to an existing gold standard (if available).
2 Feature Selection
SenseClusters distinguishes among the different contexts
in which a target word occurs based on a set of features
that are identified from raw corpora. SenseClusters uses
the Ngram Statistics Package (Banerjee and Pedersen,
2003), which is able to extract surface lexical features
from large corpora using frequency cutoffs and various
measures of association, including the log?likelihood ra-
tio, Pearson?s Chi?Squared test, Fisher?s Exact test, the
Dice Coefficient, Pointwise Mutual Information, etc.
SenseClusters currently supports the use of unigram,
bigram, and co-occurrence features. Unigrams are indi-
vidual words that occur above a certain frequency cutoff.
These can be effective discriminating features if they are
shared by a minimum of 2 contexts, but not shared by all
contexts. Very common non-content words are excluded
by providing a stop?list.
Bigrams are pairs of words that occur above a given
frequency cutoff and that have a statistically significant
score on a test of association. There may optionally be
intervening words between them that are ignored. Co?
occurrences are bigrams that include the target word. In
effect co?occurrences localize the scope of the unigram
features by selecting only those words that occur within
some number of positions from the target word.
SenseClusters allows for the selection of lexical fea-
tures either from a held out corpus of training data, or
from the same data that is to be clustered, which we refer
to as the test data. Selecting features from separate train-
ing data is particularly useful when the amount of the test
data to be clustered is too small to identify interesting
features.
The following is a summary of some of the options
provided by SenseClusters that make it possible for a user
to customize feature selection to their needs:
?training FILE A held out file of training data to be
used to select features. Otherwise, features will be se-
lected from the data to be clustered.
?token FILE A file containing Perl regular expressions
that defines the tokenization scheme.
?stop FILE A file containing a user provided stoplist.
?feature STRING The feature type to be selected.
Valid options include unigrams, bigrams, and co-
occurrences.
?remove N Ignore features that occur less N times.
?window M Allow up to M-2 words to intervene be-
tween pairs of words when identifying bigram and co-
occurrence features.
?stat STRING The statistical test of association to
identify bigram and co?occurrence features. Valid values
include any of the tests supported by the Ngram Statistics
Package.
3 Context Representation
Once features are selected, SenseClusters creates a vector
for each test instance to be discriminated where each se-
lected feature is represented by an entry/index. Each vec-
tor shows if the feature represented by the corresponding
index occurs or not in the context of the instance (binary
vectors), or how often the feature occurs in the context
(frequency vectors). This is referred to as a first order
context vector, since this representation directly indicates
which features make up the contexts. Here we are follow-
ing (Pedersen and Bruce, 1997), who likewise took this
approach to feature representation.
(Schu?tze, 1998) utilized second order context vectors
that represent the context of a target word to be discrim-
inated by taking the average of the first order vectors as-
sociated with the unigrams that occur in that context. In
SenseClusters we have extended this idea such that these
first order vectors can also be based on co?occurrence or
bigram features from the training corpus.
Both the first and second order context vectors repre-
sent the given instances as vectors in a high dimensional
word space. This approach suffers from two limitations.
First, there may be synonyms represented by separate di-
mensions in the space. Second, and conversely, a single
dimension in the space might be polysemous and associ-
ated with several different underlying concepts. To com-
bat these problems, SenseClusters follows the lead of LSI
(Deerwester et al, 1990) and LSA (Landauer et al, 1998)
and allows for the conversion of word level feature spaces
into a concept level semantic space by carrying out di-
mensionality reduction with Singular Value Decomposi-
tion (SVD). In particular, the package SVDPACK (Berry
et al, 1993) is integrated into SenseClusters to allow for
fast and efficient SVD.
4 Clustering
Clustering can be carried out using either a first or sec-
ond order vector representation of instances. SenseClus-
ters provides a seamless interface to CLUTO, a Cluster-
ing Toolkit (Karypis, 2002), which implements a range
of clustering techniques suitable for both representations,
including repeated bisections, direct, nearest neighbor,
agglomerative, and biased agglomerative.
The first or second order vector representations of con-
texts can be directly clustered using vector space meth-
ods provided in CLUTO. As an alternative, each context
vector can be represented as a point in similarity space
such that the distance between it and any other context
vector reflects the pairwise similarity of the underlying
instances.
SenseClusters provides support for a number of simi-
larity measures, such as simple matching, the cosine, the
Jaccard coefficient, and the Dice coefficient. A similar-
ity matrix created by determining all pairwise measures
of similarity between contexts can be used as an input
to CLUTO?s clustering algorithms, or to SenseClusters?
own agglomerative clustering implementation.
5 Evaluation
SenseClusters produces clusters of instances where each
cluster refers to a particular sense of the given target
word. SenseClusters supports evaluation of these clus-
ters in two ways. First, SenseClusters provides external
evaluation techniques that require knowledge of correct
senses or clusters of the given instances. Second, there
are internal evaluation methods provided by CLUTO that
report the intra-cluster and inter-cluster similarity.
5.1 External Evaluation
When a gold standard clustering of the instances is avail-
able, SenseClusters builds a confusion matrix that shows
S1 S2 S3 S4 S5 S6
C0: 2 3 3 1 99 3 111
C1: 11 5 43 11 11 8 89
C2: 1 19 7 19 208 7 261
C3: 3 15 13 7 37 12 87
C4: 6 5 8 16 143 8 186
C5: 37 18 8 18 186 20 287
C6: 17 7 11 59 14 13 121
C7: 4 9 13 14 163 12 215
C8: 54 20 15 6 16 35 146
C9: 29 51 12 18 11 35 156
164 152 133 169 888 153 1659
Figure 1: Confusion Matrix: Prior to Mapping
S3 S5 S6 S4 S1 S2
C1: 43 11 8 11 11 5 89
C2: 7 208 7 19 1 19 261
C5: 8 186 20 18 37 18 287
C6: 11 14 13 59 17 7 121
C8: 15 16 35 6 54 20 146
C9: 12 11 35 18 29 51 156
C0:* 3 99 3 1 2 3 111
C3:* 13 37 12 7 3 15 87
C4:* 8 143 8 16 6 5 186
C7:* 13 163 12 14 4 9 215
133 888 153 169 164 152 1659
Figure 2: Confusion Matrix: After Mapping
the distribution of the known senses in each of the dis-
covered clusters. A gold standard most typically exists in
the form of sense?tagged text, where each sense tag can
be considered to represent a different cluster that could
be discovered.
In Figure 1, the rows C0 ? C9 represent ten discovered
clusters while the columns represent six gold-standard
senses. The value of cell (i,j) shows the number of in-
stances in the ith discovered cluster that actually belong
to the gold standard sense represented by the jth column.
Note that the bottom row represents the true distribution
of the instances across the senses, while the right hand
column shows the distribution of the discovered clusters.
To carry out evaluation of the discovered clusters,
SenseClusters finds the mapping of gold standard senses
to discovered clusters that would result in maximally ac-
curate discrimination. The problem of assigning senses
to clusters becomes one of re-ordering the columns of the
confusion matrix to maximize the diagonal sum. Thus,
each possible re-ordering shows one assignment scheme
and the sum of the diagonal entries indicates the total
number of instances in the discovered clusters that would
be in their correct sense given that alignment. This corre-
sponds to several well known problems, among them the
Assignment Problem in Operations Research and finding
the maximal matching of a bipartite graph.
Figure 2 shows that cluster C1 maps most closely to
sense S3, while discovered cluster C2 corresponds best
to sense S5, and so forth. The clusters marked with *
are not assigned to any sense. The accuracy of discrim-
ination is simply the sum of the diagonal entries of the
row/column re-ordered confusion matrix divided by the
total number of instances clustered (435/1659 = 26%).
Precision can also be computed by dividing the total num-
ber of correctly discriminated instances by the number
of instances in the six clusters mapped to gold standard
senses (435/1060 = 41%).
5.2 Internal Evaluation
When gold?standard sense tags of the test instances are
not available, SenseClusters relies on CLUTO?s internal
evaluation metrics to report the intra-cluster and inter-
cluster similarity. There is also a graphical component
to CLUTO known as gCLUTO that provides a visualiza-
tion tool. An example of gCLUTO?s output is provided in
Figure 3, which displays a mountain view of the clusters
shown in tables 1 and 2.
This particular visualization illustrates the case when
the gold?standard data has fewer senses (6) than the ac-
tual number requested (10). CLUTO and SenseClusters
both require that the desired number of clusters be speci-
fied prior to clustering. In this example we requested 10,
and the mountain view reveals that there were really only
5 to 7 actual distinct senses. In unsupervised word sense
discrimination, the user will usually not know the actual
number of senses ahead of time. One possible solution
to this problem is to request an arbitrarily large number
of clusters and rely on such visualizations to discover the
true number of senses. In future work, we plan to sup-
port mechanisms that automatically determine the opti-
mal number of clusters/senses to be found.
6 Summary of Unique Features
The following are some of the distinguishing characteris-
tics of SenseClusters.
Feature Types SenseClusters supports the flexible se-
lection of a variety of lexical features, including uni-
grams, bigrams, co-occurrences. These are selected by
the Ngram Statistics Package using statistical tests of as-
sociation or frequency cutoffs.
Context Representations SenseClusters supports two
different representations of context, first order context
vectors as used by (Pedersen and Bruce, 1997) and
second order context vectors as suggested by (Schu?tze,
1998). The former is a direct representation of the in-
stances to be clustered in terms of their features, while
Figure 3: Mountain View from gCLUTO
the latter uses an indirect representation that averages the
first order vector representations of the features that make
up the context.
Clustering SenseClusters seamlessly integrates
CLUTO, a clustering package that provides a wide
range of clustering algorithms and criteria functions.
CLUTO also provides evaluation functions that report
the inter-cluster and intra-cluster similarity, the most
discriminating features characterizing each cluster,
a dendogram tree view, and a 3D mountain view of
clusters. SenseClusters also provides a native imple-
mentation of single link, complete link, and average link
clustering.
Evaluation SenseClusters supports the evaluation of
discovered clusters relative to an existing gold standard.
If sense?tagged text is available, this can be immediately
used as such a gold standard. This evaluation reports pre-
cision and recall relative to the gold standard.
LSA Support SenseClusters provides all of the func-
tionality needed to carry out Latent Semantic Analysis.
LSA converts a word level feature space into a concept
level semantic space that smoothes over differences due
to polysemy and synonymy among words.
Efficiency SenseClusters is optimized to deal with a
large amount of data both in terms of the number of text
instances being clustered and the number of features used
to represent the contexts.
Integration SenseClusters transparently incorporates
several specialized tools, including CLUTO, the Ngram
Statistics Package, and SVDPACK. This provides a wide
number of options and high efficiency at various steps
like feature selection, feature space dimensionality reduc-
tion, clustering and evaluation.
Availability SenseClusters is an open source software
project that is freely distributed under the GNU Public
License (GPL) via http://senseclusters.sourceforge.net/
SenseClusters is an ongoing project, and there are al-
ready a number of published papers based on its use (e.g.,
(Purandare, 2003), (Purandare and Pedersen, 2004)).
7 Acknowledgments
This work has been partially supported by a National Sci-
ence Foundation Faculty Early CAREER Development
award (Grant #0092784).
References
S. Banerjee and T. Pedersen. 2003. The design, imple-
mentation, and use of the Ngram Statistics Package.
In Proceedings of the Fourth International Conference
on Intelligent Text Processing and Computational Lin-
guistics, pages 370?381, Mexico City, February.
M. Berry, T. Do, G. O?Brien, V. Krishna, and S. Varad-
han. 1993. SVDPACK (version 1.0) user?s guide.
Technical Report CS-93-194, University of Tennessee
at Knoxville, Computer Science Department, April.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41:391?407.
G. Karypis. 2002. CLUTO - a clustering toolkit. Tech-
nical Report 02-017, University of Minnesota, Depart-
ment of Computer Science, August.
T.K. Landauer, P.W. Foltz, and D. Laham. 1998. An in-
troduction to latent semantic analysis. Discourse Pro-
cesses, 25:259?284.
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
T. Pedersen and R. Bruce. 1997. Distinguishing word
senses in untagged text. In Proceedings of the Sec-
ond Conference on Empirical Methods in Natural Lan-
guage Processing, pages 197?207, Providence, RI,
August.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proceedings of the Conference on
Computational Natural Language Learning, Boston,
MA.
A. Purandare. 2003. Discriminating among word senses
using mcquitty?s similarity analysis. In Proceedings
of the HLT-NAACL 2003 Student Research Workshop,
pages 19?24, Edmonton, Alberta, Canada, May 27 -
June 1.
H. Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?123.
WordNet::Similarity - Measuring the Relatedness of Concepts
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
tpederse@d.umn.edu
Siddharth Patwardhan
School of Computing
University of Utah
Salt Lake City, UT 84102
sidd@cs.utah.edu
http://search.cpan.org/dist/WordNet-Similarity
http://wn-similarity.sourceforge.net
Jason Michelizzi
Department of Computer Science
University of Minnesota
Duluth, MN 55812
mich0212@d.umn.edu
Abstract
WordNet::Similarity is a freely available soft-
ware package that makes it possible to mea-
sure the semantic similarity and relatedness be-
tween a pair of concepts (or synsets). It pro-
vides six measures of similarity, and three mea-
sures of relatedness, all of which are based on
the lexical database WordNet. These measures
are implemented as Perl modules which take
as input two concepts, and return a numeric
value that represents the degree to which they
are similar or related.
1 Introduction
WordNet::Similarity implements measures of similarity
and relatedness that are all in some way based on the
structure and content of WordNet.
Measures of similarity use information found in an is?
a hierarchy of concepts (or synsets), and quantify how
much concept A is like (or is similar to) concept B. For
example, such a measure might show that an automobile
is more like a boat than it is a tree, due to the fact that
automobile and boat share vehicle as an ancestor in the
WordNet noun hierarchy.
WordNet is particularly well suited for similarity mea-
sures, since it organizes nouns and verbs into hierarchies
of is?a relations. In version 2.0, there are nine separate
noun hierarchies that include 80,000 concepts, and 554
verb hierarchies that are made up of 13,500 concepts.
Is?a relations in WordNet do not cross part of speech
boundaries, so similarity measures are limited to mak-
ing judgments between noun pairs (e.g., cat and dog) and
verb pairs (e.g., run and walk). While WordNet alo in-
cludes adjectives and adverbs, these are not organized
into is?a hierarchies so similarity measures can not be
applied.
However, concepts can be related in many ways be-
yond being similar to each other. For example, a wheel is
a part of a car, night is the opposite of day, snow is made
up of water, a knife is used to cut bread, and so forth. As
such WordNet provides relations beyond is?a, including
has?part, is?made?of, and is?an?attribute?of. In addi-
tion, each concept is defined by a short gloss that may
include an example usage. All of this information can be
brought to bear in creating measures of relatedness. As
a result these measures tend to be more flexible, and al-
low for relatedness values to be assigned across parts of
speech (e.g., the verb murder and the noun gun).
This paper continues with an overview of the mea-
sures supported in WordNet::Similarity, and then pro-
vides a brief description of how the package can be used.
We close with a summary of research that has employed
WordNet::Similarity.
2 Similarity Measures
Three of the six measures of similarity are based on the
information content of the least common subsumer (LCS)
of concepts A and B. Information content is a measure of
the specificity of a concept, and the LCS of concepts A
and B is the most specific concept that is an ancestor of
both A and B. These measures include res (Resnik, 1995),
lin (Lin, 1998), and jcn (Jiang and Conrath, 1997).
The lin and jcn measures augment the information con-
tent of the LCS with the sum of the information content
of concepts A and B themselves. The lin measure scales
the information content of the LCS by this sum, while
jcn takes the difference of this sum and the information
content of the LCS.
The default source for information content for concepts
is the sense?tagged corpus SemCor. However, there are
also utility programs available with WordNet::Similarity
that allow a user to compute information content values
from the Brown Corpus, the Penn Treebank, the British
National Corpus, or any given corpus of raw text.
> similarity.pl --type WordNet::Similarity::lin car#n#2 bus#n#1
car#n#2 bus#n#1 0.530371390319309 # railway car versus motor coach
> similarity.pl --type WordNet::Similarity::lin car#n bus#n
car#n#1 bus#n#1 0.618486790769613 # automobile versus motor coach
> similarity.pl --type WordNet::Similarity::lin --allsenses car#n bus#n#1
car#n#1 bus#n#1 0.618486790769613 # automobile versus motor coach
car#n#2 bus#n#1 0.530371390319309 # railway car versus motor coach
car#n#3 bus#n#1 0.208796988315133 # cable car versus motor coach
Figure 1: Command Line Interface
Three similarity measures are based on path lengths
between a pair of concepts: lch (Leacock and Chodorow,
1998), wup (Wu and Palmer, 1994), and path. lch finds
the shortest path between two concepts, and scales that
value by the maximum path length found in the is?a hi-
erarchy in which they occur. wup finds the depth of the
LCS of the concepts, and then scales that by the sum of
the depths of the individual concepts. The depth of a con-
cept is simply its distance to the root node. The measure
path is a baseline that is equal to the inverse of the short-
est path between two concepts.
WordNet::Similarity supports two hypothetical root
nodes that can be turned on and off. When on, one root
node subsumes all of the noun concepts, and another sub-
sumes all of the verb concepts. This allows for similarity
measures to be applied to any pair of nouns or verbs. If
the hypothetical root nodes are off, then concepts must
be in the same physical hierarchy for a measurement to
be taken.
3 Measures of Relatedness
Measures of relatedness are more general in that they can
be made across part of speech boundaries, and they are
not limited to considering is-a relations. There are three
such measures in the package: hso (Hirst and St-Onge,
1998), lesk (Banerjee and Pedersen, 2003), and vector
(Patwardhan, 2003).
The hso measures classifies relations in WordNet as
having direction, and then establishes the relatedness be-
tween two concepts A and B by finding a path that is
neither too long nor that changes direction too often.
The lesk and vector measures incorporate information
from WordNet glosses. The lesk measure finds overlaps
between the glosses of concepts A and B, as well as con-
cepts that are directly linked to A and B. The vector mea-
sure creates a co?occurrence matrix for each word used in
the WordNet glosses from a given corpus, and then repre-
sents each gloss/concept with a vector that is the average
of these co?occurrence vectors.
4 Using WordNet::Similarity
WordNet::Similarity can be utilized via a command line
interface provided by the utility program similarity.pl.
This allows a user to run the measures interactively. In
addition, there is a web interface that is based on this
utility. WordNet::Similarity can also be embedded within
Perl programs by including it as a module and calling its
methods.
4.1 Command Line
The utility similarity.pl allows a user to measure specific
pairs of concepts when given in word#pos#sense form.
For example, car#n#3 refers to the third WordNet noun
sense of car. It also allows for the specification of all
the possible senses associated with a word or word#pos
combination.
For example, in Figure 1, the first command requests
the value of the lin measure of similarity for the second
noun sense of car (railway car) and the first noun sense of
bus (motor coach). The second command will return the
score of the pair of concepts that have the highest similar-
ity value for the nouns car and bus. In the third command,
the ?allsenses switch causes the similarity measurements
of all the noun senses of car to be calculated relative to
the first noun sense of bus.
4.2 Programming Interface
WordNet::Similarity is implemented with Perl?s object
oriented features. It uses the WordNet::QueryData pack-
age (Rennie, 2000) to create an object representing Word-
Net. There are a number of methods available that allow
for the inclusion of existing measures in Perl source code,
and also for the development of new measures.
When an existing measure is to be used, an object of
that measure must be created via the new() method. Then
the getRelatedness() method can be called for a pair of
word senses, and this will return the relatedness value.
For example, the program in Figure 2 creates an object of
the lin measure, and then finds the similarity between the
#!/usr/bin/perl -w
use WordNet::QueryData; # use interface to WordNet
use WordNet::Similarity::lin; # use Lin measure
$wnObj = new WordNet::QueryData; # create a WordNet object
$linObj = new WordNet::Similarity::lin($wnObj); # create a lin object
$value = $linObj -> getRelatedness (?car#n#1?, ?bus#n#2?); # how similar?
Figure 2: Programming Interface
first sense of the noun car (automobile) and the second
sense of the noun bus (network bus).
WordNet::Similarity enables detailed tracing that
shows a variety of diagnostic information specific to each
of the different kinds of measures. For example, for the
measures that rely on path lengths (lch, wup, path) the
tracing shows all the paths found between the concepts.
Tracing for the information content measures (res, lin,
jcn) includes both the paths between concepts as well as
the least common subsumer. Tracing for the hso measure
shows the actual paths found through WordNet, while
the tracing for lesk shows the gloss overlaps in Word-
Net found for the two concepts and their nearby relatives.
The vector tracing shows the word vectors that are used
to create the gloss vector of a concept.
5 Software Architecture
Similarity.pm is the super class of all modules, and pro-
vides general services used by all of the measures such as
validation of synset identifier input, tracing, and caching
of results. There are four modules that provide all of the
functionality required by any of the supported measures:
PathFinder.pm, ICFinder.pm, DepthFinder.pm, and LCS-
Finder.pm.
PathFinder.pm provides getAllPaths(), which finds all
of the paths and their lengths between two input synsets,
and getShortestPath() which determines the length of the
shortest path between two concepts.
ICFinder.pm includes the method IC(), which gets the
information content value of a synset. probability() and
getFrequency() find the probability and frequency count
of a synset based on whatever corpus has been used to
compute information content. Note that these values are
pre?computed, so these methods are simply reading from
an information content file.
DepthFinder.pm provides methods that read values that
have been pre?computed by the wnDepths.pl utility. This
program finds the depth of every synset in WordNet,
and also shows the is?a hierarchy in which a synset oc-
curs. If a synset has multiple parents, then each possible
depth and home hierarchy is returned. The depth of a
synset is returned by getDepthOfSynset() and getTaxono-
myDepth() provides the maximum depth for a given is?a
hierarchy.
LCSFinder.pm provides methods that find the least
common subsumer of two concepts using three differ-
ent criteria. These are necessary since there is multiple
inheritance of concepts in WordNet, and different LCS
can be selected for a pair of concepts if one or both of
them have multiple parents in an is?a hiearchy. getLCS-
byIC() chooses the LCS for a pair of concepts that has the
highest information content, getLCSbyDepth() selects the
LCS with the greatest depth, and getLCSbyPath() selects
the LCS that results in the shortest path.
6 Related Work
Our work with measures of semantic similarity and relat-
edness began while adapting the Lesk Algorithm for word
sense disambiguation to WordNet (Banerjee and Peder-
sen, 2002). That evolved in a generalized approach to
disambiguation based on semantic relatedness (Patward-
han et al, 2003) that is implemented in the SenseRe-
late package (http://senserelate.sourceforge.net), which
utilizes WordNet::Similarity. The premise behind this al-
gorithm is that the sense of a word can be determined by
finding which of its senses is most related to the possible
senses of its neighbors.
WordNet::Similarity has been used by a number of
other researchers in an interesting array of domains.
(Zhang et al, 2003) use it as a source of semantic fea-
tures for identifying cross?document structural relation-
ships between pairs of sentences found in related docu-
ments. (McCarthy et al, 2004) use it in conjunction with
a thesaurus derived from raw text in order to automati-
cally identify the predominent sense of a word. (Jarmasz
and Szpakowicz, 2003) compares measures of similarity
derived from WordNet and Roget?s Thesaurus. The com-
parisons are based on correlation with human related-
ness values, as well as the TOEFL synonym identification
tasks. (Baldwin et al, 2003) use WordNet::Similarity to
provide an evaluation tool for multiword expressions that
are identified via Latent Semantic Analysis. (Diab, 2003)
combines a number of similarity measures that are then
used as a feature in the disambiguation of verb senses.
7 Availability
WordNet::Similarity is written in Perl and is freely dis-
tributed under the Gnu Public License. It is avail-
able from the Comprehensive Perl Archive Network
(http://search.cpan.org/dist/WordNet-Similarity) and via
SourceForge, an Open Source development platform
(http://wn-similarity.sourceforge.net).
8 Acknowledgements
WordNet::Similarity was preceeded by the distance.pl
program, which was released in June 2002. This was
converted into the object oriented WordNet::Similarity
package, which was first released in April 2003 as ver-
sion 0.03. The most current version as of this writing is
0.07, which was released in March 2004.
The distance.pl program and all versions of Word-
Net::Similarity up to and including 0.06 were designed
and implemented by Siddharth Patwardhan as a part of
his Master?s thesis at the University of Minnesota, Du-
luth. Version 0.07 was designed and implemented by Ja-
son Michelizzi as a part of his Master?s thesis.
The lesk measure in WordNet::Similarity was origi-
nally designed and implemented by Satanjeev Banerjee,
who developed this measure as a part of his Master?s
thesis at the University of Minnesota, Duluth. There-
after Siddharth Patwardhan ported this measure to Word-
Net::Similarity.
This work has been partially supported by a National
Science Foundation Faculty Early CAREER Develop-
ment award (#0092784), and by a Grant-in-Aid of Re-
search, Artistry and Scholarship from the Office of the
Vice President for Research and the Dean of the Gradu-
ate School of the University of Minnesota.
References
T. Baldwin, C. Bannard, T. Tanaka, and D. Widdows.
2003. An empirical model of multiword expression
decomposability. In Proceedings of the of the ACL-
2003 Workshop on Multiword Expressions: Analy-
sis, Acquisition and Treatment, pages 89?96, Sapporo,
Japan.
S. Banerjee and T. Pedersen. 2002. An adapted Lesk
algorithm for word sense disambiguation using Word-
Net. In Proceedings of the Third International Confer-
ence on Intelligent Text Processing and Computational
Linguistics, pages 136?145, Mexico City, February.
S. Banerjee and T. Pedersen. 2003. Extended gloss over-
laps as a measure of semantic relatedness. In Proceed-
ings of the Eighteenth International Joint Conference
on Artificial Intelligence, pages 805?810, Acapulco,
August.
M. Diab. 2003. Word Sense Disambiguation within a
Multilingual Framework. Ph.D. thesis, The University
of Maryland.
G. Hirst and D. St-Onge. 1998. Lexical chains as repre-
sentations of context for the detection and correction
of malapropisms. In C. Fellbaum, editor, WordNet:
An electronic lexical database, pages 305?332. MIT
Press.
M. Jarmasz and S. Szpakowicz. 2003. Roget?s thesaurus
and semantic similarity. In Proceedings of the Con-
ference on Recent Advances in Natural Language Pro-
cessing, pages 212?219, Borovets, Bulgaria.
J. Jiang and D. Conrath. 1997. Semantic similarity based
on corpus statistics and lexical taxonomy. In Proceed-
ings on International Conference on Research in Com-
putational Linguistics, pages 19?33, Taiwan.
C. Leacock and M. Chodorow. 1998. Combining local
context and WordNet similarity for word sense identifi-
cation. In C. Fellbaum, editor, WordNet: An electronic
lexical database, pages 265?283. MIT Press.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the International Con-
ference on Machine Learning, Madison, August.
D. McCarthy, R. Koeling, and J. Weeds. 2004. Rank-
ing WordNet senses automatically. Technical Report
CSRP 569, University of Sussex, January.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003. Us-
ing measures of semantic relatedness for word sense
disambiguation. In Proceedings of the Fourth Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics, pages 241?257, Mexico
City, February.
S. Patwardhan. 2003. Incorporating dictionary and cor-
pus information into a context vector measure of se-
mantic relatedness. Master?s thesis, University of Min-
nesota, Duluth, August.
J. Rennie. 2000. WordNet::QueryData: a Perl
module for accessing the WordNet database.
http://www.ai.mit.edu/people/jrennie/WordNet.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In Proceedings of
the 14th International Joint Conference on Artificial
Intelligence, pages 448?453, Montreal, August.
Z. Wu and M. Palmer. 1994. Verb semantics and lexi-
cal selection. In 32nd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 133?138,
Las Cruces, New Mexico.
Z. Zhang, J. Otterbacher, and D. Radev. 2003. Learning
cross-document structural relationships using boost-
ing. In Proceedings of the 12th International Con-
ference on Information and Knowledge Management,
pages 124?130.
Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 276?279,
New York City, June 2006. c?2006 Association for Computational Linguistics
Automatic Cluster Stopping with Criterion Functions and the Gap Statistic
Ted Pedersen and Anagha Kulkarni
Department of Computer Science
University of Minnesota, Duluth
Duluth, MN 55812 USA
{tpederse,kulka020}@d.umn.edu
http://senseclusters.sourceforge.net
Abstract
SenseClusters is a freely available sys-
tem that clusters similar contexts. It can
be applied to a wide range of problems,
although here we focus on word sense
and name discrimination. It supports
several different measures for automati-
cally determining the number of clusters
in which a collection of contexts should
be grouped. These can be used to discover
the number of senses in which a word is
used in a large corpus of text, or the num-
ber of entities that share the same name.
There are three measures based on clus-
tering criterion functions, and another on
the Gap Statistic.
1 Introduction
Word sense and name discrimination are problems
in unsupervised learning that seek to cluster the oc-
currences of a word (or name) found in multiple con-
texts based on their underlying meaning (or iden-
tity). The assumption is made that each discovered
cluster will represent a different sense of a word, or
the underlying identity of a person or organization
that has an ambiguous name.
Existing approaches to this problem usually re-
quire that the number of clusters to be discovered
(k) be specified ahead of time. However, in most re-
alistic settings, the value of k is unknown to the user.
Here we describe various cluster stopping measures
that are now implemented in SenseClusters (Puran-
dare and Pedersen, 2004) that will group N contexts
into k clusters, where the value of k will be automat-
ically determined.
Cluster stopping can be viewed as a problem in
model selection, since a number of different models
(i.e., clustering solutions) are created using different
values of k, and the one that best fits the observed
data is selected based on a criterion function. This
is reminiscent of earlier work on sequential model
selection for creating models of word sense disam-
biguation (e.g., (O?Hara et al, 2000)), where it was
found that forward sequential search strategies were
most effective. These methods start with simpler
models and then add to them in a stepwise fash-
ion until no further improvement in model fit is ob-
served. This is in fact very similar to what we have
done here, where we start with solutions based on
one cluster, and steadily increase the number of clus-
ters until we find the best fitting solution.
SenseClusters supports four cluster stopping mea-
sures, each of which is based on interpreting a clus-
tering criterion function in some way. The first three
measures (PK1, PK2, PK3) look at the successive
values of the criterion functions as k increases, and
try to identify the point at which the criterion func-
tion stops improving significantly. We have also cre-
ated an adaptation of the Gap Statistic (Tibshirani
et al, 2001), which compares the criterion function
from the clustering of the observed data with the
clustering of a null reference distribution and selects
the value of k for which the difference between them
is greatest.
In order to evaluate our results, we sometimes
conduct experiments with words that have been
manually sense tagged. We also create name con-
276
flations where some number of names of persons,
places, or organizations are replaced with a single
name to create pseudo or false ambiguities. For ex-
ample, in this paper we refer to an example where
we have replaced all mentions of Sonia Gandhi and
Leonid Kuchma with a single ambiguous name.
Clustering methods are typically either partitional
or agglomerative. The main difference is that ag-
glomerative methods start with 1 or N clusters and
then iteratively arrive at a pre?specified number (k)
of clusters, while partitional methods start by ran-
domly dividing the contexts into k clusters and then
iteratively rearranging the members of the k clusters
until the selected criterion function is maximized. In
this work we have used K-means clustering, which
is a partitional method, and the H2 criterion func-
tion, which is the ratio of within?cluster similarity
(I2) to between?cluster similarity (E1).
2 Methodology
In word sense or name discrimination, the num-
ber of contexts (N) to cluster is usually very large,
and considering all possible values of k from 1...N
would be inefficient. As the value of k increases,
the criterion function will reach a plateau, indicat-
ing that dividing the contexts into more and more
clusters does not improve the quality of the solution.
Thus, we identify an upper bound to k that we refer
to as deltaK by finding the point at which the cri-
terion function only changes to a small degree as k
increases.
According to the H2 criterion function, the higher
its ratio of within?cluster similarity to between?
cluster similarity, the better the clustering. A large
value indicates that the clusters have high internal
similarity, and are clearly separated from each other.
Intuitively then, one solution to selecting k might
be to examine the trend of H2 scores, and look for
the smallest k that results in a nearly maximum H2
value.
However, a graph of H2 values for a clustering
of the 2 sense name conflation Sonia Gandhi and
Leonid Kuchma as shown in Figure 1 (top) reveals
the difficulties of such an approach. There is a grad-
ual curve in this graph and there is no obvious knee
point (i.e., sharp increase) that indicates the appro-
priate value of k.
0.0045
0.0050
0.0055
0.0060
0.0065
0.0070
0.0075
0.0080
0 2 4 6 8 10 12 14 16
H2 vs k
r
r
r
r
r
r
r
r
r
r
r
r
r r r
r
-1.5000
-1.0000
-0.5000
0.0000
0.5000
1.0000
0 2 4 6 8 10 12 14 16
PK1 vs k
r
r
r
r
r
r
r
r
r
r
r
r r r
r
0.9500
1.0000
1.0500
1.1000
1.1500
1.2000
1.2500
1.3000
2 4 6 8 10 12 14 16
PK2 vs kr
r
r
r
r
r
r r r r
r
r r r
0.9900
1.0000
1.0100
1.0200
1.0300
1.0400
1.0500
1.0600
2 4 6 8 10 12 14
PK3 vs k
r
r
r
r r r r
r r
r
r
r
r
Figure 1: H2 (top) and PK1, PK2, and PK3 for
the name conflate pair Sonia Gandhi and Leonid
Kuchma. The predicted number of senses is 2 for
all the measures.
277
2.1 PK1
The PK1 measure is based on (Mojena, 1977),
which finds clustering solutions for all values of k
from 1..N , and then determines the mean and stan-
dard deviation of the criterion function. Then, a
score is computed for each value of k by subtracting
the mean from the criterion function, and dividing
by the standard deviation. We adapt this technique
by using the H2 criterion function, and limit k from
1...deltaK:
PK1(k) = H2(k) ? mean(H2[1...deltaK])std(H2[1...deltaK])
(1)
To select a value of k, a threshold must be set.
Then, as soon as PK1(k) exceeds this threshold,
k-1 is selected as the appropriate number of clus-
ters. Mojena suggests values of 2.75 to 3.50, but also
states they would need to be adjusted for different
data sets. We have arrived at an empirically deter-
mined value of -0.70, which coincides with the point
in the standard normal distribution where 75% of the
probability mass is associated with values greater
than this.
We observe that the distribution of PK1 scores
tends to change with different data sets, making it
hard to apply a single threshold. The graph of the
PK1 scores shown in Figure 1 illustrates the diffi-
culty : the slope of these scores is nearly linear, and
as such any threshold is a somewhat arbitrary cutoff.
2.2 PK2
PK2 is similar to (Hartigan, 1975), in that both take
the ratio of a criterion function at k and k-1, in order
to assess the relative improvement when increasing
the number of clusters.
PK2(k) = H2(k)H2(k ? 1) (2)
When this ratio approaches 1, the clustering has
reached a plateau, and increasing k will have no
benefit. If PK2 is greater than 1, then we should
increase k. We compute the standard deviation of
PK2 and use that to establish a boundary as to what
it means to be ?close enough? to 1 to consider that
we have reached a plateau. Thus, PK2 will select k
where PK2(k) is the closest to (but not less than) 1
+ standard deviation(PK2[1...deltaK]).
The graph of PK2 in Figure 1 shows an elbow
that is near the actual number of senses. The critical
region defined by the standard deviation is shaded,
and note that PK2 selected the value of k that was
outside of (but closest to) that region. This is inter-
preted as being the last value of k that resulted in a
significant improvement in clustering quality. Note
that here PK2 predicts 2 senses, which corresponds
to the number of underlying entities.
2.3 PK3
PK3 utilizes three k values, in an attempt to find a
point at which the criterion function increases and
then suddenly decreases. Thus, for a given value of
k we compare its criterion function to the preceding
and following value of k:
PK3(k) = 2 ? H2(k)H2(k ? 1) + H2(k + 1) (3)
The form of this measure is identical to that of the
Dice Coefficient, although in set theoretic or prob-
abilistic applications Dice tends to be used to com-
pare two variables or sets with each other.
PK3 is close to 1 if the H2 values form a line,
meaning that they are either ascending, or they are
on the plateau. However, our use of deltaK elimi-
nates the plateau, so in our case values of 1 show that
k is resulting in consistent improvements to clus-
tering quality, and that we should continue. When
PK3 rises significantly above 1, we know that k+1
is not climbing as quickly, and we have reached a
point where additional clustering may not be help-
ful. To select k we select the largest value of
PK3(k) that is closest to (but still greater than) the
critical region defined by the standard deviation of
PK3.
PK3 is similar in spirit to (Salvador and Chan,
2004), which introduces the L measure. This tries to
find the point of maximum curvature in the criterion
function graph, by fitting a pair of lines to the curve
(where the intersection of these lines represents the
selected k).
278
2.4 The Gap Statistic
SenseClusters includes an adaptation of the Gap
Statistic (Tibshirani et al, 2001). It is distinct from
the measures PK1, PK2, and PK3 since it does not
attempt to directly find a knee point in the graph of
a criterion function. Rather, it creates a sample of
reference data that represents the observed data as
if it had no meaningful clusters in it and was sim-
ply made up of noise. The criterion function of the
reference data is then compared to that of the ob-
served data, in order to identify the value of k in the
observed data that is least like noise, and therefore
represents the best clustering of the data.
To do this, it generates a null reference distri-
bution by sampling from a distribution where the
marginal totals are fixed to the observed marginal
values. Then some number of replicates of the ref-
erence distribution are created by sampling from it
with replacement, and each of these replicates is
clustered just like the observed data (for successive
values of k using a given criterion function).
The criterion function scores for the observed and
reference data are compared, and the point at which
the distance between them is greatest is taken to pro-
vide the appropriate value of k. An example of this
is seen in Figure 2. The reference distribution repre-
sents the noise in the observed data, so the value of
k where the distance between the reference and ob-
served data is greatest represents the most effective
clustering of the data.
Our adaption of the Gap Statistic allows us to
use any clustering criterion function to make the
comparison of the observed and reference data,
whereas the original formulation is based on using
the within?cluster dispersion.
3 Acknowledgments
This research is supported by a National Science
Foundation Faculty Early CAREER Development
Award (#0092784).
References
J. Hartigan. 1975. Clustering Algorithms. Wiley, New
York.
R. Mojena. 1977. Hierarchical grouping methods and
40
60
80
100
120
140
160
180
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
I2(obs) vs k
r
r
r
r
r
r
r
r
r r
r
r
r r
r
r r
r
r r
r
r r
r r
r r
r r
r
r
I2(ref) vs k
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r
r r
r
r
r
r
r
r r
r r
r r
r r
r
10
15
20
25
30
35
40
45
50
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
Gap vs k
r
r
r
r
r
r
r
r
r
r r r r
r
r r
r
r r
r r r
r r
r
r
r r r r
Figure 2: I2 for observed and reference data (top)
and the Gap between them (bottom) for the name
conflate pair Sonia Gandhi and Leonid Kuchma. The
predicted number of senses is 3.
stopping rules: An evaluation. The Computer Journal,
20(4):359?363.
T. O?Hara, J. Wiebe, and R. Bruce. 2000. Selecting
decomposable models for word-sense disambiguation:
The grling-sdm system. Computers and the Humani-
ties, 34(1?2):159?164.
A. Purandare and T. Pedersen. 2004. Word sense
discrimination by clustering contexts in vector and
similarity spaces. In Proceedings of the Conference
on Computational Natural Language Learning, pages
41?48, Boston, MA.
S. Salvador and P. Chan. 2004. Determining the
number of clusters/segments in hierarchical cluster-
ing/segmentation algorithms. In Proceedings of the
16th IEEE International Conference on Tools with AI,
pages 576?584.
R. Tibshirani, G. Walther, and T. Hastie. 2001. Esti-
mating the number of clusters in a dataset via the Gap
statistic. Journal of the Royal Statistics Society (Series
B), pages 411?423.
279
Proceedings of NAACL HLT 2009: Demonstrations, pages 17?20,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
WordNet::SenseRelate::AllWords -
A Broad Coverage Word Sense Tagger
that Maximizes Semantic Relatedness
Ted Pedersen and Varada Kolhatkar
Department of Computer Science
University of Minnesota
Duluth, MN 55812 USA
{tpederse,kolha002}@d.umn.edu
http://senserelate.sourceforge.net
Abstract
WordNet::SenseRelate::AllWords is a freely
available open source Perl package that as-
signs a sense to every content word (known
to WordNet) in a text. It finds the sense of
each word that is most related to the senses
of surrounding words, based on measures
found in WordNet::Similarity. This method is
shown to be competitive with results from re-
cent evaluations including SENSEVAL-2 and
SENSEVAL-3.
1 Introduction
Word sense disambiguation is the task of assigning
a sense to a word based on the context in which it
occurs. This is one of the central problems in Nat-
ural Language Processing, and has a long history of
research. A great deal of progress has been made in
using supervised learning to build models of disam-
biguation that assign a sense to a single target word
in context. This is sometimes referred to as the lexi-
cal sample or target word formulation of the task.
However, to be effective, supervised learning re-
quires many manually disambiguated examples of
a single target word in different contexts to serve
as training data to learn a classifier for that word.
While the resulting models are often quite accurate,
manually creating training data in sufficient volume
to cover even a few words is very time consuming
and error prone. Worse yet, creating sufficient train-
ing data to cover all the different words in a text is
essentially impossible, and has never even been at-
tempted.
Despite these difficulties, word sense disambigua-
tion is often a necessary step in NLP and can?t sim-
ply be ignored. The question arises as to how to de-
velop broad coverage sense disambiguation modules
that can be deployed in a practical setting without in-
vesting huge sums in manual annotation efforts. Our
answer is WordNet::SenseRelate::AllWords (SR-
AW), a method that uses knowledge already avail-
able in the lexical databaseWordNet to assign senses
to every content word in text, and as such offers
broad coverage and requires no manual annotation
of training data.
SR-AW finds the sense of each word that is most
related or most similar to those of its neighbors in the
sentence, according to any of the ten measures avail-
able in WordNet::Similarity (Pedersen et al, 2004).
It extends WordNet::SenseRelate::TargetWord, a
lexical sample word sense disambiguation algorithm
that finds the maximum semantic relatedness be-
tween a target word and its neighbors (Patward-
han et al, 2003). SR-AW was originally developed
by (Michelizzi, 2005) (through version 0.06) and is
now being significantly enhanced.
2 Methodology
SR-AW processes a text sentence by sentence. It
proceeds through each sentence word by word from
left to right, centering each content word in a bal-
anced window of context whose size is determined
by the user. Note that content words at the start
or end of a sentence will have unbalanced windows
associated with them, since the algorithm does not
cross sentence boundaries and treats each sentence
independently.
17
All of the possible senses of the word in the center
of the window are measured for similarity relative to
the possible senses of each of the surrounding words
in the window in a pairwise fashion. The sense of
the center word that has the highest total when those
pairwise scores are summed is considered to be the
sense of that word. SR-AW then moves the center
of the window to the next content word to the right.
The user has the option of fixing the senses of the
words that precede it to those that were discovered
by SR-AW, or allowing all their senses to be consid-
ered in subsequent steps.
WordNet::Similarity1 offers six similarity mea-
sures and four measures of relatedness. Measures
of similarity are limited to making noun to noun and
verb to verb comparisons, and are based on using
the hierarchical information available for nouns and
verbs in WordNet. These measures may be based
on path lengths (path, wup, lch) or on path lengths
augmented with Information Content derived from
corpora (res, lin, jcn). The measures of relatedness
may make comparisons between words in any part
of speech, and are based on finding paths between
concepts that are not limited to hierarchical relations
(hso), or on using gloss overlaps either for string
matching (lesk) or for creating a vector space model
(vector and vector-pairs) that are used for measuring
relatedness.
The availability of ten different measures that can
be used with SR-AW leads to an incredible richness
and variety in this approach. In general word sense
disambiguation is based on the presumption that
words that occur together will have similar or related
meanings, so SR-AW allows for a wide range of op-
tions in deciding how to assess similarity and relat-
edness. SR-AW can be viewed as a graph based ap-
proach when using the path based measures, where
words are assigned the senses that are located most
closely together in WordNet. These path based
methods can be easily augmented with Information
Content in order to allow for finer grained distinc-
tions to be made. It is also possible to lessen the
impact of the physical structure of WordNet by us-
ing the content of the glosses as the primary source
of information.
1http://wn-similarity.sourceforge.net
3 WordNet::SenseRelate::AllWords Usage
Input : The input to SR-AW can either be plain
untagged text (raw), or it may be tagged with Penn
Treebank part of speech tags (tagged : 47 tags; e.g.,
run/VBD), or with WordNet part of speech tags (wn-
tagged: 4 tags for noun, verb, adjective, adverb;
e.g., run#v). Penn Treebank tags are mapped to
WordNet POS tags prior to SR-AW processing, so
even though this tag set is very rich, it is used sim-
ply to distinguish between the four parts of speech
WordNet knows, and identify function words (which
are ignored as WordNet only includes open class
words). In all cases simple morphological process-
ing as provided by WordNet is utilized to identify
the root form of a word in the input text.
Examples of each input format are shown below:
? (raw) : The astronomer married a movie star.
? (tagged) : The/DT astronomer/NN mar-
ried/VBD a/DT movie star/NN
? (wntagged) : The astronomer#n married#v a
movie star#n
If the format is raw, SR-AW will identify Word-
Net compounds before processing. These are multi-
word terms that are usually nouns with just one
sense, so their successful identification can signif-
icantly improve overall accuracy. If a compound
is not identified, then it often becomes impossible
to disambiguate. For example, if White House is
treated as two separate words, there is no combina-
tion of senses that will equal the residence of the
US president, where that is the only sense of the
compound White House. To illustrate the scope of
compounds, of the 155,287 unique strings in Word-
Net 3.0, more than 40% (64,331) of them are com-
pounds. If the input is tagged or wntagged, it is
assumed that the user has identified compounds by
connecting the words that make up a compound with
(e.g., white house, movie star).
In the tagged and wntagged formats, the user must
identify compounds and also remove punctuation.
In the raw format SR-AW will simply ignore punc-
tuation unless it happens to be part of a compound
(e.g., adam?s apple, john f. kennedy). In all formats
the upper/lower case distinction is ignored, and it is
18
assumed that the input is already formatted one line
per sentence, one sentence per line.
SR-AW will then check to see if a stoplist has
been provided by the user, or if the user would like to
use the default stoplist. In general a stoplist is highly
recommended, since there are quite a few words in
WordNet that have unexpected senses and might be
problematic unless they are excluded. For example,
who has a noun sense of World Health Organization.
A has seven senses, including angstrom, vitamin A,
a nucleotide, a purine, an ampere, the letter, and the
blood type. Many numbers have noun senses that
define them as cardinal numbers, and some have ad-
jective senses as well.
In the raw format, the stoplist check is done after
compounding, because certain compounds include
stop words (e.g., us house of representatives). In
the wntagged and tagged formats the stoplist check
is still performed, but the stoplist must take into ac-
count the form of the part of speech tags. How-
ever, stoplists are expressed using regular expres-
sions, making it quite convenient to deal with part
of speech tags, and also to specify entire classes of
terms to be ignored, such as numbers or single char-
acter words.
Disambiguation Options : The user has a number
of options to control the direction of the SR-AW al-
gorithm. These include the very powerful choices
regarding the measure of similarity or relatedness
that is to be used. There are ten such measures as
has been described previously. As was also already
mentioned, the user also can choose to fix the senses
of words that have already been processed.
In addition to these options, the user can con-
trol the size of the window used to determine which
words are involved in measuring relatedness or simi-
larity. A window size ofN includes the center word,
and then extends out to the left and right of the cen-
ter for N/2 content words, unless it encounters the
sentence boundaries. IfN is odd then the number of
words to the left and right (N ? 1)/2, and if N is
even there are N/2 words to the left, and (N/2)? 1
words to the right.
When using a measure of similarity and tagged or
wntagged text, it may be desirable to coerce the part
of speech of surrounding words to that of the word
in the center of the window of context. If this is
not done, then any word with a part of speech other
than that of the center word will not be included in
the calculation of semantic similarity. Coercion is
performed by first checking for forms of the word in
a different part of speech, and then checking if there
are any derivational relations from the word to the
part of speech of the center word. Note that in the
raw format part of speech coercion is not necessary,
since the algorithm will consider all possible parts of
speech for each word. If the sense of previous words
has already been fixed, then part of speech coercion
does not override those fixed assignments.
Finally, the user is able to control several scoring
thresholds in the algorithm. The user may specify a
context score which indicates a minimum threshold
that a sense of the center word should achieve with
all the words in the context in order to be selected.
If this threshold is not met, no sense is assigned and
it may be that the window should be increased.
The pair score is a finer grained threshold that in-
dicates the minimum values that a relatedness score
between a sense of the center word and a sense of
one of the neighbors must achieve in order to be
counted in the overall score of the center word. If
this threshold is not met then the pair will contribute
0 to that score. This can be useful for filtering out
noise from the scores when set to modest values.
Output : The output of SR-AW is the original text
with WordNet sense tags assigned. WordNet sense
tags are given in WPS form, which means word, part
of speech, and sense number. In addition, glosses are
displayed for each of the selected senses.
There are also numerous trace options available,
which can be combined in order to provide more de-
tailed diagnostic output. This includes displaying
the window of context with the center word desig-
nated (1), the winning score for each context win-
dow (2), the non-zero scores for each sense of the
center word (4), the non-zero pairwise scores (8),
the zero values for any of the previous trace levels
(16), and the traces from the semantic relatedness
measures from WordNet::Similarity (32).
4 Experimental Results
We have evaluated SR-AW using three corpora that
have been manually annotated with senses from
WordNet. These include the SemCor corpus, and
19
Table 1: SR-AW Results (%)
2 5 15
SC P R F P R F P R F
lch 56 13 21 54 29 36 52 35 42
jcn 65 15 24 64 31 42 62 41 49
lesk 58 49 53 62 60 61 62 61 61
S2 P R F P R F P R F
lch 48 10 16 50 24 32 48 31 38
jcn 55 9 15 55 21 31 55 31 39
lesk 54 44 48 58 56 57 59 59 59
S3 P R F P R F P R F
lch 48 13 20 49 29 37 48 35 41
jcn 55 14 22 55 31 40 53 38 46
lesk 51 43 47 54 52 53 54 53 54
the SENSEVAL-2 and SENSEVAL-3 corpora. Sem-
Cor is made up of more than 200,000 words of run-
ning text from news articles found in the Brown Cor-
pus. The SENSEVAL data sets are each approxi-
mately 4,000 words of running text from Wall Street
Journal news articles from the Penn Treebank. Note
that only the words known to WordNet in these cor-
pora have been sense tagged. As a result, there are
185,273 sense tagged words in SemCor, 2,260 in
SENSEVAL-2, and 1,937 in SENSEVAL-3. We have
used versions of these corpora where the WordNet
senses have been mapped to WordNet 3.02.
In Table 4 we report results using Precision (P),
Recall (R), and F-Measure (F). We use three window
sizes in these experiments (2, 5, and 15), threeWord-
Net::Similarity measures (lch, jcn, and lesk),and
three different corpora : SemCor (SC), SENSEVAL-
2 (S2), SENSEVAL-3 (S3). These experiments were
carried out with version 0.17 of SR-AW.
For all corpora we observe the same patterns.
The lesk measure tends to result in much higher re-
call with smaller window sizes, since it is able to
measure similarity between words with any parts of
speech, whereas lch and jcn are limited to making
noun-noun and verb-verb measurements. But, as the
window size increases so does recall. Precision con-
tinues to increase for lesk as the window size in-
creases. Our best results come from using the lesk
measure with a window size of 15. For SemCor this
results in an F-measure of 61%. For SENSEVAL-2 it
2http://www.cse.unt.edu/?rada/downloads.html
results in an F-measure of 59%, and for SENSEVAL-
3 it results in an F-measure of 54%. These results
would have ranked 4th of 22 teams and 15th of 26 in
the respective SENSEVAL events.
A well known baseline for all words disambigua-
tion is to assign the first WordNet sense to each am-
biguous word. This results in an F-measure of 76%
for SemCor, 69% for SENSEVAL-2, and 68% for
SENSEVAL-3. A lower bound can be established
by randomly assigning senses to words. This re-
sults in an F-Measure of 41% for SemCor, 41% for
SENSEVAL-2, and 37% for SENSEVAL-3. This is
relatively high due to the large number of words that
have just one possible sense (so randomly selecting
will result in a correct assignment). For example,
in SemCor approximately 20% of the ambiguous
words have just one sense. From these results we
can see that SR-AW lags behind the sense one base-
line (which is common among all words systems),
but significantly outperforms the random baseline.
5 Conclusions
WordNet::SenseRelate::AllWords is a highly flexi-
ble method of word sense disambiguation that of-
fers broad coverage and does not require training of
any kind. It uses WordNet and measures of seman-
tic similarity and relatedness to identify the senses
of words that are most related to each other in a sen-
tence. It is implemented in Perl and is freely avail-
able from the URL on the title page both as source
code and via a Web interface.
References
J. Michelizzi. 2005. Semantic relatedness applied to all
words sense disambiguation. Master?s thesis, Univer-
sity of Minnesota, Duluth, July.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003. Us-
ing measures of semantic relatedness for word sense
disambiguation. In Proceedings of the Fourth Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics, pages 241?257, Mexico
City, February.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
Wordnet::Similarity - Measuring the relatedness of
concepts. In Proceedings of Fifth Annual Meeting
of the North American Chapter of the Association for
Computational Linguistics, pages 38?41, Boston, MA.
20
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 73?76, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
SenseRelate::TargetWord ? A Generalized Framework
for Word Sense Disambiguation
Siddharth Patwardhan
School of Computing
University of Utah
Salt Lake City, UT 84112
sidd@cs.utah.edu
Satanjeev Banerjee
Language Technologies Inst.
Carnegie Mellon University
Pittsburgh, PA 15213
satanjeev@cmu.edu
Ted Pedersen
Dept. of Computer Science
University of Minnesota
Duluth, MN 55812
tpederse@d.umn.edu
Abstract
We have previously introduced a method
of word sense disambiguation that com-
putes the intended sense of a target word,
using WordNet-based measures of seman-
tic relatedness (Patwardhan et al, 2003).
SenseRelate::TargetWord is a Perl pack-
age that implements this algorithm. The
disambiguation process is carried out by
selecting that sense of the target word
which is most related to the context words.
Relatedness between word senses is mea-
sured using the WordNet::Similarity Perl
modules.
1 Introduction
Many words have different meanings when used in
different contexts. Word Sense Disambiguation is
the task of identifying the intended meaning of a
given target word from the context in which it is
used. (Lesk, 1986) performed disambiguation by
counting the number of overlaps between the dic-
tionary definitions (i.e., glosses) of the target word
and those of the neighboring words in the con-
text. (Banerjee and Pedersen, 2002) extended this
method of disambiguation by expanding the glosses
of words to include glosses of related words, accord-
ing to the structure of WordNet (Fellbaum, 1998).
In subsequent work, (Patwardhan et al, 2003) and
(Banerjee and Pedersen, 2003) proposed that mea-
suring gloss overalps is just one way of determin-
ing semantic relatedness, and that word sense dis-
ambiguation can be performed by finding the most
related sense of a target word to its surrounding con-
text using a wide variety of measures of relatedness.
SenseRelate::TargetWord is a Perl package that
implements these ideas, and is able to disambiguate
a target word in context by finding the sense that is
most related to its neighbors according to a speci-
fied measure. A user of this package is able to make
a variety of choices for text preprocessing options,
context selection, relatedness measure selection and
the selection of an algorithm for computing the over-
all relatedness between each sense of the target word
and words in the surrounding context. The user can
customize each of these choices to fit the needs of
her specific disambiguation task. Further, the vari-
ous sub-tasks in the package are implemented in a
modular fashion, allowing the user to easily replace
a module with her own module if needed.
The following sections describe the generalized
framework for Word Sense Disambiguation, the ar-
chitecture and usage of SenseRelate::TargetWord,
and a description of the user interfaces (command
line and GUI).
2 The Framework
The package has a highly modular architecture. The
disambiguation process is divided into a number of
smaller sub-tasks, each of which is represented by
a separate module. Each of the sequential sub-tasks
or stages accepts data from a previous stage, per-
forms a transformation on the data, and then passes
on the processed data structures to the next stage in
the pipeline. We have created a protocol that defines
the structure and format of the data that is passed
between the stages. The user can create her own
73
Relatedness 
Measure
Context
Target Sense
Preprocessing
Format Filter
Sense Inventory
Context Selection Postprocessing
Pick Sense
Figure 1: A generalized framework for Word Sense Disambiguation.
modules to perform any of these sub-tasks as long
as the modules adhere to the protocol laid down by
the package.
Figure 1 projects an overview of the architecture
of the system and shows the various sub-tasks that
need to be performed to carry out word sense dis-
ambiguation. The sub-tasks in the dotted boxes are
optional. Further, each of the sub-tasks can be per-
formed in a number of different ways, implying that
the package can be customized in a large number of
ways to suit different disambiguation needs.
2.1 Format Filter
The filter takes as input file(s) annotated in the
SENSEVAL-2 lexical sample format, which is an
XML?based format that has been used for both the
SENSEVAL-2 and SENSEVAL-3 exercises. A file in
this format includes a number of instances, each one
made up of 2 to 3 lines of text where a single tar-
get word is designated with an XML tag. The fil-
ter parses the input file to build data structures that
represent the instances to be disambiguated, which
includes a single target word and the surrounding
words that define the context.
2.2 Preprocessing
SenseRelate::TargetWord expects zero or more text
preprocessing modules, each of which perform a
transformation on the input words. For example, the
Compound Detection Module identifies sequences
of tokens that form compound words that are known
as concepts to WordNet (such as ?New York City?).
In order to ensure that compounds are treated as a
single unit, the package replaces them in the instance
with the corresponding underscore?connected form
(?New York City?).
Multiple preprocessing modules can be chained
together, the output of one connected to the input of
the next, to form a single preprocessing stage. For
example, a part of speech tagging module could be
added after compound detection.
2.3 Context Selection
Disambiguation is performed by finding the sense of
the target word that is most related to the words in
its surrounding context. The package allows for var-
ious methods of determining what exactly the sur-
rounding context should consist of. In the current
implementation, the context selection module uses
an n word window around the target word as con-
text. The window includes the target word, and ex-
tends to both the left and right. The module selects
the n? 1 words that are located closest to the target
word, and sends these words (and the target) on to
the next module for disambiguation. Note that these
words must all be known to WordNet, and should
not include any stop?words.
However, not all words in the surrounding context
are indicative of the correct sense of the target word.
An intelligent selection of the context words used in
the disambiguation process could potentially yield
much better results and generate a solution faster
than if all the nearby words were used. For exam-
ple, we could instead select the nouns from the win-
dow of context that have a high term?frequency to
document?frequency ratio. Or, we could identify
lexical chains in the surrounding context, and only
include those words that are found in chains that in-
clude the target word.
74
2.4 Sense Inventory
After having reduced the context to n words, the
Sense Inventory stage determines the possible senses
of each of the n words. This list can be obtained
from a dictionary, such as WordNet. A thesaurus
could also be used for the purpose. Note however,
that the subsequent modules in the pipeline should
be aware of the codes assigned to the word senses.
In our system, this module first decides the base
(uninflected) form of each of the n words. It then
retrieves all the senses for each word from the sense
inventory. We use WordNet for our sense inventory.
2.5 Postprocessing
Some optional processing can be performed on the
data structures generated by the Sense Inventory
module. This would include tasks such as sense
pruning, which is the process of removing some
senses from the inventory, based on simple heuris-
tics, algorithms or options. For example, the user
may decide to preclude all verb senses of the target
word from further consideration in the disambigua-
tion process.
2.6 Identifying the Sense
The disambiguation module takes the lists of senses
of the target word and those of the context words and
uses this information to pick one sense of the tar-
get word as the answer. Many different algorithms
could be used to do this. We have modules Local
and Global that (in different ways) determine the re-
latedness of each of the senses of the target word
with those of the context words, and pick the most
related sense as the answer. These are described
in greater detail by (Banerjee and Pedersen, 2002),
but in general the Local method compares the target
word to its neighbors in a pair-wise fashion, while
the Global method carries out an exhaustive compar-
ison between all the senses of the target word and all
the senses of the neighbors.
3 Using SenseRelate::TargetWord
SenseRelate::TargetWord can be used via the
command-line interface provided by the utility pro-
gram called disamb.pl. It provides a rich variety of
options for controlling the process of disambigua-
tion. Or, it can be embedded into Perl programs,
by including it as a module and calling its various
methods. Finally, there is a graphical interface to
the package that allows a user to highlight a word in
context to be disambiguated.
3.1 Command Line
The command-line interface disamb.pl takes as input
a SENSEVAL-2 formatted lexical sample file. The
program disambiguates the marked up word in each
instance and prints to screen the instance ID, along
with the disambiguated sense of the target word.
Command line options are available to control the
disambiguation process. For example, a user can
specify which relatedness measure they would like
to use, whether disambiguation should be carried out
using Local or Global methods, how large a win-
dow of context around the target word is to be used,
and whether or not all the parts of speech of a word
should be considered.
3.2 Programming Interface
SenseRelate::TargetWord is distributed as a Perl
package. It is programmed in object-oriented Perl
as a group of Perl classes. Objects of these classes
can be instantiated in user programs, and meth-
ods can be called on these objects. The pack-
age requires that the Perl interface to WordNet,
WordNet::QueryData1 be installed on the system.
The disambiguation algorithms also require that the
semantic relatedness measures WordNet::Similarity
(Pedersen et al, 2004) be installed.
3.3 Graphical User Interface
We have developed a graphical interface for the
package in order to conveniently access the disam-
biguation modules. The GUI is written in Gtk-Perl
? a Perl API to the Gtk toolkit. Unlike the command
line interface, the graphical interface is not tied to
any input file format. The interface allows the user to
input text, and to select the word to disambiguate. It
also provides the user with numerous configuration
options corresponding to the various customizations
described above.
1http://search.cpan.org/dist/WordNet-QueryData
75
4 Related Work
There is a long history of work in Word Sense Dis-
ambiguation that uses Machine Readable Dictionar-
ies, and are highly related to our approach.
One of the first approaches was that of (Lesk,
1986), which treated every dictionary definition of
a concept as a bag of words. To identify the in-
tended sense of the target word, the Lesk algorithm
would determine the number of word overlaps be-
tween the definitions of each of the meanings of the
target word, and those of the context words. The
meaning of the target word with maximum defini-
tion overlap with the context words was selected as
the intended sense.
(Wilks et al, 1993) developed a context vector
approach for performing word sense disambigua-
tion. Their algorithm built co-occurrence vectors
from dictionary definitions using Longman?s Dictio-
nary of Contemporary English (LDOCE). They then
determined the extent of overlap between the sum of
the vectors of the words in the context and the sum
of the vectors of the words in each of the definitions
(of the target word). For vectors, the extent of over-
lap is defined as the dot product of the vectors. The
meaning of the target word that had the maximum
overlap was selected as the answer.
More recently, (McCarthy et al, 2004) present a
method that performs disambiguation by determing
the most frequent sense of a word in a particular do-
main. This is based on measuring the relatedness
of the different possible senses of a target word (us-
ing WordNet::Similarity) to a set of words associated
with a particular domain that have been identified
using distributional methods. The relatedness scores
between a target word and the members of this set
are scaled by the distributional similarity score.
5 Availability
SenseRelate::TargetWord is written in Perl and is
freely distributed under the Gnu Public License. It
is available via SourceForge, an Open Source de-
velopment platform2, and the Comprehensive Perl
Archive Network (CPAN)3.
2http://senserelate.sourceforge.net
3http://search.cpan.org/dist/WordNet-SenseRelate-
TargetWord
6 Acknowledgements
This research is partially supported by a National
Science Foundation Faculty Early CAREER Devel-
opment Award (#0092784).
References
S. Banerjee and T. Pedersen. 2002. An adapted Lesk
algorithm for word sense disambiguation using Word-
Net. In Proceedings of the Third International Confer-
ence on Intelligent Text Processing and Computational
Linguistics, Mexico City, February.
S. Banerjee and T. Pedersen. 2003. Extended gloss over-
laps as a measure of semantic relatedness. In Pro-
ceedings of the Eighteenth International Conference
on Artificial Intelligence (IJCAI-03), Acapulco, Mex-
ico, August.
C. Fellbaum, editor. 1998. WordNet: An electronic lexi-
cal database. MIT Press.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: How to tell a pine cone
from a ice cream cone. In Proceedings of SIGDOC
?86.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses
in untagged text. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 279?286, Barcelona,
Spain, July.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003. Us-
ing measures of semantic relatedness for word sense
disambiguation. In Proceedings of the Fourth In-
ternational Conference on Intelligent Text Processing
and Computational Linguistics (CICLING-03), Mex-
ico City, Mexico, February.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::Similarity - Measuring the Re-
latedness of Concepts. In Daniel Marcu Susan Du-
mais and Salim Roukos, editors, HLT-NAACL 2004:
Demonstration Papers, pages 38?41, Boston, Mas-
sachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
Y. Wilks, D. Fass, C. Guo, J. McDonald, T. Plate, and
B. Slator. 1993. Providing machine tractable dictio-
nary tools. In J. Pustejovsky, editor, Semantics and
the Lexicon. Kluwer Academic Press, Dordrecht and
Boston.
76
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 105?108, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
SenseClusters: Unsupervised Clustering and Labeling of Similar Contexts
Anagha Kulkarni and Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
{kulka020,tpederse}@d.umn.edu
http://senseclusters.sourceforge.net
Abstract
SenseClusters is a freely available system
that identifies similar contexts in text. It
relies on lexical features to build first and
second order representations of contexts,
which are then clustered using unsuper-
vised methods. It was originally devel-
oped to discriminate among contexts cen-
tered around a given target word, but can
now be applied more generally. It also
supports methods that create descriptive
and discriminating labels for the discov-
ered clusters.
1 Introduction
SenseClusters seeks to group together units of text
(referred to as contexts) that are similar to each other
using lexical features and unsupervised clustering.
Our initial work (Purandare and Pedersen, 2004)
focused on word sense discrimination, which takes
as input contexts that each contain a given target
word, and produces as output clusters that are pre-
sumed to correspond to the different senses of the
word. This follows the hypothesis of (Miller and
Charles, 1991) that words that occur in similar con-
texts will have similar meanings.
We have shown that these methods can be ex-
tended to proper name discrimination (Pedersen et
al., 2005). People, places, or companies often share
the same name, and this can cause a considerable
amount of confusion when carrying out Web search
or other information retrieval applications. Name
discrimination seeks to group together the contexts
that refer to a unique underlying individual, and al-
low the user to recognize that the same name is being
used to refer to multiple entities.
We have also extended SenseClusters to clus-
ter contexts that are not centered around any tar-
get word, which we refer to as headless clustering.
Automatic email categorization is an example of a
headless clustering task, since each message can be
considered a context. SenseClusters will group to-
gether messages if they are similar in content, with-
out requiring that they share any particular target
word between them.
We are also addressing a well known limitation to
unsupervised clustering approaches. After cluster-
ing contexts, it is often difficult to determine what
underlying concepts or entities each cluster repre-
sents without manually inspecting their contents.
Therefore, we are developing methods that automat-
ically assign descriptive and discriminating labels to
each discovered cluster that provide a characteriza-
tion of the contents of the clusters that a human can
easily understand.
2 Clustering Methodology
We begin with the collection of contexts to be clus-
tered, referred to as the test data. These may all in-
clude a given target word, or they may be headless
contexts. We can select the lexical features from the
test data, or from a separate source of data. In either
case, the methodology proceeds in exactly the same
way.
SenseClusters is based on lexical features, in par-
ticular unigrams, bigrams, co?occurrences, and tar-
105
get co?occurrences. Unigrams are single words that
occur more than five times, bigrams are ordered
pairs of words that may have intervening words be-
tween them, while co-occurrences are simply un-
ordered bigrams. Target co-occurrences are those
co?occurrences that include the given target word.
We select bigrams and co?occurrences that occur
more than five times, and that have a log?likelihood
ratio of more than 3.841, which signifies a 95% level
of certainty that the two words are not independent.
We do not allow unigrams to be stop words, and we
eliminate any bigram or co?occurrence feature that
includes one or more stop words.
Previous work in word sense discrimination has
shown that contexts of an ambiguous word can be ef-
fectively represented using first order (Pedersen and
Bruce, 1997) or second order (Schu?tze, 1998) rep-
resentations. SenseClusters provides extensive sup-
port for both, and allows for them to be applied in a
wider range of problems.
In the first order case, we create a context (rows)
by lexical features (columns) matrix, where the fea-
tures may be any of the above mentioned types. The
cell values in this matrix record the frequencies of
each feature occurring in the context represented by
a given row. Since most lexical features only occur a
small number of times (if at all) in each context, the
resulting matrix tends to be very sparse and nearly
binary. Each row in this matrix forms a vector that
represents a context. We can (optionally) use Sin-
gular Value Decomposition (SVD) to reduce the di-
mensionality of this matrix. SVD has the effect of
compressing a sparse matrix by combining redun-
dant columns and eliminating noisy ones. This al-
lows the rows to be represented with a smaller num-
ber of hopefully more informative columns.
In the second order context representation we start
with creating a word by word co-occurrence ma-
trix where each row represent the first word and the
columns represent the second word of either bigram
or co?occurrence features previously identified. If
the features are bigrams then the word matrix is
asymmetric whereas for co-occurrences it is sym-
metric and the rows and columns do not suggest any
ordering. In either case, the cell values indicate how
often the two words occur together, or contains their
log?likelihood score of associativity. This matrix is
large and sparse, since most words do not co?occur
with each other. We may optionally apply SVD to
this co-occurrence matrix to reduce its dimension-
ality. Each row of this matrix is a vector that repre-
sents the given word at the row via its co?occurrence
characteristics. We create a second order represen-
tation of a context by replacing each word in that
context with its associated vector, and then averag-
ing together all these word vectors. This results in a
single vector that represents the overall context.
For contexts with target words we can restrict the
number of words around the target word that are av-
eraged for the creation of the context vector. In our
name discrimination experiments we limit this scope
to five words on either side of the target word which
is based on the theory that words nearer to the tar-
get word are more related to it than the ones that are
farther away.
The goal of the second order context represen-
tation is to capture indirect relationships between
words. For example, if the word Dictionary occurs
with Words but not with Meanings, and Words oc-
curs with Meanings, then the words Dictionary and
Meanings are second order co-occurrences via the
first order co-occurrence of Words.
In either the first or second order case, once we
have each context represented as a vector we pro-
ceed with clustering. We employ the hybrid clus-
tering method known as Repeated Bisections, which
offers nearly the quality of agglomerative clustering
at the speed of partitional clustering.
3 Labeling Methodology
For each discovered cluster, we create a descriptive
and a discriminating label, each of which is made
up of some number of bigram features. These are
identified by treating the contexts in each cluster as
a separate corpora, and applying our bigram feature
selection methods as described previously on each
of them.
Descriptive labels are the top N bigrams accord-
ing to the log?likelihood ratio. Our goal is that these
labels will provide clues as to the general nature of
the contents of a cluster. The discriminating labels
are any descriptive labels for a cluster that are not
descriptive labels of another cluster. Thus, the dis-
criminating label may capture the content that sep-
arates one cluster from another and provide a more
106
Table 1: Name Discrimination (F-measure)
MAJ. O1 O2
2-Way Name(M);+ (N) k=2 k=2
AAIRLINES(1075); 50.0 66.6 58.8
TCRUISE(1075) (2150)
AAIRLINES(3966); 51.7 61.7 59.6
HPACKARD(3690) (7656)
BGATES(1981); 64.8 63.4 53.8
TCRUISE(1075) (3056)
BSPEARS(1380); 50.0 56.6 65.8
GBUSH(1380) (2760)
3-Way Name (M);+ k=3 k=3
AAIRLINES(2500); 33.3 41.4 45.1
HPACKARD(2500); (7500)
BMW(2500);
AAIRLINES(1300); 33.3 46.0 45.3
HPACKARD(1300); (3900)
BSPEARS(1300);
BGATES(1075); 33.3 53.7 53.6
TCRUISE(1075); (3225)
GBUSH(1075)
detailed level of information.
4 Experimental Data
We evaluate these methods on proper name discrim-
ination and email (newsgroup) categorization.
For name discrimination we use the 700 million
word New York Times portion of the English Giga-
Word corpus as the source of contexts. While there
are many ambiguous names in this data, it is difficult
to evaluate the results of our approach given the ab-
sence of a disambiguated version of the text. Thus,
we automatically create ambiguous names by con-
flating the occurrences associated with two or three
relatively unambiguous names into a single obfus-
cated name.
For example, we combine Britney Spears and
George Bush into an ambiguous name Britney Bush,
and then see how well SenseClusters is able to cre-
ate clusters that reflect the true underlying identity
of the conflated name.
Our email experiments are based on the 20-
NewsGroup Corpus of USENET articles. This is
a collection of approximately 20,000 articles that
Table 2: Email Categorization (F-measure)
MAJ. O1 O2
Newsgroup(M);+ (N) k=2 k=2
comp.graphics(389); 50.1 61.1 63.9
misc.forsale(390) (779)
comp.graphics(389); 50.8 73.6 54.8
talk.pol.mideast(376) (756)
rec.motorcycles(398); 50.13 83.1 60.5
sci.crypt(396) (794)
rec.sport.hockey(399); 50.1 77.6 58.5
soc.relig.christian(398) (797)
sci.electronics(393); 50.3 67.8 52.3
soc.relig.christian(398) (791)
have been taken from 20 different newsgroups. As
such they are already classified, but since our meth-
ods are unsupervised we ignore this information un-
til it is time to evaluate our approach. We present
results that make two way distinctions between se-
lected pairs of newsgroups.
5 Experimental Results and Discussion
Table 1 presents the experimental results for 2-way
and 3-way name discrimination experiments, and
Table 2 presents results for a 2-way email cate-
gorization experiment. The results are reported in
terms of the F-measure, which is the harmonic mean
of precision and recall.
The first column in both tables indicates the possi-
ble names or newgroups, and the number of contexts
associated with each. The next column indicates the
percentage of the majority class (MAJ.) and count
(N) of the total number of contexts for the names
or newsgroups. The majority percentage provides a
simple baseline for level of performance, as this is
the F?measure that would be achieved if every con-
text were simply placed in a single cluster. We refer
to this as the unsupervised majority classifier.
The next two columns show the F?measure asso-
ciated with the order 1 and order 2 representations
of context, with all other options being held con-
stant. These experiments used bigram features, SVD
was performed as appropriate for each representa-
tion, and the method of Repeated Bisections was
used for clustering.
107
Table 3: Cluster Labels (for Table 1)
True Name Created Labels
CLUSTER 0: Flight 11, Flight 587, Sept 11,
AMERICAN Trade Center, World Trade,
AIRLINES Los Angeles, New York
CLUSTER 1: Jerry Maguire,
TOM Mission Impossible,
CRUISE Minority Report, Tom Cruise,
Penelope Cruz, Nicole Kidman,
United Airlines, Vanilla Sky,
Los Angeles, New York
CLUSTER 0: George Bush , George W,
GEORGE Persian Gulf, President, U S,
BUSH W Bush, former President,
lifting feeling, White House
CLUSTER 1: Chairman , Microsoft ,
BILL Microsoft Chairman,
GATES co founder, News Service,
operating system,
chief executive, White House
CLUSTER 2: Jerry Maguire,
TOM Mission Impossible,
CRUISE Minority Report, Al Gore,
New York , Nicole Kidman,
Penelope Cruz, Vanilla Sky,
Ronald Reagan, White House
Finally, note that the number of clusters to be dis-
covered must be provided by the user. In these ex-
periments we have taken the best case approach and
asked for a number of clusters equal to that which
actually exists. We are currently working to develop
methods that will automatically stop at an optimal
number of clusters, to avoid setting this value man-
ually.
In general all of our results significantly improve
upon the majority classifier, which suggests that the
clustering of contexts is successfully discriminating
among ambiguous names and uncategorized email.
Table 3 shows the descriptive and discriminating
labels assigned to the 2?way experimental case of
American Airlines and Tom Cruise, as well as the
3?way case of George Bush, Bill Gates and Tom
Cruise. The bold face labels are those that serve
as both descriptive and discriminating labels. The
fact that most labels serve both roles suggests that
the highest ranked bigrams in each cluster were also
unique to that cluster. The normal font indicates
labels that are only descriptive, and are shared be-
tween multiple clusters. There are only a few such
cases, for example White House happens to be a sig-
nificant bigram in all three of the clusters in the 3?
way case. There were no labels that were exclu-
sively discriminating in these experiments, suggest-
ing that the clusters are fairly clearly distinguished.
Please note that some labels include unigrams
(e.g., President for George Bush). These are created
from bigrams where the other word is the conflated
form, which is not included in the labels since it is
by definition ambiguous.
6 Acknowledgements
This research is partially supported by a National
Science Foundation Faculty Early CAREER Devel-
opment Award (#0092784).
References
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
T. Pedersen and R. Bruce. 1997. Distinguishing word
senses in untagged text. In Proceedings of the Sec-
ond Conference on Empirical Methods in Natural Lan-
guage Processing, pages 197?207, Providence, RI,
August.
T. Pedersen, A. Purandare, and A. Kulkarni. 2005. Name
discrimination by clustering similar contexts. In Pro-
ceedings of the Sixth International Conference on In-
telligent Text Processing and Computational Linguis-
tics, pages 220?231, Mexico City, February.
A. Purandare and T. Pedersen. 2004. Word sense
discrimination by clustering contexts in vector and
similarity spaces. In Proceedings of the Conference
on Computational Natural Language Learning, pages
41?48, Boston, MA.
H. Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?123.
108
Assessing System Agreement and Instance Difficulty
in the Lexical Sample Tasks of SENSEVAL-2
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN, 55812 USA
tpederse@d.umn.edu
Abstract
This paper presents a comparative evalua-
tion among the systems that participated
in the Spanish and English lexical sam-
ple tasks of SENSEVAL-2. The focus is
on pairwise comparisons among systems
to assess the degree to which they agree,
and on measuring the difficulty of the test
instances included in these tasks.
1 Introduction
This paper presents a post-mortem analysis of
the English and Spanish lexical sample tasks of
SENSEVAL-2. Two closely related questions are
considered. First, to what extent did the compet-
ing systems agree? Did systems tend to be redun-
dant and have success with many of the same test in-
stances, or were they complementary and able to dis-
ambiguate different portions of the instance space?
Second, how much did the difficulty of the test in-
stances vary? Are there test instances that proved
unusually difficult to disambiguate relative to other
instances?
We address the first question via a series of pair-
wise comparisons among the participating systems
that measures their agreement via the kappa statis-
tic. We also introduce a simple measure of the de-
gree to which systems are complementary called op-
timal combination. We analyze the second question
by rating the difficulty of test instances relative to the
number of systems that were able to disambiguate
them correctly.
Nearly all systems that received official scores
in the Spanish and English lexical sample tasks of
SENSEVAL-2 are included in this study. There are
23 systems included from the English lexical sam-
ple task and eight from the Spanish. Table 1 lists the
systems and shows the number of test instances that
each disambiguated correctly, both by part of speech
and in total.
2 Pairwise System Agreement
Assessing agreement among systems sheds light on
whether their combined performance is potentially
more accurate than that of any of the individual sys-
tems. If several systems are largely in agreement,
then there is little benefit in combining them since
they are redundant and will simply reinforce one an-
other. However, if some systems disambiguate in-
stances that others do not, then they are complemen-
tary and it may be possible to combine them to take
advantage of the different strengths of each system
to improve overall accuracy.
The kappa statistic (Cohen, 1960) is a measure of
agreement between multiple systems (or judges) that
is scaled by the agreement that would be expected
just by chance. A value of 1.00 suggests complete
agreement, while 0.00 indicates pure chance agree-
ment. Negative values indicate agreement less than
what would be expected by chance. (Krippendorf,
1980) points out that it is difficult to specify a par-
ticular value of kappa as being generally indicative
of agreement. As such we simply use kappa as a
tool for comparison and relative ranking. A detailed
discussion on the use of kappa in natural language
processing is presented in (Carletta, 1996).
                     July 2002, pp. 40-46.  Association for Computational Linguistics.
                 Disambiguation: Recent Successes and Future Directions, Philadelphia,
                             Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense
Table 1: Lexical Sample Systems
system correct instances
name noun verb adj total (%)
English
1754 1806 768 4328 (1.00)
jhu final 1196 1022 562 2780 (0.64)
smuls 1219 1016 528 2763 (0.64)
kunlp 1171 1040 513 2724 (0.63)
cs224n 1198 945 527 2670 (0.62)
lia 1177 966 510 2653 (0.61)
talp 1149 927 495 2571 (0.59)
duluth3 1137 840 497 2473 (0.57)
umcp 1081 891 487 2459 (0.57)
ehu all 1069 891 480 2440 (0.56)
duluth4 1065 806 476 2346 (0.54)
duluth2 1056 795 483 2334 (0.54)
lesk corp 960 804 454 2218 (0.51)
duluthB 1004 729 467 2200 (0.51)
uned ls t 987 699 469 2155 (0.50)
common 880 728 453 2061 (0.48)
alicante 427 866 486 1779 (0.41)
uned ls u 781 519 437 1736 (0.40)
clr ls 602 393 272 1267 (0.29)
iit2 541 348 166 1054 (0.24)
iit1 516 337 182 1034 (0.24)
lesk 467 328 182 977 (0.23)
lesk def 438 159 108 704 (0.16)
random 303 153 155 611 (0.14)
Spanish
799 745 681 2225 (1.00)
jhu 560 478 546 1584 (0.71)
cs224 520 443 526 1489 (0.67)
umcp 482 435 479 1396 (0.63)
duluth8 494 382 494 1369 (0.62)
duluth7 470 374 480 1324 (0.60)
duluth9 445 359 446 1250 (0.56)
duluthY 411 325 434 1170 (0.53)
alicante 269 381 468 1118 (0.50)
To study agreement we have made a series of pair-
wise comparisons among the systems included in the
English and Spanish lexical sample tasks. Each pair-
wise combination is represented in a 2  2 contin-
gency table, where one cell represents the number of
test instances that both systems disambiguate cor-
rectly, one cell represents the number of instances
where both systems are incorrect, and there are two
cells to represent the counts when only one system is
correct. Agreement does not imply accuracy, since
two systems may get a large number of the same in-
stances incorrect and have a high rate of agreement.
Tables 2 and 3 show the system pairs in the
English and Spanish lexical sample tasks that ex-
hibit the highest level of agreement according to the
kappa statistic. The values in the both?one?zero col-
umn indicate the percentage of instances where both
systems are correct, where only one is correct, and
where neither is correct. The top 15 pairs are shown
for nouns and verbs, and the top 10 for adjectives.
A complete list would include about 250 pairs for
each part of speech for English and 24 such pairs for
Spanish.
The utility of kappa agreement is confirmed in
that system pairs known to be very similar have cor-
respondingly high measures. In Table 2, duluth2
and duluth3 exhibit a high kappa value for all parts
of speech. This is expected since duluth3 is an en-
semble approach that includes duluth2 as one of its
members. The same relationship exists between du-
luth7 and duluth8 in the Spanish lexical sample, and
comparable behavior is seen in Table 3.
A more surprising case is the even higher level of
agreement between the most common sense base-
line and the lesk corpus baseline shown in Table 2.
This is not necessarily expected, and suggests that
lesk corpus may not be finding a significant number
of matches between the Senseval contexts and the
WordNet glosses (as the lesk algorithm would hope
to do) but instead may be relying on a simple default
in many cases.
In previous work (Pedersen, 2001) we propose a
50-25-25 rule that suggests that about half of the in-
stances in a supervised word sense disambiguation
evaluation will be fairly easy for most systems to
resolve, another quarter will be harder but possible
for at least some systems, and that the final quar-
ter will be very difficult for any system to resolve.
Table 2: Pairwise Agreement English
system pair both-one-zero kappa
Nouns
common lesk corp 0.49 0.06 0.44 0.87
duluth2 duluth3 0.60 0.08 0.32 0.82
lesk corp umcp 0.53 0.11 0.36 0.78
duluth2 duluthB 0.54 0.14 0.32 0.70
iit1 iit2 0.24 0.13 0.63 0.69
duluth3 duluthB 0.56 0.15 0.29 0.68
common umcp 0.48 0.16 0.36 0.68
ehu all umcp 0.55 0.17 0.29 0.64
uned ls t uned ls u 0.41 0.19 0.40 0.63
duluth3 duluth4 0.55 0.18 0.27 0.61
duluth2 duluth4 0.52 0.19 0.29 0.60
duluth4 duluthB 0.51 0.19 0.29 0.59
ehu all lesk corp 0.50 0.20 0.30 0.59
cs224n duluth3 0.58 0.19 0.24 0.58
cs224n duluth4 0.55 0.19 0.25 0.58
Verbs
common lesk corp 0.39 0.06 0.55 0.88
duluth2 duluth3 0.43 0.07 0.50 0.85
duluth3 duluth4 0.39 0.14 0.47 0.72
duluth2 duluth4 0.38 0.15 0.47 0.69
lesk corp umcp 0.38 0.17 0.44 0.65
common umcp 0.36 0.18 0.46 0.65
cs224n duluth3 0.40 0.20 0.40 0.60
cs224n duluth4 0.38 0.21 0.41 0.59
cs224n duluth2 0.38 0.21 0.40 0.57
uned ls t uned ls u 0.24 0.20 0.55 0.56
duluth3 lia 0.39 0.23 0.38 0.54
lesk corp talp 0.36 0.23 0.40 0.54
cs224n lia 0.41 0.24 0.35 0.53
common talp 0.34 0.24 0.42 0.52
kunlp talp 0.42 0.24 0.33 0.51
Adjectives
common lesk corp 0.59 0.00 0.41 0.99
duluth2 duluth3 0.63 0.03 0.34 0.93
lesk corp umcp 0.58 0.07 0.35 0.86
duluth2 duluthB 0.59 0.07 0.35 0.86
duluth3 duluthB 0.60 0.07 0.34 0.86
common umcp 0.58 0.07 0.35 0.86
duluth4 duluthB 0.55 0.14 0.32 0.71
duluth3 duluth4 0.57 0.14 0.30 0.71
cs224n duluth3 0.60 0.13 0.27 0.70
cs224n duluth2 0.59 0.14 0.27 0.70
Table 3: Pairwise Agreement Spanish
system pair both-one-zero kappa
Nouns
duluth7 duluth8 0.57 0.11 0.32 0.76
umcp duluth9 0.50 0.17 0.33 0.65
duluth7 duluthY 0.49 0.21 0.30 0.57
umcp duluthY 0.48 0.21 0.31 0.56
duluth8 duluthY 0.50 0.21 0.29 0.56
umcp duluth8 0.50 0.23 0.27 0.51
umcp duluth7 0.50 0.23 0.27 0.51
cs224 umcp 0.51 0.24 0.25 0.49
duluth9 duluthY 0.44 0.26 0.30 0.47
duluth8 duluth9 0.47 0.27 0.27 0.45
cs224 duluth9 0.47 0.27 0.26 0.44
cs224 jhu 0.55 0.25 0.20 0.43
cs224 duluth8 0.51 0.27 0.22 0.42
jhu umcp 0.51 0.29 0.21 0.38
jhu duluth8 0.53 0.28 0.19 0.37
Verbs
duluth7 duluth8 0.48 0.08 0.44 0.84
duluth8 duluth9 0.44 0.14 0.42 0.72
umcp duluth8 0.48 0.14 0.37 0.71
umcp duluth9 0.46 0.16 0.38 0.69
duluth7 duluth9 0.42 0.16 0.42 0.68
umcp duluth7 0.47 0.16 0.37 0.68
duluth8 duluthY 0.44 0.16 0.39 0.67
duluth9 duluthY 0.42 0.18 0.40 0.65
duluth7 duluthY 0.43 0.18 0.39 0.64
umcp duluthY 0.46 0.19 0.35 0.61
cs224 umcp 0.49 0.19 0.32 0.61
cs224 duluth8 0.44 0.25 0.32 0.50
alicante umcp 0.42 0.25 0.33 0.50
cs224 duluth7 0.43 0.26 0.32 0.48
cs224 jhu 0.49 0.25 0.26 0.48
Adjectives
duluth7 duluth8 0.69 0.06 0.25 0.85
duluth7 duluthY 0.60 0.14 0.26 0.68
umcp duluthY 0.60 0.15 0.26 0.67
umcp duluth9 0.61 0.14 0.25 0.67
duluth8 duluthY 0.61 0.15 0.24 0.67
umcp duluth8 0.64 0.15 0.21 0.64
duluth9 duluthY 0.56 0.18 0.26 0.60
duluth8 duluth9 0.61 0.17 0.22 0.59
umcp duluth7 0.62 0.17 0.21 0.59
duluth7 duluth9 0.58 0.20 0.22 0.54
This same idea could also be expressed by stating
that the kappa agreement between two word sense
disambiguation systems will likely be around 0.50.
In fact this is a common result in the full set of pair-
wise comparisons, particularly for overall results not
broken down by part of speech. Tables 2 and 3 only
list the largest kappa values, but even there kappa
quickly reduces towards 0.50. These same tables
show that it is rare for two systems to agree on more
than 60% of the correctly disambiguated instances.
3 Optimal Combination
An optimal combination is the accuracy that could
be attained by a hypothetical tool called an optimal
combiner that accepts as input the sense assignments
for a test instance as generated by several different
systems. It is able to select the correct sense from
these inputs, and will only be wrong when none of
the sense assignments is the correct one. Thus, the
percentage accuracy of an optimal combiner is equal
to one minus the percentage of instances that no sys-
tem can resolve correctly.
Of course this is only a tool for thought experi-
ments and is not a practical algorithm. An optimal
combiner can establish an upper bound on the accu-
racy that could reasonably be attained over a partic-
ular sample of test instances.
Tables 4 and 5 list the top system pairs ranked
by optimal combination (1.00 - value in zero col-
umn) for the English and Spanish lexical samples.
Kappa scores are also shown to illustrate the inter-
action between agreement and optimal combination.
Optimal combination is maximized when the per-
centage of instances where both systems are wrong
is minimized. Kappa agreement is maximized by
minimizing the percentage of instances where one
or the other system (but not both) is correct. Thus,
the only way a system pair could have a high mea-
sure of kappa and a high measure of optimal com-
bination is if they were very accurate systems that
disambiguated many of the same test instances cor-
rectly.
System pairs with low measures of agreement
are potentially quite interesting because they are the
most likely to make complementary errors. For ex-
ample, in Table 5 under nouns, the alicante system
has a low level of agreement with all of the other
Table 4: Optimal Combination English
system pair both-one-zero kappa
Nouns
kunlp smuls 0.49 0.39 0.12 0.11
smuls talp 0.48 0.39 0.13 0.11
cs224n kunlp 0.48 0.39 0.13 0.11
ehu all smuls 0.47 0.40 0.13 0.10
cs224n talp 0.48 0.39 0.14 0.13
jhu final kunlp 0.49 0.37 0.14 0.16
smuls umcp 0.45 0.41 0.14 0.10
kunlp lia 0.48 0.38 0.14 0.14
lia talp 0.47 0.39 0.14 0.13
jhu final talp 0.48 0.38 0.14 0.15
duluth3 kunlp 0.47 0.38 0.15 0.15
cs224n ehu all 0.48 0.38 0.15 0.16
ehu all lia 0.47 0.38 0.15 0.15
ehu all jhu final 0.48 0.36 0.16 0.19
duluth3 talp 0.47 0.38 0.16 0.17
Verbs
jhu final kunlp 0.34 0.46 0.20 0.06
ehu all jhu final 0.31 0.44 0.21 0.07
ehu all smuls 0.31 0.44 0.21 0.07
ehu all kunlp 0.33 0.41 0.22 0.13
kunlp smuls 0.36 0.43 0.22 0.13
cs224n ehu all 0.29 0.45 0.22 0.05
ehu all lia 0.30 0.44 0.22 0.08
cs224n kunlp 0.33 0.44 0.23 0.11
alicante ehu all 0.26 0.47 0.23 0.03
kunlp lia 0.34 0.42 0.23 0.14
jhu final talp 0.32 0.44 0.24 0.12
duluth3 ehu all 0.26 0.46 0.24 0.05
ehu all talp 0.30 0.41 0.24 0.13
alicante jhu final 0.30 0.45 0.24 0.09
jhu final umcp 0.30 0.45 0.24 0.09
Adjectives
alicante jhu final 0.46 0.37 0.08 0.03
alicante smuls 0.41 0.41 0.09 -0.04
alicante cs224n 0.42 0.40 0.09 -0.01
alicante kunlp 0.41 0.39 0.11 0.03
alicante lia 0.41 0.39 0.11 0.03
alicante duluth3 0.40 0.40 0.11 0.02
alicante talp 0.40 0.41 0.11 0.02
alicante ehu all 0.41 0.39 0.11 0.05
alicante umcp 0.39 0.40 0.12 0.04
alicante duluth2 0.39 0.40 0.12 0.03
Table 5: Optimal Combination Spanish
system pair both-one-zero kappa
Nouns
alicante jhu 0.29 0.32 0.11 0.06
alicante duluth7 0.27 0.34 0.12 0.03
alicante duluthY 0.25 0.35 0.12 0.01
alicante duluth8 0.28 0.32 0.13 0.08
alicante cs224 0.28 0.32 0.13 0.09
alicante umcp 0.26 0.33 0.14 0.06
alicante duluth9 0.26 0.31 0.16 0.14
jhu duluthY 0.46 0.36 0.18 0.24
jhu duluth7 0.51 0.29 0.19 0.35
jhu duluth8 0.53 0.28 0.19 0.37
cs224 jhu 0.55 0.25 0.20 0.43
jhu duluth9 0.46 0.34 0.20 0.29
jhu umcp 0.51 0.29 0.21 0.38
cs224 duluth7 0.49 0.30 0.22 0.36
cs224 duluth8 0.51 0.27 0.22 0.42
Verbs
jhu duluthY 0.39 0.38 0.23 0.23
jhu umcp 0.48 0.27 0.25 0.44
jhu duluth9 0.39 0.36 0.26 0.29
jhu duluth8 0.42 0.32 0.26 0.35
cs224 jhu 0.49 0.25 0.26 0.48
jhu duluth7 0.42 0.32 0.26 0.36
alicante jhu 0.45 0.26 0.29 0.47
cs224 duluthY 0.43 0.27 0.30 0.46
alicante cs224 0.41 0.28 0.31 0.44
alicante duluthY 0.35 0.34 0.31 0.32
cs224 umcp 0.49 0.19 0.32 0.61
cs224 duluth7 0.43 0.26 0.32 0.48
cs224 duluth8 0.44 0.25 0.32 0.50
cs224 duluth9 0.41 0.27 0.32 0.47
alicante umcp 0.42 0.25 0.33 0.50
Adjectives
jhu duluth8 0.66 0.22 0.12 0.39
jhu duluth7 0.64 0.24 0.12 0.36
jhu duluthY 0.56 0.31 0.12 0.25
alicante jhu 0.62 0.26 0.13 0.33
jhu duluth9 0.59 0.29 0.13 0.29
cs224 jhu 0.70 0.16 0.13 0.51
jhu umcp 0.64 0.23 0.13 0.38
alicante cs224 0.61 0.24 0.15 0.39
cs224 duluth8 0.66 0.19 0.16 0.50
cs224 duluth7 0.64 0.20 0.16 0.49
Table 6: Difficulty of Instances
# noun verb adj total
English
0 59 (16) 174 (6) 29 (8) 262 (8)
1 51 (15) 116 (10) 26 (14) 193 (12)
2 59 (18) 122 (12) 41 (21) 222 (15)
3 64 (19) 117 (16) 29 (23) 210 (18)
4 84 (17) 102 (16) 28 (18) 214 (17)
5 76 (23) 76 (18) 24 (20) 176 (21)
6 53 (28) 61 (30) 23 (31) 137 (29)
7 51 (29) 65 (22) 23 (34) 139 (27)
8 62 (27) 58 (34) 18 (31) 138 (30)
9 47 (32) 69 (28) 17 (26) 133 (29)
10 62 (28) 61 (32) 18 (30) 141 (30)
11 55 (39) 56 (26) 21 (38) 132 (34)
12 80 (40) 61 (41) 22 (35) 163 (40)
13 86 (58) 56 (34) 21 (45) 163 (48)
14 125 (65) 62 (49) 33 (51) 220 (59)
15 131 (77) 125 (99) 36 (60) 292 (84)
16 141 (83) 107 (117) 61 (70) 309 (92)
17 133 (75) 100 (162) 86 (74) 319 (101)
18 92 (73) 80 (203) 102 (80) 274 (113)
19 97 (68) 59 (170) 49 (77) 205 (100)
20 65 (66) 38 (192) 30 (49) 133 (96)
21 42 (68) 15 (155) 17 (47) 74 (79)
22 29 (70) 15 (73) 7 (39) 51 (67)
23 10 (49) 11 (52) 7 (38) 28 (47)
Spanish
0 50 (16) 126 (12) 52 (24) 228 (16)
1 81 (18) 63 (17) 32 (36) 176 (21)
2 63 (24) 69 (18) 42 (50) 174 (28)
3 63 (27) 55 (23) 39 (81) 157 (39)
4 74 (32) 47 (23) 43 (101) 164 (47)
5 94 (35) 49 (28) 35 (77) 178 (42)
6 87 (40) 61 (39) 57 (90) 205 (53)
7 182 (47) 94 (46) 88 (93) 364 (58)
8 105 (44) 181 (62) 293 (166) 579 (111)
Table 7: Difficulty of English Word Types
word-pos (test) mean word-pos (test) mean
collaborate-v (30) 20.2 circuit-n (85) 10.7
solemn-a (25) 18.3 sense-n (53) 10.6
holiday-n (31) 17.7 authority-n (92) 10.5
dyke-n (28) 17.5 replace-v (45) 10.4
graceful-a (29) 17.3 restraint-n (45) 10.3
vital-a (38) 16.7 live-v (67) 10.2
detention-n (32) 16.5 treat-v (44) 10.1
faithful-a (23) 16.5 free-a (82) 10.0
yew-n (28) 16.1 nature-n (46) 10.0
chair-n (69) 16.0 simple-a (66) 9.8
ferret-v (1) 16.0 dress-v (59) 9.7
blind-a (55) 15.7 cool-a (52) 9.7
lady-n (53) 15.5 bar-n (151) 9.5
spade-n (33) 15.3 stress-n (39) 9.5
hearth-n (32) 15.1 channel-n (73) 9.2
face-v (93) 15.1 match-v (42) 9.0
green-a (94) 14.9 natural-a (103) 9.0
fatigue-n (43) 14.9 serve-v (51) 8.8
oblique-a (29) 14.3 train-v (63) 8.7
nation-n (37) 14.0 post-n (79) 8.7
church-n (64) 13.8 fine-a (70) 8.6
local-a (38) 13.6 drift-v (32) 7.7
fit-a (29) 13.4 leave-v (66) 7.7
use-v (76) 13.4 play-v (66) 7.5
child-n (64) 13.0 wash-v (12) 7.4
wander-v (50) 12.9 keep-v (67) 7.4
begin-v (280) 12.6 work-v (60) 7.0
bum-n (45) 12.5 drive-v (42) 6.8
feeling-n (51) 11.4 develop-v (69) 6.6
facility-n (58) 11.1 carry-v (66) 6.3
colorless (35) 11.1 see-v (69) 6.3
grip-n (51) 11.1 strike-v (54) 5.9
day-n (145) 11.0 call-v (66) 5.8
mouth-n (60) 11.0 pull-v (60) 5.7
material-n (69) 11.0 turn-v (67) 5.0
art-n (98) 10.7 draw-v (41) 4.7
find-v (68) 4.2
Table 8: Difficulty of Spanish Word Types
word-pos (test) mean word-pos (test) mean
claro-a (66) 7.6 verde-a (33) 5.3
local-a (55) 7.4 canal-n (41) 5.3
popular-a (204) 7.1 clavar-v (44) 5.1
partido-n (57) 7.0 masa-n (41) 5.1
bomba-n (37) 6.8 apuntar-v (49) 4.9
brillante-a (87) 6.7 autoridad-n (34) 4.9
usar-v (56) 6.5 tocar-v (74) 4.8
tabla-n (41) 6.3 explotar-v (41) 4.7
vencer-v (65) 6.3 programa-n (47) 4.7
simple-a (57) 6.2 circuito-n (49) 4.3
hermano-n (57) 6.1 copiar-v (53) 4.3
apoyar-v (73) 6.0 actuar-v (55) 4.2
vital-a (79) 5.9 operacion-n (47) 4.2
gracia-n (61) 5.9 pasaje-n (41) 4.1
organo-n (81) 5.8 saltar-v (37) 4.1
corona-n (40) 5.5 tratar-v (70) 3.9
ciego-a (42) 5.5 natural-a (58) 3.9
corazon-n (47) 5.5 grano-n (22) 3.9
coronar-v (74) 5.4 conducir-v (54) 3.8
naturaleza-n (56) 5.4
systems. However, the measure of optimal combi-
nation is quite high, reaching 0.89 (1.00 - 0.11) for
the pair of alicante and jhu. In fact, all seven of the
other systems achieve their highest optimal combi-
nation value when paired with alicante.
This combination of circumstances suggests that
the alicante system is fundamentally different than
the other systems, and is able to disambiguate a cer-
tain set of instances where the other systems fail. In
fact the alicante system is different in that it is the
only Spanish lexical sample system that makes use
of the structure of Euro-WordNet, the source of the
sense inventory.
4 Instance Difficulty
The difficulty of disambiguating word senses can
vary considerably. A word with multiple closely re-
lated senses is likely to be more difficult than one
with a few starkly drawn differences. In supervised
learning, a particular sense of a word can be diffi-
cult to disambiguate if there are a small number of
training examples available.
Table 6 shows the distribution of the number of
instances that are successfully disambiguated by a
particular number of systems in both the English
and Spanish lexical samples. The value under the
# column shows the number of systems that are able
to disambiguate the number of noun, verb, adjec-
tive and total instances shown in the row. The aver-
age number of training examples available for the
correct answers associated with these instances is
shown in parenthesis. For example, the first line
shows that there were 59 noun instances that no sys-
tem (of 23) could disambiguate, and that there were
on average 16 training examples available for each
of the correct senses for these 59 instances.
Two very clear trends emerge. First, there are a
substantial number of instances that are not disam-
biguated correctly by any system (262 in English,
228 in Spanish) and there are a large number of in-
stances that are disambiguated by just a handful of
systems. In the English lexical sample, there are
1,277 test instances that are correctly disambiguated
by five or fewer of the 23 systems. This is nearly
30% of the test data, and confirms that this was a
very challenging set of test instances.
There is also a very clear correlation between the
number of training examples available for a particu-
lar sense of a word and the number of systems that
are able to disambiguate instances of that word cor-
rectly. For example, Table 6 shows that there were
174 English verb instances that no system disam-
biguated correctly. On average there were only 6
training examples for the correct senses of these in-
stances. However, there were 28 instances that all
23 English systems were able to disambiguate. For
these instances an average of 47 training examples
were available for each correct sense.
This correlation between instance difficulty and
number of training examples may suggest that future
SENSEVAL exercises provide a minimum number of
training examples for each sense, or adjust the scor-
ing to reflect the difficulty of disambiguating a sense
with very few training examples.
Finally, we assess the difficulty associated with
word types by calculating the average number of
systems that were able to disambiguate the instances
associated with that type. This information is pro-
vided for the English and Spanish lexical samples in
Tables 7 and 8. Each word is shown with its part of
speech, the number of test instances, and the average
number of systems that were able to disambiguate
each of the test instances.
The verb collaborate is the easiest according to
this metric in the English lexical sample. It has 30
test instances that were disambiguated correctly by
an average of 20.2 of the 23 systems. The verb find
proves to be the most difficult, with 68 test instances
disambiguated correctly by an average of 4.2 sys-
tems. A somewhat less extreme range of values is
observed for the Spanish lexical sample in Table 8.
The adjective claro had 66 test instances that were
disambiguated correctly by an average of 7.6 of the
8 systems. The most difficult word was the verb con-
ducir, which has 54 test instances that were disam-
biguated correctly by an average of 3.8 systems.
5 Conclusion
This paper presents an analysis of the results from
the English and Spanish lexical sample tasks of
SENSEVAL-2. The analysis is based on the kappa
statistic and a measure known as optimal combina-
tion. It also assesses the difficulty of the test in-
stances in these lexical samples. We find that there
are a significant number of test instances that were
not disambiguated correctly by any system, and that
there is some correlation between instance difficulty
and the number of available training examples.
6 Acknowledgments
This work has been partially supported by a National
Science Foundation Faculty Early CAREER Devel-
opment award (#0092784).
References
J. Carletta. 1996. Assessing agreement on classification
tasks: the kappa statistic. Computational Linguistics,
22(2).
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educ. Psychol. Meas., 20:37?46.
K. Krippendorf. 1980. Content Analysis: An Introduc-
tion to its Methodology. Sage Publications, Thousand
Oaks, CA.
T. Pedersen. 2001. Machine learning with lexical fea-
tures: The Duluth approach to SENSEVAL-2. In Pro-
ceedings of the Senseval-2 Workshop, Toulouse, July.
Evaluating the Effectiveness of Ensembles of Decision Trees
in Disambiguating Senseval Lexical Samples
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN, 55812 USA
tpederse@d.umn.edu
Abstract
This paper presents an evaluation of an
ensemble?based system that participated
in the English and Spanish lexical sample
tasks of SENSEVAL-2. The system com-
bines decision trees of unigrams, bigrams,
and co?occurrences into a single classi-
fier. The analysis is extended to include
the SENSEVAL-1 data.
1 Introduction
There were eight Duluth systems that participated
in the English and Spanish lexical sample tasks of
SENSEVAL-2. These systems were all based on the
combination of lexical features with standard ma-
chine learning algorithms. The most accurate of
these systems proved to be Duluth3 for English and
Duluth8 for Spanish. These only differ with respect
to minor language specific issues, so we refer to
them generically as Duluth38, except when the lan-
guage distinction is important.
Duluth38 is an ensemble approach that assigns a
sense to an instance of an ambiguous word by taking
a vote among three bagged decision trees. Each tree
is learned from a different view of the training ex-
amples associated with the target word. Each view
of the training examples is based on one of the fol-
lowing three types of lexical features: single words,
two word sequences that occur anywhere within the
context of the word being disambiguated, and two
word sequences made up of this target word and an-
other word within one or two positions. These fea-
tures are referred to as unigrams, bigrams, and co?
occurrences.
The focus of this paper is on determining if the
member classifiers in the Duluth38 ensemble are
complementary or redundant with each other and
with other participating systems. Two classifiers
are complementary if they disagree on a substantial
number of disambiguation decisions and yet attain
comparable levels of overall accuracy. Classifiers
are redundant if they arrive at the same disambigua-
tion decisions for most instances of the ambiguous
word. There is little advantage in creating an ensem-
ble of redundant classifiers, since they will make the
same disambiguation decisions collectively as they
would individually. An ensemble can only improve
upon the accuracy of its member classifiers if they
are complementary to each other, and the errors of
one classifier are offset by the correct judgments of
others.
This paper continues with a description of the
lexical features that make up the Duluth38 system,
and then profiles the SENSEVAL-1 and SENSEVAL-
2 lexical sample data that is used in this evaluation.
There are two types of analysis presented. First, the
accuracy of the member classifiers in the Duluth38
ensemble are evaluated individually and in pair-
wise combinations. Second, the agreement between
Duluth38 and the top two participating systems in
SENSEVAL-1 and SENSEVAL-2 is compared. This
paper concludes with a review of the origins of our
approach. Since the focus here is on analysis, imple-
mentation level details are not extensively discussed.
Such descriptions can be found in (Pedersen, 2001b)
or (Pedersen, 2002).
                     July 2002, pp. 81-87.  Association for Computational Linguistics.
                 Disambiguation: Recent Successes and Future Directions, Philadelphia,
                             Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense
2 Lexical Features
Unigram features represent words that occur five or
more times in the training examples associated with
a given target word. A stop?list is used to eliminate
high frequency function words as features.
For example, if the target word is water and the
training example is I water the flowering flowers,
the unigrams water, flowering and flowers are eval-
uated as possible unigram features. No stemming
or other morphological processing is performed, so
flowering and flowers are considered as distinct uni-
grams. I and the are not considered as possible fea-
tures since they are included in the stop?list.
Bigram features represent two word sequences
that occur two or more times in the training exam-
ples associated with a target word, and have a log?
likelihood value greater than or equal to 6.635. This
corresponds to a p?value of 0.01, which indicates
that according to the log?likelihood ratio there is a
99% probability that the words that make up this bi-
gram are not independent.
If we are disambiguating channel and have the
training example Go to the channel quickly, then
the three bigrams Go to, the channel, and channel
quickly will be considered as possible features. to
the is not included since both words are in the stop?
list.
Co?occurrence features are defined to be a pair
of words that include the target word and another
word within one or two positions. To be selected as
a feature, a co?occurrence must occur two or more
times in the lexical sample training data, and have a
log?likelihood value of 2.706, which corresponds to
a p?value of 0.10. A slightly higher p?value is used
for the co?occurrence features, since the volume of
data is much smaller than is available for the bigram
features.
If we are disambiguating art and have the training
example He and I like art of a certain period, we
evaluate I art, like art, art of, and art a as possible
co?occurrence features.
All of these features are binary, and indicate if the
designated unigram, bigram, or co?occurrence ap-
pears in the context with the ambiguous word. Once
the features are identified from the training examples
using the methods described above, the decision tree
learner selects from among those features to deter-
mine which are most indicative of the sense of the
ambiguous word. Decision tree learning is carried
out with the Weka J48 algorithm (Witten and Frank,
2000), which is a Java implementation of the classic
C4.5 decision tree learner (Quinlan, 1986).
3 Experimental Data
The English lexical sample for SENSEVAL-1 is
made up of 35 words, six of which are used in mul-
tiple parts of speech. The training examples have
been manually annotated based on the HECTOR
sense inventory. There are 12,465 training exam-
ples, and 7,448 test instances. This corresponds to
what is known as the trainable lexical sample in the
SENSEVAL-1 official results.
The English lexical sample for SENSEVAL-2 con-
sists of 73 word types, each of which is associ-
ated with a single part of speech. There are 8,611
sense?tagged examples provided for training, where
each instance has been manually assigned a Word-
Net sense. The evaluation data for the English lexi-
cal sample consists of 4,328 held out test instances.
The Spanish lexical sample for SENSEVAL-2 con-
sists of 39 word types. There are 4,480 training ex-
amples that have been manually tagged with senses
from Euro-WordNet. The evaluation data consists of
2,225 test instances.
4 System Results
This section (and Table 1) summarizes the per-
formance of the top two participating systems in
SENSEVAL-1 and SENSEVAL-2, as well as the Du-
luth3 and Duluth8 systems. Also included are base-
line results for a decision stump and a majority clas-
sifier. A decision stump is simply a one node deci-
sion tree based on a co?occurrence feature, while the
majority classifier assigns the most frequent sense in
the training data to every occurrence of that word in
the test data.
Results are expressed using accuracy, which is
computed by dividing the total number of correctly
disambiguated test instances by the total number of
test instances. Official results from SENSEVAL are
reported using precision and recall, so these are con-
verted to accuracy to provide a consistent point of
comparison. We utilize fine grained scoring, where
a word is considered correctly disambiguated only if
it is assigned exactly the sense indicated in the man-
ually created gold standard.
In the English lexical sample task of SENSEVAL-1
the two most accurate systems overall were hopkins-
revised (77.1%) and ets-pu-revised (75.6%). The
Duluth systems did not participate in this exercise,
but have been evaluated using the same data after
the fact. The Duluth3 system reaches accuracy of
70.3%. The simple majority classifier attains accu-
racy of 56.4%.
In the English lexical sample task of SENSEVAL-
2 the two most accurate systems were JHU(R)
(64.2%) and SMUls (63.8%). Duluth3 attains an ac-
curacy of 57.3%, while a simple majority classifier
attains accuracy of 47.4%.
In the Spanish lexical sample task of SENSEVAL-
2 the two most accurate systems were JHU(R)
(68.1%) and stanford-cs224n (66.9%). Duluth8 has
accuracy of 61.2%, while a simple majority classi-
fier attains accuracy of 47.4%.
The top two systems from the first and sec-
ond SENSEVAL exercises represent a wide range of
strategies that we can only hint at here. The SMUls
English lexical sample system is perhaps the most
distinctive in that it incorporates information from
WordNet, the source of the sense distinctions in
SENSEVAL-2. The hopkins-revised, JHU(R), and
stanford-cs224n systems use supervised algorithms
that learn classifiers from a rich combination of syn-
tactic and lexical features. The ets-pu-revised sys-
tem may be the closest in spirit to our own, since it
creates an ensemble of two Naive Bayesian classi-
fiers, where one is based on topical context and the
other on local context.
More detailed description of the SENSEVAL-1
and SENSEVAL-2 systems and lexical samples can
be found in (Kilgarriff and Palmer, 2000) and (Ed-
monds and Cotton, 2001), respectively.
5 Decomposition of Ensembles
The three bagged decision trees that make up Du-
luth38 are evaluated both individually and as pair-
wise ensembles. In Table 1 and subsequent discus-
sion, we refer to the individual bagged decision trees
based on unigrams, bigrams and co?occurrences as
U, B, and C, respectively. We designate ensembles
that consist of two or three bagged decision trees by
Table 1: Accuracy in Lexical Sample Tasks
system accuracy correct
English SENSEVAL-1
hopkins-revised 77.1% 5,742.4
ets-pu-revised 75.6% 5,630.7
UC 71.3% 5,312.8
UBC 70.3% 5,233.9
BC 70.1% 5,221.7
UB 69.5% 5,176.0
C 69.0% 5,141.8
B 68.1% 5,074.7
U 63.6% 4,733.7
stump 60.7% 4,521.0
majority 56.4% 4,200.0
English SENSEVAL-2
JHU(R) 64.2% 2,778.6
SMUls 63.8% 2,761.3
UBC 57.3% 2,480.7
UC 57.2% 2,477.5
BC 56.7% 2,452.0
C 56.0% 2,423.7
UB 55.6% 2,406.0
B 54.4% 2,352.9
U 51.7% 2,238.2
stump 50.0% 2,165.8
majority 47.4% 2,053.3
Spanish SENSEVAL-2
JHU(R) 68.1% 1,515.2
stanford-cs224n 66.9% 1,488.5
UBC 61.2% 1,361.3
BC 60.1% 1,337.0
UC 59.4% 1,321.9
UB 59.0% 1,312.5
B 58.6% 1,303.7
C 58.6% 1,304.2
stump 52.6% 1,171.0
U 51.5% 1,146.0
majority 47.4% 1,053.7
using the relevant combinations of letters. For exam-
ple, UBC refers to a three member ensemble consist-
ing of unigram (U), bigram (B), and co?occurrence
(C) decision trees, while BC refers to a two member
ensemble of bigram (B) and co-occurrence (C) deci-
sion trees. Note of course that UBC is synonymous
with Duluth38.
Table 1 shows that Duluth38 (UBC) achieves ac-
curacy significantly better than the lower bounds
represented by the majority classifier and the de-
cision stump, and comes within seven percentage
points of the most accurate systems in each of the
three lexical sample tasks. However, UBC does not
significantly improve upon all of its member clas-
sifiers, suggesting that the ensemble is made up of
redundant rather than complementary classifiers.
In general the accuracies of the bigram (B) and
co?occurrence (C) decision trees are never signifi-
cantly different than the accuracy attained by the en-
sembles of which they are members (UB, BC, UC,
and UBC), nor are they significantly different from
each other. This is an intriguing result, since the
co?occurrences represent a much smaller feature set
than bigrams, which are in turn much smaller than
the unigram feature set. Thus, the smallest of our
feature sets is the most effective. This may be due to
the fact that small feature sets are least likely to suf-
fer from fragmentation during decision tree learning.
Of the three individual bagged decision trees U,
B, and C, the unigram tree (U) is significantly less
accurate for all three lexical samples. It is only
slightly more accurate than the decision stump for
both English lexical samples, and is less accurate
than the decision stump in the Spanish task.
The relatively poor performance of unigrams can
be accounted for by the large number of possible
features. Unigram features consist of all words not
in the stop?list that occur five or more times in the
training examples for a word. The decision tree
learner must search through a very large feature
space, and under such circumstances may fall vic-
tim to fragmentation.
Despite these results, we are not prepared to dis-
miss the use of ensembles or unigram decision trees.
An ensemble of unigram and co?occurrence de-
cision trees (UC) results in greater accuracy than
any other lexical decision tree for the English
SENSEVAL-1 lexical sample, and is essentially tied
with the most accurate of these approaches (UBC) in
the English SENSEVAL-2 lexical sample. In princi-
ple unigrams and co?occurrence features are com-
plementary, since unigrams represent topical con-
text, and co?occurrences represent local context.
This follows the line of reasoning developed by
(Leacock et al, 1998) in formulating their ensemble
of Naive Bayesian classifiers for word sense disam-
biguation.
Adding the bigram decision tree (B) to the ensem-
ble of the unigram and co?occurrence decision trees
(UC) to create UBC does not result in significant
improvements in accuracy for the any of the lexical
samples. This reflects the fact that the bigram and
co?occurrence feature sets can be redundant. Bi-
grams are two word sequences that occur anywhere
within the context of the ambiguous word, while
co?occurrences are bigrams that include the target
word and a word one or two positions away. Thus,
any consecutive two word sequence that includes the
word to be disambiguated and has a log?likelihood
ratio greater than the specified threshold will be con-
sidered both a bigram and a co?occurrence.
Despite the partial overlap between bigrams and
co?occurrences, we believe that retaining them as
separate feature sets is a reasonable idea. We have
observed that an ensemble of multiple decision trees
where each is learned from a representation of the
training examples that has a small number of fea-
tures is more accurate than a single decision tree
that is learned from one large representation of the
training examples. For example, we mixed the bi-
gram and co?occurrence features into a single fea-
ture set, and then learned a single bagged decision
tree from this representation of the training exam-
ples. We observed drops in accuracy in both the
Spanish and English SENSEVAL-2 lexical sample
tasks. For Spanish it falls from 59.4% to 58.2%, and
for English it drops from 57.2% to 54.9%. Interest-
ingly enough, this mixed feature set of bigrams and
co?occurrences results in a slight increase over an
ensemble of the two in the SENSEVAL-1 data, rising
from 71.3% to 71.5%.
6 Agreement Among Systems
The results in Table 1 show that UBC and its mem-
ber classifiers perform at levels of accuracy signif-
icantly higher than the majority classifier and de-
cision stumps, and approach the level of some of
the more accurate systems. This poses an intrigu-
ing possibility. If UBC is making complementary
errors to those other systems, then it might be pos-
sible to combine these systems to achieve an even
higher level of accuracy. The alternative is that the
decision trees based on lexical features are largely
redundant with these other systems, and that there
is a hard core of test instances that are resistant to
disambiguation by any of these systems.
We performed a series of pairwise comparisons
to establish the degree to which these systems agree.
We included the two most accurate participating sys-
tems from each of the three lexical sample tasks,
along with UBC, a decision stump, and a majority
classifier.
In Table 2 the column labeled ?both? shows the
percentage and count of test instances where both
systems are correct, the column labeled ?one? shows
the percentage and count where only one of the two
systems is correct, and the column labeled ?none?
shows how many test instances were not correctly
disambiguated by either system. We note that in
the pairwise comparisons there is a high level of
agreement for the instances that both systems were
able to disambiguate, regardless of the systems in-
volved. For example, in the SENSEVAL-1 results the
three pairwise comparisons among UBC, hopkins-
revised, and ets-pu-revised all show that approxi-
mately 65% of the test instances are correctly dis-
ambiguated by both systems. The same is true
for the English and Spanish lexical sample tasks in
SENSEVAL-2, where each pairwise comparison re-
sults in agreement in approximately half the test in-
stances.
Next we extend this study of agreement to a three?
way comparison between UBC, hopkins-revised,
and ets-pu-revised for the SENSEVAL-1 lexical sam-
ple. There are 4,507 test instances where all
three systems agree (60.5%), and 973 test instances
(13.1%) that none of the three is able to get correct.
These are remarkably similar values to the pair?wise
comparisons, suggesting that there is a fairly consis-
tent number of test instances that all three systems
handle in the same way. When making a five?way
comparison that includes these three systems and the
decision stump and the majority classifier, the num-
Table 2: System Pairwise Agreement
system pair both one zero
English SENSEVAL-1
hopkins ets-pu 67.8% 17.1% 12.1%
5,045 1,274 1,126
UBC hopkins 64.8% 18.3% 17.0%
4,821 1,361 1,263
UBC ets-pu 64.4% 17.4% 18.2%
4,795 1,295 1,355
stump majority 53.4% 13.7% 32.9%
3,974 1,022 2,448
English SENSEVAL-2
JHU(R) SMUls 50.4% 27.3% 22.3%
2,180 1,183 965
UBC JHU(R) 49.2% 24.1% 26.8%
2,127 1,043 1,158
UBC SMUls 47.2% 27.5% 25.2%
2,044 1,192 1,092
stump majority 45.2% 11.8% 43.0%
1,955 511 1,862
Spanish SENSEVAL-2
JHU(R) cs224n 52.9% 29.3% 17.8%
1,177 651 397
UBC cs224n 52.8% 23.2% 24.0%
1,175 517 533
UBC JHU(R) 48.3% 33.5% 18.2%
1,074 746 405
stump majority 45.4% 20.4% 34.2%
1,011 453 761
ber of test instances that no system can disambiguate
correctly drops to 888, or 11.93%. This is interest-
ing in that it shows there are nearly 100 test instances
that are only disambiguated correctly by the decision
stump or the majority classifier, and not by any of the
other three systems. This suggests that very simple
classifiers are able to resolve some test instances that
more complex techniques miss.
The agreement when making a three way compar-
ison between UBC, JHU(R), and SMUls in the En-
glish SENSEVAL-2 lexical sample drops somewhat
from the pair?wise levels. There are 1,791 test in-
stances that all three systems disambiguate correctly
(41.4%) and 828 instances that none of these sys-
tems get correct (19.1%). When making a five way
comparison between these three systems, the deci-
sion stump and the majority classifier, there are 755
test instances (17.4%) that no system can resolve.
This shows that these three systems are performing
somewhat differently, and do not agree as much as
the SENSEVAL-1 systems.
The agreement when making a three way com-
parison between UBC, JHU(R), and cs224n in the
Spanish lexical sample task of SENSEVAL-2 re-
mains fairly consistent with the pairwise compar-
isons. There are 960 test instances that all three
systems get correct (43.2%), and 308 test instances
where all three systems failed (13.8%). When mak-
ing a five way comparison between these three sys-
tems and the decision stump and the majority classi-
fier, there were 237 test instances (10.7%) where no
systems was able to resolve the sense. Here again
we see three systems that are handling quite a few
test instances in the same way.
Finally, the number of cases where neither the de-
cision stump nor the majority classifier is correct
varies from 33% to 43% across the three lexical sam-
ples. This suggests that the optimal combination of
a majority classifier and decision stump could attain
overall accuracy between 57% and 66%, which is
comparable with some of the better results for these
lexical samples. Of course, how to achieve such an
optimal combination is an open question. This is
still an interesting point, since it suggests that there
is a relatively large number of test instances that
require fairly minimal information to disambiguate
successfully.
7 Duluth38 Background
The origins of Duluth38 can be found in an ensem-
ble approach based on multiple Naive Bayesian clas-
sifiers that perform disambiguation via a majority
vote (Pedersen, 2000). Each member of the ensem-
ble is based on unigram features that occur in vary-
ing sized windows of context to the left and right of
the ambiguous word. The sizes of these windows are
0, 1, 2, 3, 4, 5, 10, 25, and 50 words to the left and
to the right, essentially forming bags of words to the
left and right. The accuracy of this ensemble disam-
biguating the nouns interest (89%) and line (88%) is
as high as any previously published results. How-
ever, each ensemble consists of 81 Naive Bayesian
classifiers, making it difficult to determine which
features and classifiers were contributing most sig-
nificantly to disambiguation.
The frustration with models that lack an intuitive
interpretation led to the development of decision
trees based on bigram features (Pedersen, 2001a).
This is quite similar to the bagged decision trees
of bigrams (B) presented here, except that the ear-
lier work learns a single decision tree where training
examples are represented by the top 100 ranked bi-
grams, according to the log?likelihood ratio. This
earlier approach was evaluated on the SENSEVAL-
1 data and achieved an overall accuracy of 64%,
whereas the bagged decision tree presented here
achieves an accuracy of 68% on that data.
Our interest in co?occurrence features is inspired
by (Choueka and Lusignan, 1985), who showed that
humans determine the meaning of ambiguous words
largely based on words that occur within one or
two positions to the left and right. Co?occurrence
features, generically defined as bigrams where one
of the words is the target word and the other oc-
curs within a few positions, have been widely used
in computational approaches to word sense disam-
biguation. When the impact of mixed feature sets
on disambiguation is analyzed, co?occurrences usu-
ally prove to contribute significantly to overall ac-
curacy. This is certainly our experience, where the
co?occurrence decision tree (C) is the most accurate
of the individual lexical decision trees. Likewise,
(Ng and Lee, 1996) report overall accuracy for the
noun interest of 87%, and find that that when their
feature set only consists of co?occurrence features
the accuracy only drops to 80%.
Our interest in bigrams was indirectly motivated
by (Leacock et al, 1998), who describe an ensem-
ble approach made up of local context and topical
context. They suggest that topical context can be
represented by words that occur anywhere in a win-
dow of context, while local contextual features are
words that occur within close proximity to the target
word. They show that in disambiguating the adjec-
tive hard and the verb serve that the local context is
most important, while for the noun line the topical
context is most important. We believe that statisti-
cally significant bigrams that occur anywhere in the
window of context can serve the same role, in that
such a two word sequence is likely to carry heavy
semantic (topical) or syntactic (local) weight.
8 Conclusion
This paper analyzes the performance of the Duluth3
and Duluth8 systems that participated in the English
and Spanish lexical sample tasks in SENSEVAL-
2. We find that an ensemble offers very limited
improvement over individual decision trees based
on lexical features. Co?occurrence decision trees
are more accurate than bigram or unigram decision
trees, and are nearly as accurate as the full ensemble.
This is an encouraging result, since the number of
co?occurrence features is relatively small and easy
to learn from compared to the number of bigram or
unigram features.
9 Acknowledgments
This work has been partially supported by a National
Science Foundation Faculty Early CAREER Devel-
opment award (#0092784).
The Duluth38 system (and all other Du-
luth systems that participated in SENSEVAL-2)
can be downloaded from the author?s web site:
http://www.d.umn.edu/?tpederse/code.html.
References
Y. Choueka and S. Lusignan. 1985. Disambiguation
by short contexts. Computers and the Humanities,
19:147?157.
P. Edmonds and S. Cotton, editors. 2001. Proceedings of
the Senseval?2 Workshop. Association for Computa-
tional Linguistics, Toulouse, France.
A. Kilgarriff and M. Palmer. 2000. Special issue on
SENSEVAL: Evaluating word sense disambiguation
programs. Computers and the Humanities, 34(1?2).
C. Leacock, M. Chodorow, and G. Miller. 1998. Using
corpus statistics and WordNet relations for sense iden-
tification. Computational Linguistics, 24(1):147?165,
March.
H.T. Ng and H.B. Lee. 1996. Integrating multiple
knowledge sources to disambiguate word sense: An
exemplar-based approach. In Proceedings of the 34th
Annual Meeting of the Association for Computational
Linguistics, pages 40?47.
T. Pedersen. 2000. A simple approach to building en-
sembles of Naive Bayesian classifiers for word sense
disambiguation. In Proceedings of the First Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 63?69,
Seattle, WA, May.
T. Pedersen. 2001a. A decision tree of bigrams is an ac-
curate predictor of word sense. In Proceedings of the
Second Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 79?86, Pittsburgh, July.
T. Pedersen. 2001b. Machine learning with lexical fea-
tures: The duluth approach to senseval-2. In Pro-
ceedings of the Senseval-2 Workshop, pages 139?142,
Toulouse, July.
T. Pedersen. 2002. A baseline methodology for word
sense disambiguation. In Proceedings of the Third In-
ternational Conference on Intelligent Text Processing
and Computational Linguistics, pages 126?135, Mex-
ico City, February.
J. Quinlan. 1986. Induction of decision trees. Machine
Learning, 1:81?106.
I. Witten and E. Frank. 2000. Data Mining - Practi-
cal Machine Learning Tools and Techniques with Java
Implementations. Morgan?Kaufmann, San Francisco,
CA.
An Evaluation Exercise for Word Alignment
Rada Mihalcea
Department of Computer Science
University of North Texas
Denton, TX 76203
rada@cs.unt.edu
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
tpederse@umn.edu
Abstract
This paper presents the task definition, re-
sources, participating systems, and compara-
tive results for the shared task on word align-
ment, which was organized as part of the
HLT/NAACL 2003 Workshop on Building and
Using Parallel Texts. The shared task in-
cluded Romanian-English and English-French
sub-tasks, and drew the participation of seven
teams from around the world.
1 Defining a Word Alignment Shared Task
The task of word alignment consists of finding correspon-
dences between words and phrases in parallel texts. As-
suming a sentence aligned bilingual corpus in languages
L1 and L2, the task of a word alignment system is to indi-
cate which word token in the corpus of language L1 cor-
responds to which word token in the corpus of language
L2.
As part of the HLT/NAACL 2003 workshop on ?Build-
ing and Using Parallel Texts: Data Driven Machine
Translation and Beyond?, we organized a shared task on
word alignment, where participating teams were provided
with training and test data, consisting of sentence aligned
parallel texts, and were asked to provide automatically
derived word alignments for all the words in the test set.
Data for two language pairs were provided: (1) English-
French, representing languages with rich resources (20
million word parallel texts), and (2) Romanian-English,
representing languages with scarce resources (1 million
word parallel texts). Similar with the Machine Transla-
tion evaluation exercise organized by NIST1, two sub-
tasks were defined, with teams being encouraged to par-
ticipate in both subtasks.
1http://www.nist.gov/speech/tests/mt/
1. Limited resources, where systems are allowed to use
only the resources provided.
2. Unlimited resources, where systems are allowed to
use any resources in addition to those provided.
Such resources had to be explicitly mentioned in the
system description.
Test data were released one week prior to the deadline
for result submissions. Participating teams were asked
to produce word alignments, following a common format
as specified below, and submit their output by a certain
deadline. Results were returned to each team within three
days of submission.
1.1 Word Alignment Output Format
The word alignment result files had to include one line
for each word-to-word alignment. Additionally, lines in
the result files had to follow the format specified in Fig.1.
While the  and confidence fields overlap in their
meaning, the intent of having both fields available is to
enable participating teams to draw their own line on what
they consider to be a Sure or Probable alignment. Both
these fields were optional, with some standard values as-
signed by default.
1.1.1 A Running Word Alignment Example
Consider the following two aligned sentences:
[English] s snum=18 They had gone . /s
[French] s snum=18 Ils etaient alles . /s
A correct word alignment for this sentence is
18 1 1
18 2 2
18 3 3
18 4 4
stating that: all the word alignments pertain to sentence
18, the English token 1 They aligns with the French to-
ken 1 Ils, the English token 2 had, aligns with the French
token 2 etaient, and so on. Note that punctuation is also
sentence no position L1 position L2 [ ] [confidence]
where:
? sentence no represents the id of the sentence within the
test file. Sentences in the test data already have an id as-
signed. (see the examples below)
? position L1 represents the position of the token that is
aligned from the text in language L1; the first token in each
sentence is token 1. (not 0)
? position L2 represents the position of the token that is
aligned from the text in language L2; again, the first token
is token 1.
?  can be either S or P, representing a Sure or Probable
alignment. All alignments that are tagged as S are also con-
sidered to be part of the P alignments set (that is, all align-
ments that are considered ?Sure? alignments are also part of
the ?Probable? alignments set). If the  field is missing, a
value of S will be assumed by default.
? confidence is a real number, in the range (0-1] (1 meaning
highly confident, 0 meaning not confident); this field is op-
tional, and by default confidence number of 1 was assumed.
Figure 1: Word Alignment file format
aligned (English token 4 aligned with French token 4),
and counts towards the final evaluation figures.
Alternatively, systems could also provide an 
marker and/or a confidence score, as shown in the fol-
lowing example:
18 1 1 1
18 2 2 P 0.7
18 3 3 S
18 4 4 S 1
with missing  fields considered by default to be S,
and missing confidence scores considered by default 1.
1.2 Annotation Guide for Word Alignments
The annotation guide and illustrative word alignment ex-
amples were mostly drawn from the Blinker Annotation
Project. Please refer to (Melamed, 1999, pp. 169?182)
for additional details.
1. All items separated by a white space are considered
to be a word (or token), and therefore have to be
aligned. (punctuation included)
2. Omissions in translation use the NULL token, i.e.
token with id 0. For instance, in the examples below:
[English]: s snum=18 And he said , appoint me
thy wages , and I will give it . /s
[French]: s snum=18 fixe moi ton salaire , et je
te le donnerai . /s
and he said from the English sentence has no cor-
responding translation in French, and therefore all
these words are aligned with the token id 0.
...
18 1 0
18 2 0
18 3 0
18 4 0
...
3. Phrasal correspondences produce multiple word-to-
word alignments. For instance, in the examples be-
low:
English: s snum=18 cultiver la terre /s
French: s snum=18 to be a husbandman /s
since the words do not correspond one to one, and
yet the two phrases mean the same thing in the given
context, the phrases should be linked as wholes, by
linking each word in one to each word in another.
For the example above, this translates into 12 word-
to-word alignments:
18 1 1 18 1 2
18 1 3 18 1 4
18 2 1 18 2 2
18 2 3 18 2 4
18 3 1 18 3 2
18 3 3 18 3 4
2 Resources
The shared task included two different language pairs:
the alignment of words in English-French parallel texts,
and in Romanian-English parallel texts. For each lan-
guage pair, training data were provided to participants.
Systems relying only on these resources were considered
part of the Limited Resources subtask. Systems making
use of any additional resources (e.g. bilingual dictionar-
ies, additional parallel corpora, and others) were classi-
fied under the Unlimited Resources category.
2.1 Training Data
Two sets of training data were made available.
1. A set of Romanian-English parallel texts, consist-
ing of about 1 million Romanian words, and about
the same number of English words. These data con-
sisted of:
 Parallel texts collected from the Web using a
semi-supervised approach. The URLs format
for pages containing potential parallel transla-
tions were manually identified (mainly from
the archives of Romanian newspapers). Next,
texts were automatically downloaded and sen-
tence aligned. A manual verification of the
alignment was also performed. These data col-
lection process resulted in a corpus of about
850,000 Romanian words, and about 900,000
English words.
 Orwell?s 1984, aligned within the MULTEXT-
EAST project (Erjavec et al, 1997), with about
130,000 Romanian words, and a similar num-
ber of English words.
 The Romanian Constitution, for about 13,000
Romanian words and 13,000 English words.
2. A set of English-French parallel texts, consisting of
about 20 million English words, and about the same
number of French words. This is a subset of the
Canadian Hansards, processed and sentence aligned
by Ulrich Germann at ISI (Germann, 2001).
All data were pre-tokenized. For English and French,
we used a version of the tokenizers provided within the
EGYPT Toolkit2. For Romanian, we used our own tok-
enizer. Identical tokenization procedures were used for
training, trial, and test data.
2.2 Trial Data
Two sets of trial data were made available at the same
time training data became available. Trial sets consisted
of sentence aligned texts, provided together with man-
ually determined word alignments. The main purpose
of these data was to enable participants to better under-
stand the format required for the word alignment result
files. Trial sets consisted of 37 English-French, and 17
Romanian-English aligned sentences.
2.3 Test Data
A total of 447 English-French aligned sentences (Och
and Ney, 2000), and 248 Romanian-English aligned sen-
tences were released one week prior to the deadline. Par-
ticipants were required to run their word alignment sys-
tems on these two sets, and submit word alignments.
Teams were allowed to submit an unlimited number of
results sets for each language pair.
2.3.1 Gold Standard Word Aligned Data
The gold standard for the two language pair alignments
were produced using slightly different alignment proce-
dures, which allowed us to study different schemes for
producing gold standards for word aligned data.
For English-French, annotators where instructed to as-
sign a Sure or Probable tag to each word alignment they
produced. The intersection of the Sure alignments pro-
duced by the two annotators led to the final Sure aligned
set, while the reunion of the Probable alignments led to
the final Probable aligned set. The Sure alignment set is
2http://www.clsp.jhu.edu/ws99/projects/mt/toolkit/
guaranteed to be a subset of the Probable alignment set.
The annotators did not produce any NULL alignments.
Instead, we assigned NULL alignments as a default back-
up mechanism, which forced each word to belong to at
least one alignment. The English-French aligned data
were produced by Franz Och and Hermann Ney (Och and
Ney, 2000).
For Romanian-English, annotators were instructed to
assign an alignment to all words, with specific instruc-
tions as to when to assign a NULL alignment. Annota-
tors were not asked to assign a Sure or Probable label.
Instead, we had an arbitration phase, where a third anno-
tator judged the cases where the first two annotators dis-
agreed. Since an inter-annotator agreement was reached
for all word alignments, the final resulting alignments
were considered to be Sure alignments.
3 Evaluation Measures
Evaluations were performed with respect to four differ-
ent measures. Three of them ? precision, recall, and F-
measure ? represent traditional measures in Information
Retrieval, and were also frequently used in previous word
alignment literature. The fourth measure was originally
introduced by (Och and Ney, 2000), and proposes the no-
tion of quality of word alignment.
Given an alignment , and a gold standard alignment
, each such alignment set eventually consisting of two
sets 

, 

, and 

, 

corresponding to Sure and
Probable alignments, the following measures are defined
(where  is the alignment type, and can be set to either S
or P).











(1)











(2)
	










(3)

  




 






 


(4)
Each word alignment submission was evaluated in
terms of the above measures. Moreover, we conducted
two sets of evaluations for each submission:
 NULL-Align, where each word was enforced to be-
long to at least one alignment; if a word did not be-
long to any alignment, a NULL Probable alignment
was assigned by default. This set of evaluations per-
tains to full coverage word alignments.
 NO-NULL-Align, where all NULL alignments were
removed from both submission file and gold stan-
dard data.
Team System name Description
Language Technologies Institute, CMU BiBr (Zhao and Vogel, 2003)
MITRE Corporation Fourday (Henderson, 2003)
RALI - Universite? the Montre?al Ralign (Simard and Langlais, 2003)
Romanian Academy Institute of Artificial Intelligence RACAI (Tufis? et al, 2003)
University of Alberta ProAlign (Lin and Cherry, 2003)
University of Minnesota, Duluth UMD (Thomson McInnes and Pedersen, 2003)
Xerox Research Centre Europe XRCE (Dejean et al, 2003)
Table 1: Teams participating in the word alignment shared task
We conducted therefore 14 evaluations for each
submission file: AER, Sure/Probable Precision,
Sure/Probable Recall, and Sure/Probable F-measure,
with a different figure determined for NULL-Align and
NO-NULL-Align alignments.
4 Participating Systems
Seven teams from around the world participated in the
word alignment shared task. Table 1 lists the names of
the participating systems, the corresponding institutions,
and references to papers in this volume that provide de-
tailed descriptions of the systems and additional analysis
of their results.
All seven teams participated in the Romanian-English
subtask, and five teams participated in the English-French
subtask.3 There were no restrictions placed on the num-
ber of submissions each team could make. This resulted
in a total of 27 submissions from the seven teams, where
14 sets of results were submitted for the English-French
subtask, and 13 for the Romanian-English subtask. Of
the 27 total submissions, there were 17 in the Limited re-
sources subtask, and 10 in the Unlimited resources sub-
task. Tables 2 and 3 show all of the submissions for each
team in the two subtasks, and provide a brief description
of their approaches.
While each participating system was unique, there
were a few unifying themes.
Four teams had approaches that relied (to varying de-
grees) on an IBM model of statistical machine translation
(Brown et al, 1993). UMD was a straightforward imple-
mentation of IBM Model 2, BiBr employed a boosting
procedure in deriving an IBM Model 1 lexicon, Ralign
used IBM Model 2 as a foundation for their recursive
splitting procedure, and XRCE used IBM Model 4 as a
base for alignment with lemmatized text and bilingual
lexicons.
Two teams made use of syntactic structure in the text
to be aligned. ProAlign satisfies constraints derived from
a dependency tree parse of the English sentence being
3The two teams that did not participate in English-French
were Fourday and RACAI.
aligned. BiBr also employs syntactic constraints that
must be satisfied. However, these come from parallel text
that has been shallowly parsed via a method known as
bilingual bracketing.
Three teams approached the shared task with baseline
or prototype systems. Fourday combines several intuitive
baselines via a nearest neighbor classifier, RACAI car-
ries out a greedy alignment based on an automatically
extracted dictionary of translations, and UMD?s imple-
mentation of IBMModel 2 provides an experimental plat-
form for their future work incorporating prior knowledge
about cognates. All three of these systems were devel-
oped within a short period of time before and during the
shared task.
5 Results and Discussion
Tables 4 and 5 list the results obtained by participating
systems in the Romanian-English task. Similarly, results
obtained during the English-French task are listed in Ta-
bles 6 and 7.
For Romanian-English, limited resources, XRCE sys-
tems (XRCE.Nolem-56k.RE.2 and XRCE.Trilex.RE.3)
seem to lead to the best results. These are systems that
are based on GIZA++, with or without additional re-
sources (lemmatizers and lexicons). For unlimited re-
sources, ProAlign.RE.1 has the best performance.
For English-French, Ralign.EF.1 has the best perfor-
mance for limited resources, while ProAlign.EF.1 has
again the largest number of top ranked figures for unlim-
ited resources.
To make a cross-language comparison, we paid partic-
ular attention to the evaluation of the Sure alignments,
since these were collected in a similar fashion (an agree-
ment had to be achieved between two different anno-
tators). The results obtained for the English-French
Sure alignments are significantly higher (80.54% best F-
measure) than those for Romanian-English Sure align-
ments (71.14% best F-measure). Similarly, AER for
English-French (5.71% highest error reduction) is clearly
better than the AER for Romanian-English (28.86% high-
est error reduction).
This difference in performance between the two data
sets is not a surprise. As expected, word alignment, like
many other NLP tasks (Banko and Brill, 2001), highly
benefits from large amounts of training data. Increased
performance is therefore expected when larger training
data sets are available.
The only evaluation set where Romanian-English data
leads to better performance is the Probable alignments
set. We believe however that these figures are not di-
rectly comparable, since the English-French Probable
alignments were obtained as a reunion of the align-
ments assigned by two different annotators, while for
the Romanian-English Probable set two annotators had
to reach an agreement (that is, an intersection of their in-
dividual alignment assignments).
Interestingly, in an overall evaluation, the limited re-
sources systems seem to lead to better results than those
with unlimited resources. Out of 28 different evaluation
figures, 20 top ranked figures are provided by systems
with limited resources. This suggests that perhaps using
a large number of additional resources does not seem to
improve a lot over the case when only parallel texts are
employed.
Ranked results for all systems are plotted in Figures 2
and 3. In the graphs, systems are ordered based on their
AER scores. System names are preceded by a marker to
indicate the system type: L stands for Limited Resources,
and U stands for Unlimited Resources.
6 Conclusion
A shared task on word alignment was organized as part
of the HLT/NAACL 2003 Workshop on Building and
Using Parallel Texts. In this paper, we presented the
task definition, and resources involved, and shortly de-
scribed the participating systems. The shared task in-
cluded Romanian-English and English-French sub-tasks,
and drew the participation of seven teams from around the
world. Comparative evaluations of results led to interest-
ing insights regarding the impact on performance of (1)
various alignment algorithms, (2) large or small amounts
of training data, and (3) type of resources available. Data
and evaluation software used in this exercise are available
online at http://www.cs.unt.edu/?rada/wpt.
Acknowledgments
There are many people who contributed greatly to mak-
ing this word alignment evaluation task possible. We are
grateful to all the participants in the shared task, for their
hard work and involvement in this evaluation exercise.
Without them, all these comparative analyses of word
alignment techniques would not be possible.
We are very thankful to Franz Och from ISI and Her-
mann Ney from RWTH Aachen for kindly making their
English-French word aligned data available to the work-
shop participants; the Hansards made available by Ul-
rich Germann from ISI constituted invaluable data for
the English-French shared task. We would also like to
thank the student volunteers from the Department of En-
glish, Babes-Bolyai University, Cluj-Napoca, Romania
who helped creating the Romanian-English word aligned
data.
We are also grateful to all the Program Committee
members of the current workshop, for their comments
and suggestions, which helped us improve the definition
of this shared task. In particular, we would like to thank
Dan Melamed for suggesting the two different subtasks
(limited and unlimited resources), and Michael Carl and
Phil Resnik for initiating interesting discussions regard-
ing phrase-based evaluations.
References
M. Banko and E. Brill. 2001. Scaling to very very large
corpora for natural language disambiguation. In Pro-
ceedings of the 39th Annual Meeting of the Association
for Computational Lingusitics (ACL-2001), Toulouse,
France, July.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguis-
tics, 19(2):263?311.
Herve Dejean, Eric Gaussier, Cyril Goutte, and Kenji
Yamada. 2003. Reducing parameter space for word
alignment. In HLT-NAACL 2003 Workshop: Building
and Using Parallel Texts: Data Driven Machine Trans-
lation and Beyond, pages 23?26, Edmonton, Alberta,
Canada, May 31. Association for Computational Lin-
guistics.
T. Erjavec, N. Ide, and D. Tufis?. 1997. Encoding and
parallel alignment of linguistic corpora in six central
and Eastern European languages. In Proceedings of
the Joint ACH/ALL Conference, Queen?s University,
Kingston, Ontario, June.
U. Germann. 2001. Aligned hansards of the 36th
parliament of canada. http://www.isi.edu/natural-
language/download/hansard/.
John C. Henderson. 2003. Word alignment baselines. In
HLT-NAACL 2003Workshop: Building and Using Par-
allel Texts: Data Driven Machine Translation and Be-
yond, pages 27?30, Edmonton, Alberta, Canada, May
31. Association for Computational Linguistics.
Dekang Lin and Colin Cherry. 2003. Proalign: Shared
task system description. In HLT-NAACL 2003 Work-
shop: Building and Using Parallel Texts: Data Driven
Machine Translation and Beyond, pages 11?14, Ed-
monton, Alberta, Canada, May 31. Association for
Computational Linguistics.
D.I. Melamed. 1999. Empirical Methods for Exploiting
Parallel Texts. MIT Press.
F. Och and H. Ney. 2000. A comparison of alignment
models for statistical machine translation. In Proceed-
ings of the 18th International Conference on Computa-
tional Linguistics (COLING-ACL 2000), Saarbrucken,
Germany, August.
Michel Simard and Philippe Langlais. 2003. Statisti-
cal translation alignment with compositionality con-
straints. In HLT-NAACL 2003 Workshop: Building
and Using Parallel Texts: Data Driven Machine Trans-
lation and Beyond, pages 19?22, Edmonton, Alberta,
Canada, May 31. Association for Computational Lin-
guistics.
Bridget Thomson McInnes and Ted Pedersen. 2003. The
duluth word alignment system. In HLT-NAACL 2003
Workshop: Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond, pages 40?
43, Edmonton, Alberta, Canada, May 31. Association
for Computational Linguistics.
Dan Tufis?, Ana-Maria Barbu, and Radu Ion. 2003. Treq-
al: A word alignment system with limited language
resources. In HLT-NAACL 2003 Workshop: Building
and Using Parallel Texts: Data Driven Machine Trans-
lation and Beyond, pages 36?39, Edmonton, Alberta,
Canada, May 31. Association for Computational Lin-
guistics.
D Tufis?. 2002. A cheap and fast way to build useful
translation lexicons. In Proceedings of the 19th In-
ternational Conference on Computational Linguistics,
pages 1030?1036, Taipei, August.
Bing Zhao and Stephan Vogel. 2003. Word alignment
based on bilingual bracketing. In HLT-NAACL 2003
Workshop: Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond, pages 15?
18, Edmonton, Alberta, Canada, May 31. Association
for Computational Linguistics.
System Resources Description
BiBr.EF.1 Limited baseline of bilingual bracketing
BiBr.EF.2 Unlimited baseline of bilingual bracketing + English POS tagging
BiBr.EF.3 Unlimited baseline of bilingual bracketing + English POS tagging and base NP
BiBr.EF.4 Limited reverse direction of BiBr.EF.1
BiBr.EF.5 Unlimited reverse direction of BiBr.EF.2
BiBr.EF.6 Unlimited reverse direction of BiBr.EF.3
BiBr.EF.7 Limited intersection of BiBr.EF.1 & BiBr.EF.3
BiBr.EF.8 Unlimited intersection of BiBr.EF.3 & BiBr.EF.6
ProAlign.EF.1 Unlimited cohesion between source and target language + English parser +
distributional similarity for English words
Ralign.EF.1 Limited Giza (IBM Model 2) + recursive parallel segmentation
UMD.EF.1 Limited IBM Model 2, trained with 1/20 of the corpus, distortion 2, iterations 4
XRCE.Base.EF.1 Limited GIZA++ (IBM Model 4) with English and French lemmatizer
XRCE.Nolem.EF.2 Limited GIZA++ only (IBM Model 4), trained with 1/4 of the corpus
XRCE.Nolem.EF.3 Limited GIZA++ only (IBM Model 4), trained with 1/2 of the corpus
Table 2: Short description for English-French systems
System Resources Description
BiBr.RE.1 Limited baseline of bilingual bracketing
BiBr.RE.2 Unlimited baseline of bilingual bracketing + English POS tagging
BiBr.RE.3 Unlimited baseline of bilingual bracketing + English POS tagging and base NP
Fourday.RE.1 Limited nearest neighbor combination of baseline measures
ProAlign.RE.1 Unlimited cohesion between source and target language + English parser +
distributional similarity for English words
RACAI.RE.1 Unlimited translation equivalence dictionary (Tufis?, 2002) + POS tagging
Ralign.RE.1 Limited Giza (IBM Model 2) + recursive parallel segmentation
UMD.RE.1 Limited IBM Model 2, trained with all the corpus, distortion 4, iterations 4
UMD.RE.2 Limited IBM Model 2, trained with all the corpus, distortion 2, iterations 4
XRCE.Base.RE.1 Limited GIZA++ (IBM Model 4), with English lemmatizer
XRCE.Nolem.RE.2 Limited GIZA++ only (IBM Model 4)
XRCE.Trilex.RE.3 Limited GIZA++ only (IBM Model 4), with English lemmatizer and trinity lexicon
XRCE.Trilex.RE.4 Limited GIZA++ only (IBM Model 4), with English lemmatizer and trinity lexicon
Table 3: Short description for Romanian-English systems
System 



	





	

AER
Limited Resources
BiBr.RE.1 70.65% 55.75% 62.32% 59.60% 57.65% 58.61% 41.39%
Fourday.RE.1 0.00% 0.00% 0.00% 52.83% 42.86% 47.33% 52.67%
Ralign.RE.1 92.00% 45.06% 60.49% 63.63% 65.92% 64.76% 35.24%
UMD.RE.1 57.67% 49.70% 53.39% 57.67% 49.70% 53.39% 46.61%
UMD.RE.2 58.29% 49.99% 53.82% 58.29% 49.99% 53.82% 46.18%
XRCE.Base.RE.1 79.28% 61.14% 69.03% 79.28% 61.14% 69.03% 30.97%
XRCE.Nolem-56K.RE.2 82.65% 62.44% 71.14% 82.65% 62.44% 71.14% 28.86%
XRCE.Trilex.RE.3 80.97% 61.89% 70.16% 80.97% 61.89% 70.16% 29.84%
XRCE.Trilex.RE.4 79.76% 61.31% 69.33% 79.76% 61.31% 69.33% 30.67%
Unlimited Resources
BiBr.RE.2 70.46% 55.51% 62.10% 58.40% 57.59% 57.99% 41.39%
BiBr.RE.3 70.36% 55.47% 62.04% 58.17% 58.12% 58.14% 41.86%
RACAI.RE.1 81.29% 60.26% 69.21% 81.29% 60.26% 69.21% 30.79%
ProAlign.RE.1 88.22% 58.91% 70.64% 88.22% 58.91% 70.64% 29.36%
Table 4: Results for Romanian-English, NO-NULL-Align
System 



	





	

AER
Limited Resources
BiBr.RE.1 70.65% 48.32% 57.39% 57.38% 52.62% 54.90% 45.10%
Fourday.RE.1 0.00% 0.00% 0.00% 35.85% 45.88% 40.25% 59.75%
Ralign.RE.1 92.00% 39.05% 54.83% 63.63% 57.13% 60.21% 39.79%
UMD.RE.1 56.21% 43.17% 48.84% 45.51% 47.76% 46.60% 53.40%
UMD.RE.2 56.58% 43.45% 49.15% 46.00% 47.88% 46.92% 53.08%
XRCE.Base.RE.1 79.28% 52.98% 63.52% 61.59% 61.50% 61.54% 38.46%
XRCE.Nolem-56K.RE.2 82.65% 54.12% 65.41% 61.59% 61.50% 61.54% 38.46%
XRCE.Trilex.RE.3 80.97% 53.64% 64.53% 63.64% 61.58% 62.59% 37.41%
XRCE.Trilex.RE.4 79.76% 53.14% 63.78% 62.22% 61.37% 61.79% 38.21%
Unlimited Resources
BiBr.RE.2 70.46% 48.11% 57.18% 56.01% 52.26% 54.07% 45.93%
BiBr.RE.3 70.36% 48.08% 57.12% 56.05% 52.87% 54.42% 45.58%
RACAI.RE.1 60.30% 62.38% 61.32% 59.87% 62.42% 61.12% 38.88%
ProAlign.RE.1 88.22% 51.06% 64.68% 61.71% 62.05% 61.88% 38.12%
Table 5: Results for Romanian-English, NULL-Align
System 



	





	

AER
Limited Resources
BiBr.EF.1 49.85% 79.45% 61.26% 67.23% 29.24% 40.76% 28.23%
BiBr.EF.4 51.46% 82.42% 63.36% 66.65% 32.68% 43.86% 28.01%
BiBr.EF.7 63.03% 74.59% 68.32% 66.11% 30.06% 41.33% 29.38%
Ralign.EF.1 72.54% 80.61% 76.36% 77.56% 38.19% 51.18% 18.50%
UMD.EF.1 37.98% 64.66% 47.85% 59.69% 23.53% 33.75% 38.47%
XRCE.Base.EF.1 50.89% 84.67% 63.57% 83.22% 32.05% 46.28% 16.23%
XRCE.Nolem.EF.2 55.54% 93.46% 69.68% 89.65% 34.92% 50.27% 8.93%
XRCE.Nolem.EF.3 55.43% 93.81% 69.68% 90.09% 35.30% 50.72% 8.53%
Unlimited Resources
BiBr.EF.2 50.05% 79.89% 61.54% 66.92% 29.14% 40.60% 28.24%
BiBr.EF.3 50.21% 80.26% 61.80% 63.79% 30.52% 41.29% 30.38%
BiBr.EF.5 51.27% 82.17% 63.15% 67.22% 32.56% 43.87% 27.71%
BiBr.EF.6 51.91% 83.26% 63.95% 62.21% 34.58% 44.45% 31.32%
BiBr.EF.8 66.34% 74.86% 70.34% 61.62% 31.37% 41.57% 32.48%
ProAlign.EF.1 71.94% 91.48% 80.54% 96.49% 28.41% 43.89% 5.71%
Table 6: Results for English-French, NO-NULL-Align
System 



	





	

AER
Limited Resources
BiBr.EF.1 49.85% 79.45% 61.26% 60.32% 29.12% 39.28% 33.37%
BiBr.EF.4 51.46% 82.42% 63.36% 61.64% 32.41% 42.48% 31.91%
BiBr.EF.7 63.03% 74.59% 68.32% 51.35% 30.45% 38.23% 40.97%
Ralign.EF.1 72.54% 80.61% 76.36% 77.56% 36.79% 49.91% 18.50%
UMD.EF.1 37.19% 64.66% 47.22% 41.93% 24.08% 30.59% 51.71%
XRCE.Base.EF.1 50.89% 84.67% 63.57% 64.96% 32.73% 43.53% 28.99%
XRCE.Nolem.EF.2 55.54% 93.46% 69.68% 70.98% 35.61% 47.43% 22.10%
XRCE.Nolem.EF.3 55.43% 93.81% 69.68% 72.01% 36.00% 48.00% 21.27%
Unlimited Resources
BiBr.EF.2 50.05% 79.89% 61.54% 59.89% 28.96% 39.04% 33.48%
BiBr.EF.3 50.21% 80.26% 61.80% 57.85% 30.28% 39.75% 35.03%
BiBr.EF.5 51.27% 82.17% 63.15% 62.05% 32.23% 42.43% 31.69%
BiBr.EF.6 51.91% 83.26% 63.95% 58.41% 34.20% 43.14% 34.47%
BiBr.EF.8 66.34% 74.86% 70.34% 48.50% 31.76% 38.38% 43.37%
ProAlign.EF.1 71.94% 91.48% 80.54% 56.02% 30.05% 39.62% 33.71%
Table 7: Results for English-French, NULL-Align
Figure 2: Ranked results for Romanian-English data
Figure 3: Ranked results for English-French data
The Duluth Word Alignment System
Bridget Thomson McInnes
Department of Computer Science
University of Minnesota
Duluth, MN 55812
bthomson@d.umn.edu
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
tpederse@umn.edu
Abstract
The Duluth Word Alignment System partici-
pated in the 2003 HLT-NAACL Workshop on
Parallel Text shared task on word alignment for
both English?French and Romanian?English.
It is a Perl implementation of IBM Model 2.
We used approximately 50,000 aligned sen-
tences as training data for each language pair,
and found the results for Romanian?English to
be somewhat better. We also varied the Model
2 distortion parameters among the values 2, 4,
and 6, but did not observe any significant dif-
ferences in performance as a result.
1 Introduction
Word alignment is a crucial part of any Machine Transla-
tion system, since it is the process of determining which
words in a given source and target language sentence
pair are translations of each other. This is a token level
task, meaning that each word (token) in the source text
is aligned with its corresponding translation in the target
text.
The Duluth Word Alignment System is a Perl imple-
mentation of IBM Model 2 (Brown et al, 1993). It learns
a probabilistic model from sentence aligned parallel text
that can then be used to align the words in another such
text (that was not a part of the training process).
A parallel text consists of a source language text and
its translation into some target language. If we have de-
termined which sentences are translations of each other
then the text is said to be sentence aligned, where we call
a source and target language sentence that are translations
of each other a sentence pair.
(Brown et al, 1993) introduced five statistical transla-
tion models (IBM Models 1 ? 5). In general a statistical
machine translation system is composed of three com-
ponents: a language model, a translation model, and a
decoder (Brown et al, 1988).
The language model tells how probable a given sen-
tence is in the source language, the translation model in-
dicates how likely it is that a particular target sentence is
a translation of a given source sentence, and the decoder
is what actually takes a source sentence as input and pro-
duces its translation as output. Our focus is on translation
models, since that is where word alignment is carried out.
The IBM Models start very simply and grow steadily
more complex. IBM Model 1 is based solely on the prob-
ability that a given word in the source language translates
as a particular word in the target language. Thus, a word
in the first position of the source sentence is just as likely
to translate to a word in the target sentence that is in the
first position versus one at the last position. IBM Model
2 augments these translation probabilities by taking into
account how likely it is for words at particular positions
in a sentence pair to be alignments of each other.
This paper continues with a more detailed description
of IBM Model 2. It goes on to present the implementa-
tion details of the Duluth Word Alignment System. Then
we describe the data and the parameters that were used
during the training and testing stages of the shared task
on word alignment. Finally, we discuss our experimental
results and briefly outline our future plans.
2 IBM Model 2
Model 2 is trained with sentence aligned parallel corpora.
However, our goal is learn a model that can perform word
alignment, and there are no examples of word alignments
given in the training data. Thus, we must cast the train-
ing process as a missing data problem, where we learn
about word alignments from corpora where only sentence
(but not word) alignments are available. As is common
with missing data problems, we use the Expectation?
Maximization (EM) Algorithm (Dempster et al, 1977)
to estimate the probabilities of word alignments in this
model.
The objective of Model 2 is to estimate the probability
that a given sentence pair is aligned a certain way. This
is represented by   	
 , where  is the source sen-
tence,

 is the target sentence, and  is the proposed word
alignment for the sentence pair. However, since this prob-
ability can?t be estimated directly from the training data,
we must reformulate it so we can use the EM algorithm.
From Bayes Rule we arrive at:
    


  
   


  
   
 (1)
where   
    is the probability of a proposed align-
ment of the words in the target sentence to the words in
the given source sentence. To estimate a probability for
a particular alignment, we must estimate the numerator
and then divide it by the sum of the probabilities of all
possible alignments given the source sentence.
While clear in principle, there are usually a huge num-
ber of possible word alignments between a source and
target sentence, so we can?t simply estimate this for ev-
ery possible alignment. Model 2 incorporates a distortion
factor to limit the number of possible alignments that are
considered. This factor defines the number of positions
a source word may move when it is translated into the
target sentence. For example, given a distortion factor of
two, a source word could align with a word up to two
positions to the left or right of the corresponding target
word?s position.
Model 2 is based on the probability of a source and tar-
get word being translations of each other, and the proba-
bility that words at particular source and target positions
are translations of each other (without regard to what
those words are). Thus, the numerator in Equation 1 is
estimated as follows:
  
   


	


  


 
	
  

      (2)
The translation probability, 
  
       , is the likelihood
that 
  , the target word at position  , is the translation
of a given source word  that occurs at position   . The
alignment probability,         , is the likelihood that
position   in the source sentence can align to a given
position  in the target sentence, where  and  are the
given lengths of the source and target sentences.
The denominator in Equation 1 is the sum of all the
probabilities of all the possible alignments of a sentence
pair. This can be estimated by taking the product of the
sums of the translational and positional alignment proba-
bilities.


  
   


	


 


  


 

The SENSEVAL?3 Multilingual English?Hindi Lexical Sample Task
Timothy Chklovski
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
timc@isi.edu
Rada Mihalcea
Department of Computer Science
University of North Texas
Dallas, TX 76203
rada@cs.unt.edu
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
tpederse@d.umn.edu
Amruta Purandare
Department of Computer Science
University of Minnesota
Duluth, MN 55812
pura0010@d.umn.edu
Abstract
This paper describes the English?Hindi Multilingual
lexical sample task in SENSEVAL?3. Rather than
tagging an English word with a sense from an En-
glish dictionary, this task seeks to assign the most
appropriate Hindi translation to an ambiguous tar-
get word. Training data was solicited via the Open
Mind Word Expert (OMWE) from Web users who
are fluent in English and Hindi.
1 Introduction
The goal of the MultiLingual lexical sample task
is to create a framework for the evaluation of sys-
tems that perform Machine Translation, with a fo-
cus on the translation of ambiguous words. The
task is very similar to the lexical sample task, ex-
cept that rather than using the sense inventory from
a dictionary we follow the suggestion of (Resnik and
Yarowsky, 1999) and use the translations of the tar-
get words into a second language. In this task for
SENSEVAL-3, the contexts are in English, and the
?sense tags? for the English target words are their
translations in Hindi.
This paper outlines some of the major issues that
arose in the creation of this task, and then describes
the participating systems and summarizes their re-
sults.
2 Open Mind Word Expert
The annotated corpus required for this task was
built using the Open Mind Word Expert system
(Chklovski and Mihalcea, 2002), adapted for mul-
tilingual annotations 1.
To overcome the current lack of tagged data and
the limitations imposed by the creation of such data
using trained lexicographers, the Open Mind Word
1Multilingual Open Mind Word Expert can be accessed at
http://teach-computers.org/word-expert/english-hindi
Expert system enables the collection of semantically
annotated corpora over the Web. Tagged examples
are collected using a Web-based application that al-
lows contributors to annotate words with their mean-
ings.
The tagging exercise proceeds as follows. For
each target word the system extracts a set of sen-
tences from a large textual corpus. These examples
are presented to the contributors, together with all
possible translations for the given target word. Users
are asked to select the most appropriate translation
for the target word in each sentence. The selection
is made using check-boxes, which list all possible
translations, plus two additional choices, ?unclear?
and ?none of the above.? Although users are encour-
aged to select only one translation per word, the se-
lection of two or more translations is also possible.
The results of the classification submitted by other
users are not presented to avoid artificial biases.
3 Sense Inventory Representation
The sense inventory used in this task is the set of
Hindi translations associated with the English words
in our lexical sample. Selecting an appropriate
English-Hindi dictionary was a major decision early
in the task, and it raised a number of interesting is-
sues.
We were unable to locate any machine readable
or electronic versions of English-Hindi dictionaries,
so it became apparent that we would need to manu-
ally enter the Hindi translations from printed mate-
rials. We briefly considered the use of Optical Char-
acter Recognition (OCR), but found that our avail-
able tools did not support Hindi. Even after deciding
to enter the Hindi translations manually, it wasn?t
clear how those words should be encoded. Hindi is
usually represented in Devanagari script, which has
a large number of possible encodings and no clear
standard has emerged as yet.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
We decided that Romanized or transliterated
Hindi text would be the the most portable encoding,
since it can be represented in standard ASCII text.
However, it turned out that the number of English?
Hindi bilingual dictionaries is much less than the
number of Hindi?English, and the number that use
transliterated text is smaller still.
Still, we located one promising candidate, the
English?Hindi Hippocrene Dictionary (Raker and
Shukla, 1996), which represents Hindi in a translit-
erated form. However, we found that many English
words only had two or three translations, making it
too coarse grained for our purposes2 .
In the end we selected the Chambers English?
Hindi dictionary (Awasthi, 1997), which is a high
quality bilingual dictionary that uses Devanagari
script. We identified 41 English words from the
Chambers dictionary to make up our lexical sam-
ple. Then one of the task organizers, who is
fluent in English and Hindi, manually transliter-
ated the approximately 500 Hindi translations of
the 41 English words in our lexical sample from
the Chambers dictionary into the ITRANS format
(http://www.aczone.com/itrans/). ITRANS software
was used to generate Unicode for display in the
OMWE interfaces, although the sense tags used in
the task data are the Hindi translations in transliter-
ated form.
4 Training and Test Data
The MultiLingual lexical sample is made up of 41
words: 18 nouns, 15 verbs, and 8 adjectives. This
sample includes English words that have varying de-
grees of polysemy as reflected in the number of pos-
sible Hindi translations, which range from a low of
3 to a high of 39.
Text samples made up of several hundred in-
stances for each of 31 of the 41 words were drawn
from the British National Corpus, while samples for
the other 10 words came from the SENSEVAL-2 En-
glish lexical sample data. The BNC data is in a
?raw? text form, where the part of speech tags have
been removed. However, the SENSEVAL-2 data in-
cludes the English sense?tags as determined by hu-
man taggers.
After gathering the instances for each word in
the lexical sample, we tokenized each instance and
removed those that contain collocations of the tar-
get word. For example, the training/test instances
for arm.n do not include examples for contact arm,
2We have made available transcriptions of the entries for
approximately 70 Hippocrene nouns, verbs, and adjectives
at http://www.d.umn.edu/?pura0010/hindi.html, although these
were not used in this task.
pickup arm, etc., but only examples that refer to arm
as a single lexical unit (not part of a collocation). In
our experience, disambiguation accuracy on collo-
cations of this sort is close to perfect, and we aimed
to concentrate the annotation effort on the more dif-
ficult cases.
The data was then annotated with Hindi transla-
tions by web volunteers using the Open Mind Word
Expert (bilingual edition). At various points in time
we offered gift certificates as a prize for the most
productive tagger in a given day, in order to spur
participation. A total of 40 volunteers contributed to
this task.
To create the test data we collected two indepen-
dent tags per instance, and then discarded any in-
stances where the taggers disagreed. Thus, each
instance that remains in the test data has complete
agreement between two taggers. For the training
data, we only collected one tag per instance, and
therefore this data may be noisy. Participating sys-
tems could choose to apply their own filtering meth-
ods to identify and remove the less reliably anno-
tated examples.
After tagging by the Web volunteers, there were
two data sets provided to task participants: one
where the English sense of the target word is un-
known, and another where it is known in both the
training and test data. These are referred to as the
translation only (t) data and the translation and sense
(ts) data, respectively. The t data is made up of in-
stances drawn from the BNC as described above,
while the ts data is made up of the instances from
SENSEVAL-2. Evaluations were run separately for
each of these two data sets, which we refer to as the
t and ts subtasks.
The t data contains 31 ambiguous words: 15
nouns, 10 verbs, and 6 adjectives. The ts data con-
tains 10 ambiguous words: 3 nouns, 5 verbs, and 2
adjectives, all of which have been used in the En-
glish lexical sample task of SENSEVAL-2. These
words, the number of possible translations, and the
number of training and test instances are shown in
Table 1. The total number of training instances in
the two sub-tasks is 10,449, and the total number of
test instances is 1,535.
5 Participating Systems
Five teams participated in the t subtask, submitting
a total of eight systems. Three teams (a subset of
those five) participated in the ts subtask, submitting
a total of five systems. All submitted systems em-
ployed supervised learning, using the training ex-
amples provided. Some teams used additional re-
sources as noted in the more detailed descriptions
Table 1: Target words in the SENSEVAL-3 English-Hindi task
Lexical Unit Translations Train Test Lexical Unit Translations Train Test Lexical Unit Translations Train Test
TRANSLATION ONLY (T?DATA)
band.n 8 224 91 bank.n 21 332 52 case.n 13 348 42
different.a 4 320 25 eat.v 3 271 48 field.n 14 300 100
glass.n 8 379 13 hot.a 18 348 32 line.n 39 360 11
note.v 11 220 12 operate.v 9 280 50 paper.n 8 264 73
plan.n 8 210 35 produce.v 7 265 67 rest.v 14 172 10
rule.v 8 160 18 shape.n 8 320 32 sharp.a 16 248 48
smell.v 5 210 17 solid.a 16 327 37 substantial.a 15 250 100
suspend.v 4 370 28 table.n 21 378 16 talk.v 6 341 35
taste.n 6 350 40 terrible.a 4 200 99 tour.n 5 240 9
vision.n 14 318 20 volume.n 9 309 54 watch.v 10 300 100
way.n 16 331 22 TOTAL 348 8945 1336
TRANSLATION AND SENSE ONLY (TS?DATA)
bar.n 19 278 39 begin.v 6 360 15 channel.n 6 92 16
green.a 9 175 26 nature.n 15 71 14 play.v 14 152 10
simple.a 9 166 19 treat.v 7 100 32 wash.v 16 10 11
work.v 24 100 17 TOTAL 125 1504 199
below.
5.1 NUS
The NUS team from the National University of Sin-
gapore participated in both the t and ts subtasks. The
t system (nusmlst) uses a combination of knowledge
sources as features, and the Support Vector Machine
(SVM) learning algorithm. The knowledge sources
used include part of speech of neighboring words,
single words in the surrounding context, local col-
locations, and syntactic relations. The ts system
(nusmlsts) does the same, but adds the English sense
of the target word as a knowledge source.
5.2 LIA-LIDILEM
The LIA-LIDILEM team from the Universite? d?
Avignon and the Universite? Stendahl Grenoble had
two systems which participated in both the t and ts
subtasks. In the ts subtask, only the English sense
tags were used, not the Hindi translations.
The FL-MIX system uses a combination of three
probabilistic models, which compute the most prob-
able sense given a six word window of context. The
three models are a Poisson model, a Semantic Clas-
sification Tree model, and a K nearest neighbors
search model. This system also used a part of speech
tagger and a lemmatizer.
The FC-MIX system is the same as the FL-MIX
system, but replaces context words by more gen-
eral synonym?like classes computed from a word
aligned English?French corpus which number ap-
proximately 850,000 words in each language.
5.3 HKUST
The HKUST team from the Hong Kong University
of Science and Technology had three systems that
participated in both the t and ts subtasks
The HKUST me t and HKUST me ts sys-
tems are maximum entropy classifiers. The
HKUST comb t and HKUST comb ts systems
are voted classifiers that combine a new Kernel
PCA model with a maximum entropy model and
a boosting?based model. The HKUST comb2 t
and HKUST comb2 ts are voted classifiers that
combine a new Kernel PCA model with a maximum
entropy model, a boosting?based model, and a
Naive Bayesian model.
5.4 UMD
The UMD team from the University of Maryland en-
tered (UMD?SST) in the t task. UMD?SST is a su-
pervised sense tagger based on the Support Vector
Machine learning algorithm, and is described more
fully in (Cabezas et al, 2001).
5.5 Duluth
The Duluth team from the University of Minnesota,
Duluth had one system (Duluth-ELSS) that partici-
pated in the t task. This system is an ensemble of
three bagged decision trees, each based on a differ-
ent type of lexical feature. This system was known
as Duluth3 in SENSEVAL-2, and it is described more
fully in (Pedersen, 2001).
6 Results
All systems attempted all of the test instances, so
precision and recall are identical, hence we report
Table 2: t Subtask Results
System Accuracy
nusmlst 63.4
HKUST comb t 62.0
HKUST comb2 t 61.4
HKUST me t 60.6
FL-MIX 60.3
FC-MIX 60.3
UMD-SST 59.4
Duluth-ELSS 58.2
Baseline (majority) 51.9
Table 3: ts Subtask Results
System Accuracy
nusmlsts 67.3
FL-MIX 64.1
FC-MIX 64.1
HKUST comb ts 63.8
HKUST comb2 ts 63.8
HKUST me ts 60.8
Baseline (majority) 55.8
the single Accuracy figure. Tables 2 and 3 show re-
sults for the t and ts subtasks, respectively.
We note that the participating systems all ex-
ceeded the baseline (majority) classifier by some
margin, suggesting that the sense distinctions made
by the translations are clear and provide sufficient
information for supervised methods to learn effec-
tive classifiers.
Interestingly, the average results on the ts data are
higher than the average results on the t data, which
suggests that sense information is likely to be helpful
for the task of targeted word translation. Additional
investigations are however required to draw some fi-
nal conclusions.
7 Conclusion
The Multilingual Lexical Sample task in
SENSEVAL-3 featured English ambiguous words
that were to be tagged with their most appropriate
Hindi translation. The objective of this task is to
determine feasibility of translating words of various
degrees of polysemy, focusing on translation of
specific lexical items. The results of five teams
that participated in this event tentatively suggest
that machine learning techniques can significantly
improve over the most frequent sense baseline.
Additionally, this task has highlighted creation
of testing and training data by leveraging the
knowledge of bilingual Web volunteers. The
training and test data sets used in this exercise are
available online from http://www.senseval.org and
http://teach-computers.org.
Acknowledgments
Many thanks to all those who contributed to the Mul-
tilingual Open Mind Word Expert project, making
this task possible. We are also grateful to all the par-
ticipants in this task, for their hard work and involve-
ment in this evaluation exercise. Without them, all
these comparative analyses would not be possible.
We are particularly grateful to a research grant
from the University of North Texas that provided the
funding for contributor prizes, and to the National
Science Foundation for their support of Amruta Pu-
randare under a Faculty Early CAREER Develop-
ment Award (#0092784).
References
S. Awasthi, editor. 1997. Chambers English?Hindi
Dictionary. South Asia Books, Columbia, MO.
C. Cabezas, P. Resnik, and J. Stevens. 2001. Su-
pervised sense tagging using Support Vector Ma-
chines. In Proceedings of the Senseval-2 Work-
shop, Toulouse, July.
T. Chklovski and R. Mihalcea. 2002. Building a
sense tagged corpus with the Open Mind Word
Expert. In Proceedings of the ACL Workshop on
Word Sense Disambiguation: Recent Successes
and Future Directions, Philadelphia.
T. Pedersen. 2001. Machine learning with lexical
features: The Duluth approach to Senseval-2. In
Proceedings of the Senseval-2 Workshop, pages
139?142, Toulouse, July.
J. Raker and R. Shukla, editors. 1996. Hip-
pocrene Standard Dictionary English-Hindi
Hindi-English (With Romanized Pronunciation).
Hippocrene Books, New York, NY.
P. Resnik and D. Yarowsky. 1999. Distinguish-
ing systems and distinguishing senses: New eval-
uation methods for word sense disambiguation.
Natural Language Engineering, 5(2):113?133.
Complementarity of Lexical and Simple Syntactic Features:
The SyntaLex Approach to SENSEVAL-3
Saif Mohammad
University of Toronto
Toronto, ON M5S1A1 Canada
smm@cs.toronto.edu
http://www.cs.toronto.edu/?smm
Ted Pedersen
University of Minnesota
Duluth, MN 55812 USA
tpederse@d.umn.edu
http://www.d.umn.edu/?tpederse
Abstract
This paper describes the SyntaLex entries in
the English Lexical Sample Task of SENSEVAL-3.
There are four entries in all, where each of the dif-
ferent entries corresponds to use of word bigrams
or Part of Speech tags as features. The systems rely
on bagged decision trees, and focus on using pairs
of lexical and syntactic features individually and in
combination. They are descendants of the Duluth
systems that participated in SENSEVAL-2.
1 Introduction
The SyntaLex systems are supervised learners
that identify the intended sense of a word (target
word) given its context. They are derived from the
Duluth systems that participated in SENSEVAL-2,
and which are more fully described in (Pedersen,
2001b).
The context of a word is a rich source of dis-
crete features which lend themselves nicely to de-
cision tree learning. Prior research (e.g., (McRoy,
1992), (Ng and Lee, 1996), (Stevenson and Wilks,
2001), (Yarowsky and Florian, 2002)) suggests that
use of both syntactic and lexical features will im-
prove disambiguation accuracies. There has also
been considerable work on word sense disambigua-
tion using various supervised learning algorithms.
However, both (Pedersen, 2001a) and (Lee and Ng,
2002) show that different learning algorithms pro-
duce similar results and that the use of appropriate
features may dramatically improve results. Thus,
our focus is not on the learning algorithm but on the
features used and their dynamics.
Our systems use bigrams and Part of Speech fea-
tures individually, in a simple ensemble and as part
of single classifier using both kinds of features. We
also show that state of the art results (72.1%, coarse
grained accuracy) can be achieved using just these
simple sets of features.
2 Feature Space
Simple lexical and syntactic features are used to rep-
resent the context. The lexical features used are
word bigrams. The Part of Speech (PoS) of the tar-
get word and its neighbors make up the the syntactic
features. Bigrams are readily captured from the text
while Part of Speech taggers are widely available
for a variety of languages.
2.1 Bigrams
A bigram is a pair of words that occur close to each
other in text and in a particular order. Consider:
 	 
	 
 	The Duluth Lexical Sample Systems in SENSEVAL-3
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
tpederse@d.umn.edu
http://www.d.umn.edu/?tpederse
Abstract
Two systems from the University of Minnesota,
Duluth participated in various SENSEVAL-3 lexi-
cal sample tasks. The supervised learning system
is based on lexical features and bagged decision
trees. It participated in lexical sample tasks for
the English, Spanish, Catalan, Basque, Romanian
and MultiLingual English-Hindi data. The unsuper-
vised system uses measures of semantic relatedness
to find the sense of the target word that is most re-
lated to the senses of its neighbors. It participated
in the English lexical sample task.
1 Introduction
The Duluth systems participated in various lexical
sample tasks in SENSEVAL-3, using both super-
vised and unsupervised methodologies.
The supervised lexical sample system that partic-
ipated in SENSEVAL-3 is the Duluth3 (English) or
Duluth8 (Spanish) system as used in SENSEVAL-
2 (Pedersen, 2001b). It has been renamed for
SENSEVAL-3 as Duluth-xLSS, where x is a one let-
ter abbreviation of the language to which it is be-
ing applied, and LSS stands for Lexical Sample Su-
pervised. The idea behind this system is to learn
three bagged decision trees, one using unigram fea-
tures, another using bigram features, and a third
using co?occurrences with the target word as fea-
tures. This system only uses surface lexical fea-
tures, so it can be easily applied to a wide range
of languages. For SENSEVAL-3 this system partici-
pated in the English, Spanish, Basque, Catalan, Ro-
manian, and MultiLingual (English-Hindi) tasks.
The unsupervised lexical sample system is based
on the SenseRelate algorithm (Patwardhan et al,
2003) for word sense disambiguation. It is known
as Duluth-ELSU, for English Lexical Sample Un-
supervised. This system relies on measures of
semantic relatedness in order to determine which
sense of a word is most related to the possible
senses of nearby content words. This system de-
termines relatedness based on information extracted
from the lexical database WordNet using the Word-
Net::Similarity package. In SENSEVAL-3 this sys-
tem was restricted to English text, although in fu-
ture it and the WordNet::Similarity package could
be ported to WordNets in other languages.
This paper continues by describing our super-
vised learning technique which is based on the use
of bagged decision trees, and then introduces the
dictionary based unsupervised algorithm. We dis-
cuss our results from SENSEVAL-3, and conclude
with some ideas for future work.
2 Lexical Sample Supervised
The Duluth-xLSS system creates an ensemble of
three bagged decision trees, where each is based
on a different set of features. A separate ensemble
is learned for each word in the lexical sample, and
only the training data that is associated with a par-
ticular target word is used in creating the ensemble
for that word.
This approach is based on the premise that these
different views of the training examples for a given
target word will result in classifiers that make com-
plementary errors, and that their combined perfor-
mance will be better than any of the individual clas-
sifiers that make up the ensemble. A decision tree
is learned from each of the three representations of
the training examples. Each resulting classifier as-
signs probabilities to every possible sense of a test
instance. The ensemble is created by summing these
probabilities and assigning the sense with the largest
associated probability.
The objective of the Duluth-xLSS system?s par-
ticipating in multiple lexical sample tasks is to test
the hypothesis that simple lexical features identified
using standard statistical techniques can provide
reasonably good performance at word sense disam-
biguation. While we doubt that the Duluth-xLSS
approach will result in the top ranked accuracy in
SENSEVAL-3, we believe that it should always im-
prove upon a simple baseline like the most frequent
sense (i.e., majority classifier), and may be compet-
itive with other more feature?rich approaches.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
2.1 Feature Sets
The first feature set is made up of bigrams, which
are consecutive two word sequences that can occur
anywhere in the context with the ambiguous word.
To be selected as a feature, a bigram must occur two
or more times in the training examples associated
with the target word, and have a log-likelihood ratio
(G2) value ? 6.635, which is associated with a p-
value of .01.
The second feature set is based on unigrams, i.e.,
one word sequences, that occur five or more times in
the training data for the given target word. Since the
number of training examples for most words is rel-
atively small (100-200 instances in many cases) the
number of unigram features that are actually identi-
fied by this criteria are rather small.
The third feature set is made up of co?occurrence
features that represent words that occur on the im-
mediate left or right of the target word. In effect,
these are bigrams that include the target word. To
be selected as features these must occur two or more
times in the training data and have a log?likelihood
ratio (G2) value ? 2.706, which is associated with a
p-value of .10. Note that we are using a more lenient
level of significance for the co?occurrences than the
bigrams (.10 versus .01), which is meant to increase
the number of features that include the target word.
The Duluth-xLSS system is identical for each of
the languages to which it is applied, except that
in the English lexical sample we used a stoplist of
function words, while in the other tasks we did not.
The use of a stoplist would likely be helpful, but
we lacked the time to locate and evaluate candi-
date stoplists for other languages. For English, un-
igrams in the stop list are not included as features,
and bigrams or co?occurrences made up of two stop
words are excluded. The stop list seems particularly
relevant for the unigram features, since the bigram
and co?occurrence feature selection process tends
to eliminate some features made up of stop words
via the log?likelihood ratio score cutoff.
In all of the tasks tokenization was based on
defining a word as a white space separated string.
There was no stemming or lemmatizing performed
for any of the languages.
2.2 Decision Trees
Decision trees are among the most widely used ma-
chine learning algorithms.
They perform a general to specific search of a fea-
ture space, adding the most informative features to a
tree structure as the search proceeds. The objective
is to select a minimal set of features that efficiently
partitions the feature space into classes of observa-
tions and assemble them into a tree. In our case, the
observations are manually sense?tagged examples
of an ambiguous word in context and the partitions
correspond to the different possible senses.
Each feature selected during the search process is
represented by a node in the learned decision tree.
Each node represents a choice point between a num-
ber of different possible values for a feature. Learn-
ing continues until all the training examples are ac-
counted for by the decision tree. In general, such
a tree will be overly specific to the training data
and not generalize well to new examples. Therefore
learning is followed by a pruning step where some
nodes are eliminated or reorganized to produce a
tree that can generalize to new circumstances.
When a decision tree is bagged (Breiman, 1996),
all of the above is still true. However, what is differ-
ent is that the training data is sampled with replace-
ment during learning. This is instead of having the
training data as a static or fixed set of data. This
tends to result in a learned decision tree where out-
liers or anomalous training instances are smoothed
out or eliminated (since it is more likely that the
resampling operation will find more typical train-
ing examples). The standard approach in bagging
it to learn multiple decision trees from the same
training data (each based on a different sampling of
the data), and then create an averaged decision tree
from these trees.
In our experiments we learn ten bagged decision
trees for each feature set, and then take the resulting
averaged decision tree as a member in our ensemble.
Thus, to create each ensemble, we learn 30 decision
trees, ten for each feature set. The decision trees
associated with each feature set are averaged into
a single tree, leaving us with three decision trees
in the ensemble, one which represents the bigram
features, another the unigrams, and the third the co?
occurrence features.
Our experience has been that variations in learn-
ing algorithms are far less significant contributors
to disambiguation accuracy than are variations in
the feature set. In other words, an informative fea-
ture set will result in accurate disambiguation when
used with a wide range of learning algorithms, but
there is no learning algorithm that can perform well
given an uninformative or misleading set of fea-
tures. Therefore, our interest in these experiments
is more in the effect of the different features sets
than in the variations that would be possible if we
used learning algorithms other than decision trees.
We are satisfied that decision trees are a reason-
able choice of learning algorithm. They have a long
history of use in word sense disambiguation, dat-
ing back to early work by (Black, 1988), and have
fared well in comparative studies such as (Mooney,
1996) and (Pedersen and Bruce, 1997). In the for-
mer they were used with unigram features and in the
latter they were used with a small set of features that
included the part-of-speech of neighboring words,
three collocations, and the morphology of the am-
biguous word. In (Pedersen, 2001a) we introduced
the use of decision trees based strictly on bigram
features.
While we might squeeze out a few extra points
of performance by using more complicated meth-
ods, we believe that this would obscure our abil-
ity to study and understand the effects of different
kinds of features. Decision trees have the further
advantage that a wide range of implementations are
available, and they are known to be robust and ac-
curate across a range of domains. Most important,
their structure is easy to interpret and may provide
insights into the relationships that exist among fea-
tures and more general rules of disambiguation.
2.3 Software Resources
The Duluth-xLSS system is based completely on
software that is freely available. All of the software
mentioned below has been developed at the Univer-
sity of Minnesota, Duluth, with the exception of the
Weka machine learning system.
The Ngram Statistics Package (NSP) (Banerjee
and Pedersen, 2003a) version 0.69 was used to iden-
tify the lexical features for all of the different lan-
guages. NSP is written in Perl and is freely available
for download from the Comprehensive Perl Archive
(CPAN) (http://search.cpan.org/dist/Text-NSP) or
SourceForge (http://ngram.sourceforge.net).
The SenseTools package converts unigram, bi-
gram, and co?occurrence features as discov-
ered by NSP into the ARFF format required
by the Weka Machine Learning system (Witten
and Frank, 2000). It also takes the output of
Weka and builds our ensembles. We used ver-
sion 0.03 of SenseTools, which is available from
http://www.d.umn.edu/?tpederse/sensetools.html.
Weka is a freely available Java based suite of
machine learning methods. We used their J48
implementation of the C4.5 decision tree learn-
ing algorithm (Quinlan, 1986), which includes
support for bagging. Weka is available from
http://www.cs.waikato.ac.nz/ml/weka/
A set of driver scripts known as the DuluthShell
integrates NSP, Weka, and SenseTools, and is avail-
able from the same page as SenseTools. Version 0.3
of the DuluthShell was used to create the Duluth-
xLSS system.
3 Lexical Sample Unsupervised
The unsupervised Duluth-ELSU system is a dictio-
nary based approach. It uses the content of WordNet
to measure the similarity or relatedness between the
senses of a target word and its surrounding words.
The general idea behind the SenseRelate algo-
rithm is that a target word will tend to have the sense
that is most related to the senses of its neighbors.
Here we define neighbor as a content word that oc-
curs in close proximity to the target word, but this
could be extended to include words that may be syn-
tactically related without being physically nearby.
The objective of the Duluth-ELSU system?s par-
ticipation in the English lexical sample task is to test
the hypothesis that disambiguation based on mea-
sures of semantic relatedness can perform effec-
tively even in very diverse text and possibly noisy
data such as is used for SENSEVAL-3.
3.1 Algorithm Description
In the SenseRelate algorithm, a window of context
around the target word is selected, and a set of can-
didate senses from WordNet is identified for each
content word in the window. Assume that the win-
dow of context consists of 2n + 1 words denoted
by wi, ?n ? i ? +n, where the target word is
w0. Further let |wi| denote the number of candidate
senses of word wi, and let these senses be denoted
by si,j , 1 ? j ? |wi|. In these experiments we used
a window size of 3, which means we considered a
content word to the right and left of the target word.
Next the algorithm assigns to each possible sense
k of the target word a Scorek computed by adding
together the relatedness scores obtained by compar-
ing the sense of the target word in question with ev-
ery sense of every non?target word in the window of
context using a measure of relatedness. The Score
for sense s0,k is computed as follows:
Scorek =
n
?
i=?n
|wi|
?
j=1
relatedness(s0,k, si,j), i 6= 0
That sense with the highest Score is judged to be
the most appropriate sense for the target word. If
there are on average a senses per word and the win-
dow of context is N words long, there are a2?(N?
1) pairs of sets of senses to be compared, which in-
creases linearly with N .
Since the part of speech of the target word is
given in the lexical sample tasks, this information is
used to limit the possible senses of the target word.
However, the part of speech of the other words in
the window of context was unknown. In previous
experiments we have found that the use of a part of
speech tagger has the potential to considerably re-
duce the search space for the algorithm, but does not
actually affect the quality of the results to a signifi-
cant degree. This suggests that the measure of relat-
edness tends to eventually identify the correct part
of speech for the context words, however, it would
certainly be more efficient to allow a part of speech
tagger to do that apriori.
In principle any measure of relatedness can be
employed, but here we use the Extended Gloss
Overlap measure (Banerjee and Pedersen, 2003b).
This assigns a score to a pair of concepts based
on the number of words they share in their Word-
Net glosses, as well as the number of words shared
among the glosses of concepts to which they are di-
rectly related according to WordNet. This particular
measure (known as lesk in WordNet::Similarity) has
the virtue that it is able to measure relatedness be-
tween mixed parts of speech, that is between nouns
and verbs, adjectives and nouns, etc. Measures of
similarity are generally limited to noun?noun and
possibly verb?verb comparisons, thus reducing their
generality in a disambiguation system.
3.2 Software Resources
The unsupervised Duluth-ELSU system is freely
available, and is based on version 0.05 of
the SenseRelate algorithm which was devel-
oped at the University of Minnesota, Duluth.
SenseRelate is distributed via SourceForge at
http://sourceforge.net/projects/senserelate. This
package uses WordNet::Similarity (version 0.07)
to measure the similarity and relatedness among
concepts. WordNet::Similarity is available from
the Comprehensive Perl Archive Network at
http://search.cpan.org/dist/WordNet-Similarity.
4 Experimental Results
Table 1 shows the results as reported for the various
SENSEVAL-3 lexical sample tasks. In this table we
refer to the language and indicate whether the learn-
ing was supervised (S) or unsupervised (U). Thus,
Spanish-S refers to the system Duluth-SLSS. Also,
the English and Romanian lexical sample tasks pro-
vided both fine and coarse grained scoring, which is
indicated by (f) and (c) respectively. The other tasks
only used fine grained scoring. We also report the
results from a majority classifier which simply as-
signs each instance of a word to its most frequent
sense as found in the training data (x-MFS). The
majority baseline values were either provided by a
task organizer, or were computed using an answer
key as provided by a task organizer.
Table 1: Duluth-xLSy Results
System (x-y) Prec. Recall F
English-S (f) 61.80 61.80 61.80
English-MFS (f) 55.20 55.20 55.20
English-U (f) 40.30 38.50 39.38
English-S (c) 70.10 70.10 70.10
English-MFS (c) 64.50 64.50 64.50
English-U (c) 51.00 48.70 49.82
Romanian-S (f) 71.40 71.40 71.40
Romanian-MFS (f) 55.80 55.80 55.80
Romanian-S (c) 75.20 75.20 75.20
Romanian-MFS (c) 59.60 59.60 59.60
Catalan-S 75.37 76.48 75.92
Catalan-MFS 66.36 66.36 66.36
Basque-S 60.80 60.80 60.80
Basque-MFS 55.80 55.80 55.80
Spanish-S 74.29 75.02 74.65
Spanish-MFS 67.72 67.72 67.72
MultLing-S 58.20 58.20 58.20
MultLing-MFS 51.80 51.80 51.80
4.1 Supervised
The results of the supervised Duluth-xLSS system
are fairly consistent across languages. Generally
speaking it is more accurate than the majority clas-
sifier by approximately 5 to 9 percentage points de-
pending on the language. The Romanian results are
even better than this, with Duluth-RLSS attaining
accuracies more than 15 percentage points better
than the majority sense.
We are particularly pleased with our results for
Basque, since it is an agglutinating language and
yet we did nothing to account for this. We tok-
enized all the languages in the same way, by sim-
ply defining a word to be any string separated by
white spaces. While this glosses over many dis-
tinctions between the languages, in general it still
seemed to result in sufficiently informative features
to create reliable classifiers. Thus, our unigrams,
bigrams, and co?occurrences are composed of these
words, and we find it interesting that such simple
and easy to obtain features fare reasonably well.
This suggests to use that these techniques might
form a somewhat language independent foundation
upon which more language dependent disambigua-
tion techniques might be built.
4.2 Unsupervised
The unsupervised system Duluth-ELSU in the En-
glish lexical sample task did not perform as well as
the supervised majority classifier method, but this
is not entirely surprising. The unsupervised method
made no use of the training data available for the
task, nor did it use any of the first sense information
available in WordNet. We decided not to use the
information that WordNet provides about the most
frequent sense of a word, since that is based on the
sense?tagged corpus SemCor, and we wanted this
system to remain purely unsupervised.
Also, the window of context used was quite nar-
row, and only consisted of one content word to the
left and right of the target word. It may well be that
expanding the window, or choosing the words in
the window on criteria other than immediate prox-
imity to the target word would result in improved
performance. However, larger windows of context
are computationally more complex and we did not
have sufficient time during the evaluation period to
run more extensive experiments with different sized
windows of context.
As a final factor in our evaluation, Duluth-ELSU
is a WordNet based system. However, the verb
senses in the English lexical sample task came
from WordSmyth. Despite this our system re-
lied on WordNet verb senses and glosses to make
relatedness judgments, and then used a mapping
from WordNet senses to WordSmyth to produce re-
portable answers. There were 178 instances where
the WordNet sense found by our system was not
mapped to WordSmyth. Rather than attempt to cre-
ate our own mapping of WordNet to WordSmyth,
we simply threw these instances out of the evalua-
tion set, which does lead to somewhat less coverage
for the unsupervised system for the verbs.
5 Future Work
The Duluth-xLSS system was originally inspired
by (Pedersen, 2000), which presents an ensemble
of eighty-one Naive Bayesian classifiers based on
varying sized windows of context to the left and
right of the target word that define co-occurrence
features. However, the Duluth-ELSS system only
uses a three member ensemble to explore the ef-
ficacy of combinations of different lexical features
via simple ensembles. We plan to carry out a more
detailed analysis of the degree to which unigram, bi-
gram, and co?occurrence features are useful sources
of information for disambiguation.
We will also conduct an analysis of the comple-
mentary and redundant nature of lexical and syn-
tactic features, as we have done in (Mohammad and
Pedersen, 2004a) for the SENSEVAL-1, SENSEVAL-
2, and line, hard, serve, and interest data. The Syn-
taLex system (Mohammad and Pedersen, 2004b)
also participated in the English lexical sample task
of SENSEVAL?3 and is a sister system to Duluth-
ELSS. It uses lexical and syntactic features with
bagged decision trees and serves as a convenient
point of comparison. We are particularly inter-
ested to see if there are words that are better dis-
ambiguated using syntactic versus lexical features,
and in determining how to best combine classifiers
based on different feature sets in order to attain im-
proved accuracy.
The Duluth-ELSU system is an unsupervised ap-
proach that is based on WordNet content, in partic-
ular relatedness scores that are computed by mea-
suring gloss overlaps of the candidate senses of a
target word with the possible senses of neighbor-
ing words. There are several variations to this ap-
proach that can easily be taken, including increas-
ing the size of the window of context, and the use
of measures of relatedness other than the Extended
Gloss Overlap method. We are also interested in
choosing words that are included in the window of
context more cleverly. For example, we are study-
ing the possibility of letting the window of context
be defined by words that make up a lexical chain
with the target word.
The Duluth-ELSU system could be adapted for
use in the all-words task as well, where all content
words in a text are assigned a sense. One important
issue that must be resolved is whether we would at-
tempt to disambiguate a sentence globally, that is by
assinging the senses that maximize the relatedness
of all the words in the sentence at the same time.
The alternative would be to simply proceed left to
right, fixing the senses that are assigned as we move
through a sentence. We are also considering the use
of more general discourse level topic restrictions on
the range of possible senses in an all-words task.
We also plan to extend our study of comple-
mentary and related behavior between systems to
include an analysis of our supervised and unsu-
pervised results, to see if a combination of super-
vised and unsupervised systems might prove advan-
tageous. While the level of redundancy between
supervised systems can be rather high (Moham-
mad and Pedersen, 2004a), we are optimistic that a
corpus based supervised approach and a dictionary
based unsupervised approach might be highly com-
plementary.
6 Conclusions
This paper has described two lexical sample sys-
tems from the University of Minnesota, Duluth that
participated in the SENSEVAL-3 exercise. We found
that our supervised approach, Duluth-xLSS, fared
reasonably well in a wide range of lexical sample
tasks, thus suggesting that simple lexical features
can serve as a firm foundation upon which to build a
disambiguation system in a range of languages. The
unsupervised approach of Duluth-ELSU to the En-
glish lexical sample task did not fare as well as the
supervised approach, but performed at levels com-
parable to that attained by unsupervised systems in
SENSEVAL-1 and SENSEVAL-2.
7 Acknowledgments
This research has been partially supported by a
National Science Foundation Faculty Early CA-
REER Development award (#0092784), and by two
Grants?in?Aid of Research, Artistry and Scholar-
ship from the Office of the Vice President for Re-
search and the Dean of the Graduate School of the
University of Minnesota.
Satanjeev Banerjee, Jason Michelizzi, Saif Mo-
hammad, Siddharth Patwardhan, and Amruta Pu-
randare have all made significant contributions to
the development of the various tools that were used
in these experiments. This includes the Ngram
Statistics Package, SenseRelate, SenseTools, the
DuluthShell, and WordNet::Similarity. All of this
software is freely available at the web sites men-
tioned in this paper, and make it possible to easily
reproduce and extend the results described in this
paper.
References
S. Banerjee and T. Pedersen. 2003a. The design,
implementation, and use of the Ngram Statistics
Package. In Proceedings of the Fourth Interna-
tional Conference on Intelligent Text Processing
and Computational Linguistics, pages 370?381,
Mexico City, February.
S. Banerjee and T. Pedersen. 2003b. Extended
gloss overlaps as a measure of semantic related-
ness. In Proceedings of the Eighteenth Interna-
tional Joint Conference on Artificial Intelligence,
pages 805?810, Acapulco, August.
E. Black. 1988. An experiment in computa-
tional discrimination of English word senses.
IBM Journal of Research and Development,
32(2):185?194.
L. Breiman. 1996. The heuristics of instability in
model selection. Annals of Statistics, 24:2350?
2383.
S. Mohammad and T. Pedersen. 2004a. Combin-
ing lexical and syntactic features for supervised
word sense disambiguation. In Proceedings of
the Conference on Computational Natural Lan-
guage Learning, pages 25?32, Boston, MA.
S. Mohammad and T. Pedersen. 2004b. Comple-
mentarity of lexical and simple syntactic features:
The Syntalex approach to SENSEVAL-3. In Pro-
ceedings of the Third International Workshop on
the Evaluation of Systems for the Semantic Anal-
ysis of Text, Barcelona, Spain.
R. Mooney. 1996. Comparative experiments on
disambiguating word senses: An illustration of
the role of bias in machine learning. In Pro-
ceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 82?
91, May.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003.
Using measures of semantic relatedness for word
sense disambiguation. In Proceedings of the
Fourth International Conference on Intelligent
Text Processing and Computational Linguistics,
pages 241?257, Mexico City, February.
T. Pedersen and R. Bruce. 1997. A new supervised
learning algorithm for word sense disambigua-
tion. In Proceedings of the Fourteenth National
Conference on Artificial Intelligence, pages 604?
609, Providence, RI, July.
T. Pedersen. 2000. A simple approach to building
ensembles of Naive Bayesian classifiers for word
sense disambiguation. In Proceedings of the First
Annual Meeting of the North American Chapter
of the Association for Computational Linguistics,
pages 63?69, Seattle, WA, May.
T. Pedersen. 2001a. A decision tree of bigrams is
an accurate predictor of word sense. In Proceed-
ings of the Second Annual Meeting of the North
American Chapter of the Association for Com-
putational Linguistics, pages 79?86, Pittsburgh,
July.
T. Pedersen. 2001b. Machine learning with lexical
features: The Duluth approach to senseval-2. In
Proceedings of the Senseval-2 Workshop, pages
139?142, Toulouse, July.
J. Quinlan. 1986. Induction of decision trees. Ma-
chine Learning, 1:81?106.
I. Witten and E. Frank. 2000. Data Mining - Practi-
cal Machine Learning Tools and Techniques with
Java Implementations. Morgan?Kaufmann, San
Francisco, CA.
Combining Lexical and Syntactic Features for
Supervised Word Sense Disambiguation
Saif Mohammad
University of Toronto
Toronto, ON M4M2X6 Canada
smm@cs.toronto.edu
http://www.cs.toronto.edu/?smm
Ted Pedersen
University of Minnesota
Duluth, MN 55812 USA
tpederse@d.umn.edu
http://www.d.umn.edu/?tpederse
Abstract
The success of supervised learning approaches
to word sense disambiguation is largely de-
pendent on the features used to represent the
context in which an ambiguous word occurs.
Previous work has reached mixed conclusions;
some suggest that combinations of syntactic
and lexical features will perform most effec-
tively. However, others have shown that sim-
ple lexical features perform well on their own.
This paper evaluates the effect of using differ-
ent lexical and syntactic features both individu-
ally and in combination. We show that it is pos-
sible for a very simple ensemble that utilizes a
single lexical feature and a sequence of part of
speech features to result in disambiguation ac-
curacy that is near state of the art.
1 Introduction
Most words in natural language exhibit polysemy, that
is, they have multiple possible meanings. Each of these
meanings is referred to as a sense, and word sense disam-
biguation is the process of identifying the intended sense
of a target word based on the context in which it is used.
The context of the target word consists of the sentence
in which it occurs, and possibly one or two surrounding
sentences. Consider the following sentence:
Harry cast a bewitching spell (1)
The target word spell has many possible senses, such as,
a charm or incantation, to read out letter by letter, and
a period of time. The intended sense, a charm or incan-
tation, can be identified based on the context, which in
this case includes bewitching and a reference to a famous
young wizard.
Word sense disambiguation is often approached by su-
pervised learning techniques. The training data consists
of sentences which have potential target words tagged
by a human expert with their intended sense. Numerous
learning algorithms, such as, Naive Bayesian classifiers,
Decision Trees and Neural Networks have been used to
learn models of disambiguation. However, both (Peder-
sen, 2001a) and (Lee and Ng, 2002) suggest that different
learning algorithms result in little change in overall dis-
ambiguation results, and that the real determiner of accu-
racy is the set of features that are employed.
Previous work has shown that using different combi-
nations of features is advantageous for word sense dis-
ambiguation (e.g., (McRoy, 1992), (Ng and Lee, 1996),
(Stevenson and Wilks, 2001), (Yarowsky and Florian,
2002)). However, less attention is paid to determining
what the minimal set of features necessary to attain high
accuracy disambiguation are. In this paper we present
experiments that measure the redundancy in disambigua-
tion accuracy achieved by classifiers using two different
sets of features, and we also determine an upper bound on
the accuracy that could be attained via the combination of
such classifiers into an ensemble.
We find that simple combinations of lexical and syn-
tactic features can result in very high disambiguation
accuracy, via an extensive set of experiments using the
SENSEVAL-1, SENSEVAL-2, line, hard, serve and inter-
est data. Together, this consists of more than 50,000
sense-tagged instances. This paper also introduces a tech-
nique to quantify the optimum gain that is theoretically
possible when two feature sets are combined in an ensem-
ble. In the process, we identify some of the most useful
part of speech and parse features.
2 Feature Space
We employ lexical and syntactic features in our word
sense disambiguation experiments. The lexical features
are unigrams, bigrams, and the surface form of the target
word, while the syntactic features are part of speech tags
and various components from a parse tree.
2.1 Lexical Features
The surface form of a target word may restrict its possible
senses. Consider the noun case which has the surface
forms: case, cases and casing. These have the following
senses: object of investigation, frame or covering and a
weird person. Given an occurrence of the surface form
casing, we can immediately conclude that it was used in
the sense of a frame or covering and not the other two.
Each possible surface form as observed in the training
data is represented as a binary feature, and indicates if
that particular surface form occurs (or not).
Unigrams are individual words that appear in the text.
Consider the following sentence:
the judge dismissed the case (2)
Here the, judge, dismissed, the and case are unigrams.
Both judge and dismissed suggest that case has been used
in the judicial sense and not the others. Every unigram
that occurs above a certain frequency threshold in the
training corpus is represented as a binary feature. For
example, there is a feature that represents whether or not
judge occurs in the context of a target word.
Bigrams are pairs of words that occur in close proxim-
ity to each other, and in a particular order. For example,
in the following sentence:
the interest rate is lower in state banks (3)
the interest, interest rate, rate is, is lower, lower in, in
state and state banks are bigrams, where interest rate
suggests that bank has been used in the financial insti-
tution sense and not the river bank sense. Every bigram
that reaches a given frequency and measure of association
score threshold is represented as a binary feature. For ex-
ample, the bigram feature interest rate has value of 1 if it
occurs in the context of the target word, and 0 if it does
not.
We use the Ngram Statistics Package1 to identify fre-
quent unigrams and statistically significant bigrams in the
training corpus for a particular word. However, unigrams
or bigrams that occur commonly in text are ignored by
specifying a stop list composed mainly of prepositions,
articles and conjunctions.
2.2 Part of Speech Features
The parts of speech of words around the target word
are also useful clues for disambiguation. It is likely
that when used in different senses, the target word
will have markedly different configuration of parts of
speech around it. The following sentences have the
word turn in changing sides/parties sense and changing
course/direction senses, respectively:
1http://ngram.sourceforge.net
Did/VBD Jack/NNP turn/VB against/IN
his/PRP$ team/NN ?/. (4)
Did/VBD Jack/NNP turn/VB left/NN
at/IN the/DT crossing/NN ?/. (5)
Observe that the parts of speech following each occur-
rence of turn are significantly different, and that this dis-
tinction can be captured both by individual and combina-
tions of part of speech features.
The parts of speech of individual words at particular
positions relative to the target word serve as features. The
part of speech of the target word is P
0
. The POS of words
following the target are denoted by P
1
, P
2
and so on. The
POS of words to the left of the target word are P
?1
, P
?2
,
etc. There is a binary feature for each part of speech tag
observed in the training corpus at the given position or
positions of interest.
Suppose we would like to use part of speech features
for the target word and one word to the right of the target.
If the target word has 3 different parts of speech observed
in the training data, and the word to the right (without
regard to what that word is) has 32 different part of speech
tags, then there will be 35 binary features that represent
the occurrence of those tags at those positions.
We also consider combinations of part of speech tags
as features. These indicate when a particular sequence
of part of speech tags occurs at a given set of positions.
These features are boolean, and indicate if a particular se-
quence of tags has occurred or not. In the scenario above,
there would be 96 different binary features represented,
each of which indicates if a particular combination of val-
ues for the two positions of interest, occurs.
2.3 Parse Features
A sentence is made up of multiple phrases and each
phrase, in turn, is made of phrases or words. Each phrase
has a head word which may have strong syntactic re-
lations with other words in the sentence. Consider the
phrases, her hard work and the hard surface. The head
words work and surface are indicative of the calling for
stamina/endurance and not easily penetrable senses of
hard.
Thus, the head word of the phrase housing the target
word is used as a feature. The head word of its parent
phrase is also suggestive of the intended sense of the tar-
get word. Consider the sentence fragments fasten the line
and cross the line. The noun phrases (the line) have the
verbs fasten and cross as the head of parent phrases. Verb
fasten is indicative of the cord sense of line while cross
suggests the division sense.
The phrase housing the target word and the parent
phrase are also used as features. For example, phrase
housing the target word is a noun phrase, parent phrase
is a verb phrase and so on. Similar to the part of speech
features, all parse features are boolean.
3 Experimental Data
We conducted experiments using part of speech tagged
and parsed versions of the SENSEVAL-2, SENSEVAL-
1, line, hard, serve and interest data. The packages
posSenseval and parseSenseval part of speech
tagged and parsed the data, respectively. posSenseval
uses the Brill Tagger while parseSenseval employs
the Collins Parser. We used the training and test data
divisions that already exist in the SENSEVAL-2 and
SENSEVAL-1 data. However, the line, hard, serve and
interest data do not have a standard division, so we ran-
domly split the instances into test (20%) and training
(80%) portions.
The SENSEVAL-2 and SENSEVAL-1 data were cre-
ated for comparative word sense disambiguation exer-
cises held in the summers of 2001 and 1998, respectively.
The SENSEVAL-2 data consists of 4,328 test instances
and 8,611 training instances and include a total of 73
nouns, verbs and adjectives. The training data has the
target words annotated with senses from WordNet. The
target words have a varied number of senses ranging from
two for collaborate, graceful and solemn to 43 for turn.
The SENSEVAL-1 data has 8,512 test and 13,276 training
instances, respectively. The number of possible senses
for these words range from 2 to 15, and are tagged with
senses from the dictionary Hector.
The line data (Leacock, 1993) consists of 4,149 in-
stances where the noun line is used in one of six possible
WordNet senses. This data was extracted from the 1987-
1989 Wall Street Journal (WSJ) corpus, and the American
Printing House for the Blind (APHB) corpus. The distri-
bution of senses is somewhat skewed with more than 50%
of the instances used in the product sense while all the
other instances more or less equally distributed among
the other five senses.
The hard data (Leacock, 1998) consists of 4,337 in-
stances taken from the San Jose Mercury News Corpus
(SJM) and are annotated with one of three senses of the
adjective hard, from WordNet. The distribution of in-
stances is skewed with almost 80% of the instances used
in the not easy - difficult sense.
The serve data (Leacock, 1998) consists of 5,131 in-
stances with the verb serve as the target word. They are
annotated with one of four senses from WordNet. Like
line it was created from the WSJ and APHB corpora.
The interest data (Bruce, 1994) consists of 2,368 in-
stances where the noun interest is used in one of six
senses taken from the Longman Dictionary of Contempo-
rary English (LDOCE). The instances are extracted from
the part of speech tagged subset of the Penn Treebank
Wall Street Journal Corpus (ACL/DCI version).
4 Experiments and Discussion
The SyntaLexword sense disambiguation package was
used to carry out our experiments. It uses the C4.5 algo-
rithm, as implemented by the J48 program in the Waikato
Environment for Knowledge Analysis (Witten and Frank,
2000) to learn a decision tree for each word to be disam-
biguated.
We use the majority classifier as a baseline point of
comparison. This is a classifier that assigns all instances
to the most frequent sense in the training data. Our sys-
tem defaults to the majority classifier if it lacks any other
recourse, and therefore it disambiguates all instances. We
thus, report our results in terms of accuracy. Table 1
shows our overall experimental results, which will be dis-
cussed in the sections that follow. Note that the results of
the majority classifier appear at the bottom of that table,
and that the most accurate result for each set of of data is
shown in bold face.
4.1 Lexical Features
We utilized the following lexical features in our experi-
ments: the surface form of the target word, unigrams and
bigrams. The entries under Lexical in Table 1 show dis-
ambiguation accuracy when using those features individ-
ually.
It should be noted that the experiments for the
SENSEVAL-2 and SENSEVAL-1 data using unigrams and
bigrams are re-implementations of (Pedersen, 2001a),
and that our results are comparable. However, the exper-
iments on line, hard, serve and interest have been carried
out for the first time.
We observe that in general, surface form does not
improve significantly on the baseline results provided
by the majority classifier. While in most of the data
(SENSEVAL-2, line, hard and serve data) there is hardly
any improvement, we do see noticeable improvements in
SENSEVAL-1 and interest data. We believe that this is
due to the nature of the feature. Certain words have many
surface forms and senses. In many such cases, certain
senses can be represented by a restricted subset of possi-
ble surface forms. Such words are disambiguated better
than others using this feature.
4.2 Part of Speech Features
Word sense disambiguation using individual part of
speech features is done in order to compare the effect of
single POS features versus possibly more powerful com-
bination part of speech features. They are not expected
to be powerful enough to do very good classification but
may still capture certain intuitive notions. For example,
it is very likely that if the noun line is preceded by a wh
Table 1: Supervised WSD Accuracy by Feature Type
Features SENSEVAL-2 SENSEVAL-1 line hard serve interest
Lexical
Surface Form 49.3% 62.9% 54.3% 81.5% 44.2% 64.0%
Unigrams 55.3% 66.9% 74.5% 83.4% 73.3% 75.7%
Bigrams 55.1% 66.9% 72.9% 89.5% 72.1% 79.9%
POS
P
?2
47.1% 57.5% 54.9% 81.6% 52.1% 56.0%
P
?1
49.6% 59.2% 56.2% 82.1% 54.8% 62.7%
P
0
49.9% 60.3% 54.3% 81.6% 47.4% 64.0%
P
1
53.1% 63.9% 54.2% 81.6% 55.6% 65.3%
P
2
48.9% 59.9% 54.3% 81.7% 48.9% 62.3%
POS Combos
P
?1
, P
0
50.8% 62.2% 56.5% 82.3% 60.3% 67.7%
P
0
, P
1
54.3% 66.7% 54.1% 81.9% 60.2% 70.5%
P
1
, P
2
53.2% 64.0% 55.9% 82.2% 58.0% 68.6%
P
?1
, P
0
, P
1
54.6% 68.0% 60.4% 84.8% 73.0% 78.8%
P
?2
, P
?1
, P
0
, P
1
, P
2
54.6% 67.8% 62.3% 86.2% 75.7% 80.6%
Parse
Head (H) 51.7% 64.3% 54.7% 87.8% 47.4% 69.1%
Head of Parent (HP) 50.0% 60.6% 59.8% 84.5% 57.2% 67.8%
Phrase POS (P) 52.9% 58.5% 54.3% 81.5% 41.4% 54.9%
Parent Phrase POS (PP) 52.7% 57.9% 54.3% 81.7% 41.6% 54.9%
Parse Combos
H + HP 52.6% 65.1% 60.4% 87.7% 58.1% 73.2%
H + P 51.9% 65.1% 54.7% 87.8% 45.9% 69.1%
H + HP + P 52.9% 65.5% 60.4% 87.7% 57.6% 73.2%
H + P + HP + PP 52.7% 65.6% 60.5% 87.7% 56.7% 73.5%
Majority Classifier 47.7% 56.3% 54.3% 81.5% 42.2% 54.9%
word such as whose or which, it is used in the phone line
sense. If the noun line is preceded by a preposition, say in
or of, then there is a good chance that line has been used
in the formation sense. The accuracies achieved by part
of speech features on SENSEVAL-2, SENSEVAL-1, line,
hard, serve and interest data are shown in Table 1. The
individual part of speech feature results are under POS,
and the combinations under POS Combos.
We observe that the individual part of speech features
result in accuracies that are significantly better than the
majority classifier for all the data except for the line and
hard. Like the surface form, we believe that the part
of speech features are more useful to disambiguate cer-
tain words than others. We show averaged results for the
SENSEVAL-2 and SENSEVAL-1, and even there the part
of speech features fare well. In addition, when looking
at a more detailed breakdown of the 73 and 36 words in-
cluded in these samples respectively, a considerable num-
ber of those words experience improved accuracy using
part of speech features.
In particular, we observed that while verbs and adjec-
tives are disambiguated best by part of speech of words
one or two positions on their right (P
1
, P
2
), nouns in gen-
eral are aided by the part of speech of immediately adja-
cent words on either side (P
?1
, P
1
). In the case of tran-
sitive verbs (which are more frequent in this data than
intransitive verbs), the words at positions P
1
and P
2
are
usually the objects of the verb (for example, drink water).
Similarly, an adjective is usually immediately followed
by the noun which it qualifies (for example, short discus-
sion). Thus, in case of both verbs and adjectives, the word
immediately following (P
1
) is likely to be a noun having
strong syntactic relation to it. This explains the higher
accuracies for verbs and adjectives using P
1
and would
imply high accuracies for nouns using P
?1
, which too we
observe. However, we also observe high accuracies for
nouns using P
1
. This can be explained by the fact that
nouns are often the subjects in a sentence and the words
at positions P
1
and P
2
may be the syntactically related
verbs, which aid in disambiguation.
To summarize, verbs are aided by P
1
and P
2
, adjectives
by P
1
and nouns by P
?1
and P
1
. Thus, P
1
is the the most
potent individual part of speech feature to disambiguate a
set of noun, verb and adjective target words.
4.2.1 Combining Part of Speech features
A combination of parts of speech of words surround-
ing (and possibly including) the target word may better
capture the overall context than single part of speech fea-
tures. Following is an example of how a combination
of part of speech features may help identify the intended
sense of the noun line. If the target word line is used in
the plural form, is preceded by a personal pronoun and
the word following it is not a preposition, then it is likely
that the intended sense is line of text as in the actor forgot
his lines or they read their lines slowly. However, if the
word preceding line is a personal pronoun and the word
following it is a preposition, then it is probably used in
the product sense, as in, their line of clothes. POS Com-
bos in Table 1 shows the accuracies achieved using such
combinations with the SENSEVAL-2, SENSEVAL-1, line,
hard, serve and interest data. Again due to space con-
straints we do not give a break down of the accuracies for
the SENSEVAL-2 and SENSEVAL-1 data for noun, verb
and adjective target words.
We note that decision trees based on binary features
representing the possible values of a given sequence of
part of speech tags outperforms one based on individ-
ual features. The combinations which include P
1
obtain
higher accuracies. In the the case of the verbs and ad-
jectives in SENSEVAL-2 and SENSEVAL-1 data, the best
results are obtained using the parts of speech of words
following the target word. The nouns are helped by parts
of speech of words on both sides. This is in accordance
with the hypothesis that verbs and adjectives have strong
syntactic relations to words immediately following while
nouns may have strong syntactic relations on either side.
However, the hard and serve data are found to be helped
by features from both sides. We believe this is because
of the much larger number of instances per task in case
of hard and serve data as compared to the adjectives and
verbs in SENSEVAL-1 and SENSEVAL-2 data. Due to the
smaller amount of training data available for SENSEVAL-
2 and SENSEVAL-1 words, only the most potent features
help. The power of combining features is highlighted by
the significant improvement of accuracies above the base-
line for the line and hard data, which was not the case
using individual features (Table 1).
4.3 Parse Features
We employed the following parse features in these exper-
iments: the head word of the phrase housing the target
word, the type of phrase housing the target word (Noun
phrase, Verb Phrase, etc), the head of the parent phrase,
and the type of parent phrase. These results are shown
under Parse in Table 1.
The head word feature yielded the best results in all the
data except line, where the head of parent phrase is most
potent. Further, the nouns and adjectives benefit most by
the head word feature. We believe this the case because
the head word is usually a content word and thus likely
to be related to other nouns in the vicinity. Nouns are
usually found in noun phrases or prepositional phrases.
When part of a noun phrase, the noun is likely to be the
head and thus does not benefit much from the head word
feature. In such cases, the head of the parent phrase may
prove to be more useful as is the case in the line data.
In case of adjectives, the relation of the head word to the
target word is expected to be even stronger as it is likely to
be the noun modified by the adjective (target word). The
verb is most often found in a verb phrase and is usually
the head word. Hence, verb target words are not expected
to be benefited by the head word feature, which is what
we find here. The phrase housing the target word and the
parent phrase were not found to be beneficial when used
individually.
4.3.1 Combining Parse Features
Certain parse features, such as, the phrase of the target
word, take very few distinct values. For example, the tar-
get word shirt may occur in at most just two distinct kinds
of phrases: noun phrase and prepositional phrase. Such
features are not expected to perform much better than the
majority classifier. However, when used in combination
with other features, they may be useful. Thus, like part
of speech features, experiments were conducted using a
combination of parse features in an effort to better capture
the context and to identify sets of features which work
well together. Consider the parse features head word and
parent word. Head words such as magazine, situation and
story are indicative of the quality of causing attention to
be given sense of interest while parent words such as ac-
crue and equity are indicative of the interest rate sense.
A classifier based on both features can confidently clas-
sify both kinds of instances. Table 1 has the results under
Parse Combos. The Head and Head of Parent combina-
tions have in general yielded significantly higher accura-
cies than simply the head word or any other parse feature
used individually. The improvement is especially note-
worthy in case of line, serve and interest data. The in-
clusion of other features along with these two does not
help much more. We therefore find the Head and Head
of Parent combination to be the most potent parse feature
combination. It may be noted that a break down of ac-
curacies (not shown here for sake of brevity) for noun,
verb and adjective target words, of the SENSEVAL-1 and
SENSEVAL-2 data revealed that the adjectives were dis-
ambiguated best using the Head word and Phrase combi-
nation. This is observed in the hard data results as well,
albeit marginally.
Table 2: The Best Combinations of Syntactic and Lexical Features
Feature-Set Pair Baseline Maj. Simple Optimal Best
Data Set 1 Acc. Set2 Acc. Ens. Class. Ens. Ens.
SVAL-2 Unigram 55.3% P
?1
, P
0
, P
1
54.6% 43.6% 47.7% 57.0% 67.9% 66.7%
SVAL-1 Unigram 66.9% P
?1
, P
0
, P
1
68.0% 57.6% 56.3% 71.1% 78.0% 81.1%
line Unigram 74.5% P
?1
, P
0
, P
1
60.4% 55.1% 54.3% 74.2% 82.0% 88.0%
hard Bigram 89.5% Head, Parent 87.7% 86.1% 81.5% 88.9% 91.3% 83.0%
serve Unigram 73.3% P
?1
, P
0
, P
1
73.0% 58.4% 42.2% 81.6% 89.9% 83.0%
interest Bigram 79.9% P
?1
, P
0
, P
1
78.8% 67.6% 54.9% 83.2% 90.1% 89.0%
5 Complementary/Redundant Features
As can be observed in the previous results, many different
kinds of features can lead to roughly comparable word
sense disambiguation results.
Different types of features are expected to be redun-
dant to a certain extent. In other words, the features
will individually classify an identical subset of the in-
stances correctly. Likewise, the features are expected to
be complementary to some degree, that is, while one set
of features correctly disambiguates a certain subset of in-
stances, use of another set of features results in the cor-
rect disambiguation of an entirely distinct subset of the
instances.
The extent to which the feature sets are complementary
and redundant justify or obviate the combining of the fea-
ture sets. In order to accurately capture the amount of re-
dundancy and complementarity among two feature sets,
we introduce two measures: the Baseline Ensemble and
the Optimal Ensemble. Consider the scenario where the
outputs of two classifiers based on different feature sets
are to be combined using a simple voting or ensemble
technique for word sense disambiguation.
The Baseline Ensemble is the accuracy attained by a
hypothetical ensemble technique which correctly disam-
biguates an instance only when both the classifiers iden-
tify the intended sense correctly. In effect, the Baseline
Ensemble quantifies the redundancy among the two fea-
ture sets. The Optimal Ensemble is the accuracy of a
hypothetical ensemble technique which accurately dis-
ambiguates an instance when any of the two classifiers
correctly disambiguates the intended sense. We say that
these are hypothetical in that they can not be imple-
mented, but rather serve as a post disambiguation anal-
ysis technique.
Thus, the Optimal Ensemble is the upper bound to the
accuracy achievable by combining the two feature sets
using an ensemble technique. If the accuracies of indi-
vidual classifiers is X and Y, the Optimal Ensemble can
be defined as follows:
OptimalEnsemble = (X ?BaselineEnsemble) +
(Y ?BaselineEnsemble) + BaselineEnsemble
We use a simple ensemble technique to combine some
of the best lexical and syntactic features identified in the
previous sections. The probability of a sense to be the
intended sense as identified by lexical and syntactic fea-
tures is summed. The sense which attains the highest
score is chosen as the intended sense. Table 2 shows
the best results achieved using this technique along with
the baseline and optimal ensembles for the SENSEVAL-
2, SENSEVAL-1, line, hard, serve and interest data. The
table also presents the feature sets that achieved these re-
sults. In addition, the last column of this table shows rep-
resentative values for some of the best results attained in
the published literature for these data sets. Note that these
are only approximate points of comparison, in that there
are differences in how individual experiments are con-
ducted for all of the non?SENSEVAL data.
From the Baseline Ensemble we observe that there is a
large amount of redundancy across the feature sets. That
said, there is still a significant amount of complementar-
ity as may be noted by the difference between the Optimal
Ensemble and the greater of the individual accuracies.
For example, in the SENSEVAL-2 data, unigrams alone
achieve 55.3% accuracy and part of speech features attain
an accuracy of 54.6%. The Baseline Ensemble attains ac-
curacy of 43.6%, which means that this percentage of the
test instances are correctly tagged, independently, by both
unigrams and part of speech features. The unigrams get
an additional 11.7% of the instances correct which the
part of speech features tag incorrectly.
Similarly, the part of speech features are able to cor-
rectly tag an additional 11% of the instances which are
tagged erroneously when using only bigrams. The above
values suggest a high amount of redundancy among the
unigrams and part of speech features but not high enough
to suggest that there is no significant benefit in combin-
ing the two kinds of features. The difference between the
Optimal Ensemble and the accuracy attained by unigrams
is 12.6% (67.9% - 55.3%). This is a significant improve-
ment in accuracy which may be achieved by a suitable
ensemble technique. The difference is a quantification of
the complementarity between unigram and part of speech
features based on the data. Further, we may conclude that
given these unigram and part of speech features, the best
ensemble techniques will not achieve accuracies higher
than 67.9%.
It may be noted that a single unified classifier based
on multiple features may achieve accuracies higher than
the Optimal Ensemble. However, we show that an ac-
curate ensemble method (Optimal Ensemble), based on
simple lexical and syntactic features, achieves accuracies
comparable or better than some of the best previous re-
sults. The point here is that using information from two
distinct feature sets (lexical features and part of speech)
could lead to state of the art results. However, it is as
yet unclear how to most effectively combine such simple
classifiers to achieve these optimal results.
Observation of the pairs of lexical and syntactic fea-
tures which provide highest accuracies for the various
data suggest that the part of speech combination feature -
P
?1
, P
0
, P
1
, is likely to be most complementary with the
lexical features (bigrams or unigrams).
The hard data did particularly well with combinations
of parse features, the Head and Parent words. The Op-
timal Ensemble attains accuracy of over 91%, while the
best previous results were approximately 83%. This indi-
cates that not only are the Head and Parent word features
very useful in disambiguating adjectives but are also a
source of complementary information to lexical features.
6 Related Work
(McRoy, 1992) was one of the first to use multiple kinds
of features for word sense disambiguation in the semantic
interpretation system, TRUMP. The system aims at dis-
ambiguating all words in the text and relies extensively
on dictionaries and is not corpus based. Scores are as-
signed based on morphology, part of speech, collocations
and syntactic cues. The sense with the highest score is
chosen as the intended sense. TRUMP was used to tag a
subset of the Wall Street Journal (around 2500 words) but
was not evaluated due to lack of gold standard.
The LEXAS system of (Ng and Lee, 1996) uses part
of speech, morphology, co-occurrences, collocations and
verb object relation in nearest neighbor implementation.
The system was evaluated using the interest data on
which it achieved an accuracy of 87.3%. They studied
the utility of individual features and found collocations
to be most useful, followed by part of speech and mor-
phological form.
(Lin, 1997) takes a supervised approach that is unique
as it did not create a classifier for every target word. The
system compares the context of the target word with that
of training instances which are similar to it. The sense of
the target word most similar to these contexts is chosen
as the intended sense. Similar to McRoy, the system at-
tempts to disambiguate all words in the text. Lin relies on
syntactic relations, such as, subject-verb agreement and
verb object relations to capture the context. The system
achieved accuracies between 59% and 67% on the Sem-
Cor corpus.
(Pedersen, 2001b) compares decision trees, decision
stumps and a Naive Bayesian classifier to show that bi-
grams are very useful in identifying the intended sense
of a word. The accuracies of 19 out of the total 36 tasks
in SENSEVAL-1 data were greater than the best reported
results in that event. Bigrams are easily captured from
raw text and the encouraging results mean that they can
act as a powerful baseline to build more complex systems
by incorporating other sources of information. Pedersen
points out that decision trees can effectively depict the re-
lations among the various features used. With the use of
multiple sources of information this quality of decision
trees gains further significance.
(Lee and Ng, 2002) compare the performances of Sup-
port Vector Machines, Naive Bayes, AdaBoost and De-
cision Trees using unigrams, parts of speech, colloca-
tions and syntactic relations. The experiments were con-
ducted on SENSEVAL-2 and SENSEVAL-1 data. They
found the combination of features achieved highest ac-
curacy (around 73%) in SENSEVAL-1 data, irrespective
of the learning algorithm. Collocations(57.2%), part of
speech tags(55.3%) and syntactic relations(54.2%) per-
formed better than decision trees using all features in the
SENSEVAL-2 data.
(Yarowsky and Florian, 2002) performed experiments
with different learning algorithms and multiple features.
Three kinds of Bayes Classifier, Decision lists and Trans-
formation Based Learning Model (TBL) were used with
collocations, bag of words and syntactic relations as fea-
tures. Experiments on SENSEVAL-2 data revealed that
the exclusion of any of the three kinds of features resulted
in a significant drop in accuracy. Lee and Ng as well as
Yarowsky and Florian conclude that the combination of
features is beneficial.
(Pedersen, 2002) does a pairwise study of the systems
that participated in SENSEVAL-2 English and Spanish
disambiguation exercises. The study approaches the sys-
tems as black boxes, looking only at the assigned tags
whatever the classifier and sources of information may
be. He introduces measures to determine the similarity
of the classifications and optimum results obtainable by
combining the systems. He points out that pairs of sys-
tems having low similarity and high optimal accuracies
are of interest as they are markedly complementary and
the combination of such systems is beneficial.
There still remain questions regarding the use of mul-
tiple sources of information, in particular which features
should be combined and what is the upper bound on the
accuracies achievable by such combinations. (Pedersen,
2002) describes how to determine the upper bound when
combining two systems. This paper extends that idea to
provide measures which determine the upper bound when
combining two sets of features in a single disambiguation
system. We provide a measure to determine the redun-
dancy in classification done using two different feature
sets. We identify particular part of speech and parse fea-
tures which were found to be very useful and the com-
binations of lexical and syntactic features which worked
best on SENSEVAL-2, SENSEVAL-1, line, hard, serve and
interest data.
7 Conclusions
We conducted an extensive array of word sense disam-
biguation experiments using a rich set of lexical and syn-
tactic features. We use the SENSEVAL-2, SENSEVAL-1,
line, hard, serve and interest data which together have
more than 50,000 sense tagged instances. We show that
both lexical and syntactic features achieve reasonably
good accuracies when used individually, and that the part
of speech of the word immediately following the target
word is particularly useful in disambiguation as com-
pared to other individual part of speech features. A com-
bination of part of speech features attains even better ac-
curacies and we identify (P
0
, P
1
) and (P
?1
, P
0
, P
1
) as the
most potent combinations. We show that the head word
of a phrase is particularly useful in disambiguating adjec-
tives and nouns. We identify the head and parent as the
most potent parse feature combination.
We introduce the measures Baseline Ensemble and Op-
timal Ensemble which quantify the redundancy among
two feature sets and the maximum accuracy attainable by
an ensemble technique using the two feature sets. We
show that even though lexical and syntactic features are
redundant to a certain extent, there is a significant amount
of complementarity. In particular, we showed that sim-
ple lexical features (unigrams and bigrams) used in con-
junction with part of speech features have the potential to
achieve state of the art results.
8 Acknowledgments
This work has been partially supported by a National Sci-
ence Foundation Faculty Early CAREER Development
award (#0092784).
References
R. Bruce and L. Wiebe. 1994 Word-Sense Disambigua-
tion using Decomposable Models In Proceedings of
the 32nd Annual Meeting of the Association for Com-
putational Linguistics.
C. Leacock and M. Chodorow and G. Miller. 1998 Us-
ing Corpus Statistics and WordNet Relations for Sense
Identification Computational Linguistics, 24(1):147?
165.
C. Leacock and E. Voorhees. 1993 Corpus-Based Sta-
tistical Sense Resolution In Proceedings of the ARPA
Workshop on Human Language Technology.
K.L. Lee and H.T. Ng. 2002. An empirical evaluation of
knowledge sources and learning algorithms for word
sense disambiguation. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 41?48.
D. Lin. 1997. Using syntactic dependency as a local
context to resolve word sense ambiguity. In Proceed-
ings of the 35th Annual Meeting of the Association for
Computational Linguistics, pages 64?71, Madrid, July.
S. McRoy. 1992. Using multiple knowledge sources for
word sense discrimination. Computational Linguis-
tics, 18(1):1?30.
H.T. Ng and H.B. Lee. 1996. Integrating multiple
knowledge sources to disambiguate word sense: An
exemplar-based approach. In Proceedings of the 34th
Annual Meeting of the Association for Computational
Linguistics, pages 40?47.
T. Pedersen. 2001a. A decision tree of bigrams is an ac-
curate predictor of word sense. In Proceedings of the
Second Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 79?86, Pittsburgh, July.
T. Pedersen. 2001b. Machine learning with lexical fea-
tures: The duluth approach to senseval-2. In Pro-
ceedings of the Senseval-2 Workshop, pages 139?142,
Toulouse, July.
T. Pedersen. 2002. Assessing system agreement and
instance difficulty in the lexical samples tasks of
senseval-2. In Proceedings of the ACL Workshop on
Word Sense Disambiguation: Recent Successes and
Future Directions, pages 40?46, Philadelphia.
M. Stevenson and Y. Wilks. 2001. The interaction
of knowledge sources in word sense disambiguation.
Computational Linguistics, 27(3):321?349, Septem-
ber.
I. Witten and E. Frank. 2000. Data Mining - Practi-
cal Machine Learning Tools and Techniques with Java
Implementations. Morgan?Kaufmann, San Francisco,
CA.
D. Yarowsky and R. Florian. 2002. Evaluating sense
disambiguation performance across diverse parameter
spaces. Journal of Natural Language Engineering,
8(2).
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association
for Computational Linguistics, pages 189?196, Cam-
bridge, MA.
Word Sense Discrimination by Clustering Contexts
in Vector and Similarity Spaces
Amruta Purandare and Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812 USA
{pura0010,tpederse}@d.umn.edu
http://senseclusters.sourceforge.net
Abstract
This paper systematically compares unsuper-
vised word sense discrimination techniques
that cluster instances of a target word that oc-
cur in raw text using both vector and similarity
spaces. The context of each instance is repre-
sented as a vector in a high dimensional fea-
ture space. Discrimination is achieved by clus-
tering these context vectors directly in vector
space and also by finding pairwise similarities
among the vectors and then clustering in sim-
ilarity space. We employ two different repre-
sentations of the context in which a target word
occurs. First order context vectors represent
the context of each instance of a target word
as a vector of features that occur in that con-
text. Second order context vectors are an indi-
rect representation of the context based on the
average of vectors that represent the words that
occur in the context. We evaluate the discrim-
inated clusters by carrying out experiments us-
ing sense?tagged instances of 24 SENSEVAL-
2 words and the well known Line, Hard and
Serve sense?tagged corpora.
1 Introduction
Most words in natural language have multiple possible
meanings that can only be determined by considering
the context in which they occur. Given a target word
used in a number of different contexts, word sense dis-
crimination is the process of grouping these instances of
the target word together by determining which contexts
are the most similar to each other. This is motivated by
(Miller and Charles, 1991), who hypothesize that words
with similar meanings are often used in similar contexts.
Hence, word sense discrimination reduces to the problem
of finding classes of similar contexts such that each class
represents a single word sense. Put another way, contexts
that are grouped together in the same class represent a
particular word sense.
While there has been some previous work in sense dis-
crimination (e.g., (Schu?tze, 1992), (Pedersen and Bruce,
1997), (Pedersen and Bruce, 1998), (Schu?tze, 1998),
(Fukumoto and Suzuki, 1999)), by comparison it is much
less than that devoted to word sense disambiguation,
which is the process of assigning a meaning to a word
from a predefined set of possibilities. However, solutions
to disambiguation usually require the availability of an
external knowledge source or manually created sense?
tagged training data. As such these are knowledge inten-
sive methods that are difficult to adapt to new domains.
By contrast, word sense discrimination is an unsuper-
vised clustering problem. This is an attractive methodol-
ogy because it is a knowledge lean approach based on ev-
idence found in simple raw text. Manually sense tagged
text is not required, nor are specific knowledge rich re-
sources like dictionaries or ontologies. Instances are clus-
tered based on their mutual contextual similarities which
can be completely computed from the text itself.
This paper presents a systematic comparison of dis-
crimination techniques suggested by Pedersen and Bruce
((Pedersen and Bruce, 1997), (Pedersen and Bruce,
1998)) and by Schu?tze ((Schu?tze, 1992), (Schu?tze,
1998)). This paper also proposes and evaluates several
extensions to these techniques.
We begin with a summary of previous work, and then
a discussion of features and two types of context vec-
tors. We summarize techniques for clustering in vector
versus similarity spaces, and then present our experimen-
tal methodology, including a discussion of the data used
in our experiments. Then we describe our approach to
the evaluation of unsupervised word sense discrimina-
tion. Finally we present an analysis of our experimental
results, and conclude with directions for future work.
2 Previous Work
(Pedersen and Bruce, 1997) and (Pedersen and Bruce,
1998) propose a (dis)similarity based discrimination ap-
proach that computes (dis)similarity among each pair of
instances of the target word. This information is recorded
in a (dis)similarity matrix whose rows/columns repre-
sent the instances of the target word that are to be dis-
criminated. The cell entries of the matrix show the de-
gree to which the pair of instances represented by the
corresponding row and column are (dis)similar. The
(dis)similarity is computed from the first order context
vectors of the instances which show each instance as a
vector of features that directly occur near the target word
in that instance.
(Schu?tze, 1998) introduces second order context vec-
tors that represent an instance by averaging the feature
vectors of the content words that occur in the context of
the target word in that instance. These second order con-
text vectors then become the input to the clustering algo-
rithm which clusters the given contexts in vector space,
instead of building the similarity matrix structure.
There are some significant differences in the ap-
proaches suggested by Pedersen and Bruce and by
Schu?tze. As yet there has not been any systematic study
to determine which set of techniques results in better
sense discrimination. In the sections that follow, we high-
light some of the differences between these approaches.
2.1 Context Representation
Pedersen and Bruce represent the context of each test in-
stance as a vector of features that directly occur near the
target word in that instance. We refer to this representa-
tion as the first order context vector. Schu?tze, by contrast,
uses the second order context representation that averages
the first order context vectors of individual features that
occur near the target word in the instance. Thus, Schu?tze
represents each feature as a vector of words that occur
in its context and then computes the context of the target
word by adding the feature vectors of significant content
words that occur near the target word in that context.
2.2 Features
Pedersen and Bruce use a small number of local features
that include co?occurrence and part of speech informa-
tion near the target word. They select features from the
same test data that is being discriminated, which is a com-
mon practice in clustering in general. Schu?tze represents
contexts in a high dimensional feature space that is cre-
ated using a separate large corpus (referred to as the train-
ing corpus). He selects features based on their frequency
counts or log-likelihood ratios in this corpus.
In this paper, we adopt Schu?tze?s approach and select
features from a separate corpus of training data, in part
because the number of test instances may be relatively
small and may not be suitable for selecting a good feature
set. In addition, this makes it possible to explore varia-
tions in the training data while maintaining a consistent
test set. Since the training data used in unsupervised clus-
tering does not need to be sense tagged, in future work we
plan to develop methods of collecting very large amounts
of raw corpora from the Web and other online sources
and use it to extract features.
Schu?tze represents each feature as a vector of words
that co?occur with that feature in the training data. These
feature vectors are in fact the first order context vectors
of the feature words (and not target word). The words
that co?occur with the feature words form the dimensions
of the feature space. Schu?tze reduces the dimensional-
ity of this feature space using Singular Value Decompo-
sition (SVD), which is also employed by related tech-
niques such as Latent Semantic Indexing (Deerwester et
al., 1990) and Latent Semantic Analysis (Landauer et al,
1998). SVD has the effect of converting a word level
feature space into a concept level semantic space that
smoothes the fine distinctions between features that rep-
resent similar concepts.
2.3 Clustering Space
Pedersen and Bruce represent instances in a
(dis)similarity space where each instance can be seen as
a point and the distance between any two points is a func-
tion of their mutual (dis)similarities. The (dis)similarity
matrix showing the pair-wise (dis)similarities among
the instances is given as the input to the agglomerative
clustering algorithm. The context group discrimination
method used by Schu?tze, on the other hand, operates on
the vector representations of instances and thus works
in vector space. Also he employs a hybrid clustering
approach which uses both an agglomerative and the
Estimation Maximization (EM) algorithm.
3 First Order Context Vectors
First order context vectors directly indicate which fea-
tures make up a context. In all of our experiments, the
context of the target word is limited to 20 surrounding
content words on either side. This is true both when we
are selecting features from a set of training data, or when
we are converting test instances into vectors for cluster-
ing. The particular features we are interested in are bi-
grams and co?occurrences.
Co-occurrences are words that occur within five po-
sitions of the target word (i.e., up to three intervening
words are allowed). Bigrams are ordered pairs of words
that co?occur within five positions of each other. Thus,
co?occurrences are unordered word pairs that include the
target word, whereas bigrams are ordered pairs that may
or may not include the target. Both the co?occurrences
and the bigrams must occur in at least two instances in
the training data, and the two words must have a log?
likelihood ratio in excess of 3.841, which has the effect
of removing co?occurrences and bigrams that have more
than 95% chance of being independent of the target word.
After selecting a set of co-occurrences or bigrams from
a corpus of training data, a first order context representa-
tion is created for each test instance. This shows how
many times each feature occurs in the context of the tar-
get word (i.e., within 20 positions from the target word)
in that instance.
4 Second Order Context Vectors
A test instance can be represented by a second order con-
text vector by finding the average of the first order context
vectors that are associated with the words that occur near
the target word. Thus, the second order context represen-
tation relies on the first order context vectors of feature
words. The second order experiments in this paper use
two different types of features, co?occurrences and bi-
grams, defined as they are in the first order experiments.
Each co?occurrence identified in training data is as-
signed a unique index and occupies the corresponding
row/column in a word co?occurrence matrix. This is
constructed from the co?occurrence pairs, and is a sym-
metric adjacency matrix whose cell values show the log-
likelihood ratio for the pair of words representing the
corresponding row and column. Each row of the co?
occurrence matrix can be seen as a first order context vec-
tor of the word represented by that row. The set of words
forming the rows/columns of the co?occurrence matrix
are treated as the feature words.
Bigram features lead to a bigram matrix such that
for each selected bigram WORDi<>WORDj, WORDi
represents a single row, say the ith row, and WORDj
represents a single column, say the jth column, of
the bigram matrix. Then the value of cell (i,j) indi-
cates the log?likelihood ratio of the words in the bigram
WORDi<>WORDj. Each row of the bigram matrix can
be seen as a bigram vector that shows the scores of all
bigrams in which the word represented by that row oc-
curs as the first word. Thus, the words representing the
rows of the bigram matrix make the feature set while the
words representing the columns form the dimensions of
the feature space.
5 Clustering
The objective of clustering is to take a set of instances
represented as either a similarity matrix or context vec-
tors and cluster together instances that are more like each
other than they are to the instances that belong to other
clusters.
Clustering algorithms are classified into three main
categories, hierarchical, partitional, and hybrid methods
that incorporate ideas from both. The algorithm acts as a
search strategy that dictates how to proceed through the
instances. The actual choice of which clusters to split
or merge is decided by a criteria function. This section
describes the clustering algorithms and criteria functions
that have been employed in our experiments.
5.1 Hierarchical
Hierarchical algorithms are either agglomerative or divi-
sive. They both proceed iteratively, and merge or divide
clusters at each step. Agglomerative algorithms start with
each instance in a separate cluster and merge a pair of
clusters at each iteration until there is only a single clus-
ter remaining. Divisive methods start with all instances
in the same cluster and split one cluster into two during
each iteration until all instances are in their own cluster.
The most widely known criteria functions used with hi-
erarchical agglomerative algorithms are single link, com-
plete link, and average link, also known as UPGMA.
(Schu?tze, 1998) points out that single link clustering
tends to place all instances into a single elongated clus-
ter, whereas (Pedersen and Bruce, 1997) and (Purandare,
2003) show that hierarchical agglomerative clustering
using average link (via McQuitty?s method) fares well.
Thus, we have chosen to use average link/UPGMA as our
criteria function for the agglomerative experiments.
In similarity space, each instance can be viewed as a
node in a weighted graph. The weights on edges joining
two nodes indicate their pairwise similarity as measured
by the cosine between the context vectors that represent
the pair of instances.
When agglomerative clustering starts, each node is in
its own cluster and is considered to be the centroid of that
cluster. At each iteration, average link selects the pair
of clusters whose centroids are most similar and merges
them into a single cluster. For example, suppose the clus-
ters I and J are to be merged into a single cluster IJ . The
weights on all other edges that connect existing nodes to
the new node IJ must now be revised. Suppose that Q is
such a node. The new weight in the graph is computed by
averaging the weight on the edge between nodes I and Q
and that on the edge between J and Q. In other words:
W
?
(IJ,Q) =
W (I,Q) +W (J,Q)
2
(1)
In vector space, average link starts by assigning each
vector to a single cluster. The centroid of each cluster is
found by calculating the average of all the context vec-
tors that make up the cluster. At each iteration, average
link selects the pair of clusters whose centroids are clos-
est with respect to their cosines. The selected pair of clus-
ters is merged and a centroid is computed for this newly
created cluster.
5.2 Partitional
Partitional algorithms divide an entire set of instances
into a predetermined number of clusters (K) without go-
ing through a series of pairwise comparisons. As such
these methods are somewhat faster than hierarchical al-
gorithms.
For example, the well known K-means algorithm is
partitional. In vector space, each instance is represented
by a context vector. K-means initially selects K random
vectors to serve as centroids of these initial K clusters. It
then assigns every other vector to one of the K clusters
whose centroid is closest to that vector. After all vectors
are assigned, it recomputes the cluster centroids by av-
eraging all of the vectors assigned to that cluster. This
repeats until convergence, that is until no vector changes
its cluster across iterations and the centroids stabilize.
In similarity space, each instance can be viewed as a
node of a fully connected weighted graph whose edges in-
dicate the similarity between the instances they connect.
K-means will first select K random nodes that represent
the centroids of the initial K clusters. It will then assign
every other node I to one of the K clusters such that the
edge joining I and the centroid of that cluster has maxi-
mum weight among the edges joining I to all centroids.
5.3 Hybrid Methods
It is generally believed that the quality of clustering by
partitional algorithms is inferior to that of the agglom-
erative methods. However, a recent study (Zhao and
Karypis, 2002) has suggested that these conclusions are
based on experiments conducted with smaller data sets,
and that with larger data sets partitional algorithms are
not only faster but lead to better results.
In particular, Zhao and Karypis recommend a hybrid
approach known as Repeated Bisections. This overcomes
the main weakness with partitional approaches, which is
the instability in clustering solutions due to the choice of
the initial random centroids. Repeated Bisections starts
with all instances in a single cluster. At each iteration it
selects one cluster whose bisection optimizes the chosen
criteria function. The cluster is bisected using standard
K-means method with K=2, while the criteria function
maximizes the similarity between each instance and the
centroid of the cluster to which it is assigned. As such this
is a hybrid method that combines a hierarchical divisive
approach with partitioning.
6 Experimental Data
We use 24 of the 73 words in the SENSEVAL-2 sense?
tagged corpus, and the Line, Hard and Serve sense?
tagged corpora. Each of these corpora are made up of
instances that consist of 2 or 3 sentences that include a
single target word that has a manually assigned sense tag.
However, we ignore the sense tags at all times except
during evaluation. At no point do the sense tags enter into
the clustering or feature selection processes. To be clear,
we do not believe that unsupervised word sense discrim-
ination needs to be carried out relative to a pre-existing
set of senses. In fact, one of the great advantages of un-
supervised technique is that it doesn?t need a manually
annotated text. However, here we employ sense?tagged
text in order to evaluate the clusters that we discover.
The SENSEVAL-2 data is already divided into training
and test sets, and those splits were retained for these ex-
periments. The SENSEVAL-2 data is relatively small, in
that each word has approximately 50-200 training and
test instances. The data is particularly challenging for
unsupervised algorithms due to the large number of fine
grained senses, generally 8 to 12 per word. The small
volume of data combined with large number of possible
senses leads to very small set of examples for most of the
senses.
As a result, prior to clustering we filter the training
and test data independently such that any instance that
uses a sense that occurs in less than 10% of the available
instances for a given word is removed. We then elimi-
nate any words that have less than 90 training instances
after filtering. This process leaves us with a set of 24
SENSEVAL-2 words, which includes the 14 nouns, 6 ad-
jectives and 4 verbs that are shown in Table 1.
In creating our evaluation standard, we assume that
each instance will be assigned to at most a single clus-
ter. Therefore if an instance has multiple correct senses
associated with it, we treat the most frequent of these as
the desired tag, and ignore the others as possible correct
answers in the test data.
The Line, Hard and Serve corpora do not have a stan-
dard training?test split, so these were randomly divided
into 60?40 training?test splits. Due to the large number
of training and test instances for these words, we filtered
out instances associated with any sense that occurred in
less than 5% of the training or test instances.
We also randomly selected five pairs of words from
the SENSEVAL-2 data and mixed their instances together
(while retaining the training and test distinction that al-
ready existed in the data). After mixing, the data was
filtered such that any sense that made up less than 10%
in the training or test data of the new mixed sample was
removed; this is why the total number of instances for the
mixed pairs is not the same as the sum of those for the
individual words. These mix-words were created in order
to provide data that included both fine grained and coarse
grained distinctions.
Table 1 shows all words that were used in our exper-
iments along with their parts of speech. Thereafter we
show the number of training (TRN) and test instances
(TST) that remain after filtering, and the number of
senses found in the test data (S). We also show the per-
centage of the majority sense in the test data (MAJ). This
is particularly useful, since this is the accuracy that would
be attained by a baseline clustering algorithm that puts all
test instances into a single cluster.
7 Evaluation Technique
When we cluster test instances, we specify an upper limit
on the number of clusters that can be discovered. In these
experiments that value is 7. This reflects the fact that
we do not know a?priori the number of possible senses a
word will have. This also allows us to verify the hypothe-
sis that a good clustering approach will automatically dis-
cover approximately same number of clusters as senses
for that word, and the extra clusters (7?#actual senses)
will contain very few instances. As can be seen from col-
umn S in Table 1, most of the words have 2 to 4 senses on
an average. Of the 7 clusters created by an algorithm, we
detect the significant clusters by ignoring (throwing out)
clusters that contain less than 2% of the total instances.
The instances in the discarded clusters are counted as un-
clustered instances and are subtracted from the total num-
ber of instances.
Our basic strategy for evaluation is to assign available
sense tags to the discovered clusters such that the assign-
ment leads to a maximally accurate mapping of senses to
clusters. The problem of assigning senses to clusters be-
comes one of reordering the columns of a confusion ma-
trix that shows how senses and clusters align such that the
diagonal sum is maximized. This corresponds to several
well known problems, among them the Assignment Prob-
lem in Operations Research, or determining the maximal
matching of a bipartite graph in Graph Theory.
During evaluation we assign one sense to at most one
cluster, and vice versa. When the number of discovered
clusters is the same as the number of senses, then there
is a one to one mapping between them. When the num-
ber of clusters is greater than the number of actual senses,
then some clusters will be left unassigned. And when the
number of senses is greater than the number of clusters,
some senses will not be assigned to any cluster. The rea-
son for not assigning a single sense to multiple clusters
or multiple senses to one cluster is that, we are assuming
one sense per instance and one sense per cluster.
We measure the precision and recall based on this max-
imally accurate assignment of sense tags to clusters. Pre-
cision is defined as the number of instances that are clus-
tered correctly divided by the number of instances clus-
tered, while recall is the number of instances clustered
correctly over the total number of instances. From that we
compute the F?measure, which is two times the precision
and recall, divided by the sum of precision and recall.
8 Experimental Results
We present the discrimination results for six configura-
tions of features, context representations and clustering
algorithms. These were run on each of the 27 target
words, and also on the five mixed words. What follows is
a concise description of each configuration.
 PB1 : First order context vectors, using co?
occurrence features, are clustered in similarity space
using the UPGMA technique.
 PB2 : Same as PB1, except that the first order con-
text vectors are clustered in vector space using Re-
peated Bisections.
 PB3: Same as PB1, except the first order con-
text vectors used bigram features instead of co?
occurrences.
All of the PB experiments use first order context repre-
sentations that correspond to the approach suggested by
Pedersen and Bruce.
 SC1: Second order context vectors of instances were
clustered in vector space using the Repeated Bisec-
tions technique. The context vectors were created
from the word co?occurrence matrix whose dimen-
sions were reduced using SVD.
 SC2: Same as SC1 except that the second order con-
text vectors are converted to a similarity matrix and
clustered using the UPGMA method.
 SC3: Same as SC1, except the second order context
vectors were created from the bigram matrix.
All of the SC experiments use second order context
vectors and hence follow the approach suggested by
Schu?tze.
Experiment PB2 clusters the Pedersen and Bruce style
(first order) context vectors using the Schu?tze like cluster-
ing scheme, while SC2 tries to see the effect of using the
Pedersen and Bruce style clustering method on Schu?tze
style (second order) context vectors. The motivation be-
hind experiments PB3 and SC3 is to try bigram features
in both PB and SC style context vectors.
The F?measure associated with the discrimination of
each word is shown in Table 1. Any score that is sig-
nificantly greater than the majority sense (according to a
paired t?test) is shown in bold face.
9 Analysis and Discussion
We employ three different types of data in our experi-
ments. The SENSEVAL-2 words have a relatively small
number of training and test instances (around 50-200).
However, the Line, Hard and Serve data is much larger,
word.pos TRN TST S PB1 SC1 PB2 SC2 PB3 SC3 MAJ
art.n 159 83 4 37.97 45.52 45.46 46.15 43.03 55.34 46.32
authority.n 168 90 4 38.15 51.25 43.93 53.01 41.86 34.94 37.76
bar.n 220 119 5 34.63 37.23 50.66 40.87 41.05 58.26 45.93
channel.n 135 67 6 40.63 37.21 40.31 41.54 36.51 39.06 31.88
child.n 116 62 2 45.04 46.85 51.32 50.00 55.17 53.45 56.45
church.n 123 60 2 57.14 49.09 48.21 55.36 52.73 46.43 59.02
circuit.n 129 75 8 25.17 34.72 32.17 33.33 27.97 25.35 30.26
day.n 239 128 3 60.48 46.15 55.65 45.76 62.65 55.65 62.94
facility.n 110 56 3 40.00 58.00 38.09 58.00 38.46 64.76 48.28
feeling.n 98 45 2 58.23 51.22 52.50 56.10 46.34 53.66 61.70
grip.n 94 49 5 45.66 43.01 58.06 53.76 49.46 49.46 46.67
material.n 111 65 5 32.79 40.98 41.32 47.54 32.79 47.54 42.25
mouth.n 106 55 4 54.90 47.53 60.78 43.14 43.14 47.06 46.97
post.n 135 72 5 32.36 37.96 48.17 30.88 30.88 32.36 32.05
blind.a 97 53 3 53.06 61.18 63.64 58.43 76.29 79.17 82.46
cool.a 102 51 5 35.42 39.58 38.71 34.78 33.68 38.71 42.86
fine.a 93 59 5 47.27 47.71 47.71 33.93 38.18 47.71 41.10
free.a 105 64 3 48.74 49.54 52.54 55.46 45.00 52.99 49.23
natural.a 142 75 4 34.72 35.21 33.56 30.99 32.40 38.03 35.80
simple.a 126 64 4 38.33 50.00 47.06 38.33 38.33 47.06 50.75
begin.v 507 255 3 59.36 40.46 40.40 43.66 70.12 42.55 64.31
leave.v 118 54 5 43.14 38.78 27.73 40.00 46.00 53.47 38.18
live.v 112 59 4 37.83 40.00 48.21 45.45 36.37 41.82 57.63
train.v 116 56 5 28.57 33.96 28.57 34.28 26.67 32.08 33.93
line.n 1615 1197 3 72.67 26.77 62.00 55.47 68.40 37.97 72.10
hard.a 2365 1592 2 86.75 67.42 41.18 73.22 87.06 63.41 87.44
serve.v 2365 1752 4 40.50 33.20 36.82 34.37 45.66 31.46 40.53
cool.a-train.v 197 102 8 22.34 39.00 25.25 40.61 22.57 41.00 22.86
fine.a-cool.a 185 104 7 27.86 42.36 33.83 47.72 35.00 42.05 24.79
fine.a-grip.n 177 99 7 36.84 49.48 33.50 45.02 31.41 49.48 24.19
leave.v-post.n 204 113 8 29.36 48.18 32.11 41.44 23.85 41.82 21.01
post.n-grip.n 208 117 8 28.44 43.67 28.44 41.05 26.55 34.21 20.90
Table 1: F-measures
where each contains around 4200 training and test in-
stances combined. Mixed word are unique because they
combined the instances of multiple target words and
thereby have a larger number of senses to discriminate.
Each type of data brings with it unique characteristics,
and sheds light on different aspects of our experiments.
9.1 Senseval-2 data
Table 2 compares PB1 against PB3, and SC1 against
SC3, when these methods are used to discriminate the 24
SENSEVAL-2 words. Our objective is to study the effect
of using bigram features against co?occurrences in first
(PB) and second (SC) order context vectors while using
relatively small amounts of training data per word. Note
that PB1 and SC1 use co?occurrence features, while PB3
and SC3 rely on bigram features.
This table shows the number of nouns (N), adjec-
tives (A) and verbs (V) where bigrams were more effec-
tive than co-occurrences (bigram>co-occur), less effec-
tive (bigram<co-occur), and had no effect (bigram=co-
occur).
Table 2 shows that there is no clear advantage to us-
ing either bigrams or co?occurrence features in first or-
der context vectors (PB). However, bigram features show
clear improvement in the results of second order context
vectors (SC).
Our hypothesis is that first order context vectors (PB)
represent a small set of bigram features since they are
selected from the relatively small SENSEVAL-2 words.
These features are very sparse, and as such most instances
do not share many common features with other instances,
making first order clustering difficult.
N A V
7 1 2 bigram>co-occur
PB 6 4 2 bigram<co-occur
1 1 0 bigram=co-occur
9 3 3 bigram>co-occur
SC 4 1 1 bigram<co-occur
1 2 0 bigram=co-occur
Table 2: Bigrams vs. Co-occurrences
N A V
PB 9 4 1 rbr>upgma
4 0 2 rbr<upgma
1 2 1 rbr=upgma
SC 8 1 3 rbr>upgma
2 5 0 rbr<upgma
4 0 1 rbr=upgma
Table 3: Repeated Bisections vs. UPGMA
However, second order context vectors indirectly rep-
resent bigram features, and do not require an exact match
between vectors in order to establish similarity. Thus,
the poor performance of bigrams in the case of first or-
der context vectors suggests that when dealing with small
amounts of data, we need to boost or enrich our bigram
feature set by using some other larger training source like
a corpus drawn from the Web.
Table 3 shows the results of using the Repeated Bi-
sections algorithm in vector space (PB) against that of
using UPGMA method in similarity space. This ta-
ble shows the number of Nouns, Adjectives and Verbs
SENSEVAL-2 words that performed better (rbr>upgma),
worse (rbr<upgma), and equal (rbr=upgma) when using
Repeated Bisections clustering versus the UPGMA tech-
nique, on first (PB) and second (SC) order vectors.
In short, Table 3 compares PB1 against PB2 and SC1
against SC2. From this, we observe that with both first
order and second order context vectors Repeated Bisec-
tions is more effective than UPGMA. This suggests that it
is better suited to deal with very small amounts of sparse
data.
Table 4 summarizes the overall performance of each of
these experiments compared with the majority class. This
table shows the number of words for which an experi-
ment performed better than the the majority class, broken
down by part of speech. Note that SC3 and SC1 are most
often better than the majority class, followed closely by
PB2 and SC2. This suggests that the second order con-
text vectors (SC) have an advantage over the first order
vectors for small training data as is found among the 24
SENSEVAL-2 words.
We believe that second order methods work better on
N A V TOTAL
SC3 > MAJ 8 3 1 12
SC1 > MAJ 6 2 2 10
PB2 > MAJ 7 2 0 9
SC2 > MAJ 6 1 2 9
PB1 > MAJ 4 1 1 6
PB3 > MAJ 3 0 2 5
Table 4: All vs. Majority Class
smaller amounts of data, in that the feature spaces are
quite small, and are not able to support the degree of ex-
act matching of features between instances that first order
vectors require. Second order context vectors succeed in
such cases because they find indirect second order co?
occurrences of feature words and hence describe the con-
text more extensively than the first order representations.
With smaller quantities of data, there is less possibil-
ity of finding instances that use exactly the same set of
words. Semantically related instances use words that are
conceptually the same but perhaps not lexically. Sec-
ond order context vectors are designed to identify such
relationships, in that exact matching is not required, but
rather words that occur in similar contexts will have sim-
ilar vectors.
9.2 Line, Hard and Serve data
The comparatively good performance of PB1 and PB3 in
the case of the Line, Hard and Serve data (see Table 1)
suggests that first order context vectors when clustered
with UPGMA perform relatively well on larger samples
of data.
Moreover, among the SC experiments on this data, the
performance of SC2 is relatively high. This further sug-
gests that UPGMA performs much better than Repeated
Bisections with larger amounts of training data.
These observations correspond with the hypothesis
drawn from the SENSEVAL-2 results. That is, a large
amount of training data will lead to a larger feature space
and hence there is a greater chance of matching more fea-
tures directly in the context of the test instances. Hence,
the first order context vectors that rely on the immedi-
ate context of the target word succeed as the contexts are
more likely to use similar sets of words that in turn are
selected from a large feature collection.
9.3 Mix-Word Results
Nearly all of the experiments carried out with the 6 dif-
ferent methods perform better than the majority sense in
the case of the mix-words. This is partially due to the fact
that these words have a large number of senses, and there-
fore have low majority classifiers. In addition, recall that
this data is created by mixing instances of distinct target
words, which leads to a subset of coarse grained (distinct)
senses within the data that are easier to discover than the
senses of a single word.
Table 1 shows that the top 3 experiments for each of
the mixed-words are all second order vectors (SC). We
believe that this is due to the sparsity of the feature spaces
of this data. Since there are so many different senses, the
number of first order features that would be required to
correctly discriminate them is very high, leading to better
results for second order vectors.
10 Future Directions
We plan to conduct experiments that compare the ef-
fect of using very large amounts of training data versus
smaller amounts where each instance includes the tar-
get word (as is the case in this paper). We will draw
our large corpora from a variety of sources, including
the British National Corpus, the English GigaWord Cor-
pus, and the Web. Our motivation is that the larger cor-
pora will provide more generic co?occurrence informa-
tion about words without regard to a particular target
word. However, the data specific to a given target word
will capture the word usages in the immediate context of
the target word. Thus, we will test the hypothesis that
a smaller sample of data where each instance includes
the target word is more effective for sense discrimination
than a more general corpus of training data.
We are also planning to automatically attach descrip-
tive labels to the discovered clusters that capture the un-
derlying word sense. These labels will be created from
the most characteristic features used by the instances be-
longing to the same cluster. By comparing such descrip-
tive features of each cluster with the words that occur in
actual dictionary definitions of the target word, we plan
to carry out fully automated word sense disambiguation
that does not rely on any manually annotated text.
11 Conclusions
We present an extensive comparative analysis of word
sense discrimination techniques using first order and sec-
ond order context vectors, where both can be employed in
similarity and vector space. We conclude that for larger
amounts of homogeneous data such as the Line, Hard and
Serve data, the first order context vector representation
and the UPGMA clustering algorithm are the most effec-
tive at word sense discrimination. We believe this is the
case because in a large sample of data, it is very likely that
the features that occur in the training data will also occur
in the test data, making it possible to represent test in-
stances with fairly rich feature sets. When given smaller
amounts of data like SENSEVAL-2, second order context
vectors and a hybrid clustering method like Repeated Bi-
sections perform better. This occurs because in small and
sparse data, direct first order features are seldom observed
in both the training and the test data. However, the in-
direct second order co?occurrence relationships that are
captured by these methods provide sufficient information
for discrimination to proceed.
12 Acknowledgments
This research is supported by a National Science Foun-
dation Faculty Early CAREER Development Award
(#0092784).
All of the experiments in this paper were carried out
with version 0.47 of the SenseClusters package, freely
available from the URL shown on the title page.
References
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41:391?407.
F. Fukumoto and Y. Suzuki. 1999. Word sense disam-
biguation in untagged text based on term weight learn-
ing. In Proceedings of the Ninth Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, pages 209?216, Bergen.
T.K. Landauer, P.W. Foltz, and D. Laham. 1998. An in-
troduction to latent semantic analysis. Discourse Pro-
cesses, 25:259?284.
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
T. Pedersen and R. Bruce. 1997. Distinguishing word
senses in untagged text. In Proceedings of the Sec-
ond Conference on Empirical Methods in Natural Lan-
guage Processing, pages 197?207, Providence, RI,
August.
T. Pedersen and R. Bruce. 1998. Knowledge lean word
sense disambiguation. In Proceedings of the Fifteenth
National Conference on Artificial Intelligence, pages
800?805, Madison, WI, July.
A. Purandare. 2003. Discriminating among word senses
using McQuitty?s similarity analysis. In Proceedings
of the HLT-NAACL 2003 Student Research Workshop,
pages 19?24, Edmonton, Alberta, Canada, May.
H. Schu?tze. 1992. Dimensions of meaning. In Pro-
ceedings of Supercomputing ?92, pages 787?796, Min-
neapolis, MN.
H. Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?123.
Y. Zhao and G. Karypis. 2002. Evaluation of hierar-
chical clustering algorithms for document datasets. In
Proceedings of the 11th Conference of Information and
Knowledge Management (CIKM), pages 515?524.
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 65?74,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Word Alignment for Languages with Scarce Resources
Joel Martin
National Research Council
Ottawa, ON, K1A 0R6
Joel.Martin@cnrc-nrc.gc.ca
Rada Mihalcea
University of North Texas
Denton, TX 76203
rada@cs.unt.edu
Ted Pedersen
University of Minnesota
Duluth, MN 55812
tpederse@umn.edu
Abstract
This paper presents the task definition,
resources, participating systems, and
comparative results for the shared task
on word alignment, which was organized
as part of the ACL 2005 Workshop on
Building and Using Parallel Texts. The
shared task included English?Inuktitut,
Romanian?English, and English?Hindi
sub-tasks, and drew the participation of ten
teams from around the world with a total of
50 systems.
1 Defining a Word Alignment Shared Task
The task of word alignment consists of finding cor-
respondences between words and phrases in parallel
texts. Assuming a sentence aligned bilingual corpus
in languages L1 and L2, the task of a word alignment
system is to indicate which word token in the corpus
of language L1 corresponds to which word token in
the corpus of language L2.
This year?s shared task follows on the success of
the previous word alignment evaluation that was or-
ganized during the HLT/NAACL 2003 workshop on
?Building and Using Parallel Texts: Data Driven Ma-
chine Translation and Beyond? (Mihalcea and Ped-
ersen, 2003). However, the current edition is dis-
tinct in that it has a focus on languages with scarce
resources. Participating teams were provided with
training and test data for three language pairs, ac-
counting for different levels of data scarceness: (1)
English?Inuktitut (2 million words training data),
(2) Romanian?English (1 million words), and (3)
English?Hindi (60,000 words).
Similar to the previous word alignment evaluation
and with the Machine Translation evaluation exercises
organized by NIST, two different subtasks were de-
fined: (1) Limited resources, where systems were al-
lowed to use only the resources provided. (2) Un-
limited resources, where systems were allowed to use
any resources in addition to those provided. Such re-
sources had to be explicitly mentioned in the system
description.
Test data were released one week prior to the dead-
line for result submissions. Participating teams were
asked to produce word alignments, following a com-
mon format as specified below, and submit their out-
put by a certain deadline. Results were returned to
each team within three days of submission.
1.1 Word Alignment Output Format
The word alignment result files had to include one line
for each word-to-word alignment. Additionally, they
had to follow the format specified in Figure 1. Note
that the
  
and confidence fields overlap in their
meaning. The intent of having both fields available
was to enable participating teams to draw their own
line on what they considered to be a Sure or Probable
alignment. Both these fields were optional, with some
standard values assigned by default.
1.1.1 A Running Word Alignment Example
Consider the following two aligned sentences:
[English]  s snum=18  They had gone .  /s 
[French]  s snum=18  Ils e?taient alle?s .  /s 
A correct word alignment for this sentence is:
18 1 1
18 2 2
18 3 3
18 4 4
65
sentence no position L1 position L2 [    ] [confidence]
where:
sentence no represents the id of the sentence within the
test file. Sentences in the test data already have an id as-
signed. (see the examples below)
position L1 represents the position of the token that is
aligned from the text in language L1; the first token in each
sentence is token 1. (not 0)
position L2 represents the position of the token that is
aligned from the text in language L2; again, the first token
is token 1.
S P can be either S or P, representing a Sure or Probable
alignment. All alignments that are tagged as S are also con-
sidered to be part of the P alignments set (that is, all align-
ments that are considered ?Sure? alignments are also part of
the ?Probable? alignments set). If the    field is missing, a
value of S will be assumed by default.
confidence is a real number, in the range (0-1] (1 meaning
highly confident, 0 meaning not confident); this field is op-
tional, and by default confidence number of 1 was assumed.
Figure 1: Word Alignment file format
stating that: all the word alignments pertain to sen-
tence 18, the English token 1 They aligns with the
French token 1 Ils, the English token 2 had aligns with
the French token 2 e?taient, and so on. Note that punc-
tuation is also aligned (English token 4 aligned with
French token 4), and counts toward the final evalua-
tion figures.
Alternatively, systems could also provide an
  
marker and/or a confidence score, as shown in the fol-
lowing example:
18 1 1 1
18 2 2 P 0.7
18 3 3 S
18 4 4 S 1
with missing
   
fields considered by default S, and
missing confidence scores considered by default 1.
1.2 Annotation Guide for Word Alignments
The word alignment annotation guidelines are similar
to those used in the 2003 evaluation.
1. All items separated by a white space are consid-
ered to be a word (or token), and therefore have
to be aligned (punctuation included).
2. Omissions in translation use the NULL token,
i.e. token with id 0.
3. Phrasal correspondences produce multiple word-
to-word alignments.
2 Resources
The shared task included three different language
pairs, accounting for different language and data
characteristics. Specifically, the three subtasks ad-
dressed the alignment of words in English?Inuktitut,
Romanian?English, and English?Hindi parallel texts.
For each language pair, training data were provided to
participants. Systems relying only on these resources
were considered part of the Limited Resources sub-
task. Systems making use of any additional resources
(e.g. bilingual dictionaries, additional parallel cor-
pora, and others) were classified under the Unlimited
Resources category.
2.1 Training Data
Three sets of training data were made available. All
data sets were sentence-aligned, and pre-processed
(i.e. tokenized and lower-cased), with identical pre-
processing procedures used for training, trial, and test
data.
English?Inuktitut. A collection of sentence-
aligned English?Inuktitut parallel texts from the
Legislative Assembly of Nunavut (Martin et al,
2003). This collection consists of approximately
2 million Inuktitut tokens (1.6 million words) and
4 million English tokens (3.4 million words). The
Inuktitut data was originally encoded in Unicode
representing a syllabics orthography (qaniujaaqpait),
but was transliterated to an ASCII encoding of the
standardized roman orthography (qaliujaaqpait) for
this evaluation.
Romanian?English. A set of Romanian?English
parallel texts, consisting of about 1 million Romanian
words, and about the same number of English words.
This is the same training data set as used in the 2003
word alignment evaluation (Mihalcea and Pedersen,
2003). The data consists of:
 Parallel texts collected from the Web using a
semi-supervised approach. The URLs format
for pages containing potential parallel transla-
tions were manually identified (mainly from the
archives of Romanian newspapers). Next, texts
were automatically downloaded and sentence
aligned. A manual verification of the alignment
was also performed. These data collection pro-
cess resulted in a corpus of about 850,000 Roma-
nian words, and about 900,000 English words.
66
 Orwell?s 1984, aligned within the MULTEXT-
EAST project (Erjavec et al, 1997), with about
130,000 Romanian words, and a similar number
of English words.
 The Romanian Constitution, for about 13,000
Romanian words and 13,000 English words.
English?Hindi. A collection of sentence aligned
English?Hindi parallel texts, from the Emille project
(Baker et al, 2004), consisting of approximately En-
glish 60,000 words and about 70,000 Hindi words.
The Hindi data was encoded in Unicode Devangari
script, and used the UTF?8 encoding. The English?
Hindi data were provided by Niraj Aswani and Robert
Gaizauskas from University of Sheffield (Aswani and
Gaizauskas, 2005b).
2.2 Trial Data
Three sets of trial data were made available at the
same time training data became available. Trial sets
consisted of sentence aligned texts, provided together
with manually determined word alignments. The
main purpose of these data was to enable participants
to better understand the format required for the word
alignment result files. For some systems, the trial data
has also played the role of a validation data set used
for system parameter tuning. Trial sets consisted of
25 English?Inuktitut and English?Hindi aligned sen-
tences, and a larger set of 248 Romanian?English
aligned sentences (the same as the test data used in
the 2003 word alignment evaluation).
2.3 Test Data
A total of 75 English?Inuktitut, 90 English?Hindi,
and 200 Romanian?English aligned sentences were
released one week prior to the deadline. Participants
were required to run their word alignment systems on
one or more of these data sets, and submit word align-
ments. Teams were allowed to submit an unlimited
number of results sets for each language pair.
2.3.1 Gold Standard Word Aligned Data
The gold standard for the three language pair align-
ments were produced using slightly different align-
ment procedures.
For English?Inuktitut, annotators were instructed to
align Inuktitut words or phrases with English phrases.
Their goal was to identify the smallest phrases that
permit one-to-one alignments between English and
Inuktitut. These phrase alignments were converted
into word-to-word alignments in the following man-
ner. If the aligned English and Inuktitut phrases
each consisted of a single word, that word pair was
assigned a Sure alignment. Otherwise, all possi-
ble word-pairs for the aligned English and Inuktitut
phrases were assigned a Probable alignment. Dis-
agreements between the two annotators were decided
by discussion.
For Romanian?English and English?Hindi, anno-
tators were instructed to assign an alignment to all
words, with specific instructions as to when to as-
sign a NULL alignment. Annotators were not asked
to assign a Sure or Probable label. Instead, we had an
arbitration phase, where a third annotator judged the
cases where the first two annotators disagreed. Since
an inter-annotator agreement was reached for all word
alignments, the final resulting alignments were con-
sidered to be Sure alignments.
3 Evaluation Measures
Evaluations were performed with respect to four dif-
ferent measures. Three of them ? precision, recall,
and F-measure ? represent traditional measures in In-
formation Retrieval, and were also frequently used
in previous word alignment literature. The fourth
measure was originally introduced by (Och and Ney,
2000), and proposes the notion of quality of word
alignment.
Given an alignment   , and a gold standard align-
ment  , each such alignment set eventually consist-
ing of two sets   ,   , and  ,  corresponding
to Sure and Probable alignments, the following mea-
sures are defined (where  is the alignment type, and
can be set to either S or P).
	

 


 

 (1)




 




 (2)



 
	





 (3)



ffImproving Name Discrimination: A Language Salad Approach
Ted Pedersen and Anagha Kulkarni
Department of Computer Science
University of Minnesota, Duluth
Duluth, MN 55812 USA
{tpederse,kulka020}@d.umn.edu
Roxana Angheluta
Attentio SA
B-1030 Brussels, Belgium
roxana@attentio.com
Zornitsa Kozareva
Dept. de Lenguajes y Sistemas Informa?ticos
University of Alicante
03080 Alicante, Spain
zkozareva@dlsi.ua.es
Thamar Solorio
Department of Computer Science
University of Texas at El Paso
El Paso, TX 79902 USA
tsolorio@utep.edu
Abstract
This paper describes a method of discrim-
inating ambiguous names that relies upon
features found in corpora of a more abun-
dant language. In particular, we discrim-
inate ambiguous names in Bulgarian, Ro-
manian, and Spanish corpora using infor-
mation derived from much larger quan-
tities of English data. We also mix to-
gether occurrences of the ambiguous name
found in English with the occurrences of
the name in the language in which we are
trying to discriminate. We refer to this as
a language salad, and find that it often re-
sults in even better performance than when
only using English or the language itself
as the source of information for discrimi-
nation.
1 Introduction
Name ambiguity is a problem that is increasing
in complexity and scope as online information
sources grow and expand their coverage. Like
words, names are often ambiguous and can refer
to multiple underlying entities or concepts. Web
searches for names can often return results asso-
ciated with multiple people or organizations in a
disorganized and unclear fashion. For example,
the top 10 results of a Google search for George
Miller includes a mixture of entries for two dif-
ferent entities, one a psychology professor from
Princeton University and the other the director of
the film Mad Max.1
Name discrimination takes some number of
contexts that include an ambiguous name, and di-
vides them into groups or clusters, where the con-
1Search conducted January 4, 2006.
texts in each cluster should ideally refer to the
same underlying entity (and each cluster should
refer to a different entity). Thus, if we are given
10,000 contexts that include the name John Smith,
we would want to divide those contexts into clus-
ters corresponding to each of the different under-
lying entities that share that name.
We have developed an unsupervised method of
name discrimination (Pedersen et al, 2005). We
have shown the method to be language indepen-
dent (Pedersen et al, 2006), which is to say we
can apply it to English contexts as easily as we
can apply it to Romanian or French. However,
we have observed that there are situations where
the number of contexts in which an ambiguous
name occurs is relatively small, perhaps because
the name itself is unusual, or because the quantity
of data available for language is limited in general.
These problems of scarcity can make it difficult to
apply these methods and discriminate ambiguous
names, especially in languages with fewer online
resources.
This paper presents a method of name discrim-
ination is based on using a larger number of con-
texts in English that include an ambiguous name,
and applying information derived from these con-
texts to the discrimination of that name in another
language, where there are many fewer contexts.
We also show that mixing English contexts with
the contexts to be discriminated can result in a
performance improvement over only using the En-
glish or the original contexts alone.
2 Discrimination by Clustering Contexts
Our method of name discrimination is described in
more detail in (Pedersen et al, 2005), but in gen-
eral is based on an unsupervised approach to word
sense discrimination introduced by (Purandare and
25
Pedersen, 2004), which builds upon earlier work
in word sense discrimination, including (Schu?tze,
1998) and (Pedersen and Bruce, 1997).
Our method treats each occurrence of an am-
biguous name as a context that is to be clustered
with other contexts that also include the same
name. In this paper, each context consists of about
50 words, where the ambiguous name is generally
in the middle of the context. The goal is to cluster
similar contexts together, based on the presump-
tion that the occurrences of a name that appear
in similar contexts will refer to the same underly-
ing entity. This approach is motivated by both the
distributional hypothesis (Harris, 1968) and the
strong contextual hypothesis (Miller and Charles,
1991).
2.1 Feature Selection
The contexts to be clustered are represented by
lexical features which may be selected from either
the contexts being clustered, or from a separate
corpus. In this paper we use both approaches. We
cluster the contexts based on features identified in
those very same contexts, and we also cluster the
contexts based on features identified in a separate
set of data (in this case English). We explore the
use of a mixed feature selection strategy where we
identify features both from the data to be clustered
and the separate corpus of English text. Thus, our
feature selection data may come from one of three
sources: the contexts to be clustered (which we
will refer to as the evaluation contexts), English
contexts which include the same name but are not
to be clustered, and the combination of these two
(our so-called Language Salad or Mix).
The lexical features we employ are bigrams,
that is consecutive words that occur together in the
corpora from which we are identifying features. In
this work we identify bigram features using Point-
wise Mutual Information (PMI). This is defined as
the log of the ratio of the observed frequency with
which the two words occur together in the feature
selection data, to the expected number of times
the two words would occur together in a corpus if
they were independent. This expected value is es-
timated simply by taking the product of the num-
ber of times the two words occur individually, and
dividing this by the total number of bigrams in the
feature selection data. Thus, larger values of PMI
indicate that the observed frequency of the bigram
is greater than would be expected if the two words
were independent.
In these experiments we take the top 500 ranked
bigrams that occur five or more times in the feature
selection data. We also exclude any bigram from
consideration that is made up of one or two stop
words, which are high frequency function words
that have been specified in a manually created list.
Note that with smaller numbers of contexts (usu-
ally 200 or fewer), we lower the frequency thresh-
old to two or more.
In general PMI is known to have a bias towards
pairs of words (bigrams) that occur a small num-
ber of times and only with each other. In this work
that is a desirable quality, since that will tend to
identify pairs of words that are very strongly as-
sociated with each other and also provide unique
discriminating information.
2.2 Context Representation
Once the bigram features have been identified,
then the contexts to be clustered are represented
using second order co-occurrences that are de-
rived from those bigrams. In general a second
order co-occurrence is a pair of words that may
not occur with each other, but that both occur fre-
quently with a third word. For example, garden
and fire may not occur together often, but both
commonly occur with hose. Thus, garden hose
and fire hose represent first order co?occurrences,
and garden and fire represent a second order co?
occurrence.
The process of creating the second order repre-
sentation has several steps. First, the bigram fea-
tures identified by PMI (the top ranked 500 bi-
grams that have occurred 5 or more times in the
feature selection data) are used to create a word
by word co?occurrence matrix. The first word in
each bigram represents a row in the matrix, and the
second word in each bigram represents a column.
The cells in the matrix contain the PMI scores.
Note that this matrix is not symmetric, and that
there are many words that only occur in either a
row or a column (and not both) because they tend
to occur as the first or second word in a bigram.
For example, President might tend to be a first
word in a bigram (e.g., President Clinton, Presi-
dent Putin), whereas last names will tend to be the
second word.
Once the co?occurrence matrix is created, then
the contexts to be clustered can be represented.
Each word in the context is checked to see if it
26
has a corresponding row (i.e., vector) in the co?
occurrence matrix. If it does, that word is replaced
in the context by the row from the matrix, so that
the word in the context is now represented by the
vector of words with which it occurred in the fea-
ture selection data. If a word does not have a corre-
sponding entry in the co?occurrence matrix, then
it is simply removed from the context. After all
the words in the context are checked, then all of
the vectors that are selected are averaged together
to create a vector representation of the context.
Then these contexts are clustered into a pre?
specified number of clusters using the k?means
algorithm. Note that we are currently develop-
ing methods to automatically select the number of
clusters in the data (e.g., (Pedersen and Kulkarni,
2006)), although we have not yet applied them to
this particular work.
3 The Language Salad
In this paper, we explore the creation of a second
order representation for a set of evaluation con-
texts using three different sets of feature selection
data. The co?occurrence matrix may be derived
from the evaluation contexts themselves, or from
a separate set of contexts in a different language,
or from the combination of these two (the Salad or
Mix).
For example, suppose we have 100 Romanian
evaluation contexts that include an ambiguous
name, and that same name also occurs 10,000
times in an English language corpus.2 Our goal
is to cluster the 100 Romanian contexts, which
contain all the information that we have about the
name in Romanian. While we could derive a sec-
ond order representation of the contexts, the re-
sulting co?occurrence matrix would likely be very
small and sparse, and insufficient for making good
discrimination decisions. We could instead rely
on first order features, that is look for frequent
words or bigrams that occur in the evaluation con-
texts, and try and find evaluation contexts that
share some of the same words or phrases, and clus-
ter them based on this type of information. How-
ever, again, the small number of contexts available
would likely result in very sparse representations
for the contexts, and unreliable clustering results.
Thus, our method is to derive a co?occurrence
matrix from a language for which we have many
2We assume that the names either have the same spelling
in both languages, or that translations are readily available.
occurrences of the ambiguous name, and then use
that co?occurrence matrix to represent the evalua-
tion contexts. This relies on the fact that the eval-
uation contexts will contain at least a few names
or words that are also used in the larger corpus (in
this case English). In general, we have found that
while this is not always true, it is often the case.
We have also experimented with combining the
English contexts with the evaluation contexts, and
building a co?occurrence matrix based on this
combined or mixed collection of contexts. This
is the language salad that we refer to, a mixture of
contexts in two different languages that are used to
derive a representation of the evaluation contexts.
4 Experimental Data
We use data in four languages in these experi-
ments, Bulgarian, English, Romanian, and Span-
ish.
4.1 Raw Corpora
The Romanian data comes from the 2004 archives
of the newspaper Adevarul (The Truth)3. This is a
daily newspaper that is among the most popular in
Romania. While Romanian normally has diacrit-
ical markings, this particular newspaper does not
include those in their online edition, so the alpha-
bet used was the same as English.
The Bulgarian data is from the Sega 2002 news
corpus, which was originally prepared for the
CLEF competition.4 This is a corpus of news arti-
cles from the Newspaper Sega5, which is based in
Sofia, Bulgaria. The Bulgarian text was translit-
erated (phonetically) from Cyrillic to the Roman
alphabet. Thus, the alphabet used was the same
as English, although the phonetic transliteration
leads to fewer cognates and borrowed English
words that are spelled exactly the same as in En-
glish text.
The Spanish corpora comes from the Spanish
news agency EFE from the year 1994 and 1995.
This collection was used in the Question Answer-
ing Track at CLEF-2003, and also for CLEF-2005.
This text is represented in Latin-1, and includes
the usual accents that appear in Spanish.
The English data comes from the GigaWord
corpus (2nd edition) that is distributed by the Lin-
guistic Data Consortium. This consists of more
3http://www.adevarulonline.ro/arhiva
4http://www.clef-campaign.org
5http://www.segabg.com
27
than 2 billion words of newspaper text that comes
from five different news sources between the years
1994 and 2004. In fact, we subdivide the English
data into three different corpora, where one is from
2004, another from 2002, and the third from 1994-
95, so that for each of the evaluation languages
(Bulgarian, Spanish, and Romanian) we have an
English corpus from the same time period.
4.2 Evaluation Contexts
Our experimental data consists of evaluation con-
texts derived from the Bulgarian, Romanian, and
Spanish corpora mentioned above. We also have
English corpora that includes the same ambiguous
names as found in the evaluation contexts.
In order to quickly generate a large volume of
experimental data, we created evaluation contexts
from the corpora for each of our four languages
by conflating together pairs of well known names
or places, and that are generally not highly am-
biguous (although some might be rather general).
For example, one of the pairs of names we con-
flate is George Bush and Tony Blair. To do that,
every occurrence of both of these names is con-
verted to an ambiguous form (GB TB, for exam-
ple), and the discrimination task is to cluster these
contexts such that their original and correct name
is re?discovered. We retain a record of the orig-
inal name for each occurrence, so as to evaluate
the results of our method. Of course we do not use
this information anywhere in the process outside
of evaluation.
The following pairs of names were conflated in
all four of the languages: George Bush-Tony Blair,
Mexico-India, USA-Paris, Ronaldo-David Beck-
ham (2002 and 2004), Diego Maradona-Roberto
Baggio (1994-95 only), and NATO-USA. Note
that some of these names have different spellings
in some of our languages, so we look for and con-
flate the native spelling of the names in the differ-
ent language corpora. These pairs were selected
because they occur in all four of our languages,
and they represent name distinctions that are com-
monly of interest, that is they represent ambiguity
in names of people and places. With these pairs
we are also following (Nakov and Hearst, 2003)
who suggest that if one is introducing ambiguity
by creating pseudo?words or conflating names,
then these words should be related in some way
(in order to avoid the creation of very sharp or ob-
vious sense distinctions).
4.3 Discussion
For each of the three evaluation languages (Bul-
garian, Romanian, and Spanish) we have contexts
for five different name conflate pairs that we wish
to discriminate. We have corresponding English
contexts for each evaluation language, where the
dates of both are approximately the same. This
temporal consistency between the evaluation lan-
guage and English is important because the con-
texts in which a name is used may change over
time. In 1994, for example, Tony Blair was not
yet Prime Minister of England (he became PM in
1997), and references to George Bush most likely
refer to the US President who served from 1988
until 1992, rather than the current US President
(who began his term in office in 2001). In 1994
the current (as of 2006) US President had just been
elected governor of Texas, and was not yet a na-
tional figure. This points out that George Bush is
an example of an ambiguous name, but our ob-
servation has been that in the 2002 and 2004 data
(Romanian and Bulgarian) nearly all occurrences
are associated with the current president, and that
most of the occurrences in 1994-95 (Spanish) re-
fer to the former US President. This illustrates
an important point: it is necessary to consider the
perspective represented by the different corpora.
There is little reason to expect that news articles
from Spain in 1994 and 1995 would focus much
attention on the newly elected governor of Texas
in the United States.
Tables 1, 2, and 3 show the number of contexts
that have been collected for each name conflate
pair. For example, in Table 1 we see that there are
746 Bulgarian contexts that refer to either Mex-
ico or India, and that of these 51.47% truly re-
fer to Mexico, and 48.53% to India. There are
149,432 English contexts that mention Mexico or
India, and the Mix value shown is simply the sum
of the number of Bulgarian and English contexts.
In general these tables show that the English
contexts are much larger in number, however,
there are a few exceptions with the Spanish data.
This is because the EFE corpus is relatively large
as compared to the Bulgarian and Romanian cor-
pora, and provides frequency counts that are in
some cases comparable to those in the English cor-
pus.
28
5 Experimental Methodology
For each of the three evaluation languages (Bul-
garian, Romanian, Spanish) there are five name
conflate pairs. The same name conflate pairs
are used for all three languages, except for
Diego Maradona-Roberto Baggio which is only
used with Spanish, and Ronaldo-David Beckham,
which is only used with Bulgarian and Romanian.
This is due to the fact that in 1994-95 (the era
of the Spanish data) neither Ronaldo nor David
Beckham were as famous as they later became, so
they were mentioned somewhat less often than in
the 2002 and 2004 corpora. The other four name
conflate pairs are used in all of the languages.
For each name conflate pair we create a second
order representation using three different sources
of features selection data: the evaluation contexts
themselves, the corresponding English contexts,
and then the mix of the evaluation contexts and the
English contexts (the Mix). The objective of these
experiments is to determine which of these sources
of feature selection data results in the highest F-
Measure, which is the harmonic mean of the pre-
cision and recall of an experiment.
The precision of each experiment is the num-
ber of evaluation contexts clustered correctly, di-
vided by the number of contexts that are clustered.
The clustering algorithm may choose not to assign
every context to a cluster, which is why that de-
nominator may not be the same as the number of
evaluation contexts. The recall of each experiment
is the the number of correctly clustered evaluation
contexts divided by the total number of evaluation
contexts. Note that for each of the three variations
for each name conflate pair experiment exactly the
same evaluation language contexts are being dis-
criminated, all that is changing in each experiment
is the source of the feature selection data. Thus the
F-measures for a name conflate pair in a particular
language can be compared directly. Note however
that the F-measures across languages are harder to
compare directly, since different evaluation con-
texts are used, and different English contexts are
used as well.
There is a simple baseline that can be used as a
point of comparison, and that is to place all of the
contexts for each name conflate pair into one clus-
ter, and say that there is no ambiguity. If that is
done, then the resulting F-Measure will be equal
to the majority percentage of the true underlying
entity as shown in Tables 1, 2, and 3. For exam-
ple, for Bulgarian, if the 746 Bulgarian contexts
for Mexico and India are all put into the same clus-
ter, the resulting F-Measure would be 51.47%, be-
cause we would simply assign all the contexts in
the cluster to the more common of the two entities,
which is Mexico in this case.
6 Experimental Results
Tables 1, 2, and 3 show the results for our exper-
iments, language by language. Each table shows
the results for the 15 experiments done for each
language: five name conflate pairs, each with
three different sources of feature selection data.
The row labeled with the name of the evalua-
tion language reports the F-Measure for the eval-
uation contexts (whose number of occurrences is
shown in the far right column) when the fea-
ture selection data is the evaluation contexts them-
selves. The rows labeled English and Mix report
the F-Measures obtained for the evaluation con-
texts when the feature selection data is the English
contexts, or the Mix of the English and evaluation
contexts.
6.1 Bulgarian Results
The Bulgarian results are shown in Table 1. Note
that the number of contexts for English is consid-
erably larger than for Bulgarian for all five name
conflate pairs. The Bulgarian and English data
came from 2002 news reports.
The Mix of feature selection data results in the
best performance for three of the five name con-
flate pairs: George Bush - Tony Blair, Ronaldo -
David Beckham, and NATO - USA. For remain-
ing two name conflate pairs, just using the Bul-
garian evaluation contexts results in the highest F-
Measure (Mexico-India, USA-Paris).
We believe that this may be partially due to the
fact that the two cases where Bulgarian leads to the
best results are for very general or generic underly-
ing entities: Mexico and India, and then the USA
and Paris. In both cases, contexts that mention
these entities could be discussing a wide range of
topics, and the larger volumes of English data may
simply overwhelm the process with a huge num-
ber of second order features. In addition, it may
be that the English and Bulgarian corpora contain
different content that reflects the different interests
of the original readership of this text. For example,
news that is reported about India might be rather
different in the United States (the source of most
29
Table 1: Bulgarian Results (2002): Feature Selec-
tion Data, F-Measure, and Number of Contexts
George Bush (73.43) - Tony Blair (26.57)
Mix 68.37 11,570
Bulgarian 55.78 651
English 36.15 10,919
Mexico (51.47) - India (48.53)
Bulgarian 70.97 746
Mix 55.01 150,178
English 48.15 149,432
USA (79.53) - Paris (20.47)
Bulgarian 58.67 3,283
Mix 51.68 56,044
English 49.66 52,761
Ronaldo (61.25) - David Beckham (38.75)
Mix 64.88 8,649
Bulgarian 52.75 320
English 48.11 8,329
NATO (87.37) - USA (12.63)
Mix 75.44 54,193
Bulgarian 65.92 3,770
English 60.44 50,423
of the English data) than in Bulgaria. Thus, the
use of the English corpora might not have been
as helpful in those cases where the names to be
discriminated are more global figures. For exam-
ple, Tony Blair and George Bush are probably in
the news in the USA and Bulgaria for many of the
same reasons, thus the underlying content is more
comparable than that of the more general entities
(like Mexico and India) that might have much dif-
ferent content associated with them.
We observed that Bulgarian tends to have fewer
cognates or shared names with English than do
Romanian and English. This is due to the fact
that the Bulgarian text is transliterated. This may
account for the fact that the English-only results
for Bulgarian are very poor, and it is only in com-
bination with the Bulgarian contexts that the En-
glish contexts show any positive effect. This sug-
gests that there are only a few words in the Bulgar-
ian contexts that also occur in English, but those
that do have a positive impact on clustering per-
formance.
6.2 Romanian Results
The Romanian results are shown in Table 2. The
Romanian and English contexts come from 2004.
Table 2: Romanian Results (2004): Feature Selec-
tion Data, F-Measure, and Number of Contexts
Tony Blair (72.00) - George Bush (28.00)
English 64.23 11,616
Mix 54.31 11,816
Romanian 50.75 200
India (53.66) - Mexico (46.34)
Romanian 50.93 82
English 47.30 88,247
Mix 42.55 88,329
USA (60.29) - Paris (39.71)
English 59.05 45,346
Romanian 58.76 700
Mix 57.91 46,046
David Beckham (55.56) - Ronaldo (44.44)
Mix 81.00 4,365
English 70.85 4,203
Romanian 52.47 162
NATO (58.05) - USA (41.95)
Mix 60.48 43,508
Romanian 51.20 1,168
English 38.91 42,340
The Mix of Romanian and English contexts for
feature selection results in improvements for two
of the five pairs (David Beckham - Ronaldo, and
NATO - USA). The use of English contexts only
provides the best results for two other pairs (Tony
Blair - George Bush, and USA - Paris, although in
the latter case the difference in the F-Measures that
result from the three sources of data is minimal).
There is one case (Mexico-India) where using the
Romanian contexts as feature selection data re-
sults in a slightly better F-measure than when us-
ing English contexts.
The improvement that the Mix shows for David
Beckham-Ronaldo is significant, and is perhaps
due to fact that in both English and Romanian text,
the content about Beckham and Ronaldo is simi-
lar, making it more likely that the mix of English
and Romanian contexts will be helpful. However,
it is also true that the Mix results in a significant
improvement for NATO-USA, and it seems likely
that the local perspective in Romania and the USA
would be somewhat different on these two entities.
However, NATO-USA has a relatively large num-
ber of contexts in Romanian as well as English, so
perhaps the difference in perspective had less of
an impact in those cases where the number of Ro-
30
Table 3: Spanish Results (1994-95): Feature Se-
lection Data, F-Measure, and Number of Contexts
George Bush (75.58) - Tony Blair (24.42)
Mix 78.59 2,353
Spanish 64.45 1,163
English 54.29 1,190
D. Maradona (51.55) - R. Baggio (48.45)
English 67.65 1,588
Mix 61.35 3,594
Spanish 60.70 2,006
India (92.34) - Mexico (7.66)
English 72.76 19,540
Spanish 66.57 2,377
Mix 61.54 21,917
USA (62.30) - Paris (37.70)
Spanish 69.31 1,000
English 64.30 17,344
Mix 59.40 18,344
NATO (63.86) - USA (36.14)
Spanish 62.04 2,172
Mix 58.47 27,426
English 56.00 25,254
manian contexts is much smaller (as is the case for
Beckham and Ronaldo).
6.3 Spanish Results
The Spanish results are shown in Table 3. The
Spanish and English contexts come from 1994-
1995, which puts them in a slightly different his-
torical era than the Bulgarian and Romanian cor-
pora.
Due to this temporal difference, we used Diego
Maradona and Roberto Baggio as a conflated pair,
rather than David Beckham and Ronaldo, who
were much younger and somewhat less famous at
that time. Also, Ronaldo is a highly ambiguous
name in Spanish, as it is a very common first name.
This is true in English text as well, although casual
inspection of the English text from 2002 and 2004
(where the Ronaldo-Beckham pair was included
experimentally) reveals that Ronaldo the soccer
player tends to occur more so than any other single
entity named Ronaldo, so while there is a bit more
noise for Ronaldo, there is not really a significant
ambiguity.
For the Spanish results we only note one pair
(George Bush - Tony Blair) where the Mix of En-
glish and Spanish results in the best performance.
This again suggests that the perspective of the
Spanish and English corpora were similar with re-
spect to these entities, and their combination was
helpful. In two other cases (Maradona-Baggio,
India-Mexico) English only contexts achieve the
highest F-Measure, and then in the two remaining
cases (USA-Paris, NATO-USA) the Spanish con-
texts are the best source of features.
Note that for Spanish we have reasonably large
numbers of contexts (as compared to Bulgarian
and Romanian). Given that, it is especially inter-
esting that English-only contexts are the most ef-
fective in two of five cases. This suggests that this
approach may have merit even when the evalua-
tion language does not suffer from problems of ex-
treme scarcity. It may simply be that the informa-
tion in the English corpora provides more discrim-
inating information than does the Spanish, and that
it is somewhat different in content than the Span-
ish, otherwise we would expect the Mix of English
and Spanish contexts to do better than being most
accurate for just one of five cases.
7 Discussion
Of the 15 name conflate experiments (five pairs,
three languages), in only five cases did the use of
the evaluation contexts as a source of feature se-
lection data result in better F-Measure scores than
did either using the English contexts alone or as a
Mix with the evaluation language contexts. Thus,
we conclude that there is a clear benefit to using
feature selection data that comes from a different
language than the one for which discrimination is
being performed.
We believe that this is due to the volume of
the English data, as well as to the nature of the
name discrimination task. For example, a per-
son is often best described or identified by observ-
ing the people he or she tends to associate with,
or the places he or she visits, or the companies
with which he or she does business. If we ob-
serve that George Miller and Mel Gibson occur
together, then it seems we can safely infer that
George Miller the movie director is being referred
to, rather than George Miller the psychologist and
father of WordNet.
This argument might suggest that first order
co?occurrences would be sufficient to discrimi-
nate among the names. That is, simply group the
evaluation contexts based on the features that oc-
cur within them, and essentially cluster evaluation
31
contexts based on the number of features they have
in common with other evaluation contexts. In fact,
results on word sense discrimination (Purandare
and Pedersen, 2004) suggest that first order rep-
resentations are more effective with larger number
of context than second order methods. However,
we see examples in these results that suggests this
may not always be the case. In the Bulgarian re-
sults, the largest number of Bulgarian contexts are
for NATO-USA, but the Mix performs quite a bit
better than Bulgarian only. In the case of Roma-
nian, again NATO-USA has the largest number of
contexts, but the Mix still does better than Roma-
nian only. And in Spanish, Mexico-India has the
largest number of contexts and English-only does
better. Thus, even in cases where we have an abun-
dant number of evaluation contexts, the indirect
nature of the second order representation provides
some added benefit.
We believe that the perspective of the news or-
ganizations providing the corpora certainly has an
impact on the results. For example, in Romanian,
the news about David Beckham and Ronaldo is
probably much the same as in the United States.
These are international figures that are both ex-
ternal to countries where the news originates, and
there is no reason to suppose there would be a
unique local perspective represented by any of the
news sources. The only difference among them
might be in the number of contexts available. In
this situation, the addition of the English contexts
may provide enough additional information to im-
prove discrimination performance in another lan-
guage.
For example, in the 162 Romanian contexts
for Ronaldo-Beckham, there is one occurrence of
Posh, which was the stage name of Beckham?s
wife Victoria. This is below our frequency cut-
off threshold for feature selection, so it would be
discarded when using Romanian?only contexts.
However, in the English contexts Posh is men-
tioned 6 times, and is included as a feature. Thus,
the one occurrence of Posh in the Romanian cor-
pus can be well represented by information found
in the English contexts, thus allowing that Roma-
nian context to be correctly discriminated.
8 Conclusions
This paper shows that a method of name discrim-
ination based on second order context representa-
tions can take advantage of English contexts, and
the mix of English and evaluation contexts, in or-
der to perform more accurate name discrimination.
9 Acknowledgments
This research is supported by a National Sci-
ence Foundation Faculty Early CAREER Devel-
opment Award (#0092784). All of the experiments
in this paper were carried out with version 0.71
SenseClusters package, which is freely available
from http://senseclusters.sourceforge.net.
References
Z. Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.
G.A. Miller and W.G. Charles. 1991. Contextual cor-
relates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1?28.
P. Nakov and M. Hearst. 2003. Category-based pseu-
dowords. In Companion Volume to the Proceedings
of HLT-NAACL 2003 - Short Papers, pages 67?69,
Edmonton, Alberta, Canada, May 27 - June 1.
T. Pedersen and R. Bruce. 1997. Distinguishing word
senses in untagged text. In Proceedings of the Sec-
ond Conference on Empirical Methods in Natural
Language Processing, pages 197?207, Providence,
RI, August.
T. Pedersen and A. Kulkarni. 2006. Selecting the
r?ightn?umber of senses based on clustering criterion
functions. In Proceedings of the Posters and Demo
Program of the Eleventh Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, Trento, Italy, April.
T. Pedersen, A. Purandare, and A. Kulkarni. 2005.
Name discrimination by clustering similar contexts.
In Proceedings of the Sixth International Conference
on Intelligent Text Processing and Computational
Linguistics, pages 220?231, Mexico City, February.
T. Pedersen, A. Kulkarni, R. Angheluta, Z. Kozareva,
and T. Solorio. 2006. An unsupervised language in-
dependent method of name discrimination using sec-
ond order co-occurrence features. In Proceedings
of the Seventh International Conference on Intelli-
gent Text Processing and Computational Linguistics,
pages 208?222, Mexico City, February.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proceedings of the Conference on
Computational Natural Language Learning, pages
41?48, Boston, MA.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
32
Using WordNet-based Context Vectors
to Estimate the Semantic Relatedness of Concepts
Siddharth Patwardhan
School of Computing
University of Utah
Salt Lake City, UT, 84112, USA
sidd@cs.utah.edu
Ted Pedersen
Department of Computer Science
University of Minnesota, Duluth
Duluth, MN, 55812, USA
tpederse@d.umn.edu
Abstract
In this paper, we introduce a WordNet-
based measure of semantic relatedness
by combining the structure and content
of WordNet with co?occurrence informa-
tion derived from raw text. We use the
co?occurrence information along with the
WordNet definitions to build gloss vectors
corresponding to each concept in Word-
Net. Numeric scores of relatedness are as-
signed to a pair of concepts by measuring
the cosine of the angle between their re-
spective gloss vectors. We show that this
measure compares favorably to other mea-
sures with respect to human judgments
of semantic relatedness, and that it per-
forms well when used in a word sense dis-
ambiguation algorithm that relies on se-
mantic relatedness. This measure is flex-
ible in that it can make comparisons be-
tween any two concepts without regard to
their part of speech. In addition, it can
be adapted to different domains, since any
plain text corpus can be used to derive the
co?occurrence information.
1 Introduction
Humans are able to quickly judge the relative se-
mantic relatedness of pairs of concepts. For exam-
ple, most would agree that feather is more related
to bird than it is to tree.
This ability to assess the semantic relatedness
among concepts is important for Natural Lan-
guage Understanding. Consider the following sen-
tence: He swung the bat, hitting the ball into the
stands. A reader likely uses domain knowledge of
sports along with the realization that the baseball
senses of hitting, bat, ball and stands are all se-
mantically related, in order to determine that the
event being described is a baseball game.
Consequently, a number of techniques have
been proposed over the years, that attempt to au-
tomatically compute the semantic relatedness of
concepts to correspond closely with human judg-
ments (Resnik, 1995; Jiang and Conrath, 1997;
Lin, 1998; Leacock and Chodorow, 1998). It has
also been shown that these techniques prove use-
ful for tasks such as word sense disambiguation
(Patwardhan et al, 2003), real-word spelling cor-
rection (Budanitsky and Hirst, 2001) and informa-
tion extraction (Stevenson and Greenwood, 2005),
among others.
In this paper we introduce a WordNet-based
measure of semantic relatedness inspired by Har-
ris? Distributional Hypothesis (Harris, 1985). The
distributional hypothesis suggests that words that
are similar in meaning tend to occur in similar lin-
guistic contexts. Additionally, numerous studies
(Carnine et al, 1984; Miller and Charles, 1991;
McDonald and Ramscar, 2001) have shown that
context plays a vital role in defining the mean-
ings of words. (Landauer and Dumais, 1997) de-
scribe a context vector-based method that simu-
lates learning of word meanings from raw text.
(Schu?tze, 1998) has also shown that vectors built
from the contexts of words are useful representa-
tions of word meanings.
Our Gloss Vector measure of semantic related-
ness is based on second order co?occurrence vec-
tors (Schu?tze, 1998) in combination with the struc-
ture and content of WordNet (Fellbaum, 1998), a
semantic network of concepts. This measure cap-
tures semantic information for concepts from con-
textual information drawn from corpora of text.
We show that this measure compares favorably
1
to other measures with respect to human judg-
ments of semantic relatedness, and that it performs
well when used in a word sense disambiguation al-
gorithm that relies on semantic relatedness. This
measure is flexible in that it can make comparisons
between any two concepts without regard to their
part of speech. In addition, it is adaptable since
any corpora can be used to derive the word vec-
tors.
This paper is organized as follows. We start
with a description of second order context vectors
in general, and then define the Gloss Vector mea-
sure in particular. We present an extensive evalua-
tion of the measure, both with respect to human re-
latedness judgments and also relative to its perfor-
mance when used in a word sense disambiguation
algorithm based on semantic relatedness. The pa-
per concludes with an analysis of our results, and
some discussion of related and future work.
2 Second Order Context Vectors
Context vectors are widely used in Information
Retrieval and Natural Language Processing. Most
often they represent first order co?occurrences,
which are simply words that occur near each other
in a corpus of text. For example, police and car are
likely first order co?occurrences since they com-
monly occur together. A first order context vector
for a given word would simply indicate all the first
order co?occurrences of that word as found in a
corpus.
However, our Gloss Vector measure is based on
second order co?occurrences (Schu?tze, 1998). For
example, if car and mechanic are first order co?
occurrences, then mechanic and police would be
second order co?occurrences since they are both
first order co?occurrences of car.
Schu?tze?s method starts by creating a Word
Space, which is a co?occurrence matrix where
each row can be viewed as a first order context
vector. Each cell in this matrix represents the fre-
quency with which two words occur near one an-
other in a corpus of text. The Word Space is usu-
ally quite large and sparse, since there are many
words in the corpus and most of them don?t occur
near each other. In order to reduce the dimension-
ality and the amount of noise, non?content stop
words such as the, for, a, etc. are excluded from
being rows or columns in the Word Space.
Given a Word Space, a context can then be rep-
resented by second order co?occurrences (context
vector). This is done by finding the resultant of the
first order context vectors corresponding to each
of the words in that context. If a word in a context
does not have a first order context vector created
for it, or if it is a stop word, then it is excluded
from the resultant.
For example, suppose we have the following
context:
The paintings were displayed in the art
gallery.
The second order context vector would be the
resultant of the first order context vectors for
painting, display, art, and gallery. The words
were, in, and the are excluded from the resultant
since we consider them as stop words in this ex-
ample. Figure 1 shows how the second order con-
text vector might be visualized in a 2-dimensional
space.
dim1
dim2
Context
Vector
gallery
display
art
painting
Figure 1: Creating a context vector from word vec-
tors
Intuitively, the orientation of each second order
context vector is an indicator of the domains or
topics (such as biology or baseball) that the con-
text is associated with. Two context vectors that lie
close together indicate a considerable contextual
overlap, which suggests that they are pertaining to
the same meaning of the target word.
3 Gloss Vectors in Semantic Relatedness
In this research, we create a Gloss Vector for each
concept (or word sense) represented in a dictio-
nary. While we use WordNet as our dictionary,
the method can apply to other lexical resources.
3.1 Creating Vectors from WordNet Glosses
A Gloss Vector is a second order context vector
formed by treating the dictionary definition of a
2
concept as a context, and finding the resultant of
the first order context vectors of the words in the
definition.
In particular, we define a Word Space by cre-
ating first order context vectors for every word w
that is not a stop word and that occurs above a min-
imum frequency in our corpus. The specific steps
are as follows:
1. Initialize the first order context vector to a
zero vector
?w.
2. Find every occurrence of w in the given cor-
pus.
3. For each occurrence of w, increment those di-
mensions of ?w that correspond to the words
from the Word Space and are present within
a given number of positions around w in the
corpus.
The first order context vector ?w, therefore, en-
codes the co?occurrence information of word w.
For example, consider the gloss of lamp ? an ar-
tificial source of visible illumination. The Gloss
Vector for lamp would be formed by adding the
first order context vectors of artificial, source, vis-
ible and illumination.
In these experiments, we use WordNet as the
corpus of text for deriving first order context vec-
tors. We take the glosses for all of the concepts
in WordNet and view that as a large corpus of
text. This corpus consists of approximately 1.4
million words, and results in a Word Space of
approximately 20,000 dimensions, once low fre-
quency and stop words are removed. We chose the
WordNet glosses as a corpus because we felt the
glosses were likely to contain content rich terms
that would distinguish between the various con-
cepts more distinctly than would text drawn from
a more generic corpus. However, in our future
work we will experiment with other corpora as the
source of first order context vectors, and other dic-
tionaries as the source of glosses.
The first order context vectors as well as the
Gloss Vectors usually have a very large number
of dimensions (usually tens of thousands) and it is
not easy to visualize this space. Figure 2 attempts
to illustrate these vectors in two dimensions. The
words tennis and food are the dimensions of this 2-
dimensional space. We see that the first order con-
text vector for serve is approximately halfway be-
tween tennis and food, since the word serve could
Normalized
gloss vector
for "fork"
Food
Tennis
Eat
Serve
= Word Vector
= Gloss Vector
Cutlery
Figure 2: First Order Context Vectors and a Gloss
Vector
mean to ?serve the ball? in the context of tennis or
could mean ?to serve food? in another context.
The first order context vectors for eat and cut-
lery are very close to food, since they do not have
a sense that is related to tennis. The gloss for the
word fork, ?cutlery used to serve and eat food?,
contains the words cutlery, serve, eat and food.
The Gloss Vector for fork is formed by adding the
first order context vectors of cutlery, serve, eat and
food. Thus, fork has a Gloss Vector which is heav-
ily weighted towards food. The concept of food,
therefore, is in the same semantic space as and is
related to the concept of fork.
Similarly, we expect that in a high dimensional
space, the Gloss Vector of fork would be heavily
weighted towards all concepts that are semanti-
cally related to the concept of fork. Additionally,
the previous demonstration involved a small gloss
for representing fork. Using augmented glosses,
described in section 3.2, we achieve better repre-
sentations of concepts to build Gloss Vectors upon.
3.2 Augmenting Glosses Using WordNet
Relations
The formulation of the Gloss Vector measure de-
scribed above is independent of the dictionary
used and is independent of the corpus used. How-
ever, dictionary glosses tend to be rather short, and
it is possible that even closely related concepts will
be defined using different sets of words. Our be-
lief is that two synonyms that are used in different
glosses will tend to have similar Word Vectors (be-
cause their co?occurrence behavior should be sim-
ilar). However, the brevity of dictionary glosses
may still make it difficult to create Gloss Vectors
that are truly representative of the concept.
3
(Banerjee and Pedersen, 2003) encounter a sim-
ilar issue when measuring semantic relatedness by
counting the number of matching words between
the glosses of two different concepts. They ex-
pand the glosses of concepts in WordNet with the
glosses of concepts that are directly linked by a
WordNet relation. We adopt the same technique
here, and use the relations in WordNet to augment
glosses for the Gloss Vector measure. We take the
gloss of a given concept, and concatenate to it the
glosses of all the concepts to which it is directly
related according to WordNet. The Gloss Vector
for that concept is then created from this big con-
catenated gloss.
4 Other Measures of Relatedness
Below we briefly describe five alternative mea-
sures of semantic relatedness, and then go on to
include them as points of comparison in our exper-
imental evaluation of the Gloss Vector measure.
All of these measures depend in some way upon
WordNet. Four of them limit their measurements
to nouns located in the WordNet is-a hierarchy.
Each of these measures takes two WordNet con-
cepts (i.e., word senses or synsets) c1 and c2 as in-
put and return a numeric score that quantifies their
degree of relatedness.
(Leacock and Chodorow, 1998) finds the path
length between c1 and c2 in the is-a hierarchy of
WordNet. The path length is then scaled by the
depth of the hierarchy (D) in which they reside to
obtain the relatedness of the two concepts.
(Resnik, 1995) introduced a measure that is
based on information content, which are numeric
quantities that indicate the specificity of concepts.
These values are derived from corpora, and are
used to augment the concepts in WordNet?s is-a hi-
erarchy. The measure of relatedness between two
concepts is the information content of the most
specific concept that both concepts have in com-
mon (i.e., their lowest common subsumer in the
is-a hierarchy).
(Jiang and Conrath, 1997) extends Resnik?s
measure to combine the information contents of
c1, c2 and their lowest common subsumer.
(Lin, 1998) also extends Resnik?s measure, by
taking the ratio of the shared information content
to that of the individual concepts.
(Banerjee and Pedersen, 2003) introduce Ex-
tended Gloss Overlaps, which is a measure that de-
termines the relatedness of concepts proportional
to the extent of overlap of their WordNet glosses.
This simple definition is extended to take advan-
tage of the complex network of relations in Word-
Net, and allows the glosses of concepts to include
the glosses of synsets to which they are directly
related in WordNet.
5 Evaluation
As was done by (Budanitsky and Hirst, 2001), we
evaluated the measures of relatedness in two ways.
First, they were compared against human judg-
ments of relatedness. Second, they were used in an
application that would benefit from the measures.
The effectiveness of the particular application was
an indirect indicator of the accuracy of the related-
ness measure used.
5.1 Comparison with Human Judgment
One obvious metric for evaluating a measure of se-
mantic relatedness is its correspondence with the
human perception of relatedness. Since semantic
relatedness is subjective, and depends on the hu-
man view of the world, comparison with human
judgments is a self-evident metric for evaluation.
This was done by (Budanitsky and Hirst, 2001) in
their comparison of five measures of semantic re-
latedness. We follow a similar approach in evalu-
ating the Gloss Vector measure.
We use a set of 30 word pairs from a study
carried out by (Miller and Charles, 1991). These
word pairs are a subset of 65 word pairs used by
(Rubenstein and Goodenough, 1965), in a similar
study almost 25 years earlier. In this study, human
subjects assigned relatedness scores to the selected
word pairs. The word pairs selected for this study
ranged from highly related pairs to unrelated pairs.
We use these human judgments for our evaluation.
Each of the word pairs have been scored by hu-
mans on a scale of 0 to 5, where 5 is the most re-
lated. The mean of the scores of each pair from all
subjects is considered as the ?human relatedness
score? for that pair. The pairs are then ranked with
respect to their scores. The most related pair is the
first on the list and the least related pair is at the
end of the list. We then have each of the measures
of relatedness score the word pairs and a another
ranking of the word pairs is created corresponding
to each of the measures.
4
Table 1: Correlation to human perception
Relatedness Measures M & C R & G
Gloss Vector 0.91 0.90
Extended Gloss Overlaps 0.81 0.83
Jiang & Conrath 0.73 0.75
Resnik 0.72 0.72
Lin 0.70 0.72
Leacock & Chodorow 0.74 0.77
Spearman?s Correlation Coefficient (Spearman,
1904) is used to assess the equivalence of two
rankings. If the two rankings are exactly the
same, the Spearman?s correlation coefficient be-
tween these two rankings is 1. A completely re-
versed ranking gets a value of ?1. The value is 0
when there is no relation between the rankings.
We determine the correlation coefficient of the
ranking of each measure with that of the human
relatedness. We use the relatedness scores from
both the human studies ? the Miller and Charles
study as well as the Rubenstein and Goodenough
research. Table 1 summarizes the results of our
experiment. We observe that the Gloss Vector has
the highest correlation with humans in both cases.
Note that in our experiments with the Gloss
Vector measure, we have used not only the gloss
of the concept but augmented that with the gloss
of all the concepts directly related to it accord-
ing to WordNet. We observed a significant drop
in performance when we used just the glosses of
the concept alone, showing that the expansion is
necessary. In addition, the frequency cutoffs used
to construct the Word Space played a critical role.
The best setting of the frequency cutoffs removed
both low and high frequency words, which elimi-
nates two different sources of noise. Very low fre-
quency words do not occur enough to draw dis-
tinctions among different glosses, whereas high
frequency words occur in many glosses, and again
do not provide useful information to distinguish
among glosses.
5.2 Application-based Evaluation
An application-oriented comparison of five mea-
sures of semantic relatedness was presented in
(Budanitsky and Hirst, 2001). In that study they
evaluate five WordNet-based measures of seman-
tic relatedness with respect to their performance in
context sensitive spelling correction.
We present the results of an application-oriented
Table 2: WSD on SENSEVAL-2 (nouns)
Measure Nouns
Jiang & Conrath 0.45
Extended Gloss Overlaps 0.44
Gloss Vector 0.41
Lin 0.36
Resnik 0.30
Leacock & Chodorow 0.30
evaluation of the measures of semantic related-
ness. Each of the seven measures of semantic re-
latedness was used in a word sense disambigua-
tion algorithm described by (Banerjee and Peder-
sen, 2003).
Word sense disambiguation is the task of deter-
mining the meaning (from multiple possibilities)
of a word in its given context. For example, in the
sentence The ex-cons broke into the bank on Elm
street, the word bank has the ?financial institution?
sense as opposed to the ?edge of a river? sense.
Banerjee and Pedersen attempt to perform this
task by measuring the relatedness of the senses of
the target word to those of the words in its context.
The sense of the target word that is most related to
its context is selected as the intended sense of the
target word.
The experimental data used for this evaluation
is the SENSEVAL-2 test data. It consists of 4,328
instances (or contexts) that each includes a single
ambiguous target word. Each instance consists of
approximately 2-3 sentences and one occurrence
of a target word. 1,754 of the instances include
nouns as target words, while 1,806 are verbs and
768 are adjectives. We use the noun data to com-
pare all six of the measures, since four of the mea-
sures are limited to nouns as input. The accuracy
of disambiguation when performed using each of
the measures for nouns is shown in Table 2.
6 Gloss Vector Tuning
As discussed in earlier sections, the Gloss Vector
measure builds a word space consisting of first or-
der context vectors corresponding to every word in
a corpus. Gloss vectors are the resultant of a num-
ber of first order context vectors. All of these vec-
tors encode semantic information about the con-
cepts or the glosses that the vectors represent.
We note that the quality of the words used as the
dimensions of these vectors plays a pivotal role in
5
getting accurate relatedness scores. We find that
words corresponding to very specific concepts and
are highly indicative of a few topics, make good
dimensions. Words that are very general in nature
and that appear all over the place add noise to the
vectors.
In an earlier section we discussed using stop
words and frequency cutoffs to keep only the high
?information content? words. In addition to those,
we also experimented with a term frequency ? in-
verse document frequency cutoff.
Term frequency and inverse document frequency
are commonly used metrics in information re-
trieval. For a given word, term frequency (tf ) is
the number of times a word appears in the corpus.
The document frequency is number of documents
in which the word occurs. Inverse document fre-
quency (idf ) is then computed as
idf = logNumber of DocumentsDocument Frequency (1)
The tf ? idf value is an indicator of the speci-
ficity of a word. The higher the tf ? idf value, the
lower the specificity.
Figure 3 shows a plot of tf ? idf cutoff on the
x-axis against the correlation of the Gloss Vector
measure with human judgments on the y-axis.
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  500  1000  1500  2000  2500  3000  3500  4000  4500
Co
rre
la
tio
n
tf.idf cutoff
M&C
R&G
Figure 3: Plot of tf ? idf cutoff vs. correlation
The tf ? idf values ranged from 0 to about 4200.
Note that we get lower correlation as the cutoff is
raised.
7 Analysis
We observe from the experimental results that the
Gloss Vector measure corresponds the most with
human judgment of relatedness (with a correlation
of almost 0.9). We believe this is probably be-
cause the Gloss Vector measure most closely im-
itates the representation of concepts in the human
mind. (Miller and Charles, 1991) suggest that the
cognitive representation of a word is an abstrac-
tion derived from its contexts (encountered by the
person). Their study also suggested the semantic
similarity of two words depends on the overlap be-
tween their contextual representations. The Gloss
Vector measure uses the contexts of the words and
creates a vector representation of these. The over-
lap between these vector representations is used to
compute the semantic similarity of concepts.
(Landauer and Dumais, 1997) additionally per-
form singular value decomposition (SVD) on their
context vector representation of words and they
show that reducing the number of dimensions of
the vectors using SVD more accurately simulates
learning in humans. We plan to try SVD on the
Gloss Vector measure in future work.
In the application-oriented evaluation, the Gloss
Vector measure performed relatively well (about
41% accuracy). However, unlike the human study,
it did not outperform all the other measures. We
think there are two possible explanations for this.
First, the word pairs used in the human relatedness
study are all nouns, and it is possible that the Gloss
Vector measure performs better on nouns than on
other parts of speech. In the application-oriented
evaluation the measure had to make judgments for
all parts of speech. Second, the application itself
affects the performance of the measure. The Word
Sense Disambiguation algorithm starts by select-
ing a context of 5 words from around the target
word. These context words contain words from all
parts of speech. Since the Jiang-Conrath measure
assigns relatedness scores only to noun concepts,
its behavior would differ from that of the Vector
measure which would accept all words and would
be affected by the noise introduced from unrelated
concepts. Thus the context selection factors into
the accuracy obtained. However, for evaluating
the measure as being suitable for use in real ap-
plications, the Gloss Vector measure proves rela-
tively accurate.
The Gloss Vector measure can draw conclu-
sions about any two concepts, irrespective of part-
of-speech. The only other measure that can make
this same claim is the Extended Gloss Overlaps
measure. We would argue that Gloss Vectors
present certain advantages over it. The Extended
6
Gloss Overlap measure looks for exact string over-
laps to measure relatedness. This ?exactness?
works against the measure, in that it misses po-
tential matches that intuitively would contribute to
the score (For example, silverware with spoon).
The Gloss Vector measure is more robust than the
Extended Gloss Overlap measure, in that exact
matches are not required to identify relatedness.
The Gloss Vector measure attempts to overcome
this ?exactness? by using vectors that capture the
contextual representation of all words. So even
though silverware and spoon do not overlap, their
contextual representations would overlap to some
extent.
8 Related Work
(Wilks et al, 1990) describe a word sense disam-
biguation algorithm that also uses vectors to de-
termine the intended sense of an ambiguous word.
In their approach, they use dictionary definitions
from LDOCE (Procter, 1978). The words in these
definitions are used to build a co?occurrence ma-
trix, which is very similar to our technique of
using the WordNet glosses for our Word Space.
They augment their dictionary definitions with
similar words, which are determined using the co?
occurrence matrix. Each concept in LDOCE is
then represented by an aggregate vector created by
adding the co?occurrence counts for each of the
words in the augmented definition of the concept.
The next step in their algorithm is to form a con-
text vector. The context of the ambiguous word
is first augmented using the co?occurrence ma-
trix, just like the definitions. The context vector
is formed by taking the aggregate of the word vec-
tors of the words in the augmented context. To
disambiguate the target word, the context vector
is compared to the vectors corresponding to each
meaning of the target word in LDOCE, and that
meaning is selected whose vector is mathemati-
cally closest to that of the context.
Our approach differs from theirs in two primary
respects. First, rather than creating an aggregate
vector for the context we compare the vector of
each meaning of the ambiguous word with the vec-
tors of each of the meanings of the words in the
context. This adds another level of indirection in
the comparison and attempts to use only the rele-
vant meanings of the context words. Secondly, we
use the structure of WordNet to augment the short
glosses with other related glosses.
(Niwa and Nitta, 1994) compare dictionary
based vectors with co?occurrence based vectors,
where the vector of a word is the probability that
an origin word occurs in the context of the word.
These two representations are evaluated by apply-
ing them to real world applications and quantify-
ing the results. Both measures are first applied to
word sense disambiguation and then to the learn-
ing of positives or negatives, where it is required
to determine whether a word has a positive or neg-
ative connotation. It was observed that the co?
occurrence based idea works better for the word
sense disambiguation and the dictionary based ap-
proach gives better results for the learning of pos-
itives or negatives. From this, the conclusion is
that the dictionary based vectors contain some dif-
ferent semantic information about the words and
warrants further investigation. It is also observed
that for the dictionary based vectors, the network
of words is almost independent of the dictionary
that is used, i.e. any dictionary should give us al-
most the same network.
(Inkpen and Hirst, 2003) also use gloss?based
context vectors in their work on the disambigua-
tion of near?synonyms ? words whose senses
are almost indistinguishable. They disambiguate
near?synonyms in text using various indicators,
one of which is context-vector-based. Context
Vectors are created for the context of the target
word and also for the glosses of each sense of the
target word. Each gloss is considered as a bag
of words, where each word has a corresponding
Word Vector. These vectors for the words in a
gloss are averaged to get a Context Vector corre-
sponding to the gloss. The distance between the
vector corresponding to the text and that corre-
sponding to the gloss is measured (as the cosine
of the angle between the vectors). The nearness
of the vectors is used as an indicator to pick the
correct sense of the target word.
9 Conclusion
We introduced a new measure of semantic relat-
edness based on the idea of creating a Gloss Vec-
tor that combines dictionary content with corpus
based data. We find that this measure correlates
extremely well with the results of these human
studies, and this is indeed encouraging. We be-
lieve that this is due to the fact that the context vec-
tor may be closer to the semantic representation
of concepts in humans. This measure can be tai-
7
lored to particular domains depending on the cor-
pus used to derive the co?occurrence matrices, and
makes no restrictions on the parts of speech of the
concept pairs to be compared.
We also demonstrated that the Vector measure
performs relatively well in an application-oriented
setup and can be conveniently deployed in a real
world application. It can be easily tweaked and
modified to work in a restricted domain, such as
bio-informatics or medicine, by selecting a spe-
cialized corpus to build the vectors.
10 Acknowledgments
This research was partially supported by a Na-
tional Science Foundation Faculty Early CAREER
Development Award (#0092784).
All of the experiments in this paper were
carried out with the WordNet::Similarity pack-
age, which is freely available for download from
http://search.cpan.org/dist/WordNet-Similarity.
References
S. Banerjee and T. Pedersen. 2003. Extended gloss
overlaps as a measure of semantic relatedness. In
Proceedings of the Eighteenth International Confer-
ence on Artificial Intelligence (IJCAI-03), Acapulco,
Mexico, August.
A. Budanitsky and G. Hirst. 2001. Semantic distance
in WordNet: An experimental, application-oriented
evaluation of five measures. In Workshop on Word-
Net and Other Lexical Resources, Second meeting of
the North American Chapter of the Association for
Computational Linguistics, Pittsburgh, June.
D. Carnine, E. J. Kameenui, and G. Coyle. 1984. Uti-
lization of contextual information in determining the
meaning of unfamiliar words. Reading Research
Quarterly, 19:188?204.
C. Fellbaum, editor. 1998. WordNet: An electronic
lexical database. MIT Press.
Z. Harris. 1985. Distributional structure. In J. J. Katz,
editor, The Philosophy of Linguistics, pages 26?47.
Oxford University Press, New York.
D. Inkpen and G. Hirst. 2003. Automatic sense disam-
biguation of the near-synonyms in a dictionary en-
try. In Proceedings of the 4th Conference on Intel-
ligent Text Processing and Computational Linguis-
tics (CICLing-2003), pages 258?267, Mexico City,
February.
J. Jiang and D. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proceedings on International Conference on Re-
search in Computational Linguistics, Taiwan.
T. K. Landauer and S. T. Dumais. 1997. A solution
to plato?s problem: The latent semantic analysis the-
ory of acquisition, induction and representation of
knowledge. Psychological Review, 104:211?240.
C. Leacock and M. Chodorow. 1998. Combining local
context and WordNet similarity for word sense iden-
tification. In C. Fellbaum, editor, WordNet: An elec-
tronic lexical database, pages 265?283. MIT Press.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of International Confer-
ence on Machine Learning, Madison, Wisconsin,
August.
S. McDonald and M. Ramscar. 2001. Testing the dis-
tributional hypothesis: The influence of context on
judgements of semantic similarity. In Proceedings
of the 23rd Annual Conference of the Cognitive Sci-
ence Society, Edinburgh, Scotland.
G.A. Miller and W.G. Charles. 1991. Contextual cor-
relates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1?28.
Y. Niwa and Y. Nitta. 1994. Co-occurrence vec-
tors from corpora versus distance vectors from dic-
tionaries. In Proceedings of the Fifteenth Inter-
national Conference on Computational Linguistics,
pages 304?309, Kyoto, Japan.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003.
Using measures of semantic relatedness for word
sense disambiguation. In Proceedings of the Fourth
International Conference on Intelligent Text Pro-
cessing and Computational Linguistics (CICLING-
03), Mexico City, Mexico, February.
P. Procter, editor. 1978. Longman Dictionary of Con-
temporary English. Longman Group Ltd., Essex,
UK.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceed-
ings of the 14th International Joint Conference on
Artificial Intelligence, Montreal, August.
H. Rubenstein and J.B. Goodenough. 1965. Contex-
tual correlates of synonymy. Communications of the
ACM, 8:627?633, October.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
C. Spearman. 1904. Proof and measurement of as-
sociation between two things. American Journal of
Psychology, 15:72?101.
M. Stevenson and M. Greenwood. 2005. A seman-
tic approach to ie pattern induction. In Proceedings
of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 379?386, Ann Ar-
bor, Michigan, June.
Y. Wilks, D. Fass, C. Guo, J. McDonald, T. Plate, and
B. Slator. 1990. Providing machine tractable dictio-
nary tools. Machine Translation, 5:99?154.
8
BioNLP 2007: Biological, translational, and clinical language processing, pages 9?16,
Prague, June 2007. c?2007 Association for Computational Linguistics
Determining the Syntactic Structure of Medical Terms in Clinical Notes
Bridget T. McInnes
Dept. of Computer Science
and Engineering
University of Minnesota
Minneapolis, MN, 55455
bthomson@cs.umn.edu
Ted Pedersen
Dept. of Computer Science
University of Minnesota Duluth
Duluth, MN, 55812
tpederse@d.umn.edu
Serguei V. Pakhomov
Dept. of Pharmaceutical Care
and Health Systems Center
for Health Informatics
University of Minnesota
Minneapolis, MN, 55455
pakh0002@umn.edu
Abstract
This paper demonstrates a method for de-
termining the syntactic structure of medi-
cal terms. We use a model-fitting method
based on the Log Likelihood Ratio to clas-
sify three-word medical terms as right or
left-branching. We validate this method by
computing the agreement between the clas-
sification produced by the method and man-
ually annotated classifications. The results
show an agreement of 75% - 83%. This
method may be used effectively to enable
a wide range of applications that depend
on the semantic interpretation of medical
terms including automatic mapping of terms
to standardized vocabularies and induction
of terminologies from unstructured medical
text.
1 Introduction
Most medical concepts are expressed via a domain
specific terminology that can either be explicitly
agreed upon or extracted empirically from domain
specific text. Regardless of how it is constructed,
a terminology serves as a foundation for informa-
tion encoding, processing and exchange in a special-
ized sub-language such as medicine. Concepts in the
medical domain are encoded through a variety of lin-
guistic forms, the most typical and widely accepted
is the noun phrase (NP). In some even further spe-
cialized subdomains within medicine, such as nurs-
ing and surgery, an argument can be made that some
concepts are represented by an entire predication
rather than encapsulated within a single nominal-
ized expression. For example, in order to describe
someone?s ability to lift objects 5 pounds or heav-
ier above their head, it may be necessary to use a
term consisting of a predicate such as [LIFT] and a
set of arguments corresponding to various thematic
roles such as <PATIENT> and <PATH> (Ruggieri
et al, 2004). In this paper, we address typical med-
ical terms encoded as noun phrases (NPs) that are
often structurally ambiguous, as in Example 1, and
discuss a case for extending the proposed method to
non-nominalized terms as well.
small1 bowel2 obstruction3 (1)
The NP in Example 1 can have at least two interpre-
tations depending on the syntactic analysis:
[[small1 bowel2] obstruction3] (2)
[small1 [bowel2 obstruction3]] (3)
The term in Example 2 denotes an obstruction in
the small bowel, which is a diagnosable disorder;
whereas, the term in Example 3 refers to a small un-
specified obstruction in the bowel.
Unlike the truly ambiguous general English cases
such as the classical ?American History Professor?
where the appropriate interpretation depends on the
context, medical terms, such as in Example 1, tend
to have only one appropriate interpretation. The
context, in this case, is the discourse domain of
medicine. From the standpoint of the English lan-
guage, the interpretation that follows from Example
3 is certainly plausible, but unlikely in the context
of a medical term. The syntax of a term only shows
9
what interpretations are possible without restricting
them to any particular one. From the syntactic anal-
ysis, we know that the term in Example 1 has the po-
tential for being ambiguous; however, we also know
that it does have an intended interpretation by virtue
of being an entry term in a standardized terminology
with a unique identifier anchoring its meaning. What
we do not know is which syntactic structure gen-
erated that interpretation. Being able to determine
the structure consistent with the intended interpreta-
tion of a clinical term can improve the analysis of
unrestricted medical text and subsequently improve
the accuracy of Natural Language Processing (NLP)
tasks that depend on semantic interpretation.
To address this problem, we propose to use a
model-fitting method which utilizes an existing sta-
tistical measure, the Log Likelihood Ratio. We val-
idate the application of this method on a corpus
of manually annotated noun-phrase-based medical
terms. First, we present previous work on structural
ambiguity resolution. Second, we describe the Log
Likelihood Ratio and then its application to deter-
mining the structure of medical terms. Third, we
describe the training corpus and discuss the compi-
lation of a test set of medical terms and human ex-
pert annotation of those terms. Last, we present the
results of a preliminary validation of the method and
discuss several possible future directions.
2 Previous Work
The problem of resolving structural ambiguity has
been previously addressed in the computational lin-
guistics literature. There are multiple approaches
ranging from purely statistical (Ratnaparkhi, 1998),
to hybrid approaches that take into account the lexi-
cal semantics of the verb (Hindle and Rooth, 1993),
to corpus-based, which is the approach discussed
in this paper. (Marcus, 1980) presents an early ex-
ample of a corpus-based approach to syntactic am-
biguity resolution. One type of structural ambigu-
ity that has received much attention has to do with
nominal compounds as seen in the work of (Resnik,
1993), (Resnik and Hearst, 1993), (Pustejovsky et
al., 1993), and (Lauer, 1995).
(Lauer, 1995) points out that the existing ap-
proaches to resolving the ambiguity of noun phrases
fall roughly into two camps: adjacency and de-
pendency. The proponents of the adjacency model
((Liberman and Sproat, 1992), (Resnik, 1993) and
(Pustejovsky et al, 1993)) argue that, given a three
word noun phrase XYZ, there are two possible an-
alyzes [[XY]Z] and [X[YZ]]. The correct analysis
is chosen based on the ?acceptability? of the adja-
cent bigrams A[XY] and A[YZ]. If A[XY] is more
acceptable than A[YZ], then the left-branching anal-
ysis [[XY]Z] is preferred.
(Lauer and Dras, 1994) and (Lauer, 1995) address
the issue of structural ambiguity by developing a de-
pendency model where instead of computing the ac-
ceptability of A[YZ] one would compute the accept-
ability of A[XZ]. (Lauer, 1995) argues that the de-
pendency model is not only more intuitive than the
adjacency model, but also yields better results. (La-
pata and Keller, 2004) results also support this as-
sertion.
The difference between the approaches within the
two models is the computation of acceptability. Pro-
posals for computing acceptability (or preference)
include raw frequency counts ((Evans and Zhai,
1996) and (Lapata and Keller, 2004)), Latent Se-
mantic Indexing ((Buckeridge and Sutcliffe, 2002))
and statistical measures of association ((Lapata et
al., 1999) and (Nakov and Hearst, 2005)).
One of the main problems with using frequency
counts or statistical methods for structural ambigu-
ity resolution is the sparseness of data; however,
(Resnik and Hearst, 1993) used conceptual associa-
tions (associations between groups of terms deemed
to form conceptual units) in order to alleviate this
problem. (Lapata and Keller, 2004) use the doc-
ument counts returned by WWW search engines.
(Nakov and Hearst, 2005) use the ?2 measure based
on statistics obtained from WWW search engines to
compute values to determine acceptability of a syn-
tactic analysis for nominal compounds. This method
is tested using a set of general English nominal com-
pounds developed by (Lauer, 1995) as well as a set
of nominal compounds extracted from MEDLINE
abstracts.
The novel contribution of our study is in demon-
strating and validating a corpus-based method for
determining the syntactic structure of medical terms
that relies on using the statistical measure of asso-
ciation, the Log Likelihood Ratio, described in the
following section.
10
3 Log Likelihood Ratio
The Log Likelihood Ratio (G2) is a ?goodness of
fit? statistic first proposed by (Wilks, 1938) to test if
a given piece of data is a sample from a set of data
with a specific distribution described by a hypothe-
sized model. It was later applied by (Dunning, 1993)
as a way to determine if a sequence of N words (N-
gram) came from an independently distributed sam-
ple.
(Pedersen et al, 1996) pointed out that there ex-
ists theoretical assumptions underlying the G2 mea-
sure that were being violated therefore making them
unreliable for significance testing. (Moore, 2004)
provided additional evidence that although G2 may
not be useful for determining the significance of an
event, its near equivalence to mutual information
makes it an appropriate measure of word associa-
tion. (McInnes, 2004) applied G2 to the task of ex-
tracting three and four word collocations from raw
text.
G2, formally defined for trigrams in Equation 4,
compares the observed frequency counts with the
counts that would be expected if the words in the
trigram (3-gram; a sequence of three words) corre-
sponded to the hypothesized model.
G2 = 2 ?
?
x,y,z
nxyz ? log(
nxyz
mxyz
) (4)
The parameter nxyz is the observed frequency of
the trigram where x, y, and z respectively represent
the occurrence of the first, second and third words
in the trigram. The variable mxyz is the expected
frequency of the trigram which is calculated based
on the hypothesized model. This calculation varies
depending on the model used. Often the hypothe-
sized model used is the independence model which
assumes that the words in the trigram occur together
by chance. The calculation of the expected values
based on this model is as follows:
mxyz = nx++ ? n+y+ ? n++z/n+++ (5)
The parameter, n+++, is the total number of tri-
grams that exist in the training data, and nx++,
n+y+, and n++z are the individual marginal counts
of seeing words x, y, and z in their respective posi-
tions in a trigram. A G2 score reflects the degree to
which the observed and expected values diverge. A
G2 score of zero implies that the observed values are
equal to the expected and the trigram is represented
perfectly by the hypothesized model. Hence, we
would say that the data ?fits? the model. Therefore,
the higher the G2 score, the less likely the words
in the trigram are represented by the hypothesized
model.
4 Methods
4.1 Applying Log Likelihood to Structural
Disambiguation
The independence model is the only hypothesized
model used for bigrams (2-gram; a sequence of
two words). As the number of words in an N-
gram grows, the number of hypothesized models
also grows. The expected values for a trigram can
be based on four models. The first model is the
independence model discussed above. The second
is the model based on the probability that the first
word and the second word in the trigram are depen-
dent and independent of the third word. The third
model is based on the probability that the second
and third words are dependent and independent of
the first word. The last model is based on the prob-
ability that the first and third words are dependent
and independent of the second word. Table 1 shows
the different models for the trigram XYZ.
Table 1: Models for the trigram XYZ
Model 1 P(XYZ) / P(X) P(Y) P(Z)
Model 2 P(XYZ) / P(XY) P(Z)
Model 3 P(XYZ) / P(X) / P(YZ)
Model 4 P(XYZ) / P(XZ) P(Y)
Slightly different formulas are used to calculate
the expected values for the different hypothesized
models. The expected values for Model 1 (the in-
dependence model) are given above in Equation 5.
The calculation of expected values for Model 2, 3, 4
are seen in Equations 6, 7, 8 respectively.
mxyz = nxy+ ? n++z/n+++ (6)
mxyz = nx++ ? n+yz/n+++ (7)
mxyz = nx+z ? n+y+/n+++ (8)
The parameter nxy+ is the number of times words
x and y occur in their respective positions, n+yz is
11
the number of times words y and z occur in their
respective positions and nx+z is the number of times
that words x and z occur in their respective positions
in the trigram.
The hypothesized models result in different ex-
pected values which results in a different G2 score.
A G2 score of zero implies that the data are perfectly
represented by the hypothesized model and the ob-
served values are equal to the expected. Therefore,
the model that returns the lowest score for a given
trigram is the model that best represents the struc-
ture of that trigram, and hence, best ?fits? the trigram.
For example, Table 2 shows the scores returned for
each of the four hypothesized models for the trigram
?small bowel obstruction?.
Table 2: Example for the term ?small bowel obstruc-
tion?
Model G2 score Model G2 score
Model 1 11,635.45 Model 2 5,169.81
Model 3 8,532.90 Model 4 7,249.90
The smallest G2 score is returned by Model 2
which is based on the first and second words be-
ing dependent and independent of the third. Based
on the data, Model 2 best represents or ?fits? the tri-
gram, ?small bowel obstruction?. In this particular
case that happens to be the correct analysis.
The frequency counts and G2 scores for each
model were obtained using the N-gram Statistics
Package 1 (Banerjee and Pedersen, 2003).
4.2 Data
The data for this study was collected from two
sources: the Mayo Clinic clinical notes and
SNOMED-CT terminology (Stearns et al, 2001).
4.2.1 Clinical Notes
The corpus used in this study consists of over
100,000 clinical notes covering a variety of ma-
jor medical specialties at the Mayo Clinic. These
notes document each patient-physician contact and
are typically dictated over the telephone. They range
in length from a few lines to several pages of text
and represent a quasi-spontaneous discourse where
the dictations are made partly from notes and partly
1http://www.d.umn.edu/ tpederse/nsp.html
from memory. At the Mayo Clinic, the dictations
are transcribed by trained personnel and are stored
in the patient?s chart electronically.
4.2.2 SNOMED-CT
SNOMED-CT (Systematized Nomenclature of
Medicine, Clinical Terminology) is an ontologi-
cal resource produced by the College of American
Pathologists and distributed as part of the Unified
Medical Language System2 (UMLS) Metathesaurus
maintained by the National Library of Medicine.
SNOMED-CT is the single largest source of clini-
cal terms in the UMLS and as such lends itself well
to the analysis of terms found in clinical reports.
SNOMED-CT is used for many applications in-
cluding indexing electronic medical records, ICU
monitoring, clinical decision support, clinical trials,
computerized physician order entry, disease surveil-
lance, image indexing and consumer health informa-
tion services. The version of SNOMED-CT used in
this study consists of more than 361,800 unique con-
cepts with over 975,000 descriptions (entry terms)
(SNOMED-CT Fact Sheet, 2004).
4.3 Testset of Three Word Terms
We used SNOMED-CT to compile a list of terms
in order to develop a test set to validate the G2
method. The test set was created by extracting all
trigrams from the corpus of clinical notes and all
three word terms found in SNOMED-CT. The inter-
section of the SNOMED-CT terms and the trigrams
found in the clinical notes was further restricted to
include only simple noun phrases that consist of a
head noun modified with a set of other nominal or
adjectival elements including adjectives and present
and past participles. Adverbial modification of ad-
jectives was also permitted (e.g. ?partially edentu-
lous maxilla?). Noun phrases with nested prepo-
sitional phrases such as ?fear of flying? as well as
three word terms that are not noun phrases such as
?does not eat? or ?unable to walk? were excluded
from the test set. The resulting test set contains 710
items.
The intended interpretation of each three word
term (trigram) was determined by arriving at a
2Unified Medical Language System is a compendium of
over 130 controlled medical vocabularies encompassing over
one million concepts.
12
consensus between two medical index experts
(kappa=0.704). These experts have over ten years of
experience with classifying medical diagnoses and
are highly qualified to carry out the task of deter-
mining the intended syntactic structure of a clinical
term.
Table 3: Four Types of Syntactic Structures of Tri-
gram Terms
left-branching ((XY)Z):
[[urinary tract] infection]
[[right sided] weakness]
right-branching (X(YZ)):
[chronic [back pain]]
[low [blood pressure]]
non-branching ((X)(Y)(Z)):
[[follicular][thyroid][carcinoma]]
[[serum][dioxin][level]]
monolithic (XYZ):
[difficulty finding words]
[serous otitis media]
In the process of annotating the test set of tri-
grams, four types of terms emerged (Table 3). The
first two types are left and right-branching where the
left-branching phrases contain a left-adjoining group
that modifies the head of the noun phrase. The right-
branching phrases contain a right-adjoining group
that forms the kernel or the head of the noun phrase
and is modified by the remaining word on the left.
The non-branching type is where the phrase contains
a head noun that is independently modified by the
other two words. For example, in ?follicular thyroid
carcinoma?, the experts felt that ?carcinoma? was
modified by both ?follicular? and ?thyroid? indepen-
dently, where the former denotes the type of cancer
and the latter denotes its location. This intuition is
reflected in some formal medical classification sys-
tems such as the Hospital International Classifica-
tion of Disease Adaptation (HICDA) where cancers
are typically classified with at least two categories -
one for location and one for the type of malignancy.
This type of pattern is rare. We were able to iden-
tify only six examples out of the 710 terms. The
monolithic type captures the intuition that the terms
function as a collocation and are not decomposable
into subunits. For example, ?leg length discrepancy?
denotes a specific disorder where one leg is of a dif-
ferent length from the other. Various combinations
of subunits within this term result in nonsensical ex-
pressions.
Table 4: Distribution of term types in the test set
Type Count %total
Left-branching 251 35.5
Right-branching 378 53.4
Non-branching 6 0.8
Monolithic 73 10.3
Total 708 100
Finally, there were two terms for which no con-
sensus could be reached: ?heart irregularly irregu-
lar? and ?subacute combined degeneration?. These
cases were excluded from the final set. Table 4
shows the distribution of the four types of terms in
the test set.
5 Evaluation
We hypothesize that general English typically has
a specific syntactic structure in the medical domain,
which provides a single semantic interpretation. The
patterns observed in the set of 710 medical terms
described in the previous section suggest that the
G2 method offers an intuitive way to determine the
structure of a term that underlies its syntactic struc-
ture.
Table 5: G2 Model Descriptions
left-branching Model 2 [ [XY] Z ]
right-branching Model 3 [ X [YZ] ]
The left and right-branching patterns roughly cor-
respond to Models 2 and 3 in Table 5. Models 1
and 4 do not really correspond to any of the pat-
terns we were able to identify in the set of terms.
Model 1 would represent a term where words are
completely independent of each other, which is an
unlikely scenario given that we are working with
terms whose composition is dependent by definition.
This is not to say that in other applications (e.g.,
syntactic parsing) this model would not be relevant.
Model 4 suggests dependence between the outer
edges of a term and their independence from the
13
Figure 1: Comparison of the results with two base-
lines: L-branching and R-branching assumptions
middle word, which is not motivated from the stand-
point of a traditional context free grammar which
prohibits branch crossing. However, this model may
be welcome in a dependency grammar paradigm.
One of the goals of this study is to test an ap-
plication of the G2 method trained on a corpus of
medical data to distinguish between left and right-
branching patterns. The method ought to suggest
the most likely analysis for an NP-based medical
term based on the empirical distribution of the term
and its components. As part of the evaluation, we
compute the G2 scores for each of the terms in the
test set, and picked the model with the lowest score
to represent the structural pattern of the term. We
compared these results with manually identified pat-
terns. At this preliminary stage, we cast the problem
of identifying the structure of a three word medical
term as a binary classification task where a term is
considered to be either left or right-branching, ef-
fectively forcing all terms to either be represented
by either Model 2 or Model 3.
6 Results and Discussion
In order to validate the G2 method for determin-
ing the structure of medical terms, we calculated
the agreement between human experts? interpreta-
tion of the syntactic structure of the terms and the
interpretation suggested by the G2 method. The
agreement was computed as the ratio of match-
ing interpretations to the total number of terms be-
ing interpreted. We used two baselines, one estab-
lished by assuming that each term is left-branching
and the other by assuming that each term is right-
branching. As is clear from Table 4, the left-
branching baseline is 35.5% and the right-branching
baseline is 53.4% meaning that if we simply as-
sign left-branching pattern to each three word term,
we would agree with human experts 35.5% of the
time. The G2 method correctly identifies 185 tri-
grams as being left-branching (Model 2) and 345 tri-
grams as being right-branching (Model 3). There are
116 right-branching trigrams incorrectly identified
as left-branching, and 62 left-branching trigrams in-
correctly identified as right- branching. Thus the
method and the human experts agreed on 530 (75%)
terms out of 708 (kappa=0.473), which is better than
both baselines (Figure 1). We did not find any over-
lap between the terms that human experts annotated
as non-branching and the terms whose corpus dis-
tribution can be represented by Model 4 ([[XZ]Y]).
This is not surprising as this pattern is very rare.
Most of the terms are represented by either Model 2
(left-branching) or Model 3 (right-branching). The
monolithic terms that the human experts felt were
not decomposable constitute 10% of all terms and
may be handled through some other mechanism
such as collocation extraction or dictionary lookup.
Excluding monolithic terms from testing results in
83.5% overall agreement (kappa=0.664).
We observed that 53% of the terms in our test
set are right-branching while only 35% are left-
branching. (Resnik, 1993) found between 64% and
67% of nominal compounds to be left-branching and
used that finding to establish a baseline for his exper-
iments with structural ambiguity resolution. (Nakov
and Hearst, 2005) also report a similar percentage
(66.8%) of left-branching noun compounds. Our
test set is not limited to nominal compounds, which
may account for the fact that a slight majority of the
terms are found to be right-branching as adjectival
modification in English is typically located to the
left of the head noun. This may also help explain
the fact that the method tends to have higher agree-
ment within the set of right-branching terms (85%)
vs. left-branching (62%).
We also observed that many of the terms marked
as monolithic by the experts are of Latin origin such
as the term in Example 9 or describe the functional
14
status of a patient such as the term in Example 10.
erythema1 ab2 igne3 (9)
difficulty1 swallowing2 solids3 (10)
Example 10 merits further discussion as it illus-
trates another potential application of the method
in the domain of functional status terminology. As
was mentioned in the introduction, functional status
terms may be be represented as a predication with
a set of arguments. Such view of functional status
terminology lends itself well to a frame-based repre-
sentation of functional status terms in the context of
a database such as FrameNet 3 or PropBank4. One of
the challenging issues in representing functional sta-
tus terminology in terms of frames is the distinction
between the core predicate and the frame elements
(Ruggieri et al, 2004). It is not always clear what
lexical material should be part of the core predicate
and what lexical material should be part of one or
more arguments. Consider the term in Example 10
which represents a nominalized form of a predica-
tion. Conceivably, we could analyze this term as a
frame shown in Example 11 where the predication
consists of a predicate [DIFFICULTY] and two ar-
guments. Alternatively, Example 12 presents a dif-
ferent analysis where the predicate is a specific kind
of difficulty with a single argument.
[P:DIFFICULTY]
[ARG1:SWALLOWING<ACTIVITY>]
[ARG2:SOLIDS<PATIENT>]
(11)
[P:SWALLOWING DIFFICULTY]
[ARG1: SOLIDS<PATIENT>]
(12)
The analysis dictates the shape of the frames
and how the frames would fit into a network of
frames. The G2 method identifies Example 10 as
left-branching (Model 2), which suggests that it
would be possible to have a parent DIFFICULTY
frame and a child CLIMBING DIFFICULTY that
would inherit form its parent. An example where
this is not possible is the term ?difficulty staying
asleep? where it would probably be nonsensical or at
least impractical to have a predicate such as [STAY-
ING DIFFICULTY]. It would be more intuitive to
3http://www.icsi.berkeley.edu/framenet/
4http://www.cis.upenn.edu/ ace/
assign this term to the DIFFICULTY frame with
a frame element whose lexical content is ?staying
asleep?. The method appropriately identifies the
term ?difficulty staying asleep? as right-branching
(Model 3) where the words ?staying asleep? are
grouped together. This is an example based on in-
formal observations; however, it does suggest a util-
ity in constructing frame-based representation of at
least some clinical terms.
7 Limitations
The main limitation of the G2 method is the expo-
nential growth in the number of models to be evalu-
ated with the growth in the length of the term. This
limitation can be partly alleviated by either only con-
sidering adjacent models and limiting the length to
5-6 words, or using a forward or backward sequen-
tial search proposed by (Pedersen et al, 1997) for
the problem of selecting models for the Word Sense
Disambiguation task.
8 Conclusions and Future Work
This paper presented a simple but effective method
based on G2 to determine the internal structure of
three-word noun phrase medical terms. The abil-
ity to determine the syntactic structure that gives
rise to a particular semantic interpretation of a med-
ical term may enable accurate mapping of unstruc-
tured medical text to standardized terminologies and
nomenclatures. Future directions to improve the ac-
curacy of our method include determining how other
measures of association, such as dice coefficient and
?2, perform on this task. We feel that there is a pos-
sibility that no single measure performs best over all
types of terms. In that case, we plan to investigate in-
corporating the different measures into an ensemble-
based algorithm.
We believe the model-fitting method is not lim-
ited to structural ambiguity resolution. This method
could be applied to automatic term extraction and
automatic text indexing of terms from a standard-
ized vocabulary. More broadly, the principles of us-
ing distributional characteristics of word sequences
derived from large corpora may be applied to unsu-
pervised syntactic parsing.
15
Acknowledgments
We thank Barbara Abbott, Debra Albrecht and
Pauline Funk for their contribution to annotating the
test set and discussing aspects of medical terms.
This research was supported in part by the
NLM Training Grant in Medical Informatics (T15
LM07041-19). Ted Pedersen?s participation in this
project was supported by the NSF Faculty Early Ca-
reer Development Award (#0092784).
References
S. Banerjee and T. Pedersen. 2003. The design, imple-
mentation, and use of the Ngram Statistic Package. In
Proc. of the Fourth International Conference on Intel-
ligent Text Processing and Computational Linguistics,
Mexico City, February.
A.M. Buckeridge and R.F.E. Sutcliffe. 2002. Disam-
biguating noun compounds with latent semantic index-
ing. International Conference On Computational Lin-
guistics, pages 1?7.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
D.A. Evans and C. Zhai. 1996. Noun-phrase analysis in
unrestricted text for information retrieval. Proc. of the
34th conference of ACL, pages 17?24.
D. Hindle and M. Rooth. 1993. Structural Ambigu-
ity and Lexical Relations. Computational Linguistics,
19(1):103?120.
M. Lapata and F. Keller. 2004. The Web as a Base-
line: Evaluaing the Performance of Unsupervised
Web-based Models for a Range of NLP Tasks. Proc.
of HLT-NAACL, pages 121?128.
M. Lapata, S. McDonald, and F. Keller. 1999. Determi-
nants of Adjective-Noun Plausibility. Proc. of the 9th
Conference of the European Chapter of ACL, 30:36.
M. Lauer and M. Dras. 1994. A Probabilistic Model of
Compound Nouns. Proc. of the 7th Australian Joint
Conference on AI.
M. Lauer. 1995. Corpus Statistics Meet the Noun Com-
pound: Some Empirical Results. Proc. of the 33rd An-
nual Meeting of ACL, pages 47?55.
M. Liberman and R. Sproat. 1992. The stress and struc-
ture of modified noun phrases in English. Lexical Mat-
ters, CSLI Lecture Notes, 24:131?181.
M.P. Marcus. 1980. Theory of Syntactic Recognition
for Natural Languages. MIT Press Cambridge, MA,
USA.
B.T. McInnes. 2004. Extending the log-likelihood ratio
to improve collocation identification. Master?s thesis,
University of Minnesota.
R. Moore. 2004. On log-likelihood-ratios and the sig-
nificance of rare events. In Dekang Lin and Dekai
Wu, editors, Proc. of EMNLP 2004, pages 333?340,
Barcelona, Spain, July. Association for Computational
Linguistics.
P. Nakov and M. Hearst. 2005. Search engine statistics
beyond the n-gram: Application to noun compound
bracketing. In Proceedings of the Ninth Conference on
Computational Natural Language Learning (CoNLL-
2005), pages 17?24, Ann Arbor, Michigan, June. As-
sociation for Computational Linguistics.
T. Pedersen, M. Kayaalp, and R. Bruce. 1996. Signifi-
cant lexical relationships. In Howard Shrobe and Ted
Senator, editors, Proc. of the Thirteenth National Con-
ference on Artificial Intelligence and the Eighth Inno-
vative Applications of Artificial Intelligence Confer-
ence, Vol. 2, pages 455?460, Menlo Park, California.
AAAI Press.
T. Pedersen, R. Bruce, and J. Wiebe. 1997. Sequen-
tial model selection for word sense disambiguation. In
Proc. of the Fifth Conference on Applied Natural Lan-
guage Processing, pages 388?395, Washington, DC,
April.
J. Pustejovsky, P. Anick, and S. Bergler. 1993. Lexi-
cal semantic techniques for corpus analysis. Compu-
tational Linguistics, 19(2):331?358.
A. Ratnaparkhi. 1998. Maximum Entropy Models for
Natural Lnaguage Ambiguity Resolution. Ph.D. thesis,
University of Pennsylvania.
P. Resnik and M. Hearst. 1993. Structural Ambiguity
and Conceptual Relations. Proc. of the Workshop on
Very Large Corpora: Academic and Industrial Per-
spectives, June, 22(1993):58?64.
P.S. Resnik. 1993. Selection and Information: A Class-
Based Approach to Lexical Relationships. Ph.D. the-
sis, University of Pennsylvania.
A.P. Ruggieri, S. Pakhomov, and C.G. Chute. 2004. A
Corpus Driven Approach Applying the ?Frame Se-
mantic? Method for Modeling Functional Status Ter-
minology. Proc. of MedInfo, 11(Pt 1):434?438.
M.Q. Stearns, C. Price, KA Spackman, and AY Wang.
2001. SNOMED clinical terms: overview of the de-
velopment process and project status. Proc AMIA
Symp, pages 662?6.
S. S. Wilks. 1938. The large-sample distribution of the
likelihood ratio for testing composite hypotheses. The
Annals of Mathematical Statistics, 9(1):60?62, March.
16
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 390?393,
Prague, June 2007. c?2007 Association for Computational Linguistics
UMND1: Unsupervised Word Sense Disambiguation Using Contextual
Semantic Relatedness
Siddharth Patwardhan
School of Computing
University of Utah
Salt Lake City, UT 84112.
sidd@cs.utah.edu
Satanjeev Banerjee
Language Technologies Inst.
Carnegie Mellon University
Pittsburgh, PA 15217.
banerjee@cs.cmu.edu
Ted Pedersen
Dept. of Computer Science
University of Minnesota
Duluth, MN 55812.
tpederse@d.umn.edu
Abstract
In this paper we describe an unsuper-
vised WordNet-based Word Sense Disam-
biguation system, which participated (as
UMND1) in the SemEval-2007 Coarse-
grained English Lexical Sample task. The
system disambiguates a target word by using
WordNet-based measures of semantic relat-
edness to find the sense of the word that
is semantically most strongly related to the
senses of the words in the context of the tar-
get word. We briefly describe this system,
the configuration options used for the task,
and present some analysis of the results.
1 Introduction
WordNet::SenseRelate::TargetWord1 (Patwardhan
et al, 2005; Patwardhan et al, 2003) is an unsuper-
vised Word Sense Disambiguation (WSD) system,
which is based on the hypothesis that the intended
sense of an ambiguous word is related to the
words in its context. For example, if the ?financial
institution? sense of bank is intended in a context,
then it is highly likely the context would contain
related words such as money, transaction, interest
rate, etc. The algorithm, therefore, determines
the intended sense of a word (target word) in a
given context by measuring the relatedness of each
sense of that word with the words in its context.
The sense of the target word that is most related
to its context is selected as the intended sense of
the target word. The system uses WordNet-based
1http://senserelate.sourceforge.net
measures of semantic relatedness2 (Pedersen et
al., 2004) to measure the relatedness between the
different senses of the target word and the words in
its context.
This system is completely unsupervised and re-
quires no annotated data for training. The lexical
database WordNet (Fellbaum, 1998) is the only re-
source that the system uses to measure the related-
ness between words and concepts. Thus, our system
is classified under the closed track of the task.
2 System Description
Our WSD system consists of a modular framework,
which allows different algorithms for the different
subtasks to be plugged into the system. We divide
the disambiguation task into two primary subtasks:
context selection and sense selection. The context
selection module tries to select words from the con-
text that are most likely to be indicative of the sense
of the target word. The sense selection module then
uses the set of selected context words to choose one
of the senses of the target word as the answer.
Figure 1 shows a block schematic of the system,
which takes SemEval-2007 English Lexical Sample
instances as input. Each instance is a made up of
a few English sentences, and one word from these
sentences is marked as the target word to be dis-
ambiguated. The system processes each instance
through multiple modules arranged in a sequential
pipeline. The final output of the pipeline is the sense
that is most appropriate for the target word in the
given context.
2http://wn-similarity.sourceforge.net
390
Instance
Preprocessing
Format Filter
Target Sense
Context Selection
Postprocessing
Sense Selection Relatedness Measure
Figure 1: System Architecture
2.1 Data Preparation
The input text is first passed through a format fil-
ter, whose task is to parse the input XML file. This
is followed by a preprocessing step. Each instance
passed to the preprocessing stage is first segmented
into words, and then all compound words are iden-
tified. Any sequence of words known to be a com-
pound in WordNet is combined into a single entity.
2.2 Context Selection
Although each input instance consists of a large
number of words, only a few of these are likely to
be useful for disambiguating the target word. We
use the context selection algorithm to select a subset
of the context words to be used for sense selection.
By removing the unimportant words, the computa-
tional complexity of the algorithm is reduced.
In this work, we use the NearestWords context
selection algorithm. This algorithm algorithm se-
lects 2n + 1 content words surrounding the target
word (including the target word) as the context. A
stop list is used to identify closed-class non-content
words. Additionally, any word not found in Word-
Net is also discarded. The algorithm then selects n
content words before and n content words follow-
ing the target word, and passes this unordered set of
2n + 1 words to the Sense Selection module.
2.3 Sense Selection Algorithm
The sense selection module takes the set of words
output by the context selection module, one of which
is the target word to be disambiguated. For each of
the words in this set, it retrieves a list of senses from
WordNet, based on which it determines the intended
sense of the target word.
The package provides two main algorithms for
Sense Selection: the local and the global algorithms,
as described in previous work (Banerjee and Peder-
sen, 2002; Patwardhan et al, 2003). In this work,
we use the local algorithm, which is faster and was
shown to perform as well as the global algorithm.
The local sense selection algorithm measures the
semantic relatedness of each sense of the target word
with the senses of the words in the context, and se-
lects that sense of the target word which is most re-
lated to the context word-senses. Given the 2n + 1
context words, the system scores each sense of the
target word. Suppose the target word t has T senses,
enumerated as t1, t2, . . . , tT . Also, suppose w1, w2,
. . . , w2n are the words in the context of t, each hav-
ing W1, W2, . . . , W2n senses, respectively. Then for
each ti a score is computed as
score(ti) =
2n
?
j=1
max
k=1 to Wj
(relatedness(ti, wjk))
where wjk is the kth sense of word wj . The sense ti
of target word t with the highest score is selected as
the intended sense of the target word.
The relatedness between two word senses is com-
puted using a measure of semantic relatedness de-
fined in the WordNet::Similarity software package
(Pedersen et al, 2004), which is a suite of Perl mod-
ules implementing a number WordNet-based mea-
sures of semantic relatedness. For this work, we
used the Context Vector measure (Patwardhan and
Pedersen, 2006). The relatedness of concepts is
computed based on word co-occurrence statistics
derived from WordNet glosses. Given two WordNet
senses, this module returns a score between 0 and 1,
indicating the relatedness of the two senses.
Our system relies on WordNet as its sense inven-
tory. However, this task used OntoNotes (Hovy et
al., 2006) as the sense inventory. OntoNotes word
senses are groupings of similar WordNet senses.
Thus, we used the training data answer key to gen-
erate a mapping between the OntoNotes senses of
the given lexical elements and their corresponding
WordNet senses. We had to manually create the
mappings for some of the WordNet senses, which
had no corresponding OntoNotes senses. The sense
selection algorithm performed all of its computa-
tions with respect to the WordNet senses, and finally
the OntoNotes sense corresponding to the selected
WordNet sense of the target word was output as the
391
answer for each instance.
3 Results and Analysis
For this task, we used the freely available Word-
Net::SenseRelate::TargetWord v0.10 and the Word-
Net::Similarity v1.04 packages. WordNet v2.1 was
used as the underlying knowledge base for these.
The context selection module used a window size
of five (including the target word). The semantic re-
latedness of concepts was measured using the Con-
text Vector measure, with configuration options as
defined in previous research (Patwardhan and Ped-
ersen, 2006). Since we always predict exactly one
sense for each instance, the precision and recall val-
ues of all our experiments were always the same.
Therefore, in this section we will use the name ?ac-
curacy? to mean both precision and recall.
3.1 Overall Results, and Baselines
The overall accuracy of our system on the test data
is 0.538. This represents 2,609 correctly disam-
biguated instances, out of a total of 4,851 instances.
As baseline, we compare against the random al-
gorithm where for each instance, we randomly pick
one of the WordNet senses for the lexical element
in that instance, and report the OntoNotes senseid it
maps to as the answer. This algorithm gets an ac-
curacy of 0.417. Thus, our algorithm gets an im-
provement of 12% absolute (29% relative) over this
random baseline.
Additionally, we compare our algorithm against
the WordNet SenseOne algorithm. In this algorithm,
we pick the first sense among the WordNet senses
of the lexical element in each instance, and report
its corresponding OntoNotes sense as the answer for
that instance. This algorithm leverages the fact that
(in most cases) the WordNet senses for a particular
word are listed in the database in descending order
of their frequency of occurrence in the corpora from
which the sense inventory was created. If the new
test data has a similar distribution of senses, then this
algorithm amounts to a ?majority baseline?. This
algorithm achieves an accuracy of 0.681 which is
15% absolute (27% relative) better than our algo-
rithm. Although this seemingly na??ve algorithm out-
performs our algorithm, we choose to avoid using
this information in our algorithms because it repre-
sents a large amount of human supervision in the
form of manual sense tagging of text, whereas our
goal is to create a purely unsupervised algorithm.
Additionally, our algorithms can, with little change,
work with other sense inventories besides WordNet
that may not have this information.
3.2 Results Disaggregated by Part of Speech
In our past experience, we have found that av-
erage disambiguation accuracy differs significantly
between words of different parts of speech. For the
given test data, we separately evaluated the noun and
verb instances. We obtained an accuracy of 0.399
for the noun targets and 0.692 for the verb targets.
Thus, we find that our algorithm performs much bet-
ter on verbs than on nouns, when evaluated using the
OntoNotes sense inventory. This is different from
our experience with SENSEVAL data from previous
years where performance on nouns was uniformly
better than that on verbs. One possible reason for the
better performance on verbs is that the OntoNotes
sense inventory has, on average, fewer senses per
verb word (4.41) than per noun word (5.71). How-
ever, additional experimentation is needed to more
fully understand the difference in performance.
3.3 Results Disaggregated by Lexical Element
To gauge the accuracy of our algorithm on different
words (lexical elements), we disaggregated the re-
sults by individual word. Table 1 lists the accuracy
values over instances of individual verb lexical ele-
ments, and Table 2 lists the accuracy values for noun
lexical elements. Our algorithm gets all instances
correct for 13 verb lexical elements, and for none of
the noun lexical elements. More generally, our al-
gorithm gets an accuracy of 50% or more on 45 out
of the 65 verb lexical elements, and on 15 out of the
35 noun lexical elements. For nouns, when the ac-
curacy results are viewed in sorted order (as in Table
2), one can observe a sudden degradation of results
between the accuracy of the word system.n ? 0.443
? and the word source.n ? 0.257. It is unclear why
there is such a jump; there is no such sudden degra-
dation in the results for the verb lexical elements.
4 Conclusions
This paper describes our system UMND1, which
participated in the SemEval-2007 Coarse-grained
392
Word Accuracy Word Accuracy
remove 1.000 purchase 1.000
negotiate 1.000 improve 1.000
hope 1.000 express 1.000
exist 1.000 estimate 1.000
describe 1.000 cause 1.000
avoid 1.000 attempt 1.000
affect 1.000 say 0.969
explain 0.944 complete 0.938
disclose 0.929 remember 0.923
allow 0.914 announce 0.900
kill 0.875 occur 0.864
do 0.836 replace 0.800
maintain 0.800 complain 0.786
believe 0.764 receive 0.750
approve 0.750 buy 0.739
produce 0.727 regard 0.714
propose 0.714 need 0.714
care 0.714 feel 0.706
recall 0.667 examine 0.667
claim 0.667 report 0.657
find 0.607 grant 0.600
work 0.558 begin 0.521
build 0.500 keep 0.463
go 0.459 contribute 0.444
rush 0.429 start 0.421
raise 0.382 end 0.381
prove 0.364 enjoy 0.357
see 0.296 set 0.262
promise 0.250 hold 0.250
lead 0.231 prepare 0.222
join 0.222 ask 0.207
come 0.186 turn 0.048
fix 0.000
Table 1: Verb Lexical Element Accuracies
English Lexical Sample task. The system is based
on WordNet::SenseRelate::TargetWord, which is a
freely available unsupervised Word Sense Disam-
biguation software package. The system uses
WordNet-based measures of semantic relatedness to
select the intended sense of an ambiguous word. The
system required no training data and using WordNet
as its only knowledge source achieved an accuracy
of 54% on the blind test set.
Acknowledgments
This research was partially supported by a National
Science Foundation Early CAREER Development
award (#0092784).
References
S. Banerjee and T. Pedersen. 2002. An Adapted Lesk Al-
gorithm for Word Sense Disambiguation Using Word-
Net. In Proceedings of the Third International Con-
Word Accuracy Word Accuracy
policy 0.949 people 0.904
future 0.870 drug 0.870
space 0.857 capital 0.789
effect 0.767 condition 0.765
job 0.692 bill 0.686
area 0.676 base 0.650
management 0.600 power 0.553
development 0.517 chance 0.467
exchange 0.459 order 0.456
part 0.451 president 0.446
system 0.443 source 0.257
network 0.218 state 0.208
share 0.192 rate 0.186
hour 0.167 plant 0.109
move 0.085 point 0.080
value 0.068 defense 0.048
position 0.044 carrier 0.000
authority 0.000
Table 2: Noun Lexical Element Accuracies
ference on Intelligent Text Processing and Computa-
tional Linguistics, pages 136?145, Mexico City, Mex-
ico, February.
C. Fellbaum, editor. 1998. WordNet: An electronic lexi-
cal database. MIT Press.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. OntoNotes: The 90% Solu-
tion. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the ACL, pages 57?60, New York, NY, June.
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based Context Vectors to Estimate the Semantic Relat-
edness of Concepts. In Proceedings of the EACL 2006
Workshop on Making Sense of Sense: Bringing Com-
putational Linguistics and Psycholinguistics Together,
pages 1?8, Trento, Italy, April.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003. Us-
ing Measures of Semantic Relatedness for Word Sense
Disambiguation. In Proceedings of the Fourth In-
ternational Conference on Intelligent Text Processing
and Computational Linguistics, pages 241?257, Mex-
ico City, Mexico, February.
S. Patwardhan, T. Pedersen, and S. Banerjee. 2005.
SenseRelate::TargetWord - A Generalized Framework
for Word Sense Disambiguation. In Proceedings of
the Twentieth National Conference on Artificial In-
telligence (Intelligent Systems Demonstrations), pages
1692?1693, Pittsburgh, PA, July.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet::Similarity - Measuring the Relatedness of
Concepts. In Human Language Technology Confer-
ence of the North American Chapter of the Association
for Computational Linguistics Demonstrations, pages
38?41, Boston, MA, May.
393
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 394?397,
Prague, June 2007. c?2007 Association for Computational Linguistics
UMND2 : SenseClusters Applied to the
Sense Induction Task of SENSEVAL-4
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
tpederse@d.umn.edu
http://senseclusters.sourceforge.net
Abstract
SenseClusters is a freely?available open?
source system that served as the Univer-
sity of Minnesota, Duluth entry in the
SENSEVAL-4 sense induction task. For this
task SenseClusters was configured to con-
struct representations of the instances to be
clustered using the centroid of word co-
occurrence vectors that replace the words
in an instance. These instances are then
clustered using k?means where the number
of clusters is discovered automatically using
the Adapted Gap Statistic. In these experi-
ments SenseClusters did not use any infor-
mation outside of the raw untagged text that
was to be clustered, and no tuning of the sys-
tem was performed using external corpora.
1 Introduction
The object of the sense induction task of
SENSEVAL-4 (Agirre and Soroa, 2007) was to
cluster 27,132 instances of 100 different words
(35 nouns and 65 verbs) into senses or classes.
The task data consisted of the combination of the
test and training data (minus the sense tags) from
the English lexical sample task. Each instance is
a context of several sentences which contains an
occurrence of a given word that serves as the target
of sense induction.
SenseClusters is based on the presumption that
words that occur in similar contexts will have similar
meanings. This intuition has been presented as both
the Distributional Hypothesis (Harris, 1968) and the
Strong Contextual Hypothesis (Miller and Charles,
1991).
SenseClusters has been in active development at
the University of Minnesota, Duluth since 2002. It is
an open?source project that is freely?available from
sourceforge, and has been been described in detail
in numerous publications (e.g., (Purandare and Ped-
ersen, 2004), (Pedersen et al, 2005), (Pedersen and
Kulkarni, 2007)).
SenseClusters supports a variety of techniques for
selecting lexical features, representing contexts to
be clustered, determining the appropriate number of
cluster automatically, clustering, labeling of clus-
ters, and evaluating cluster quality. The configu-
ration used in SENSEVAL-4 was just one possible
combination of these techniques.
2 Methodology in Sense Induction Task
For this task, SenseClusters represents the instances
to be clustered using second order co?occurrence
vectors. These are constructed by first identifying
word co?occurrences, and then replacing each word
in an instance to be clustered with its co-occurrence
vector. Then all the vectors that make up an instance
are averaged together to represent that instance.
A co?occurrence matrix is constructed by identi-
fying bigrams that occur in the contexts to be clus-
tered two or more times and have a Pointwise Mu-
tual Information (PMI) score greater than five. If the
value of PMI is near 1.0, this means that the words in
the bigram occur together approximately the num-
ber of times expected by chance, and they are not
strongly associated. If this value is greater than 1,
then the words in the bigram are occurring more of-
394
ten than expected by chance, and they are therefore
associated.
The rows of the co?occurrence matrix represent
the first word in the selected bigrams, and the
columns represent the second word. A window size
of 12 is allowed, which means that up to 10 inter-
vening words can be observed between the pair of
words in the bigram. This rather large window size
was employed since the sample sizes for each word
were relatively small, often no more than a few hun-
dred instances.
A stop list was used to eliminate bigrams where
either word is a high?frequency low?content word.
The particular list used is distributed with the Ngram
Statistics Package and is loosely based on the
SMART stop list. It consists of 295 words; in addi-
tion, all punctuation, single letter words, and num-
bers (with the exception of years) were eliminated.
Each of the contexts that contain a particular tar-
get word is represented by a single vector that is the
average (or the centroid) of all the co-occurrence
vectors found for the words that make up the con-
text. This results in a context by feature matrix,
where the features are the words that occur with
the words in the contexts (i.e., second order co?
occurrences). The k?means algorithm is used for
clustering the contexts, where the number of clus-
ters is automatically discovered using the Adapted
Gap Statistic (Pedersen and Kulkarni, 2006). The
premise of this method is to create a randomized
sample of data with the same characteristics of the
observed data (i.e., the contexts to be clustered).
This is done by fixing the marginal totals of the con-
text by feature matrix and then generating random-
ized values that are consistent with those marginal
totals. This creates a matrix that is can be viewed
as being from the same population as the observed
data, except that the data is essentially noise (be-
cause it is randomly generated).
The randomized data is clustered for successive
values of k from 1 to some upper limit (the num-
ber of contexts or the point at which the criterion
functions have plateaued). For each value of k the
criterion function measures the quality of the clus-
tering solution. The same is done for that observed
data, and the difference between the criterion func-
tion for the observed data and the randomized data
is determined, and the value of k where that differ-
ence is largest is selected as the best solution for k,
since that is when the clustered data least resembles
noise, and is therefore the most organized or best
solution. In these experiments the criterion function
was intra-cluster similarity.
3 Results and Discussion
There was an unsupervised and a supervised eval-
uation performed in the sense induction task. Of-
ficial scores were reported for 6 participating sys-
tems, plus the most frequent sense (MFS) baseline,
so rankings (when available) are provided from 1
(HIGH) to 7 (LOW). We also conducted an evalu-
ation using the SenseClusters method.
3.1 Unsupervised Evaluation
The unsupervised evaluation was based on the tradi-
tional clustering measures of F-score, entropy, and
purity. While the participating systems clustered the
full 27,132 instances, only the 4,581 instance subset
that corresponds to the English lexical sample eval-
uation data was scored in the evaluation. Table 1
shows the averaged F-scores over all 100 words, all
35 nouns, and all 65 verbs.
In this table the SenseClusters system (UMND2)
is compared to the MFS baseline, which is attained
by assigning all the instances of a word to a sin-
gle cluster. We also include several random base-
lines, where randomX indicates that one of X pos-
sible clusters was randomly assigned to each in-
stance of a word. Thus, approximately 100 ? X
distinct clusters are created across the 100 words.
The random results are not ranked as they were not
a part of the official evaluation. We also present the
highest (HIGH, rank 1) and lowest (LOW, rank 7)
scores from participating systems, to provide points
of comparison.
The randomX baseline is useful in determining
the sensitivity of the evaluation technique to the
number of clusters discovered. The average num-
ber of classes in the gold standard test data is 2.9, so
random3 approximates a system that randomly as-
signs the correct number of clusters. It attains an
F-score of 50.0. Note that random2 performs some-
what better (59.7), suggesting that all other things
being equal, the F-score is biased towards methods
that find a smaller than expected number of clusters.
395
Table 1: Unsupervised F-Score (test)
All Nouns Verbs Rank
MFS/HIGH 78.9 80.7 76.8 1
UMND2 66.1 67.1 65.0 4
random2 59.7 60.9 58.4
LOW 56.1 65.8 45.1 7
random3 50.0 49.9 50.1
random4 44.9 44.2 45.7
random10 29.7 28.0 31.7
random50 17.9 14.9 21.1
As the number of random clusters increases the F-
score declines sharply, showing that it is highly sen-
sitive to the number of clusters discovered, and sig-
nificantly penalizes systems that find more clusters
than indicated in the gold standard data.
We observed for UMND2 that purity (81.7) is
quite a bit higher than the F-score (66.1), and that
it discovered a smaller number of clusters on aver-
age (1.4) than exists in the gold standard data (2.9).
This shows that while SenseClusters was able to find
relatively pure clusters, it errored in finding too few
clusters, and was therefore penalized to some degree
by the F-score.
3.2 Supervised Evaluation
A supervised evaluation was also carried out on
the same clustering of the 27,132 instances as was
used in the unsupervised evaluation, following the
method defined in (Agirre et al, 2006). Here the
train portion (22,281 instances) is used to learn a ta-
ble of probabilities that is used to map discovered
clusters in the test data to gold standard classes. The
cluster assigned to each instance in the test portion
(4,851 instances) is mapped (assigned) to the most
probable class associated with that cluster as defined
by this table.
After this transformation is performed, the newly
mapped test results are scored using the scorer2 pro-
gram, which is the official evaluation program of
the English lexical sample task and reports the F-
measure, which in these experiments is simply ac-
curacy since precision and recall are the same.
In Table 2 we show the results of the super-
vised evaluation, which includes the highest and
lowest score from participating systems, as well as
Table 2: Supervised Accuracy (test)
All Nouns Verbs Rank
HIGH 81.6 86.8 75.7 1
UMND2 80.6 84.5 76.2 2
random2 78.9 81.6 75.8
MFS 78.7 80.9 76.2 4
LOW 78.5 81.4 75.2 7
random4 78.4 81.1 75.5
random3 78.3 80.5 75.9
random10 77.9 79.8 75.8
random50 75.6 78.5 72.4
UMND2, MFS, and the same randomX baselines as
included in the unsupervised evaluation.
We observed that the difference between the score
of the best performing system (HIGH) and the ran-
dom50 baseline is six points (81.6 - 75.6). In the
unsupervised evaluation of this same data this dif-
ference is 61 points (78.9 - 17.9) according to the
F-score.
The smaller range of values for the supervised
measure can be understood by noting that the map-
ping operation alters the number and distribution of
clusters as discovered in the test data. For exam-
ple, random3 results in an average of 2.9 clusters per
word in the test data, but after mapping the average
number of clusters is 1.1. The average number of
clusters discovered by UMND2 is 1.4, but after map-
ping this average is reduced to 1.1. For random50,
the average number of clusters per word is 24.1, but
after mapping is 2.0. This shows that the super-
vised evaluation has a tendency to converge upon
the MFS, which corresponds to assigning 1 cluster
per word.
When looking at the randomX results in the su-
pervised evaluation, it appears that this method does
not penalize systems for getting the number of clus-
ters incorrect (as the F-score does). This is shown by
the very similar results for the randomX baselines,
where the only difference in their results is the num-
ber of clusters. This lack of a penalty is due to the
fact that the mapping operation takes a potentially
large number of clusters and maps them to relatively
few classes (e.g., random50) and then performs the
evaluation.
396
3.3 SenseClusters Evaluation (F-Measure)
An evaluation was carried out on the full 27,132
instance train+test data set using the SenseClusters
evaluation methodology, which was first defined in
(Pedersen and Bruce, 1997). This corresponds to
an unsupervised version of the F-measure, which
in these experiments can be viewed as an accuracy
measure since precision and recall are the same (as
is the case for the supervised measure).
It aligns discovered clusters with classes such that
their agreement is maximized. The clusters and
classes must be aligned one to one, so a large penalty
can result if the number of discovered clusters dif-
fers from the number of gold standard classes.1
For UMND2, there were 145 discovered clusters
and 368 gold standard classes. Due to the one to
one alignment that is required, the 145 discovered
clusters were aligned with 145 gold standard classes
such that there was agreement for 15,291 of 27,132
instances, leading to an F-measure (accuracy) of
56.36 percent. Note that this is significantly lower
than the F-score of UMND2 for the train+test data,
which was 63.1. This illustrates that the SenseClus-
ters F-measure and the F-score are not equivalent.
4 Conclusions
One of the strengths of SenseClusters (UMND2) is
that it is able to automatically identify the number of
clusters without any manual intervention or setting
of parameters. In these experiments the Adapted
Gap statistic was quite conservative, only discover-
ing on average 1.4 classs per word, where the ac-
tual number of classes in the gold standard data was
2.9. However, this is a reasonable result, since for
many words there were just a few hundred instances.
Also, the gold standard class distinctions were heav-
ily skewed, with the majority sense occurring 80%
of the time on average. Under such conditions,
there may not be sufficient information available for
an unsupervised clustering algorithm to make fine
grained distinctions, and so discovering one cluster
for a word may be a better course of action that mak-
ing divisions that are not well supported by the data.
1An implementation of this measure is available in the
SenseClusters system, or by contacting the author.
5 Acknowledgments
These experiments were conducted with version
0.95 of the SenseClusters system. Many thanks to
Amruta Purandare and Anagha Kulkarni for their
invaluable work on this and previous versions of
SenseClusters.
This research was partially supported by the Na-
tional Science Foundation Faculty Early Career De-
velopment (CAREER) Program (#0092784).
References
E. Agirre and A. Soroa. 2007. Semeval-2007 task 2:
Evaluating word sense induction and discrimination
systems. In Proceedings of SemEval-2007: 4th Inter-
national Workshop on Semantic Evaluations, June.
E. Agirre, D. Mart??nez, O. Lo?pez de Lacalle, and
A. Soroa. 2006. Two graph-based algorithms for
state-of-the-art wsd. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 585?593, Sydney, Australia, July.
Z. Harris. 1968. Mathematical Structures of Language.
Wiley, New York.
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
T. Pedersen and R. Bruce. 1997. Distinguishing word
senses in untagged text. In Proceedings of the Sec-
ond Conference on Empirical Methods in Natural Lan-
guage Processing, pages 197?207, Providence, RI,
August.
T. Pedersen and A. Kulkarni. 2006. Automatic cluster
stopping with criterion functions and the Gap Statistic.
In Proceedings of the Demo Session of HLT/NAACL,
pages 276?279, New York City, June.
T. Pedersen and A. Kulkarni. 2007. Unsupervised dis-
crimination of person names in web contexts. In Pro-
ceedings of the Eighth International Conference on In-
telligent Text Processing and Computational Linguis-
tics, pages 299?310, Mexico City, February.
T. Pedersen, A. Purandare, and A. Kulkarni. 2005. Name
discrimination by clustering similar contexts. In Pro-
ceedings of the Sixth International Conference on In-
telligent Text Processing and Computational Linguis-
tics, pages 220?231, Mexico City, February.
A. Purandare and T. Pedersen. 2004. Word sense
discrimination by clustering contexts in vector and
similarity spaces. In Proceedings of the Conference
on Computational Natural Language Learning, pages
41?48, Boston, MA.
397
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 329?332,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Information Content Measures of Semantic Similarity
Perform Better Without Sense-Tagged Text
Ted Pedersen
Department of Computer Science
University of Minnesota, Duluth
Duluth, MN 55812
tpederse@d.umn.edu
http://wn-similarity.sourceforge.net
Abstract
This paper presents an empirical comparison
of similarity measures for pairs of concepts
based on Information Content. It shows that
using modest amounts of untagged text to de-
rive Information Content results in higher cor-
relation with human similarity judgments than
using the largest available corpus of manually
annotated sense?tagged text.
1 Introduction
Measures of semantic similarity based on WordNet
have been widely used in Natural Language Pro-
cessing. These measures rely on the structure of
WordNet to produce a numeric score that quantifies
the degree to which two concepts (represented by
a sense or synset) are similar (or not). In their sim-
plest form these measures use path length to identify
concepts that are physically close to each other and
therefore considered to be more similar than con-
cepts that are further apart.
While this is a reasonable first approximation to
semantic similarity, there are some well known limi-
tations. Most significant is that path lengths between
very specific concepts imply much smaller distinc-
tions in semantic similarity than do comparable path
lengths between very general concepts. One pro-
posed improvement is to augment concepts in Word-
Net with Information Content values derived from
sense?tagged corpora or from raw unannotated cor-
pora (Resnik, 1995).
This paper shows that Information Content mea-
sures based on modest amounts of unannotated cor-
pora have greater correlation with human similarity
judgements than do those based on the largest corpus
of sense-tagged text currently available.1 The key
to this success is not in the specific type of corpora
used, but rather in increasing the number of con-
cepts in WordNet that have counts associated with
them. These results show that Information Content
measures of semantic similarity can be significantly
improved without requiring the creation of sense?
tagged corpora (which is very expensive).
1.1 Information Content
Information Content (IC) is a measure of specificity
for a concept. Higher values are associated with
more specific concepts (e.g., pitch fork), while those
with lower values are more general (e.g., idea). In-
formation Content is computed based on frequency
counts of concepts as found in a corpus of text. The
frequency associated with a concept is incremented
in WordNet each time that concept is observed, as
are the counts of the ancestor concepts in the Word-
Net hierarchy (for nouns and verbs). This is neces-
sary because each occurrence of a more specific con-
cept also implies the occurrence of the more general
ancestor concepts.
When a corpus is sense?tagged, mapping occur-
rences of a word to a concept is straightforward
(since each sense of a word corresponds with a con-
cept or synset in WordNet). However, if the text has
not been sense?tagged then all of the possible senses
of a given word are incremented (as are their ances-
tors). For example, if tree (as a plant) occurs in a
sense?tagged text, then only the concept associated
1These experiments were done with version 2.05 of Word-
Net::Similarity (Pedersen et al, 2004).
329
with tree as a kind of plant would be incremented. If
the text is untagged, then all of the possible senses
of tree would be incremented (such as the mathe-
matical sense of tree, a shoe tree, a plant, etc.) In
this case the frequency of all the occurrences of a
word are divided equally among the different pos-
sible senses. Thus, if a word occurs 42 times in a
corpus and there are six possible senses (concepts),
each sense and all of their ancestors would have their
frequency incremented by seven.2
For each concept (synset) c in WordNet, Informa-
tion Content is defined as the negative log of the
probability of that concept (based on the observed
frequency counts):
IC(c) = ?logP (c)
Information Content can only be computed for
nouns and verbs in WordNet, since these are the only
parts of speech where concepts are organized in hi-
erarchies. Since these hierarchies are separate, In-
formation Content measures of similarity can only
be applied to pairs of nouns or pairs of verbs.
2 Semantic Similarity Measures
There are three Information Content measures im-
plemented in WordNet::Similarity: (res) (Resnik,
1995), (jcn) (Jiang and Conrath, 1997), and (lin)
(Lin, 1998).
These measures take as input two concepts c1 and
c2 (i.e., senses or synsets in WordNet) and output a
numeric measure of similarity. These measures all
rely to varying degrees on the idea of a least com-
mon subsumer (LCS); this is the most specific con-
cept that is a shared ancestor of the two concepts.
For example, the LCS of automobile and scooter is
vehicle.
The Resnik (res) measure simply uses the Infor-
mation Content of the LCS as the similarity value:
res(c1, c2) = IC(LCS(c1, c2))
The Resnik measure is considered somewhat
coarse, since many different pairs of concepts may
share the same LCS. However, it is less likely to
suffer from zero counts (and resulting undefined val-
ues) since in general the LCS of two concepts will
not be a very specific concept (i.e., a leaf node in
2This is the ?resnik counting option in WordNet::Similarity.
WordNet), but will instead be a somewhat more gen-
eral concept that is more likely to have observed
counts associated with it.
Both the Lin and Jiang & Conrath measures at-
tempt to refine the Resnik measure by augmenting it
with the Information Content of the individual con-
cepts being measured in two different ways:
lin(c1, c2) =
2?res(c1,c2)
IC(c1)+IC(c2)
jcn(c1, c2) = 1IC(c1)+IC(c2)?2?res(c1,c2)
All three of these measures have been widely
used in the NLP literature, and have tended to per-
form well in a wide range of applications such as
word sense disambiguation, paraphrase detection,
and Question Answering (c.f., (Resnik, 1999)).
3 Experimental Data
Information Content in WordNet::Similarity is (by
default) derived from SemCor (Miller et al, 1993), a
manually sense?tagged subset of the Brown Corpus.
It is made up of approximately 676,000 words, of
which 226,000 are sense?tagged. SemCor was orig-
inally created using sense?tags from version 1.6 of
WordNet, and has been mapped to subsequent ver-
sions to stay current.3 This paper uses version 3.0 of
WordNet and SemCor.
WordNet::Similarity also includes a utility (raw-
textFreq.pl) that allows a user to derive Information
Content values from any corpus of plain text. This
utility is used with the untagged version of SemCor
and with various portions of the English GigaWord
corpus (1st edition) to derive alternative Information
Content values.
English GigaWord contains more than 1.7 billion
words of newspaper text from the 1990?s and early
21st century, divided among four different sources:
Agence France Press English Service (afe), Associ-
ated Press Worldstream English Service (apw), The
New York Times Newswire Service (nyt), and The
Xinhua News Agency English Service (xie).
This paper compares the ranking of pairs of con-
cepts according to Information Content measures in
WordNet::Similarity with a number of manually cre-
ated gold standards. These include the (RG) (Ruben-
stein and Goodenough, 1965) collection of 65 noun
3http://www.cse.unt.edu/?rada/downloads.html
330
Table 1: Rank Correlation of Existing Measures
measure WS MC RG
vector .46 .89 .73
lesk .42 .83 .68
wup .34 .74 .69
lch .28 .71 .70
path .26 .68 .69
random -.20 -.16 .15
pairs, the (MC) (Miller and Charles, 1991) collec-
tion of 30 noun pairs (a subset of RG), and the (WS)
WordSimilarity-353 collection of 353 pairs (Finkel-
stein et al, 2002). RG and MC have been scored for
similarity, while WS is scored for relatedness, which
is a more general and less well?defined notion than
similarity. For example aspirin and headache are
clearly related, but they aren?t really similar.
4 Experimental Results
Table 1 shows the Spearman?s rank correlation of
several other measures of similarity and relatedness
in WordNet::Similarity with the gold standards dis-
cussed above. The WordNet::Similarity vector relat-
edness measure achieves the highest correlation, fol-
lowed closely by the adapted lesk measure. These
results are consistent with previous findings (Pat-
wardhan and Pedersen, 2006). This table also shows
results for several path?based measures.4
Table 2 shows the correlation of jcn, res, and lin
when Information Content is derived from 1) the
sense-tagged version of SemCor (semcor), 2) Sem-
Cor without sense tags (semcor-raw), and 3) steadily
increasing subsets of the 133 million word xie por-
tion of the English GigaWord corpus. These sub-
sets start with the entire first month of xie (199501,
from January 1995) and then two months (199501-
02), three months (199501-03), up through all of
1995 (199501-12). Thereafter the increments are an-
nual, with two years of data (1995-1996), then three
(1995-1997), and so on until the entire xie corpus is
used (1995-2001). The afe, apw, and nyt portions of
GigaWord are also used individually and then com-
bined all together along with xie (all).
4wup is the Wu & Palmer measure, lch is the Leacock &
Chodorow measure, path relies on edge counting, and random
provides a simple sanity check.
The size (in tokens) of each corpus is shown in the
second column of Table 2 (size), which is expressed
in thousands (k), millions (m), and billions (b).
The third column (cover) shows what percentage
of the 96,000 noun and verb synsets in WordNet re-
ceive a non-zero frequency count when Information
Content is derived from the specified corpus. These
values show that the 226,000 sense?tagged instances
in SemCor cover about 24%, and the untagged ver-
sion of SemCor covers 37%. As it happens the cor-
relation results for semcor-raw are somewhat better
than semcor, suggesting that coverage is at least as
important (if not more so) to the performance of In-
formation Content measures than accurate mapping
of words to concepts.
A similar pattern can be seen with the xie results
in Table 2. This again shows that an increase in
WordNet coverage is associated with increased per-
formance of the Information Content measures. As
coverage increases the correlation improves, and in
fact the results are better than the path?based mea-
sures and approach those of lesk and vector (see Ta-
ble 1). The one exception is with respect to the WS
gold standard, where vector and lesk perform much
better than the Information Content measures. How-
ever, this seems reasonable since they are related-
ness measures, and the WS corpus is annotated for
relatedness rather than similarity.
As a final test of the hypothesis that coverage
matters as much or more than accurate mapping of
words to concepts, a simple baseline method was
created that assigns each synset a count of 1, and
then propagates that count up to the ancestor con-
cepts. This is equivalent to doing add-1 smoothing
without any text (add1only). This results in corre-
lation nearly as high as the best results with xie and
semcor-raw, and is significantly better than semcor.
5 Conclusions
This paper shows that semantic similarity mea-
sures based on Information Content can be signif-
icantly improved by increasing the coverage of the
frequency counts used to derive Information Con-
tent. Increased coverage can come from unannotated
text or simply assigning counts to every concept in
WordNet and does not require sense?tagged text.
331
Table 2: Rank Correlation of Information Content Measures From Different Corpora
jcn lin res
corpus size cover WS MC RG WS MC RG WS MC RG
semcor 226 k .24 .21 .72 .51 .30 .73 .58 .38 .74 .69
semcor-raw 670 k .37 .26 .82 .58 .32 .79 .65 .38 .76 .70
xie:
199501 1.2 m .35 .35 .78 .57 .37 .75 .63 .37 .73 .68
199501-02 2.3 m .39 .31 .79 .65 .32 .75 .67 .36 .73 .68
199501-03 3.8 m .42 .34 .88 .69 .34 .81 .70 .37 .75 .69
199501-06 7.9 m .46 .36 .88 .69 .36 .81 .70 .37 .75 .69
199501-09 12 m .49 .36 .88 .69 .36 .81 .70 .37 .75 .69
199501-12 16 m .51 .37 .87 .73 .36 .81 .71 .37 .75 .69
1995-1996 34 m .56 .37 .88 .73 .36 .81 .72 .37 .75 .69
1995-1997 53 m .58 .37 .88 .73 .36 .81 .71 .37 .75 .69
1995-1998 73 m .60 .37 .89 .73 .36 .81 .72 .37 .75 .69
1995-1999 94 m .62 .36 .88 .73 .36 .81 .72 .37 .76 .69
1995-2000 115 m .63 .36 .89 .73 .36 .81 .71 .37 .76 .70
1995-2001 133 m .64 .36 .88 .73 .36 .81 .71 .37 .76 .70
afe 174 m .66 .36 .88 .81 .36 .80 .78 .37 .77 .79
apw 560 m .75 .36 .84 .78 .36 .79 .78 .37 .76 .79
nyt 963 m .83 .36 .84 .78 .36 .79 .77 .37 .77 .80
all 1.8 b .85 .34 .85 .79 .35 .80 .78 .37 .77 .79
add1only 96 k 1.00 .36 .85 .73 .37 .77 .73 .39 .76 .70
Acknowledgements
Many thanks to Siddharth Patwardhan and Jason
Michelizzi for their exceptional work on Word-
Net::Similarity over the years, which has made this
and a great deal of other research possible.
References
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. ACM
Transactions on Information Systems, 20(1):116?131.
J. Jiang and D. Conrath. 1997. Semantic similarity based
on corpus statistics and lexical taxonomy. In Proceed-
ings on International Conference on Research in Com-
putational Linguistics, pages 19?33, Taiwan.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the International Con-
ference on Machine Learning, Madison, August.
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
G.A. Miller, C. Leacock, R. Tengi, and R. Bunker. 1993.
A semantic concordance. In Proceedings of the Work-
shop on Human Language Technology, pages 303?
308.
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based Context Vectors to Estimate the Semantic Relat-
edness of Concepts. In Proceedings of the EACL 2006
Workshop on Making Sense of Sense: Bringing Com-
putational Linguistics and Psycholinguistics Together,
pages 1?8, Trento, Italy, April.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
Wordnet::Similarity - Measuring the relatedness of
concepts. In Proceedings of Fifth Annual Meeting
of the North American Chapter of the Association for
Computational Linguistics, pages 38?41, Boston, MA.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In Proceedings of
the 14th International Joint Conference on Artificial
Intelligence, pages 448?453, Montreal, August.
P. Resnik. 1999. Semantic similarity in a taxonomy: An
information-based measure and its application to prob-
lems of ambiguity in natural language. Journal of Ar-
tificial Intelligence Research, 11:95?130.
H. Rubenstein and J.B. Goodenough. 1965. Contextual
correlates of synonymy. Computational Linguistics,
8:627?633.
332
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 28?31,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
UMLS::Similarity: Measuring the Relatedness
and Similarity of Biomedical Concepts
Bridget T. McInnes? & Ying Liu
Minnesota Supercomputing Institute
University of Minnesota
Minneapolis, MN 55455
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
Genevieve B. Melton
Institute for Health Informatics
University of Minnesota
Minneapolis, MN 55455
Serguei V. Pakhomov
College of Pharmacy
University of Minnesota
Minneapolis, MN 55455
Abstract
UMLS::Similarity is freely available open
source software that allows a user to mea-
sure the semantic similarity or relatedness of
biomedical terms found in the Unified Medi-
cal Language System (UMLS). It is written in
Perl and can be used via a command line in-
terface, an API, or a Web interface.
1 Introduction
UMLS::Similarity1 implements a number of seman-
tic similarity and relatedness measures that are based
on the structure and content of the Unified Medical
Language System. The UMLS is a data warehouse
that provides a unified view of many medical termi-
nologies, ontologies and other lexical resources, and
is also freely available from the National Library of
Medicine.2
Measures of semantic similarity quantify the de-
gree to which two terms are similar based on their
proximity in an is-a hierarchy. These measures are
often based on the distance between the two con-
cepts and their common ancestor. For example, lung
disease and Goodpasture?s Syndrome share the con-
cept disease as a common ancestor. Or in general
English, scalpel and switchblade would be consid-
ered very similar since both are nearby descendents
of the concept knife.
However, concepts that are not technically similar
can still be very closely related. For example, Good-
pasture?s Syndrome and Doxycycline are not similar
?Contact author : bthomson@umn.edu.
1http://umls-similarity.sourceforge.net
2http://www.nlm.nih.gov/research/umls/
since they do not have a nearby common ancestor,
but they are very closely related since Doxycycline
is a possible treatment for Goodpasture?s Syndrome.
A more general example might be elbow and arm,
while they are not similar, an elbow is a part-of an
arm and is therefore very closely related. Measures
of relatedness quantify these types of relationships
by using information beyond that which is found
in an is-a hierarchy, which the UMLS contains in
abundance.
2 Related Work
Measures of semantic similarity and relatedness
have been used in a number of different biomedi-
cal and clinical applications. Early work relied on
the Gene Ontology (GO)3, which is a hierarchy of
terms used to describe genomic information. For
example, (Lord et al, 2003) measured the similar-
ity of gene sequence data and used this in an appli-
cation for conducting semantic searches of textual
resources. (Guo et al, 2006) used semantic simi-
larity measures to identify direct and indirect pro-
tein interactions within human regulatory pathways.
(Ne?ve?ol et al, 2006) used semantic similarity mea-
sures based on MeSH (Medical Subject Headings)4
to evaluate automatic indexing of biomedical arti-
cles by measuring the similarity between their rec-
ommended terms and the gold standard index terms.
UMLS::Similarity was first released in 2009, and
since that time has been used in various different
applications. (Sahay and Ram, 2010) used it in a
3http://www.geneontology.org/
4http://www.ncbi.nlm.nih.gov/mesh
28
health information search and recommendation sys-
tem. (Zhang et al, 2011) used the measures to
identify redundancy within clinical records, while
(Mathur and Dinakarpandian, 2011) used them to
help identify similar diseases. UMLS::Similarity
has also enabled the development and evaluation
of new measures by allowing them to be compared
to existing methods, e.g., (Pivovarov and Elhadad,
2012). Finally, UMLS::Similarity can serve as a
building block in other NLP systems, for exam-
ple UMLS::SenseRelate (McInnes et al, 2011) is a
word sense disambiguation system for medical text
based on semantic similarity and relatedness.
3 UMLS::Similarity
UMLS::Similarity is a descendent of Word-
Net::Similarity (Pedersen et al, 2004), which
implements various measures of similarity and
relatedness for WordNet.5 However, the structure,
nature, and size of the UMLS is quite different from
WordNet, and the adaptations from WordNet were
not always straightforward. One very significant
difference, for example, is that the UMLS is stored
in a MySQL database while WordNet has its own
customized storage format. As a result, the core
of UMLS::Similarity is different and offers a
great deal of functionality specific to the UMLS.
Table 1 lists the measures currently provided in
UMLS::Similarity (as of version 1.27).
The Web interface provides a subset of the func-
tionality offered by the API and command line inter-
face, and allows a user to utilize UMLS::Similarity
without requiring the installation of the UMLS
(which is an admittedly time?consuming process).
4 Unified Medical Language System
The UMLS is a data warehouse that includes over
100 different biomedical and clinical data resources.
One of the largest individual sources is the System-
atized Nomenclature of Medicine?Clinical Terms
(SNOMED CT), a comprehensive terminology cre-
ated for the electronic exchange of clinical health in-
formation. Perhaps the most fine?grained source is
the Foundational Model of Anatomy (FMA), an on-
tology created for biomedical and clinical research.
One of the most popular sources is MeSH (MSH), a
5http://wordnet.princeton.edu/
Table 1: UMLS::Similarity Measures
Type Citation Name
Similarity
(Rada et al, 1989) path
(Caviedes and Cimino, 2004) cdist
(Wu and Palmer, 1994) wup
(Leacock and Chodorow, 1998) lch
(Nguyen and Al-Mubaid, 2006) nam
(Zhong et al, 2002) zhong
(Resnik, 1995) res
(Lin, 1998) lin
(Jiang and Conrath, 1997) jcn
Relatedness
(Banerjee and Pedersen, 2003) lesk
(Patwardhan and Pedersen, 2006) vector
terminology that is used for indexing medical jour-
nal articles in PubMed.
These many different resources are semi-
automatically combined into the Metathesaurus,
which provides a unified view of nearly 3,000,000
different concepts. This is very important since the
same concept can exist in multiple different sources.
For example, the concept Autonomic nerve exists in
both SNOMED CT and FMA. The Metathesaurus
assigns synonymous concepts from multiple sources
a single Concept Unique Identifier (CUI). Thus
both Autonomic nerve concepts in SNOMED CT
and FMA are assigned the same CUI (C0206250).
These shared CUIs essentially merge multiple
sources into a single resource in the Metathesaurus.
Some sources in the Metathesaurus contain addi-
tional information about the concept such as syn-
onyms, definitions,6 and related concepts. Paren-
t/child (PAR/CHD) and broader/narrower (RB/RN)
are the main types of hierarchical relations between
concepts in the Metathesaurus. Parent/child rela-
tions are already defined in the sources before they
are integrated into the UMLS, whereas broader/-
narrower relations are added by the UMLS edi-
tors. For example, Splanchnic nerve has an is-a
relation with Autonomic nerve in FMA. This re-
lation is carried forward in the Metathesaurus by
creating a parent/child relation between the CUIs
C0037991 [Splanchnic nerve] and C0206250 [Au-
tonomic nerve].
6However, not all concepts in the UMLS have a definition.
29
Table 2: Similarity scores for finger and arm
Source Relations CUIs path cdist wup lch nam zhong res lin jcn
FMA PAR/CHD 82,071 0.14 0.14 0.69 1.84 0.15 0.06 0.82 0.34 0.35
SNOMED CT PAR/CHD 321,357 0.20 0.20 0.73 2.45 0.15 0.16 2.16 0.62 0.48
MSH PAR/CHD 26,685 0.25 0.25 0.76 2.30 0.18 0.19 2.03 0.68 0.55
5 Demonstration System
The UMLS::Similarity Web interface7 allows a user
to enter two terms or UMLS CUIs as input in term
boxes. The user can choose to calculate similarity or
relatedness by clicking on the Calculate Similarity
or Calculate Relatedness button. The user can also
choose which UMLS sources and relations should
be used in the calculation. For example, if the terms
finger and arm are entered and the Compute Simi-
larity button is pressed, the following is output:
View D e f i n i t i o n s
View S h o r t e s t Pa th
R e s u l t s :
The s i m i l a r i t y o f f i n g e r
( C0016129 ) and arm ( C0446516 )
u s i n g Pa th Length ( p a t h ) i s
0 . 2 5 .
Using :
SAB : : i n c l u d e MSH
REL : : i n c l u d e PAR/CHD
The Results show the terms and their assigned
CUIs. If a term has multiple possible CUIs associ-
ated with it, UMLS::Similarity returns the CUI pair
that obtained the highest similarity score. In this
case, finger was assigned CUI C0016129 and arm
assigned CUI C0449516 and the resulting similarity
score for the path measure using the MeSH hierar-
chy was 0.25.
Additionally, the paths between the concepts and
their definitions are shown. The View Definitions
and View Shortest Path buttons show the definition
and shortest path between the concepts in a sepa-
rate window. In the example above, the shortest path
between finger (C0016129) and arm (C0446516) is
C0016129 (Finger, NOS) => C0018563 (Hand,
NOS) => C1140618 (Extremity, Upper) =>
7http://atlas.ahc.umn.edu/
C0446516 (Upper arm), and one of the definitions
shown for arm (C0446516) is The superior part
of the upper extremity between the shoulder and
the elbow.
SAB :: include and REL :: include are config-
uration parameters that define the sources and rela-
tions used to find the paths between the two CUIs
when measuring similarity. In the example above,
similarity was calculated using PAR/CHD relations
in the MeSH hierarchy.
All similarity measures default to the use of
MeSH as the source (SAB) with PAR/CHD rela-
tions. While these are reasonable defaults, for many
use cases these should be changed. Table 2 shows
the similarity scores returned for each measure us-
ing different sources. It also shows the number of
CUIs connected via PAR/CHD relations per source.
A similar view is displayed when pressing the
Compute Relatedness button:
View D e f i n i t i o n s
View S h o r t e s t Pa th
R e s u l t s :
The r e l a t e d n e s s o f f i n g e r
( C0016129 ) and arm ( C0446516 )
u s i n g Vec to r Measure ( v e c t o r )
i s 0 . 5 5 1 3 .
Using :
SABDEF : : i n c l u d e
UMLS ALL
RELDEF : : i n c l u d e
CUI /PAR/CHD/RB/RN
Relatedness measures differ from similarity in
their use of the SABDEF and RELDEF parameters.
SABDEF :: include andRELDEF :: include define
the source(s) and relation(s) used to extract defini-
tions for the relatedness measures. In this example,
the definitions come from any source in the UMLS
and include not only the definition of the concept but
30
Table 3: Relatedness scores for finger and arm
Source Relations lesk vector
UMLS ALLCUI/PAR/CHD/RB/RN10,607 0.55
UMLS ALLCUI 39 0.05
also the definition of its PAR/CHD and RB/RN rela-
tions. Table 3 shows the relatedness scores returned
for each of the relatedness measures using just the
concept?s definition (CUI) from all of the sources in
the UMLS (UMLS ALL) and when the definitions
are extended to include the definitions of the con-
cept?s PAR/CHD and RB/RN relations.
6 Acknowledgments
This work was supported by the National Insti-
tute of Health, National Library of Medicine Grant
#R01LM009623-01. It was carried out in part using
computing resources at the University of Minnesota
Supercomputing Institute.
The results reported here are based on the
2012AA version of the UMLS and were computed
using version 1.23 of UMLS::Similarity and version
1.27 of UMLS::Interface.
References
S. Banerjee and T. Pedersen. 2003. Extended gloss over-
laps as a measure of semantic relatedness. In Proceed-
ings of the Eighteenth International Joint Conference
on Artificial Intelligence, pages 805?810, Acapulco,
August.
J.E. Caviedes and J.J. Cimino. 2004. Towards the devel-
opment of a conceptual distance metric for the umls.
Journal of Biomedical Informatics, 37(2):77?85.
X. Guo, R. Liu, C.D. Shriver, H. Hu, and M.N. Lieb-
man. 2006. Assessing semantic similarity measures
for the characterization of human regulatory pathways.
Bioinformatics, 22(8):967?973.
J. Jiang and D. Conrath. 1997. Semantic similarity based
on corpus statistics and lexical taxonomy. In Proceed-
ings on Intl Conf on Research in CL, pages pp. 19?33.
C. Leacock and M. Chodorow. 1998. Combining local
context and WordNet similarity for word sense iden-
tification. WordNet: An electronic lexical database,
49(2):265?283.
D. Lin. 1998. An information-theoretic definition of
similarity. In Intl Conf ML Proc., pages 296?304.
PW Lord, RD Stevens, A. Brass, and CA Goble. 2003.
Semantic similarity measures as tools for exploring the
gene ontology. In Pacific Symposium on Biocomput-
ing, volume 8, pages 601?612.
S. Mathur and D. Dinakarpandian. 2011. Finding dis-
ease similarity based on implicit semantic similarity.
Journal of Biomedical Informatics, 45(2):363?371.
B.T. McInnes, T. Pedersen, Y. Liu, S. Pakhomov, and
G. Melton. 2011. Knowledge-based method for deter-
mining the meaning of ambiguous biomedical terms
using information content measures of similarity. In
Proceedings of the Annual Symposium of the Ameri-
canMedical Informatics Association, pages 895 ? 904,
Washington, DC.
A. Ne?ve?ol, K. Zeng, and O. Bodenreider. 2006. Besides
Precision & Recall: ExploringAlternative Approaches
to Evaluating an Automatic Indexing Tool for MED-
LINE. In AMIA Annu Symp Proc., page 589.
H.A. Nguyen and H. Al-Mubaid. 2006. New ontology-
based semantic similarity measure for the biomedical
domain. In Proc of the IEEE Intl Conf on Granular
Computing, pages 623?628.
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based Context Vectors to Estimate the Semantic Relat-
edness of Concepts. In Proc of the EACL 2006 Work-
shop Making Sense of Sense, pages 1?8.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet::Similarity - Measuring the Relatedness of
Concepts. In The Annual Meeting of the HLT and
NAACL: Demonstration Papers, pages 38?41.
R. Pivovarov and N. Elhadad. 2012. A hybrid
knowledge-based and data-driven approach to iden-
tifying semantically similar concepts. Journal of
Biomedical Informatics, 45(3):471?481.
R. Rada, H. Mili, E. Bicknell, and M. Blettner. 1989.
Development and application of a metric on semantic
nets. IEEE Transactions on Systems, Man, and Cyber-
netics, 19(1):17?30.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In Proceedings of
the 14th Intl Joint Conf on AI, pages 448?453.
S. Sahay and A. Ram. 2010. Socio-semantic health in-
formation access. In Proceedings of the AAAI Spring
Symposium on AI and Health Communication.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexical
selection. In Proceedings of the 32nd Meeting of ACL,
pages 133?138, Las Cruces, NM, June.
R. Zhang, S. Pakhomov, B.T. McInnes, and G.B. Melton.
2011. Evaluating measures of redundancy in clinical
texts. In AMIA Annual Symposium Proceedings, vol-
ume 2011, page 1612.
J. Zhong, H. Zhu, J. Li, and Y. Yu. 2002. Concep-
tual graph matching for semantic search. Proceedings
of the 10th International Conference on Conceptual
Structures, pages 92?106.
31
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1691?1701,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Offspring from Reproduction Problems:
What Replication Failure Teaches Us
Antske Fokkens and Marieke van Erp
The Network Institute
VU University Amsterdam
Amsterdam, The Netherlands
{a.s.fokkens,m.g.j.van.erp}@vu.nl
Marten Postma
Utrecht University
Utrecht, The Netherlands
martenp@gmail.com
Ted Pedersen
Dept. of Computer Science
University of Minnesota
Duluth, MN 55812 USA
tpederse@d.umn.edu
Piek Vossen
The Network Institute
VU University Amsterdam
Amsterdam, The Netherlands
piek.vossen@vu.nl
Nuno Freire
The European Library
The Hague, The Netherlands
nfreire@gmail.com
Abstract
Repeating experiments is an important in-
strument in the scientific toolbox to vali-
date previous work and build upon exist-
ing work. We present two concrete use
cases involving key techniques in the NLP
domain for which we show that reproduc-
ing results is still difficult. We show that
the deviation that can be found in repro-
duction efforts leads to questions about
how our results should be interpreted.
Moreover, investigating these deviations
provides new insights and a deeper under-
standing of the examined techniques. We
identify five aspects that can influence the
outcomes of experiments that are typically
not addressed in research papers. Our use
cases show that these aspects may change
the answer to research questions leading
us to conclude that more care should be
taken in interpreting our results and more
research involving systematic testing of
methods is required in our field.
1 Introduction
Research is a collaborative effort to increase
knowledge. While it includes validating previous
approaches, our experience is that most research
output in our field focuses on presenting new ap-
proaches, and to a somewhat lesser extent building
upon existing work.
In this paper, we argue that the value of research
that attempts to replicate previous approaches goes
beyond simply validating what is already known.
It is also an essential aspect for building upon
existing approaches. Especially when validation
fails or variations in results are found, systematic
testing helps to obtain a clearer picture of both the
approach itself and of the meaning of state-of-the-
art results leading to a better insight into the qual-
ity of new approaches in relation to previous work.
We support our claims by presenting two use
cases that aim to reproduce results of previous
work in two key NLP technologies: measuring
WordNet similarity and Named Entity Recogni-
tion (NER). Besides highlighting the difficulty of
repeating other researchers? work, new insights
about the approaches emerged that were not pre-
sented in the original papers. This last point shows
that reproducing results is not merely part of good
practice in science, but also an essential part in
gaining a better understanding of the methods we
use. Likewise, the problems we face in reproduc-
ing previous results are not merely frustrating in-
conveniences, but also pointers to research ques-
tions that deserve deeper investigation.
We investigated five aspects that cause exper-
imental variation that are not typically described
in publications: preprocessing (e.g. tokenisa-
tion), experimental setup (e.g. splitting data for
cross-validation), versioning (e.g. which version
of WordNet), system output (e.g. the exact fea-
tures used for individual tokens in NER), and sys-
tem variation (e.g. treatment of ties).
As such, reproduction provides a platform for
systematically testing individual aspects of an ap-
proach that contribute to a given result. What is
the influence of the size of the dataset, for exam-
ple? How does using a different dataset affect the
results? What is a reasonable divergence between
different runs of the same experiment? Finding
answers to these questions enables us to better in-
terpret our state-of-the-art results.
1691
Moreover, the experiments in this paper show
that even while strictly trying to replicate a pre-
vious experiment, results may vary up to a point
where they lead to different answers to the main
question addressed by the experiment. The Word-
Net similarity experiment use case compares the
performance of different similarity measures. We
will show that the answer as to which measure
works best changes depending on factors such as
the gold standard used, the strategy towards part-
of-speech or the ranking coefficient, all aspects
that are typically not addressed in the literature.
The main contributions of this paper are the
following:
1) An in-depth analysis of two reproduction use
cases in NLP
2) New insights into the state-of-the-art results
for WordNet similarities and NER, found because
of problems in reproducing prior research
3) A categorisation of aspects influencing
reproduction of experiments and suggestions on
testing their influence systematically
The code, data and experimental setup
for the WordNet experiments are avail-
able at http://github.com/antske/
WordNetSimilarity, and for the NER exper-
iments at http://github.com/Mvanerp/
NER. The experiments presented in this paper
have been repeated by colleagues not involved in
the development of the software using the code
included in these repositories. The remainder of
this paper is structured as follows. In Section 2,
previous work is discussed. Sections 3 and 4
describe our real-world use cases. In Section 5,
we present our observations, followed by a more
general discussion in Section 6. In Section 7, we
present our conclusions.
2 Background
This section provides a brief overview of recent
work addressing reproduction and benchmark re-
sults in computer science related studies and dis-
cusses how our research fits in the overall picture.
Most researchers agree that validating results
entails that a method should lead to the same over-
all conclusions rather than producing the exact
same numbers (Drummond, 2009; Dalle, 2012;
Buchert and Nussbaum, 2012, etc.). In other
words, we should strive to reproduce the same an-
swer to a research question by different means,
perhaps by re-implementing an algorithm or eval-
uating it on a new (in domain) data set. Replica-
tion has a somewhat more limited aim, and simply
involves running the exact same system under the
same conditions in order to get the exact same re-
sults as output.
According to Drummond (2009) replication is
not interesting, since it does not lead to new in-
sights. On this point we disagree with Drum-
mond (2009) as replication allows us to: 1) vali-
date prior research, 2) improve on prior research
without having to rebuild software from scratch,
and 3) compare results of reimplementations and
obtain the necessary insights to perform reproduc-
tion experiments. The outcome of our use cases
confirms the statement that deeper insights into an
approach can be obtained when all resources are
available, an observation also made by Ince et al
(2012).
Even if exact replication is not a goal many
strive for, Ince et al (2012) argue that insightful
reproduction can be an (almost) impossible un-
dertaking without the source code being available.
Moreover, it is not always clear where replication
stops and reproduction begins. Dalle (2012) dis-
tinguishes levels of reproducing results related to
how close they are to the original work and how
each contributes to research. In general, an in-
creasing awareness of the importance of reproduc-
tion research and open code and data can be ob-
served based on publications in high-profile jour-
nals (e.g. Nature (Ince et al, 2012)) and initiatives
such as myExperiment.1
Howison and Herbsleb (2013) point out that,
even though this is important, often not enough
(academic) credit is gained from making resources
available. What is worse, the same holds for re-
search that investigates existing methods rather
than introducing new ones, as illustrated by the
question that is found on many review forms ?how
novel is the presented approach??. On the other
hand, initiatives for journals addressing exactly
this issue (Neylon et al, 2012) and tracks focus-
ing on results verification at conferences such as
VLDB2 show that this opinion is not universal.
A handful of use cases on reproducing or repli-
cating results have been published. Louridas and
Gousios (2012) present a use case revealing that
source code alone is not enough for reproducing
1http://www.myexperiment.org
2http://www.vldb.org/2013/
1692
results, a point that is also made by Mende (2010)
who provides an overview of all information re-
quired to replicate results.
The experiments in this paper provide use cases
that confirm the points brought out in the litera-
ture mentioned above. This includes both obser-
vations that a detailed level of information is re-
quired for truly insightful reproduction research as
well as the claim that such research leads to better
understanding of our techniques. Furthermore, the
work in this paper relates to Bikel (2004)?s work.
He provides all information needed in addition to
Collins (1999) to replicate Collins? benchmark re-
sults. Our work is similar in that we also aim to fill
in the blanks needed to replicate results. It must
be noted, however, that the use cases in this paper
have a significantly smaller scale than Bikel?s.
Our research distinguishes itself from previous
work, because it links the challenges of reproduc-
tion to what they mean for reported results be-
yond validation. Ruml (2010) mentions variations
in outcome as a reason not to emphasise compar-
isons to benchmarks. Vanschoren et al (2012)
propose to use experimental databases to system-
atically test variations for machine learning, but
neither links the two issues together. Raeder et al
(2010) come closest to our work in a critical study
on the evaluation of machine learning. They show
that choices in the methodology, such as data sets,
evaluation metrics and type of cross-validation can
influence the conclusions of an experiment, as we
also find in our second use case. However, they
focus on the problem of evaluation and recom-
mendations on how to achieve consistent repro-
ducible results. Our contribution is to investigate
how much results vary. We cannot control how
fellow researchers carry out their evaluation, but
if we have an idea of the variations that typically
occur within a system, we can better compare ap-
proaches for which not all details are known.
3 WordNet Similarity Measures
Patwardhan and Pedersen (2006) and Pedersen
(2010) present studies where the output of a va-
riety of WordNet similarity and relatedness mea-
sures are compared. They rank Miller and Charles
(1991)?s set (henceforth ?mc-set?) of 30 word
pairs according to their semantic relatedness with
several WordNet similarity measures.
Each measure ranks the mc-set of word pairs
and these outputs are compared to Miller and
Charles (1991)?s gold standard based on human
rankings using the Spearman?s Correlation Coeffi-
cient (Spearman, 1904, ?). Pedersen (2010) also
ranks the original set of 65 word pairs ranked
by humans in an experiment by Rubenstein and
Goodenough (1965) (rg-set) which is a superset of
Miller and Charles?s set.
3.1 Replication Attempts
This research emerged from a project run-
ning a similar experiment for Dutch on Cor-
netto (Vossen et al, 2013). First, an attempt
was made to reproduce the results reported in
Patwardhan and Pedersen (2006) and Peder-
sen (2010) on the English WordNet using their
WordNet::Similarity web-interface.3 Results dif-
fered from those reported in the aforementioned
works, even when using the same versions as
the original, WordNet::Similarity-1.02 and Word-
Net 2.1 (Patwardhan and Pedersen, 2006) and
WordNet::Similarity-2.05 and WordNet 3.0 (Ped-
ersen, 2010), respectively.4
The fact that results of similarity measures on
WordNet can differ even while the same software
and same versions are used indicates that proper-
ties which are not addressed in the literature may
influence the output of similarity measures. We
therefore conducted a range of experiments that,
in addition to searching for the right settings to
replicate results of previous research, address the
following questions:
1) Which properties have an impact on the per-
formance of WordNet similarity measures?
2) How much does the performance of individ-
ual measures vary?
3) How do commonly used measures compare
when the variation of their performance are taken
into account?
3.2 Methodology and first observations
The questions above were addressed in two stages.
In the first stage, Fokkens, who was not involved
in the first replication attempt implemented a
script to calculate similarity measures using Word-
Net::Similarity. This included similarity mea-
sures introduced by Wu and Palmer (1994) (wup),
3Obtained from http://talisker.d.umn.edu/
cgi-bin/similarity/similarity.cgi, Word-
Net::Similarity version 2.05. This web interface has now
moved to http://maraca.d.umn.edu
4WordNet::Similarity were obtained http://
search.cpan.org/dist/WordNet-Similarity/.
1693
Leacock and Chodorow (1998) (lch), Resnik
(1995) (res), Jiang and Conrath (1997) (jcn),
Lin (1998) (lin), Banerjee and Pedersen (2003)
(lesk), Hirst and St-Onge (1998) (hso) and
Patwardhan and Pedersen (2006) (vector and
vpairs) respectively.
Consequently, settings and properties were
changed systematically and shared with Pedersen
who attempted to produce the new results with his
own implementations. First, we made sure that
the script implemented by Fokkens could produce
the same WordNet similarity scores for each in-
dividual word pair as those used to calculate the
ranking on the mc-set by Pedersen (2010). Finally,
the gold standard and exact implementation of the
Spearman ranking coefficient were compared.
Differences in results turned out to be related
to variations in the experimental setup. First,
we made different assumptions on the restriction
of part-of-speech tags (henceforth ?PoS-tag?) con-
sidered in the comparison. Miller and Charles
(1991) do not discuss how they deal with words
with more than one PoS-tag in their study. Ped-
ersen therefore included all senses with any PoS-
tag in his study. The first replication attempt had
restricted PoS-tags to nouns based on the idea
that most items are nouns and subjects would be
primed to primarily think of the noun senses. Both
assumptions are reasonable. Pos-tags were not re-
stricted in the second replication attempt, but be-
cause of a bug in the code only the first identified
PoS-tag (?noun? in all cases) was considered. We
therefore mistakenly assumed that PoS-tag restric-
tions did not matter until we compared individual
scores between Pedersen and the replication at-
tempts.
Second, there are two gold standards for the
Miller and Charles (1991) set: one has the scores
assigned during the original experiment run by
Rubenstein and Goodenough (1965), the other
has the scores assigned during Miller and Charles
(1991)?s own experiment. The ranking correlation
between the two sets is high, but they are not iden-
tical. Again, there is no reason why one gold stan-
dard would be a better choice than the other, but in
order to replicate results, it must be known which
of the two was used. Third, results changed be-
cause of differences in the treatment of ties while
calculating Spearman ?. The influence of the ex-
act gold standard and calculation of Spearman ?
could only be found because Pedersen could pro-
measure Spearman ? Kendall ? ranking
min max min max variation
path based similarity
path 0.70 0.78 0.55 0.62 1-8
wup 0.70 0.79 0.53 0.61 1-6
lch 0.70 0.78 0.55 0.62 1-7
path based information content
res 0.65 0.75 0.26 0.57 4-11
lin 0.49 0.73 0.36 0.53 6-10
jcn 0.46 0.73 0.32 0.55 5, 7-11
path based relatedness
hso 0.73 0.80 0.36 0.41 1-3,5-10
dictionary and corpus based relatedness
vpairs 0.40 0.70 0.26 0.50 7-11
vector 0.48 0.92 0.33 0.76 1,2,4,6-11
lesk 0.66 0.83 -0.02 0.61 1-8,11,12
Table 1: Variation WordNet measures? results
vide the output of the similarity measures he used
to calculate the coefficient. It is unlikely we would
have been able to replicate his results at all with-
out the output of this intermediate step. Finally,
results for lch, lesk and wup changed accord-
ing to measure specific configuration settings such
as including a PoS-tag specific root node or turn-
ing on normalisation.
In the second stage of this research, we ran ex-
periments that systematically manipulate the influ-
ential factors described above. In this experiment,
we included both the mc-set and the complete rg-
set. The implementation of Spearman ? used in
Pedersen (2010) assigned the lowest number in
ranking to ties rather than the mean, resulting in
an unjustified drop in results for scores that lead
to many ties. We therefore experimented with a
different correlation measure, Kendall tau coeffi-
cient (Kendall, 1938, ? ) rather than two versions
of Spearman ?.
3.3 Variation per measure
All measures varied in their performance.
The complete outcome of our experiments
(both the similarity measures assigned to
each pair as well as the output of the ranking
coefficients) are included in the data set pro-
vided at http://github.com/antske/
WordNetSimilarity. Table 1 presents an
overview of the main point we wish to make
through this experiment: the minimal and maxi-
mal results according to both ranking coefficients.
Results for similarity measures varied from 0.06-
0.42 points for Spearman ? and from 0.05-0.60
points for Kendall ? . The last column indi-
cates the variation of performance of a measure
1694
compared to the other measures, where 1 is the
best performing measure and 12 is the worst.5
For instance, path has been best performing
measure, second best, eighth best and all positions
in between, vector has ranked first, second and
fourth, but also occupied all positions from six to
eleven.
In principle, it is to be expected that num-
bers are not exactly the same while evaluating
against a different data set (the mc-set versus the
rg-set), taking a different set of synsets to evalu-
ate on (changing PoS-tag restrictions) or changing
configuration settings that influence the similarity
score. However, a variation of up to 0.44 points
in Spearman ? and 0.60 in Kendall ? 6 leads to
the question of how indicative these results really
are. A more serious problem is the fact that the
comparative performance of individual measure
changes. Which measure performs best depends
on the evaluation set, ranking coefficient, PoS-tag
restrictions and configuration settings. This means
that the answer to the question of which similarity
measure is best to mimic human similarity scores
depends on aspects that are often not even men-
tioned, let alne systematically compared.
3.4 Variation per category
For each influential category of experimental vari-
ation, we compared the variation in Spearman ?
and Kendall ? , while similarity measure and other
influential categories were kept stable. The cat-
egories we varied include WordNet and Word-
Net::Similarity version, the gold standard used to
evaluate, restrictions on PoS-tags, and measure
specific configurations. Table 2 presents the maxi-
mum variation found across measures for each cat-
egory. The last column indicates how often the
ranking of a specific measure changed as the cat-
egory changed, e.g. did the measure ranking third
using specific configurations, PoS-tag restrictions
and a specific gold standard using WordNet 2.1
still rank third when WordNet 3.0 was used in-
stead? The number in parentheses next to the ?dif-
ferent ranks? in the table presents the total num-
ber of scores investigated. Note that this num-
ber changes for each category, because we com-
5Some measures ranked differently as their individual
configuration settings changed. In these cases, the measure
was included in the overall ranking multiple times, which is
why there are more ranking positions than measures.
6Section 3.4 explains why the variation in Kendall is this
extreme and ? is more appropriate for this task.
Variation Maximum difference Different
Spearman ? Kendall ? rank (tot)
WN version 0.44 0.42 223 (252)
gold standard 0.24 0.21 359 (504)
PoS-tag 0.09 0.08 208 (504)
configuration 0.08 0.60 37 (90)
Table 2: Variations per category
pared two WordNet versions (WN version), three
gold standard and PoS-tag restriction variations
and configuration only for the subset of scores
where configuration matters.
There are no definite statements to make as to
which version (Patwardhan and Pedersen (2006)
vs Pedersen (2010)), PoS-tag restriction or con-
figuration gives the best results. Likewise, while
most measures do better on the smaller data set,
some achieve their highest results on the full set.
This is partially due to the fact that ranking coef-
ficients are sensitive to outliers. In several cases
where PoS-tag restrictions led to different results,
only one pair received a different score. For in-
stance, path assigns a relatively high score to
the pair chord-smile when verbs are included, be-
cause the hierarchy of verbs in WordNet is rela-
tively flat. This effect is not observed in wup and
lch which correct for the depth of the hierarchy.
On the other hand, res, lin and jcn score bet-
ter on the same set when verbs are considered, be-
cause they cannot detect any relatedness for the
pair crane-implement when restricted to nouns.
On top of the variations presented above, we no-
tice a discrepancy between the two coefficients.
Kendall ? generally leads to lower coefficiency
scores than Spearman ?. Moreover, they each
give different relative indications: where lesk
achieves its highest Spearman ?, it has an ex-
tremely low Kendall ? of 0.01. Spearman ? uses
the difference in rank as its basis to calculate a cor-
relation, where Kendall ? uses the number of items
with the correct rank. The low Kendall ? for lesk
is the result of three pairs receiving a score that is
too high. Other pairs that get a relatively accurate
score are pushed one place down in rank. Because
only items that receive the exact same rank help to
increase ? , such a shift can result in a drastic drop
in the coefficient. In our opinion, Spearman ? is
therefore preferable over Kendall ? . We included
? , because many authors do not mention the rank-
ing coefficient they use (cf. Budanitsky and Hirst
(2006), Resnik (1995)) and both ? and ? are com-
1695
monly used coefficients.
Except for WordNet, which Budanitsky and
Hirst (2006) hold accountable for minor variations
in a footnote, the influential categories we investi-
gated in this paper, to our knowledge, have not yet
been addressed in the literature. Cramer (2008)
points out that results from WordNet-Human sim-
ilarity correlations lead to scattered results report-
ing variations similar to ours, but she compares
studies using different measures, data and exper-
imental setup. This study shows that even if
the main properties are kept stable, results vary
enough to change the identity of the measure that
yields the best performance. Table 1 reveals a
wide variation in ranking relative to alternative ap-
proaches. Results in Table 2 show that it is com-
mon for the ranking of a score to change due to
variations that are not at the core of the method.
This study shows that it is far from clear how
different WordNet similarity measures relate to
each other. In fact, we do not know how we can
obtain the best results. This is particularly chal-
lenging, because the ?best results? may depend on
the intended use of the similarity scores (Meng
et al, 2013). This is also the reason why we
presented the maximum variation observed, rather
than the average or typical variation (mostly be-
low 0.10 points). The experiments presented in
this paper resulted in a vast amount of data. An
elaborate analysis of this data is needed to get a
better understanding of how measures work and
why results vary to such an extent. We leave this
investigation to future work. If there is one take-
home message from this experiment, it is that one
should experiment with parameters such as restric-
tions on PoS-tags or configurations and determine
which score to use depending on what it is used
for, rather than picking something that did best in
a study using different data for a different task and
may have used a different version of WordNet.
4 Reproducing a NER method
Freire et al (2012) describe an approach to clas-
sifying named entities in the cultural heritage do-
main. The approach is based on the assumption
that domain knowledge, encoded in complex fea-
tures, can aid a machine learning algorithm in
NER tasks when only little training data is avail-
able. These features include information about
person and organisation names, locations, as well
as PoS-tags. Additionally, some general features
are used such as a window of three preceding and
two following tokens, token length and capitalisa-
tion information. Experiments are run in a 10-fold
cross-validation setup using an open source ma-
chine learning toolkit (McCallum, 2002).
4.1 Reproducing NER Experiments
This experiment can be seen as a real-world case
of the sad tale of the Zigglebottom tagger (Peder-
sen, 2008). The (fictional) Zigglebottom tagger is
a tagger with spectacular results that looks like it
will solve some major problems in your system.
However, the code is not available and a new im-
plementation does not yield the same results. The
original authors cannot provide the necessary de-
tails to reproduce their results, because most of the
work has been done by a PhD student who has fin-
ished and moved on to something else. In the end,
the newly implemented Zigglebottom tagger is not
used, because it does not lead to the promised bet-
ter results and all effort went to waste.
Van Erp was interested in the NER approach
presented in Freire et al (2012). Unfortunately,
the code could not be made available, so she de-
cided to reimplement the approach. Despite feed-
back from Freire about particular details of the
system, results remained 20 points below those
reported in Freire et al (2012) in overall F-score
(Van Erp and Van der Meij, 2013).
The reimplementation process involved choices
about seemingly small details such as rounding
to how many decimals, how to tokenise or how
much data cleanup to perform (normalisation of
non-alphanumeric characters for example). Try-
ing different parameter combinations for feature
generation and the algorithm never yielded the ex-
act same results as Freire et al (2012). The results
of the best run in our first reproduction attempt,
together with the original results from Freire et al
(2012) are presented in Table 3. Van Erp and Van
der Meij (2013) provide an overview of the imple-
mentation efforts.
4.2 Following up from reproduction
Since the experiments in Van Erp and Van der Meij
(2013) introduce several new research questions
regarding the influence of data cleaning and the
limitations of the dataset, we performed some ad-
ditional experiments.
First, we varied the tokenisation, removing non-
alphanumeric characters from the data set. This
yielded a significantly smaller data set (10,442
1696
(Freire et al, 2012) results Van Erp and Van der Meij?s replication results
Precision Recall F?=1 Precision Recall F?=1
LOC (388) 92% 55% 69 77.80% 39.18% 52.05
ORG (157) 90% 57% 70 65.75% 30.57% 41.74
PER (614) 91% 56% 69 73.33% 37.62% 49.73
Overall (1,159) 91% 55% 69 73.33% 37.19% 49.45
Table 3: Precision, recall and F?=1 scores for the original experiments from Freire et al 2012 and our
replication of their approach as presented in Van Erp and Van der Meij (2013)
tokens vs 12,510), and a 15 point drop in over-
all F-score. Then, we investigated whether vari-
ation in the cross-validation splits made any dif-
ference as we noticed that some NEs were only
present in particular fields in the data, which can
have a significant impact on a small dataset. We
inspected the difference between different cross-
validation folds by computing the standard devi-
ations of the scores and found deviations of up
to 25 points in F-score between the 10 splits. In
the general setup, database records were randomly
distributed over the folds and cut off to balance the
fold sizes. In a different approach to dividing the
data by distributing individual sentences from the
records over the folds, performance increases by
8.57 points in overall F-score to 58.02. This is not
what was done in the original Freire et al (2012)
paper, but shows that the results obtained with this
dataset are quite fragile.
As we worried about the complexity of the fea-
ture set relative to the size of the data set, we de-
viated somewhat from Freire et al (2012)?s exper-
iments in that we switched some features on and
off. Removal of complex features pertaining to the
window around the focus token improved our re-
sults by 3.84 points in overall F-score to 53.39.
The complex features based on VIAF,7 GeoN-
ames8 and WordNet do contribute to the classifica-
tion in the Mallet setup as removing them and only
using the focus token, window and generic fea-
tures causes a slight drop in overall F-score from
49.45 to 47.25.
When training the Stanford NER system (Finkel
et al, 2005) on just the tokens from the
Freire data set and the parameters from en-
glish.all.3class.distsim.prop (included in the Stan-
ford NER release, see also Van Erp and Van der
Meij (2013)), our F-scores come very close to
those reported by Freire et al (2012), but mostly
with a higher recall and lower precision. It is puz-
zling that the Stanford system obtains such high
7http://www.viaf.org
8http://www.geonames.org
results with only very simple features, whereas
for Mallet the complex features show improve-
ment over simpler features. This leads to ques-
tions about the differences between the CRF im-
plementations and the influence of their parame-
ters, which we hope to investigate in future work.
4.3 Reproduction difficulties explained
Several reasons may be the cause of why we fail to
reproduce results. As mentioned, not all resources
and data were available for this experiment, thus
causing us to navigate in the dark as we could not
reverse-engineer intermediate steps, but only com-
pare to the final precision, recall and F-scores.
The experiments follow a general machine
learning setup consisting roughly of four steps:
preprocess data, generate features, train model and
test model. The novelty and replication problems
lie in the first three steps. How the data was pre-
processed is a major factor here. The data set con-
sisted of XML files marked up with inline named
entity tags. In order to generate machine learn-
ing features, this data has to be tokenised, possi-
bly cleaned up and the named entity markup had
to be converted to a token-based scheme. Each of
these steps can be carried out in several ways, and
choices made here can have great influence on the
rest of the pipeline.
Similar choices have to be made for prepro-
cessing external resources. From the descriptions
in the original paper, it is unclear how records
in VIAF and GeoNames were preprocessed, or
even which versions of these resources were used.
Preprocessing and calculating occurrence statis-
tics over VIAF takes 30 hours for each run. It
is thus not feasible to identify the main potential
variations without the original data to verify this
prepatory step.
Numbers had to be rounded when generating
the features, leading to the question of how many
decimals are required to be discriminative with-
out creating an overly sparse dataset. Freire recalls
that encoding features as multi-value discrete fea-
1697
tures versus several boolean features can have sig-
nificant impact. These settings are not mentioned
in the paper, making reproduction very difficult.
As the project in which the original research
was performed has ended, and there is no cen-
tral repository where such information can be re-
trieved, we are left to wonder how to reuse this
approach in order to further domain-specific NER.
5 Observations
In this section, we generalise the observations
from our use cases to the main categories that can
influence reproduction.
Despite our efforts to describe our systems as
clearly as possible, details that can make a tremen-
dous difference are often omitted in papers. It will
be no surprise to researchers in the field that pre-
processing of data can make or break an experi-
ment.
The choice of which steps we perform, and how
each of these steps is carried out exactly are part
of our experimental setup. A major difference in
the results for the NER experiments was caused by
variations in the way in which we split the data for
cross-validation.
As we fine-tune our techniques, software gets
updated, data sets are extended or annotation bugs
are fixed. In the WordNet experiment, we found
that there were two different gold standard data
sets. There are also different versions of Word-
Net, and the WordNet::Similarity packages. Sim-
ilarly for the NER experiment, GeoNames, VIAF
and Mallet are updated regularly. It is therefore
critical to pay attention to versioning.
Our experiments often consist of several differ-
ent steps whose outputs may be difficult to retrace.
In order to check the output of a reproduction ex-
periment at every step of the way, system out-
put of experiments, including intermediate steps,
is vital. The WordNet replication was only pos-
sible, because Pedersen could provide the similar-
ity scores of each word pair. This enabled us to
compare the intermediate output and identify the
source of differences in output.
Lastly, there may be inherent system variations
in the techniques used. Machine learning algo-
rithms may for instance use coin flips in case of
a tie. This was not observed in our experiments,
but such variations may be determined by running
an experiment several times and taking the average
over the different runs (cf. Raeder et al (2010)).
All together, these observations show that shar-
ing data and software play a key role in gaining in-
sight into how our methods work. Vanschoren et
al. (2012) propose a setup that allows researchers
to provide their full experimental setup, which
should include exact steps followed in preprocess-
ing the data, documentation of the experimen-
tal setup, exact versions of the software and re-
sources used and experimental output. Having
access to such a setup allows other researchers
to validate research, but also tweak the approach
to investigate system variation, systematically test
the approach in order to learn its limitations and
strengths and ultimately improve on it.
6 Discussion
Many of the aspects addressed in the previous sec-
tion such as preprocessing are typically only men-
tioned in passing, or not at all. There is often not
enough space to capture all details, and they are
generally not the core of the research described.
Still, our use cases have shown that they can have a
tremendous impact on reproduction, and can even
lead to different conclusions. This leads to serious
questions on how we can interpret our results and
how we can compare the performance of different
methods. Is an improvement of a few per cent re-
ally due to the novelty of the approach if larger
variations are found when the data is split differ-
ently? Is a method that does not quite achieve the
highest reported state-of-the-art result truly less
good? What does a state-of-the-art result mean if
it is only tested on one data set?
If one really wants to know whether a result
is better or worse than the state-of-the-art, the
range of variation within the state-of-the-art must
be known. Systematic experiments such as the
ones we carried out for WordNet similarity and
NER, can help determine this range. For results
that fall within the range, it holds that they can
only be judged by evaluations going beyond com-
paring performance numbers, i.e. an evaluation of
how the approach achieves a given result and how
that relates to alternative approaches.
Naturally, our use cases do not represent the en-
tire gamut of research methodologies and prob-
lems in the NLP community. However, they do
represent two core technologies and our observa-
tions align with previous literature on replication
and reproduction.
Despite the systematic variation we employed
1698
in our experiments, they do not answer all ques-
tions that the problems in reproduction evoked.
For the WordNet experiments, deeper analysis is
required to gain full understanding of how indi-
vidual influential aspects interact with each mea-
surement. For the NER experiments, we are yet to
identify the cause of our failure to reproduce.
The considerable time investment required for
such experiments forms a challenge. Due to pres-
sure to publish or other time limitations, they can-
not be carried out for each evaluation. There-
fore, it is important to share our experiments, so
that other researchers (or students) can take this
up. This could be stimulated by instituting repro-
duction tracks in conferences, thus rewarding sys-
tematic investigation of research approaches. It
can also be aided by adopting initiatives that en-
able authors to easily include data, code and/or
workflows with their publications such as the
PLOS/figshare collaboration.9 We already do a
similar thing for our research problems by organ-
ising challenges or shared tasks, why not extend
this to systematic testing of our approaches?
7 Conclusion
We have presented two reproduction use cases for
the NLP domain. We show that repeating other
researchers? experiments can lead to new research
questions and provide new insights into and better
understanding of the investigated techniques.
Our WordNet experiments show that the perfor-
mance of similarity measures can be influenced by
the PoS-tags considered, measure specific varia-
tions, the rank coefficient and the gold standard
used for comparison. We not only find that such
variations lead to different numbers, but also dif-
ferent rankings of the individual measures, i.e.
these aspects lead to a different answer to the
question as to which measure performs best. We
did not succeed in reproducing the NER results
of Freire et al (2012), showing the complexity
of what seems a straightforward reproduction case
based on a system description and training data
only. Our analyses show that it is still an open
question whether additional complex features im-
prove domain specific NER and that this may par-
tially depend on the CRF implementation.
Some observations go beyond our use cases. In
particular, the fact that results vary significantly
9http://blogs.plos.org/plos/2013/01/
easier-access-to-plos-data/
because of details that are not made explicit in
our publications. Systematic testing can provide
an indication of this variation. We have classi-
fied relevant aspects in five categories occurring
across subdisciplines of NLP: preprocessing, ex-
perimental setup, versioning, system output,
and system variation.
We believe that knowing the influence of differ-
ent aspects in our experimental workflow can help
increase our understanding of the robustness of
the approach at hand and will help understand the
meaning of the state-of-the-art better. Some tech-
niques are reused so often (the papers introducing
WordNet similarity measures have around 1,000-
2,000 citations each as of February 2013, for ex-
ample) that knowing their strengths and weak-
nesses is essential for optimising their use.
As mentioned many times before, sharing is key
to facilitating reuse, even if the code is imper-
fect and contains hacks and possibly bugs. In the
end, the same holds for software as for documen-
tation: it is like sex: if it is good, it is very good
and if it is bad, it is better than nothing!10 But
most of all: when reproduction fails, regardless of
whether original code or a reimplementation was
used, valuable insights can emerge from investi-
gating the cause of this failure. So don?t let your
failing reimplementations of the Zigglebottom tag-
ger collect dusk on a shelf while others reimple-
ment their own failing Zigglebottoms. As a com-
munity, we need to know where our approaches
fail, as much ?if not more? as where they succeed.
Acknowledgments
We would like to thank the anonymous review-
ers for their eye to detail and useful comments
to make this a better paper. We furthermore
thank Ruben Izquierdo, Lourens van der Meij,
Christoph Zwirello, Rebecca Dridan and the Se-
mantic Web Group at VU University for their
help and useful feedback. The research leading to
this paper was supported by the European Union?s
7th Framework Programme via the NewsReader
Project (ICT-316404), the Agora project, by NWO
CATCH programme, grant 640.004.801, and the
BiographyNed project, a joint project with Huy-
gens/ING Institute of the Dutch Academy of Sci-
ences funded by the Netherlands eScience Center
(http://esciencecenter.nl/).
10The documentation variant of this quote is attributed to
Dick Brandon.
1699
References
Stanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of the Eighteenth International Joint
Conference on Artificial Intelligence, pages 805?
810, Acapulco, August.
Daniel M. Bikel. 2004. Intricacies of Collins? parsing
model. Computational Linguistics, 30(4):479?511.
Tomasz Buchert and Lucas Nussbaum. 2012. Lever-
aging business workflows in distributed systems re-
search for the orchestration of reproducible and scal-
able experiments. In Anne Etien, editor, 9e`me
e?dition de la confe?rence MAnifestation des JE-
unes Chercheurs en Sciences et Technologies de
l?Information et de la Communication - MajecSTIC
2012 (2012).
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Phd dissertation,
University of Pennsylvania.
Irene Cramer. 2008. How well do semantic related-
ness measures perform? a meta-study. In Semantics
in Text Processing. STEP 2008 Conference Proceed-
ings, volume 1, pages 59?70.
Olivier Dalle. 2012. On reproducibility and trace-
ability of simulations. In WSC-Winter Simulation
Conference-2012.
Chris Drummond. 2009. Replicability is not repro-
ducibility: nor is it good science. In Proceedings of
the Twenty-Sixth International Conference on Ma-
chine Learning: Workshop on Evaluation Methods
for Machine Learning IV.
Jenny Finkel, Trond Grenager, and Christopher D.
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 363?370, Ann Arbor, USA.
Nuno Freire, Jose? Borbinha, and Pa?vel Calado. 2012.
An approach for named entity recognition in poorly
structured data. In Proceedings of ESWC 2012.
Graeme Hirst and David St-Onge. 1998. Lexical
chains as representations of context for the detection
and correction of malapropisms. In C. Fellbaum, ed-
itor, WordNet: An electronic lexical database, pages
305?332. MIT Press.
James Howison and James D. Herbsleb. 2013. Shar-
ing the spoils: incentives and collaboration in sci-
entific software development. In Proceedings of the
2013 conference on Computer Supported Coopera-
tive Work, pages 459?470.
Darrel C. Ince, Leslie Hatton, and John Graham-
Cumming. 2012. The case for open computer pro-
grams. Nature, 482(7386):485?488.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. In Proceedings of the International Confer-
ence on Research in Computational Linguistics (RO-
CLING X), pages 19?33, Taiwan.
Maurice Kendall. 1938. A new measure of rank corre-
lation. Biometrika, 30(1-2):81?93.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In C. Fellbaum, edi-
tor, WordNet: An electronic lexical database, pages
265?283. MIT Press.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th In-
ternational Conference on Machine Learning, pages
296?304, Madison, USA.
Panos Louridas and Georgios Gousios. 2012. A note
on rigour and replicability. SIGSOFT Softw. Eng.
Notes, 37(5):1?4.
Andrew K. McCallum. 2002. MALLET: A machine
learning for language toolkit. http://mallet.
cs.umass.edu.
Thilo Mende. 2010. Replication of defect prediction
studies: problems, pitfalls and recommendations. In
Proceedings of the 6th International Conference on
Predictive Models in Software Engineering. ACM.
Lingling Meng, Runqing Huang, and Junzhong Gu.
2013. A review of semantic similarity measures in
wordnet. International Journal of Hybrid Informa-
tion Technology, 6(1):1?12.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, 6(1):1?28.
Cameron Neylon, Jan Aerts, C Titus Brown, Si-
mon J Coles, Les Hatton, Daniel Lemire, K Jar-
rod Millman, Peter Murray-Rust, Fernando Perez,
Neil Saunders, Nigam Shah, Arfon Smith, Gae?l
Varoquaux, and Egon Willighagen. 2012. Chang-
ing computational research. the challenges ahead.
Source Code for Biology and Medicine, 7(2).
Siddharth Patwardhan and Ted Pedersen. 2006. Us-
ing wordnet based context vectors to estimate the
semantic relatedness of concepts. In Proceedings of
the EACL 2006 Workshop Making Sense of Sense -
Bringing Computational Linguistics and Psycholin-
guistics Together, pages 1?8, Trento, Italy.
Ted Pedersen. 2008. Empiricism is not a matter of
faith. Computational Linguistics, 34(3):465?470.
1700
Ted Pedersen. 2010. Information content measures
of semantic similarity perform better without sense-
tagged text. In Proceedings of the 11th Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL-HLT
2010), pages 329?332, Los Angeles, USA.
Troy Raeder, T. Ryan Hoens, and Nitesh V. Chawla.
2010. Consequences of variability in classifier per-
formance estimates. In Proceedings of ICDM?2010.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence (IJCAI), pages 448?453,
Montreal, Canada.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627?633.
Wheeler Ruml. 2010. The logic of benchmarking: A
case against state-of-the-art performance. In Pro-
ceedings of the Third Annual Symposium on Combi-
natorial Search (SOCS-10).
Charles Spearman. 1904. Proof and measurement of
association between two things. American Journal
of Psychology, 15:72?101.
Marieke Van Erp and Lourens Van der Meij. 2013.
Reusable research? a case study in named entity
recognition. CLTL 2013-01, Computational Lexi-
cology & Terminology Lab, VU University Amster-
dam.
Joaquin Vanschoren, Hendrik Blockeel, Bernhard
Pfahringer, and Geoffrey Holmes. 2012. Experi-
ment databases. Machine Learning, 87(2):127?158.
Piek Vossen, Isa Maks, Roxane Segers, Hennie van der
Vliet, Marie-Francine Moens, Katja Hofmann, Erik
Tjong Kim Sang, and Maarten de Rijke. 2013. Cor-
netto: a Combinatorial Lexical Semantic Database
for Dutch. In Peter Spyns and Jan Odijk, editors, Es-
sential Speech and Language Technology for Dutch
Results by the STEVIN-programme, number XVII in
Theory and Applications of Natural Language Pro-
cessing, chapter 10. Springer.
Zhibiao Wu and Martha Palmer. 1994. Verb seman-
tics and lexical selection. In Proceedings of the
32nd Annual Meeting of the Association for Com-
putational Linguistics, pages 133?138, Las Cruces,
USA.
1701
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 363?366,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Duluth-WSI: SenseClusters Applied to the
Sense Induction Task of SemEval-2
Ted Pedersen
Department of Computer Science
University of Minnesota, Duluth
Duluth, MN 55812
tpederse@d.umn.edu
http://senseclusters.sourceforge.net
Abstract
The Duluth-WSI systems in SemEval-2
built word co?occurrence matrices from
the task test data to create a second order
co?occurrence representation of those test
instances. The senses of words were in-
duced by clustering these instances, where
the number of clusters was automatically
predicted. The Duluth-Mix system was a
variation of WSI that used the combina-
tion of training and test data to create the
co-occurrence matrix. The Duluth-R sys-
tem was a series of random baselines.
1 Introduction
The Duluth systems in the sense induction task
of SemEval-2 (Manandhar et al, 2010) were
based on SenseClusters (v1.01), a freely available
open source software package which relies on the
premise that words with similar meanings will oc-
cur in similar contexts (Purandare and Pedersen,
2004). The data for the sense induction task in-
cluded 100 ambiguous words made up of 50 nouns
and 50 verbs. There were a total of 8,915 test in-
stances and 879,807 training instances provided.
Note that neither the training nor the test data was
sense tagged. The training data was made avail-
able as a resource for participants, with the under-
standing that system evaluation would be done on
the test instances only. The organizers held back a
gold standard annotation of the test data that was
only used for evaluation.
Five Duluth-WSI systems participated in this
task, six Duluth-Mix systems, and five Duluth
Random systems. The WSI and Mix systems al-
most always represented the test instances using
second order co?occurrences, where each word in
a test instance is replaced by a vector that shows
the words with which it co-occurs. The word vec-
tors that make up a test instance are averaged to-
gether to make up a new representation for that
instance. All the test instances for a word are clus-
tered, and the number of senses is automatically
predicted by either the PK2 measure or Adapted
Gap Statistic (Pedersen and Kulkarni, 2006).
In the Duluth systems the co-occurrence matri-
ces are either based on order-dependent bigrams
or unordered pairs of words, both of which can be
separated by up to some given number of interven-
ing words. Bigrams are used to preserve distinc-
tions between collocations such as cat house and
house cat, whereas co?occurrences do not con-
sider order and would treat these two as being
equivalent.
2 Duluth-WSI systems
The Duluth-WSI systems build co-occurrence ma-
trices from the test data by identifying bigrams or
co?occurrences that occur with up to eight inter-
mediate words between them in instances of am-
biguous nouns, and up to 23 intermediate words
for the verbs. Any bigram (bi) or co?occurrence
(co) that occurs more than 5 times with up to the
allowed number of intervening words and has sta-
tistical significance of 0.95 or above according to
the left-sided Fisher?s exact test was selected (Ped-
ersen et al, 1996). Some of the WSI systems re-
duce the co?occurrence matrix to 300 dimensions
using Singular Value Decomposition (SVD).
The resulting co-occurrence matrix was used to
create second order co?occurrence vectors to rep-
resent the test instances, which were clustered us-
ing the method of repeated bisections (rb), where
similarity was measured using the cosine. Table
1 summarizes the distinctions between the various
Duluth-WSI systems.
3 Duluth-Mix systems
The Duluth-Mix systems used the combination of
the test and training data to identify features to rep-
resent the test instances. The goal of this combi-
363
Table 1: Duluth-WSI Distinctions
name options
Duluth-WSI bigrams, no SVD, PK2
Duluth-WSI-Gap bigrams, no SVD, Gap
Duluth-WSI-SVD bigrams, SVD, PK2
Duluth-WSI-Co co-occur, no SVD, PK2
Duluth-WSI-Co-Gap co-occur, no SVD, Gap
nation was to increase the amount of data that was
available for feature identification. Since there
was a larger amount of data, some parameter set-
tings as used in Duluth-WSI were reduced.
For example, the Duluth-Mix-PK2 and Duluth-
Mix-Gap are identical to the Duluth-WSI and
Duluth-WSI-Gap systems, except that they limit
both nouns and verbs to 8 intervening words.
Duluth-Mix-Narrow-PK2 and Duluth-Mix-
Narrow-Gap are identical to Duluth-Mix-PK2
and Duluth-Mix-Gap except that bigrams and
co?occurrences must be made up of adjacent
words, with no intermediate words allowed.
Duluth-Mix-Uni-PK2 and Duluth-Mix-Uni-
Gap are unique among the Duluth systems in
that they do not use second order co-occurrences,
but instead rely on first order co-occurrences.
These are simply individual words (unigrams) that
occur more than 5 times in the combined test and
training data. These features are used to generate
co-occurrence vectors for the test instances which
are then clustered (this is very similar to a bag of
words model).
4 Duluth-Random systems
Duluth-R12, Duluth-R13, Duluth-R15, and
Duluth-R110 provide random baselines. R12
randomly assigns each instance to one of two
senses, R13 to one of three, R15 to one of five,
and R110 to one of ten senses. Random numbers
are generated in the given range with equal
probability, so the distribution of assigned senses
is balanced.
5 Discussion
The evaluation of unsupervised sense discrimina-
tion and induction systems is still not standard-
ized, so an important part of any exercise like
SemEval-2 is to scrutinize the evaluation measures
used in order to determine to what degree they are
providing a useful and reasonable way of evaluat-
ing system results.
5.1 Evaluation Measures
Each participating system was scored by three dif-
ferent evaluation methods: the V-measure (Rosen-
berg and Hirschberg, 2007), the supervised recall
measure (Agirre and Soroa, 2007), and the paired
F-score (Artiles et al, 2009). The results of the
evaluation are in some sense confusing - a sys-
tem that ranks near the top according to one mea-
sure may rank at the bottom or middle of another.
There was not any single system that did well ac-
cording to all of the different measures. The sit-
uation is so extreme that in some cases a system
would perform near the top in one measure, and
then below random baselines in another. These
stark differences suggest a real need for continued
development of other methods for evaluating un-
supervised sense induction.
One minimum expectation of an evaluation
measure is that it should expose and identify ran-
dom baselines by giving them low scores that
clearly distinguish them from actual participating
systems. The scores of all the evaluation mea-
sures used in this task when applied to different
random baseline systems are summarized in Table
2. These include a number of post-evaluation ran-
dom clustering systems, which are referred to as
post-R1k, where k is the number of random clus-
ters.
5.1.1 V-measure
The V-measure appears to be quite easily mislead
by random baselines. As evidence of that, the
Duluth-R (random) systems got increasingly bet-
ter scores the more random they became, and in
fact the post-evaluation random systems reached
levels of performance better than any of the partic-
ipating systems. Table 2 shows that the V-measure
continues to improve (rather dramatically) as ran-
domness increases.
The average number of senses in the gold stan-
dard data for all 100 words was 3.79. The offi-
cial random baseline assigned one of four random
senses to each instance of a word, and achieved
a V-measure of 4.40. Duluth-R15 improved the
V-measure to 5.30 by assigning one of five ran-
dom senses, and Duluth-R110 improved it again
to 8.60 by assigning one of ten random senses.
The more random the result, the better the score.
In fact Duluth-R110 placed sixth in the sense in-
364
duction task according to the V-measure. In post-
evaluation experiments a number of additional
random baselines were explored, where instances
were assigned senses randomly from 20, 33, and
50 possible values per word. The V-measures for
these random systems were 13.9, 18.7, and 23.2
respectively, where the latter two were better than
the first place participating system (which scored
16.2). In a post-evaluation experiment, the task
organizers found that assigning one sense per in-
stance resulted in a V?measure of 31.7.
5.1.2 Supervised Recall
The supervised recall measure takes the sense in-
duction results (on the 8,915 test instances) as sub-
mitted by a participating system and splits that into
a training and test portion for supervised learning.
The recall attained on the test split by a classifier
learned on the training split becomes the measure
of the unsupervised system. Two different splits
were used, with 80% or 60% of the test instances
for training, and the remainder for testing.
This evaluation method was also used in
SemEval-1, where (Pedersen, 2007) noted that it
seemed to compress the results of all the systems
into a narrow band that converged around the Most
Frequent Sense result. The same appears to have
happened in 2010. The supervised recall of the
Most Frequent Sense baseline (MFS) is 58 or .59
(depending on the split), and the majority of par-
ticipating systems (and even some of the random
baselines) fall in a range of scores from .56 to .62
(a band of .06). This blurs distinctions among par-
ticipating systems with each other and with ran-
dom baselines.
The number of senses actually assigned by the
classifier learned from the training split to the in-
stances in the test split is quite small, regardless of
the number of senses discovered by the participat-
ing system. There were at most 2.06 senses identi-
fied per word based on the 80-20 split, and at most
2.27 senses per word based on the 60-40 split.
For most systems, regardless of their underlying
methodology, the number of senses the classifier
actually assigns is approximately 1.5 per word.
This shows that the supervised learning algorithm
that underlies this evaluation method gravitates to-
wards a very small number of senses and there-
fore tends to converge on the MFS baseline. This
could be caused by noise in the induced senses,
a small number of examples in the training split
for a sense, or it may be that the supervised recall
Table 2: Evaluation of Random Systems
name k V F 60-40 80-20
MFS 1 0.0 63.4 58.3 58.7
Duluth-R12 2 2.3 47.8 57.7 58.5
Duluth-R13 3 3.6 38.4 57.6 58.0
Random 4 4.4 31.9 56.5 57.3
Duluth-R15 5 5.3 27.6 56.5 56.8
Duluth-R110 10 8.6 16.1 53.6 54.8
post-R120 20 13.9 7.5 46.2 48.6
post-R133 33 18.7 4.0 38.3 42.5
post-R150 50 23.2 2.3 30.0 34.2
measure is making different distinctions than are
found by the unsupervised sense induction method
it seeks to evaluate.
5.1.3 Paired F-score
The paired F-score was the only evaluation mea-
sure that seemed able to identify and expose ran-
dom baselines. Duluth-R110 was by far the most
random of the officially participating systems, and
it was by far the lowest ranked system according
to the paired F-score, which assigned it a score of
16.1. All the Duluth-R systems ranked relatively
low (20th or below). When presented with the 20,
33, and 50 random sense post?evaluation systems,
the F-score assigned those scores of 7.46, 4.00,
and 2.33, which placed them far below any of the
other systems.
However, the paired F-score also showed that
the Most Frequent Sense baseline outperformed
all of the participating systems. The systems that
scored close to the MFS tended to predict very
small numbers of senses, and so were in effect act-
ing much like the MFS baseline themselves. The
F-score is not bounded by MFS and in fact it is
possible (theoretically) to reach a score of 1.00
with a perfect assignment of instances to senses.
The lesson learned in this task is that it would have
been more effective to simply assume that there
was just one sense per word, rather than using the
senses induced by participating systems. While
this may be a frustrating conclusion, in fact it is
a reasonable observation given that in many do-
mains a single sense for a given word can tend to
dominate.
5.2 Duluth-WSI and Duluth-Mix Results
The Duluth-WSI systems used the test data to
build co-occurrence matrices, while the Duluth-
365
Mix systems used both the training and test
data. Within those frameworks bigrams or co?
occurrences were used to represent features, the
number of senses was automatically discovered
with the PK2 measure or the Adapted Gap Statis-
tic, and SVD was optionally used to reduce the
dimensionality of the resulting matrix. Previous
studies using SenseClusters have noted that the
Adapted Gap Statistic tends to find a relatively
small number of clusters, and that SVD typically
does not help to improve results of unsupervised
sense induction. These findings were again con-
firmed in this task.
Mixing together all of the training and test data
for building the co?occurrence matrices was no
more effective than just using the test data. How-
ever, the Duluth-Mix systems did not finish be-
fore the end of the evaluation period. The Duluth-
Mix-Narrow-Gap and PK2 systems were able to
finish 8,211 of the 8,915 test instances (92%),
the Duluth-Mix-Gap and PK2 systems completed
7,417 instances (83%), and Duluth-Mix-Uni-PK2
and Gap systems completed 2,682 of these in-
stances (30%). While these are partial results they
seem sufficient to support this conclusion.
To be usable in practical settings, an unsuper-
vised sense induction system should discover the
number of senses accurately and automatically.
Duluth-WSI and Duluth-WSI-SVD were very suc-
cessful in that regard, and predicted 4.15 senses on
average per word (with the PK2 measure) while
the actual number of senses was 3.79.
The Duluth-WSI systems are direct descen-
dents of UMND2 which participated in SemEval-
1 (Pedersen, 2007), where Duluth-WSI-Gap is
the closest relative. However, UMND2 used
Pointwise Mutual Information (PMI) rather than
Fisher?s left sided test, and it performed clustering
with k-means rather than the method of repeated
bisections. Both UMND2 and Duluth-WSI-Gap
used the Adapted Gap Statistic, and interestingly
enough both discovered approximately 1.4 senses
on average per word.
6 Conclusion
The SemEval-2 sense induction task was an oppor-
tunity to compare participating systems with each
other, and also to analyze evaluation measures. At
the very least, an evaluation measure should penal-
ize random results in a fairly significant way. This
task showed that the paired F-score is able to iden-
tify and expose random baselines, and that it drives
them far down the rankings and places them well
below participating systems. This seems prefer-
able to the V-measure, which tends to rank random
systems above all others, and to supervised recall,
which provides little or no separation between ran-
dom baselines and participating systems.
References
E. Agirre and A. Soroa. 2007. SemEval-2007 Task
02: Evaluating word sense induction and discrim-
ination systems. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 7?12, Prague, Czech Repub-
lic, June.
J. Artiles, E. Amigo?, and J. Gonzalo. 2009. The role of
named entities in Web People Search. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 534?542,
Singapore, August.
S. Manandhar, I. Klapaftis, D. Dligach, and S. Prad-
han. 2010. SemEval-2010 Task 14: Word sense
induction and disambiguation. In Proceedings of
the SemEval 2010 Workshop : the 5th International
Workshop on Semantic Evaluations, Uppsala, Swe-
den, July.
T. Pedersen and A. Kulkarni. 2006. Automatic cluster
stopping with criterion functions and the gap statis-
tic. In Proceedings of the Demonstration Session of
the Human Language Technology Conference and
the Sixth Annual Meeting of the North American
Chapter of the Association for Computational Lin-
guistics, pages 276?279, New York City, June.
T. Pedersen, M. Kayaalp, and R. Bruce. 1996. Sig-
nificant lexical relationships. In Proceedings of the
Thirteenth National Conference on Artificial Intelli-
gence, pages 455?460, Portland, OR, August.
T. Pedersen. 2007. UMND2 : SenseClusters applied
to the sense induction task of Senseval-4. In Pro-
ceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 394?
397, Prague, Czech Republic, June.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proceedings of the Conference on
Computational Natural Language Learning, pages
41?48, Boston, MA.
A. Rosenberg and J. Hirschberg. 2007. V-measure:
A conditional entropy-based external cluster eval-
uation measure. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 410?420, Prague, Czech Re-
public, June.
366
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 497?501,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Duluth : Measuring Degrees of Relational Similarity
with the Gloss Vector Measure of Semantic Relatedness
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812 USA
tpederse@d.umn.edu
Abstract
This paper describes the Duluth systems that
participated in Task 2 of SemEval?2012.
These systems were unsupervised and relied
on variations of the Gloss Vector measure
found in the freely available software pack-
age WordNet::Similarity. This method was
moderately successful for the Class-Inclusion,
Similar, Contrast, and Non-Attribute cate-
gories of semantic relations, but mimicked a
random baseline for the other six categories.
1 Introduction
This paper describes the Duluth systems that par-
ticipated in Task 2 of SemEval?2012, Measuring
the Degree of Relational Similarity (Jurgens et al,
2012). The goal of the task was to rank sets of
word pairs according to the degree to which they
represented an underlying category of semantic re-
lation. A highly ranked pair would be considered
a good or prototypical example of the relation. For
example, given the relation Y functions as an X the
pair weapon:knife (X:Y) would likely be considered
more representative of that relation than would be
tool:spoon.
The task included word pairs from 10 different
categories of relational similarity, each with a num-
ber of subcategories. In total the evaluation data
consisted of 69 files, each containing a set of ap-
proximately 40 word pairs. While training examples
were also provided, these were not used by the Du-
luth systems. The system?generated rankings were
compared with gold standard data created via Ama-
zon Mechanical Turk.
The Duluth systems relied on the Gloss Vec-
tor measure of semantic relatedness (Patwardhan
and Pedersen, 2006) as implemented in Word-
Net::Similarity (Pedersen et al, 2004)1. This quanti-
fies the degree of semantic relatedness between two
word senses. It does not, however, discover or in-
dicate the nature of the relation between the words.
When given two words as input (as was the case in
this task), it measures the relatedness of all possi-
ble combinations of word senses associated with this
pair and reports the highest resulting score. Note
that throughout this paper we use word and word
sense somewhat interchangeably. In general it may
be assumed that the term word or examples of words
refers to a word sense.
A key characteristic of this task was that the word
pairs in each of the 69 sets were scored assuming
a particular specified underlying semantic relation.
Given this, the limitation that the Gloss Vector mea-
sure does not discover the nature of relations was
less of a concern, and led to the hypothesis that a
word pair that was highly related would also be a
prototypical example of the underlying category of
semantic relation. Unfortunately the results from
this task do not generally support this hypothesis,
although for a few categories at least it appears to
have some validity.
This paper continues with a review of the Gloss
Vector measure, and explains its connections to the
Adapted Lesk measure. The paper then summarizes
the results of the three Duluth systems in this task,
and concludes with some discussion and analysis of
where this method had both successes and failures.
1wn-similarity.sourceforge.net
497
2 Semantic Relatedness
Semantic relatedness is a more general notion than
semantic similarity. We follow (Budanitsky and
Hirst, 2006) and limit semantic similarity to those
measures based on distances and perhaps depths in
a hierarchy made up of is?a relations. For exam-
ple, car and motorcycle are similar in that they are
connected via an is?a relation with vehicle. Seman-
tic similarity is most often applied to nouns, but can
also be used with verbs.
Two word senses can be related in many ways,
including similarity. car and furnace might be con-
sidered related because they are both made of steel,
and firefighter and hose might be considered related
because one uses the other, but neither pair is likely
to be considered similar. Measures of relatedness
generally do not specify the nature of the relation-
ship between two word senses, but rather indicate
that they are related to a certain degree in some un-
specified way. As a result, measures of relatedness
tend to be symmetric, so A is related to B to the same
degree that B is related to A. It should be noted that
some of the relations in Task 2 were not symmetric,
which was no doubt a complicating factor for the
Duluth systems.
3 Adapted Lesk Measure
The Gloss Vector measure was originally devel-
oped in an effort to generalize and improve upon
the Adapted Lesk measure (Banerjee and Pedersen,
2003).2 Both the Gloss Vector measure and the
Adapted Lesk measure start with the idea of a su-
pergloss. A supergloss is the definition (or gloss) of
a word sense that is expanded by concatenating it
with the glosses of other surrounding senses that are
connected to it via some WordNet relation. For ex-
ample, a supergloss for car might consist of the def-
inition of car, the definition of car?s hypernym (e.g.,
vehicle), and the definitions of the meronyms (part-
of) of car (e.g., wheel, brake, bumper, etc.) Other
relations as detailed later in this paper may also be
used to expand a supergloss.
In the Adapted Lesk measure, the relatedness be-
tween two word senses is a function of the number
and length of their matching overlaps in their super-
glosses. Consecutive words that match are scored
2WordNet::Similarity::lesk
more highly than single words, and a higher score
for a pair of words indicates a stronger relation. The
Adapted Lesk measure was developed to overcome
the fact that most dictionary definitions are rela-
tively short, which was a concern noted by (Lesk,
1986) when he introduced the idea of using defini-
tion overlaps for word sense disambiguation. While
the Adapted Lesk measure expands the size of the
definitions, there are still difficulties. In particular,
the matches between words in superglosses must be
exact, so morphological variants (run versus ran),
synonyms (gas versus petrol), and closely related
words (tree versus shrub) won?t be considered over-
laps and will be treated the same as words with no
apparent connection (e.g., goat and vase).
4 Gloss Vector Measure
The Gloss Vector measure3 is inspired by a 2nd or-
der word sense discrimination approach (Schu?tze,
1998) which is in turn related to Latent Semantic
Indexing or Analysis (Deerwester et al, 1990). The
basic idea is to replace each word in a written con-
text with a vector of co-occurring words as observed
in some corpus. In this task, the contexts are def-
initions (and example text) from WordNet. A su-
pergloss is formed exactly as described for Adapted
Lesk, and then each word in the supergloss is re-
placed by a vector of co?occurring words. Then, all
the vectors in the supergloss are averaged together to
create a new high dimensional representation of that
word sense. The semantic relatedness between two
word senses is measured by taking the cosine be-
tween their two averaged vectors. The end result is
that rather than finding overlaps in definitions based
on exact matches, a word in a definition is matched
to whatever degree its co-occurrences match with
the co-occurrences of the words in the other super-
gloss. This results in a more subtle and fine grained
measure of relatedness than Adapted Lesk.
The three Duluth systems only differ in the re-
lations used to create the superglosses, otherwise
they are identical. The corpus used to collect co-
occurrence information was the complete collection
of glosses and examples from WordNet 3.0, which
consists of about 1.46 million word tokens and al-
most 118,000 glosses. Words that appeared in a
3WordNet::Similarity::vector
498
stop list of about 200 common words were excluded
as co-occurrences, as were words that occurred less
than 5 times or more than 50 times in the WordNet
corpus. Two words are considered to co-occur if
they occur in the same definition (including the ex-
ample) and are adjacent to each other. These are the
default settings as used in WordNet::Similarity.
5 Creating the Duluth Systems
There were three Duluth systems, V0, V1, and V2.
These all used the Gloss Vector measure, and differ
only in how their superglosses were created. The su-
pergloss is defined using a set of relations that indi-
cate which additional definitions should be included
in the definition for a sense. All systems start with
a gloss and example for each sense in a pair, which
is then augmented with definitions from additional
senses as defined for each system.
5.1 Duluth-V0
V0 is identical to the default configuration of the
Gloss Vector measure in WordNet::Similarity. This
consists of the following relations:
hypernym (hype) : class that includes a member,
e.g., a car is a kind of vehicle (hypernym).
hyponym (hypo) : the member of a class, e.g., a
car (hyponym) is a kind of vehicle.
holonym (holo) : whole that includes the part,
e.g., a ship (holonym) includes a mast.
meronym (mero) : part included in a whole, e.g.,
a mast (meronym) is a part of a ship.
see also (also) : related adjectives, e.g., egocentric
see also selfish.
similar to (sim) : similar adjectives, satanic is
similar to evil.
is attribute of (attr) : adjective related to a noun,
e.g., measurable is an attribute of magnitude.
synset words (syns) : synonyms of a word, e.g.,
car and auto are synonyms.4
For V0 the definition and example of a noun
is augmented with its synonyms and the defini-
tions and examples of any hypernyms, hyponyms,
meronyms, and holonyms to which it is directly con-
nected. If the word is a verb it is augmented with
4Since synonyms have the same definition, this relation aug-
ments the supergloss with the synonyms themselves.
its synonyms and any hypernyms/troponyms and hy-
ponyms to which it is directly connected. If the
word is an adjective then its definition and exam-
ple are augmented with those of adjectives directly
connected via see also, similar to, and is attribute of
relations.
5.2 Duluth-V1
V1 uses the relations in V0, plus the holonyms, hy-
pernyms, hyponyms, and meronyms (X) of the see
also, holonym, hypernym, hyponym, and meronym
relations (Y). This leads to an additional 20 relations
that bring in definitions ?2 steps? away from the
original word. These take the form of the holonym
of the hypernym of the word sense, or more gener-
ally the X of the Y of the word sense, where X and Y
are as noted above.
5.3 Duluth-V2
V2 uses the relations in V0 and V1, and then adds
the holonym, hypernyms, hyponyms, and meronyms
of the 20 relations added for V1. This leads to an
additional 80 relations of the form the hypernyms of
the meronym of the hyponym, or more generally the
X of the X of the Y of the word.
For example, if the word is weapon, then a hyper-
nym of the meronym of the hyponym (of weapon)
would add the definitions and example of bow (hy-
ponym), bowstring (meronym of the hyponym), and
cord (hypernym of the meronym of the hyponym) to
the gloss of weapon to create the supergloss.
6 Results
There were two evaluation scores reported for the
participating systems, Spearman?s Rank Correlation
Coefficient, and a score based on Maximum Differ-
ence Scaling. Since the Gloss Vector measure is
based on WordNet, there was a concern that a lack
of WordNet coverage might negatively impact the
results. However, of the 2,791 pairs used in the eval-
uation, there were only 3 that contained words un-
known to WordNet.
6.1 Spearman?s Rank Correlation
The ranking of word pairs in each of the 69 files
were evaluated relative to the gold standard using
Spearman?s Rank Correlation Coefficient. The av-
erage of these results over all 10 categories of se-
499
Table 1: Selected Spearman?s Values
Category rand v0 v1 v2
SIMILAR .026 .183 .206 .198
CLASS-INCLUSION .057 .045 .178 .168
CONTRAST -.049 .142 .120 .198
average (of all 10) .018 .050 .039 .038
Table 2: Selected MaxDiff Values
Category rand v0 v1 v2
SIMILAR 31.5 37.1 39.2 37.4
CLASS-INCLUSION 31.0 29.2 35.6 33.1
CONTRAST 30.4 38.3 36.0 33.8
NON-ATTRIBUTE 28.9 36.0 33.0 33.5
average (of all 10) 31.2 32.4 31.5 31.1
mantic relations was quite low. Random guessing
achieved an averaged Spearman?s value 0.018, while
Duluth-V0 scored 0.050, Duluth-V1 scored 0.039,
and Duluth-V2 scored 0.038.
However, there were specific categories where the
Duluth systems fared somewhat better. In particular,
results for category 1 (CLASS-INCLUSION), cate-
gory 3 (SIMILAR) and category 4 (CONTRAST)
represent improvements on the random baseline
(shown in Table 1) and at least some modest agree-
ment with the gold standard.
The results from the other categories were gener-
ally equivalent to what would be obtained with ran-
dom selection.
6.2 Maximum Difference Scaling
Maximum Difference Scaling is based on identify-
ing the least and most prototypical pair for a given
relation from among a set of four pairs. A ran-
dom baseline scores 31.2%, meaning that it got ap-
proximately 1 in 3 of the MaxDiff questions correct.
None of the Duluth systems improved upon random
to any significant degree : Duluth-V0 scored 32.4,
Duluth-V1 scored 31.5, and Duluth-V2 scored 31.1.
However, the same categories that did well with
Spearman?s also did well with MaxDiff (see Table
2). In addition, there is some improvement in cat-
egory 6 (NON-ATTRIBUTE) at least with MaxDiff
scoring.
7 Discussion and Conclusions
The Gloss Vector measure was able to perform rea-
sonably well in measuring the degree of relatedness
for the following four categories (where the defini-
tions come from (Bejar et al, 1991)):
CLASS-INCLUSION : one word names a class
that includes the entity named by the other word
SIMILAR : one word represents a different de-
gree or form of the ... other
CONTRAST : one word names an opposite or
incompatible of the other word
NON-ATTRIBUTE : one word names a quality,
property or action that is characteristically not an at-
tribute of the other word
Of these, CLASS-INCLUSION and SIMILAR
are well represented by the hypernym/hyponym re-
lations present in WordNet and used by the Gloss
Vector measure. WordNet?s greatest strength lies
in its hypernym tree for nouns, and that was most
likely the basis for the success of the CLASS-
INCLUSION and SIMILAR categories. While the
success with CONTRAST may seem unrelated, in
fact it may be that pairs of opposites are often quite
similar, for example happy and sad are both emo-
tions and are similar except for their polarity.
A number of the relations used in Task 2 are
not well represented in WordNet. For example,
there was a CASE RELATION which could ben-
efit from information about selectional restrictions
or case frames that just isn?t available in WordNet.
The same is true of the CAUSE-PURPOSE relation
as there is relatively little information about casual
relations in WordNet. While there are part-of rela-
tions in WordNet (meronyms/holonyms), these did
not prove to be common enough to be a significant
benefit for the PART-WHOLE relations in the task.
For many of the relations in the task the Gloss
Vector measure was most likely relying primarily on
hypernym and hyponym relations, which explains
the bias towards categories that featured similarity-
based relations. We are however optimistic that
a Gloss Vector approach could be more successful
given a richer set of relations from which to draw
information for superglosses.
500
References
S. Banerjee and T. Pedersen. 2003. Extended gloss over-
laps as a measure of semantic relatedness. In Proceed-
ings of the Eighteenth International Joint Conference
on Artificial Intelligence, pages 805?810, Acapulco,
August.
I. Bejar, R. Chaffin, and S. Embretson. 1991. Cogni-
tive and Psychometric Analysis of Analogical Problem
Solving. Springer?Verlag, New York, NY.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of semantic distance. Computational
Linguistics, 32(1):13?47.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by Latent Semantic
Analysis. Journal of the American Society for Infor-
mation Science, 41:391?407.
D. Jurgens, S. Mohammad, P. Turney, and K. Holyoak.
2012. Semeval-2012 task 2: Measuring degrees of
relational similarity. In Proceedings of the 6th Inter-
national Workshop on Semantic Evaluation (SemEval
2012), Montreal, June.
M.E. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proceedings of the
5th annual international conference on Systems docu-
mentation, pages 24?26. ACM Press.
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based Context Vectors to Estimate the Semantic Relat-
edness of Concepts. In Proceedings of the EACL 2006
Workshop on Making Sense of Sense: Bringing Com-
putational Linguistics and Psycholinguistics Together,
pages 1?8, Trento, Italy, April.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
Wordnet::Similarity - Measuring the relatedness of
concepts. In Proceedings of the Nineteenth National
Conference on Artificial Intelligence, pages 1024?
1025, San Jose.
H. Schu?tze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?123.
501
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 202?206, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
Duluth : Word Sense Induction Applied to Web Page Clustering
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812 USA
tpederse@d.umn.edu
http://senseclusters.sourceforge.net
Abstract
The Duluth systems that participated in task
11 of SemEval?2013 carried out word sense
induction (WSI) in order to cluster Web search
results. They relied on an approach that repre-
sented Web snippets using second?order co?
occurrences. These systems were all imple-
mented using SenseClusters, a freely available
open source software package.
1 Introduction
The goal of task 11 of SemEval?2013 was to clus-
ter Web search results (Navigli and Vannella, 2013).
The test data consisted of the top 64 Google results
for each of 100 potentially ambiguous queries, for a
total of 6,400 test instances. The Web snippets re-
turned for each query were clustered and evaluated
separately, with an overall evaluation score provided
for each system.
The problem of Web page clustering is one of
the use cases envisioned for SenseClusters (Peder-
sen and Kulkarni, 2007; Pedersen, 2010a), a freely
available open source software package developed
at the University of Minnesota, Duluth starting in
2002. It supports first and second?order clustering
of contexts using both co?occurrence matrices (Pu-
randare and Pedersen, 2004; Kulkarni and Pedersen,
2005) and Latent Semantic Analysis (Landauer and
Dumais, 1997).
SenseClusters has participated in various forms at
different SenseEval and SemEval shared tasks, in-
cluding SemEval-2007 (Pedersen, 2007), SemEval-
2010 (Pedersen, 2010b) and also in an i2b2 clinical
medicine task (Pedersen, 2006).
2 Duluth System
While we refer to three Duluth systems (sys1, sys7,
and sys9), in reality these are all variations of the
same overall system. All three are based on second?
order context clustering as provided in SenseClus-
ters. The query terms are treated exactly like any
other word in the snippets, which is called headless
clustering in SenseClusters.
2.1 Common aspects to all systems
The input to sys1, sys7, and sys9 consists of 64
Web search snippets, each approximately 25 words
in length. All text was converted to upper case prior
to processing. The goal was to group the 64 snip-
pets for each query into k distinct clusters, where k
was automatically determined by the PK2 method of
SenseClusters (Pedersen and Kulkarni, 2006a; Ped-
ersen and Kulkarni, 2006b). Each discovered clus-
ter represents a different underlying meaning of the
given query term that resulted in those snippets be-
ing returned. Word sense induction was carried out
separately on the Web snippets associated with each
query term, meaning that the algorithm was run 100
times and clustered 64 Web page snippets each time.
In second?order context clustering, the words in
a context (i.e., Web snippet) to be clustered are re-
placed by vectors that are derived from some cor-
pus of text. The corpora used are among the main
differences in the Duluth systems. Once the words
in a context are replaced by vectors, those vectors
are averaged together to create a new representa-
tion of the context. That representation is said to
be second?order because each word is represented
by its direct or first order co?occurrences, and simi-
202
larities between words in the same Web snippet are
captured by the set of words that mutually co?occur
with them.
If car is represented by the vector [motor, mag-
azine, insurance], and if life is represented by the
vector [sentence, force, insurance], then car and life
are said to be second?order co-occurrences because
they both occur with insurance. A second?order co-
occurrence can capture more indirect relationships
between words, and so these second?order connec-
tions tend to be more numerous and more subtle
than first?order co?occurrences (which would re-
quire that car and life co?occur near or adjacent to
each other in a Web snippet to establish a relation-
ship).
The co-occurrence matrix is created by finding bi-
grams that occur more than a given number of times
(this varies per system) and have a log-likelihood ra-
tio greater than 3.84.1 Then, the first word in a bi-
gram is represented in the rows of the matrix, the
second word is represented in the columns. The
value in the corresponding cell is the log-likelihood
score. This matrix is therefore not symmetric, and
has different entries for old age and age old. Also,
any bigram that includes one or two stop words (e.g.,
to fire, running to, for the) will be excluded and not
included in the co-occurrence matrix and will not be
included in the overall sample count used for com-
puting the log?likelihood ratio. To summarize then,
words in a Web snippet are represented by the words
with which they occur in bigrams, where the context
word is the first word in the bigram, and the vector
is the set of words that follow it in bigrams.
Once the co?occurrence matrix is created, it may
be optionally reduced by Singular Value Decompo-
sition. The result of this will be a matrix with the
same number of rows prior to SVD, but a reduced
number of columns. The goal of SVD is to com-
press together columns of words with similar co?
occurrence patterns, and thereby reduce the size and
noisiness of the data. Whether the matrix is reduced
or not, then each word in each snippet to be clustered
is replaced by a vector from that matrix. A word is
1This value corresponds with a p-value of 0.05 when testing
for significance, meaning that bigrams with log-likelihood at
least equal to 3.84 have at least a 95% chance of having been
drawn from a population where their co-occurrence is not by
chance.
replaced by the row in the co-occurrence matrix to
which it corresponds. Any words that do not have
an entry in the co-occurrence matrix will not be rep-
resented. Then, the contexts are clustered using the
method of repeated bisections (Zhao and Karypis,
2004), where the number of clusters is automatically
discovered using the PK2 method.
2.2 Differences among systems
The main difference among the systems was the cor-
pora used to create their co-occurrence matrices.
The smallest corpus was used by sys7, which sim-
ply treated the 64 snippets returned by each query
as the corpus for creating a co?occurrence matrix.
Thus, each query term had a unique co-occurrence
matrix that was created from the Web snippets re-
turned by that query. This results in a very small
amount of data per query (approx. 25 words/snip-
pet * 64 snippets = 1600 words), and so bigrams
were allowed to have up to three intervening words
that were skipped (in order to increase the number
of bigrams used to create the co?occurrence ma-
trix). Bigrams were excluded if they only occurred 1
time, had a log?likelihood ratio of less than 3.84, or
were made up of one or two stop words. Even with
this more flexible definition of bigram, the resulting
co?occurrence matrices were still quite small. The
largest resulting co?occurrence matrix for any query
was 221 x 222, with 602 non?zero values (meaning
there were 602 different bigrams used as features).
The smallest of the co-occurrence matrices was 102
x 113 with 242 non?zero values. Given these small
sizes, SVD was not employed in sys7.
sys1 and sys9 used larger corpora, and therefore
required bigrams to be made up of adjacent words
that occurred 5 or more times, had log?likelihood
ratio scores of 3.84 or above, and contained no stop
words. Rather than having a different co?occurrence
matrix for each query, sys1 and sys9 created a single
co-occurrence matrix for all queries.
In sys1, all the Web snippet results for all 100
queries were combined into a single corpus. Thus,
the co?occurrence matrix was based on bigram fea-
tures found in a corpus of 6,400 Web snippets that
consisted of approximately 160,000 words. This
resulted in a co?occurrence matrix of size 771 x
952 with 1,558 non?zero values prior to SVD. After
SVD the matrix was 771 x 90, and all cells had non-
203
zero values (as a result of SVD). Note that if there
are less than 3,000 columns in a co-occurrence ma-
trix, the columns are reduced down to 10% of their
original size. If there are more than 3,000 columns
then it is reduced to 300 dimensions. This follows
recommendations for SVD given for Latent Seman-
tic Analysis (Landauer and Dumais, 1997).
Rather than using task data, sys9 uses the first
10,000 paragraphs of Associated Press newswire
(APW) that appear in the English Gigaword corpus
(1st edition) (Graff and Cieri, 2003). This created
a corpus of approximately 3.6 million words which
resulted in a co-occurrence matrix prior to SVD of
9,853 x 10,995 with 43,199 non-zero values. After
SVD the co?occurrence matrix was 9,853 by 300.
3 Results
Various measures were reported by the task organiz-
ers, including F1 (F1-13), the Rand Index (RI), the
Adjusted Rand Index (ARI), and the Jaccard Coeffi-
cient. More details can be found in (Di Marco and
Navigli, 2013).
In addition we computed the paired F-Score (F-
10) (Artiles et al, 2009) as used in the 2010 Se-
mEval word sense induction task (Manandhar et al,
2010) and the F-Measure (F-SC), which is provided
by SenseClusters. This allows for the comparison of
results from this task with the 2010 task and various
results from SenseClusters.
The organizers also provided scores for S-recall
and S-precision (Zhai et al, 2003), however for
these to be meaningful the results for each cluster
must be output in ranked order. The Duluth sys-
tems did not make a ranking distinction among the
instances in each cluster, and so these scores are not
particularly meaningful for the Duluth systems.
3.1 Comparisons to Baselines
Table 1 includes the results of the three submitted
Duluth systems, plus numerous baselines. RandX
designates a random baseline where senses were as-
signed by randomly assigning a value between 1 and
X. In word sense induction, the labels assigned to
discovered clusters are arbitrary, so a random base-
line is a convenient sanity check. MFS replicates the
most frequent sense baseline from supervised learn-
ing by simply assigning all instances for a word to
a single cluster. This is sometimes also known as
the ?all?in?one? baseline. Gold are the evaluation
results when the gold standard data is provided as
input (and compared to itself).
The various baselines give us a sense of the char-
acteristics of the different evaluation measures, and
a few points emerge. We have argued previously
(Pedersen, 2010a) that any evaluation measure used
for word sense induction needs to be able to ex-
pose random baselines and distinguish them from
more systematic results. By this standard a number
of measures are found to be lacking. In SemEval?
2010 we demonstrated that the V-Measure (Rosen-
berg and Hirschberg, 2007) had an overwhelming
bias towards systems that produce larger numbers of
clusters ? as a result it scored random baselines that
generated larger number of clusters (like Rand25
and Rand50) very highly.
The Rand Index (RI), which does not correct
for chance agreement, also scores random baselines
higher than both non?random systems and MFS.
The Adjusted Rand Index (ARI) corrects for chance
and scores random systems near 0, but it also scores
MFS near 0. According to ARI, random systems
and MFS perform at essentially the same level. This
is a troublesome tendency when evaluating word
sense induction systems, since MFS is often consid-
ered a reasonable baseline that provides useful re-
sults. Many words have relatively skewed distribu-
tions where they are mostly used in one sense, and
this is exactly what is approximated by MFS.
Of the measures included in Table 1, the paired
FScore (F-10), the F-Measure (F-SC), and the Jac-
card Coefficient provide results that seem most ap-
propriate for word sense induction. This is because
these measures score random baselines lower than
MFS, and that RandX scores lower than RandY,
when (X > Y). The paired FScore (F-10) and the
Jaccard Coefficient arrived at similar results, where
Rand50 received an extremely low score, and MFS
scored the highest. The F-measure (F-SC) had a
similar profile, except that the decline in evaluation
scores as X grows in RandX is somewhat less.
The paired F-Score (F-10), the F-Measure (F-SC),
and F1 (F1-13) all score MFS at approximately 54%,
which is intuitively appealing since that is the per-
centage of instances correctly clustered if all in-
stances are placed into a single cluster. However, in
204
Table 1: Experimental Results
System F-10 F-SC Jaccard F1-13 RI ARI clusters size
sys1 46.53 46.90 31.79 56.83 52.18 5.75 2.5 26.5
sys7 45.89 44.03 31.03 58.78 52.04 6.78 3.0 25.2
sys9 35.56 37.21 22.24 57.02 54.63 2.59 3.3 19.8
Rand2 41.49 42.86 26.99 54.89 50.06 -0.04 2.0 32.0
Rand5 25.17 31.28 14.52 56.73 56.13 0.12 5.0 12.8
Rand10 15.05 28.71 8.18 59.67 58.10 0.02 10.0 6.4
Rand25 7.01 26.78 3.63 66.89 59.24 -0.15 23.2 2.8
Rand50 4.07 25.97 2.00 76.19 59.73 0.10 35.9 1.8
MFS 54.06 54.42 39.90 54.42 39.90 0.0 1.0 64.0
Gold 100.00 100.00 100.00 100.00 100.00 99.0 7.7 11.6
other cases these measures begin to diverge. F1 (F1-
13) tends to score random baselines even higher than
MFS, and Rand50 gets a higher score than Rand2,
which is somewhat counter intuitive. In fact accord-
ing to F1 (F1-13), Rand50 would have been the top
ranked system in task 11. It appears that F1 (F1-
13) is strongly influenced by cluster purity, but does
not penalize a system for creating too many clusters.
Thus, as the number of clusters increases, F1 (F1-
13) will consistently improve since smaller clusters
are nearly always more pure than larger ones.
Interestingly enough, the Rand Index (RI) and the
Jaccard Coefficient both score MFS at 39%. This
number does not have an intuitively appealing inter-
pretation, and thereafter RI and Jaccard diverge. RI
scores random baselines higher than MFS, whereas
the Jaccard Coefficient takes the more reasonable
path of scoring random baselines well below MFS.
3.2 Duluth Systems Evaluation
The FScore (F-10), F-Measure (F-SC), and Jaccard
Coefficient result in a comparable and consistent
view of the system results. sys1 was found to be the
most accurate, followed closely by sys7. All three
measures showed that sys9 lagged considerably.
While all three systems relied on second?order
co-occurrences, sys7 used the least amount of data,
while sys9 used the most. This shows that better re-
sults can be obtained using the Web snippets to be
clustered as the source of the co?occurrence data (as
sys1 and sys7 did) rather than larger amounts of pos-
sibly less relevant text (as sys9 did).
Each of these systems created a roughly compara-
ble number of clusters (on average, per query term,
shown in the column labeled clusters). sys7 created
2.53, while sys9 created 3.01, and sys1 found 3.32.
The average number of web snippets in the discov-
ered clusters (shown in the column labeled size) are
likewise somewhat consistent: sys1 was the largest
at 26.5, sys7 had 25.2, and sys9 was the smallest
with 19.8. The gold standard found an average of
7.7 queries per cluster and 11.6 snippets per cluster.
After the competition sys1 and sys9 were run
without SVD. There was no significant difference in
results with or without SVD. This is consistent with
previous work that found SVD had relatively little
impact in name discrimination experiments (Peder-
sen et al, 2005).
4 Conclusions
sys7 achieved the best results by using very small
co-occurrence matrices of approximately one to two
hundred rows and columns. While small, this data
was most relevant to the task since it was made up of
the Web snippets to be clustered. sys1 increased the
size of the co?occurrence matrix to 771 x 96 by us-
ing all of the test data, but saw no increase in perfor-
mance. sys9 used the largest corpus, which resulted
in a co?occurrence matrix of 9,853 x 300, and had
the poorest results of the Duluth systems.
Sixty?four instances is a small amount of data for
clustering. In future we will augment each query
with additional unannotated web snippets that will
be discarded after clustering. Hopefully the core 64
instances that remain will be clustered more effec-
tively given the cushion provided by the extra data.
205
References
J. Artiles, E. Amigo?, and J. Gonzalo. 2009. The role of
named entities in Web People Search. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 534?542, Singapore,
August.
A. Di Marco and R. Navigli. 2013. Clustering and di-
versifying web search results with graph-based word
sense induction. Computational Linguistics, 39(4):1?
46.
D. Graff and C. Cieri. 2003. English Gigaword. Lin-
guistic Data Consortium, Philadelphia.
A. Kulkarni and T Pedersen. 2005. SenseClusters: Un-
supervised discrimination and labeling of similar con-
texts. In Proceedings of the Demonstration and Inter-
active Poster Session of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
105?108, Ann Arbor, MI, June.
T. Landauer and S. Dumais. 1997. A solution to Plato?s
problem: The Latent Semantic Analysis theory of ac-
quisition, induction and representation of knowledge.
Psychological Review, 104:211?240.
S. Manandhar, I. Klapaftis, D. Dligach, and S. Pradhan.
2010. SemEval-2010 Task 14: Word sense induction
and disambiguation. In Proceedings of the SemEval
2010 Workshop : the 5th International Workshop on
Semantic Evaluations, Uppsala, Sweden, July.
R. Navigli and D. Vannella. 2013. Semeval-2013 task
11: Evaluating word sense induction and disambigua-
tion within an end-user application. In Proceedings
of the 7th International Workshop on Semantic Eval-
uation (SemEval 2013), in conjunction with the Sec-
ond Joint Conference on Lexical and Computational
Semantics (*SEM-2013), Atlanta, June.
T. Pedersen and A. Kulkarni. 2006a. Automatic clus-
ter stopping with criterion functions and the gap statis-
tic. In Proceedings of the Demonstration Session of
the Human Language Technology Conference and the
Sixth Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 276?279, New York City, June.
T. Pedersen and A. Kulkarni. 2006b. Selecting the
right number of senses based on clustering criterion
functions. In Proceedings of the Posters and Demo
Program of the Eleventh Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 111?114, Trento, Italy, April.
T. Pedersen and A. Kulkarni. 2007. Discovering identi-
ties in web contexts with unsupervised clustering. In
Proceedings of the IJCAI-2007 Workshop on Analytics
for Noisy Unstructured Text Data, pages 23?30, Hy-
derabad, India, January.
T. Pedersen, A. Purandare, and A. Kulkarni. 2005. Name
discrimination by clustering similar contexts. In Pro-
ceedings of the Sixth International Conference on In-
telligent Text Processing and Computational Linguis-
tics, pages 220?231, Mexico City, February.
T. Pedersen. 2006. Determining smoker status using su-
pervised and unsupervised learning with lexical fea-
tures. In Working Notes of the i2b2 Workshop on Chal-
lenges in Natural Language Processing for Clinical
Data, Washington, DC, November.
T. Pedersen. 2007. UMND2 : SenseClusters applied
to the sense induction task of Senseval-4. In Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), pages 394?397, Prague,
Czech Republic, June.
T. Pedersen. 2010a. Computational approaches to mea-
suring the similarity of short contexts. Technical re-
port, University of Minnesota Supercomputing Insti-
tute Research Report UMSI 2010/118, October.
T. Pedersen. 2010b. Duluth-WSI: SenseClusters applied
to the sense induction task of semEval-2. In Proceed-
ings of the SemEval 2010 Workshop : the 5th Interna-
tional Workshop on Semantic Evaluations, pages 363?
366, Uppsala, July.
A. Purandare and T. Pedersen. 2004. Word sense
discrimination by clustering contexts in vector and
similarity spaces. In Proceedings of the Conference
on Computational Natural Language Learning, pages
41?48, Boston, MA.
A. Rosenberg and J. Hirschberg. 2007. V-measure: A
conditional entropy-based external cluster evaluation
measure. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 410?420, Prague, Czech Republic, June.
C. X. Zhai, W. Cohen, and J. Lafferty. 2003. Be-
yond independent relevance: methods and evaluation
metrics for subtopic retrieval. In Proceedings of the
26th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 10?17. ACM.
Y. Zhao and G. Karypis. 2004. Empirical and theoretical
comparisons of selected criterion functions for docu-
ment clustering. Machine Learning, 55:311?331.
206
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 247?251,
Dublin, Ireland, August 23-24, 2014.
Duluth : Measuring Cross?Level Semantic Similarity
with First and Second?Order Dictionary Overlaps
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
tpederse@d.umn.edu
Abstract
This paper describes the Duluth systems
that participated in the Cross?Level Se-
mantic Similarity task of SemEval?2014.
These three systems were all unsupervised
and relied on a dictionary melded together
from various sources, and used first?order
(Lesk) and second?order (Vector) over-
laps to measure similarity. The first?order
overlaps fared well according to Spear-
man?s correlation (top 5) but less so rela-
tive to Pearson?s. Most systems performed
at comparable levels for both Spearman?s
and Pearson?s measure, which suggests
the Duluth approach is potentially unique
among the participating systems.
1 Introduction
Cross?Level Semantic Similarity (CLSS) is a
novel variation on the problem of semantic simi-
larity. As traditionally formulated, pairs of words,
pairs of phrases, or pairs of sentences are scored
for similarity. However, the CLSS shared task
(Jurgens et al., 2014) included 4 subtasks where
pairs of different granularity were measured for
semantic similarity. These included : word-2-
sense (w2s), phrase-2-word (p2w), sentence-2-
phrase (s2p), and paragraph-2-sentence (g2s). In
addition to different levels of granularity, these
pairs included slang, jargon and other examples of
non?standard English.
We were drawn to this task because of our long?
standing interest in semantic similarity. We have
pursued approaches ranging from those that rely
on structured knowledge sources like WordNet
(e.g., WordNet::Similarity) (Pedersen et al., 2004)
to those that use distributional information found
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
in raw text (e.g., SenseClusters) (Purandare and
Pedersen, 2004). Our approach in this shared task
is a bit of both, but relies on using definitions for
each item in a pair so that similarity can be mea-
sured using first or second?order overlaps.
A first?order approach finds direct matches be-
tween the words in a pair of definitions. In a
second?order approach each word in a definition
is replaced by a vector of the words it co?occurs
with, and then the vectors for all the words in a
definition are averaged together to represent the
definition. Then, similarity can be measured by
finding the cosine between pairs of these vectors.
We decided on a definition based approach since
it had the potential to normalize the differences in
granularity of the pairs.
The main difficulty in comparing definitions is
that they can be very brief or may not even ex-
ist at all. This is why we combined various dif-
ferent kinds of resources to arrive at our dictio-
nary. While we achieved near total coverage of
words and senses, phrases were sparsely covered,
and sentences and paragraphs had no coverage. In
those cases we used the text of the phrase, sentence
or paragraph to serve as its own definition.
The Duluth systems were implemented using
the UMLS::Similarity package (McInnes et al.,
2009) (version 1.35)1, which includes support for
user?defined dictionaries, first?order Lesk meth-
ods, and second?order Vector methods. As a result
the Duluth systems required minimal implementa-
tion, so once a dictionary was ready experiments
could begin immediately.
This paper is organized as follows. First, the
first?order Lesk and second?order Vector mea-
sures are described. Then we discuss the details
of the three Duluth systems that participated in
this task. Finally, we review the task results and
consider future directions for this problem and our
system.
1http://umls-similarity.sourceforge.net
247
2 Measures
The Duluth systems use first?order Lesk meth-
ods (Duluth1 and Duluth3) and second?order Vec-
tor methods (Duluth2). These require that defini-
tions be available for both items in a pair, with the
caveat that we use the term definition somewhat
loosely to mean both traditional dictionary defini-
tions as well as various proxies when those are not
available.
2.1 First?order Overlaps : Lesk
The Lesk measure (Lesk, 1986) was originally a
method of word sense disambiguation that mea-
sured the overlap among the definitions of the
possible senses of an ambiguous word with those
of surrounding words (Lesk, 1986). The senses
which have the largest number of overlaps are pre-
sumed to be the correct or intended senses for the
given context. A modified approach compares the
glosses of an ambiguous word with the surround-
ing context (Kilgarriff and Rosenzweig, 2000).
These are both first?order methods where defini-
tions are directly compared with each other, or
with the surrounding context.
In the Duluth systems, we measure overlaps by
summing the number of words shared between
definitions. Sequences of words that match are
weighted more heavily and contribute the square
of their length, while individual matching words
just count as one. For example, given the defini-
tions a small noisy collie and a small noisy bor-
der collie the stop word a would not be matched,
and then small noisy would match (and be given
a score of 4) and then collie would also match
(receiving a score of 1). So, the total Lesk score
would be 5. The scores of the Duluth systems were
normalized by dividing by the maximum Lesk
score for any pair in a subtask. This moves the
scores to a 0?1 scale, where 1.00 means the def-
initions are exactly the same, and where 0 means
they share no words.
One of the main drawbacks of the original Lesk
method is that glosses tend to be very short. Vari-
ous methods have been proposed to overcome this.
For example, (Banerjee and Pedersen, 2003) intro-
duced the Extended Gloss Overlap measure which
creates super?glosses by augmenting the glosses
of the senses to be measured with the glosses of
semantically related senses (which are connected
via relation links in WordNet). This adaptation
of the Lesk measure was first implemented in
WordNet::Similarity (Pedersen et al., 2004) and
then later in UMLS::Similarity (McInnes et al.,
2009). It has been applied to both word sense
disambiguation and semantic similarity, and gen-
erally found to improve on original Lesk (Baner-
jee, 2002; Banerjee and Pedersen, 2002; Patward-
han et al., 2003; McInnes and Pedersen, 2013).
However, the Duluth systems do not build super?
glosses in this way since many of the items in the
pairs are not found in WordNet. However, def-
initions are expanded in a simpler way, by merg-
ing together various different resources to increase
both coverage and the length of definitions.
2.2 Second?order Overlaps : Vector
The main limitation of first?order Lesk ap-
proaches is that if terminology differs from one
definition to another, then meaningful matches
may not be found. For example, consider the def-
initions a small noisy collie and a dog that barks
a lot. A first?order overlap approach would find
no similarity (other than the stop word a) between
these definitions.
In cases like this some form of term expansion
could improve the chances of matching. Synonym
expansion is a well?known possibility, although in
the Duluth systems we opted to expand words with
their co?occurrence vectors. This follows from an
approach to word sense discrimination developed
by (Schu?tze, 1998). Once words are expanded
then all the vectors in a definition are averaged to-
gether and this averaged vector becomes the rep-
resentation of the definition. This idea was first
implemented in WordNet::Similarity (Pedersen et
al., 2004) and then later in UMLS::Similarity
(McInnes et al., 2009), and has been applied to
word sense disambiguation and semantic similar-
ity (Patwardhan, 2003; Patwardhan and Pedersen,
2006; Liu et al., 2012).
The co?occurrences for the words in the defi-
nitions can come from any corpus of text. Once
a co?occurrence matrix is constructed, then each
word in each definition is replaced by its vector
from that matrix. If no such vector is found the
word is removed from the definition. Then, all the
vectors representing a definition are averaged to-
gether, and this vector is used to measure against
other vectors created in the same way. The scores
returned by the Vector measure are between 0 and
1 (inclusive) where 1.00 means exactly the same
and 0 means no similarity.
248
3 Duluth Systems
There were three Duluth systems. Duluth1 and
Duluth3 use first?order Lesk, and Duluth2 uses
second?order Vector. Duluth3 was an ensemble
made up of Duluth1 and a close variant of it (Du-
luth1a, where the only difference was the stop list
employed).
Duluth1 and Duluth2 use the NSP stoplist2
which includes approximately 390 words and
comes from the SMART stoplist. Duluth1a treated
any word with 4 or fewer characters as a stop
word. Stemming was performed by all Duluth sys-
tems using the Porter algorithm as implemented in
the Lingua::Stem::en Perl module.
Before processing, all of the similarity pairs and
the dictionary entries were converted to lower case
and any non alpha-numeric characters were re-
placed with spaces. Also, any stop listed words
were removed.
3.1 Dictionary Creation
The key step for all the Duluth systems is the
creation of the dictionary. We elected to treat
senses as word forms, and so our dictionary did
not make sense distinctions (and would include all
the senses of a word or phrase in its entry).
Since the words and phrases used in some pairs
are slang or non?standard English, traditional lex-
ical resources like WordNet do not provide ad-
equate coverage. However, WordNet provides
a good foundation for coverage of standard En-
glish, so we began by extracting the glosses from
WordNet v3.0 using the WordNet::QueryData Perl
module.
Wiktionary is a crowd sourced lexical resource
that includes more slang and jargon, so we also ex-
tracted entries from it using the Wiktionary::Parser
Perl module. In hopes of increasing our coverage
of phrases in particular, we looked up words and
phrases in Wikipedia using the WWW::Wikipedia
Perl module and used the first paragraph of an en-
try (up to the first heading) as a definition. Finally,
we also used the dict program in Linux which
we configured to use the following resources :
the Collaborative International Dictionary of En-
glish v.0.48 (gcide), Moby Thesaurus II by Grady
Ward, 1.0 (moby-thes), V.E.R.A. ? Virtual Entity
of Relevant Acronyms (June 2006) (vera), the Jar-
gon File (version 4.4.7, 29 Dec 2003) (argon), the
2http://cpansearch.perl.org/src/TPEDERSE/Text-NSP-
1.27/bin/utils/stoplist-nsp.regex
Free On-line Dictionary of Computing (26 July
2010) (foldoc), and the CIA World Factbook 2002
(world02).
The most obvious question that arises about
these resources is how much coverage they pro-
vide for the pairs in the task. Based on experi-
ments on the trial data, we found that none of the
resources individually provided satisfactory cov-
erage, but if they were all combined then coverage
was reasonably good (although still not complete).
In the test data, it turned out there were only 20
items in the w2s subtask for which we did not have
a dictionary entry (out of 1000). However, for p2w
(phrase-2-word) there were 407 items not included
in the dictionary (most of which were phrases).
In the s2p (sentence-2-phrase) subtask there were
only 15 phrases which had definitions, so for this
subtask and also for g2s (paragraph-2-sentence)
the items themselves were the definitions for es-
sentially all the pairs.
Also of interest might be the total size of the
dictionaries created. The number of tokens in
g2s (paragraph-2-sentence) was 46,252, and in s2p
(sentence-2-phrase) it was 12,361. This is simply
the token count for the pairs included in each sub-
task. However, the dictionaries were much larger
for p2w (phrase-2-word), where the token count
was 262,876, and for w2s (word-2-sense) where it
was 499,767.
3.2 Co?occurrence Matrix for Vector
In the Duluth systems, the co?occurrence matrix
comes from treating the WordNet glosses as a cor-
pus. Any pair of words that occur together in a
WordNet gloss are considered a co?occurrence.
There are 117,659 glosses, made up of
1,460,921 words. This resulted in a matrix of
90,516 rows and 99,493 columns, representing
708,152 unique bigrams. The matrix is not sym-
metric since the co?occurrences are bigrams, so
dog house is treated differently than house dog.
The WordNet glosses were extracted from ver-
sion 3.0 using the glossExtract Perl program3.
4 Results
Results for the CLSS task were ranked both
by Pearson?s and Spearman?s Correlation coeffi-
cients. Duluth system results are shown in Tables
1 and 2. These tables also include the results of
3http://www.d.umn.edu/?tpederse/Code/glossExtract-
v0.03.tar.gz
249
Table 1: Spearman?s Results
rank
g2s s2p p2w w2s (of 38)
Top .801 .728 .424 .343 1
Duluth3 .725 .660 .399 .327 3
Duluth1 .726 .658 .385 .316 5
Duluth2 .553 .473 .235 .231 21
Baseline .613 .626 .162 .128
Table 2: Pearson?s Results
rank
g2s s2p p2w w2s (of 38)
Top .811 .742 .415 .355 1
Duluth2 .501 .450 .241 .224 23
Duluth1 .458 .440 .075 .076 30
Duluth3 .455 .426 .075 .080 31
Baseline .527 .562 .165 .110
the top ranked system (which was the same sys-
tem according to both measures) and results from
a baseline system that measures the Least Com-
mon Substring between the terms in a pair, except
in the w2s subtask, where it measured the LCS be-
tween the associated WordNet glosses.
Table 1 shows that the Duluth3 system offers a
slight improvement upon Duluth1. Recall that Du-
luth3 is an ensemble that includes Duluth1 and its
minor variant Duluth1a. Both of these are first?
order methods, and significantly outperform the
second?order method Duluth2.
However, Table 2 tells a completely different
story. There the second?order system Duluth2
performs better, although overall rankings suffer
according to Pearson?s measure. It is also very ap-
parent that the ranks between Pearson?s and Spear-
man?s for Duluth1 and Duluth3 differ significantly
(from 3 to 30 and 5 to 31). This is very atypical,
and most systems maintained approximately the
same rankings between the two correlation mea-
sures. Note that Duluth2 behaves in this way,
where the relative ranking is 21 and 23.
Table 3 shows the number of pairs in each sub-
task which returned a score of 0. This could be due
to missing definitions, or no matches occurring be-
tween the definitions. Interestingly Duluth2 has a
much smaller number of 0 valued scores, which
shows the second?order method provides greater
coverage due to its more flexible notion of match-
ing. However, despite much higher numbers of
Table 3: Number of Pairs with Score of 0
g2s s2p p2w w2s
Duluth1 107 197 211 23
Duluth2 9 101 40 15
Duluth3 101 196 205 23
0s, Duluth1 and Duluth3 perform much better with
Spearman?s rank correlation coefficient. This sug-
gests that there is a kind of precision?recall trade-
off between these systems, where Duluth2 has
higher recall and Duluth1 and Duluth3 have higher
precision.
5 Future Directions
The relatively good performance of the first?order
Duluth systems (at least with respect to rank cor-
relation) shows again the important role of lexical
resources. Our first?order method was not appre-
ciably more complex than the baseline method, yet
it performed significantly better (especially for the
p2w and w2s tasks). This is no doubt due to the
more extensive dictionary that we employed.
That said, our approach to building the dictio-
nary was relatively crude, and could be substan-
tially improved. For example, we could be more
selective in the content we add to the entries for
words or phrases. We could also do more than
simply use the sentences and paragraphs as their
own definitions. For example, we could replace
words or phrases in sentences and paragraphs with
their definitions, and then carry out first or second?
order matching.
Second?order matching did not perform as well
as we had hoped. We believe this is due to the
somewhat noisy nature of the dictionaries we con-
structed, and expanding those definitions by re-
placing words with vectors created even more
noise. We believe that a more refined approach
to creating dictionaries would certainly improve
these results, as would a more selective method of
combining the co?occurrence vectors (rather than
simply averaging them).
Acknowledgments
The Duluth systems relied heavily on the freely
available software package UMLS::Similarity. We
are grateful to Bridget McInnes and Ying Liu for
their work in developing this package, and in par-
ticular for the --dict functionality.
250
References
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted Lesk algorithm for word sense disambigua-
tion using WordNet. In Proceedings of the Third In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics, pages 136?145,
Mexico City, February.
Satanjeev Banerjee and Ted Pedersen. 2003. Ex-
tended gloss overlaps as a measure of semantic re-
latedness. In Proceedings of the Eighteenth Inter-
national Joint Conference on Artificial Intelligence,
pages 805?810, Acapulco, August.
Satanjeev Banerjee. 2002. Adapting the Lesk algo-
rithm for word sense disambiguation to WordNet.
Master?s thesis, University of Minnesota, Duluth,
December.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. Semeval-2014 task 3:
Cross?level semantic similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval 2014), Dublin, August.
Adam Kilgarriff and Joseph Rosenzweig. 2000. Spe-
cial issue on SENSEVAL: Framework and results
for english SENSEVAL. Computers and the Hu-
manities, 34(1?2):15?48.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a
pine cone from an ice cream cone. In Proceedings of
the 5th annual international conference on Systems
documentation, pages 24?26. ACM Press.
Ying Liu, Bridget McInnes, Ted Pedersen, Genevieve
Melton-Meaux, and Serguei Pakhomov. 2012. Se-
mantic relatedness study using second?order co?
occurrence vectors computed from biomedical cor-
pora, UMLS, and WordNet. In Proceedings of the
2nd ACM SIGHIT International Health Informatics
Symposium, pages 363?371, Miami, FL.
Bridget McInnes and Ted Pedersen. 2013. Evaluating
measures of semantic similarity and relatedness to
disambiguate terms in biomedical text. Journal of
Biomedical Informatics, 46:1116?1124.
Bridget McInnes, Ted Pedersen, and Serguei Pakho-
mov. 2009. UMLS-Interface and UMLS-Similarity
: Open source software for measuring paths and
semantic similarity. In Proceedings of the Annual
Symposium of the American Medical Informatics As-
sociation, pages 431?435, San Francisco.
Siddharth Patwardhan and Ted Pedersen. 2006. Us-
ing WordNet-based Context Vectors to Estimate the
Semantic Relatedness of Concepts. In Proceed-
ings of the EACL 2006 Workshop on Making Sense
of Sense: Bringing Computational Linguistics and
Psycholinguistics Together, pages 1?8, Trento, Italy,
April.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted
Pedersen. 2003. Using measures of semantic re-
latedness for word sense disambiguation. In Pro-
ceedings of the Fourth International Conference on
Intelligent Text Processing and Computational Lin-
guistics, pages 241?257, Mexico City, February.
Siddharth Patwardhan. 2003. Incorporating dictionary
and corpus information into a context vector mea-
sure of semantic relatedness. Master?s thesis, Uni-
versity of Minnesota, Duluth, August.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::Similarity - Measuring the
relatedness of concepts. In Proceedings of Fifth An-
nual Meeting of the North American Chapter of the
Association for Computational Linguistics, pages
38?41, Boston, MA.
Amruta Purandare and Ted Pedersen. 2004. Word
sense discrimination by clustering contexts in vector
and similarity spaces. In Proceedings of the Confer-
ence on Computational Natural Language Learning,
pages 41?48, Boston, MA.
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123.
251
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 145?153,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Using Second-order Vectors in a
Knowledge-based Method for Acronym Disambiguation
Bridget T. McInnes?
College of Pharmacy
University of Minnesota
Minneapolis, MN 55455
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
Ying Liu
College of Pharmacy
University of Minnesota
Minneapolis, MN 55455
Serguei V. Pakhomov
College of Pharmacy
University of Minnesota
Minneapolis, MN 55455
Genevieve B. Melton
Institute for Health Informatics
University of Minnesota
Minneapolis, MN 55455
Abstract
In this paper, we introduce a knowledge-based
method to disambiguate biomedical acronyms
using second-order co-occurrence vectors. We
create these vectors using information about a
long-form obtained from the Unified Medical
Language System and Medline. We evaluate
this method on a dataset of 18 acronyms found
in biomedical text. Our method achieves an
overall accuracy of 89%. The results show
that using second-order features provide a dis-
tinct representation of the long-form and po-
tentially enhances automated disambiguation.
1 Introduction
Word Sense Disambiguation (WSD) is the task
of automatically identifying the appropriate sense of
a word with multiple senses. For example, the word
culture could refer to anthropological culture
(e.g., the culture of the Mayan civilization), or a
laboratory culture (e.g., cell culture).
Acronym disambiguation is the task of automat-
ically identifying the contextually appropriate long-
form of an ambiguous acronym. For example, the
acronym MS could refer to the disease Multiple Scle-
rosis, the drug Morphine Sulfate, or the state Missis-
sippi, among others. Acronym disambiguation can
be viewed as a special case of WSD, although, un-
like terms, acronyms tend to be complete phrases
or expressions, therefore collocation features are
not as easily identified. For example, the feature
rate when disambiguating the term interest, as in
?Contact author : bthomson@umn.edu.
interest rate, may not be available. Acronyms also
tend to be noun phrases, therefore syntactic features
do not provide relevant information for the purposes
of disambiguation.
Identifying the correct long-form of an acronym
is important not only for the retrieval of information
but the understanding of the information by the re-
cipient. In general English, Park and Byrd (2001)
note that acronym disambiguation is not widely
studied because acronyms are not as prevalent in lit-
erature and newspaper articles as they are in specific
domains such as government, law, and biomedicine.
In the biomedical sublanguage domain, acronym
disambiguation is an extensively studied problem.
Pakhomov (2002) note acronyms in biomedical lit-
erature tend to be used much more frequently than in
news media or general English literature, and tend
to be highly ambiguous. For example, the Uni-
fied Medical Language System (UMLS), which in-
cludes one of the largest terminology resources in
the biomedical domain, contains 11 possible long-
forms of the acronym MS in addition to the four
examples used above. Liu et al (2001) show that
33% of acronyms are ambiguous in the UMLS. In a
subsequent study, Liu et al (2002a) found that 80%
of all acronyms found in Medline, a large repository
of abstracts from biomedical journals, are ambigu-
ous. Wren and Garner (2002) found that there exist
174,000 unique acronyms in the Medline abstracts
in which 36% of them are ambiguous. The authors
also estimated that the number of unique acronyms
is increasing at a rate of 11,000 per year.
Supervised and semi-supervised methods have
been used successfully for acronym disambiguation
145
but are limited in scope due to the need for sufficient
training data. Liu et al (2004) state that an acronym
could have approximately 16 possible long-forms in
Medline but could not obtain a sufficient number of
instances for each of the acronym-long-form pairs
for their experiments. Stevenson et al (2009) cite
a similar problem indicating that acronym disam-
biguation methods that do not require training data,
regardless if it is created manually or automatically,
are needed.
In this paper, we introduce a novel knowledge-
based method to disambiguate acronyms using
second-order co-occurrence vectors. This method
does not rely on training data, and therefore, is not
limited to disambiguating only commonly occurring
possible long-forms. These vectors are created us-
ing the first-order features obtained from the UMLS
about the acronym?s long-forms and second-order
features obtained from Medline. We show that us-
ing second-order features provide a distinct repre-
sentation of the long-form for the purposes of dis-
ambiguation and obtains a significantly higher dis-
ambiguation accuracy than using first order features.
2 Unified Medical Language System
The Unified Medical Language System (UMLS) is
a data warehouse that stores a number of distinct
biomedical and clinical resources. One such re-
source, used in this work, is the Metathesaurus.
The Metathesaurus contains biomedical and clin-
ical concepts from over 100 disparate terminol-
ogy sources that have been semi-automatically in-
tegrated into a single resource containing a wide
range of biomedical and clinical information. For
example, it contains the Systematized Nomencla-
ture of Medicine?Clinical Terms (SNOMED CT),
which is a comprehensive clinical terminology cre-
ated for the electronic exchange of clinical health
information, the Foundational Model of Anatomy
(FMA), which is an ontology of anatomical concepts
created specifically for biomedical and clinical re-
search, and MEDLINEPLUS, which is a terminol-
ogy source containing health related concepts cre-
ated specifically for consumers of health services.
The concepts in these sources can overlap. For
example, the concept Autonomic nerve exists in both
SNOMED CT and FMA. The Metathesaurus assigns
the synonymous concepts from the various sources
a Concept Unique Identifiers (CUIs). Thus both
the Autonomic nerve concepts in SNOMED CT and
FMA are assigned the same CUI (C0206250). This
allows multiple sources in the Metathesaurus to be
treated as a single resource.
Some sources in the Metathesaurus contain ad-
ditional information about the concept such as a
concept?s synonyms, its definition and its related
concepts. There are two main types of relations
in the Metathesaurus that we use: the parent/child
and broader/narrower relations. A parent/child re-
lation is a hierarchical relation between two con-
cepts that has been explicitly defined in one of the
sources. For example, the concept Splanchnic nerve
has an is-a relation with the concept Autonomic
nerve in FMA. This relation is carried forward to
the CUI level creating a parent/child relations be-
tween the CUIs C0037991 (Splanchnic nerve) and
C0206250 (Autonomic nerve) in the Metathesaurus.
A broader/narrower relation is a hierarchical relation
that does not explicitly come from a source but is
created by the UMLS editors. We use the entire
UMLS including the RB/RN and PAR/CHD rela-
tions in this work.
3 Medline
Medline (Medical Literature Analysis and Retrieval
System Online) is a bibliographic database contain-
ing over 18.5 million citations to journal articles
in the biomedical domain which is maintained by
the National Library of Medicine (NLM). The 2010
Medline Baseline, used in this study, encompasses
approximately 5,200 journals starting from 1948 and
is 73 Gigabytes; containing 2,612,767 unique uni-
grams and 55,286,187 unique bigrams. The majority
of the publications are scholarly journals but a small
number of newspapers, and magazines are included.
4 Acronym Disambiguation
Existing acronym disambiguation methods can be
classified into two categories: form-based and
context-based methods. Form-based methods, such
as the methods proposed by Taghva and Gilbreth
(1999), Pustejovsky et al (2001), Schwartz and
Hearst (2003) and Nadeau and Turney (2005), dis-
ambiguate the acronym by comparing its letters di-
146
rectly to the initial letters in the possible long-forms
and, therefore, would have difficulties in distin-
guishing between acronyms with similar long-forms
(e.g., RA referring to Refractory anemia or Rheuma-
toid arthritis).
In contrast, context-based methods disambiguate
between acronyms based on the context in which the
acronym is used with the assumption that the context
surrounding the acronym would be different for each
of the possible long-forms. In the remainder of this
section, we discuss these types of methods in more
detail.
4.1 Context-based Acronym Disambiguation
Methods
Liu et al (2001) and Liu et al (2002b) introduce
a semi-supervised method in which training and
test data are automatically created by extracting ab-
stracts from Medline that contain the acronym?s
long-forms. The authors use collocations and a bag-
of-words approach to train a Naive Bayes algorithm
and report an accuracy of 97%. This method be-
gins to treat acronym disambiguation as more of a
WSD problem by looking at the context in which
the acronym exists to determine its long-form, rather
than the long-form itself. In a subsequent study, Liu
et al (2004) explore using additional features and
machine learning algorithms and report an accuracy
of 99% using the Naive Bayes.
Joshi (2006) expands on Liu, et als work. They
evaluate additional machine learning algorithms us-
ing unigrams, bigrams and trigrams as features.
They found that given their feature set, SVMs ob-
tain the highest accuracy (97%).
Stevenson et al (2009) re-recreate this dataset us-
ing the method described in Liu et al (2001) to auto-
matically create training data for their method which
uses a mixture of linguistics features (e.g., colloca-
tions, unigrams, bigrams and trigrams) in combina-
tion with the biomedical features CUIs and Medi-
cal Subject Headings, which are terms manually as-
signed to Medline abstracts for indexing purposes.
The authors evaluate the Naive Bayes, SVM and
Vector Space Model (VSM) described by Agirre and
Martinez (2004), and report that VSM obtained the
highest accuracy (99%).
Pakhomov (2002) also developed a semi-
supervised method in which training data was
automatically created by first identifying the long-
form found in the text of clinical reports, replacing
the long-form with the acronym to use as training
data. A maximum entropy model trained and tested
on a corpus of 10,000 clinical notes achieved an
accuracy of 89%. In a subsequent study, Pakhomov
et al (2005) evaluate obtaining training data from
three sources: Medline, clinical records and the
world wide web finding using a combination of
instances from clinical records and the web obtained
the highest accuracy.
Joshi et al (2006) compare using the Naive
Bayes, Decision trees and SVM on ambiguous
acronyms found in clinical reports. The authors
use the part-of-speech, the unigrams and the bi-
grams of the context surrounding the acronym as
features. They evaluate their method on 7,738
manually disambiguated instances of 15 ambiguous
acronyms obtaining an accuracy of over 90% for
each acronym.
5 Word Sense Disambiguation
Many knowledge-based WSD methods have been
developed to disambiguate terms which are closely
related to the work presented in this paper. Lesk
(1986) proposes a definition overlap method in
which the appropriate sense of an ambiguous term
was determined based on the overlap between its
definition in a machine readable dictionary (MRD).
Ide and Ve?ronis (1998) note that this work provided
a basis for most future MRD disambiguation meth-
ods; including the one presented in this paper.
Banerjee and Pedersen (2002) use the Lesk?s
overlap method to determine the relatedness be-
tween two concepts (synsets) in WordNet. They ex-
tend the method to not only include the definition
(gloss) of the two synsets in the overlap but also the
glosses of related synsets.
Wilks et al (1990) expand upon Lesk?s method by
calculating the number of times the words in the def-
inition co-occur with the ambiguous words. In their
method, a vector is created using the co-occurrence
information for the ambiguous word and each of its
possible senses. The similarity is then calculated be-
tween the ambiguous word?s vector and each of the
sense vectors. The sense whose vector is most simi-
lar is assigned to the ambiguous word.
147
0
.3
0 0 0 0 0 0disphosphoric
glucose
fructose
phosphoric
esters
changed
effect
0 0 0 0 0
glycolyte
en
zym
es
co
m
bined
decreases
intensity
acid
0
m
etabolites
FEATURES
0 0 0 0 .2 0acid 0 0 0 .1 0 0
0 0 0 0 .5 0 0esters 0 0 0 0 0 0
0 .1 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0
fructose
0 0 0 0 0 0
0 0 0 0 0 0 0
diphosphate
0 0 0 0 0 0isomer
0 0 0 0 0 0 0prevalent 0 0 0 0 0 0
0 .1 0 .3 .5 .2 02nd order vector forFructose Diphosphate 0 0 0 .1 0 0
Ex
te
nd
ed
 D
ef
in
iti
on

fo
r F
ru
ct
os
e 
Di
ph
os
ph
at
e
Figure 1: 2nd Order Vector for Fructose Diphosphate (FDP)
Patwardhan and Pedersen (2006) introduce a vec-
tor measure to determine the relatedness between
pairs of concepts. In this measure, a second order
co-occurrence vector is created for each concept us-
ing the words in each of the concepts definition and
calculating the cosine between the two vectors. This
method has been used in the task of WSD by calcu-
lating the relatedness between each possible sense
of the ambiguous word and its surrounding context.
The context whose sum is the most similar is as-
signed to the ambiguous word.
Second-order co-occurrence vectors were first in-
troduced by Schu?tze (1992) for the task of word
sense discrimination and later extended by Puran-
dare and Pedersen (2004). As noted by Pedersen
(2010), disambiguation requires a sense-inventory
in which the long-forms are known ahead of time,
where as in discrimination this information is not
known a priori.
6 Method
In our method, a second-order co-occurrence vec-
tor is created for each possible long-form of the
acronym, and the acronym itself. The appropriate
long-form of the acronym is then determined by
computing a cosine between the vector represent-
ing the ambiguous acronym and each of the vectors
representing the long-forms. The long-form whose
vector has the smallest angle between it and the
acronym vector is chosen as the most likely long-
form of the acronym.
To create a second-order vector for a long-form,
we first obtain a textual description of the long-form
in the UMLS, which we refer to as the extended defi-
nition. Each long-form, from our evaluation set, was
mapped to a concept in the UMLS, therefore, we use
the long-form?s definition plus the definition of its
parent/children and narrow/broader relations and the
terms in the long-form.
We include the definition of the related concepts
because not all concepts in the UMLS have a defini-
tion. In our evaluation dataset, not a single acronym
has a definition for each possible long-form. On
average, each extended definition contains approx-
imately 453 words. A short example of the extended
definition for the acronym FDP when referring to
148
fructose diphosphate is: ? Diphosphoric acid esters
of fructose. The fructose diphosphate isomer is most
prevalent. fructose diphosphate.?
After the extended definition is obtained, we cre-
ate the second-order vector by first creating a word
by word co-occurrence matrix in which the rows
represent the content words in the long-forms, ex-
tended definition, and the columns represent words
that co-occur in Medline abstracts with the words in
the definition. Each cell in this matrix contains the
Log Likelihood Ratio (Dunning (1993)) of the word
found in the row and the word in the column. Sec-
ond, each word in the long-forms, extended defini-
tion is replaced by its corresponding vector, as given
in the co-occurrence matrix. The centroid of these
vectors constitutes the second order co-occurrence
vector used to represent the long-form.
For example, given the example corpus contain-
ing two instances: 1) The metabolites, glucose fruc-
tose and their phosphoric acid esters are changed
due to the effect of glycolytic enzymes, and 2)
The phosphoric acid combined with metabolites de-
creases the intensity. Figure 1 shows how the
second-order co-occurrence vector is created for the
long-form fructose diphosphate using the extended
definition and features from our given corpus above.
The second-order co-occurrence vector for the
ambiguous acronym is created in a similar fashion,
only rather than using words in the extended defini-
tion, we use the words surrounding the acronym in
the instance.
Vector methods are subject to noise introduced by
features that do not distinguish between the differ-
ent long-forms of the acronym. To reduce this type
of noise, we select the features to use in the second
order co-occurrence vectors based on the following
criteria: 1) second order feature cannot be a stop-
word, and 2) second order feature must occur at least
twice in the feature extraction dataset and not occur
more than 150 times. We also experiment with the
location of the second-order feature with respect to
the first-order feature by varying the window size of
zero, four, six and ten words to the right and the left
of the first-order feature. The experiments in this
paper were conducted using CuiTools v0.15. 1
Our method is different from other context-based
1http://cuitools.sourceforge.net
acronym disambiguation methods discussed in the
related work because it does not require annotated
training data for each acronym that needs to be dis-
ambiguated. Our method differs from the method
proposed by Wilks et al (1990) in two fundamen-
tal aspects: 1) using the extended definition of
the possible long-forms of an acronym, and 2) using
second-order vectors to represent the instance con-
taining the acronym and each of the acronym?s pos-
sible long-forms.
7 Data
7.1 Acronym Dataset
We evaluated our method on the ?Abbrev? dataset 2
made available by Stevenson et al (2009). The
acronyms and long-forms in the data were initially
presented by Liu et al (2001). Stevenson et al
(2009) automatically re-created this dataset by iden-
tifying the acronyms and long-forms in Medline ab-
stracts and replacing the long-form in the abstract
with its acronym. Each abstract contains approxi-
mately 216 words. The dataset consists of three sub-
sets containing 100 instances, 200 instances and 300
instances of the ambiguous acronym referred to as
Abbrev.100, Abbrev.200, Abbrev.300, respectively.
The acronyms long-forms were manually mapped to
concepts in the UMLS by Stevenson, et al
A sufficient number of instances were not found
for each of the 21 ambiguous acronyms by Steven-
son et al (2009). For example, ?ASP? only con-
tained 71 instances and therefore not included in any
of the subsets. ?ANA? and ?FDP? only contained
just over 100 instances and therefore, are only in-
cluded in the Abbrev.100 subset. ?ACE?, ?ASP?
and ?CSF? were also excluded because several of
the acronyms? long-forms did not occur frequently
enough in Medline to create a balanced dataset.
We evaluate our method on the same subsets that
Stevenson et al (2009) used to evaluate their super-
vised method. The average number of long-forms
per acronym is 2.6 and the average majority sense
across all subsets is 70%.
7.2 Feature Extraction Dataset
We use abstracts from Medline, containing ambigu-
ous acronym or long-form, to create the second-
2http://nlp.shef.ac.uk/BioWSD/downloads/corpora
149
order co-occurrence vectors for our method as de-
scribed in Section 6. Table 1 shows the number of
Medline abstracts extracted for the acronyms.
Acronyms # Abstracts Acronym # Abstracts
ANA 3,267 APC 11,192
BPD 3,260 BSA 10,500
CAT 44,703 CML 8,777
CMV 13,733 DIP 2,912
EMG 16,779 FDP 1,677
LAM 1,572 MAC 6,528
MCP 2,826 PCA 11,044
PCP 5,996 PEG 10,416
PVC 2,780 RSV 5,091
Table 1: Feature Extraction Data for Acronyms
8 Results
Table 2 compares the majority sense baseline and the
first-order baseline with the results obtained using
our method on the Acronym Datasets (Abbrev.100,
Abbrev.200 and Abbrev.300) using a window size
of zero, four, six and ten. Differences between the
means of disambiguation accuracy produced by var-
ious approaches were tested for statistical signifi-
cance using the pair-wise Student?s t-tests with the
significance threshold set to 0.01.
Window Abbrev
Size 100 200 300
Maj. Sense Baseline 0.70 0.70 0.70
1-order Baseline 0.57 0.61 0.61
Our Method
0 0.83 0.83 0.81
4 0.86 0.87 0.86
6 0.88 0.90 0.89
10 0.88 0.90 0.89
Table 2: Overall Disambiguation Results
The majority sense baseline is often used to evalu-
ate supervised learning algorithms and indicates the
accuracy that would be achieved by assigning the
most frequent sense (long-form) to every instance.
The results in Table 2 demonstrate that our method is
significantly more accurate than the majority sense
baseline (p ? 0.01).
We compare the results using second-order vec-
tors to first-order vectors. Table 2 shows that ac-
curacy of the second-order results is significantly
higher than the first-order results (p ? 0.01).
The results in Table 2 also show that, as the win-
dow size grows from zero to six, the accuracy of the
system increases and plateaus at a window size of
ten. There is no statistically significant difference
between using a window size of six and ten but there
is a significant difference between a window size of
zero and six, as well as four and six (p ? 0.01).
Acronym # Long Abbrev Abbrev Abbrev
forms 100 200 300
ANA 3 0.84
APC 3 0.88 0.87 0.87
BPD 3 0.96 0.95 0.95
BSA 2 0.95 0.93 0.92
CAT 2 0.88 0.87 0.87
CML 2 0.81 0.84 0.83
CMV 2 0.98 0.98 0.98
DIP 2 0.98 0.98
EMG 2 0.88 0.89 0.88
FDP 4 0.65
LAM 2 0.86 0.87 0.88
MAC 4 0.94 0.95 0.95
MCP 4 0.73 0.67 0.68
PCA 4 0.78 0.79 0.79
PCP 2 0.97 0.96 0.96
PEG 2 0.89 0.89 0.88
PVC 2 0.95 0.95
RSV 2 0.97 0.98 0.98
Table 3: Individual Results using a Window Size of 6.
9 Error Analysis
Table 3 shows the results obtained by our method for
the individual acronyms using a window size of six,
and the number of possible long-forms per acronym.
Of the 18 acronyms, three obtain an accuracy below
80 percent: FDP, MCP and PCA.
FPD has four possible long-forms: Fructose
Diphosphate (E1), Formycin Diphosphate (E2), Fib-
rinogen Degradation Product (E3) and Flexor Dig-
itorum Profundus (E4). The confusion matrix in
Table 4 shows that the method was unable to dis-
tinguish between the two long-forms, E1 and E2,
which are both diphosphates, nor E2 and E3.
Long-Form E1 E2 E3 E4
E1: Fructose Diphosphate
E2: Formycin Diphosphate 5 2 11 19
E3: Fibrinogen Degradation Product 4
E4: Flexor Digitorum Profundus 59
Table 4: FDP Confusion Matrix
MCP also has four possible long-forms: Multicat-
alytic Protease (E1), Metoclopramide (E2), Mono-
cyte Chemoattractant Protein (E3) and Membrane
150
Cofactor Protein (E4). The confusion matrix in Ta-
ble 5 shows that the method was not able to distin-
guish between E3 and E4, which are both proteins,
and E1, which is a protease (an enzyme that breaks
down a protein).
Long-Form E1 E2 E3 E4
E1: Multicatalytic Protease 1 5 6 1
E2: Metoclopramide 15
E3: Monocyte Chemoattractant Protein 1 3 44 11
E4: Membrane Cofactor Protein 13
Table 5: MCP Confusion Matrix
PCA has four possible long-forms: Passive Cu-
taneous Anaphylaxis (E1), Patient Controlled Anal-
gesia (E2), Principal Component Analysis (E3), and
Posterior Cerebral Artery (E4). The confusion ma-
trix in Table 6 shows that the method was not able
to distinguish between E2 and E3. Analyzing the
extended definitions of the concepts showed that E2
includes the definition to the concept Pain Manage-
ment. The words in this definition overlap with
many of the words used in E3s extended definition.
Long-Form E1 E2 E3 E4
E1:Passive Cutaneous Anaphylaxis 18 6 1
E2:Patient Controlled Analgesia 5 15
E3:Principal Component Analysis 48
E4:Posterior Cerebral Artery 7
Table 6: PCA Confusion Matrix
10 Comparison with Previous Work
Of the previously developed methods, Liu et al
(2004) and Stevenson et al (2009) evaluated their
semi-supervised methods on the same dataset as we
used for the current study. A direct comparison
can not be made between our method and Liu et al
(2004) because we do not have an exact duplication
of the dataset that they use. Their results are com-
parable to Stevenson et al (2009) with both report-
ing results in the high 90s. Our results are directly
comparable to Stevenson et al (2009) who report
an overall accuracy of 98%, 98% and 99% on the
Abbrev.100, Abbrev.200 and Abbrev.300 datasets
respectively. This is approximately 10 percentage
points higher than our results.
The advantage of the methods proposed by
Stevenson et al (2009) and Liu et al (2004) is that
they are semi-supervised which have been shown to
obtain higher accuracies than methods that do not
use statistical machine learning algorithms. The dis-
advantage is that sufficient training data are required
for each possible acronym-long-form pair. Liu et
al. (2004) state that an acronym could have approxi-
mately 16 possible long-forms in Medline but a suf-
ficient number of instances for each of the acronym-
long-form pairs were not found in Medline and,
therefore, evaluated their method on 15 out of the
original 34 acronyms. Stevenson et al (2009) cite
a similar problem in re-creating this dataset. This
shows the limitation to these methods is that a suffi-
cient number of training examples can not be ob-
tained for each acronym that needs to be disam-
biguated. The method proposed in the paper does
not have this limitation and can be used to disam-
biguate any acronym in Medline.
11 Discussion
In this paper, we presented a novel method to disam-
biguate acronyms in biomedical text using second-
order features extracted from the UMLS and Med-
line. The results show that using second-order fea-
tures provide a distinct representation of the long-
form that is useful for disambiguation.
We believe that this is because biomedical text
contains technical terminology that has a rich source
of co-occurrence information associated with them
due to their compositionality. Using second-order
information works reasonably well because when
the terms in the extended definition are broken up
into their individual words, information is not being
lost. For example, the term Patient Controlled Anal-
gesia can be understood by taking the union of the
meanings of the three terms and coming up with an
appropriate definition of the term (patient has con-
trol over their analgesia).
We evaluated various window sizes to extract the
second-order co-occurrence information from, and
found using locally occurring words obtains a higher
accuracy. This is consistent with the finding reported
by Choueka and Lusignan (1985) who conducted an
experiment to determine what size window is needed
for humans to determine the appropriate sense of an
ambiguous word.
The amount of data used to extract the second-
151
order features for each ambiguous acronym varied
depending on its occurrence in Medline. Table 1 in
Section 7.2 shows the number of abstracts in Med-
line used for each acronym. We compared the accu-
racy obtained by our method using a window size of
six on the Abbrev.100 dataset with the number of ab-
stracts in the feature extraction data. We found that
the accuracy was not correlated with the amount of
data used (r = 0.07). This confirms that it is not the
quantity but the content of the contextual informa-
tion that determines the accuracy of disambiguation.
We compared using second-order features and
first-order features showing that the second-order re-
sults obtained a significantly higher accuracy. We
believe that this is because the definitions of the pos-
sible concepts are too sparse to provide enough in-
formation to distinguish between them. This find-
ing coincides to that of Purandare and Pedersen
(2004) and Pedersen (2010) who found that with
large amounts of data, first-order vectors perform
better than second-order vectors, but second-order
vectors are a good option when large amounts of
data are not available.
The results of the error analysis indicate that
for some acronyms using the extended definition
does not provide sufficient information to make
finer grained distinctions between the long-forms.
This result also indicates that, although many long-
forms of acronyms can be considered coarse-grained
senses, this is not always the case. For example, the
analysis of MCP showed that two of its possible
long-forms are proteins which are difficult to differ-
entiate from given the context.
The results of the error analysis also show that
indicative collocation features for acronyms are not
easily identified because acronyms tend to be com-
plete phrases. For example, two of the possible
long-forms of DF are Fructose Diphosphate and
Formycin Diphosphate.
Two main limitations of this work must be men-
tioned to facilitate the interpretation of the results.
The first is the small number of acronyms and the
small number of long-forms per acronym in the
dataset; however, the acronyms in this dataset are
representative of the kinds of acronyms one would
expect to see in biomedical text. The second limita-
tion is that the dataset contains only those acronyms
whose long-forms were found in Medline abstracts.
The main goal of this paper was to determine if the
context found in the long-forms, extended definition
was distinct enough to distinguish between them us-
ing second-order vectors. For this purpose, we feel
that the dataset was sufficient although a more ex-
tensive dataset may be needed in the future for im-
proved coverage.
12 Future Work
In the future, we plan to explore three different
avenues. The first avenue is to look at obtaining
contextual descriptions of the possible long-forms
from resources other than the UMLS such as the
MetaMapped Medline baseline and WordNet. The
second avenue is limiting the features that are used
in the instance vectors. The first-order features in
the instance vector contain the words from the entire
abstract. As previously mentioned, vector methods
are subject to noise, therefore, in the future we plan
to explore using only those words that are co-located
next to the ambiguous acronym. The third avenue is
expanding the vector to allow for terms. Currently,
we use word vectors, in the future, we plan to extend
the method to use terms, as identified by the UMLS,
as features rather than single words.
We also plan to test our approach in the clinical
domain. We believe that acronym disambiguation
may be more difficult in this domain due to the in-
crease amount of long-forms as seen in the datasets
used by Joshi et al (2006) and Pakhomov (2002).
13 Conclusions
Our study constitutes a significant step forward in
the area of automatic acronym ambiguity resolu-
tion, as it will enable the incorporation of scalable
acronym disambiguation into NLP systems used for
indexing and retrieval of documents in specialized
domains such as medicine. The advantage of our
method over previous methods is that it does not re-
quire manually annotated training for each acronym
to be disambiguated while still obtaining an overall
accuracy of 89%.
Acknowledgments
This work was supported by the National Insti-
tute of Health, National Library of Medicine Grant
#R01LM009623-01.
152
References
E. Agirre and D. Martinez. 2004. The Basque Country
University system: English and Basque tasks. In Pro-
ceedings of the 3rd ACL workshop on the Evaluation
of Systems for the Semantic Analysis of Text (SENSE-
VAL), pages 44?48.
S. Banerjee and T. Pedersen. 2002. An adapted lesk al-
gorithm for word sense disambiguation using Word-
Net. In Proceedings of the 3rd International Confer-
ence on Intelligent Text Processing and Computational
Linguistics, pages 136?145.
Y. Choueka and S. Lusignan. 1985. Disambiguation
by short contexts. Computers and the Humanities,
19(3):147?157.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
N. Ide and J. Ve?ronis. 1998. Introduction to the special
issue on word sense disambiguation: the state of the
art. Computational Linguistics, 24(1):2?40.
M. Joshi, S. Pakhomov, T. Pedersen, and C.G. Chute.
2006. A comparative study of supervised learning as
applied to acronym expansion in clinical reports. In
Proceedings of the Annual Symposium of AMIA, pages
399?403.
M. Joshi. 2006. Kernel Methods for Word Sense Disam-
biguation and Abbreviation Expansion. Master?s the-
sis, University of Minnesota.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: how to tell a pine cone
from an ice cream cone. Proceedings of the 5th Annual
International Conference on Systems Documentation,
pages 24?26.
H. Liu, YA. Lussier, and C. Friedman. 2001. Disam-
biguating ambiguous biomedical terms in biomedical
narrative text: an unsupervised method. Journal of
Biomedical Informatics, 34(4):249?261.
H. Liu, A.R. Aronson, and C. Friedman. 2002a. A study
of abbreviations in MEDLINE abstracts. In Proceed-
ings of the Annual Symposium of AMIA, pages 464?
468.
H. Liu, S.B. Johnson, and C. Friedman. 2002b. Au-
tomatic resolution of ambiguous terms based on ma-
chine learning and conceptual relations in the UMLS.
JAMIA, 9(6):621?636.
H. Liu, V. Teller, and C. Friedman. 2004. A multi-
aspect comparison study of supervised word sense dis-
ambiguation. JAMIA, 11(4):320?331.
D. Nadeau and P. Turney. 2005. A supervised learning
approach to acronym identification. In Proceedings
of the 18th Canadian Conference on Artificial Intelli-
gence, pages 319?329.
S. Pakhomov, T. Pedersen, and C.G. Chute. 2005. Ab-
breviation and acronym disambiguation in clinical dis-
course. In Proceedings of the Annual Symposium of
AMIA, pages 589?593.
S. Pakhomov. 2002. Semi-supervised maximum en-
tropy based approach to acronym and abbreviation
normalization in medical texts. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 160?167.
Y. Park and R.J. Byrd. 2001. Hybrid text mining for find-
ing abbreviations and their definitions. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 126?133.
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based context vectors to estimate the semantic related-
ness of concepts. In Proceedings of the EACL 2006
Workshop Making Sense of Sense - Bringing Com-
putational Linguistics and Psycholinguistics Together,
pages 1?8.
T. Pedersen. 2010. The effect of different context repre-
sentations on word sense discrimination in biomedical
texts. In Proceedings of the 1st ACM International IHI
Symposium, pages 56?65.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proceedings of the Conference on
Computational Natural Language Learning (CoNLL),
pages 41?48.
J. Pustejovsky, J. Castano, B. Cochran, M. Kotecki,
M. Morrell, and A. Rumshisky. 2001. Extraction and
disambiguation of acronym-meaning pairs in medline.
Unpublished manuscript.
H. Schu?tze. 1992. Dimensions of meaning. In Proceed-
ings of the 1992 ACM/IEEE Conference on Supercom-
puting, pages 787?796.
A.S. Schwartz and M.A. Hearst. 2003. A simple
algorithm for identifying abbreviation definitions in
biomedical text. In Proceedings of the Pacific Sym-
posium on Biocomputing (PSB), pages 451?462.
M. Stevenson, Y. Guo, A. Al Amri, and R. Gaizauskas.
2009. Disambiguation of biomedical abbreviations.
In Proceedings of the ACL BioNLP Workshop, pages
71?79.
K. Taghva and J. Gilbreth. 1999. Recognizing acronyms
and their definitions. ISRI UNLV, 1:191?198.
Y. Wilks, D. Fass, C.M. Guo, J.E. McDonald, T. Plate,
and B.M. Slator. 1990. Providing machine tractable
dictionary tools. Machine Translation, 5(2):99?154.
J.D. Wren and H.R. Garner. 2002. Heuristics for iden-
tification of acronym-definition patterns within text:
towards an automated construction of comprehensive
acronym-definition dictionaries. Methods of Informa-
tion in Medicine, 41(5):426?434.
153
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 131?133,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
The Ngram Statistics Package (Text::NSP) - A Flexible Tool for Identifying
Ngrams, Collocations, and Word Associations
Ted Pedersen?
Department of Computer Science
University of Minnesota
Duluth, MN 55812
Satanjeev Banerjee
Twitter, Inc.
795 Folsom Street
San Francisco, CA 94107
Bridget T. McInnes
College of Pharmacy
University of Minnesota
Minneapolis, MN 55455
Saiyam Kohli
SDL Language Weaver, Inc.
6060 Center Drive, Suite 150
Los Angeles, CA 90045
Mahesh Joshi
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
Ying Liu
College of Pharmacy
University of Minnesota
Minneapolis, MN 55455
Abstract
The Ngram Statistics Package (Text::NSP)
is freely available open-source software that
identifies ngrams, collocations and word as-
sociations in text. It is implemented in Perl
and takes advantage of regular expressions to
provide very flexible tokenization and to allow
for the identification of non-adjacent ngrams.
It includes a wide range of measures of associ-
ation that can be used to identify collocations.
1 Introduction
The identification of multiword expressions is a key
problem in Natural Language Processing. Despite
years of research, there is still no single best way
to proceed. As such, the availability of flexible and
easy to use toolkits remains important. Text::NSP
is one such package, and includes programs for
counting ngrams (count.pl, huge-count.pl), measur-
ing the association between the words that make up
an ngram (statistic.pl), and for measuring correlation
between the rankings of ngrams created by differ-
ent measures (rank.pl). It is also able to identify n-
th order co-occurrences (kocos.pl) and pre?specified
compound words in text (find-compounds.pl).
This paper briefly describes each component of
NSP. Additional details can be found in (Banerjee
and Pedersen, 2003) or in the software itself, which
is freely available from CPAN 1 or Sourceforge 2.
?Contact author : tpederse@d.umn.edu. Note that authors
Banerjee, McInnes, Kohli and Joshi contributed to Text::NSP
while they were at the University of Minnesota, Duluth.
1http://search.cpan.org/dist/Text-NSP/
2http://sourceforge.net/projects/ngram/
2 count.pl
The program count.pl takes any number of plain
text files or directories of such files and counts the
total number of ngrams as well their marginal to-
tals. It provides the ability to define what a token
may be using regular expressions (via the --token
option). An ngram is an ordered sequence of n to-
kens, and under this scheme tokens may be almost
anything, including space separated strings, charac-
ters, etc. Also, ngrams may be made up of nonadja-
cent tokens due to the --window option that allows
users to specify the number of tokens within which
an ngram must occur.
Counting is done using hashes in Perl which are
memory intensive. As a result, NSP also provides
the huge-count.pl program and various other huge-
*.pl utilities that carry out count.pl functionality us-
ing hard drive space rather than memory. This can
scale to much larger amounts of text, although usu-
ally taking more time in the process.
By default count.pl treats ngrams as ordered se-
quences of tokens; dog house is distinct from house
dog. However, it may be that order does not always
matter, and a user may simply want to know if two
words co-occur. In this case the combig.pl program
adjusts counts from count.pl to reflect an unordered
count, where dog house and house dog are consid-
ered the same. Finally, find-compounds.pl allows a
user to specify a file of already known multiword ex-
pressions (like place names, idioms, etc.) and then
identify all occurrences of those in a corpus before
running count.pl
131
3 statistic.pl
The core of NSP is a wide range of measures of
association that can be used to identify interest-
ing ngrams, particularly bigrams and trigrams. The
measures are organized into families that share com-
mon characteristics (which are described in detail in
the source code documentation). This allows for an
object oriented implementation that promotes inher-
itance of common functionality among these mea-
sures. Note that all of the Mutual Information mea-
sures are supported for trigrams, and that the Log-
likelihood ratio is supported for 4-grams. The mea-
sures in the package are shown grouped by family
in Table 1, where the name by which the measure is
known in NSP is in parentheses.
Table 1: Measures of Association in NSP
Mutual Information (MI)
(ll) Log-likelihood Ratio (Dunning, 1993)
(tmi) true MI (Church and Hanks, 1990)
(pmi) Pointwise MI (Church and Hanks, 1990)
(ps) Poisson-Stirling (Church, 2000)
Fisher?s Exact Test (Pedersen et al, 1996)
(leftFisher) left tailed
(rightFisher) right tailed
(twotailed) two tailed
Chi-squared
(phi) Phi Coefficient (Church, 1991)
(tscore) T-score (Church et al, 1991)
(x2) Pearson?s Chi-Squared (Dunning, 1993)
Dice
(dice) Dice Coefficient (Smadja, 1993)
(jaccard) Jaccard Measure
(odds) Odds Ratio (Blaheta and Johnson, 2001)
3.1 rank.pl
One natural experiment is to compare the output of
statistic.pl for the same input using different mea-
sures of association. rank.pl takes as input the out-
put from statistic.pl for two different measures, and
computes Spearman?s Rank Correlation Coefficient
between them. In general, measures within the same
family correlate more closely with each other than
with measures from a different family. As an ex-
ample tmi and ll as well as dice and jaccard differ
by only constant terms and therefore produce identi-
cal rankings. It is often worthwhile to conduct ex-
ploratory studies with multiple measures, and the
rank correlation can help recognize when two mea-
sures are very similar or different.
4 kocos.pl
In effect kocos.pl builds a word network by finding
all the n-th order co-occurrences for a given literal
or regular expression. This can be viewed somewhat
recursively, where the 3-rd order co-occurrences of
a given target word are all the tokens that occur with
the 2-nd order co-occurrences, which are all the to-
kens that occur with the 1-st order (immediate) co-
occurrences of the target. kocos.pl outputs chains of
the form king -> george -> washington,
where washington is a second order co-occurrence
(of king) since both king and washington are first
order co-occurrences of george. kocos.pl takes as
input the output from count.pl, combig.pl, or statis-
tic.pl.
5 API
In addition to command line support, Test::NSP of-
fers an extensive API for Perl programmers. All of
the measures described in Table 1 can be included
in Perl programs as object?oriented method calls
(Kohli, 2006), and it is also easy to add new mea-
sures or modify existing measures within a program.
6 Development History of Text::NSP
The Ngram Statistics Package was originally imple-
mented by Satanjeev Banerjee in 2000-2002 (Baner-
jee and Pedersen, 2003). Amruta Purandare in-
corporated NSP into SenseClusters (Purandare and
Pedersen, 2004) and added huge-count.pl, com-
big.pl and kocos.pl in 2002-2004. Bridget McInnes
added the log-likelihood ratio for longer ngrams
in 2003-2004 (McInnes, 2004). Saiyam Kohli
rewrote the measures of association to use object-
oriented methods in 2004-2006, and also added
numerous new measures for bigrams and trigams
(Kohli, 2006). Mahesh Joshi improved cross plat-
form support and created an NSP wrapper for Gate
in 2005-2006. Ying Liu wrote find-compounds.pl
and rewrote huge-count.pl in 2010-2011.
132
References
S. Banerjee and T. Pedersen. 2003. The design, imple-
mentation, and use of the Ngram Statistics Package.
In Proceedings of the Fourth International Conference
on Intelligent Text Processing and Computational Lin-
guistics, pages 370?381, Mexico City, February.
D. Blaheta and M. Johnson. 2001. Unsupervised learn-
ing of multi-word verbs. In ACL/EACL Workshop on
Collocations, pages 54?60, Toulouse, France.
K. Church and P. Hanks. 1990. Word association norms,
mutual information and lexicography. Computational
Linguistics, pages 22?29.
K. Church, W. Gale, P. Hanks, and D. Hindle. 1991. Us-
ing statistics in lexical analysis. In U. Zernik, editor,
Lexical Acquisition: Exploiting On-Line Resources to
Build a Lexicon. Lawrence Erlbaum Associates, Hills-
dale, NJ.
K. Church. 1991. Concordances for parallel text. In
Seventh Annual Conference of the UW Centre for New
OED and Text Research, Oxford, England.
K. Church. 2000. Empirical estimates of adaptation:
The chance of two noriegas is closer to p/2 than p2.
In Proceedings of the 18th International Conference
on Computational Linguistics (COLING-2000), pages
180?186, Saarbru?cken, Germany.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
S. Kohli. 2006. Introducing an object oriented design to
the ngram statistics package. Master?s thesis, Univer-
sity of Minnesota, Duluth, July.
B. McInnes. 2004. Extending the log-likelihood ratio
to improve collocation identification. Master?s thesis,
University of Minnesota, Duluth, December.
T. Pedersen, M. Kayaalp, and R. Bruce. 1996. Signifi-
cant lexical relationships. In Proceedings of the Thir-
teenth National Conference on Artificial Intelligence,
pages 455?460, Portland, OR, August.
A. Purandare and T. Pedersen. 2004. Word sense
discrimination by clustering contexts in vector and
similarity spaces. In Proceedings of the Conference
on Computational Natural Language Learning, pages
41?48, Boston, MA.
F. Smadja. 1993. Retrieving collocations from text:
Xtract. Computational Linguistics, 19(1):143?177.
133
Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 33?37,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Identifying Collocations to Measure Compositionality :
Shared Task System Description
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812 USA
tpederse@d.umn.edu
Abstract
This paper describes three systems from the
University of Minnesota, Duluth that partici-
pated in the DiSCo 2011 shared task that eval-
uated distributional methods of measuring se-
mantic compositionality. All three systems
approached this as a problem of collocation
identification, where strong collocates are as-
sumed to be minimally compositional. duluth-
1 relies on the t-score, whereas duluth-2 and
duluth-3 rely on Pointwise Mutual Informa-
tion (pmi). duluth-1 was the top ranked sys-
tem overall in coarse?grained scoring, which
was a 3-way category assignment where pairs
were assigned values of high, medium, or low
compositionality.
1 Introduction
An ngram or phrase that means more than the sum
of its parts is said to be non-compositional. Well
known examples include kick the bucket (i.e., to die)
and red tape (i.e., bureaucratic steps). The ability
to measure the degree of semantic compositionality
in a unit of text is a key capability of NLP systems,
since non-compositional phrases can be treated as
a single unit, rather than as a series of individual
words. This has a tremendous impact on word sense
disambiguation systems, for example, since a non-
compositional phrase will often have just one pos-
sible sense and thereby be reduced to a trivial case,
whereas the combination of possible sense assign-
ments for the words that make up a phrase can grow
exponentially.
Identifying collocations is another key capability
of NLP systems. Collocations are generally consid-
ered to be units of text that occur with some regular-
ity and may have some non-compositional meaning.
The Duluth systems that participated in the DiSCo
2011 shared task (Biemann and Giesbrecht, 2011)
seek to determine the degree to which collocation
identification techniques can be used to measure se-
mantic compositionality. In particular, these systems
are based on the following hypothesis:
An ngram that has a high score accord-
ing to a measure of association (for iden-
tifying collocations) will be less composi-
tional (and less literal) than those that have
lower scores.
The intuition underlying this hypothesis is a high
score from a measure of association shows that the
words in the ngram are occurring together more of-
ten than would be expected by chance, and that
a non-compositional phrase is unlikely to occur in
such a way that it looks like a chance event.
2 System Development
The Duluth systems were developed by identify-
ing collocations based on frequency counts obtained
from the WaCky English corpus (Baroni et al,
2009), hereafter referred to as the corpus. The part
of speech tags were removed from the corpus, and
the text was converted to lower case. A set of 139
training pairs was provided by the task organizers
that had been manually rated for compositionality.
This gold standard data was used to select which
measures of association would form the basis of the
Duluth systems. Thereafter a separate set of 174 test
pairs were provided by the organizers for evaluation.
33
2.1 Collocation Discovery
The Ngram Statistics Package (Text::NSP) (Baner-
jee and Pedersen, 2003) was used to measure the
association between the training pairs based on fre-
quency count data collected from the corpus. All
thirteen measures in the Ngram Statistics Package
were employed, including the Log-likelihood Ra-
tio (ll) (Dunning, 1993), Pointwise Mutual Informa-
tion (pmi) (Church and Hanks, 1990), Mutual Infor-
mation (tmi) (Church and Hanks, 1990), Poisson-
Stirling (ps) (Church, 2000), Fisher?s Exact Test
(leftFisher, rightFisher, and twotailed) (Pedersen et
al., 1996), Jaccard Coefficient (jaccard), Dice Coef-
ficient (dice), Phi Coefficient (phi), t-score (tscore)
(Church and Hanks, 1990), Pearson?s Chi-Squared
Test (x2), and the Odds Ratio (odds).
These measure the co-occurrence of word pairs
(bigrams) relative to their individual frequencies and
assess how likely it is that the word pair is occurring
together by chance (and is therefore likely composi-
tional) or has some significant pattern of occurrence
as a pair (in which case it is non-compositional).
More formally, many of these methods compare
the observed empirical data with a model that casts
the words in the bigram as independent statistical
events. The measures determine the degree to which
the observed data deviates from what would be ex-
pected under the model of independence. If the ob-
served data differs significantly from that, then there
is no evidence to support the hypothesis that the bi-
gram is a chance event, and we assume that there is
some interesting or significant pattern that implies
non-compositionality. In some cases the training
and test pairs are not adjacent (e.g., reinvent wheel
for reinvent the wheel), and so window sizes of 2, 4,
and 10 words were used when measuring the asso-
ciation between pairs of words. This means that 0, 2
and 8 intervening words were allowed, respectively.
Frequency count data for the word pairs are tabu-
lated as shown in the example in Figure 1. The vari-
able W1 represents the presence or absence of red
in the first position of each word pair, and W2 rep-
resents the presence or absence of tape in the sec-
ond position. This table tells us, for example, that
red tape occurs 5,363 times (n11), that red occurs
18,493 times (n1+), and that bigrams that contain
neither red nor tape occur 68,824,813 times (n22).
The total number of bigrams found in the corpus is
68,845,263 (n++). Note that these counts are based
on a window size of 2. Counts increase with a larger
window size. If the window size were 10, then n11
would tell us how many times red and tape occurred
within 8 words of each other (in order).
W1
W2
tape ?tape totals
n11= n12= n1+=
red 5,363 13,130 18,493
n21= n22= n2+=
?red 1,957 68,824,813 68,826,770
n+1= n+2= n++=
totals 7,320 68,837,943 68,845,263
Figure 1: Contingency Table Counts
2.2 Scoring Word Pairs
The training pairs were ranked according to each of
the measures in Text::NSP, where high scores in-
dicate that two words (w1 and w2) are not occur-
ring together by chance, and that there is a non-
compositional meaning. However, high scores in the
shared task meant exactly the opposite; that a word
pair was highly compositional (and literal). In addi-
tion, the fine grained scoring in the shared task was
on a scale of 0 to 100, and it was required that partic-
ipating systems use that same scale. Thus, the scores
from the measures were converted to this scale as
follows:
Let the maximum value of the Text::NSP mea-
sure for all the pairs in the set under consideration be
max(m(W1, W2)), where m represents the specific
measure being used. Then the score for each word
pair is normalized by dividing it by this maximum
value, and subtracted from 1 and then multiplied by
100. More generally, the fine grained score for any
word pair (w1, w2) as computed by a specific duluth-
x system is dx(w1, w2) and is calculated as follows:
dx(w1, w2) = 100 ? (1?
m(w1, w2)
max(m(W1, W2))
) (1)
Coarse grained scoring is automatically per-
formed by binning all of the resulting scores in the
range 0-33 to low, 34 - 66 to medium and 67 - 100
to high.
34
Table 1: Text::NSP Rank Correlation with Gold Standard
- duluth-1 corresponds to t-score window 10, duluth-2
with pmi window 10 and duluth-3 with pmi window 2
Window Size
Measure 2 4 10
tscore 0.1484 0.2114 0.2674
tmi 0.1335 0.1908 0.2361
ll 0.1336 0.1913 0.2358
frequency 0.1865 0.2100 0.2126
ps 0.0992 0.1554 0.1874
x2 0.1157 0.1172 0.1654
phi 0.1157 0.1167 0.1646
jaccard 0.1253 0.1255 0.1602
dice 0.1253 0.1255 0.1602
odds 0.0216 0.0060 0.0257
pmi -0.0241 -0.0145 0.0143
rightFisher -0.1768 -0.0817 0.0740
leftFisher 0.1316 0.0686 -0.0870
twotailed -0.1445 -0.0651 -0.1064
2.3 Correlation of Word Pairs
Before the evaluation period, it was decided that
duluth-1 (our flagship system) would be based on the
measure of association that had the highest Spear-
man?s rank correlation with the fine grained gold
standard annotations of the training pairs. As can
be seen from Table 1, that measure was the t-score
based on a window size of 10.
As an additional experiment, the ranking of
the training pairs according to each measure in
Text::NSP was compared to the frequency ranking
in the corpus. As can be seen in Table 2, once again
it was the t-score that had the highest correlation.
While the correlation with the training pairs by
the t-score was encouraging, the correlation with
frequency was something of a surprise, and in fact
caused some concern. Could a measure that corre-
lated so highly with frequency really be successful
in measuring semantic compositionality? However,
upon reflection it seemed that correlation with fre-
quency might be quite desirable, and led to the for-
mulation of a second hypothesis:
Very frequent word pairs are more likely
to be compositional (i.e., highly literal)
than are less frequent word pairs.
Table 2: Text::NSP Rank Correlation with Frequency -
duluth-1 corresponds to t-score window 10, duluth-2 with
pmi window 10 and duluth-3 with pmi window 2
Window Size
Measure 2 4 10
tscore 0.9857 0.9578 0.8477
ps 0.8856 0.8423 0.8299
ll 0.9082 0.8459 0.6953
tmi 0.9080 0.8459 0.6951
jaccard 0.7170 0.6128 0.5527
dice 0.7170 0.6128 0.5527
phi 0.7038 0.5743 0.4308
x2 0.7039 0.5744 0.4303
rightFisher -0.5998 -0.3279 0.2004
odds 0.3714 0.1483 -0.0353
pmi 0.2487 0.0789 -0.1390
leftFisher 0.5675 0.3500 -0.1726
twotailed -0.5965 -0.4434 -0.2712
The assumption that underlies this hypothesis is that
the most frequent word pairs tend to be very literal
and non-compositional (e.g., for the, in that) and it
would (in general) be a surprise to expect a compo-
sitional pair (e.g., above board, rip saw) to attain as
high a frequency.
3 duluth-1 (t-score in a 10 word window)
The duluth-1 system is based on the t-score in a 10
word window, and was selected because of its high
correlation to the gold standard annotations of the
training pairs and to the frequency ranking of the
training pairs. The t-score optimizes both of our
previous hypotheses, which suggests it should be a
good choice for measuring compositionality.
By way of background, the t-score (t) is formu-
lated as follows (Church et al, 1991), using the no-
tation introduced in Figure 1 :
t = n11 ?m11?n11
(2)
where n11 is the observed count of the word pair,
and m11 is the expected value based on the hypothe-
sized model of independence between variables W1
and W2. As such,
m11 =
n1+ ? n+1
n++
(3)
35
If there is little difference between the observed
and expected values, then the t-score is closer to zero
(or even less than zero) and the pair of words can be
judged to occur together simply by chance (i.e., the
hypothesis of independence is true).
The t-scores for the test pairs were converted fol-
lowing equation (1), and then submitted for evalu-
ation. duluth-1 placed in the middle ranks in the
fine grain evaluation according to mean distance,
and was the top ranked system according to the label
precision evaluation of coarse grained scoring.
4 duluth-2 (pmi with window size of 10)
In studying Tables 1 and 2, it?s clear that Point-
wise Mutual Information (pmi) deviates rather sig-
nificantly from frequency and the t-score. At the
time of the evaluation, we did not know if our hy-
potheses that motivated the use of the t-score would
prove to be true. If they did not, it seemed sensible to
include the most opposite measure to the t-score, as
a kind of fail safe mechanism for our systems over-
all. In addition, pmi has a fairly significant history of
use in identifying collocations and features for other
NLP tasks (e.g., (Pantel and Lin, 2002)), and so it
seemed like a credible candidate.
pmi has a well known bias towards identifying
words that only occur together, and tends to prefer
less frequent word pairs, and this is why it diverges
so significantly from the t-score and frequency. In-
terestingly, pmi is also based on the same observed
and expected values n11 and m11 as used in the t-
score (and many of the other measures), and is cal-
culated as follows:
pmi = log n11m11
(4)
If there is little difference between the observed
and expected values, then pmi tends towards 0 and
we treat the word pairs as independent and compo-
sitional.
duluth-2 relies on a window size of 10, since it di-
verges dramatically from the t-score and frequency.
5 duluth-3 (pmi with window size of 2)
duluth-3 is a very close relative of duluth-2, and dif-
fers only in that it requires word pairs to be adjacent.
Given the wider window sizes in duluth-2, it is clear
that if a pair has a high pmi score, they must only oc-
cur (mostly) together. duluth-3 only considers adja-
cent words, and so the words that make up the pairs
may also appear elsewhere in the corpus. As such
duluth-3 may tend to assign higher pmi scores than
the more exacting duluth-2 (where high scores mean
low compositionality). And in fact this is what oc-
curred. In the coarse scoring scheme, duluth-1 only
identified 2 low compositional word pairs, whereas
duluth-2 identified 46 and duluth-3 identified 70.
Despite the difference in the window size the rank
correlation between duluth-2 and duluth-3 is rela-
tively high (.9330). Both performed comparably in
the evaluation, being near the bottom of both the
fine and coarse grained evaluations. By comparison,
duluth-1 and duluth-2 have a relatively low rank cor-
relation of .1756, and duluth-1 and duluth-3 have a
modest correlation of .3438.
6 Conclusions
The Duluth systems seek to evaluate the degree to
which measures of collocation are able to measure
semantic compositionality as well. The results of
this shared task suggest that the t-score is well suited
to make coarse grained distinctions between high,
medium, and low levels of compositionality, since
duluth-1 was the top ranked system in the coarse
grained evaluation. While this success might be
considered surprising due to the simplicity of the
approach, it should not be underestimated. There
are two separate hypotheses that underly the t-score
and its use in measuring semantic compositionality.
These hold that word pairs with high measures of as-
sociation are more likely to be non?compositional,
and that more frequent word pairs are more likely to
be compositional. Of the measures evaluated in this
study, the t-score was best able to optimize both of
these conditions.
7 Acknowledgements
The experiments in this paper were conducted
with version 1.23 of the Ngram Statistics Pack-
age (Text::NSP), which is implemented in Perl and
freely available from http://ngram.sourceforge.net.
36
References
S. Banerjee and T. Pedersen. 2003. The design, imple-
mentation, and use of the Ngram Statistics Package.
In Proceedings of the Fourth International Conference
on Intelligent Text Processing and Computational Lin-
guistics, pages 370?381, Mexico City, February.
M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta.
2009. The WaCky wide web: A collection of very
large linguistically processed web-crawled corpora.
Language Resources and Evaluation, 43(3):209?226.
C. Biemann and E. Giesbrecht. 2011. Distributional
semantics and compositionality 2011: Shared task
description and results. In Proceedings of DiSCo?
2011 in conjunction ACL HLT 2011, Portland, Oregon,
June. Association for Computational Linguistics.
K. Church and P. Hanks. 1990. Word association norms,
mutual information and lexicography. In Proceed-
ings of the 28th Annual Meeting of the Association for
Computational Linguistics, pages 76?83.
K. Church, W. Gale, P. Hanks, and D. Hindle. 1991. Us-
ing statistics in lexical analysis. In U. Zernik, editor,
Lexical Acquisition: Exploiting On-Line Resources to
Build a Lexicon. Lawrence Erlbaum Associates, Hills-
dale, NJ.
K. Church. 2000. Empirical estimates of adaptation:
The chance of two noriegas is closer to p/2 than p2.
In Proceedings of the 18th International Conference
on Computational Linguistics (COLING-2000), pages
180?186, Saarbru?cken, Germany.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Proceedings of ACM SIGKDD Confer-
ence on Knowledge Discovery and Data Mining-2002.
T. Pedersen, M. Kayaalp, and R. Bruce. 1996. Signifi-
cant lexical relationships. In Proceedings of the Thir-
teenth National Conference on Artificial Intelligence,
pages 455?460, Portland, OR, August.
37
