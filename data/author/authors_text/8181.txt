Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 761?768
Manchester, August 2008
Event Frame Extraction Based on a Gene Regulation Corpus 
Yutaka Sasaki 1    Paul Thompson 1    Philip Cotter 1    John McNaught 1, 2 
Sophia Ananiadou1, 2 
 
1 School of Computer Science, University of Manchester 
2  National Centre for Text Mining 
MIB, 131 Princess Street, Manchester, M1 7DN, United Kingdom 
Yutaka.Sasaki@manchester.ac.uk 
 
 Abstract 
This paper describes the supervised ac-
quisition of semantic event frames  based 
on a corpus of biomedical abstracts, in 
which the biological process of E. coli 
gene regulation has been linguistically 
annotated by a group of biologists in the 
EC research project "BOOTStrep". Gene 
regulation is one of the rapidly advancing 
areas for which information extraction 
could boost research. Event frames are an 
essential linguistic resource for extraction 
of information from biological literature.  
This paper presents a specification for 
linguistic-level annotation of gene regu-
lation events, followed by novel methods 
of automatic event frame extraction from 
text.  The event frame extraction per-
formance has been evaluated with 10-
fold cross validation.  The experimental 
results show that a precision of nearly 
50% and a recall of around 20% are 
achieved.  Since the goal of this paper is 
event frame extraction, rather than event 
instance extraction, the issue of low re-
call could be solved by applying the 
methods to a larger-scale corpus. 
1 Introduction 
This paper describes the automatic extraction of 
linguistic event frames based on a corpus of 
MEDLINE abstracts that has been annotated 
with gene regulation events by a group of do-
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
 
main experts. Annotation is centred on both 
verbs and nominalised verbs that describe rele-
vant events. For each event, semantic arguments 
that occur within the same sentence are marked 
and labelled with semantic roles and named en-
tity (NE) types. 
The focus of the paper is the extraction of 
event frames on the basis of the annotated corpus 
using machine learning techniques. Event frames 
are linguistic specifications concerning the be-
haviour of verbs and nominalised verbs, in terms 
of the number and types of semantic arguments 
with which they typically co-occur in texts. Our 
eventual goal is to exploit such information to 
improve information extraction. Event frame ex-
traction is different to event instance extraction 
(or template filling). Our event frames are des-
tined for incorporation in the BOOTStrep 
BioLexicon to support identification of relevant 
event instances and  discovery of event instance 
participants by NLP systems. 
2 Background 
There are several well-established, large-scale 
repositories of semantic frames for general lan-
guage, e.g., VerbNet (Kipper-Schuler, 2005), 
PropBank (Palmer et al, 2005) and FrameNet 
(Rupenhoffer et al 2006). These all aim to char-
acterise verb behaviour in terms of the semantic 
arguments with which verbs occur but differ in 
how they represent semantic arguments and 
groupings of verbs.  
In VerbNet, the semantic roles of arguments 
come from frame-independent roles, e.g. Agent, 
Patient, Location and Instrument.  
In contrast, PropBank and FrameNet use a 
mixture of role types: some are common amongst 
a number of frames; others are specific to par-
ticular frames.  
Whilst FrameNet and VerbNet differ in their 
treatment of semantic roles, they both specify  
761
semantic frames that correspond to groups of 
verbs with similar behaviour. However, frames 
in PropBank correspond to individual verbs. 
   Biology-specific extensions have been at-
tempted both for PropBank (Wattarujeekrit et al, 
2004) and FrameNet (Dolbey et al, 2006). How-
ever, to our knowledge, there has been no such 
attempt at extending VerbNet into the biological 
domain. 
In common with VerbNet, our work is focus-
sed on producing event frames that use a set of 
frame-independent semantic roles. However, we 
adopt a smaller set of roles tailored to the domain. 
This use of frame-independent roles allows lin-
guistic generalisations to be captured more easily 
(Cohen and Hunter, 2006). Also, the use of such 
roles is more suitable for direct exploitation by 
NLP systems (Zaphirain et al, 2008).  
Unlike VerbNet, we aim to produce a set of 
frames that are verb-specific (rather than frames 
that apply to groups of verbs). Verb-specific 
frames are able to provide more detailed argu-
ment specifications?particularly important in 
the biomedical field, where phrases that identify 
information such as location, manner, timing and 
condition are essential for correct interpretation 
of events (Tsai et al 2007).  
3 Annotated corpus 
To aid semantic event frame extraction, we need 
a corpus annotated with event-level information.  
Several already exist for biology.  Some target 
extraction of PropBank-style frames (e.g. Chou 
et al (2006), Kulick et al (2004)). The corpus 
produced by Kim et al (2008) uses frame-
independent roles. However, only a few semantic 
argument types are annotated.  
The target of our event frame extraction is a 
set of semantic frames which specify all potential 
arguments of gene regulation events. For this 
purpose, we had to produce our own annotated 
corpus, using a larger set of event-independent 
semantic roles than Kim et al (2008). Our roles 
had to cover sufficiently wide scope to allow an-
notation and characterization of all instantiated 
arguments of relevant events within texts. To our 
knowledge, this makes our scheme unique within 
the biomedical field. 
In contrast to many other comparable re-
sources, annotated events are centred on both 
verbs and nominalised verbs, such as transcrip-
tion and control. Nominalised verbs play an im-
portant and possibly dominant role in biological 
texts (Cohen and Hunter, 2006). Our own corpus 
confirms this, in that the nominalised verb ex-
pression is the most commonly annotated word 
on which gene regulation events are centred. By 
annotating events centred on nominalised verbs 
in a similar way to verbs, it becomes possible to 
extract separate event frames for nominalised 
verbs. This enables their potentially idiosyncratic 
behaviour to be accounted for.  
Role Name Description Example (bold = semantic argument, italics = focussed verb)  
AGENT Drives/instigates event The narL gene product activates the nitrate reductase operon 
THEME a) Affected by/results from event 
b) Focus of events describing states 
recA protein was induced by UV radiation 
The FNR protein resembles CRP 
MANNER Method/way in which event is car-
ried out 
cpxA gene increases the levels of csgA transcription by dephosphoryla-
tion of CpxR 
INSTRUMENT Used to carry out event EnvZ functions through OmpR to control NP porin gene expression in 
Escherichia coli K-12. 
LOCATION Where complete event takes place Phosphorylation of OmpR modulates expression of the ompF and ompC 
genes in Escherichia coli 
SOURCE Start point of event A transducing lambda phage was isolated from a strain harboring a 
glpD??lacZ fusion  
DESTINATION End point of event Transcription of gntT is activated by binding of the cyclic AMP (cAMP)-
cAMP receptor protein (CRP) complex to a CRP binding site 
TEMPORAL Situates event in time w.r.t another 
event 
The Alp protease activity is detected in cells after introduction of plas-
mids carrying the alpA gene 
CONDITION Environmental conditions/changes 
in conditions 
Strains carrying a mutation in the crp structural gene fail to repress ODC 
and ADC activities in response to increased cAMP 
RATE Change of level or rate marR mutations elevated inaA expression by  10-  to 20-fold over that of 
the wild-type. 
DESCRIPTIVE-
AGENT 
Provides descriptive information 
about the AGENT of the event 
It is likely that HyfR acts as a formate-dependent regulator of the hyf 
operon 
DESCRIPTIVE-
THEME 
Provides descriptive information 
about the AGENT of the event 
The FNR protein resembles CRP. 
PURPOSE Purpose/reason for the event occur-
ring 
The fusion strains were used to study the regulation of the cysB gene by 
assaying the fused lacZ gene product 
Table 1. Semantic Roles 
762
Our annotated corpus consists of 677 MED-
LINE abstracts on E. Coli. Within them, a total 
of 4770 gene regulation events have been anno-
tated. 
3.1 Semantic Roles 
Based on the observations of Tsai et al(2007) 
regarding the most important types of informa-
tion specified for biomedical events, together 
with detailed examination of a large number of 
relevant events within our corpus, in discussion 
with biologists, we defined a set of 13 frame-
independent semantic roles that are suitable for 
the domain.   
 Certain roles within the set are domain-
independent, and are based on those used in 
VerbNet, e.g. AGENT, THEME, and LOCA-
TION. To these, we have added a number of do-
main-dependent roles, e.g. CONDITION and 
MANNER. The size of the role set attempts to 
balance the need for a sufficiently wide-ranging 
set of roles with the need for one that is as small 
and general as possible, to reduce the burden on 
annotators, whilst also helping to ensure consis-
tency across extracted verb frames. The full set 
of semantic roles used is shown in Table 1.  
3.2  Named Entity Categorisation 
 Although our semantic roles are rather general, 
the annotation scheme allows more detailed in-
formation about semantic arguments to be en-
coded in the corpus through the assignment of 
named entity (NE) tags. Unlike other corpus pro-
jects, we do not annotate all entities within each 
abstract, but just those entities that occur as se-
mantic arguments of annotated gene regulation 
events. 
Our set of NE tags goes beyond the traditional 
view of NEs,  in that labelling is extended to in-
clude events represented by nominalised verbs 
(e.g. repression). A total of 61 NE classes have 
been defined as being relevant to the gene regu-
lation field, which are divided into four entity-
specific super-classes (DNA, PROTEIN, EX-
PERIMENTAL and ORGANISMS) and one 
event-specific super-class (PROCESSES). The 
NEs within each of these classes are hierarchi-
cally-structured. Table 2 provides definitions of 
each of these five super-classes. The NEs corre-
spond to classes in the Gene Regulation Ontol-
ogy (Splendiani et al 2007), which has been de-
veloped as part of the BOOTStrep project in 
which this work has been carried out. The Gene 
Regulation Ontology integrates parts of other 
established bio-ontologies, such as Gene Ontol-
ogy (Ashburner et al, 2000) and Sequence On-
tology (Eilbeck,2005). 
3.3 Annotation process 
Annotation was carried out over a period of three 
months by seven PhD students with experience 
in gene regulation and with native or near-native 
competence in English. 
 Prior to annotation, each abstract was auto-
matically processed. Firstly, linguistic pre-
processing (i.e. morphological analysis, POS 
tagging and syntactic chunking)1 was carried out.  
 Secondly, all occurrences from a list of 700 
biologically relevant verbs were automatically 
marked. Annotators then considered each marked 
verb within an abstract. If the verb denoted a 
gene regulation event, annotators then: 
a. Identified all semantic arguments of the 
verb within the sentence 
b. Assigned a semantic role to each identi-
fied argument 
                                                 
1 Each abstract to be annotated is first pre-processed with 
the GENIA tagger (Tsuruoka et al 2005). 
NE class Definition 
DNA 
Entities chiefly composed of nucleic 
acids and their structural or positional 
references. This includes the physical 
structure of all DNA-based entities 
and the functional roles associated 
with regions thereof. 
PROTEIN 
Entities chiefly composed of amino 
acids and their positional references. 
This includes the physical structure 
and functional roles associated with 
each type. 
EXPERIMENTAL 
Both physical and methodological 
entities, either used, consumed or 
required for a reaction to take place. 
ORGANISMS 
Entities representing individuals or 
collections of living things and their 
component parts. 
PROCESSES A set of event classes used to label biological processes described in text.  
Table 2. Description of NE super-classes  Table 3. Most commonly annotated verbs and 
nominalised verbs 
Word Count Type 
expression 409 NV 
encode 351 V 
transcription 125 NV 
bind 110 V 
require 100 V 
express 93 V 
regulate 91 V 
synthesis 90 NV 
contain 80 V 
induce 78 V 
763
c. If appropriate, assigned named entity 
categories to (parts of) the semantic ar-
gument span 
d. If the argument corresponded to a nomi-
nalised verb, repeated steps a?c to iden-
tify its own arguments. 
Syntactic chunks were made visible to annota-
tors. In conjunction with annotation guidelines, 
the chunks were used to help ensure consistency 
of annotated semantic arguments. For example, 
the guidelines state that semantic arguments 
should normally consist of complete (and pref-
erably single) syntactic chunks.  The annotation 
was performed using a customised version of 
WordFreak (Morton and LaCivita, 2003), a Java-
based linguistic annotation tool.  
3.4  Corpus statistics 
The corpus is divided into 2 parts, i.e. 
1) 597 abstracts, each annotated by a single 
annotator, containing a total of 3612 
events, 
2) 80 pairs of double-annotated documents, 
allowing checking of inter-annotator 
agreement and consistency, and contain-
ing 1158 distinct events.  
 
 In the corpus, 277 distinct verbs were annotated 
as denoting gene regulation events, of which 73 
were annotated 10 times or more. In addition, 
annotation has identified 135 relevant nominal-
ised verbs, of which 22 were annotated 10 times 
or more. The most commonly annotated verbs 
and nominalised verbs are shown in Table 3.  
3.5 Inter-annotator agreement 
Inter-annotator agreement statistics for the 80 
pairs of duplicate-annotated abstracts are shown 
in Table 4.  
The figures shown in Table 4 are direct 
agreement rates. Whilst the Kappa statistic is 
very familiar for calculating inter-annotator 
agreement, we follow Wilbur et al (2006) and 
Pyysalo (2007) in choosing not to use it, because 
it is not appropriate or possible to calculate it for 
all of the above statistics. For instance: 
 
1. For some tasks, like annotation of events and 
arguments spans, deciding how to calculate 
random agreement is not clear. 
2. The Kappa statistic assumes that annotation 
categories are discrete and mutually exclu-
sive. This is not the case for the NE catego-
ries, which are hierarchically structured.   
 
 Table 4 shows that, in terms of identifying 
events  (i.e. determining which verbs denote gene 
regulation events), agreement between annotators 
is reached about half the time. The main reason 
for this relatively low figure is that reaching a 
consensus on the specific types of events to be 
annotated under the heading of ?gene regulation? 
required a large amount of discussion. Thus, par-
ticularly towards the start of the annotation phase, 
annotators tended to either under- or over-
annotate the events. 
Greater amounts of consistency seem to be 
achievable for other sub-tasks of the annotation, 
with agreement rates for the identification and 
subsequent labelling of semantic arguments be-
ing achieved in around three quarters of cases.  
Comparable, but slightly lower rates of agree-
ment were achieved in the identification of NEs. 
In terms of assigning categories to them, the 
agreement rate for exact category matches is a 
little lower (62%). However, if we relax the 
matching conditions by exploiting the hierarchi-
cal structure of the NE categories (i.e. if we 
count as a match the cases where the category 
assigned by one annotator was the ancestor of the 
category assigned by the other annotator), then 
the agreement increases by around 11%.  
The large number of NE categories (61), 
makes the decision of the most appropriate cate-
gory rather complex; this was verified by the an-
notators themselves. Based on this, we will con-
sider the use of a more coarse-grained scheme 
when carrying out further annotation of this type. 
However, in the current corpus, the hierarchical 
structuring of the NE categories means that it 
would be possible to use a smaller set of catego-
ries by mapping the specific categories to more 
general ones.   
4 Corpus Format 
For the purposes of event frame extraction, the 
annotations in the corpus were converted to an 
XML-style inline format consisting of three dif-
ferent types of element: 
 
Table 4. Inter-annotator agreement rates  
AGREEMENT RATE VALUE 
Event identification 0.49 
Argument identification (partial span match) 0.73 
Semantic role assignment 0.78 
NE identification (partial span match) 0.68 
NE category assignment (exact) 0.62 
NE category assignment (including parent) 0.65 
NE category assignment (including ancestors) 0.73 
  
764
EVENT ? surrounds text spans (i.e. verb 
phrases and nominalised verbs) on which 
events are centred. 
SLOT ? surrounds spans corresponding to se-
mantic arguments (i.e. slots) of events.  The 
head verb/nominalised verb of the event is also 
treated as a SLOT, with role type Verb. The 
eventid attribute links each slot with its respec-
tive event, whilst the Role attribute indicates 
the semantic role assigned to the slot.  
NE ? surrounds text spans annotated as named 
entities. The cat attribute stores the NE cate-
gory assigned. 
 
Where there are several annotations over some 
text span, elements are embedded inside each 
other. If more than one annotation begins at a 
particular offset, then the ordering of the embed-
ding is fixed, so that SLOT elements are embed-
ded inside EVENT elements, and that NE ele-
ments are embedded inside SLOT elements. An 
example of the annotation for the sentence "TaqI 
restriction endonuclease has been subcloned 
downstream from an inducible phoA promoter" 
is shown below: 
 
<SLOT argid="4" eventid="5" Role="Theme">  
<NE cat="ENZYME">TaqI restriction endonucle-
ase</NE></SLOT> <EVENT id="5"> 
has been <SLOT argid="6" eventid="5" 
Role="Verb">subcloned </SLOT></EVENT>  
<SLOT argid="8" eventid="5" 
Role="Location">downstream from  
<NE cat="PROMOTER">an inducible phoA pro-
moter</NE></SLOT>. 
 
The EVENT created over the VP chunk has 
been subcloned has been annotated as having 2 
semantic arguments (SLOTs), i.e. a THEME,  
TaqI restriction endonuclease and a LOCATION, 
i.e. downstream from an inducible phoA pro-
moter. A 3rd SLOT element corresponds to the 
head verb in the VP chunk. Named entity tags 
have also been assigned to the THEME span and 
part of the LOCATION span.  
5 Event Patterns and Event Frames 
This section defines event patterns and event 
frames.  Event patterns are syntactic patterns of 
sequences of surface words, NEs, and semantic 
roles, whilst event frames are the record-like data 
structures consisting of event slots and event slot 
values. 
5.1 Event Patterns 
Event patterns are fragments of event annotations 
in which semantic arguments are generalized to 
their semantic role and NE categories, if present. 
An event pattern is extracted for each unique 
event id within an abstract. An event annotation 
span begins with the earliest SLOT span, and 
ends with the latest SLOT assigned to the event. 
An example event span is as follows: 
 
<SLOT eventid="9" Role="Agent">  
<NE cat="OPERON"> transfer operon</NE></SLOT> 
<EVENT id="9"><SLOT eventid="9" Role="Verb"> 
expression </SLOT></EVENT></SLOT> of  
<SLOT eventid="9" Role="Theme">  
<NE cat="DNA_FRAGMENT"> F-like plasmids 
</NE></SLOT> 
 
For each event, each event span is generalized 
into an event pattern as follows:  
? ?Verb? role slots of the event are converted 
into a tuple consisting of the role type, part-
of-speech and surface form, i.e., 
[Verb:POS:verb].  
? Other semantic role slots and their NE slots 
for the event are generalized to tuples con-
sisting of the role and NE super class, i.e., 
[role:NE_super_class]. 
? Other XML tags are removed. 
 
The above example event span is thus general-
ized to the following event pattern: 
 
[Agent:DNA] [Verb:NN:expression] of [Theme:DNA]. 
 
5.2 Event frames 
Event frames are directly extracted from event 
patterns, and take the following general form: 
 
event_frame_name( 
     slot_name => slot_value, 
     ? 
     slot_name => slot_value). 
where 
? event_frame_name is the base form of the 
event verb or nominalized verb; 
? slot_names are  the names of the semantic 
roles within the event pattern; 
? slot_values are NE categories, if present 
within the event pattern. 
 
For example, the event frame corresponding to 
the event pattern shown in the previous section is 
as follows: 
expression( Agent=>DNA, 
            Theme=>DNA ). 
 
765
6 Event Frame Extraction 
Our event frame extraction is a fusion of sequen-
tial labelling based on Conditional Random 
Fields (CRF), and event pattern matching. Event 
frames are extracted in three steps.  Firstly, a 
CRF-based Named Entity Recognizer (NER) 
assigns biological NEs to word sequences. Sec-
ondly, a CRF-based semantic role labeller deter-
mines the semantic roles of word sequences with 
NE labels.  Thirdly, word sequences are com-
pared with event patterns derived from the cor-
pus.  Only those event frames whose semantic 
roles, NEs, and verb POS satisfy event pattern 
conditions will be extracted. 
6.1 Biological NER  
Since it is costly and time-consuming to create a 
large-scale training corpus annotated by biolo-
gists, we need to concede to use coarse-grained 
biological NE categories. That is, the NER com-
ponent is trained on the five NE super classes, 
i.e., Protein, DNA, Experimental, Organisms, 
and Processes. 
The NER models are trained by CRFs 
(Lafferty et al, 2001) using the standard IOB2 
labelling method.  That is, the label ``B-NE'' is 
given to the first token of the target NE sequence, 
?I-NE? to each remaining token in the target se-
quence,  and ``O'' to other tokens. 
Features used are as follows: 
? word feature 
- orthographic features: 
 the first letter and the last four letters of the 
word form, in which capital letters in a word are 
normalized to ?A?, lower case letters are normal-
ized to ?a?, and digits are replaced by ?0?. For 
example, the word form ?IL-2? is normalised to 
?AA-0?. 
- postfix features:  the last two and four let-
ters 
? POS feature 
 
We applied first-order CRFs using the above fea-
tures for the tokens within a window size of  ?2 
of the current token. 
6.2 Semantic Role Labelling  
First of all, each NE token sequence identified by 
B and I labels is merged into a single token with 
the NE category name. Then, the semantic role 
labelling models are trained by CRFs in a similar 
way to NER.  That is, the label ``B-Role'' is given 
to the first token of the target Role sequence, ?I-
Role? to each remaining token in the target se-
quence, and ?O? to other tokens. 
Features used here are as follows: 
? word feature 
?  base form feature 
? POS feature 
? NE feature 
 
The window size was ?2 of the current token. 
6.3 Event pattern matching  
When a new sentence is given, sequential label-
ling models decide NE and semantic role labels 
of tokenized input sentences. Then, the token 
sequences are converted into the following token 
sequences with POS, semantic role, and NE in-
formation (called augmented token sequences): 
 
1. Each token sequence labelled by IOB seman-
tic role labels is merged into a token labelled 
with the role. 
2. Verbs and nominalized verbs are converted 
to [Verb:POS:surface_form]. 
3. Tokens with semantic role label and NE su-
per-class are converted into the form 
[Role:NE_super_class]. 
4. Other tokens with O label are converted to 
surface tokens. 
 
Then, event patterns are generalized: 
5. Event patterns are modified so that elements 
corresponding to verbs and nominalized 
verbs will match any words with the same 
POS, e.g., [Verb:POS:*]. 
 
Finally, each event pattern is applied to aug-
mented token sequences one by one:  
6. By matching the generalized event patterns 
with augmented token sequences, i.e. when 
verbs or nominalized verbs and the surround-
ing semantic roles and NEs satisfy the event 
pattern conditions, then successfully unified 
event patterns are extracted as new event pat-
terns. 
7. The newly obtained event patterns are con-
verted into event frames in the same way as 
described in Section 5.2.  
7 Experimental Results 
The aim of this section is to evaluate semantic 
frame extraction performance, given a set of an-
notated training data. 
The annotated corpus was randomly separated 
into 10 document groups and their event patterns 
766
and event frames were segmented into 10 groups 
according to the document separation. 
We conducted 10-fold cross validation based 
on the 10 document groups.  Named entity rec-
ognizers and semantic role labellers were trained 
using 9 groups of annotated documents.  Event 
frames were then extracted from the remaining 
group of documents.  Micro-average precision 
and recall for the set of event frames extracted 
from all the folds were evaluated. 
Table 5 shows the event frame extraction per-
formance.  #TP, #FN, and #FP indicate the num-
ber of true positives, false negatives, and false 
positives, respectively.   
Named entity recognition performance was 
also evaluated (Table 6).  Since the training data 
size is small, the performance is between ap-
proximately 20-60% F-measure. However, this 
will not cause a problem for the event frame ex-
traction task.  This is because, if a particular 
event frame occurs multiple times in a corpus, it 
is sufficient to extract only a single occurrence of 
the event description. So, whilst the NE and se-
mantic role labelling may not be successful for 
all occurrences of the event frame, there is a 
good chance that at least one occurrence of the 
event will be realized in the text in such a way as 
to allow the labelling to be carried out success-
fully, thus allowing the extraction of an appro-
priate event frame.  
8 Discussion 
Linguistic-level event annotation of biological 
events is an inherently difficult task.  This is 
supported by the fact that the inter-annotator 
agreement level for the identification of events 
was 0.49 (see Table 4).  Therefore, in terms of 
event extraction performance, a precision of 
49.0% on 10-fold cross validation is almost 
comparable to human experts. The low recall of 
18.6% may not be an issue, as the recall is likely 
to improve with the size of the target corpus.   
The precision may additionally be underesti-
mated in the evaluation due to inconsistencies in 
the annotation.  We found that the average preci-
sion of our event frame extraction over 10 folds 
is around 30%, despite the fact that the precision 
of all event frames extracted from 10 folds is 
almost 50% compared with the annotated event 
frames in the whole corpus.  This happens be-
cause some events not annotated in a particular 
fold are annotated in the rest of corpus.  From 
this insight, our conjecture is that the true preci-
sion against the whole corpus would be some-
what higher (potentially 70-80%) if we were us-
ing an annotated corpus 10 times larger for the 
evaluation. 
The automatic NER performance was also 
comparable to human annotators. 
There are several approaches to the generation 
of information extraction patterns (e.g. Soderland 
et al, 1995; Califf et al, 1997; Kim and Moldo-
van, 1995).  Our event patterns are similar to in-
formation extraction rules used in conventional 
IE systems.  However, the goal of this paper is 
not event instance extraction but event (or se-
mantic) frame extraction. We also combined 
CRF-based NER and semantic role labelling 
tuned for gene regulation with event extraction 
from sentences so that the clues of gene regula-
tion event frames could be assigned automati-
cally to un-annotated text. 
9 Conclusion  
This paper has presented linguistic annotation of 
gene regulation events in MEDLINE abstracts, 
and automatic event frame extraction based on 
the annotated corpus. Semantic event frames are 
linguistic resources effective in bridging between 
domain knowledge and text in IE tasks. 
Although biological event annotations carried 
out by domain experts is a challenging task, ex-
perimental results on event frame extraction 
demonstrate a precision of almost 50%, which is 
close to the inter-annotator agreement rate of 
human annotators. 
The extracted event frames will be included in 
the BOOTStrep BioLexicon, which will be made 
available for research purposes. 
Acknowledgement 
This research is supported by EC IST project 
FP6-028099 (BOOTStrep), whose Manchester 
team is hosted by the JISC/BBSRC/EPSRC 
sponsored National Centre for Text Mining. 
 
Table 5. 10-fold cross validation results 
 Score #TP #FN #FP 
Recall  0.186 165 730  
Precision 0.490 165  172 
 
Table 6.  NE identification performance 
NE Type Recall Precision F 
DNA 0.627  0.660  0.643  
Protein 0.525  0.633  0.574  
Experimental 0.224  0.512  0.312  
Processes 0.125  0.337  0.182  
Organisms 0.412  0.599  0.488  
 
767
References 
Califf, Mary E. and Raymond J. Mooney (1997).  
Relational Learning of Pattern-Match Rules for In-
formation Extraction, In Proceedings of the ACL-
97 Workshop in Natural Language Learning, pp 9?
15. 
Chou, Wen-Chi., Richard T.H. Tsai, Ying-Shan Su, 
Wei Ku, Ting-Yi Sung and Wen-Lian Hsu (2006). 
A Semi-Automatic Method for Annotating a Bio-
medical Proposition Bank. In Proceedings of the 
Workshop on Frontiers in Linguistically Annotated 
Corpora 2006, pp 5?12. 
Cohen, K. Bretonnel and Laurence Hunter (2006). A 
critical review of PASBio's argument structures for 
biomedical verbs. BMC Bioinformatics 7 (Suppl. 3), 
S5.  
Dolbey, Andrew, Michael Ellsworth and Jan 
Scheffczykx (2006). BioFrameNet: A Domain-
Specific FrameNet Extension with Links to Bio-
medical Ontologies. In O. Bodenreider (Ed.), In 
Proceedings of KR-MED, pp 87?94. 
Eilbeck, Karen, Suzanna .E Lewis., Christopher J. 
Mungall, Mark Yandell, Lincoln Stein, Richard 
Durbin and Michael Ashburner. (2005) The Se-
quence Ontology: A tool for the unification of ge-
nome annotations. Genome Biology 6:R44 
Kim, Jin-Dong,  Tomoko Ohta and Jun?ichi Tsujii 
(2008).  Corpus annotation for mining biomedical 
events from literature. BMC Bioinformatics 9:10.   
Kim, Jun-Tae and Dan I. Moldovan (1995).  Acquisi-
tion of Linguistic Patterns for Knowledge-Based 
Information Extraction. IEEE Transaction on 
Knowledge and Data Engineering (IEEE TKDE), 
7(5), pp.713?724.   
Kipper-Schuler, Karen (2005). VerbNet: A broad-
coverage, comprehensive verb lexicon. PhD Thesis. 
Computer and Information Science Dept., Univer-
sity of Pennsylvania. Philadelphia, PA. 
Kulick Seth, Ann Bies, Mark Liberman, Mark Mandel,  
Ryan McDonald, Martha Palmer, Andrew Schein, 
and Lyle Ungar  (2004) Integrated Annotation for 
Biomedical Information Extraction. In HLT-
NAACL 2004 Workshop: BioLink 2004, Linking 
Biological Literature, Ontologies and Databases, 
pp 61?68.  
Lafferty John, Andrew McCallum and Fernando 
Pereira (2001).  Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labelling Se-
quence Data. In Proceedings of the Eighteenth In-
ternational Conference on    Machine Learning 
(ICML-2001), pp 282?289.  
Morton, Thomas and Jeremy LaCivita (2003). Word-
Freak: an open tool for linguistic annotation. In 
Proceedings of the 2003 Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology, 
pp 17?18. 
Palmer Martha, Paul Kingsbury and Daniel Gildea 
(2005). The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics, 
31(1), pp 71?106. 
Pyysalo, Sampo, Filip Ginter, Juho Heimonen, Jari 
Bj?rne, Jorma Boberg, Jouni J?rvinen and  Tapio 
Salakoski (2007). BioInfer: a corpus for informa-
tion extraction in the biomedical domain?.  BMC 
Bioinformatics 8:50. 
Ruppenhofer, Josef, Michael Ellsworth, Miriam R.L. 
Petruck, Christopher R. Johnson, and Jan  
Scheffczyk (2006).   FrameNet II: Extended The-
ory and Practice. Available online at 
http://framenet.icsi.berkeley.edu/ 
Soderland, Steven, David Fisher, Jonathan Aseltine 
and  Wendy Lenert (1995). CRYSTAL: Inducing a 
Conceptual Dictionary, In Proceedings of The 13th 
International Joint Conference on Artificial Intelli-
gence (IJCAI-95). pp.1314?1319. 
The Gene Ontology Consortium. (2000). Gene Ontol-
ogy: tool for the unification of biology. Nature Ge-
netetics 25, pp 25?29. 
Tsai Richard T.H, Wen-Chi Chou, Ying-San Su, Yu-
Chun Lin, Chen-Lung Sung, Hong-Jie Dai, Irene 
T.H Yeh, Wei Ku, Ting-Yi Sung and Wen-Lian 
Hsu (2007). BIOSMILE: A semantic role labeling 
system for biomedical verbs using a maximum-
entropy model with automatically generated tem-
plate features, BMC Bioinformatics 8:325  
Tsuruoka, Yoshimasa, Yuka Tateishi, Jin-Dong Kim, 
Tomoko Ohta, John McNaught, Sophia Ananiadou, 
and Jun?ichi Tsujii (2005). Developing a Robust 
Part-of-Speech Tagger for Biomedical Text, In Ad-
vances in Informatics - 10th Panhellenic Confer-
ence on Informatics, pp 382?392. 
Wattarujeekrit, Tuangthong, Parantu K. Shah and 
Nigel Collier (2004). PASBio: predicate-argument 
structures for event extraction in molecular biology, 
BMC Bioinformatics 5:155. 
Wilbur, W.John, Andrey Rzhetsky, and Hagit Shatkay 
(2006). New Directions in Biomedical Text Anno-
tations: Definitions. Guidelines and Corpus Con-
struction. BMC Bioinformatics. 7:356 
Zapirain, Be?at, Eneko Agirre, Llu?s M?rquez (2008). 
A Preliminary Study on the Robustness and Generali-
zation of Role Sets for Semantic Role Labeling. In 
Alexander F. Gelbukh (Ed.), Computational Linguis-
tics and Intelligent Text Processing, 9th International 
Conference, CICLing 2008. 
 
768
Dynamic Integration of Distributed Semantic Services:   
Infrastructure for Process Queries and Question Answering 
 
Paul Thompson 
45 Lyme Road, Suite 200 
Dartmouth College 
Hanover, New Hampshire 03755 
Paul.Thompson@dartmouth.edu 
 
 
 
 
1  The IR2P Prototype 
 
The DARPA IXO mission is to develop ?systems 
for real-time sensing, exploitation, and decision making 
in a rich tactical environment?.  The mission includes 
the development of individual technologies for sensors, 
sensor exploitation and command/control as well as the 
technology of information integration. Our research 
focuses on how to integrate distributed services in a 
dynamic networked environment to support IXO 
applications. This dynamic networked environment 
should include the following capabilities (DARPA, 
2002): 
 
? Information users should have scalable dynamically 
changing subscription services to heterogeneous 
information services; 
? Information providers should have scalable 
publishing services for their dynamically changing 
information products; 
? Scalable intelligent middleware to dynamically 
broker, compose, and manage the intermediate 
services necessary to connect information users to 
the right information products at the right time. 
 
While some of these capabilities can be realized 
with existing technologies, several challenging 
problems, particularly in the areas of scalability, 
semantic interoperability and dynamic extensibility, 
may need 5 or 10 years basic research efforts to 
adequately address. The Information Integration 
Research Prototype (I2RP) we describe is a vehicle for 
exploring which new paradigms and frameworks are 
most promising for future investment while calibrating 
what existing technologies can do today (Jiang et al, 
2002). This demonstration illustrates the I2RP 
architecture and the underlying technology approaches.  
In six months, we implemented an extensible prototype 
system with basic capability as a proof-of-concept to 
show some fundamental new ideas for implementing 
next generation dynamic information integration 
systems.   
 
2  Current Status 
 
IR2P is an information integration research 
prototype built with commercial-off-shelf technologies. 
With the prototype system, the declaration-composition-
production process of semantic integration has been 
investigated, with specific emphasis on semantic 
interoperability issues (Sirin et al, 2002). A target-
tracking scenario was developed to test the system and 
all components in the I2RP were successfully integrated 
?on the fly? to support this mission. Meanwhile we 
proposed and analyzed some metrics to quantify the 
semantic integration process such as semantic depth, 
markup complexity and information fluidity. From 
different aspects, we built some models to analyze the 
relationship between semantic depth and markup 
complexity, and between semantic interoperability and 
information fluidity.  
 
3  Process Queries and Natural Language 
Queries 
 
The work on IR2P has evolved into a more general 
engine for tracking, fusing, and querying processes.  We 
are presently developing a generic process querying 
capability that can be used to retrieve information about 
objects as diverse as physical objects on a battlefield or 
worms propagating through the Internet.  Later we plan 
to build a natural language querying capability as 
proposed by Lulich and Thompson (2002). 
 
References 
 
DARPA.  2002.  http://dtsn.darpa.mil/ixo/ 
 
Guofei Jiang, George Cybenko, Wayne Chung, Paul 
Thompson, Glenn Nofsinger Annarita Giani, Yong 
Sheng, Diego Hernando and Han Li, Jim Hendler, 
Evren Sirin, Bijan Parsia , Jennifer Golbeck, Kenneth 
Whitebread and Martin Hoffman. 2002.  IXO 
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 27-28
                                                         Proceedings of HLT-NAACL 2003
Seedling Project Technical Report Dynamic 
Integration of Distributed Semantic Services Thayer 
School of Engineering, Dartmouth College. 
 
Steven Lulich and Paul Thompson.  2002.  Question 
Answering in the Infosphere:  Semantic 
Interoperability and Lexicon Development  Language 
Evaluation Resources Conference Workshop on 
Question Answering Strategies, Las Palmas de Gran 
Canaria, Spain. 
 
Evren Sirin, James Hendler, and Bijan Parsia, 2002.  
Semi-automatic Composition of Web Services using 
Semantic Descriptions. Accepted to Web Services: 
Modeling, Architecture and Infrastructure workshop 
in conjunction with ICEIS2003. 
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2270?2279, Dublin, Ireland, August 23-29 2014.
Comparable Study of Event Extraction in Newswire and Biomedical
Domains
Makoto Miwa
?,?
Paul Thompson
?
Ioannis Korkontzelos
?
Sophia Ananiadou
?
?
National Centre for Text Mining and School of Computer Science,
University of Manchester, United Kingdom
?
Graduate School of Engineering, Toyota Technological Institute, Japan
{makoto.miwa, paul.thompson, ioannis.korkontzelos, sophia.ananiadou}@manchester.ac.uk
Abstract
Event extraction is a popular research topic in natural language processing. Several event extrac-
tion tasks have been defined for both the newswire and biomedical domains. In general, different
systems have been developed for the two domains, despite the fact that the tasks in both domains
share a number of characteristics. In this paper, we analyse the commonalities and differences
between the tasks in the two domains. Based on this analysis, we demonstrate how an event
extraction method originally designed for the biomedical domain can be adapted for application
to the newswire domain. The performance is state-of-the-art for both domains, with F-scores of
52.7% for the biomedical domain and 52.1% for the newswire domain in terms of their primary
evaluation metrics.
1 Introduction
Research into event extraction was initially focussed on the general language domain, largely driven by
the Message Understanding Conferences (MUC) series (e.g., Chinchor (1998)) and the Automated Con-
tent Extraction (ACE) evaluations
1
. More recently, the focus of research has been widened to the biomed-
ical domain, motivated by the ongoing series of biomedical natural language processing (BioNLP) shared
tasks (STs) (e.g., Kim et al. (2013)).
Although the textual characteristics and the types of relevant events to be extracted can vary consid-
erably between domains, the same general features of events normally hold across domains. An event
usually consists of a trigger and arguments (see Figures 1 and 2.) A trigger is typically a verb or a nom-
inalised verb that denotes the presence of the event in the text, while the arguments are usually entities.
In general, arguments are assigned semantic roles that characterise their contribution towards the event
description.
Until now, however, there has been little, if any, effort by researchers working on event extraction in
different domains to share ideas and techniques, unlike syntactic tasks (e.g., (Miyao and Tsujii, 2008))
and other information extraction tasks, such as named entity recognition (e.g., (Giuliano et al., 2006))
and relation extraction (e.g., (Qian and Zhou, 2012)). This means that the potential to exploit cross-
domain features of events to develop more adaptable event extraction systems is an under-studied area.
Consequently, although there is a large number of published studies on event extraction, proposing many
different methods, no work has previously been reported that aims to adapt an event extraction method
developed for one domain to a new domain.
In response to the above, we have investigated the feasibility of adapting an event extraction method
developed for the biomedical domain to the newswire domain. To facilitate this, we firstly carry out a
detailed static analysis of the differences that hold between event extraction tasks in the newswire and
biomedical domains. Specifically, we consider the ACE 2005 event extraction task (Walker et al., 2006)
for the newswire domain and the Genia Event Extraction task (GENIA) in BioNLP ST 2013 (Kim et al.,
2013) for the biomedical domain. Based on the results of this analysis, we adapt the biomedical event
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
itl.nist.gov/iad/mig/tests/ace
2270
Jim McMahon was body slammed to the ground in the mid 80's about five seconds after he had released a pass.
PER_Individual Conflict_Attack ?? timex2 PER_Individual
timex2
Target Time-Within
Time-At-End
Figure 1: ACE 2005 event example (ID: MARKBACKER 20041220.0919)
p300 immunoprecipitated Foxp3 when both proteins were overexpressed in HEK 293T cells
Pro Binding Pro +Reg
+Reg
Gene expression
Gene expressionTheme Theme2 Cause
CauseTheme
Theme
Theme
Theme
Figure 2: GENIA event example (ID: PMC-1447668-08-Results)
extraction method to the task of extracting events in the newswire domain, according to the specification
of the ACE 2005 event extraction task. The original method consists of a classification pipeline that has
previously been applied to extract events according to task descriptions that are similar to GENIA. In
order to address the differences between this task and the ACE task, we have made a number of changes
to the original method, including modifications to the classification labels assigned, the pipeline itself
and the features used. We retrained the model of the adapted system on the ACE task, compared the
performance, and empirically analysed the differences between the two tasks in terms of entity-related
information. We demonstrate that the resulting system achieves state-of-the-art performance for tasks in
both domains.
2 Related Work
In this section, we introduce the two domain specific event extraction tasks on which we will focus, i.e.,
the ACE 2005 event extraction task, which concerns events in the newswire domain, and the GENIA
event task from the BioNLP ST 2013, which deals with biomedical event extraction. We also examine
state-of-the-art systems that have been developed to address each task.
2.1 Newswire Event Extraction
The extraction of events from news-related texts has been widely researched, largely due to motivation
from the various MUC and ACE shared tasks. Whilst MUC focussed on filling a single event template
on a single topic by gathering information from different parts of a document, ACE defined a more
comprehensive task, involving the recognition of multiple fine-grained and diverse types of entities and
associated intra-sentential events within each document.
A common approach to tackling the MUC template filling task has involved the employment of
pattern-based methods, e.g., Riloff (1996). In contrast, supervised learning approaches have constituted
a more popular means of approaching the ACE tasks
2
. In this paper, we choose to focus on adapting
our biomedical-focussed event extraction method to the ACE 2005 task. Our choice is based on the task
definition for ACE 2005 having more in common with the BioNLP 2013 GENIA ST definition than the
MUC event template task definition.
In terms of the characteristics of state-of-the-art event extraction systems designed according to the
ACE 2005 model, pipeline-based approaches have been popular (Grishman et al., 2005; Ahn, 2006).
Grishman et al. (2005) proposed a method that sequentially identifies textual spans of arguments, role
types, and event triggers. This pipeline approach has been further extended in several subsequent studies.
For example, Liao et al. (2010) investigated document-level cross-event consistency using co-occurrence
of events and event arguments, while Hong et al. (2011) exploited information gathered from the web to
ensure cross-entity consistency.
2
Note that there are also approaches using few or no training data (e.g., (Ji and Grishman, 2008; Lu and Roth, 2012)) for
the ACE 2005 task, but they are not so many and we will focus on the supervised learning approaches in this paper.
2271
Li et al. (2013) recently proposed a joint detection method to detect both triggers and arguments
(together with their role types) using a structured perceptron model. The system outperformed the best
results reported for the ACE 2005 task in the literature, without the use of any external resources.
2.2 Biomedical Event Extraction
The task of event extraction has received a large amount of attention from BioNLP researchers in recent
years. Interest in this task was largely initiated by the BioNLP 2009 ST, and has been sustained through
the organisation of further STs in 2011 and 2013. The STs consist of a number of different sub-tasks, the
majority of which concern the extraction of events from biomedical papers from the PubMed database.
Events generally concern interactions between biomedical entities, such as proteins, cells and chemicals.
Similarly to newswire event extraction systems, pipeline-based methods have constituted a popular
approach to extracting events in the biomedical domain (Bj?orne and Salakoski, 2013; Miwa et al., 2012).
The pipeline developed by Miwa et al. (2012) consists of a number of modules, which sequentially
detect event triggers, event arguments, event structures and hedges (i.e., speculations and negations).
The system has been applied to several event extraction tasks, and has achieved the best performance on
most of these, in comparison to other systems. It should be noted that the ordering of the components
in biomedical event extraction pipelines often differs from pipelines designed for news event extraction,
e.g., Grishman et al. (2005), which was described above.
As in newswire event detection, some joint (non pipeline-based) approaches have also been proposed
for biomedical event extraction. For example, McClosky et al. (2012) used a stacking model to combine
the results of applying two different methods to event extraction. The first method is a joint method,
similar to Li et al. (2013), that detects triggers, arguments and their roles. However, in contrast to
the structured perceptron employed in Li et al. (2013), McClosky et al. (2012) use a dual-decomposition
approach for the detection. The second method is based on dependency parsing and treats event structures
as dependency trees.
3 Adaptation of Biomedical Event Extraction to Newswire Event Extraction
In this section, we firstly analyse the differences between the domain-specific ACE 2005 and GENIA
event extraction tasks. Based on our findings, we propose an approach to adapting an existing event ex-
traction method, originally developed for biomedical event extraction, to the ACE 2005 task, by resolving
the observed differences between the two task definitions.
3.1 Differences in event extraction tasks
Both the ACE 2005 and GENIA tasks concern the task of event extraction, i.e., the identification of
relationships between entities. For both tasks, the requirement is to extract events from text that conform
to the general event description introduced earlier, i.e., a trigger and its arguments, each of which is
assigned a semantic role. Despite this high-level similarity between the tasks, their finer-grained details
diverge in a number of ways. Apart from the different textual domain, the tasks adopt varying annotation
schemes. The exact kinds of annotations provided at training time are also different, as are the evaluation
settings.
Several variants of the official task setting for the ACE 2005 corpus have been defined. This is partly
due to the demanding nature of the official task definition, which requires the detection of events from
scratch, including the recognition of named entities participating in events, together with the resolution
of coreferences. Alternative task settings (such as Ji and Grishman (2008); Liao and Grishman (2010)))
generally simplify the official task definition, e.g., by omitting the requirement to perform coreference
resolution. A further issue is that the test data sets for the official task setting have not been made publicly
available. As a result of the multiple existing variations of the ACE 2005 task definition that have been
employed by different research efforts, direct comparison of our results with those obtained by other
state-of-the art systems is problematic. The solution we have chosen is to adopt the same ACE 2005
event extraction task specification that has been adopted in recent research, by Hong et al. (2011) and Li
et al. (2013). For GENIA, we follow the specification of the original GENIA event extraction task.
2272
ACE 2005 GENIA
# of entity types 13 (type) / 53 (subtype) 2
Argument Entity/Nominal/Value/Time Entity
# of event types 8 (type) / 33 (subtype) 13
# of argument role types 35 7
Max # of arguments for an event 11 4
Nested events None Possible
Overlaps of events None Possible
Correspondences of arguments None Possible
Entity Available (Given) Available (Partially given)
Entity attributes Available (Given) Not available
Event attributes Available (Not given) Available (Not given)
Entity coreference Available (Given) Available (Not given)
Event coreference Available (Not given) Not available
Evaluation Trigger/Role Event
Table 1: Comparison of event definitions and event extraction tasks. ?Available annotations? are annota-
tions available in the corresponding corpus, while ?Given annotations? are annotations provided during
(training and) prediction. ?Given annotations? do not need to be predicted during event extraction.
Event annotation examples for ACE 2005 and GENIA are shown in Figures 1 and 2, respectively.
Table 1 summarises the following comparison between the two event extraction tasks.
Semantic types There are more event, role and entity types and a greater potential number of arguments
in ACE 2005 events than in GENIA events. There is also a hierarchy of event types and entity types
in ACE 2005. For example, the Life event type has Be-Born, Marry, Divorce, Injure, Die event
subtypes. Some GENIA event types can also be arranged to have a hierarchy but they are limited.
Events in ACE 2005 can take non-entity arguments, e.g., Time.
Nested events/Overlapping events Event structures are flat in ACE 2005, but they can be nested in
GENIA, i.e., an event can take other events as its arguments. Events in GENIA can also be over-
lapping, in the sense that a particular word or phrase can be a trigger for multiple events. Figure 2
illustrates both nesting and overlapping in GENIA events. These properties of GENIA events are
not addressed by methods developed for event extraction according to the ACE 2005 specification,
making direct application of these methods to the GENIA task impossible.
Links amongst arguments A specific feature of the GENIA event extraction task, which is completely
absent from the ACE 2005 task, is that links amongst arguments sometimes have to be identified.
For example, the Binding event type in the GENIA task can take the following argument role types:
Theme, Theme2, Site and Site2. The number 2 is attached to differentiate specific linkages between
arguments: Site is the location of Theme, while Site2 is the location of Theme2.
Entities, events and their attributes Entities in ACE 2005 have rich attributes associated with them.
For example, the Time entity type has an attribute to store a normalised temporal format (e.g., 2003-
03-04 for entities ?20030304?, ?March 4? and ?Tuesday?) while the GPE (Geo-Political Entity)
type has attributes such as subtypes (e.g., Nation), mention type (proper name, common noun or
pronoun), roles (location of a group or person) and style (literal or metonymic). In contrast, GENIA
entities have no attributes
3
. In ACE 2005, all entities are provided (gold) in the training and test
data and they do not need to be predicted. In GENIA, some named entities (i.e., Proteins) are also
provided, but other types of named or non-named entities that can constitute event arguments, such
as locations and sites of proteins, are not provided in the test data and thus need to be predicted
as part of the extraction process. Events in both corpora also have associated attributes: modality,
3
Types are not counted as attributes in this paper.
2273
polarity, genericity and tense in ACE 2005 and negation and speculation in GENIA. The GENIA
task definition requires event attributes to be predicted, but the ACE 2005 task definition does not.
Coreference Both entity and event coreference are annotated in ACE 2005, but only entity coreference is
annotated in GENIA. Events in ACE 2005 can take non-entity mentions, such as pronouns, as their
arguments. However, events in GENIA can take only entity mentions as arguments. Thus, instead
of non-entity mentions, coreferent entity mentions that are the closest to triggers are annotated as
arguments in GENIA. For example, in Figure 2, ?p300? and ?Foxp3? are annotated as Themes of
Gene expression events instead of ?both proteins?.
Evaluation In ACE 2005, the accuracy of extracted events is evaluated at the level of individual ar-
guments and their roles. Completeness of events is not taken into consideration (Li et al., 2013),
presumably because each event can take many arguments. Evaluation is performed by taking into
account the 33 event subtypes, rather than the 8 coarser-grained event types. In contrast, evaluation
of events according to the GENIA specification considers only the correctness of complete events,
after nested events have been broken down.
In summary, the ACE 2005 task is in some respects more complex than the GENIA task, because it
concerns a greater number event types, whose arguments may constitute a greater range of entity types,
and whose semantic roles are drawn from a larger set, some of which are specific to particular event
types and entities. In other respects, the task is more straightforward than the GENIA task, because of
the simpler nature of the event structures in ACE 2005, i.e., there are no nested or overlapping event
structures.
3.2 Adaptation of event extraction method
Since event structures are simpler in ACE 2005 than GENIA, we choose to adapt a biomedical event
extraction method to the ACE 2005 task rather than the other way around. The inverse adaptation,
starting from a newswire event extraction method, is considered more complex, since we would need to
extend the method to capture the more complex event structures required in the GENIA task. It would
additionally be inappropriate to employ domain adaptation methods (Daum?e III and Marcu, 2006; Pan
and Yang, 2010) to allow GENIA-trained models to be applied to the ACE 2005 tasks. This is because
such methods require that there is at least a certain degree of overlap between the target information
types, which is not the case in this scenario.
We employ the biomedical event extraction pipeline method described in Miwa et al. (2012) as our
starting point. Our motivation is that, due to their modular nature, pipeline approaches are often easier
to adapt to other task settings than joint approaches, e.g., (McClosky et al., 2012; Li et al., 2013).
In addition, the method has previously been shown to achieve state-of-the-art performance in several
biomedical event extraction tasks (Miwa et al., 2012).
The pipeline consists of four detectors, i.e., trigger/entity, event role, event structure, and hedge de-
tectors. The trigger/entity detector finds triggers and entities in text. The event role detector determines
which triggers/entities constitute arguments of events, links them to the appropriate event trigger and as-
signs semantic roles to the arguments. The event structure detector merges trigger-argument pairs into all
possible complete event structures, and determines which of these structures constitute actual events. The
same detector determines links between arguments, such as Theme2 and Site2. The hedge detector finds
negation and speculation information associated with events. Each detector solves multi-label multi-
class classification problems using lexical and syntactic features obtained from multiple parsers. These
features include character n-grams, word n-grams, and shortest paths between triggers and participants
within parse structures. More detailed information can be found in Miwa et al. (2012).
We have updated the original method by simplifying the format of the classification labels used by
both the event role detector and event structure detector modules. We refer to this method as BioEE,
which we have applied to the GENIA task. We use only the role types (e.g., Theme) as classification
labels for instances in the event role detector, instead of the more complex labels used in the original
version of the module, which combined event types, roles and semantic entity types of arguments (e.g.,
2274
Binding:Theme-Protein). Similarly, in the event structure detector, we use only two labels (?EVENT?
or ?NOT-EVENT?), instead of the previously used composite labels, which consisted of the event type,
together with the roles and semantic entity types of all arguments of the event (e.g., Regulation:Cause-
Protein:Theme-Protein.) We employed the simplified labels, since they increase the number of training
instances for each label. The use of such labels, compared to the more complex ones, could reduce the
potential of carrying out detailed modelling of specific aspects of the task. However, this was found not
to be an issue, since the use of the simplified labels improved the performance of the pipeline in detecting
events within the GENIA development data set (about 1% improvement in F-score). The simplification of
the set of classification labels was also vital to ensure the tractability of the classification problems within
the context of the ACE 2005 task. For example, using the same conventions to formulate classification
labels as in the original system would result in 345 possible labels (compared to 91 in GENIA) to be
predicted by the event role detector (and an even greater number of labels for the event structure detector),
based on event-role-semantic type combinations found in the ACE training/development sets.
In order to adapt the system to extract events according to the ACE 2005 specification, we modified
BioEE in several ways, making changes to both the pipeline itself and the features employed by the
different modules. We refer to this method as Adapted BioEE, and we applied this method to the ACE
2005 task. These changes were made in an attempt to address the two major differences between the
GENIA and ACE 2005 tasks, i.e., the simpler event structures and the availability of entity attribute and
coreference information in ACE.
The pipeline-based modifications consisted of removing certain modules from the original pipeline,
such that only two modules remained, i.e., the trigger/entity and event role detectors. The other two
modules of the original pipeline, i.e., the event structure and hedge detectors, were designed to deal with
problems that do not exist in the ACE 2005 extraction task, and thus their usage would be redundant.
Instead of using the event structure detector to piece the different elements of an event, we simply aggre-
gate all the arguments of the same trigger into a single event structure, after the event role detector has
been applied.
As mentioned above, the ACE 2005 task definition includes rich information about entities, including
attributes and coreference information. Existing systems developed to address this task have exploited
this information to generate rich feature sets for classification (Liao and Grishman, 2010; Li et al.,
2013). Based on the demonstrated utility of this information within the context of event extraction, we
also choose to use it, by adding binary feature that indicate the presence of base forms, entity subtypes,
and attributes of the entities and their coreferent entities to features in both detectors above. We choose
to use base forms, since surface forms of entities are not used by most biomedical event extraction
systems, including BioEE. We also add the features for Brown clusters (Brown et al., 1992) following Li
et al. (2013). Further details can be found in Li et al. (2013).
4 Evaluation
4.1 Evaluation settings
To assess the performance of Adapted BioEE on the ACE 2005 task, we followed the evaluation process
and settings used in previously reported studies (Hong et al., 2011; Li et al., 2013). ACE 2005 consists
of 599 documents. In order to facilitate direct comparison with other systems trained on the same data,
we conducted a blind test on the same 40 newswire documents that were used for evaluation in (Ji and
Grishman, 2008; Li et al., 2013), and used the remaining documents as training/development sets. We
use precision (P), recall (R) and F-score (F) to report the performance of the adapted system in classifying
triggers and argument roles. We use the latter F-score as our primary metric for comparing our system
with other systems, since this score better reflects the performance of the extraction of event structures.
GENIA consists of 34 full paper articles (Kim et al., 2013). To evaluate the performance of BioEE
on the GENIA task, we followed the task setting in BioNLP ST 2013 and used the official evaluation
systems provided by the organisers. We also used the same partitioning of data that was employed in
the official BioNLP ST 2013 evaluation, with 20 articles being used as the training/development set, and
the remaining 14 articles being held back as the test set. For brevity, we show the only the primary P,
2275
Arg. Role Decomposition Event Detection
P R F P R F (%)
BioEE 71.76 47.44 57.12 64.36 44.62 52.71
BioEE (+Entity) 69.47 46.94 56.02 61.81 44.11 51.48
EVEX 64.30 48.51 55.30 58.03 45.44 50.97
TEES-2.1 62.69 49.40 55.26 56.32 46.17 50.74
Table 2: Overall performance of BioEE on the GENIA data set
Trigger Classification Arg. Role Classification Event Detection
P R F P R F P R F (%)
Adapted BioEE 59.9 72.6 65.7 54.2 50.2 52.1 20.7 21.7 21.2
Adapted BioEE (-Entity) 57.9 71.5 64.0 51.0 48.1 49.5 19.7 19.3 19.5
Li et al. (2013) 73.7 62.3 67.5 64.7 44.4 52.7 - - -
Hong et al. (2011) 72.9 64.3 68.3 51.6 45.5 48.4 - - -
Table 3: Overall performance of Adapted BioEE on the ACE 2005 data set
R and F scores in the shared task, i.e., the EVENT TOTAL results obtained using the approximate span
& recursive evaluation method, as recommended by the organisers. The method individually evaluates
each complete core event, i.e., event triggers with their Theme and/or Cause role arguments, with relaxed
span matching, after nested events have been broken down as explained in Section 3.1. Note that the
scores do not count the non-named entities, hedges, and links between arguments, since only core events
are considered in the official evaluation.
We applied both a deep parser, Enju (Miyao and Tsujii, 2008) and a dependency parser, ksdep (Sagae
and Tsujii, 2007) to generate features for the ACE 2005 task, and their bio-adapted versions for the
GENIA task. We also employed the GENIA sentence splitter (S?tre et al., 2007) for sentence splitting,
and the snowball (Porter2) stemmer
4
for stemming. We did not make use of any other external resources,
such as dictionaries, since this would hinder direct comparison of the two versions of the system.
4.2 Evaluation on GENIA
The ?Event Detection? column in Table 2 shows evaluation results of BioEE on GENIA. The effects
on performance by including entity-related features, i.e., entity base forms and Brown clustering, as
introduced in Section 3.2, are shown as ?BioEE (+Entity)?. The inclusion of these features slightly
degrades the performance.
For completeness, we also show in Table 2 the best and second best performing systems that took
part in the official BioNLP 2013 ST evaluation: EVEX (Hakala et al., 2013) and TEES-2.1 (Bj?orne and
Salakoski, 2013). TEES-2.1 consists of a modular pipeline similar to BioEE, but it uses a different set
of features. EVEX enhances the output of TEES-2.1, by using information obtained from the results of
large-scale event extraction. The comparison shows that BioEE achieves state-of-the-art event extraction
performance on the GENIA task.
4.3 Evaluation on ACE 2005
The ?Trigger Classification? and ?Arg. Role Classification? columns of Table 3 summarise the evaluation
results of the Adapted BioEE system (as described in Section 3.2) on the ACE 2005 task.
We analysed the effects of incorporating features based on entity-related information into the extrac-
tion process, by repeating the experiments with such features omitted (-Entity). As can be observed in
Table 3, the removal of entity-related features led to 3% performance decrease in F-score.
For completeness, Table 3 also illustrates the results of state-of-the-art systems that were specifi-
cally developed for ACE 2005: the system based on a joint approach (Li et al., 2013) and the pipeline-
based system enhanced with web-gathered information (Hong et al., 2011). The difference between the
4
snowball.tartarus.org
2276
Adapted BioEE and the best system is small and insignificant and the Adapted BioEE achieved perfor-
mance that is comparable to or better than these other systems, in terms of the F-scores in argument role
classification.
5 Discussion
To further investigate the differences in performance of the BioEE and Adapted BioEE systems on the
two tasks, we evaluate the scores achieved for each task using the evaluation criteria originally designed
for the other task. Specifically, we apply the ACE 2005 argument role classification criteria to the out-
put of GENIA task, and we apply the complete event-based evaluation, originally used to evaluate the
GENIA task, to the events extracted for the ACE 2005 task. The ?Arg. Role Decomposition? column of
Table 2 depicts the former evaluation, while the ?Event Detection? column of Table 3 shows the latter.
Table 2 also shows the performance of the other biomedical event extraction systems introduced above
in carrying out argument role classification, since such information was provided as ?Decomposition?
within the results of the original task evaluation
5
. Although the results shown for ?Arg. Role Decompo-
sition? in Table 2 are not directly comparable to those shown for ?Arg. Role Classification? in Table 3
(given the different characteristics of GENIA and ACE 2005 tasks), the scores are broadly comparable.
This demonstrates that the task of argument role classifications is equally challenging for both tasks.
The ?Event Detection? column of Table 3 illustrates event-based evaluation scores on ACE 2005.
The event structure detector was added to the pipeline to facilitate comparison of the results of the two
different tasks in a similar setting, and performance was evaluated according to the GENIA evaluation
criteria. Evaluation scores on ACE 2005 are unexpectedly low compared to those in Table 2. Considering
that the performance of argument role classification is similar in both tasks, this low performance is likely
to be due to the large number of potential event arguments in ACE 2005. This means that, in comparison
to GENIA events, which have a small number of possible argument types, there is a greater chance that
some arguments of more complex ACE 2005 events will fail to be detected. According to the GENIA
evaluation criteria, even if the majority of arguments has been correctly identified, the complete event
structure will still be evaluated as incorrect. This helps to explain why such evaluation criteria may have
been deemed inappropriate in the original ACE 2005 evaluations.
Subsequently, we analysed the effects of utilising entity-related features. We show the results obtained
by adding entity information (+Entity) in Table 2 and the results obtained by removing entity information
(-Entity) in Table 3. The positive or negative effect on performance of adding or removing these features
is consistent across all subtask evaluations shown in the two tables, although the exact level of perfor-
mance improvement or degradation depends on the subtask under evaluation. Overall, the inclusion of
the features degraded the performance of BioEE on the GENIA task, but improved the performance of
Adapted BioEE on the ACE 2005 task. These differences may be due to the increased richness of en-
tity information in the ACE 2005 corpus, suggesting that enriching entities in the GENIA corpus with
attribute information could be a possible way to further improve the performance of the system on this
task.
6 Conclusions and Future Work
In this paper, we have described our adaptation of a biomedical event extraction method to the newswire
domain. We firstly evaluated the method on a biomedical event extraction task (GENIA), and showed
that its performance was superior to other state-of-the-art systems designed for the task. We then adapted
the method to a newswire event extraction task (ACE 2005), by addressing the major differences between
the tasks. With only a small number of adaptations, the resulting system was also able to achieve state-of-
the-art performance on the newswire extraction task. These results show that there is no need to develop
separate systems for event extraction tasks in different domains, as long as the types of tasks being
addressed exhibit domain-independent features. However, further discussion and evaluation is needed to
better understand how different potential methods for adapting such tools from one domain to another
can be used and/or combined effectively.
5
bionlp-st.dbcls.jp/GE/2013/results
2277
As future work, we intend to further investigate the adaptation of alternative methods proposed for
use in one domain to another domain. Several interesting approaches have been described, such as the
utilisation of contextual information beyond the boundaries of individual sentences in the newswire do-
main (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) and joint approaches in the
biomedical domain (McClosky et al., 2012), but their adaptability to other domains has not yet been
investigated. We also intend to investigate the possibility of discovering and utilising shared information
between the two domains (Goldwasser and Roth, 2013). Encouraging greater levels of communication
between researchers working on NLP tasks in different domains will help to stimulate such new direc-
tions of research, both for event extraction and for other related information extraction tasks, such as
relation extraction and coreference resolution.
Acknowledgements
This work was supported by the Arts and Humanities Research Council (AHRC) [grant number
AH/L00982X/1], the Medical Research Council [grant number MR/L01078X/1], the European Commu-
nity?s Seventh Program (FP7/2007-2013) [grant number 318736 (OSSMETER)], and the JSPS Grant-in-
Aid for Young Scientists (B) [grant number 25730129].
References
David Ahn. 2006. The stages of event extraction. In Proceedings of the Workshop on Annotating and Reasoning
about Time and Events, pages 1?8, Sydney, Australia, July. ACL.
Jari Bj?orne and Tapio Salakoski. 2013. Tees 2.1: Automated annotation scheme learning in the bionlp 2013 shared
task. In Proceedings of the BioNLP Shared Task 2013 Workshop, pages 16?25, Sofia, Bulgaria, August. ACL.
Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based
n-gram models of natural language. Computational linguistics, 18(4):467?479.
Nancy A. Chinchor. 1998. Overview of MUC-7/MET-2. In Proceedings of the 7th Message Understanding
Conference (MUC-7/MET-2).
Hal Daum?e III and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. Journal of Artificial Intelli-
gence Research, 26:101?126.
Claudio Giuliano, Alberto Lavelli, and Lorenza Romano. 2006. Simple information extraction (sie): A portable
and effective ie system. In Proceedings of the Workshop on Adaptive Text Extraction and Mining (ATEM 2006),
pages 9?16, Trento, Italy, April. Association for Computational Linguistics.
Dan Goldwasser and Dan Roth. 2013. Leveraging domain-independent information in semantic parsing. In
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short
Papers), pages 462?466, Sofia, Bulgaria, August. Association for Computational Linguistics.
Ralph Grishman, David Westbrook, and Adam Meyers. 2005. NYU?s english ACE 2005 system description. In
Proceedings of ACE 2005 Evaluation Workshop, Washington, US.
Kai Hakala, Sofie Van Landeghem, Tapio Salakoski, Yves Van de Peer, and Filip Ginter. 2013. Evex in st?13:
Application of a large-scale text mining resource to event extraction and network construction. In Proceedings
of the BioNLP Shared Task 2013 Workshop, pages 26?34, Sofia, Bulgaria, August. ACL.
Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao, Guodong Zhou, and Qiaoming Zhu. 2011. Using cross-entity in-
ference to improve event extraction. In Proceedings of the 49th ACL-HLT, pages 1127?1136, Portland, Oregon,
USA, June. ACL.
Heng Ji and Ralph Grishman. 2008. Refining event extraction through cross-document inference. In Proceedings
of ACL-08: HLT, pages 254?262, Columbus, Ohio, June. ACL.
Jin-Dong Kim, Yue Wang, and Yamamoto Yasunori. 2013. The genia event extraction shared task, 2013 edition
- overview. In Proceedings of the BioNLP Shared Task 2013 Workshop, pages 8?15, Sofia, Bulgaria, August.
ACL.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event extraction via structured prediction with global features. In
Proceedings of the 51st ACL, pages 73?82, Sofia, Bulgaria, August. ACL.
2278
Shasha Liao and Ralph Grishman. 2010. Using document level cross-event inference to improve event extraction.
In Proceedings of the 48th ACL, pages 789?797, Uppsala, Sweden, July. ACL.
Wei Lu and Dan Roth. 2012. Automatic event extraction with structured preference modeling. In Proceedings
of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
835?844, Jeju Island, Korea, July. Association for Computational Linguistics.
David McClosky, Sebastian Riedel, Mihai Surdeanu, Andrew McCallum, and Christopher Manning. 2012. Com-
bining joint models for biomedical event extraction. BMC Bioinformatics, 13(Suppl 11):S9.
Makoto Miwa, Paul Thompson, and Sophia Ananiadou. 2012. Boosting automatic event extraction from the
literature using domain adaptation and coreference resolution. Bioinformatics, 28(13):1759?1765.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest models for probabilistic HPSG parsing. Computational
Linguistics, 34(1):35?80, March.
Sinno Jialin Pan and Qiang Yang. 2010. A survey on transfer learning. IEEE Transactions on Knowledge and
Data Engineering, 22(10):1345?1359.
Longhua Qian and Guodong Zhou. 2012. Tree kernel-based protein?protein interaction extraction from biomedi-
cal literature. Journal of biomedical informatics, 45(3):535?543.
Ellen Riloff. 1996. Automatically generating extraction patterns from untagged text. In Proceedings of the
national conference on artificial intelligence, pages 1044?1049.
Rune S?tre, Kazuhiro Yoshida, Akane Yakushiji, YusukeMiyao, Yuichiro Matsubayashi, and Tomoko Ohta. 2007.
AKANE System: Protein-protein interaction pairs in BioCreAtIvE2 Challenge, PPI-IPS subtask. In Proceed-
ings of the Second BioCreative Challenge Evaluation Workshop, pages 209?212, CNIO, Madrid, Spain, April.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 1044?1050,
Prague, Czech Republic, June. ACL.
Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda. 2006. Ace 2005 multilingual training
corpus. Linguistic Data Consortium.
2279
Proceedings of the EACL 2009 Demonstrations Session, pages 61?64,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
  
Three BioNLP Tools Powered by a Biological Lexicon 
 
Abstract 
In this paper, we demonstrate three NLP 
applications of the BioLexicon, which is a 
lexical resource tailored to the biology 
domain. The applications consist of a 
dictionary-based POS tagger, a syntactic 
parser, and query processing for biomedical 
information retrieval.  Biological 
terminology is a major barrier to the 
accurate processing of literature within 
biology domain. In order to address this 
problem, we have constructed the 
BioLexicon using both manual and semi-
automatic methods. We demonstrate the 
utility of the biology-oriented lexicon 
within three separate NLP applications. 
1 Introduction 
Processing of biomedical text can frequently be 
problematic, due to the huge number of technical 
terms and idiosyncratic usages of those terms.  
Sometimes, general English words are used in 
different ways or with different meanings in 
biology literature. 
There are a number of linguistic resources 
that can be use to improve the quality of 
biological text processing.  WordNet (Fellbaum, 
1998) and the NLP Specialist Lexicon 1  are 
dictionaries commonly used within biomedical 
NLP. 
WordNet is a general English thesaurus which 
additionally covers biological terms. However, 
since WordNet is not targeted at the biology 
domain, many biological terms and derivational 
relations are missing.   
The Specialist Lexicon is a syntactic lexicon 
of biomedical and general English words, 
providing linguistic information about individual 
vocabulary items (Browne et al, 2003).  Whilst 
it contains a large number of biomedical terms, 
                                                 
1 http://SPECIALIST.nlm.hih.gov 
its focus is on medical terms. Therefore some 
biology-specific terms, e.g., molecular biology 
terms, are not the main target of the lexicon.  
In response to this, we have constructed the 
BioLexicon (Sasaki et al, 2008), a lexical 
resource tailored to the biology domain.  We will 
demonstrate three applications of the BioLexicon, 
in order to illustrate the utility of the lexicon 
within the biomedical NLP field.  
The three applications are: 
 
? BLTagger: a dictionary-based POS tagger 
based on the BioLexicon 
? Enju full parser enriched by the 
BioLexicon 
? Lexicon-based query processing for 
information retrieval 
2. Summary of the BioLexicon 
In this section, we provide a summary of the 
BioLexicon (Sasaki et al, 2008). It contains 
words belonging to four part-of-speech 
categories: verb, noun, adjective, and adverb.  
Quochi et al(2008) designed the database 
model of the BioLexicon which follows the 
Lexical Markup Framework (Francopoulo et al, 
2008).    
2.1 Entries in the Biology Lexicon 
The BioLexicon accommodates both general 
English words and terminologies. Biomedical 
terms were gathered from existing biomedical 
databases. Detailed information regarding the 
sources of biomedical terms can be found in  
(Rebholz-Schuhmann et al, 2008). The lexicon 
entries consist of the following: 
 
(1) Terminological verbs: 759 base forms (4,556 
inflections) of terminological verbs with 
automatically extracted verb 
subcategorization frames 
 
Yutaka Sasaki 1   Paul Thompson1   John McNaught 1, 2   Sophia Ananiadou1, 2 
 
1 School of Computer Science, University of Manchester 
2  National Centre for Text Mining 
MIB, 131 Princess Street, Manchester, M1 7DN, United Kingdom 
{Yutaka.Sasaki,Paul.Thompson,John.McNaught,Sophia.Ananiadou}@manchester.ac.uk 
61
(2)Terminological adjectives: 1,258 
terminological adjectives.   
(3) Terminological adverbs: 130 terminological 
adverbs. 
(4) Nominalized verbs: 1,771  nominalized verbs.   
(5) Biomedical terms: Currently, the BioLexicon 
contains biomedical terms in the categories of 
cell (842 entries, 1,400 variants), chemicals 
(19,637 entries, 106,302 variants), enzymes 
(4,016 entries, 11,674 variants), diseases 
(19,457 entries, 33,161 variants), genes and 
proteins (1,640,608 entries, 3,048,920 
variants), gene ontology concepts (25,219 
entries, 81,642 variants), molecular role 
concepts (8,850 entries, 60,408 variants), 
operons (2,672 entries, 3,145 variants), 
protein complexes (2,104 entries, 2,647 
variants), protein domains (16,940 entries, 
33,880 variants), Sequence ontology concepts 
(1,431 entries, 2,326 variants), species 
(482,992 entries, 669,481 variants), and 
transcription factors (160 entries, 795 
variants).   
In addition to the existing gene/protein names, 
70,105 variants of gene/protein names have been 
newly extracted from 15 million MEDLINE 
abstracts. (Sasaki et al, 2008) 
2.2. Comparison to existing lexicons 
This section focuses on the words and 
derivational relations of words that are covered 
by our BioLexicon but not by comparable 
existing resources. Figures 1 and 2 show the 
percentage of the terminological words and 
derivational relations (such as the word 
retroregulate and the derivational relation 
retroregulate ? retroregulation) in our lexicon 
that are also found in WorNet and the Specialist 
Lexicion. 
Since WordNet is not targeted at the biology 
domain, many biological terms and derivational 
relations are not included.   
Because the Specialist Lexicon is a 
biomedical lexicon and the target is broader than 
our lexicon, some biology-oriented words and 
relations are missing.  For example, the 
Specialist Lexicon includes the term retro-
regulator but not retro-regulate. This means that 
derivational relations of retro-regulate are not 
covered by the Specialist Lexicon.  
3. Application 1: BLTagger 
Dictionary-based POS tagging is advantageous 
when a sentence contains technical terms that 
conflict with general English words. If the POS 
tags are decided without considering possible 
occurrences of biomedical terms, then POS 
errors could arise.  
For example, in the protein name ?met proto-
oncogene precursor?, met might be incorrectly 
recognized as a verb by a non dictionary-based 
tagger.   
Input sentence: 
?IL-2-mediated activation of ??
IL/NP
IL-2/NN-BIOMED
-/-
2/CD
mediated/VVD
IL-2-mediated/UNKNOWN
IL/NP
2/CD
IL-2/NN-BIOMED
??????????
mediated/VVD
mediate/VVP
mediate/VV
of/IN
mediated/VVN
-/-
-/-
mediated/VVNdictionary-based tagging of/IN
Fig. 3 BLTagger example 
coverage
0
20
40
60
80
100
verb
noun
adjective
adverb
nom
inalization
adjetivial 
adverbal
Terminologies Derivational relations
 
Fig. 1  Comparison with WordNet 
coverage
0
20
40
60
80
100
verb
noun
adjective
adverb
nom
inalization
adjetivial 
adverbal
Terminologies Derivational relations
 
 
Fig. 2  Comparison with Specialist Lexicon 
62
In the dictionary, biomedical terms are given 
POS tag "NN-BIOMED". Given a sentence, the 
dictionary-based POS tagger works as follows.  
 
? Find all word sequences that match the 
lexical entries, and create a token graph (i.e., 
trellis) according to the word order.  
? Estimate the score of every path using the 
weights of the nodes and edges, through 
training using Conditional Random Fields.  
? Select the best path. 
 
Figure 3 shows an example of our dictionary-
based POS tagger BLTagger. 
Suppose that the input is ?IL-2-mediated 
activation of?. A trellis is created based on the 
lexical entries in the dictionary. The selection 
criteria for the best path are determined by the 
CRF tagging model trained on the Genia corpus 
(Kim et al, 2003). In this example,  
 
IL-2/NN-BIOMED -/- mediated/VVN 
activation/NN of/IN 
 
is selected as the best path.  
Following Kudo et al (2004), we adapted the 
core engine of the CRF-based morphological 
analyzer, MeCab2, to our POS tagging task.  
The features used were: 
 
? POS 
? BIOMED 
? POS-BIOMED 
? bigram of adjacent POS 
? bigram of adjacent BIOMED 
? bigram of adjacent POS-BIOMED 
 
During the construction of the trellis, white 
space is considered as the delimiter unless 
otherwise stated within dictionary entries. This 
means that unknown tokens are character 
sequences without spaces. 
As the BioLexicon associates biomedical 
semantic IDs with terms, the BLTagger attaches 
semantic IDs to the tokenizing/tagging results. 
4. Application 2: Enju full parser with the 
BioLexicon 
Enju (Miyao, et al, 2003) is an HPSG parser, 
which is tuned to the biomedical domain.  
Sentences are parsed based on the output of the 
                                                 
2 http://sourceforge.net/project/showfiles.php?group 
id=177856/ 
Stepp POS tagger, which is also tuned to the 
biomedical domain. 
To further tune Enju to the biology domain, 
(especially molecular biology), we have 
modified Enju to parse sentences based on the 
output of the BLTagger. 
As the BioLexicon contains many multi-word 
biological terms, the modified version of Enju 
parses token sequences in which some of the 
tokens are multi-word expressions.  This is 
effective when very long technical terms (e.g., 
more than 20 words) are present in a sentence. 
To use the dictionary-based tagging for 
parsing, unknown words should be avoided as 
much as possible. In order to address this issue, 
we added entries in WordNet and the Specialist 
Lexicion to the dictionary of BLTagger. 
The enhancement in the performance of Enju 
based on these changes is still under evaluation. 
However, we demonstrate a functional, modified 
version of Enju. 
5. Application 3: Query processing for IR 
It is sometimes the case that queries for 
biomedical IR systems contain long technical 
terms that should be handled as single multi-
word expressions.  
We have applied BLTagger to the TREC 2007 
Genomics Track data (Hersh et al, 2007).  The 
goal of the TREC Genomics Track 2007 was to 
generate a ranked list of passages for 36 queries 
that relate to biological events and processes.    
Firstly, we processed the documents with a 
conventional tokenizer and standard stop-word 
remover, and then created an index containing 
the words in the documents. Queries are 
processed with the BLTagger and multi-word 
expressions are used as phrase queries.  Passages 
are ranked with Okapi BM25 (Robertson et al, 
1995). 
Table 1 shows the preliminary Mean Average 
Precision (MAP) scores of applying the 
BLTagger to the TREC data set.   
By adding biology multi-word expressions 
identified by the BLTagger to query terms (row 
(a)), we were able to obtain a slightly better 
Passage2 score. As the BLTagger outputs 
semantic IDs which are defined in the 
BioLexicon, we tried to use these semantic IDs 
for query expansion (rows (b) and (d)).  However, 
the MAP scores degraded. 
63
6. Conclusions 
We have demonstrated three applications of the 
BioLexicon, which is a resource comprising 
linguistic information, targeted for use within 
bio-text mining applications.   
We have described the following three 
applications that will be useful for processing of 
biological literature. 
 
? BLTagger: dictionary-based POS tagger 
based on the BioLexicon 
? Enju full parser enriched by the 
BioLexicon 
? Lexicon-based query processing for 
information retrieval 
 
Our future work will include further intrinsic 
and extrinsic evaluations of the BioLexicon in 
NLP, including its  application to information 
extraction tasks in the biology domain. The 
BioLexicon is available for non-commercial 
purposes under the Creative Commons license. 
Acknowledgements 
This research has been supported by the EC IST 
project FP6-028099 (BOOTStrep), whose 
Manchester team is hosted by the 
JISC/BBSRC/EPSRC sponsored National Centre 
for Text Mining.   
References 
Browne, A.C., G. Divita, A.R. Aronson, and A.T. 
McCray. 2003. UMLS Language and Vocabulary 
Tools. In Proc. of AMIA Annual Symposium 2003, 
p.798. 
Dietrich Rebholz-Schuhmann, Piotr Pezik, Vivian Lee, 
Jung-Jae Kim, Riccardo del Gratta, Yutaka Sasaki, 
Jock McNaught, Simonetta Montemagni, Monica 
Monachini, Nicoletta Calzolari, Sophia Ananiadou, 
BioLexicon: Towards a Reference Terminological 
Resource in the Biomedical Domain, the 16th 
Annual International Conference on Intelligent 
Systems for Molecular Biology (ISMB-2008) 
(Poster), Toronto, Canada, 2008. 
(http://www.ebi.ac.uk/Rebholz-srv/BioLexicon/ 
BioLexicon_Poster_EBI_UoM_ILC.pdf) 
Fellbaum, C., editor. 1998. WordNet: An Electronic 
Lexical Database.  MIT Press, Cambridge, MA.. 
Francopoulo, G., M. George, N. Calzolari, M. 
Monachini, N. Bel, M. Pet, and C. Soria. 2006. 
Lexical Markup Framework (LMF). In Proc. of  
LREC 2006, Genova, Italy. 
Hersh, W., Aaron Cohen, Lynn Ruslen, and Phoebe 
Roberts, TREC 2007 Genomics Track Overview, 
TREC-2007, 2007. 
Kim, J-D., T. Ohta, Y. Tateisi, and J. Tsujii. 2003.  
GENIA Corpus - Semantically Annotated Corpus 
for Bio-Text Mining. Bioinformatics, 19:i180-i182. 
Kudo T., Yamamoto K., Matsumoto Y., Applying 
Conditional Random Fields to Japanese Mor- 
phological Analysis. In Proc. of Empirical 
Methods in Natural Language Processing 
(EMNLP-04), pp. 230?237, 2004. 
Lafferty, J., A. McCallum, and F. Pereira. 2001. 
Conditional Random Fields: Probabilistic Models 
for Segmenting and Labelling Sequence Data. In 
Proc. of the Eighteenth International Conference 
on Machine Learning (ICML-2001), pages 282-289.  
Miyao, Y. and J. Tsujii, 2003. Probabilistic modeling 
of argument structures including non-local 
dependencies. In Proc. of the Conference on 
Recent Advances in Natural Language Processing 
(RANLP 2003), pages 285-291. 
Quochi, V., Monachini, M., Del Gratta, R., Calzolari, 
N., A lexicon for biology and bioinformatics: the 
BOOTStrep experience. In Proc. of LREC 2008, 
Marrakech, 2008. 
Robertson, S.E., Walker S., Jones, S., Hancock-
Beaulieu M.M., and Gatford, M., 1995. Okapi at 
TREC-3. In Proc of Overview of the Third Text 
REtrieval Conference (TREC-3), pp. 109?126. 
Yutaka Sasaki, Simonetta Montemagni, Piotr Pezik, 
Dietrich Rebholz-Schuhmann, John McNaught, 
and Sophia Ananiadou, BioLexicon: A Lexical 
Resource for the Biology Domain, In Proc. of the 
Third International Symposium on Semantic 
Mining in Biomedicine (SMBM 2008), 2008. 
Table 1 Preliminary MAP scores for TREC Genomics Track 2007 data 
 
Query expansion method Passage2 MAP Aspect MAP Document MAP 
(a) BioLexicon terms 0.0702 0.1726 0.2158 
(b) BioLexicon terms 
 + semantic IDs 
0.0696 0.1673 0.2148 
(c) no query expansion  (baseline) 0.0683 0.1726 0.2183 
(d) semantic IDs 0.0677 0.1670 0.2177 
 
64
Advances in Open Domain Question Answering
Tomek Strzalkowski and Sanda Harabagiu (editors)
(SUNY Albany and University of Texas at Dallas)
Springer (Text, speech and language technology series, edited by Nancy Ide and
Jean Ve?ronis, volume 32), 2006, xxvi+566 pp; hardbound, ISBN 978-1-4020-4744-2,
$259.00
Reviewed by
Paul Thompson
Dartmouth College
This edited volume provides an excellent, comprehensive introduction to recent re-
search on open-domain question answering, meaning that the question-answering sys-
tems are not used merely in specific, narrow domains, but rather in domains such as
news or the Internet. Most of the chapters describe systems that have been developed
for the Advanced Question Answering for Intelligence, or AQUAINT, program spon-
sored by the U.S. Government Advanced Research and Development Activity (ARDA),
or the related Text Retrieval Conference (TREC) Question Answering (QA) Track. Both
of these activities began in 1999 and, as of this writing, still continue. Although much
of the research described here has this common origin, a wide variety of approaches
to question answering are represented in these chapters. The book is divided into six
sections:
1. Approaches to question answering
2. Question processing
3. Question answering as information retrieval
4. Answer extraction
5. Evaluating question-answering systems
6. Perspectives on question answering
Each section consists of two or three chapters.
Automatic question answering has long been studied in artificial intelligence re-
search. Early systems were constrained to answering questions about limited domains,
for example, baseball statistics or lunar rocks. By the 1990s, question answering had
become a less active field of research. During the 1990s the U.S. Government funded
the Tipster program for research on information retrieval and information extraction.
As discussed by Maiorano in his chapter ?Question answering: Technology for in-
telligence analysis,? it was hoped that the template-filling of Tipster?s information-
extraction tasks would require researchers to develop systems capable of deep natural
language understanding. Instead, shallow techniques were able to perform well on
those tasks. Similarly, the AQUAINT program is seen as building on information-
retrieval and information-extraction technology to provide systems that can extract
answers from open-domain free text for information seekers, rather than just ranked
lists of documents that might answer the question when read. Again, there is the
view that achieving a question-answering capability will require deep natural language
Computational Linguistics Volume 33, Number 4
understanding. Although some of the chapters in this volume describe long-range goals
of achieving levels of question answering requiring deep understanding, much of the
research described here so far has focused on simpler question-answering tasks, such
as the factoid question answering of the TREC QA track. As the book shows, however,
factoid, or slightly more complex, question answering can benefit from a variety of more
or less deep approaches.
Section 1 of the book contains three chapters presenting very different approaches
to question answering. The first chapter in the book, by Moldovan et al, describes how
definition-style questions can be answered using deep language processing and logic
theorem proving. The next chapter, by Ittycheriah, describes the application of maxi-
mum entropy and other machine-learning approaches to answering factoid questions.
The chapter by Vicedo and Ferra?ndez shows how anaphora resolution can improve
question-answering performance. Most contemporary question-answering systems use
an information-retrieval component to find candidate documents, followed by an ex-
traction component that produces an answer. Section 3 describes three approaches to
question answering that rely primarily on information-retrieval techniques.
Contemporary question-answering systems generally include question-processing
and answer-processing components, where a question type is first determined from
an analysis of the question, and then answers are sought using algorithms specific to
that question type. Sections 2 and 4 describe question processing and answer extrac-
tion. Harabagiu?s chapter, ?Questions and intentions,? provides a particularly useful
discussion of the importance of pragmatic knowledge in question answering. In Sec-
tion 4, Prager et al?s chapter, ?Question answering by predictive annotation,? discusses
preprocessing of text, for example, marking up named entities, and Srihari et al in
?Question answering supported by multiple levels of information extraction? focus
on applying information-extraction techniques at retrieval time. On the other hand,
Echihabi et al, in their chapter ?How to select an answer string,? use statistical and
information-theoretic techniques, which interpret an answer as a ?translation? of the
question.
The Tipster program, with its Message Understanding Conferences, and TREC both
placed a strong emphasis on evaluation. The TREC QA Track and the Interactive Ques-
tion Answering Track likewise have developed formal evaluation metrics, in particular
for factoid question answering. Though evaluation for the more advanced forms of
question answering in the AQUAINT program has not been as formal, some interesting
new approaches to evaluation involving analysts performing realistic tasks have been
developed, as discussed, for example, by Strzalkowski et al in their chapter, ?Question
answering as dialogue with data.? Voorhees?s ?Evaluating question answering system
performance? and Hersh?s ?Evaluating interactive question answering? describe the
TREC QA Track evaluations; Ogden et al?s ?Habitability in question-answering sys-
tems? takes a step back from current question-answering technology and considers user
requirements with future question-answering systems.
The final section of the book considers question answering from the perspectives of
intelligence analysis, statistical machine learning, and the long-range future. Maiorano?s
chapter has been mentioned earlier. Riloff et al in ?Reverse-engineering question /
answer collections? describe a technique with which to bootstrap training data to
support machine-learning approaches to question answering. Maybury?s concluding
chapter, ?New directions in question answering,? provides a long-range perspective
on question-answering research, describing three future question-answering visions.
The first is a user-centered one, where personal assistants model the user?s interests
and cognitive style. Another vision is of embedded intelligence, where intelligence and
598
Book Reviews
question-answering capabilities reside in the environment. The third vision is of em-
bodied agents, for example, virtual avatars or physical humanoid robots. Maybury
notes that one of the crucial issues will be discovering the right mixed-initiative sym-
biosis among human and artificial agents.
This book is based largely on an ongoing, but soon to be completed, research
program. The chapters necessarily reflect research from early in the program. It will be
useful to compare the present volume to a volume that might appear once the program
has been completed. So far, question answering has not had the commercial success of,
say, information-retrieval technology. It will be interesting to see how research in this
area proceeds once the stimulus from the AQUAINT program ends and whether the
expected utility of question answering for analysts can be extended to the more general
user.
Paul Thompson is a Research Associate Professor in the Department of Computer Science
at Dartmouth College. His research interests include probabilistic information retrieval, nat-
ural language processing, and information assurance. Thompson?s address is Department of
Computer Science, Dartmouth College, Hinman Box 6211, Hanover, NH 03755-3510; e-mail:
Paul.Thompson@dartmouth.edu.
599

Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 43?48,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Extending an interoperable platform to facilitate the creation
of multilingual and multimodal NLP applications
Georgios Kontonatsios?, Paul Thompson?, Riza Theresa Batista-Navarro?,
Claudiu Miha?ila??, Ioannis Korkontzelos and Sophia Ananiadou
The National Centre for Text Mining,
School of Computer Science, The University of Manchester
131 Princess Street, Manchester M1 7DN, UK
{kontonag,batistar,thompsop,mihailac,
korkonti,ananiads}@cs.man.ac.uk
Abstract
U-Compare is a UIMA-based workflow
construction platform for building natu-
ral language processing (NLP) applica-
tions from heterogeneous language re-
sources (LRs), without the need for pro-
gramming skills. U-Compare has been
adopted within the context of the META-
NET Network of Excellence, and over
40 LRs that process 15 European lan-
guages have been added to the U-Compare
component library. In line with META-
NET?s aims of increasing communication
between citizens of different European
countries, U-Compare has been extended
to facilitate the development of a wider
range of applications, including both mul-
tilingual and multimodal workflows. The
enhancements exploit the UIMA Subject
of Analysis (Sofa) mechanism, that allows
different facets of the input data to be rep-
resented. We demonstrate how our cus-
tomised extensions to U-Compare allow
the construction and testing of NLP appli-
cations that transform the input data in dif-
ferent ways, e.g., machine translation, au-
tomatic summarisation and text-to-speech.
1 Introduction
Currently, there are many repositories that con-
tain a range of NLP components, e.g., OpenNLP1,
Stanford CoreNLP2, JULIE NLP Toolsuite3 and
NaCTeM software tools4. The ability to chain
components from these repositories into pipelines
is a prerequisite to facilitate the development of
?The authors have contributed equally to the development
of this work and production of the manuscript.
1http://opennlp.sourceforge.net/projects.html
2http://nlp.stanford.edu/software/corenlp.shtml
3http://www.julielab.de/Resources/Software/NLP Tools.html
4http://nactem.ac.uk/software.php
complex NLP applications. Combining together
heterogeneous components is not, however, al-
ways straightforward. The various components
used in a pipeline may be implemented using dif-
ferent programming languages, may have incom-
patible input/output formats, e.g., stand-off or in-
line annotations, or may require or produce incom-
patible data types, e.g., a particular named entity
recogniser (NER) may require specific types of
syntactic constituents as input, making it impor-
tant to choose the right type of syntactic parser to
run prior to the NER. Thus, the tools required to
build a new application may not be interoperable
with each other, and considerable extra work may
be required to make the tools talk to each other.
The Unstructured Information Management Ar-
chitecture (UIMA) (Ferrucci and Lally, 2004) was
created as a means to alleviate such problems. It
is a framework that facilitates the straightforward
combination of LRs, i.e., tools and corpora, into
workflow applications. UIMA is an OASIS stan-
dard that enables interoperability of LRs by defin-
ing a standard workflow metadata format and stan-
dard input/output representations.
U-Compare (Kano et al, 2011) is a graphical
NLP workflow construction platform built on top
of UIMA. It facilitates the rapid construction, test-
ing and evaluation of NLP workflows using drag-
and-drop actions within its graphical user inter-
face (GUI). U-Compare enhances interoperabil-
ity among UIMA-compliant LRs, by defining a
common and sharable Type System, i.e., a hier-
archy of annotation types, which models a wide
range of NLP data types, e.g., sentence, token,
part-of-speech tag, named entity and discourse
annotations. The aim is for all components in
U-Compare?s library to be compliant with this
type system. In the context of META-NET, U-
Compare?s library has been extended with 46 new
LRs supporting 15 European languages, all of
which are compliant with the same type system.
43
This makes U-Compare the world?s largest repos-
itory of type system-compatible LRs, allowing
users to seamlessly combine together resources to
create a range of NLP applications.
Previously, U-Compare was able to support the
development of a wide range of monolingual lex-
ical, syntactic and semantic processing tasks ap-
plications that enriched textual input documents
by adding annotations of various types. However,
not all NLP applications operate in this way; some
workflows transform the input data to create new
?views? of the input data. The META-NET project
aims to ensure equal access to information by all
European citizens. This aim implies the devel-
opment of both multilingual applications, which
transform input data from one language into an-
other, or multimodal applications, in which text
may be transformed into speech, or vice versa.
U-Compare has been extended in several ways
to support the construction of these more complex
workflow types. Specifically, information about
both the original and transformed data, together
with annotations associated with each view, can
now be visualised in a straightforward manner.
The changes support two new categories of work-
flow. Firstly, workflows that produce two or more
textual views of an input text are useful not only
for multilingual applications, such as those that
carry out machine translation, but also applica-
tions that transform the input text in other ways,
such as those that produce a summary of an in-
put text. Secondly, workflows that output audio as
well as textual views, e.g., text-to-speech applica-
tions, are also supported.
2 Related work
Over the past few years, an increasing num-
bers of researchers have begun to create and dis-
tribute their own workflow construction architec-
tures (Ferrucci and Lally, 2004; Cunningham et
al., 2002; Grishman et al, 1997; Scha?fer, 2006)
or platforms (Kano et al, 2011; Rak et al, 2012;
Ogrodniczuk and Karagiozov, 2011; Savova et al,
2010) that allow the rapid development of NLP ap-
plications.
GATE (Cunningham et al, 2002) is a workflow
construction framework that has been used to de-
velop several types of NLP applications, including
summarisation systems. It facilitates the develop-
ment of a wide range of NLP applications by pro-
viding a collection of components that can process
various languages, together with Java libraries that
handle character encoding for approximately 100
languages. However, GATE does not formally de-
fine any standards to model multilingual or mul-
timodal applications, but rather aims to boost the
development process of NLP applications.
TIPSTER (Grishman et al, 1997) is a generic
framework for the development of NLP applica-
tions. TIPSTER provides multilingual function-
alities by associating text segments of a paral-
lel document with one or more languages. This
allows language-dependent NLP components to
process only the appropriate mono-lingual sub-
documents. However, TIPSTER does not provide
explicit guidelines regarding the annotation types
and attributes that are produced by components.
This lack of a common and sharable system of
annotation types discourages interoperability be-
tween LRs. However, TIPSTER does not provide
a mechanism that facilitates the development of
multilingual or multimodal NLP applications.
Heart of Gold (Scha?fer, 2006) is an XML-
based workflow construction architecture that en-
ables interoperability of tools developed in dif-
ferent programming languages to be combined
into pipelines. Heart of Gold contains a rich li-
brary of shallow and deep parsing components
supporting several languages, e.g., English, Ger-
man, Japanese and Greek. Nonetheless, Heart of
Gold does not specifically support the construction
of multilingual or multimodal workflows.
In contrast to the other frameworks introduced
above, UIMA (Ferrucci and Lally, 2004) provides
an abstract-level mechanism that can be used to
support the development of workflows that carry
out transformations of the input data. This mech-
anism is called the Subject of Analysis or Sofa.
Multiple Sofas can be linked with an input file,
each of which stores different data and associ-
ated annotations. This mechanism can thus be ex-
ploited to represent alternative ?views? of the in-
put data, such as a source text and its translation.
The data stored in different Sofas is not restricted
to textual information; it can also correspond to
other modalities, such as audio data. This makes
the Sofa mechanism equally suitable for storing
the output of text-to-speech workflows. Our ex-
tensions to U-Compare are thus implemented by
reading and displaying the contents of different
types of Sofas.
The Sofa mechanism has previously been
44
under-exploited by UIMA developers, despite its
power in allowing more complex NLP workflows
to be constructed. Indeed, no other existing
UIMA-based platform (Kano et al, 2011; Rak et
al., 2012; Savova et al, 2010; Hahn et al, 2008)
has demonstrated the use of Sofas to construct
multilingual or multimodal applications. Thus, to
our knowledge, our enhancements to U-Compare
constitute the first attempt to make the construc-
tion of workflows that carry out transformations of
input data more readily available to UIMA users,
without the need for programming skills.
3 METANET4U Components in
U-Compare
The two dozen national and many regional lan-
guages of Europe present linguistic barriers that
can severely limit the free flow of goods, infor-
mation and services. The META-NET Network
of Excellence was created to respond to this is-
sue. Consisting of 60 research centres from 34
countries, META-NET has aimed to stimulate a
concerted, substantial and continent-wide effort to
push forward language technology research and
engineering, in order to ensure equal access to
information and knowledge for all European cit-
izens.
META-NET?s aims are dependent on the ready
availability of LRs that can carry out NLP and
text mining (TM) on a range of European lan-
guages. Such resources constitute the building
blocks for constructing language technology ap-
plications that can help European citizens to gain
easy access to the information they require. One
of the major outcomes of META-NET has been
the development of META-SHARE, an open, dis-
tributed facility for sharing and exchange of LRs
in a large number of European languages.
Within the context of META-NET, interoper-
ability of LRs is clearly of utmost importance, to
expedite the process of developing new NLP ap-
plications. In order to provide a concrete demon-
stration of the utility and power of promoting in-
teroperability within META-SHARE, one of the
sub-projects of META-NET, i.e., METANET4U,
has carried out a pilot study on interoperability,
making use of the UIMA framework and the U-
Compare platform. It is in this context that a set
of 46 new LRs, available in META-SHARE, were
wrapped as UIMA components and made avail-
able in U-Compare. Of these components, 37 op-
erate on one or more specific languages other than
English and 4 are language-independent. Table 1
shows the full set of categories of UIMA com-
ponents created during the METANET4U project,
together with the languages supported.
Several of these new components output mul-
tiple Sofas, i.e., two machine translation compo-
nents, two automatic summarisation components
and a text-to-speech component. It is hoped that
our U-Compare extensions will help to stimulate
the development of a greater number of related
UIMA components, and thus promote a new level
of complexity for future UIMA workflows.
Component Function Supported Languages
Language Identifier 54 modern languages
Paragraph breaker pt, mt
Sentence splitter en, pt ,mt, es, ca, ast,cy, gl, it
Tokeniser en, pt, mt, es, ca, ast,cy, gl, it, fr
Morph. Analyser en, pt, es, ca, ast,cy, gl, it, ro, eu, fr
POS Tagger en, es, ca, cy, gl, it,pt, ro, eu, fr, mt
Syntactic chunker en, es, ca, gl,ast, ro, fr
NP chunker ro
Segmenter ro, en
FDG Parser ro
Dependency Parser en, es, ca, gl, ast
Discourse Parser ro
NER Languageindependent
Summariser ro, en
Machine translation es?{gl,pt,ca}en?es, eu?es
Table 1: METANET4U UIMA components
4 Enhancements to U-Compare
In UIMA, an artefact, i.e., raw text, audio, im-
age, video, and its annotations, e.g., part-of-
speech tags, are represented in a standard format,
namely the Common Analysis Structure (CAS).
A CAS can contain any number of smaller sub-
CASes, i.e., Sofas, that carry different artefacts
with their linked annotations. Figure 1 illustrates
the different types of Sofas that are created by the
three types of workflows that we will demonstrate.
Firstly, for a machine translation workflow, at least
45
Multi-lingualMulti-modalWorkflowsDocuments aZ ??
CAS
?CASCAS
SOFA SOFA
SOFA SOFA
SOFA SOFA
Figure 1: UIMA based multilingual and multi-
modal workflow architecture
two CAS views, i.e., Sofas, are created, the first
corresponding to the text in the source language,
and the other Sofas corresponding to the transla-
tion(s) of the source text into target language(s).
The second type of workflow, i.e., automatic sum-
marisation, is related to the former workflow, in
that the two Sofas produced by the workflow are
both textual, one containing the input text and one
containing a summary of the original text. The
third type of workflow is different, in that a Sofa
containing audio data is used to represent the out-
put of a multimodal workflow.
Two specific extensions have been made to U-
Compare to handle both textual and audio So-
fas. When the output of a workflow consists of
multiple textual views (Sofas), the default anno-
tation viewer is automatically split to allow mul-
tiple views of the text to be displayed and side-
by-side. This can be useful, e.g., to allow careful
comparison of a source text and target translation
in a machine translation workflow. To handle au-
dio Sofas, we have developed a new, customised
viewer that can visualise and play audio data. The
visualisation consists of a graphical display of the
waveform, power information and spectrogram, as
well as segmentation of the audio data into re-
gions (such as individual tokens) and transcrip-
tions, if such information is present in the audio
Sofa. The viewer makes use the open-source li-
brary Java Speech Toolkit (JSTK)5.
5 Workflow applications
In order to provide a practical demonstration of
the enhanced capabilities of U-Compare, we show
5http://code.google.com/p/jstk
three different workflows that transform the input
data in different ways, namely translation, auto-
matic summarisation and speech synthesis. In this
section, we provide brief details of these work-
flows.
5.1 Machine translation
The University of Manchester has created UIMA
wrapper components corresponding to different
modules of Apertium (Corb??-Bellot et al, 2005), a
free rule-based machine translation engine. These
components consist of a morphological analyser,
POS tagger and translator. The three components
must be run in sequence to carry out translation,
although the first two components can be used
in other workflows to carry out monolingual
analyses. The UIMA components currently
handle a subset of the 27 languages dealt with
by the complete Apertium system, corresponding
to the languages of the METANET4U partners,
i.e., English?Spanish, Galician?Spanish,
Portuguese?Spanish, Catalan?Spanish and
Basque?Spanish. However, additional language
pairs can be added straightforwardly. Our sample
workflow includes as its initial component the
Language Identifier from the Romanian Academy
Research Institute for Artificial Intelligence
(RACAI), to automatically detect the language of
the text in the input Sofa. The subsequent compo-
nents in the workflow are the Apertium modules.
The workflow demonstrates how heterogeneous
components from different research groups can
be combined into workflows to create new NLP
applications. A sample output from running the
workflow is shown in Figure 2. The input text
was detected as English by the RACAI Language
Identifier. The English text was subsequently
analysed by the morphological analyser and POS
Tagger, and translated to Spanish by the translator.
Figure 2 illustrates the side-by-side display of the
contents of the two Sofas.
5.2 Automatic summarisation
Automatic summarisation for Romanian text can
be carried out by creating a workflow consisting
of two components developed by the Universitatea
?Alexandru Ioan Cuza? din Ias?i (UAIC). Firstly,
a segmenter (UAICSeg) splits the input text into
fragments, which are in turn used as input to the
summariser component (UAICSum). The length
of the output summary (percentage of the whole
document) is parameterised. As can be seen in
46
Figure 2: Translation of English text to Spanish
Figure 3: Summarisation of Romanian text
Figure 3, the output of this workflow is displayed
using the same parallel Sofa viewer. In this case,
the full text is displayed in the left-hand pane and
the summary is shown in the right-hand pane.
5.3 Speech synthesis
The Universitat Polite`cnica de Catalunya (UPC)
developed a speech synthesiser component that
is based around their Ogmios text-to-speech sys-
tem (Bonafonte et al, 2006). The UIMA com-
ponent version of this tool generates separate text
and audio Sofas; the former stores the textual to-
kens and textual representations of their pronun-
ciations, whilst the latter stores the start and end
time offsets of each of the tokens in the audio file,
together with their transcriptions. Fig. 4 shows
how the textual Sofa information is displayed in
U-Compare?s default annotation viewer, whilst the
audio Sofa information is shown in the new au-
dio visualiser mentioned above. The three differ-
ent types of visual information are displayed be-
low each other, and the segments (tokens) of the
audio file, together with their transcriptions, are
displayed at the bottom of the window. A ?Play?
button allows either the complete file or a selected
segment to be played.
6 Conclusions
The requirements of META-NET have motivated
several new enhancements to the U-Compare plat-
form, which, to our knowledge, make it the first
UIMA-based workflow construction platform that
is fully geared towards the development of NLP
applications that support a wide range of European
languages. The 46 new UIMA-wrapped LRs that
have been made available through U-Compare,
supporting 15 different European languages and
all compliant with the same type system, mean
that the improved U-Compare is essentially a hub
of multilingual resources, which can be freely and
flexibly combined to create new workflows. In
47
Figure 4: Speech Synthesis
addition, our enhancements to U-Compare mean
that various types of multilingual and multimodal
workflows can now be created with the minimum
effort. These enhancements are intended to make
U-Compare more attractive to users, and to help
stimulate the development of a new generation of
more complex UIMA-based NLP applications. As
future work, we intend to extend the library of
components that output multiple Sofas, and further
extend the functionalities of U-Compare to handle
other data modalities, e.g., video.
Acknowledgements
This work was partially funded by the Euro-
pean Community?s Seventh Framework Program
(FP7/2007-2013) [grant number 318736 (OSS-
METER)]; MetaNet4U project (ICT PSP Pro-
gramme) [grant number 270893]; and Engineer-
ing and Physical Sciences Research Council [grant
numbers EP/P505631/1, EP/J50032X/1].
References
A. Bonafonte, P. Agu?ero, J. Adell, J. Pe?rez, and
A. Moreno. 2006. Ogmios: The upc text-to-speech
synthesis system for spoken translation. In TC-
STAR Workshop on Speech-to-Speech Translation,
pages 199?204.
A. Corb??-Bellot, M. Forcada, S. Ortiz-Rojas, J. Pe?rez-
Ortiz, G. Ram??rez-Sa?nchez, F. Sa?nchez-Mart??nez,
I. Alegria, A. Mayor, and K. Sarasola. 2005.
An open-source shallow-transfer machine transla-
tion engine for the romance languages of Spain. In
Proceedings of the 10th Conference of the EAMT,
pages 79?86.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: an architecture for devel-
opment of robust HLT applications.
D. Ferrucci and A. Lally. 2004. Building an ex-
ample application with the unstructured information
management architecture. IBM Systems Journal,
43(3):455?475.
R. Grishman, B. Caid, J. Callan, J. Conley, H. Corbin,
J. Cowie, K. DiBella, P. Jacobs, M. Mettler, B. Og-
den, et al 1997. TIPSTER text phase ii architecture
design version 2.1 p 19 june 1996.
U. Hahn, E. Buyko, R. Landefeld, M. Mu?hlhausen,
M. Poprat, K. Tomanek, and J. Wermter. 2008. An
overview of JCoRe, the JULIE lab UIMA compo-
nent repository. In LREC?08 Workshop ?Towards
Enhanced Interoperability for Large HLT Systems:
UIMA for NLP?, pages 1?7, Marrakech, Morocco,
May.
Y. Kano, M. Miwa, K. Cohen, L. Hunter, S. Ananiadou,
and J. Tsujii. 2011. U-compare: A modular nlp
workflow construction and evaluation system. IBM
Journal of Research and Development, 55(3):11.
M. Ogrodniczuk and D. Karagiozov. 2011. Atlas - the
multilingual language processing platform. Proce-
samiento de Lenguaje Natural, 47(0):241?248.
R. Rak, A. Rowley, W. Black, and S. Ananiadou.
2012. Argo: an integrative, interactive, text mining-
based workbench supporting curation. Database:
The Journal of Biological Databases and Curation,
2012.
G. Savova, J. Masanz, P. Ogren, J. Zheng, S. Sohn,
K. Kipper-Schuler, and C. Chute. 2010. Mayo clin-
ical text analysis and knowledge extraction system
(ctakes): architecture, component evaluation and ap-
plications. Journal of the American Medical Infor-
matics Association, 17(5):507?513.
U. Scha?fer. 2006. Middleware for creating and com-
bining multi-dimensional nlp markup. In Proceed-
ings of the 5th Workshop on NLP and XML: Multi-
Dimensional Markup in Natural Language Process-
ing, pages 81?84. ACL.
48
Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 69?77,
Uppsala, July 2010.
Evaluating a Meta-Knowledge Annotation Scheme for Bio-Events 
 
 
Raheel Nawaz1 Paul Thompson1,2 Sophia Ananiadou1,2 
1School of Computer Science, University of Manchester, UK 
2National Centre for Text Mining, University of Manchester, UK  
E-mail: nawazr@cs.man.ac.uk, paul.thompson@manchester.ac.uk, 
sophia.ananiadou@manchester.ac.uk 
 
  
 
Abstract 
The correct interpretation of biomedical texts 
by text mining systems requires the recogni-
tion of a range of types of high-level informa-
tion (or meta-knowledge) about the text. Ex-
amples include expressions of negation and 
speculation, as well as pragmatic/rhetorical in-
tent (e.g. whether the information expressed 
represents a hypothesis, generally accepted 
knowledge, new experimental knowledge, 
etc.) Although such types of information have 
previously been annotated at the text-span 
level (most commonly sentences), annotation 
at the level of the event is currently quite 
sparse. In this paper, we focus on the evalua-
tion of the multi-dimensional annotation 
scheme that we have developed specifically 
for enriching bio-events with meta-knowledge 
information. Our annotation scheme is in-
tended to be general enough to allow integra-
tion with different types of bio-event annota-
tion, whilst being detailed enough to capture 
important subtleties in the nature of the meta-
knowledge expressed in the text. To our 
knowledge, our scheme is unique within the 
field with regards to the diversity of meta-
knowledge aspects annotated for each event, 
whilst the evaluation results have confirmed 
its feasibility and soundness.  
1 Introduction 
The ability to recognise high-level information 
(or meta-knowledge) relating to the interpreta-
tion of texts is an important task for text mining 
systems. There are several types of meta-
knowledge that fall under this category. For ex-
ample, the detection of expressions of specula-
tion and negation is important across all do-
mains, although the way in which these phenom-
ena are expressed may be domain-specific. In 
scientific texts, it is also important to be able to 
determine other types of information, such as the 
author?s rhetorical/pragmatic intent (de Waard et 
al., 2009). This would correspond to whether the 
information expressed represents a hypothesis, 
accepted knowledge, new experimental knowl-
edge, etc.  
The ability to distinguish between these dif-
ferent types of information can be important for 
tasks such as  building and updating models of 
biological processes, like pathways (Oda et al, 
2008), and curation of biological databases 
(Ashburner et al, 2000). Central to both of these 
tasks is the identification of new knowledge that 
can enhance these resources, e.g. to build upon 
an existing, but incomplete model of a biological 
process (Lisacek et al, 2005) or to ensure that 
the database is kept up to date. Any new knowl-
edge added should be supported though evi-
dence, which could include linking hypotheses 
with experimental findings. It is also important to 
take into account inconsistencies and contradic-
tions reported in the literature. 
The production of annotated corpora can help 
to train text mining systems to recognise types of 
meta-knowledge, such as the above. Although a 
number of such corpora have already been pro-
duced, different annotation schemes are required 
according to the exact domain under considera-
tion, as well as the types of task that will be un-
dertaken by the text mining system.  
The work described in this paper is focused on 
the design and evaluation of the meta-knowledge 
annotation scheme described in Nawaz et al, 
(2010). The annotation scheme has been specifi-
cally designed to recognise a range of meta-
knowledge types for events extracted from bio-
medical texts (henceforth bio-events). The aim is 
to facilitate the development of more useful sys-
tems in the context of various biomedical infor-
mation extraction (IE) and textual inference (TI) 
tasks. Although the scheme has been designed 
69
for application to existing bio-event corpora, it is 
intended to be applied to any type of bio-relation 
corpora, and can easily be tailored for other types 
of relations/events within the domain. 
1.1  Bio-Event Representation of Text 
Searching for relevant information in electronic 
documents is most commonly carried out by en-
tering keywords into a search engine. However, 
such searches will normally return a huge num-
ber of documents, many of which will be irrele-
vant to the user?s needs.  
A more promising and efficient way of search-
ing is over events that have been extracted from 
texts through the application of natural language 
processing methods. An event is a structured rep-
resentation of a certain piece of information con-
tained within the text, which is usually anchored 
to a particular word in the text (typically a verb 
or noun) that is central to the description of the 
event. Events are often represented by a tem-
plate-like structure with slots that are filled by 
the event participants. Each event participant is 
also assigned a role within the event. These par-
ticipants can be entities, concepts or even other 
events. This kind of event representation allows 
the information contained in a text to be repre-
sented as a collection of nested events.  
A bio-event is an event specialised for the 
biomedical domain. Kim et al (2008) define a 
bio-event as a dynamic bio-relation involving 
one or more participants. These participants can 
be bio-entities or (other) bio-events, and are each 
assigned a semantic role/slot like theme and 
cause etc. Each bio-event is typically assigned a 
type/class from a chosen bio-event taxon-
omy/ontology, e.g., the GENIA Event Ontology 
(Kim et al, 2008). Similarly, the bio-entities are 
also assigned types/classes from a chosen bio-
term taxonomy/ontology, e.g., the Gene Ontol-
ogy (Ashburner et al, 2000). 
As an example, consider the simple sentence 
shown in Figure 1. 
This sentence contains a single bio-event, an-
chored to the verb activates. Figure 2 shows a 
typical structured representation of this bio-
event. 
The fact that the verb is anchored to the verb 
activates allows the event-type of positive regu-
lation to be assigned. The event has two slots, 
i.e. theme and cause whose labels help to charac-
terise the contribution that the slot filler makes 
towards the meaning of the event. In this case, 
the slots are filled by the subject and object of 
the verb activates, both of which correspond to 
different types of bio-entities (i.e. operon and 
protein).  
IE systems trained to extract bio-events from 
texts allow users to formulate semantic queries 
over the extracted events. Such queries can  
specify semantic restrictions on the events in 
terms of event types, semantic role labels and 
named entity types etc. (Miyao et al, 2006), in 
addition to particular keywords. For example, it 
would be possible to search only for those texts 
containing bio-events of type nega-
tive_regulation where the cause is an entity of 
type protein. Such queries provide a great deal 
more descriptive power than traditional keyword 
searches over unstructured documents.  Bio-
medical corpora that have been manually anno-
tated with event level information (e.g., Pyysalo 
et al, 2007; Kim et al, 2008; Thompson et al, 
2009) facilitate the training of systems such as 
those described above.  
Whilst event-based querying has advantages 
for efficient searching, the extracted events have 
little practical use if they are not accompanied by 
meta-knowledge information to aid in their inter-
pretation.  
1.2 Existing Meta-knowledge Annotation 
Various corpora of biomedical literature (ab-
stracts and/or full papers) have been produced 
that feature some degree of meta-knowledge an-
notation. These corpora vary in both the richness 
of the annotation added, and the type/size of the 
units at which the meta-knowledge annotation 
has been performed. Taking the unit of annota-
tion into account, we can distinguish between 
annotations that apply to continuous text-spans, 
and annotations that have been performed at the 
event level. 
Text-Span Annotation: Such annotations have 
mostly been carried out at the sentence level. 
They normally concentrate on a single aspect (or 
The results suggest that the narL gene product 
activates the nitrate reductase operon. 
 
Figure 1. A Simple Sentence from a Biomedi-
cal Abstract 
Figure 2. Typical Structured Representation 
of the Bio-Event mentioned in Figure 1 
EVENT-TRIGGER: activates 
EVENT-TYPE: positive_regulation 
THEME: nitrate reductase operon: operon 
CAUSE: narL gene product: protein 
 
70
dimension) of meta-knowledge, normally either 
speculation/certainty level, (e.g., Light et al, 
2004; Medlock & Briscoe, 2007; Vincze et al, 
2008) or general information content/rhetorical 
intent, e.g., background, methods, results, in-
sights. This latter type of annotation has been 
attempted both on abstracts, (e.g., McKnight & 
Srinivasan, 2003; Ruch et al, 2007) and full pa-
pers, (e.g. Teufel et al, 1999; Langer et al, 2004; 
Mizuta & Collier, 2004), with the number of dis-
tinct annotation categories varying between 4 
and 14.  
Despite the availability of these corpora, anno-
tation at the sentence level can often be too 
granular. In terms of information content, a sen-
tence may describe, for example, both an ex-
perimental method and its results. The situation 
becomes more complicated if a sentence contains 
an expression of speculation. If this is only 
marked at the sentence level, there may be con-
fusion about which part(s) of the sentence are 
affected by the speculative expression.  
Certain corpora and associated systems have 
attempted to address these issues. The BioScope 
corpus (Vincze et al, 2008) annotates the scopes 
of negative and speculative keywords, whilst 
Morante & Daelemans (2009) have trained a sys-
tem to undertake this task. The scheme described 
by Wilbur et al (2006) applies annotation to 
fragments of sentences, which are created on the 
basis of changes in the meta-knowledge ex-
pressed. The scheme consists of multiple annota-
tion dimensions which capture aspects of both 
certainty and rhetorical/pragmatic intent, 
amongst other things. Training a system to auto-
matically annotate these dimensions is shown to 
be highly feasible (Shatkay et al, 2008). 
Event-Level Annotation: Explicit annotation of 
meta-knowledge at the event-level is currently 
rather minimal within biomedical corpora. 
Whilst several corpora contain annotations to 
distinguish positive and negative events (e.g. 
Sanchez-Graillet & Poesio, 2007; Pyysalo et al, 
2007), the annotation of the GENIA Event Cor-
pus (Kim et al, 2008) is slightly more extensive, 
in that it additionally annotates certainty level. 
To our knowledge, no existing bio-event corpus 
has attempted annotation that concerns rhetori-
cal/pragmatic intent.  
 
1.3 The Need for an Event-Centric Meta-
Knowledge Annotation Scheme 
In comparison to meta-knowledge annotation 
carried out at the text-span level, the amount of 
annotation carried out at the event level is quite 
sparse. The question thus arises as to whether it 
is possible to use systems trained on text-span 
annotated corpora to assign meta-knowledge to 
bio-events, or whether new annotation at the 
event level is required.  
Some corpora seem better suited to this pur-
pose than others ? whilst sentence-level annota-
tions are certainly too granular for an event-
centric view of the text, sentence fragments, such 
as those identified by Wilbur et al (2006), are 
likely to correspond more closely to the extent of 
text that describes an event and its slots. Like-
wise, knowing the scopes of negative and specu-
lative keywords within a sentence may be a use-
ful aid in determining whether they affect the 
interpretation of a particular event.   
However, the information provided in these 
corpora is still not sufficiently precise for event-
level meta-knowledge annotation. Even within a 
text fragment, there may be several different bio-
events, each with slightly different meta-
knowledge interpretations. In a similar way, not 
all events that occur within the scope of a nega-
tion or speculation keyword are necessarily af-
fected by it.  
  Based on these observations, we have devel-
oped a meta-knowledge annotation scheme that 
is specifically tailored to bio-events. Our scheme 
annotates various different aspects or dimensions 
of meta-knowledge. A close examination of a 
large number of relevant bio-events has resulted 
in a scheme that has some similarities to previ-
ously proposed schemes, but has a number of 
differences that seem especially relevant when 
dealing with events, e.g. the annotation of the 
manner of the event. The scheme is intended to 
be general enough to allow integration with ex-
isting bio-event annotation schemes, whilst being 
detailed enough to capture important subtleties in 
the nature of the meta-knowledge expressed 
about the event.  
1.4 Lexical Markers of Meta-Knowledge 
Most of the existing corpora mentioned above 
annotate text spans or events with particular 
categories (e.g. certainty level or general infor-
mation type) in different meta-knowledge di-
mensions. However, what they do not normally 
do is to annotate lexical clues or keywords used 
to determine the correct values.  
A number of previous studies have demon-
strated the importance of lexical markers (i.e., 
words or phrases) that can accompany statements 
in scientific articles in determining the intended 
71
interpretation of the text (e.g. Hyland, 1996; Ri-
zomilioti 2006). We also performed a similar 
study (Thompson et al, 2008) although, in con-
trast to other studies, we took a multi-
dimensional approach to the categorisation of 
such lexical items, acknowledging that several 
types of important information may be expressed 
through different words in the same sentence. As 
an example, let us consider the example sentence 
in Figure 3.  
The author?s pragmatic/rhetorical intent to-
wards the statement that the catalytic role of 
these side chains is associated with their interac-
tion with the DNA substrate is encoded by the 
word indicate, which shows that the statement 
represents an analysis of the evidence stated at 
the beginning of the sentence, i.e., that the muta-
tions at positions 849 and 668 have DNA-
binding properties. Furthermore, the author?s 
certainty level (i.e., their degree of confidence) 
towards this analysis is shown by the word may. 
Here, the author is uncertain about the validity of 
their analysis. 
Whilst our previous work served to demon-
strate that the different aspects of meta-
knowledge that can be specified lexically within 
texts require a multi-dimensional analysis to cor-
rectly capture their subtleties, it showed that the 
presence of particular lexical items is not the 
only important feature for determining meta-
knowledge categories. In particular, their pres-
ence does not guarantee that the ?expected? in-
terpretation can be assumed (S?ndor, 2007). In 
addition, not all types of meta-knowledge are 
indicated through explicit markers. Mizuta & 
Collier (2004) note that  rhetorical zones may be 
indicated not only through explicit lexical mark-
ers, but also through features such as the main 
verb in the clause and the position of the sen-
tence within the article or abstract. 
For these reasons, we perform annotation on 
all relevant instances, regardless of the presence 
of lexical markers. This will allow systems to be 
trained that can learn to determine the correct 
meta-knowledge category, even when lexical 
markers are not present. However, due to the 
proven importance of lexical markers in deter-
mining certain meta-knowledge dimensions, our 
annotation scheme annotates such markers, 
whenever they are present. 
2 Annotation Scheme 
The annotation scheme we present here is a 
slightly modified version of our original meta-
knowledge annotation scheme (Nawaz et al, 
2010). The modified scheme consists of five 
meta-knowledge dimensions, each with a set of 
complete and mutually-exclusive categories, i.e., 
any given bio-event belongs to exactly one cate-
gory in each dimension. Our chosen set of anno-
tation dimensions has been motivated by the 
major information needs of biologists discussed 
earlier, i.e., the ability to distinguish between 
different intended interpretations of events. 
In order to minimise the annotation burden, 
the number of possible categories within each 
dimension has been kept as small as possible, 
whilst still respecting important distinctions in 
meta-knowledge that have been observed during 
our corpus study.     
The advantage of using a multi-dimensional 
scheme is that the interplay between different 
values of each dimension can reveal both subtle 
and substantial differences in the types of meta-
knowledge expressed in the surrounding text. 
Therefore, in most cases, the exact rhetori-
cal/pragmatic intent of an event can only be de-
termined by considering a combination of the 
values of different dimensions. This aspect of our 
scheme is further discussed in section 3. 
 
Figure 4 provides an overview of the annota-
tion scheme. The boxes with the light-coloured 
(grey) background correspond to information 
that is common to most bio-event annotation 
schemes, i.e., the participants in the event, to-
gether with an indication of the class or type of 
Figure 4. Bio-Event Annotation 
 
Figure 3. Example Sentence 
 
The DNA-binding properties of mutations at posi-
tions 849 and 668 may indicate that the catalytic 
role of these side chains is associated with their 
interaction with the DNA substrate. 
 
72
the event. The boxes with the darker (green) 
backgrounds correspond to our proposed meta-
knowledge annotation dimensions and their pos-
sible values. The remainder of this section pro-
vides brief details of each annotation dimension.  
2.1 Knowledge Type (KT) 
This dimension is responsible for capturing the 
general information content of the event. Whilst 
less detailed than some of the previously pro-
posed sentence-level schemes, its purpose is to 
form the basis of distinguishing between the 
most critical types of rhetorical/pragmatic intent, 
according to the needs of biologists. Each event 
is thus classified into one of the following four 
categories: 
Investigation: Enquiries or investigations, which 
have either already been conducted or are 
planned for the future, typically marked by lexi-
cal clues like examined, investigated and studied, 
etc.  
Observation: Direct observations, often repre-
sented by lexical clues like found, observed and 
report, etc.  Simple past tense sentences typically 
also describe observations. Such events represent 
experimental knowledge.  
Analysis: Inferences, interpretations, specula-
tions or other types of cognitive analysis, typi-
cally expressed by lexical clues like suggest, in-
dicate, therefore and conclude etc. Such events, 
if they are interpretations or reliable inferences 
based on experimental results, can also constitute 
another type of (indirect) experimental knowl-
edge. Weaker inferences or speculations, how-
ever, may be considered as hypotheses which 
need further proof through experiments.  
General: Scientific facts, processes, states or 
methodology. This is the default category for the 
knowledge type dimension. 
2.2 Certainty Level (CL) 
The value of this dimension is almost always 
indicated through the presence/absence of an ex-
plicit lexical marker. In scientific literature, it is 
normally only applicable to events whose KT 
corresponds either to Analysis or General. In the 
case of Analysis events, CL encodes confidence 
in the truth of the event, whilst for General 
events, there is a temporal aspect, to account for 
cases where a particular process is explicitly 
stated to occur most (but not all) of the time, us-
ing a marker such as normally, or only occasion-
ally, using a marker like sometimes.  Events cor-
responding to direct Observations are not open to 
judgements of certainty, nor are Investigation 
events, which refer to things which have not yet 
happened or have not been verified.  
Regarding the choice of values for the CL di-
mension, there is an ongoing discussion as to 
whether it is possible to partition the epistemic 
scale into discrete categories (Rubin, 2007). 
However, the use of a number of distinct catego-
ries is undoubtedly easier for annotation pur-
poses and has been proposed in a number of pre-
vious schemes. Although recent work has sug-
gested the use of  four or more categories (Shat-
kay et al, 2008; Thompson et al, 2008), our ini-
tial analysis of bio-event corpora has shown that 
only three levels of certainty seem readily distin-
guishable for bio-events. This is in line with 
Hoye (1997), whose analysis of general English 
showed that there are at least three articulated 
points on the epistemic scale.  
We have chosen to use numerical values for 
this dimension, in order to reduce potential anno-
tator confusions or biases that may be introduced 
through the use of labels corresponding to par-
ticular lexical markers of each category, such as 
probable or possible, and also to account for the 
fact that slightly different interpretations apply to 
the different levels, according to whether the 
event has a KT value of Analysis or General.  
L3: No expression of uncertainty or speculation 
(default category)  
L2: High confidence or slight speculation.  
L1: Low confidence or considerable speculation; 
typical lexical markers include may, might and 
perhaps.  
2.3 Source 
The source of experimental evidence provides 
important information for biologists. This is 
demonstrated by its annotation during the crea-
tion of the Gene Ontology (Ashburner et al, 
2000) and in the corpus created by Wilbur et al 
(2006). The Source dimension can also help in 
distinguishing new experimental knowledge 
from previously reported knowledge. Our 
scheme distinguishes two categories, namely: 
Other: The event is attributed to a previous 
study. In this case, explicit clues (citations or 
phrases like previous studies etc.) are normally 
present. 
Current: The event makes an assertion that can 
be (explicitly or implicitly) attributed to the cur-
rent study. This is the default category, and is 
assigned in the absence of explicit lexical or con-
textual clues. 
73
2.4 Polarity 
This dimension identifies negated events. Al-
though certain bio-event corpora are annotated 
with this information, it is still missing from oth-
ers. The indication of whether an event is ne-
gated is vital, as the interpretation of a negated 
event instance is completely opposite to the in-
terpretation of a non-negated (positive) instance 
of the same event.  
We define negation as the absence or non-
existence of an entity or a process. Negation is 
typically expressed by the adverbial not and the 
nominal no. However, other lexical devices like 
negative affixals (un- and in-, etc.), restrictive 
verbs (fail, lack, and unable, etc.), restrictive 
nouns (exception, etc.), certain adjectives (inde-
pendent, etc.), and certain adverbs (without, etc.) 
can also be used. 
2.5 Manner 
Events may be accompanied by a word or phrase 
which provides an indication of the rate, level, 
strength or intensity of the interaction. We refer 
to this as the Manner of the event. Information 
regarding manner is absent from the majority of 
existing bio-event corpora, but yet the presence 
of such words can be significant in the correct 
interpretation of the event. Our scheme distin-
guishes 3 categories of Manner, namely:  
High: Typically expressed by adverbs and adjec-
tives like strongly, rapidly and high, etc.  
Low: Typically expressed by adverbs and adjec-
tives like weakly, slightly and slow, etc.  
Neutral: Default category assigned to all events 
without an explicit indication of manner. 
3 Hyper-Dimensions 
Determining the pragmatic/rhetorical intent be-
hind an event is not completely possible using 
any one of our explicitly annotated dimensions. 
Although the Knowledge Type value forms the 
basis for this, it is not in itself sufficient. How-
ever, a defining feature of our annotation scheme 
is that additional information can be inferred by 
considering combinations of some of the explic-
itly annotated dimensions. We refer to this addi-
tional information as ?latent? or ?hyper? dimen-
sions of our scheme. We have identified two 
such hyper-dimensions. 
3.1 New Knowledge 
The isolation of events describing new knowl-
edge can be important in certain tasks undertaken 
by biologists, as explained earlier. Events with 
the Knowledge Type of Observation could corre-
spond to new knowledge, but only if they repre-
sent observations from the current study, rather 
than observations cited from elsewhere. In a 
similar way, an Analysis drawn from experimen-
tal results in the current study could be treated as 
new knowledge, but generally only if it repre-
sents a straightforward interpretation of results, 
rather than something more speculative.  
 Hence, we consider New Knowledge to be a 
hyper-dimension of our scheme. Its value (either 
Yes or No) is inferred by considering a combina-
tion of the value assignments for the KT, Source 
and CL dimensions.  
Table 1 shows the inference table that can be 
used to obtain the value for the New Knowledge 
hyper-dimension from the assigned values of the 
Source, KT and CL dimensions. The symbol ?X? 
indicates a ?don?t care condition?, meaning that 
this value does not have any impact on the result.  
 
Source 
(Annotated) 
KT 
(Annotated) 
CL 
(Annotated) 
New  
Knowledge 
(Inferred) 
Other X X No 
X X L2 No 
X X L1 No 
Current Observation L3 Yes 
Current Analysis L3 Yes 
X General X No 
X Investigation X No 
 
Table 1. Inference-Table for New Knowledge 
Hyper-Dimension 
 
3.2 Hypothesis 
A further hyper-dimension of our scheme is Hy-
pothesis. The binary value of this hyper-
dimension can be inferred by considering the 
values of KT and CL. Events with a KT value of 
Investigation can always be assumed to be a hy-
pothesis, However, if the KT value is Analysis, 
then only those events with a CL value of L1 or 
L2 (speculative inferences made on the basis of 
results) should be considered as hypothesis, to be 
matched with more definite experimental evi-
dence when available. A value of L3 in this in-
stance would normally be classed as new knowl-
edge, as explained in the previous section.   
Table 2 shows the inference table that can be 
used to get the value for the Hypothesis hyper-
dimension.  
 
74
KT 
(Annotated) 
CL 
(Annotated) 
Hypothesis 
(Inferred) 
General X No 
Observation X No 
Analysis L3 No 
Analysis L2 Yes 
Analysis L1 Yes 
Investigation X Yes 
 
Table 2. Inference-Table for Hypothesis 
Hyper-Dimension 
4 Evaluation 
The annotation scheme has been evaluated 
through a small annotation experiment. We ran-
domly choose 70 abstracts from the GENIA 
Pathway Corpus, which collectively contain over 
2600 annotated bio-events. Two of the authors 
independently annotated these bio-events using a 
set of annotation guidelines. These guidelines 
were developed following an analysis of the 
various bio-event corpora and the output of the 
initial case study (Nawaz et al, 2010). 
The highly favourable results of this experi-
ment further confirmed the feasibility and 
soundness of the annotation scheme. The re-
mainder of this section discusses the results in 
more detail. 
 
Dimension Cohen?s Kappa 
Knowledge Type 0.9017 
Certainty Level 0.9329 
Polarity 0.9059 
Manner 0.8944 
Source 0.9520 
Table 3. Inter-Annotator Agreement 
4.1 Inter-Annotator Agreement 
We have used the familiar measure of Cohen?s 
kappa (Cohen, 1960) for assessing the quality of 
annotation. Table 3 shows the kappa values for 
each annotated dimension. The highest value of 
kappa was achieved for the Source dimension, 
while the KT dimension yielded the lowest kappa 
value. Nevertheless, the kappa scores for all an-
notation dimensions were in the good region 
(Krippendorff, 1980).  
4.2 Category Distribution 
Knowledge Type:  The most prevalent category 
found in this dimension was Observation, with 
45% of all annotated events belonging to this 
category. Only a small fraction (4%) of these 
events was represented by an explicit lexical clue 
(mostly sensory verbs).  In most cases the tense, 
local context (position within the sentence) or 
global context (position within the document) 
were found to be important factors. 
The second most common category (37% of 
all annotated events) was General. We discov-
ered that most (64%) of the events belonging to 
this category were processes or states embedded 
in noun phrases (such as c-fos expression). More 
than a fifth of the General events (22%) ex-
pressed known scientific facts, whilst a smaller 
fraction (14%) expressed experimental/scientific 
methods (such as stimulation and incubation 
etc.). Explicit lexical clues were found only for 
facts, and even then in only 1% of cases. 
Analysis was the third most common category, 
comprising 16% of all annotated events. Of the 
events belonging to this category, 44% were de-
ductions (CL=L1), whilst the remaining 54% 
were hedged interpretations (CL=L2/L3). All 
Analysis events were marked with explicit lexical 
clues. 
The least common category was Investigation 
(1.5% of all annotated events). All Investigation 
events were marked with explicit lexical clues. 
Certainty Level: L3 was found to be the most 
prevalent category, corresponding to 93% of all 
events. The categories L2 and L1 occurred with 
frequencies of 4.3% and 2.5%, respectively. The 
relative scarcity of speculative sentences in sci-
entific literature is a well documented phenome-
non (Thompson et al, 2008; Vincze et al, 2008). 
Vincze et al (2008) found that less than 18% of 
sentences occurring in biomedical abstracts are 
speculative. Similarly, we found that around 20% 
of corpus events belong to speculative sentences. 
Since speculative sentences contain non-
speculative events as well, the frequency of 
speculative events is expected to be much less 
than the frequency of speculative sentences. In 
accordance with this hypothesis, we found that 
only 7% of corpus events were expressed with 
some degree of speculation. We also found that 
almost all speculated events had explicit lexical 
clues.  
Polarity:  Our event-centric view of negation 
showed just above 3% of the events to be ne-
gated. Similarly to speculation, the expected fre-
75
quency of negated events is lower than the fre-
quency of negated sentences. Another reason for 
finding fewer negated events is the fact that, in 
contrast to previous schemes, we draw a distinc-
tion between events that are negated and events 
expressed with Low manner. For example, cer-
tain words like limited and barely are often con-
sidered as negation clues. However, we consider 
them as clues for Low manner. In all cases, nega-
tion was expressed through explicit lexical clues. 
Manner: Whilst only a small fraction (4%) of 
events contains an indication of Manner, we 
found that where present, manner conveys vital 
information about the event. Our results also re-
vealed that indications of High manner are three 
times more frequent than the indications of Low 
manner. We also noted that both High and Low 
manners were always indicated through the use 
of explicit clues. 
Source: Most (99%) of the events were found to 
be of the Current category. This is to be ex-
pected, as authors tend to focus on current work 
in within abstracts. It is envisaged, however, that 
this dimension will be more useful for analyzing 
full papers. 
Hyper-dimensions: Using the inference tables 
shown in section 3, we calculated that almost 
57% of the events represent New Knowledge, and 
just above 8% represent Hypotheses.  
5 Conclusion and Future Work 
We have evaluated a slightly modified version of 
our meta-knowledge annotation scheme for bio-
events, first presented in Nawaz et al (2010). 
The scheme captures key information regarding 
the correct interpretation of bio-events, which is 
not currently annotated in existing bio-event cor-
pora, but which we have shown to be critical in a 
number of text mining tasks undertaken by bi-
ologists. The evaluation results have shown high 
inter-annotator agreement and a sufficient num-
ber of annotations along each category in every 
dimension. These results have served to confirm 
the feasibility and soundness of the annotation 
scheme, and provide promising prospects for its 
application to existing and new bio-event cor-
pora. 
We are currently working on a large scale an-
notation effort, involving multiple independent 
annotators. Although our main objective is to 
enrich the entire GENIA event corpus with meta-
knowledge information, we also plan to create a 
small corpus of full papers enriched with bio-
event and meta-knowledge annotations. 
Acknowledgments 
The work described in this paper has been 
funded by the Biotechnology and Biological Sci-
ences Research Council through grant numbers 
BBS/B/13640, BB/F006039/1 (ONDEX) 
References  
M. Ashburner, C. A. Ball, J. A. Blake, D. Botstein, H. 
Butler, J. M. Cherry, A. P. Davis, K. Dolinski, S. 
S. Dwight, J. T. Eppig, M. A. Harris, D. P. Hill, L. 
Issel-Tarver, A. Kasarskis, S. Lewis, J. C. Matese, 
J. E. Richardson, M. Ringwald, G. M. Rubin and 
G. Sherlock.  2000. Gene ontology: tool for the 
unification of biology.  Nature Genetics 25:25-29. 
J. Cohen. 1960. A coefficient of agreement for nomi-
nal scales. Educational and Psychological Meas-
urement 20: 37?46. 
A. de Waard, B. Shum, A. Carusi, J. Park, M. Sam-
wald and ?. S?ndor. 2009. Hypotheses, Evidence 
and Relationships: The HypER Approach for Rep-
resenting Scientific Knowledge Claims. In Pro-
ceedings of the Workshop on Semantic Web Appli-
cations in Scientific Discourse. Available at:  
http://oro.open.ac.uk/18563/ 
L. Hoye. 1997. Adverbs and Modality in English. 
London & New York: Longman 
K. Hyland. 1996. Talking to the Academy: Forms of 
Hedging in Science Research Articles. Written 
Communication 13(2):251-281. 
K. Hyland. 2005. Metadiscourse: Exploring Interac-
tion in Writing. London: Continuum 
J. Kim, T. Ohta and J. Tsujii. 2008. Corpus annotation 
for mining biomedical events from literature. BMC 
Bioinformatics 9:10 
K. Krippendorff. 1980. Content Analysis: An Intro-
duction to Its Methodology. Beverly Hills: Sage 
Publications 
H. Langer, H. Lungen and P. S. Bayerl. 2004. Text 
type structure and logical document structure. In 
Proceedings of the ACL Workshop on Discourse 
Annotation, pages 49-56 
M. Light, X. T. Qui and P. Srinivasan. 2004. The lan-
guage of bioscience: Facts, speculations, and 
statements in between. In Proceedings of the Bio-
Link 2004 Workshop on Linking Biological Litera-
ture, Ontologies and Databases: Tools for Users, 
pages 17-24. 
F. Lisacek, C. Chichester, A. Kaplan and A. Sandor. 
2005. Discovering Paradigm Shift Patterns in Bio-
medical Abstracts: Application to Neurodegenera-
tive Diseases. In Proceedings of SMBM 2005, 
pages 212-217 
76
L. McKnight and P. Srinivasan. 2003. Categorization 
of sentence types in medical abstracts. In Proceed-
ings of the 2003 Annual Symposium of AMIA, 
pages 440-444. 
B. Medlock and T. Briscoe. 2007. Weakly supervised 
learning for hedge classification in scientific litera-
ture. In Proceedings of ACL 2007, pages 992- 999. 
Y. Miyao, T. Ohta, K. Masuda, Y. Tsuruoka, K. Yo-
shida, T. Ninomiya and J. Tsujii. 2006. Semantic 
Retrieval for the Accurate Identification of Rela-
tional Concepts in Massive Textbases. In Proceed-
ings of COLING-ACL 2006, pages 1017-1024. 
Y. Mizuta and N. Collier. 2004. Zone identification in 
biology articles as a basis for information extrac-
tion. In Proceedings of the joint NLPBA/BioNLP 
Workshop on Natural Language for Biomedical 
Applications, pages 119-125. 
R. Morante and W. Daelemans. 2009. A metalearning 
approach to processing the scope of negation. In 
Proceedings of CoNLL 2009, pages 21-29. 
R. Nawaz, P. Thompson, J. McNaught and S. 
Ananiadou. 2010. Meta-Knowledge Annotation of 
Bio-Events. In Proceedings of LREC 2010, pages 
2498-2507. 
K. Oda, J. Kim, T. Ohta, D. Okanohara, T. Matsuzaki,  
Y. Tateisi and J. Tsujii. 2008. New challenges for 
text mining: mapping between text and manually 
curated pathways. BMC Bioinformatics 9(Suppl 3): 
S5. 
S. Pyysalo, F. Ginter, J. Heimonen, J. Bjorne, J. 
Boberg, J. Jarvinen and T. Salakoski. 2007. BioIn-
fer: a corpus for information extraction in the bio-
medical domain. BMC Bioinformatics 8:50. 
V. Rizomilioti. 2006. "Exploring Epistemic Modality 
in Academic Discourse Using Corpora." Informa-
tion Technology in Languages for Specific Pur-
poses 7, pages 53-71 
V. L. Rubin. 2007. Stating with certainty or stating 
with doubt: Intercoder reliability results for manual 
annotation of epistemically modalized statements. 
In Proceedings of NAACL-HLT 2007, Companion 
Volume,  pages 141-144. 
P. Ruch, C. Boyer, C. Chichester, I. Tbahriti, A. 
Geissb?hler, P. Fabry, J. Gobeill, V. Pillet, D. 
Rebholz-Schuhmann and C. Lovis. 2007. Using 
argumentation to extract key sentences from bio-
medical abstracts. International Journal of Medical 
Informatics 76(2-3):195-200. 
O. Sanchez-Graillet and M. Poesio. 2007. Negation of 
protein-protein interactions: analysis and extrac-
tion. Bioinformatics 23(13):i424-i432 
?. S?ndor. 2007. Modeling metadiscourse conveying 
the author?s rhetorical strategy in biomedical re-
search abstracts. Revue Fran?aise de Linguistique 
Appliqu?e 200(2):97-109. 
H. Shatkay, F. Pan, A. Rzhetsky and W. J. Wilbur.  
2008. Multi-dimensional classification of biomedi-
cal text: toward automated, practical provision of 
high-utility text to diverse users. Bioinformatics 
24(18): 2086-2093. 
S. Teufel, J. Carletta and M. Moens. 1999. An annota-
tion scheme for discourse-level argumentation in 
research articles. In Proceedings of EACL 1999, 
pages  110-117. 
S. Teufel, A. Siddharthan and C. Batchelor. 2009. 
Towards discipline-independent argumentative 
zoning: Evidence from chemistry and computa-
tional linguistics. In Proceedings of EMNLP-09, 
pages 1493-1502 
P. Thompson, S. Iqbal, J. McNaught and S. 
Ananiadou. 2009. Construction of an annotated 
corpus to support biomedical information extrac-
tion. BMC Bioinformatics 10: 349. 
P. Thompson, G. Venturi, J. McNaught, S. Monte-
magni and S. Ananiadou. 2008. Categorising Mo-
dality in Biomedical Texts. In Proceedings of the 
LREC 2008 Workshop on Building and Evaluating 
Resources for Biomedical Text Mining, pages 27-
34. 
V. Vincze, G. Szarvas, R. Farkas, G. Mora and J. 
Csirik. 2008. The BioScope corpus: biomedical 
texts annotated for uncertainty, negation and their 
scopes. BMC Bioinformatics 9(Suppl 11): S9. 
W. J. Wilbur, A. Rzhetsky and H. Shatkay. 2006. 
New directions in biomedical text annotations: 
definitions, guidelines and corpus construction. 
BMC Bioinformatics 7: 356. 
 
77
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 37?46,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
A Three-Way Perspective on Scientific Discourse Annotation for       Knowledge Extraction   Maria Liakata Aberystwyth University, UK / EMBL-EBI, UK liakata@ebi.ac.uk Paul Thompson University of Manchester, UK paul.thompson@manchester.ac.uk Anita de Waard Elsevier Labs, USA / UiL-OTS, Universiteit Utrecht, NL a.dewaard@elsevier.com    Raheel Nawaz University of Manchester, UK raheel.nawaz@cs.man.ac.uk Henk Pander Maat UiL-OTS, Universiteit Utrecht, NL h.l.w.pandermaat@uu.nl Sophia Ananiadou University of Manchester, UK sophia.ananiadou@manchester.ac.uk   Abstract 
This paper presents a three-way perspective on the annotation of discourse in scientific literature. We use three different schemes, each of which focusses on different aspects of discourse in scientific articles, to annotate a corpus of three full-text papers, and compare the results. One scheme seeks to identify the core components of scientific investigations at the sentence level, a second annotates meta-knowledge pertaining to bio-events and a third considers how epistemic knowledge is conveyed at the clause level. We present our analysis of the comparison, and a discussion of the contributions of each scheme.  1 Introduction The literature boom in the life sciences over the past few years has sparked increasing interest into text mining tools, which facilitate the automatic extraction of useful knowledge from text (Ananiadou et al, 2006; Ananiadou  &  McNaught, 2006; Zweigenbaum et al, 2007; Cohen  &  Hunter, 2008). Most of these tools have focussed on entity recognition and relation extraction and with few exceptions, e.g., (Hyland, 1996; Light et al, 2004; S?ndor, 2007; Vincze et al, 2008), do not take into account the discourse context of the knowledge extracted. However, failure to take this context into account results in the loss of information vital for the correct interpretation of extracted knowledge, e.g. the scope of the relations, or the level of certainty with which they are expressed. A particular piece of 
knowledge may represent, e.g., an accepted fact, hypothesis, results of an experiment, analysis based on experimental results, factual or speculative statements etc. Furthermore, this knowledge may represent the author's current work, or work reported elsewhere. The ability to recognise different discourse elements automatically provides crucial information for the correct interpretation of extracted knowledge, allowing scientific claims to be linked to experimental evidence, or newly reported experimental knowledge to be isolated. The importance of categorising such knowledge becomes more pronounced as analysis moves from abstracts to full papers, where the content is richer and linguistic constructions are more complex (Cohen et al, 2010). Analysis of full papers is extremely important, since less than 8% of scientific claims occur in abstracts (Blake, 2010). Various different schemes for annotating discourse elements in scientific texts have been proposed. The schemes vary along several axes, including perspective, motivation, complexity and the granularity of the units of text to which the scheme is applied. Faced with such variety, it is important to be able to select the best scheme(s) for the purpose at hand. Answers to questions such as the following can help in the selection process: 1. What are the relative merits of the different schemes? 2. What are the similarities and differences between schemes? 3. Can annotation according to multiple schemes provide enhanced information?  
37
Category Description Hypothesis An unconfirmed statement which is a stepping stone of the investigation Motivation The reasons behind an investigation Background Generally accepted background knowledge and previous work Goal A target state of the investigation where intended discoveries are made Object-New An entity which is a product or main theme of the investigation Object-New-Advantage Advantage of an object Object-New-Disadvantage Disadvantage of an object Method-New Means by which authors seek to achieve a goal of the investigation Method-New-Advantage Advantage of a Method Method-New-Disadvantage Disadvantage of a Method Method-Old A method mentioned pertaining to previous work Method-Old-Advantage Advantage of a Method Method-Old-Disadvantage Disadvantage of a Method Experiment An experimental method Model A statement about a theoretical model or framework Observation The data/phenomena recorded in an investigation Result Factual statements about the outputs, interpretation of observations Conclusion Statements inferred from observations & results  Table 1. The CoreSC Annotation scheme: layers 1 & 2 	 ?4. Is there any advantage in merging annotation schemes or is it better to allow complementary and different dimensions of scientific discourse annotation?	 ?As a starting point to addressing such questions, we provide a comparison of three different schemes for the annotation of discourse elements within scientific papers. Each scheme has a different perspective and motivation:, one is content-driven, seeking to identify the main components of a scientific investigation, another is driven by the need to describe events of biomedical relevance and the third focusses on how epistemic knowledge is conveyed in discourse.  These different viewpoints mean that the schemes vary in both the type and complexity of the discourse elements identified, as well as the types of units to which the annotation is applied, i.e. complete sentences, segments of sentences, or specific relations/events occurring within these sentences. To facilitate the comparison, we have annotated three full papers according to each of the schemes. The analysis resulting from this three-way annotation considers mappings between schemes, their relative merits, and how the information annotated by the different schemes can 
complement each other to provide enriched details about knowledge extracted from the texts. In the following sections, we firstly provide a description of the three schemes, and then explain how they have been used in our corpus annotation. Finally we discuss the results from the comparison, and the features of each scheme. 2 Sentence annotation: CoreSC scheme  The reasoning behind this scheme is that a paper is the human-readable representation of a scientific investigation. Therefore, the goal of the annotation is to retrieve the content model of scientific investigations as reflected within scientific discourse. The hypothesis is that there is a set of core scientific concepts (CoreSC), which constitute the key components of a scientific investigation. CoreSCs consist of 11 concepts originating from the CISP (Core Information about Scientific Papers) meta-data (Soldatova  &  Liakata, 2007), which are a subset of classes from the EXPO ontology for the description of scientific experiments (Soldatova  &  King, 2006). The CoreSCs are: Motivation, Goal, Object, Background, Hypothesis, Method, Model, Experiment, Observation, Result and Conclusion. 
38
Figure 1.  Bio-Event Representation 
The CoreSC scheme (Liakata et al, 2010; Liakata et al, 2012) implements the above-mentioned concepts as a 3-layered sentence-based annotation scheme. This means that each sentence in a document is assigned one of the 11 CoreSC concepts. The scheme also considers a layer designated to properties of the concepts (e.g. New Method vs Old Method) as well as identifiers which link instances of the same concept across sentences. A short definition of CoreSC categories and their properties can be found in Table 1.  The CoreSC scheme is accompanied by 47-page annotation guidelines, and has been used by 16 domain experts to annotate a corpus of 265 full papers from physical chemistry & biochemistry (Liakata  &  Soldatova, 2009; Liakata et al, 2010). This corpus consists of 40,000 sentences, containing over 1 million words and was developed in three phases (for details see Liakata et al (2012)). Inter-annotator agreement between experts was measured in terms of Cohen?s kappa (Cohen, 1960) on 41 papers and ranged between 0.5 and 0.7. Machine learning classifiers have been trained on the CoreSC corpus, achieving > 51% accuracy across the eleven categories. The most accurately predicted category is Experiment, the category describing experimental methods (Liakata et al, 2012). Classifiers trained on 1000 Biology abstracts annotated with CoreSC have obtained an accuracy of over 80% (Guo et al, 2010). Models trained on the CoreSC corpus papers have been used to create automatic summaries of the papers, which have been evaluated in a question answering task (Liakata et al, 2012). Lastly, the CoreSC scheme was used to annotate 50 papers from Pubmed Central pertaining to Cancer Risk Assessment. A web tool (SAPIENTA 1 ) allows users to annotate their full papers with Core Scientific concepts, and can be combined with manual annotation. A UIMA framework 2 implementation of this code for large-scale annotation of CoreSC concepts is in progress. 3 Event annotation: Meta-knowledge for bio-events The motivation for this annotation scheme is to allow the training of more sophisticated event-                                                1 http://www.sapientaproject.com/software 2 http://uima.apache.org/ 
based information extraction systems. In contrast to the sentence-based scheme described in section 2, this scheme is applied at the level of events (Ananiadou et al, 2010), of which there may be several within a single sentence. 3.1 Bio-Events Events are template-like, structured representations of pieces of knowledge contained within sentences. Normally, events are ?anchored? to a trigger (typically a verb or noun) around which the knowledge expressed is organised. Each event has one of more participants, which describe different aspects of the event. Participants can correspond to entities or other events, and are often labelled with semantic roles, e.g., CAUSE, THEME, LOCATION, etc. The work described here focusses specifically on bio-events, which are complex structured relations representing fine-grained relations between bio-entities and their modifiers. Figure 1 provides some examples of bio-events. Event extraction systems (Bj?rne et al, 2009; Miwa et al, 2010; Miwa et al, 2012; Quirk et al, 2011) are typically trained on text corpora, in which events and their participants have been manually annotated by domain experts. Research into bio-event extraction has been boosted by the two recent shared tasks at BioNLP 2009/2011 (Kim et al, 2011; Pyysalo et al, In Press). Several gold standard event annotated corpora exist; examples include the GENIA Event Corpus (Kim et al, 2008), GREC (Thompson et al, 2009) and BioInfer (Pyysalo et al, 2007), in addition to the corpora produced for the shared tasks. 
3.2 Meta-knowledge Annotation Until recently, the only attempts to recognise information relating to the correct interpretation of events were restricted to sparse details regarding negation and speculation (Kim et al, 2011). 
39
In order to address this problem, a multi-dimensional annotation scheme especially tailored to bio-events was developed (Nawaz et al, 2010; Thompson et al, 2011). The scheme identifies and categorises several different types of contextual details regarding events (termed meta-knowledge), including discourse information. Different types of meta-knowledge are encoded through five distinct dimensions (Figure 2). The advantage of using multiple dimensions is that the interplay between the assigned values in each dimension can reveal both subtle and substantial differences in the types of meta-knowledge expressed. In the majority of cases, meta-knowledge is expressed through the presence of particular ?clue? words or phrases, although other features can also come into play, such as the tense of the event trigger, or the relative position within the text. 
Figure 2: Meta-knowledge annotation 	 ?The annotation task consists of assigning an appropriate value from a fixed set for each dimension, as well as marking the textual evidence for this assignment. The five meta-knowledge dimensions and their values are as follows: Knowledge Type (KT): Captures the general information content of the event. Each event is classified as one of: Investigation (enquiries and examinations, etc.), Observation (direct experimental observations), Analysis (inferences, interpretations and conjectures, etc.), Fact (known facts), Method (methods) or Other (general events that provide incomplete information or do not fit into any other category).  Certainty Level (CL): Encodes the confidence or certainty level ascribed to the event in the given text. The epistemic scale is partitioned into three distinct levels: L3 (no expression of uncertainty), 
L2 (high confidence or slight speculation) and L1 (low confidence or considerable speculation). Polarity: Identifies negated events. Negation is defined as the absence or non-existence of an entity or a process. Manner: Captures information about the rate, level, strength or intensity of the event, using three values: High, Low, or Neutral (no indication of rate/intensity). Source:  Encodes the source of the knowledge being expressed by the event as Current (the current study) or Other (any other source).      Of these five dimensions, only KT, CL and Source were considered during the comparison with the other two schemes, since they are directly related to discourse analysis.  The GENIA event corpus, consisting of 1000 abstracts with 36,115 events (Kim et al, 2008) has been annotated with meta-knowledge by 2 annotators, supported by 64-page annotation guidelines 3  (Thompson et al, 2011). Inter-annotator agreement rates ranged between 0.84?0.93 (Cohen?s Kappa).  Research has been carried out into the automatic assignment of Manner values to events (Nawaz et al, In Press).  In addition, the EventMine-MK service (Miwa et al, In Press), based on EventMine (Miwa et al, 2010) facilitates automatic extraction of biomedical events with meta-knowledge assigned. The performance of EventMine-MK in assigning different meta-knowledge values to events ranges between 57% and 87% (macro-averaged F-Score) on the BioNLP?09 Shared Task corpus (Kim et al 2011). EventMine-MK is available as a component of the U-Compare interoperable text mining system4 (Kano et al, 2011). 4 Clause annotation: Segments for epistemic knowledge The third scheme we consider uses a Discourse Segment Type classification of segments at, roughly, a clause level, i.e., each segment has a main verb. This means that the level of granularity of argumentational elements in this scheme lies between the other two schemes, i.e. it is usually more granular than CoreSC, but sometimes less granular than the event-based scheme.                                                  3 http://www.nactem.ac.uk/meta-knowledge/ 4 http://www.nactem.ac.uk/ucompare/ 
40
 Table 2:  Discourse Segment Types 	 ?The segment annotation scheme identifies a taxonomy of discourse segment types that seem to be exclusive and useful (de Waard & Pander Maat, 2009). Three classes of segment types are defined:  ? Basic segment types: segments referring directly to the topic of study ? see Table 2.  ? ?Other?-segment types: segments referring to conceptual or experimental work in other research papers than the current one ? Regulatory segment types: ?regulatory? clauses that control and introduce other segments.  A list of segment types is presented in Table 2; further details, including a list of all segment types and correlations with verb tense can be found in de Waard  &  Pander Maat (2009). The focus of this work is to identify linguistic features that characterise these discourse segment types, according to three aspects: ? Verb tense, aspect, mood and voice ? Semantic verb class ? Epistemic modality markers So far, 6 full-text papers (comprising about 2300 segments) have been manually annotated with segment types and correlated with the above features. A first automated validation was promising (de Waard, Buitelaar and Eigener, 2009). The need for parsing at a clause level is especially prominent in biological text, since specific semantic roles are played by particular clause types. We give four examples of typical 
clause constructions that play a specific rhetorical role: firstly, reporting clauses are often sentence-initial ?that? matrix clauses (1a): 1. a.  This suggests that  1.b. miR-372 and miR-373 caused the observed  selective growth advantage. Secondly, descriptions confirming certain accepted characteristics of biological entities are often given as nonrestrictive relative clauses (2b):  2.a. We also generated BJ/ET cells expressing the  RASV12-ERTAM chimera gene,  2. b. which is only active when tamoxifen is added  Thirdly, a subordinate gerund clause is often used to describe a method (3a), with a main (finite) clause describing a result (3b) and fourthly, experimental goals are often given as a (mostly sentence-initial) clause with a to-infinitive (4a) often preceding a past-tense methods clause (4b). 3. a. Using fluorescence microscopy and luciferase assays, b. we observed potent and specific miRNA activity expressed from each miR-Vec (Figure S2). 4. a. To identify miRNAs that can interfere with this process  4. b. we transduced BJ/ET fibroblasts with miR-Lib  However, the lack of simple robust clause parsers has prevented the automated identification of semantic roles at the clause level. Therefore, this scheme has so far only been manually 
Segment Description Examples  Fact knowledge accepted to be true, a known fact. mature miR-373 is a homolog of miR-372,  Hypothesis  a proposed idea, not supported by evidence This could for instance be a result of high mdm2 levels  Problem unresolved, contradictory, or unclear issue However, further investigation is required to demonstrate the exact mechanism of LATS2 action Goal research goal To identify novel functions of miRNAs, Method  experimental method Using fluorescence microscopy and luciferase assays,  Result a restatement of the outcome of an experiment all constructs yielded high expression levels of mature miRNAs   Implication  an interpretation of the results, in light of data our procedure is sensitive enough to detect mild growth differences   Other-Hypothesis an idea proposed by others [It is generally believed that] transcription factors are the final common pathway driving differentiation] Regulatory-Hypothesis a matrix clause introducing a hypothesis It is generally believed that [transcription factors are the final common pathway driving differentiation] 
41
implemented. Despite being less widely implemented than the other two schemes, we believe that the segment scheme offers some useful pointers for linguistic features that can identify particular rhetorical classes in the text, and secondly, offers an interesting perspective on the fact that in biological text, several rhetorical moves are made within a single sentence.  5 Data and methods Three papers already annotated according to the GENIA event annotation scheme (Kim et al, 2008), were further annotated according to the three annotation schemes described above. We obtained all corresponding CoreSCs, events and segments per sentence. Each sentence has a single CoreSC annotation and one or more segment annotations (depending on the number of clauses). Event annotations in a sentence may range from zero to multiple, according to whether any relevant biomedical events are described in the sentence.  Events within a sentence are mapped to segments by identifying which segment contains the trigger for a particular event. The three meta-knowledge dimensions for events considered in this comparison, i.e., KT, CL and Source, result in 16 different combinations of values encountered in the three papers. The numbers for CoreSC and Segment labels encountered were 12 and 22, respectively. Confusion matrices were obtained for each paper and for each pair of annotation schemes. Note that, as bio-events are largely unconcerned with describing methodology, the Methods sections of these papers do not contain event annotation or meta-knowledge annotation. The pairwise confusion matrices from each paper were combined, resulting in three matrices (Tables 3, 4 and 5), which describe the associations between the annotation schemes in the three papers examined. We have highlighted the highest frequencies per row and where appropriate also the highest values per column. The use of two different colours aims to facilitate readability. 6 Results and Discussion We present the results from analysing the pairwise confusion matrices for the three schemes and discuss the merits of each scheme. 
6.1 Event Meta-knowledge v. CoreSC In Tables 3 (and 5), the meta-knowledge categories combine KT, CL and Source ((O)ther) values. Table 3 shows some straightforward and expected mappings, e.g.,Method (Met,L3) events are almost always found within CoreSC Experiment or Method sentences, whilst Investigation events (Inv,L3) occur most frequently within CoreSC Goal or Motivation sentences.  For other categories, information from the two schemes can complement each other in different ways. For example, KT and Source information about events can help to distinguish different types of information within CoreSC Background sentences (top left corner of Table 3). Such information mainly corresponds to facts, observations from previous studies, or analyses of information. Conversely, information from the CoreSC scheme can help to further classify the interpretation of events. For example, events with an analytical interpretation (Ana,L1,L2,L3) may occur as background information to a study (Bac), as hypotheses (Hyp),  as part of observations (Obs), when reporting the results of the current study (Res) or when making concluding remarks about the study (Con). CoreSCs can also help to further refine events relating to outcomes (Obs,L3) according to whether they pertain to (Obs)ervations, (Res)ults or (Con)clusions. CoreSC Conclusion, Result and Observation sentences contain mainly Observation events concerned with the current study. However, such sentences often also include an analytical part, with varying levels of certainty, which event information can help to isolate. The CL annotated for events is also useful in helping to determine the confidence with which information is stated in CoreSC Conclusion and Hypothesis sentences.  Due to the nature of bio-event annotation, only a small number of events correspond to methods. Thus, CoreSC provides a more detailed characterisation of method-related sentences, i.e., Experiment, Method_New, Model and Object. 6.2 Discourse Segments v. CoreSC In most cases, there seems to be natural mapping between the two schemes (See Table 4). CoreSC Observation maps to Result, CoreSC Method and Experiment map to Method, CoreSC Hypothesis maps to Hypothesis, CoreSC Goal maps to Goal, 
42
CoreSC Conclusion maps to Implication and Hypothesis, CoreSC Result maps to Implication and Result, and Problem is equivalent to CoreSC Motivation. The bulk of CoreSC Background maps to Fact and Other-Implication, but the ?Other? Segment categories provide a substantial refinement of the CoreSC Background category.   
 Table 3. Event Meta-knowledge vs CoreSC    On the other hand, CoreSC refines Method, Result and Implication segments. CoreSC Result may include both Fact and Method clauses, which can be captured by the Segment scheme, since annotation is performed at the clause level. CoreSC Conclusion maps to both Implication and Hypothesis segments, suggesting that there may be differences in the certainty levels of these conclusions. This is supported by preliminary classification experiments (paper in progress).  	 ?6.3 Discourse Segments v. Event Meta-Knowledge	 ? Some straightforward mappings exist between segment and event meta-knowledge categories (Table 5). For example, Investigation events (Inv, L3) are generally found within Goal and Problem segments; Method events (Met,L3) are normally found within Method segments, Observation events (Obs,L3) are found mainly within Result, Fact and Implication segments and (Ana,L1,L2) events correspond mainly to Hypotheses and Implications. Whilst these are similar findings to the comparison between event meta-knowledge and CoreSCs, the variance of the distribution is often smaller when mapping from Events to Segments. This is to be expected ? the information encoded by many events has the scope of roughly a clause, which corresponds closely to the scope of 
discourse segments. This could permit cleaner one-to-one mappings between categories. 
 Table 4: Segments vs CoreSC 	 ?  Hypothesis and Implication segments mainly contain (Ana)lysis events. The differing certainty levels of events can help to refine information about the statements made within these segments. Likewise, these segment types could help to refine the nature of the analysis described by the event.   Similarly to the CoreSC scheme, the results suggest that Result segments could be refined by the meta-knowledge scheme to distinguish between results emerging from direct experimental observations, and those obtained through analysis of experimental observations. Another interesting result is that Fact segments can contain Fact, (Ana)lysis or (Obs)ervation events. This may suggest that Fact segments are actually a rather general category, containing a range of different information. Few events occur within the Regulatory segments, as these mainly introduce content-bearing segments.  The majority of Method segments and a significant number of the Result segments do not correspond to events, as none of the methods sections have been annotated with event information, for reasons explained previously.  
	 ?Table 5: Segments vs Event Meta-Knowledge 
Sheet1
Page 1
Bac Con Exp Goa Hyp Met_New Met_Old Mod Mot Obj_New Obs Res0 42 24 49 7 7 25 1 13 6 7 47 54Obs,L3,O 166 0 0 0 0 0 3 0 12 0 0 2Ana,L3,O 33 1 0 0 0 0 0 1 0 0 0 0Ana,L2,O 3 0 0 0 0 0 0 0 0 0 0 0Fact,L3,O 7 0 0 0 0 0 0 0 0 0 0 0Fact,L3 24 1 0 1 0 0 0 0 5 3 0 2Oth,L3 125 30 0 8 16 5 3 2 8 3 9 42Ana,L1 2 10 0 0 6 0 0 0 1 0 0 6Ana,L2 30 15 0 1 14 0 0 2 1 0 8 33Ana,L3 11 11 0 0 2 1 2 0 3 0 14 28Met,L3 4 1 15 1 0 5 0 0 0 0 2 6Inv,L2 0 0 0 0 0 0 0 0 0 0 1 1Inv,L3 5 3 1 6 2 4 3 0 8 0 1 1Inv,L3,O 0 0 0 0 2 0 1 0 0 0 0 0Obs,L1 1 0 0 0 0 0 0 0 0 0 0 0Obs,L2 1 0 0 0 0 0 0 0 0 0 1 0Obs,L3 31 34 3 1 10 3 0 2 7 1 59 87
Sheet1
Page 1
Bac Con Exp Goa Hyp Met_New Met_Old Mod Mot Obj_New Obs ResFact 118 3 0 3 7 0 0 1 15 7 5 34OtherFact 70 4 0 0 0 0 0 3 1 0 0 0OtherGoal 2 1 0 0 0 0 0 0 0 0 0 0OtherHypothesis 14 0 0 0 0 0 0 0 0 0 0 0OtherImplication 124 1 0 0 3 0 0 1 5 0 0 1OtherMethod 5 0 0 0 0 0 3 0 0 0 0 2OtherProblem 1 0 0 0 0 0 0 0 0 0 0 0OtherResult 64 1 0 0 0 0 6 0 0 3 0 9RegFact 1 3 0 0 0 0 0 0 0 0 0 2Implication 13 58 0 0 2 0 0 3 1 0 3 80RegImplication 5 6 0 1 0 0 0 0 0 0 1 10Method 6 2 54 2 2 32 0 6 1 0 8 13Goal 2 0 5 12 6 9 2 2 4 0 0 5RegGoal 0 1 0 0 0 0 0 0 0 0 0 0Hypothesis 24 31 0 5 34 1 0 5 0 0 0 12RegHypothesis 6 4 0 0 2 0 0 1 0 0 0 2Problem 7 6 0 0 0 0 2 0 11 0 0 2RegProblem 0 3 0 0 0 0 0 0 0 0 0 0Result 13 6 1 1 2 0 0 2 8 0 112 75RegResult 1 0 0 0 0 0 0 0 0 1 1 2Intertextual 4 0 7 0 1 0 0 0 0 0 0 3Intratextual 2 0 1 0 0 0 0 2 0 0 8 4
Sheet1
Page 1
0 Ana Ana Ana Ana Ana Fact Fact Met Oth Inv Inv Inv ObsObsObsObsL1 L2 L2,OL3 L3,OL3 L3,O L3 L3 L2 L3 L3,OL1 L2 L3 L3,OHypothesis 8 18 26 1 0 0 0 0 1 39 0 4 1 0 0 14 0Implication 22 2 30 0 34 2 2 0 0 38 2 1 0 0 0 27 0OtherHypothesis 0 0 3 1 0 0 0 0 0 9 0 1 0 0 0 0 0OtherImplication 8 1 6 1 4 28 0 3 3 27 0 2 0 1 0 5 46RegImplication 11 0 2 0 0 1 0 0 0 5 0 1 0 0 0 3 0RegHypothesis 1 0 0 0 0 0 0 0 0 6 0 1 0 0 0 7 0Fact 15 0 18 0 6 0 28 0 0 55 0 1 0 0 1 44 25RegFact 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 5 0OtherGoal 1 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0OtherProblem 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0Method 80 0 1 0 0 0 0 0 23 9 0 2 0 0 0 8 3OtherMethod 7 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0 0Goal 13 0 0 0 0 0 1 0 0 18 0 11 1 0 0 3 0RegGoal 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0Problem 9 4 0 0 2 0 0 0 0 5 0 8 0 0 0 0 0RegProblem 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0Result 51 0 14 0 20 0 0 0 6 18 0 0 0 0 1 103 7OtherResult 11 0 1 0 0 1 0 1 0 10 0 0 0 0 0 12 47OtherFact 4 0 1 0 0 2 5 3 0 7 0 0 0 0 0 2 54RegResult 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0Intertextual 13 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0Intratextual 17 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0280 4 35 0 28 3 34 4 29 127 0 24 2 0 2 178 136
43
7 Related Work A number of schemes for annotating scientific discourse elements at the sentence level have been proposed. Certain schemes have been aimed at abstracts, e.g., (McKnight  &  Srinivasan, 2003; Ruch et al, 2007; Hirohata et al, 2008; Bj?rne et al, 2009). The work of Hirohata et al (2009) has been integrated with the MEDIE service5 (Miyao et al, 2006), allowing the user to query facts using conclusions, results, etc. For full papers, the most notable work has focussed on argumentative zoning (AZ) (Teufel et al, 1999; Teufel  &  Moens, 2002; Teufel et al, 2009; Teufel, 2010). An important aspect of AZ involves capturing the attribution of knowledge claims and citation function, and the scheme has been tested on information extraction and summarisation tasks with Computational Linguistics papers. AZ was modified for the annotation of biology papers by Mizuta et al (2005) in order to facilitate information extraction, and more recently Teufel et al (2009) extended the AZ scheme to better accommodate the life sciences and chemistry in particular, producing AZ-II. Scientific discourse annotation has also targeted the retrieval of speculative text to help improve curation. For a recent overview see de Waard and Pander Maat (2012).  Modality and negation in text have also been the focus of recent workshops (Farkas et al(2010), Morante & Sporleder (2012)). Finally, Shatkay et al(2008) define a multi-dimensional scheme, which combines several of the above-mentioned aspects.      Recent work has compared schemes to discover mappings and relative merits. Liakata et al (2010) compared AZ-II and CoreSC on 36 papers annotated with both schemes and found that CoreSC provides finer granularity in distinguishing content categories (e.g. methods, goals and outcomes) while the strength of AZ-II lies in detecting the attribution of knowledge claims and identifying the different functions of background information. Guo et al (2010) compared three schemes for the identification of discourse structure in scientific abstracts from cancer research assessment articles. The work showed a subsumption relation between the scheme of Hirohata et al (2008), a cut-down version of the                                                 5 http://www.nactem.ac.uk/medie/ 
scheme proposed by Teufel et al (2009) and CoreSC (1st layer), from general to specific.	 ?8  Conclusion We have compared three different schemes, each taking a different perspective to the annotation of scientific discourse. The comparison shows that the three schemes are complementary, with different strengths and points of focus. CoreSC offers a fine-grained characterisation of methods, outcomes and objectives. It has been used to annotate a collection of 265 full papers, and subsequently CoreSC recognition has been fully automated, creating the online SAPIENTA tool. The discourse segment annotation scheme can help to provide a finer-grained characterisation of background work, and could also help to split multi-clause CoreSC sentences into appropriate segments. Recognition of event meta-knowledge has been fully automated in the U-Compare framework, and the KT values of the scheme can help to provide a finer-grained analysis of certain segment and sentence types. The CL dimension also allows confidence values to be ascribed to the Conclusion, Result, Implication and Hypothesis categories of the other two schemes.   Future work will focus on annotating texts with several discourse perspectives to investigate the advantages of the schemes. Ideally we would like to propose a unified approach for scientific discourse annotation, but recognize that choices such as the unit of annotation are often task-oriented, and that users should be able to mix and match discourse segments as required. This said, the analysis in this paper paves the way for potential harmonisation, revealing points of union and intersection between the schemes. Acknowledgements This work has been supported through funding for Maria  Liakata by JISC, the Leverhulme Trust and EBI-EMBL. It has also been supported by the BBSRC through grant number BB/G013160/1UK (Automated Biological Event Extraction from the Literature for Drug Discovery), the MetaNet4U project (ICT PSP Programme, Grant Agreement: No. 270893) and the JISC-funded ISHER project.  
44
References  Ananiadou, S., Kell, D.B. and Tsujii, J. (2006). Text mining and its potential applications in systems biology. Trends Biotechnol, 24(12): 571-9. Ananiadou, S. and McNaught, J., Eds. (2006). Text Mining for Biology and Biomedicine. Boston / London, Artech House. Ananiadou, S., Pyysalo, S., Tsujii, J. and Kell, D.B. (2010). Event extraction for systems biology by text mining the literature. Trends Biotechnol, 28(7): 381-90. Bj?rne, J., Heimonen, J., Ginter, F., Airola, A., Pahikkala, T. and Salakoski, T. (2009). Extracting Complex Biological Events with Rich Graph-Based Feature Sets. In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task, pp.  10-18. Blake, C. (2010). Beyond genes, proteins, and abstracts: Identifying scientific claims from full-text biomedical articles. Journal of Biomedical Informatics, 43(2): 173-189. Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and psychological measurement, 20: 37-46. Cohen, K.B. and Hunter, L. (2008). Getting started in text mining. PLoS Comput Biol, 4(1): e20. Cohen, K.B., Johnson, H.L., Verspoor, K., Roeder, C. and Hunter, L.E. (2010). The structural and content aspects of abstracts versus bodies of full text journal articles are different. BMC Bioinformatics, 11: 492. de Waard, A., Buitelaar, P., Eigner, T. (2009). Identifying the epistemic value of discourse segments in biology texts. Proceedings of the Eighth International Conference on Computational Semantics, pp. 351-354 de Waard, A. and Pander Maat, H. (2009). Categorizing Epistemic Segment Types in Biology Research Articles. In Proceedings of the Workshop on Linguistic and Psycholinguistic Approaches to Text Structuring (LPTS 2009) de Waard, A. and Pander Maat, H. (2012). Knowledge Attribution in Scientific Discourse: A Taxonomy of Types and Overview of Features, In Proceedings of the Workshop on Detecting Structure in Scholarly Discourse (DSDD), ACL 2012. Farkas, R.	 ?Vincze, V., M?ra, G., Csirik, J. and Szarvas, G. 2010. The CoNLL-2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, Uppsala, Sweden. 
Association for Computational Linguistics, pp. 1- 12. Guo, Y., Korhonen, A., Liakata, M., Silins, I., LiSun, L. and Stenius, U. (2010). Identifying the information structure of scientific abstracts: An investigation of three different schemes. In Proceedings of BioNLP 2010, pp.  99-107. Hirohata, K., Okazaki, N., Ananiadou, S. and Ishizuka, M. (2008). Identifying Sections in Scientific Abstracts using Conditional Random Fields. In Proceedings of the 3rd International Joint Conference on Natural Language Processing, pp.  381-388. Hyland, K. (1996). Writing without conviction? Hedging in science research articles. Applied Linguistics, 17(4): 433-454. Kano, Y., Miwa, M., Cohen, K.B., Hunter, L.E., Ananiadou, S. and Tsujii, J. (2011). U-Compare: A modular NLP workflow construction and evaluation system. IBM Journal of Research and Development, 55(3): 11:1-11:10. Kilicoglu, H. and Bergler, S. (2008). Recognizing speculative language in biomedical research articles: a linguistically motivated perspective. BMC Bioinformatics, 9(Suppl 11): S10. Kim, J.-D., Ohta, T. and Tsujii, J. (2008). Corpus annotation for mining biomedical events from literature. BMC Bioinformatics, 9(10). Kim, J.D., Ohta, T., Pyysalo, S., Kano, Y. and Tsujii, J. (2011). Extracting Bio-Molecular Events from Literature - The BioNLP'09 Shared Task. Computational Intelligence, 27(4): 513-540. Liakata, M., Saha, S., Dobnik, S., Batchelor, C. and Rebholz-Schuhmann, D. (2012). Automatic recognition of conceptualisation zones in scientific articles and two life science applications. Bioinformatics, 28 (7). Liakata, M. and Soldatova, L.N. (2009). The ART corpus. Technical Report. Aberystwth University. Liakata, M., Teufel, S., Siddharthan, A. and Batchelor, C. (2010). Corpora for the conceptualisation and zoning of scientific papers. In Proceedings of LREC, pp.  2054-2061. Light, M., Qiu, X.Y. and Srinivasan, P. (2004). The language of bioscience: Facts, speculations, and statements in between. In Proceedings of the BioLink 2004 Workshop at HLT/NAACL, pp.  17?24. McKnight, L. and Srinivasan, P. (2003). Categorization of sentence types in medical abstracts. In AMIA Annu Symp Proc, pp.  440-4. Miwa, M., Saetre, R., Kim, J.D. and Tsujii, J. (2010). Event extraction with complex event 
45
classification using rich features. J Bioinform Comput Biol, 8(1): 131-46. Miwa, M., Thompson, P. and Ananiadou, S. (2012). Boosting automatic event extraction from the literature using domain adaptation and coreference resolution. Bioinformatics.  Miwa, M., Thompson, P., McNaught, J, Kell, D.B and Ananiadou, S. (In Press). Extracting semantically enriched events from biomedical literature. BMC Bioinformatics.  Miyao, Y., Ohta, T., Masuda, K., Tsuruoka, Y., Yoshida, K., Ninomiya, T. and Tsujii, J. (2006). Semantic Retrieval for the Accurate Identification of Relational Concepts in Massive Textbases. In Proceedings of ACL, pp.  1017-1024. Mizuta, Y., Korhonen, A., Mullen, T. and Collier, N. (2005). Zone Analysis in Biology Articles as a Basis for Information Extraction. International Journal of Medical Informatics,75(6): 468-487. Morante R., and Sporleder C, (2012). Modality and negation: An introduction to the special issue. Computational Linguistics, 38(2): 1?38. Nawaz, R., Thompson, P. and Ananiadou, S. (In Press). Identification of Manner in Bio-Events. Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC 2012). Nawaz, R., Thompson, P., McNaught, J. and Ananiadou, S. (2010). Meta-Knowledge Annotation of Bio-Events. In Proceedings of LREC 2010, pp.  2498-2507. Pyysalo, S., Ginter, F., Heimonen, J., Bjorne, J., Boberg, J., Jarvinen, J. and Salakoski, T. (2007). BioInfer: a corpus for information extraction in the biomedical domain. BMC Bioinformatics, 8: 50. Pyysalo, S., Ohta, T., Rak, R., Sullivan, D., Mao, C., Wang, C., Sobral, B., Tsujii, J. and Ananiadou, S. (In Press). Overview of the ID, EPI and REL tasks of BioNLP Shared Task 2011. BMC Bioinformatics. Quirk, C., Choudhury, P., Gamon, M. and Vanderwende, L. (2011). MSR-NLP Entry in BioNLP Shared Task 2011. In Proceedings of BioNLP Shared Task 2011 Workshop, pp.  155-163. Ruch, P., Boyer, C., Chichester, C., Tbahriti, I., Geissbuhler, A., Fabry, P., Gobeill, J., Pillet, V., Rebholz-Schuhmann, D., Lovis, C. and Veuthey, A.L. (2007). Using argumentation to extract key sentences from biomedical abstracts. Int J Med Inform, 76(2-3): 195-200. S?ndor, ?. (2007). Modeling metadiscourse conveying the author?s rhetorical strategy in biomedical 
research abstracts. Revue Fran?aise de Linguistique Appliqu?e, 200(2): 97-109. Shatkay, H., Pan, F., Rzhetsky, A. and Wilbur, W.J. (2008). Multi-dimensional classification of biomedical text: toward automated, practical provision of high-utility text to diverse users. Bioinformatics, 24(18): 2086-2093. Soldatova, L.N. and King, R.D. (2006). An ontology of scientific experiments. Journal of the Royal Society Interface, 3(11): 795-803. Soldatova, L.N. and Liakata, M. (2007). An ontology methodology and cisp-the proposed core information about scientific papers., Aberystwyth University. Technical Report JISC Project Report. Teufel, S. (2010). The Structure of Scientific Articles: Applications to Citation Indexing and Summarization. Stanford, CA, CSLI Publications. Teufel, S., Carletta, J. and Moens, M. (1999). An annotation scheme for discourse-level argumentation in research articles. In Proceedings of EACL, pp.  110-117. Teufel, S. and Moens, M. (2002). Summarizing scientific articles: experiments with relevance and rhetorical status. Computational Linguistics, 28(4): 409-445. Teufel, S., Siddharthan, A. and Batchelor, C. (2009). Towards discipline-independent argumentative zoning: Evidence from chemistry and computational linguistics. In Proceedings of EMNLP 2009, pp.  1493-1502. Thompson, P., Iqbal, S.A., McNaught, J. and Ananiadou, S. (2009). Construction of an annotated corpus to support biomedical information extraction. BMC Bioinformatics, 10: 349. Thompson, P., Nawaz, R., McNaught, J. and Ananiadou, S. (2011). Enriching a biomedical event corpus with meta-knowledge annotation. BMC Bioinformatics, 12: 393. Vincze, V., Szarvas, G., Farkas, R., Mora, G. and Csirik, J. (2008). The BioScope corpus: biomedical texts annotated for uncertainty, negation and their scopes. BMC Bioinformatics, 9(Suppl 11): S9. Zweigenbaum, P., Demner-Fushman, D., Yu, H. and Cohen, K.B. (2007). Frontiers of biomedical text mining: current progress. Briefings in Bioinformatics, 8(5): 358-375.  
46
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 79?88,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Towards a Better Understanding of Discourse:
Integrating Multiple Discourse Annotation Perspectives Using UIMA
Claudiu Miha?ila??, Georgios Kontonatsios?, Riza Theresa Batista-Navarro?,
Paul Thompson?, Ioannis Korkontzelos and Sophia Ananiadou
The National Centre for Text Mining,
School of Computer Science, The University of Manchester
{mihailac,kontonag,batistar,thompsop,
korkonti,ananiads}@cs.man.ac.uk
Abstract
There exist various different discourse an-
notation schemes that vary both in the
perspectives of discourse structure consid-
ered and the granularity of textual units
that are annotated. Comparison and inte-
gration of multiple schemes have the po-
tential to provide enhanced information.
However, the differing formats of cor-
pora and tools that contain or produce
such schemes can be a barrier to their
integration. U-Compare is a graphical,
UIMA-based workflow construction plat-
form for combining interoperable natu-
ral language processing (NLP) resources,
without the need for programming skills.
In this paper, we present an extension
of U-Compare that allows the easy com-
parison, integration and visualisation of
resources that contain or output annota-
tions based on multiple discourse anno-
tation schemes. The extension works by
allowing the construction of parallel sub-
workflows for each scheme within a single
U-Compare workflow. The different types
of discourse annotations produced by each
sub-workflow can be either merged or vi-
sualised side-by-side for comparison. We
demonstrate this new functionality by us-
ing it to compare annotations belonging
to two different approaches to discourse
analysis, namely discourse relations and
functional discourse annotations. Integrat-
ing these different annotation types within
an interoperable environment allows us to
study the correlations between different
types of discourse and report on the new
insights that this allows us to discover.
?The authors have contributed equally to the development
of this work and production of the manuscript.
1 Introduction
Over the past few years, there has been an increas-
ing sophistication in the types of available natural
language processing (NLP) tools, with named en-
tity recognisers being complemented by relation
and event extraction systems. Such relations and
events are not intended to be understood in isola-
tion, but rather they are arranged to form a coher-
ent discourse. In order to carry out complex tasks
such as automatic summarisation to a high degree
of accuracy, it is important for systems to be able
to analyse the discourse structure of texts automat-
ically. To facilitate the development of such sys-
tems, various textual corpora containing discourse
annotations have been made available to the NLP
community. However, there is a large amount of
variability in the types of annotations contained
within these corpora, since different perspectives
on discourse have led to the development of a
number of different annotation schemes.
Corpora containing discourse-level annotations
usually treat the text as a sequence of coherent tex-
tual zones (e.g., clauses and sentences). One line
of research has been to identify which zones are
logically connected to each other, and to charac-
terise these links through the assignment of dis-
course relations. There are variations in the com-
plexity of the schemes used to annotate these dis-
course relations. For example, Rhetorical Struc-
ture Theory (RST) (Mann and Thompson, 1988)
defines 23 types of discourse relations that are
used to structure the text into complex discourse
trees. Whilst this scheme was used to enrich the
Penn TreeBank (Carlson et al, 2001), the Penn
Discourse TreeBank (PDTB) (Prasad et al, 2008)
used another scheme to identify discourse rela-
tions that hold between pairs of text spans. It cate-
gorises the relations into types such as ?causal?,
?temporal? and ?conditional?, which can be ei-
ther explicit or implicit, depending on whether or
79
not they are represented in text using overt dis-
course connectives. In the biomedical domain, the
Biomedical Discourse Relation Bank (BioDRB)
(Prasad et al, 2011) annotates a similar set of re-
lation types, whilst BioCause focusses exclusively
on causality (Miha?ila? et al, 2013).
A second line of research does not aim to link
textual zones, but rather to classify them accord-
ing to their specific function in the discourse. Ex-
amples of functional discourse annotations include
whether a particular zone asserts new information
into the discourse or represents a speculation or
hypothesis. In scientific texts, knowing the type
of information that a zone represents (e.g., back-
ground knowledge, hypothesis, experimental ob-
servation, conclusion, etc.) allows for automatic
isolation of new knowledge claims (Sa?ndor and de
Waard, 2012). Several annotation schemes have
been developed to classify textual zones accord-
ing to their rhetorical status or general informa-
tion content (Teufel et al, 1999; Mizuta et al,
2006; Wilbur et al, 2006; de Waard and Pan-
der Maat, 2009; Liakata et al, 2012a). Related
to these studies are efforts to capture information
relating to discourse function at the level of events,
i.e., structured representations of pieces of knowl-
edge which, when identified, facilitate sophisti-
cated semantic searching (Ananiadou et al, 2010).
Since there can be multiple events in a sentence
or clause, the identification of discourse informa-
tion at the event level can allow for a more de-
tailed analysis of discourse elements than is possi-
ble when considering larger units of text. Certain
event corpora such as ACE 2005 (Walker, 2006)
and GENIA-MK (Thompson et al, 2011) have
been annotated with various types of functional
discourse information.
It has previously been shown that considering
several functional discourse annotation schemes in
parallel can be beneficial (Liakata et al, 2012b),
since each scheme offers a different perspective.
For a common set of documents, the cited study
analysed and compared functional discourse an-
notations at different levels of textual granular-
ity (i.e., sentences, clauses and events), showing
how the different schemes could complement each
other in order to lay the foundations for a possible
future harmonisation of the schemes. The results
of this analysis provide evidence that it would be
useful to carry out further such analyses involv-
ing other such schemes, including an investiga-
tion of how discourse relations and functional dis-
course annotations could complement each other,
e.g., which types of functional annotations occur
within the arguments of discourse relations. There
are, however, certain barriers to carrying out such
an analysis. For example, a comparison of an-
notation schemes would ideally allow the differ-
ent types of annotations to be visualised simul-
taneously or seamlessly merged together. How-
ever, the fact that annotations in different corpora
are encoded using different formats (e.g., stand-off
or in-line) and different encoding schemes means
that this can be problematic.
A solution to the challenges introduced above is
offered by the Unstructured Information Manage-
ment Architecture (UIMA) (Ferrucci and Lally,
2004), which defines a common workflow meta-
data format facilitating the straightforward combi-
nation of NLP resources into a workflow. Based
on the interoperability of the UIMA framework,
numerous researchers distribute their own tools as
UIMA-compliant components (Kano et al, 2011;
Baumgartner et al, 2008; Hahn et al, 2008;
Savova et al, 2010; Gurevych et al, 2007; Rak
et al, 2012b). However, UIMA is only intended
to provide an abstract framework for the interop-
erability of language resources, leaving the actual
implementation to third-party developers. Hence,
UIMA does not explicitly address interoperability
issues of tools and corpora.
U-Compare (Kano et al, 2011) is a UIMA-
based workflow construction platform that pro-
vides a graphical user interface (GUI) via which
users can rapidly create NLP pipelines using a
drag-and-drop mechanism. Conforming to UIMA
standards, U-Compare components and pipelines
are compatible with any UIMA application via a
common and sharable type system (i.e., a hier-
archy of annotation types). In defining this type
system, U-Compare promotes interoperability of
tools and corpora, by exhaustively modelling a
wide range of NLP data types (e.g., sentences, to-
kens, part-of-speech tags, named entities). This
type system was recently extended to include dis-
course annotations to model three discourse phe-
nomena, namely causality, coreference and meta-
knowledge (Batista-Navarro et al, 2013).
In this paper, we describe our extensions to U-
Compare, supporting the integration and visuali-
sation of resources annotated according to mul-
tiple discourse annotation schemes. Our method
80
decomposes pipelines into parallel sub-workflows,
each linked to a different annotation scheme.
The resulting annotations produced by each sub-
workflow can be either merged within a single
document or visualised in parallel views.
2 Related work
Previous studies have shown the advantages of
comparing and integrating different annotation
schemes on a corpus of documents (Guo et al,
2010; Liakata et al, 2010; Liakata et al, 2012b).
Guo et al (2010) compared three different dis-
course annotation schemes applied to a corpus
of biomedical abstracts on cancer risk assess-
ment and concluded that two of the schemes pro-
vide more fine-grained information than the other
scheme. They also revealed a subsumption rela-
tion between two schemes. Such outcomes from
comparing schemes are meaningful for users who
wish to select the most appropriate scheme for an-
notating their data. Liakata et al (2012) under-
line that different discourse annotation schemes
capture different dimensions of discourse. Hence,
there might be complementary information across
different schemes. Based on this hypothesis, they
provide a comparison of three annotation schemes,
namely CoreSC (Liakata et al, 2012a), GENIA-
MK (Thompson et al, 2011) and DiscSeg (de
Waard, 2007), on a corpus of three full-text pa-
pers. Their results showed that the categories in
the three schemes can complement each other. For
example, the values of the Certainty Level dimen-
sion of the GENIA-MK scheme can be used to as-
sign confidence values to the Conclusion, Result,
Implication and Hypothesis categories of CoreSC
and DiscSeg. In contrast to previous studies, our
proposed approach automatically integrates mul-
tiple annotation schemes. The proposed mecha-
nism allows users to easily compare, integrate and
visualise multiple discourse annotation schemes
in an interoperable NLP infrastructure, i.e., U-
Compare.
There are currently a number of freely-available
NLP workflow infrastructures (Ferrucci and Lally,
2004; Cunningham et al, 2002; Scha?fer, 2006;
Kano et al, 2011; Grishman, 1996; Baumgartner
et al, 2008; Hahn et al, 2008; Savova et al, 2010;
Gurevych et al, 2007; Rak et al, 2012b). Most
of the available infrastructures support the devel-
opment of standard NLP applications, e.g., part-
of-speech tagging, deep parsing, chunking, named
entity recognition and several of them allow the
representation and analysis of discourse phenom-
ena (Kano et al, 2011; Cunningham et al, 2002;
Savova et al, 2010; Gurevych et al, 2007). How-
ever, none of them has demonstrated the integra-
tion of resources annotated according to multiple
annotation schemes within a single NLP pipeline.
GATE (Cunningham et al, 2002) is an open
source NLP infrastructure that has been used for
the development of various language processing
tasks. It is packaged with an exhaustive number
of NLP components, including discourse analy-
sis modules, e.g., coreference resolution. Further-
more, GATE offers a GUI environment and wrap-
pers for UIMA-compliant components. However,
GATE implements a limited workflow manage-
ment mechanism that does not support the execu-
tion of parallel or nested workflows. In addition to
this, GATE does not promote interoperability of
language resources since it does not define any hi-
erarchy of NLP data types and components do not
formally declare their input/output capabilities.
In contrast to GATE, UIMA implements a more
sophisticated workflow management mechanism
that supports the construction of both parallel
and nested pipelines. In this paper, we exploit
this mechanism to integrate multiple annotation
schemes in NLP workflows. cTAKES (Savova
et al, 2010) and DKPro (Gurevych et al, 2007)
are two repositories containing UIMA-compliant
components that are tuned for the medical and
general domain, respectively. However, both of
these repositories support the representation of
only one discourse phenomenon, i.e., coreference.
Argo (Rak et al, 2012a; Rak et al, 2012b) is a
web-based platform that allows multiple branch-
ing and merging of UIMA pipelines. It incorpo-
rates several U-Compare components and conse-
quently, supports the U-Compare type system.
3 A UIMA architecture for processing
multiple annotation schemes
In UIMA, a document, together with its associated
annotations, is represented as a standardised data
structure, namely the Common Analysis Struc-
ture (CAS). Each CAS can contain any number
of nested sub-CASes, i.e., Subjects of Analysis
(Sofas), each of which can associate a different
type of annotation with the input document. In
this paper, we employ this UIMA mechanism to
allow the integration and comparison of multiple
81
Collection of Documents
Multi-SofaReader
Parallel Annotation Viewer Annotation Merger
ComparingSchemes Integrating Schemes
ComponentC_1
Sofa S_1
ComponentC_2
Sofa S_2
ComponentC_N-1
Sofa S_N-1
ComponentC_N
Sofa S_N
sub-workflows
Figure 1: Integrating annotations from multiple
annotation schemes in UIMA workflows
annotation schemes in a single U-Compare work-
flow. Assume that we have a corpus of documents
which has been annotated according to n different
schemes, S1, S2, ..., Sn?1, Sn. Also, assume that
we will use a library of m text analysis compo-
nents, C1, C2, ..., Cm?1, Cm, to enrich the corpus
with further annotations.
Our implemented architecture is illustrated in
Figure 1. Using multiple Sofas, we are able to split
a UIMA workflow into parallel sub-workflows.
Starting from a Multi-Sofa reader, we create n
sub-workflows, i.e., Sofas, each of which is linked
to a particular scheme for a different annotation
type. Each sub-workflow can then apply the anal-
ysis components that are most suitable for pro-
cessing the annotations from the corresponding
scheme.
U-Compare offers two different modes for visu-
alising corpora that have been annotated accord-
ing to multiple schemes. In the comparison mode,
the default annotation viewer is automatically split
to allow annotations from different schemes to be
displayed side-by-side. The second type of visu-
alisation merges the annotations produced by the
parallel sub-workflows into a single view. The
most appropriate view may depend on the prefer-
ences of the user and the task at hand, e.g., iden-
tifying similarities, differences or complementary
information between different schemes.
4 Application Workflows
In this section, we demonstrate two workflow ap-
plications that integrate multiple discourse anno-
tation schemes. The first workflow exploits U-
Compare?s comparison mode to visualise in par-
allel functional discourse annotations from two
schemes, namely, CoreSC (Liakata et al, 2012a)
and GENIA-MK (Thompson et al, 2011). The
second application integrates functional discourse
annotations in the ACE 2005 corpus with dis-
course relations obtained by an automated tool.
4.1 Visualising functional discourse
annotations from different schemes
The purpose of this workflow application is to re-
veal the different interpretations given by two dis-
course annotation schemes applied to a biomed-
ical corpus of three full-text papers (Liakata et
al., 2012b). The pipeline contains two read-
ers that take as input the annotations (in the
BioNLP Shared Task stand-off format) from the
two schemes and map them to U-Compare?s
type system. In this way, the annotations be-
come interoperable with existing components in
U-Compare?s library. U-Compare detects that the
workflow contains two annotation schemes and
automatically creates two parallel sub-workflows
as explained earlier. Furthermore, we configure
the workflow to use the comparison mode. There-
fore, the annotation viewer will display the two
different types of annotations based on the input
schemes side-by-side. Figure 2 illustrates the par-
allel viewing of a document annotated according
to both the CoreSC (left-hand side) and GENIA-
MK (right-hand side) annotation schemes. The
CoreSC scheme assigns a single category per sen-
tence. The main clause in the highlighted sen-
tence on the left-hand side constitutes the hypoth-
esis that transcription factors bind to exon-1. Ac-
cordingly, as can be confirmed from the annota-
tion table on the far right-hand side of the figure,
the (Hyp)othesis category has been assigned to the
sentence.
In the GENIA-MK corpus, the different pieces
of information contained within the sentence have
been separately annotated as structured events.
One of these events corresponds to the hypothe-
sis, but this is not the only information expressed:
information about a previous experimental out-
come from the authors, i.e., that exon1 is impli-
cated in CCR3 transcription, is annotated as a sep-
82
Figure 2: Comparing discourse annotations schemes in U-Compare. The pipeline uses two Sofas corre-
sponding to the CoreSC (left panel) and GENIA-MK (right panel) schemes.
arate event. Since functional discourse informa-
tion is annotated directly at the event level in the
GENIA-MK corpus, the bind event is considered
independently from the other event as represent-
ing an Analysis. Furthermore, the word hypoth-
esized is annotated as a cue for this categorisa-
tion. There are several ways in which the an-
notations of the two schemes can be seen to be
complementary to each other. For example, the
finer-grained categorisation of analytical informa-
tion in the CoreSC scheme could help to determine
that the analytical bind event in the GENIA-MK
corpus specifically represents a hypothesis, rather
than, e.g., a conclusion. Conversely, the event-
based annotation in the GENIA-MK corpus can
help to determine exactly which part of the sen-
tence represents the hypothesis. Furthermore, the
cue phrases annotated in the GENIA-MK corpus
could be used as additional features in a system
trained to assign CoreSC categories. Although in
this paper we illustrate only the visualisation of
different types of functional discourse annotations,
it is worth noting that U-Compare provides sup-
port for further processing. Firstly, unlike annota-
tion platforms such as brat (Stenetorp et al, 2012),
U-Compare allows for analysis components to be
integrated into workflows in a straightforward and
user-interactive manner. If, for example, it is of in-
terest to determine the tokens (and the correspond-
ing parts-of-speech) which frequently act as cues
in Analysis events, syntactic analysis components
(e.g., tokenisers and POS taggers) can be incorpo-
rated via a drag-and-drop mechanism. Also, U-
Compare allows the annotations to be saved in a
computable format using the provided Xmi Writer
CAS Consumer component. This facilitates fur-
ther automatic comparison of annotations.
4.2 Integrating discourse relations with
functional discourse annotations
To demonstrate the integration of annotations orig-
inating from two completely different perspectives
on discourse, we have created a workflow that
merges traditional discourse relations with func-
tional discourse annotations in a general domain
corpus. For this application, we used the ACE
2005 corpus, which consists of 599 documents
coming from broadcast conversation, broadcast
news, conversational telephone speech, newswire,
weblog and usenet newsgroups. This corpus
contains event annotations which have been en-
riched by attributes such as polarity (positive or
negative), modality (asserted or other), generic-
ity (generic or specific) and tense (past, present,
future or unspecified). We treat the values of
these attributes as functional discourse annota-
tions, since they provide further insight into the
interpretation of the events. We created a compo-
nent that reads the event annotations in the corpus
and maps them to U-Compare?s type system.
To obtain discourse relation annotations (which
are not available in the ACE corpus) we em-
ployed an end-to-end discourse parser trained
on the Penn Discourse TreeBank (Lin et al,
2012). It outputs three general types of anno-
tations, namely, explicit relations, non-explicit
relations and attribution spans. Explicit rela-
tions (i.e., those having overt discourse connec-
tives) are further categorised into the following 16
PDTB level-2 types: Asynchronous, Synchrony,
Cause, Pragmatic cause, Contrast, Concession,
Conjunction, Instantiation, Restatement, Alterna-
tive, List, Condition, Pragmatic condition, Prag-
matic contrast, Pragmatic concession and Excep-
83
Figure 3: Integrating different discourse annotation schemes in U-Compare.
tion. Non-explicit relations, on the other hand,
consist of EntRel and NoRel types, in addition to
the same first 11 explicit types mentioned above.
We created a workflow consisting of the ACE
corpus reader and the discourse parser (available
in U-Compare as a UIMA web service). This al-
lowed us to merge traditional discourse relations
with event-based functional discourse annotations,
and to visualise them in the same document (Fig-
ure 3). Furthermore, with the addition of the
Xmi Writer CAS Consumer in the workflow, the
merged annotations can be saved in a computable
format for further processing, allowing users to
perform deeper analyses on the discourse annota-
tions. This workflow has enabled us to gain some
insights into the correlations between functional
discourse annotations and discourse relations.
5 Correlations between discourse
relations and functional discourse
annotations
Based on the merged annotation format described
in the previous section, we computed cases in
which at least one of the arguments of a discourse
relation also contains an event. Figure 4 is a
heatmap depicting the correlations between differ-
ent types of discourse relations and the attribute
values of ACE events that co-occur with these re-
lations. The darker the colour, the smaller the ratio
of the given discourse relation co-occurring with
the specified ACE event attribute value. For in-
stance, the Cause relation co-occurs mostly with
positive events (over 95%) and the correspond-
ing cell is a very light shade of green. These are
discussed and exemplified below. In the exam-
ples, the following marking convention is used:
discourse connectives are capitalised, whilst argu-
ments are underlined. Event triggers are shown in
bold, and cues relating to functional discourse cat-
egories are italicised.
For all discourse relation types, at least 50% of
co-occurring events are assigned the specific value
of the Genericity attribute. Specific events are
those that describe a specific occurrence or situ-
ation, rather than a more generic situation. In gen-
eral, this high proportion of specific events is to be
expected. The types of text contained within the
corpus, consisting largely of news and transcrip-
tions of conversions, would be expected to intro-
duce a large amount of information about specific
events.
For two types of discourse relations, i.e. Condi-
tion and Concession, there are more or less equal
numbers of specific and generic events. The na-
ture of these relation types helps to explain these
proportions. Conditional relations often describe
how a particular, i.e., specific, situation will hold
if some hypothetical situation is true. Since hypo-
thetical situations do not denote specific instances,
they will usually be labelled as generic. Con-
cessions, meanwhile, usually describe how a spe-
cific situation holds, even though another (more
generic) situation would normally hold, that would
be inconsistent with this. For the Instantiation re-
lation category, it may once again be expected that
similar proportions of generic and specific events
would co-occur within their arguments, since an
instantiation describes a specific instance of a
more generic situation. However, contrary to these
84
AlternativeAsynchronousAttributionCauseConcessionConditionConjunctionContrastEntRelInstantiationListNoRelRestatementSynchronous
GenericSpecific AssertedOther NegativePositive FuturePast PresentUnspecif
ied
0
1
0.5
0.25
0.75
Genericity Modality Polarity Tense0
1
0.5
0.25
0.75
0
1
0.5
0.25
0.75
0
1
0.5
0.25
0.75
Figure 4: Heatmap showing the distribution of correlations between discourse relations and event-based
functional discourse categories. A darker shade indicates a smaller percentage of instances of a discourse
relation co-occurring with an event attribute.
expectations, the ratio of specific to generic events
is 3:1. A reason for this is that discourse argu-
ments corresponding to the description of a spe-
cific instance may contain several different events,
as illustrated in Example (1).
(1) Toefting has been convicted before. In
1999 he was given a 20-day suspended sentence
for assaulting a fan who berated him for
playing with German club Duisburg.
In terms of the Modality attribute, most dis-
course relations correlate with definite, asserted
events. Simillarly to the Genericity attribute, this
can be largely explained by the nature of the texts.
However, there are two relation types, i.e., Condi-
tion and Consession, which have particularly high
proportions of co-occurring events whose modal-
ity is other. Events that are assigned this attribute
value correspond to those that are not described as
though they are real occurrences. This includes,
e.g., speculated or hypothetical events. The fact
that Condition relations are usually hypothetical
in nature explain why 76% of events that co-occur
with such relations are assigned the other value
for the Modality attribute. Example (2) illustrates
a sentence containing this relation type.
(2) And I?ve said many times, IF we all
agreed on everything, everybody would want to
marry Betty and we would really be in a mess,
wouldn?t we, Bob.
An even higher proportion of Concession re-
lations co-occurs with events whose modality is
other. Example (3) helps to explain this. In the
first clause (the generic situation), the mention of
minimising civilian casualties is only described as
an effort, rather than a definite situation. The hedg-
ing of this generic situation is necessary in order to
concede that the more specific situation described
in the second clause could actually be true, i.e.,
that a large number of civilians have already been
killed. Due to the nature of news reporting, which
may come from potentially unreliable sources, the
killed event in this second clause is also hedged,
through the use of the word reportedly.
(3) ALTHOUGH the coalition leaders have
repeatedly assured that every effort would be
made to minimize civilian casualties in the
current Iraq war, at least 130 Iraqi civilians have
been reportedly killed since the war started five
days ago.
Almost 96% of events that co-occur with argu-
ments of discourse relations have positive polarity.
Indeed, for eight relation types, 100% of the cor-
responding events are positive. This can partly be
explained by the fact that, in texts reporting news,
85
there is an emphasis on reporting events that have
happened, rather than events that did not happen.
It can, however, be noted that events that co-occur
with certain discourse relation types have a greater
likelihood of having negative polarity. These rela-
tions include Contrast (9% of events having neg-
ative polarity) and Cause (5% negative events).
Contrasts can include comparisons of positive and
negative situations, as in Example (4), whilst for
Causes, it can sometimes be relevant to state that
a particular situation caused a specific event not to
take place, as shown in Example (5).
(4) The message from the Israeli government
is that its soldiers are not targeting journalists,
BUT that journalists who travel to places where
there could be live fire exchange between
Israeli forces and Palestinian gunmen have a
responsibility to take greater precautions.
(5) His father didn?t want to invade Iraq, BE-
CAUSE of all these problems they?re having
now.
For most relation types, around 60% of their co-
occurring events are annotated as describing past
tense situations. This nature of newswire and con-
versations mean that this is largely to be expected,
since they normally report mainly on events that
have already happened. The proportion of events
assigned the future tense value is highest when
they co-occur with discourse relations of type Al-
ternative. In this relation type, it is often the case
that one of the arguments describes a possible fu-
ture alternative to a current situation, as the case in
Example (6). This possible information pattern for
Alternative relations, where one of the arguments
represents a currently occurring situation, would
also help to explain why, even though very few
events in general are annotated as present tense,
almost 10% of events that co-occur with Alter-
native relations describe events that are currently
ongoing. As for events whose Tense value is un-
specified, two of the most common discourse re-
lation types with which they occur are Condition
and Concession. As exemplified above, Condition
relations are often hypothetical in nature, meaning
that no specific tense can be assigned. The generic
argument of a Concession relation can also remain
unmarked for tense. As in Example (3), it is not
clear whether the effort to minimise civilian casu-
alties has already been initiated, or will be initiated
in the future.
(6) Saddam wouldn?t be destroying missiles
UNLESS he thought he was going to be
destroyed if he didn?t.
6 Conclusions
Given the level of variability in existing discourse-
annotated corpora, it is meaningful for users to
identify the relative merits of different schemes.
In this paper, we have presented an extension of
the U-Compare infrastructure that facilitates the
comparison, integration and visualisation of doc-
uments annotated according to different annota-
tion schemes. U-Compare constructs multiple and
parallel annotation sub-workflows nested within a
single workflow, with each sub-workflow corre-
sponding to a distinct scheme. We have applied
the implemented method to visualise the similar-
ities and differences of two functional discourse
annotation schemes, namely CoreSC and GENIA-
MK. To demonstrate the integration of multiple
schemes in U-Compare, we developed a work-
flow that merged event annotations from the ACE
2005 corpus (which include certain types of func-
tional discourse information) with discourse rela-
tions obtained by an end-to-end parser. Moreover,
we have analysed the merged annotations obtained
by this workflow and this has allowed us to iden-
tify various correlations between the two different
types of discourse annotations.
Based on the intuition that there is comple-
mentary information across different types of dis-
course annotations, we intend to examine how the
integration of multiple discourse schemes, e.g.,
features obtained by merging annotations, affects
the performance of machine learners for discourse
analysis.
7 Acknowledgements
We are grateful to Dr. Ziheng Lin (Na-
tional University of Singapore) for providing us
with the discourse parser used for this work.
This work was partially funded by the Euro-
pean Community?s Seventh Framework Program
(FP7/2007-2013) [grant number 318736 (OSS-
METER)]; Engineering and Physical Sciences Re-
search Council [grant numbers EP/P505631/1,
EP/J50032X/1]; and MRC Text Mining and
Screening (MR/J005037/1).
86
References
Sophia Ananiadou, Sampo Pyysalo, Junichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology, 28(7):381 ? 390.
Riza Theresa B. Batista-Navarro, Georgios Kontonat-
sios, Claudiu Miha?ila?, Paul Thompson, Rafal Rak,
Raheel Nawaz, Ioannis Korkontzelos, and Sophia
Ananiadou. 2013. Facilitating the analysis of dis-
course phenomena in an interoperable NLP plat-
form. In Computational Linguistics and Intelligent
Text Processing, volume 7816 of Lecture Notes in
Computer Science, pages 559?571. Springer Berlin
Heidelberg, March.
William A. Baumgartner, Kevin Bretonnel Cohen, and
Lawrence Hunter. 2008. An open-source frame-
work for large-scale, flexible evaluation of biomedi-
cal text mining systems. Journal of biomedical dis-
covery and collaboration, 3:1+, January.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a discourse-tagged cor-
pus in the framework of Rhetorical Structure Theory.
In Proceedings of the Second SIGdial Workshop on
Discourse and Dialogue - Volume 16, SIGDIAL ?01,
pages 1?10, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
an architecture for development of robust HLT
applications. In In Recent Advanced in Language
Processing, pages 168?175.
Anita de Waard and Henk Pander Maat. 2009. Epis-
temic segment types in biology research articles. In
Proceedings of the Workshop on Linguistic and Psy-
cholinguistic Approaches to Text Structuring (LPTS
2009).
Anita de Waard. 2007. A pragmatic structure for re-
search articles. In Proceedings of the 2nd inter-
national conference on Pragmatic web, ICPW ?07,
pages 83?89, New York, NY, USA. ACM.
David Ferrucci and Adam Lally. 2004. Building an
example application with the unstructured informa-
tion management architecture. IBM Systems Jour-
nal, 43(3):455?475.
Ralph Grishman. 1996. TIPSTER Text Phase II archi-
tecture design version 2.1p 19 june 1996. In Pro-
ceedings of the TIPSTER Text Program: Phase II,
pages 249?305, Vienna, Virginia, USA, May. Asso-
ciation for Computational Linguistics.
Yufan Guo, Anna Korhonen, Maria Liakata,
Ilona Silins Karolinska, Lin Sun, and Ulla Ste-
nius. 2010. Identifying the information structure of
scientific abstracts: An investigation of three differ-
ent schemes. In Proceedings of the 2010 Workshop
on Biomedical Natural Language Processing, pages
99?107. Association for Computational Linguistics.
Iryna Gurevych, Max Mu?hlha?user, Christof Mu?ller,
Ju?rgen Steimle, Markus Weimer, and Torsten Zesch.
2007. Darmstadt knowledge processing repository
based on UIMA. In Proceedings of the First Work-
shop on Unstructured Information Management Ar-
chitecture at Biannual Conference of the GSCL.
Udo Hahn, Ekaterina Buyko, Rico Landefeld, Matthias
Mu?hlhausen, Michael Poprat, Katrin Tomanek, and
Joachim Wermter. 2008. An overview of JCoRe,
the JULIE lab UIMA component repository. In
LREC?08 Workshop ?Towards Enhanced Interoper-
ability for Large HLT Systems: UIMA for NLP?,
pages 1?7, Marrakech, Morocco, May.
Yoshinobu Kano, Makoto Miwa, Kevin Cohen,
Lawrence Hunter, Sophia Ananiadou, and Jun?ichi
Tsujii. 2011. U-Compare: A modular NLP work-
flow construction and evaluation system. IBM Jour-
nal of Research and Development, 55(3):11.
Maria Liakata, Simone Teufel, Advaith Siddharthan,
and Colin Batchelor. 2010. Corpora for the concep-
tualisation and zoning of scientific papers. In Pro-
ceedings of LREC, volume 10.
Maria Liakata, Shyamasree Saha, Simon Dobnik,
Colin Batchelor, and Dietrich Rebholz-Schuhmann.
2012a. Automatic recognition of conceptualization
zones in scientific articles and two life science appli-
cations. Bioinformatics, 28(7):991?1000.
Maria Liakata, Paul Thompson, Anita de Waard, Ra-
heel Nawaz, Henk Pander Maat, and Sophia Ana-
niadou. 2012b. A three-way perspective on scien-
tic discourse annotation for knowledge extraction.
In Proceedings of the ACL Workshop on Detecting
Structure in Scholarly Discourse (DSSD), pages 37?
46, July.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012. A
PDTB-styled end-to-end discourse parser. Natural
Language Engineering, FirstView:1?34, 10.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a functional
theory of text organization. Text, 8(3):243?281.
Claudiu Miha?ila?, Tomoko Ohta, Sampo Pyysalo, and
Sophia Ananiadou. 2013. BioCause: Annotating
and analysing causality in the biomedical domain.
BMC Bioinformatics, 14(1):2, January.
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel
Collier. 2006. Zone analysis in biology articles as a
basis for information extraction. International Jour-
nal of Medical Informatics, 75(6):468 ? 487. Re-
cent Advances in Natural Language Processing for
Biomedical Applications Special Issue.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bon-
nie Webber. 2008. The Penn Discourse Tree-
Bank 2.0. In Nicoletta Calzolari, Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios
87
Piperidis, and Daniel Tapias, editors, In Proceedings
of the 6th International Conference on language Re-
sources and Evaluation (LREC), pages 2961?2968.
Rashmi Prasad, Susan McRoy, Nadya Frid, Aravind
Joshi, and Hong Yu. 2011. The biomedical
discourse relation bank. BMC Bioinformatics,
12(1):188.
Rafal Rak, Andrew Rowley, and Sophia Ananiadou.
2012a. Collaborative development and evaluation
of text-processing workflows in a UIMA-supported
web-based workbench.
Rafal Rak, Andrew Rowley, William Black, and Sophia
Ananiadou. 2012b. Argo: an integrative, in-
teractive, text mining-based workbench supporting
curation. Database: The Journal of Biological
Databases and Curation, 2012.
A?gnes Sa?ndor and Anita de Waard. 2012. Identifying
claimed knowledge updates in biomedical research
articles. In Proceedings of the Workshop on De-
tecting Structure in Scholarly Discourse, ACL ?12,
pages 10?17, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Guergana Savova, James Masanz, Philip Ogren, Jiap-
ing Zheng, Sunghwan Sohn, Karin Kipper-Schuler,
and Christopher Chute. 2010. Mayo clini-
cal text analysis and knowledge extraction system
(cTAKES): architecture, component evaluation and
applications. Journal of the American Medical In-
formatics Association, 17(5):507?513.
Ulrich Scha?fer. 2006. Middleware for creating and
combining multi-dimensional nlp markup. In Pro-
ceedings of the 5th Workshop on NLP and XML:
Multi-Dimensional Markup in Natural Language
Processing, pages 81?84. ACL.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a web-based tool for NLP-assisted
text annotation. In Proceedings of the Demonstra-
tions Session at EACL 2012, Avignon, France, April.
Association for Computational Linguistics.
Simone Teufel, Jean Carletta, and Marc Moens. 1999.
An annotation scheme for discourse-level argumen-
tation in research articles. In Proceedings of the
ninth conference on European chapter of the Asso-
ciation for Computational Linguistics, EACL ?99,
pages 110?117, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Paul Thompson, Raheel Nawaz, John McNaught, and
Sophia Ananiadou. 2011. Enriching a biomedi-
cal event corpus with meta-knowledge annotation.
BMC Bioinformatics, 12(1):393.
Christopher Walker. 2006. ACE 2005 Multilingual
Training Corpus.
W John Wilbur, Andrey Rzhetsky, and Hagit Shatkay.
2006. New directions in biomedical text annota-
tion: definitions, guidelines and corpus construction.
BMC Bioinformatics, 7(1):356.
88
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 69?74,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Building a semantically annotated corpus for congestive heart and 
renal failure from clinical records and the literature 
Noha Alnazzawi, Paul Thompson and Sophia Ananiadou 
School of Computer Science, University of Manchester, UK 
alnazzan@cs.man.ac.uk, {paul.thompson, 
sophia.ananiadou@manchester.ac.uk} 
       
Abstract 
Narrative information in Electronic Health Records 
(EHRs) and literature articles contains a wealth of 
clinical information about treatment, diagnosis, 
medication and family history. This often includes 
detailed phenotype information for specific 
diseases, which in turn can help to identify risk 
factors and thus determine the susceptibility of 
different patients.  Such information can help to 
improve healthcare applications, including Clinical 
Decision Support Systems (CDS). Clinical text 
mining (TM) tools can provide efficient automated 
means to extract and integrate vital information 
hidden within the vast volumes of available text. 
Development or adaptation of TM tools is reliant 
on the availability of annotated training corpora, 
although few such corpora exist for the clinical 
domain.  In response, we have created a new 
annotated corpus (PhenoCHF), focussing on the 
identification of phenotype information for a 
specific clinical sub-domain, i.e., congestive heart 
failure (CHF). The corpus is unique in this domain, 
in its integration of information from both EHRs 
(300 discharge summaries) and literature articles (5 
full-text papers). The annotation scheme, whose 
design was guided by a domain expert, includes 
both entities and relations pertinent to CHF.  Two 
further domain experts performed the annotation, 
resulting in high quality annotation, with 
agreement rates up to 0.92 F-Score.   
1 Introduction 
An ever-increasing number of scientific articles 
is published every year. For example, in 2012, 
more than 500,000 articles were published in 
MEDLINE (U.S. National Library of Medicine , 
2013). A researcher would thus need to review at 
least 20 articles per day in order to keep up to 
date with latest knowledge and evidence in the 
literature (Perez-Rey et al., 2012). 
EHRs constitute a further rich source of 
information about patients? health, representing 
different aspects of care (Jensen et al., 2012). 
However, clinicians at the point of care have 
very limited time to review the potentially large 
amount of data contained within EHRs. This 
presents significant barriers to clinical 
practitioners and computational applications 
(Patrick et al., 2006).  
TM tools can be used to extract phenotype 
information from EHRs and the literature and 
help researchers to identify the characteristics of 
CHF and to better understand the role of the 
deterioration in kidney function in the cycle of 
progression of CHF. 
2 Related work 
There are many well-known publicly available 
corpora of scientific biomedical literature, which 
are annotated for biological entities and/or their 
interactions (often referred to as events) (Roberts 
et al., 2009; Xia  &  Yetisgen-Yildiz, 2012). 
Examples include GENIA (Kim et al., 2008), 
BioInfer (Pyysalo et al., 2007)  GREC 
(Thompson et al., 2009), PennBioIE (Kulick et 
al., 2004), GENETAG (Tanabe et al., 2005) and 
LLL?05 (Hakenberg et al., 2005). However, none 
of these corpora is annotated with the types of 
entities and relationships that are relevant to the 
study of phenotype information.  
On the other hand, corpora of clinical text 
drawn from EHRs are rare, due to privacy and 
confidentiality concerns, but also because of the 
time-consuming, expensive and tedious nature of 
producing high quality annotations, which are 
reliant on the expertise of domain experts 
(Uzuner et al., 2011). A small number of corpora, 
however, have been made available, mainly in 
the context of shared task challenges, which aim 
to encourage the development of information 
extraction (IE) systems. These corpora vary in 
terms of the text type and annotation granularity. 
For example, the corpus presented in (Pestian et 
al., 2007) concerns only structured data from 
radiology reports, while the corpus presented in 
(Meystre  &  Haug, 2006) contains unstructured 
parts of EHRs, but annotated with medical 
problem only at the document level.   
Other corpora are more similar to ours, in that 
that they include text-bound annotations 
69
corresponding to entities or relations.  CLEF 
(Clinical E-Science Framework) (Roberts et al., 
2008) was one of the first such corpora to 
include detailed semantic annotation. It consists 
of a number of different types of clinical records, 
including clinic letters, radiology and 
histopathology reports, which are annotated with 
a variety of clinical entities, relations between 
them and co-reference. However, the corpus has 
not been made publicly available. The more 
recent 2013 CLEF-eHEALTH challenge 
(Suominen et al., 2013) corpus consists of EHRs 
annotated with named entities referring to 
disorders and acronyms/abbreviations, mapped 
to UMLS concept identifiers.  
The Informatics for Integrating Biology at the 
Bedside (i2b2) NLP series of challenges have 
released a corpus of de-identified clinical records 
annotated to support a number of IE challenges 
with multiple levels of annotation, i.e., entities 
and relations (Uzuner et al., 2008; Uzuner, 
2009). The 2010 challenge included the release 
of a corpus of discharge summaries and patient 
reports in which named entities and relations 
concerning medical problems, tests and 
treatments were annotated (Uzuner et al., 2011).  
A corpus of EHRs from Mayo Clinic has been 
annotated with both linguistic information (part-
of?speech tags and shallow parsing results) and 
named entities corresponding to disorders (Ogren 
et al., 2008; Savova et al., 2010).    
3 Description of the corpus 
The discharge summaries in our PhenoCHF 
corpus constitute a subset of the data released for 
the second i2b2 shared task, known as 
?recognising obesity? (Uzuner, 2009). 
PhenoCHF corpus was created by filtering the 
original i2b2 corpus, such that only those 
summaries (a total of 300) for patients with CHF 
and kidney failure were retained.  
The second part of PhenoCHF consists of the 
5 most recent full text articles (at the time of 
query submission) concerning the characteristics 
of CHF and renal failure, retrieved from the 
PubMed Central Open Access database. 
4 Methods and results 
The design of the annotation schema was guided 
by an analysis of the relevant discharge 
summaries, in conjunction with a review of 
comparable domain specific schemata and 
guidelines, i.e., those from the CLEF and i2b2 
shared tasks. The schema is based on a set of 
requirements developed by a cardiologist. Taking 
into account our chosen focus of annotating 
phenotype information relating to the CHF 
disease, the cardiologist was asked firstly to 
determine a set of relevant entity types that relate 
to CHF phenotype information and the role of 
the decline in kidney function in the cycle of 
CHF (exemplified in Table 1), secondly to locate 
words that modify the entity (such as polarity 
clues) and thirdly to identify the types of 
relationships that exist between these entity types 
in the description of phenotype information 
(Table 2) .  
Secondly, medical terms in the records are 
mapped semi-automatically onto clinical 
concepts in UMLS, with the aid of MetaMap 
(Aronson, 2001). 
The same annotation schema and guidelines 
were used for both the discharge summaries and 
the scientific full articles. In the latter, certain 
annotations were omitted, i.e., organ entities, 
polarity clues and relations. This decision was 
taken due to the differing ways in which 
phenotype information is expressed in discharge 
summaries and scientific articles. In discharge 
summaries, phenotype information is explicitly 
described in the patient?s medical history, 
diagnoses and test results. On the other hand, 
scientific articles summarise results and research 
findings. This means that certain types of 
information that occur frequently in discharge 
summaries are extremely rare in scientific 
articles, such that their occurrences are too sparse 
to be useful in training TM systems, and hence 
they were not annotated. 
The annotation was carried out by two medical 
doctors, using the Brat Rapid Annotation Tool 
(brat) (Stenetorp et al., 2012), a highly-
configurable and flexible web-based tool for 
textual annotation.  
Annotations in the corpus should reflect the 
instructions provided in the guidelines as closely 
as possible, in order to ensure that the 
annotations are of ahigh quality. A standard 
means of providing evidence regarding the 
reliability of annotations in a corpus is to 
calculate a statistic known as the inter-annotator 
agreement (IAA). IAA provides assurance that 
different annotators can produce the same 
annotations when working independently and 
separately. There are several different methods of 
calculating IAA, which can be influenced by the 
exact nature of the annotation task. We use the 
measures of precision, recall and F-measure to 
70
indicate the level of inter-annotator reliability 
(Hripcsak  &  Rothschild, 2005). In order to 
carry out such calculations, one set of 
annotations is considered as a gold standard and 
the total number of correct entities is the total 
number of entities annotated by this annotator. 
Precision is the percentage of correct positive 
predictions annotated by the second annotator, 
compared to the first annotator?s assumed gold 
standard. It is calculated as follows: 
 
P = TP / TP + FP 
Recall is the percentage of positive cases 
recognised by the second annotator. It is 
calculated as follows: 
R = TP / TP + FN 
F-score is the harmonic mean between 
precision and recall. 
 F-score =  
2* (Precision * Recall) / Precision + Recall 
 We have calculated separate IAA scores for 
the discharge summaries and the scientific 
articles. Table 3 summarises agreement rates for 
term annotation in the discharge summaries, 
showing results for both individual entity types 
and macro-averaged scores over all entity types. 
Relaxed matching criteria were employed, such 
that annotations added by the two annotators 
were considered as a match if their spans 
overlapped. In comparison to related efforts, the 
IAA rates shown in Table 3 are high.  However, 
it should be noted that the number of targeted 
classes and relations in our corpus is small and 
focused, compared to other related corpora.   
Agreement statistics for scientific articles are 
shown in Table 4. Agreement is somewhat lower 
than for discharge summaries, which this could 
be due to the fact that the annotators (doctors) 
are more used to dealing with discharge 
summaries in their day-to-day work, and so are 
more accustomed to locating information in this 
type of text. Scientific articles are much longer 
and generally include more complex language, 
ideas and analyses, which may require more than 
one reading to fully comprehend the information 
within them. Table 5 shows the agreement rates 
for relation annotation in the discharge 
summaries. The agreement rates for relationships 
are relatively high. This can partly be explained 
by the deep domain knowledge possessed by the 
annotators and partly by the fact that the 
relationships to be identified were relatively 
simple, linking only two pre-annotated entities.
Table 1. Annotated phenotype entity classes 
 
Entity Type Description Example 
Cause any medical problem that 
contributes to the occurrence of 
CHF 
 
 
Risk factors A condition that increases the 
chance of a patient having the 
CHF disease 
 
 
Sign & 
symptom 
any observable manifestation 
of a disease which is 
experienced by a patient and 
reported to the physician 
 
 
Non-
traditional 
risk factor 
Conditions  associated with 
abnormalities in kidney 
functions that put the patient at 
higher risk of developing 
?signs & symptoms? and 
causes of CHF 
 
 
Organ Any body part 
 
71
Relation 
Type 
Description Example 
Causality This relationship links two 
concepts in cases in which 
one concept causes the 
other to occur. 
 
 
 
Finding This relationship links the 
organ to the manifestation 
or abnormal variation that 
is observed during the 
diagnosis process. 
 
 
Negate This is one-way relation to 
relate a negation attribute 
(polarity clue) to the 
condition it negates. 
 
 
Table 2. Description of Annotated Relations 
Table 3. Term annotation agreement statistics for discharge summaries 
Table 4. Overall agreement statistics for terms annotation in scientific articles 
 Causality Finding Negate Macro-
average 
F-score 0.86 0.94 0.95 0.91 
Table 5. Relation annotation and agreement statistics for discharge summaries 
5 Conclusion 
This paper has described the creation of a new 
annotated corpus to facilitate the customisation 
of TM tools for the clinical domain. The corpus1 
consists of 300 discharge summaries and 5 full-
text articles from the literature, annotated for 
CHF phenotype information, including causes, 
risk factors, sign & symptoms and non- 
traditional risk factors. Discharge summaries 
have also been annotated with relationships 
holding between pairs of annotated entities. A 
total 7236 of entities and 1181 relationships have 
been annotated. Extracting phenotype 
                                                          
1 Guidelines and stand-off annotation are publicly available 
at https://code.google.com/p/phenochf-
corpus/source/browse/trunk 
information can have a major impact on our 
deeper understanding of disease ethology, 
treatment and prevention (Xu et al., 2013). 
Currently we are working on confirming the 
utility of the annotated corpus in training and 
customising TM tools, i.e., adapting different 
sequence tagging algorithms (such as 
Conditional Random Fields (CRF) and Hidden 
Markov Model (HMM)) to extract 
comprehensive clinical information from both 
discharge summaries and scientific articles.
 
 Causality Risk 
factor 
Sign & 
Symptom 
Non-
traditional 
risk factor 
Polarity 
clue 
Organ Macro-
average 
F-score 0.95 0.94 0.97 0.83 0.94 0.92 0.92 
 Cause Risk factor Sign & 
Symptoms 
Non-
traditional 
risk factor 
Macro-average 
F-score 0.82 0.84 0.82 .77 0.81 
72
References 
MEDLINE citation counts by year of publication. 
Aronson, A.R. (2001). Effective mapping of 
biomedical text to the UMLS Metathesaurus: the 
MetaMap program. Proceedings of the AMIA 
Symposium, American Medical Informatics 
Association. 
Hakenberg, J., Plake, C., Leser, U., Kirsch, H. and 
Rebholz-Schuhmann, D. (2005). LLL?05 
challenge: Genic interaction extraction-
identification of language patterns based on 
alignment and finite state automata. Proceedings 
of the 4th Learning Language in Logic workshop 
(LLL05). 
Hripcsak, G. and Rothschild, A.S. (2005). Agreement, 
the f-measure, and reliability in information 
retrieval. Journal of the American Medical 
Informatics Association, 12(3): 296-298. 
Jensen, P.B., Jensen, L.J. and Brunak, S. (2012). 
Mining electronic health records: towards better 
research applications and clinical care. Nature 
Reviews Genetics, 13(6): 395-405. 
Kim, J.-D., Ohta, T. and Tsujii, J. (2008). Corpus 
annotation for mining biomedical events from 
literature. BMC Bioinformatics, 9(10). 
Kulick, S., Bies, A., Liberman, M., Mandel, M., 
McDonald, R., Palmer, M., Schein, A., Ungar, L., 
Winters, S. and White, P. (2004). Integrated 
annotation for biomedical information extraction. 
Proc. of the Human Language Technology 
Conference and the Annual Meeting of the North 
American Chapter of the Association for 
Computational Linguistics (HLT/NAACL). 
Meystre, S. and Haug, P.J. (2006). Natural language 
processing to extract medical problems from 
electronic clinical documents: performance 
evaluation. Journal of Biomedical Informatics, 
39(6): 589-599. 
Ogren, P.V., Savova, G.K. and Chute, C.G. (2008). 
Constructing Evaluation Corpora for Automated 
Clinical Named Entity Recognition. LREC. 
Patrick, J., Wang, Y. and Budd, P. (2006). Automatic 
Mapping Clinical Notes to Medical 
Terminologies. Australasian Language 
Technology Workshop. 
Perez-Rey, D., Jimenez-Castellanos, A., Garcia-
Remesal, M., Crespo, J. and Maojo, V. (2012). 
CDAPubMed: a browser extension to retrieve 
EHR-based biomedical literature. BMC Medical 
Informatics and Decision Making, 12(1): 29. 
Pestian, J.P., Brew, C., Matykiewicz, P., Hovermale, 
D., Johnson, N., Cohen, K.B. and Duch, W. 
(2007). A shared task involving multi-label 
classification of clinical free text. Proceedings of 
the Workshop on BioNLP 2007: Biological, 
Translational, and Clinical Language Processing, 
Association for Computational Linguistics. 
Pyysalo, S., Ginter, F., Heimonen, J., Bj?rne, J., 
Boberg, J., J?rvinen, J. and Salakoski, T. (2007). 
BioInfer: a corpus for information extraction in 
the biomedical domain. BMC Bioinformatics, 
8(1): 50. 
Roberts, A., Gaizauskas, R., Hepple, M., Demetriou, 
G., Guo, Y., Setzer, A. and Roberts, I. (2008). 
Semantic annotation of clinical text: The CLEF 
corpus. Proceedings of the LREC 2008 workshop 
on building and evaluating resources for 
biomedical text mining. 
Roberts, A., Gaizauskas, R., Hepple, M., Demetriou, 
G., Guo, Y., Roberts, I. and Setzer, A. (2009). 
Building a semantically annotated corpus of 
clinical texts. Journal of Biomedical Informatics, 
42(5): 950-966. 
Savova, G.K., Masanz, J.J., Ogren, P.V., Zheng, J., 
Sohn, S., Kipper-Schuler, K.C. and Chute, C.G. 
(2010). Mayo clinical Text Analysis and 
Knowledge Extraction System (cTAKES): 
architecture, component evaluation and 
applications. Journal of the American Medical 
Informatics Association, 17(5): 507-513. 
Stenetorp, P., Pyysalo, S., Topi?, G., Ohta, T., 
Ananiadou, S. and Tsujii, J.i. (2012). BRAT: a 
web-based tool for NLP-assisted text annotation. 
Proceedings of the Demonstrations at the 13th 
Conference of the European Chapter of the 
Association for Computational Linguistics, 
Association for Computational Linguistics. 
Suominen, H., Salanter?, S., Velupillai, S., Chapman, 
W.W., Savova, G., Elhadad, N., Pradhan, S., 
South, B.R., Mowery, D.L. and Jones, G.J. (2013). 
Overview of the ShARe/CLEF eHealth Evaluation 
Lab 2013. Information Access Evaluation. 
Multilinguality, Multimodality, and Visualization, 
Springer: 212-231. 
Tanabe, L., Xie, N., Thom, L.H., Matten, W. and 
Wilbur, W.J. (2005). GENETAG: a tagged corpus 
for gene/protein named entity recognition. BMC 
Bioinformatics, 6(Suppl 1): S3. 
Thompson, P., Iqbal, S., McNaught, J. and 
Ananiadou, S. (2009). Construction of an 
annotated corpus to support biomedical 
information extraction. BMC Bioinformatics, 
10(1): 349. 
Uzuner, ?., Goldstein, I., Luo, Y. and Kohane, I. 
(2008). Identifying patient smoking status from 
medical discharge records. Journal of the 
American Medical Informatics Association, 15(1): 
14-24. 
73
Uzuner, ?. (2009). Recognizing obesity and 
comorbidities in sparse data. Journal of the 
American Medical Informatics Association, 16(4): 
561-570. 
Uzuner, ?., South, B.R., Shen, S. and DuVall, S.L. 
(2011). 2010 i2b2/VA challenge on concepts, 
assertions, and relations in clinical text. Journal of 
the American Medical Informatics Association, 
18(5): 552-556. 
Xia, F. and Yetisgen-Yildiz, M. (2012). Clinical 
corpus annotation: challenges and strategies. 
Proceedings of the Third Workshop on Building 
and Evaluating Resources for Biomedical Text 
Mining (BioTxtM'2012) in conjunction with the 
International Conference on Language Resources 
and Evaluation (LREC), Istanbul, Turkey. 
Xu, R., Li, L. and Wang, Q. (2013). Towards 
building a disease-phenotype knowledge base: 
extracting disease-manifestation relationship from 
literature. Bioinformatics, 29(17): 2186-2194. 
 
 
 
 
74
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 1?6,
Baltimore, Maryland USA, June 27, 2014. c?2014 Association for Computational Linguistics
Predicting military and veteran suicide risk: 
Cultural aspects 
 
Paul Thompson 
Dartmouth College 
 
Paul.Thompson@dartmouth.edu 
Chris Poulin 
Durkheim Project 
 
chris@durkheimproject.org 
Craig J. Bryan 
National Center for  
Veterans Studies 
craig.bryan@utah.edu 
 
  
 
Abstract 
This paper describes the three phases of 
the Durkheim Project.  For this project 
we developed a clinician's dashboard that 
displays output of models predicting sui-
cide risk of veterans and active duty mili-
tary personnel.   During phase one, we 
built the clinician?s dashboard and com-
pleted a Veterans Affairs (VA) predictive 
risk medical records study, based on an 
analysis of the narrative, or free text, por-
tions of VA medical records, In phase 
two, we will predict suicide risk based on 
opt-in social media postings by patients 
using social media websites, e.g., Face-
book. We describe the software infra-
structure that we have completed for this 
phase two system.  During phase three 
we will provide a three layer intervention 
strategy.  We discuss our methodology 
for the three phases, including IRB-
approved protocols for the first two phas-
es and a soon-to-be approved IRB proto-
col for phase three. 
1 Introduction 
Diagnosis of psychological health and the predic-
tion of negative events, such as suicide, or sui-
cide ideation, is limited by:  a) a lack of under-
standing of the true differentiating risks of sui-
cidality (Health Promotion, 2010; Treating Sol-
diers, 2010) and b) a lack of near real-time reac-
tion capability to large volumes of data.  There is 
a need for broader coverage suicide risk detec-
tion and a better understanding of the expression 
of suicide ideation through data mining of text 
and images.  The Durkheim Project?s proposed 
solution is to provide continuous monitoring of 
text based information, such as found in social 
network user behavioral intent enabling interven-
tion; facilitated by social / online data sources, 
powered by a medically-validated suicide risk 
classifier. 
 
2   Suicide risk and military culture 
 
The suicide rate among members of the United 
States Armed Forces has continued to rise for the 
past decade, beginning soon after the onset of 
military operations in Iraq and Afghanistan. Sui-
cide is now the second-leading cause of death 
among military personnel, with more service 
members dying by suicide in 2012 than by com-
bat-related causes (Zoroya, 2012). In response to 
steadily rising suicide rates among military per-
sonnel and veterans, researchers, clinicians, poli-
cy-makers, and military leaders have responded 
with an overwhelming and concerted effort to 
reverse these trends. Despite these considerable 
efforts, however, no evidence of effectiveness 
has been observed to date, resulting in consider-
able frustration for all involved. Although specif-
ic reasons explaining the lack of success to date 
are not yet known, it has been noted that most 
suicide prevention efforts used with military and 
veteran populations lack cultural relevance and 
do not incorporate several critical characteristics 
of the military culture that can create unique 
challenges from a suicide prevention perspective 
(Bryan et al., 2012). For instance, mental tough-
ness and suppressive coping, fearlessness of 
death, and self-sacrifice are qualities that are val-
ued in the military, but can serve as barriers to 
traditional prevention efforts. 
 
The military culture values strength, resilience, 
courage, and personal sacrifice when faced with 
adversity. Weakness is not tolerated, and service 
members are expected to ?shake it off? or ?suck 
it up? when experiencing problems or illness. 
1
Suppression and avoidance have long been 
linked to mental health problems and emotional 
distress (Hayes et al., 1996), including suicidal 
ideation and suicide attempts (Najmi et al., 
2007).  Yet despite this ?common sense? piece of 
knowledge, suppression and avoidance are none-
theless taught and reinforced within the military 
culture as a coping strategy because, in the short 
term after a stressful or traumatic event, suppres-
sion can actually reduce emotional distress and 
foster adaptation to extreme adversity (Beck et 
al., 2006; Bonanno 2004). This is especially rel-
evant in combat situations, when natural grief 
responses may need to be suppressed to sustain 
adequate performance and achieve mission ob-
jectives. For example, crying in the midst of a 
fire fight is not adaptive or conducive to survival, 
and therefore must be stifled. Suppression and 
avoidance therefore presents the first paradox for 
understanding military and veteran suicide: a 
skill that is adaptive and useful in the short-term 
following a traumatic event can be detrimental 
and impair adaptive functioning in the long-term.   
 
Military personnel are also explicitly trained to 
overcome their fear of injury and death, typically 
through repeated exposure to scenarios and envi-
ronments that increasingly mimic actual combat 
situations, which habituates them to fear and 
eventually replaces this fear with exhilaration 
and/or other positive emotions (i.e., the oppo-
nent-process). Indeed, greater exposure to com-
bat, especially combat marked by higher levels 
of violence and injury, are associated with less 
fear of death among military personnel (Bryan 
and Cukrowicz, 2011; Bryan et al. 2011). Fear-
lessness is an essential quality of a service mem-
ber; retreating from danger and life-threatening 
situations are generally not conducive to an ef-
fective fighting force. Yet at the same time, fear 
of death is a well-known protective factor for 
suicide, given that individuals who are afraid to 
die are unlikely to attempt suicide, and fearless-
ness is associated with more severe levels of sui-
cide risk among military personnel relative to 
civilian samples, and is associated with increased 
severity of suicide risk among military personnel 
(Bryan et al., 2010). Consequently, fearlessness 
about death paradoxically serves both as a neces-
sary strength and asset for military personnel, yet 
also serves as a risk factor for suicide. 
 
The military culture also places a premium on 
selflessness in the service of a higher good, and 
does not necessarily view life as the highest good 
in every situation. In the military, one?s life 
might actually be viewed as subordinate to other, 
higher ?goods? such as the well-being of others 
or ideals and principles such as freedom and jus-
tice. Laying down one?s life for a greater good is 
widely considered to be one of the highest hon-
ors a service member can achieve. A considera-
ble amount of research has converged on a very 
suicide-specific and dangerous thought process 
for suicidal behavior: perceived burdensomeness. 
Perceived burdensomeness entails the mistaken 
perception that ?others would be better off with-
out me? or that one?s death is of greater value 
than one?s life. Perceived burdensomeness and 
self-sacrifice are in many ways opposite sides of 
the same coin, and it is not yet clear how or when 
perceived burdensomeness (?taking? one?s life) 
becomes mistaken for self-sacrifice (?giving? 
one?s life) among military personnel and veter-
ans.  
 
These characteristics simultaneously function as 
an asset (in terms of military performance) and 
as a liability (in terms of suicide prevention) for 
military personnel and veterans, thereby creating 
a paradox for suicide prevention in military and 
veteran populations, and contributing directly to 
mental health stigma. Furthermore, the values of 
the military culture are generally at odds with the 
values and ideals of mental health systems, 
which value emotional vulnerability and help-
seeking, and focus on deficiencies and clinical 
disorders, thereby reinforcing stigma even more. 
In essence, traditional prevention approaches 
have conceptualized suicide in a way that con-
flicts with the core identity and values of military 
personnel and veterans. To be effective, suicide 
prevention efforts must be culturally-relevant 
and integrate these values and ideals of military 
personnel and veterans. 
 
3    Related work 
 
In addition to the work related to military culture 
issues discussed in section 2, there are many lin-
guistic approaches to analyzing suicide risk 
(Barak and Miron, 2005; Jones and Bennell, 
2007; Lester, 2008a; Lester, 2008b; Lester, 
2010a; Lester, 2010b; Lester et al., 2010; Lester 
and McSwain, 2010; Stirman and Pennebaker, 
2001).  In 2011, one of the Informatics for Inte-
grating Biology & the Bedside (i2b2) shared 
tasks was a sentiment analysis task to identify 
emotion in suicide notes (Combined Objective, 
2011).  Of this literature only Barak and Miron 
2
(2005) considers online text.  Most other text 
analysis suicide research concerns analysis of 
suicide notes.  There are studies of the writings 
of suicidal poets (Lester and McSwain, 2010; 
Stirman and Pennebaker, 2001) and studies in-
volving distinguishing genuine and simulated 
suicide notes (Jones and Bennell, 2007; Lester, 
2010a). 
4     The Durkheim Project 
4.1 Overview 
The Durkheim Project consists of three phases.  
During the first phase, described in section 4.2, a 
clinician?s dashboard was built and a Veterans 
Affairs (VA) predictive risk medical records 
study was completed, based on an analysis of the 
narrative, or free text, portions of VA medical 
records.  Also during the first phase, the initial 
software infrastructure to collect and analyze the 
social media data for phase two, was designed 
and implemented.  During the second phase, sec-
tion 4.3, now underway, opt-in social media 
postings are being collected and will be ana-
lyzed.  During the third phase, section 4.4, a pilot 
program will isolate serious suicide risk for indi-
viduals in real-time, and develop a prediction 
triage model for improved suicide intervention 
 
4.2 Phase 1:  Veteran Affairs medical records 
study 
 
During phase 1 linguistics-driven prediction 
models were developed to estimate the risk of 
suicide. These models were generated from un-
structured clinical notes taken from a national 
sample of United States VA medical records. 
The protocol for this study was approved by the 
Institutional Review Board (IRB) of the VA 
Medical Center, where the study was conducted. 
We created three matched cohorts: veterans who 
completed suicide, veterans who used mental 
health services and did not complete suicide, and 
veterans who did not use mental health services 
and did not complete suicide during the observa-
tion period (n = 70 in each group). From the clin-
ical notes, we generated datasets of single key-
words and multi-word phrases, and constructed 
prediction models using a supervised machine-
learning algorithm based on a genetic program-
ming framework, MOSES (Looks, 2006, 2007; 
Goertzel et al., 2013). MOSES can be described 
as a variant of a decision-tree forest, with certain 
genetic and maximum entropy techniques mixed 
in:  maximum entropy to apply pressure to min-
imize tree size and genetic to ensure tree species 
diversity.  In our prior research we have found 
that MOSES consistently outperforms standard 
text classification approaches, such as Support 
Vector Machines (SVMs).  The primary hyper-
parameter that we used was the dynamic feature 
size.  The resulting inference accuracy was at 
first 65% and then consistently 67% or more. 
This was the prediction accuracy for assigning a 
patient to the correct cohort.  These data suggest 
that computerized text analytics can be applied to 
unstructured sections of medical records to esti-
mate the risk of suicide (Poulin et al. 2014). The 
resulting system could potentially allow clini-
cians to screen seemingly healthy patients at the 
primary care level, and to continuously evaluate 
suicide risk among psychiatric patients. 
4.3 Phase 2:  Predicting risk with opt-in social 
media postings 
Although data collection and analysis for phase 2 
is just beginning, the software development re-
quired for this data collection and analysis was 
completed during phase 1.  A phase 2 protocol 
for collecting and analyzing opt-in social media 
postings and presenting predictions to clinicians 
via the Durkheim Project?s Clinicians? dashboard 
has also been approved by our IRB.  When the 
system is fully operational, a clinician will see 
predictive models of suicide risk for a patient 
constructed from the patient?s medical records 
and the patient?s opt-in social media postings.  
Subjects are being recruited via targeted efforts.  
Subjects will be recruited through our collabora-
tion with Facebook (PR Newswire 2013).  A Fa-
cebook pop-up window will be used to recruit 
people that Facebook has identified as being mil-
itary personnel or veterans. 
4.4 Phase 3:  Intervention 
For phase 3, a protocol has been completed, 
which will soon be submitted to a final IRB.  
This protocol includes an unblinded, 3-cohort 
design, for a pilot program, which proposes to 
isolate serious suicide risks for individuals in 
real-time and to develop a prediction triage mod-
el for improved suicide intervention. Plans are to 
use and improve upon the linguistically-based 
prediction capabilities of the model developed 
during phase 1.  The phase 1 retrospective study 
was able to predict with limited accuracy before 
suicides occurred. The theoretic assumption is 
that wording chosen by those at risk will vary at 
different stages of risk. By building from ongo-
ing observations from the phase 2 study and 
3
feedback obtained during the conduct of the 
phase 3 study, the aim is to adjust the linguistics-
driven model to predict suicide risk within the 
critical period for interventions of various levels 
of severity. 
 
In this protocol, ongoing monitoring of the net-
work will allow continuous updating and change 
in value of risk alert levels among the green-to-
red color coding. When the predictive system 
detects postings that indicate a certain threshold 
level of potential suicide risk, risk alerts are trig-
gered in real-time and sent to either a monitoring 
clinician or a pre-identified buddy monitor, or to 
an automated system, which will generate sup-
portive messages that are sent to the at-risk indi-
vidual. 
 
To better characterize the risk for the population 
of active-duty military and veterans, the analysis 
for this study will be limited to the primary par-
ticipants. These primary participants may be 
newly recruited via the dedicated Facebook and 
mobile applications or, through that same dedi-
cated application, from among those already par-
ticipating in the phase 2 study. In either case, all 
primary participants must provide informed con-
sent for this specific study. That is, those already 
involved in the phase 2 study must provide sepa-
rate consent to participate in the phase 3 study.  
However, outside of the context of this study, the 
computerized intervention will be open to mem-
bers of the general public who might wish to take 
advantage of the program?s intervention poten-
tial.  Primary participants are active duty U.S. 
military or veterans with English as a primary or 
secondary language, who agree to post to social 
media using English.  The age limit for primary 
participants in the phase 3 study, as with phase 2 
study, targets the age group most likely to active-
ly use social media, i.e., those between the ages 
of 18 and 45. 
5    Results 
So far results are only available for the phase 
1 study.  For single-word models, the predic-
tive accuracy was approximately 59% (the 
average for 100 models), and scores for indi-
vidual candidate models ranged from 46-
67%. Because our training sets are balanced, 
we have used accuracy as a surrogate for 
precision and recall.  Accuracy was comput-
ed using five-way cross-validation.  Models 
that used certain word pairs had significantly 
better scores than single-word models, 
though they are far less human readable. The 
phrases ?negative assessment for PTSD? and 
?positive assessment for PTSD" carry differ-
ent meanings. This phrase-based approach 
was more accurate than a single-word ap-
proach. For pre-selected word pairs, the in-
dividual model scores ranged from 52-69%, 
with an average of 64% (for 100 models).  In 
the final experiments, the combined Cohorts 
?1v2v3 classifier? had a peak performance of 
70%, and an average performance of 67%.  
6    Discussion 
Our analyses were successful at determining 
useful text-based signals of suicide risk. We 
obtained accuracies of greater than 60% for 
ensemble averages of 100 models, and our 
individual model accuracies reached 67-
69%. Given the small size of the dataset and 
the fragmentary nature of the clinical notes, 
this performance level represents a signifi-
cant achievement. For a classifier, these re-
sults represent a statistically significant ?sig-
nal?. Meanwhile, we showed that, methodo-
logically, word pairs are more useful than 
single words for model construction on elec-
tronic medical record (EMR) data.   Fur-
thermore, the predictive feature words that 
distinguished each group were highly reveal-
ing, especially for the suicidal cohort, and 
were consistent with the existing medical 
literature on suicide.  Many medical condi-
tions have been associated with an increased 
risk for suicide, but these conditions have 
generally not been included in suicide risk 
assessment tools. These conditions include 
gastrointestinal conditions, cardiopulmonary 
conditions, oncologic conditions, and pain 
conditions. Also, some research has emerged 
that links care processes to suicide risk. The 
word "integrated" emerged as a key term in 
our study and is also reflected in the inte-
grated care literature (Bauer et al., 2013). 
Although the text on which our predictive 
model was based for the phase 1 medical 
records study was text written by a physician 
or other healthcare provider, our hypothesis 
4
is that some of the highly predictive features 
learned during phase 1 will carry over to the 
predictive modeling of opt-in social media 
postings during phase 2.  This text is written 
by the patient.  However, we expect that 
some of the features, or concepts, will be the 
same due to the ability to do software based 
synonym matches  Additionally, a physician 
or other healthcare worker may sometimes 
quote or paraphrase what a patient said when 
adding a note to the clinical record.  A key 
predictive feature, such as the word ?anxie-
ty,? may be used either by a clinician or a 
patient.  We believe that the use of special-
ized text-analytic resources such as linguistic 
inquiry and word count (LIWC) would also 
help improve our results.  Some preliminary 
results have been obtained using LIWC on 
our dataset. 
In future research we plan to scale up the 
phase 1 medical records study from our cur-
rent study where each cohort had 70 subjects 
to a study, using the same protocol, with at 
least 1000 subjects in each cohort.  We also 
plan to transfer the predictive model built 
from the phase 1 study to the analysis of 
phase 2 opt-in social media postings.  Once 
our phase 3 protocol has IRB approval, we 
plan to begin the phase 3 of the Durkheim 
Project, informed by the results, and on-
going follow-on research, of our phase 1 and 
2 studies.  In our future research we plan to 
use additional features from the structured 
portions of the medical record, as well as to 
use LIWC.  In both our medical records and 
social media research we plan to use tem-
poral analysis. 
7     Conclusion 
Although the phase 1 study was successful in 
distinguishing the cohort of completed sui-
cides both from the control group cohort and 
the psychiatric cohort, it was difficult to dis-
tinguish text based noise from signal with 
high accuracy in our initial results.  We ex-
pect that our planned follow-on study with 
1000 subjects in each cohort will have much 
less problem in distinguishing signal from 
noise.  Suicide risk prediction is a very diffi-
cult problem.  We believe that studies such 
as our phases 1 and 2 studies, which use su-
pervised machine learning techniques, can 
uncover predictive risk factors that are not 
clearly understood by the medical communi-
ty.  At the same time, we also believe that 
more effective suicide risk prediction sys-
tems can be built based on the integration of 
machine learning methods and the expertise 
of suicidologists.  In particular, building an 
understanding of military culture into our 
methods will be important. 
References 
Amy M. Bauer, Ya-Fen Chan, Hsiang Huang, Steven 
Vannoy, Jurgen Un?tzer.  2013.  Characteristics, 
Management, and Depression Outcomes of Pri-
mary Care Patients Who Endorse Thoughts of 
Death or Suicide on the PHQ-9. J Gen Intern 
Med. Mar; 28(3):363-9. doi: 10.1007/s11606-
012-2194-2. Epub 2012 Aug 31. 
Azy Barak, Ofra Miron.  2005.  Writing Characteris-
tics of Suicidal People on the Internet:  A Psycho-
logical Investigation of Emerging Social Envi-
ronments. Suicide and Life-Threatening Behavior 
35(5) October. 
Ben Goertzel, Nil Geisweiller, Pennachin, Cassio.  
2013. Integrating Feature Selection into Program 
Learning. Proceedings of AGI-13, Springer. 
http://goertzel.org/agi-13/FS-MOSES_v1.pdf. 
Chris Poulin, Brian Shiner, Paul Thompson, Linas 
Vepstas,Yinong Young-Xu, Benjamin Goertzel, 
Bradley Watts, Laura Flashman, Thomas McAl-
lister.  2014.  Predicting the Risk of Suicide by 
Analyzing the Text of Clinical Notes. PLoS ONE 
9(1): e85733. doi:10.1371/journal.pone.0085733. 
 Combined Objective & Subjective Shared Task An-
nouncement:  Call for Participation.  2011. 
https://www.i2b2.org/NLP/Coreference/Call.php. 
Craig J. Bryan, Kelly C. Cukrowicz.  2011. Associa-
tions between types of combat violence and the ac-
quired capability for suicide. Suicide and Life-
Threatening Behavior, 41,126-136. 
Craig J. Bryan, Kelly C. Cukrowicz, Christopher L. 
West, Chad E. Morrow.  2010. Combat experience 
and the acquired capability for suicide. Journal of 
Clinical Psychology, 66, 1044-1056. 
Craig J. Bryan, Keith W. Jennings, David A. Jobes, 
John C. Bradley.  2012.  Understanding and pre-
venting military suicide. Archives of Suicide Re-
search, 16, 95-110. 
Craig J. Bryan, Chad E. Morrow, Michael D. Anestis, 
Thomas E. Joiner.  2010.  A preliminary test of the 
5
interpersonal-psychological theory of suicidal be-
havior in a military sample. Personality and Indi-
vidual Differences, 48, 347-350. 
David Lester.  2008a. Computer Analysis of the Con-
tent of Suicide Notes from Men and Women.  Psy-
chological Reports, 102, 575-576. 
David Lester.  2008b. Differences Between Genuine 
and Simulated Suicide Notes.  Psychological Re-
ports, 103, 527-528. 
David Lester.  2010a. Linguistic Analysis of a Blog 
from a Murder-Suicide.  Psychological Reports, 
106(2): 342. 
 David Lester.  2010b. The Final Hours:  A Linguistic 
Analysis of the Final Words of a Suicide.  Psycho-
logical Reports, 106(3): 791-797. 
David Lester, Janet Haines, Christopher Williams.  
2010.  Content Differences in Suicide Notes by 
Sex, Age, and Method:  A Study of Australian Sui-
cide Notes.  Psychological Reports, 106(2): 475-
476. 
David Lester, Stephanie McSwain.  2010.  Poems by 
a Suicide:  Sara Teasdale.  Psychological Reports, 
106(3): 811-812. 
George Bonanno.  2004. Loss, trauma, and human 
resilience: Have we underestimated the human ca-
pacity to thrive after extremely aversive events? 
American Psychologist, 59, 20-28. 
Gregg Zoroya. Army, Navy suicides at record high.  
2012.   USA Today. 
http://www.usatoday.com/story/news/nation/2012/
11/18/navy-suicides-army/1702403/, November 
18. 
Health Promotion Risk Reduction Suicide Prevention.  
2010. U.S. ARMY HP/RR/SP REPORT: 
http://usarmy.vo.llnwd.net/e1/HPRRSP/HP-RR-
SPReport2010_v00.pdf. 
J. Gayle Beck, Berglind Gudmundsdottir, Sarah 
Palyo, Luana M. Miller, DeMond Grant.  2006.  
Rebound effects following deliberate thought sup-
pression: does PTSD make a difference? Behavior 
Therapy, 37, 170-180. 
LIWC.  2014.  Linguistic Inquery and Word Count.  
http://www.liwc.net/ Accessed 28 April 2014. 
Moshe Looks. 2006.  Competent Program Evolution. 
PhD thesis, Washington University. 
 Moshe Looks. 2007.  Meta-optimizing semantic evo-
lutionary search. In: Lipson, H. (ed.), Genetic and 
Evolutionary Computation Conference, GECCO 
2007, Proceedings, London, England, UK, July 7-
11, p. 626. 
Natalie J. Jones, Craig Bennell.  2007.  The Develop-
ment and Validation of Statistical Prediction Rules 
for Discriminating Between Genuine and Simulat-
ed Suicide Notes.  Archives of Suicide Research, 
11:21-233. 
PR Newswire.  2013 
http://www.prnewswire.com/news-releases/the-
durkheim-project-will-analyze-opt-in-data-from-
veterans-social-media-and-mobile-content----
seeking-real-time-predictive-analytics-for-suicide-
risk-213922041.html Accessed 28 April 2014. 
Sadia Najmi, Daniel M. Wegner, and Matthew K. 
Nock. 2007.  Thought suppression and self-
injurious thoughts and behaviors. Behaviour Re-
search and Therapy, 45, 1957-1965. 
Shannon W. Stirman, James W. Pennebaker.  2001.  
Word Use in the Poetry of Suicidal and Nonsuicid-
al Poets. Psychosomatic Medicine 63:517-522. 
Steven Hayes, Kelly G. Wilson, Elizabeth V. Gifford, 
Victoria M. Follette, and Kirk Strosahl.  1996.  Ex-
periential avoidance and behavioral disorders: A 
functional dimensional approach to diagnosis and 
treatment. Journal of Consulting and Clinical Psy-
chology, 64, 1152-1168. 
Treating Soldiers with Brain Injuries.  2010. Diane 
Rehm, NPR: June 24. 
6
