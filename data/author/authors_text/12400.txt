Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 459?467,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Employing Topic Models for Pattern-based Semantic Class Discovery 
 
 
Huibin Zhang1*     Mingjie Zhu2*     Shuming Shi3     Ji-Rong Wen3 
1Nankai University 
2University of Science and Technology of China 
3Microsoft Research Asia 
{v-huibzh, v-mingjz, shumings, jrwen}@microsoft.com 
 
  
 
Abstract? 
 
A semantic class is a collection of items 
(words or phrases) which have semantically 
peer or sibling relationship. This paper studies 
the employment of topic models to automati-
cally construct semantic classes, taking as the 
source data a collection of raw semantic 
classes (RASCs), which were extracted by ap-
plying predefined patterns to web pages. The 
primary requirement (and challenge) here is 
dealing with multi-membership: An item may 
belong to multiple semantic classes; and we 
need to discover as many as possible the dif-
ferent semantic classes the item belongs to. To 
adopt topic models, we treat RASCs as ?doc-
uments?, items as ?words?, and the final se-
mantic classes as ?topics?. Appropriate 
preprocessing and postprocessing are per-
formed to improve results quality, to reduce 
computation cost, and to tackle the fixed-k 
constraint of a typical topic model. Experi-
ments conducted on 40 million web pages 
show that our approach could yield better re-
sults than alternative approaches. 
1 Introduction 
Semantic class construction (Lin and Pantel, 
2001; Pantel and Lin, 2002; Pasca, 2004; Shinza-
to and Torisawa, 2005; Ohshima et al, 2006) 
tries to discover the peer or sibling relationship 
among terms or phrases by organizing them into 
semantic classes. For example, {red, white, 
black?} is a semantic class consisting of color 
instances. A popular way for semantic class dis-
covery is pattern-based approach, where prede-
fined patterns (Table 1) are applied to a 
                                                   
? This work was performed when the authors were interns at 
Microsoft Research Asia 
collection of web pages or an online web search 
engine to produce some raw semantic classes 
(abbreviated as RASCs, Table 2). RASCs cannot 
be treated as the ultimate semantic classes, be-
cause they are typically noisy and incomplete, as 
shown in Table 2. In addition, the information of 
one real semantic class may be distributed in lots 
of RASCs (R2 and R3 in Table 2). 
 
Type Pattern 
SENT NP {, NP}*{,} (and|or) {other} NP 
TAG <UL>  <LI>item</LI>  ?  <LI>item</LI>  </UL> 
TAG <SELECT> <OPTION>item?<OPTION>item </SELECT> 
* SENT: Sentence structure patterns; TAG: HTML Tag patterns 
Table 1. Sample patterns 
 
R1: {gold, silver, copper, coal, iron, uranium} 
R2: {red, yellow, color, gold, silver, copper} 
R3: {red, green, blue, yellow} 
R4: {HTML, Text, PDF, MS Word, Any file type} 
R5: {Today, Tomorrow, Wednesday, Thursday, Friday, 
Saturday, Sunday} 
R6: {Bush, Iraq, Photos, USA, War} 
Table 2. Sample raw semantic classes (RASCs) 
 
This paper aims to discover high-quality se-
mantic classes from a large collection of noisy 
RASCs. The primary requirement (and chal-
lenge) here is to deal with multi-membership, i.e., 
one item may belong to multiple different seman-
tic classes. For example, the term ?Lincoln? can 
simultaneously represent a person, a place, or a 
car brand name. Multi-membership is more pop-
ular than at a first glance, because quite a lot of 
English common words have also been borrowed 
as company names, places, or product names. 
For a given item (as a query) which belongs to 
multiple semantic classes, we intend to return the 
semantic classes separately, rather than mixing 
all their items together. 
Existing pattern-based approaches only pro-
vide very limited support to multi-membership. 
For example, RASCs with the same labels (or 
hypernyms) are merged in (Pasca, 2004) to gen-
459
erate the ultimate semantic classes. This is prob-
lematic, because RASCs may not have (accurate) 
hypernyms with them. 
In this paper, we propose to use topic models 
to address the problem. In some topic models, a 
document is modeled as a mixture of hidden top-
ics. The words of a document are generated ac-
cording to the word distribution over the topics 
corresponding to the document (see Section 2 for 
details). Given a corpus, the latent topics can be 
obtained by a parameter estimation procedure. 
Topic modeling provides a formal and conve-
nient way of dealing with multi-membership, 
which is our primary motivation of adopting top-
ic models here. To employ topic models, we treat 
RASCs as ?documents?, items as ?words?, and 
the final semantic classes as ?topics?. 
There are, however, several challenges in ap-
plying topic models to our problem. To begin 
with, the computation is intractable for 
processing a large collection of RASCs (our da-
taset for experiments contains 2.7 million unique 
RASCs extracted from 40 million web pages). 
Second, typical topic models require the number 
of topics (k) to be given. But it lacks an easy way 
of acquiring the ideal number of semantic classes 
from the source RASC collection. For the first 
challenge, we choose to apply topic models to 
the RASCs containing an item q, rather than the 
whole RASC collection. In addition, we also per-
form some preprocessing operations in which 
some items are discarded to further improve effi-
ciency. For the second challenge, considering 
that most items only belong to a small number of 
semantic classes, we fix (for all items q) a topic 
number which is slightly larger than the number 
of classes an item could belong to. And then a 
postprocessing operation is performed to merge 
the results of topic models to generate the ulti-
mate semantic classes. 
Experimental results show that, our topic 
model approach is able to generate higher-quality 
semantic classes than popular clustering algo-
rithms (e.g., K-Medoids and DBSCAN). 
We make two contributions in the paper: On 
one hand, we find an effective way of construct-
ing high-quality semantic classes in the pattern-
based category which deals with multi-
membership. On the other hand, we demonstrate, 
for the first time, that topic modeling can be uti-
lized to help mining the peer relationship among 
words. In contrast, the general related relation-
ship between words is extracted in existing topic 
modeling applications. Thus we expand the ap-
plication scope of topic modeling. 
2 Topic Models 
In this section we briefly introduce the two wide-
ly used topic models which are adopted in our 
paper. Both of them model a document as a mix-
ture of hidden topics. The words of every docu-
ment are assumed to be generated via a 
generative probability process. The parameters of 
the model are estimated from a training process 
over a given corpus, by maximizing the likelih-
ood of generating the corpus. Then the model can 
be utilized to inference a new document. 
pLSI: The probabilistic Latent Semantic In-
dexing Model (pLSI) was introduced in Hof-
mann (1999), arose from Latent Semantic 
Indexing (Deerwester et al, 1990). The follow-
ing process illustrates how to generate a docu-
ment d in pLSI: 
1. Pick a topic mixture distribution ?(? |?). 
2. For each word wi in d 
a. Pick a latent topic z with the probabil-
ity ?(?|?) for wi 
b. Generate wi with probability ?(?? |?) 
So with k latent topics, the likelihood of gene-
rating a document d is 
 ?(?) =  ? ?? ? ?(?|?)
??
 (2.1) 
LDA (Blei et al, 2003): In LDA, the topic 
mixture is drawn from a conjugate Dirichlet prior 
that remains the same for all documents (Figure 
1). The generative process for each document in 
the corpus is, 
1. Choose document length N from a Pois-
son distribution Poisson(?). 
2. Choose ?  from a Dirichlet distribution 
with parameter ?. 
3. For each of the N words wi. 
a. Choose a topic z from a Multinomial 
distribution with parameter ?. 
b. Pick a word wi from ? ??  ?,? . 
So the likelihood of generating a document is 
 ?(?) =  ?(?|?)
?
  ?(?|?)? ?? ?,? ??
??
 (2.2) 
 
 
Figure 1. Graphical model representation of LDA, 
from Blei et al (2003) 
 
w? z?
?
N
M
460
3 Our Approach 
The source data of our approach is a collection 
(denoted as CR) of RASCs extracted via applying 
patterns to a large collection of web pages. Given 
an item as an input query, the output of our ap-
proach is one or multiple semantic classes for the 
item. To be applicable in real-world dataset, our 
approach needs to be able to process at least mil-
lions of RASCs. 
3.1 Main Idea 
As reviewed in Section 2, topic modeling pro-
vides a formal and convenient way of grouping 
documents and words to topics. In order to apply 
topic models to our problem, we map RASCs to 
documents, items to words, and treat the output 
topics yielded from topic modeling as our seman-
tic classes (Table 3). The motivation of utilizing 
topic modeling to solve our problem and building 
the above mapping comes from the following 
observations. 
1) In our problem, one item may belong to 
multiple semantic classes; similarly in topic 
modeling, a word can appear in multiple top-
ics. 
2) We observe from our source data that 
some RASCs are comprised of items in mul-
tiple semantic classes. And at the same time, 
one document could be related to multiple 
topics in some topic models (e.g., pLSI and 
LDA). 
 
Topic modeling Semantic class construction 
word item (word or phrase) 
document RASC 
topic semantic class 
Table 3. The mapping from the concepts in topic 
modeling to those in semantic class construction 
 
Due to the above observations, we hope topic 
modeling can be employed to construct semantic 
classes from RASCs, just as it has been used in 
assigning documents and words to topics. 
There are some critical challenges and issues 
which should be properly addressed when topic 
models are adopted here. 
Efficiency: Our RASC collection CR contains 
about 2.7 million unique RASCs and 26 million 
(1 million unique) items. Building topic models 
directly for such a large dataset may be computa-
tionally intractable. To overcome this challenge, 
we choose to apply topic models to the RASCs 
containing a specific item rather than the whole 
RASC collection. Please keep in mind that our 
goal in this paper is to construct the semantic 
classes for an item when the item is given as a 
query. For one item q, we denote CR(q) to be all 
the RASCs in CR containing the item. We believe 
building a topic model over CR(q) is much more 
effective because it contains significantly fewer 
?documents?, ?words?, and ?topics?. To further 
improve efficiency, we also perform preprocess-
ing (refer to Section 3.4 for details) before build-
ing topic models for CR(q), where some low-
frequency items are removed. 
Determine the number of topics: Most topic 
models require the number of topics to be known 
beforehand1. However, it is not an easy task to 
automatically determine the exact number of se-
mantic classes an item q should belong to. Ac-
tually the number may vary for different q. Our 
solution is to set (for all items q) the topic num-
ber to be a fixed value (k=5 in our experiments) 
which is slightly larger than the number of se-
mantic classes most items could belong to. Then 
we perform postprocessing for the k topics to 
produce the final properly semantic classes. 
In summary, our approach contains three 
phases (Figure 2). We build topic models for 
every CR(q), rather than the whole collection CR. 
A preprocessing phase and a postprocessing 
phase are added before and after the topic model-
ing phase to improve efficiency and to overcome 
the fixed-k problem. The details of each phase 
are presented in the following subsections. 
 
 
Figure 2. Main phases of our approach 
 
3.2 Adopting Topic Models 
For an item q, topic modeling is adopted to 
process the RASCs in CR(q) to generate k seman-
tic classes. Here we use LDA as an example to 
                                                   
1 Although there is study of non-parametric Bayesian mod-
els (Li et al, 2007) which need no prior knowledge of topic 
number, the computational complexity seems to exceed our 
efficiency requirement and we shall leave this to future 
work. 
R580 
R1 
R2 
CR 
Item q 
Preprocessing 
?400
?  
?1
? 
?2
? 
T5 
T1 
T2 
C3 
C1 
C2 
Topic  
modeling 
Postprocessing 
T3 
T4 
CR(q) 
461
illustrate the process. The case of other genera-
tive topic models (e.g., pLSI) is very similar. 
According to the assumption of LDA and our 
concept mapping in Table 3, a RASC (?docu-
ment?) is viewed as a mixture of hidden semantic 
classes (?topics?). The generative process for a 
RASC R in the ?corpus? CR(q) is as follows, 
1) Choose a RASC size (i.e., the number of 
items in R): NR ~ Poisson(?). 
2) Choose a k-dimensional vector ??  from a 
Dirichlet distribution with parameter ?. 
3) For each of the NR items an: 
a) Pick a semantic class ??  from a mul-
tinomial distribution with parameter 
?? . 
b) Pick an item an from ?(?? |?? ,?) , 
where the item probabilities are pa-
rameterized by the matrix ?. 
There are three parameters in the model: ? (a 
scalar), ?  (a k-dimensional vector), and ?  (a 
? ? ? matrix where V is the number of distinct 
items in CR(q)). The parameter values can be ob-
tained from a training (or called parameter esti-
mation) process over CR(q), by maximizing the 
likelihood of generating the corpus. Once ?  is 
determined, we are able to compute ?(?|?,?), 
the probability of item a belonging to semantic 
class z. Therefore we can determine the members 
of a semantic class z by selecting those items 
with high ? ? ?,?  values. 
The number of topics k is assumed known and 
fixed in LDA. As has been discussed in Section 
3.1, we set a constant k value for all different 
CR(q). And we rely on the postprocessing phase 
to merge the semantic classes produced by the 
topic model to generate the ultimate semantic 
classes. 
When topic modeling is used in document 
classification, an inference procedure is required 
to determine the topics for a new document. 
Please note that inference is not needed in our 
problem. 
One natural question here is: Considering that 
in most topic modeling applications, the words 
within a resultant topic are typically semantically 
related but may not be in peer relationship, then 
what is the intuition that the resultant topics here 
are semantic classes rather than lists of generally 
related words? The magic lies in the ?docu-
ments? we used in employing topic models. 
Words co-occurred in real documents tend to be 
semantically related; while items co-occurred in 
RASCs tend to be peers. Experimental results 
show that most items in the same output seman-
tic class have peer relationship. 
It might be noteworthy to mention the exchan-
geability or ?bag-of-words? assumption in most 
topic models. Although the order of words in a 
document may be important, standard topic mod-
els neglect the order for simplicity and other rea-
sons2. The order of items in a RASC is clearly 
much weaker than the order of words in an ordi-
nary document. In some sense, topic models are 
more suitable to be used here than in processing 
an ordinary document corpus. 
3.3 Preprocessing and Postprocessing 
Preprocessing is applied to CR(q) before we build 
topic models for it. In this phase, we discard 
from all RASCs the items with frequency (i.e., 
the number of RASCs containing the item) less 
than a threshold h. A RASC itself is discarded 
from CR(q) if it contains less than two items after 
the item-removal operations. We choose to re-
move low-frequency items, because we found 
that low-frequency items are seldom important 
members of any semantic class for q. So the goal 
is to reduce the topic model training time (by 
reducing the training data) without sacrificing 
results quality too much. In the experiments sec-
tion, we compare the approaches with and with-
out preprocessing in terms of results quality and 
efficiency. Interestingly, experimental results 
show that, for some small threshold values, the 
results quality becomes higher after preprocess-
ing is performed. We will give more discussions 
in Section 4. 
In the postprocessing phase, the output seman-
tic classes (?topics?) of topic modeling are 
merged to generate the ultimate semantic classes. 
As indicated in Sections 3.1 and 3.2, we fix the 
number of topics (k=5) for different corpus CR(q) 
in employing topic models. For most items q, 
this is a larger value than the real number of se-
mantic classes the item belongs to. As a result, 
one real semantic class may be divided into mul-
tiple topics. Therefore one core operation in this 
phase is to merge those topics into one semantic 
class. In addition, the items in each semantic 
class need to be properly ordered. Thus main 
operations include, 
1) Merge semantic classes 
2) Sort the items in each semantic class 
Now we illustrate how to perform the opera-
tions. 
Merge semantic classes: The merge process 
is performed by repeatedly calculating the simi-
                                                   
2 There are topic model extensions considering word order 
in documents, such as Griffiths et al (2005). 
462
larity between two semantic classes and merging 
the two ones with the highest similarity until the 
similarity is under a threshold. One simple and 
straightforward similarity measure is the Jaccard 
coefficient, 
 ??? ?1 ,?2 =
 ?1 ? ?2 
 ?1 ? ?2 
 (3.1) 
where ?1 ? ?2  and ?1 ? ?2  are respectively the 
intersection and union of semantic classes C1 and 
C2. This formula might be over-simple, because 
the similarity between two different items is not 
exploited. So we propose the following measure, 
 ??? ?1 ,?2 =
  ???(?, ?)???2???1
 ?1 ?  ?2 
 (3.2) 
where |C| is the number of items in semantic 
class C, and sim(a,b) is the similarity between 
items a and b, which will be discussed shortly. In 
Section 4, we compare the performance of the 
above two formulas by experiments. 
Sort items: We assign an importance score to 
every item in a semantic class and sort them ac-
cording to the importance scores. Intuitively, an 
item should get a high rank if the average simi-
larity between the item and the other items in the 
semantic class is high, and if it has high similari-
ty to the query item q. Thus we calculate the im-
portance of item a in a semantic class C as 
follows, 
 ? ?|? = ? ?sim(a,C)+(1-?) ?sim(a,q) (3.3) 
where ? is a parameter in [0,1], sim(a,q) is the 
similarity between a and the query item q, and 
sim(a,C) is the similarity between a and C, calcu-
lated as, 
 ??? ?,? =
 ???(?, ?)???
 ? 
 (3.4) 
Item similarity calculation: Formulas 3.2, 
3.3, and 3.4 rely on the calculation of the similar-
ity between two items. 
One simple way of estimating item similarity 
is to count the number of RASCs containing both 
of them. We extend such an idea by distinguish-
ing the reliability of different patterns and pu-
nishing term similarity contributions from the 
same site. The resultant similarity formula is, 
 ???(?,?) = log(1 + ?(?(?? ,? ))
??
?=1
)
?
?=1
 (3.5) 
where Ci,j is a RASC containing both a and b, 
P(Ci,j) is the pattern via which the RASC is ex-
tracted, and w(P) is the weight of pattern P. As-
sume all these RASCs belong to m sites with Ci,j 
extracted from a page in site i, and ki being the 
number of RASCs corresponding to site i. To 
determine the weight of every type of pattern, we 
randomly selected 50 RASCs for each pattern 
and labeled their quality. The weight of each 
kind of pattern is then determined by the average 
quality of all labeled RASCs corresponding to it. 
The efficiency of postprocessing is not a prob-
lem, because the time cost of postprocessing is 
much less than that of the topic modeling phase. 
3.4 Discussion 
3.4.1 Efficiency of processing popular items 
Our approach receives a query item q from users 
and returns the semantic classes containing the 
query. The maximal query processing time 
should not be larger than several seconds, be-
cause users would not like to wait more time. 
Although the average query processing time of 
our approach is much shorter than 1 second (see 
Table 4 in Section 4), it takes several minutes to 
process a popular item such as ?Washington?, 
because it is contained in a lot of RASCs. In or-
der to reduce the maximal online processing 
time, our solution is offline processing popular 
items and storing the resultant semantic classes 
on disk. The time cost of offline processing is 
feasible, because we spent about 15 hours on a 4-
core machine to complete the offline processing 
for all the items in our RASC collection. 
3.4.2 Alternative approaches 
One may be able to easily think of other ap-
proaches to address our problem. Here we dis-
cuss some alternative approaches which are 
treated as our baseline in experiments. 
RASC clustering: Given a query item q, run a 
clustering algorithm over CR(q) and merge all 
RASCs in the same cluster as one semantic class. 
Formula 3.1 or 3.2 can be used to compute the 
similarity between RASCs in performing cluster-
ing. We try two clustering algorithms in experi-
ments: K-Medoids and DBSCAN. Please note k-
means cannot be utilized here because coordi-
nates are not available for RASCs. One draw-
back of RASC clustering is that it cannot deal 
with the case of one RASC containing the items 
from multiple semantic classes. 
Item clustering: By Formula 3.5, we are able 
to construct an item graph GI to record the 
neighbors (in terms of similarity) of each item. 
Given a query item q, we first retrieve its neigh-
bors from GI, and then run a clustering algorithm 
over the neighbors. As in the case of RASC clus-
tering, we try two clustering algorithms in expe-
riments: K-Medoids and DBSCAN. The primary 
disadvantage of item clustering is that it cannot 
assign an item (except for the query item q) to 
463
multiple semantic classes. As a result, when we 
input ?gold? as the query, the item ?silver? can 
only be assigned to one semantic class, although 
the term can simultaneously represents a color 
and a chemical element. 
4 Experiments 
4.1 Experimental Setup 
Datasets: By using the Open Directory Project 
(ODP3) URLs as seeds, we crawled about 40 mil-
lion English web pages in a breadth-first way. 
RASCs are extracted via applying a list of sen-
tence structure patterns and HTML tag patterns 
(see Table 1 for some examples). Our RASC col-
lection CR contains about 2.7 million unique 
RASCs and 1 million distinct items. 
Query set and labeling: We have volunteers 
to try Google Sets4, record their queries being 
used, and select overall 55 queries to form our 
query set. For each query, the results of all ap-
proaches are mixed together and labeled by fol-
lowing two steps. In the first step, the standard 
(or ideal) semantic classes (SSCs) for the query 
are manually determined. For example, the ideal 
semantic classes for item ?Georgia? may include 
Countries, and U.S. states. In the second step, 
each item is assigned a label of ?Good?, ?Fair?, 
or ?Bad? with respect to each SSC. For example, 
?silver? is labeled ?Good? with respect to ?col-
ors? and ?chemical elements?. We adopt metric 
MnDCG (Section 4.2) as our evaluation metric. 
Approaches for comparison: We compare 
our approach with the alternative approaches dis-
cussed in Section 3.4.2. 
LDA: Our approach with LDA as the topic 
model. The implementation of LDA is based 
on Blei?s code of variational EM for LDA5. 
pLSI: Our approach with pLSI as the topic 
model. The implementation of pLSI is based 
on Schein, et al (2002). 
KMedoids-RASC: The RASC clustering ap-
proach illustrated in Section 3.4.2, with the 
K-Medoids clustering algorithm utilized. 
DBSCAN-RASC: The RASC clustering ap-
proach with DBSCAN utilized. 
KMedoids-Item: The item clustering ap-
proach with the K-Medoids utilized. 
DBSCAN-Item: The item clustering ap-
proach with the DBSCAN clustering algo-
rithm utilized. 
                                                   
3 http://www.dmoz.org 
4 http://labs.google.com/sets 
5 http://www.cs.princeton.edu/~blei/lda-c/ 
K-Medoids clustering needs to predefine the 
cluster number k. We fix the k value for all dif-
ferent query item q, as has been done for the top-
ic model approach. For fair comparison, the same 
postprocessing is made for all the approaches. 
And the same preprocessing is made for all the 
approaches except for the item clustering ones 
(to which the preprocessing is not applicable). 
4.2 Evaluation Methodology 
Each produced semantic class is an ordered list 
of items. A couple of metrics in the information 
retrieval (IR) community like Precision@10, 
MAP (mean average precision), and nDCG 
(normalized discounted cumulative gain) are 
available for evaluating a single ranked list of 
items per query (Croft et al, 2009). Among the 
metrics, nDCG (Jarvelin and Kekalainen, 2000) 
can handle our three-level judgments (?Good?, 
?Fair?, and ?Bad?, refer to Section 4.1), 
 ????@? =
 ? ? /log(? + 1)??=1
 ?? ? /log(? + 1)??=1
 (4.1) 
where G(i) is the gain value assigned to the i?th 
item, and G*(i) is the gain value assigned to the 
i?th item of an ideal (or perfect) ranking list. 
Here we extend the IR metrics to the evalua-
tion of multiple ordered lists per query. We use 
nDCG as the basic metric and extend it to 
MnDCG. 
Assume labelers have determined m SSCs 
(SSC1~SSCm, refer to Section 4.1) for query q 
and the weight (or importance) of SSCi is wi. As-
sume n semantic classes are generated by an ap-
proach and n1 of them have corresponding SSCs 
(i.e., no appropriate SSC can be found for the 
remaining n-n1 semantic classes). We define the 
MnDCG score of an approach (with respect to 
query q) as, 
 ????? ? =
?1
?
?
 ?? ? ?????(SSC?)
?
i=1
 ??
m
i=1
 (4.2) 
where 
 ????? ???? =  
0                                         ?? ?? = 0
1
??
max
? ?[1, ??]
(???? ?? ,?  )  ?? ?? ? 0
  (4.3) 
In the above formula, nDCG(Gi,j) is the nDCG 
score of semantic class Gi,j; and ki denotes the 
number of semantic classes assigned to SSCi. For 
a list of queries, the MnDCG score of an algo-
rithm is the average of all scores for the queries. 
The metric is designed to properly deal with 
the following cases, 
464
i). One semantic class is wrongly split into 
multiple ones: Punished by dividing ??  in 
Formula 4.3; 
ii). A semantic class is too noisy to be as-
signed to any SSC: Processed by the 
?n1/n? in Formula 4.2; 
iii). Fewer semantic classes (than the number 
of SSCs) are produced: Punished in For-
mula 4.3 by assigning a zero value. 
iv). Wrongly merge multiple semantic 
classes into one: The nDCG score of the 
merged one will be small because it is 
computed with respect to only one single 
SSC. 
The gain values of nDCG for the three relev-
ance levels (?Bad?, ?Fair?, and ?Good?) are re-
spectively -1, 1, and 2 in experiments. 
4.3 Experimental  Results 
4.3.1 Overall performance comparison 
Figure 3 shows the performance comparison be-
tween the approaches listed in Section 4.1, using 
metrics MnDCG@n (n=1?10). Postprocessing 
is performed for all the approaches, where For-
mula 3.2 is adopted to compute the similarity 
between semantic classes. The results show that 
that the topic modeling approaches produce 
higher-quality semantic classes than the other 
approaches. It indicates that the topic mixture 
assumption of topic modeling can handle the 
multi-membership problem very well here. 
Among the alternative approaches, RASC clus-
tering behaves better than item clustering. The 
reason might be that an item cannot belong to 
multiple clusters in the two item clustering ap-
proaches, while RASC clustering allows this. For 
the RASC clustering approaches, although one 
item has the chance to belong to different seman-
tic classes, one RASC can only belong to one 
semantic class. 
 
 
Figure 3. Quality comparison (MnDCG@n) among 
approaches (frequency threshold h = 4 in preprocess-
ing; k = 5 in topic models) 
4.3.2 Preprocessing experiments 
Table 4 shows the average query processing time 
and results quality of the LDA approach, by va-
rying frequency threshold h. Similar results are 
observed for the pLSI approach. In the table, h=1 
means no preprocessing is performed. The aver-
age query processing time is calculated over all 
items in our dataset. As the threshold h increases, 
the processing time decreases as expected, be-
cause the input of topic modeling gets smaller. 
The second column lists the results quality 
(measured by MnDCG@10). Interestingly, we 
get the best results quality when h=4 (i.e., the 
items with frequency less than 4 are discarded). 
The reason may be that most low-frequency 
items are noisy ones. As a result, preprocessing 
can improve both results quality and processing 
efficiency; and h=4 seems a good choice in pre-
processing for our dataset. 
 
h 
Avg. Query Proc. 
Time (seconds) 
Quality 
(MnDCG@10) 
1 0.414 0.281 
2 0.375 0.294 
3 0.320 0.322 
4 0.268 0.331 
5 0.232 0.328 
6 0.210 0.315 
7 0.197 0.315 
8 0.184 0.313 
9 0.173 0.288 
Table 4. Time complexity and quality comparison 
among LDA approaches of different thresholds 
 
4.3.3 Postprocessing experiments 
 
Figure 4. Results quality comparison among topic 
modeling approaches with and without postprocessing 
(metric: MnDCG@10) 
 
The effect of postprocessing is shown in Figure 
4. In the figure, NP means no postprocessing is 
performed. Sim1 and Sim2 respectively mean 
Formula 3.1 and Formula 3.2 are used in post-
processing as the similarity measure between 
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
1 2 3 4 5 6 7 8 9 10
pLSI LDA KMedoids-RASC
DBSCAN-RASC KMedoids-Item DBSCAN-Item
n
0.27
0.28
0.29
0.3
0.31
0.32
0.33
0.34
LDA pLSI
NP
Sim1 
Sim2
465
semantic classes. The same preprocessing (h=4) 
is performed in generating the data. It can be 
seen that postprocessing improves results quality. 
Sim2 achieves more performance improvement 
than Sim1, which demonstrates the effectiveness 
of the similarity measure in Formula 3.2. 
4.3.4 Sample results 
Table 5 shows the semantic classes generated by 
our LDA approach for some sample queries in 
which the bad classes or bad members are hig-
hlighted (to save space, 10 items are listed here, 
and the query itself is omitted in the resultant 
semantic classes).  
 
Query Semantic Classes 
apple 
C1: ibm, microsoft, sony, dell, toshiba,  sam-
sung, panasonic, canon, nec, sharp ? 
C2: peach, strawberry, cherry, orange, bana-
na, lemon, pineapple, raspberry, pear, grape 
? 
gold 
C1: silver, copper, platinum, zinc, lead, iron, 
nickel, tin, aluminum, manganese ? 
C2: silver, red, black, white, blue, purple, 
orange, pink, brown, navy ? 
C3: silver, platinum, earrings, diamonds, 
rings, bracelets, necklaces, pendants, jewelry, 
watches ? 
C4: silver, home, money, business, metal, 
furniture, shoes, gypsum, hematite, fluorite 
?  
lincoln 
C1: ford, mazda, toyota, dodge, nissan, hon-
da, bmw, chrysler, mitsubishi, audi ? 
C2: bristol, manchester, birmingham, leeds, 
london, cardiff, nottingham, newcastle, shef-
field, southampton ? 
C3: jefferson, jackson, washington, madison, 
franklin, sacramento, new york city, monroe, 
Louisville, marion ? 
computer 
science 
C1: chemistry, mathematics, physics, biolo-
gy, psychology, education, history, music, 
business, economics ? 
Table 5. Semantic classes generated by our approach 
for some sample queries (topic model = LDA) 
 
5 Related Work 
Several categories of work are related to ours. 
The first category is about set expansion (i.e., 
retrieving one semantic class given one term or a 
couple of terms). Syntactic context information is 
used (Hindle, 1990; Ruge, 1992; Lin, 1998) to 
compute term similarities, based on which simi-
lar words to a particular word can directly be 
returned. Google sets is an online service which, 
given one to five items, predicts other items in 
the set. Ghahramani and Heller (2005) introduce 
a Bayesian Sets algorithm for set expansion. Set 
expansion is performed by feeding queries to 
web search engines in Wang and Cohen (2007) 
and Kozareva (2008). All of the above work only 
yields one semantic class for a given query. 
Second, there are pattern-based approaches in the 
literature which only do limited integration of 
RASCs (Shinzato and Torisawa, 2004; Shinzato 
and Torisawa, 2005; Pasca, 2004), as discussed 
in the introduction section. In Shi et al (2008), 
an ad-hoc approach was proposed to discover the 
multiple semantic classes for one item. The third 
category is distributional similarity approaches 
which provide multi-membership support (Har-
ris, 1985; Lin  and Pantel, 2001; Pantel and Lin, 
2002). Among them, the CBC algorithm (Pantel 
and Lin, 2002) addresses the multi-membership 
problem. But it relies on term vectors and centro-
ids which are not available in pattern-based ap-
proaches. It is therefore not clear whether it can 
be borrowed to deal with multi-membership here. 
Among the various applications of topic 
modeling, maybe the efforts of using topic model 
for Word Sense Disambiguation (WSD) are most 
relevant to our work. In Cai et al(2007), LDA is 
utilized to capture the global context information 
as the topic features for better performing the 
WSD task. In Boyd-Graber et al (2007), Latent 
Dirichlet with WordNet (LDAWN) is developed 
for simultaneously disambiguating a corpus and 
learning the domains in which to consider each 
word. They do not generate semantic classes. 
6 Conclusions 
We presented an approach that employs topic 
modeling for semantic class construction. Given 
an item q, we first retrieve all RASCs containing 
the item to form a collection CR(q). Then we per-
form some preprocessing to CR(q) and build a 
topic model for it. Finally, the output semantic 
classes of topic modeling are post-processed to 
generate the final semantic classes. For the CR(q) 
which contains a lot of RASCs, we perform of-
fline processing according to the above process 
and store the results on disk, in order to reduce 
the online query processing time. 
We also proposed an evaluation methodology 
for measuring the quality of semantic classes. 
We show by experiments that our topic modeling 
approach outperforms the item clustering and 
RASC clustering approaches. 
 
Acknowledgments 
We wish to acknowledge help from Xiaokang 
Liu for mining RASCs from web pages, Chan-
gliang Wang and Zhongkai Fu for data process.  
  
466
References  
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 
2003. Latent dirichlet alocation. J. Mach. Learn. 
Res., 3:993?1022. 
Bruce Croft, Donald Metzler, and Trevor Strohman. 
2009. Search Engines: Information Retrieval in 
Practice. Addison Wesley.  
Jordan Boyd-Graber, David Blei, and Xiaojin 
Zhu.2007. A topic model for word sense disambig-
uation. In Proceedings EMNLP-CoNLL 2007, pag-
es 1024?1033, Prague, Czech Republic, June. 
Association for Computational Linguistics. 
Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh. 2007. 
NUS-ML: Improving word sense disambiguation 
using topic features. In Proceedings of the Interna-
tional Workshop on Semantic Evaluations, volume 
4. 
Scott Deerwester, Susan T. Dumais, GeorgeW. Fur-
nas, Thomas K. Landauer, and Richard Harshman. 
1990. Indexing by latent semantic analysis. Journal 
of the American Society for Information Science, 
41:391?407. 
Zoubin Ghahramani and Katherine A. Heller. 2005. 
Bayesian Sets. In Advances in Neural Information 
Processing Systems (NIPS05). 
Thomas L. Griffiths, Mark Steyvers, David M. 
Blei,and Joshua B. Tenenbaum. 2005. Integrating 
topics and syntax. In Advances in Neural Informa-
tion Processing Systems 17, pages 537?544. MIT 
Press 
Zellig Harris. Distributional Structure. The Philoso-
phy of Linguistics. New York: Oxford University 
Press. 1985. 
Donald Hindle. 1990. Noun Classification from Pre-
dicate-Argument Structures. In Proceedings of 
ACL90, pages 268?275.  
Thomas Hofmann. 1999. Probabilistic latent semantic 
indexing. In Proceedings of the 22nd annual inter-
national ACM SIGIR99, pages 50?57, New York, 
NY, USA. ACM. 
Kalervo Jarvelin, and Jaana Kekalainen. 2000. IR 
Evaluation Methods for Retrieving Highly Rele-
vant Documents. In Proceedings of the 23rd An-
nual International ACM SIGIR Conference on 
Research and Development in Information Retriev-
al (SIGIR2000). 
Zornitsa Kozareva, Ellen Riloff and Eduard Hovy. 
2008. Semantic Class Learning from the Web with 
Hyponym Pattern Linkage Graphs, In Proceedings 
of ACL-08. 
Wei Li, David M. Blei, and Andrew McCallum. Non-
parametric Bayes Pachinko Allocation. In Proceed-
ings of Conference on Uncertainty in Artificial In-
telligence (UAI), 2007. 
Dekang Lin. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. In Proceedings of COLING-
ACL98, pages 768-774. 
Dekang Lin and Patrick Pantel. 2001. Induction of 
Semantic Classes from Natural Language Text. In 
Proceedings of SIGKDD01, pages 317-322.  
Hiroaki Ohshima, Satoshi Oyama, and Katsumi Tana-
ka. 2006. Searching coordinate terms with their 
context from the web. In WISE06, pages 40?47. 
Patrick Pantel and Dekang Lin. 2002. Discovering 
Word Senses from Text. In Proceedings of 
SIGKDD02.  
Marius Pasca. 2004. Acquisition of Categorized 
Named Entities for Web Search. In Proc. of 2004 
CIKM.  
Gerda Ruge. 1992. Experiments on Linguistically-
Based Term Associations. In Information 
Processing & Management, 28(3), pages 317-32. 
Andrew I. Schein,  Alexandrin Popescul,  Lyle H. 
Ungar and David M. Pennock. 2002. Methods and 
metrics for cold-start recommendations. In Pro-
ceedings of SIGIR02, pages  253-260. 
Shuming Shi, Xiaokang Liu and Ji-Rong Wen. 2008. 
Pattern-based Semantic Class Discovery with Mul-
ti-Membership Support. In CIKM2008, pages 
1453-1454.  
Keiji Shinzato and Kentaro Torisawa. 2004. Acquir-
ing Hyponymy Relations from Web Documents. In 
HLT/NAACL04, pages 73?80. 
Keiji Shinzato and Kentaro Torisawa. 2005. A Simple 
WWW-based Method for Semantic Word Class 
Acquisition. In RANLP05.  
Richard C. Wang and William W. Cohen. 2007. Lan-
gusage-Independent Set Expansion of Named Enti-
ties Using the Web. In ICDM2007. 
 
467
Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 10?18,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Anchor Text Extraction for Academic Search 
 
 
Shuming Shi1     Fei Xing2*     Mingjie Zhu3*     Zaiqing Nie1     Ji-Rong Wen1 
1Microsoft Research Asia 
2Alibaba Group, China 
3University of Science and Technology of China 
{shumings, znie, jrwen}@microsoft.com 
fei_c_xing@yahoo.com; mjzhu@ustc.edu 
 
 
 
Abstract* 
 
Anchor text plays a special important role in 
improving the performance of general Web 
search, due to the fact that it is relatively ob-
jective description for a Web page by poten-
tially a large number of other Web pages. 
Academic Search provides indexing and 
search functionality for academic articles. It 
may be desirable to utilize anchor text in aca-
demic search as well to improve the search re-
sults quality. The main challenge here is that 
no explicit URLs and anchor text is available 
for academic articles. In this paper we define 
and automatically assign a pseudo-URL for 
each academic article. And a machine learning 
approach is adopted to extract pseudo-anchor 
text for academic articles, by exploiting the ci-
tation relationship between them. The ex-
tracted pseudo-anchor text is then indexed and 
involved in the relevance score computation of 
academic articles. Experiments conducted on 
0.9 million research papers show that our ap-
proach is able to dramatically improve search 
performance. 
1 Introduction 
Anchor text is a piece of clickable text that links 
to a target Web page. In general Web search, 
anchor text plays an extremely important role in 
improving the search quality. The main reason 
for this is that anchor text actually aggregates the 
opinion (which is more comprehensive, accurate, 
and objective) of a potentially large number of 
people for a Web page. 
                                                 
* This work was performed when Fei Xing and Mingjie Zhu 
were interns at Microsoft Research Asia. 
In recent years, academic search (Giles et al, 
1998; Lawrence et al, 1999; Nie et al, 2005; 
Chakrabarti et al, 2006) has become an impor-
tant supplement to general web search for re-
trieving research articles. Several academic 
search systems (including Google Scholar?, Cite-
seer?, DBLP?, Libra**, ArnetMiner??, etc.) have 
been deployed. In order to improve the results 
quality of an academic search system, we may 
consider exploiting the techniques which are 
demonstrated to be quite useful and critical in 
general Web search. In this paper, we study the 
possibility of extracting anchor text for research 
papers and using them to improve the search per-
formance of an academic search system. 
 
 
Figure 1. An example of one paper citing other papers 
 
The basic search unit in most academic search 
systems is a research paper. Borrowing the con-
cepts of URL and anchor-text in general Web 
search, we may need to assign a pseudo-URL for 
one research paper as its identifier and to define 
the pseudo-anchor text for it by the contextual 
description when this paper is referenced (or 
mentioned). The pseudo-URL of a research pa-
per could be the combination of its title, authors 
and publication information. Figure-1 shows an 
excerpt where one paper cites a couple of other 
                                                 
? http://scholar.google.com/ 
? http://citeseerx.ist.psu.edu/ 
? http://www.informatik.uni-trier.de/~ley/db/ 
** http://libra.msra.cn/ 
?? http://www.arnetminer.org/ 
10
papers. The grayed text can be treated as the 
pseudo-anchor text of the papers being refe-
renced. Once the pseudo-anchor text of research 
papers is acquired, it can be indexed and utilized 
to help ranking, just as in general web search. 
However it remains a challenging task to cor-
rectly identify and extract these pseudo-URLs 
and pseudo-anchor texts. First, unlike the situa-
tion in general web search where one unique 
URL is assigned to each web page as a natural 
identifier, the information of research papers 
need to be extracted from web pages or PDF files. 
As a result, in constructing pseudo-URLs for 
research papers, we may face the problem of ex-
traction errors, typos, and the case of one re-
search paper having different expressions in dif-
ferent places. Second, in general Web search, 
anchor text is always explicitly specified by 
HTML tags (<a> and </a>). It is however much 
harder to perform anchor text extraction for re-
search papers. For example, human knowledge 
may be required in Figure-1 to accurately identi-
fy the description of every cited paper. 
To address the above challenges, we propose 
an approach for extracting and utilizing pseudo-
anchor text information in academic search to 
improve the search results quality. Our approach 
is composed of three phases. In the first phase, 
each time a paper is cited in another paper, we 
construct a tentative pseudo-URL for the cited 
paper and extract a candidate anchor block for it. 
The tentative pseudo-URL and the candidate 
anchor block are allowed to be inaccurate. In the 
second phase, we merge the tentative pseudo-
URLs that should represent the same paper. All 
candidate anchor blocks belong to the same pa-
per are grouped accordingly in this phase. In the 
third phase, the final pseudo-anchor text of each 
paper is generated from all its candidate blocks, 
by adopting a SVM-based machine learning me-
thodology. We conduct experiments upon a data-
set containing 0.9 million research papers. The 
experimental results show that lots of useful anc-
hor text can be successfully extracted and accu-
mulated using our approach, and the ultimate 
search performance is dramatically improved 
when anchor information is indexed and used for 
paper ranking. 
The remaining part of this paper is organized 
as follows. In Section 2, we describe in detail our 
approach for pseudo-anchor text extraction and 
accumulation. Experimental results are reported 
in Section 3. We discuss related work in Section 
4 and finally conclude the paper in Section 5. 
2 Our Approach 
2.1 Overview 
Before describing our approach in detail, we first 
recall how anchor text is processed in general 
Web search. Assume that there have been a col-
lection of documents being crawled and stored 
on local disk. In the first step, each web page is 
parsed and the out links (or forward links) within 
the page are extracted. Each link is comprised of 
a URL and its corresponding anchor text. In the 
second step, all links are accumulated according 
to their destination URLs (i.e. the anchor texts of 
all links pointed to the same URL are merged). 
Thus, we can get al anchor text corresponding to 
each web page. Figure-2 (a) demonstrates this 
process. 
 
 
Figure 2. The main process of extracting (a) anchor 
text in general web search and (b) pseudo-anchor text 
in academic search 
 
For academic search, we need to extract and 
parse the text content of papers. When a paper A 
mentions another paper B, it either explicitly or 
implicitly displays the key information of B to let 
the users know that it is referencing B instead of 
other papers. Such information can be extracted 
to construct the tentative pseudo-URL of B. The 
pseudo-URLs constructed in this phase are tenta-
tive because different tentative pseudo-URLs 
may be merged to generate the same final pseu-
do-URL. All information related to paper B in 
different papers can be accumulated and treated 
Web pages 
HTML parsing 
Links 
Anchor text 
for pages 
 
Group by link 
destination 
Papers 
Paper parsing 
Tentative pseudo-URLs 
Candidate anchor blocks 
Anchor block accumulation 
Papers with their  
candidate anchor blocks 
Papers with their  
pseudo-anchor text 
Anchor-text learning 
11
as the potential anchor text of B. Our goal is to 
get the anchor text related to each paper. 
Our approach for pseudo-anchor text extrac-
tion is shown in Figure-2 (b). The key process is 
similar to that in general Web search for accumu-
lating and utilizing page anchor text. One prima-
ry difference between Figure-2 (a) and (b) is the 
latter accumulates candidate anchor blocks rather 
than pieces of anchor text. A candidate anchor 
block is a piece of text that contains the descrip-
tion of one paper. The basic idea is: Instead of 
extracting the anchor text for a paper directly (a 
difficult task because of the lack of enough in-
formation), we first construct a candidate anchor 
block to contain the "possible" or "potential" de-
scription of the paper. After we accumulate all 
candidate anchor blocks, we have more informa-
tion to provide a better estimation about which 
pieces of texts are anchor texts. Following this 
idea, our proposed approach adopts a three-phase 
methodology to extract pseudo-anchor text. In 
the first phase, each time a paper B appearing in 
another paper A, a candidate anchor block is ex-
tracted for B. All candidate anchor blocks belong 
to the same paper are grouped in the second 
phase. In the third phase, the final pseudo-anchor 
text of each paper is selected among all candidate 
blocks. 
Extracting tentative pseudo-URLs and can-
didate anchor blocks: When one paper cites 
another paper, a piece of short text (e.g. "[1]" or 
?(xxx et al, 2008)?) is commonly inserted to 
represent the paper to be cited, and the detail in-
formation (key attributes) of it are typically put 
at the end of the document (in the references sec-
tion). We call each paper listed in the references 
section a reference item. The references section 
can be located by searching for the last occur-
rence of term 'reference' or 'references' in larger 
fonts. Then, we adopt a rule-based approach to 
divide the text in the references section into ref-
erence items. Another rule-based approach is 
used to extract paper attributes (title, authors, 
year, etc) from a reference item. We observed 
some errors in our resulting pseudo-URLs caused 
by the quality of HTML files converted from 
PDF format, reference item extraction errors, 
paper attribute extraction errors, and other fac-
tors. We also observed different reference item 
formats for the same paper. The pseudo-URL for 
a paper is defined according to its title, authors, 
publisher, and publication year, because these 
four kinds of information can readily be used to 
identify a paper. 
For each citation of a paper, we treat the sen-
tence containing the reference point (or citation 
point) as one candidate anchor block. When mul-
tiple papers are cited in one sentence, we treat 
the sentence as the candidate anchor block of 
every destination paper. 
Candidate Anchor Block Accumulation: 
This phase is in charge of merging all candidate 
blocks of the same pseudo-URL. As has been 
discussed, tentative pseudo-URLs are often inac-
curate; and different tentative pseudo-URLs may 
correspond to the same paper. The primary chal-
lenge here is perform the task in an efficient way 
and with high accuracy. We will address this 
problem in Subsection 2.2. 
Pseudo-Anchor Generation: In the previous 
phase, all candidate blocks of each paper have 
been accumulated. This phase is to generate the 
final anchor text for each paper from all its can-
didate blocks. Please refer to Subsection 2.3 for 
details. 
2.2 Candidate Anchor Block Accumulation 
via Multiple Feature-String Hashing 
Consider this problem: Given a potentially huge 
number of tentative pseudo-URLs for papers, we 
need to identify and merge the tentative pseudo-
URLs that represent the same paper. This is like 
the problems in the record linkage (Fellegi and 
Sunter, 1969), entity matching, and data integra-
tion which have been extensively studied in da-
tabase, AI, and other areas. In this sub-section, 
we will first show the major challenges and the 
previous similar work on this kind of problem. 
Then a possible approach is described to achieve 
a trade-off between accuracy and efficiency. 
 
 
Figure 3. Two tentative pseudo-URLs representing 
the same paper 
 
2.2.1 Challenges and candidate techniques 
Two issues should be addressed for this problem: 
similarity measurement, and the efficiency of the 
algorithm. On one hand, a proper similarity func-
tion is needed to identify two tentative pseudo-
URLs representing the same paper. Second, the 
12
integration process has to be accomplished effi-
ciently. 
We choose to compute the similarity between 
two papers to be a linear combination of the si-
milarities on the following fields: title, authors, 
venue (conference/journal name), and year. The 
similarity function on each field is carefully de-
signed. For paper title, we adopt a term-level edit 
distance to compute similarity. And for paper 
authors, person name abbreviation is considered. 
The similarity function we adopted is fairly well 
in accuracy (e.g., the similarity between the two 
pseudo-URLs in Figure-3 is high according to 
our function); but it is quite time-consuming to 
compute the similarity for each pair of papers 
(roughly 1012 similarity computation operations 
are needed for 1 million different tentative pseu-
do-URLs). 
Some existing methods are available for de-
creasing the times of similarity calculation opera-
tions. McCallum et al (2000) addresses this high 
dimensional data clustering problem by dividing 
data into overlapping subsets called canopies 
according to a cheap, approximate distance mea-
surement. Then the clustering process is per-
formed by measuring the exact distances only 
between objects from the same canopy. There are 
also other subspace methods (Parsons et al, 2004) 
in data clustering areas, where data are divided 
into subspaces of high dimensional spaces first 
and then processing is done in these subspaces. 
Also there are fast blocking approaches for 
record linkage in Baxter et al (2003). Though 
they may have different names, they hold similar 
ideas of dividing data into subsets to reduce the 
candidate comparison records. The size of data-
set used in the above papers is typically quite 
small (about thousands of data items). For effi-
ciency issue, Broder et al (1997) proposed a 
shingling approach to detect similar Web pages. 
They noticed that it is infeasible to compare 
sketches (which are generated by shingling) of 
all pairs of documents. So they built an inverted 
index that contains a list of shingle values and 
the documents they appearing in. With the in-
verted index, they can effectively generate a list 
of all the pairs of documents that share any shin-
gles, along with the number of shingles they 
have in common. They did experiments on a da-
taset containing 30 million documents. 
By adopting the main ideas of the above tech-
niques to our pseudo-URL matching problem, a 
possible approach can be as follows. 
 
 
Figure 4. The Multiple Feature-String Hashing algo-
rithm for candidate anchor block accumulation 
 
2.2.2 Method adopted 
The method utilized here for candidate anchor 
block accumulation is shown in Figure 4. The 
main idea is to construct a certain number of fea-
ture strings for a tentative pseudo-URL (abbre-
viated as TP-URL) and do hash for the feature 
strings. A feature string of a paper is a small 
piece of text which records a part of the paper?s 
key information, satisfying the following condi-
tions: First, multiple feature strings can typically 
be built from a TP-URL. Second, if two TP-
URLs are different representations of the same 
paper, then the probability that they have at least 
one common feature string is extremely high. We 
can choose the term-level n-grams of paper titles 
(referring to Section 3.4) as feature strings. 
The algorithm maintains an in-memory hash-
table which contains a lot of slots each of which 
is a list of TP-URLs belonging to this slot. For 
each TP-URL, feature strings are generated and 
hashed by a specified hash function. The TP-
URL is then added into some slots according to 
the hash values of its feature strings. Any two 
TP-URLs belonging to the same slot are further 
compared by utilizing our similarity function. If 
their similarity is larger than a threshold, the two 
TP-URLs are treated as being the same and 
therefore their corresponding candidate anchor 
blocks are merged. 
The above algorithm tries to achieve good bal-
ance between accuracy and performance. On one 
hand, compared with the na?ve algorithm of per-
forming one-one comparison between all pairs of 
TP-URLs, the algorithm needs only to compute 
Algorithm Multiple Feature-String Hashing for candidate anchor 
block accumulation 
Input: A list of papers (with their tentative pseudo-URLs 
and candidate anchor blocks) 
Output: Papers with all candidate anchor blocks of the 
same paper aggregated 
 
Initial: An empty hashtable h (each slot of h is a list of pa-
pers) 
For each paper A in the input list { 
For each feature-string of A { 
Lookup by the feature-string in h to get a slot s; 
Add A into s; 
} 
} 
For each slot s with size smaller than a threshold { 
For any two papers A1, A2 in s { 
float fSim = Similarity(A1, A2); 
if(fSim > the specified threshold) { 
Merge A1 and A2; 
} 
} 
} 
13
the similarity for the TP-URLs that share a 
common slot. On the other hand, because of the 
special property of feature strings, most TP-
URLs representing the same paper can be de-
tected and merged. 
The basic idea of dividing data into over-
lapped subsets is inherited from McCallum et al 
(2000), Broder et al (1997), and some subspace 
clustering approaches. Slightly different, we do 
not count the number of common feature strings 
between TP-URLs. Common bins (or inverted 
indices) between data points are calculated in 
McCallum et al (2000) as a ?cheap distance? for 
creating canopies. The number of common Shin-
gles between two Web documents is calculated 
(efficiently via inverted indices), such that Jac-
card similarity could be used to measure the si-
milarity between them. In our case, we simply 
compare any two TP-URLs in the same slot by 
using our similarity function directly. 
The effective and efficiency of this algorithm 
depend on the selection of feature strings. For a 
fixed feature string generation method, the per-
formance of this algorithm is affected by the size 
of each slot, especially the number and size of 
big slots (slots with size larger than a threshold). 
Big slots will be discarded in the algorithm to 
improve performance, just like removing com-
mon Shingles in Broder et al (1997). In Section 
4, we conduct experiments to test the perfor-
mance of the above algorithm with different fea-
ture string functions and different slot size thre-
sholds. 
2.3 Pseudo-Anchor Text Learning 
In this subsection, we address the problem of 
extracting the final pseudo-anchor text for a pa-
per, given all its candidate anchor blocks (see 
Figure 5 for an example). 
2.3.1 Problem definition 
A candidate anchor block is a piece of text with 
one or some reference points (a reference point is 
one occurrence of citation in a paper) specified, 
where a reference point is denoted by a 
<start_pos, end_pos> pair (means start position 
and end position respectively): ref = <start_pos, 
end_pos>. We represent a candidate anchor 
block to be the following format, 
AnchorBlock = (Text, ref1, ref2, ?) 
We define a block set to be a set of candidate 
anchor blocks for a paper, 
BlockSet = {AnchorBlock1, AnchorBlock2, ?} 
Now the problem is: Given a block set con-
taining N elements, extract some text excerpts 
from them as the anchor text of the paper. 
2.3.2 Learn term weights 
We adopt a machine-learning approach to assign, 
for each term in the anchor blocks, a discrete de-
gree of being anchor text. The main reasons for 
taking such an approach is twofold: First, we 
believe that assigning each term a fuzzy degree 
of being anchor text is more appropriate than a 
binary judgment as either an anchor-term or non-
anchor-term. Second, since the importance of a 
term for a ?link? may be determined by many 
factors in paper search, a machine-learning could 
be more flexible and general than the approaches 
that compute term degrees by a specially de-
signed formula. 
 
 
Figure 5. The candidate pseudo-anchor blocks of a 
paper 
 
The features used for learning are listed in Ta-
ble-1. 
We observed that it would be more effective if 
some of the above features are normalized before 
being used for learning. For a term in candidate 
anchor block B, its TF are normalized by the 
BM25 formula (Robertson et al, 1999), 
 
TFL
Bbbk
TFkTFnorm ?????
???
)||)1((
)1(
1
1
 
 
where L is average length of the candidate blocks, 
|B| is the length of B, and k1, b are parameters. 
DF is normalized by the following formula, 
 )1log( DF
NIDF ??
  
where N is the number of elements in the block 
set (i.e. total number of candidate anchor blocks 
for the current paper). 
Features RefPos and Dist are normalized as, 
 
RefPosnorm = RefPos / |B| 
Distnorm = (Dist-RefPos) / |B| 
 
And the feature BlockLen is normalized as, 
14
 BlockLennorm = log(1+BlockLen)  
 
Features Description 
DF 
Document frequency: Number of candidate blocks in 
which the term appears, counted among all candidate 
blocks of all papers. It is used to indicate whether the 
term is a stop word or not. 
BF 
Block frequency: Number of candidate blocks in 
which the term appears, counted among all candidate 
blocks of this paper. 
CTF 
Collection term frequency: Total number of times the 
term appearing in the blocks. For multiple times of 
occurrences in one block, all of them are counted. 
IsInURL 
Specify whether the term appears in the pseudo-URL 
of the paper. 
TF 
Term frequency: Number of times the terms appearing 
in the candidate block. 
Dist 
Directed distance from the nearest reference point to 
the term location 
RefPos 
Position of the nearest reference point in the candidate 
pseudo-anchor block. 
BlockLen Length of the candidate pseudo-anchor block 
Table 1. Features for learning 
 
We set four term importance levels, from 1 
(unrelated terms or stop words) to 4 (words par-
ticipating in describing the main ideas of the pa-
per). 
We choose support vector machine (SVM) for 
learning term weights here, because of its power-
ful classification ability and well generalization 
ability (Burges, 1998). We believe some other 
machine learning techniques should also work 
here. The input of the classifier is a feature vec-
tor of a term and the output is the importance 
level of the term. Given a set of training data 
? ?liii levelfeature 1, ?, a decision function f(x) can be 
acquired after training. Using the decision func-
tion, we can assign an importance level for each 
term automatically. 
 
3 Experiments 
3.1 Experimental Setup 
Our experimental dataset contains 0.9 million 
papers crawled from the web. All the papers are 
processed according to the process in Figure-2 
(b). We randomly select 300 queries from the 
query log of Libra (libra.msra.cn) and retrieve 
the results in our indexing and ranking system 
with/without the pseudo-anchors generated by 
our approach. Then the volunteer researchers and 
students in our group are involved to judge the 
search results. The top 30 results of different 
ranking algorithms for each query are labeled 
and assigned a relevance value from 1 (meaning 
'poor match') to 5 (meaning 'perfect match'). The 
search results quality is measured by NDCG 
(Jarvelin and Kekalainen, 2000). 
3.2 Overall Effect of our Approach 
Figure 6 shows the performance comparison be-
tween the results of two baseline paper ranking 
algorithms and the results of including pseudo-
anchor text in ranking. 
 
0.466
0.426
0.388
0.597
0.619
0.689
0.673 0.672
0.627
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
NDCG@1 NDCG@3 NDCG@10
Base(Without CitationCount)
Base
Pseudo-Anchor Included
 
Figure 6. Comparison between the baseline approach 
and our approach (measure: nDCG) 
 
The ?Base? algorithm considers the title, ab-
stract, full-text and static-rank (which is a func-
tion of the citation count) of a paper. In a bit 
more detail, for each paper, we adopt the BM25 
formula (Robertson et al, 1999) over its title, 
abstract, and full-text respectively. And then the 
resulting score is linearly combined with the stat-
ic-rank to get its final score. The static-rank is 
computed as follows, 
 StaticRank = log(1+CitationCount) (3.1) 
To test the performance of including pseudo-
anchor text in ranking, we compute an anchor 
score for each paper and linearly combine it with 
its baseline score (i.e. the score computed by the 
baseline algorithm). 
We tried two kinds of ways for anchor score 
computation. The first is to merge all pieces of 
anchor excerpts (extracted in the previous section) 
into a larger piece of anchor text, and use BM25 
to compute its relevance score. In another ap-
proach called homogeneous evidence combina-
tion (Shi et al, 2006), a relevance score is com-
puted for each anchor excerpt (still using BM25), 
and all the scores for the excerpts are sorted des-
cending and then combined by the following 
formula, 
 ?
?
?????
m
i
ianchor sicS 1 2))1(1(
1 (3.2) 
where si (i=1, ?, m) are scores for the m anchor 
excerpts, and c is a parameter. The primary idea 
15
here is to let larger scores to have relative greater 
weights. Please refer to Shi et al (2006) for a 
justification of this approach. As we get slightly 
better results with the latter way, we use it as our 
final choice for computing anchor scores. 
From Figure 6, we can see that the overall per-
formance is greatly improved by including pseu-
do-anchor information. Table 2 shows the t-test 
results, where a ?>? indicates that the algorithm 
in the row outperforms that in the column with a 
p-value of 0.05 or less, and a ?>>? means a p-
value of 0.01 or less. 
 
 
 
Base 
Base (without 
CitationCount) 
Our approach > >> 
Base  >> 
Base (without Cita-
tionCount) 
  
Table 2. Statistical significance tests (t-test over 
nDCG@3) 
 
Table 3 shows the performance comparison by 
using some traditional IR measures based on bi-
nary judgments. Since the results of not includ-
ing CitationCount are much worse than the other 
two, we omit it in the table. 
 
Measure 
Approach 
MAP MRR P@1 P@10 
Base (including 
CitationCount) 
0.364 0.727 0.613 0.501 
Our Approach 0.381 0.734 0.625 0.531 
Table 3. Performance compassion using binary judg-
ment measures 
 
3.3 Sample Query Analysis 
Here we analyze some sample queries to get 
some insights about why and how pseudo-anchor 
improves search performance. Figure-7 and Fig-
ure-8 show the top-3 results of two sample que-
ries: {TF-IDF} and {Page Rank}. 
For query "TF-IDF", the top results of the 
baseline approach have keyword "TF-IDF" ap-
peared in the title as well as in other places of the 
papers. Although the returned papers are relevant 
to the query, they are not excellent because typi-
cally users may want to get the first TF-IDF pa-
per or some papers introducing TF-IDF. When 
pseudo-anchor information is involved, some 
excellent results (B1, B2, B3) are generated. The 
main reason for getting the improved results is 
that these papers (or books) are described with 
"TF-IDF" when lots of other papers cite them. 
 
 
Figure 7. Top-3 results for query TF-IDF 
 
 
Figure 8. Top-3 results for query Page Rank 
 
Figure-8 shows another example about how 
pseudo-anchor helps to improve search results 
quality. For query "Page Rank" (note that there is 
a space in between), the results returned by the 
baseline approach are not satisfactory. In the pa-
pers returned by our approach, at least B1 and B2 
are very good results. Although they did not la-
bel themselves "Page Rank", other papers do so 
in citing them. Interestingly, although the result 
B3 is not about the "PageRank" algorithm, it de-
scribes another popular "Page Rank" algorithm 
in addition to PageRank. 
Another interesting observation from the two 
figures is that our approach retrieves older papers 
than the baseline method, because old papers 
tend to have more anchor text (due to more cita-
tions). So our approach may not be suitable for 
retrieve newer papers. To overcome this problem, 
maybe publication year should be considered in 
our ranking functions. 
3.4 Anchor Accumulation Experiments 
We conduct experiments to test the effectiveness 
and efficiency of the multiple-feature-string-
hashing algorithm presented in Section 2.2. The 
duplication detection quality of this algorithm is 
determined by the appropriate selection of fea-
A1. V Safronov, M Parashar, Y Wang et al Optimizing Web servers 
using Page rank prefetching for clustered accesses. Information 
Sciences. 2003. 
A2. AO Mendelzon, D Rafiei. An autonomous page ranking method for 
metasearch engines. WWW, 2002. 
A3. FB Kalhoff. On formally real Division Algebras and Quasifields of 
Rank two. 
(a) Without anchor 
B1. S Brin, L Page. The Anatomy of a Large-Scale Hypertextual Web 
Search Engine. WWW, 1998 
B2. L Page, S Brin, R Motwani, T Winograd. The pagerank citation 
ranking: Bringing order to the web. 1998. 
B3. JM Kleinberg. Authoritative sources in a hyperlinked environment. 
Journal of the ACM, 1999. 
(b) With anchor 
 
A1. K Sugiyama, K Hatano, M Yoshikawa, S Uemura. Refinement of TF-
IDF schemes for web pages using their hyperlinked neighboring pages. 
Hypertext?03 
A2. A Aizawa. An information-theoretic perspective of tf-idf measures. 
IPM?03. 
A3. N Oren. Reexamining tf.idf based information retrieval with Genet-
ic Programming. SAICSIT?02. 
(a) Without anchor 
B1. G Salton, MJ McGill. Introduction to Modern Information Retriev-
al. McGraw-Hill, 1983. 
B2. G Salton and C Buckley. Term weighting approaches in automatic 
text retrieval. IPM?98. 
B3. R Baeza-Yates, B Ribeiro-Neto. Modern Information Retrieval. 
Addison-Wesley, 1999 
(b) With anchor 
 
16
ture strings. When feature strings are fixed, the 
slot size threshold can be used to tune the tra-
deoff between accuracy and performance. 
 
Feature Strings 
Slot Distr. 
Ungram Bigram Trigram 4-gram 
# of Slots 1.4*105 1.2*106 2.8*106 3.4*106 
# of Slots with 
size > 100 
5240 6806 1541 253 
# of Slots with 
size > 1000 
998 363 50 5 
# of Slots with 
size > 10000 
59 11 0 0 
Table 4. Slot distribution with different feature strings 
 
We take all the papers extracted from PDF 
files as input to run the algorithm. Identical TP-
URLs are first eliminated (therefore their candi-
date anchor blocks are merged) by utilizing a 
hash table. This pre-process step results in about 
1.46 million distinct TP-URLs. The number is 
larger than our collection size (0.9 million), be-
cause some cited papers are not in our paper col-
lection. We tested four kinds of feature strings all 
of which are generated from paper title: uni-
grams, bigrams, trigrams, and 4-grams. Table-4 
shows the slot size distribution corresponding to 
each kind of feature strings. The performance 
comparison among different feature strings and 
slot size thresholds is shown in Table 5. It seems 
that bigrams achieve a good trade-off between 
accuracy and performance. 
 
Feature 
Strings 
Slot Size 
Threshold 
Dup. papers 
Detected 
Processing 
Time (sec) 
Unigram 
5000 529,717  119,739.0  
500 327,357 7,552.7  
Bigram 500 528,981 8,229.6  
Trigram 
Infinite 518,564 8,420.4  
500 516,369 2,654.9  
4-gram 500 482,299 1,138.2  
Table 5. Performance comparison between different 
feature strings and slot size thresholds 
 
4 Related Work 
There has been some work which uses anchor 
text or their surrounding text for various Web 
information retrieval tasks. It was known at the 
very beginning era of internet that anchor text 
was useful to Web search (McBryan, 1994). 
Most Web search engines now use anchor text as 
primary and power evidence for improving 
search performance. The idea of using contextual 
text in a certain vicinity of the anchor text was 
proposed in Chakrabarti et al (1998) to automat-
ically compile some lists of authoritative Web 
resources on a range of topics. An anchor win-
dow approach is proposed in Chakrabarti et al
(1998) to extract implicit anchor text. Following 
this work, anchor windows were considered in 
some other tasks (Amitay  et al, 1998; Haveli-
wala et al, 2002; Davison, 2002; Attardi et al, 
1999). Although we are inspired by these ideas, 
our work is different because research papers 
have many different properties from Web pages. 
From the viewpoint of implicit anchor extraction 
techniques, our approach is different from the 
anchor window approach. The anchor window 
approach is somewhat simpler and easy to im-
plement than ours. However, our method is more 
general and flexible. In our approach, the anchor 
text is not necessarily to be in a window. 
Citeseer (Giles et al, 1998; Lawrence  et al, 
1999) has been doing a lot of valuable work on 
citation recognition, reference matching, and pa-
per indexing. It has been displaying contextual 
information for cited papers. This feature has 
been shown to be helpful and useful for re-
searchers. Differently, we are using context de-
scription for improving ranking rather than dis-
play purpose. In addition to Citeseer, some other 
work (McCallum et al, 1999; Nanba and Oku-
mura, 1999; Nanba et al, 2004; Shi et al, 2006) 
is also available for extracting and accumulating 
reference information for research papers. 
5 Conclusions and Future Work 
In this paper, we propose to improve academic 
search by utilizing pseudo-anchor information. 
As pseudo-URL and pseudo-anchor text are not 
as explicit as in general web search, more efforts 
are needed for pseudo-anchor extraction. Our 
machine-learning approach has proven success-
ful in automatically extracting implicit anchor 
text. By using the pseudo-anchors in our academ-
ic search system, we see a significant perfor-
mance improvement over the basic approach. 
 
 
Acknowledgments 
We would like to thank Yunxiao Ma and Pu 
Wang for converting paper full-text from PDF to 
HTML format. Jian Shen has been helping us do 
some reference extraction and matching work. 
Special thanks are given to the researchers and 
students taking part in data labeling. 
 
 
 
 
17
References 
E. Amitay. 1998. Using common hypertext links to 
identify the best phrasal description of target web 
documents. In Proc. of the SIGIR'98 Post Confe-
rence Workshop on Hypertext Information Re-
trieval for the Web, Melbourne, Australia. 
G. Attardi, A. Gulli, and F. Sebastiani. 1999. Theseus: 
categorization by context. In Proceedings of the 8th 
International World Wide Web Conference. 
A. Baxter, P. Christen, T. Churches. 2003. A compar-
ison of fast blocking methods for record linkage. In 
ACM SIGKDD'03 Workshop on Data Cleaning, 
Record Linkage and Object consolidation. Wash-
ington DC. 
A. Broder, S. Glassman, M. Manasse, and G. Zweig. 
1997. Syntactic clustering of the Web. In Proceed-
ings of the Sixth International World Wide Web 
Conference, pp. 391-404. 
C.J.C. Burges. 1998. A tutorial on support vector ma-
chines for pattern recognition. Data Mining and 
Knowledge Discovery, 2, 121-167. 
S. Chakrabarti, B. Dom, D. Gibson, J. Kleinberg, P. 
Raghavan, and S. Rajagopalan. 1998. Automatic 
resource list compilation by analyzing hyperlink 
structure and associated text. In Proceedings of the 
7th International World Wide Web Conference. 
K. Chakrabarti, V. Ganti, J. Han, and D. Xin. 2006. 
Ranking objects based on relationships. In SIG-
MOD ?06: Proceedings of the 2006 ACM SIG-
MOD international conference on Management of 
data, pages 371?382, New York, NY, USA. ACM. 
B. Davison. 2000. Topical locality in the web. In SI-
GIR'00: Proceedings of the 23rd annual interna-
tional ACM SIGIR conference on Research and 
development in information retrieval, pages 272- 
279, New York, NY, USA. ACM. 
I.P. Fellegi, and A.B. Sunter. A Theory for Record 
Linkage, Journal of the American Statistical Asso-
ciation, 64, (1969), 1183-1210. 
C. L. Giles, K. Bollacker, and S. Lawrence. 1998. 
CiteSeer: An automatic citation indexing system. 
In IanWitten, Rob Akscyn, and Frank M. Shipman 
III, editors, Digital Libraries 98 - The Third ACM 
Conference on Digital Libraries, pages 89?98, 
Pittsburgh, PA, June 23?26. ACM Press. 
T.H. Haveliwala, A. Gionis, D. Klein, and P. Indyk. 
2002. Evaluating strategies for similarity search on 
the web. In WWW ?02: Proceedings of the 11th in-
ternational conference on World Wide Web, pages 
432?442, New York, NY, USA. ACM. 
K. Jarvelin, and J. Kekalainen. 2000. IR Evaluation 
Methods for Retrieving Highly Relevant Docu-
ments. In Proceedings of the 23rd Annual Interna-
tional ACM SIGIR Conference on Research and 
Development in Information Retrieval (SI-
GIR2000). 
S. Lawrence, C.L. Giles, and K. Bollacker. 1999. Dig-
ital libraries and Autonomous Citation Indexing. 
IEEE Computer, 32(6):67?71. 
A. McCallum, K. Nigam, J. Rennie, and K. Seymore. 
1999. Building Domain-specific Search Engines 
with Machine Learning Techniques. In Proceed-
ings of the AAAI-99 Spring Symposium on Intelli-
gent Agents in Cyberspace. 
A. McCallum, K. Nigam, and L. Ungar. 2000. Effi-
cient clustering of high-dimensional data sets with 
application to reference matching. In Proc. 6th 
ACM SIGKDD Int. Conf. on Knowledge Discov-
ery and Data Mining. 
O.A. McBryan. 1994. Genvl and wwww: Tools for 
taming the web. In In Proceedings of the First In-
ternational World Wide Web Conference, pages 
79-90. 
H. Nanba, M. Okumura. 1999. Towards Multi-paper 
Summarization Using Reference Information. In 
Proc. of the 16th International Joint Conference on 
Artificial Intelligence, pp.926-931. 
H. Nanba, T. Abekawa, M. Okumura, and S. Saito. 
2004. Bilingual PRESRI: Integration of Multiple 
Research Paper Databases. In Proc. of RIAO 2004, 
195-211. 
L. Parsons, E. Haque, H. Liu. 2004. Subspace cluster-
ing for high dimensional data: a review. SIGKDD 
Explorations 6(1): 90-105. 
S.E. Robertson, S. Walker, and M. Beaulieu. 1999. 
Okapi at TREC-7: automatic ad hoc, filtering, VLC 
and filtering tracks. In Proceedings of TREC?99. 
S. Shi, R. Song, and J-R Wen. 2006. Latent Additivity: 
Combining Homogeneous Evidence. Technique 
report, MSR-TR-2006-110, Microsoft Research, 
August 2006. 
S. Shi, F. Xing, M. Zhu, Z.Nie, and J.-R. Wen. 2006. 
Pseudo-Anchor Extraction for Search Vertical Ob-
jects. In Proc. of the 2006 ACM 15th Conference 
on Information and Knowledge Management. Ar-
lington, USA. 
Z. Nie, Y. Zhang, J.-R. Wen, and W.-Y. Ma. 2005. 
Object-level ranking: bringing order to web objects. 
InWWW?05: Proceedings of the 14th international 
conference on World Wide Web, pages 567?574, 
New York, NY, USA. ACM. 
 
18
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 993?1001,
Beijing, August 2010
Corpus-based Semantic Class Mining: 
Distributional vs. Pattern-Based Approaches 
Shuming Shi1    Huibin Zhang2*    Xiaojie Yuan2    Ji-Rong Wen1 
1 Microsoft Research Asia 
2 Nankai University 
{shumings, jrwen}@microsoft.com 
zhanghuibin@126.com; yuanxj@nankai.edu.cn 
 
 
Abstract 
Main approaches to corpus-based seman-
tic class mining include distributional 
similarity (DS) and pattern-based (PB). 
In this paper, we perform an empirical 
comparison of them, based on a publicly 
available dataset containing 500 million 
web pages, using various categories of 
queries. We further propose a frequency-
based rule to select appropriate approach-
es for different types of terms. 
1 Introduction1 
Computing the semantic relationship between 
terms, which has wide applications in natural 
language processing and web search, has been a 
hot topic nowadays. This paper focuses on cor-
pus-based semantic class mining (Lin 1998; Pan-
tel and Lin 2002; Pasca 2004; Shinzato and 
Torisawa, 2005; Ohshima, et al, 2006; Zhang et 
al., 2009), where peer terms (or coordinate terms) 
are discovered from a corpus. 
Existing approaches to semantic class mining 
could roughly be divided into two categories: 
distributional similarity (DS), and pattern-based 
(PB). The first type of work (Hindle, 1990; Lin 
1998; Pantel and Lin 2002) is based on the distri-
butional hypothesis (Harris, 1985), saying that 
terms occurring in analogous (lexical or syntactic) 
contexts tend to be similar. DS approaches basi-
cally exploit second-order co-occurrences to dis-
cover strongly associated concepts. In pattern-
based approaches (Hearst 1992; Pasca 2004; 
Shinzato and Torisawa, 2005; Ohshima, et al, 
2006; Zhang et al, 2009), patterns are applied to 
                                                 
* Work done during an internship at Microsoft 
discover specific relationships between terms, 
from the general first-order co-occurrences. For 
example, ?NP such as NP, NP?, and NP? is a 
popular and high-quality pattern for extracting 
peer terms (and also hyponyms). Besides the nat-
ural language patterns, some HTML tag tree pat-
terns (e.g., the drop down list) are also effective 
in semantic class mining. 
It is worth-noting that the word ?pattern? also 
appears in some DS approaches (Pasca et al, 
2006; Tanev and Magnini, 2006; Pennacchiotti 
and Pantel, 2009), to represent the context of a 
term or a term-pair, e.g., ?(invent, subject-of)? 
for the term ?Edison?, and ?- starring -? for the 
term-pair ?(The Terminal, Tom Hanks)?. Alt-
hough ?patterns? are utilized, we categorize them 
as DS approaches rather than PB, because they 
match the DS framework well. In this paper, PB 
only refers to the approaches that utilize patterns 
to exploit first-order co-occurrences. And the 
patterns in DS approaches are called contexts in 
the following part of this paper. 
Progress has been made and promising results 
have been reported in the past years for both DS 
and PB approaches. However, most previous re-
search work (some exceptions are discussed in 
related work) involves solely one category of ap-
proach. And there is little work studying the 
comparison of their performance for different 
types of terms (we use ?term? to represent a sin-
gle word or a phrase). 
In this paper, we make an empirical study of 
this problem, based on a large-scale, publicly 
available dataset containing 500 million web 
pages. For each approach P, we build a term-
similarity graph G(P), with vertices representing 
terms, and edges being the confidence that the 
two terms are peers. Approaches are compared 
by the quality of their corresponding term graphs. 
993
We measure the quality of a term graph by set 
expansion. Two query sets are adopted: One con-
tains 49 semantic classes of named entities and 
20220 trials (queries), collected by Pantel et al 
(2009) from Wikipedia2; and the other contains 
100 queries of five lexical categories (proper 
nouns, common nouns, verbs, adjectives, and 
adverbs), built in this paper for studying the per-
formance comparison on different term types. 
With the dataset and the query sets, we study the 
comparison of DS and PB. Key observations and 
preliminary conclusions are, 
?   DS vs. PB: DS approaches perform much 
better on common nouns, verbs, adjectives, 
and adverbs; while PB generates higher-
quality semantic classes for proper nouns. 
?   Lexical vs. Html-tag patterns: If only lexi-
cal patterns are adopted in PB, the perfor-
mance drops significantly; while the perfor-
mance only becomes slightly worse with only 
Html-tag patterns being included. 
?   Corpus-size: For proper nouns, PB beats 
DS even based on a much smaller corpus; 
similarly, for other term types, DS performs 
better even with a smaller corpus. 
Given these observations, we further study the 
feasibility of selecting appropriate approaches for 
different term types to obtain better results. A 
simple and effective frequency-based rule is pro-
posed for approach-selection. Our online seman-
tic mining system (NeedleSeek)3 adopts both PB 
and DS to build semantic classes. 
2 Related Work 
Existing efforts for semantic class mining has 
been done upon various types of data sources, 
including text-corpora, search-results, and query 
logs. In corpus-based approaches (Lin 1998; Lin 
and Pantel 2001; Pantel and Lin 2002; Pasca 
2004; Zhang et al, 2009), semantic classes are 
obtained by the offline processing of a corpus 
which can be unstructured (e.g., plain text) or 
semi-structured (e.g., web pages). Search-results-
based approaches (Etzioni et al, 2004; Kozareva 
et al, 2008; Wang and Cohen, 2008) assume that 
multiple terms (or, less often, one term) in a se-
mantic class have been provided as seeds. Other 
terms in the class are retrieved by sending queries 
                                                 
2 http://www.wikipedia.org/ 
3 http://needleseek.msra.cn/ 
(constructed according to the seeds) to a web 
search engine and mining the search results. Que-
ry logs are exploited in (Pasca 2007; Komachi 
and Suzuki, 2008; Yamaguchi 2008) for semantic 
class mining. This paper focuses on corpus-based 
approaches. 
As has been mentioned in the introduction 
part, primarily two types of methodologies are 
adopted: DS and PB. Syntactic context infor-
mation is used in (Hindle, 1990; Ruge, 1992; Lin 
1998; Lin and Pantel, 2001; Pantel and Lin, 2002) 
to compute term similarities. The construction of 
syntactic contexts requires sentences to be parsed 
by a dependency parser, which may be extremely 
time-consuming on large corpora. As an alterna-
tive, lexical context (such as text window) has 
been studied (Pantel et al, 2004; Agirre et al, 
2009; Pantel et al, 2009). In the pattern-based 
category, a lot of work has been done to discover 
term relations by sentence lexical patterns 
(Hearst 1992; Pasca 2004), HTML tag patterns 
(Shinzato and Torisawa, 2005), or both (Shi et al, 
2008; Zhang et al, 2009). In this paper, our focus 
is not one specific methodology, but the compari-
son and combination of them. 
A small amount of existing work is related to 
the comparison or combination of multiple meth-
ods. Pennacchiotti and Pantel (2009) proposed a 
feature combination framework (named ensemble 
semantic) to combine features generated by dif-
ferent extractors (distributional and ?pattern-
based?) from various data sources. As has been 
discussed in the introduction, in our terminology, 
their ?pattern-based? approaches are actually DS 
for term-pairs. In addition, their study is based on 
three semantic classes (actors, athletes, and musi-
cians), all of which are proper nouns. Differently, 
we perform the comparison by classifying terms 
according to their lexical categories, based on 
which additional insights are obtained about the 
pros and cons of each methodology. Pantel et al, 
(2004) proposed, in the scenario of extracting is-
a relations, one pattern-based approach and com-
pared it with a baseline syntactic distributional 
similarity method (called syntactic co-occurrence 
in their paper). Differently, we study the compar-
ison in a different scenario (semantic class min-
ing). In addition, they did not differentiate the 
lexical types of terms in the study. The third dif-
ference is that we proposed a rule for method-
selection while they did not. In (Pasca and Durme, 
994
2008), clusters of distributional similar terms 
were adopted to expand the labeled semantic 
classes acquired from the ?such as | including? 
pattern. Although both patterns and distributional 
similarity were used in their paper, they did not 
do any comparison about their performance. 
Agirre et al (2009) compared DS approaches 
with WordNet-based methods in computing word 
similarity and relatedness; and they also studied 
the combination of them. Differently, the meth-
ods for comparison in our paper are DS and PB. 
3 Similarity Graph Construction 
A key operation in corpus-based semantic class 
mining is to build a term similarity graph, with 
vertices representing terms, and edges being the 
similarity (or distance) between terms. Given the 
graph, a clustering algorithm can be adopted to 
generate the final semantic classes. Now we de-
scribe the state-of-the-art DS and PB approaches 
for computing term similarities. 
3.1 Distributional Similarity 
DS approaches are based on the distributional 
hypothesis (Harris, 1985), which says that terms 
appearing in analogous contexts tend to be simi-
lar. In a DS approach, a term is represented by a 
feature vector, with each feature corresponding to 
a context in which the term appears. The similari-
ty between two terms is computed as the similari-
ty between their corresponding feature vectors. 
Different approaches may have different ways of 
1) defining a context, 2) assigning feature values, 
or 3) measuring the similarity between two fea-
ture vectors. 
 
Contexts 
Text window (window size: 2, 4) 
Syntactic 
Feature value PMI 
Similarity measure Cosine, Jaccard 
Table 1. DS approaches implemented in this paper 
 
Mainly two kinds of contexts have been exten-
sively studied: syntactic context and lexical con-
text. The construction of syntactic contexts relies 
on the syntactic parsing trees of sentences, which 
are typically the output of a syntactic parser. Giv-
en a syntactic tree, a syntactic context of a term w 
can be defined as the parent (or one child) of w in 
the tree together with their relationship (Lin, 
1998; Pantel and Lin, 2002; Pantel et al, 2009). 
For instance, in the syntactic tree of sentence 
?this is an interesting read for anyone studying 
logic?, one context of the word ?logic? can be 
defined as ?study V:obj:N?. In this paper, we 
adopt Minipar (Lin, 1994) to parse sentences and 
to construct syntactic trees. 
One popular lexical context is text window, 
where a context c for a term w in a sentence S is 
defined as a substring of the sentence containing 
but removing w. For example, for sentence 
??w1w2w3ww4w5w6??, a text window context 
(with size 4) of w can be ?w2w3w4w5?. It is typi-
cally time-consuming to construct the syntactic 
trees for a large-scale dataset, even with a light-
weight syntactic parser like Minipar. The con-
struction of lexical contexts is much more effi-
cient because it does not require the syntactic 
dependency between terms. Both contexts are 
studied in this paper. 
After defining contexts for a term w, the next 
step is to construct a feature vector for the term: 
F(w)=(fw1, fw2?, fw,m), where m is the number of 
distinct contexts, and fw,c is the feature value of 
context c with respect to term w. Among all the 
existing approaches, the dominant way of assign-
ing feature values (or context values) is compu-
ting the pointwise mutual information (PMI) be-
tween the feature and the term, 
                
             
             
 (3.1) 
where F(w,c) is the frequency of context c occur-
ring for term w, F(w,*) is the total frequency of 
all contexts for term w, F(*,c) is the frequency of 
context c for all terms, and F(*,*) is the total fre-
quency of all context for all terms. They are cal-
culated as follows respectively, 
 
       ?             
       ?             
       ? ?           
 
     
(3.2) 
where m and n are respectively the distinct num-
bers of contexts and terms. 
Following state-of-the-art, we adopt PMI in 
this paper for context weighting. 
Given the feature vectors of terms, the simi-
larity of any two terms is naturally computed as 
the similarity of their corresponding feature vec-
tors. Cosine similarity and Jaccard similarity 
(weighted) are implemented in our experiments, 
         ?  ?  
?      
??   
 
  ??   
 
 
  (3.3) 
995
          ?  ?  
?           
?     ?     ?           
  (3.4) 
Jaccard similarity is finally used in presenting 
our experimental results (in Section 6), because it 
achieves higher performance. 
3.2 Pattern-based Approaches 
In PB approaches, a list of carefully-designed (or 
automatically learned) patterns is exploited and 
applied to a text collection, with the hypothesis 
that the terms extracted by applying each of the 
patterns to a specific piece of text tend to be simi-
lar. Two categories of patterns have been studied 
in the literature: sentence lexical patterns, and 
HTML tag patterns. Table-2 lists some popular 
patterns utilized in existing semantic class mining 
work (Heast 1992; Pasca 2004; Kozareva et al, 
2008; Zhang et al, 2009). In the table, ?T? means 
a term (a word or a phrase). Exactly the same set 
of patterns is employed in implementing our pat-
tern-based approaches in this paper. 
 
Type Pattern 
Lexical 
T {, T}*{,} (and|or) {other} T 
(such as | including) T (and|,|.) 
T, T, T {,T}* 
Tag 
<ul>  <li> T </li>  ?  <li> T </li>  </ul> 
<ol> <li> T </li> ?  <li> T </li> </ol> 
<select> <option> T ?<option> T </select> 
<table>  <tr> <td> T </td> ? <td> T </td> </tr> ... </table> 
Other Html-tag repeat patterns 
Table 2. Patterns employed in this paper (Lexical: 
sentence lexical patterns; Tag: HTML tag patterns) 
We call the set of terms extracted by applying 
a pattern one time as a raw semantic class 
(RASC). The term similarity graph needs to be 
built by aggregating the information of the ex-
tracted RASCs. 
One basic idea of estimating term similarity is 
to count the number of RASCs containing both of 
them. This idea is extended in the state-of-the-art 
approaches (Zhang et al, 2009) to distinguish the 
reliability of different patterns and to punish term 
similarity contributions from the same domain 
(or site), as follows, 
          ?      ?          
  
   
 
 
   
 (3.5) 
where Ci,j is a RASC containing both term a and 
term b, P(Ci,j) is the pattern via which the RASC 
is extracted, and w(P) is the weight of pattern P. 
The above formula assumes all these RASCs be-
long to m sites (or domains) with Ci,j extracted 
from a page in site i, and ki being the number of 
RASCs corresponding to site i. 
In this paper, we adopt an extension of the 
above formula which considers the frequency of 
a single term, as follows, 
 Sim*(a, b) = Sim(a, b)  ?              (3.6) 
where IDF(a)=log(1+N/N(a)), N is the total num-
ber of RASCs, and N(a) is the number of RASCs 
containing a. In the experiments, we simply set 
the weight of every pattern type to be the same 
value (1.0). 
4 Compare PB and DS 
We compare PB and DS by the quality of the 
term similarity graphs they generated. The quali-
ty of a term graph is measured by set expansion: 
Given a list of seed terms (e.g., S={lent, epipha-
ny}) belonging to a semantic class, our task is to 
find other members of this class, such as advent, 
easter, and christmas. 
In this section, we first describe our set expan-
sion algorithm adopted in our study. Then DS 
and PB are compared in terms of their set-
expansion performance. Finally we discuss ways 
of selecting appropriate approaches for different 
types of seeds to get better expansion results. 
4.1 Set Expansion Algorithm 
Having at hand the similarity graph, set expan-
sion can be implemented by selecting the terms 
most similar to the seeds. So given a query 
Q={s1, s2, ?, sk}, the key is to compute       , 
the similarity between a term t and the seed-set 
Q. Naturally, we define it as the weighted aver-
age similarity between t and every seed in Q, 
        ?             
 
     (4.1) 
where   is the weight of seed   , which can be a 
constant value, or a function of the frequency of 
term    in the corpus. Although Formula 3.6 can 
be adopted directly for calculating Sim(t,si), we 
use the following rank-based formula because it 
generate better expansion results. 
           
 
              
 (4.2) 
where         is the rank of term t among the 
neighbors of   . 
In our experiments, we fix  =1 and  =10. 
996
4.2 Compare DS with PB 
In order to have a comprehensive comparison of 
the two approaches, we intentionally choose 
terms of diverse types and do experiments based 
on various data scales. We classify terms into 5 
types by their lexical categories: proper nouns, 
common nouns, verbs, adjectives, and adverbs. 
The data scales for experiments are from one mil-
lion to 500 million web pages. Please refer to 
sections 5.1 and 5.2 for more details about the 
corpora and seeds used for experiments. 
Experimental results (refer to Section 6) will 
show that, for proper nouns, the ranking of ap-
proaches (in terms of performance) is: 
PB > PB-HtmlTag > DS  PB-Lexical 
While for common nouns, verbs, adjectives, 
and adverbs, we have: 
DS > PB 
Here ?PB-lexical? means only the lexical pat-
terns of Table 2 are adopted. Similarly, ?PB-
HtmlTag? represents the PB approach with only 
Html-tag patterns being utilized. 
Please pay attention that this paper by no 
means covers all PB or DS approaches (although 
we have tried our best to include the most popu-
lar ones). For PB, there are of course other kinds 
of patterns (e.g., patterns based on deeper linguis-
tic analysis). For DS, other types of contexts may 
exist in addition to those listed in Table 1. So in 
interpreting experimental results, making obser-
vations, and drawing preliminary conclusions, we 
only means the patterns in Table 2 for PB and 
Table 1 for DS. It will be an interesting future 
work to include more DS and PB approaches in 
the study. 
In order to understand why PB performs so 
well in dealing with proper nouns while so badly 
for other term categories, we calculated the fre-
quency of each seed term in the extracted RASCs, 
the output of the pattern-matching algorithm. We 
define the normalized frequency of a term to be 
its frequency in the RASCs divided by the fre-
quency in the sentences of the original documents 
(with duplicate sentences merged). Then we de-
fine the mean normalized frequency (MNF) of a 
seed set S, as follows, 
        
?            
   
 (4.3) 
where Fnorm(t) is the normalized frequency of t. 
The MNF values for the five seed sets are 
listed in Table 3, where we can see that proper 
nouns have the largest MNF values, followed by 
common nouns. In other words, the patterns in 
Table 2 capture the relations of more proper 
nouns than other term categories. 
 
Seed Categories Terms MNF 
Proper nouns 40 0.2333 
Common nouns 40 0.0716 
Verbs 40 0.0099 
Adjectives 40 0.0126 
Adverbs 40 0.0053 
Table 3. MNF values of different seed categories 
As mentioned in the introduction, the PB and 
DS approaches we studied capture first-order and 
second-order term co-occurrences respectively. 
Some existing work (e.g., Edmonds, 1997) 
showed that second-order co-occurrence leads to 
better results for detecting synonymy. Consider-
ing that a high proportion of coordinate terms of 
verbs, adjectives, and adverbs are their synonyms 
and antonyms, it is reasonable that DS behaves 
better for these term types because it exploits se-
cond-order co-occurrence. For PB, different from 
the standard way of dealing with first-order co-
occurrences where statistics are performed on all 
pairs of near terms, a subset of co-occurred terms 
are selected in PB by specific patterns. The pat-
terns in Table-2 help detecting coordinate proper 
nouns, because they are frequently occurred to-
gether obeying the patterns in sentences or web 
pages. But it is not the case for other term types. 
It will be interesting to study the performance of 
PB when more pattern types are added. 
4.3 Approach Selection 
Having observed that the two approaches per-
form quite differently on every type of queries 
we investigated, we hope we can improve the 
expansion performance by smartly selecting an 
approach for each query. In this section, we pro-
pose and study several approach-selection meth-
ods, by which we hope to gain some insights 
about the possibility and effectiveness of combin-
ing DS and PB for better set expansion. 
Oracle selection: In order to get an insight 
about the upper bound that we could obtain when 
combing the two methods, we implement an ora-
cle that chooses, for each query, the approach 
that generates better expansion results. 
997
Frequency-based selection: It is shown in 
Table 3 that the mean normalized frequency of 
proper nouns is much larger than other terms. 
Motivated by this observation, we select a set 
expansion methodology for each query as fol-
lows: Select PB if the normalized frequency val-
ues of all terms in the query are larger than 0.1; 
otherwise choose DS. 
We demonstrate, in Section 6.3, the effective-
ness of the above selection methods. 
5 Experimental Setup 
5.1 Dataset and Exp. Environment 
We adopt a public-available dataset in our exper-
iments: ClueWeb094. This is a very large dataset 
collected by Carnegie Mellon University in early 
2009 and has been used by several tracks of the 
Text Retrieval Conference (TREC)5. The whole 
dataset consists of 1.04 billion web pages in ten 
languages while only those in English, about 500 
million pages, are used in our experiments. The 
reason for selecting such a dataset is twofold: 
First, it is a corpus large enough for conducting 
web-scale experiments and getting meaningful 
results. Second, since it is publicly available, it is 
possible for other researchers to reproduce the 
experiments in the paper. 
 
Corpora 
Docs 
(millions) 
Sentences 
(millions) 
Description 
Clue500 500 13,000 All En pages in ClueWeb09 
Clue050  50   1,600 ClueWeb09 category B  
Clue010  10      330 Sampling from Clue050 
Clue001   1       42 Sampling from Clue050 
Table 4. Corpora used in experiments 
To test the impact of corpus size on set expan-
sion performance, four corpora are derived from 
the dataset, as outlined in Table 4. The Clue500 
corpus contains all the 500 million English web 
pages in the dataset; while Clue050 is a subset of 
ClueWeb09 (named category B) containing 50 
million English web pages. The remaining two 
corpora are respectively the 1/5 and 1/50 random 
sampling of web pages from Clue050. 
Documents in the corpora are stored and pro-
cessed in a cluster of 40 four-core machines. 
                                                 
4 http://boston.lti.cs.cmu.edu/Data/clueweb09/  
5 http://trec.nist.gov/  
5.2 Query Sets 
We perform our study using two query sets. 
WikiGold: It was collected by Pantel et al 
(2009) from the ?List of? pages in Wikipedia and 
used as the gold standard in their paper. This gold 
standard consists of 49 entity sets, and 20220 tri-
als (used as queries) of various numbers of seeds. 
Most seeds in the query set are named entities. 
Please refer to Pantel et al (2009) for details of 
the gold standard. 
Mix100: This query set consists of 100 queries 
in five categories: verbs, adjectives, adverbs, 
common nouns, and proper nouns. There are 20 
queries in every category and two seeds in every 
query. The query set was built by the following 
steps: First, 20 terms of each category were ran-
domly selected from a term list (which is con-
structed by part-of-speech tagging the Clue050 
corpus and removing low-frequency terms), and 
were treated as the first seed of the each query. 
Then, we manually added one additional seed for 
each query. The reason for utilizing two seeds 
instead of one is the observation that a large por-
tion of the terms selected in the previous step be-
long to multiple categories. For example, ?color-
ful? is both an adjective and a proper noun (a 
Japanese manga). 
5.3 Results Labeling 
No human labeling efforts are needed for the ex-
pansion results of the WikiGold query set. Every 
returned term is automatically judged to be 
?Good? (otherwise ?Bad?) if it appears in the 
corresponding gold standard entity set. 
For Mix100, the search results of various ap-
proaches are merged and labeled by three human 
labelers. Each labeler assigns each term in the 
search results a label of ?Good?, ?Fair? or ?Bad?. 
The labeling agreement values (measured by per-
centage agreement) between labelers I and II, I 
and III, II and III are respectively 0.82, 0.81, and 
0.81. The ultimate judgment of each result term 
is obtained from the three labelers by majority 
voting. In the case of three labelers giving mutu-
ally different results (i.e., one ?Good?, one ?Fair? 
and one ?Bad?), the ultimate judgment is set to 
?Fair? (the average). 
5.4 Evaluation Metrics 
After removing seeds from the expansion results, 
we adopt the following metrics to evaluate the 
998
results of each query. The evaluation score on a 
query set is the average over all the queries. 
Precision@k: The percentage of relevant 
(good or fair) terms in the top-k expansion results 
(terms labeled as ?Fair? are counted as 0.5) 
Recall@k: The ratio of relevant terms in the 
top-k results to the total number of relevant terms 
R-Precision: Precision@R where R is the total 
number of terms labeled as ?Good? 
Mean average precision (MAP): The average 
of precision values at the positions of all good or 
fair results 
6 Experimental Results 
6.1 Overall Performance Comparison 
Table 5 lists the performance (measured by 
MAP, R-precision, and the precisions at ranks 25, 
50, and 100) of some key approaches on corpus 
Clue050 and query set WikiGold. The results of 
query set Mix100 are shown in Table 6. In the 
results, TWn represents the DS approach with 
text-window of size n as contexts, Syntactic is the 
DS approach with syntactic contexts, PB-Lexical 
means only the lexical patterns of Table 2 are 
adopted, and PB-HtmlTag represents the PB ap-
proach with only Html-tag patterns utilized. 
 
Approach MAP R-Prec P@25 P@50 P@100 
TW2 0.218 0.287 0.359 0.278 0.204 
TW4 0.152 0.210 0.325 0.244 0.173 
Syntactic 0.170 0.247 0.314 0.242 0.178 
PB-Lexical 0.227 0.276 0.352 0.272 0.190 
PB-HtmlTag 0.354 0.417 0.513 0.413 0.311 
PB 0.362 0.424 0.520 0.418 0.314 
Pantel-24M N/A 0.264 0.353 0.298 0.239 
Pantel-120M N/A 0.356 0.377 0.319 0.250 
Pantel-600M N/A 0.404 0.407 0.347 0.278 
Table 5. Performance comparison on the Clue050 cor-
pus (query set: WikiGold) 
It is shown that PB gets much higher evalua-
tion scores than other approaches on the WikiG-
old query set and the proper-nouns category of 
Mix100. While for other seed categories in 
Mix100, TW2 return significantly better results. 
We noticed that most seeds in WikiGold are 
proper nouns. So the experimental results tend to 
indicate that the performance comparison be-
tween state-of-the-art DS and PB approaches de-
pends on the types of terms to be mined, specifi-
cally, DS approaches perform better in mining 
semantic classes of common nouns, verbs, adjec-
tives, and adverbs; while state-of-the-art PB ap-
proaches are more suitable for mining semantic 
classes of proper nouns. The performance of PB 
is low in dealing with other types of terms (espe-
cially adverbs). The performance of PB drops 
significantly if only lexical patterns are used; and 
the HtmlTag-only version of PB performs only 
slightly worse than PB. 
The observations are verified by the precision-
recall graph in Figure 1 on Clue500. The results 
of the syntactic approach on Clue500 are not in-
cluded, because it is too time-consuming to parse 
all the 500 million web pages by a dependency 
parser (even using a high-performance parser like 
Minipar). It took overall about 12,000 CPU-hours 
to parse all the sentences in Clue050 by Minipar. 
 
Query types & 
Approaches 
MAP P@5 P@10 P@20 
Proper 
Nouns 
TW2 0.302 0.835 0.810 0.758 
PB 0.336 0.920 0.838 0.813 
Common 
Nouns 
TW2 0.384 0.735 0.668 0.595 
PB 0.212 0.640 0.548 0.485 
Verbs 
TW2 0.273 0.655 0.543 0.465 
PB 0.176 0.415 0.373 0.305 
Adjectives 
TW2 0.350 0.655 0.563 0.473 
PB 0.120 0.335 0.285 0.234 
Adverbs 
TW2 0.432 0.605 0.505 0.454 
PB 0.043 0.100 0.095 0.089 
Table 6. Performance comparison on different query 
types (Corpus: Clue050; query set: Mix100) 
 
Figure 1. Precision and recall of various approaches 
(query set: WikiGold) 
The methods labeled Pantel-24M etc. (in Table 
5 and Figure 1) are the approaches presented in 
(Pantel et al, 2009) on their corpus (called 
Web04, Web20, and Web100 in the paper) con-
taining respectively 24 million, 120 million, and 
600 million web pages. Please pay attention that 
their results and ours may not be directly compa-
rable, because different corpora and set-
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
R
e
c
a
ll
 
Precision 
TW221 (Clue500) PB (Clue500)
PB (Clue010) PB (Clue001)
Pantel-600M Pantel-120M
999
expansion algorithms were used. Their results are 
listed here for reference purpose only. 
6.2 Corpus Size Effect 
Table 7 shows the performance (measured by 
MAP) of two approaches on query set Mix100, 
by varying corpus size. We observed that the per-
formance of TW2 improves rapidly along with 
the growth of corpus size from one million to 50 
million documents. From Clue050 to Clue500, 
the performance is slightly improved. 
 
Query types & 
Approaches 
Clue001 Clue010 Clue050 Clue500 
Proper 
Nouns 
TW2 0.209 0.265 0.302 0.311 
PB 0.355 0.351 0.336 0.327 
Common 
Nouns 
TW2 0.259 0.348 0.384 0.393 
PB 0.200 0.234 0.212 0.205 
Verbs 
TW2 0.224 0.268 0.273 0.278 
PB 0.101 0.134 0.176 0.148 
Adjectives 
TW2 0.309 0.326 0.350 0.353 
PB 0.077 0.158 0.120 0.129 
Adverbs 
TW2 0.413 0.423 0.432 0.437 
PB 0.028 0.058 0.043 0.059 
Table 7. Effect of different corpus size (query set: 
Mix100; metric: MAP) 
For PB, however, the performance change is 
not that simple. For proper nouns, the best per-
formance (in terms of MAP) is got on the two 
small corpora Clue001 and Clue010; and the 
score does not increase when corpus size grows. 
Different observations are made on WikiGold 
(see Figure 1), where the performance improves a 
lot with the data growth from Clue001 to 
Clue010, and then stabilizes (from Clue010 to 
Clue500). For other term types, the MAP scores 
do not grow much after Clue010. To our current 
understanding, the reason may be due to the two-
fold effect of incorporating more data in mining: 
bringing useful information as well as noise. 
Clue001 contains enough information, which is 
fully exploited by the PB approach, for expand-
ing the proper-nouns in Mix100. So the perfor-
mance of PB on Clue001 is excellent. The named 
entities in WikiGold are relatively rare, which 
requires a larger corpus (Clue010) for extracting 
peer terms from. But when the corpus gets larger, 
we may not be able to get more useful infor-
mation to further improve results quality. 
Another interesting observation is that, for 
proper nouns, the performance of PB on Clue001 
is even much better than that of TW2 on corpus 
Clue500. Similarly, for other query types (com-
mon nous, verbs, adjectives, and adverbs), TW2 
easily beats PB even with a much smaller corpus. 
6.3 Approach Selection 
Here we demonstrate the experimental results of 
combining DS and PB with the methods we pro-
posed in Section 4.3. Table 8 shows the combina-
tion of PB and TW2 on corpus Clue050 and que-
ry set Mix100. The overall performance relies on 
the number (or percentage) of queries in each 
category. Two ways of mixing the queries are 
tested: avg(4:1:1:1:1) and avg(1:1:1:1:1), where 
the numbers are the proportion of proper nouns, 
common nouns, verbs, adjectives, and adverbs. 
 
Approach 
Avg (1:1:1:1:1) Avg (4:1:1:1:1) 
P@5 P@10 P@20 P@5 P@10 P@20 
TW2 0.697 0.618 0.548 0.749 0.690 0.627 
PB 0.482 0.428 0.385 0.646 0.581 0.545 
Oracle 0.759 0.663 0.591 0.836 0.759 0.695 
Freq-based 0.721 0.633 0.570 0.799 0.723 0.671 
Table 8. Experiments of combining both approaches 
(Corpus: Clue050; query set: Mix100) 
The expansion performance is improved a lot 
with our frequency-based combination method. 
As expected, oracle selection achieves great per-
formance improvement, which shows the large 
potential of combining DS and PB. Similar re-
sults (omitted due to space limitations) are ob-
served on the other corpora. 
Our online semantic mining system (Needle-
Seek, http://needleseek.msra.cn) adopts both PB 
and DS for semantic class construction. 
7 Conclusion 
We compared two mainstream methods (DS and 
PB) for semantic class mining, based on a dataset 
of 500 million pages and using five term types. 
We showed that PB is clearly adept at extracting 
semantic classes of proper nouns; while DS is 
relatively good at dealing with other types of 
terms. In addition, a small corpus is sufficient for 
each approach to generate better semantic classes 
of its ?favorite? term types than those obtained 
by its counterpart on a much larger corpus. Final-
ly, we tried a frequency-based method of com-
bining them and saw apparent performance im-
provement. 
 
1000
References  
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana 
Kravalova, Marius Pasca, Aitor Soroa. A Study on 
Similarity and Relatedness Using Distributional 
and WordNet-based Approaches. NAACL-HLT 
2009. 
Philip Edmonds. 1997. Choosing the Word most Typ-
ical in Context Using a Lexical Co-Occurrence 
Network. ACL'97, pages 507-509. 
Oren Etzioni, Michael Cafarella, Doug Downey, Stan-
ley Kok, Ana-Maria Popescu, Tal Shaked, Stephen 
Soderland, Daniel Weld, and Alexander Yates. 
2004. Web-Scale Information Extraction in 
KnowItAll. WWW?04, pages 100?110, New York. 
Zelig S. Harris. 1985. Distributional Structure. The 
Philosophy of Linguistics. New York: Oxford Uni-
versity Press. 
Marti A. Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. COLING?92, 
Nantes, France. 
Donald Hindle. 1990. Noun Classification from Predi-
cate-Argument Structures. In ACL?90, pages 268?
275, Pittsburg, Pennsylvania, June. 
Mamoru Komachi and Hisami Suzuki. Minimally Su-
pervised Learning of Semantic Knowledge from 
Query Logs. IJCNLP 2008, pages 358?365, 2008. 
Zornitsa Kozareva, Ellen Riloff, Eduard Hovy. 2008. 
Semantic Class Learning from the Web with Hypo-
nym Pattern Linkage Graphs. ACL?08: HLT. 
Dekang Lin. 1994. Principar - an Efficient, Broad-
Coverage, Principle-based Parser. COLING?94, pp. 
482-488. 
Dekang Lin. 1998. Automatic Retrieval and Cluster-
ing of Similar Words. COLING-ACL?98, pages 
768-774. 
Dekang Lin and Patrick Pantel. 2001. Induction of 
Semantic Classes from Natural Language Text. 
SIGKDD?01, pages 317-322. 
Hiroaki Ohshima, Satoshi Oyama and Katsumi 
Tanaka. 2006. Searching Coordinate Terms with 
their Context from the Web. WISE?06. 
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu and Vishnu Vyas. 2009. Web-Scale 
Distributional Similarity and Entity Set Expansion. 
EMNLP?09. Singapore. 
Patrick Pantel and Dekang Lin. 2002. Discovering 
Word Senses from Text. SIGKDD'02. 
Patric Pantel, Deepak Ravichandran, and Eduard 
Hovy. 2004. Towards Terascale Knowledge Acqui-
sition. COLING?04, Geneva, Switzerland. 
Marius Pasca. 2004. Acquisition of Categorized 
Named Entities for Web Search. CIKM?04. 
Marius Pasca. 2007. Weakly-Supervised Discovery of 
Named Entities Using Web Search Queries. 
CIKM?07. pp. 683-690. 
Marius Pasca and Benjamin Van Durme. 2008. Weak-
ly-supervised Acquisition of Open-Domain Classes 
and Class Attributes from Web Documents and 
Query Logs. ACL?08. 
Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei 
Lifchits, and Alpa Jain. 2006. Organizing and 
Searching the World Wide Web of Facts - Step 
One: The One-Million Fact Extraction Challenge. 
In AAAI?06. 
Marco Pennacchiotti and Patrick Pantel. 2009. Entity 
Extraction via Ensemble Semantics.  EMNLP?09. 
Gerda Ruge. 1992. Experiments on Linguistically-
Based Term Associations. Information Processing 
& Management, 28(3): 317-32. 
Keiji Shinzato and Kentaro Torisawa. 2005. A Simple 
WWW-based Method for Semantic Word Class 
Acquisition. Recent Advances in Natural Language 
Processing (RANLP?05), Borovets, Bulgaria. 
Shuming Shi, Xiaokang Liu, Ji-Rong Wen. 2008. Pat-
tern-based Semantic Class Discovery with Multi-
Membership Support. CIKM?08, Napa Valley, Cali-
fornia, USA. 
Hristo Tanev and Bernardo Magnini. 2006. Weakly 
Supervised Approaches for Ontology Population. 
EACL'2006, Trento, Italy. 
Richard C. Wang and William W. Cohen. 2008. Itera-
tive Set Expansion of Named Entities Using the 
Web. ICDM?08, pages 1091?1096. 
Masashi Yamaguchi, Hiroaki Ohshima, Satoshi Oya-
ma, and Katsumi Tanaka. Unsupervised Discovery 
of Coordinate Terms for Multiple Aspects from 
Search Engine Query Logs. The 2008 
IEEE/WIC/ACM International Conference on Web 
Intelligence and Intelligent Agent Technology. 
Huibin Zhang, Mingjie Zhu, Shuming Shi, and Ji-
Rong Wen. 2009. Employing Topic Models for 
Pattern-based Semantic Class Discovery. ACL?09, 
Singapore. 
 
1001
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 468?478,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Improving Web Search Ranking by Incorporating Structured 
Annotation of Queries* 
 
 
Xiao Ding1, Zhicheng Dou2, Bing Qin1, Ting Liu1, Ji-Rong Wen3 
 
1Research Center for Social Computing and Information Retrieval 
Harbin Institute of Technology, China 
 
2Microsoft Research Asia, Beijing 100190, China 
 
3Renmin University of China, Beijing, China 
1{xding, qinb, tliu}@ir.hit.edu.cn; 
2zhichdou@microsoft.com; 3jirong.wen@gmail.com 
 
 
 
 
 
 
Abstract? 
Web users are increasingly looking for 
structured data, such as lyrics, job, or recipes, 
using unstructured queries on the web. 
However, retrieving relevant results from such 
data is a challenging problem due to the 
unstructured language of the web queries. In 
this paper, we propose a method to improve 
web search ranking by detecting Structured 
Annotation of queries based on top search 
results. In a structured annotation, the original 
query is split into different units that are 
associated with semantic attributes in the 
corresponding domain. We evaluate our 
techniques using real world queries and achieve 
significant improvement. 
1 Introduction 
Search engines are getting more sophisticated by 
utilizing information from multiple diverse sources. 
One such valuable source of information is 
structured and semi-structured data, which is not 
very difficult to access, owing to information 
extraction (Wong et al, 2009; Etzioni et al, 2008; 
Zhai and Liu 2006) and semantic web efforts. 
                                                          
? *Work was done when the first author was visiting Microsoft 
Research Asia 
Driving the web search evolution are the user 
needs. Users usually have a template in mind when 
formulating queries to search for information. 
Agarwal et al, (2010) surveyed a search log of 15 
million queries from a commercial search engine. 
They found that 90% of queries follow certain 
templates. For example, by issuing the query 
?taylor swift lyrics falling in love?, the users are 
actually seeking for the lyrics of the song ?Mary's 
Song (oh my my my)? by artist Taylor Swift. The 
words ?falling in love? are actually part of the 
lyrics they are searching for. However, some top 
search results are irrelevant to the query, although 
they contain all the query terms. For example, the 
first top search result shown in Figure 1(a) does 
not contain the required lyrics. It just contains the 
lyrics of another song of Taylor Swift, rather than 
the song that users are seeking. 
A possible way to solve the above ranking 
problem is to understand the underlying query 
structure. For example, after recognizing that 
?taylor swift? is an artist name and ?falling in love? 
are part of the lyrics, we can improve the ranking 
by comparing the structured query with the 
corresponding structured data in documents 
(shown in Figure 1(b)). Some previous studies 
investigated how to extract structured information 
from user queries, such as query segmentation 
(Bergsma and Wang, 2007). The task of query 
segmentation is to separate the query words into 
468
disjointed segments so that each segment maps to a 
semantic unit (Li et al, 2011). For example, the 
segmentation of the query ?taylor swift lyrics 
falling in love? can be ?taylor swift | lyrics | falling 
in love?. Since query segmentation cannot tell 
?talylor swift? is an artist name and ?falling in love? 
are part of lyrics, it is still difficult for us to judge 
whether each part of the query segmentations 
matches the right field of the documents or not 
(such as judge whether ?talylor swift? matches the 
artist name in the document). Recently, a lot of 
work (Sarkas et al, 2010; Li et al, 2009) proposed 
the task of structured annotation of queries which 
aims to detect the structure of the query and assign 
a specific label to it. However, to our knowledge, 
the previous methods do not exploit an effective 
approach for improving web search ranking by 
incorporating structured annotation of queries. 
In this paper, we investigate the possibility of 
using structured annotation of queries to improve 
web search ranking. Specifically, we propose a 
greedy algorithm which uses the structured data 
(named annotated tokens in Figure 1(b)) extracted 
from the top search results to annotate the latent 
structured semantics in web queries. We then 
compute matching scores between the annotated 
query and the corresponding structured 
information contained in documents. The top 
search results can be re-ranked according to the 
matching scores. However, it is very difficult to 
extract structured data from all of the search results. 
Hence, we propose a relevance feedback based re-
ranking model. We use these structured documents 
whose matching scores are greater than a threshold 
as feedback documents, to effectively re-rank other 
search results to bring more relevant and novel 
information to the user. 
Experiments on a large web search dataset from 
a major commercial search engine show that the F-
Measure of structured annotation generated by our 
approach is as high as 91%. On this dataset, our re-
ranking model using the structured annotations 
significantly outperforms two baselines. 
The main contributions of our work include: 
1. We propose a novel approach to generate 
structured annotation of queries based on top 
search results. 
2. Although structured annotation of queries has 
been studied previously, to the best of our 
knowledge this is the first paper that attempts 
to improve web search ranking by 
incorporating structured annotation of queries. 
The rest of this paper is organized as follows. 
We briefly introduce related work in Section 2. 
Section 3 presents our method for generating 
structured annotation of queries. We then propose 
two novel re-ranking models based on structured 
annotation in Section 4. Section 5 introduces the 
data used in this paper. We report experimental 
results in Section 6. Finally we conclude the work 
in Section 7. 
 
Figure 1. Search results of query ?taylor swift lyrics falling in love? and processing pipeline 
[Taylor Swift, #artist_name, 0.34]
...
[Mary?s Song (oh my my my), #song_name, 0.16]
[Crazier, #song_name, 0.1]
[Jump Then Fall, #song_name, 0.08]
...
[Growing up and falling in love?, #lyrics, 0.16]
[Feel like I?m falling and ?, #lyrics, 0.1]
[I realize your love is the best ?, #lyrics, 0.08]
d1 [Taylor Swift, #artist_name]
[Crazier, #song_name]
[Feel like I?m falling and ?, #lyrics]
d2 [Taylor Swift, #artist_name]
[Mary?s Song (oh my my my), #song_name]
[Growing up and falling in love?, #lyrics]
d3 [Taylor Swift, #artist_name]
[Jump Then Fall, #song_name]
[I realize your love is the best ?, #lyrics]
d4 [Taylor Swift, #artist_name]
[Mary?s Song (oh my my my), #song_name]
[Growing up and falling in love?, #lyrics]
Search Results (a)
Weighted Annotated Tokens (c)Query Structured Annotation Generation (d)Top Results Re-ranking (e)
Annotated Tokens (b)
1.
2.
3.
4.
Query: taylor swift lyrics falling in love
<[taylor swift, #artist_name] lyrics 
[falling in love, #lyrics]>
 
1. 
  
2. 
  
3. 
  
4. 
1.
2.
3.
4.
469
2 Related Work 
There is a great deal of prior research that 
identifies query structured information. We 
summarize this research according to their 
different approaches. 
2.1 Structured Annotation of Queries 
Recently, a lot of work has been done on 
understanding query structure (Sarkas et al, 2010; 
Li et al, 2009; Bendersky et al, 2010). One 
important method is structured annotation of 
queries which aims to detect the structure of the 
query and assign a specific label to it. Li et al, 
(2009) proposed web query tagging and its goal is 
to assign to each query term a specified category, 
roughly corresponding to a list of attributes. A 
semi-supervised Conditional Random Field (CRF) 
is used to capture dependencies between query 
words and to identify the most likely joint 
assignment of words to ?categories.? Comparing 
with previous work, the advantages of our 
approach are on the following aspects. First, we 
generate structured annotation of queries based on 
top search results, not some global knowledge base 
or query logs. Second, they mainly focus on the 
method of generating structured annotation of 
queries, rather than leverage the generated query 
structures to improve web search rankings. In this 
paper, we not only offer a novel solution for 
generating structured annotation of queries, but 
also propose a re-ranking approach to improve 
Web search based on structured annotation of 
queries. Bendersky et al, (2011) also used top 
search results to generate structured annotation of 
queries. However, the annotations in their 
definition are capitalization, POS tags, and 
segmentation indicators, which are different from 
ours. 
2.2 Query Template Generation 
The concept of query template has been discussed 
in a few recent papers (Agarwal et al, 2010; Pasca 
2011; Liu et al, 2011; Szpektor et al, 2011). A 
query template is a sequence of terms, where each 
term could be a word or an attribute. For example, 
<#artist_name lyrics #lyrics> is a query template, 
?#artist_name? and ?#lyrics? are attributes, and 
?lyrics? is a word. Structured annotation of queries 
is different from query template, as a query 
template can instantiate multiple queries while a 
structured annotation only serves for a specific 
query. Unlike query template, our work is ranking-
oriented. We aim to automatically annotate query 
structure based on top search results, and further 
use these structured annotations to re-rank top 
search results for improving search performance. 
2.3 Query Segmentation 
The task of query segmentation is to separate the 
query words into disjointed segments so that each 
segment maps to a semantic unit (Li et al, 2011). 
Query segmentation techniques have been well 
studied in recent literature (Tan and Peng, 2008; 
Yu and Shi, 2009). However, structured annotation 
of queries cannot only separate the query words 
into disjoint segments but can also assign each 
segment a semantic label which can help the search 
engine to judge whether each part of query 
segmentation matches the right field of the 
documents or not. 
2.4 Entity Search 
The problem of entity search has received a great 
deal of attention in recent years (Guo et al, 2009; 
Bron et al, 2010; Cheng et al, 2007). Its goal is to 
answer information needs that focus on entities. 
The problem of structured annotation of queries is 
related to entity search because for some queries, 
structured annotation items are entities or attributes. 
Some existing entity search approaches also 
exploit knowledge from the structure of webpages 
(Zhao et al, 2005). Annotating query structured 
information differs from entity search in the 
following aspects. First, structured annotation 
based ranking is applicable for all queries, rather 
than just entity related queries. Second, the result 
of an entity search is usually a list of entities, their 
attributes, and associated homepages, whereas our 
work uses the structured information from 
webpages to annotate query structured information 
and further leverage structured annotation of 
queries to re-rank top search results. 
Table 1. Example domain schemas 
Domain Schema Example structured annotations 
lyrics #artist_name 
#song_name 
#lyrics 
<lyrics of [hey jude, #song_name] [beatles, 
#artist_name]> 
job #category 
#location 
<[teacher, #category] job in [America, 
#location]> 
recipe  #directions 
#ingredients 
<[baking, # directions] [bread, # 
ingredients] recipe> 
 
470
3 Structured Annotation of Queries  
3.1 Problem Definition 
We start our discussion by defining some basic 
concepts. A token is defined as a sequence of 
words including space, i.e., one or more words. For 
example, the bigram ?taylor swift? can be a single 
token. As our objective is to find structured 
annotation of queries in a specific domain, we 
begin with a definition of domain schema. 
Definition 1 (Domain Schema): For a given 
domain of interest, the domain schema is the set of 
attributes. We denote the domain schema as ? =
{?1, ?2,? , ??}, where each ??  is the name of an 
attribute of the domain. Sample domain schemas 
are shown in Table 1. In contrast to previous 
methods (Agarwal et al, 2010), our definition of 
domain schema does not need attribute values. For 
the sake of simplicity, this paper assumes that 
attributes in domain schema are available. 
However, it is not difficult to pre-specify attributes 
in a specific domain. 
Definition 2 (Annotated Token): An annotated 
token in a specific domain is a pair [?, ?], where v 
is a token and a is a corresponding attribute for v 
in this domain. [hey jude, #song_name] is an 
example of an annotated token for the ?lyrics? 
domain shown in Table 1. The words ?hey jude? 
comprise a token, and its corresponding attribute 
name is #song_name. If a token does not have any 
corresponding attributes, we denote it as free token. 
Definition 3 (Structured Annotation): A 
structured annotation p is a sequence of terms <
?1,?2,?,?? >, where each ?? could be a free token or 
an annotated token, and at least one of the terms is 
an annotated token, i.e., ?? ? [1, ?] for which ?? is 
an annotated token. 
Given the schema for the domain ?lyrics?, 
<[taylor swift, #artist_name] lyrics [falling in love, 
#lyrics]> is a possible structured annotation for the 
query ?taylor swift lyrics falling in love?. In this 
annotation, [taylor swift, #artist_name] and 
[falling in love, #lyrics] are two annotated tokens. 
The word ?lyrics? is a free token. 
Intuitively, a structured annotation corresponds 
to an interpretation of the query as a request for 
some structured information from documents. The 
set of annotated tokens expresses the information 
need of the documents that have been requested. 
The free tokens may provide more diverse 
information. Annotated tokens and free tokens 
together cover all query terms, reflecting the 
complete user intent of the query. 
3.2 Generating Structured Annotation 
In this paper, given a domain schema A, we 
generate structured annotation for a query q based 
on the top search results of q. We propose using 
top search results, rather than some global 
knowledge base or query logs, because: 
(1) Top search results have been proven to be 
a successful technique for query explanation 
(Bendersky et al, 2010). 
(2) We have observed that in most cases, a 
reasonable percentage of the top search results are 
relevant to the query. By aggregating structured 
information from the top search results, we can get 
more query-dependent annotated tokens than using 
global data sources which may contain more noise 
and outdated. 
(3) Our goal for generating structured 
annotation is to improve the ranking quality of 
queries. Using top search results enables 
simultaneous and consistent detection of structured 
information from documents and queries. 
As mentioned in Section 3.1, we generate 
structured annotation of queries based on annotated 
tokens, which are actually structured data (shown 
in Figure 1(b)) embedded in web documents. In 
this paper, we assume that the annotated tokens are 
Algorithm 1: Query Structured Annotation Generation 
Input: a list of weighted annotated tokens T = {t1, ? , tm} ; 
          a query q = ?w1, ? , wn?  where wi ? W; 
a pre-defined threshold score ?. 
Output: a query structured annotation p = <s1, ? , sk>. 
  1: Set p = q = {s1, ?, sn}, where si = wi 
  2: for u = 1 to T.size do 
  3:       compute ?????(?, ??) 
            = ?????(?, ??. ?)  
            = ??. ? ? ???0??<??????(??? , ??. ?), 
            where pij = si,?,sj, s.t. sl ? W for l ? [i, j]. //pij is just 
in the remaining query words 
  4: end for 
  5: find the maximum matching tu with  
            ???? = ??????1?????????(?, ??) 
  6: if ?????(?, ????) > ? then 
  7:      replace si,?,sj in p with [si,?,sj, tmax.a ] 
  8:      remove tmax from T 
9:      n ? n ? (j - i) 
10:      go to step 2 
11: else  
12:      return p 
13: end if 
 
471
available and we mainly focus on how to use these 
annotated tokens from top search results to 
generate structured annotation of queries. The 
approach is comprised of two parts, one for 
weighting annotated tokens and the other for 
generating structured annotation of queries based 
on the weighted annotated tokens. 
Weighting: As shown in Figure 1, annotated 
tokens extracted from top results may be 
inconsistent, and hence some of the extracted 
annotated tokens are less useful or even useless for 
generating structured annotation. 
We assume that a better annotated token should 
be supported by more top results; while a worse 
annotated token may appear in fewer results. 
Hence we aggregate all the annotated tokens 
extracted from top search results, and evaluate the 
importance of each unique one by a ranking-aware 
voting model as follows. For an annotated token [v, 
a], its weight w is defined as: 
                      ? =
1
?
? ??1????                           (1) 
where wj is a voting from document dj, and 
?? = {
? ? ? + 1
?
,             if [?, ?] ? ??
0,                      else        
 
Here, N is the number of top search results and j 
is the ranking position of document dj. We then 
generate a weighted annotated token [v, a, w] for 
each original unique token [v, a]. 
Generating: The process by which we map a 
query q to Structured Annotation is shown in 
Algorithm 1. The algorithm takes as input a list of 
weighted annotated tokens and the query q, and 
outputs the structured annotation of the query q. 
The algorithm first partitions the query q by 
comparing each sub-sequence of the query with all 
the weighted annotated tokens, and find the 
maximum matching annotated token (line 1 to line 
5). Then, if the degree of match is greater than the 
threshold ? which is a pre-defined threshold score 
for fuzzy string matching, the query substring will 
be assigned the attribute label of the maximum 
matching annotated token (line 6 to line 8). The 
algorithm stops when all the weighted annotated 
tokens have been scanned, and outputs the 
structured annotation of the query.  
Note that in some cases, the query may fail to 
exactly match with the annotated tokens, due to 
spelling errors, acronyms or abbreviations in users? 
queries. For example, in the query ?broken and 
beatuful lyrics?, ?broken and beatuful? is a 
misspelling of ?broken and beautiful.? We adopt a 
fuzzy string matching function for comparing a 
sub-sequence string s with a token v: 
          ???(?, ?) = 1 ?
????????????(?,?)
max (|?|,|?|)
                (2) 
where EditDistance(s, v) measures the edit 
distances of two strings, |s| is the length of string s 
and |v| is the length of string v. 
4 Ranking with Structured Annotation 
Given a domain schema ? = {?1, ?2, ? , ??}, and a 
query q, suppose that ? = < ?1,?2,?,?? >  is the 
structured annotation for query q obtained using 
the method introduced in the above sections. p can 
better reflects the user?s real search intent than the 
original q, as it presents the structured semantic 
information needed instead of a simple word string. 
Therefore, a document di can better satisfy a user?s 
information need if it contains corresponding 
structured semantic information in p. Suppose that 
Ti is the set of annotated tokens extracted from 
document di, we compute a re-ranking score, 
denoted by RScore, for document di as follows: 
RScore(q, di) = ?????(?, ??) 
                      = ?????(?, ??) 
                      = ? ? ?????(?? , ?)????1????  
where 
  ?????(?? , ?)= {
???(?? . ?? , ?. ?),        if ?? . ?? = ?. ?
0,                                else
      (3) 
where ??  is an annotated token in p and t is an 
annotated token in di. We use Equation (2) to 
compute the similarity between values in query 
annotated tokens and values in document annotated 
tokens. We propose two re-ranking models, 
namely the conservative re-ranking model, to re-
rank top results based on RScore and relevance 
feedback based re-ranking model. 
4.1 Conservative Re-ranking Model 
A nature way to re-rank top search results is 
according to their RScore. However, we fail to 
obtain annotated tokens from some retrieved 
documents, and hence the RScore of these 
documents are not available. In the conservative 
re-ranking model, we only re-rank search results 
that have an RScore. For example, suppose there 
are five retrieved documents {d1, d2, d3, d4, d5} for 
query q, we can extract structured information 
from document d3 and d4 and RScore(q, d4) > 
RScore(q, d3). Note that we cannot obtain 
472
structured information from d1, d2, and d5.  In the 
conservative re-ranking method, d1, d2, and d5 
retain their original positions; while d3 and d4 will 
be re-ranked according to their RScore. Therefore, 
the final ranking generated by our conservative re-
ranking model should be {d1, d2, d4, d3, d5}, in 
which the documents are re-ranked among the 
affected positions. 
There is also useful information in the 
documents without structured data, such as 
community question answering websites. However, 
in the conservative re-ranking model they will not 
be re-ranked. This may hurt the performance of our 
re-ranking model. One reasonable solution is 
relevance feedback model. 
4.2 Relevance Feedback based Re-ranking 
Model 
The disadvantage of the conservative re-ranking 
model is that it only can re-rank those top search 
results with structured data. To make up its 
limitation, we propose a relevance feedback based 
re-ranking model. The key idea of this model is 
based on the observation that the search results 
with the corrected annotated tokens could give 
implicit feedback information. Hence, we use these 
structured documents whose RScore are greater 
than a threshold ? (empirically set it as 0.6) as 
feedback documents, to effectively re-rank other 
search results to bring more relevant and novel 
information to the user. 
Formally, given a query Q and a document 
collection C, a retrieval system returns a ranked list 
of documents D. Let di denote the i-th ranked 
document in the ranked list. Our goal is to study 
how to use these feedback documents, J ? {d1,?, 
dk}, to effectively re-rank the other r search results: 
U ? {dk+1,?, dk+r}. A general formula of relevance 
feedback model (Salton et al 1990) R is as follows: 
?(??) = (1 ? ?)??(Q) + ???(J)             (4) 
where ? ? [0, 1] is the feedback coefficient, and ?? 
and ?? are two models that map a query and a set 
of relevant documents, respectively, into some 
comparable representations. For example, they can 
be represented as vectors of weighted terms or 
language models. 
In this paper, we explore the problem in the 
language model framework, particularly the KL-
divergence retrieval model and mixture-model 
feedback method (Zhai and Lafferty, 2001), mainly 
because language models deliver state-of-the-art 
retrieval performance and the mixture-model based 
feedback is one of the most effective feedback 
techniques which outperforms Rocchio feedback. 
4.2.1 The KL-Divergence Retrieval Model 
The KL-divergence retrieval model was introduced 
in Lafferty and Zhai, (2001) as a special case of the 
risk minimization retrieval framework and can 
support feedback more naturally. In this model, 
queries and documents are represented by unigram 
language models. Assuming that these language 
models can be appropriately estimated, KL-  
divergence retrieval model measures the relevance 
value of a document D with respect to a query Q 
by computing the negative Kullback-Leibler 
divergence between the query language model ?? 
and the document language model ?? as follows: 
?(?, ?) = ??(??||??) = ?? ?(?|??)???
?(?|??)
?(?|??)
???       (5) 
where V is the set of words in our vocabulary. 
Intuitively, the retrieval performance of the KL-
divergence relies on the estimation of the 
document model ?? and the query model ??.  
For the set of k relevant documents, the 
document model ??  is estimated as ?(w|??) =
1
?
?
?(?,??)
|??|
?
?=1 , where ?(?, ??) is the count of word 
w in the i-th relevant document, and |??| is the total 
number of words in that document. The document 
model ??  needs to be smoothed and an effective 
method is Dirichlet smoothing (Zhai et al, 2001). 
The query model intuitively captures what the 
user is interested in, and thus would affect retrieval 
performance. With feedback documents, ??  is 
estimated by the mixture-model feedback method. 
4.2.2 The Mixture Model Feedback Method 
As the problem definition in Equation (4), the 
query model can be estimated by the original query 
model ?(?|??) =
?(?,?)
|?|
 (where c(w,Q) is the count 
of word w in the query Q, and |Q| is the total 
number of words in the query) and the feedback 
document model. Zhai and Lafferty, (2001) 
proposed a mixture model feedback method to 
estimate the feedback document model. More 
specifically, the model assumes that the feedback 
documents can be generated by a background 
language model ?(?|?) estimated using the whole 
collection and an unknown topic language model 
473
?? to be estimated. Formally, let F ? C be a set of 
feedback documents. In this paper, F is comprised 
of documents that RScore are greater than?. The 
log-likelihood function of the mixture model is: 
???(?|??) = 
      ? ? ?(?, ?)??? log [(1 ? ?)?(?|??) + ??(?|?)]???     (6) 
where ? ? [0,1)  is a mixture noise parameter 
which controls the weight of the background 
model. Given a fixed ?, a standard EM algorithm 
can then be used to estimate ?(?|??), which is 
then interpolated with the original query model 
?(?|Q) to obtain an improved estimation of the  
query model: 
?(?|??) = (1 ? ?)?(?|?) + ??(?|??)         (7) 
 where ? is the feedback coefficient. 
5 Data 
We used a dataset composed of 12,396 queries 
randomly sampled from query logs of a search 
engine. For each query, we retrieved its top 100 
results from a commercial search engine. The 
documents were judged by human editors. A five-
grade (from 0 to 4 meaning from bad to perfect) 
relevance rating was assigned for each document. 
We used a proprietary query domain classifier to 
identify queries in three domains, namely ?lyrics,?  
?recipe,? and ?job,? from the dataset. The statistics 
about these domains are shown in Table 2. To 
investigate how many queries may potentially have 
structured annotations, we manually created 
structured annotations for these queries. The last 
column of Table 2 shows the percentage of queries 
that have structured annotations created by 
annotators. We found that for each domain, there 
was on average more than 90% of queries 
identified by us that had a certain structured 
annotation. This indicates that a large percentage 
of these queries contain structured information, as 
we expected. 
6 Experimental Results 
In this section, we present the structured annotation 
of queries and further re-rank the top search results 
for the three domains introduced in Section 5. We 
used the ranking returned by a commercial search 
engine as our one of the Baselines. Note that as the 
baseline already uses a large number of ranking 
signals, it is very difficult to improve it any further. 
We evaluate the ranking quality using the widely 
used Normalized Discounted Cumulative Gain 
measure (NDCG) (Javelin and Kekalainen., 2000). 
We use the same configuration for NDCG as 
(Burges et al 2005). More specifically, for a given 
query q, the NDCG@K is computed as: 
                        ?? = 
1
??
? (2?(?)?1)??=1
log (1 + ?)
                            (4) 
Mq is a normalization constant (the ideal NDCG) 
so that a perfect ordering would obtain an NDCG 
of 1; and r(j) is the rating score of the j-th  
document in the ranking list.  
6.1 Overall Results 
6.1.1 Quality of Structured Annotation of 
Queries 
We generated the structured annotation of queries 
based on the top 10 search results and used ? =
0.04  for Algorithm 1. We used several existing 
metrics, P (Precision), R (Recall), and F-Measure 
to evaluate the quality of the structured annotation. 
As a query structured annotation may contain more 
than one annotated token, we concluded that the 
 
Figure 2. Ranking Quality (* indicates significant 
improvement) 
0.54
0.55
0.56
0.57
0.58
0.59
0.6
0.61
0.62
NDCG@1 NDCG@3 NDCG@5
V
a
lu
e
 o
f 
m
e
a
s
u
r
e
m
e
n
t
Measurement
Seg-Ranker Ori-Ranker Con-Ranker FB-Ranker
*
*
*
*
*
*
Table 3. Quality of Structured Annotation. All the 
improvements are significant (p < 0.05) 
Domain Method Precision Recall F-Measure 
lyrics Baseline 
Our 
90.06% 
95.45% 
84.92% 
89.83% 
87.41% 
92.55% 
job Baseline 
Our 
89.62% 
95.31% 
80.14% 
84.93% 
84.62% 
89.82% 
recipe Baseline 
Our 
83.96% 
89.68% 
84.23% 
88.44% 
84.09% 
89.06% 
All Baseline 
Our 
87.88% 
93.61% 
83.10% 
88.45% 
85.42% 
90.96% 
 
Table 2. Domain queries used in our experiment 
Domain Containing 
Keyword 
Queries 
 
Structured  
Annotation% 
lyrics ?lyrics? 196 95% 
job ?job? 124 92% 
recipe ?recipe? 76   93% 
 
474
annotation was correct only if the entire annotation 
was completely the same as the annotation labeled 
by annotators. Otherwise we treated the structured 
annotation as incorrect. Experimental results for 
the three domains are shown in Table 3. We 
compare our approach with Xiao Li, (2010) 
(denoted as baseline), on the dataset described in 
Section 5. They labeled the semantic structure of 
noun phrase queries based on semi-Markov CRFs. 
Our approach achieves better performance than the 
baseline (about 5.5% significant improvement on 
F-Measure). This indicates that the approach of 
generating structured annotation based on the top 
search results is more effective. With the high-
quality structured annotation of queries in hand, it 
may be possible to obtain better ranking results 
using our proposed re-ranking models. 
6.1.2 Re-ranking Result 
We used the models introduced in Section 4 to re-
rank the top 10 search results, based on structured 
annotation of queries and annotated tokens.  
Recall that our goal is to quantify the 
effectiveness of structured annotation of queries 
for real web search. One dimension is to compare 
with the original search results of a commercial 
search engine (denoted as Ori-Ranker). The other 
is to compare with the query segmentation based 
re-ranking model (denoted as Seg-Ranker; Li et 
al., 2011) which tries to improve web search 
ranking by incorporating query segmentation. Li et 
al., (2011) incorporated query segmentation in the 
BM25, unigram language model and bigram 
language model retrieval framework, and bigram 
language model achieved the best performance. In 
this paper, Seg-Ranker integrates bigram language 
model with query segmentation. 
The ranking results of these models are shown 
in Figure 2. This figure shows that all our two 
rankers significantly outperform the Ori-Ranker? 
the original search results of a commercial search 
engine. This means that using high-quality 
structured annotation does help better 
understanding of user intent. By comparing these 
structured annotations and the annotated tokens in 
documents, we can re-rank the more relevant 
results higher and yield better ranking quality. 
Figure 2 also suggests that structured annotation 
based re-ranking models outperform query 
segmentation based re-ranking model. This is 
mainly because structured annotation can not only 
separate the query words into disjoint segments but 
can also assign each segment a semantic label. 
Taking full advantage of the semantic label can 
lead to better ranking performance. 
Furthermore, Figure 2 shows that FB-Ranker 
outperforms Con-Ranker. The main reason is that 
in Con-Ranker, we can only reasonably re-rank the 
search results with structured data. However, in 
FB-Ranker we can not only re-rank the structured 
search results but also can re-rank other documents 
by incorporating implicit information from those 
structured documents.  
On average, FB-Ranker achieves the best 
ranking performance. Table 4 shows more detailed 
Table 4. Detailed ranking results on three domains. 
All the improvements are significant (p < 0.05) 
Domain Ranking Method NDCG@1 NDCG@3 NDCG@5 
lyrics Seg-Ranker 0.572 0.574 0.575 
Ori-Ranker 
FB-Ranker 
0.621 
0.637 
0.628 
0.639 
0.636 
0.647 
recipe Seg-Ranker 0.629 0.631 0.634 
Ori-Ranker 
FB-Ranker 
0.678 
0.707 
0.687 
0.704 
0.696 
0.709 
job Seg-Ranker 0.438 0.413 0.408 
Ori-Ranker 
FB-Ranker 
0.470 
0.504 
0.453 
0.474 
0.442 
0.459 
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.02 0.04 0.06 0.08 0.1 0.3 0.5 0.7 0.9
V
a
l
u
e
 
o
f
 
m
e
a
s
u
r
e
m
e
n
t
Query structured annotation generation threshold ?
Precision
Recall
F-Measure
0.54
0.55
0.56
0.57
0.58
0.59
0.6
0.61
0 0.02 0. 4 .06 0.08 .1 0.3 0.5 0.7 0.9 perfect
N
D
C
G
@
3
Que tr ctured annotation generation threshold ?
Seg-Ranker Ori-Ranker FB-Ranker
 
                          (a) Quality of re-ranking                                    (b) Quality of query structured annotation 
Figure 3. Quality of re-ranking and quality of query structured annotation with different number of search results 
475
results for the three selected domains. This table 
shows that FB-Ranker consistently outperforms the 
two baseline rankers on these domains. In the 
remaining part of this paper, we will only report 
the results for this ranker, due to space limitations. 
Table 4 also indicates that we can get robust 
ranking improvement in different domains, and we 
will consider applying it to more domains. 
6.2 Experiment with Different Thresholds of 
Query Structured Annotation Algorithm 
As introduced in Algorithm 1, we pre-defined a 
threshold ? for fuzzy string matching. We 
evaluated the quality of re-ranking and query 
structured annotation with different settings for ?. 
The results are shown in Figure 3. We found that: 
(1) When we use ? = 0, which means that the 
structured annotations can be generated no matter 
how small the similarity between the query string 
and a weighted annotated token is, we can get a 
significant NDCG@3 gain of 2.15%. Figure 3(b) 
shows that the precision of the structured 
annotation is lowest when ? = 0 . However, the 
precision is still as high as 0.7375, and the highest 
recall is obtained in this case. This means that the 
quality of the generated structured annotations is 
still reasonable, and hence we can get a ranking 
improvement when ? = 0, as shown in Figure 3(a). 
(2) Figure 3(a) suggests that the quality of re-
ranking increases when the threshold ? increases 
from 0 to 0.05. It then decreases when ? increases 
from 0.06 to 0.5. Comparing these two figures 
shows that the trend of re-ranking performance 
adheres to the quality of the structured annotation. 
The settings for ? dramatically affect the recall and 
precision of the structured annotation; and hence 
the ranking quality is impacted. The larger ? is, the 
lower the recall of the structured annotation is. 
(3) Since the re-ranking performance 
dramatically changes along with the quality of the 
structured annotation, we conducted a re-ranking 
experiment with perfect structured annotations (F-
Measure equal to 1.0). Perfect structured 
annotations mean the annotations created by 
annotators as introduced in Section 5. The results 
are shown in the last bar of Figure 3(a). We did not 
find a large space for ranking improvement. The 
NDCG@3 when using perfect structured 
annotations was 0.606, which is just slightly better 
than our best result (yield when ?=0.05). It 
indicates that our structured annotation generation 
algorithm is already quite effective. 
(4) Figure 3(a) shows that our approach 
outperforms the two baseline approaches with most 
settings for ?. This indicates that our approach is 
relatively stable with different settings for ?. 
6.3 Experiment with Number of Top Search 
Results 
The above experiments are conducted based on the 
top 10 search results. In this section, by adjusting 
the number of top search results, ranging from 2 to 
100, we investigate whether the quality of 
structured annotation of queries and the 
performance of re-ranking are affected by the 
quantity of search results. The results shown in 
Figure 4 indicate that the number of search results 
does affect the quality of structured annotation of 
queries and the performance of re-ranking. 
Structured annotations of queries become better 
when more search results are used from 2 to 20. 
This is because more search results cover more 
websites in our domain list, and hence can generate 
more annotated tokens. More results also provide 
more evidence for voting the importance of 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
2 3 4 5 6 7 8 9 10 20 30 40 50 60 70 80 90 100
V
a
u
l
u
e
 
o
f
 
m
e
a
s
u
r
e
m
e
n
t
Number of search results
Precision
Recall
F-Measure
0.54
0.55
0.56
0.57
0.58
0.59
0.6
0.61
2 3 4 5 6 7 8 9 10 0 30 4 50 60 70 80 90 100
N
D
C
G
@
3
Number of search result
Seg-Ranker Ori-Ranker FB-Ranker
 
                              (a) Quality of re-ranking                                   (b) Quality of query structured annotation 
Figure 4. Quality of re-ranking and quality of query structured annotation with different number of search results 
476
annotated tokens, and hence can improve the 
quality of structured annotation of queries. 
In addition, we also found that structured 
annotation of queries become worse when too 
many lower ranked results are used (e.g, using 
results ranked lower than 20). This is because the 
lower ranked results are less relevant than the 
higher ranked results. They may contain more 
irrelevant or noisy annotated tokens than higher 
ranked documents; and hence using them may 
harm the precision of the structured annotations. 
Figure 4 also indicates that the quality of ranking 
and the accuracy of structured annotations are 
correlated. 
7 Conclusions 
In this paper, we studied the problem of improving 
web search ranking by incorporating structured 
annotation of queries. We proposed a systematic 
solution, first to generate structured annotation of 
queries based on top search results, and then 
launching two structured annotation based re-
ranking models. We performed a large-scale 
evaluation over 12,396 queries from a major search 
engine. The experiment results show that the F-
Measure of query structured annotation generated 
by our approach is as high as 91%. In the same 
dataset, our structured annotation based re-ranking 
model significantly outperforms the original ranker 
? the ranking of a major search engine, with 
improvements 5.2%. 
 
Acknowledgments 
This work was supported by National Natural Science 
Foundation of China (NSFC) via grant 61273321, 
61133012 and the Nation-al 863 Leading Technology 
Research Project via grant 2012AA011102. 
References  
G. Agarwal, G. Kabra, and K. C.-C. Chang. Towards 
rich query interpretation: walking back and forth for 
mining query templates. In Proc. of WWW '10. 
M. Bendersky, W. Bruce Croft and D. A. Smith. Joint 
Annotation of Search Queries, In Proc. of ACL-HLT 
2011. 
M. Bendersky, W. Bruce Croft and D. A. Smith. 
Structural Annotation of Search Queries Using 
Pseudo-Relevance Feedback, In Proc. Of CIKM 2010. 
S. Bergsma and Q. I. Wang. Learning noun phrase 
query segmentation. In Proceedings of EMNLP-
CoNLL'07. 
M. Bron, K. Balog, and M. de Rijke. Ranking related 
entities: components and analyses. In Proc. of 
CIKM ?10. 
C. Buckley. Automatic query expansion using SMART. 
InProc. of TREC-3, pages 69?80, 1995. 
C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, 
N. Hamilton, and G. Hullender. Learning to rank 
usinggradient descent. In Proceedings of ICML '05. 
T. Cheng, X. Yan, and K. C.-C. Chang. Supporting 
entity search: a large-scale prototype search engine. 
In Proc. of SIGMOD ?07. 
O. Etzioni, M. Banko, S. Soderland, and D.S. Weld, 
(2008). Open Information Extraction from the Web, 
Communications of the ACM, 51(12): 68-74. 
J. Guo, G. Xu, X. Cheng, and H. Li. Named entity 
recognition in query. In Proc. Of SIGIR? 2009. 
K. Jarvelin and J. Kekalainen. Ir evaluation methods for 
retrieving highly relevant documents. In SIGIR '00. 
J. Lafferty and C. Zhai, Document language models, 
query models, and risk minimization for information 
retrieval, In Proceedings of SIGIR'01, pages 111-119, 
2001. 
V. Lavrenko and W. B. Croft. Relevance based 
language models. In Proc. of SIGIR, pages 120?127, 
2001. 
Y. Li, BJP. Hsu, CX. Zhai and K. Wang. Unsupervised 
Query Segmentation Using Clickthrough for 
Information Retrieval. In Proc. of SIGIR'11. 
X. Li, Y.-Y. Wang, and A. Acero. Extracting structured 
information from user queries with semi-supervised 
conditional random fields. In Proc. of SIGIR'09. 
Y. Liu, X. Ni, J-T. Sun, Z. Chen. Unsupervised 
Transactional Query Classification Based on 
Webpage Form Understanding. In Proc. of CIKM '11. 
Y. Liu, M. Zhang, L. Ru, and S. Ma. Automatic query 
type identification based on click-through 
information. In LNCS, 2006. 
M. Pasca. Asking What No One Has Asked Before: 
Using Phrase Similarities To Generate Synthetic 
Web Search Queries. In Proc. of CIKM '11. 
G. Salton and C. Buckley. Improving retrieval 
performance by relevance feedback. Journal of the 
American Society for Information Science, 
41(4):288-297, 1990. 
477
N. Sarkas, S. Paparizos, and P. Tsaparas. Structured 
annotations of web queries. In Proc. of SIGMOD'10. 
I. Szpektor, A. Gionis, and Y. Maarek. Improving 
recommendation for long-tail queries via templates. 
In Proc. of WWW '11 
B. Tan and F. Peng. Unsupervised query segmentation 
using generative language models and wikipedia. In 
WWW?08. 
T.-L. Wong, W. Lam, and B. Chen. Mining 
employment market via text block detection and 
adaptive cross-domain information extraction. In 
Proc. SIGIR, pages 283?290, 2009. 
X. Yu and H. Shi. Query segmentation using 
conditional random fields. In Proceedings of KEYS 
'09. 
C. Zhai and J. Lafferty, Model-based feedback in the 
language modeling approach to information 
retrieval , In Proceedings of CIKM'01, pages 403-410, 
2001. 
C. Zhai and J. Lafferty, A study of smoothing methods 
for language models applied to ad hoc information 
retrieval, In Proceedings of SIGIR'01, pages 334-342, 
2001. 
Y. Zhai and B. Liu. Structured data extraction from the 
Web based on partial tree alignment. IEEE Trans. 
Knowl. Data Eng., 18(12):1614?1628, Dec. 2006. 
H. Zhao, W. Meng, Z. Wu, V. Raghavan, and C. Yu. 
Fully automatic wrapper generation for search 
engines. In Proceedings of WWW ?05. 
S. Zheng, R. Song, J.-R. Wen, and D. Wu. Joint 
optimization of wrapper generation and template 
detection. In Proc. of SIGKDD'07. 
478
