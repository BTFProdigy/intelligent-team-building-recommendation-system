Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 405?413,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Jointly Identifying Temporal Relations with Markov Logic
Katsumasa Yoshikawa
NAIST, Japan
katsumasa-y@is.naist.jp
Sebastian Riedel
University of Tokyo, Japan
sebastian.riedel@gmail.com
Masayuki Asahara
NAIST, Japan
masayu-a@is.naist.jp
Yuji Matsumoto
NAIST, Japan
matsu@is.naist.jp
Abstract
Recent work on temporal relation iden-
tification has focused on three types of
relations between events: temporal rela-
tions between an event and a time expres-
sion, between a pair of events and between
an event and the document creation time.
These types of relations have mostly been
identified in isolation by event pairwise
comparison. However, this approach ne-
glects logical constraints between tempo-
ral relations of different types that we be-
lieve to be helpful. We therefore propose a
Markov Logic model that jointly identifies
relations of all three relation types simul-
taneously. By evaluating our model on the
TempEval data we show that this approach
leads to about 2% higher accuracy for all
three types of relations ?and to the best
results for the task when compared to those
of other machine learning based systems.
1 Introduction
Temporal relation identification (or temporal or-
dering) involves the prediction of temporal order
between events and/or time expressions mentioned
in text, as well as the relation between events in a
document and the time at which the document was
created.
With the introduction of the TimeBank corpus
(Pustejovsky et al, 2003), a set of documents an-
notated with temporal information, it became pos-
sible to apply machine learning to temporal order-
ing (Boguraev and Ando, 2005; Mani et al, 2006).
These tasks have been regarded as essential for
complete document understanding and are useful
for a wide range of NLP applications such as ques-
tion answering and machine translation.
Most of these approaches follow a simple
schema: they learn classifiers that predict the tem-
poral order of a given event pair based on a set of
the pair?s of features. This approach is local in the
sense that only a single temporal relation is consid-
ered at a time.
Learning to predict temporal relations in this iso-
lated manner has at least two advantages over any
approach that considers several temporal relations
jointly. First, it allows us to use off-the-shelf ma-
chine learning software that, up until now, has been
mostly focused on the case of local classifiers. Sec-
ond, it is computationally very efficient both in
terms of training and testing.
However, the local approach has a inherent
drawback: it can lead to solutions that violate logi-
cal constraints we know to hold for any sets of tem-
poral relations. For example, by classifying tempo-
ral relations in isolation we may predict that event
A happened before, and event B after, the time
of document creation, but also that event A hap-
pened after event B?a clear contradiction in terms
of temporal logic.
In order to repair the contradictions that the local
classifier predicts, Chambers and Jurafsky (2008)
proposed a global framework based on Integer Lin-
ear Programming (ILP). They showed that large
improvements can be achieved by explicitly incor-
porating temporal constraints.
The approach we propose in this paper is similar
in spirit to that of Chambers and Jurafsky: we seek
to improve the accuracy of temporal relation iden-
tification by predicting relations in a more global
manner. However, while they focused only on the
temporal relations between events mentioned in a
document, we also jointly predict the temporal or-
der between events and time expressions, and be-
tween events and the document creation time.
Our work also differs in another important as-
pect from the approach of Chambers and Jurafsky.
Instead of combining the output of a set of local
classifiers using ILP, we approach the problem of
joint temporal relation identification using Markov
Logic (Richardson and Domingos, 2006). In this
405
framework global correlations can be readily cap-
tured through the addition of weighted first order
logic formulae.
Using Markov Logic instead of an ILP-based ap-
proach has at least two advantages. First, it allows
us to easily capture non-deterministic (soft) rules
that tend to hold between temporal relations but do
not have to. 1 For example, if event A happens be-
fore B, and B overlaps with C, then there is a good
chance that A also happens before C, but this is not
guaranteed.
Second, the amount of engineering required to
build our system is similar to the efforts required
for using an off-the-shelf classifier: we only need
to define features (in terms of formulae) and pro-
vide input data in the correct format. 2 In particu-
lar, we do not need to manually construct ILPs for
each document we encounter. Moreover, we can
exploit and compare advanced methods of global
inference and learning, as long as they are imple-
mented in our Markov Logic interpreter of choice.
Hence, in our future work we can focus entirely
on temporal relations, as opposed to inference or
learning techniques for machine learning.
We evaluate our approach using the data of the
?TempEval? challenge held at the SemEval 2007
Workshop (Verhagen et al, 2007). This challenge
involved three tasks corresponding to three types
of temporal relations: between events and time ex-
pressions in a sentence (Task A), between events of
a document and the document creation time (Task
B), and between events in two consecutive sen-
tences (Task C).
Our findings show that by incorporating global
constraints that hold between temporal relations
predicted in Tasks A, B and C, the accuracy for
all three tasks can be improved significantly. In
comparison to other participants of the ?TempE-
val? challenge our approach is very competitive:
for two out of the three tasks we achieve the best
results reported so far, by a margin of at least 2%. 3
Only for Task Bwe were unable to reach the perfor-
mance of a rule-based entry to the challenge. How-
ever, we do perform better than all pure machine
1It is clearly possible to incorporate weighted constraints
into ILPs, but how to learn the corresponding weights is not
obvious.
2This is not to say that picking the right formulae in
Markov Logic, or features for local classification, is always
easy.
3To be slightly more precise: for Task C we achieve this
margin only for ?strict? scoring?see sections 5 and 6 for more
details.
learning-based entries.
The remainder of this paper is organized as fol-
lows: Section 2 describes temporal relation identi-
fication including TempEval; Section 3 introduces
Markov Logic; Section 4 explains our proposed
Markov Logic Network; Section 5 presents the set-
up of our experiments; Section 6 shows and dis-
cusses the results of our experiments; and in Sec-
tion 7 we conclude and present ideas for future re-
search.
2 Temporal Relation Identification
Temporal relation identification aims to predict
the temporal order of events and/or time expres-
sions in documents, as well as their relations to the
document creation time (DCT). For example, con-
sider the following (slightly simplified) sentence of
Section 1 in this paper.
With the introduction of the TimeBank cor-
pus (Pustejovsky et al, 2003), machine
learning approaches to temporal ordering
became possible.
Here we have to predict that the ?Machine learn-
ing becoming possible? event happened AFTER
the ?introduction of the TimeBank corpus? event,
and that it has a temporal OVERLAP with the year
2003. Moreover, we need to determine that both
events happened BEFORE the time this paper was
created.
Most previous work on temporal relation iden-
tification (Boguraev and Ando, 2005; Mani et al,
2006; Chambers and Jurafsky, 2008) is based on
the TimeBank corpus. The temporal relations in
the Timebank corpus are divided into 11 classes;
10 of them are defined by the following 5 relations
and their inverse: BEFORE, IBEFORE (immedi-
ately before), BEGINS, ENDS, INCLUDES; the re-
maining one is SIMULTANEOUS.
In order to drive forward research on temporal
relation identification, the SemEval 2007 shared
task (Verhagen et al, 2007) (TempEval) included
the following three tasks.
TASK A Temporal relations between events and
time expressions that occur within the same
sentence.
TASK B Temporal relations between the Docu-
ment Creation Time (DCT) and events.
TASK C Temporal relations between the main
events of adjacent sentences.4
4The main event of a sentence is expressed by its syntacti-
cally dominant verb.
406
To simplify matters, in the TempEval data, the
classes of temporal relations were reduced from
the original 11 to 6: BEFORE, OVERLAP, AFTER,
BEFORE-OR-OVERLAP, OVERLAP-OR-AFTER,
and VAGUE.
In this work we are focusing on the three tasks of
TempEval, and our running hypothesis is that they
should be tackled jointly. That is, instead of learn-
ing separate probabilistic models for each task, we
want to learn a single one for all three tasks. This
allows us to incorporate rules of temporal consis-
tency that should hold across tasks. For example, if
an event X happens before DCT, and another event
Y after DCT, then surely X should have happened
before Y. We illustrate this type of transition rule in
Figure 1.
Note that the correct temporal ordering of events
and time expressions can be controversial. For in-
stance, consider the example sentence again. Here
one could argue that ?the introduction of the Time-
Bank? may OVERLAP with ?Machine learning be-
coming possible? because ?introduction? can be
understood as a process that is not finished with
the release of the data but also includes later adver-
tisements and announcements. This is reflected by
the low inter-annotator agreement score of 72% on
Tasks A and B, and 68% on Task C.
3 Markov Logic
It has long been clear that local classification
alone cannot adequately solve all prediction prob-
lems we encounter in practice.5 This observa-
tion motivated a field within machine learning,
often referred to as Statistical Relational Learn-
ing (SRL), which focuses on the incorporation
of global correlations that hold between statistical
variables (Getoor and Taskar, 2007).
One particular SRL framework that has recently
gained momentum as a platform for global learn-
ing and inference in AI is Markov Logic (Richard-
son and Domingos, 2006), a combination of first-
order logic and Markov Networks. It can be under-
stood as a formalism that extends first-order logic
to allow formulae that can be violated with some
penalty. From an alternative point of view, it is an
expressive template language that uses first order
logic formulae to instantiate Markov Networks of
repetitive structure.
From a wide range of SRL languages we chose
Markov Logic because it supports discriminative
5It can, however, solve a large number of problems surpris-
ingly well.
Figure 1: Example of Transition Rule 1
training (as opposed to generative SRL languages
such as PRM (Koller, 1999)). Moreover, sev-
eral Markov Logic software libraries exist and are
freely available (as opposed to other discrimina-
tive frameworks such as Relational Markov Net-
works (Taskar et al, 2002)).
In the following we will explain Markov Logic
by example. One usually starts out with a set
of predicates that model the decisions we need to
make. For simplicity, let us assume that we only
predict two types of decisions: whether an event e
happens before the document creation time (DCT),
and whether, for a pair of events e1 and e2, e1
happens before e2. Here the first type of deci-
sion can be modeled through a unary predicate
beforeDCT(e), while the latter type can be repre-
sented by a binary predicate before(e1, e2). Both
predicates will be referred to as hidden because we
do not know their extensions at test time. We also
introduce a set of observed predicates, representing
information that is available at test time. For ex-
ample, in our case we could introduce a predicate
futureTense(e) which indicates that e is an event
described in the future tense.
With our predicates defined, we can now go on
to incorporate our intuition about the task using
weighted first-order logic formulae. For example,
it seems reasonable to assume that
futureTense (e) ? ?beforeDCT (e) (1)
often, but not always, holds. Our remaining un-
certainty with regard to this formula is captured
by a weight w we associate with it. Generally
we can say that the larger this weight is, the more
likely/often the formula holds in the solutions de-
scribed by our model. Note, however, that we do
not need to manually pick these weights; instead
they are learned from the given training corpus.
The intuition behind the previous formula can
also be captured using a local classifier.6 However,
6Consider a log-linear binary classifier with a ?past-tense?
407
Markov Logic also allows us to say more:
beforeDCT (e1) ? ?beforeDCT (e2)
? before (e1, e2) (2)
In this case, we made a statement about more
global properties of a temporal ordering that can-
not be captured with local classifiers. This formula
is also an example of the transition rules as seen in
Figure 2. This type of rule forms the core idea of
our joint approach.
A Markov Logic Network (MLN) M is a set of
pairs (?,w) where ? is a first order formula and w
is a real number (the formula?s weight). It defines a
probability distribution over sets of ground atoms,
or so-called possible worlds, as follows:
p (y) = 1
Z
exp
?
?
?
(?,w)?M
w
?
c?C?
f?c (y)
?
? (3)
Here each c is a binding of free variables in ? to
constants in our domain. Each f?c is a binary fea-
ture function that returns 1 if in the possible world
y the ground formula we get by replacing the free
variables in ? with the constants in c is true, and
0 otherwise. C? is the set of all bindings for the
free variables in ?. Z is a normalisation constant.
Note that this distribution corresponds to a Markov
Network (the so-called Ground Markov Network)
where nodes represent ground atoms and factors
represent ground formulae.
Designing formulae is only one part of the game.
In practice, we also need to choose a training
regime (in order to learn the weights of the formu-
lae we added to the MLN) and a search/inference
method that picks the most likely set of ground
atoms (temporal relations in our case) given our
trained MLN and a set of observations. How-
ever, implementations of these methods are often
already provided in existing Markov Logic inter-
preters such as Alchemy 7 and Markov thebeast. 8
4 Proposed Markov Logic Network
As stated before, our aim is to jointly tackle
Tasks A, B and C of the TempEval challenge. In
this section we introduce the Markov Logic Net-
work we designed for this goal.
We have three hidden predicates, corresponding
to Tasks A, B, and C: relE2T(e, t, r) represents the
temporal relation of class r between an event e
feature: here for every event e the decision ?e happens be-
fore DCT? becomes more likely with a higher weight for this
feature.
7http://alchemy.cs.washington.edu/
8http://code.google.com/p/thebeast/
Figure 2: Example of Transition Rule 2
and a time expression t; relDCT(e, r) denotes the
temporal relation r between an event e and DCT;
relE2E(e1, e2, r) represents the relation r between
two events of the adjacent sentences, e1 and e2.
Our observed predicates reflect information we
were given (such as the words of a sentence), and
additional information we extracted from the cor-
pus (such as POS tags and parse trees). Note that
the TempEval data also contained temporal rela-
tions that were not supposed to be predicted. These
relations are represented using two observed pred-
icates: relT2T(t1, t2, r) for the relation r between
two time expressions t1 and t2; dctOrder(t, r) for
the relation r between a time expression t and a
fixed DCT.
An illustration of all ?temporal? predicates, both
hidden and observed, can be seen in Figure 3.
4.1 Local Formula
Our MLN is composed of several weighted for-
mulae that we divide into two classes. The first
class contains local formulae for the Tasks A, B
and C. We say that a formula is local if it only
considers the hidden temporal relation of a single
event-event, event-time or event-DCT pair. The
formulae in the second class are global: they in-
volve two or more temporal relations at the same
time, and consider Tasks A, B and C simultane-
ously.
The local formulae are based on features em-
ployed in previous work (Cheng et al, 2007;
Bethard andMartin, 2007) and are listed in Table 1.
What follows is a simple example in order to illus-
trate how we implement each feature as a formula
(or set of formulae).
Consider the tense-feature for Task C. For this
feature we first introduce a predicate tense(e, t)
that denotes the tense t for an event e. Then we
408
Figure 3: Predicates for Joint Formulae; observed
predicates are indicated with dashed lines.
Table 1: Local Features
Feature A B C
EVENT-word X X
EVENT-POS X X
EVENT-stem X X
EVENT-aspect X X X
EVENT-tense X X X
EVENT-class X X X
EVENT-polarity X X
TIMEX3-word X
TIMEX3-POS X
TIMEX3-value X
TIMEX3-type X
TIMEX3-DCT order X X
positional order X
in/outside X
unigram(word) X X
unigram(POS) X X
bigram(POS) X
trigram(POS) X X
Dependency-Word X X X
Dependency-POS X X
add a set of formulae such as
tense(e1, past) ? tense(e2, future)
? relE2E(e1, e2, before) (4)
for all possible combinations of tenses and tempo-
ral relations.9
4.2 Global Formula
Our global formulae are designed to enforce con-
sistency between the three hidden predicates (and
the two observed temporal predicates we men-
tioned earlier). They are based on the transition
9This type of ?template-based? formulae generation can be
performed automatically by the Markov Logic Engine.
rules we mentioned in Section 3.
Table 2 shows the set of formula templates we
use to generate the global formulae. Here each
template produces several instantiations, one for
each assignment of temporal relation classes to the
variables R1, R2, etc. One example of a template
instantiation is the following formula.
dctOrder(t1, before) ? relDCT(e1, after)
? relE2T(e1, t1, after) (5a)
This formula is an expansion of the formula tem-
plate in the second row of Table 2. Note that it
utilizes the results of Task B to solve Task A.
Formula 5a should always hold,10 and hence we
could easily implement it as a hard constraint in
an ILP-based framework. However, some transi-
tion rules are less determinstic and should rather
be taken as ?rules of thumb?. For example, for-
mula 5b is a rule which we expect to hold often,
but not always.
dctOrder(t1, before) ? relDCT(e1, overlap)
? relE2T(e1, t1, after) (5b)
Fortunately, this type of soft rule poses no prob-
lem for Markov Logic: after training, Formula 5b
will simply have a lower weight than Formula 5a.
By contrast, in a ?Local Classifier + ILP?-based
approach as followed by Chambers and Jurafsky
(2008) it is less clear how to proceed in the case
of soft rules. Surely it is possible to incorporate
weighted constraints into ILPs, but how to learn the
corresponding weights is not obvious.
5 Experimental Setup
With our experiments we want to answer two
questions: (1) does jointly tackling Tasks A, B,
and C help to increase overall accuracy of tempo-
ral relation identification? (2) How does our ap-
proach compare to state-of-the-art results? In the
following we will present the experimental set-up
we chose to answer these questions.
In our experiments we use the test and training
sets provided by the TempEval shared task. We
further split the original training data into a training
and a development set, used for optimizing param-
eters and formulae. For brevity we will refer to the
training, development and test set as TRAIN, DEV
and TEST, respectively. The numbers of temporal
relations in TRAIN, DEV, and TEST are summa-
rized in Table 3.
10However, due to inconsistent annotations one will find vi-
olations of this rule in the TempEval data.
409
Table 2: Joint Formulae for Global Model
Task Formula
A? B dctOrder(t, R1) ? relE2T(e, t, R2) ? relDCT(e,R3)
B ? A dctOrder(t, R1) ? relDCT(e,R2) ? relE2T(e, t, R3)
B ? C relDCT(e1, R1) ? relDCT(e2, R2) ? relE2E(e1, e2, R3)
C ? B relDCT(e1, R1) ? relE2E(e1, e2, R2) ? relDCT(e2, R3)
A? C relE2T(e1, t1, R1) ? relT2T(t1, t2, R2) ? relE2T(e2, t2, R3) ? relE2E(e1, e2, R4)
C ? A relE2T(e2, t2, R1) ? relT2T(t1, t2, R2) ? relE2E(e1, e2, R3) ? relE2T(e1, t1, R4)
Table 3: Numbers of Labeled Relations for All
Tasks
TRAIN DEV TEST TOTAL
Task A 1359 131 169 1659
Task B 2330 227 331 2888
Task C 1597 147 258 2002
For feature generation we use the following
tools. 11 POS tagging is performed with TnT
ver2.2;12 for our dependency-based features we use
MaltParser 1.0.0.13 For inference in our models
we use Cutting Plane Inference (Riedel, 2008) with
ILP as a base solver. This type of inference is ex-
act and often very fast because it avoids instantia-
tion of the complete Markov Network. For learning
we apply one-best MIRA (Crammer and Singer,
2003) with Cutting Plane Inference to find the cur-
rent model guess. Both training and inference algo-
rithms are provided by Markov thebeast, a Markov
Logic interpreter tailored for NLP applications.
Note that there are several ways to manually op-
timize the set of formulae to use. One way is to
pick a task and then choose formulae that increase
the accuracy for this task on DEV. However, our
primary goal is to improve the performance of all
the tasks together. Hence we choose formulae with
respect to the total score over all three tasks. We
will refer to this type of optimization as ?averaged
optimization?. The total scores of the all three tasks
are defined as follows:
Ca + Cb + Cc
Ga +Gb +Gc
where Ca, Cb, and Cc are the number of the cor-
rectly identified labels in each task, and Ga, Gb,
and Gc are the numbers of gold labels of each task.
Our system necessarily outputs one label to one re-
lational link to identify. Therefore, for all our re-
11Since the TempEval trial has no restriction on pre-
processing such as syntactic parsing, most participants used
some sort of parsers.
12http://www.coli.uni-saarland.de/
?thorsten/tnt/
13http://w3.msi.vxu.se/?nivre/research/
MaltParser.html
sults, precision, recall, and F-measure are the exact
same value.
For evaluation, TempEval proposed the two scor-
ing schemes: ?strict? and ?relaxed?. For strict scor-
ing we give full credit if the relations match, and no
credit if they do not match. On the other hand, re-
laxed scoring gives credit for a relation according
to Table 4. For example, if a system picks the re-
lation ?AFTER? that should have been ?BEFORE?
according to the gold label, it gets neither ?strict?
nor ?relaxed? credit. But if the system assigns
?B-O (BEFORE-OR-OVERLAP)? to the relation,
it gets a 0.5 ?relaxed? score (and still no ?strict?
score).
6 Results
In the following we will first present our com-
parison of the local and global model. We will then
go on to put our results into context and compare
them to the state-of-the-art.
6.1 Impact of Global Formulae
First, let us show the results on TEST in Ta-
ble 5. You will find two columns, ?Global? and
?Local?, showing scores achieved with and with-
out joint formulae, respectively. Clearly, the global
models scores are higher than the local scores for
all three tasks. This is also reflected by the last row
of Table 5. Here we see that we have improved
the averaged performance across the three tasks by
approximately 2.5% (? < 0.01, McNemar?s test 2-
tailed). Note that with 3.5% the improvements are
particularly large for Task C.
The TempEval test set is relatively small (see Ta-
ble 3). Hence it is not clear how well our results
would generalize in practice. To overcome this is-
sue, we also evaluated the local and global model
using 10-fold cross validation on the training data
(TRAIN + DEV). The corresponding results can be
seen in Table 6. Note that the general picture re-
mains: performance for all tasks is improved, and
the averaged score is improved only slightly less
than for the TEST results. However, this time the
score increase for Task B is lower than before. We
410
Table 4: Evaluation Weights for Relaxed Scoring
B O A B-O O-A V
B 1 0 0 0.5 0 0.33
O 0 1 0 0.5 0.5 0.33
A 0 0 1 0 0.5 0.33
B-O 0.5 0.5 0 1 0.5 0.67
O-A 0 0.5 0.5 0.5 1 0.67
V 0.33 0.33 0.33 0.67 0.67 1
B: BEFORE O: OVERLAP
A: AFTER B-O: BEFORE-OR-OVERLAP
O-A: OVERLAP-OR-AFTER V: VAGUE
Table 5: Results on TEST Set
Local Global
task strict relaxed strict relaxed
Task A 0.621 0.669 0.645 0.687
Task B 0.737 0.753 0.758 0.777
Task C 0.531 0.599 0.566 0.632
All 0.641 0.682 0.668 0.708
Table 6: Results with 10-fold Cross Validation
Local Global
task strict relaxed strict relaxed
Task A 0.613 0.645 0.662 0.691
Task B 0.789 0.810 0.799 0.819
Task C 0.533 0.608 0.552 0.623
All 0.667 0.707 0.689 0.727
see that this is compensated by much higher scores
for Task A and C. Again, the improvements for all
three tasks are statistically significant (? < 10?8,
McNemar?s test, 2-tailed).
To summarize, we have shown that by tightly
connecting tasks A, B and C, we can improve tem-
poral relation identification significantly. But are
we just improving a weak baseline, or can joint
modelling help to reach or improve the state-of-the-
art results? We will try to answer this question in
the next section.
6.2 Comparison to the State-of-the-art
In order to put our results into context, Table 7
shows them along those of other TempEval par-
ticipants. In the first row, TempEval Best gives
the best scores of TempEval for each task. Note
that all but the strict scores of Task C are achieved
by WVALI (Puscasu, 2007), a hybrid system that
combines machine learning and hand-coded rules.
In the second row we see the TempEval average
scores of all six participants in TempEval. The
third row shows the results of CU-TMP (Bethard
and Martin, 2007), an SVM-based system that
achieved the second highest scores in TempEval for
all three tasks. CU-TMP is of interest because it is
the best pure Machine-Learning-based approach so
far.
The scores of our local and global model come
in the fourth and fifth row, respectively. The last
row in the table shows task-adjusted scores. Here
we essentially designed and applied three global
MLNs, each one tailored and optimized for a dif-
ferent task. Note that the task-adjusted scores are
always about 1% higher than those of the single
global model.
Let us discuss the results of Table 7 in detail. We
see that for task A, our global model improves an
already strong local model to reach the best results
both for strict scores (with a 3% points margin) and
relaxed scores (with a 5% points margin).
For Task C we see a similar picture: here adding
global constraints helped to reach the best strict
scores, again by a wide margin. We also achieve
competitive relaxed scores which are in close range
to the TempEval best results.
Only for task B our results cannot reach the best
TempEval scores. While we perform slightly better
than the second-best system (CU-TMP), and hence
report the best scores among all pure Machine-
Learning based approaches, we cannot quite com-
pete with WVALI.
6.3 Discussion
Let us discuss some further characteristics and
advantages of our approach. First, notice that
global formulae not only improve strict but also re-
laxed scores for all tasks. This suggests that we
produce more ambiguous labels (such as BEFORE-
OR-OVERLAP) in cases where the local model has
been overconfident (and wrongly chose BEFORE
or OVERLAP), and hence make less ?fatal errors?.
Intuitively this makes sense: global consistency is
easier to achieve if our labels remain ambiguous.
For example, a solution that labels every relation
as VAGUE is globally consistent (but not very in-
formative).
Secondly, one could argue that our solution to
joint temporal relation identification is too com-
plicated. Instead of performing global inference,
one could simply arrange local classifiers for the
tasks into a pipeline. In fact, this has been done by
Bethard and Martin (2007): they first solve task B
and then use this information as features for Tasks
A and C.While they do report improvements (0.7%
411
Table 7: Comparison with Other Systems
Task A Task B Task C
strict relaxed strict relaxed strict relaxed
TempEval Best 0.62 0.64 0.80 0.81 0.55 0.64
TempEval Average 0.56 0.59 0.74 0.75 0.51 0.58
CU-TMP 0.61 0.63 0.75 0.76 0.54 0.58
Local Model 0.62 0.67 0.74 0.75 0.53 0.60
Global Model 0.65 0.69 0.76 0.78 0.57 0.63
Global Model (Task-Adjusted) (0.66) (0.70) (0.76) (0.79) (0.58) (0.64)
on Task A, and about 0.5% on Task C), generally
these improvements do not seem as significant as
ours. What is more, by design their approach can
not improve the first stage (Task B) of the pipeline.
On the same note, we also argue that our ap-
proach does not require more implementation ef-
forts than a pipeline. Essentially we only have to
provide features (in the form of formulae) to the
Markov Logic Engine, just as we have to provide
for a SVM or MaxEnt classifier.
Finally, it became more clear to us that there are
problems inherent to this task and dataset that we
cannot (or only partially) solve using global meth-
ods. First, there are inconsistencies in the training
data (as reflected by the low inter-annotator agree-
ment) that often mislead the learner?this prob-
lem applies to learning of local and global formu-
lae/features alike. Second, the training data is rela-
tively small. Obviously, this makes learning of re-
liable parameters more difficult, particularly when
data is as noisy as in our case. Third, the tempo-
ral relations in the TempEval dataset only directly
connect a small subset of events. This makes global
formulae less effective.14
7 Conclusion
In this paper we presented a novel approach to
temporal relation identification. Instead of using
local classifiers to predict temporal order in a pair-
wise fashion, our approach uses Markov Logic to
incorporate both local features and global transi-
tion rules between temporal relations.
We have focused on transition rules between
temporal relations of the three TempEval subtasks:
temporal ordering of events, of events and time ex-
pressions, and of events and the document creation
time. Our results have shown that global transition
rules lead to significantly higher accuracy for all
three tasks. Moreover, our global Markov Logic
14See (Chambers and Jurafsky, 2008) for a detailed discus-
sion of this problem, and a possible solution for it.
model achieves the highest scores reported so far
for two of three tasks, and very competitive results
for the remaining one.
While temporal transition rules can also be cap-
tured with an Integer Linear Programming ap-
proach (Chambers and Jurafsky, 2008), Markov
Logic has at least two advantages. First, handling
of ?rules of thumb? between less specific tempo-
ral relations (such as OVERLAP or VAGUE) is
straightforward?we simply let the Markov Logic
Engine learn weights for these rules. Second, there
is less engineering overhead for us to perform, be-
cause we do not need to generate ILPs for each doc-
ument.
However, potential for further improvements
through global approaches seems to be limited by
the sparseness and inconsistency of the data. To
overcome this problem, we are planning to use ex-
ternal or untagged data along with methods for un-
supervised learning in Markov Logic (Poon and
Domingos, 2008).
Furthermore, TempEval-2 15 is planned for 2010
and it has challenging temporal ordering tasks in
five languages. So, we would like to investigate the
utility of global formulae for multilingual tempo-
ral ordering. Here we expect that while lexical and
syntax-based features may be quite language de-
pendent, global transition rules should hold across
languages.
Acknowledgements
This work is partly supported by the Integrated
Database Project, Ministry of Education, Culture,
Sports, Science and Technology of Japan.
References
Steven Bethard and James H. Martin. 2007. Cu-tmp:
Temporal relation classification using syntactic and
semantic features. In Proceedings of the 4th Interna-
tional Workshop on SemEval-2007., pages 129?132.
15http://www.timeml.org/tempeval2/
412
Branimir Boguraev and Rie Kubota Ando. 2005.
Timeml-compliant text analysis for temporal reason-
ing. In Proceedings of the 19th International Joint
Conference on Artificial Intelligence, pages 997?
1003.
Nathanael Chambers and Daniel Jurafsky. 2008.
Jointly combining implicit constraints improves tem-
poral ordering. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 698?706, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
Yuchang Cheng, Masayuki Asahara, and Yuji Mat-
sumoto. 2007. Naist.japan: Temporal relation iden-
tification using dependency parsed tree. In Proceed-
ings of the 4th International Workshop on SemEval-
2007., pages 245?248.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Lise Getoor and Ben Taskar. 2007. Introduction to Sta-
tistical Relational Learning (Adaptive Computation
and Machine Learning). The MIT Press.
Daphne Koller, 1999. Probabilistic Relational Models,
pages 3?13. Springer, Berlin/Heidelberg, Germany.
Inderjeet Mani, Marc Verhagen, Ben Wellner,
Chong Min Lee, and James Pustejovsky. 2006.
Machine learning of temporal relations. In ACL-44:
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 753?760, Morristown, NJ, USA.
Association for Computational Linguistics.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov Logic.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
650?659, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Georgiana Puscasu. 2007. Wvali: Temporal relation
identification by syntactico-semantic analysis. In
Proceedings of the 4th International Workshop on
SemEval-2007., pages 484?487.
James Pustejovsky, Jose Castano, Robert Ingria, Reser
Sauri, Robert Gaizauskas, Andrea Setzer, and Gra-
ham Katz. 2003. The timebank corpus. In Proceed-
ings of Corpus Linguistics 2003, pages 647?656.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. In Machine Learning.
Sebastian Riedel. 2008. Improving the accuracy and
efficiency of map inference for markov logic. In Pro-
ceedings of UAI 2008.
Ben Taskar, Abbeel Pieter, and Daphne Koller. 2002.
Discriminative probabilistic models for relational
data. In Proceedings of the 18th Annual Conference
on Uncertainty in Artificial Intelligence (UAI-02),
pages 485?492, San Francisco, CA. Morgan Kauf-
mann.
Marc Verhagen, Robert Gaizaukas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal re-
lation identification. In Proceedings of the 4th Inter-
national Workshop on SemEval-2007., pages 75?80.
413
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 349?353,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Sentence Compression with Semantic Role Constraints
Katsumasa Yoshikawa
Precision and Intelligence Laboratory,
Tokyo Institute of Technology, Japan
IBM Research-Tokyo, IBM Japan, Ltd.
katsumasay@gmail.com
Ryu Iida
Department of Computer Science,
Tokyo Institute of Technology, Japan
ryu-i@cl.cs.titech.ac.jp
Tsutomu Hirao
NTT Communication Science Laboratories,
NTT Corporation, Japan
hirao.tsutomu@lab.ntt.co.jp
Manabu Okumura
Precision and Intelligence Laboratory,
Tokyo Institute of Technology, Japan
oku@lr.pi.titech.ac.jp
Abstract
For sentence compression, we propose new se-
mantic constraints to directly capture the relations
between a predicate and its arguments, whereas
the existing approaches have focused on relatively
shallow linguistic properties, such as lexical and
syntactic information. These constraints are based
on semantic roles and superior to the constraints
of syntactic dependencies. Our empirical eval-
uation on the Written News Compression Cor-
pus (Clarke and Lapata, 2008) demonstrates that
our system achieves results comparable to other
state-of-the-art techniques.
1 Introduction
Recent work in document summarization do not
only extract sentences but also compress sentences.
Sentence compression enables summarizers to re-
duce the redundancy in sentences and generate in-
formative summaries beyond the extractive summa-
rization systems (Knight and Marcu, 2002). Con-
ventional approaches to sentence compression ex-
ploit various linguistic properties based on lexical
information and syntactic dependencies (McDonald,
2006; Clarke and Lapata, 2008; Cohn and Lapata,
2008; Galanis and Androutsopoulos, 2010).
In contrast, our approach utilizes another property
based on semantic roles (SRs) which improves weak-
nesses of syntactic dependencies. Syntactic depen-
dencies are not sufficient to compress some complex
sentences with coordination, with passive voice, and
with an auxiliary verb. Figure 1 shows an example
with a coordination structure. 1
1This example is from Written News Compression Cor-
pus (http://jamesclarke.net/research/resources).
Figure 1: Semantic Role vs. Dependency Relation
In this example, a SR labeler annotated thatHarari
is an A0 argument of left and an A1 argument of
became. Harari is syntactically dependent on left ?
SBJ(left-2, Harari-1). However, Harari is not depen-
dent on became and we are hence unable to utilize a
dependency relation between Harari and became di-
rectly. SRs allow us to model the relations between
a predicate and its arguments in a direct fashion.
SR constraints are also advantageous in that we
can compress sentences with semantic information.
In Figure 1, became has three arguments, Harari as
A1, businessman as A2, and shortly afterward as
AM-TMP. As shown in this example, shortly after-
word can be omitted (shaded boxes). In general,
modifier arguments like AM-TMP or AM-LOC are
more likely to be reduced than complement cases
like A0-A4. We can implement such properties by
SR constraints.
Liu and Gildea (2010) suggests that SR features
contribute to generating more readable sentence in
machine translation. We expect that SR features also
help our system to improve readability in sentence
compression and summarization.
2 Why are Semantic Roles Useful for Com-
pressing Sentences?
Before describing our system, we show the statis-
tics in terms of predicates, arguments and their rela-
349
Label In Compression / Total Ratio
A0 1454 / 1607 0.905
A1 1916 / 2208 0.868
A2 427 / 490 0.871
AM-TMP 261 / 488 0.535
AM-LOC 134 / 214 0.626
AM-ADV 115 / 213 0.544
AM-DIS 8 / 85 0.094
Table 1: Statistics of Arguments in Compression
tions in the Written News Compression (WNC) Cor-
pus. It has 82 documents (1,629 sentences). We di-
vided them into three: 55 documents are used for
training (1106 sentences); 10 for development (184
sentences); 17 for testing (339 sentences).
Our investigation was held in training data. There
are 3137 verbal predicates and 7852 unique argu-
ments. We performed SR labeling by LTH (Johans-
son and Nugues, 2008), an SR labeler for CoNLL-
2008 shared task. Based on the SR labels annotated
by LTH, we investigated that, for all predicates in
compression, how many their arguments were also
in. Table 1 shows the survival ratio of main argu-
ments in compression. Labels A0, A1, and A2 are
complement case roles and over 85% of them survive
with their predicates. On the other hand, for modifier
arguments (AM-X), survival ratios are down to lower
than 65%. Our SR constraints implement the differ-
ence of survival ratios by SR labels. Note that de-
pendency labels SBJ and OBJ generally correspond
to SR labels A0 and A1, respectively. But their total
numbers are 777 / 919 (SBJ) and 918 / 1211 (OBJ)
and much fewer than A0 and A1 labels. Thus, SR la-
bels can connect much more arguments to their pred-
icates.
3 Approach
This section describes our new approach to sen-
tence compression. In order to introduce rich syn-
tactic and semantic constraints to a sentence com-
pression model, we employ Markov Logic (Richard-
son and Domingos, 2006). Since Markov Logic sup-
ports both soft and hard constraints, we can imple-
ment our SR constraints in simple and direct fash-
ion. Moreover, implementations of learning and
inference methods are already provided in existing
Markov Logic interpreters such as Alchemy 2 and
Markov thebeast. 3 Thus, we can focus our effort
2http://alchemy.cs.washington.edu/
3http://code.google.com/p/thebeast/
on building a set of formulae called Markov Logic
Network (MLN). So, in this section, we describe our
proposed MLN in detail.
3.1 Proposed Markov Logic Network
First, let us define our MLN predicates. We sum-
marize the MLN predicates in Table 2. We have only
one hidden MLN predicate, inComp(i) which mod-
els the decision we need to make: whether a token i
is in compression or not. The other MLN predicates
are called observed which provide features. With our
MLN predicates defined, we can now go on to in-
corporate our intuition about the task using weighted
first-order logic formulae. We define SR constraints
and the other formulae in Sections 3.1.1 and 3.1.2,
respectively.
3.1.1 Semantic Role Constraints
Semantic role labeling generally includes the three
subtasks: predicate identification; argument role la-
beling; sense disambiguation. Our model exploits
the results of predicate identification and argument
role labeling. 4 pred(i) and role(i, j, r) indicate the
results of predicate identification and role labeling,
respectively.
First, the formula describing a local property of a
predicate is
pred(i) ? inComp(i) (1)
which denotes that, if token i is a predicate then i is
in compression. A formula with exact one hidden
predicate is called local formula.
A predicate is not always in compression. The for-
mula reducing some predicates is
pred(i) ? height(i,+n) ? ?inComp(i) (2)
which implies that a predicate i is not in compression
with n height in a dependency tree. Note the + nota-
tion indicates that the MLN contains one instance of
the rule, with a separate weight, for each assignment
of the variables with a plus sign.
As mentioned earlier, our SR constraints model
the difference of the survival rate of role labels in
compression. Such SR constraints are encoded as:
role(i, j, +r) ? inComp(i) ? inComp( j) (3)
role(i, j,+r) ? ?inComp(i) ? ?inComp( j) (4)
which represent that, if a predicate i is (not) in com-
pression, then its argument j is (not) also in with
4Sense information is too sparse because the size of the
WNC Corpus is not big enough.
350
predicate definition
inComp(i) Token i is in compression
pred(i) Token i is a predicate
role(i, j, r) Token i has an argument j with role r
word(i, w) Token i has word w
pos(i, p) Token i has Pos tag p
dep(i, j, d) Token i is dependent on token j with
dependency label d
path(i, j, l) Tokens i and j has syntactic path l
height(i, n) Token i has height n in dependency tree
Table 2: MLN Predicates
role r. These formulae are called global formulae
because they have more than two hidden MLN pred-
icates. With global formulae, our model makes two
decisions at a time. When considering the example
in Figure 1, Formula (3) will be grounded as:
role(9, 1, A0) ? inComp(9) ? inComp(1) (5)
role(9, 7, AM-TMP) ? inComp(9) ? inComp(7). (6)
In fact, Formula (5) gains a higher weight than For-
mula (6) by learning on training data. As a re-
sult, our system gives ?1-Harari? more chance to
survive in compression. We also add some exten-
sions of Formula (3) combined with dep(i, j, +d) and
path(i, j, +l) which enhance SR constraints. Note, all
our SR constraints are ?predicate-driven? (only ?
not ? as in Formula (13)). Because an argument is
usually related to multiple predicates, it is difficult to
model ?argument-driven? formula.
3.1.2 Lexical and Syntactic Features
For lexical and syntactic features, we mainly refer
to the previous work (McDonald, 2006; Clarke and
Lapata, 2008). The first two formulae in this sec-
tion capture the relation of the tokens with their lexi-
cal and syntactic properties. The formula describing
such a local property of a word form is
word(i,+w) ? inComp(i) (7)
which implies that a token i is in compression with a
weight that depends on the word form.
For part-of-speech (POS), we add unigram and bi-
gram features with the formulae,
pos(i, +p) ? inComp(i) (8)
pos(i, +p1) ? pos(i + 1,+p2) ? inComp(i). (9)
POS features are often more reasonable than word
form features to combine with the other properties.
The formula,
pos(i, +p) ? height(i, +n) ? inComp(i). (10)
is a combination of POS features and a height in a
dependency tree.
The next formula combines POS bigram features
with dependency relations.
pos(i,+p1) ? pos( j, +p2) ?
dep(i, j,+d) ? inComp(i). (11)
Moreover, our model includes the following
global formulae,
dep(i, j,+d) ? inComp(i) ? inComp( j) (12)
dep(i, j,+d) ? inComp(i) ? inComp( j) (13)
which enforce the consistencies between head and
modifier tokens. Formula (12) represents that if
we include a head token in compression then its
modifier must also be included. Formula (13) en-
sures that head and modifier words must be simul-
taneously kept in compression or dropped. Though
Clarke and Lapata (2008) implemented these depen-
dency constraints by ILP, we implement them by
soft constraints of MLN. Note that Formula (12) ex-
presses the same properties as Formula (3) replacing
dep(i, j, +d) by role(i, j,+r).
4 Experiment and Result
4.1 Experimental Setup
Our experimental setting follows previous
work (Clarke and Lapata, 2008). As stated in
Section 2, we employed the WNC Corpus. For
preprocessing, we performed POS tagging by
stanford-tagger. 5 and dependency parsing by
MST-parser (McDonald et al, 2005). In addition,
LTH 6 was exploited to perform both dependency
parsing and SR labeling. We implemented our
model by Markov Thebeast with Gurobi optimizer. 7
Our evaluation consists of two types of automatic
evaluations. The first evaluation is dependency based
evaluation same as Riezler et al (2003). We per-
formed dependency parsing on gold data and system
outputs by RASP. 8 Then we calculated precision, re-
call, and F1 for the set of label(head, modi f ier).
In order to demonstrate how well our SR con-
straints keep correct predicate-argument structures
in compression, we propose SRL based evalua-
tion. We performed SR labeling on gold data
5http://nlp.stanford.edu/software/tagger.shtml
6http://nlp.cs.lth.se/software/semantic_
parsing:_propbank_nombank_frames
7http://www.gurobi.com/
8http://www.informatics.susx.ac.uk/research/
groups/nlp/rasp/
351
Original [A0 They] [pred say] [A1 the refugees will enhance productivity and economic growth].
MLN with SRL [A0 They] [pred say] [A1 the refugees will enhance growth].
Gold Standard [A1? the refugees will enhance productivity and growth].
Original [A0 A ?16.1m dam] [AM?MOD will] [pred hold] back [A1 a 2.6-mile-long artificial lake to be
known as the Roadford Reservoir].
MLN with SRL [A0 A dam] will [pred hold] back [A1 a artificial lake to be known as the Roadford Reservoir].
Gold Standard [A0 A ?16.1m dam] [AM?MOD will] [pred hold back [A1 a 2.6-mile-long Roadford Reservoir].
Table 4: Analysis of Errors
Model CompR F1-Dep F1-SRL
McDonald 73.6% 38.4% 49.9%
MLN w/o SRL 68.3% 51.3% 57.2%
MLN with SRL 73.1% 58.9% 64.1%
Gold Standard 73.3% ? ?
Table 3: Results of Sentence Compression
and system outputs by LTH. Then we calculated
precision, recall, and F1 value for the set of
role(predicate, argument).
The training time of our MLN model are approx-
imately 8 minutes on all training data, with 3.1GHz
Intel Core i3 CPU and 4G memory. While the pre-
diction can be done within 20 seconds on the test
data.
4.2 Results
Table 3 shows the results of our compression
models by compression rate (CompR), dependency-
based F1 (F1-Dep), and SRL-based F1 (F1-SRL). In
our experiment, we have three models. McDonald
is a re-implementation of McDonald (2006). Clarke
and Lapata (2008) also re-implemented McDonald?s
model with an ILP solver and experimented it on the
WNC Corpus. 9 MLN with SRL and MLN w/o
SRL are our Markov Logic models with and with-
out SR Constraints, respectively.
Note our three models have no constraint for the
length of compression. Therefore, we think the com-
pression rate of the better system should get closer to
that of human compression. In comparison between
MLNmodels and McDonald, the former models out-
perform the latter model on both F1-Dep and F1-
SRL. Because MLN models have global constraints
and can generate syntactically correct sentences.
Our concern is how a model with SR constraints
is superior to a model without them. MLN with
SRL outperforms MLN without SRL with a 7.6
points margin (F1-Dep). The compression rate of
MLN with SRL goes up to 73.1% and gets close
9Clarke?s re-implementation got 60.1% for CompR and
36.0%pt for F1-Dep
to that of gold standard. SRL-based evaluation also
shows that SR constraints actually help extract cor-
rect predicate-argument structures. These results are
promising to improve readability.
It is difficult to directly compare our results with
those of state-of-the-art systems (Cohn and Lapata,
2009; Clarke and Lapata, 2010; Galanis and An-
droutsopoulos, 2010) since they have different test-
ing sets and the results with different compression
rates. However, though our MLN model with SR
constraints utilizes no large-scale data, it is the only
model which achieves close on 60% in F1-Dep.
4.3 Error Analysis
Table 4 indicates two critical examples which our
SR constraints failed to compress correctly. For the
first example, our model leaves an argument with its
predicate because our SR constraints are ?predicate-
driven?. In addition, ?say? is the main verb in this
sentence and hard to be deleted due to the syntactic
significance.
The second example in Table 4 requires to iden-
tify a coreference relation between artificial lake and
Roadford Reservour. We consider that discourse
constraints (Clarke and Lapata, 2010) help our model
handle these cases. Discourse and coreference infor-
mation enable our model to select important argu-
ments and their predicates.
5 Conclusion
In this paper, we proposed new semantic con-
straints for sentence compression. Our model with
global constraints of semantic roles selected correct
predicate-argument structures and successfully im-
proved performance of sentence compression.
As future work, we will compare our model with
the other state-of-the-art systems. We will also inves-
tigate the correlation between readability and SRL-
based score by manual evaluations. Furthermore, we
would like to combine discourse constraints with SR
constraints.
352
References
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31(1):399?429.
James Clarke and Mirella Lapata. 2010. Discourse con-
straints for document compression. Computational
Linguistics, 36(3):411?441.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
the 22nd International Conference on Computational
Linguistics-Volume 1, pages 137?144. Association for
Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research, 34:637?674.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ?10, pages 885?893, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic-semantic analysis
with propbank and nombank. In Proceedings of
the Twelfth Conference on Computational Natural
Language Learning, pages 183?187. Association for
Computational Linguistics.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelligence,
139(1):91?107.
Ding Liu and Daniel Gildea. 2010. Semantic role fea-
tures for machine translation. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 716?724, Beijing, China,
August. Coling 2010 Organizing Committee.
RyanMcDonald, Fernando Pereira, Kiril Ribarov, and Jan
Hajic?. 2005. Non-projective dependency parsing us-
ing spanning tree algorithms. In Proceedings of the
conference on Human Language Technology and Em-
pirical Methods in Natural Language Processing, HLT
?05, pages 523?530, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL, pages 297?304.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62(1-
2):107?136.
Stefan Riezler, Tracy H. King, Richard Crouch, and An-
nie Zaenen. 2003. Statistical sentence condensation
using ambiguity packing and stochastic disambigua-
tion methods for lexical-functional grammar. In Pro-
ceedings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology-Volume 1, pages
118?125. Association for Computational Linguistics.
353
