Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 521?528
Manchester, August 2008
Modeling Semantic Containment and Exclusion
in Natural Language Inference
Bill MacCartney
Stanford University
wcmac@cs.stanford.edu
Christopher D. Manning
Stanford University
manning@cs.stanford.edu
Abstract
We propose an approach to natural lan-
guage inference based on a model of nat-
ural logic, which identifies valid infer-
ences by their lexical and syntactic fea-
tures, without full semantic interpretation.
We greatly extend past work in natural
logic, which has focused solely on seman-
tic containment and monotonicity, to in-
corporate both semantic exclusion and im-
plicativity. Our system decomposes an in-
ference problem into a sequence of atomic
edits linking premise to hypothesis; pre-
dicts a lexical entailment relation for each
edit using a statistical classifier; propagates
these relations upward through a syntax
tree according to semantic properties of in-
termediate nodes; and composes the result-
ing entailment relations across the edit se-
quence. We evaluate our system on the
FraCaS test suite, and achieve a 27% re-
duction in error from previous work. We
also show that hybridizing an existing RTE
system with our natural logic system yields
significant gains on the RTE3 test suite.
1 Introduction
A necessary (if not sufficient) condition for true
natural language understanding is a mastery of
open-domain natural language inference (NLI):
the task of determining whether a natural-language
hypothesis can be inferred from a given premise.
Indeed, NLI can enable more immediate applica-
tions, such as semantic search and question an-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
swering (Harabagiu and Hickl, 2006). In recent
years a spectrum of approaches to robust, open-
domain NLI have been explored within the con-
text of the Recognizing Textual Entailment chal-
lenge (Dagan et al, 2005). Up to now, the most
successful approaches have used fairly shallow
semantic representations, relying on measures of
lexical or semantic overlap (Jijkoun and de Ri-
jke, 2005), pattern-based relation extraction (Ro-
mano et al, 2006), or approximate matching of
predicate-argument structure (Hickl et al, 2006).
Such methods, while robust and often effective,
are at best partial solutions, unable to explain even
simple forms of logical inference. For example,
most shallow approaches would fail to license the
introduction of large in the following example:
(1) Every firm saw costs grow more than expected,
even after adjusting for inflation.
Every large firm saw costs grow.
At the other extreme, some researchers have ap-
proached NLI as logical deduction, building on
work in theoretical semantics to translate sentences
into first-order logic (FOL), and then applying
a theorem prover or model builder (Akhmatova,
2005; Fowler et al, 2005). Regrettably, such ap-
proaches tend to founder on the myriad complexi-
ties of full semantic interpretation, including tense,
aspect, causality, intensionality, modality, vague-
ness, idioms, indexicals, ellipsis, and many other
issues. (What is the right FOL representation of
(1), for example?) FOL-based systems that have
attained high precision (Bos and Markert, 2006)
have done so at the cost of very poor recall.
This work explores a middle way, by develop-
ing a computational model of what Lakoff (1970)
called natural logic, which characterizes valid pat-
terns of inference in terms of syntactic forms re-
521
sembling natural language as much as possible.
1
For example, natural logic might sanction (1) by
observing that: in ordinary (upward monotone)
contexts, deleting modifiers preserves truth; in
downward monotone contexts, inserting modifiers
preserves truth; and every is downward monotone
in its restrictor NP. A natural logic system can thus
achieve the expressivity and precision needed to
handle a great variety of simple logical inferences,
while sidestepping the difficulties of full semantic
interpretation.
2 A theory of natural logic
The natural logic approach originated in traditional
logic (e.g., Aristotle?s syllogisms), and was re-
vived in a formal form by van Benthem (1986) and
S?anchez Valencia (1991), who proposed a natural
logic based on categorial grammar to handle infer-
ences involving containment relations and upward
and downward monotonicity, such as (1). Their
monotonicity calculus explains inferences involv-
ing even nested inversions of monotonicity, but be-
cause it lacks any representation of exclusion (as
opposed to containment), it cannot explain simple
inferences such as (38) and (205) in table 2, below.
Another model which arguably follows the nat-
ural logic tradition (though not presented as such)
was developed by Nairn et al (2006) to explain in-
versions and nestings of implicative (and factive)
predicates, as in Ed did not forget to force Dave to
leave |= Dave left. Their implication projection al-
gorithm bears some resemblance to the monotonic-
ity calculus, but does not incorporate containment
relations or explain interactions between implica-
tives and monotonicity, and thus fails to license
John refused to dance |= John didn?t tango.
We propose a new model of natural logic which
generalizes the monotonicity calculus to cover in-
ferences involving exclusion, and (partly) unifies
it with Nairn et al?s model of implicatives. We
(1) augment the set of entailment relations used
in monotonicity calculus to include representations
of exclusion; (2) generalize the concept of mono-
tonicity to one of projectivity, which describes how
the entailments of a compound expression depend
on the entailments of its parts; and (3) describe a
weak proof procedure based on composing entail-
ment relations across chains of atomic edits.
1
Natural logic should not be confused with natural deduc-
tion, a proof system for first-order logic.
Entailment relations. We employ an inventory
of seven mutually exclusive basic entailment rela-
tions, defined by analogy with set relations: equiv-
alence (couch = sofa); forward entailment (crow
@ bird) and its converse (European A French);
negation, or exhaustive exclusion (human ? non-
human); alternation, or non-exhaustive exclusion
(cat | dog); cover, or non-exclusive exhaustion (an-
imal ` nonhuman); and independence (hungry #
hippo), which covers all other cases. As in the
monotonicity calculus, we define these relations
for expressions of every semantic type: sentences,
common and proper nouns, transitive and intran-
sitive verbs, adjectives, and so on. For example,
among generalized quantifiers, we find that all =
every, every @ some, some?no, no | every, at least
four ` at most six, and most # ten or more.
2
Projectivity. In order to explain the entailments
of a compound expression as a function of the
entailments of its parts, we categorize semantic
functions according to their projectivity class, a
concept which generalizes both S?anchez Valen-
cia?s monotonicity classes (upward, downward,
and non-monotone) and the nine implication sig-
natures of Nairn et al The projectivity class of
a function f describes how the entailment rela-
tion between f(x) and f(y) depends on the en-
tailment relation between x and y. Consider sim-
ple negation (not). Like most functions, it projects
= and # without change (not happy = not glad
and isn?t swimming # isn?t hungry). As a down-
ward monotone function, it swaps @ and A (didn?t
kiss A didn?t touch). But we can also establish
that it projects ? without change (not human ? not
nonhuman) and swaps | and ` (not French ` not
German, not more than 4 | not less than 6). By
contrast, an implicative like refuse, though it also
swaps@ andA (refuse to tangoA refuse to dance),
projects ? as | (refuse to stay | refuse to go) and
projects both | and ` as # (refuse to tango #
refuse to waltz).
Projectivity thus allows us to determine the en-
tailments of a compound expression recursively,
by propagating entailments upward through a se-
mantic composition tree according to the projec-
tivity class of each node on the path to the root. For
example, the semantics of Nobody can enter with-
2
Some of these assertions assume existential import, i.e.,
that the predicates to which the quantifiers are applied have
non-empty denotations. This assumption, standard in tradi-
tional logic, seems justifiable in the context of informal natu-
ral language inference (B?ottner, 1988).
522
out a shirt might be represented by the tree (no-
body (can ((without (a shirt)) enter))). Since shirt
@ clothes, and since without is downward mono-
tone, we have without shirt A without clothes.
Since nobody is also downward monotone, it fol-
lows that Nobody can enter without a shirt @ No-
body can enter without clothes.
Inference. Let x
?
= e(x) be the result of ap-
plying an atomic edit e (the insertion, deletion, or
substitution of a subexpression) to a compound ex-
pression x. The entailment relation between x and
x
?
is found by projecting the entailment relation
generated by e upward through x?s semantic com-
position tree. Substitutions generate relations ac-
cording to the meanings of the substituends. Most
deletions generate the @ relation (red socks @
socks). (Insertions are symmetric: they typically
generate A.) However, some items have special
behavior. For example, deleting (or inserting) not
generates? (not hungry?hungry).
If two expressions are connected by a chain of
atomic edits, we can determine the entailment re-
lation between them by composing (as in Tarskian
relation algebra) the entailment relations generated
by each edit. The result may be a basic entailment
relation, or may be a union of such relations, with
larger unions conveying less information about en-
tailment. This possibility, coupled with the need
to find a chain of atomic edits which preserves rel-
evant entailment relations, limits the power of the
proof procedure described.
Implicatives. The account of implicatives and
factives given by Nairn et al hinges on a classi-
fication of implicative and factive operators into
nine implication signatures, according to their
implications?positive (+), negative (?), or null
(?)?in both positive and negative contexts. Thus
refuse has implication signature ?/?, because it car-
ries a negative implication in a positive context (re-
fused to dance implies didn?t dance), and no impli-
cation in a negative context (didn?t refuse to dance
implies neither danced nor didn?t dance).
Most of the phenomena observed by Nairn et al
can be explained within our framework by spec-
ifying, for each signature, the relation generated
when an operator of that signature is deleted from a
compound expression. For example, deleting sig-
nature ?/? generates | (Jim refused to dance | Jim
danced); under negation, this is projected as `
(Jim didn?t refuse to dance ` Jim didn?t dance).
By contrast, deleting signature ?/? generates A
(Jim attempted to dance A Jim danced); under
negation, this is projected as @ (Jim didn?t attempt
to dance @ Jim didn?t dance).
3
We can also account for monotonicity ef-
fects of implicative and factive operators
by describing the projectivity properties of
each implication signature: signatures +/?,
+/?, and ?/? are upward monotone (attempt
to tango @ attempt to dance); signatures
?/+, ?/?, and ?/+ are downward monotone (refuse
to dance @ refuse to tango); and signatures +/+,
?/?, and ?/? are non-monotone (think dancing is
fun # think tangoing is fun).
3 The NatLog system
Our implementation of natural logic, the NatLog
system, uses a multi-stage architecture like those
of (Marsi and Krahmer, 2005; MacCartney et al,
2006), comprising (1) linguistic analysis, (2) align-
ment, (3) lexical entailment classification, (4) en-
tailment projection, and (5) entailment composi-
tion. We?ll use the following inference as a run-
ning example:
(2) Jimmy Dean refused to move without blue jeans.
James Dean didn?t dance without pants.
The example is admittedly contrived, but it com-
pactly exhibits containment, exclusion, and im-
plicativity. How the NatLog system handles this
example is depicted in table 1.
Linguistic analysis. Relative to other NLI sys-
tems, the NatLog system does comparatively lit-
tle linguistic pre-processing. We rely on the Stan-
ford parser (Klein and Manning, 2003), a Penn
Treebank-trained statistical parser, for tokeniza-
tion, lemmatization, part-of-speech tagging, and
phrase-structure parsing.
By far the most important analysis performed
at this stage, however, is projectivity marking, in
which we compute the effective projectivity for
each token span in each input sentence. In the
premise of (2), for example, we want to determine
that the effective projectivity is upward monotone
3
Factives, however, do not fit as neatly as implicatives:
For example, deleting signature +/+ generates @ (Jim forgot
that dancing is fun @ dancing is fun); yet under negation, this
is projected not as A, but as | (Jim didn?t forget that danc-
ing is fun | dancing isn?t fun). The problem arises because the
implication carried by a factive is not an entailment, but a pre-
supposition. As is well known, the projection behavior of pre-
suppositions differs from that of entailments (van der Sandt,
1992). In the current work, we set presuppositions aside.
523
premise Jimmy Dean refused to move without blue jeans
hypothesis James Dean did n?t dance without pants
edit index 1 2 3 4 5 6 7 8
edit type SUB DEL INS INS SUB MAT DEL SUB
lex features str sim=0.67 implic:+/? cat:aux cat:neg hyponym hypernym
lex entrel = | = ? A = @ @
projectivity ? ? ? ? ? ? ? ?
atomic entrel = | = ? @ = @ @
composition = | | @ @ @ @ @
Table 1: An example of the operation of the NatLog model.
unary operator: without
pattern: IN < /?[Ww]ithout$/
argument 1: projectivity ? on dominating PP
pattern: __ > PP=proj
binary operator: most
pattern: JJS < /?[Mm]ost$/ !> QP
argument 1: projectivity 6?? on dominating NP
pattern: __ >+(NP) (NP=proj !> NP)
argument 2: projectivity ? on dominating S
pattern: __ >> (S=proj !> S)
Figure 1: Some projectivity operator definitions.
for Jimmy Dean and refused to, downward mono-
tone for move and without, and upward monotone
for blue and jeans. Our choice of a Treebank-
trained parser (driven by the goal of broad cov-
erage) complicates this effort, because the nesting
of constituents in phrase-structure parses does not
always correspond to the structure of idealized se-
mantic composition trees. Our solution is imper-
fect but effective. We define a list of operator types
affecting projectivity (e.g., implicatives like refuse
to, prepositions like without), and for each type we
specify its arity and a Tregex tree pattern (Levy and
Andrew, 2006) which permits us to identify its oc-
currences in our Treebank parses. We also specify,
for each argument position of each type, both the
projectivity class and another Tregex pattern which
helps us to determine the sentence span over which
the operator?s effect is projected. (Figure 1 shows
some example definitions.) The marking process
computes these projections, performs projectivity
composition where needed, and marks each token
span with its final effective projectivity.
Alignment. Next, we establish an alignment be-
tween the premise P and hypothesis H , repre-
sented by a sequence of atomic edits over spans
of word tokens. This alignment representation
is symmetric and many-to-many, and is general
enough to include various other alignment repre-
sentations as special cases. We define four edit
types: deletion (DEL) of a span from P , insertion
(INS) of a span into H , substitution (SUB) of an H
span for a P span, and match (MAT) of an H span
to a P span. Each edit is parameterized by the to-
ken indices at which it operates, and edit indices
may ?cross?, permitting representation of move-
ment. The first four lines of table 1 depict a possi-
ble alignment for our example problem.
An alignment decomposes an inference problem
into a sequence of atomic inference problems, one
for each atomic edit. Note that edits are ordered,
and that this ordering defines a path from P to H
through intermediate forms. (Edit order need not
correspond to sentence order, though it does in our
example.) The relative ordering of certain kinds
of edits (e.g., the insertion of not) may influence
the effective projectivity applicable for other edits;
consequently, the NatLog system can reorder edits
to maximize the benefit of the projectivity marking
performed during linguistic analysis.
This paper does not present new algorithms for
alignment; we focus instead on identifying en-
tailment relations between aligned sentence pairs.
The experiments described in sections 4 and 5 use
alignments from other sources.
Lexical entailment classification. Much of the
heavy lifting in the NatLog system is done by the
lexical entailment model, which uses a classifier
to predict an entailment relation for each atomic
edit based solely on features of the lexical items in-
volved, independent of context. (For example, this
model should assign the entailment relation A to
the edit SUB(move, dance), regardless of whether
the effective projectivity at the locus of the edit is
upward monotone, downward monotone, or some-
thing else.) In the case of a SUB edit, the features
include:
? WordNet-derived measures of synonymy,
hyponymy, and antonymy between sub-
524
stituends;
? other features indicating semantic related-
ness: the WordNet-based Jiang-Conrath mea-
sure (Jiang and Conrath, 1997) and a feature
based on NomBank (Meyers et al, 2004);
? string similarity features based on Leven-
shtein string-edit distance between lemmas;
? lexical category features, indicating whether
the substituends are prepositions, possessives,
articles, auxiliaries, pronouns, proper nouns,
operator adjectives, punctuation, etc.;
? quantifier category features, which identify
classes of quantifiers with similar properties;
? a feature for unequal numeric expressions
For DEL edits, we use only the lexical cate-
gory features and a feature based on a custom-
built resource which maps implicatives and fac-
tives to their implication signatures. (As noted in
section 2, however, most DEL edits just have @ as
the target lexical entailment relation.) INS edits are
treated symmetrically.
The model uses a decision tree classifier trained
on 2,449 hand-annotated training examples (1,525
SUB edits and 924 DEL/INS edits). The decision
tree is minimally pruned, and contains about 180
leaves. When tested on the training data, the clas-
sifier achieves >99% accuracy, indicating that our
feature representation successfully captures nearly
all relevant distinctions between examples.
Lexical features and lexical entailment relations
for our example appear on lines 5 and 6 of table 1.
Entailment projection. The lexical entailment
relations generated by each atomic edit can now be
projected upward to determine the corresponding
atomic entailment relations, that is, the entailment
relations between successive intermediate forms
on the path from P to H , as defined by the align-
ment. Strictly speaking, the effective projectivity
for a particular edit should be computed based on
the intermediate form upon which the edit oper-
ates, since the projectivity properties of this form
can depend on preceding edits. However, the Nat-
Log system minimizes the need to compute projec-
tivity in intermediate forms by reordering the edits
in an alignment in such a way that effective projec-
tivity can, in most cases, simply be taken from the
projectivity marking of P and H performed during
the linguistic analysis stage.
The effective projectivity and resulting atomic
entailment relation for each edit in our running ex-
ample are depicted in lines 7 and 8 of table 1. For
all (non-MAT) edits but one, the effective projec-
tivity is upward monotone, so that the atomic en-
tailment relation is identical with the lexical en-
tailment relation. However, the SUB(move, dance)
edit occurs in a downward monotone context, so
that the lexical relation A is converted to @ at the
atomic level.
Entailment composition. Finally, the atomic
entailment relations predicted for each edit are
combined, via relation composition, to produce an
overall prediction for the inference problem. Re-
lation composition is deterministic, and for the
most part follows intuitive rules: @ composed with
@ yields @; A composed with A yields A; #
composed with any relation yields #; = com-
posed with any relation yields that relation, and
so on. Composition tends to ?degenerate? towards
#, in the sense that the composition of a chain
of randomly-selected relations tends toward # as
the chain grows longer. This chaining of entail-
ments across edits can be compared to the method
presented in (Harmeling, 2007); however, that ap-
proach assigns to each edit merely a probability of
preserving truth, not an entailment relation.
The last line of table 1 shows the cumulative
composition of the atomic entailment relations in
the line above. Particular noteworthy is the fact
that | and?compose to yield @. (To illustrate: if A
excludes B (fish | human) and B is the negation of
C (human ? nonhuman), then A entails C (fish @
nonhuman).) The final entailment relation in this
line, @, is NatLog?s final (and correct) answer for
our example problem.
4 Evaluating on FraCaS problems
The FraCaS test suite (Cooper et al, 1996) con-
tains 346 NLI problems, divided into nine sections,
each focused on a specific category of semantic
phenomena (listed in table 3). Each problem con-
sists of one or more premise sentences, a question
sentence, and one of three answers: yes (the union
of @ and =), no (the union of | and )?, or unknown
(the union of A, `, and #). Table 2 shows some
example problems.
To facilitate comparison with previous work, we
have evaluated our system using a version of the
FraCas data prepared by (MacCartney and Man-
ning, 2007), in which multiple-premise problems
(44% of the total) and problems lacking a hypoth-
esis or a well-defined answer (3% of the total) are
excluded; question sentences have been converted
525
? ID Premise Hypothesis Ans
1 38 No delegate finished the report. Some delegate finished the report on time. no
1 48 At most ten commissioners spend time at home. At most ten c...s spend a lot of time at home. yes
2 83 Either Smith, Jones or Anderson signed the contract. Jones signed the contract. unk
5 205 Dumbo is a large animal. Dumbo is a small animal. no
6 233 ITEL won more orders than APCOM. ITEL won some orders. yes
9 335 Smith believed that ITEL had won the contract in 1992. ITEL won the contract in 1992. unk
Table 2: Illustrative examples from the FraCaS test suite
System # P % R % Acc %
most common class 183 55.74 100.00 55.74
MacCartney07 183 68.89 60.78 59.56
NatLog 183 89.33 65.69 70.49
? Section # P % R % Acc %
1 Quantifiers 44 95.24 100.00 97.73
2 Plurals 24 90.00 64.29 75.00
3 Anaphora 6 100.00 60.00 50.00
4 Ellipsis 25 100.00 5.26 24.00
5 Adjectives 15 71.43 83.33 80.00
6 Comparatives 16 88.89 88.89 81.25
7 Temporal 36 85.71 70.59 58.33
8 Verbs 8 80.00 66.67 62.50
9 Attitudes 9 100.00 83.33 88.89
1, 2, 5, 6, 9 108 90.38 85.45 87.04
Table 3: Performance on FraCaS problems (three-
way classification). The columns show the number
of problems, precision and recall for the yes class,
and accuracy. Results for NatLog are broken out
by section.
to declarative hypotheses; and alignments between
premise and hypothesis have been automatically
generated and manually corrected.
Results are shown in table 3. We achieve over-
all accuracy of 70.49%, representing a 27% error
reduction from (MacCartney and Manning, 2007).
In the section concerning quantifiers, which is both
the largest and the most amenable to natural logic,
all problems but one are answered correctly.
4
We
also answer all but one problems correctly in the
(admittedly small) section on attitudes, which in-
volves implicatives and factives. Unsurprisingly,
performance is mediocre in four sections concern-
ing semantic phenomena (e.g., ellipsis) not rele-
vant to natural logic and not modeled by the sys-
tem. But in the other five sections (about 60%
of the problems), we achieve accuracy of 87.04%,
an error reduction of 61% from (MacCartney and
4
In fact, the sole exception is disputable, since it hinges on
whether many refers to proportion (apparently, the view held
by the FraCaS authors) or absolute quantity.
guess
yes no unk total
yes 67 4 31 102
answer no 1 16 4 21
unk 7 7 46 60
total 75 27 81 183
Table 4: Confusions on FraCaS data (all sections)
Manning, 2007). What?s more, precision is high in
nearly every section: even outside its areas of ex-
pertise, the system rarely predicts entailment when
none exists.
Since the NatLog system was developed with
FraCaS problems in mind, these results do not con-
stitute a proper evaluation on unseen test data. On
the other hand, the system does no training on Fra-
CaS data, and has had no opportunity to learn its
biases. (Otherwise, accuracy on ?4 could not fall
so far below the baseline.) The system not only an-
swers most problems correctly, but usually does so
for valid reasons, particular within its areas of ex-
pertise. All in all, the results fulfill our main goal
in testing on FraCaS: to demonstrate the represen-
tational and inferential adequacy of our model of
natural logic.
The confusion matrix shown in table 4 reveals
an interesting property of the NatLog system. The
commonest confusions are those where the answer
is yes but we guess unknown. This reflects both
the bias toward yes in the FraCaS data, and the
system?s tendency to predict unknown (entailment
relation #) when confused: given the composition
rules for entailment relations, the system can pre-
dict yes only if all atomic-level predictions are ei-
ther @ or =.
5 Evaluating on RTE problems
NLI problems from the PASCAL RTE Challenge
(Dagan et al, 2005) differ from FraCaS problems
in several important ways. (See table 5 for ex-
amples.) Instead of textbook examples of seman-
526
ID Premise Hypothesis Answer
71 As leaders gather in Argentina ahead of this weekends
regional talks, Hugo Ch?avez, Venezuela?s populist pres-
ident is using an energy windfall to win friends and pro-
mote his vision of 21st-century socialism.
Hugo Ch?avez acts as Venezuela?s president. yes
788 Democrat members of the Ways and Means Committee,
where tax bills are written and advanced, do not have
strong small business voting records.
Democrat members had strong small business
voting records.
no
Table 5: Illustrative examples from the RTE3 development set
tic phenomena, RTE problems are more natural-
seeming, with premises collected ?in the wild?
from newswire. The premises are much longer,
averaging 35 words (vs. 11 words for FraCaS).
Also, RTE aims at binary classification: the RTE
no combines the no and unk answers in FraCaS.
Due to the character of RTE problems, we do
not expect NatLog to be a good general-purpose
solution to solving all RTE problems. First, most
RTE problems depend on forms of inference, such
as paraphrase, temporal reasoning, or relation ex-
traction, which NatLog is not designed to address.
Second, in most RTE problems, the edit distance
between premise and hypothesis is relatively large.
More atomic edits means a greater chance that er-
rors made in lexical entailment classification or
projection will propagate, via entailment compo-
sition, to the system?s final output. Rather, in ap-
plying NatLog to RTE, we hope to make reliable
predictions on a subset of RTE problems, trading
recall for precision. If we succeed, then we may
be able to hybridize with a broad-coverage RTE
system to obtain better results than either system
individually?the same strategy that was adopted
by (Bos and Markert, 2006) for their FOL-based
system. For this purpose, we have chosen to use
the Stanford RTE system described in (de Marn-
effe et al, 2006). We also use the Stanford system
to generate alignments when evaluating NatLog on
RTE problems.
Table 6 shows the performance of NatLog
on RTE3 data. Relative to the Stanford sys-
tem, NatLog achieves high precision on its
yes predictions?above 70%?suggesting that hy-
bridizing may be effective. For comparison, the
FOL-based system reported in (Bos and Markert,
2006) attained a similarly high precision of 76%
on RTE2 problems, but was able to make a pos-
itive prediction in only about 4% of cases. Nat-
Log makes positive predictions far more often?in
about 25% of cases.
The Stanford system makes yes/no predictions
System Data % Yes P % R % Acc %
Stanford dev 50.25 68.66 66.99 67.25
test 50.00 61.75 60.24 60.50
NatLog dev 22.50 73.89 32.38 59.25
test 26.38 70.14 36.10 59.38
Hybrid, bal. dev 50.00 70.25 68.20 68.75
test 50.00 65.50 63.90 64.25
Hybrid, opt. dev 56.00 69.20 75.24 70.00
test 54.50 64.45 68.54 64.50
Table 6: Performance of various systems on RTE3
(two-way classification). The columns show the
data set used (800 problems each), the proportion
of yes predictions, precision and recall for the yes
class, and accuracy.
by thresholding a real-valued inference score. To
construct a hybrid system, we adjust the Stanford
inference scores by +? or ??, depending on
whether or not NatLog predicts yes. We choose
? by optimizing development set accuracy, while
adjusting the threshold to generate balanced pre-
dictions (that is, equal numbers of yes and no pre-
dictions). As an additional experiment, we fix ?
at this value and then adjust the threshold to op-
timize development set accuracy, resulting in an
excess of yes predictions. (Since this optimiza-
tion is based solely on development data, its use
on test data is fully legitimate.) Results for these
two cases are shown in table 6. The parameters
tuned on development data gave good results on
test data. The optimized hybrid system attained
an absolute accuracy gain of 4% over the Stanford
system, corresponding to an extra 32 problems an-
swered correctly. This result is statistically signifi-
cant (p < 0.05, McNemar?s test, 2-tailed).
The gains attributable to NatLog are exempli-
fied by problem 788 (table 5). NatLog sanctions
the deletion of a restrictive modifier and an appos-
itive from the premise, and recognizes that delet-
ing a negation generates a contradiction; thus it
correctly answers no. On the other hand, there
527
are many RTE problems where NatLog?s precision
works against it. For example, NatLog answers no
to problem 71 because it cannot account for the
insertion of acts as in the hypothesis. Fortunately,
both the Stanford system and the hybrid system an-
swer this problem correctly.
6 Conclusion
We do not claim natural logic to be a universal
solution for NLI. Many important types of infer-
ence are not amenable to natural logic, includ-
ing paraphrase (Eve was let go |= Eve lost her
job), verb alternation (he drained the oil |= the
oil drained), relation extraction (Aho, a trader at
UBS, ... |= Aho works for UBS), common-sense
reasoning (the sink overflowed |= the floor got
wet), and so on.
Moreover, because natural logic has a weaker
proof theory than FOL, some inferences lie beyond
its deductive power. For example, it cannot explain
inferences involving De Morgan?s laws for quanti-
fiers, as in Not all birds fly = Some birds don?t fly.
However, by incorporating semantic contain-
ment, semantic exclusion, and implicativity, the
model of natural logic developed in this paper suc-
ceeds in explaining a great variety of everyday pat-
terns of inference. Ultimately, open-domain NLI
is likely to require combining disparate reasoners,
and a facility for natural logic inference is a good
candidate to be a component of such a solution.
Acknowledgements The authors wish to thank
the anonymous reviewers for their helpful com-
ments. This work was supported in part by
ARDA?s Advanced Question Answering for Intel-
ligence (AQUAINT) Program.
References
Akhmatova, Elena. 2005. Textual entailment resolution via
atomic propositions. In Proceedings of the PASCAL Chal-
lenges Workshop on Recognising Textual Entailment.
Bos, Johan and Katja Markert. 2006. When logical inference
helps determining textual entailment (and when it doesn?t).
In Proceedings of the Second PASCAL Challenges Work-
shop on Recognizing Textual Entailment.
B?ottner, Michael. 1988. A note on existential import. Studia
Logica, 47(1):35?40.
Dagan, Ido, Oren Glickman, and Bernardo Magnini. 2005.
The PASCAL Recognising Textual Entailment Challenge.
In Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment.
de Marneffe, Marie-Catherine, Bill MacCartney, Trond
Grenager, Daniel Cer, Anna Rafferty, and Christopher D.
Manning. 2006. Learning to distinguish valid textual en-
tailments. In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognizing Textual Entailment.
Fowler, Abraham, Bob Hauser, Daniel Hodges, Ian Niles,
Adrian Novischi, and Jens Stephan. 2005. Applying CO-
GEX to recognize textual entailment. In Proceedings of
the PASCAL Challenges Workshop on Recognising Textual
Entailment.
Harabagiu, Sanda and Andrew Hickl. 2006. Using scenario
knowledge in automatic question answering. In Proceed-
ings of the Workshop on Task-Focused Summarization and
Question Answering, pages 32?39, Sydney.
Harmeling, Stefan. 2007. An extensible probabilistic
transformation-based approach to the Third Recognizing
Textual Entailment Challenge. In ACL-07 Workshop on
Textual Entailment and Paraphrasing, Prague.
Hickl, Andrew, John Williams, Jeremy Bensley, Kirk Roberts,
Bryan Rink, and Ying Shi. 2006. Recognizing textual
entailment with LCC?s GROUNDHOG system. In Pro-
ceedings of the Second PASCAL Challenges Workshop on
Recognizing Textual Entailment.
Jiang, Jay J. and David W. Conrath. 1997. Semantic simi-
larity based on corpus statistics and lexical taxonomy. In
Proceedings of the International Conference on Research
in Computational Linguistics.
Jijkoun, Valentin and Maarten de Rijke. 2005. Recognizing
textual entailment using lexical similarity. In Proceedings
of the PASCAL Challenges Workshop on Recognizing Tex-
tual Entailment, pages 73?76.
Klein, Dan and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL-03, Sapporo.
Lakoff, George. 1970. Linguistics and natural logic. Synthese,
22:151?271.
Levy, Roger and Galen Andrew. 2006. Tregex and Tsurgeon:
tools for querying and manipulating tree data structures. In
Proceedings of LREC-06, Genoa.
MacCartney, Bill and Christopher D. Manning. 2007. Natural
logic for textual inference. In ACL-07 Workshop on Textual
Entailment and Paraphrasing, Prague.
MacCartney, Bill, Trond Grenager, Marie-Catherine de Marn-
effe, Daniel Cer, and Christopher D. Manning. 2006.
Learning to recognize features of valid textual entailments.
In Proceedings of NAACL-06, New York.
Marsi, Erwin and Emiel Krahmer. 2005. Classification of
semantic relations by humans and machines. In ACL-
05 Workshop on Empirical Modeling of Semantic Equiv-
alence and Entailment, Ann Arbor.
Nairn, Rowan, Cleo Condoravdi, and Lauri Karttunen. 2006.
Computing relative polarity for textual inference. In Pro-
ceedings of ICoS-5 (Inference in Computational Seman-
tics), Buxton, UK.
Romano, Lorenza, Milen Kouylekov, Idan Szpektor, Ido Da-
gan, and Alberto Lavelli. 2006. Investigating a generic
paraphrase-based approach for relation extraction. In Pro-
ceedings of EACL 2006.
S?anchez Valencia, Victor. 1991. Studies on Natural Logic and
Categorial Grammar. Ph.D. thesis, University of Amster-
dam.
van Benthem, Johan. 1986. Essays in logical semantics. Rei-
del, Dordrecht.
van der Sandt, Rob A. 1992. Presupposition projection as
anaphora resolution. Journal of Semantics, 9(4):333?377.
528
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 802?811,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
A Phrase-Based Alignment Model for Natural Language Inference
Bill MacCartney, Michel Galley, Christopher D. Manning
Natural Language Processing Group, Stanford University
{wcmac,mgalley,manning}@stanford.edu
Abstract
The alignment problem?establishing links
between corresponding phrases in two related
sentences?is as important in natural language
inference (NLI) as it is in machine transla-
tion (MT). But the tools and techniques of
MT alignment do not readily transfer to NLI,
where one cannot assume semantic equiva-
lence, and for which large volumes of bitext
are lacking. We present a new NLI aligner,
the MANLI system, designed to address these
challenges. It uses a phrase-based alignment
representation, exploits external lexical re-
sources, and capitalizes on a new set of su-
pervised training data. We compare the per-
formance of MANLI to existing NLI and MT
aligners on an NLI alignment task over the
well-known Recognizing Textual Entailment
data. We show that MANLI significantly out-
performs existing aligners, achieving gains of
6.2% in F1 over a representative NLI aligner
and 10.5% over GIZA++.
1 Introduction
The problem of natural language inference (NLI) is
to determine whether a natural-language hypothesis
H can reasonably be inferred from a given premise
text P . In order to recognize that Kennedy was killed
can be inferred from JFK was assassinated, one
must first recognize the correspondence between
Kennedy and JFK, and between killed and assas-
sinated. Consequently, most current approaches to
NLI rely, implicitly or explicitly, on a facility for
alignment?that is, establishing links between cor-
responding entities and predicates in P and H . Re-
cent entries in the annual Recognizing Textual En-
tailment (RTE) competition (Dagan et al, 2005)
have addressed the alignment problem in a variety
of ways, though often without distinguishing it as
a separate subproblem. Glickman et al (2005) and
Jijkoun and de Rijke (2005), among others, have ex-
plored approaches based on measuring the degree of
lexical overlap between bags of words. While ig-
noring structure, such methods depend on matching
each word in H to the word in P with which it is
most similar?in effect, an alignment. At the other
extreme, Tatu and Moldovan (2007) and Bar-Haim
et al (2007) have formulated the inference problem
as analogous to proof search, using inferential rules
which encode (among other things) knowledge of
lexical relatedness. In such approaches, the corre-
spondence between the words of P andH is implicit
in the steps of the proof.
Increasingly, however, the most successful RTE
systems have made the alignment problem explicit.
Marsi and Krahmer (2005) and MacCartney et al
(2006) first advocated pipelined system architec-
tures containing a distinct alignment component, a
strategy crucial to the top-performing systems of
Hickl et al (2006) and Hickl and Bensley (2007).
However, each of these systems has pursued align-
ment in idiosyncratic and poorly-documented ways,
often using proprietary data, making comparisons
and further development difficult.
In this paper we undertake the first systematic
study of alignment for NLI. We propose a new NLI
alignment system which uses a phrase-based repre-
sentation of alignment, exploits external resources
for knowledge of semantic relatedness, and capi-
talizes on the recent appearance of new supervised
training data for NLI alignment. In addition, we
examine the relation between NLI alignment and
MT alignment, and investigate whether existing MT
aligners can usefully be applied in the NLI setting.
2 NLI alignment vs. MT alignment
The alignment problem is familiar in machine trans-
lation (MT), where recognizing that she came is a
good translation for elle est venue requires establish-
802
ing a correspondence between she and elle, and be-
tween came and est venue. The MT community has
developed not only an extensive literature on align-
ment (Brown et al, 1993; Vogel et al, 1996; Marcu
and Wong, 2002; DeNero et al, 2006), but also
standard, proven alignment tools such as GIZA++
(Och and Ney, 2003). Can off-the-shelf MT aligners
be applied to NLI? There is reason to be doubtful.
Alignment for NLI differs from alignment for MT
in several important respects, including:
1. Most obviously, it is monolingual rather than
cross-lingual, opening the door to utilizing
abundant (monolingual) sources of information
on semantic relatedness, such as WordNet.
2. It is intrinsically asymmetric: P is often much
longer thanH , and commonly contains phrases
or clauses which have no counterpart in H .
3. Indeed, one cannot assume even approximate
semantic equivalence?usually a given in MT.
Because NLI problems include both valid and
invalid inferences, the semantic content of H
may diverge substantially from P . An NLI
aligner must be designed to accommodate fre-
quent unaligned words and phrases.
4. Little training data is available. MT align-
ment models are typically trained in unsu-
pervised fashion, inducing lexical correspon-
dences from massive quantities of sentence-
aligned bitexts. While NLI aligners could in
principle do the same, large volumes of suit-
able data are lacking. NLI aligners must there-
fore depend on smaller quantities of supervised
training data, supplemented by external lexi-
cal resources. Conversely, while existing MT
aligners can make use of dictionaries, they are
not designed to harness other sources of infor-
mation on degrees of semantic relatedness.
Consequently, the tools and techniques of MT align-
ment may not transfer readily to NLI alignment. We
investigate the matter empirically in section 5.2.
3 Data
Until recently, research on alignment for NLI has
been hampered by a paucity of high-quality, publicly
available data from which to learn. Happily, that has
begun to change, with the release by Microsoft Re-
search (MSR) of human-generated alignment anno-
In
most
Pacific
countries
there
are
very
few
women
in
parliament
.
Wom
en
are poor
ly
repre
sente
d
in parlia
ment
.
Figure 1: The MSR gold-standard alignment for problem
116 from the RTE2 development set.
tations (Brockett, 2007) for inference problems from
the second Recognizing Textual Entailment (RTE2)
challenge (Bar-Haim et al, 2006). To our knowl-
edge, this work is the first to exploit this data for
training and evaluation of NLI alignment models.
The RTE2 data consists of a development set and
a test set, each containing 800 inference problems.
Each problem consists of a premise and a hypoth-
esis. The premises contain 29 words on average;
the hypotheses, 11 words. Each problem is marked
as a valid or invalid inference (50% each); how-
ever, these annotations are ignored during align-
ment, since they would not be available during test-
ing of a complete NLI system.
The MSR annotations use an alignment repre-
sentation which is token-based, but many-to-many,
and thus allows implicit alignment of multi-word
phrases. Figure 1 shows an example in which very
few has been aligned with poorly represented.
In the MSR data, every alignment link is marked
as SURE or POSSIBLE. In making this distinction,
the annotators have followed a convention common
in MT, which permits alignment precision to be
measured against both SURE and POSSIBLE links,
while recall is measured against only SURE links.
In this work, however, we have chosen to ignore
POSSIBLE links, embracing the argument made by
(Fraser and Marcu, 2007) that their use has impeded
progress in MT alignment models, and that SURE-
803
only annotation is to be preferred.
Each RTE2 problem was independently annotated
by three people, following carefully designed an-
notation guidelines. Inter-annotator agreement was
high: Brockett (2007) reports Fleiss? kappa1 scores
of about 0.73 (?substantial agreement?) for map-
pings from H tokens to P tokens; and all three an-
notators agreed on ?70% of proposed links, while
at least two of three agreed on more than 99.7%
of proposed links,2 attesting to the high quality of
the annotation data. For this work, we merged the
three independent annotations, using majority rule,3
to obtain a gold-standard annotation containing an
average of 7.3 links per RTE problem.
4 The MANLI aligner
In this section, we describe the MANLI aligner, a
new alignment system designed expressly for NLI
alignment. The MANLI system consists of four el-
ements: (1) a phrase-based representation of align-
ment, (2) a feature-based linear scoring function for
alignments, (3) a decoder which uses simulated an-
nealing to find high-scoring alignments, and (4) per-
ceptron learning to optimize feature weights.
4.1 A phrase-based alignment representation
MANLI uses an alignment representation which is
intrinsically phrase-based. (Following the usage
common in MT, we use ?phrase? to mean any con-
tiguous span of tokens, not necessarily correspond-
ing to a syntactic phrase.) We represent an alignment
E between a premise P and a hypothesis H as a set
of phrase edits {e1, e2, . . .}, each belonging to one
of four types:
? an EQ edit connects a phrase in P with an equal
(by word lemmas) phrase in H
? a SUB edit connects a phrase in P with an un-
equal phrase in H
? a DEL edit covers an unaligned phrase in P
? an INS edit covers an unaligned phrase in H
For example, the alignment shown in fig-
ure 1 can be represented by the set {DEL(In1),
1Fleiss? kappa generalizes Cohen?s kappa to the case where
there are more than two annotators.
2The SURE/POSSIBLE distinction is taken as significant in
computing all these figures.
3The handful of three-way disagreements were treated as
POSSIBLE links, and thus were not used here.
DEL(most2), DEL(Pacific3), DEL(countries4),
DEL(there5), EQ(are6, are2), SUB(very7 few8,
poorly3 represented4), EQ(women9, Women1),
EQ(in10, in5), EQ(parliament11, parliament6),
EQ(.12, .7)}.4
Alignments are constrained to be one-to-one at
the phrase level: every token in P and H belongs
to exactly one phrase, which participates in exactly
one edit (possibly DEL or INS). However, the phrase
representation permits alignments which are many-
to-many at the token level. In fact, this is the chief
motivation for the phrase-based representation: we
can align very few and poorly represented as units,
without being forced to make an arbitrary choice as
to which word goes with which word. Moreover, our
scoring function can make use of lexical resources
which have information about semantic relatedness
of multi-word phrases, not merely individual words.
About 23% of the MSR gold-standard align-
ments are not one-to-one (at the token level), and
are therefore technically unreachable for MANLI,
which is constrained to generate one-to-one align-
ments. However, by merging contiguous token links
into phrase edits of size > 1, most MSR align-
ments (about 92%) can be straightforwardly con-
verted into MANLI-reachable alignments. For the
purpose of model training (but not for the evalua-
tion described in section 5.4), we generated a ver-
sion of the MSR data in which all alignments were
converted to MANLI-reachable form.5
4.2 A feature-based scoring function
To score alignments, we use a simple feature-based
linear scoring function, in which the score of an
alignment is the sum of the scores of the edits it con-
tains (including not only SUB and EQ edits, but also
DEL and INS edits), and the score of an edit is the
dot product of a vector encoding its features and a
vector of weights. If E is a set of edits constituting
4DEL and INS edits of size > 1 are possible in principle, but
are not used in our training data.
5About 8% of the MSR alignments contain non-contiguous
links, most commonly because P contains two references to
an entity (e.g., Christian Democrats and CDU) which are both
linked to a reference to the same entity in H (e.g., Christian
Democratic Union). In such cases, one or more links must be
eliminated to achieve a MANLI-reachable alignment. We used
a string-similarity heuristic to break such conflicts, but were
obliged to make an arbitrary choice in about 2% of cases.
804
an alignment, and ? is a vector of feature functions,
the score s is given by:
s(E) =
?
e?E
s(e) =
?
e?E
w ??(e)
We?ll explain how the feature weights w are set in
section 4.4. The features used to characterize each
edit are as follows:
Edit type features. We begin with boolean fea-
tures encoding the type of each edit. We expect EQs
to score higher than SUBs, and (sinceP is commonly
longer than H) DELs to score higher than INSs.
Phrase features. Next, we have features which
encode the sizes of the phrases involved in the edit,
and whether these phrases are non-constituents (in
syntactic parses of the sentences involved).
Lexical similarity feature. For SUB edits, a very
important feature represents the lexical similarity of
the substituends, as a real value in [0, 1]. This simi-
larity score is computed as a max over a number of
component scoring functions, some based on exter-
nal lexical resources, including:
? various string similarity functions, of which
most are applied to word lemmas
? measures of synonymy, hypernymy, antonymy,
and semantic relatedness, including a widely-
used measure due to Jiang and Conrath (1997),
based on manually constructed lexical re-
sources such as WordNet and NomBank
? a function based on the well-known distribu-
tional similarity metric of Lin (1998), which
automatically infers similarity of words and
phrases from their distributions in a very large
corpus of English text
The ability to leverage external lexical resources?
both manually and automatically constructed?is
critical to the success of MANLI.
Contextual features. Even when the lexical sim-
ilarity for a SUB edit is high, it may not be a
good match. If P or H contains multiple occur-
rences of the same word?which happens frequently
with function words, and occasionally with content
words?lexical similarity may not suffice to deter-
mine the right match. To remedy this, we introduce
contextual features for SUB and EQ edits. A real-
valued distortion feature measures the difference
Inputs
? an alignment problem ?P,H?
? a number of iterations N (e.g. 100)
? initial temperature T0 (e.g. 40) and multiplier r (e.g. 0.9)
? a bound on edit size max (e.g. 6)
? an alignment scoring function, SCORE(E)
Initialize
? Let E be an ?empty? alignment for ?P,H? (containing
only DEL and INS edits, no EQ or SUB edits)
? Set E? = E
Repeat for i = 1 to N
? Let {F1, F2, ...} be the set of possible successors of E.
To generate this set:
? Consider every possible edit f up to size max
? Let C(E, f) be the set of edits in E which ?con-
flict? with f (i.e., involve at least some of the same
tokens as f )
? Let F = E ? {f} \ C(E, f)
? Let s(F ) be a map from successors of E to scores gener-
ated by SCORE
? Set p(F ) = exp s(F ), and then normalize p(F ), trans-
forming the score map to a probability distribution
? Set Ti = r ? Ti?1
? Set p(F ) = p(F )1/Ti , smoothing or sharpening p(F )
? Renormalize p(F )
? Choose a new value for E by sampling from p(F )
? If SCORE(E) > SCORE(E?), set E? = E
Return E?
Figure 2: The MANLI-ALIGN algorithm
between the relative positions of the substituends
within their respective sentences, while boolean
matching neighbors features indicate whether the to-
kens before and after the substituends are equal or
similar.
4.3 Decoding using simulated annealing
The problem of decoding?that is, finding a
high-scoring alignment for a particular inference
problem?is made more complex by our choice of a
phrase-based alignment representation. For a model
which uses a token-based representation (say, one
which simply maps H tokens to P tokens), decod-
ing is trivial, since each token can be aligned inde-
pendently of its neighbors. (This is the case for the
bag-of-words aligner described in section 5.1.) But
with a phrase-based representation, things are more
complicated. The segmentation into phrases is not
given in advance, and every phrase pair considered
for alignment must be consistent with its neighbors
with respect to segmentation. Consequently, the de-
coding problem cannot be factored into a number of
805
independent decisions.
To address this difficulty, we have devised a
stochastic alignment algorithm, MANLI-ALIGN (fig-
ure 2), which uses a simulated annealing strategy.
Beginning from an arbitrary alignment, we make a
series of local steps, at each iteration sampling from
a set of possible successors according to scores as-
signed by our scoring function. The sampling is con-
trolled by a ?temperature? which falls over time. At
the beginning of the process, successors are sampled
with nearly uniform probability, which helps to en-
sure that the space of possibilities is explored and
local maxima are avoided. As the temperature falls,
there is a ever-stronger bias toward high-scoring suc-
cessors, so that the algorithm converges on a near-
optimal alignment. Clever use of memoization helps
to ensure that computational costs remain manage-
able. Using the parameter values suggested in fig-
ure 2, aligning an average RTE problem takes about
two seconds.
While MANLI-ALIGN is not guaranteed to pro-
duce optimal alignments, there is reason to believe
that it usually comes very close. After training, the
alignment found by MANLI scored at least as high
as the gold alignment for 99.6% of RTE problems.6
4.4 Perceptron learning
To tune the parameters w of the model, we use
an adaptation of the averaged perceptron algorithm
(Collins, 2002), which has proven successful on a
range of NLP tasks. The algorithm is shown in fig-
ure 3. After initializing w to 0, we perform N train-
ing epochs. (Our experiments used N = 50.) In
each epoch, we iterate through the training data, up-
dating the weight vector at each training example ac-
cording to the difference between the features of the
target algnment and the features of the alignment
produced by the decoder using the current weight
vector. The size of the update is controlled by a
learning rate which decreases over time. At the end
of each epoch, the weight vector is normalized and
stored. The final result is the average of the stored
6This figure is based on the MANLI-reachable version of
the gold-standard data described in section 4.1. For the raw
gold-standard data, the figure is 88.1%. The difference is almost
entirely attributable to unreachable gold alignments, which tend
to score higher simply because they contain more edits (and
because the learned weights are mostly positive).
Inputs
? training problems ?Pj , Hj?, j = 1..n
? corresponding gold-standard alignments Ej
? a number of learning epochs N (e.g. 50)
? a ?burn-in? period N0 < N (e.g. 10)
? initial learning rate R0 (e.g. 1) and multiplier r (e.g. 0.8)
? a vector of feature functions ?(E)
? an alignment algorithm ALIGN(P,H;w) which finds a
good alignment for ?P,H? using weight vector w
Initialize
? Set w = 0
Repeat for i = 1 to N
? Set Ri = r ?Ri?1, reducing the learning rate
? Randomly shuffle the training problems
? For j = 1 to n:
? Set E?j = ALIGN(Pj , Hj ; w)
? Set w = w + Ri ? (?(Ej)??(E?j))
? Set w = w/?w?2 (L2 normalization)
? Set w[i] = w, storing the weight vector for this epoch
Return an averaged weight vector:
? wavg = 1/(N ?N0)
PN
i=N0+1
w[i]
Figure 3: The MANLI-LEARN algorithm
weight vectors, omitting vectors from a fixed num-
ber of epochs at the beginning of the run (which tend
to be of poor quality). Using the parameter values
suggested in figure 3, training runs on the RTE2 de-
velopment set required about 20 hours.
5 Evaluating aligners on MSR data
In this section, we describe experiments designed to
evaluate the performance of various alignment sys-
tems on the MSR gold-standard data described in
section 3. For each system, we report precision,
recall, and F-measure (F1).7 Note that these are
macro-averaged statistics, computed per problem by
counting aligned token pairs,8 and then averaged
over all problems in a problem set.9 We also re-
7MT researchers conventionally report results in terms of
alignment error rate (AER). Since we use only SURE links in the
gold-standard data (see section 3), AER is equivalent to 1?F1.
8For phrase-based alignments like those generated by
MANLI, two tokens are considered to be aligned iff they are
contained within phrases which are aligned.
9MT evaluations conventionally use micro-averaging, which
gives greater weight to problems containing more aligned pairs.
This makes sense in MT, where the purpose of alignment is to
induce phrase tables. But in NLI, where the ultimate goal is
to maximize the number of inference problems answered cor-
rectly, it is more fitting to give all problems equal weight, and
so we macro-average. We have also generated all results using
micro-averaging, and found that the relative comparisons are
806
port the exact match rate, that is, the proportion of
problems in which the guessed alignment exactly
matches the gold alignment. The results are sum-
marized in table 1.
5.1 A robust baseline: the bag-of-words aligner
As a baseline, we use a simple alignment algorithm
inspired by the lexical entailment model of Glick-
man et al (2005), and similar to the simple heuristic
model described in (Och and Ney, 2003). Each hy-
pothesis word h is aligned to the premise word p to
which it is most similar, according to a lexical sim-
ilarity function sim(p, h) which returns scores in
[0, 1]. While Glickman et al used a function based
on web co-occurrence statistics, we use a much sim-
pler function based on string edit distance:
sim(w1, w2) = 1?
dist(lem(w1), lem(w2))
max(|lem(w1)|, |lem(w2)|)
(Here lem(w) denotes the lemma of word w; dist()
denotes Levenshtein string edit distance; and | ? | de-
notes string length.)
This model can be easily extended to generate an
alignment score, which will be of interest in sec-
tion 6. We define the score for a specific hypoth-
esis token h to be the log of its similarity with
the premise token p to which it is aligned, and the
score for the complete alignment of hypothesis H
to premise P to be the sum of the scores of the to-
kens in H , weighted by inverse document frequency
in a large corpus10 (so that common words get less
weight), and normalized by the length of H:
score(h|P ) = logmax
p?P
sim(p, h)
score(H|P ) =
1
|H|
?
h?H
idf(h) ? score(h|P )
Despite the simplicity of this alignment model, its
performance is fairly robust, with good recall. Its
precision, however, its mediocre?chiefly because,
by design, it aligns every h with some p. The model
could surely be improved by allowing it to leave
some H tokens unaligned, but this was not pursued.
not greatly affected.
10We use idf(w) = log(N/Nw), where N is the number of
documents in the corpus, and Nw is the number of documents
containing word w.
System Data P % R % F1 % E %
Bag-of-words dev 57.8 81.2 67.5 3.5
(baseline) test 62.1 82.6 70.9 5.3
GIZA++ dev 83.0 66.4 72.1 9.4
(using lex, ?) test 85.1 69.1 74.8 11.3
Cross-EM dev 67.6 80.1 72.1 1.3
(using lex, ?) test 70.3 81.0 74.1 0.8
Stanford RTE dev 81.1 61.2 69.7 0.5
test 82.7 61.2 70.3 0.3
Stanford RTE dev 81.1 75.8 78.4 ?
(punct. corr.) test 82.7 75.8 79.1 ?
MANLI dev 83.4 85.5 84.4 21.7
(this work) test 85.4 85.3 85.3 21.3
Table 1: Performance of various aligners on the MSR
RTE2 alignment data. The columns show the data set
used (800 problems each); average precision, recall, and
F-measure; and the exact match rate (see text).
5.2 MT aligners: GIZA++ and Cross-EM
Given the importance of alignment for NLI, and the
availability of standard, proven tools for MT align-
ment, an obvious question presents itself: why not
use an off-the-shelf MT aligner for NLI? Although
we have argued (section 2) that this is unlikely to
succeed, to our knowledge, we are the first to inves-
tigate the matter empirically.11
The best-known MT aligner is undoubtedly
GIZA++ (Och and Ney, 2003), which contains im-
plementations of various IBM models (Brown et al,
1993), as well as the HMM model of Vogel et al
(1996). Most practitioners use GIZA++ as a black
box, via the Moses MT toolkit (Koehn et al, 2007).
We followed this practice, running with Moses? de-
fault parameters on the RTE2 data to obtain asym-
metric word alignments in both directions (P -to-H
and H-to-P ). We then performed symmetrization
using the well-known INTERSECTION heuristic.
Unsurprisingly, the out-of-the-box performance
was quite poor, with most words aligned apparently
at random. Precision was fair (72%) but recall was
very poor (46%). Even equal words were usually not
aligned?because GIZA++ is designed for cross-
linguistic use, it does not consider word equality be-
tween source and target sentences. To remedy this,
we supplied GIZA++ with a lexicon, using a trick
11However, Dolan et al (2004) explore a closely-related
topic: using an MT aligner to identify paraphrases.
807
common in MT: we supplemented the training data
with synthetic data consisting of matched pairs of
equal words. This gives GIZA++ a better chance
of learning that, e.g., man should align with man.
The result was a big boost in recall (+23%), and a
smaller gain in precision. The results for GIZA++
shown in table 1 are based on using the lexicon and
INTERSECTION. With these settings, GIZA++ prop-
erly aligned most pairs of equal words, but contin-
ued to align other words apparently at random.
Next, we compared the performance of INTER-
SECTION with other symmetrization heuristics de-
fined in Moses?including UNION, GROW, GROW-
DIAG, GROW-DIAG-FINAL (the default), and GROW-
DIAG-FINAL-AND?and with asymmetric align-
ments in both directions. While all these alterna-
tives achieved better recall than INTERSECTION, all
showed substantially worse precision and F1. On
the RTE2 test set, the asymmetric alignment from
H to P scored 68% in F1; GROW scored 58%; and
all other alternatives scored below 52%.
As an additional experiment, we tested the Cross-
EM aligner (Liang et al, 2006) from the Berke-
leyAligner package on the MSR data. While this
aligner is in many ways simpler than GIZA++ (it
lacks any model of fertility, for example), its method
of jointly training two simple asymmetric HMM
models has outperformed GIZA++ on standard eval-
uations of MT alignment. As with GIZA++, we ex-
perimented with a variety of symmetrization heuris-
tics, and ran trials with and without a supplemental
lexicon. The results were broadly similar: INTER-
SECTION greatly outperformed alternative heuris-
tics, and using a lexicon provided a big boost (up
to 12% in F1). Under optimal settings, the Cross-
EM aligner showed better recall and worse preci-
sion than GIZA++, with F1 just slightly lower. Like
GIZA++, it did well at aligning equal words, but
aligned most other words at random.
The mediocre performance of MT aligners on
NLI alignment comes as no surprise, for reasons dis-
cussed in section 2. Above all, the quantity of train-
ing data is simply too small for unsupervised learn-
ing to succeed. A successful NLI aligner will need
to exploit supervised training data, and will need ac-
cess to additional sources of knowledge about lexi-
cal relatedness.
5.3 The Stanford RTE aligner
A better comparison is thus to an alignment sys-
tem expressly designed for NLI. For this purpose,
we used the alignment component of the Stanford
RTE system (Chambers et al, 2007). The Stanford
aligner performs decoding and learning in a simi-
lar fashion to MANLI, but uses a simpler, token-
based alignment representation, along with a richer
set of features for alignment scoring. It represents
alignments as an injective map from H tokens to
P tokens. Phrase alignments are not directly repre-
sentable, although the effect can be approximated by
a pre-processing step which collapses multi-token
named entities and certain collocations into single
tokens. The features used for alignment scoring in-
clude not only measures of lexical similarity, but
also syntactic features intended to promote the align-
ment of similar predicate-argument structures.
Despite this sophistication, the out-of-the-box
performance of the Stanford aligner is mediocre, as
shown in table 1. The low recall figures are partic-
ularly noteworthy. However, a partial explanation
is readily available: by design, the Stanford system
ignores punctuation.12 Because punctuation tokens
constitute about 15% of the aligned pairs in the MSR
data, this sharply reduces measured recall. However,
since punctuation matters little in inference, such re-
call errors probably should be forgiven. Thus, ta-
ble 1 also shows adjusted statistics for the Stanford
system in which all recall errors involving punctua-
tion are (generously) ignored.
Even after this adjustment, the recall figures are
unimpressive. Error analysis reveals that the Stan-
ford aligner does a poor job of aligning function
words. About 13% of the aligned pairs in the MSR
data are matching prepositions or articles; the Stan-
ford aligner misses about 67% of such pairs. (By
contrast, MANLI misses only 10% of such pairs.)
While function words matter less in inference than
nouns and verbs, they are not irrelevant, and because
sentences often contain multiple instances of a par-
ticular function word, matching them properly is by
no means trivial. If matching prepositions and ar-
ticles were ignored (in addition to punctuation), the
gap in F1 between the MANLI and Stanford systems
12In fact, it operates on a dependency-graph representation
from which punctuation is omitted.
808
would narrow to about 2.8%.
Finally, the Stanford aligner is handicapped by its
token-based alignment representation, often failing
(partly or completely) to align multi-word phrases
such as peace activists with protesters, or hackers
with non-authorized personnel.
5.4 The MANLI aligner
As table 1 indicates, the MANLI aligner was found
to outperform all other aligners evaluated on ev-
ery measure of performance, achieving an F1 score
10.5% higher than GIZA++ and 6.2% higher than
the Stanford aligner (even with the punctuation cor-
rection).13 MANLI achieved a good balance be-
tween precision and recall, and matched more than
20% of the gold-standard alignments exactly.
Three factors seem to have contributed most to
MANLI?s success. First, MANLI is able to outper-
form the MT aligners principally because it is able
to leverage lexical resources to identify the similar-
ity between pairs of words such as jail and prison,
prevent and stop, or injured and wounded. Second,
MANLI?s contextual features enable it to do bet-
ter than the Stanford aligner at matching function
words, a weakness of the Stanford aligner discussed
in section 5.3. Third, MANLI gains a marginal ad-
vantage because its phrase-based representation of
alignment permits it to properly align phrase pairs
such as death penalty and capital punishment, or ab-
dicate and give up.
However, the phrase-based representation con-
tributed far less than we had hoped. Setting
MANLI?s maximum phrase size to 1 (effectively,
restricting it to token-based alignments) caused F1
to fall by just 0.2%. We do not interpret this to
mean that phrase alignments are not useful?indeed,
about 2.6% of the links in the gold-standard data in-
volve phrases of size > 1. Rather, we think it shows
that we have failed to fully exploit the advantages
of the phrase-based representation, chiefly because
we lack lexical resources providing good informa-
tion on similarity of multi-word phrases.
Error analysis suggests that there is ample room
for improvement. A large proportion of recall errors
(perhaps 40%) occur because the lexical similarity
function assigns too low a value to pairs of words
13Reported results for MANLI are averages over 10 runs.
or phrases which are clearly similar, such as con-
servation and protecting, server and computer net-
works, organization and agencies, or bone fragility
and osteoporosis. Better exploitation of lexical re-
sources could help to reduce such errors. Another
important category of recall errors (about 12%) re-
sult from the failure to identify one- and multi-word
versions of the name of some entity, such as Lennon
and John Lennon, or Nike Inc. and Nike. A special-
purpose similarity function could help here. Note,
however, that about 10% of recall errors are un-
avoidable, given our choice of alignment represen-
tation, since they involve cases where the gold stan-
dard aligns one or more tokens on one side to a non-
contiguous set of tokens on the other side.
Precision errors may be harder to reduce. These
errors are dominated by cases where we mistakenly
align two equal function words (49% of precision er-
rors), two forms of the verb to be (21%), two equal
punctuation marks (7%), or two words or phrases
of other types having equal lemmas (18%). Be-
cause such errors often occur because the aligner
is forced to choose between nearly equivalent alter-
natives, they may be difficult to eliminate. The re-
maining 5% of precision errors result mostly from
aligning words or phrases rightly judged to be highly
similar, such as expanding and increasing, labor and
birth, figures and number, or 223,000 and 220,000.
6 Using alignment to predict RTE answers
In section 5, we evaluated the ability of aligners to
recover gold-standard alignments. But since align-
ment is just one component of the NLI problem, we
might also examine the impact of different align-
ers on the ability to recognize valid inferences. If a
high-scoring alignment indicates a close correspon-
dence between H and P , does this also indicate a
valid inference? We have previously emphasized
(MacCartney et al, 2006) that there is more to infer-
ential validity than close lexical or structural corre-
spondence: negations, modals, non-factive and im-
plicative verbs, and other linguistic constructs can
affect validity in ways hard to capture in alignment.
Nevertheless, alignment score can be a strong pre-
dictor of inferential validity, and some NLI systems
(e.g., (Glickman et al, 2005)) rely entirely on some
measure of alignment quality to predict validity.
809
System data acc % avgP %
Bag-of-words aligner dev 61.3 61.5
test 57.9 58.9
Stanford RTE aligner dev 63.1 64.9
test 60.9 59.2
MANLI aligner dev 59.3 69.0
(this work) test 60.3 61.0
RTE2 entries (average) test 58.5 59.1
LCC (Hickl et al, 2006) test 75.4 80.8
Table 2: Performance of various aligners and complete
RTE systems in predicting RTE2 answers. The columns
show the data set used, accuracy, and average precision
(the recommended metric for RTE2).
If an aligner generates real-valued alignment
scores, we can use the RTE data to test its ability to
predict inferential validity with the following simple
method. For a given RTE problem, we predict YES
(valid) if its alignment score14 exceeds a threshold
? , and NO otherwise. We tune ? to maximize accu-
racy on the RTE2 development set, and then measure
performance on the RTE2 test set using the same ? .
Table 2 shows results for several NLI aligners,
along with some results for complete RTE systems,
including the LCC system (the top performer at
RTE2) and an average of all systems participating in
RTE2. While none of the aligners rivals the perfor-
mance of the LCC system, all achieve respectable
results, and the Stanford and MANLI aligners out-
perform the average RTE2 entry. Thus, even if align-
ment quality does not determine inferential validity,
many NLI systems could be improved by harnessing
a well-designed NLI aligner.
7 Related work
Given the extensive literature on phrase-based MT,
it may be helpful further to situate our phrase-based
alignment model in relation to past work. The stan-
dard approach to training a phrase-based MT system
is to apply phrase extraction heuristics using word-
aligned training sets (Och and Ney, 2003; Koehn
et al, 2007). Unfortunately, word alignment mod-
els assume that source words are individually trans-
14For good results, it may be necessary to normalize the
alignment score. Scores from MANLI were normalized by the
number of tokens in the problem. The Stanford aligner performs
a similar normalization internally.
lated into target words, which stands at odds with
the key assumption in phrase-based systems that
many translations are non-compositional. More re-
cently, several works (Marcu and Wong, 2002; De-
Nero et al, 2006; Birch et al, 2006; DeNero and
Klein, 2008) have presented more unified phrase-
based systems that jointly align and weight phrases,
though these systems have not come close to the
state of the art when evaluated in terms of MT per-
formance.
We would argue that previous work in MT phrase
alignment is orthogonal to our work. In MANLI,
the need for phrases arises when word-based rep-
resentations are not appropriate for alignment (e.g.,
between close down and terminate), though longer
phrases are not needed to achieve good alignment
quality. In MT phrase alignment, it is beneficial to
account for arbitrarily large phrases, since the larger
contexts offered by these phrases can help realize
more dependencies among translated words (e.g.,
word order, agreement, subcategorization). Per-
haps because MT phrase alignment is dealing with
much larger contexts, no existing work in MT phrase
alignment (to our knowledge) directly models word
insertions and deletions, as in MANLI. For exam-
ple, in figure 1, MANLI can just skip In most Pacific
countries there, while an MT phrase-based model
would presumably align In most Pacific countries
there are to Women are. Hence, previous work is
of limited applicability to our problem.
8 Conclusion
While MT aligners succeed by unsupervised learn-
ing of word correspondences from massive amounts
of bitext, NLI aligners are forced to rely on smaller
quantities of supervised training data. With the
MANLI system, we have demonstrated how to over-
come this lack of data by utilizing external lexical
resources, and how to gain additional power from a
phrase-based representation of alignment.
Acknowledgements The authors wish to thank the
anonymous reviewers for their helpful comments on
an earlier draft of this paper. This paper is based
on work funded in part by the Defense Advanced
Research Projects Agency through IBM and in part
by the CIA ATP as part of the OCCAM project.
810
References
R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Giampic-
colo, B. Magnini, and I. Szpektor. 2006. The Sec-
ond PASCAL Recognising Textual Entailment Chal-
lenge. In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognising Textual Entailment.
R. Bar-Haim, I. Dagan, I. Greental, and E. Shnarch. 2007.
Semantic Inference at the Lexical-Syntactic Level. In
Proceedings of AAAI-07.
A. Birch, C. Callison-Burch, M. Osborne, and P. Koehn.
2006. Constraining the Phrase-Based, Joint Probabil-
ity Statistical Translation Model. In Proceedings of the
ACL-06 Workshop on Statistical Machine Translation.
C. Brockett. 2007. Aligning the RTE 2006 Corpus. Tech-
nical Report MSR-TR-2007-77, Microsoft Research.
P. F. Brown, S. D. Pietra, V. J. D. Pietra, and R. L. Mercer.
1993. The Mathematics of Statistical Machine Trans-
lation: Parameter Estimation. Computational Linguis-
tics, 19(2):263?311.
N. Chambers, D. Cer, T. Grenager, D. Hall, C. Kid-
don, B. MacCartney, M. C. de Marneffe, D. Ramage,
E. Yeh, and C. D. Manning. 2007. Learning Align-
ments and Leveraging Natural Logic. In Proceedings
of the ACL-07 Workshop on Textual Entailment and
Paraphrasing.
M. Collins. 2002. Discriminative training methods for
hidden Markov models. In Proceedings of EMNLP-
02.
I. Dagan, O. Glickman, and B. Magnini. 2005. The PAS-
CAL Recognising Textual Entailment Challenge. In
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment.
J. DeNero and D. Klein. 2008. The Complexity of Phrase
Alignment Problems. In Proceedings of ACL/HLT-08:
Short Papers, pages 25?28.
J. DeNero, D. Gillick, J. Zhang, and D. Klein. 2006.
Why Generative Phrase Models Underperform Surface
Heuristics. In Proceedings of the ACL-06 Workshop on
Statistical Machine Translation, pages 31?38.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsupervised
construction of large paraphrase corpora. In Proceed-
ings of COLING-04.
A. Fraser and D. Marcu. 2007. Measuring Word
Alignment Quality for Statistical Machine Translation.
Computational Linguistics, 33(3):293?303.
O. Glickman, I. Dagan, and M. Koppel. 2005. Web
based probabilistic textual entailment. In Proceedings
of the PASCAL Challenges Workshop on Recognizing
Textual Entailment.
A. Hickl and J. Bensley. 2007. A Discourse
Commitment-Based Framework for Recognizing Tex-
tual Entailment. In ACL-07 Workshop on Textual En-
tailment and Paraphrasing, Prague.
A. Hickl, J. Williams, J. Bensley, K. Roberts, B. Rink,
and Y. Shi. 2006. Recognizing Textual Entailment
with LCC?s GROUNDHOG System. In Proceedings
of the Second PASCAL Challenges Workshop on Rec-
ognizing Textual Entailment.
J. J. Jiang and D. W. Conrath. 1997. Semantic simi-
larity based on corpus statistics and lexical taxonomy.
In Proceedings of the International Conference on Re-
search in Computational Linguistics.
V. Jijkoun and M. de Rijke. 2005. Recognizing tex-
tual entailment using lexical similarity. In Proceedings
of the PASCAL Challenges Workshop on Recognizing
Textual Entailment, pages 73?76.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL-07, demonstration session.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
Agreement. In Proceedings of NAACL-06, New York.
D. Lin. 1998. Automatic retrieval and clustering of simi-
lar words. In Proceedings of COLING/ACL-98, pages
768?774, Montreal, Canada.
B. MacCartney, T. Grenager, M. C. de Marneffe, D. Cer,
and C. D. Manning. 2006. Learning to Recognize
Features of Valid Textual Entailments. In Proceedings
of NAACL-06, New York.
D. Marcu and W. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Proceedings of EMNLP-02, pages 133?139.
E. Marsi and E. Krahmer. 2005. Classification of se-
mantic relations by humans and machines. In ACL-05
Workshop on Empirical Modeling of Semantic Equiv-
alence and Entailment, Ann Arbor.
F. J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
M. Tatu and D. Moldovan. 2007. COGEX at RTE3. In
Proceedings of ACL-07.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
Proceedings of COLING-96, pages 836?841, Copen-
hagen, Denmark.
811
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 41?48,
New York, June 2006. c?2006 Association for Computational Linguistics
Learning to recognize features of valid textual entailments
Bill MacCartney, Trond Grenager, Marie-Catherine de Marneffe,
Daniel Cer, and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{wcmac, grenager, mcdm, cerd, manning}@cs.stanford.edu
Abstract
This paper advocates a new architecture for tex-
tual inference in which finding a good alignment is
separated from evaluating entailment. Current ap-
proaches to semantic inference in question answer-
ing and textual entailment have approximated the
entailment problem as that of computing the best
alignment of the hypothesis to the text, using a lo-
cally decomposable matching score. We argue that
there are significant weaknesses in this approach,
including flawed assumptions of monotonicity and
locality. Instead we propose a pipelined approach
where alignment is followed by a classification
step, in which we extract features representing
high-level characteristics of the entailment prob-
lem, and pass the resulting feature vector to a statis-
tical classifier trained on development data. We re-
port results on data from the 2005 Pascal RTE Chal-
lenge which surpass previously reported results for
alignment-based systems.
1 Introduction
During the last five years there has been a surge in
work which aims to provide robust textual inference
in arbitrary domains about which the system has no
expertise. The best-known such work has occurred
within the field of question answering (Pasca and
Harabagiu, 2001; Moldovan et al, 2003); more re-
cently, such work has continued with greater focus
in addressing the PASCAL Recognizing Textual En-
tailment (RTE) Challenge (Dagan et al, 2005) and
within the U.S. Government AQUAINT program.
Substantive progress on this task is key to many
text and natural language applications. If one could
tell that Protestors chanted slogans opposing a free
trade agreement was a match for people demonstrat-
ing against free trade, then one could offer a form of
semantic search not available with current keyword-
based search. Even greater benefits would flow to
richer and more semantically complex NLP tasks.
Because full, accurate, open-domain natural lan-
guage understanding lies far beyond current capa-
bilities, nearly all efforts in this area have sought
to extract the maximum mileage from quite lim-
ited semantic representations. Some have used sim-
ple measures of semantic overlap, but the more in-
teresting work has largely converged on a graph-
alignment approach, operating on semantic graphs
derived from syntactic dependency parses, and using
a locally-decomposable alignment score as a proxy
for strength of entailment. (Below, we argue that
even approaches relying on weighted abduction may
be seen in this light.) In this paper, we highlight the
fundamental semantic limitations of this type of ap-
proach, and advocate a multi-stage architecture that
addresses these limitations. The three key limita-
tions are an assumption of monotonicity, an assump-
tion of locality, and a confounding of alignment and
evaluation of entailment.
We focus on the PASCAL RTE data, examples
from which are shown in table 1. This data set con-
tains pairs consisting of a short text followed by a
one-sentence hypothesis. The goal is to say whether
the hypothesis follows from the text and general
background knowledge, according to the intuitions
of an intelligent human reader. That is, the standard
is not whether the hypothesis is logically entailed,
but whether it can reasonably be inferred.
2 Approaching a robust semantics
In this section we try to give a unifying overview
to current work on robust textual inference, to
present fundamental limitations of current meth-
ods, and then to outline our approach to resolving
them. Nearly all current textual inference systems
use a single-stage matching/proof process, and differ
41
ID Text Hypothesis Entailed
59 Two Turkish engineers and an Afghan translator kidnapped
in December were freed Friday.
translator kidnapped in Iraq no
98 Sharon warns Arafat could be targeted for assassination. prime minister targeted for assassination no
152 Twenty-five of the dead were members of the law enforce-
ment agencies and the rest of the 67 were civilians.
25 of the dead were civilians. no
231 The memorandum noted the United Nations estimated that
2.5 million to 3.5 million people died of AIDS last year.
Over 2 million people died of AIDS last
year.
yes
971 Mitsubishi Motors Corp.?s new vehicle sales in the US fell
46 percent in June.
Mitsubishi sales rose 46 percent. no
1806 Vanunu, 49, was abducted by Israeli agents and convicted
of treason in 1986 after discussing his work as a mid-level
Dimona technician with Britain?s Sunday Times newspaper.
Vanunu?s disclosures in 1968 led experts
to conclude that Israel has a stockpile of
nuclear warheads.
no
2081 The main race track in Qatar is located in Shahaniya, on the
Dukhan Road.
Qatar is located in Shahaniya. no
Table 1: Illustrative examples from the PASCAL RTE data set, available at http://www.pascal-network.org/Challenges/RTE.
Though most problems shown have answer no, the data set is actually balanced between yes and no.
mainly in the sophistication of the matching stage.
The simplest approach is to base the entailment pre-
diction on the degree of semantic overlap between
the text and hypothesis using models based on bags
of words, bags of n-grams, TF-IDF scores, or some-
thing similar (Jijkoun and de Rijke, 2005). Such
models have serious limitations: semantic overlap is
typically a symmetric relation, whereas entailment
is clearly not, and, because overlap models do not
account for syntactic or semantic structure, they are
easily fooled by examples like ID 2081.
A more structured approach is to formulate the
entailment prediction as a graph matching problem
(Haghighi et al, 2005; de Salvo Braz et al, 2005).
In this formulation, sentences are represented as nor-
malized syntactic dependency graphs (like the one
shown in figure 1) and entailment is approximated
with an alignment between the graph representing
the hypothesis and a portion of the corresponding
graph(s) representing the text. Each possible align-
ment of the graphs has an associated score, and the
score of the best alignment is used as an approxi-
mation to the strength of the entailment: a better-
aligned hypothesis is assumed to be more likely to
be entailed. To enable incremental search, align-
ment scores are usually factored as a combination
of local terms, corresponding to the nodes and edges
of the two graphs. Unfortunately, even with factored
scores the problem of finding the best alignment of
two graphs is NP-complete, so exact computation is
intractable. Authors have proposed a variety of ap-
proximate search techniques. Haghighi et al (2005)
divide the search into two steps: in the first step they
consider node scores only, which relaxes the prob-
lem to a weighted bipartite graph matching that can
be solved in polynomial time, and in the second step
they add the edges scores and hillclimb the align-
ment via an approximate local search.
A third approach, exemplified by Moldovan et al
(2003) and Raina et al (2005), is to translate de-
pendency parses into neo-Davidsonian-style quasi-
logical forms, and to perform weighted abductive
theorem proving in the tradition of (Hobbs et al,
1988). Unless supplemented with a knowledge
base, this approach is actually isomorphic to the
graph matching approach. For example, the graph
in figure 1 might generate the quasi-LF rose(e1),
nsubj(e1, x1), sales(x1), nn(x1, x2), Mitsubishi(x2),
dobj(e1, x3), percent(x3), num(x3, x4), 46(x4).
There is a term corresponding to each node and arc,
and the resolution steps at the core of weighted ab-
duction theorem proving consider matching an indi-
vidual node of the hypothesis (e.g. rose(e1)) with
something from the text (e.g. fell(e1)), just as in
the graph-matching approach. The two models be-
come distinct when there is a good supply of addi-
tional linguistic and world knowledge axioms?as in
Moldovan et al (2003) but not Raina et al (2005).
Then the theorem prover may generate intermedi-
ate forms in the proof, but, nevertheless, individ-
ual terms are resolved locally without reference to
global context.
Finally, a few efforts (Akhmatova, 2005; Fowler
et al, 2005; Bos and Markert, 2005) have tried to
42
translate sentences into formulas of first-order logic,
in order to test logical entailment with a theorem
prover. While in principle this approach does not
suffer from the limitations we describe below, in
practice it has not borne much fruit. Because few
problem sentences can be accurately translated to
logical form, and because logical entailment is a
strict standard, recall tends to be poor.
The simple graph matching formulation of the
problem belies three important issues. First, the
above systems assume a form of upward monotonic-
ity: if a good match is found with a part of the text,
other material in the text is assumed not to affect
the validity of the match. But many situations lack
this upward monotone character. Consider variants
on ID 98. Suppose the hypothesis were Arafat tar-
geted for assassination. This would allow a perfect
graph match or zero-cost weighted abductive proof,
because the hypothesis is a subgraph of the text.
However, this would be incorrect because it ignores
the modal operator could. Information that changes
the validity of a proof can also exist outside a match-
ing clause. Consider the alternate text Sharon denies
Arafat is targeted for assassination.1
The second issue is the assumption of locality.
Locality is needed to allow practical search, but
many entailment decisions rely on global features of
the alignment, and thus do not naturally factor by
nodes and edges. To take just one example, drop-
ping a restrictive modifier preserves entailment in a
positive context, but not in a negative one. For exam-
ple, Dogs barked loudly entails Dogs barked, but No
dogs barked loudly does not entail No dogs barked.
These more global phenomena cannot be modeled
with a factored alignment score.
The last issue arising in the graph matching ap-
proaches is the inherent confounding of alignment
and entailment determination. The way to show that
one graph element does not follow from another is
to make the cost of aligning them high. However,
since we are embedded in a search for the lowest
cost alignment, this will just cause the system to
choose an alternate alignment rather than recogniz-
ing a non-entailment. In ID 152, we would like the
hypothesis to align with the first part of the text, to
1This is the same problem labeled and addressed as context
in Tatu and Moldovan (2005).
be able to prove that civilians are not members of
law enforcement agencies and conclude that the hy-
pothesis does not follow from the text. But a graph-
matching system will to try to get non-entailment
by making the matching cost between civilians and
members of law enforcement agencies be very high.
However, the likely result of that is that the final part
of the hypothesis will align with were civilians at
the end of the text, assuming that we allow an align-
ment with ?loose? arc correspondence.2 Under this
candidate alignment, the lexical alignments are per-
fect, and the only imperfect alignment is the subject
arc of were is mismatched in the two. A robust in-
ference guesser will still likely conclude that there is
entailment.
We propose that all three problems can be re-
solved in a two-stage architecture, where the align-
ment phase is followed by a separate phase of en-
tailment determination. Although developed inde-
pendently, the same division between alignment and
classification has also been proposed by Marsi and
Krahmer (2005), whose textual system is developed
and evaluated on parallel translations into Dutch.
Their classification phase features an output space
of five semantic relations, and performs well at dis-
tinguishing entailing sentence pairs.
Finding aligned content can be done by any search
procedure. Compared to previous work, we empha-
size structural alignment, and seek to ignore issues
like polarity and quantity, which can be left to a
subsequent entailment decision. For example, the
scoring function is designed to encourage antonym
matches, and ignore the negation of verb predicates.
The ideas clearly generalize to evaluating several
alignments, but we have so far worked with just
the one-best alignment. Given a good alignment,
the determination of entailment reduces to a simple
classification decision. The classifier is built over
features designed to recognize patterns of valid and
invalid inference. Weights for the features can be
hand-set or chosen to minimize a relevant loss func-
tion on training data using standard techniques from
machine learning. Because we already have a com-
plete alignment, the classifier?s decision can be con-
2Robust systems need to allow matches with imperfect arc
correspondence. For instance, given Bill went to Lyons to study
French farming practices, we would like to be able to conclude
that Bill studied French farming despite the structural mismatch.
43
ditioned on arbitrary global features of the aligned
graphs, and it can detect failures of monotonicity.
3 System
Our system has three stages: linguistic analysis,
alignment, and entailment determination.
3.1 Linguistic analysis
Our goal in this stage is to compute linguistic rep-
resentations of the text and hypothesis that contain
as much information as possible about their seman-
tic content. We use typed dependency graphs, which
contain a node for each word and labeled edges rep-
resenting the grammatical relations between words.
Figure 1 gives the typed dependency graph for ID
971. This representation contains much of the infor-
mation about words and relations between them, and
is relatively easy to compute from a syntactic parse.
However many semantic phenomena are not repre-
sented properly; particularly egregious is the inabil-
ity to represent quantification and modality.
We parse input sentences to phrase structure
trees using the Stanford parser (Klein and Manning,
2003), a statistical syntactic parser trained on the
Penn TreeBank. To ensure correct parsing, we pre-
process the sentences to collapse named entities into
new dedicated tokens. Named entities are identi-
fied by a CRF-based NER system, similar to that
described in (McCallum and Li, 2003). After pars-
ing, contiguous collocations which appear in Word-
Net (Fellbaum, 1998) are identified and grouped.
We convert the phrase structure trees to typed de-
pendency graphs using a set of deterministic hand-
coded rules (de Marneffe et al, 2006). In these rules,
heads of constituents are first identified using a mod-
ified version of the Collins head rules that favor se-
mantic heads (such as lexical verbs rather than aux-
iliaries), and dependents of heads are typed using
tregex patterns (Levy and Andrew, 2006), an exten-
sion of the tgrep pattern language. The nodes in the
final graph are then annotated with their associated
word, part-of-speech (given by the parser), lemma
(given by a finite-state transducer described by Min-
nen et al (2001)) and named-entity tag.
3.2 Alignment
The purpose of the second phase is to find a good
partial alignment between the typed dependency
graphs representing the hypothesis and the text. An
alignment consists of a mapping from each node
(word) in the hypothesis graph to a single node in
the text graph, or to null.3 Figure 1 gives the align-
ment for ID 971.
The space of alignments is large: there are
O((m + 1)n) possible alignments for a hypothesis
graph with n nodes and a text graph with m nodes.
We define a measure of alignment quality, and a
procedure for identifying high scoring alignments.
We choose a locally decomposable scoring function,
such that the score of an alignment is the sum of
the local node and edge alignment scores. Unfor-
tunately, there is no polynomial time algorithm for
finding the exact best alignment. Instead we use an
incremental beam search, combined with a node or-
dering heuristic, to do approximate global search in
the space of possible alignments. We have exper-
imented with several alternative search techniques,
and found that the solution quality is not very sensi-
tive to the specific search procedure used.
Our scoring measure is designed to favor align-
ments which align semantically similar subgraphs,
irrespective of polarity. For this reason, nodes re-
ceive high alignment scores when the words they
represent are semantically similar. Synonyms and
antonyms receive the highest score, and unrelated
words receive the lowest. Our hand-crafted scor-
ing metric takes into account the word, the lemma,
and the part of speech, and searches for word relat-
edness using a range of external resources, includ-
ing WordNet, precomputed latent semantic analysis
matrices, and special-purpose gazettes. Alignment
scores also incorporate local edge scores, which are
based on the shape of the paths between nodes in
the text graph which correspond to adjacent nodes
in the hypothesis graph. Preserved edges receive the
highest score, and longer paths receive lower scores.
3.3 Entailment determination
In the final stage of processing, we make a deci-
sion about whether or not the hypothesis is entailed
by the text, conditioned on the typed dependency
graphs, as well as the best alignment between them.
3The limitations of using one-to-one alignments are miti-
gated by the fact that many multiword expressions (e.g. named
entities, noun compounds, multiword prepositions) have been
collapsed into single nodes during linguistic analysis.
44
rose
sales
Mitsubishi
percent
46
nsubj dobj
nn num
Alignment
rose ? fell
sales ? sales
Mitsubishi ? Mitsubishi Motors Corp.
percent ? percent
46 ? 46
Alignment score: ?0.8962
Features
Antonyms aligned in pos/pos context ?
Structure: main predicate good match +
Number: quantity match +
Date: text date deleted in hypothesis ?
Alignment: good score +
Entailment score: ?5.4262
Figure 1: Problem representation for ID 971: typed dependency graph (hypothesis only), alignment, and entailment features.
Because we have a data set of examples that are la-
beled for entailment, we can use techniques from su-
pervised machine learning to learn a classifier. We
adopt the standard approach of defining a featural
representation of the problem and then learning a
linear decision boundary in the feature space. We
focus here on the learning methodology; the next
section covers the definition of the set of features.
Defined in this way, one can apply any statistical
learning algorithm to this classification task, such
as support vector machines, logistic regression, or
naive Bayes. We used a logistic regression classifier
with a Gaussian prior parameter for regularization.
We also compare our learning results with those
achieved by hand-setting the weight parameters for
the classifier, effectively incorporating strong prior
(human) knowledge into the choice of weights.
An advantage to the use of statistical classifiers
is that they can be configured to output a proba-
bility distribution over possible answers rather than
just the most likely answer. This allows us to get
confidence estimates for computing a confidence
weighted score (see section 5). A major concern in
applying machine learning techniques to this clas-
sification problem is the relatively small size of the
training set, which can lead to overfitting problems.
We address this by keeping the feature dimensional-
ity small, and using high regularization penalties in
training.
4 Feature representation
In the entailment determination phase, the entail-
ment problem is reduced to a representation as a
vector of 28 features, over which the statistical
classifier described above operates. These features
try to capture salient patterns of entailment and
non-entailment, with particular attention to contexts
which reverse or block monotonicity, such as nega-
tions and quantifiers. This section describes the most
important groups of features.
Polarity features. These features capture the pres-
ence (or absence) of linguistic markers of negative
polarity contexts in both the text and the hypothesis,
such as simple negation (not), downward-monotone
quantifiers (no, few), restricting prepositions (with-
out, except) and superlatives (tallest).
Adjunct features. These indicate the dropping or
adding of syntactic adjuncts when moving from the
text to the hypothesis. For the common case of
restrictive adjuncts, dropping an adjunct preserves
truth (Dogs barked loudly |= Dogs barked), while
adding an adjunct does not (Dogs barked 6|= Dogs
barked today). However, in negative-polarity con-
texts (such as No dogs barked), this heuristic is
reversed: adjuncts can safely be added, but not
dropped. For example, in ID 59, the hypothesis
aligns well with the text, but the addition of in Iraq
indicates non-entailment.
We identify the ?root nodes? of the problem: the
root node of the hypothesis graph and the corre-
sponding aligned node in the text graph. Using de-
pendency information, we identify whether adjuncts
have been added or dropped. We then determine
the polarity (negative context, positive context or
restrictor of a universal quantifier) of the two root
nodes to generate features accordingly.
Antonymy features. Entailment problems might
involve antonymy, as in ID 971. We check whether
an aligned pairs of text/hypothesis words appear to
be antonymous by consulting a pre-computed list
of about 40,000 antonymous and other contrasting
pairs derived from WordNet. For each antonymous
pair, we generate one of three boolean features, in-
dicating whether (i) the words appear in contexts of
matching polarity, (ii) only the text word appears in
a negative-polarity context, or (iii) only the hypoth-
esis word does.
45
Modality features. Modality features capture
simple patterns of modal reasoning, as in ID 98,
which illustrates the heuristic that possibility does
not entail actuality. According to the occurrence
(or not) of predefined modality markers, such as
must or maybe, we map the text and the hypoth-
esis to one of six modalities: possible, not possi-
ble, actual, not actual, necessary, and not necessary.
The text/hypothesis modality pair is then mapped
into one of the following entailment judgments: yes,
weak yes, don?t know, weak no, or no. For example:
(not possible |= not actual)? ? yes
(possible |= necessary)? ? weak no
Factivity features. The context in which a verb
phrase is embedded may carry semantic presuppo-
sitions giving rise to (non-)entailments such as The
gangster tried to escape 6|= The gangster escaped.
This pattern of entailment, like others, can be re-
versed by negative polarity markers (The gangster
managed to escape |= The gangster escaped while
The gangster didn?t manage to escape 6|= The gang-
ster escaped). To capture these phenomena, we
compiled small lists of ?factive? and non-factive
verbs, clustered according to the kinds of entail-
ments they create. We then determine to which class
the parent of the text aligned with the hypothesis
root belongs to. If the parent is not in the list, we
only check whether the embedding text is an affir-
mative context or a negative one.
Quantifier features. These features are designed
to capture entailment relations among simple sen-
tences involving quantification, such as Every com-
pany must report |= A company must report (or
The company, or IBM). No attempt is made to han-
dle multiple quantifiers or scope ambiguities. Each
quantifier found in an aligned pair of text/hypothesis
words is mapped into one of five quantifier cate-
gories: no, some, many, most, and all. The no
category is set apart, while an ordering over the
other four categories is defined. The some category
also includes definite and indefinite determiners and
small cardinal numbers. A crude attempt is made to
handle negation by interchanging no and all in the
presence of negation. Features are generated given
the categories of both hypothesis and text.
Number, date, and time features. These are de-
signed to recognize (mis-)matches between num-
bers, dates, and times, as in IDs 1806 and 231. We
do some normalization (e.g. of date representations)
and have a limited ability to do fuzzy matching. In
ID 1806, the mismatched years are correctly iden-
tified. Unfortunately, in ID 231 the significance of
over is not grasped and a mismatch is reported.
Alignment features. Our feature representation
includes three real-valued features intended to rep-
resent the quality of the alignment: score is the
raw score returned from the alignment phase, while
goodscore and badscore try to capture whether the
alignment score is ?good? or ?bad? by computing
the sigmoid function of the distance between the
alignment score and hard-coded ?good? and ?bad?
reference values.
5 Evaluation
We present results based on the First PASCAL RTE
Challenge, which used a development set contain-
ing 567 pairs and a test set containing 800 pairs.
The data sets are balanced to contain equal num-
bers of yes and no answers. The RTE Challenge
recommended two evaluation metrics: raw accuracy
and confidence weighted score (CWS). The CWS is
computed as follows: for each positive integer k up
to the size of the test set, we compute accuracy over
the k most confident predictions. The CWS is then
the average, over k, of these partial accuracies. Like
raw accuracy, it lies in the interval [0, 1], but it will
exceed raw accuracy to the degree that predictions
are well-calibrated.
Several characteristics of the RTE problems
should be emphasized. Examples are derived from a
broad variety of sources, including newswire; there-
fore systems must be domain-independent. The in-
ferences required are, from a human perspective,
fairly superficial: no long chains of reasoning are
involved. However, there are ?trick? questions ex-
pressly designed to foil simplistic techniques. The
definition of entailment is informal and approx-
imate: whether a competent speaker with basic
knowledge of the world would typically infer the hy-
pothesis from the text. Entailments will certainly de-
pend on linguistic knowledge, and may also depend
on world knowledge; however, the scope of required
46
Algorithm RTE1 Dev Set RTE1 Test Set
Acc CWS Acc CWS
Random 50.0% 50.0% 50.0% 50.0%
Jijkoun et al 05 61.0% 64.9% 55.3% 55.9%
Raina et al 05 57.8% 66.1% 55.5% 63.8%
Haghighi et al 05 ? ? 56.8% 61.4%
Bos & Markert 05 ? ? 57.7% 63.2%
Alignment only 58.7% 59.1% 54.5% 59.7%
Hand-tuned 60.3% 65.3% 59.1% 65.0%
Learning 61.2% 74.4% 59.1% 63.9%
Table 2: Performance on the RTE development and test sets.
CWS stands for confidence weighted score (see text).
world knowledge is left unspecified.4
Despite the informality of the problem definition,
human judges exhibit very good agreement on the
RTE task, with agreement rate of 91?96% (Dagan
et al, 2005). In principle, then, the upper bound
for machine performance is quite high. In practice,
however, the RTE task is exceedingly difficult for
computers. Participants in the first PASCAL RTE
workshop reported accuracy from 49% to 59%, and
CWS from 50.0% to 69.0% (Dagan et al, 2005).
Table 2 shows results for a range of systems and
testing conditions. We report accuracy and CWS on
each RTE data set. The baseline for all experiments
is random guessing, which always attains 50% accu-
racy. We show comparable results from recent sys-
tems based on lexical similarity (Jijkoun and de Ri-
jke, 2005), graph alignment (Haghighi et al, 2005),
weighted abduction (Raina et al, 2005), and a mixed
system including theorem proving (Bos and Mark-
ert, 2005).
We then show results for our system under several
different training regimes. The row labeled ?align-
ment only? describes experiments in which all fea-
tures except the alignment score are turned off. We
predict entailment just in case the alignment score
exceeds a threshold which is optimized on devel-
opment data. ?Hand-tuning? describes experiments
in which all features are on, but no training oc-
curs; rather, weights are set by hand, according to
human intuition. Finally, ?learning? describes ex-
periments in which all features are on, and feature
weights are trained on the development data. The
4Each RTE problem is also tagged as belonging to one of
seven tasks. Previous work (Raina et al, 2005) has shown that
conditioning on task can significantly improve accuracy. In this
work, however, we ignore the task variable, and none of the
results shown in table 2 reflect optimization by task.
figures reported for development data performance
therefore reflect overfitting; while such results are
not a fair measure of overall performance, they can
help us assess the adequacy of our feature set: if
our features have failed to capture relevant aspects
of the problem, we should expect poor performance
even when overfitting. It is therefore encouraging
to see CWS above 70%. Finally, the figures re-
ported for test data performance are the fairest ba-
sis for comparison. These are significantly better
than our results for alignment only (Fisher?s exact
test, p < 0.05), indicating that we gain real value
from our features. However, the gain over compara-
ble results from other teams is not significant at the
p < 0.05 level.
A curious observation is that the results for hand-
tuned weights are as good or better than results for
learned weights. A possible explanation runs as fol-
lows. Most of the features represent high-level pat-
terns which arise only occasionally. Because the
training data contains only a few hundred exam-
ples, many features are active in just a handful of
instances; their learned weights are therefore quite
noisy. Indeed, a feature which is expected to fa-
vor entailment may even wind up with a negative
weight: the modal feature weak yes is an example.
As shown in table 3, the learned weight for this fea-
ture was strongly negative ? but this resulted from
a single training example in which the feature was
active but the hypothesis was not entailed. In such
cases, we shouldn?t expect good generalization to
test data, and human intuition about the ?value? of
specific features may be more reliable.
Table 3 shows the values learned for selected fea-
ture weights. As expected, the features added ad-
junct in all context, modal yes, and text is factive
were all found to be strong indicators of entailment,
while date insert, date modifier insert, widening
from text to hyp all indicate lack of entailment. Inter-
estingly, text has neg marker and text & hyp diff po-
larity were also found to disfavor entailment; while
this outcome is sensible, it was not anticipated or
designed.
6 Conclusion
The best current approaches to the problem of tex-
tual inference work by aligning semantic graphs,
47
Feature class & condition weight
Adjunct added adjunct in all context 1.40
Date date mismatch 1.30
Alignment good score 1.10
Modal yes 0.70
Modal no 0.51
Factive text is factive 0.46
. . . . . . . . .
Polarity text & hyp same polarity ?0.45
Modal don?t know ?0.59
Quantifier widening from text to hyp ?0.66
Polarity text has neg marker ?0.66
Polarity text & hyp diff polarity ?0.72
Alignment bad score ?1.53
Date date modifier insert ?1.57
Modal weak yes ?1.92
Date date insert ?2.63
Table 3: Learned weights for selected features. Positive weights
favor entailment. Weights near 0 are omitted. Based on training
on the PASCAL RTE development set.
using a locally-decomposable alignment score as a
proxy for strength of entailment. We have argued
that such models suffer from three crucial limita-
tions: an assumption of monotonicity, an assump-
tion of locality, and a confounding of alignment and
entailment determination.
We have described a system which extends
alignment-based systems while attempting to ad-
dress these limitations. After finding the best align-
ment between text and hypothesis, we extract high-
level semantic features of the entailment problem,
and input these features to a statistical classifier to
make an entailment decision. Using this multi-stage
architecture, we report results on the PASCAL RTE
data which surpass previously-reported results for
alignment-based systems.
We see the present work as a first step in a promis-
ing direction. Much work remains in improving the
entailment features, many of which may be seen as
rough approximations to a formal monotonicity cal-
culus. In future, we aim to combine more precise
modeling of monotonicity effects with better mod-
eling of paraphrase equivalence.
Acknowledgements
We thank Anna Rafferty, Josh Ainslie, and partic-
ularly Roger Grosse for contributions to the ideas
and system reported here. This work was supported
in part by the Advanced Research and Development
Activity (ARDA)?s Advanced Question Answering
for Intelligence (AQUAINT) Program.
References
E. Akhmatova. 2005. Textual entailment resolution via atomic
propositions. In Proceedings of the PASCAL Challenges
Workshop on Recognising Textual Entailment, 2005.
J. Bos and K. Markert. 2005. Recognising textual entailment
with logical inference. In EMNLP-05.
I. Dagan, O. Glickman, and B. Magnini. 2005. The PASCAL
recognising textual entailment challenge. In Proceedings of
the PASCAL Challenges Workshop on Recognising Textual
Entailment.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In LREC 2006.
R. de Salvo Braz, R. Girju, V. Punyakanok, D. Roth, and
M. Sammons. 2005. An inference model for semantic entail-
ment and question-answering. In Proceedings of the Twenti-
eth National Conference on Artificial Intelligence (AAAI).
C. Fellbaum. 1998. WordNet: an electronic lexical database.
MIT Press.
A. Fowler, B. Hauser, D. Hodges, I. Niles, A. Novischi, and
J. Stephan. 2005. Applying COGEX to recognize textual
entailment. In Proceedings of the PASCAL Challenges Work-
shop on Recognising Textual Entailment.
A. Haghighi, A. Ng, and C. D. Manning. 2005. Robust textual
inference via graph matching. In EMNLP-05.
J. R. Hobbs, M. Stickel, P. Martin, and D. D. Edwards. 1988.
Interpretation as abduction. In 26th Annual Meeting of the
Association for Computational Linguistics: Proceedings of
the Conference, pages 95?103, Buffalo, New York.
V. Jijkoun and M. de Rijke. 2005. Recognizing textual entail-
ment using lexical similarity. In Proceedings of the PAS-
CAL Challenge Workshop on Recognising Textual Entail-
ment, 2005, pages 73?76.
D. Klein and C. D. Manning. 2003. Accurate unlexicalized
parsing. In Proceedings of the 41st Meeting of the Associa-
tion of Computational Linguistics.
Roger Levy and Galen Andrew. 2006. Tregex and Tsurgeon:
tools for querying and manipulating tree data structures. In
LREC 2006.
E. Marsi and E. Krahmer. 2005. Classification of semantic re-
lations by humans and machines. In Proceedings of the ACL
2005 Workshop on Empirical Modeling of Semantic Equiva-
lence and Entailment.
A. McCallum and W. Li. 2003. Early results for named entity
recognition with conditional random fields, feature induction
and web-enhanced lexicons. In Proceedings of CoNLL 2003.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied morpho-
logical processing in English. In Natural Language Engi-
neering, volume 7(3), pages 207?233.
D. Moldovan, C. Clark, S. Harabagiu, and S. Maiorano. 2003.
COGEX: A logic prover for question answering. In NAACL-
03.
M. Pasca and S. Harabagiu. 2001. High performance ques-
tion/answering. In SIGIR-01, pages 366?374.
R. Raina, A .Ng, and C. D. Manning. 2005. Robust textual
inference via learning and abductive reasoning. In Proceed-
ings of the Twentieth National Conference on Artificial Intel-
ligence (AAAI).
M. Tatu and D. Moldovan. 2005. A semantic approach to rec-
ognizing textual entailment. In HLT/EMNLP 2005, pages
371?378.
48
Solving Logic Puzzles: From Robust Processing to Precise
Semantics
Iddo Lev,? Bill MacCartney,? Christopher D. Manning,?? and Roger Levy?
? Department of Computer Science ? Department of Linguistics
Stanford University Stanford University
Stanford, CA 94305-9040, USA Stanford, CA 94305-2150, USA
{iddolev|wcmac|manning}@cs.stanford.edu rog@stanford.edu
Abstract
This paper presents intial work on a system that
bridges from robust, broad-coverage natural lan-
guage processing to precise semantics and auto-
mated reasoning, focusing on solving logic puzzles
drawn from sources such as the Law School Admis-
sion Test (LSAT) and the analytic section of the
Graduate Record Exam (GRE). We highlight key
challenges, and discuss the representations and per-
formance of the prototype system.
1 Introduction
Traditional approaches to natural language un-
derstanding (Woods, 1973; Warren and Pereira,
1982; Alshawi, 1992) provided a good account
of mapping from surface forms to semantic rep-
resentations, when confined to a very limited
vocabulary, syntax, and world model, and re-
sulting low levels of syntactic/semantic ambi-
guity. It is, however, difficult to scale these
methods to unrestricted, general-domain natu-
ral language input because of the overwhelming
problems of grammar coverage, unknown words,
unresolvable ambiguities, and incomplete do-
main knowledge. Recent work in NLP has
consequently focused on more robust, broad-
coverage techniques, but with the effect of
overall shallower levels of processing. Thus,
state-of-the-art work on probabilistic parsing
(e.g., (Collins, 1999)) provides a good solution
to robust, broad coverage parsing with auto-
matic and frequently successful ambiguity reso-
lution, but has largely ignored issues of semantic
interpretation. The field of Question Answering
(Pasca and Harabagiu, 2001; Moldovan et al,
2003) focuses on simple-fact queries. And so-
called semantic parsing (Gildea and Jurafsky,
2002) provides as end output only a flat clas-
sification of semantic arguments of predicates,
ignoring much of the semantic content, such as
quantifiers.
A major research question that remains unan-
swered is whether there are methods for get-
ting from a robust ?parse-anything? statisti-
cal parser to a semantic representation precise
enough for knowledge representation and auto-
mated reasoning, without falling afoul of the
same problems that stymied the broad appli-
cation of traditional approaches. This paper
presents initial work on a system that addresses
this question. The chosen task is solving logic
puzzles of the sort found in the Law School Ad-
mission Test (LSAT) and the old analytic sec-
tion of the Graduate Record Exam (GRE) (see
Figure 1 for a typical example). The system in-
tegrates statistical parsing, ?on-the-fly? combi-
natorial synthesis of semantic forms, scope- and
reference-resolution, and precise semantic repre-
sentations that support the inference required
for solving the puzzles. Our work comple-
ments research in semantic parsing and TREC-
style Question Answering by emphasizing com-
plex yet robust inference over general-domain
NL texts given relatively minimal lexical and
knowledge-base resources.
1.1 Why Logic Puzzles?
Logic puzzles have a number of attractive char-
acteristics as a target domain for research plac-
ing a premium on precise inference.
First, whereas for humans the language un-
derstanding part of logic puzzles is trivial but
the reasoning is difficult, for computers it is
clearly the reverse. It is straightforward for a
computer to solve a formalized puzzle, so the
research effort is on the NLP parts rather than
a difficult back-end AI problem. Moreover, only
a small core of world knowledge (prominently,
temporal and spatial entailments) is typically
crucial to solving the task.
Second, the texts employ everyday language:
there are no domain-restrictions on syntactic
and semantic constructions, and the situations
described by the texts are diverse.
Third, and most crucial, answers to puzzle
questions never explicitly appear in the text and
Preamble: Six sculptures ? C, D, E, F, G, and H
? are to be exhibited in rooms 1, 2, and 3 of an art
gallery. The exhibition conforms to the following
conditions:
(1) Sculptures C and E may not be exhibited in
the same room.
(2) Sculptures D and G must be exhibited in the
same room.
(3) If sculptures E and F are exhibited in the same
room, no other sculpture may be exhibited in that
room.
(4) At least one sculpture must be exhibited in each
room, and no more than three sculptures may be
exhibited in any room.
Question 1: If sculpture D is exhibited in room
3 and sculptures E and F are exhibited in room 1,
which of the following may be true?
(A) Sculpture C is exhibited in room 1.
(B) No more than 2 sculptures are exhibited in
room 3.
(C) Sculptures F and H are exhibited in the same
room.
(D) Three sculptures are exhibited in room 2.
(E) Sculpture G is exhibited in room 2.
Question 2: If sculptures C and G are exhibited
in room 1, which of the following may NOT be a
complete list of the sculpture(s) exhibited in room
2?
(A) Sculpture D (B) Sculptures E and H (C). . .
Adapted from (Weber, 1999).
Figure 1: Example of a Puzzle Text
must be logically inferred from it, so there is
very little opportunity to use existing superficial
analysis methods of information-extraction and
question-answering as a substitute for deep un-
derstanding. A prerequisite for successful infer-
ence is precise understanding of semantic phe-
nomena like modals and quantifiers, in contrast
with much current NLP work that just ignores
such items. We believe that representations
with a well-defined model-theoretic semantics
are required.
Finally, the task has a clear evaluation metric
because the puzzle texts are designed to yield
exactly one correct answer to each multiple-
choice question. Moreover, the domain is an-
other example of ?found test material? in the
sense of (Hirschman et al, 1999): puzzle texts
were developed with a goal independent of the
evaluation of natural language processing sys-
tems, and so provide a more realistic evaluation
framework than specially-designed tests such as
TREC QA.
While our current system is not a real world
application, we believe that the methods being
developed could be used in applications such as
a computerized office assistant that must under-
stand requests such as: ?Put each file contain-
ing a task description in a different directory.?
2 System Overview
This section explains the languages we use to
represent the content of a puzzle. Computing
the representations from a text is a complex pro-
cess with several stages, as shown in Figure 2.
Most of the stages are independent of the puz-
zles domain. Section 3 reviews the main chal-
lenges in this process, and later sections outline
the various processing stages. More details of
some of these stages can be found at (Stanford
NLP Group, 2004).
2.1 First-Order Logic (FOL)
An obvious way of solving logic puzzles is to
use off-the-shelf FOL reasoners, such as theo-
rem provers and model builders. Although most
GRE logic puzzles can also be cast as constraint-
satisfaction problems (CSPs), FOL representa-
tions are more general and more broadly ap-
plicable to other domains, and they are closer
to the natural language semantics. GRE logic
puzzles have finite small domains, so it is prac-
ticable to use FOL reasoners.
The ultimate representation of the content of
a puzzle is therefore written in FOL. For ex-
ample, the representation for the first part of
constraint (4) in Figure 1 is: ?x.room(x) ?
?y.sculpture(y)? exhibit(y, x). (The treatment
of the modal ?must? is explained in ?9.2).
2.2 Semantic Logic (SL)
Representing the meaning of natural language
texts in FOL is not straightforward because
human languages employ events, plural enti-
ties, modal operations, and complex numeric
expressions. We therefore use an intermedi-
ate representation, written in Semantic Logic
(SL), which is intended to be a general-purpose
semantic representation language. SL extends
FOL with event and group variables, the modal
operators ? (necessarily) and ? (possibly), and
Generalized Quantifiers (Barwise and Cooper,
statistical
parser
combinatorial
semantics
scope
resolution
reference
resolution
plurality
disambig.
lex.
sem.
info gaps
filler
text parse trees URs DL formulas SL formulas
answer reasoningmodule
to
FOL
FOL formulas
specific to
logic puzzles
general
Figure 2: System Overview
1981) Q(type, var, restrictor, body), where type
can be ?, ?, at-least(n), etc. To continue the ex-
ample, the intermediate representation for the
constraint is:
?Q(?, x1, room(x1), Q(?1, x2, sculpture(x2),
?e.exhibit(e) ? subj(e, x2) ? in(e, x1)))
2.3 Non-determinism
Although logic puzzles are carefully designed
to reduce ambiguities to ensure that there
is exactly one correct answer per question,
there are still many ambiguities in the analy-
sis, such as multiple possibilities for syntactic
structures, pronominal reference, and quantifier
scope. Each module ranks possible output rep-
resentations; in the event that a later stage re-
veals an earlier choice to be wrong (it may be
inconsistent with the rest of the puzzle, or lead
to a non-unique correct answer to a question),
the system backtracks and chooses the next-best
output representation for the earlier stage.
3 Challenges
3.1 Combinatorial Semantics
The challenge of combinatorial semantics is to
be able to assign exactly one semantic repre-
sentation to each word and sub-phrase regard-
less of its surrounding context, and to combine
these representations in a systematic way until
the representation for the entire sentence is ob-
tained. There are many linguistic constructions
in the puzzles whose compositional analysis is
difficult, such as a large variety of noun-phrase
structures (e.g., ?Every sculpture must be ex-
hibited in a different room?) and ellipses (e.g.,
?Brian saw a taller man than Carl [did]?).
3.2 Scope Ambiguities
A sentence has a scope ambiguity when quan-
tifiers and other operators in the sentence can
have more than one relative scope. E.g., in con-
straint (4) of Figure 1, ?each room? outscopes
?at least one sculpture?, but in other contexts,
the reverse scoping is possible. The challenge
is to find, out of all the possible scopings, the
appropriate one, to understand the text as the
writer intended.
3.3 Reference Resolution
The puzzle texts contain a wide variety of
anaphoric expressions, including pronouns, defi-
nite descriptions, and anaphoric adjectives. The
challenge is to identify the possible antecedents
that these expressions refer to, and to select
the correct ones. The problem is complicated
by the fact that anaphoric expressions interact
with quantifiers and may not refer to any par-
ticular context element. E.g., the anaphoric ex-
pressions in ?Sculptures C and E are exhibited
in the same room? and in ?Each man saw a dif-
ferent woman? interact with sets ({C,E} and
the set of all men, respectively).
3.4 Plurality Disambiguation
Sentences that include plural entities are po-
tentially ambiguous between different readings:
distributive, collective, cumulative, and combi-
nations of these. For example, sentence 1 in
Figure 1 says (among other things) that each
of the six sculptures is displayed in one of the
three rooms ? the group of sculptures and the
group of rooms behave differently here. Plu-
rality is a thorny topic which interacts in com-
plex ways with other semantic issues, including
quantification and reference.
3.5 Lexical Semantics
The meaning of open-category words is often
irrelevant to solving a puzzle. For example,
the meaning of ?exhibited?, ?sculpture?, and
?room? can be ignored because it is enough to
understand that the first is a binary relation
that holds between elements of groups described
by the second and third words.1 This observa-
1The meanings are still important for the implicit
knowledge that a sculpture cannot be exhibited in more
than one room. However, such knowledge can be
guessed, as explained in ?8.
tion provides the potential for a general system
that solves logic puzzles.
Of course, in many cases, the particular
meaning of open-category words and other ex-
pressions is crucial to the solution. An example
is provided in question 2 of Figure 1: the sys-
tem has to understand what ?a complete list?
means. Therefore, to finalize the meaning com-
puted for a sentence, such expressions should be
expanded to their explicit meaning. Although
there are many such cases and their analysis is
difficult, we anticipate that it will be possible to
develop a relatively compact library of critical
puzzle text expressions. We may also be able
to use existing resources such as WordNet and
FrameNet.
3.6 Information Gaps
Natural language texts invariably assume some
knowledge implicitly. E.g., Figure 1 does not ex-
plicitly specify that a sculpture may not be ex-
hibited in more than one room at the same time.
Humans know this implicit information, but a
computer reasoning from texts must be given
it explicitly. Filling these information gaps is
a serious challenge; representation and acquisi-
tion of the necessary background knowledge are
very hard AI problems. Fortunately, the puz-
zles domain allows us to tackle this issue, as
explained in ?8.
3.7 Presuppositions and Implicatures
In addition to its semantic meaning, a natural
language text conveys two other kinds of con-
tent.
Presuppositions are pieces of information as-
sumed in a sentence. Anaphoric expressions
bear presuppositions about the existence of en-
tities in the context; the answer choice ?Sculp-
tures C and E? conveys the meaning {C,E},
but has the presupposition sculpture(C) ?
sculpture(E); and a question of the form A ?
B, such as question 1 in Figure 1, presupposes
that A is consistent with the preamble.
Implicatures are pieces of information sug-
gested by the very fact of saying, or not say-
ing, something. Two maxims of (Grice, 1989)
dictate that each sentence should be both con-
sistent and informative (i.e. not entailed) with
respect to its predecessors. Another maxim dic-
tates saying as much as required, and hence the
sentence ?No more than three sculptures may be
exhibited in any room? carries the implicature
that in some possible solution, three sculptures
are indeed exhibited in the same room.
Systematic calculation of presuppositions and
implicatures has been given less attention in
NLP and is less understood than the calcula-
tion of meaning. Yet computing and verifying
them can provide valuable hints to the system
whether it understood the meaning of the text
correctly.
4 Morpho-Syntactic Analysis
While traditional hand-built grammars often in-
clude a rich semantics, we have found their
coverage inadequate for the logic puzzles task.
For example, the English Resource Grammar
(Copestake and Flickinger, 2000) fails to parse
any of the sentences in Figure 1 for lack of cover-
age of some words and of several different syn-
tactic structures; and parsable simplified ver-
sions of the text produce dozens of unranked
parse trees. For this reason, we use a broad-
coverage statistical parser (Klein and Manning,
2003) trained on the Penn Treebank. In addi-
tion to robustness, treebank-trained statistical
parsers have the benefit of extensive research
on accurate ambiguity resolution. Qualitatively,
we have found that the output of the parser on
logic puzzles is quite good (see ?10). After pars-
ing, each word in the resulting parse trees is
converted to base form by a stemmer.
A few tree-transformation rules are applied
on the parse trees to make them more conve-
nient for combinatorial semantics. Most of them
are general, e.g. imposing a binary branching
structure on verb phrases, and grouping expres-
sions like ?more than?. A few of them correct
some parsing errors, such as nouns marked as
names and vice-versa. There is growing aware-
ness in the probabilistic parsing literature that
mismatches between training and test set genre
can degrade parse accuracy, and that small
amounts of correct-genre data can be more im-
portant than large amounts of wrong-genre data
(Gildea, 2001); we have found corroborating ev-
idence in misparsings of noun phrases common
in puzzle texts, such as ?Sculptures C and E?,
which do not appear in the Wall Street Journal
corpus. Depending on the severity of this prob-
lem, we may hand-annotate a small amount of
puzzle texts to include in parser training data.
5 Combinatorial Semantics
Work in NLP has shifted from hand-built gram-
mars that need to cover explicitly every sen-
tence structure and that break down on unex-
pected inputs to more robust statistical parsing.
However, grammars that involve precise seman-
tics are still largely hand-built (e.g. (Carpenter,
1998; Copestake and Flickinger, 2000)). We aim
at extending the robustness trend to the seman-
tics. We start with the compositional semantics
framework of (Blackburn and Bos, 2000; Bos,
2001) and modify it to achieve greater robust-
ness and coverage.2
One difference is that our lexicon is kept
very small and includes only a few words with
special semantic entries (like pronouns, con-
nectives, and numbers). Open-category words
come with their part-of-speech information in
the parse trees (e.g. (NN dog)), so their seman-
tics can be obtained using generic semantic tem-
plates (but cf. ?3.5).
In classic rule-to-rule systems of semantics
like (Blackburn and Bos, 2000), each syntactic
rule has a separate semantic combination rule,
and so the system completely fails on unseen
syntactic structures. The main distinguishing
goal of our approach is to develop a more robust
process that does not need to explicitly specify
how to cover every bit of every sentence. The
system incorporates a few initial ideas in this
direction.
First, role and argument-structure informa-
tion for verbs is expensive to obtain and unre-
liable anyway in natural texts. So to deal with
verbs and VPs robustly, their semantics in our
system exports only an event variable rather
than variables for the subject, the direct object,
etc. VP modifiers (such as PPs and ADVPs)
combine to the VP by being applied on the ex-
ported event variable. NP modifiers (including
the sentence subject) are combined to the event
variable through generic roles: subj, np1, np2,
etc. The resulting generic representations are
suitable in the puzzles domain because usually
only the relation between objects is important
and not their particular roles in the relation.
This is true for other tasks as well, including
some broad-coverage question answering.
All NPs are analyzed as generalized quanti-
fiers, but a robust compositional analysis for
the internal semantics of NPs remains a serious
challenge. For example, the NP ?three rooms?
should be analyzed as Q(num(3), x, room(x), ..),
but the word ?three? by itself does not con-
tribute the quantifier ? compare with ?at least
three rooms? Q(?3, x, room(x), ..). Yet another
case is ?the three rooms? (which presupposes
2Our system uses a reimplementation in Lisp rather
than their Prolog code.
a group g such that g ? room ? |g| = 3). The
system currently handles a number of NP struc-
tures by scanning the NP left-to-right to iden-
tify important elements. This may make it eas-
ier than a strictly compositional analysis to ex-
tend the coverage to additional cases.
All other cases are handled by a flexible com-
bination process. In case of a single child, its
semantics is copied to its parent. With more
children, all combinations of applying the se-
mantics of one child to its siblings are tried,
until an application does not raise a type er-
ror (variables are typed to support type check-
ing). This makes it easier to extend the coverage
to new grammatical constructs, because usually
only the lexical entry needs to be specified, and
the combination process takes care to apply it
correctly in the parse tree.
6 Scope Resolution
One way of dealing with scope ambiguities is by
using underspecified representations (URs). A
UR is a meta-language construct, describing a
set of object-language formulas.3 It describes
the pieces shared by these formulas, but possi-
bly underspecifies how they combine with each
other. A UR can then be resolved to the specific
readings it implicitly describes.
We use an extension of Hole Semantics
(Blackburn and Bos, 2000)4 for expressing URs
and calculating them from parse trees (modulo
the modifications in ?5). There are several ad-
vantages to this approach. First, it supports
the calculation of just one UR per sentence in
a combinatorial process that visits each node of
the parse tree once. This contrasts with ap-
proaches such as Categorial Grammars (Car-
penter, 1998), which produce explicitly all the
scopings by using type raising rules for different
combinations of scope, and require scanning the
entire parse tree once per scoping.
Second, the framework supports the expres-
sion of scoping constraints between different
parts of the final formula. Thus it is possible
to express hierarchical relations that must exist
between certain quantifiers, avoiding the prob-
lems of naive approaches such as Cooper stor-
age (Cooper, 1983). The expression of scoping
constraints is not limited to quantifiers and is
applicable to all other operators as well. More-
over, it is possible to express scope islands by
3In our case, DL formulas ? see footnote 6.
4The approach is similar to MRS (Copestake et al,
2003).
constraining all the parts of a subformula to be
outscoped by a particular node.
Another advantage is that URs support ef-
ficient elimination of logically-equivalent read-
ings. Enumerating all scopings and using
a theorem-prover to determine logical equiva-
lences requires O(n2) comparisons for n scop-
ings. Instead, filtering methods (Chaves, 2003)
can add tests to the UR-resolution process,
disallowing certain combinations of operators.
Thus, only one ordering of identical quantifiers
is allowed, so ?A man saw a woman? yields
only one of its two equivalent scopings. We also
filter ?? and ?? combinations, allowing only
the equivalent ?? and ??. However, numeric
quantifiers are not filtered (the two scopings of
?Three boys saw three films? are not equiva-
lent). Such filtering can result in substantial
speed-ups for sentences with a few quantifiers
(see (Chaves, 2003) for some numbers).
Finally, our true goal is determining the cor-
rect relative scoping in context rather than enu-
merating all possibilities. We are developing
a probabilistic scope resolution module that
learns from hand-labeled training examples to
predict the most probable scoping, using fea-
tures such as the quantifiers? categories and
their positions and grammatical roles in the sen-
tence.5
7 Reference Resolution
SL is not convenient for representing directly
the meaning of referring expressions because (as
in FOL) the extent of a quantifier in a formula
cannot be extended easily to span variables in
subsequent formulas. We therefore use Dis-
course Logic (DL), which is SL extended with
DRSes and ?-expressions as in (Blackburn and
Bos, 2000) (which is based on Discourse Repre-
sentation Theory (Kamp and Reyle, 1993) and
its recent extensions for dealing with presuppo-
sitions).6 This approach (like other dynamic se-
mantics approaches) supports the introduction
of entities that can later be referred back to,
and explains when indefinite NPs should be in-
5E.g. there is a strong preference for ?each? to take
wide scope, a moderate preference for the first quantifier
in a sentence to take wide scope, and a weak preference
for a quantifier of the grammatical subject to take wide
scope.
6Thus, the URs calculated from parse trees are ac-
tually URs of DL formulas. The scope resolution phase
resolves the URs to explicit DL formulas, and the ref-
erence resolution phase converts these formulas to SL
formulas.
terpreted as existential or universal quantifiers
(such as in the antecedent of conditionals). The
reference resolution framework from (Blackburn
and Bos, 2000) provides a basis for finding all
possible resolutions, but does not specify which
one to choose. We are working on a probabilis-
tic reference-resolution module, which will pick
from the legal resolutions the most probable one
based on features such as: distance, gender, syn-
tactic place and constraints, etc.
8 Filling Information Gaps
To find a unique answer to every question of a
puzzle, background information is required be-
yond the literal meaning of the text. In Ques-
tion 1 of Figure 1, for example, without the con-
straint that a sculpture may not be exhibited in
multiple rooms, answers B, D and E are all cor-
rect. Human readers deduce this implicit con-
straint from their knowledge that sculptures are
physical objects, rooms are locations, and phys-
ical objects can have only one location at any
given time. In principle, such information could
be derived from ontologies. Existing ontologies,
however, have limited coverage, so we also plan
to leverage information about expected puzzle
structures.
Most puzzles we collected are formaliz-
able as constraints on possible tuples of ob-
jects. The crucial information includes: (a)
the object classes; (b) the constants nam-
ing the objects; and (c) the relations used to
link objects, together with their arguments?
classes. For the sculptures puzzle, this infor-
mation is: (a) the classes are sculpture and
room; (b) the constants are C,D,E, F,G,H for
sculpture and 1, 2, 3 for room; (c) the relation
is exhibit(sculpture, room). This information is
obtainable from the parse trees and SL formu-
las.
Within this framework, implicit world knowl-
edge can often be recast as mathematical prop-
erties of relations. The unique location con-
straint on sculptures, for example, is equivalent
to constraining the mapping from sculptures to
rooms to be injective (one-to-one); other cases
exist of constraining mappings to be surjective
(onto) and/or total. Such properties can be ob-
tained from various sources, including cardinal-
ity of object classes, pure lexical semantics, and
even through a systematic search for sets of im-
plicit constraints that, in combination with the
explicitly stated constraints, yield exactly one
answer per question. Figure 3 shows the num-
only object classes
23?6 = 262, 144 models
explicit constraints
2,916 models
implicit constraints
36 = 729 models
explicit and implicit constraints
78 models
Figure 3: Effect of explicit and implicit con-
straints on constraining the number of possible
models
ber of possible models for the sculptures puzzle
as affected by explicit and implicit constraints
in the preamble.
9 Solving the Puzzle
9.1 Expanding the answer choices
The body of a logic puzzle question contains a
(unique) wh-term (typically ?which of the fol-
lowing?), a modality (such as ?must be true? or
?could be true?), and (possibly) an added condi-
tion. Each answer choice is expanded by substi-
tuting its SL form for the wh-term in the ques-
tion body. For example, the expansion for an-
swer choice (A) of question 1 in Figure 1 would
be the SL form corresponding to: ?If sculpture
D is exhibited . . . , then [Sculpture C is exhibited
in room 1 ] must be true?.
9.2 Translating SL to FOL
To translate an SL representation to pure FOL,
we eliminate event variables by replacing an SL
form ?e.P (e)?R1(e, t1)? ..?Rn(e, tn) with the
FOL form P (t1, .., tn). An ordering is imposed
on role names to guarantee that arguments are
always used in the same order in relations. Nu-
meric quantifiers are encoded in FOL in the ob-
vious way, e.g., Q(?2, x, ?, ?) is translated to
?x1?x2. x1 6= x2? (???)[x1/x]? (???)[x2/x].
Each expanded answer choice contains one
modal operator. Modals are moved outward
of negation as usual, and outward of condition-
als by changing A ? ?B to ?(A ? B) and
A ? ?B to ?(A?B). A modal operator in the
outermost scope can then be interpreted as a
directive to the reasoning module to test either
entailment (?) or consistency (?) between the
preamble and the expanded answer choice.
9.3 Using FOL reasoners
There are two reasons for using both theo-
rem provers and model builders. First, they
are complementary reasoners: while a theorem
prover is designed to demonstrate the incon-
sistency of a set of FOL formulas, and so can
find the correct answer to ?must be true? ques-
tions through proof by contradiction, a model
builder is designed to find a satisfying model,
and is thus suited to finding the correct an-
swer to ?could be true? questions.7 Second, a
reasoner may take a very long time to halt on
some queries, but the complementary reasoner
may still be used to answer the query in the
context of a multiple-choice question through
a process of elimination. Thus, if the model
builder is able to show that the negations of four
choices are consistent with the preamble (indi-
cating they are not entailed), then it can be
concluded that the remaining choice is entailed
by the preamble, even if the theorem prover has
not yet found a proof.
We use the Otter 3.3 theorem prover and
the MACE 2.2 model builder (McCune, 1998).8
The reasoning module forks parallel sub-
processes, two per answer choice (one for Otter,
one for MACE). If a reasoner succeeds for an an-
swer choice, the choice is marked as correct or
incorrect, and the dual sub-process is killed. If
all answer-choices but one are marked incorrect,
the remaining choice is marked correct even if
its sub-processes did not yet terminate.
10 Progress
Using the sculptures puzzle (a set of four ques-
tions partly shown in Figure 1) as an initial test
case, we have built a prototype end-to-end sys-
tem. In its present state, the system analyzes
and solves correctly all questions in this puzzle,
except that there is still no understanding of the
phrase ?complete list? in question 2. The back-
end reasoning module is finished and works for
any puzzle formalized in FOL+modals. The
probabilistic scope resolution module, trained
on 259 two-quantifier sentences extracted from
122 puzzles and tested on 46 unseen sentences,
attains an accuracy of about 94% over an 82%
linear-order baseline. A preliminary evaluation
on another unseen puzzle shows that on 60%
of the sentences, the parser?s output is accurate
enough to support the subsequent computation
of the semantics, and we expect this to be better
after it is trained on puzzle texts. However, the
7GRE puzzles always deal with finite domains, so a
model builder is guaranteed to halt on consistent sets of
formulas.
8An advantage of using Otter and MACE is that they
are designed to work together, and use the same input
syntax.
system as a whole worked end-to-end on only
one of the unseen sentences in that puzzle; key
losses come from unhandled semantic phenom-
ena (e.g. ?only?, ?except?, ellipses), unhandled
lexical semantics of words that must be under-
stood (e.g. ?complete list?), and unhandled im-
plicit constraint types that need to be filled.
11 Conclusion and Further Work
The key open problem is identifying sufficiently
robust and general methods for building precise
semantic representations, rather than requiring
hand-built translation rules for a seemingly end-
less list of special phenomena. Immediate fu-
ture work will include extending and generaliz-
ing the system?s coverage of syntax-to-semantics
mappings, incorporating classifiers for suggest-
ing likely coreference resolutions and operator
scopings, and developing methods for calculat-
ing presuppositions and inferences. This work
may be sufficient to give good coverage of the
problem domain, or we may need to develop
new more robust models of syntactic to seman-
tic transductions.
Acknowledgements
Thanks to Kristina Toutanova for useful discus-
sions.
This work was supported in part by the Advanced
Research and Development Activity (ARDA)?s
Advanced Question Answering for Intelligence
(AQUAINT) Program; in part by the Department
of the Navy under grant no. N000140010660, a Mul-
tidisciplinary University Research Initiative on Nat-
ural Language Interaction with Intelligent Tutoring
Systems; and in part by Department of Defense
award no. NBCH-D-03-0010(1) ?A Person-Centric
Enduring, Personalized, Cognitive Assistant.?
References
Hiyan Alshawi, editor. 1992. The Core Language
Engine. MIT Press.
J. Barwise and R. Cooper. 1981. Generalized quan-
tifiers and natural language. Linguistics and Phi-
losophy, 4:159?219.
Patrick Blackburn and Johan Bos. 2000. Rep-
resentation and Inference for Natural Language:
A First Course in Computational Semantics.
http://www.comsem.org/.
Johan Bos. 2001. Doris 2001: Underspecification,
resolution and inference for discourse representa-
tion structures. In Blackburn and Kohlhase, edi-
tors, ICoS-3. Inference in Compuational Seman-
tics. Workshop Proceedings.
Bob Carpenter. 1998. Type-Logical Semantics. MIT
Press.
Rui P. Chaves. 2003. Non-redundant scope disam-
biguation in underspecified semantics. In Balder
ten Cate, editor, Proc. of the 8th ESSLLI Student
Session, pages 47?58.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Robin Cooper. 1983. Quantification and Syntactic
Theory. Reidel, Dordrecht.
A. Copestake and D. Flickinger. 2000. An open-
source grammar development environment and
broad-coverage english grammar using HPSG. In
Proceedings of LREC.
A. Copestake, D. Flickinger, C. Pol-
lard, and I. Sag. 2003. Minimal re-
cursion semantics: an introduction.
http://lingo.stanford.edu/sag/publications.html.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
Daniel Gildea. 2001. Corpus variation and parser
performance. In Proceedings of EMNLP, pages
167?202.
Paul H. Grice. 1989. Studies in the way of words.
Harvard University Press.
Lynette Hirschman, Marc Light, Eric Breck, and
John D. Burger. 1999. Deep Read: A reading
comprehension system. In Proc. of the 37th An-
nual Meeting of the ACL, pages 325?332.
Hans Kamp and Uwe Reyle. 1993. From Discourse
to Logic. Kluwer, Dordrecht.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proc. of the 41st
Annual Meeting of the ACL, pages 423?430.
W. McCune. 1998. Automatic proofs and counterex-
amples for some ortholattice identities. Informa-
tion Processing Letters, 65:285?291.
Dan I. Moldovan, Christine Clark, Sanda M.
Harabagiu, and Steven J. Maiorano. 2003. CO-
GEX: A logic prover for question answering. In
Proc. of HLT/NAACL, pages 87?93.
Marius Pasca and Sanda M. Harabagiu. 2001. High
performance question/answering. In Proc. of SI-
GIR, pages 366?374.
Stanford NLP Group. 2004. Project website.
http://nlp.stanford.edu/nlkr/.
David Warren and Fernando Pereira. 1982. An effi-
cient easily adaptable system for interpreting nat-
ural language queries. Computational Linguistics,
8(3-4):110?122.
Karl Weber. 1999. The Unofficial Guide to the GRE
Test. ARCO Publishing, 2000 edition.
W. A. Woods. 1973. Progress in natural language
understanding: An application to lunar geology.
In AFIPS Conference Proceedings, volume 42,
pages 441?450.
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 165?170,
Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Alignments and Leveraging Natural Logic
Nathanael Chambers, Daniel Cer, Trond Grenager, David Hall, Chloe Kiddon
Bill MacCartney, Marie-Catherine de Marneffe, Daniel Ramage
Eric Yeh, Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{natec,dcer,grenager,dlwh,loeki,wcmac,mcdm,dramage,yeh1,manning}@stanford.edu
Abstract
We describe an approach to textual infer-
ence that improves alignments at both the
typed dependency level and at a deeper se-
mantic level. We present a machine learning
approach to alignment scoring, a stochas-
tic search procedure, and a new tool that
finds deeper semantic alignments, allowing
rapid development of semantic features over
the aligned graphs. Further, we describe a
complementary semantic component based
on natural logic, which shows an added gain
of 3.13% accuracy on the RTE3 test set.
1 Introduction
Among the many approaches to textual inference,
alignment of dependency graphs has shown utility
in determining entailment without the use of deep
understanding. However, discovering alignments
requires a scoring function that accurately scores
alignment and a search procedure capable of approx-
imating the optimal mapping within a large search
space. We address the former requirement through
a machine learning approach for acquiring lexical
feature weights, and we address the latter with an
approximate stochastic approach to search.
Unfortunately, the most accurate aligner can-
not capture deeper semantic relations between two
pieces of text. For this, we have developed a tool,
Semgrex, that allows the rapid development of de-
pendency rules to find specific entailments, such as
familial or locative relations, a common occurence
in textual entailment data. Instead of writing code by
hand to capture patterns in the dependency graphs,
we develop a separate rule-base that operates over
aligned dependency graphs. Further, we describe a
separate natural logic component that complements
our textual inference system, making local entail-
ment decisions based on monotonic assumptions.
The next section gives a brief overview of the sys-
tem architecture, followed by our proposal for im-
proving alignment scoring and search. New coref-
erence features and the Semgrex tool are then de-
scribed, followed by a description of natural logic.
2 System Overview
Our system is a three stage architecture that con-
ducts linguistic analysis, builds an alignment be-
tween dependency graphs of the text and hypothesis,
and performs inference to determine entailment.
Linguistic analysis identifies semantic entities, re-
lationships, and structure within the given text and
hypothesis. Typed dependency graphs are passed
to the aligner, as well as lexical features such as
named entities, synonymity, part of speech, etc. The
alignment stage then performs dependency graph
alignment between the hypothesis and text graphs,
searching the space of possible alignments for the
highest scoring alignment. Improvements to the
scorer, search algorithm, and automatically learned
weights are described in the next section.
The final inference stage determines if the hy-
pothesis is entailed by the text. We construct a set
of features from the previous stages ranging from
antonyms and polarity to graph structure and seman-
tic relations. Each feature is weighted according to a
set of hand-crafted or machine-learned weights over
165
the development dataset. We do not describe the fea-
tures here; the reader is referred to de Marneffe et al
(2006a) for more details. A novel component that
leverages natural logic is also used to make the final
entailment decisions, described in section 6.
3 Alignment Model
We examine three tasks undertaken to improve the
alignment phase: (1) the construction of manu-
ally aligned data which enables automatic learning
of alignment models, and effectively decouples the
alignment and inference development efforts, (2) the
development of new search procedures for finding
high-quality alignments, and (3) the use of machine
learning techniques to automatically learn the pa-
rameters of alignment scoring models.
3.1 Manual Alignment Annotation
While work such as Raina et al (2005) has tried
to learn feature alignment weights by credit assign-
ment backward from whether an item is answered
correctly, this can be very difficult, and here we fol-
low Hickl et al (2006) in using supervised gold-
standard alignments, which help us to evaluate and
improve alignment and inference independently.
We built a web-based tool that allows annotators
to mark semantic relationships between text and hy-
pothesis words. A table with the hypothesis words
on one axis and the text on the other allows re-
lationships to be marked in the corresponding ta-
ble cell with one of four options. These relation-
ships include text to hypothesis entailment, hypothe-
sis to text entailment, synonymy, and antonymy. Ex-
amples of entailment (from the RTE 2005 dataset)
include pairs such as drinking/consumption, coro-
navirus/virus, and Royal Navy/British. By distin-
guishing between these different types of align-
ments, we can capture some limited semantics in the
alignment process, but full exploitation of this infor-
mation is left to future work.
We annotated the complete RTE2 dev and
RTE3 dev datasets, for a total of 1600 aligned
text/hypothesis pairs (the data is available at
http://nlp.stanford.edu/projects/rte/).
3.2 Improving Alignment Search
In order to find ?good? alignments, we define both a
formal model for scoring the quality of a proposed
alignment and a search procedure over the alignment
space. Our goal is to build a model that maximizes
the total alignment score of the full datasetD, which
we take to be the sum of the alignment scores for all
individual text/hypothesis pairs (t, h).
Each of the text and hypothesis is a semantic de-
pendency graph; n(h) is the set of nodes (words)
and e(h) is the set of edges (grammatical relations)
in a hypothesis h. An alignment a : n(h) 7? n(t) ?
{null} maps each hypothesis word to a text word
or to a null symbol, much like an IBM-style ma-
chine translation model. We assume that the align-
ment score s(t, h, a) is the sum of two terms, the first
scoring aligned word pairs and the second the match
between an edge between two words in the hypoth-
esis graph and the corresponding path between the
words in the text graph. Each of these is a sum, over
the scoring function for individual word pairs sw and
the scoring function for edge path pairs se:
s(t, h, a) =
?
hi?n(h)
sw(hi, a(hi))
+
?
(hi,hj)?e(h)
se((hi, hj), (a(hi), a(hj)))
The space of alignments for a hypothesis with m
words and a text with n words contains (n + 1)m
possible alignments, making exhaustive search in-
tractable. However, since the bulk of the alignment
score depends on local factors, we have explored
several search strategies and found that stochastic
local search produces better quality solutions.
Stochastic search is inspired by Gibbs sampling
and operates on a complete state formulation of the
search problem. We initialize the algorithm with the
complete alignment that maximizes the greedy word
pair scores. Then, in each step of the search, we
seek to randomly replace an alignment for a single
hypothesis word hi. For each possible text word tj
(including null), we compute the alignment score if
we were to align hi with tj . Treating these scores as
log probabilities, we create a normalized distribution
from which we sample one alignment. This Gibbs
sampler is guaranteed to give us samples from the
posterior distribution over alignments defined im-
plicitly by the scoring function. As we wish to find a
maximum of the function, we use simulated anneal-
ing by including a temperature parameter to smooth
166
the sampling distribution as a function of time. This
allows us to initially explore the space widely, but
later to converge to a local maximum which is hope-
fully the global maximum.
3.3 Learning Alignment Models
Last year, we manually defined the alignment scor-
ing function (de Marneffe et al, 2006a). However,
the existence of the gold standard alignments de-
scribed in section 3.1 enables the automatic learning
of a scoring function. For both the word and edge
scorers, we choose a linear model where the score is
the dot product of a feature and a weight vector:
sw(hi, tj) = ?w ? f(hi, tj), and
se((hi, hj), (tk, t`)) = ?e ? f((hi, hj), (tk, t`)).
Recent results in machine learning show the ef-
fectiveness of online learning algorithms for struc-
ture prediction tasks. Online algorithms update their
model at each iteration step over the training set. For
each datum, they use the current weight vector to
make a prediction which is compared to the correct
label. The weight vector is updated as a function
of the difference. We compared two different up-
date rules: the perceptron update and the MIRA up-
date. In the perceptron update, for an incorrect pre-
diction, the weight vector is modified by adding a
multiple of the difference between the feature vector
of the correct label and the feature vector of the pre-
dicted label. We use the adaptation of this algorithm
to structure prediction, first proposed by (Collins,
2002). TheMIRA update is a proposed improvement
that attempts to make the minimal modification to
the weight vector such that the score of the incorrect
prediction for the example is lower than the score of
the correct label (Crammer and Singer, 2001).
We compare the performance of the perceptron
and MIRA algorithms on 10-fold cross-validation
on the RTE2 dev dataset. Both algorithms improve
with each pass over the dataset. Most improve-
ment is within the first five passes. Table 1 shows
runs for both algorithms over 10 passes through the
dataset. MIRA consistently outperforms perceptron
learning. Moreover, scoring alignments based on the
learned weights marginally outperforms our hand-
constructed scoring function by 1.7% absolute.
A puzzling problem is that our overall per-
formance decreased 0.87% with the addition of
Perfectly aligned
Individual words Text/hypothesis pairs
Perceptron 4675 271
MIRA 4775 283
Table 1: Perceptron and MIRA results on 10-fold cross-
validation on RTE2 dev for 10 passes.
RTE3 dev alignment data. We believe this is due
to a larger proportion of ?irrelevant? and ?relation?
pairs. Irrelevant pairs are those where the text and
hypothesis are completely unrelated. Relation pairs
are those where the correct entailment judgment re-
lies on the extraction of relations such as X works
for Y, X is located in Y, or X is the wife of Y. Both
of these categories do not rely on alignments for en-
tailment decisions, and hence introduce noise.
4 Coreference
In RTE3, 135 pairs in RTE3 dev and 117 in
RTE3 test have lengths classified as ?long,? with
642 personal pronouns identified in RTE3 dev and
504 in RTE3 test. These numbers suggest that re-
solving pronomial anaphora plays an important role
in making good entailment decisions. For exam-
ple, identifying the first ?he? as referring to ?Yunus?
in this pair from RTE3 dev can help alignment and
other system features.
P: Yunus, who shared the 1.4 million prize Friday with the
Grameen Bank that he founded 30 years ago, pioneered the con-
cept of ?microcredit.?
H: Yunus founded the Grameen Bank 30 years ago.
Indeed, 52 of the first 200 pairs from RTE3 dev
were deemed by a human evaluator to rely on ref-
erence information. We used the OpenNLP1 pack-
age?s maximum-entropy coreference utility to per-
form resolution on parse trees and named-entity data
from our system. Found relations are stored and
used by the alignment stage for word similarity.
We evaluated our system with and without coref-
erence over RTE3 dev and RTE3 test. Results are
shown in Table 3. The presence of reference infor-
mation helped, approaching significance on the de-
velopment set (p < 0.1, McNemar?s test, 2-tailed),
but not on the test set. Examination of alignments
and features between the two runs shows that the
alignments do not differ significantly, but associated
1http://opennlp.sourceforge.net/
167
weights do, thus affecting entailment threshold tun-
ing. We believe coreference needs to be integrated
into all the featurizers and lexical resources, rather
than only with word matching, in order to make fur-
ther gains.
5 Semgrex Language
A core part of an entailment system is the ability to
find semantically equivalent patterns in text. Pre-
viously, we wrote tedious graph traversal code by
hand for each desired pattern. As a remedy, we
wrote Semgrex, a pattern language for dependency
graphs. We use Semgrex atop the typed dependen-
cies from the Stanford Parser (de Marneffe et al,
2006b), as aligned in the alignment phase, to iden-
tify both semantic patterns in a single text and over
two aligned pieces of text. The syntax of the lan-
guage was modeled after tgrep/Tregex, query lan-
guages used to find syntactic patterns in trees (Levy
and Andrew, 2006). This speeds up the process of
graph search and reduces errors that occur in com-
plicated traversal code.
5.1 Semgrex Features
Rather than providing regular expression match-
ing of atomic tree labels, as in most tree pattern
languages, Semgrex represents nodes as a (non-
recursive) attribute-value matrix. It then uses regular
expressions for subsets of attribute values. For ex-
ample, {word:run;tag:/?NN/} refers to any
node that has a value run for the attribute word and
a tag that starts with NN, while {} refers to any node
in the graph.
However, the most important part of Semgrex is
that it allows you to specify relations between nodes.
For example, {} <nsubj {} finds all the depen-
dents of nsubj relations. Logical connectives can
be used to form more complex patterns and node
naming can help retrieve matched nodes from the
patterns. Four base relations, shown in figure 1, al-
low you to specify the type of relation between two
nodes, in addition to an alignment relation (@) be-
tween two graphs.
5.2 Entailment Patterns
A particularly useful application of Semgrex is to
create relation entailment patterns. In particular, the
IE subtask of RTE has many characteristics that are
Semgrex Relations
Symbol #Description
{A} >reln {B} A is the governor of a reln relation
with B
{A} <reln {B} A is the dependent of a reln relation
with B
{A} >>reln {B} A dominates a node that is the
governor of a reln relation with B
{A} <<reln {B} A is the dependent of a node that is
dominated by B
{A} @ {B} A aligns to B
Figure 1: Semgrex relations between nodes.
not well suited to the core alignment features of our
system. We began integrating Semgrex into our sys-
tem by creating semantic alignment rules for these
IE tasks.
T: Bill Clinton?s wife Hillary was in Wichita today, continuing
her campaign.
H: Bill Clinton is married to Hillary. (TRUE)
Pattern:
({}=1
<nsubjpass ({word:married} >pp to {}=2))
@ ({} >poss ({lemma:/wife/} >appos {}=3))
This is a simplified version of a pattern that looks
for marriage relations. If it matches, additional pro-
grammatic checks ensure that the nodes labeled 2
and 3 are either aligned or coreferent. If they are,
then we add a MATCH feature, otherwise we add a
MISMATCH. Patterns included other familial rela-
tions and employer-employee relations. These pat-
terns serve both as a necessary component of an IE
entailment system and as a test drive of Semgrex.
5.3 Range of Application
Our rules for marriage relations correctly matched
six examples in the RTE3 development set and one
in the test set. Due to our system?s weaker per-
formance on the IE subtask of the data, we ana-
lyzed 200 examples in the development set for Sem-
grex applicability. We identified several relational
classes, including the following:
? Work: works for, holds the position of
? Location: lives in, is located in
? Relative: wife/husband of, are relatives
? Membership: is an employee of, is part of
? Business: is a partner of, owns
? Base: is based in, headquarters in
These relations make up at least 7% of the data, sug-
gesting utility from capturing other relations.
168
6 Natural Logic
We developed a computational model of natural
logic, the NatLog system, as another inference en-
gine for our RTE system. NatLog complements our
core broad-coverage system by trading lower recall
for higher precision, similar to (Bos and Markert,
2006). Natural logic avoids difficulties with translat-
ing natural language into first-order logic (FOL) by
forgoing logical notation and model theory in favor
of natural language. Proofs are expressed as incre-
mental edits to natural language expressions. Edits
represent conceptual contractions and expansions,
with truth preservation specified natural logic. For
further details, we refer the reader to (Sa?nchez Va-
lencia, 1995).
We define an entailment relation v between
nouns (hammer v tool), adjectives (deafening v
loud), verbs (sprint v run), modifiers, connectives
and quantifiers. In ordinary (upward-monotone)
contexts, the entailment relation between compound
expressions mirrors the entailment relations be-
tween their parts. Thus tango in Paris v dance
in France, since tango v dance and in Paris v in
France. However, many linguistic constructions cre-
ate downward-monotone contexts, including nega-
tion (didn?t sing v didn?t yodel), restrictive quanti-
fiers (few beetles v few insects) and many others.
NatLog uses a three-stage architecture, compris-
ing linguistic pre-processing, alignment, and entail-
ment classification. In pre-processing, we define a
list of expressions that affect monotonicity, and de-
fine Tregex patterns that recognize each occurrence
and its scope. This monotonicity marking can cor-
rectly account for multiple monotonicity inversions,
as in no soldier without a uniform, and marks each
token span with its final effective monotonicity.
In the second stage, word alignments from our
RTE system are represented as a sequence of atomic
edits over token spans, as entailment relations
are described across incremental edits in NatLog.
Aligned pairs generate substitution edits, unaligned
premise words yield deletion edits, and unaligned
hypothesis words yield insertion edits. Where pos-
sible, contiguous sequences of word-level edits are
collected into span edits.
In the final stage, we use a decision-tree classi-
fier to predict the elementary entailment relation (ta-
relation symbol in terms of v RTE
equivalent p = h p v h, h v p yes
forward p < h p v h, h 6v p yes
reverse p = h h v p, p 6v h no
independent p # h p 6v h, h 6v p no
exclusive p | h p v ?h, h v ?p no
Table 2: NatLog?s five elementary entailment relations. The last
column indicates correspondences to RTE answers.
ble 2) for each atomic edit. Edit features include
the type, effective monotonicity at affected tokens,
and their lexical features, including syntactic cate-
gory, lemma similarity, and WordNet-derived mea-
sures of synonymy, hyponymy, and antonymy. The
classifier was trained on a set of 69 problems de-
signed to exercise the feature space, learning heuris-
tics such as deletion in an upward-monotone context
yields<, substitution of a hypernym in a downward-
monotone context yields =, and substitution of an
antonym yields |.
To produce a top-level entailment judgment, the
atomic entailment predictions associated with each
edit are composed in a fairly obvious way. If r is any
entailment relation, then (= ? r) ? r, but (# ? r) ?
#. < and= are transitive, but (< ? =) ? #, and so
on.
We do not expect NatLog to be a general-purpose
solution for RTE problems. Many problems depend
on types of inference that it does not address, such
as paraphrase or relation extraction. Most pairs have
large edit distances, and more atomic edits means
a greater chance of errors propagating to the final
output: given the entailment composition rules, the
system can answer yes only if all atomic-level pre-
dictions are either< or =. Instead, we hope to make
reliable predictions on a subset of the RTE problems.
Table 3 shows NatLog performance on RTE3. It
makes positive predictions on few problems (18%
on development set, 24% on test), but achieves good
precision relative to our RTE system (76% and 68%,
respectively). For comparison, the FOL-based sys-
tem reported in (Bos and Markert, 2006) attained a
precision of 76% on RTE2, but made a positive pre-
diction in only 4% of cases. This high precision sug-
gests that superior performance can be achieved by
hybridizing NatLog with our core RTE system.
The reader is referred to (MacCartney and Man-
169
ID Premise(s) Hypothesis Answer
518 The French railway company SNCF is cooperating in
the project.
The French railway company is called SNCF. yes
601 NUCOR has pioneered a giant mini-mill in which steel
is poured into continuous casting machines.
Nucor has pioneered the first mini-mill. no
Table 4: Illustrative examples from the RTE3 test suite
RTE3 Development Set (800 problems)
System % yes precision recall accuracy
Core +coref 50.25 68.66 66.99 67.25
Core -coref 49.88 66.42 64.32 64.88
NatLog 18.00 76.39 26.70 58.00
Hybrid, bal. 50.00 69.75 67.72 68.25
Hybrid, opt. 55.13 69.16 74.03 69.63
RTE3 Test Set (800 problems)
System % yes precision recall accuracy
Core +coref 50.00 61.75 60.24 60.50
Core -coref 50.00 60.25 58.78 59.00
NatLog 23.88 68.06 31.71 57.38
Hybrid, bal. 50.00 64.50 62.93 63.25
Hybrid, opt. 54.13 63.74 67.32 63.62
Table 3: Performance on the RTE3 development and test sets.
% yes indicates the proportion of yes predictions made by the
system. Precision and recall are shown for the yes label.
ning, 2007) for more details on NatLog.
7 System Results
Our core systemmakes yes/no predictions by thresh-
olding a real-valued inference score. To construct
a hybrid system, we adjust the inference score by
+x if NatLog predicts yes, ?x otherwise. x is cho-
sen by optimizing development set accuracy when
adjusting the threshold to generate balanced predic-
tions (equal numbers of yes and no). As another
experiment, we fix x at this value and adjust the
threshold to optimize development set accuracy, re-
sulting in an excess of yes predictions. Results for
these two cases are shown in Table 3. Parameter
values tuned on development data yielded the best
performance. The optimized hybrid system attained
an absolute accuracy gain of 3.12% over our RTE
system, corresponding to an extra 25 problems an-
swered correctly. This result is statistically signifi-
cant (p < 0.01, McNemar?s test, 2-tailed).
The gain cannot be fully attributed to NatLog?s
success in handling the kind of inferences about
monotonicity which are the staple of natural logic.
Indeed, such inferences are quite rare in the RTE
data. Rather, NatLog seems to have gained primarily
by being more precise. In some cases, this precision
works against it: NatLog answers no to problem 518
(table 4) because it cannot account for the insertion
of called. On the other hand, it correctly rejects the
hypothesis in problem 601 because it cannot account
for the insertion of first, whereas the less-precise
core system was happy to allow it.
Acknowledgements
This material is based upon work supported in
part by the Disruptive Technology Office (DTO)?s
AQUAINT Phase III Program.
References
Johan Bos and Katja Markert. 2006. When logical inference
helps determining textual entailment (and when it doesn?t).
In Proceedings of the Second PASCAL RTE Challenge.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of EMNLP-2002.
Koby Crammer and Yoram Singer. 2001. Ultraconservative
online algorithms for multiclass problems. In Proceedings
of COLT-2001.
Marie-Catherine de Marneffe, Bill MacCartney, Trond Grena-
ger, Daniel Cer, Anna Rafferty, and Christopher D. Manning.
2006a. Learning to distinguish valid textual entailments. In
Second Pascal RTE Challenge Workshop.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006b. Generating typed dependency
parses from phrase structure parses. In 5th Int. Conference
on Language Resources and Evaluation (LREC 2006).
Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts,
Bryan Rink, and Ying Shi. 2006. Recognizing textual entail-
ment with LCC?s GROUNDHOG system. In Proceedings of
the Second PASCAL RTE Challenge.
Roger Levy and Galen Andrew. 2006. Tregex and Tsurgeon:
tools for querying and manipulating tree data structures. In
Proceedings of the Fifth International Conference on Lan-
guage Resources and Evaluation.
Bill MacCartney and Christopher D. Manning. 2007. Natu-
ral logic for textual inference. In ACL Workshop on Textual
Entailment and Paraphrasing.
Rajat Raina, Andrew Y. Ng, and Christopher D. Manning. 2005.
Robust textual inference via learning and abductive reason-
ing. In AAAI 2005, pages 1099?1105.
Victor Sa?nchez Valencia. 1995. Parsing-driven inference: Nat-
ural logic. Linguistic Analysis, 25:258?285.
170
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 193?200,
Prague, June 2007. c?2007 Association for Computational Linguistics
Natural Logic for Textual Inference
Bill MacCartney
Stanford University
wcmac@cs.stanford.edu
Christopher D. Manning
Stanford University
manning@cs.stanford.edu
Abstract
This paper presents the first use of a com-
putational model of natural logic?a sys-
tem of logical inference which operates
over natural language?for textual infer-
ence. Most current approaches to the PAS-
CAL RTE textual inference task achieve ro-
bustness by sacrificing semantic precision;
while broadly effective, they are easily con-
founded by ubiquitous inferences involving
monotonicity. At the other extreme, systems
which rely on first-order logic and theorem
proving are precise, but excessively brittle.
This work aims at a middle way. Our system
finds a low-cost edit sequence which trans-
forms the premise into the hypothesis; learns
to classify entailment relations across atomic
edits; and composes atomic entailments into
a top-level entailment judgment. We pro-
vide the first reported results for any system
on the FraCaS test suite. We also evaluate
on RTE3 data, and show that hybridizing an
existing RTE system with our natural logic
system yields significant performance gains.
1 Introduction
The last five years have seen a surge of interest in
the problem of textual inference, that is, automat-
ically determining whether a natural-language hy-
pothesis can be inferred from a given premise. A
broad spectrum of approaches have been explored,
ranging from shallow-but-robust to deep-but-brittle.
Up to now, the most successful approaches have
used fairly impoverished semantic representations,
relying on measures of lexical or semantic overlap
(Jijkoun and de Rijke, 2005), pattern-based relation
extraction (Romano et al, 2006), or approximate
matching of predicate-argument structure (Hickl et
al., 2006). Such methods, while robust and broadly
effective, are imprecise, and are easily confounded
by ubiquituous inferences involving monotonicity,
particularly in negative polarity contexts, as in:
P: No case of indigenously acquired rabies
infection has been confirmed in the past 2 years.
H: No rabies cases have been confirmed.
Because it drops important qualifiers in a negative
context, the hypothesis does not follow; yet both the
lexical content and the predicate-argument structure
of the hypothesis closely match the premise.
At the other extreme, textual inference can be ap-
proached as deduction, building on work in formal
computational semantics to translate sentences into
first-order logic (FOL), and then applying a theo-
rem prover or a model builder (Akhmatova, 2005;
Fowler et al, 2005). However, such approaches
tend to founder on the difficulty of accurately trans-
lating natural language in FOL?tricky issues in-
clude idioms, intensionality and propositional at-
titudes, modalities, temporal and causal relations,
certain quantifiers, and so on. FOL-based systems
that have attained high precision (Bos and Markert,
2006) have done so at the cost of very poor recall.
In this work, we explore a different point on the
spectrum, by developing a computational model of
natural logic, that is, a logic whose vehicle of in-
ference is natural language.1 Natural logic eschews
logical notation and model theory. Its proofs pro-
ceed by incremental edits to expressions of natural
language, and its inference rules specify conditions
under which semantic expansions or contractions
preserve truth. It thus permits us to do precise rea-
soning about monotonicity, while sidestepping the
difficulties of translating sentences into FOL.
It should be emphasized that there are many
1Natural logic should not be confused with natural deduc-
tion, a proof system for first-order logic.
193
important kinds of inference which are not ad-
dressed by a natural logic system, including tem-
poral reasoning, causal reasoning (Khan sold nu-
clear plans ? Khan possessed nuclear plans), para-
phrase (McEwan flew to Rome ? McEwan took a
flight to Rome), relation extraction (Bill Gates and
his wife, Melinda... ? Melinda Gates is married
to Bill Gates), etc. Moreover, a natural logic sys-
tem will struggle with inferences requiring model-
building or deep proof search, which are more suit-
able for formal deduction systems. However, the ap-
plicability of natural logic is broader than it might at
first appear, and a natural logic system can be de-
signed to integrate with other kinds of reasoners.
2 Foundations of natural logic
Natural logic aims to explain inferences involving
monotonicity, in which the concepts or constraints
expressed are expanded or contracted. Consider, for
example, the sentence Every meal without wine is a
terrible crime. Some semantic elements can be ex-
panded (but not contracted) salva veritate, and are
therefore said to have positive polarity: wine may be
broadened to drink, terrible crime may be relaxed to
crime, or every may be weakened to some. Other el-
ements can only be contracted (not expanded) salva
veritate, and thus have negative polarity: meal can
be narrowed to dinner. The monotonicity calcu-
lus developed in (Sa?nchez Valencia, 1991) explains
these polarity effects by (1) defining an entailment
relation over multifarious expressions of natural lan-
guage, (2) defining monotonicity properties of se-
mantic functions, and finally (3) specifying how
monotonicities combine during Fregean composi-
tion of semantic functions.
The entailment relation. Most work in textual
inference reflects a simple concept of entailment:
one sentence entails another, or does not. In nat-
ural logic, however, entailment is a semantic con-
tainment relation (analogous to the set containment
relation ?) over expressions of all types, including
words and phrases as well as sentences. We define
the entailment relation v recursively over the se-
mantic types familiar from Montague semantics. If
c and d are of type t (truth values), then c v d iff
c ? d. If c and d are of type e (entities), then c v d
iff c = d. Finally, if c and d are of functional type
??, ??, then c v d iff for all a ? ?, c(a) v d(a).
Otherwise, if c 6v d and d 6v c, we write c # d.
Using these formal definitions, we can establish
entailment relations between common nouns (pen-
guin v bird), common and proper adjectives (tiny v
small, French v European), transitive and intransi-
tive verbs (kick v strike, hover v fly), temporal and
locative modifiers (this morning v today, in Beijing
v in China), connectives (and v or), and quanti-
fiers (everyone v someone, all v most v some).2
Among noun phrases, we have everyone v Einstein
v some physicist. Finally, observe that dropping a
modifier generally yields entailment (eat quickly v
eat) though this heuristic can be violated, e.g., by
operator adjectives (fake vaccine 6v vaccine).
Monotonicity. Under the Fregean hypothesis, the
meaning of a compound expression is the result of
function application. In semantics as in mathemat-
ics, we can describe a function as upward mono-
tone if ?larger? inputs yield larger outputs. Formally,
given a function f of functional type ??, ??:
? f is upward-monotone (?) iff for all x, y ? ?,
x v y entails f(x) v f(y).
? f is downward-monotone (?) iff for all x, y ?
?, x v y entails f(y) v f(x).
? f is non-monotone ( 6??) iff it is neither upward-
nor downward-monotone.
Most linguistic expressions may be regarded as
upward-monotone semantic functions. Thus tango
in Paris v dance in France, since tango v
dance and in Paris v in France. However, a
number of important linguistic constructions are
downward-monotone, including negation (not), re-
strictive quantifiers (no, few, at most n), restrictive
verbs (lack, fail, prohibit), certain adverbs (without,
except), the antecedent of a conditional, and so on.
We thus have didn?t dance v didn?t tango, few ath-
letes v few sprinters, lack weapons v lack guns,
2The entailment relations among quantifiers may be coun-
terintuitive to those prone to what Peter Geach called ?quantifi-
catious thinking?, who might consider someone ?smaller? than
everyone. But in the theory of generalized quantifiers, the deno-
tation of a quantified noun phrase is the set of predicates which
it satisfies, and the predicates satisfied by everyone are a subset
of those satisfied by someone. Note also that logicians will deny
that the universal entails the existential: ?x P (x) 6? ?x P (x).
However, most people are happy to infer someone is hungry
from everyone is hungry.
194
without clothes v without pants, and If stocks rise,
we win v If stocks soar, we win. Finally, a few
expressions must be considered non-monotone, in-
cluding superlative adjectives and quantifiers such
as most. Thus prettiest butterfly # prettiest insect
and most boats # most vehicles. Note that certain
generalized quantifiers must be treated as binary
functions having different monotonicities in differ-
ent arguments. Thus every is downward-monotone
in its first argument (every fish swims v every shark
swims) but upward-monotone in its second argument
(every shark swims v every shark moves).
Composition of monotonicity. Finally, we must
specify howmonotonicities combine during Fregean
composition of semantic functions. In Sa?nchez Va-
lencia?s marking algorithm, we represent each input
expression as a parse in the Lambek categorial gram-
mar. We then (1) mark leaf nodes with appropriate
lexical monotonicity values, (2) project monotonic-
ities to internal nodes representing function applica-
tions, and finally (3) compose monotonicities along
the path from the root to each leaf in order to deter-
mine effective polarities. The composition of mono-
tonicities is straightforward. Suppose h = f ? g. If
either f or g is non-monotone, then so is h. Other-
wise, if the monotonicities of f and g are the same,
then h is upward-monotone; if they are different,
then h is downward-monotone. (Thus, wine has pos-
itive polarity in no meal without wine because it falls
under two downward-monotone operators.)
3 The NatLog System
Our natural logic system, dubbed the NatLog sys-
tem, has a three-stage architecture similar to those
in (Marsi and Krahmer, 2005; MacCartney et al,
2006), comprising (1) linguistic pre-preprocessing,
(2) alignment, and (3) entailment classification.
3.1 Linguistic pre-processing
Relative to other textual inference systems, the Nat-
Log system does comparatively little linguistic pre-
processing. We rely on the Stanford parser (Klein
and Manning, 2003), a Treebank-trained statistical
parser, for tokenization, part-of-speech tagging, and
phrase-structure parsing. By far the most impor-
tant analysis performed at this stage is monotonicity
marking, in which we compute the effective mono-
unary operator: without
pattern: IN < /?[Ww]ithout\$/
argument 1: monotonicity ? on dominating PP
pattern: __ > PP=proj
binary operator: most
pattern: JJS < /?[Mm]ost\$/ !> QP
argument 1: monotonicity 6?? on dominating NP
pattern: __ >+(NP) (NP=proj !> NP)
argument 2: monotonicity ? on dominating S
pattern: __ >+(/.*/) (S=proj !> S)
Figure 1: Two examples of monotonicity operator
definitions. The patterns employ Tregex syntax.
tonicity for each token span in each input sentence.
For this, we use an adaptation of the marking algo-
rithm of Sa?nchez Valencia (section 2); however, our
choice of a Treebank-trained parser (driven by the
goal of broad coverage) requires us to modify the
algorithm substantially. Unlike the categorial gram-
mar parses assumed by Sa?nchez Valencia, the nest-
ing of constituents in phrase-structure parses does
not always correspond to the composition of seman-
tic functions, which introduces a number of com-
plications. We define a list of downward-monotone
and non-monotone expressions, and for each item
we specify its arity and a Tregex pattern (Levy and
Andrew, 2006) which permits us to identify its oc-
currences. We also specify, for each argument, both
the monotonicity and another Tregex pattern which
helps us to determine the sentence span over which
the monotonicity is projected. (Figure 1 shows
some example definitions.) The marking process
computes these projections, performs monotonicity
composition where needed, and marks each token
span with its final effective monotonicity.
3.2 Alignment
The second stage of processing establishes an align-
ment between the premise and the hypothesis. While
there are many notions of alignment, in this work we
have chosen to represent alignments as sequences of
atomic edits over spans of word tokens. We define
four types of atomic edits: deletion of a span from
the premise, insertion of a span into the hypothesis,
substitution of a hypothesis span for a premise span,
and advance over a span without modification. Each
atomic edit is parameterized by the token indices at
which it operates. As an example, the first problem
195
in table 3 may be aligned using following edits:
An Irishman =? An Irishman ADV
won =? won ADV
a =? the SUB
Nobel prize =? Nobel prize ADV
=? for literature INS
. =? . ADV
Clearly, this representation imposes certain lim-
itations: there is no atomic edit type representing
the movement of a token span from one sentence
location to another (instead a combination of dele-
tion and insertion must be used), and there can be no
alignments to non-contiguous sets of tokens. How-
ever, the span edit representation also offers impor-
tant benefits. First, there is always a well-defined
sequence of intermediate forms through which the
sentence progresses during editing, which is impor-
tant for the computation of monotonicity features.
Second, given a cost function over edits, it is possi-
ble to construct an efficient dynamic program to find
the lowest-cost edit sequence between any pair of
sentences, using a straightforward extension of the
Levenshtein string-edit algorithm.
For this purpose, we have designed a cost function
which prefers edits which operate on longer spans;
penalizes edits operating on spans which are not
parse-tree constituents; imposes nomimal cost on
substitutions of words having the same lemma; and
imposes little cost on certain ?light? edits, involving
prepositions, articles, auxiliaries, etc. When applied
to problems like those in the FraCaS test suite (sec-
tion 4), this cost model gives intuitively pleasing re-
sults. However, because our focus in this work is on
entailment, we have not devoted much energy to op-
timizing our alignment model, and will not discuss it
further. (For the RTE experiments described in sec-
tion 5, we use alignments derived from an indepen-
dent RTE system. Translating those alignments into
the span edit representation requires relaxing some
of its constraints, as we?ll explain.)
3.3 Entailment classification
The edit sequence obtained during the alignment
stage effectively decomposes the global entailment
problem into a sequence of atomic entailment prob-
lems, one for each atomic edit. In the final stage, we
train a model for atomic entailment classification,
and predict an entailment relation for each atomic
relation symbol in terms of v FraCaS RTE
equivalent p = h p v h, h v p yes yes
forward p @ h p v h, h 6v p yes yes
reverse p A h h v p, p 6v h unk no
independent p # h p 6v h, h 6v p unk no
exclusive p | h p v ?h no no
Table 1: The five elementary entailment relations.
The last two columns indicate correspondences to
FraCaS and RTE answers; see sections 4 and 5.
edit. We then compose our atomic entailment pre-
dictions to produce a global entailment prediction.
The atomic entailment model uses a classifier to
predict one of five elementary entailment relations
(table 1) for each atomic edit. This model uses a
feature representation designed to capture character-
istics of the edit pertinent to a natural logic analysis:
the type of the edit (DEL, INS, or SUB), the effec-
tive monotonicity at the affected token span (?, ?, or
6??), and various lexical features of the affected to-
kens. In the case of a SUB edit, the lexical features
help to indicate whether the substitution constitutes
a semantic expansion, contraction, equivalence, or
exclusion, using WordNet-derived measures of syn-
onymy, hyponymy, and antonymy, and a measure
of lemma similarity based on Levenshtein string-
edit distance. In addition, for edits of all types, we
have found it useful to generate a ?light edit? fea-
ture indicating whether the affected tokens belong to
categories which are usually negligible for inferen-
tial purposes, including prepositions, articles, auxil-
iaries, and punctuation.
The entailment model uses a decision tree clas-
sifier, trained on a small data set of 69 problems
custom-designed to exercise diverse regions of the
feature space.3 From these examples, the decision
tree effectively learns such heuristics as deletion in
an upward-monotone context yields @, substitution
of a hypernym in a downward-monotone context
yields A, and substitution of an antonym yields |.
To produce a top-level entailment judgment, the
atomic entailment predictions associated with each
3Thus, in using learning, we are not trying to estimate statis-
tical properties of some natural distribution of data. Rather, the
learning framework provides (1) a modular way to add features
which may impact the entailment decision, (2) a principled way
to combine evidence from diverse features, such as real-valued
lexical features, and (3) a convenient way to verify the proper
functioning of the system.
196
atomic edit: SUB(a, the)
features:
type: SUB, monotonicity: ?, isLightEdit: true,
wnSyno: 0.0, wnHypo: 0.0, wnAnto: 0.0, lemmaSim: 0.0
predicted entailment relation: =
atomic edit: INS(for literature)
features:
type: INS, monotonicity: ?, isLightEdit: false
predicted entailment relation: A
top-level inference:
composition of entailment relations: = ? A? A
mapping to FraCaS answer: A? unk
Figure 2: The operation of the entailment model on
FraCaS problem 33 (see table 3).
edit are composed in a fairly obvious way. If r is any
entailment relation, then = ? r ? r, but # ? r ? #.
@ and A are transitive, but @ ? A ? #, and so on.
Compositions are commutative and associative.
Figure 2 shows an example of the operation of the
entailment model.
4 Experiments with the FraCaS test suite
The FraCaS test suite (Cooper et al, 1996) was de-
veloped as part of a collaborative research effort in
computational semantics. It contains 346 inference
problems reminiscent of a textbook on formal se-
mantics. In the authors? view, ?inferencing tasks
[are] the best way of testing an NLP system?s se-
mantic capacity.? Yet, to our knowledge, this work
is the first to present a quantitative system evaluation
using FraCaS.4
The problems are divided into nine sections, each
focused on a category of semantic phenomena, such
as quantifiers or anaphora (see table 2). Each prob-
lem consists of one or more premise sentences, fol-
lowed by a one-sentence question. For this project,
the questions were converted into declarative hy-
potheses. Each problem also has an answer, which
(usually) takes one of three values: yes (the hypoth-
esis can be inferred from the premise(s)), no (the
negation of the hypothesis can be inferred), or unk
(neither the hypothesis nor its negation can be in-
ferred). Some examples are shown in table 3.
4Indeed, our first step was to put the FraCaS data into
machine-readable form, which we make publicly available at
http://nlp.stanford.edu/?wcmac/downloads/ fracas.xml.
? Category Count % Acc.
1 Quantifiers 44 84.09
2 Plurals 24 41.67
3 Anaphora 6 50.00
4 Ellipsis 25 28.00
5 Adjectives 15 60.00
6 Comparatives 16 68.75
7 Temporal 36 61.11
8 Verbs 8 62.50
9 Attitudes 9 55.56
Applicable sections: 1, 5, 6 75 76.00
All sections 183 59.56
Table 2: NatLog?s accuracy on the FraCaS test suite,
by section. We exclude degenerate problems and
multiple-premise problems; see text.
Not all of the 346 problems were used in this
work. First, 12 of the problems were excluded
because they are degenerate, lacking either a hy-
pothesis or a well-defined answer. Second, an
additional 151 problems (about 45% of the to-
tal) were excluded because they involve multiple
premises. While many of the multiple-premise prob-
lems should be feasible for NatLog in the future,
such inferences require search, and for now we have
chosen to sidestep this complexity.
Finally, it should be noted that several sections of
the test suite involve semantic phenomena, such as
ellipsis, which the NatLog system makes no attempt
to model. While we report results for these sections,
we do not expect performance to be good, and in
development we have concentrated on the sections
where we expect NatLog to have relevant expertise.
In table 2, results for these sections are aggregated
under the label ?applicable sections?.
Results are shown in table 2. On the ?applica-
ble? sections, performance is good. (Not supris-
ingly, we make little headway with, e.g., ellipsis.)
Of course, this does not constitute a proper evalua-
tion on unseen test data?but on the other hand, the
system was never trained on the FraCaS problems,
and has had no opportunity to learn biases implicit
in the data.5 Our main goal in testing on FraCaS is
to evaluate the representational and inferential ade-
quacy of our model of natural logic, and from that
perspective, the strong performance in quantifiers,
5This also explains why NatLog?s performance on some
FraCaS sections falls below that of a baseline most-common-
label classifier.
197
? ID Premise(s) Hypothesis Ans
1 33 An Irishman won a Nobel prize. An Irishman won the Nobel prize for literature. unk
1 38 No delegate finished the report. Some delegate finished the report on time. no
2 99 Clients at the demonstration were all impressed by the sys-
tem?s performance. Smith was a client at the demonstration.
Smith was impressed by the system?s perfor-
mance.
yes
9 335 Smith believed that ITEL had won the contract in 1992. ITEL won the contract in 1992. unk
Table 3: Illustrative examples from the FraCaS test suite
guess
answer yes unk no total
yes 62 40 ? 102
unk 15 45 ? 60
no 6 13 2 21
total 90 91 2 183
Table 4: Confusions on FraCaS data (all sections)
adjectives, and comparatives is satisfying.
The confusion matrix shown in table 4 is instruc-
tive. By far the largest category of confusions com-
prise problems where we guess unk when the cor-
rect answer is yes. This reflects both the bias to-
ward yes in the FraCaS data, and the system?s ten-
dency to predict unk (entailment relation #) when
confused: given the composition rules for entail-
ment relations, the system can predict yes only if all
atomic-level predictions are either @ or =. On the
other hand, there are a number of problems where
we predict yes mistakenly. Several of these errors
arise in a series of problems in ?5 which concern
operator adjectives such as former. The entailment
model wrongly assumes that such modifiers, like any
others, can safely be deleted in upward-monotone
contexts, but in fact former student 6v student. If
the feature set used by the entailment model were
extended to represent occurrences of operator adjec-
tives, and if appropriate examples were included in
the training data, our accuracy in ?5?and the av-
erage accuracy for the ?applicable? sections?could
easily be boosted over 80%.
5 Experiments with RTE data
Textual inference problems from the PASCAL RTE
Challenge (Dagan et al, 2005) differ from FraCaS
problems in several important ways. (See table 5
for examples.) Instead of textbook examples of se-
mantic phenomena, RTE problems are more natural-
seeming, with premises collected ?in the wild? from
newswire text. The premises are much longer, aver-
aging 35 words (vs. 11 words for FraCaS). Also, the
RTE task aims at a binary classification: the RTE no
answer combines the no and unk answers in FraCaS.
Due to the character of RTE problems, we do not
expect NatLog to be a good general-purpose solu-
tion to solving RTE problems. First, most RTE prob-
lems depend on forms of inference, such as para-
phrase, temporal reasoning, or relation extraction,
which NatLog is not designed to address. Second,
in most RTE problems, the edit distance between
premise and hypothesis is relatively large. More
atomic edits means a greater chance that prediction
errors made by the atomic entailment model will
propagate, via entailment composition, to the sys-
tem?s final output. Rather, in applying NatLog to
RTE, we hope to make reliable predictions on a sub-
set of RTE problems, trading recall for precision. If
we succeed, then we may be able to hybridize with a
broad-coverage RTE system to obtain better results
than either system individually?the same strategy
that was adopted by (Bos and Markert, 2006) for
their FOL-based system.
For this purpose, we have chosen to use the Stan-
ford RTE system described in (de Marneffe et al,
2006). In applying NatLog to RTE problems, we use
alignments from the Stanford system as input to our
entailment model. A Stanford alignment is a map
from hypothesis words to premise words. When we
translate such alignments into the NatLog represen-
tation described in section 3, each pair of aligned
words generates a substitution edit (or, if the words
are identical, an advance edit). Unaligned premise
words yield deletion edits, while unaligned hypothe-
sis words yield insertion edits. Where possible, con-
tiguous sequences of word-level edits are then col-
lected into equivalent span edits. While the result
of this translation method cannot be interpreted as a
conventional edit script (there is no well-defined or-
198
ID Premise(s) Hypothesis Answer
518 The French railway company SNCF is cooperating in
the project.
The French railway company is called SNCF. yes
601 NUCOR has pioneered a giant mini-mill in which steel
is poured into continuous casting machines.
Nucor has pioneered the first mini-mill. no
Table 5: Illustrative examples from the RTE3 test suite
RTE3 Development Set (800 problems)
System % yes precision recall accuracy
Stanford 50.25 68.66 66.99 67.25
NatLog 18.00 76.39 26.70 58.00
Hybrid, bal. 50.00 69.75 67.72 68.25
Hybrid, opt. 55.13 69.16 74.03 69.63
RTE3 Test Set (800 problems)
System % yes precision recall accuracy
Stanford 50.00 61.75 60.24 60.50
NatLog 23.88 68.06 31.71 57.38
Hybrid, bal. 50.00 64.50 62.93 63.25
Hybrid, opt. 54.13 63.74 67.32 63.62
Table 6: Performance on the RTE3 development and
test sets. % yes indicates the proportion of yes pre-
dictions made by the system. Precision and recall
are shown for the yes label.
dering of edits, and multiple edits can operate on the
same input spans), we find that this poses no great
impediment to subsequent processing by the entail-
ment model.
Table 6 shows the performance of the NatLog
system on RTE3 data. Relative to the Stanford
RTE system, NatLog achieves high precision on its
yes predictions?about 76% on the development set,
and 68% on the test set?suggesting that hybridizing
may be effective. For comparison, the FOL-based
system reported in (Bos and Markert, 2006) attained
a similarly high precision of 76% on RTE2 prob-
lems, but was able to make a positive prediction in
only about 4% of cases. NatLog makes positive pre-
dictions far more often?at a rate of 18% on the de-
velopment set, and 24% on the test set.
The Stanford RTE system makes yes/no predic-
tions by thresholding a real-valued inference score.
To construct a hybrid system, we adjust the Stan-
ford inference scores by +x or ?x, depending on
whether NatLog predicts yes or no/unk. We choose
the value of x by optimizing development set accu-
racy, while adjusting the threshold to generate bal-
anced predictions (that is, equal numbers of yes and
no predictions). As an additional experiment, we
fix x at this value and then adjust the threshold to
optimize development set accuracy, resulting in an
excess of yes predictions. (Since this optimization
is based solely on development data, its use on test
data is fully legitimate.) Results for these two cases
are shown in table 6. The parameters tuned on devel-
opment data were found to yield good performance
on test data. The optimized hybrid system attained
an absolute accuracy gain of 3.12% over the Stan-
ford system, corresponding to an extra 25 problems
answered correctly. This result is statistically signif-
icant (p < 0.01, McNemar?s test, 2-tailed).
However, the gain cannot be attributed to Nat-
Log?s success in handling the kind of inferences
about monotonicity which are the staple of natural
logic. Indeed, such inferences are quite rare in the
RTE data. Rather, NatLog seems to have gained
primarily by being more precise. In some cases,
this precision works against it: NatLog answers no
to problem 518 (table 5) because it cannot account
for the insertion of called in the hypothesis. On
the other hand, it correctly rejects the hypothesis in
problem 601 because it cannot account for the inser-
tion of first, whereas the less-precise Stanford sys-
tem was happy to allow it.
6 Related work
While the roots of natural logic can be traced back
to Aristotle?s syllogisms, the modern conception of
natural logic began with George Lakoff, who pro-
posed ?a logic for natural language? which could
?characterize all the valid inferences that can be
made in natural language? (Lakoff, 1970). The
study of natural logic was formalized by Johan van
Benthem, who crucially connected it with catego-
rial grammar (van Benthem, 1986), and later was
brought to fruition by Victor Sa?nchez Valencia, who
first gave a precise definition of a calculus of mono-
199
tonicity (Sa?nchez Valencia, 1991). A small current
of theoretical work has continued up to the present,
for example (Zamansky et al, 2006).
There has been surprisingly little work on build-
ing computational models of natural logic. (Fyo-
dorov et al, 2003) describes a Prolog implementa-
tion for a small fragment of English, based on a cat-
egorial grammar parser.6 In an unpublished draft,
(van Eijck, 2005) describes a preliminary implemen-
tation in Haskell.
Doing inference with representations close to nat-
ural language has also been advocated by Jerry
Hobbs, as in (Hobbs, 1985).
To our knowledge, the FraCaS results reported
here represent the first such evaluation. (Sukkarieh,
2003) describes applying a deductive system to
some FraCaS inferences, but does not perform a
complete evaluation or report quantitative results.
7 Conclusion
Our NatLog implementation of natural logic suc-
cessfully handles a broad range of inferences involv-
ing monotonicity, as demonstrated on the FraCaS
test suite. While a post-hoc analysis of performance
on the RTE3 Challenge suggests that monotonicity-
related inferences have limited applicability in RTE
data, the greater precision of the NatLog system nev-
ertheless significantly improved the performance of
a hybrid RTE system. An area for future work is
further consideration of what kinds of inference are
prevalent and important in prominent computational
linguistic applications.
Acknowledgements The authors wish to thank
Marie-Catherine de Marneffe and the anonymous
reviewers for their helpful comments on an earlier
draft of this paper. This work was supported in part
by ARDA?s Advanced Question Answering for In-
telligence (AQUAINT) Program.
References
Elena Akhmatova. 2005. Textual entailment resolution via
atomic propositions. In Proc. of the PASCAL RTE Challenge
Workshop.
Johan Bos and Katja Markert. 2006. When logical inference
helps determining textual entailment (and when it doesn?t).
In Proc. of the 2nd PASCAL RTE Challenge Workshop.
6Available at http://yeda.cs.technion.ac.il/?yaroslav/oc/
Robin Cooper, Dick Crouch, Jan Van Eijck, Chris Fox, Jo-
han Van Genabith, Jan Jaspars, Hans Kamp, David Milward,
Manfred Pinkal, Massimo Poesio, and Steve Pulman. 1996.
Using the framework. Technical Report LRE 62-051 D-16,
The FraCaS Consortium.
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The
PASCAL Recognising Textual Entailment Challenge. In
Proc. of the PASCAL RTE Challenge Workshop.
Marie-Catherine de Marneffe, Bill MacCartney, Trond
Grenager, Daniel Cer, Anna Rafferty, and Christopher D.
Manning. 2006. Learning to distinguish valid textual entail-
ments. In Proc. of the 2nd PASCAL RTE Challenge Work-
shop.
Abraham Fowler, Bob Hauser, Daniel Hodges, Ian Niles,
Adrian Novischi, and Jens Stephan. 2005. Applying CO-
GEX to recognize textual entailment. In Proc. of the PAS-
CAL RTE Challenge Workshop.
Yaroslav Fyodorov, Yoad Winter, and Nissim Francez. 2003.
Order-based inference in natural logic. Logic Journal of the
IGPL, 11(4):385?416.
Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts,
Bryan Rink, and Ying Shi. 2006. Recognizing textual en-
tailment with LCC?s GROUNDHOG system. In Proc. of the
2nd PASCAL RTE Challenge Workshop.
Jerry R. Hobbs. 1985. Ontological promiscuity. In Proc. of
ACL-85, pages 61?69.
Valentin Jijkoun and Maarten de Rijke. 2005. Recognizing
textual entailment using lexical similarity. In Proc. of the
PASCAL RTE Challenge Workshop.
Dan Klein and Christopher D. Manning. 2003. Accurate unlex-
icalized parsing. In Proc. of ACL-03.
George Lakoff. 1970. Linguistics and natural logic. Synthese,
22:151?271.
Roger Levy and Galen Andrew. 2006. Tregex and Tsurgeon:
tools for querying and manipulating tree data structures. In
Proc. of LREC-06.
Bill MacCartney, Trond Grenager, Marie-Catherine de Marn-
effe, Daniel Cer, and Christopher D. Manning. 2006. Learn-
ing to recognize features of valid textual entailments. In
Proc. of HLT-NAACL-06.
Erwin Marsi and Emiel Krahmer. 2005. Classification of se-
mantic relations by humans and machines. In Proc. of the
ACL 2005 Workshop on Empirical Modeling of Semantic
Equivalence and Entailment.
Lorenza Romano, Milen Kouylekov, Idan Szpektor, Ido Da-
gan, and Alberto Lavelli. 2006. Investigating a generic
paraphrase-based approach for relation extraction. In Proc.
of EACL-06.
Victor Sa?nchez Valencia. 1991. Studies on Natural Logic and
Categorial Grammar. Ph.D. thesis, Univ. of Amsterdam.
Jana Z. Sukkarieh. 2003. An expressive efficient representation:
Bridging a gap between NLP and KR. In Proc. of the 7th
Int?l Conf. on Knowledge-Based Intelligent Information and
Engineering Systems.
Johan van Benthem. 1986. Essays in logical semantics. Reidel,
Dordrecht.
Jan van Eijck. 2005. Natural logic for natural language. http:
//homepages.cwi.nl/?jve/papers/05/nlnl/NLNL.pdf .
Anna Zamansky, Nissim Francez, and Yoad Winter. 2006. A
?natural logic? inference system using the Lambek calculus.
Journal of Logic, Language and Information, 15:273?295.
200
Proceedings of the 8th International Conference on Computational Semantics, pages 140?156,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
An extended model of natural logic
Bill MacCartney and Christopher D. Manning
Natural Language Processing Group, Stanford University
Abstract
We propose a model of natural language inference which identifies
valid inferences by their lexical and syntactic features, without full se-
mantic interpretation. We extend past work in natural logic, which has
focused on semantic containment and monotonicity, by incorporating
both semantic exclusion and implicativity. Our model decomposes an
inference problem into a sequence of atomic edits linking premise to hy-
pothesis; predicts a lexical semantic relation for each edit; propagates
these relations upward through a semantic composition tree according
to properties of intermediate nodes; and joins the resulting semantic
relations across the edit sequence. A computational implementation
of the model achieves 70% accuracy and 89% precision on the FraCaS
test suite. Moreover, including this model as a component in an ex-
isting system yields significant performance gains on the Recognizing
Textual Entailment challenge.
1 Introduction
Natural language inference (NLI) is the problem of determining whether
a natural language hypothesis h can reasonably be inferred from a given
premise p. For example:
(1) p: Every firm polled saw costs grow more than expected, even after adjusting for inflation.
h: Every big company in the poll reported cost increases.
A capacity for open-domain NLI is clearly necessary for full natural
language understanding, and NLI can also enable more immediate applica-
tions, such as semantic search and question answering. Consequently, NLI
has been the focus of intense research effort in recent years, centered around
the annual Recognizing Textual Entailment (RTE) competition (6).
For a semanticist, the most obvious approach to NLI relies on full se-
mantic interpretation: first, translate p and h into some formal meaning
representation, such as first-order logic (FOL), and then apply automated
140
reasoning tools to determine inferential validity. While the formal approach
can succeed in restricted domains, it struggles with open-domain NLI tasks
such as RTE. For example, the FOL-based system of (1) was able to find
a proof for less than 4% of the problems in the RTE1 test set. The dif-
ficulty is plain: truly natural language is fiendishly complex. The formal
approach faces countless thorny problems: idioms, ellipsis, paraphrase, am-
biguity, vagueness, lexical semantics, the impact of pragmatics, and so on.
Consider for a moment the difficulty of fully and accurately translating (1)
to a formal meaning representation.
Yet (1) also demonstrates that full semantic interpretation is often not
necessary to determining inferential validity. To date, the most successful
NLI systems have relied on surface representations and approximate mea-
sures of lexical and syntactic similarity to ascertain whether p subsumes h
(9, 13, 10). However, these approaches face a different problem: they lack
the precision needed to properly handle such commonplace phenomena as
negation, antonymy, downward-monotone quantifiers, non-factive contexts,
and the like. For example, if every were replaced by some or most through-
out (1), the lexical and syntactic similarity of h to p would be unaffected,
yet the inference would be rendered invalid.
In this paper, we explore a middle way, by developing a model of what
(11) called natural logic, which characterizes valid patterns of inference in
terms of syntactic forms which are as close as possible to surface forms. For
example, the natural logic approach might sanction (1) by observing that: in
ordinary upward monotone contexts, deleting modifiers preserves truth; in
downward monotone contexts, inserting modifiers preserves truth; and every
is downward monotone in its restrictor NP. Natural logic thus achieves the
semantic precision needed to handle inferences like (1), while sidestepping
the difficulties of full semantic interpretation.
The natural logic approach has a very long history,
1
originating in the
syllogisms of Aristotle and continuing through the medieval scholastics and
the work of Leibniz. It was revived in recent times by (19, 20) and (17),
whose monotonicity calculus explains inferences involving semantic contain-
ment and inversions of monotonicity, even when nested, as in Nobody can
enter without a valid passport |= Nobody can enter without a passport. How-
ever, because the monotonicity calculus lacks any representation of semantic
exclusion, it fails to license many simple inferences, such as Stimpy is a cat
|= Stimpy is not a poodle.
1
For a useful overview of the history of natural logic, see (21). For recent work on
theoretical aspects of natural logic, see (7, 18, 23).
141
Another model which arguably belongs to the natural logic tradition
(though not presented as such) was developed by (15) to explain inferences
involving implicatives and factives, even when negated or nested, as in Ed did
not forget to force Dave to leave |= Dave left. While the model bears some
resemblance to the monotonicity calculus, it does not incorporate semantic
containment or explain interactions between implicatives and monotonicity,
and thus fails to license inferences such as John refused to dance |= John
didn?t tango.
In this paper, we propose a new model of natural logic which extends the
monotonicity calculus to incorporate semantic exclusion, and partly unifies
it with Nairn et al?s account of implicatives. We first define an inventory of
basic semantic relations which includes representations of both containment
and exclusion (section 2). We then describe a general method for establish-
ing the semantic relation between a premise p and a hypothesis h. Given a
sequence of atomic edits which transforms p into h, we determine the lexical
semantic relation generated by each edit (section 4); project each lexical
semantic relation into an atomic semantic relation, according to properties
of the context in which the edit occurs (section 5); and join atomic semantic
relations across the edit sequence (section 3). We have previously presented
an implemented system based on this model (14); here we offer a detailed
account of its theoretical foundations.
2 An inventory of semantic relations
The simplest formulation of the NLI task is as a binary decision prob-
lem: the relation between p and h is to be classified as either entailment
(p |= h) or non-entailment (p 6|= h). The three-way formulation refines this
by dividing non-entailment into contradiction (p |= ?h) and compatibility
(p 6|= h ? p 6|= ?h).
2
The monotonicity calculus carves things up differently:
it interprets entailment as a semantic containment relation ? analogous to
the set containment relation ?, and thus permits us to distinguish forward
entailment (p ? h) from reverse entailment (p ? h). Moreover, it defines ?
for expressions of every semantic type, including not only complete sentences
but also individual words and phrases. Unlike the three-way formulation,
however, it lacks any way to represent contradiction (semantic exclusion).
For our model, we want the best of both worlds: a comprehensive inven-
tory of semantic relations that includes representations of both semantic
2
The first three RTE competitions used the binary formulation, while the three-way
formulation was adopted for RTE4. The three-way formulation was also employed in the
FraCaS test suite (5) and has been investigated in depth by (4).
142
containment and semantic exclusion.
Following Sa?nchez Valencia, we proceed by analogy with set relations.
In a universe U , the set of ordered pairs ?x, y? of subsets of U can be parti-
tioned into 16 equivalence classes, according to whether each of the four sets
x ? y, x ? y, x ? y, and x ? y is empty or non-empty.
3
Of these 16 classes,
nine represent degenerate cases in which either x or y is either empty or
universal. Since expressions having empty denotations (e.g., round square
cupola) or universal denotations (e.g., exists) fail to divide the world into
meaningful categories, they can be regarded as semantically vacuous. Con-
tradictions and tautologies may be common in logic textbooks, but they
are rare in everyday speech. Thus, in a practical model of informal natural
language inference, we will rarely go wrong by assuming the non-vacuity of
the expressions we encounter.
4
We therefore focus on the remaining seven
classes, which we designate as the set B of basic semantic relations.
symbol
5
name example set theoretic definition
6
x ? y equivalence couch ? sofa x = y
x ? y forward entailment crow ? bird x ? y
x ? y reverse entailment European ? French x ? y
x
?
y negation human
?
nonhuman x ? y = ? ? x ? y = U
x | y alternation cat | dog x ? y = ? ? x ? y 6= U
x ` y cover animal ` nonhuman x ? y 6= ? ? x ? y = U
x # y independence hungry # hippo (all other cases)
First, the semantic containment relations (? and ?) of the monotonicity
calculus are preserved, but are factored into three mutually exclusive rela-
3
We use x to denote the complement of set x in universe U ; thus x ? x = ? and
x ? x = U .
4
Our model can easily be revised to accommodate vacuous expressions and relations
between them, but then becomes somewhat unwieldy. The assumption of non-vacuity is
closely related to the assumption of existential import in traditional logic. For a defense
of existential import in natural language semantics, see (2).
5
Selecting an appropriate symbol to represent each relation is a vexed problem. We
sought symbols which (a) are easily approximated by a single ASCII character, (b) are
graphically symmetric iff the relations they represent are symmetric, and (c) do not exces-
sively abuse accepted conventions. The
?
symbol was chosen to evoke the logically similar
bitwise XOR operator of the C programming language family; regrettably, it may also evoke
the Boolean AND function. The | symbol was chosen to evoke the Sheffer stroke commonly
used to represent the logically similar Boolean NAND function; regrettably, it may also
evoke the Boolean OR function. The ? and ? symbols were obviously chosen to resemble
their set-theoretic analogs, but a potential confusion arises because some logicians use the
horseshoe ? (with the opposite orientation) to represent material implication.
6
Each relation in B obeys the additional constraints that ? ? x ? U and ? ? y ? U
(i.e., x and y are non-vacuous).
143
tions: equivalence (?), (strict) forward entailment (?), and (strict) reverse
entailment (?). Next, we have two relations expressing semantic exclusion:
negation (
?
), or exhaustive exclusion, which is analogous to set complement;
and alternation (|), or non-exhaustive exclusion. The next relation is cover
(`), or non-exclusive exhaustion. Though its utility is not immediately obvi-
ous, it is the dual under negation of the alternation relation.
7
Finally, the in-
dependence relation (#) covers all other cases: it expresses non-equivalence,
non-containment, non-exclusion, and non-exhaustion. Note that # is the
least informative relation, in that it places the fewest constraints on its ar-
guments.
8
Following Sa?nchez Valencia, we define the relations in B for all seman-
tic types. For semantic types which can be interpreted as characteristic
functions of sets,
9
the set-theoretic definitions can be applied directly. The
definitions can then be extended to other types by interpreting each type as
if it were a type of set. For example, propositions can be understood (per
Montague) as denoting sets of possible worlds. Thus two propositions stand
in the | relation iff there is no world where both hold (but there is some
world where neither holds). Likewise, names can be interpreted as denoting
singleton sets, with the result that two names stand in the ? relation iff
they refer to the same entity, or the | relation otherwise.
By design, the relations in B are mutually exclusive, so that we can
define a function ?(x, y) which maps every ordered pair of expressions
10
to
the unique relation in B to which it belongs.
3 Joining semantic relations
If we know that semantic relation R holds between x and y, and that se-
mantic relation S holds between y and z, then what is the semantic relation
between x and z? The join of semantic relations R and S, which we denote
7
We describe relations R and S as duals under negation iff ?x, y : ?x, y? ? R ? ?x, y? ?
S. Thus ? and ? are dual; | and ` are dual; and ?,
?
, and # are self-dual. The
significance of this duality will become apparent in section 5.
8
Two sets selected uniformly at random from 2
U
are overwhelmingly likely to belong
to # (for large |U |).
9
That is, all functional types whose final output is a truth value. If we assume a type
system whose basic types are e (entities) and t (truth values), then this includes most of the
functional types encountered in semantic analysis: e t (common nouns, adjectives, and
intransitive verbs), ee t (transitive verbs), (e t)(e t) (adverbs), (e t)(e t) t
(binary generalized quantifiers), and so on.
10
Assuming the expressions are non-vacuous, and belong to the same semantic type.
144
R ?? S,
11
is defined by:
R ?? S
def
= {?x, z? : ?y (?x, y? ? R ? ?y, z? ? S)}
Some joins are quite intuitive. For example, it is immediately clear that
? ?? ? = ?, ? ?? ? = ?,
?
??
?
= ?, and for any R, (R ?? ?) = (? ??
R) = R. Other joins are less obvious, but still accessible to intuition. For
example, | ??
?
= ?. This can be seen with the aid of Venn diagrams, or by
considering simple examples: fish | human and human
?
nonhuman, thus
fish ? nonhuman.
But we soon stumble upon an inconvenient truth: not every join yields
a relation in B. For example, if x | y and y | z, the relation between x and
z is not determined. They could be equivalent, or one might contain the
other. They might be independent or alternative. All we can say for sure
is that they are not exhaustive (since both are disjoint from y). Thus, the
result of joining | and | is not a relation in B, but a union of such relations,
specifically
?
{?,?,?, |,#}.
12
We will refer to (non-trivial) unions of relations inB as union relations.
13
Of the 49 possible joins of relations in B, 32 yield a relation in B, while 17
yield a union relation, with larger unions conveying less information. Union
relations can be further joined, and we can establish that the smallest set
of relations which contains B and is closed under joining contains just 16
relations.
14
One of these is the total relation, which contains all pairs of
(non-vacuous) expressions. This relation, which we denote ?, is the black
hole of semantic relations, in the sense that (a) it conveys zero information
about pairs of expressions which belong to it, and (b) joining a chain of
semantic relations will, if it contains any noise and is of sufficient length,
11
In Tarskian relation algebra, this operation is known as relation composition, and is
often represented by a semi-colon: R ; S. To avoid confusion with semantic composition
(section 5), we prefer to use the term join for this operation, by analogy to the database
JOIN operation (also commonly represented by ??).
12
We use this notation as shorthand for the union ? ? ? ? ? ? | ? #. To be precise,
the result of this join is not identical with this union, but is a subset of it, since the union
contains some pairs of sets (e.g. ?U \ a, U \ a?, for any |a| = 1) which cannot participate
in the | relation. However, the approximation makes little practical difference.
13
Some union relations hold intrinsic interest. For example, in the three-way formulation
of the NLI task described in section 2, the three classes can be identified as
S
{?,?},
S
{
?
, |}, and
S
{?,`,#}.
14
That is, the relations inB plus 9 union relations. Note that this closure fails to include
most of the 120 possible union relations. Perhaps surprisingly, the unions
S
{?,?} and
S
{
?
, |} mentioned in footnote 13 do not appear.
145
lead inescapably to ?.
15
This tendency of joining to devolve toward less-
informative semantic relations places an important limitation on the power
of the inference method described in section 7.
A complete join table for relations in B is shown below:
16
?? ? ? ?
?
| ` #
? ? ? ?
?
| ` #
? ? ? ???|# | | ?
?
|`# ?|#
? ? ???`# ? ` ?
?
|`# ` ?`#
? ?
` | ? ? ? #
| | ?
?
|`# | ? ???|# ? ?|#
` ` ` ?
?
|`# ? ? ???`# ?`#
# # ?`# ?|# # ?|# ?`# ?
In an implemented model, the complexity introduced by union relations
is easily tamed. Every union relation which results from joining relations
in B contains #, and thus can safely be approximated by #. After all, #
is already the least informative relation in B?loosely speaking, it indicates
ignorance of the relationship between two expressions?and further joining
will never serve to strengthen it. Our implemented model therefore has no
need to represent union relations.
4 Lexical semantic relations
Suppose x is a compound linguistic expression, and let e(x) be the result
of applying an atomic edit e (the deletion, insertion, or substitution of a
subexpression) to x. The semantic relation ?(x, e(x)) will depend on (1) the
lexical semantic relation generated by e, which we label ?(e), and (2) other
properties of the context x in which e is applied (to be discussed in section 5).
For example, suppose x is red car. If e is sub(car, convertible), then ?(e)
is ? (because convertible is a hyponym of car). On the other hand, if e is
del(red), then ?(e) is ? (because red is an intersective modifier). Crucially,
?(e) depends solely on the lexical items involved in e, independent of context.
How are lexical semantic relations determined? Ultimately, this is the
province of lexical semantics, which lies outside the scope of this work. How-
ever, the answers are fairly intuitive in most cases, and we can make a
number of useful observations.
15
In fact, computer experiments show that if relations are selected uniformly at random
from B, it requires on average just five joins to reach ?.
16
For compactness, we omit the union notation here; thus ?|# stands for
S
{?, |,#}.
146
Substitutions. The semantic relation generated by a substitution edit is
simply the relation between the substituted terms: ?(sub(x, y)) = ?(x, y).
For open-class terms such as nouns, adjectives, and verbs, we can often
determine the appropriate relation by consulting a lexical resource such as
WordNet. Synonyms belong to the ? relation (sofa ? couch, forbid ? pro-
hibit); hyponym-hypernym pairs belong to the? relation (crow ? bird, frigid
? cold, soar ? rise); and antonyms and coordinate terms generally belong
to the | relation (hot | cold, cat | dog).
17
Proper nouns, which denote indi-
vidual entities or events, will stand in the ? relation if they denote the same
entity (USA ? United States), or the | relation otherwise (JFK | FDR).
Pairs which cannot reliably be assigned to another semantic relation will be
assigned to the # relation (hungry # hippo). Of course, there are many dif-
ficult cases, where the most appropriate relation will depend on subjective
judgments about word sense, topical context, and so on?consider, for ex-
ample, the pair system and approach. And some judgments may depend on
world knowledge not readily available to an automatic system. For example,
plausibly skiing | sleeping, but skiing # talking.
Closed-class terms may require special handling. Substitutions involving
generalized quantifiers generate a rich variety of semantic relations: all ?
every, every ? some, some
?
no, no | every, at least four ` at most six, and
most # ten or more.
18
Two pronouns, or a pronoun and a noun, should
ideally be assigned to the ? relation if it can determined from context that
they refer to the same entity, though this may be difficult for an automatic
system to establish reliably. Prepositions are somewhat problematic. Some
pairs of prepositions can be interpreted as antonyms, and thus assigned to
the | relation (above | below), but many prepositions are used so flexibly in
natural language that they are best assigned to the ? relation (on [a plane]
? in [a plane] ? by [plane]).
Generic deletions and insertions. For deletion edits, the default be-
havior is to generate the ? relation (thus red car ? car). Insertion edits are
symmetric: by default, they generate the ? relation (sing ? sing off-key).
This heuristic can safely be applied whenever the affected phrase is an in-
tersective modifier, and can usefully be applied to phrases much longer than
a single word (car which has been parked outside since last week ? car).
Indeed, this principle underlies most current approaches the RTE task, in
17
Note that most antonym pairs do not belong to the
?
relation, since they typically do
not exclude the middle.
18
Some of these assertions assume the non-vacuity (section 2) of the predicates to which
the quantifiers are applied.
147
which the premise p often contains much extraneous content not found in
the hypothesis h. Most RTE systems try to determine whether p subsumes
h: they penalize new content inserted into h, but do not penalize content
deleted from p.
Special deletions and insertions. However, some lexical items exhibit
special behavior upon deletion or insertion. The most obvious example is
negation, which generates the
?
relation (didn?t sleep
?
did sleep). Implica-
tives and factives (such as refuse to and admit that) constitute another
important class of exceptions, but we postpone discussion of them to sec-
tion 6. Then there are non-intersective adjectives such as former and alleged.
These have various behavior: deleting former seems to generate the | rela-
tion (former student | student), while deleting alleged seems to generate the
# relation (alleged spy # spy). We lack a complete typology of such cases,
but consider this an interesting problem for lexical semantics. Finally, for
pragmatic reasons, we typically assume that auxiliary verbs and punctua-
tion marks are semantically vacuous, and thus generate the ? relation upon
deletion or insertion. When combined with the assumption that morphology
matters little in inference,
19
this allows us to establish, e.g., that is sleeping
? sleeps and did sleep ? slept.
5 Semantic relations and semantic composition
How are semantic relations affected by semantic composition? In other
words, how do the semantic relations between compound expressions depend
on the semantic relations between their parts? Say we have established the
value of ?(x, y), and let f be an expression which can take x or y as an
argument. What is the value of ?(f(x), f(y)), and how does it depend on
the properties of f?
The monotonicity calculus of Sa?nchez Valencia provides a partial an-
swer. It explains the impact of semantic composition on semantic relations
?, ?, ?, and # by assigning semantic functions to one of three monotonic-
ity classes: up, down, and non. If f has monotonicity up (the default),
then the semantic relation between x and y is projected through f without
change: ?(f(x), f(y)) = ?(x, y). Thus some parrots talk ? some birds talk.
If f has monotonicity down, then ? and ? are swapped. Thus no carp
talk ? no fish talk. Finally, if f has monotonicity non, then ? and ? are
projected as #. Thus most humans talk # most animals talk.
19
Indeed, the official definition of the RTE task explicitly specifies that tense be ignored.
148
The monotonicity calculus also provides an algorithm for computing the
effect on semantic relations of multiple levels of semantic composition. Al-
though Sa?nchez Valencia?s presentation of this algorithm uses a complex
scheme for annotating nodes in a categorial grammar parse, the central idea
can be recast in simple terms: propagate a lexical semantic relation upward
through a semantic composition tree, from leaf to root, while respecting
the monotonicity properties of each node along the path. Consider the sen-
tence Nobody can enter without pants. A plausible semantic composition
tree for this sentence could be rendered as (nobody (can ((without pants)
enter))). Now consider replacing pants with clothes. We begin with the
lexical semantic relation: pants ? clothes. The semantic function without
has monotonicity down, so without pants ? without clothes. Continuing up
the semantic composition tree, can has monotonicity up, but nobody has
monotonicity down, so we get another reversal, and find that nobody can
enter without pants ? nobody can enter without clothes.
While the monotonicity calculus elegantly explains the impact of seman-
tic composition on the containment relations (chiefly, ? and ?), it lacks
any account of the exclusion relations (
?
and |, and, indirectly, `). To
remedy this lack, we propose to generalize the concept of monotonicity to
a concept of projectivity. We categorize semantic functions into a number
of projectivity signatures, which can be seen as generalizations of both the
three monotonicity classes of Sa?nchez Valencia and the nine implication sig-
natures of Nairn et al (see section 6). Each projectivity signature is defined
by a map B 7? B which specifies how each semantic relation is projected
by the function. (Binary functions can have different signatures for each
argument.) In principle, there are up to 7
7
possible signatures; in practice,
probably no more than a handful are realized by natural language expres-
sions. Though we lack a complete inventory of projectivity signatures, we
can describe a few important cases.
Negation. We begin with simple negation (not). Like most functions, it
projects ? and # without change (not happy ? not glad and isn?t swimming
# isn?t hungry). As a downward monotone function, it swaps ? and ?
(didn?t kiss ? didn?t touch). But we can also establish that it projects
?
without change (not human
?
not nonhuman) and swaps | and ` (not French
` not German and not more than 4 | not less than 6 ). Its projectivity
signature is therefore {?:?,?:?,?:?,
?
:
?
, | :`,`: |,#:#}.
149
Intersective modification. Intersective modification has monotonicity
up, but projects both
?
and | as | (living human | living nonhuman and
French wine | Spanish wine), and projects ` as # (metallic pipe # nonfer-
rous pipe). It therefore has signature {?:?,?:?,?:?,
?
: |, | : |,`:#,#:#}.
20
Quantifiers. While semanticists are well acquainted with the monotonic-
ity properties of common quantifiers, how they project the exclusion rela-
tions may be less familiar. The following table summarizes the projectivity
signatures of the most common binary generalized quantifiers for each ar-
gument position:
projectivity for 1
st
argument projectivity for 2
nd
argument
quantifier ? ? ?
?
| ` # ? ? ?
?
| ` #
some ? ? ? `
?
# `
?
# ? ? ? `
?
# `
?
#
no ? ? ? |
?
# |
?
# ? ? ? |
?
# |
?
#
every ? ? ? |
?
# |
?
# ? ? ? |
?
|
?
# #
not every ? ? ? `
?
# `
?
# ? ? ? `
?
`
?
# #
A few observations:
? All quantifiers (like most other semantic functions) project ? and #
without change.
? The table confirms well-known monotonicity properties: no is downward-
monotone in both arguments, every in its first argument, and not every
in its second argument.
? Relation | is frequently ?blocked? by quantifiers (i.e., projected as #).
Thus no fish talk # no birds talk and someone was early # someone
was late. A notable exception is every in its second argument, where
| is preserved: everyone was early | everyone was late. (Note the
similarity to intersective modification.)
? Because no is the negation of some, its projectivity signature can be
found by projecting the signature of some through the signature of
not. Likewise for not every and every.
? Some results depend on assuming the non-vacuity of the other argu-
ment to the quantifier: those marked with
?
assume it to be non-empty,
while those marked with
?
assume it to be non-universal. Without
these assumptions, # is projected.
20
At least for practical purposes. The projection of
?
and | as | depends on the assump-
tion of non-vacuity, and ` is actually projected as
S
{?,?,?, |,#}, which we approximate
by #, as described in section 3.
150
Verbs. Verbs (and verb-like constructions) exhibit diverse behavior. Most
verbs are upward-monotone (though not all?see section 6), and many verbs
project
?
, |, and ` as # (eats humans # eats nonhumans, eats cats # eats
dogs, and eats mammals # eats nonhumans). However, verbs which encode
functional relations seem to exhibit the same projectivity as intersective
modifiers, projecting
?
and | as |, and` as #.
21
Categorizing verbs according
to projectivity is an interesting problem for lexical semantics, which may
involve codifying some amount of world knowledge.
6 Implicatives and factives
(15) offer an elegant account of inferences involving implicatives and fac-
tives
22
such as manage to, refuse to, and admit that. Their model clas-
sifies such operators into nine implication signatures, according to their
implications?positive (+), negative (?), or null (?)?in both positive and
negative contexts. Thus refuse to has implication signature ?/?, because
it carries a negative implication in a positive context (refused to dance im-
plies didn?t dance), and no implication in a negative context (didn?t refuse
to dance implies neither danced nor didn?t dance).
Most of the phenomena observed by Nairn et al can be explained within
our framework by specifying, for each implication signature, the relation
generated when an operator of that signature is deleted from (or inserted
into) a compound expression, as shown in the following table:
signature ?(del(?)) ?(ins(?)) example
implicatives +/? ? ? he managed to escape ? he escaped
(up) +/? ? ? he was forced to sell ? he sold
?/? ? ? he was permitted to live ? he lived
implicatives ?/+
? ?
he forgot to pay
?
he paid
(down) ?/? | | he refused to fight | he fought
?/+ ` ` he hesitated to ask ` he asked
factives +/+ ? ? he admitted that he knew ? he knew
(non) ?/? | | he pretended he was sick | he was sick
?/? # # he wanted to fly # he flew
This table invites several observations. First, as the examples make clear,
there is room for variation regarding the appearance of infinitive arguments,
21
Consider the verbal construct is married to: is married to a German | is married to a
non-German, is married to a German | is married to an Italian, is married to a European
# is married to a non-German. The AuContraire system (16) includes an intriguing
approach to identifying such functional phrases automatically.
22
We use ?factives? as an umbrella term embracing counterfactives and nonfactives
along with factives proper.
151
complementizers, passivization, and morphology. An implemented model
must tolerate such diversity.
Second, some of the examples may seem more intuitive when one consid-
ers their negations. For example, deleting signature ?/? generates ?; under
negation, this is projected as ? (he wasn?t permitted to live ? he didn?t
live). Likewise, deleting signature ?/+ generates `; under negation, this is
projected as | (he didn?t hesitate to ask | he didn?t ask).
Third, a fully satisfactory treatment of the factives (signatures +/+, ?
/?, and ?/?) would require an extension to our present theory. For example,
deleting signature +/+ generates ?; yet under negation, this is projected
not as ?, but as | (he didn?t admit that he knew | he didn?t know). The
problem arises because the implication carried by a factive is not an entail-
ment, but a presupposition.
23
As is well known, the projection behavior of
presuppositions differs from that of entailments (22). It seems likely that
our model could be elaborated to account for projection of presuppositions
as well as entailments, but we leave this for future work.
We can further cement implicatives and factives within our model by
specifying the monotonicity class for each implication signature: signatures
+/?, +/?, and ?/? have monotonicity up (force to tango ? force to dance);
signatures ?/+, ?/?, and ?/+ have monotonicity down (refuse to tango ?
refuse to dance); and signatures +/+, ?/?, and ?/? (the propositional at-
titudes) have monotonicity non (think tangoing is fun # think dancing is
fun). We are not yet able to specify the complete projectivity signature cor-
responding to each implication signature, but we can describe a few specific
cases. For example, implication signature ?/? seems to project
?
as | (refuse
to stay | refuse to go) and both | and ` as # (refuse to tango # refuse to
waltz ).
7 Putting it all together
We now have the building blocks of a general method to establish the se-
mantic relation between a premise p and a hypothesis h. The steps are as
follows:
1. Find a sequence of atomic edits ?e
1
, . . . , e
n
? which transforms p into
h: thus h = (e
n
? . . . ? e
1
)(p). For convenience, let us define x
0
= p,
x
n
= h, and x
i
= e
i
(x
i?1
) for i ? [1, n].
2. For each atomic edit e
i
:
23
Of course, the implicatives may carry presuppositions as well (he managed to escape
 it was hard to escape), but these implications are not activated by a simple deletion, as
with the factives.
152
(a) Determine the lexical semantic relation ?(e
i
), as in section 4.
(b) Project ?(e
i
) upward through the semantic composition tree of
expression x
i?1
to find an atomic semantic relation ?(x
i?1
, e
i
) =
?(x
i?1
, x
i
), as in section 5.
3. Join atomic semantic relations across the sequence of edits, as in sec-
tion 3:
?(p, h) = ?(x
0
, x
n
) = ?(x
0
, e
1
) ?? . . . ?? ?(x
i?1
, e
i
) ?? . . . ?? ?(x
n?1
, e
n
)
However, this inference method has several important limitations, in-
cluding the need to find an appropriate edit sequence connecting p and h;
24
the tendency of the join operation toward less informative semantic rela-
tions, as described in section 3; and the lack of a general mechanism for
combining information from multiple premises.
25
Consequently, the method
has less deductive power than first-order logic, and fails to sanction some
fairly simple inferences, including de Morgan?s laws for quantifiers. But the
method neatly explains many inferences not handled by the monotonicity
calculus, including this example from section 1:
i x
i
e
i
?(e
i
) ?(x
i?1
, e
i
) ?(x
0
, x
i
)
0 Stimpy is a cat
1 Stimpy is a dog sub(cat, dog) | | |
2 Stimpy is not a dog ins(not)
? ?
?
3 Stimpy is not a poodle sub(dog, poodle) ? ? ?
Here, x
0
is transformed into x
3
by a sequence of three edits. First, replacing
cat with its coordinate term dog generates |. Next, inserting not generates
?
, and | joined with
?
yields ?. Finally, replacing dog with its hyponym
poodle generates ?. Because of the downward-monotone context created by
not, this is projected as ?, and ? joined with ? yields ?. Therefore, x
0
entails x
3
.
For an example involving an implicative, consider:
24
The order of edits can be significant, if one edit affects the projectivity properties of
the context for another edit. In practice, we typically find that different edit orders lead to
the same final result (albeit via different intermediate steps), or at worst to a result which
is compatible with, though less informative than, the desired result. But in principle, edit
sequences involving lexical items with unusual properties?not exhibited, so far as we are
aware, by any natural language expressions?could lead to incompatible results. Thus we
lack any formal guarantee of soundness.
25
However, some inferences can be enabled by auxiliary premises encoded as lexical
semantic relations. For example, men ? mortal can enable the classic syllogism Socrates
is a man ? Socrates is mortal.
153
i x
i
e
i
?(e
i
) ?(x
i?1
, e
i
) ?(x
0
, x
i
)
0 We were not permitted to smoke
1 We did not smoke del(permitted to) ? ? ?
2 We smoked del(not)
? ?
|
3 We smoked Cuban cigars ins(Cuban cigars) ? ? |
Again, x
0
is transformed into x
3
by a sequence of three edits.
26
First,
deleting permitted to generates ?, according to its implication signature; but
because not is downward-monotone, this is projected as ?. Next, deleting
not generates
?
, and ? joined with
?
yields |. Finally, inserting Cuban cigars
restricts the meaning of smoked, generating ?, and | joined with ? yields |.
So x
3
contradicts x
0
.
A more complex example is presented in (14).
8 Implementation and evaluation
The model of natural logic described here has been implemented in software
as the NatLog system. In previous work (14), we have presented a descrip-
tion and evaluation of NatLog; this section summarizes the main results.
Natlog faces three primary challenges:
1. Finding an appropriate sequence of atomic edits connecting premise
and hypothesis. NatLog does not address this problem directly, but
relies instead on edit sequences from other sources. We have investi-
gated this problem separately in (12).
2. Determining the lexical semantic relation for each edit. NatLog learns
to predict lexical semantic relations by using machine learning tech-
niques and exploiting a variety of manually and automatically con-
structed sources of information on lexical relations.
3. Computing the projection of each lexical semantic relation. NatLog
identifies expressions with non-default projectivity and computes the
likely extent of their arguments in a syntactic parse using hand-crafted
tree patterns.
We have evaluated NatLog on two different test suites. The first is the
FraCaS test suite (5), which contains 346 NLI problems, divided into nine
sections, each focused on a specific category of semantic phenomena. The
goal is three-way entailment classification, as described in section 2. On
26
We neglect edits involving auxiliaries and morphology, which simply yield the ? rela-
tion.
154
this task, NatLog achieves an average accuracy of 70%.
27
In the section
concerning quantifiers, which is both the largest and the most amenable
to natural logic, the system answers all problems but one correctly. Un-
surprisingly, performance is mediocre in four sections concerning semantic
phenomena (e.g., ellipsis) not relevant to natural logic and not modeled by
the system. But in the other five sections (representing about 60% of the
problems), NatLog achieves accuracy of 87%. What?s more, precision is uni-
formly high, averaging 89% over all sections. Thus, even outside its areas of
expertise, the system rarely predicts entailment when none exists.
The RTE3 test suite (8) differs from FraCaS in several important ways:
the goal is binary entailment classification; the problems have much longer
premises and are more ?natural?; and the problems employ a diversity of
types of inference?including paraphrase, temporal reasoning, and relation
extraction?which NatLog is not designed to address. Consequently, the
NatLog system by itself achieves mediocre accuracy (59%) on RTE3 prob-
lems. However, its precision is comparatively high, which suggests a strategy
of hybridizing with a broad-coverage RTE system. We were able to show
that adding NatLog as a component in the Stanford RTE system (3) led to
accuracy gains of 4%.
9 Conclusion
The model of natural logic presented here is by no means a universal solution
to the problem of natural language inference. Many NLI problems hinge on
types of inference not addressed by natural logic, and the inference method
we describe faces a number of limitations on its deductive power (discussed
in section 7). Moreover, there is further work to be done in fleshing out our
account, particularly in establishing the proper projectivity signatures for
a broader range of quantifiers, verbal constructs, implicatives and factives,
logical connectives, and other semantic functions.
Nevertheless, we believe our model of natural logic fills an important
niche. While approximate methods based on lexical and syntactic similarity
can handle many NLI problems, they are easily confounded by inferences in-
volving negation, antonymy, quantifiers, implicatives, and many other phe-
nomena. Our model achieves the logical precision needed to handle such
inferences without resorting to full semantic interpretation, which is in any
case rarely possible. The practical value of the model is demonstrated by
its success in evaluations on the FraCaS and RTE3 test suites.
27
Our evaluation excluded multi-premise problems, which constitute about 44% of the
test suite.
155
References
[1] J. Bos and K. Markert. Recognising textual entailment with logical inference. In Proceedings of
EMNLP-05, 2005.
[2] M. Bo?ttner. A note on existential import. Studia Logica, 47(1):35?40, 1988.
[3] N. Chambers, D. Cer, T. Grenager, D. Hall, C. Kiddon, B. MacCartney, M. C. de Marneffe,
D. Ramage, E. Yeh, and C. D. Manning. Learning Alignments and Leveraging Natural
Logic. In Proceedings of the ACL-07 Workshop on Textual Entailment and Paraphrasing,
2007.
[4] C. Condoravdi, D. Crouch, V. de Paiva, R. Stolle, and D.G. Bobrow. Entailment, Intensional-
ity and Text Understanding. In Proceedings of the HLT-NAACL 2003 Workshop on Text
Meaning, Morristown, NJ, USA, 2003.
[5] Robin Cooper et al Using the framework. Technical Report LRE 62-051 D-16, The FraCaS
Consortium, 1996.
[6] I. Dagan, O. Glickman, and B. Magnini. The PASCAL Recognising Textual Entailment Challenge.
In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment,
2005.
[7] Y. Fyodorov, Y. Winter, and N. Francez. A Natural Logic Inference System. In Proceedings of
the 2nd Workshop on Inference in Computational Semantics (ICoS-2), 2000.
[8] D. Giampiccolo, B. Magnini, I. Dagan, and B. Dolan. The Third PASCAL Recognizing Textual
Entailment Challenge. In Proceedings of the ACL-07 Workshop on Textual Entailment and
Paraphrasing, 2007.
[9] O. Glickman, I. Dagan, and M. Koppel. Web based probabilistic textual entailment. In Proceedings
of the PASCAL Challenges Workshop on Recognizing Textual Entailment, 2005.
[10] A. Hickl, J. Williams, J. Bensley, K. Roberts, B. Rink, and Y. Shi. Recognizing Textual Entail-
ment with LCC?s GROUNDHOG System. In Proceedings of the Second PASCAL Challenges
Workshop on Recognizing Textual Entailment, 2006.
[11] G. Lakoff. Linguistics and natural logic. Synthese, 22:151?271, 1970.
[12] B. MacCartney, M. Galley, and C. D. Manning. A phrase-based alignment model for natural
language inference. In Proceedings of EMNLP-08, Honolulu, HI, 2008.
[13] B. MacCartney, T. Grenager, M. C. de Marneffe, D. Cer, and C. D. Manning. Learning to
Recognize Features of Valid Textual Entailments. In Proceedings of NAACL-06, New York,
2006.
[14] B. MacCartney and C. D. Manning. Modeling semantic containment and exclusion in natural
language inference. In Proceedings of Coling-08, Manchester, UK, 2008.
[15] R. Nairn, C. Condoravdi, and L. Karttunen. Computing relative polarity for textual inference. In
Proceedings of ICoS-5, Buxton, UK, 2006.
[16] A. Ritter, D. Downey, S. Soderland, and O. Etzioni. It?s a Contradiction?No, it?s Not: A Case
Study using Functional Relations. In Proceedings of EMNLP-08, 2008.
[17] V. Sa?nchez Valencia. Studies on Natural Logic and Categorial Grammar. PhD thesis, Univ.
Amsterdam, 1991.
[18] J. Sukkarieh. Quasi-NL Knowledge Representation for Structurally-Based Inferences. In Proceed-
ings of the 3rd Workshop on Inference in Computational Semantics (ICoS-3), 2001.
[19] J. van Benthem. The semantics of variety in categorial grammars. In W. Buszkowski, W. Mar-
ciszewski, and J. van Benthem, editors, Categorial grammar, pages 33?55. John Benjamins,
Amsterdam, 1988.
[20] J. van Benthem. Language in Action: categories, lambdas and dynamic logic, volume 130 of
Studies in Logic. North-Holland, Amsterdam, 1991.
[21] J. van Benthem. A brief history of natural logic. Technical Report PP-2008-05, Institute for Logic,
Language & Computation, 2008.
[22] R. A. van der Sandt. Presupposition projection as anaphora resolution. Journal of Semantics,
9(4), 1992.
[23] J. van Eijck. Natural logic for natural language. http://homepages.cwi.nl/\texttt{\
~
}jve/
papers/05/nlnl/NLNL.pdf, 2005.
156
