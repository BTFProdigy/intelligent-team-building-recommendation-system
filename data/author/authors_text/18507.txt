Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 935?945,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Dataset for Research on Short-Text Conversation ?
Hao Wang? Zhengdong Lu? Hang Li? Enhong Chen?
? xdwangh@mail.ustc.edu.cn ?lu.zhengdong@huawei.com
?hangli.hl@huawei.com ?cheneh@ustc.edu.cn
?Univ. of Sci & Tech of China, China ?Noah?s Ark Lab, Huawei Technologies, Hong Kong
Abstract
Natural language conversation is widely re-
garded as a highly difficult problem, which
is usually attacked with either rule-based or
learning-based models. In this paper we
propose a retrieval-based automatic response
model for short-text conversation, to exploit
the vast amount of short conversation in-
stances available on social media. For this
purpose we introduce a dataset of short-text
conversation based on the real-world instances
from Sina Weibo (a popular Chinese mi-
croblog service), which will be soon released
to public. This dataset provides rich collec-
tion of instances for the research on finding
natural and relevant short responses to a given
short text, and useful for both training and test-
ing of conversation models. This dataset con-
sists of both naturally formed conversation-
s, manually labeled data, and a large repos-
itory of candidate responses. Our prelimi-
nary experiments demonstrate that the simple
retrieval-based conversation model performs
reasonably well when combined with the rich
instances in our dataset.
1 Introduction
Natural language conversation is one of the holy
grail of artificial intelligence, and has been taken as
the original form of the celebrated Turing test. Pre-
vious effort in this direction has largely focused on
analyzing the text and modeling the state of the con-
versation through dialogue models, while in this pa-
?The work is done when the first author worked as intern at
Noah?s Ark Lab, Huawei Techologies.
per we take one step back and focus on a much easi-
er task of finding the response for a given short text.
This task is in clear contrast with previous effort in
dialogue modeling in the following two aspects
? we do not consider the context or history of
conversations, and assume that the given short
text is self-contained;
? we only require the response to be natural, rel-
evant, and human-like, and do not require it to
contain particular opinion, content, or to be of
particular style.
This task is much simpler than modeling a complete
dialogue session (e.g., as proposed in Turing test),
and probably not enough for real conversation sce-
nario which requires often several rounds of interac-
tions (e.g., automatic question answering system as
in (Litman et al, 2000)). However it can shed impor-
tant light on understanding the complicated mecha-
nism of the interaction between an utterance and it-
s response. The research in this direction will not
only instantly help the applications of short session
dialogue such as automatic message replying on mo-
bile phone and the chatbot employed in voice assis-
tant like Siri1, but also it will eventually benefit the
modeling of dialogues in a more general setting.
Previous effort in modeling lengthy dialogues fo-
cused either on rule-based or learning-based models
(Carpenter, 1997; Litman et al, 2000; Williams and
Young, 2007; Schatzmann et al, 2006; Misu et al,
2012). This category of approaches require relative-
ly less data (e.g. reinforcement learning based) for
1http://en.wikipedia.org/wiki/Siri
935
training or no training at all, but much manual ef-
fort in designing the rules or the particular learning
algorithms. In this paper, we propose to attack this
problem using an alternative approach, by leverag-
ing the vast amount of training data available from
the social media. Similar ideas have appeared in (Ja-
farpour and Burges, 2010; Leuski and Traum, 2011)
as an initial step for training a chatbot.
With the emergence of social media, especially
microblogs such as Twitter, in the past decade, they
have become an important form of communication
for many people. As the result, it has collected con-
versation history with volume previously unthink-
able, which brings opportunity for attacking the con-
versation problem from a whole new angle. More
specifically, instead of generating a response to an
utterance, we pick a massive suitable one from the
candidate set. The hope is, with a reasonable re-
trieval model and a large enough candidate set, the
system can produce fairly natural and appropriate re-
sponses.
This retrieval-based model is somewhat like non-
parametric model in machine learning communities,
which performs well only when we have abundan-
t data. In our model, it needs only a relatively s-
mall labeled dataset for training the retrieval model,
but requires a rather large unlabeled set (e.g., one
million instances) for candidate responses. To fur-
ther promote the research in similar direction, we
create a dataset for training and testing the retrieval
model, with a candidate responses set of reason-
able size. Sina Weibo is the most popular Twitter-
like microblog service in China, hosting over 500
million registered users and generating over 100
million messages per day 2. As almost all mi-
croblog services, Sina Weibo allows users to com-
ment on a published post3, which forms a natural
one-round conversation. Due to the great abundance
of those (post, response) pairs, it provides an ideal
data source and test bed for one-round conversation.
We will make this dataset publicly available in the
near future.
2http://en.wikipedia.org/wiki/Sina_Weibo
3Actually it also allows users to comment on other users?
comments, but we will not consider that in the dataset.
2 The Dialogues on Sina Weibo
Sina Weibo is a Twitter-like microblog service, on
which a user can publish short messages (will be re-
ferred to as post in the remainder of the paper) visi-
ble to public or a group specified by the user. Simi-
lar to Twitter, Sina Weibo has the word limit of 140
Chinese characters. Other users can comment on a
published post, with the same length limit, as shown
in the real example given in Figure 6 (in Chinese).
Those comments will be referred to as responses in
the remainder of the paper.
Figure 1: An example of Sina Weibo post and the com-
ments it received.
We argue that the (post, response) pairs on Sina
Weibo provide rather valuable resource for studying
one round dialogue between users. The comments
to a post can be of rather flexible forms and diverse
topics, as illustrated in the example in Table 1. With
a post stating the user?s status (traveling to Hawaii),
the comments can be of quite different styles and
contents, but apparently all appropriate.
In many cases, the (post, response) pair is self-
contained, which means one does not need any back-
ground and contextual information to get the main
point of the conversation (Examples of that include
the responses from B, D, G and H). In some cas-
es, one may need extra knowledge to understand the
conversation. For example, the response from user
E will be fairly elusive if taken out of the context
that A?s Hawaii trip is for an international confer-
ence and he is going to give a talk there. We argue
that the number of self-contained (post, response)
pairs is vast, and therefore the extracted (post, re-
936
Post
User A: The first day at Hawaii. Watching sunset at the balcony with a big glass of wine in hand.
Responses
User B: Enjoy it & don?t forget to share your photos!
User C: Please take me with you next time!
User D: How long are you going to stay there?
User E: When will be your talk?
User F: Haha, I am doing the same thing right now. Which hotel are you staying in?
User G: Stop showing-off, buddy. We are still coding crazily right now in the lab.
User H: Lucky you! Our flight to Honolulu is delayed and I am stuck in the airport. Chewing French
fries in MacDonald?s right now.
Table 1: A typical example of Sina Weibo post and the comments it received. The original text is in Chinese, and we
translated it into English for easy access of readers. We did the same thing for all the examples throughout this paper.
sponse) pairs can serve as a rich resource for ex-
ploring rather sophisticated patterns and structures
in natural language conversation.
3 Content of the Dataset
The dataset consists of three parts, as illustrated in
Figure 2. Part 1 contains the original (post, re-
sponse) pairs, indicated by the dark-grey section in
Figure 2. Part 2, indicated by the light-gray section
in Figure 2, consists labeled (post, response) pairs
for some Weibo posts, including positive and nega-
tive examples. Part 3 collects all the responses, in-
cluding but not limited to the responses in Part 1 and
2. Some of the basic statistics are summarized in
Table 2.
# posts # responses vocab. # labeled pairs
4,6345 1,534,874 105,732 12,427
Table 2: Some statistics of the dataset
Original (Post, Response) Pairs This part of
dataset gives (post, response) pairs naturally pre-
sented in the microblog service. In other words,
we create a (post, response) pair there when the re-
sponse is actually given to the post in Sina Weibo.
The part of data is noisy since the responses given
to a Weibo post could still be inappropriate for d-
ifferent reasons, for example, they could be spams
or targeting some responses given earlier. We have
628, 833 pairs.
Labeled Pairs This part of data contains the (post,
response) pairs that are labeled by human. Note that
1) the labeling is only on a small subset of posts,
and 2) for each selected post, the labeled responses
are not originally given to it. The labeling is done
in an active manner (see Section 4 for more detail-
s), so the obtained labels are much more informative
than the those on randomly selected pairs (over 98%
of which are negative). This part of data can be di-
rectly used for training and testing of retrieval-based
response models. We have labeled 422 posts and for
each of them, about 30 candidate responses.
Responses This part of dataset contains only re-
sponses, but they are not necessarily for a certain
post. These extra responses are mainly filtered out
by our data cleaning strategy (see Section 4.2) for
original (post, response) pairs, including those from
filtered-out Weibo posts and those addressing oth-
er responses. Nevertheless, those responses are still
valid candidate for responses. We have about 1.5
million responses in the dataset.
3.1 Using the Dataset for Retrieval-based
Response Models
Our data can be used for training and testing of
retrieval-based response model, or just as a bank of
responses. More specifically, it can be used in at
least the following three ways.
Training Low-level Matching Features The
rather abundant original (post, response) pairs pro-
vide rather rich supervision signal for learning dif-
ferent matching patterns between a post and a re-
sponse. These matching patterns could be of dif-
937
Figure 2: Content of the dataset.
ferent levels. For example, one may discover from
the data that when the word ?Hawaii? occurs in the
post, the response are more likely to contain word-
s like ?trip?, ?flight?, or ?Honolulu?. On a slight-
ly more abstract level, one may learn that when an
entity name is mentioned in the post, it tends to be
mentioned again in the response. More complicated
matching pattern could also be learned. For exam-
ple, the response to a post asking ?how to? is statisti-
cally longer than average responses. As a particular
case, Ritter et al (2011) applied translation model
(Brown et al, 1993) on similar parallel data extract-
ed from Twitter in order to extract the word-to-word
correlation. Please note that with more sophisticat-
ed natural language processing, we can go beyond
bag-of-words for more complicated correspondence
between post and response.
Training Automatic Response Models Although
the original (post, response) pairs are rather abun-
dant, they are not enough for discriminative training
and testing of retrieval models, for the following rea-
sons. In the labeled pairs, both positive and negative
ones are ranked high by some baseline models, and
hence more difficult to tell apart. This supervision
will naturally tune the model parameters to find the
real good responses from the seemingly good ones.
Please note that without the labeled negative pairs,
we need to generate negative pairs with randomly
chosen responses, which in most of the cases are too
easy to differentiate by the ranking model and can-
not fully tune the model parameters. This intuition
has been empirically verified by our experiments.
Testing Automatic Response Models In testing a
retrieval-based system, although we can simply use
the original responses associated with the query post
as positive and treat all the others as negative, this
strategy suffers from the problem of spurious neg-
ative examples. In other words, with a reasonably
good model, the retrieved responses are often good
even if they are not the original ones, which brings
significant bias to the evaluation. With the labeled
pairs, this problem can be solved if we limit the test-
ing only in the small pool of labeled responses.
3.2 Using the Dataset for Other Purposes
Our dataset can also be used for other researches re-
lated to short-text conversations, namely anaphora
resolution, sentiment analysis, and speech act anal-
ysis, based on the large collection of original (post,
response) pairs. For example, to determine the sen-
timent of a response, one needs to consider both
the original post as well as the observed interaction
between the two. In Figure 3, if we want to un-
derstand user?s sentiment towards the ?invited talk?
mentioned in the post, the two responses should be
taken as positive, although the sentiment in the mere
responses is either negative or neutral.
4 Creation of the Dataset
The (post, comment) pairs are sampled from the
Sina Weibo posts published by users in a loosely
connected community and the comments they re-
ceived (may not be from this community). This
community is mainly posed of professors, re-
searchers, and students of natural language process-
ing (NLP) and related areas in China, and the users
938
Figure 3: An example (original Chinese and the English
translation) on the difficulty of sentiment analysis on re-
sponses.
commonly followed them.
The creation process of the dataset, as illustrated
in Figure 4, consists of three consecutive steps: 1)
crawling the community of users, 2) crawling their
Weibo posts and their responses, 3) cleaning the da-
ta, with more details described in the remainder of
this section.
4.1 Sampling Strategy
We take the following sampling strategy for collect-
ing the (post, response) pairs to make the topic rel-
atively focused. We first locate 3,200 users from a
loosely connected community of Natural Language
Processing (NLP) and Machine Learning (ML) in
China. This is done through crawling followees4 of
ten manually selected seed users who are NLP re-
searchers active on Sina Weibo (with no less than 2
posts per day on average) and popular enough (with
no less than 100 followers).
We crawl the posts and the responses they re-
ceived (not necessarily from the crawled communi-
ty) for two months (from April 5th, 2013, to June
5th, 2013). The topics are relatively limited due to
our choice of the users, with the most saliently ones
being:
? Research: discussion on research ideas, paper-
s, books, tutorials, conferences, and researchers
in NLP and machine learning, etc;
? General Arts and Science: mathematics,
physics, biology, music, painting, etc;
4When user A follows user B, A is called B?s follower, and
B is called A?s followee.
? IT Technology: Mobile phones, IT companies,
jobs opportunities, etc;
? Life: traveling (both touring or conference trip-
s), food, photography, etc.
4.2 Processing, Filtering, and Data Cleaning
On the crawled posts and responses, we first perform
a four-step filtering on the post and responses
? We first remove the Weibo posts and their re-
sponses if the length of post is less than 10 Chi-
nese characters or the length of the response is
less than 5 characters. The reason for that is
two-fold: 1) if the text is too short, it can bare-
ly contain information that can be reliably cap-
tured, e.g. the following example
P: Three down, two to go.
and 2) some of the posts or responses are too
general to be interesting for other cases, e.g. the
response in the example below,
P: Nice restaurant. I?d strong recommend it.
Everything here is good except the long
waiting line
R: wow.
? In the remained posts, we only keep the first
100 responses in the original (post, response)
pairs, since we observe that after the first 100
responses there will be a non-negligible propor-
tion of responses addressing things other than
the original Weibo post (e.g., the responses giv-
en earlier). We however will still keep the re-
sponses in the bank of responses.
? The last step is to filter out the potential adver-
tisements. We will find the long responses that
have been posted more than twice on different
posts and scrub them out of both original (post,
response) pairs and the response repository.
For the remained posts and responses, we remove
the punctuation marks and emoticons, and use ICT-
CLAS (Zhang et al, 2003) for Chinese word seg-
mentation.
939
Figure 4: Diagram of the process for creating the dataset.
4.3 Labeling
We employ a pooling strategy widely used in in-
formation retrieval for getting the instance to label
(Voorhees, 2002). More specifically, for a given
post, we use three baseline retrieval models to each
select 10 responses (see Section 5 for the descrip-
tion of the baselines), and merge them to form a
much reduced candidate set with size ? 30. Then
we label the reduced candidate set into ?suitable?
and ?unsuitable? categories. Basically we consider
a response suitable for a given post if we cannot tell
whether it is an original response. More specifically
the suitability of a response is judged based on the
following three criteria5:
Semantic Relevance: This requires the content of
the response to be semantically relevant to the post.
As shown in the example right below, the post P is
about soccer, and so is response R1 (hence seman-
tically relevant), whereas response R2 is about food
(hence semantically irrelevant).
P: There are always 8 English players in their
own penalty area. Unbelievable!
R1: Haha, it is still 0:0, no goal so far.
R2: The food in England is horrible.
Another important aspect of semantic relevance is
the entity association. This requires the entities in
the response to be correctly aligned with those in
the post. In other words, if the post is about entity
5Note that although our criteria in general favor short and
general answers like ?Well said!? or ?Nice?, most of these gen-
eral answers have already been filtered out due to their length
(see Section 4.2).
A, while the response is about entity B, they are very
likely to be mismatched. As shown in the following
example, where the original post is about Paris, and
the response R2 talks about London:
P: It is my last day in Paris. So hard to say
goodbye.
R1: Enjoy your time in Paris.
R2: Man, I wish I am in London right now.
This is however not absolute, since a response con-
taining a different entity could still be sound, as
demonstrated by the following two responses to the
post above
R1: Enjoy your time in France.
R2: The fall of London is nice too.
Logic Consistency: This requires the content of
the response to be logically consistent with the post.
For example, in the table right below, post P states
that the Huawei mobile phone ?Honor? is already in
the market of mainland China. Response R1 talk-
s about a personal preference over the same phone
model (hence logically consistent), whereas R2 asks
the question the answer to which is already clear
from P (hence logically inconsistent).
P: HUAWEI?s mobile phone, Honor, sells
well in Chinese Mainland.
R1: HUAWEI Honor is my favorite phone
R2: When will HUAWEI Honor get to the
market in mainland China?
Speech Act Alignment: Another important factor
in determining the suitability of a response is the
940
speech act. For example, when a question is posed in
the Weibo post, a certain act (e.g., answering or for-
warding it) is expected. In the example below, post
P asks a special question about location. Response
R1 and R2 either forwards or answers the question,
whereas R3 is a negative sentence and therefore does
not align well in speech act.
P: Any one knows where KDD will be held the
year after next?
R1: co-ask. Hopefully Europe
R2: New York, as I heard
R3: No, it is still in New York City
5 Retrieval-based Response Model
In a retrieval-based response model, for a given post
x we pick from the candidate set the response with
the highest ranking score, where the score is the en-
semble of several individual matching features
score(x, y) =
?
i??
wi?i(x, y). (1)
with y stands for a candidate response.
We perform a two-stage retrieval to handle the s-
calability associated with the massive candidate set,
as illustrated in Figure 5. In Stage I, the system em-
ploys several fast baseline matching models to re-
trieve a number of candidate responses for the giv-
en post x, forming a much reduced candidate set
C(reduced)x . In Stage II, the system uses a ranking
function with more and sophisticated features to fur-
ther evaluate all the responses in C(reduced)x , return-
ing a matching score for each response. Our re-
sponse model then decides whether to respond and
which candidate response to choose.
In Stage II, we use the linear score function de-
fined in Equation 1 with 15 features, trained with
RankSVM (Joachims, 2002). The training and test-
ing are both performed on the 422 labeled posts,
with about 12,000 labeled (post, response) pairs. We
use a 5-fold cross validation with a fixed penalty pa-
rameter for slack variable. 6
5.1 Baseline Matching Models
We use the following matching models as the base-
line model for Stage I fast retrieval. Moreover, the
6The performance is fairly insensitive to the choice of the
penalty, so we only report the result with a typical choice of it.
matching features used in the ranking function in
Stage II are generated, directly or indirectly, from
the those matching models:
POST-RESPONSE SEMANTIC MATCHING:
This particular matching function relies on a learned
mapping from the original sparse representation for
text to a low-dimensional but dense representation
for both Weibo posts and responses. The level of
matching score between a post and a response can
be measured as the inner product between their
images in the low-dimensional space
SemMatch(x, y) = x>LXL>Yy. (2)
where x and y are respectively the 1-in-N represen-
tations of x and y. This is to capture the seman-
tic matching between a Weibo post and a response,
which may not be well captured by a word-by-word
matching. More specifically, we find LX and LY
through a large margin variant of (Wu et al, 2013)
arg minLX ,LY
?
i
max(1?
?
i
x>i LXL
>
Yyi, 0)
s.t. ?Ln,X ?1 ? ?1, n = 1, 2, ? ? ? , Nx
?Lm,Y?1 ? ?1, m = 1, 2, ? ? ? , Ny
?Ln,X ?2 = ?2, n = 1, 2, ? ? ? , Nx
?Lm,Y?2 = ?2m = 1, 2, ? ? ? , Ny.
where i indices the original (post, response) pairs.
Our experiments (Section 6) indicate that this sim-
ple linear model can learn meaningful patterns, due
to the massive training set. For example, the im-
age of the word ?Italy? in the post in the latent s-
pace matches well word ?Sicily?, ?Mediterranean
sea? and ?travel?. Once the mapping LX and LY
are learned, the semantic matching score x>LXL>Yy
will be treated as a feature for modeling the overall
suitability of y as a response to post x.
POST-RESPONSE SIMILARITY: Here we use a
simple vector-space model for measuring the simi-
larity between a post and a response
simPR(x,y) =
x>y
?x??y?
. (3)
Although it is not necessarily true that a good re-
sponse has many common words as the post, but this
measurement is often helpful in finding relevant re-
sponses. For example, when the post and response
941
Figure 5: Diagram of the retrieval-based automatic response system.
both have ?National Palace Museum in Taipei?, it
is a strong signal that they are about similar topic-
s. Unlike the semantic matching feature, this simple
similarity requires no learning and works on infre-
quent words. Our empirical results show that it can
often capture the Post-Response relation failed with
semantic matching feature.
POST-POST SIMILARITY: The basic idea here is
to find posts similar to x and use their responses as
the candidates. Again we use the vector space model
for measuring the post-post similarity
simPP (x, x?) =
x>x?
?x??x??
. (4)
The intuition here is that if a post x? is similar to x its
responses might be appropriate for x. It however of-
ten fails, especially when a response to x? addresses
parts of x not contained by x, which fortunately can
be alleviated when combined with other measures.
5.2 Learning to Rank with Labeled Data
With all the matching features, we can learn a rank-
ing model with the labeled (post, response) pairs,
e.g., through off-the-shelf ranking algorithms. From
the labeled data, we can extract triples (x, y+, y?)
to ensure that score(x, y+) > score(x, y?). Appar-
ently y+ can be selected from labeled positive re-
sponse of x, while y? can be sampled either from
labeled negative negative or randomly selected ones.
Since the manually labeled negative instances are
top-ranked candidates according to some individual
retrieval model (see Section 5.1) and therefore gen-
erally yield slightly better results.
The matching features are mostly constructed by
combining the individual matching models, for ex-
ample the following two
? ?7(x, y): this feature measures the length of
the longest common string in the post and the
response;
? ?12(x, y): this feature considers both seman-
tic matching score between query post x and
candidate response y, as well as the similarity
between x and y?s original post x?:
?12(x, y) = SemMatch(x, y)simPP (x, x?).
In addition to the matching features, we also have
simple features describing responses only, such as
the length of it.
6 Experimental Evaluation
We perform experiments on the proposed dataset to
test our retrieval-based model as an algorithm for au-
tomatically generating response.
6.1 Performance of Models
We evaluate the retrieved models based on the fol-
lowing two metrics:
MAP This one measures the mean average preci-
sion (MAP)(Manning et al, 2008) associated
with the ranked list on C(reduced)x .
P@1 This one simply measures the precision of the
top one response in the ranked list:
P@1 =
#good top-1 responses
#posts
942
We perform a 5-fold cross-validation on the 422 la-
beled posts, with the results reported in Table 1. As
it shows, the semantic matching helps slightly im-
prove the overall performance on P@1.
Model MAP P@1
P2R 0.565 0.489
P2R + P2P 0.621 0.567
P2R + MATCH 0.575 0.513
P2R + P2P + MATCH 0.621 0.574
Table 3: Comparison of different choices of features,
where P2R stands for the features based on post-response
similarity, P2P stands for the features based on post-post
similarity, and MATCH stands for the semantic match fea-
ture.
To mimic a more realistic scenario on automatic
response model on Sina Weibo, we allow the system
to choose which post to respond to. Here we simply
set the response algorithm to respond only when the
highest score of the candidate response passes a cer-
tain threshold. Our experiments show that when we
choose to respond only to 50% of the posts, the P@1
increases to 0.76, while if the system only respond
to 25% of the posts, P@1 keeps increasing to 81%.
6.2 Case Study
Although our preliminary retrieval model does not
consider more complicated syntax, it is still able to
capture some useful coupling structure between the
appropriate (post, response) pairs, as well as the sim-
ilar (post, post) pairs.
Figure 6: An actual instance (the original Chinese text
and its English translation) of response returned by our
retrieval-based system.
Case study shows that our retrieval is fairly ef-
fective at capturing the semantic relevance (Section
6.2.1), but relative weak on modeling the logic con-
sistency (Section 6.2.2). Also it is clear that the se-
mantic matching feature (described in Section 5.1)
helps find matched responses that do not share any
words with the post (Section 6.2.3).
6.2.1 On Semantic Relevance
The features employed in our retrieval model are
mostly vector-space based, which are fairly good at
capturing the semantic relevance, as illustrated by
Example 1 & 2.
EXAMPLE 1:
P: It is a small town on an Spanish with 500
population, and guess what, they even
have a casino!
R: If you travel to Spain, you need to spend
some time there.
EXAMPLE 2:
P: One quote from Benjamin Franklin: ?We
are all born ignorant, but one must
work hard to remain stupid.?
R: Benjamin Franklin is a wise man, and
one of the founding fathers of USA.
However our retrieval model also makes bad
choice, especially when either the query post or the
response is long, as shown in Example 3. Here the
response is picked up because 1) the correspondence
between the word ?IT? in the post and the word
?mobile phone? in the candidate, and 2) the Chinese
word for ?lay off? in the post and the word for ?out-
dated? in the response are the same.
EXAMPLE 3:
P: As to the laying-off, I haven?t heard anything
about it. ?Elimination of the least competent?
is kind-off conventional in IT, but the ratio is
actually quite small.
R: Please don?t speak that way, otherwise you can
get outdated. Mobile phones are very expensive
when they were just out, but now they are fairly
cheap. Look forward, or you will be outdated.
The entity association is only partially addressed
with features like post-response cosine similarity,
treating entity name just as a word, which is appar-
ently not enough for preventing the following type
943
of mistakes (see Example 4 & 5) when the post and
response match well on other parts
EXAMPLE 4:
P: Professor Wang will give a curse on natural
language processing, starting next semester.
R: Jealous.. I wish I can attend Prof. Li?s
course too some time in the future.
EXAMPLE 5:
P: The fine China from Exhibition at the National
Palace Museum in Taipei
R: This drawing looks so nice. National Palace
Museum in Taipei is full of national treasures
6.2.2 On Logic Consistency
Our current model does not explicitly maintain
the logic consistency between the response and the
post, since Logic consistency requires a deeper anal-
ysis of the text, and therefore hard to capture with
just a vector space model. Below are two examples
which are semantically relevant, and correct with re-
spect to speech act, but logically inappropriate.
EXAMPLE 1:
P: I checked. Wang Fengyi is not my great grand-
father, although they?ve done similar deeds
and both were called ?Wang the Well-doer?.
R: wow, Wang Fengyi is your great grand-father
EXAMPLE 2:
P: We are looking for summer interns. We provide
books and lunch. If you are in Wu Han and
interested, drop us an email. Sorry we don?t
take any students outside Wu Han.
R: Are you looking for summer intern?
6.2.3 The Effect of Semantic Matching
The experiments also show that we may find inter-
esting and appropriate responses that have no com-
mon words as the post, as shown in the example be-
low. Our bi-linear semantic matching model how-
ever performs relatively poorly on long posts, where
the topics of the sentence cannot be well captured
by the sum of the latent vectors associated with each
word.
P: Eight England players stand in the penalty
area.
R1: What a classic match
R2: Haha, it is still 0:0, no goal so far
7 Summary
In this paper we propose a retrieval-based response
model for short-text based conversation, to leverage
the massive instances collected from social media.
For research in similar directions, we create a dataset
based on the posts and comments from Sina Weibo.
Our preliminary experiments show that our retrieval-
based response model, when combined with a large
candidate set, can achieve fairly good performance.
This dataset will be valuable for both training and
testing automatic response models for short texts.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematic-
s of statistical machine translation: parameter estima-
tion. Comput. Linguist., 19(2).
Rollo Carpenter. 1997. Cleverbot.
Sina Jafarpour and Christopher J. C. Burges. 2010. Fil-
ter, rank, and transfer the knowledge: Learning to chat.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?02, pages 133?
142, New York, NY, USA. ACM.
Anton Leuski and David R. Traum. 2011. Npceditor:
Creating virtual human dialogue using information re-
trieval techniques. AI Magazine, 32(2):42?56.
Diane Litman, Satinder Singh, Michael Kearns, and Mar-
ilyn Walker. 2000. Njfun: a reinforcement learning
spoken dialogue system. In Proceedings of the 2000
ANLP/NAACL Workshop on Conversational systems -
Volume 3, ANLP/NAACL-ConvSyst ?00, pages 17?
20, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, New York, NY,
USA.
Teruhisa Misu, Kallirroi Georgila, Anton Leuski, and
David Traum. 2012. Reinforcement learning of
question-answering dialogue policies for virtual muse-
um guides. In Proceedings of the 13th Annual Meeting
944
of the Special Interest Group on Discourse and Dia-
logue, SIGDIAL ?12, pages 84?93.
Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 583?593, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and
Steve Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of dia-
logue management strategies. Knowl. Eng. Rev., pages
97?126.
Ellen M Voorhees. 2002. The philosophy of infor-
mation retrieval evaluation. In Evaluation of cross-
language information retrieval systems, pages 355?
370. Springer.
Jason D. Williams and Steve Young. 2007. Partially ob-
servable markov decision processes for spoken dialog
systems. Comput. Speech Lang., 21(2):393?422.
Wei Wu, Zhengdong Lu, and Hang Li. 2013. Learning
bilinear model for matching queries and documents.
Journal of Machine Learning Research (2013 to ap-
pear).
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. Hhmm-based chinese lexical analyzer ict-
clas. SIGHAN ?03.
945
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1192?1202,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Sentiment-aligned Topic Model for Product Aspect Rating Prediction
Hao Wang
School of Computing Science
Simon Fraser University
Burnaby, BC, Canada
hwa63@sfu.ca
Martin Ester
School of Computing Science
Simon Fraser University
Burnaby, BC, Canada
ester@sfu.ca
Abstract
Aspect-based opinion mining has attracted
lots of attention today. In this paper, we
address the problem of product aspect rat-
ing prediction, where we would like to ex-
tract the product aspects, and predict as-
pect ratings simultaneously. Topic mod-
els have been widely adapted to jointly
model aspects and sentiments, but exist-
ing models may not do the prediction task
well due to their weakness in sentiment
extraction. The sentiment topics usually
do not have clear correspondence to com-
monly used ratings, and the model may
fail to extract certain kinds of sentiments
due to skewed data. To tackle this prob-
lem, we propose a sentiment-aligned topic
model(SATM), where we incorporate two
types of external knowledge: product-
level overall rating distribution and word-
level sentiment lexicon. Experiments on
real dataset demonstrate that SATM is ef-
fective on product aspect rating prediction,
and it achieves better performance com-
pared to the existing approaches.
1 Introduction
Online reviews have become an important source
of information for consumers. People tend to read
reviews to help them compare products, and make
informed decisions. As the volume of product re-
views continues to grow, it is often impossible to
read all of them, which calls for efficient methods
for opinion mining. Nowadays, for each product,
many websites aggregate the overall rating of re-
views, and display its distribution. However, this
cannot provide detailed information. For exam-
ple, two products may have similar overall rating
distributions, while people talk about different un-
satisfactory aspects. This problem has inspired a
new line of research on aspect-level opinion min-
ing(Hu and Liu, 2004).
An aspect refers to a rateable feature, such as
staff and location in hotel reviews, or size and
battery for digital camera reviews. In this paper,
we deal with the problem of product aspect rat-
ing prediction. The input is a collection of prod-
ucts, and each product is associated with a set of
reviews. The goal is to extract the corpus-level as-
pects, and predict the aspect ratings for each prod-
uct. This kind of fine-grained sentiment analysis
will help users efficiently digest the reviews, and
gain more insight into the product quality.
The product aspect rating prediction problem
usually involves two subtasks: aspect extraction
and sentiment identification(Titov and McDonald,
2008b). Given some text, we would like to know
what aspects it talks about, and what kind of sen-
timents are expressed. For example, given a sen-
tence ?the room is filthy?, we would like to know
that it talks about the aspect ?room?. Also, ?filthy?
is a sentiment word, and it expresses strongly neg-
ative sentiment towards the aspect ?room?.
Topic models(Blei et al., 2003; Hofmann, 1999)
have been popular in aspect-based opinion min-
ing(Liu, 2012). Existing works have used topic
models to extract only aspects(Titov and McDon-
ald, 2008a; Brody and Elhadad, 2010; Chen et
al., 2013), or jointly model aspects and sentiments
(Mei et al., 2007; Lin and He, 2009; Li et al.,
2010; Jo and Oh, 2011; Moghaddam and Ester,
2011; Lakkaraju et al., 2011; Sauper et al., 2011;
Mukherjee and Liu, 2012; Lazaridou et al., 2013;
Moghaddam and Ester, 2013; Kim et al., 2013). In
the joint modelling approaches, a sentiment topic
is usually modelled as a sentiment label-word dis-
tribution, analogous to the topic-word distribution
in standard topic models. However, the difference
is that the sentiment topics need to be ordered. If
the model is to be applied for aspect rating predic-
tion, the sentiment topics should have clear cor-
1192
respondence to the ratings. Suppose there are 5
sentiment topics with sentiment labels from 1 to
5. The sentiment topic with label i is expected
to correspond to the rating i on the 1-5 rating
scale. For example, the sentiment topic with label
5 should have high probability over positive sen-
timent words, so it expresses highly positive sen-
timent, which matches our natural interpretation
for the rating 5. In this case, sentiment labels and
ratings are aligned. However, in a standard topic
model, the learned sentiment topics may not have
clear correspondence with different ratings. Also,
if the positive reviews are dominant in the data, the
topic model may fail to capture the negative sen-
timents with any sentiment topic, so no sentiment
labels are matched with low ratings. If the senti-
ment labels are not correctly aligned to the ratings,
we cannot use these sentiment labels to predict as-
pect ratings. Consequently, the aspect rating pre-
diction accuracy is compromised, and the method
is less practical. We call this the sentiment label
alignment problem. To tackle this problem, mod-
els in the literature usually use some seed words
for each sentiment topic to define Dirichlet priors
with asymmetric concentration parameter vectors
(Sauper et al., 2011; Kim et al., 2013), or use seed
words to initialize word assignment to sentiment
topic(Lin and He, 2009), or both(Li et al., 2010;
Jo and Oh, 2011). However, these seed words
are usually arbitrarily selected, and how to define
asymmetric priors is not clear, especially when we
would like to capture more than two (positive and
negative) kinds of sentiments.
In this paper, we propose a sentiment-aligned
topic model(SATM) for product aspect rating pre-
diction, which focuses the sentiment label align-
ment problem. We use two kinds of external
knowledge: the product overall rating distribution,
and a sentiment lexicon. For each product, the
overall rating distribution is available on most on-
line review websites. It provides the big picture of
the product-level sentiments. In SATM, for each
product and each aspect, we define a multinomial
distribution over sentiment labels, with prior pa-
rameterized by the overall rating distribution. Sen-
timent lexicon is constructed by linguistic experts,
and every word in the lexicon is associated with a
sentiment polarity score(Taboada et al., 2011). We
treat the polarity score as an extra word feature
in a semi-supervised framework. By incorporat-
ing both product-level and word-level knowledge
into the model, the sentiment labels can be aligned
with ratings, and the extracted sentiment topics
can capture different kinds of sentiments, ranging
from highly positive to highly negative. Experi-
ments on a TripAdvisor dataset demonstrate that
our method can effectively deal with the sentiment
label alignment problem, and outperforms state-
of-the-art methods in terms of product aspect rat-
ing prediction accuracy.
2 Related work
Several methods have been proposed for product
aspect rating prediction, and many of them are
based on topic models.
In (Lu et al., 2009), the authors studied the prob-
lem of generating an aspect rating summary for
short comments. The text was first preprocessed
into phrases of the format <headterm, sentiment
word>, and the headterms are clustered by Struc-
tured PLSA to find K major aspects. Then, phrase
ratings are predicted by either Local Prediction
or Global Prediction, and they are aggregated to
get aspect ratings. The method in (Brody and El-
hadad, 2010) also first uses topic models to find as-
pects. Then, for each aspect, it extracts all the rel-
evant adjectives, and builds a conjunction graph.
A label propagation algorithm(Zhu and Ghahra-
mani, 2002) is used on the graph to learn the senti-
ment polarity score of adjective words. Although
this approach is not proposed for aspect rating pre-
diction, it can be used for this task if the polar-
ity scores of adjective words are aggregated for
each aspect. All the methods above perform as-
pect extraction and sentiment identification sepa-
rately, while our approach takes a joint modelling
approach so that different subtasks can potentially
reinforce with each other. To demonstrate this, we
use these methods as baselines in our experiments.
Wang et al. worked on the Latent Aspect Rat-
ing Analysis problem(Wang et al., 2010; Wang
et al., 2011), the task of inferring aspect ratings
for each review and the relative weights review-
ers have placed on each aspect. In (Wang et al.,
2010), aspect keywords are provided as user input,
and a two-stage method, called Latent Rating Re-
gression(LRR), is proposed. The first stage uses
a bootstrapping algorithm to obtain more related
words for each aspect, and segments the document
content. In the second stage, the overall rating is
?generated? as weighted combination of the latent
aspect ratings, and LRR is used to infer both the
1193
weights and aspect ratings. Their follow-up work
(Wang et al., 2011) does not need keyword speci-
fication from users, and replaces the bootstrapping
method with a topic model. However, both meth-
ods implicitly require that each review talks about
all aspects, which is not always true due to the data
sparsity in online reviews.
In (Moghaddam and Ester, 2011), ILDA was
proposed for product aspect rating prediction.
Later, it was extended to FLDA (Moghaddam
and Ester, 2013) to address the cold start prob-
lem, when there are few reviews associated with
a product. Similar to (Lu et al., 2009), in ILDA
and FLDA, a preprocessing step parses the text
into phrases of the format <headterm, sentiment
word>, and a review is modelled as a bag of
phrases. We also adopt this assumption in our
model. The method in (Sauper et al., 2011; Sauper
and Barzilay, 2013) does not use phrases, but in-
stead uses ?snippets?, and an snippet is a short
sentence or phrase. However, the sentiment label
alignment problem is not well addressed in these
models, which limits their practicality. ILDA
and FLDA did not deal with this problem. The
model in (Sauper et al., 2011; Sauper and Barzi-
lay, 2013) follows the most common approach of
using seed words to define asymmetric priors. It
supports only two kinds of sentiment topics: pos-
itive and negative, while how to define asymmet-
ric priors for more sentiment topics becomes un-
clear. More importantly, the prior approach may
not work well in practice(see Experiment Section).
Lakkaraju et al. try to tackle the sentiment label
alignment problem by assuming that the overall
rating is generated as response variable(Lakkaraju
et al., 2011), with the sentiment topic propor-
tions as features. However, how the sentiment la-
bels are related to ratings is still unknown until
learned, and we may not get the desired alignment.
Lazaridou et al. attempt to connect sentiment la-
bels with ratings by Kronecker symbol, but this
method only applies to three sentiment polarities:
?1(negative), 0(neutral), +1(positive), and it does
not explore the word-level lexicon, which is also
an important source of knowledge.
Another line of research on product aspect rat-
ing prediction or summarization does not use topic
models, but relies mainly on word frequency and
grammatical relations(Hu and Liu, 2004; Popescu
and Etzioni, 2005; Blair-goldensohn et al., 2008),
or specialized review selection(Long et al., 2014).
In this case, the extracted aspect words need to
be clustered manually. For example, picture and
photo may refer to the same aspect in digital cam-
era reviews. By comparison, topic modelling ap-
proaches extract aspect words and cluster them si-
multaneously.
Our method incorporates the product overall
rating distributions and sentiment lexicons into
the model, so it is also related to topic models
which use observed features or domain knowl-
edge(Mimno and McCallum, 2008; Andrzejewski
et al., 2009; Andrzejewski et al., 2011). Mimno et
al. introduces two general frameworks to integrate
observed features into the generative process:
downstream and upstream topic models(Mimno
and McCallum, 2008). In the context of aspect-
based opinion mining, MaxEnt-LDA(Zhao et al.,
2010) integrates a discriminative maximum en-
tropy component to help separate aspect words
and sentiment words. The SAS model (Mukherjee
and Liu, 2012) uses seed words to provide guid-
ance for aspect discovery, and MC-LDA (Chen et
al., 2013) uses must-links and cannot-links to ex-
tract coherent aspects. However, MaxEnt-LDA,
SAS and MC-LDA cannot be used for aspect rat-
ing prediction, since they fail to identify the senti-
ment polarity of sentiment words.
3 Method
3.1 Preliminaries
We first introduce several key concepts used in our
model.
Products: Let P = {P
1
, P
2
, . . . } be a set of
products. Each product P
i
is associated with a
set of reviews D
i
= {d
1
, d
2
, . . . d
N
i
}, and also an
overall rating distribution Y
i
. Y
i
is a multinomial
distribution on R ratings. It is available on most
online review websites, and usually R = 5.
Aspects: An aspect is a rateable feature of a
product, and each aspect is modelled as a distribu-
tion over aspect words. The number of aspects is
predefined as K.
Sentiment topics: A sentiment topic is mod-
elled as a distribution over sentiment words, and
each sentiment topic is associated with a sentiment
label. To make it consistent with commonly used
rating scale, we assume there are R sentiment la-
bels, corresponding to the R ratings. The chal-
lenge is that sentiment labels with higher values
are expected to be associated with sentiment top-
ics which express more positive sentiments, so that
1194
we can match sentiment labels with ratings.
Phrases: An opinion phrase f =< h,m > is a
pair of aspect word h and sentiment word m, such
as < room, filthy >(Lu et al., 2009; Moghad-
dam and Ester, 2011). For each product P
i
, we
first parse the related reviews D
i
into phrases F
i
,
and each product can be modelled as a bag of
phrases.
Sentiment lexicons : A sentiment lexicon L is
a list of sentiment words, and each word m ? L is
associated with a sentiment polarity score s
m
. s
m
can take T values. Note that the lexicon L usually
only covers a small subset of sentiment words in
the whole vocabulary.
Sentiment association: The sentiment label
takesR values, and there are T different values for
the polarity score in the sentiment lexicon. How-
ever, the relation between sentiment labels and po-
larity scores are unknown. If we have training in-
stances where a sentiment word m is associated
with both a sentiment label r
m
and polarity score
s
m
, we can build a classifier, where the explana-
tory variable for the classifier is a sentiment label,
and outcome is the polarity score. In this case,
H(s
m
|r
m
) can be interpreted as the probability of
observing a polarity score s
m
, given its sentiment
label r
m
. We refer to this probability H as senti-
ment association. This is a key component in our
model. It naturally bridges the gap between sen-
timent labels and polarity scores, and captures the
uncertainty in their relations. Note that H can be
trained independent of the topic model part. For
each training instance, suppose the sentiment word
is m ? L, we need to know its sentiment label
r
m
and polarity score s
m
. s
m
can be retrieved di-
rectly from the sentiment lexicon, and r
m
can be
either manually or automatically annotated. For
example, suppose the word m appears in review
d, we can assign the overall rating of d as its sen-
timent label. In this case, each word m ? L can
be associated with multiple training instances that
have the same value for s
m
but different sentiment
labels r
m
. We adopt this approach to automati-
cally annotate sentiment labels, and details are de-
scribed in the Experiments section.
3.2 Problem definition
The product aspect rating prediction problem can
be defined as follows. The input is a set of prod-
ucts P . Each product P
i
has a bag of phrases F
i
,
and an overall rating distribution Y
i
over R rat-
Figure 1: Graphical model of SATM
ings. The output is the K corpus-level aspects,
and for each product, we predict its ratings on the
K aspects, also in the [1, R] rating scale. We as-
sume products in P are in the same category so
they share the same aspects.
3.3 The SATM model
We introduce the Sentiment-aligned Topic
Model(SATM) in this section, and its graphical
representation is shown in Figure 1. Note that the
sentiment association H is observed, because it is
trained independently of the topic model part.
At the word level, each observed phrase <
h,m > is associated with two latent variables:
aspect z and sentiment label r. Aspect z models
what aspect this phrase talks about, and r deter-
mines the sentiment of m. If m is in the senti-
ment lexicon, we assume r is also responsible for
generating a word feature v
m
, based on the senti-
ment association H , which is equal to its polarity
score s
m
in the lexicon. In this case, the observed
data becomes (< h,m >, v
m
), and the latent sen-
timent label r is responsible for generating both
word m, and word feature v
m
. For example, for
the phrase<room, filthy>, we observe a word fea-
ture v = ?5, since the sentiment polarity score for
the word ?filthy? is?5. GivenH , sentiment labels
1 or 2 are more likely to generate a word feature
?5. Also, people tend to use ?filthy? to express
low ratings, like 1 or 2, so the sentiment labels and
ratings can be aligned.
At the product level, for each product p and
each aspect k, we define a multinomial distribu-
tion ?
p,k
overR sentiment labels. Since Y
p
already
gives us the big picture about the overall senti-
1195
ment expressed on this product, we assume ?
p,k
is drawn from a dirichlet distribution Dir(pip,k)
with asymmetric concentration parameters, where
pip,k = f(Yp,?k, ?
b
). We can use a linear
parametrization, and set
f(Yp,?k, ?
b
) = ?
1
k
Yp + ?
0
k
+ ?
b
(1)
?
1
k
captures the influence of the product overall
rating distribution, and can favour certain sen-
timent labels in the prior. ?
0
k
and ?
b
are the
aspect-specific and corpus-level bias, respectively.
Through this linear parametrization, we build a di-
rect matching between sentiment label i and rating
i. For example, for a product p, if its overall rat-
ing distribution Y
p
has high probability over rat-
ing 4, for aspect k, we assume its product-aspect-
sentiment label distribution also has high probabil-
ity on sentiment label 4 in the prior. The actual as-
pect rating is affected by both the text which talks
about aspect k, and also the prior.
To sum up, we assume the generative process as
follows:
? For each aspect k = 1, 2, . . .K,
? draw an aspect-word distribution ?
a
k
?
Dir(?
a
)
? For each sentiment label r = 1, 2, . . . R,
draw an aspect-sentiment label-word
distribution ?
s
k,r
? Dir(?
s
)
? For each product p ? P ,
? draw a product-aspect distribution ?
p
?
Dir(?)
? for each aspect k, draw a product-
aspect-sentiment label distribu-
tion ?
p,k
? Dir(pip,k) where
pip,k = f(Yp,?k, ?
b
)
? For each phrase f =< h,m > of product p,
1. Draw an aspect z from ?
p
2. Draw a sentiment label r from ?
p,z
3. Draw an aspect word h from ?
a
z
4. Draw a sentiment word m from ?
s
z,r
.
If m ? L, generate a word feature v
m
based on H .
By integrating out ?, ? and ?, the joint proba-
bility can be defined as:
P (z, r,h,m,v|?,?
a
,?
s
,pi,H) =
P (z|?)P (r|z,pi)P (h|z,?
a
)
P (m|z, r,?
s
)P (v|r,H) (2)
3.4 Inference
We use Gibbs Sampling(Griffiths and Steyvers,
2004) to estimate the posterior distribution given
the observed data.
We jointly sample the aspect z and sentiment
label r for the ith phrase < h,m > of product p,
given the assignments of other phrases:
P (z
i
= k, r
i
= l|z
?i
, r
?i
,h,m,v) ?
(n
p,k
+ ?)
n
a
k,h
+ ?
a
?
h
?
(n
a
k,h
?
+ ?
a
)
n
p,k,l
+ pi
p,k,l
?
l
?
(n
p,k,l
?
+ pi
p,k,l
?
)
n
s
k,l,m
+ ?
s
?
m
?
(n
s
k,l,m
?
+ ?
s
)
g(m, l)
(3)
where g(m, l) = H(v
m
|l) if m ? L. In this
case, when we sample the sentiment label r for this
phrase, the probability of generating word feature
v
m
from r is also considered. For example, the
word ?excellent? has a word feature value v
m
= 5.
Based on H , the probability of generating a word
feature 5 is higher for sentiment labels with larger
values. If m /? L, there is no g(m, l) term, since
no word feature is associated with this phrase. In
Equation 3, n
p,k
is the number of times aspect k
is assigned to phrases of product p, and n
a
k,h
is
the number of times aspect word h is assigned to
aspect k. n
p,k,l
is the number of times sentiment
label l is assigned to aspect k for product p, and
n
s
k,l,m
is the number of times sentiment word m
is assigned to aspect k and sentiment label l. All
these counts exclude assignments for the current
phrase < h,m >.
Based on the samples, we can estimate ?
p,k,r
as:
?
p,k,r
=
n
p,k,r
+ pi
p,k,r
?
r
?
(n
p,k,r
?
+ pi
p,k,r
?
)
(4)
Since sentiment labels and atings are aligned, the
aspect rating t
pk
of product p on aspect k can be
simply calculated as the expectation of ?p,k:
t
pk
=
?
r
?
p,k,r
? r (5)
4 Experiments
In this section, we describe the experiments and
analyze the results.
1196
4.1 Dataset
We use the TripAdvisor dataset
1
(Wang et al.,
2010) for evaluation, since in this dataset, reviews
are not only associated with overall ratings, but
also with ground truth aspect ratings on 7 aspects:
value, room, location, cleanliness, check in/front
desk, service, business service. All the ratings in
the dataset are in the range from 1 star to 5 stars.
We first remove reviews with any missing aspect
ratings or very short reviews(less than three sen-
tences). Then we adopt the dependency parser
technique to identify opinion phrases, and collect
phrases with adjective sentiment words. The de-
pendency parser can deal with conjunctions, nega-
tions and bigram aspect words, and it results in the
best performance according to (Moghaddam and
Ester, 2012). Some sample phrases are shown in
Table 1. All words are converted into lower case,
and we remove phrases containing words that ap-
pear no more than 10 times or stop words. Since
we are only interested in product-level aspect rat-
ing prediction, for each product, we aggregate all
the review overall ratings to get the overall rating
distribution. The statistics of the dataset is shown
in Table 2. The average rating is the rating av-
eraged over all reviews and all products. As we
can see, positive reviews are dominant in the data,
which raises the challenge of discovering negative
sentiment topics.
Sentences Phrases
The room, facing the
courtyard, was large and
comfortable.
<room, large>,
<room, comfortable>
The room was not really
clean.
<room, no clean>
Internet access was
available.
<Internet access,
available>
Table 1: Sample extracted phrases
#Products #Reviews Avg rating #Phrases
1850 61306 4.03 740982
Table 2: Statistics of the dataset
4.2 Metrics
We use three evaluation metrics for comparison.
RMSE: Root-mean-square error is used to mea-
sure the difference between the predicted aspect
1
http://sifaka.cs.uiuc.edu/
?
wang296/
Data/index.html
ratings and ground truth aspect ratings. It is de-
fined as:
RMSE =
?
?
p
?
k
(t
pk
?
?
t
pk
)
2
|P | ?K
(6)
where t
pk
is the predicted aspect rating for product
p on aspect k, and
?
t
pk
is the ground truth.
Precision@N: For each aspect k, we rank the
hotels based on their predicted aspect ratings, and
get the top N results. A hotel is considered rele-
vant if its ground truth aspect rating is in the top
10% of the ground truth aspect ratings of all ho-
tels. Precision@N is defined as the percentage of
the top N results that are relevant:
Precision@N =
|{relevant hotels} ? {top N ranked hotels}|
N
(7)
We use N = 10, and the result is averaged over K
aspects.
?hotel: Pearson correlation across hotels(Wang
et al., 2010) is defined as:
?hotel =
?
k
?(tk, ?tk)
K
(8)
where tk is the predicted aspect rating vector for
all hotels on aspect k, and
?
tk is the corresponding
ground truth vector. ?(tk, ?tk) is the Pearson cor-
relation between these two vectors. It measures
how the predicted ratings of aspect k can preserve
the order in the ground truth(Wang et al., 2010).
If we can predict an aspect-specific ranking sim-
ilar to the ground truth, we can use the predicted
aspect ratings to answer questions like ?Is hotel a
better than hotel b on aspect k??
4.3 Baselines
The first three baselines are Local Prediction,
Global Prediction and Graph Propagation.
They all separate aspect extraction and sentiment
identification. For each phrase f =< h,m > from
review d of product p, we first find the aspect as-
signment of this phrase. Then, we use three meth-
ods to get the phrase rating. Local Prediction(Lu et
al., 2009) simply uses the overall rating of d as its
phrase rating. Global Prediction(Lu et al., 2009)
trains a multi-class classifier to classify the senti-
ment word m into a rating category r ? 1, 2 . . . R,
1197
Figure 2: Method for aspect extraction in Local
Prediction, Global Prediction and Graph Propaga-
tion
then assigns r as the phrase rating. Graph Prop-
agation(Brody and Elhadad, 2010) builds a con-
junction graph for sentiment words, and uses a La-
bel Propagation algorithm on the graph to learn the
sentiment polarity score for each sentiment word.
The score of m is set as phrase rating. Finally,
we aggregate all the phrases of each aspect to pre-
dict the aspect ratings. To apply these methods in
our experiments, in the aspect extraction step, we
adapt our model to extract only aspects, as shown
in Figure 2. In this simplified model, no sentiment
labels is involved, and the latent aspect explains
both the aspect word and sentiment word.
ILDA(Moghaddam and Ester, 2011) was pro-
posed for aspect rating prediction, but it fails to
deal with the sentiment label alignment problem,
so it cannot be directly used for this task. We adopt
the common approach of providing seed words to
set priors for each sentiment topic.
LRR(Wang et al., 2010) was proposed to pre-
dict aspect ratings for each review, but it can also
be used to predict product aspect ratings by ag-
gregating all the reviews of a product into a single
?h-review?(Wang et al., 2011). First, we can run
a topic model to learn aspects, and annotate each
sentence with an aspect. Then LRR is applied on
the annotated sentences to predict aspect ratings.
This approach provided the best result, according
to (Wang et al., 2011). In the first step we use the
sentence-LDA(Jo and Oh, 2011) to annotate sen-
tences, which is slightly different from the original
method, but still provides a good analogy.
We also test two simplified version of the SATM
model. First, we remove the part which involves
sentiment lexicons, so we only use the product
overall rating distribution. We call this method
SATM-O. Second, we use only sentiment lexi-
cons, ignoring the influence of overall rating dis-
tribution. We call it SATM-L. These two baselines
can help us identify how the sentiment lexicon and
overall rating distribution can improve the results,
if used separately.
Our last baseline simply uses the overall rating
of a hotel as its aspect ratings. For each hotel, its
overall rating is defined as the average overall rat-
ing of its reviews. This method is referred to as
Overall.
4.4 Experimental Setup
For all topic modelling based approaches, the
number of aspects is set to 7. Since we can evalu-
ate aspect rating prediction only on the predefined
aspects, we need to ensure the discovered aspects
match the predefined aspects. To do this, we adopt
the common approach of providing a few seed
words for each aspect as priors, as in (Wang et al.,
2010). The seed words are listed in Table 3. There
may be better methods to use seed words for as-
pect discovery (Jagarlamudi et al., 2012; Mukher-
jee and Liu, 2012), and it would be interesting to
combine their methods with ours. However, this
is beyond the scope of this paper, and we list it as
future work.
Aspects Seed words
Value value, price, worth
Room room, rooms
Location location
Cleanliness room, dirty, smelled, clean
Check in/front desk staff
Service service, breakfast, food
Business service internet, wifi
Table 3: Seed words for aspect discovery
We use 5 sentiment labels in SATM, SATM-
L and SATM-O, as this is the number of dis-
tinct ratings. The lexicon L used in our experi-
ment is part of (Taboada et al., 2011) where words
are associated with polarity scores in the range
[?5,?1] ? [1, 5]. We observe that words with po-
larity score 1 and?1 express too weak sentiments,
so we discard them in our experiment. To get
training instances for sentiment association H , we
treat each appearance of wordm ? L in the data as
one training instance. The polarity score s
m
is di-
rectly retrieved from L, and the sentiment label r
m
is the overall rating of review d where m appears.
This approach avoids the need for manual annota-
tion of sentiment labels, and the annotation result
captures the characteristics of the dataset. How-
1198
ever, all training instances in a review will have
the same sentiment label, which means that we as-
sume all sentiment words in a review express the
same sentiment, no matter what aspects they talk
about. This is not true, thus will introduce noise to
the training. To reduce noise, for words with pos-
itive polarity score, we ignore their appearance in
reviews with rating 1 and 2, since we assume pos-
itive sentiment words rarely express negative sen-
timents, even if they appear in negative reviews.
Therefore, H(s
m
|r
m
) = 0 for r
m
= 1, 2 and s
m
in the range [2, 5]. A similar method is used to deal
with words with negative polarity score.
For Global Prediction, in (Lu et al., 2009), the
prior for the multi-class classifier is uniform, while
in our experiment, for product p, we used product
overall rating distribution on r as the prior for rat-
ing category r, which achieves better results than
uniform prior.
The Graph Propagation method requires a small
set of sentiment words as seeds, from which the al-
gorithm can learn sentiment score for other words.
The method in (Brody and Elhadad, 2010) con-
structs these seed words based on morphology in
an unsupervised way, and can only support two
kinds of sentiment: positive and negative. In our
experiment, since the sentiment lexicon is avail-
able, the sentiment seed words are from the lexi-
con, and we update the polarity score for those not
in the lexicon.
For ILDA, since we need to provide seed words
as priors for sentiment topics, we have two op-
tions, and we use both for experiment. First,
we can employ the common approach of using
two sentiment labels(R=2, positive and negative).
Then, words with positive polarity scores in lexi-
con L are used as priors for the positive sentiment
topic, and similarly words with negative polarity
scores for negative sentiment topic. An alternative
approach is to use 5 sentiment labels(R=5). It pro-
vides finer grained sentiment extraction, but raises
the question of how to choose seed words for each
sentiment topic. To do this, we use the full senti-
ment lexicon in (Taboada et al., 2011), where sen-
timent words have polarity score in the range of
[?5,?1] ? [1, 5]. We divide the lexicon, and use
words with polarity score 4 and 5 as prior for the
sentiment topic with label 5. Then, words with po-
larity score 2 and 3 are used for the sentiment topic
with label 4, and so on.
For all topic modelling based approaches, we
set the number of iterations for Gibbs Sampling
to 3000, and take samples from the markov chain
every 50 iterations after a burn-in period of 1000
iterations. In SATM and SATM-O, for all aspects
k, we need to choose the parameters ?k and also
w
b
. We use a small portion of dataset with ground
truth to choose the best value, and we set ?
1
k
= 20,
w
0
k
= 0.01, w
b
= 0. Automatically learning these
parameters are feasible. One possible option is to
use stochastic EM sampling scheme, as in (Mimno
and McCallum, 2008). For the LRR implementa-
tion
2
, we use the default parameters included in
the package, and train the model with seed words
provided by the author(Wang et al., 2010).
4.5 Results
The experimental results are listed in Table 4. For
RMSE, the smaller the better, while for the other
two measures, the larger the better. Graph Prop-
agation, ILDA and SATM-L do not use the over-
all ratings(except for training sentiment associa-
tion H), so we group them together. Similarly
we group Local Prediction, Global Prediction,
SATM-O and SATM. The Overall method is a spe-
cial baseline that does not do any aspect based pre-
diction. For the LRR method, after the first step of
sentence annotation, we notice that sentence-LDA
fails to annotate the ?h-review? of some hotel with
all 7 aspects, mainly because these hotels are as-
sociated with less reviews. In this case, the LRR
model will fail in the second step, so we do not
include LRR in Table 4. Instead, we compared
our method with LRR on a subset of products that
comment on all aspects based on the sentence an-
notation. There are 1533 hotels in this subset, and
the result is shown in Table 5. Note that our exper-
imental results for LRR are far worse than those
reported in the original paper(Wang et al., 2011).
We believe this maybe due to different parameter
settings, or due to the choice of different reviews.
We observe that SATM achieves the best RMSE
value, i.e., it produces the most accurate aspect rat-
ing prediction. The Overall method does better in
ranking all the hotels(?
hotel
), but SATM is better
at ranking top hotels(P@10). When we compare
the results of SATM with SATM-L and SATM-
O, we find that the good performance of SATM
is mainly due to the use of the overall rating distri-
bution. On one hand, this is reasonable, since in-
2
http://sifaka.cs.uiuc.edu/
?
wang296/
Codes/LARA.zip
1199
Sentiment label Top sentiment words
1 old, dirty, worn, older, dark, stained, broken, dated, outdated, bad
2 small, tiny, little, noisy, single, double, uncomfortable, smaller, larger, narrow
3 large, double, big, mini, hard, main, huge, twin, single, jacuzzi
4 nice, comfortable, modern, clean, new, good, great, flat, big, comfy
5 large, huge, great, beautiful, big, lovely, separate, spacious, wonderful, excellent
Table 6: Top sentiment words for aspect ?room? with different sentiment labels
Methods RMSE P@10 ?
hotel
ILDA,R=2 1.202 0.30 0.193
ILDA,R=5 1.096 0.257 0.222
Graph Propagation 0.718 0.271 0.442
SATM-L 0.774 0.443 0.483
Local Prediction 0.572 0.486 0.761
Global Prediction 0.625 0.30 0.778
SATM-O 0.429 0.80 0.841
SATM 0.384 0.814 0.854
Overall 0.415 0.80 0.863
Table 4: Experimental results except LRR
Methods RMSE P@10 ?
hotel
LRR 1.018 0.3 0.404
SATM 0.373 0.829 0.849
Table 5: Experimental comparison with LRR
tuitively aspect ratings usually do not diverge too
far from the overall rating, especially for hotels
with higher overall ratings. As we can see from
the result of Overall, the overall rating has good
correlation with aspect ratings, and using overall
rating only is already a strong predictor for as-
pect ratings. Also, in most cases, methods using
overall ratings(Overall and the four methods in the
middle of Table 4) are better than others(first four
methods). On the other hand, we should not rely
only on the overall rating distribution. By incor-
porating the sentiment lexicon, for RMSE, SATM
achieves 10% improvement over SATM-O and 7%
improvement than Overall. Also, the overall rating
may not always be a good aspect rating predictor,
depending on the dataset.
To take a closer look at cases where the over-
all rating is not a good aspect rating predictor,
we evaluate the RMSE on different subsets of ho-
tels. We divide the hotels into different overall rat-
ing ranges: [1.2), [2,3), [3,4) and [4,5]. The re-
sults are shown in Table 7. Going from the [4,5]
group to [1,2) group, the overall rating becomes
less and less reliable to predict aspect ratings, and
the gain of SATM increases compared to SATM-
Methods [1,2) [2,3) [3,4) [4-5]
Local Prediction 0.789 0.772 0.621 0.456
Global Prediction 1.013 0.884 0.584 0.567
SATM-O 0.703 0.564 0.446 0.359
SATM 0.606 0.494 0.394 0.332
Overall 0.735 0.612 0.431 0.320
Table 7: RMSE on hotels with different overall
rating ranges
O and Overall. For a hotel with higher overall
rating(good hotel), its aspect ratings are closer to
the overall rating. This matches our intuition that
good hotels are expected to be good on most as-
pects, if not on all aspects. For a hotel with av-
erage and lower overall rating, the average differ-
ence between aspect ratings and overall rating is
larger. In this case, the overall rating can not tell
us the whole story, which calls for aspect based
prediction. Our method achieves the best RMSE
gain on this group of hotels.
4.6 Qualitative analysis
To provide a qualitative analysis, we can list the
top words for the aspect-sentiment label-word dis-
tributions. In Table 6, we list them for the aspect
?room?, with 5 different sentiment labels. We ob-
serve that, as the sentiment label value increases,
the sentiment topics express more and more pos-
itive sentiments. This means the sentiment labels
and ratings are indeed aligned, so that we can use
these sentiment labels to predict ratings.
5 Conclusion and future work
In this paper, we proposed a sentiment aligned
topic model(SATM) for product aspect rating pre-
diction. By incorporating the overall rating distri-
bution and a sentiment lexicon, our SATM model
can align sentiment labels with ratings. Experi-
ments on a TripAdvisor dataset demonstrate the
effectiveness of SATM on aspect rating prediction.
In SATM, for each product and each aspect, the
multinomial distribution over sentiment labels has
1200
prior parameterized by product overall rating dis-
tribution. We assume linear dependency, but it will
be interesting to explore other dependencies. An-
other direction is to learn the parameters ?k auto-
matically, so that ?k can be different for different
k, capturing the influence of the overall rating on
different aspects.
Finally, we assume each phrase is associated
with one latent aspect. However, aspects may
be correlated. For example, the phrase <room,
filthy> gives us information about the aspect room
and also the aspect cleanliness. To deal with this
problem, we can relax the assumption that one
phrase talks about one aspect, or we can model
correlation among aspects.
Acknowledgments
This research is supported by NSERC Discovery
Grant. The authors thank Dr. Maite Taboada for
providing the sentiment lexicon.
References
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic
modeling via dirichlet forest priors. In Proceedings
of the 26th Annual International Conference on Ma-
chine Learning, ICML ?09, pages 25?32, New York,
NY, USA. ACM.
David Andrzejewski, Xiaojin Zhu, Mark Craven, and
Benjamin Recht. 2011. A framework for incorpo-
rating general domain knowledge into latent dirich-
let allocation using first-order logic. In Proceedings
of the Twenty-Second International Joint Conference
on Artificial Intelligence - Volume Volume Two, IJ-
CAI?11, pages 1171?1177. AAAI Press.
Sasha Blair-goldensohn, Tyler Neylon, Kerry Hannan,
George A. Reis, Ryan Mcdonald, and Jeff Reynar.
2008. Building a sentiment summarizer for local
service reviews. In WWW Workshop on NLP in the
Information Explosion Era.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993?1022, March.
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
?10, pages 804?812, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun
Hsu, Mal?u Castellanos, and Riddhiman Ghosh.
2013. Exploiting domain knowledge in aspect ex-
traction. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2013, 18-21 October 2013, Grand Hy-
att Seattle, Seattle, Washington, USA, A meeting of
SIGDAT, a Special Interest Group of the ACL, pages
1655?1667.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl. 1):5228?5235,
April.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22Nd Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ?99,
pages 50?57, New York, NY, USA. ACM.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Jagadeesh Jagarlamudi, Hal Daum?e, III, and
Raghavendra Udupa. 2012. Incorporating lex-
ical priors into topic models. In Proceedings of
the 13th Conference of the European Chapter of
the Association for Computational Linguistics,
EACL ?12, pages 204?213, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Yohan Jo and Alice H. Oh. 2011. Aspect and sen-
timent unification model for online review analy-
sis. In Proceedings of the Fourth ACM Interna-
tional Conference on Web Search and Data Mining,
WSDM ?11, pages 815?824, New York, NY, USA.
ACM.
Suin Kim, Jianwen Zhang, Zheng Chen, Alice H.
Oh, and Shixia Liu. 2013. A hierarchical aspect-
sentiment model for online reviews. In Proceedings
of the Twenty-Seventh AAAI Conference on Artificial
Intelligence, July 14-18, 2013, Bellevue, Washing-
ton, USA.
Himabindu Lakkaraju, Chiranjib Bhattacharyya, Indra-
jit Bhattacharya, and Srujana Merugu. 2011. Ex-
ploiting coherence for the simultaneous discovery
of latent facets and associated sentiments. In Pro-
ceedings of the Eleventh SIAM International Con-
ference on Data Mining, SDM 2011, April 28-30,
2011, Mesa, Arizona, USA, pages 498?509.
Angeliki Lazaridou, Ivan Titov, and Caroline
Sporleder. 2013. A bayesian model for joint
unsupervised induction of sentiment, aspect and
discourse representations. In Proceedings of the
51st Annual Meeting of the Association for Com-
putational Linguistics, ACL 2013, 4-9 August 2013,
Sofia, Bulgaria, Volume 1: Long Papers, pages
1630?1639.
1201
Fangtao Li, Minlie Huang, and Xiaoyan Zhu. 2010.
Sentiment analysis with global topics and local de-
pendency. In Proceedings of the Twenty-Fourth
AAAI Conference on Artificial Intelligence, AAAI
2010, Atlanta, Georgia, USA, July 11-15, 2010.
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Pro-
ceedings of the 18th ACM Conference on Informa-
tion and Knowledge Management, CIKM ?09, pages
375?384, New York, NY, USA. ACM.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool Publishers.
Chong Long, Jie Zhang, Minlie Huang, Xiaoyan Zhu,
Ming Li, and Bin Ma. 2014. Estimating feature rat-
ings through an effective review selection approach.
Knowl. Inf. Syst., 38(2):419?446.
Yue Lu, ChengXiang Zhai, and Neel Sundaresan.
2009. Rated aspect summarization of short com-
ments. In Proceedings of the 18th International
Conference on World Wide Web, WWW ?09, pages
131?140, New York, NY, USA. ACM.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: Modeling facets and opinions in weblogs. In
Proceedings of the 16th International Conference on
World Wide Web, WWW ?07, pages 171?180, New
York, NY, USA. ACM.
David M. Mimno and Andrew McCallum. 2008.
Topic models conditioned on arbitrary features with
dirichlet-multinomial regression. In the Conference
on Uncertainty in Artificial Intelligence, pages 411?
418.
Samaneh Moghaddam and Martin Ester. 2011. Ilda:
Interdependent lda model for learning latent aspects
and their ratings from online product reviews. In
Proceedings of the 34th International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, SIGIR ?11, pages 665?674, New
York, NY, USA. ACM.
Samaneh Moghaddam and Martin Ester. 2012. On the
design of lda models for aspect-based opinion min-
ing. In Proceedings of the 21st ACM International
Conference on Information and Knowledge Man-
agement, CIKM ?12, pages 803?812, New York,
NY, USA. ACM.
Samaneh Moghaddam and Martin Ester. 2013. The
flda model for aspect-based opinion mining: Ad-
dressing the cold start problem. In Proceedings of
the 22Nd International Conference on World Wide
Web, WWW ?13, pages 909?918.
Arjun Mukherjee and Bing Liu. 2012. Aspect extrac-
tion through semi-supervised modeling. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers - Vol-
ume 1, ACL ?12, pages 339?348, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ana-Maria Popescu and Oren Etzioni. 2005. Ex-
tracting product features and opinions from reviews.
In Proceedings of the Conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, HLT ?05, pages 339?346,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Christina Sauper and Regina Barzilay. 2013. Auto-
matic aggregation by joint modeling of aspects and
values. J. Artif. Int. Res., 46(1):89?127, January.
Christina Sauper, Aria Haghighi, and Regina Barzilay.
2011. Content models with attitude. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ?11, pages 350?358,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Comput. Lin-
guist., 37(2):267?307, June.
Ivan Titov and Ryan McDonald. 2008a. Modeling
online reviews with multi-grain topic models. In
Proceedings of the 17th International Conference on
World Wide Web, WWW ?08, pages 111?120, New
York, NY, USA. ACM.
Ivan Titov and Ryan T. McDonald. 2008b. A joint
model of text and aspect ratings for sentiment sum-
marization. In ACL 2008, Proceedings of the 46th
Annual Meeting of the Association for Computa-
tional Linguistics, June 15-20, 2008, Columbus,
Ohio, USA, pages 308?316.
Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010.
Latent aspect rating analysis on review text data:
A rating regression approach. In Proceedings of
the 16th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ?10,
pages 783?792, New York, NY, USA. ACM.
HongningWang, Yue Lu, and ChengXiang Zhai. 2011.
Latent aspect rating analysis without aspect key-
word supervision. In Proceedings of the 17th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ?11, pages 618?
626, New York, NY, USA. ACM.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a maxent-lda hybrid. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?10, pages 56?
65, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning
from labeled and unlabeled data with label propa-
gation. Technical report, Technical Report CMU-
CALD-02-107, Carnegie Mellon University.
1202
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 115?120,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A System for Real-time Twitter Sentiment Analysis of 2012 U.S. Presidential Election Cycle 
  Hao Wang*, Dogan Can**, Abe Kazemzadeh**,  Fran?ois Bar* and Shrikanth Narayanan** Annenberg Innovation Laboratory (AIL)* Signal Analysis and Interpretation Laboratory (SAIL)** University of Southern California, Los Angeles, CA {haowang@, dogancan@, kazemzad@, fbar@, shri@sipi}.usc.edu       Abstract This paper describes a system for real-time analysis of public sentiment toward presidential candidates in the 2012 U.S. election as expressed on Twitter, a micro-blogging service. Twitter has become a central site where people express their opinions and views on political parties and candidates. Emerging events or news are often followed almost instantly by a burst in Twitter volume, providing a unique opportunity to gauge the relation between expressed public sentiment and electoral events. In addition, sentiment analysis can help explore how these events affect public opinion. While traditional content analysis takes days or weeks to complete, the system demonstrated here analyzes sentiment in the entire Twitter traffic about the election, delivering results instantly and continuously. It offers the public, the media, politicians and scholars a new and timely perspective on the dynamics of the electoral process and public opinion. 
1 Introduction Social media platforms have become an important site for political conversations throughout the world. In the year leading up to the November 2012 presidential election in the United States, we 
have developed a tool for real-time analysis of sentiment expressed through Twitter, a micro-blogging service, toward the incumbent President, Barack Obama, and the nine republican challengers - four of whom remain in the running as of this writing. With this analysis, we seek to explore whether Twitter provides insights into the unfolding of the campaigns and indications of shifts in public opinion. Twitter allows users to post tweets, messages of up to 140 characters, on its social network. Twitter usage is growing rapidly. The company reports over 100 million active users worldwide, together sending over 250 million tweets each day (Twitter, 2012). It was actively used by 13% of on-line American adults as of May 2011, up from 8% a year prior (Pew Research Center, 2011). More than two thirds of U.S. congress members have created a Twitter account and many are actively using Twitter to reach their constituents (Lassen & Brown, 2010; TweetCongress, 2012). Since October 12, 2012, we have gathered over 36 million tweets about the 2012 U.S. presidential candidates, a quarter million per day on average.  During one of the key political events, the Dec 15, 2011 primary debate in Iowa, we collected more than half a million relevant tweets in just a few hours. This kind of ?big data? vastly outpaces the capacity of traditional content analysis approaches, calling for novel computational approaches.  Most work to date has focused on post-facto analysis of tweets, with results coming days or even months after the collection time. However, 
115
because tweets are short and easy to send, they lend themselves to quick and dynamic expression of instant reactions to current events. We expect automated real-time sentiment analysis of this user-generated data can provide fast indications of changes in opinion, showing for example how an audience reacts to particular candidate?s statements during a political debate. The system we present here, along with the dashboards displaying analysis results with drill-down ability, is precisely aimed at generating real-time insights as events unfold. Beyond the sheer scale of the task and the need to keep up with a rapid flow of tweets, we had to address two additional issues. First, the vernacular used on Twitter differs significantly from common language and we have trained our sentiment model on its idiosyncrasies. Second, tweets in general, and political tweets in particular, tend to be quite sarcastic, presenting significant challenges for computer models (Gonz?lez-Ib??ez et al, 2011). We will present our approaches to these issues in a separate publication. Here, we focus on presenting the overall system and the visualization dashboards we have built. In section 2, we begin with a review of related work; we then turn in section 3 to a description of our system?s architecture and its components (input, preprocessing, sentiment model, result aggregation, and visualization); in sections 4 and 5 we evaluate our early experience with this system and discuss next steps. 2 Related Work  In the last decade, interest in mining sentiment and opinions in text has grown rapidly, due in part to the large increase of the availability of documents and messages expressing personal opinions (Pang & Lee, 2008). In particular, sentiment in Twitter data has been used for prediction or measurement in a variety of domains, such as stock market, politics and social movements (Bollen et al, 2011; 
Choy et al, 2011; Tumasjan et al, 2010; Zeitzoff, 2011). For example, Tumasjan (2010) found tweet volume about the political parties to be a good predictor for the outcome of the 2009 German election, while Choy et al (2011) failed to predict with Twitter sentiment the ranking of the four candidates in Singapore?s 2011 presidential election. Past studies of political sentiment on social networks have been either post-hoc and/or carried out on small and static samples. To address these issues, we built a unique infrastructure and sentiment model to analyze in real-time public sentiment on Twitter toward the 2012 U.S. presidential candidates. Our effort to gauge political sentiment is based on bringing together social science scholarship with advanced computational methodology: our approach combines real-time data processing and statistical sentiment modeling informed by, and contributing to, an understanding of the cultural and political practices at work through the use of Twitter. 3  The System For accuracy and speed, we built our real-time data processing infrastructure on the IBM?s InfoSphere Streams platform (IBM, 2012), which enables us to write our own analysis and visualization modules and assemble them into a real-time processing pipeline. Streams applications are highly scalable so we can adjust our system to handle higher volume of data by adding more servers and by distributing processing tasks. Twitter traffic often balloons during big events (e.g. televised debates or primary election days) and stays low between events, making high scalability strongly desirable. Figure 1 shows our system?s architecture and its modules. Next, we introduce our data source and each individual module. 
Figure 1. The system architecture for real-time processing Twitter data  
Preprocessinge.g.,Tokenization Match Tweetto Candidate
Real-time Twitter data
Throttle
SentimentModel Aggregate byCandidate Visualization
OnlineHumanAnnotation
Recordeddata
116
3.1 Input/Data Source We chose the micro-blogging service Twitter as our data source because it is a major source of online political commentary and discussion in the U.S. People comment on and discuss politics by posting messages and ?re-tweeting? others? messages. It played a significant role in political events worldwide, such as the Arab Spring Movement and the Moldovian protests in 2009. In response to events, Twitter volume goes up sharply and significantly. For example, during a republican debate, we receive several hundred thousand to a million tweets in just a few hours for all the candidates combined. Twitter?s public API provides only 1% or less of its entire traffic (the ?firehose?), without control over the sampling procedure, which is likely insufficient for accurate analysis of public sentiment. Instead, we collect all relevant tweets in real-time from the entire Twitter traffic via Gnip Power Track, a commercial Twitter data provider. To cope with this challenge during the later stages of the campaign, when larger Twitter traffic is expected, our system can handle huge traffic bursts over short time periods by distributing the processing to more servers, even though most of the times its processing load is minimal. Since our application targets the political domain (specifically the current Presidential election cycle), we manually construct rules that are simple logical keyword combinations to retrieve relevant tweets ? those about candidates and events (including common typos in candidate names). For example, our rules for Mitt Romney include Romney, @MittRomney, @PlanetRomney, @MittNews, @believeinromney, #romney, #mitt, #mittromney, and #mitt2012. Our system is tracking the tweets for nine Republican candidates (some of whom have suspended their campaign) and Barack Obama using about 200 rules in total. 3.2 Preprocessing The text of tweets differs from the text in articles, books, or even spoken language. It includes many 
idiosyncratic uses, such as emoticons, URLs, RT for re-tweet, @ for user mentions, # for hashtags, and repetitions. It is necessary to preprocess and normalize the text. As standard in NLP practices, the text is tokenized for later processing. We use certain rules to handle the special cases in tweets. We compared several Twitter-specific tokenizers, such as TweetMotif (O'Connor et al, 2010) and found Christopher Potts? basic Twitter tokenizer best suited as our base. In summary, our tokenizer correctly handles URLs, common emoticons, phone numbers, HTML tags, twitter mentions and hashtags, numbers with fractions and decimals, repetition of symbols and Unicode characters (see Figure 2 for an example). 3.3 Sentiment Model The design of the sentiment model used in our system was based on the assumption that the opinions expressed would be highly subjective and contextualized.  Therefore, for generating data for model training and testing, we used a crowd-sourcing approach to do sentiment annotation on in-domain political data. To create a baseline sentiment model, we used Amazon Mechanical Turk (AMT) to get as varied a population of annotators as possible. We designed an interface that allowed annotators to perform the annotations outside of AMT so that they could participate anonymously. The Turkers were asked their age, gender, and to describe their political orientation.  Then they were shown a series of tweets and asked to annotate the tweets' sentiment (positive, negative, neutral, or unsure), whether the tweet was sarcastic or humorous, the sentiment on a scale from positive to negative, and the tweet author's political orientation on a slider scale from conservative to liberal.  Our sentiment model is based on the sentiment label and the sarcasm and humor labels. Our training data consists of nearly 17000 tweets (16% positive, 56% negative, 18% neutral, 10% unsure), including nearly 2000 that were multiply annotated 
Tweet WAAAAAH!!! RT @politico: Romney: Santorum's 'dirty tricks' could steal Michigan: http://t.co/qEns1Pmi #MIprimary #tcot #teaparty #GOP Tokens WAAAAAH !!! RT @politico : Romney : Santorum's ' dirty tricks ' could steal Michigan : http://politi.co/wYUz7m #MIprimary #tcot #teaparty #GOP Figure 2. The output tokens of a sample tweet from our tokenizer 
117
to calculate inter-annotator agreement. About 800 Turkers contributed to our annotation. The statistical classifier we use for sentiment analysis is a na?ve Bayes model on unigram features. Our features are calculated from tokenization of the tweets that attempts to preserve punctuation that may signify sentiment (e.g., emoticons and exclamation points) as well as twitter specific phenomena (e.g., extracting intact URLs). Based on the data we collected our classifier performs at 59% accuracy on the four category classification of negative, positive, neutral, or unsure. These results exceed the baseline of classifying all the data as negative, the most prevalent sentiment category (56%). The choice of our model was not strictly motivated by global accuracy, but took into account class-wise performance so that the model performed well on each sentiment category. 3.4 Aggregation Because our system receives tweets continuously and uses multiple rules to track each candidate?s tweets, our display must aggregate sentiment and tweet volume within each time period for each candidate. For volume, the system outputs the number of tweets every minute for each candidate. For sentiment, the system outputs the number of positive, negative, neutral and unsure tweets in a sliding five-minute window. 3.5 Display and Visualization  We designed an Ajax-based HTML dashboard 
(Figure 3) to display volume and sentiment by candidate as well as trending words and system statistics. The dashboard pulls updated data from a web server and refreshes its display every 30 seconds. In Figure 3, the top-left bar graph shows the number of positive and negative tweets about each candidate (right and left bars, respectively) in the last five minutes as an indicator of sentiment towards the candidates. We chose to display both positive and negative sentiment, instead of the difference between these two, because events typically trigger sharp variations in both positive and negative tweet volume. The top-right chart displays the number of tweets for each candidate every minute in the last two hours. We chose this time window because a live-broadcast primary debate usually lasts about two hours. The bottom-left shows system statistics, including the total number of tweets, the number of seconds since system start and the average data rate. The bottom-right table shows trending words of the last five minutes, computed using TF-IDF measure as follows: tweets about all candidates in a minute are treated as a single ?document?; trending words are the tokens from the current minute with the highest TF-IDF weights when using the last two hours as a corpus (i.e., 120 ?documents?). Qualitative examination suggests that the simple TF-IDF metric effectively identifies the most prominent words when an event occurs. The dashboard gives a synthetic overview of volume and sentiment for the candidates, but it is often desirable to view selected tweets and their sentiments. The dashboard includes another page 
Figure 3. Dashboard for volume, sentiment and trending words 
118
(Figure 4) that displays the most positive, negative and frequent tweets, as well as some random neutral tweets. It also shows the total volume over time and a tag cloud of the most frequent words in the last five minutes across all candidates. Another crucial feature of this page is that clicking on one of the tweets brings up an annotation interface, so the user can provide his/her own assessment of the sentiment expressed in the tweet. The next section describes the annotation interface. 3.6 Annotation Interface The online annotation interface shown in Figure 5 lets dashboard (Figure 4) users provide their own judgment of a tweet. The tweet?s text is displayed at the top, and users can rate the sentiment toward the candidate mentioned in the tweet as positive, negative or neutral or mark it as unsure. There are also two options to specify whether a tweet is sarcastic and/or funny. This interface is a simplified version of the one we used to collect annotations from Amazon Mechanical Turk so that annotation can be performed quickly on a single tweet.  The online interface is designed to be used while watching a campaign event and can be displayed on a tablet or smart phone. The feedback from users allows annotation of recent data as well as the ability to correct misclassifications. As a future step, we plan to 
establish an online feedback loop between users and the sentiment model, so users? judgment serves to train the model actively and iteratively. 4 System Evaluation In Section 3.3, we described our preliminary sentiment model that automatically classifies tweets into four categories: positive, negative, neutral or unsure. It copes well with the negative bias in political tweets. In addition to evaluating 
Figure 5. Dashboard for most positive, negative and frequent tweets 
Figure 4. Online sentiment annotation interface 
119
the model using annotated data, we have also begun conducting correlational analysis of aggregated sentiment with political events and news, as well as indicators such as poll and election results. We are exploring whether variations in twitter sentiment and tweet volume are predictive or reflective of real-world events and news. While this quantitative analysis is part of ongoing work, we present below some quantitative and qualitative expert observations indicative of promising research directions. One finding is that tweet volume is largely driven by campaign events. Of the 50 top hourly intervals between Oct 12, 2011 and Feb 29, 2012, ranked by tweet volume, all but two correspond either to President Obama?s State of the Union address, televised primary debates or moments when caucus or primary election results were released. Out of the 100 top hourly intervals, all but 18 correspond to such events. The 2012 State of the Union address on Jan 24 is another good example. It caused the biggest volume we have seen in a single day since last October, 1.37 million tweets in total for that day. Both positive and negative tweets for President Obama increased three to four times comparing to an average day. During the Republican Primary debate on Jan 19, 2012 in Charleston, NC one of the Republican candidates, Newt Gingrich, was asked about his ex-wife at the beginning of the debate. Within minutes, our dashboard showed his negative sentiment increase rapidly ? it became three times more negative in just two minutes. This illustrates how tweet volume and sentiment are extremely responsive to emerging events in the real world (Vergeer et al, 2011). These examples confirm our assessment that it is especially relevant to offer a system that can provide real-time analysis during key moments in the election cycle. As the election continues and culminates with the presidential vote this November, we hope that our system will provide rich insights into the evolution of public sentiment toward the contenders. 5 Conclusion We presented a system for real-time Twitter sentiment analysis of the ongoing 2012 U.S. presidential election. We use the Twitter ?firehose? and expert-curated rules and keywords to get a full 
and accurate picture of the online political landscape. Our real-time data processing infrastructure and statistical sentiment model evaluates public sentiment changes in response to emerging political events and news as they unfold. The architecture and method are generic, and can be easily adopted and extended to other domains (for instance, we used the system for gauging sentiments about films and actors surrounding Oscar nomination and selection). References  Bollen, J., Mao, H., & Zeng, X. (2011). Twitter mood predicts the stock market. Journal of Computational Science, 2(1), 1-8. doi: 10.1016/j.jocs.2010.12.007 Choy, M., Cheong, L. F. M., Ma, N. L., & Koo, P. S. (2011). A sentiment analysis of Singapore Presidential Election 2011 using Twitter data with census correction. Gonz?lez-Ib??ez, R., Muresan, S., & Wacholder, N. (2011). Identifying Sarcasm in Twitter: A Closer Look. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics. IBM. (2012). InfoSphere Streams, from http://www-01.ibm.com/software/data/infosphere/streams/ Lassen, D. S., & Brown, A. R. (2010). Twitter: The Electoral Connection? Social Science Computer Review. O'Connor, B., Krieger, M., & Ahn, D. (2010). TweetMotif: Exploratory Search and Topic Summarization for Twitter. In Proceedings of the the Fourth International AAAI Conference on Weblogs and Social Media, Washington, DC. Pang, B., & Lee, L. (2008). Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval, 2(1-2), 1-135. doi: 10.1561/1500000011 Pew Research Center. (2011). 13% of online adults use Twitter. Retrieved from http://www.pewinternet.org/ ~/media//Files/Reports/2011/Twitter%20Update%202011.pdf Tumasjan, A., Sprenger, T. O., Sandner, P. G., & Welpe, I. M. (2010). Predicting Elections with Twitter: What 140 Characters Reveal about Political Sentiment. TweetCongress. (2012). Congress Members on Twitter  Retrieved Mar 18, 2012, from http://tweetcongress.org/members/ Twitter. (2012). What is Twitter  Retrieved Mar 18, 2012, from https://business.twitter.com/en/basics/what-is-twitter/ Vergeer, M., Hermans, L., & Sams, S. (2011). Is the voter only a tweet away? Micro blogging during the 2009 European Parliament election campaign in the Netherlands. First Monday [Online], 16(8).  Zeitzoff, T. (2011). Using Social Media to Measure Conflict Dynamics. Journal of Conflict Resolution, 55(6), 938-969. doi: 10.1177/0022002711408014 
120


References
 Bollen, J., Mao, H., & Zeng, X. (2011). Twitter mood 
predicts the stock market.  Journal of Computational 
Science, 2(1), 1-8. doi: 10.1016/j.jocs.2010.12.007
Choy, M., Cheong, L. F. M., Ma, N. L., & Koo, P. S. 
(2011). A sentiment analysis of Singapore Presidential 
Election 2011 using Twitter data with census correction.
González-Ibáñez, R., Muresan, S., & Wacholder, N. 
(2011). Identifying Sarcasm in Twitter: A Closer Look.
In Proceedings of the 49th Annual Meeting of the 
Association for Computational Linguistics.
IBM. (2012). InfoSphere Streams, from http://www-
01.ibm.com/software/data/infosphere/streams/
Lassen, D. S., & Brown, A. R. (2010). Twitter: The 
Electoral Connection? Social Science Computer Review.
O\'Connor, B., Krieger, M., & Ahn, D. (2010). TweetMotif: 
Exploratory Search and Topic Summarization for 
Twitter. In Proceedings of the the Fourth International 
AAAI Conference on Weblogs and Social Media, 
Washington, DC.
Pang, B., & Lee, L. (2008). Opinion Mining and Sentiment 
Analysis.  Foundations and Trends in Information 
Retrieval, 2(1-2), 1-135. doi: 10.1561/1500000011
Pew Research Center. (2011). 13% of online adults use 
Twitter. Retrieved from http://www.pewinternet.org/
~/media//Files/Reports/2011/Twitter%20Update%2020
11.pdf
Tumasjan, A., Sprenger, T. O., Sandner, P. G., & Welpe, I. 
M. (2010). Predicting Elections with Twitter: What 140 
Characters Reveal about Political Sentiment.
TweetCongress. (2012). Congress Members on Twitter  
Retrieved Mar 18, 2012, from 
http://tweetcongress.org/members/
Twitter. (2012). What is Twitter  Retrieved Mar 18, 2012, 
from https://business.twitter.com/en/basics/what-istwitter/
Vergeer, M., Hermans, L., & Sams, S. (2011). Is the voter 
only a tweet away? Micro blogging during the 2009 
European Parliament election campaign in the 
Netherlands. First Monday [Online], 16(8). 
Zeitzoff, T. (2011). Using Social Media to Measure 
Conflict Dynamics.  Journal of Conflict Resolution, 
55(6), 938-969. doi: 10.1177/0022002711408014