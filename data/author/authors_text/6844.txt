Limited-Domain Speech-to-Speech Translation
between English and Pashto
Kristin Precoda, Horacio Franco, Ascander Dost, Michael Frandsen, John Fry,
Andreas Kathol, Colleen Richey, Susanne Riehemann, Dimitra Vergyri, Jing Zheng
SRI International
333 Ravenswood Ave.
Menlo Park, CA 94025
Christopher Culy
FX Palo Alto Laboratory, Inc.
3400 Hillview Ave., Building 4
Palo Alto, CA 94304
Abstract
This paper describes a prototype system for
near-real-time spontaneous, bidirectional
translation between spoken English and
Pashto, a language presenting many
technological challenges because of its lack of
resources, including both data and expert
knowledge. Development of the prototype is
ongoing, and we propose to demonstrate a
fully functional version which shows the basic
capabilities, though not yet their final depth
and breadth.
1 Introduction
This demonstration will present a prototype system for
bidirectional speech-to-speech translation within a
limited semantic domain, that of first encounters
between a patient and a medical professional. A major
goal of the work is to explore techniques that are
appropriate for languages that are not of great
commercial interest and that consequently are lacking in
data and resources of many kinds. The system has been
developed for American English and Pashto, one of the
major languages of Afghanistan, which presents a
variety of challenges for both data-intensive and
knowledge-based approaches.
It should be noted that the system must be viewed as
one component in a real-time transaction between two
cooperating humans. The ultimate goal of those humans
is to exchange information by whatever means is
effective: not, necessarily, to rely exclusively on the
system?s output, but to use it in combination with other,
non-speech modalities of conveying meaning and with
ordinary world knowledge.
The final system is intended to run on a handheld
device, such as a PDA, with its attendant memory and
speed limitations and restriction to integer-only
computation. Most components of the demonstration
system run in a PocketPC emulation environment on a
Windows laptop, with a few in the full Windows
environment. All aspects of the prototype system are
undergoing active development.
2 Overall Architecture
A simple description of the architecture is as follows.
The system is controlled by the English speaker, who is
expected to have greater technological familiarity and
who has the benefit of visual feedback on the system
performance. Spoken input, in either English or Pashto,
is recognized by SRI?s small-footprint DynaSpeak?
recognizer, and an ordered list of hypotheses is
produced. The most likely hypothesis is input to SRI's
Gemini natural language parser/generator (Dowding et
al. 1993), which attempts to parse the speech
recognition output. Handling of possible errors or
failures will be discussed in Section 3.
When a successful parse is obtained, Gemini creates
a quasi-logical form representing the meaning of the
sentence. In general, multiple quasi-logical forms may
be created, reflecting differing interpretations of the
input sentence. These forms, which are domain
independent and serve here as an interlingua, can be
ordered by heuristically assigning preferences or
dispreferences to the parsing rules applied to create
them. Gemini uses a grammar for the target language to
generate a translation from the most-preferred
interpretation possible, and outputs a textual
representation of the translation.
Theta, a small-footprint concatenative synthesizer
from Cepstral LLC (Cepstral LLC 2004), then produces
synthetic spoken output in the target language from the
textual representation generated by Gemini. The Pashto
voice was created specifically for this project.
An English and a Pashto version of each component
are called by a single application which includes a
graphical user interface. A screen shot of the interface
is shown in Figure 1.
Figure 1. Screen shot of prototype system
interface.
3 Redundancy and Handling Infelicities
As in any complex system, performance can differ from
the ideal in a number of ways, and it is important for the
system to provide alternative ways to support successful
communication. Some kinds of subideal performance
and recovery approaches are described here.
The most likely hypothesis output by the speech
recognizer may not be exactly what was spoken. When
the input speech is English, the English speaker can see
whether the most likely hypothesis (shown in the
"Input" box in Figure 1) is correct or approximately
correct. If the English speaker judges the hypothesis to
not be close enough to the intended text, s/he may either
repeat the utterance or click on the "Guesses?" button
to see an ordered list of the best hypotheses. A sample
list is shown in Figure 2. If the correct text is on this list,
the user may select it to submit for translation. When
the input speech is in Pashto, this functionality is less
useful, as the Pashto speaker is not assumed to be able
to read, even if the hypotheses were displayed in Pashto
script.
Figure 2. Sample ordered list of recognizer
hypotheses.
Once a recognition hypothesis has been submitted
for translation, several possible problems can arise.
Pashto is a moderately inflected, split-ergative Indo-
European language, and for Pashto in particular,
recognition errors may lead to apparent lack of syntactic
agreement between elements of the sentence which
should (and did in fact) agree. As Gemini tries to
generate a full parse of the input, it has the option of
using parse rules that relax agreement requirements.
These rules are dispreferred and a parse built upon them
may be kept only if a full, grammatically correct parse
cannot be completed. Another possible problem is that
unknown words, some grammatical constructions, and
input errors may render any full parse unachievable. In
this case, fallback strategies can be applied to translate
partial parses, fragments, or any known words. Other
strategies are currently in development.
Another class of approaches for assisting
communication allows the English speaker to quickly
perform certain actions or play high-frequency phrases
to the Pashto speaker. If there is background noise or
distractions or the TTS quality is not high enough for
easy comprehension, pressing the "Replay" button will
immediately play back the last translation result. "Ask
for Rep" plays a prerecorded sentence asking the Pashto
speaker to repeat what s/he said. Several other useful
phrases are available by clicking on the "Phrases?"
button and then selecting from the screen shown in
Figure 3.
Figure 3. Prerecorded Pashto phrases which can
be played back with a single click.
4 Sample Interaction
The table below shows an excerpt of a dialog between
an English and a Pashto speaker, both new to this
system, whose interaction was part of an informal trial
run by the MITRE Corporation in February 2004 for the
DARPA CAST program. The English speaker, a doctor,
had just received eighteen minutes of training in how to
use the system and had had no other exposure to it. The
Pashto speaker, playing the role of an injured patient,
had received training in complaints consistent with a
particular injury scenario and had seen others use the
system, but had not interacted with the system himself.
Except as noted, the translations are understandable.
Spoken input System result
I am a doctor, can I help
you?
[failure to translate
because two sentences]
I am a doctor z@ ddAkttar y@m
Can I help you? AyA z@ d@ tA sara
komak kaw@lay S@m
ho, mehrabAni w@krra yes make [partial
translation; full translation
should have been "yes
please do"]
Where are you hurt? [rerecorded after one
misrecognition] t@ cherta
khugx ye
ghwagx aw ugxa [misrecognized
repeatedly; unable to give
meaningful translation;
should have been "ear and
shoulder"]
Can you breathe? AyA t@ sA akhIst@lay
Se
na, mUSkel lar@m no I have the problem
Do you have pain? AyA t@ dard lare
zAyt of much [minor
misrecognition; translation
should have been "much"]
Do you take medications? AyA t@ dawAuna akhle
[incorrect plural form of
dawA, but understood by
Pashto speaker anyway]
na no
Do you want pain
medication?
AyA t@ d@ dard dawA
ghwArre
ho yes
Do you have allergies? AyA t@ hasAsiyatuna lare
ho yes
What are your allergies? stA hasAsiyat ts@ day
antibyutik [misrecognized but
correctly understood by
doctor as "antibiotics"]
5 Challenges
Three main challenges face this project. First,
commercially nonviable languages, such as Pashto,
often offer very few linguistic resources (such as
linguistic descriptions, acoustic data, texts, language
processing tools). The lack of resources makes
development more difficult, and severely constrains the
approaches that are viable: approaches that rely on large
corpora cannot be used. In addition, there is a shortage
of literate speakers who are available to act as
consultants, and a scarcity of basic knowledge about the
language. This impedes progress and renders difficult
approaches that rely on a large body of hand-crafted
translation rules. The challenge of having no single
person who has a deep understanding of both the
language and the technology and who can serve as a
bridge between them is substantial, and causes more
iterative development than is ordinarily the case when
bilingual technologists are available, as newly
discovered phenomena or new understanding cause
revisions of previous work.
A second major challenge is due to the nature of the
domain and the underlying concept of operations. Real
speech occurs in noisy environments, has disfluencies,
and is highly variable (e.g., phrasings, dialect
differences). In addition, the output of a speech
recognizer contains more and qualitatively different
errors than typical text input to automatic translation
systems. While the problems of real speech are not
unique to this project, they are magnified by the fact
that the non-English speakers will largely be
unsophisticated users of technology, who will often be
using the system for the first and only time. The system
must work well from the very first utterance ? there
cannot be much of a learning curve. This applies to the
translation quality and to other aspects of the system,
such as the synthetic speech, as speakers are not familiar
with synthesized Pashto speech. These speakers are also
not expected to be literate, and their understanding will
not be bolstered by the extra redundancy and
capabilities that the display offers to the English
speaker.
A third major challenge is posed by the handheld
device platform with its computational and storage
limitations, and the near-real-time requirement of the
envisioned usage. The restriction to integer-only
computation is most serious for the speech recognition,
as nearly all medium- or large-vocabulary speech
recognizers perform extensive floating-point
computations, and the conversion of a speech recognizer
to use only integer computation required considerable
effort. The severe memory limitations perhaps impact
most the parsing/generation components of the system.
6 Summary
We have described a prototype of a spoken language
translation system for use by English-speaking medical
personnel treating Pashto-speaking patients.  The
system, which is targeted to a handheld device, is being
developed with extremely little Pashto-language data of
any kind.  It builds on medium-vocabulary speaker-
independent HMM-based speech recognition, rule-
based translation and supporting methods, and
concatenative synthesis. While the system is certainly
still under development, it has provided a reasonable
proof of concept in informal trials.
Acknowledgments
This work was supported under SPAWAR contract N66001-
99-D-8504 with funding from DARPA. Many thanks are due
to Mohammad Shahabuddin Khan, David Kale, Robert J.
Podesva, Valerie Wagner, and many Pashto speakers who
worked with us in various capacities.
References
Cepstral, LLC. 2004. Theta: Small footprint text-to-
speech synthesizer, Pittsburgh, PA, http://
www.cepstral.com
J. Dowding, J. M. Gawron, D. E. Appelt, J. Bear, L.
Cherny, R. Moore, and D. B. Moran. 1993. Gemini:
A natural language system for spoken language
understanding.  Proceedings of the Thirty-First
Annual Meeting of the Association for Computational
Linguistics, 54-61.
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 374?378,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Detection of Agreement and Disagreement in Broadcast Conversations
Wen Wang1 Sibel Yaman2y Kristin Precoda1 Colleen Richey1 Geoffrey Raymond3
1SRI International, 333 Ravenswood Avenue, Menlo Park, CA 94025, USA
2IBM T. J. Watson Research Center P.O.Box 218, Yorktown Heights, NY 10598, USA
3University of California, Santa Barbara, CA, USA
fwwang,precoda,colleeng@speech.sri.com, syaman@us.ibm.com, graymond@soc.ucsb.edu
Abstract
We present Conditional Random Fields
based approaches for detecting agree-
ment/disagreement between speakers in
English broadcast conversation shows. We
develop annotation approaches for a variety
of linguistic phenomena. Various lexical,
structural, durational, and prosodic features
are explored. We compare the performance
when using features extracted from au-
tomatically generated annotations against
that when using human annotations. We
investigate the efficacy of adding prosodic
features on top of lexical, structural, and
durational features. Since the training data
is highly imbalanced, we explore two sam-
pling approaches, random downsampling
and ensemble downsampling. Overall, our
approach achieves 79.2% (precision), 50.5%
(recall), 61.7% (F1) for agreement detection
and 69.2% (precision), 46.9% (recall), and
55.9% (F1) for disagreement detection, on the
English broadcast conversation data.
1 Introduction
In this work, we present models for detecting
agreement/disagreement (denoted (dis)agreement)
between speakers in English broadcast conversation
shows. The Broadcast Conversation (BC) genre dif-
fers from the Broadcast News (BN) genre in that
it is more interactive and spontaneous, referring to
free speech in news-style TV and radio programs
and consisting of talk shows, interviews, call-in
programs, live reports, and round-tables. Previous
yThis work was performed while the author was at ICSI.
work on detecting (dis)agreements has been focused
on meeting data. (Hillard et al, 2003), (Galley
et al, 2004), (Hahn et al, 2006) used spurt-level
agreement annotations from the ICSI meeting cor-
pus (Janin et al, 2003). (Hillard et al, 2003) ex-
plored unsupervised machine learning approaches
and on manual transcripts, they achieved an over-
all 3-way agreement/disagreement classification ac-
curacy as 82% with keyword features. (Galley et
al., 2004) explored Bayesian Networks for the de-
tection of (dis)agreements. They used adjacency
pair information to determine the structure of their
conditional Markov model and outperformed the re-
sults of (Hillard et al, 2003) by improving the 3-
way classification accuracy into 86.9%. (Hahn et al,
2006) explored semi-supervised learning algorithms
and reached a competitive performance of 86.7%
3-way classification accuracy on manual transcrip-
tions with only lexical features. (Germesin and Wil-
son, 2009) investigated supervised machine learn-
ing techniques and yields competitive results on the
annotated data from the AMI meeting corpus (Mc-
Cowan et al, 2005).
Our work differs from these previous studies in
two major categories. One is that a different def-
inition of (dis)agreement was used. In the cur-
rent work, a (dis)agreement occurs when a respond-
ing speaker agrees with, accepts, or disagrees with
or rejects, a statement or proposition by a first
speaker. Second, we explored (dis)agreement de-
tection in broadcast conversation. Due to the dif-
ference in publicity and intimacy/collegiality be-
tween speakers in broadcast conversations vs. meet-
ings, (dis)agreement may have different character-
374
istics. Different from the unsupervised approaches
in (Hillard et al, 2003) and semi-supervised ap-
proaches in (Hahn et al, 2006), we conducted su-
pervised training. Also, different from (Hillard et
al., 2003) and (Galley et al, 2004), our classifica-
tion was carried out on the utterance level, instead
of on the spurt-level. Galley et al extended Hillard
et al?s work by adding features from previous spurts
and features from the general dialog context to in-
fer the class of the current spurt, on top of fea-
tures from the current spurt (local features) used by
Hillard et al Galley et al used adjacency pairs to
describe the interaction between speakers and the re-
lations between consecutive spurts. In this prelim-
inary study on broadcast conversation, we directly
modeled (dis)agreement detection without using ad-
jacency pairs. Still, within the conditional random
fields (CRF) framework, we explored features from
preceding and following utterances to consider con-
text in the discourse structure. We explored a wide
variety of features, including lexical, structural, du-
rational, and prosodic features. To our knowledge,
this is the first work to systematically investigate
detection of agreement/disagreement for broadcast
conversation data. The remainder of the paper is or-
ganized as follows. Section 2 presents our data and
automatic annotation modules. Section 3 describes
various features and the CRF model we explored.
Experimental results and discussion appear in Sec-
tion 4, as well as conclusions and future directions.
2 Data and Automatic Annotation
In this work, we selected English broadcast con-
versation data from the DARPA GALE pro-
gram collected data (GALE Phase 1 Release
4, LDC2006E91; GALE Phase 4 Release 2,
LDC2009E15). Human transcriptions and manual
speaker turn labels are used in this study. Also,
since the (dis)agreement detection output will be
used to analyze social roles and relations of an inter-
acting group, we first manually marked soundbites
and then excluded soundbites during annotation and
modeling. We recruited annotators to provide man-
ual annotations of speaker roles and (dis)agreement
to use for the supervised training of models. We de-
fined a set of speaker roles as follows. Host/chair
is a person associated with running the discussions
or calling the meeting. Reporting participant is a
person reporting from the field, from a subcommit-
tee, etc. Commentator participant/Topic participant
is a person providing commentary on some subject,
or person who is the subject of the conversation and
plays a role, e.g., as a newsmaker. Audience par-
ticipant is an ordinary person who may call in, ask
questions at a microphone at e.g. a large presenta-
tion, or be interviewed because of their presence at a
news event. Other is any speaker who does not fit in
one of the above categories, such as a voice talent,
an announcer doing show openings or commercial
breaks, or a translator.
Agreements and disagreements are com-
posed of different combinations of initiating
utterances and responses. We reformulated the
(dis)agreement detection task as the sequence
tagging of 11 (dis)agreement-related labels for
identifying whether a given utterance is initiating
a (dis)agreement opportunity, is a (dis)agreement
response to such an opportunity, or is neither of
these, in the show. For example, a Negative tag
question followed by a negation response forms an
agreement, that is, A: [Negative tag] This is not
black and white, is it? B: [Agreeing Response]
No, it isn?t. The data sparsity problem is serious.
Among all 27,071 utterances, only 2,589 utterances
are involved in (dis)agreement as initiating or
response utterances, about 10% only among all
data, while 24,482 utterances are not involved.
These annotators also labeled shows with a va-
riety of linguistic phenomena (denoted language
use constituents, LUC), including discourse mark-
ers, disfluencies, person addresses and person men-
tions, prefaces, extreme case formulations, and dia-
log act tags (DAT). We categorized dialog acts into
statement, question, backchannel, and incomplete.
We classified disfluencies (DF) into filled pauses
(e.g., uh, um), repetitions, corrections, and false
starts. Person address (PA) terms are terms that a
speaker uses to address another person. Person men-
tions (PM) are references to non-participants in the
conversation. Discourse markers (DM) are words
or phrases that are related to the structure of the
discourse and express a relation between two utter-
ances, for example, I mean, you know. Prefaces
(PR) are sentence-initial lexical tokens serving func-
tions close to discourse markers (e.g., Well, I think
375
that...). Extreme case formulations (ECF) are lexi-
cal patterns emphasizing extremeness (e.g., This is
the best book I have ever read). In the end, we man-
ually annotated 49 English shows. We preprocessed
English manual transcripts by removing transcriber
annotation markers and noise, removing punctuation
and case information, and conducting text normal-
ization. We also built automatic rule-based and sta-
tistical annotation tools for these LUCs.
3 Features and Model
We explored lexical, structural, durational, and
prosodic features for (dis)agreement detection. We
included a set of ?lexical? features, including n-
grams extracted from all of that speaker?s utter-
ances, denoted ngram features. Other lexical fea-
tures include the presence of negation and acquies-
cence, yes/no equivalents, positive and negative tag
questions, and other features distinguishing differ-
ent types of initiating utterances and responses. We
also included various lexical features extracted from
LUC annotations, denoted LUC features. These ad-
ditional features include features related to the pres-
ence of prefaces, the counts of types and tokens
of discourse markers, extreme case formulations,
disfluencies, person addressing events, and person
mentions, and the normalized values of these counts
by sentence length. We also include a set of features
related to the DAT of the current utterance and pre-
ceding and following utterances.
We developed a set of ?structural? and ?dura-
tional? features, inspired by conversation analysis,
to quantitatively represent the different participation
and interaction patterns of speakers in a show. We
extracted features related to pausing and overlaps
between consecutive turns, the absolute and relative
duration of consecutive turns, and so on.
We used a set of prosodic features including
pause, duration, and the speech rate of a speaker. We
also used pitch and energy of the voice. Prosodic
features were computed on words and phonetic
alignment of manual transcripts. Features are com-
puted for the beginning and ending words of an ut-
terance. For the duration features, we used the aver-
age and maximum vowel duration from forced align-
ment, both unnormalized and normalized for vowel
identity and phone context. For pitch and energy, we
calculated the minimum, maximum, range, mean,
standard deviation, skewness and kurtosis values. A
decision tree model was used to compute posteriors
from prosodic features and we used cumulative bin-
ning of posteriors as final features , similar to (Liu et
al., 2006).
As illustrated in Section 2, we reformulated the
(dis)agreement detection task as a sequence tagging
problem. We used the Mallet package (McCallum,
2002) to implement the linear chain CRF model for
sequence tagging. A CRF is an undirected graph-
ical model that defines a global log-linear distribu-
tion of the state (or label) sequence E conditioned
on an observation sequence, in our case including
the sequence of sentences S and the corresponding
sequence of features for this sequence of sentences
F . The model is optimized globally over the en-
tire sequence. The CRF model is trained to maxi-
mize the conditional log-likelihood of a given train-
ing set P (EjS; F ). During testing, the most likely
sequence E is found using the Viterbi algorithm.
One of the motivations of choosing conditional ran-
dom fields was to avoid the label-bias problem found
in hidden Markov models. Compared to Maxi-
mum Entropy modeling, the CRF model is opti-
mized globally over the entire sequence, whereas the
ME model makes a decision at each point individu-
ally without considering the context event informa-
tion.
4 Experiments
All (dis)agreement detection results are based on n-
fold cross-validation. In this procedure, we held
out one show as the test set, randomly held out an-
other show as the dev set, trained models on the
rest of the data, and tested the model on the held-
out show. We iterated through all shows and com-
puted the overall accuracy. Table 1 shows the re-
sults of (dis)agreement detection using all features
except prosodic features. We compared two condi-
tions: (1) features extracted completely from the au-
tomatic LUC annotations and automatically detected
speaker roles, and (2) features from manual speaker
role labels and manual LUC annotations when man-
ual annotations are available. Table 1 showed that
running a fully automatic system to generate auto-
matic annotations and automatic speaker roles pro-
376
duced comparable performance to the system using
features from manual annotations whenever avail-
able.
Table 1: Precision (%), recall (%), and F1 (%) of
(dis)agreement detection using features extracted from
manual speaker role labels and manual LUC annota-
tions when available, denoted Manual Annotation, and
automatic LUC annotations and automatically detected
speaker roles, denoted Automatic Annotation.
Agreement
P R F1
Manual Annotation 81.5 43.2 56.5
Automatic Annotation 79.5 44.6 57.1
Disagreement
P R F1
Manual Annotation 70.1 38.5 49.7
Automatic Annotation 64.3 36.6 46.6
We then focused on the condition of using fea-
tures from manual annotations when available and
added prosodic features as described in Section 3.
The results are shown in Table 2. Adding prosodic
features produced a 0.7% absolute gain on F1 on
agreement detection, and 1.5% absolute gain on F1
on disagreement detection.
Table 2: Precision (%), recall (%), and F1 (%) of
(dis)agreement detection using manual annotations with-
out and with prosodic features.
Agreement
P R F1
w/o prosodic 81.5 43.2 56.5
with prosodic 81.8 44.0 57.2
Disagreement
P R F1
w/o prosodic 70.1 38.5 49.7
with prosodic 70.8 40.1 51.2
Note that only about 10% utterances among all
data are involved in (dis)agreement. This indicates
a highly imbalanced data set as one class is more
heavily represented than the other/others. We sus-
pected that this high imbalance has played a ma-
jor role in the high precision and low recall results
we obtained so far. Various approaches have been
studied to handle imbalanced data for classifications,
trying to balance the class distribution in the train-
ing set by either oversampling the minority class or
downsampling the majority class. In this prelimi-
nary study of sampling approaches for handling im-
balanced data for CRF training, we investigated two
approaches, random downsampling and ensemble
downsampling. Random downsampling randomly
downsamples the majority class to equate the num-
ber of minority and majority class samples. Ensem-
ble downsampling is a refinement of random down-
sampling which doesn?t discard any majority class
samples. Instead, we partitioned the majority class
samples into N subspaces with each subspace con-
taining the same number of samples as the minority
class. Then we train N CRF models, each based
on the minority class samples and one disjoint parti-
tion from the N subspaces. During testing, the pos-
terior probability for one utterance is averaged over
the N CRF models. The results from these two sam-
pling approaches as well as the baseline are shown
in Table 3. Both sampling approaches achieved sig-
nificant improvement over the baseline, i.e., train-
ing on the original data set, and ensemble downsam-
pling produced better performance than downsam-
pling. We noticed that both sampling approaches
degraded slightly in precision but improved signif-
icantly in recall, resulting in 4.5% absolute gain on
F1 for agreement detection and 4.7% absolute gain
on F1 for disagreement detection.
Table 3: Precision (%), recall (%), and F1 (%) of
(dis)agreement detection without sampling, with random
downsampling and ensemble downsampling. Manual an-
notations and prosodic features are used.
Agreement
P R F1
Baseline 81.8 44.0 57.2
Random downsampling 78.5 48.7 60.1
Ensemble downsampling 79.2 50.5 61.7
Disagreement
P R F1
Baseline 70.8 40.1 51.2
Random downsampling 67.3 44.8 53.8
Ensemble downsampling 69.2 46.9 55.9
In conclusion, this paper presents our work on
detection of agreements and disagreements in En-
377
glish broadcast conversation data. We explored a
variety of features, including lexical, structural, du-
rational, and prosodic features. We experimented
these features using a linear-chain conditional ran-
dom fields model and conducted supervised train-
ing. We observed significant improvement from
adding prosodic features and employing two sam-
pling approaches, random downsampling and en-
semble downsampling. Overall, we achieved 79.2%
(precision), 50.5% (recall), 61.7% (F1) for agree-
ment detection and 69.2% (precision), 46.9% (re-
call), and 55.9% (F1) for disagreement detection, on
English broadcast conversation data. In future work,
we plan to continue adding and refining features, ex-
plore dependencies between features and contextual
cues with respect to agreements and disagreements,
and investigate the efficacy of other machine learn-
ing approaches such as Bayesian networks and Sup-
port Vector Machines.
Acknowledgments
The authors thank Gokhan Tur and Dilek Hakkani-
Tu?r for valuable insights and suggestions. This
work has been supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via
Army Research Laboratory (ARL) contract num-
ber W911NF-09-C-0089. The U.S. Government is
authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copy-
right annotation thereon. The views and conclusions
contained herein are those of the authors and should
not be interpreted as necessarily representing the of-
ficial policies or endorsements, either expressed or
implied, of IARPA, ARL, or the U.S. Government.
References
M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg.
2004. Identifying agreement and disagreement in con-
versational speech: Use of bayesian networks to model
pragmatic dependencies. In Proceedings of ACL.
S. Germesin and T. Wilson. 2009. Agreement detection
in multiparty conversation. In Proceedings of Interna-
tional Conference on Multimodal Interfaces.
S. Hahn, R. Ladner, and M. Ostendorf. 2006. Agree-
ment/disagreement classification: Exploiting unla-
beled data using constraint classifiers. In Proceedings
of HLT/NAACL.
D. Hillard, M. Ostendorf, and E. Shriberg. 2003. De-
tection of agreement vs. disagreement in meetings:
Training with unlabeled data. In Proceedings of
HLT/NAACL.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke,
and C. Wooters. 2003. The ICSI Meeting Corpus. In
Proc. ICASSP, Hong Kong, April.
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Dustin
Hillard, Mari Ostendorf, and Mary Harper. 2006.
Enriching speech recognition with automatic detec-
tion of sentence boundaries and disfluencies. IEEE
Transactions on Audio, Speech, and Language Pro-
cessing, 14(5):1526?1540, September. Special Issue
on Progress in Rich Transcription.
Andrew McCallum. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.
I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bour-
ban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec,
V. Karaiskos, M. Kronenthal, G. Lathoud, M. Lincoln,
A. Lisowska, W. Post, D. Reidsma, and P. Wellner.
2005. The AMI meeting corpus. In Proceedings of
Measuring Behavior 2005, the 5th International Con-
ference on Methods and Techniques in Behavioral Re-
search.
378
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 27?37,
Baltimore, Maryland USA, June 27, 2014. c?2014 Association for Computational Linguistics
Aided Diagnosis of Dementia Type through Computer-Based Analysis of Spontaneous Speech  William Jarrold Nuance Communications1 william.jarrold@gmail.com Bart Peintner  Soshoma1 bpeintner@gmail.com David Wilkins  Language & Linguistic Consulting wilkinsdavidp@gmail.com 
Dimitra Vergryi and Colleen Richey SRI International dverg@speech.sri.com, colleen.richey@sri.com 
Maria Luisa Gorno-Tempini and Jennifer Ogar University of California, San Francisco {marilu|jogar}@memory.ucsf.edu 
  Abstract This pilot study evaluates the ability of machined learned algorithms to assist with the differential diagnosis of dementia subtypes based on brief (< 10 min) spontaneous speech samples.  We ana-lyzed1recordings of a brief spontaneous speech sample from 48 participants from 5 different groups: 4 types of dementia plus healthy con-trols.  Recordings were analyzed using a speech recognition system optimized for speaker-independent spontaneous speech. Lexical and acoustic features were automatically extracted. The resulting feature profiles were used as input to a machine learning system that was trained to identify the diagnosis assigned to each research participant.  Between groups lexical and acoustic differences features were detected in accordance with expectations from prior research literature suggesting that classifications were based on fea-tures consistent with human-observed sympto-matology. Machine learning algorithms were able to identify participants' diagnostic group with accuracy comparable to existing diagnostic methods in use today. Results suggest this clini-cal speech analytic approach offers promise as an additional, objective and easily obtained source of diagnostic information for clinicians. 1 Introduction Accurately differentiating certain neurodegenera-tive disorders such as Alzheimer?s Disease (AD) and variants of Fronto-temporal Lobar Degener-ation (FTLD) is extremely difficult (Varma et al., 1999). Differential diagnosis is often left to tertiary care settings (e.g. Research I Universities with medical schools). While the most definitive diagnosis is made post-mortem using brain tissue                                                 1 Research conducted while at SRI International 
samples, the treatment and prognostic implica-tions of living patients are often determined in large part on the basis of language assessment.   Although language is clearly not the exclusive diagnostic factor for AD, existing literature sug-gests it is an important one. Studies show sig-nificant differences in the written language abili-ties of AD patients and healthy older adults (Pes-tell et al., 2008 and Platel et al., 1993). The speech of patients with AD is partly character-ized by word-finding difficulties, smaller vocab-ularies, and problems with semantic processing (Forbes at el., 2002). These symptoms appear early in the disease?s progression, however lan-guage assessment of AD patients can fail to iden-tify early symptoms that family members report to be present in their conversations (Crockford and Lesser, 1994).   FTLD has a prevalence similar to AD in patients under the age of 65 years (Mendez at el., 1993). Misdiagnosis of FTLD is common Mendez at el., 1993). Three variants are defined by the widely adopted Neary criteria (Neary at el., 1998); one with altered social conduct, the behavioral vari-ant of frontotemporal dementia (bvFTD); the second characterized by a deterioration of con-ceptual-semantic knowledge, semantic dementia (SD); and the third marked by a disorder of ex-pressive language fluency, progressive non-fluent aphasia (PNFA).   Clinicians diagnose using a wide array of evi-dence including patient history, imaging and neuropsychological assessment in which speech and language diagnostics feature prominently.  In AD, cognitive disturbance is a required diagnos-tic feature and language impairment one several sufficient signs of such impairment.  In the case 
27
of SD and PNFA, changes in speech and lan-guage are core diagnostic features, with changes in lexical content features being highly diagnos-tic of SD, and changes in the acoustic properties of speech being highly diagnostic of PNFA. Even in bvFTD, where changes in social behav-ior are the defining features, analysis of lan-guage-based differences is important, because language is an essential mediator of social be-havior. To be sure, the clinician does not diag-nose exclusively on language features -- patient history, imaging, memory functioning and more play a role.  However, language does feature prominently in the differential diagnosis of AD, FTLD and its three subtypes.  For this reason, computerized analysis of speech may offer an important aid to the clinical diagnosis of these syndromes.  Prior work in clinical speech analytics supports the possibility of computer-based diagnosis of dementia related syndromes. Singh (2001) de-scribes a means of quantifying the degree of speech deficits derived from human transcrip-tions of the speech of patient with AD. Machine Learning has already been applied to distinguish AD from controls using human transcribed spon-taneous speech (Thomas at el., 2005). Abel et al. (2009) applied a connectionist net that models patient speech errors (naming and repetition dis-orders) to the problem of diagnosis. Tur et al. (2010) have shown the ability to automatically score patient speech from a story recall and pic-ture description task that is on par with human performance. Lehr et al. (2012) have developed a system that automatically transcribes and scores patient speech obtained during the story recall portion of the Wechsler Logical Memory test.  The evaluation demonstrated it could distinguish mild cognition impairment from typical controls at performance level comparable to human scor-ers.   Our work builds upon these prior studies along a number of dimensions.  First, we distinguish be-
tween a wider array of dementia subtypes, i.e. not only AD vs controls, but also the three sub-types of FTLD. Second, we use not just lexical features but also acoustic/prosodic related fea-tures.  Third, in order to shed light on the opaque ?black box? nature of many machine-learned classifiers, we identify relationships between model features and symptoms from the clinical literature.  Fourth, our approach can claim to be more ecologically valid because it analyzes spon-taneous speech as input rather than recall of a remembered passage.  Fifth, we do not require human transcription - a labor-intensive step that hinders broader use in a clinical setting. Sixth we provide a comparison of our system performance against benchmarks obtained from practicing clinicians. Our paper is the first we know of to exhibit all of the above properties.  In sum we used computational techniques to ana-lyze acoustic and lexical features of the speech of patients with AD and FTLD variants, and we investigated whether models derived from these features via machine learning could accurately identify a patient?s diagnosis.  2 METHOD 2.1 Participant Recruitment and Diagnosis We obtained spontaneous speech data from 9 controls, 9 AD patients and 30 FTLD patients?9 with frontotemporal dementia (bvFTD), 13 with semantic dementia (SD), and 8 with progressive nonfluent aphasia (PNFA). Table 1 shows demo-graphic information.  Data were collected in an ongoing series of NIH-funded studies being performed at the UCSF Memory and Aging Center. Patients were diag-nosed by expert clinicians at the center by apply-ing current clinical criteria. Patients underwent detailed standard speech and language, cognitive, emotional, genetic, pathological, and neuroimag-ing evaluations. Age-matched healthy controls were community volunteers obtained by SRI In-
 bvFTD PNFA SD AD Controls Male/Female 5/4 1/7 6/7 5/4 3/6 Age 63.00(8.25) 62.88(7.75) 65.23(6.61) 59.11(7.47) 61.7(6.0) Education * 17.33(1.73) 16.13(2.30) 16.45(2.54) 15.44(2.30) 17.27(2.1) MMSE 24.4(5.85) 22.0(9.34) 17.09(8.15) 18.67(7.53) Not Adminis-tered Table 1. Demographic information for participants 
28
ternational and were paid $10 for their participa-tion.  2.2 Speech Samples Speech samples were recordings of Part 1 of the Western Aphasia Battery (Kertesz, 1980). Partic-ipants are administered a semi-structured inter-view (e.g., questions such as ?How are you??) and asked to describe a drawn picture of a picnic scene. The resulting 3 to 5 minutes of speech was recorded via wireless lapel microphones. Con-trols were recorded via digital audio recorder sampling at 48 kHz, 16 bit PCM, and later down-sampled to 16 kHz for use with the speech rec-ognizer. Digital audio was down-sampled at 16 kHz, 16 bit PCM. Recordings were manually segmented in order to separate the interviewee?s voice from the interviewer?s.  Only patient speech segments were subject to analysis 2.3 Procedure To tackle speech-based diagnosis of AD, bvFTD, SD and PNFA, we employ several types of com-puter-based analyses (see Figure 1). Audio re-cordings were processed via the Meeting Under-standing system (Stolcke at el., 2007), which was custom-tailored to recognize speaker-independent, multi-person speech. First, using this system we perform acoustic-level feature 
extraction (AFE), which obtains measures the duration of consonants, vowels, pauses, and oth-er acoustic-phonetic categories. In parallel, we perform a lexical feature extraction (LFE) on transcripts of participant speech producing pro-files of each speaker?s language use.  This profile characterizes frequencies of different types of words ? e.g. frequency of nouns, verbs, function words, words about emotion, etc. ? present in a language sample along ~100 dimensions.    Next, The AFE and LFE profiles are combined to form one large vector of features that collec-tively characterize the speaker. Feature selection is applied to select the most informative features. For feature selection, we performed a one-way ANOVA on each extracted feature to determine which features were significantly related to a diagnostic category using the Benjamini-Hochberg adjustment for multiple comparisons.  The vector of selected features for the speech samples in the training set is taken as input to machine learning.  Based on these data machine learning automatically induces a diagnostic mod-el that should predict any speaker?s diagnosis based the AFE and LFE profiles of his or her speech sample.   
  
  
Microphone 
Subject audio recordings Automatic Transcription (AT) POS tagger POS feature profile 
LIWC LIWC feature profile 
Acoustic Feature Extractor   Acoustic feature profile 
Machine Learning 
Disease Identifying Model 
Speech to Text  
Training Data Labels  (Each participant?s Diagnosis) 
Human  Transcriptionist Human Transcription (HT) 
Automatic Speech Analysis (ASA) 
Acoustic Feature Extraction (AFE) 
Lexical Feature Extraction (LFE) 
Figure 1. System Information Flow and Evaluation. Participant speech is subjected to automatic speech analysis of two kinds: Acoustic Feature Extraction (AFE) and Lexical Feature Extraction (LFE). Feature selection (not shown) is explained in Sects 2.3 and 2.6. Each machine learning algo-rithm produces a classification model based on labeled training data.  All models used both acoustic and lexical features. Each such disease identifying model is evaluated against held-out training data (not shown).  To measure sensitivity to ASR error, half of these models were based on lexical features derived from automatic transcription (AT), the other half from human transcription (HT). 
29
The performance of a learned diagnostic model is measured in terms of ability to generalize to cases that it has not been trained on is measured by feeding test set cases ? i.e. cases that have not been a part of the training set.  We compared the accuracy of the machine learning induced algo-rithms with accuracy studies of traditional diag-nostic methods in the literature.  In addition to the above, as part of a desire to achieve insight into the way these models were functioning, we sought verification that a differ-ences in feature profiles as a function of diagnos-tic group correspond meaningfully to existing expectations derived from the literature. To do so, we formed and tested several predictions about specific feature differences based on the clinical literature (see Hypotheses below).  Finally, we wanted to determine how sensitive the feature differences and classification models were to speech recognition error.  To do so we tested each hypothesis on both the human and automatic transcriptions.  In addition, we learned a set of models based on automatic transcriptions and a second set of models based on human tran-scriptions and compared accuracies.  2.4 Acoustic Feature Extraction   We used the automatic speech recognition (ASR) system to extract a set of acoustic-level features corresponding to the overall rate, plus the mean and standard deviation of (a) pause lengths and (b) hypothesized phoneme durations. For each speech sample, the speech rate as well as the mean and standard deviation of the duration of pauses, vowels, and consonants were comput-ed. The SRI speech processing system also fur-ther identified consonant classes based on man-ner features (e.g., fricative, stop, etc. ?)   voic-ing features (voiced, voiceless) and measured the mean and standard deviation of the duration of these classes. Our Automatic Speech Analysis system produced 41 different duration-based measures extracted from the speech stream.  2.5 Lexical Feature Extraction (LFE) For each transcript we performed two types of computer-based lexical analysis. The first deter-mined frequencies of 14 different parts of speech (e.g. nouns, verbs, pronouns etc.) using an auto-matic part-of-speech (POS) tagger. The second involved Dr. Pennebaker?s Linguistic Inquiry and Word Count (LIWC) software (Pennebaker, et al 2001), which determines word frequencies 
organized into 81 categories, such as psychologi-cal processes (e.g., emotional or cognitive) and linguistic dimensions (e.g. function words, verb tenses, negations).   To measure sensitivity to speech to text error, each ANOVA was performed twice, once for the ?ground truth? human transcriptions (HT) and once for the automatic transcriptions (AT). Dur-ing hypothesis testing, statistical significance of each pair of AT versus HT based LFEs (i.e., ?ground truth?) was compared.  Additionally different models were learned, half using HT the other half using AT. To test for lexical-level dif-ferences between diagnostic categories, we per-formed a one-way ANOVA for each of the 95 LFE features (e.g. frequency of nouns) in which diagnosis was the independent variable and the given feature?s frequency was the dependent var-iable.  2.6 Machine Learning We assessed how well a variety of machine learning algorithms predicted a patient?s diagno-sis, using his or her combined AFE and LFE pro-file.  Evaluation was conducted using five-fold cross-validation over the set of patients, with each ?fold? consisting of two phases: a training phase, where the feature profiles and diagnoses from 4/5ths of the subjects are used to select fea-tures and then train the given learning algorithm, and a test phase where the trained learner is giv-en just the feature profiles of the remaining pa-tients, and attempts to predict their diagnoses. This procedure is executed five times, each time using different sets of subjects for the train and test phases, with overall accuracy being the aver-age performance on the test subjects, across all five folds. We applied three learning methods, (1) logistic regression, a statistical learning tech-nique for determining categorical outcomes, (2) Multi-Layered Perceptrons, an artificial intelli-gence (AI) learning method that roughly mimics biological neural networks, and (3) decision trees, another AI technique which induces sets of rules used to predict outcomes. All three are commonly used machine learning techniques, and for this study we used implementations available in Weka, an off-the-shelf machine learning toolkit (Witten and Frank, 2005). 2.7 Hypotheses Machine learned classification models can be difficult to understand and often used merely as black boxes. To address this issue, we tried to 
30
draw a meaningful link between certain features and diagnosis. In particular, we formed and test-ed several hypotheses based on expectations de-rived from clinical literature.  We used all the data (rather than one of the training folds) to test these hypotheses.   The hypotheses about the lexical features are as follows.  First, based on (Forbes at el., 2002) we predicted that AD patients use more pronouns, verbs, and adjectives and fewer nouns than con-trols (H1).  In SD, one sees decreased lexical access to con-crete concepts, so patients tend to use fewer nouns (H2). To compensate for such difficulties with word retrieval, they also use more pronouns (H3).  This gives the impression of empty or cir-cumlocutory speech. For example, rather than saying ?The boy is flying a kite,? a SD patient would be more prone to say ?He is flying that.? (Grossman and Ash, 2004).   In PNFA, one sees fewer verbs (H4) (Grossman and Ash, 2004).  In addition, PNFA patients of-ten exhibit agrammatism. Such speech is simpli-fied and ungrammatical and involves fewer func-tion words, for example ?give cupcake? or ?wa-ter now?.  Thus (H5) is that the speech of pa-
tients with PNFA will have fewer function words (H5) (Saffran at el., 1989). These hypotheses, along with whether each was supported by our analyses, are listed in Table 2 in Results.  The first acoustic hypothesis about acoustic fea-tures (H6) is related to the Neary criteria (Neary et al., 1998), which notes that PNFA is character-ized by non-fluent spontaneous speech (among other required features). Additionally, patients in this group have significant apraxia of speech (Gorno-Tempini at el., 2004). Signs of this con-dition difficulty include articulatory groping ? i.e. where the mouth searches for the correct con-figurations.  Such trial and error speech often sounds ?robotic? and can involve sounds that may be held out longer. Thus, given the duration features that are generally associated with aprax-ia of speech (Samuel at el., 1996; Edythe at el., 1996; Ballard and Robin, 2002), we hypothesize that PNFA patients would exhibit significantly longer vowel and consonant durations than con-trols (H6).   The second acoustic feature hypothesis (H7) is based on the fact that in the Neary criteria (Neary at el., 1998) pressured speech is a supportive (but not a core) diagnostic feature of both SD and bvFTD. In pressured speech one sees rapid 
Hypothesis and source Supported in LFE of HT? Supported in LFE or AFE of AT? Figures (see Supplementary Materials) H1. AD patients use more pronouns, verbs, and adjectives and fewer nouns than controls (Forbes at el., 2002) 
Yes, but only significant for nouns  Yes, significant for nouns, pronouns, and adjectives Figure 3  H2. SD patients use fewer nouns (Grossman and Ash, 2004) Yes Yes, but not significant vs PNFA  Figure 3 H3. SD patients use more pronouns (Grossman and Ash, 2004)  Yes Partial: SD sig. > CNTRL only  Figure 3 H4. Lower verb frequency in PNFA (Grossman and Ash, 2004) Yes, but only significant vs. SD No Figure 3 H5. Fewer function words in PNFA (Saffran at el., 1989) Yes Yes, but only significant vs SD Figure 3 H6. PNFA patients would exhibit longer vowel and consonant durations N/A Yes Figure 2 H7. SD and bvFTD patients have shorter pauses than controls. N/A Yes Figure 2 Table 2. Hypotheses extracted from literature and whether our measures?based on human transcripts (HT) and automatic transcripts (AT)?support them [Hypotheses 1-5 relate to Lexical Feature Ex-traction; Hypotheses 6-7 relate to Acoustic Level Analyses] 
31
?flight of ideas? speech. We would thus expect some patients in these conditions to exhibit press of speech, and so hypothesize that the mean du-ration of pauses should be significantly less than controls (H7). 3 RESULTS Results suggest that analyses at the lexical and acoustic levels are capable of detecting differ-ences in accordance with expectations of prior research. Additionally, machine-learning algo-rithms predict clinical diagnosis surprisingly well. 3.1 Results: Acoustic-Level Hypotheses For each measure, we performed an ANOVA with respect to diagnosis and found that 25 out of 41 measures were significant at the (Benjamini-Hochberg multiple comparison adjusted) 0.05 level.  Hypotheses 7 and 8 in Table 2 and Figure 2 in Supplementary Materials deal specifically with AFE measures. These show that PNFA pa-
tients do exhibit significantly longer vowel and consonant durations, as the literature linking PNFA with apraxia of speech would predict. Fur-thermore, SD and bvFTD patients have signifi-cantly shorter pauses than controls, which is con-sistent with the hypothesis that some patients with these diagnoses exhibit press of speech.  3.2 Results: Lexical-Level Hypotheses  There were several lexical-level differences be-tween diagnostic groups. We checked for signifi-cant differences (hereafter, ?significant fea-tures?) with respect to diagnosis while using the Benjamini-Hochberg test for multiple compari-sons (Benjamini and Hochberg, 1995).  (We use this adjustment for all multiple comparisons). There were several more lexical level differences based on the HTs than one would predict by chance. For example, 11 of the 14 POS features were significant (p ? .05) including verbs, nouns, adjectives and adverbs. For LIWC features, 22 of 81 features were statistically significant at the p 
  (A) (B) (C) (D) 
  FTLD vs AD vs Controls 
AD vs SD vs PNFA vs bvFTD vs Control FTLD vs AD AD vs Controls 1. Random diagnosis 33% 20% 50% 50% 2. Na?ve learner (always picks largest class in training set) 63% 27% 77% 50% 
3. Our method 80% 61% 88% Sens/Spec AD .58/0.77 Sens/Spec FTLD .95/.89 
88% ! = .64 /Spec AD .83/.90 Sens/Spec Controls .92/.86 
4. Radiologists in Kl?ppel at el. (2008) using MRI data   69% Sens/Spec AD .64/.71 89% Sensi/Spec AD .88/.90 5. Frontal Behavioral Inventory in Blair at el. (2007)   75%  6. Neuropsychiatric inventory in Blair at el. (2007).   54%  7. NINCDS-ARDA criteria in Lopez at el. (1990)    ! = .36 ? .65 8. DSM-III criteria in Kukull at el. (1990)    ! = .55 9. NINCDS criteria in Kukull at el. (1990)    ! = .64 10. ECRDC criteria in Kukull at el. (1990)    ! = .37 Table 3. Accuracy, sensitivity and specificity for Layered Perceptron learned models for FTLD subtypes. (Accuracy of a random and na?ve learner id 33% and 43% respectively) 
32
<= 0.05 level, with p ? 0.005 for 17 of them. As to the question of whether the profile differences correspond meaningfully to existing literature, Table 2 shows which literature-generated hy-potheses were supported. See Figure 3 in Sup-plementary Materials which show the means and standard error for each diagnostic class on a par-ticular feature.  3.3 Machine Learning Results Using cross-validation, we tested the ability of machine learning methods to produce algorithms that could synthesize lexical-level and acoustic-level profiles and then identify the clinician di-agnosis.  We tried several different machine-learning algo-rithms and found that performance was roughly the same.  See Table 3 for the performance of the Multi-layered Perceptron algorithm, which was slightly superior. Performance was measured across several different diagnostic problems (e.g., FTLD vs AD vs Controls (Column A), AD vs Controls (Column D), etc.). For purposes of rough comparison, Table 3 also provides diag-nostic performance of other methods, including radiologists using MRI data.   In evaluating machine learning results, we wished to compare model performance against various benchmarks. The two easiest such benchmark are random guessing (see Table 3 Row 1: given N diagnostic alternatives, one has a 1 / N chance of correctly guessing) and na?ve learner guessing, (see Table 3 Row 2) which always chooses the most frequent (i.e., modal) diagnosis found in the training sample.  The row labeled ?Our method? corresponds to the accura-cy of models generated from lexical and acoustic features using AT. For this case, HT results dif-fers from AT in accuracy by only 2-3% for all prediction problems. Note that our method is at least equal to the accuracies, sensitivities, speci-ficities, and kappa?s of the other clinical bench-marks in most cases. See Table 4, which shows the performance on distinguishing FTLD sub-types.  For more detail on machine learning re-sults see Peintner et al (2008). 4 DISCUSSION The accuracy of the best machine learned diag-nostic model was 88% in the binary classifica-tions of AD versus FTLD, and AD versus Con-trols (Table 3).  Acoustic and lexical level differ-
ences are detectable despite the present level of ASA inaccuracy. Although diagnosis should never be made on the basis of one source of in-formation, our pilot data show that automatic computer-based analyses of spontaneous speech show promise as diagnostic aids by detecting the at times subtle differences that characterize these neurodegenerative disorders.  Inferences drawn from these results are subject to a variety of assumptions and limitations.  Per-haps the biggest limitation is the small number of research participants.  Larger samples will be needed in order to make valid generalizations to the population. Small samples increase the prob-ability of Type I and II Errors and decrease pow-er in testing for normality.  That said, many of our hypothesized linguistic differences based on prior research were confirmed.  Additionally, low N in each group entailed that test sets in each fold were small.  Though it is remarkable in our pilot study that we obtained classification accu-racy on par with clinical judgment, a larger sam-ple size is required to make a rigorously valid claim about on par accuracy.    Statistically minded readers may question our use of parametric statistics (ANOVA) in feature selection because we have not tested the normali-ty assumption.  There are too few observations in each group to test for normality of residuals with any power.  In future work with a larger sample we should perform such a test.  Alternatively, on the present data we could use the non-parametric Kruskal-Wallis test as a stand in for ANOVA.  Additionally, such readers may question our use of the Benjamini-Hochberg (BH) adjustment which controls false discovery rate over a more stringent correction for familywise error rate such as Bonferoni or Holm.  Our rationale was that an occasional false positive (5% if we have a 5% false positive rate) among our total set of positives isn?t a big concern.  As our focal aim was machine learning, scientific discovery, was a secondary concern.  Thus, we were less interest-ed in the question ?was there any difference be-tween the groups".  We were more interested in which features showed a difference.  Better to have a small proportion of false positives than to miss true positives.  In addition, because the false negative rate criterion is less stringent about false positives, the BH procedure tends to have greater power than multiple comparison approaches that control the familywise error rate. 
33
 The success of our methods is surprising given (1) we have performed no customization of ?off the shelf? LFE and machine learning techniques; (2) models were trained on a relatively small number of subjects; (3) speech samples were short (3-5 minutes). Larger speech samples, larg-er N and more tailored tools (e.g. language mod-els) will enable lower word error rate, higher ac-curacy and finer discrimination amongst and within diagnostic types. It also suggests that this can be accomplished without training the system to the voice of each subject.   The results also draw significance because the overall approach may be applied to other neuro-logical or psychological disorders.    Many such disorders have characteristic lexical or acoustic profiles.  For example, Jarrold (2011) and Stir-man et al (2001) have shown that depression is associated with high frequencies of first person words (I, me, I?ve) and lower frequencies of so-cial and second person words (us,we).  Sanchez et al (2011) and Keskinpala (2007) have shown acoustic prosodic features indicative of depres-sion or suicide risk.  Our results suggest a very similar study design can be applied to detect the-se kinds of depression related lexical and acous-tic/prosodic profiles.    Our results suggest we may be able train the models to assess specific highly diagnostic lan-guage symptoms ? such as fluency, circumlocu-tion, and apraxia of speech.  This can be particu-larly important where the inter-rater reliability of given symptoms is poor.  We believe that poor inter-rater reliability is mainly caused by the ina-bility to precisely delineate the objective charac-teristics of these symptoms.  Assuming we can get a range of values that characterize a given symptom, we can apply machine learning to identify symptoms in addition to diagnosis.   We view the methods described as analogous to EKG.  The EKG trace affords a more quantita-
tive and objective picture of cardiac functioning which complements the stethoscope.  Analogous-ly, if scaled-up studies can demonstrate adequate diagnostic accuracy results, then computationally extracted lexico-acoustic profiles may someday augment information provided by current speech and language diagnostic methods which are cur-rently based substantially on subjective clinical judgment.  As modern EKG?s provide automatic interpretation, our analysis suggests that classifi-cation of speech as AD-like or FTLD-like may be possible.  The competent physician never re-lies only the automated diagnosis provided by EKG but also interprets a profile of measures in the context of clinical observation. Our assump-tion is that the methods outlined above should be used in a way analogously to the EKG.  The results of our hypothesis testing show that differences in feature profiles are generally con-sistent with what we would expect from the clin-ical literature.  This may be the first of several steps required to provide assurance to clinicians who would prefer to trust a model that had somewhat transparent features to the opaque ?black box? models that are often learned.  Es-tablishing trust of clinicians is required for wide scale adoption and future work should build on these results.   Our pilot data suggest this approach provides diagnoses of comparable accuracy to other more time intensive or more invasive methods (e.g. neuropsychological testing or imaging). This is a fast, inexpensive, and non-invasive means of obtaining diagnostically useful information. Thus the tool may show most promise as a screening tool to decide which patients need deeper evalua-tion.   Additionally, it may provide objective and quantifiable measures of speech and language symptomatology ? a kind of symptomatology for which there are few objective, quantifiable measures.   5 Conclusion Clinical speech analytics applied to spontaneous speech can detect distinguish between AD, bvFTD, SD PNFA and healthy control groups via lexico-acoustic profiles. Diagnostic accuracy is comparable to other clinical data sources de-spite speech sample brevity.  Accuracy levels suggest the approach offers promise as an addi-tional, objective and easily obtained source of diagnostic information for clinicians. 
Accuracy bvFTD (Sens/Specif) PNFA SD 63% .51 / .58 .54 / .72 .76 / .62 Table 4. Accuracy, sensitivity and specificity for Lay-ered Perceptron learned models for FTLD subtypes. (Accuracy of a random and na?ve learner id 33% and 43% respectively) 
34
Reference Varma A.R., Snowden J.S., Lloyd J.J., Talbot P.R., Mann D.M.A., Neary D. 1999. Evaluation of the NINCDS-ADRDA criteria in the differentiation of Alzheimer?s disease and frontotemporal dementia, Journal of Neurology, Neurosurgery and Psychia-try, 66: 184-188. Kl?ppel, S., Stonnington, C.M., Barnes, J., Chen, F., Chu, C., Good, C.D., Mader, I., Mitchell, L.A., Pa-tel, A.C., Roberts, C.C., Fox, N.C., Jack, R. Jr, Ashburner, J., Frackowiak , RS. 2008. Accuracy of dementia diagnosis: a direct comparison between radiologists and a computerized method, Brain, 131(11): 2969?2974. S. Pestell, M. Shanks, J. Warrington, and A. Venneri. 2000. Quality of spelling breakdown in Alzheimer's disease is independent of disease progression, Journal of Clinical and Experimental Neuropsy-chology, volume 22, pages 599?612. H. Platel, J. Lambert, F. Eustache, B. Cadet, M. Dary, F. Viader, and B. Lechevalier. 1993. Character-stics and evolution of writing impairment in Alz-heimer's disease, Journal of Clinical and Experi-mental Neuropsychology, volume 22, pages 599?612. K. Forbes, A. Venneri, and M. Shanks. 2002. Distinct patterns of spontaneous speech deterioration: an early predictor of Alzheimer's disease, Brain and Cognition, volume 48(2-3): 356?61.  C. Crockford and R. Lesser. 1994. Assessing func-tional communication in aphasia: Clinical utility and time demands of three methods, European Journal of Disorders of Communication, volume 29: 165?182. Thomas, V., Keselj, N., Cercone, K., Rockwood, E. 2005. Automatic detection and rating of dementia of Alzheimer type through lexical analysis of spon-taneous speech, IEEE International Conference on Mechatronics and Automation. Mendez, M.F., Selwood, A., Mastri, A.R., Frey, W.H. 1993. 2nd, Pick's disease versus Alzheimer's dis-ease: A comparison of clinical characteristics, Neurology, 43(2): 289?92. Neary, D., Snowden, J.S., Gustafson, L., Passant, U., Stuss, D., Black, S., Freedman, M., Kertesz, A., Robert, H., Albert, M., Boone, K., Miller, B.L., Cummings, J., Benson, D.F. 1998. Frontotemporal lobar degeneration: A consensus on clinical diag-nostic criteria, Neurology, 51(6): 1546?54. Davies, R.R., Hodges, J.R., Kril, J.J., et al. 2005. The pathological basis of semantic dementia. Brain, 128(9): 1984?95.  Josephs, K.A., Duffy, J.R., Strand, E.A., et al. 2006. Clinicopathological and imaging correlates of 
progressive aphasia and apraxia of speech. Brain, 129(6): 1385?98. Bright, P., Moss, H. E., Stamatakis, E. A., & Tyler, L. K. 2008. Longitudinal studies of semantic demen-tia: The relationship between structural and func-tional changes over time, Neuropsychologia, 46: 2177-2188. S. Singh, R. Bucks, and J. Cuerden. 2001. Evaluation of an objective technique for analysing temporal variables in DAT spontaneous speech, Aphasiolo-gy, volume 15(6): 571?584.  Stefanie Abel, Walter Huber, Gary S. Dell. 2009. Connectionist diagnosis of lexical disorders in aphasia, Aphasiology, volume 23. Dilek Hakkani-T?r, Dimitra Vergyri, G?khan T?r. 2010. Speech-based automated cognitive status as-sessment. Interspeech 2010: pages 258-261. Maider Lehr, Emily T. Prud?hommeaux, Izhak Shafran and Brian Roark. 2012. Fully Automated Neuropsychological Assessment for Detecting Mild Cognitive Impairment. In Proceedings of Inter-speech. Kertesz, A. 1980. Western Aphasia Battery, London, Ontario: University of Western Ontario Press. Stolcke, A., Boakye, K., Cetin, ?., Janin, A., Magimai-Doss, M., Wooters, C., Zheng, J. 2007. The SRI-ICSI Spring 2007 meeting and lecture recognition system, Proc. NIST 2007 Rich Tran-scription Workshop. Pennebaker, J.W., Francis, M.E., Booth, R.J. 2001. Linguistic Inquiry and Word Count (LIWC): LIWC2001, Mahwah, NJ: Erlbaum Publishers. Toutanova, K., Klein, D., Manning, C., Singer, Y. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network, in Proceedings of HLT-NAACL 2003, pages 252?259. Grossman, M., Ash, S. 2004. Primary Progressive Aphasia: A Review, Neurocase, 10(1): 3?18. Gorno-Tempini, M.L, Dronkers, N.F., Rankin, K.P., Ogar, J.M., La Phengrasamy, B.A., Rosen, H.J., Johnson, J.K., Weiner, M.W., Miller, B.L, Cogni-tion and Anatomy in three variants of primary pro-gressive aphasia, Annals of Neurology, 2004. 55: 335?346. Samuel A. K. Seddoh, Donald A. Robin, Hyun-Sub Sim, Carlin Hageman, Jerald B. Moon, John W. Folkins. 1996. Speech Timing in Apraxia of Speech versus Conduction Aphasia, Journal of Speech and Hearing Research, 39: 590?603. Edythe A. Strand, E.A., McNeil, M.R. 1996. Effects of Length and Linguistic Complexity on Temporal Acoustic Measures in Apraxia of Speech, Journal of Speech and Hearing Research, 39: 1018?33. 
35
Kirrie J. Ballarrd, Ph.D., and Donald A. Robin. 2002. Assessment of AOS for Treatment Planning, Semi-nars in Speech and Language, 23(4): 281?291. Witten, I.H., Frank, E. 2005. Data mining: Practical machine learning tools and techniques, San Fran-cisco: Morgan Kaufmann. Second edition. Benjamini, Y., Hochberg Y. 1995. Controlling the False Discover Rate: A Practical and Powerful Approach to Multiple Testing, Journal of the Royal Statistical Society. Series B (Methodological), 57(1): 289?300. Saffran, E.M., Berndt, R.S., Schwartz, M.F. 1989. The quantitative analysis of agrammatic production: procedure and data, Brain and Language, 37(3): 440?79. Blair, M., Kertesz, A., Davis-Faroque, N., Hsiung, G.Y.R., Black, S.E., Bouchard, R.W., Gauthier, S., Guzman, D.A., Hogan, D.B., Rockwood, K., Feldman. H. 2007. Behavioural Measures in Fron-totemporal Lobar Dementia and Other Dementias: The Utility of the Frontal Behavioural Inventory and the Neuropsychiatric Inventory in a National Cohort Study, Dementia and Geriatric Cognitive Disorder, 23: 406-15 Lopez, O. L., Swihart, A. A., Becker, J. T., Reinmuth, O. M., Reynolds, C. F., Rezek, D. L., Daly, F. L. 1990. Reliability of NINCDS-ADRDA clinical cri-teria for the diagnosis of Alzheimer's disease, Neu-rology, 40: 1517 Kukull, W. A., Larson, E. B., Reifler, B. V., Lampe, T. H., Yerby, M., Hughes, J. 1990. Interrater reli-ability of Alzheimer's disease diagnosis, Neurolo-gy, 40(2): 257-60 Peintner, B., Jarrold, W, Vergyri, D., Richey, C., Gorno Tempini, M., and Ogar, J. 2008. Learning Diagnostic Models Using Speech and Language Measures, 30th Annual International IEEE EMBS Conference, August 20-24, Vancouver, British Co-lumbia, Canada. Jarrold, W., Javitz, H.S., Krasnow, R., Peintner, B., Yeh E., Swan, G.E. (2011) Depression and Self-Focused Language in Structured Interviews with Older Adults Psychological Reports Oct;109(2):686-700. Stirman, S.W., & Pennebaker, J.W. (2001). Word use in the poetry of suicidal and non-suicidal poets. Psychosomatic Medicine 63, 517-522. Michelle Hewlett Sanchez, Dimitra Vergyri, Luciana Ferrer,,Colleen Richey, Pablo Garcia, Bruce Knoth, William Jarrold: Using Prosodic and Spec-tral Features in Detecting Depression in Elderly-Males. INTERSPEECH 2011: 3001-3004 H. Kaymaz Keskinpala, T. Yingthawornsuk, D. Mitchell Wilkes, Richard G. Shiavi, R. M. Salo-mon: Distinguishing high risk suicidal subjects 
among depressed subjects using mel-frequency cepstrum coefficients and cross validation tech-nique. MAVEBA 2007: 157-160      
36
Supplementary Materials   
 Figure 2. Vowel, consonant, and pause  
 Figure 3. Verb, adjective, pronoun, noun and function word frequencies (H1, H2, H3, H4, H5)  
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Vowel duration Consonant duration Pause/Spurt
SD AD PNFA FTD CTRL
0%
10%
20%
30%
40%
50%
60%
70%
80%
Verbs(HT) Verbs(AT) Adjectives(HT) Adjectives(AT) Pronouns(HT) Pronouns(AT) Nouns(HT) Nouns(AT) FunctionWords(HT) FunctionWords(AT)
SD AD PNFA FTD CTRL
37
