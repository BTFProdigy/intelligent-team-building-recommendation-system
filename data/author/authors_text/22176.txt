The Telling Tail:
Signals of Success in Electronic Negotiation Texts
Marina Sokolova
IRO, Universite? de Montre?al
Montre?al, Que?bec, Canada
sokolovm@iro.umontreal.ca
Vivi Nastase
EML Research, gGmbH
Heidelberg,Germany
nastase@eml-research.de
Stan Szpakowicz
SITE, University of Ottawa
Ottawa, Ontario, Canada
ICS, Polish Academy of Sciences
Warsaw, Poland
szpak@site.uottawa.ca
Abstract
We analyze the linguistic behaviour of par-
ticipants in bilateral electronic negotiations,
and discover that particular language char-
acteristics are in contrast with face-to-face
negotiations. Language patterns in the later
part of electronic negotiation are highly in-
dicative of the successful or unsuccessful
outcome of the process, whereas in face-to-
face negotiations, the first part of the nego-
tiation is more useful for predicting the out-
come. We formulate our problem in terms of
text classification on negotiation segments
of different sizes. The data are represented
by a variety of linguistic features that cap-
ture the gist of the discussion: negotiation-
or strategy-related words. We show that,
as we consider ever smaller final segments
of a negotiation transcript, the negotiation-
related words become more indicative of the
negotiation outcome, and give predictions
with higher Accuracy than larger segments
from the beginning of the process.
1 Introduction
We use language every day to convince, explain, ma-
nipulate and thus reach our goals. This aspect of
language use is even more obvious in the context
of negotiations. The parties must reach an agree-
ment on the partitioning or sharing of a resource,
while each party usually wants to leave the negotia-
tion table with the larger piece of the pie. These ten-
dencies become stronger when negotiators use only
electronic means to communicate, that is to say, par-
ticipate in electronic negotiations. In face-to-face
contact, prosody and body language often have a
crucial role in conveying attitudes and feelings. E-
negotiators, on the other hand, must rely only on
texts. We perform automatic analysis of the textual
data in e-negotiations. We identify linguistic expres-
sions of such negotiation-specific behaviour that are
indicative of the final outcome of the process ? suc-
cess or failure ? and observe how powerful a tool
language is in helping people get what they want.
In this paper we focus on the negotiation as an on-
going process. We analyze the linguistic features of
messages exchanged at various points in the course
of the negotiation, to determine the time frame in
which the outcome becomes decided. From our ex-
perimental point of view, we determine the segment
of the negotiation which is most predictive of the
outcome. There is an imposed three-week deadline
in the electronic negotiations that we analyze. We
hypothesize that the pressure of the deadline is re-
flected in the messages exchanged. The messages
written later in the process are more indicative of
the outcome of the process. Our empirical results
support this hypothesis; an analysis of the linguis-
tic features that make this prediction possible shows
what the negotiators? main concerns are as the dead-
line draws near.
Here is what our results contribute to the field
of text analysis. Research on text records of face-
to-face negotiations suggests that the language pat-
terns used in the first half of a negotiation predict
the negotiation outcome better than those in the sec-
ond half (Simons, 1993). The explanation was that
257
in the first phase people establish contact, exchange
personal information and engage in general polite
conversation, creating a foundation of trust between
partners. No numerical data, however, supported
this diagnosis, and there was no distinction between
the prediction of successful and unsuccessful out-
comes. When it comes to text classification, our
hypothesis says that the classification of the second
parts of e-negotiation texts is more accurate with re-
spect to the outcome than the classification of the
first parts. This makes e-negotiation texts different
from newsgroup messages, newspaper articles and
other documents classified by Blatak et al (2004),
where texts showe d better classification Accuracy
on their initial parts. We report the results of several
sets of Machine Learning (ML) experiments. Per-
formed on varying-size text data segments, they sup-
port our hypothesis.
We worked with a collection of transcripts of
negotiations conducted over the Internet using the
Web-based negotiation support system Inspire (Ker-
sten and Zhang, 2003). Kersten and Zhang (2003)
and Nastase (2006) classified e-negotiation out-
comes using non-textual data. Classification based
on texts is discussed in (Sokolova et al 2005;
Sokolova and Szpakowicz, 2006). None of those ex-
periments considered segmenting the data, although
Sokolova and Szpakowicz (2006) analyzed the im-
portance of the first part of e-negotiations. The work
we present here is the first attempt to investigate the
effect of parts of e-negotiation textual data on classi-
fication quality. In this study we do not report types
of expressions that are relevant to success and failure
of negotiations. These expressions have been pre-
sented and analyzed in (Sokolova and Szpakowicz,
2005).
In section 2 we take a brief look at other work on
the connection between behaviour and language. In
section 3 we present our data and their representa-
tion for ML experiments, and we further motivate
our work. Section 4 describes the experiments. We
discuss the results in Section 5. Section 6 draws con-
clusions and discusses a few ideas for future work.
2 Background Review
Young (1991) discusses the theory that the situation
in which language is used affects the way in which it
is used. This theory was illustrated with a particular
example of academic speech.
The field of neuro-linguistic programming in-
vestigates how to program our language (among
other things) to achieve a goal. In the 1980s,
Rodger Bailey developed the Language and Be-
haviour Profile based on 60 meta-programs. Charvet
(1997) presents a simplified approach with 14 meta-
programs. This profile proposes that people?s lan-
guage patterns are indicators of behavioural pref-
erences. In the study of planning dialogues (Chu-
Carroll and Carberry, 2000), Searle?s theory of
speech acts used through the discourse analysis also
supports the fact that language carries much of peo-
ple?s behaviour and emotions. Reitter and Moore
(2007) studied repetitions in task-oriented conver-
sations. They demonstrated that a speaker?s short-
term ability to copy the interlocutor?s syntax is au-
tonomous from the success of the task, whereas
long-term adaptation varies with such success.
We consider a negotiation to be a communication
in which the participants want to reach an agreement
relative to the splitting/sharing of resources. Lan-
guage is one of the tools used to reach the goal. We
propose that not all messages exchanged throughout
a negotiation have the same effect on the negotiation
outcome. To test this hypothesis, we take an ever
smaller segment of the negotiation, and see howwell
we can predict the outcome of the process, based
only on the messages in this fragment.
We encountered several challenges in predict-
ing e-negotiation outcomes using the messages ex-
changed. First, electronic negotiations usually do
not have a sequential-stage model of behaviour
(Koeszegi et al 2007), which is common in face-
to-face negotiations (Adair and Brett, 2005). Here is
an example of behavioural phases in face-to-face ne-
gotiations: Perform Relational Positioning ? Iden-
tify the Problem ? Generate Solutions ? Reach
Agreement. Unexpected turns and moves ? typical
of human behaviour ? make prediction of the ne-
gotiation outcome difficult. In case of electronic
negotiation, the absence of the usual negotiation
structure further complicates the outcome predic-
tion. This distinguishes e-negotiations from agent-
customer phone conversations studied in (Takeuchi
et al 2007), where an agent follows the call flow
pre-defined by his company?s policy.
258
The longer an e-negotiation takes, the more elab-
orate the structure of the e-negotiation process be-
comes. Simpler e-negotiation may involve an ex-
change of well-structured business documents such
as pre-defined contract or retail transactions. A
more complex process comprises numerous offers
and counter-offers and has a high degree of uncer-
tainty because of the possible unpredictability of ne-
gotiation moves.
The next challenge stems from the limitations im-
posed by the use of electronic means. This overloads
text messages with various tasks: negotiation issues
themselves, introductions and closures traditional in
negotiations, and even socializing. On the other
hand, electronic means make the contacts less for-
mal, allowing people to communicate more freely.
As a result, the data have a high volume of informal-
ity such as abbreviations or slang.
The last challenge is specific to text analysis. E-
negotiations usually involve a multi-cultural audi-
ence of varied background, many of whom are not
native English speakers. While communicating in
English, they introduce a fair amount of spelling and
grammatical mistakes.
3 Textual Data in Electronic Negotiations
Participants in a negotiation assume well-defined
roles (such as buyers/sellers in some business nego-
tiations, or facilitators in legal disputes), have goals,
and adopt specific behaviour to achieve those goals
(Koeszegi et al 2007). These circumstances are re-
flected in the language of texts exchanged in negoti-
ations, and distinguish this type of texts from casual
e-mail exchange and postings on discussion groups
and chat boards. We claim that the language cap-
tured in e-negotiation textual data changes as a nego-
tiation progresses, and that this is clearly detectable,
even though it does not follow a sequential-stage
model common in face-to-face-negotiations (Adair
and Brett, 2005) or an agent-customer interaction
call flow recommended by a company (Takeuchi et
al, 2007). To support the language change hypothe-
sis, we have conducted a series of ML experiments
on negotiation segments of varying size and posi-
tion, using the largest available data of electronic ne-
gotiations.
Our data come from the Web-based negoti-
ation support system Inspire. Inspire has been
used in business courses to teach students about
e-negotiations and give them a chance to practice
bilateral business negotiations conducted in a lightly
controlled environment. For many users, conducting
negotiations has been a business/ course assignment.
Other users wanted to develop their English skills
by participating in an Inspire-enabled negotiation.
A negotiation would last up to three weeks, after
which, if an agreement has not been reached, the
systems would terminate the negotiation and record
it as unsuccessful. The following is an example of a
negotiation message (with the original spelling):
Dear Georg, I hope you are doig well. I send you this message
to ask you what happened to our offer. Just be aware that
we will not be indifinitely waiting on your response. As I
told you during our last meeting, Itex Manufacturing needs
a partnership. So it is important to me to know if you are
ready to negotiate with us. We can not afford losing so much
precious time. We give you now five more days to answer
our offer (1st of december 1997, 2400 Swiss time). After this
dead line, will propose our services to your concurrence. I
still believe in a good partnership and relationship between
our two societies. Let me know if you think so. For Itex
Manufacturing. Rookie.
Among the wealth of data gathered by Inspire, we
have focussed on the accompanying text messages,
extracted from the transcripts of 2557 negotiations.
Each negotiation had two different participants, and
one person participated in only one negotiation. The
total number of contributors was over 5000; most
of them were not native English speakers. The data
contain 1, 514, 623 word tokens and 27, 055 types.
Compared with benchmark corpora, for example the
Brown or the Wall Street Journal corpus (Francis
and Kucera, 1997; Paul and Baker, 1992), this col-
lection has a lower type-token ratio and a higher
presence of content words among the most frequent
words (this is typical of texts on a specific topic), and
a high frequency of singular first- and second-person
pronouns (this is typical of dialogues).
We considered all messages from one negotiation
to be a single negotiation text. We concatenated the
messages in chronological order, keeping the punc-
tuation and spelling unedited. Each negotiation had
a unique label, either positive or negative, and was
a training example in one of two classes ? success-
259
Features Split NB SVM DT
Acc F P R Acc F P R Acc F P R
negotiation-related 1/2 and 1/2 68.1 70.4 73.0 68.0 73.6 76.8 75.4 78.2 73.9 78.8 72.1 86.8
negotiation-related 3/4 and 1/4 69.1 71.3 74.1 68.7 73.7 77.0 75.5 78.5 75.4 79.4 73.8 86.0
Table 1: Accuracy and corresponding F ? score , Precision and Recall . Classifying all negotiations as successful gives a
baseline Accuracy of 55%.
ful or unsuccessful. Inspire assigned a negotiation
to the right class automatically. 55% of negotiations
in our data set were successful, i.e. ended up with
agreement.
We represented a complete negotiation, or text as
we consider it, as a combined bag of words. We
matched the tokens in the messages with an inven-
tory of domains from Longman Dictionary of Con-
temporary English (Procter, 1978). This allowed us
to select those terms that refer to negotiation specific
issues ? we call them negotiation-related words. We
select strategic words based on words and patterns
that literature shows to express the intentions, influ-
ence, self-obligations and motivations of the negoti-
ation participants. In classifying successful and un-
successful negotiations, subsets of these two types
of features provided better Accuracy than statisti-
cally selected features, e.g. most frequent unigrams
and unigrams with a higher log-likelihood values
calculated between positive and negative classes
(Sokolova et al 2005).
We halved each text, that is to say, the complete
record of a negotiation. For each half we built a
bag of 123 negotiation-related words ? more on this
in section 4. The binary attributes represented the
presence or absence of the word in its half of the
text. We concatenated the two bags, and labelled
the resulting bag by the outcome of the whole ne-
gotiation: positive if the negotiation was successful,
negative otherwise. We repeated this procedure for
the split of the negotiation text into 34 and
1
4 . Our
ML tools were Weka?s (Witten and Frank, 2005)
NAIVE BAYES (NB), the sequential minimal optimiza-
tion (SVM) version of SUPPORT VECTOR MACHINE, and
DECISION TREE (DT). In Table 1 we report Accuracy
and Precision (P ), Recall (R) and F ? score (F ).
P , R, F are calculated on the positive class. For
every classifier, the best Accuracy and correspond-
ing P , R, F are reporte d; we performed an exhaus-
tive search on adjustable parameters; the evaluation
method was tenfold cross-validation. Our Accuracy
results are comparable with those reported in pre-
vious studies (Kersten and Zhang, 2003; Nastase,
2006; Sokolova and Szpakowicz, 2006).
We used the paired t-test to generalize the results
on both splits.1 The two-tailed P value was 0.0102.
By conventional criteria, this difference is consid-
ered to be statistically significant.
Accuracy and, especially, Precision results show
that DECISION TREE is sensitive to the positions of
words in different parts of the negotiations. SUPPORT
VECTOR MACHINE and NAIVE BAYES change Accuracy
only slightly. The Precision and Recall results
give a better picture of the performance. The
presence/absence of words recorded for different
splits of negotiations influences the identification of
true positive examples (successful negotiations) and
true negative examples (unsuccessful negotiations).
Recall displays that DT classifies successful negoti-
ations better when the negotiations are split 12 and
1
2 .
Precision andRecall together imply that unsuccess-
ful negotiations have a higher rate of true classifica-
tion achieved by NB, when the split is 34 and
1
4 . This
split lets us improve the worst rates of true classifi-
cations ? unsuccessful negotiations for DT and suc-
cessful negotiations for NB. Generally, the unequal
split al lows us to reduce the difference between true
positive and true negative classification results, and
thus makes the classification of negotiations more
balanced than the equal split. For all the three clas-
sifiers, Accuracy and F ? score are better on the 34
and 14 split.
4 The Empirical Set-up
We wanted to determine the placement of the seg-
ment of a negotiation most important in deciding
whether the outcome is positive: at the beginning
or at the end of the process. To do that, we split each
negotiation in half, and built two parallel data sets,
corresponding to the two halves. We classified each
1Results on the same data require the paired version of t-test.
260
part using various ML tools. Next, we repeated the
same classification tasks using smaller and smaller
final segments, in order to monitor the variation in
performance. Thus each negotiation text N con-
sisted of the head segment (h) and the tail segment
(t): N = h
?
t, h
?
t = ?, where |t| = |N |i and t was
the segment at the end of N , and |h| = (i?1)|N |i cov-
ering the beginning of the negotiation. We stopped
when for two consecutive splits two classifiers had
better Accuracy on the head than on the tail. Each
segment got the same class label as the whole nego-
tiation.
For these experiments, as briefly explained in sec-
tion 3, we took the textual negotiation data repre-
sented as bags of words. Because of the large num-
ber of word features (27, 055 tokens), we performed
lexical feature selection.
Statistical analysis of the corpus built from the
Inspire negotiation messages has revealed that the
issues discussed in these messages can be grouped
into a small set of topics. The particular topic
or domain to which a word belongs derives from
the most frequent bigram and trigram meanings;
for instance, the second most frequent trigram
with the word delivery is payment upon delivery, so
we assign delivery to the domain negotiation process.
The data come from negotiations on a specific
topic (sale/purchase of bicycle parts), so a likely
candidate subset would be words related to it. We
select such negotiation-related words as the first
set of features. We show a text sample with the
negotiation-related words in bold:
Dear Georg, I hope you are doig well. I send you this message
to ask you what happened to our offer. Just be aware that
we will not be indifinitely waiting on your response. As I
told you during our last meeting, Itex Manufacturing needs
a partnership. So it is important to me to know if you are
ready to negotiatewith us. We can not afford losing so much
precious time. We give you now five more days to answer our
offer (1st of december 1997, 2400 Swiss time). After this
dead line, we will propose our services to your concurrence.
I still believe in a good partnership and relationship between
our two societies. Let me know if you think so. For Itex
Manufacturing. Rookie.
Strategies which the negotiators adopt (promises,
threats, exchange of information, argumentation,
and so on) affect the outcome (Sokolova and
Szpakowicz, 2006). Since the messages are dense,
short and grammatically simple, the expression of
strategies through language is straightforward and
concentrates on communicating the main goal. The
word categories that convey negotiators? strategies
are modals, personal pronouns, volition verbs,
mental verbs; we refer to them as strategic words.
Strategic words constitute the second set of features.
Our text sample with strategic words in bold looks
as follows:
Dear Georg, I hope you are doig well. I send you this mes-
sage to ask you what happened to our offer. Just be aware
that we will not be indifinitely waiting on your response. As
I told you during our last meeting, Itex Manufacturing needs
a partnership. So it is important to me to know if you are
ready to negotiate with us. We can not afford losing so much
precious time. We give you now five more days to answer
our offer (1st of december 1997, 2400 Swiss time). After
this dead line, we will propose our services to your concur-
rence. I still believe in a good partnership and relationship
between our two societies. Let me know if you think so.
For Itex Manufacturing. Rookie.
We work with kernel (SVM), decision-based (DT)
and probabilistic (NB) classifiers. Applying classi-
fiers with different working paradigms allow us to
capture and understand different aspects of the data,
as the results and our discussion in section 5 will
show. For each classifier, we used tenfold cross-
validation and exhaustive search on adjustable pa-
rameters in model selection. The best results, in par-
ticular with high overall Accuracy , appear in Fig-
ure 1.
When the data are represented using negotiation-
related words, the tail segments give more accurate
outcome classification than the head segments. This
holds for all splits and all classifiers; see Figure 1.
The increase in Accuracy when the head segments
grow was to be expected, although it does not hap-
pen with DT and SVM ? only with NB. At the same
time, there is no monotonic decline in Accuracy
when the length of the tail segments decreases. On
the contrary, NB constantly improves the Accuracy
of the classification. We note the fact that NB in-
creases theAccuracy on both head and tail segments
and makes the basic assumption of the conditional
independence of features. We explain the NB re-
sults by the decreased dependence between the pres-
261
1. DT
 71
 71.5
 72
 72.5
 73
 73.5
 74
 74.5
 75
 75.5
 1  2  3  4  5  6
Ac
cur
acy
Inverse size of segment
head part, negotiation-related wordstail part, negotiation-related wordshead part, strategic wordstail part, strategic words
2. SVM
 71
 71.5
 72
 72.5
 73
 73.5
 74
 74.5
 75
 75.5
 1  2  3  4  5  6
Ac
cur
acy
Inverse size of segment
head part, negotiation-related wordstail part, negotiation-related wordshead part, strategic wordstail part, strategic words
3. NB
 67.5
 68
 68.5
 69
 69.5
 70
 70.5
 71
 71.5
 1  2  3  4  5  6
Ac
cur
acy
Inverse size of segment
head part, negotiation-related wordstail part, negotiation-related wordshead part, strategic wordstail part, strategic words
Figure 1: The classification Accuracy with DT, SVM and
NB, for negotiation-related and strategic words.
ence/absence of negotiation-related words when the
negotiations move to the second part of the process.
The results on the strategic-word representation
are slightly different for the three classifiers; see
Classifier tail s1 s2 s1 s2 s3
DT 74.4 71.9 74.9 72.5 71.9 73.9
SVM 75.3 70.5 73.5 70.8 69.9 74.6
NB 68.8 68.5 70.1 68.7 68.9 70.9
Negotiation-related words
Classifier tail s1 s2 s1 s2 s3
DT 73.8 73.8 73.4 71.7 71.4 72.9
SVM 73.8 70.9 72.8 72.0 71.3 73.4
NB 60.8 70.6 69.5 69.2 69.3 68.7
Strategic words
Table 2: The Accuracy of the negotiation outcome classifica-
tion on 2 and 3 splits of the second half of the negotiation ? the
tail segment. Classifying all negotiations as successful gives a
baseline Accuracy of 55%.
Figure 1. SVM classifies all tail segments better
than head segments, DT classifies tail segments bet-
ter than head segments up to the 45/
1
5 split, and NB
classifies the tail segment better than the head seg-
ment only for the half-and-half split. The Accuracy
results are unstable for all three classifiers, with the
Accuracy on the head segments decreasing when
the segments grow and the Accuracy on the tail
segments increasing when the tail segments shrink.
The performance of the classifiers indicate that, as
the deadline approaches, negotiation-related words
reflect the negotiation process better than strategic
words.
To investigate which part of the tail segments is
more important for classifying the outcomes, we in-
troduced additional splits in the tail segments. We
divided the second half of each text into 2 and 3
parts and repeated the classification procedures for
every new split. The results appear at the top of
Table 2, where tail shows the classification results
when the second half of the text was classified, and
the other columns report the results on the tail splits;
both splits satisfy the conditions tail =
?
i si, where
si
?
sj = ? for every i 6= j.
The results show that adding splits in the tail seg-
ments emphasizes the importance of the last part of
a negotiation. For negotiation-related word repre-
sentation, the classification of the outcome on the
last part of the tail is more accurate than on its other
parts. This holds for all three classifiers. For the
strategic-word representation the same is true for
SVM and partially for DT, but not for NB; see the
bottom of Table 2. NB classifies the negotiation out-
comes more accurately on s1 than on s2 and on s2
rather than s3.
262
Classifier 1/3 1/4 1/5 1/6 1/7 1/8 1/9
P R P R P R P R P R P R P R
DT 74.2 85.3 74.2 84.3 75.2 82.3 73.61 83.0 74.5 82.4 72.1 81.6 74.0 81.3
SVM 76.1 78.1 76.3 76.3 77.0 75.3 78.3 75.3 77.2 73.4 76.9 72.3 77.6 71.6
NB 73.8 71.8 71.8 73.9 74.8 71.9 74.9 72.0 71.3 72.2 70.8 72.5 70.5 74.3
Table 3: Precision and Recall on the tail segments; negotiation-related words. Precision and Recall are calculated on the
positive class.
 67
 68
 69
 70
 71
 72
 73
 74
 75
 76
 1  2  3  4  5  6  7  8  9
Ac
cur
acy
Inverse size of segment
C5.0SMONaive Bayes
Figure 2: The evolution of the success and failure classifica-
tion Accuracy with decreasing segment sizes.
5 Segmentation Results
Taking into account the results reported in section
4, we chose negotiation-related words as the feature
set. We selected for further analysis the half that per-
formed better for a majority of the tools used. We
focussed on the last part of the negotiation, and we
extracted a gradually smaller fragment (12 ?
1
9 ; 9 is
the average number of text messages in one negotia-
tion). Figure 2 plots the results of the experiments
performed with decreasing segment sizes. As we
see, the tail segment of the length 17 gave a decline
of the Accuracy for SVM and NB, with a slight im-
provement on smaller tail segments.
A more detailed analysis comes from consider-
ing the Precision and Recall results on the seg-
ments; see Table 3. On 17 and
1
9 tail segments a
higher Precision indicates that all classifiers have
improved the identification of true negatives (unsuc-
cessful negotiations). This means that the trends in
the class of unsuccessful negotiations become more
noticeable for the classifiers when the deadline ap-
proaches. The 18 split is an exception, with the
abrupt drop of true negative classification by DE-
CISION TREE. The correct classification of positive
examples (successful negotiations), however, dimin-
ishes when splits become smaller; this applies to the
performance of all three classifiers. This means that
at the end of the negotiations the class of success-
ful negotiations becomes more diverse and, subse-
quently, multi-modal, and the trends are more diffi-
cult to capture by the classifiers.
As in the previous experiments, NB?s Accuracy
on the tail segments is higher than on the complete
data. The opposite is true for SVM and DT: their
Accuracy on the tail segments is lower than on the
complete data. We explain this by the fact that the
sizes of tail segments in the last splits do not give
these two classifiers sufficient information.
6 Discussion and Future Work
We have analyzed textual messages exchanged in the
course of electronic negotiations. The results sup-
port our hypothesis that texts of electronic negoti-
ation have different characteristics than records of
face-to-face negotiation. In particular, messages ex-
changed later in the process are more informative
with regard to the negotiation outcome than mes-
sages exchanged at the beginning.
We represented textual records of negotiations by
two types of word features. These features cap-
ture the important aspects of the negotiation process
? negotiation-related concepts and indicators of the
strategies employed. We performed extensive exper-
iments with different types of ML algorithms and
segments of varying sizes from the beginning and
the end of the negotiation, on a collection of over
2500 electronic negotiations. Our study shows that
words expressing negotiation-related concepts are
more useful for distinguishing successful and failed
negotiations, especially towards the end of negotia-
tions. We also have shown that there is no linear de-
pendency between the segment sizes and Accuracy
of classification of the negotiation success and fail-
ure.
263
Our research plans include a continuation of the
investigation of the negotiators? behaviour in elec-
tronic negotiations and its reflection in language. To
see whether dialogue analysis improves prediction
of the negotiation outcomes, we will look at negotia-
tions as dialogues between participants and take into
account their roles, e.g. buyer and seller. We will
split a negotiation at message boundaries to avoid
arbitrary splits of the negotiation process.
Acknowledgments
Partial support for this work came from the Natu-
ral Sciences and Engineering Research Council of
Canada.
References
W. Adair, J. M. Brett. 2005. The negotiation dance: time,
culture, and behavioral sequences in negotiation. Or-
ganization Science. 16(1): 33-51.
J. Blata?k, E. Mra?kova?, L. Popel??nsky. 2004. Fragments
and Text Categorization. Proceedings of the 42th An-
nual Meeting of the Association of Computational Lin-
guistics (ACL 2004). Companion Volume: 226-229,
Association for Computational Linguistics.
J. M. Brett. 2001. Negotiating Globally. Jossey-Bass.
S. R. Charvet. 1997. Words that Change Minds: Master-
ing the Language of Influence. Kendall/Hunt.
J. Chu-Carroll, S. Carberry. 2000. Conflict Resolution in
Collaborative Planning Dialogues. International Jour-
nal of HumanComputer Studies. 53(6): 969-1015.
W. N. Francis, H. Kuc?era. 1967. Computational Analysis
of Present-Day American English, Brown University
Press.
G. E. Kersten, G. Zhang. 2003. Mining Inspire Data
for the Determinants of Successful Internet Negotia-
tions. Central European Journal of Operational Re-
search. 11(3): 297-316.
S. Koeszegi, E.-M. Pesendorfer, R. Vetschera. 2007.
Data-driven Episodic Phase Analysis of E-negotiation.
Group Decision and Negotiation 2007. 2: 11?130.
V. Nastase. 2006. Concession curve analysis for Inspire
negotiations. Group Decision and Negotiation. 15:
18?193.
D. B. Paul and J. M. Baker 1992 The Design for the
Wall Street Journal-based CSR Corpus. Proceedings
of the 2nd International Conference on Spoken Lan-
guage Processing (ICSLP?92), 357-361.
P. Procter. 1978. Longman Dictionary of Contemporary
English. Longman Group Ltd. Essex, UK.
D. Reitter, J. Moore. 2007. Predicting Success in Dia-
logue. Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics (ACL 2007),
808-815, Association for Computational Linguistics.
T. Simons. 1993. Speech Patterns and the Concept of
Utility in Cognitive Maps: the Case of Integrative Bar-
gaining. Academy of Management Journal. 38(1):
139-156.
M. Sokolova, V. Nastase, M. Shah, S. Szpakowicz.
2005. Feature Selection in Electronic Negotiation
Texts. Proceedings of Recent Advances in Natural
Language Processing (RANLP?2005), 518-524, In-
coma Ltd, Bulgaria.
M. Sokolova, S. Szpakowicz. 2006. Language Patterns
in the Learning of Strategies from Negotiation Texts.
Proceedings of the 19th Canadian Conference on Ar-
tificial Intelligence (AI?2006), 288-299, Springer.
M. Sokolova and S. Szpakowicz. 2005 Analysis and
Classification of Strategies in Electronic Negotiations.
Proceedings of the 18th Canadian Conference on Ar-
tificial Intelligence (AI?2005), 145-157, Springer.
, H. Takeuchi, L. Subramaniam, T. Nasukawa, S. Roy.
2007 Automatic Identification of Important Segments
and Expressions for Mining of Business-Oriented
Conversations at Contact Centers. Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), 458?467, As-
sociation for Computational Linguistics.
I. Witten, E. Frank. 2005. Data Mining, 2nd ed.. Morgan
Kaufmann. www.cs.waikato.ac.nz/ml/weka/
L. Young. 1991. Language as Behaviour, Language as
Code: A Study of Academic English. John Benjamins.
264
Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 44?49,
Dublin, Ireland, August 24 2014.
Recognition of Sentiment Sequences in Online Discussions
Victoria Bobicev
Technical University of 
Moldova
vika@rol.md
Marina Sokolova 
University of Ottawa,
Institute for Big Data 
Analytics, Canada
sokolova@uottawa.ca
Michael Oakes
Research Group in Computational 
Linguistics, University of Wol-
verhampton, UK
Michael.Oakes@wlv.ac.uk
Abstract
Currently 19%-28% of Internet users participate in online health discussions. In this work, we 
study sentiments expressed on online medical forums. As well as considering the predominant 
sentiments expressed in individual posts, we analyze sequences of sentiments in online discus-
sions. Individual posts are classified into one of the five categories encouragement, gratitude, 
confusion, facts, and endorsement. 1438 messages from 130 threads were annotated manually 
by two annotators with a strong inter-annotator agreement (Fleiss kappa = 0.737 and 0.763 for 
posts in sequence and separate posts respectively). The annotated posts were used to analyse 
sentiments in consecutive posts. In automated sentiment classification, we applied HealthAf-
fect, a domain-specific lexicon of affective words.
1 Introduction
      Development of effective health care policies relies on the understanding of opinions expressed by 
the general public on major health issues.  Successful vaccination during pandemics and the incorpora-
tion of healthy choices in everyday life style are examples of policies that require such understanding. 
As online media becomes the main medium for the posting and exchange of information, analysis of 
this online data can contribute to studies of the general public?s opinions on health-related matters.  
Currently 19%-28% of Internet users participate in online health discussions (Balicco and Paganelli, 
2011). Analysis of the information posted online contributes to effectiveness of decisions on public 
health (Paul and Drezde, 2011; Chee et al., 2009).  
Our interest concentrates on sequences of sentiments in the forum discourse. It has been shown that 
sentiments expressed by a forum participant affect sentiments in messages written by other partici-
pants posted on the same discussion thread (Zafarani et al., 2010). Shared online emotions can im-
prove personal well-being and empower patients in their battle against an illness (Malik and Coulson, 
2010).  We aimed to identify the most common sentiment pairs and triads and to observe their interac-
tions. We applied our analysis to data gathered from the In Vitro Fertilization (IVF) medical forum.1
Below is an example of four consecutive messages from an embryo transfer discussion: 
Alice: Jane - whats going on??
Jane: We have our appt. Wednesday!! EEE!!!   
Beth: Good luck on your transfer! Grow embies grow!!!!
Jane: The transfer went well - my RE did it himself which was comforting. 2 embies (grade 1 but slow in devel-
opment) so I am not holding my breath for a positive. This really was my worst cycle yet!!
In automated recognition of sentiments, we use HealthAffect, a domain-specific affective lexicon.
The paper is organized as follows: Section 2 presents related work in sentiment analysis, Section 3 
introduces the data set and the annotation results, Section 4 presents HealthAffect, Section 5 describes
the automated sentiment recognition experiments, and Section 6 discusses the results.
                                               
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1 http://ivf.ca/forums
44
2 Related Work
The availability of emotion-rich text has helped to promote studies of sentiments from a boutique sci-
ence into the mainstream of Text Data Mining (TDM). The ?sentiment analysis? query on Google 
Scholar returns about 16,800 hits in scholarly publications appearing since 2010.  Sentiment analysis 
often connects its subjects with specific online media (e.g., sentiments on consumer goods are studied 
on Amazon.com).  Health-related emotions are studied on Twitter (Chew and Eysenbach, 2010; 
Bobicev et al, 2012) and online public forums (Malik and Coulson, 2010; Goeuriot et al, 2012).   
Reliable annotation is essential for a thorough analysis of text. Multiple annotations of topic-
specific opinions in blogs were evaluated in Osman et al. (2010). Sokolova and Bobicev (2013) evalu-
ated annotation agreement achieved on messages gathered from a medical forum. Bobicev et al. 
(2012) used multiple annotators to categorize tweets into positive, negative and neutral tweets. Merits 
of reader-centric and author-centric annotation models were discussed in (Balahur, Steinberger, 2009). 
In this work, we apply the reader-centric annotation model. We use Fleiss Kappa (Nichols et al, 2010)
to evaluate inter-annotator agreement. 
     An accurate sentiment classification relies on electronic sources of semantic information.  In 
(Sokolova and Bobicev, 2013; Goeuriot et al, 2011), the authors showed that the sentiment categories
of SentiWordNet2, WordNetAffect3 and the Subjectivity lexicon4 are not fully representative of health-
related emotions. In the current work, we use HealthAffect, a domain-specific lexicon, to automatical-
ly classify sentiments.  The lexicon has been introduced in (Sokolova and Bobicev, 2013).
Although there is a correlation between emotions expressed in consecutive posts (Chmiel et al, 2011; 
Tan et al, 2011; Hassan et al, 2012),  so far health-related sentiment classification has focused on indi-
vidual messages. Our current work goes beyond individual messages and studies sequences of senti-
ments in consecutive posts.
3 The IVF Data and Annotation Results
We worked with online messages posted on a medical forum. The forum communication model pro-
motes messages which disclose the emotional state of the authors.  We gathered data from the In Vitro 
Fertilization (IVF) website dedicated to reproductive technologies, a hotly debated issue in the modern 
society. Among the IVF six sub-forums, we selected the IVF Ages 35+ sub-forum5  as it contained a 
manageable number of topics and messages, i.e., 510 topics and 16388 messages, where the messages 
had 128 words on average 6. All topics were initiated by the forum participants. Among those, 340 top-
ics contained < 10 posts. These short topics often contained one initial request and a couple of replies 
and were deemed too short to form a good discussion. We also excluded topics containing > 20 posts.  
This exclusion left 80 topics with an average of 17 messages per topic for a manual analysis by two 
annotators. First, we used 292 random posts to verify whether the messages were self-evident for sen-
timent annotation or required an additional context.  The annotators reported that posts were long 
enough to convey emotions and in most cases there was no need for a wider context. We applied an 
annotation scheme which was successfully applied in (Sokolova and Bobicev, 2013). 
We started with 35 sentiment types found by annotators and generalized them into three groups: 
? confusion, which included worry, concern, doubt, impatience, uncertainty, sadness, anger, 
embarrassment, hopelessness, dissatisfaction, and dislike;
? encouragement, which included cheering, support, hope, happiness, enthusiasm, excitement, 
optimism;
? gratitude, which included thankfulness.
A special group of sentiments was presented by expressions of compassion, sorrow, and pity. Ac-
cording to the WordNetAffect classification, these sentiments should be considered negative. However, 
                                               
2 http://sentiwordnet.isti.cnr.it/
3 http://wndomains.fbk.eu/wnaffect.html
4 http://mpqa.cs.pitt.edu/#subj_lexicon
5 http://ivf.ca/forums/forum/166-ivf-ages-35/
6 We harvested the data in July 2012.
45
in the context of health discussions, these emotional expressions appeared in conjunction with moral 
support and encouragement. Hence, we treated them as a part of encouragement.  Posts presenting on-
ly factual information were marked as facts. Some posts contained factual information and strong
emotional expressions; those expressions almost always conveyed encouragement (?hope, this helps?, 
?I wish you all the best?, ?good luck?). Such posts were labeled endorsement. Note that the final cate-
gories did not manifest negative sentiments. In lieu of negative sentiments, we considered confusion
as a non-positive label. Encouragement and gratitude were considered positive labels, facts and en-
dorsement - neutral. It should be mentioned that the posts were usually long enough to express several 
sentiments. However, annotators were requested to mark messages with one sentiment category.
The posts that both annotators labelled with the same label were assigned to this category; 1256 
posts were assigned with a class label.  The posts labelled with two different sentiment labels were 
marked as ambiguous; 182 posts were marked as ambiguous.
Despite the challenging data, we obtained Fleiss Kappa = 0.737 which indicated a strong agreement 
between annotators (Osman et al, 2010). This value was obtained on 80 annotated topics. Agreement 
for the randomly extracted posts was calculated separately in order to verify whether annotation of 
separate posts was no more difficult than annotation of the post sequences. Contrary to our expecta-
tions, the obtained Fleiss Kappa = 0.763 was slightly higher than on the posts in discussions.  The final 
distribution of posts among sentiment classes is presented in Table 2. 
Classification category Num of posts Per-cent
Facts 494 34.4%
Encouragement 333 23.2%
Endorsement 166 11.5%
Confusion 146 10.2%
Gratitude 131 9.1%
Ambiguous 168 11.7%
Total 1438 100%
Table 2:  Class distribution of the IVF posts.
We computed the distribution of sentiment pairs and triads in consecutive posts. We found that the 
most frequent sequences consisted mostly of facts and/or encouragement: 39.5% in total. Confusion
was far less frequent and was followed by facts and encouragement in 80% of cases.  That sentiment 
transition shows a high level of support among the forum participants.  Approximately 10% of senti-
ment pairs are factual and/or encouragement followed by gratitude. Other less frequent sequences ap-
pear when a new participant added her post in the flow. Tables 3 and 4 list the results.
Sentiment pairs Occurrence Percent
facts,   facts 170 19.5%
encouragement, encouragement 119 13.7%
facts, encouragement 55 6.3%
endorsement,  facts 53 6.1%
encouragement, facts 44 5.1%
Table 3:  The most frequent sequences of two sentiments and their occurrence in the data. 
Sentiment triads Occurrence Percent
factual,  factual,  factual 94 12.8%
encouragement, encouragement, encouragement 63 8.6%
encouragement, gratitude, encouragement 18 2.4%
factual,  endorsement,  factual 18 2.4%
confusion,  factual,  factual 17 2.3%
Table 4:  The most frequent triads of sentiments and their occurrences in the data. 
46
4 HealthAffect
General affective lexicons were shown to be ineffective in sentiment classification of health related 
messages.  To build a domain-specific lexicon, named HealthAffect, we adapted the Pointwise Mutual 
Information (PMI) approach (Turney, 2002). The initial candidates consisted of unigrams, bigrams 
and trigrams of words with frequency ? 5 appearing in unambiguously annotated posts (i.e., we omit-
ted posts marked as uncertain). For each class and each candidate, we calculated PMI(candidate, 
class) as 
PMI(candidate, class) = log2( p(candidate in class)/( p(candidate) p(class))).
Next, we calculated Semantic Orientation (SO) for each candidate and for each class as
SO (candidate, class) = PMI(candidate, class)
- ? PMI(candidate, other_classes)
where other_classes include all the classes except the class that Semantic Orientation is calculated for.
After all the possible SO were computed, each HealthAffect candidate was assigned with the class that 
corresponded to its maximum SO.   
Domain-specific lexicons can be prone to data over-fitting (since, for example, they might contain 
personal and brand names).  To avoid the over-fitting pitfall, we manually reviewed and filtered out 
non-relevant elements, such as personal and brand names, geolocations, dates, stop-words and their 
combinations (since_then, that_was_the, to_do_it, so_you). Table 5 presents the lexicon profile.   Note 
that we do not report the endorsement profile as it combines facts and encouragement. 
Class unigrams bigrams trigrams total Examples
Facts 204 254 78 536
round_of_ivf, 
hearbeat, 
a_protocol
Encourage-
ment 127 107 68 302
congratula-
tions,  
is_hard, 
only_have_one 
Confusion 63 143 34 240
crying, 
away_from,
any_of_you
Gratitude 37 51 34 122
appreciate, 
a_huge, 
thanks_for_your
Table 5: Statistics of the HealthAffect lexicon.
5 Sentiment Recognition
Our task was to assess HealthAffect?s ability to recognise sentiments of health-related messages. We 
used the sentiment categories described in Section 3.  In the experiments, we represented the messages 
by the HealthAffect terms. There were 1200 distinct terms, and each term was assigned to one senti-
ment. 
Our algorithm was straightforward: it calculated the number of HealthAffect terms from each cat-
egory in the post and classified the post in the category for which the maximal number of terms was 
found. Table 5 demonstrates that the number of terms was quite different for each category. Hence, the 
algorithm tended to attribute posts to the classes with a larger numbers of terms. To overcome the bias, 
we normalised the number of the terms in the post by the total number of terms for each category.   
The algorithm?s performance was evaluated through two multiclass classification results: 
47
? 4-class classification where all 1269 unambiguous posts are classified into (encouragement, 
gratitude, confusion, and neutral, i.e., facts and endorsement), and 
? 3-class classification (positive: encouragement, gratitude; negative: confusion, neutral: facts
and endorsement).
We computed micro- and macro-average Precision (Pr), Recall (R) and F-score (F) (Table 6).  
Metrics 4-class classification 3-class classification
microaverage F-score 0.633 0.672
macroaverage Precision 0.593 0.625
macroaverage Recall 0.686 0.679
macroaverage F-score 0.636 0.651
Table 6: Results of 4-class and 3-class classification.
For additional assessment of HealthAffect, we ran simple Machine Learning experiments using Na?ve 
Bayes and representing the texts through the lexicon terms. The obtained results of F-score=0.44, Pre-
cision=0.49, Recall=0.47 supported our decision to use HealthAffect in the straight-forward manner as 
presented above.  For each sentiment class, our results were as follows:
? The most accurate classification occurred for gratitude. It was correctly classified in 83.6% of 
its occurrences.  It was most commonly misclassified as encouragement (9.7%). Posts classi-
fied as gratitude are mostly the shortest ones containing only some words of gratitude and ap-
preciation of others? help. As they usually do not contain any more information than this, there 
were fewer chances for them to be misclassified. 
? The second most accurate result was achieved for encouragement. It was correctly classified 
in 76.7% of cases. It was misclassified as neutral (9.8%) because the latter posts contained 
some encouraging with the purpose of cheering up the interlocutor.
? The least often correctly classified class was neutral (50.8%). One possible explanation is the 
presence of the sentiment bearing words in the description of facts in a post which is in gen-
eral objective and which was marked as factual by the annotators.
Recall from Section 3, that we consider encouragement and gratitude to be positive sentiments and 
confusion to be a negative one. The reported results show that positive sentiments were most misclas-
sified within the same group or with neutral, e.g., encouragement was misclassified more as neutral or 
gratitude than as confusion, gratitude - more as encouragement or neutral than as confusion. On the 
other hand, confusion and negative sentiments were most often misclassified as neutral.
6 Discussion and Future Work
We have presented results of sentiment recognition in messages posted on a medical forum. Sentiment 
analysis of online medical discussions differs considerably from polarity studies of consumer-written 
product reviews, financial blogs and political discussions. While in many cases positive and negative 
sentiment categories are powerful enough, such a dichotomy is not sufficient for medical forums. We 
formulate our medical sentiment analysis as a multi-class classification problem in which posts were 
classified into encouragement, gratitude, confusion, facts and endorsement. 
In spite of sentiment annotation being highly subjective, we obtained a strong inter-annotator 
agreement between two independent annotators (i.e., Fleiss Kappa = 0.73 for posts in discussions and 
Fleiss Kappa = 0.76 for separate posts). The Kappa values demonstrated an adequate selection of clas-
ses of sentiments and appropriate annotation guidelines. However, many posts contained more than 
one sentiment in most cases mixed with some factual information. The possible solutions in this case 
would be (a) to allow multiple annotations for each post; (b) to annotate every sentence of the posts. 
48
A specific set of sentiments on the IVF forum did not support the use of general affective lexicons 
in automated sentiment recognition.  Instead we applied the PMI approach to build a domain-specific 
lexicon HealthAffect and then manually reviewed and generalized it.
In our current work we went beyond analysis of individual messages: we analyzed their sequences 
in order to reveal patterns of sentiment interaction. Manual analysis of a sample of data showed that 
topics contained a coherent discourse. Some unexpected shifts in the discourse flow were introduced 
by a new participant joining the discussion. In future work, we may include the post?s author infor-
mation in the sentiment interaction analysis. The information is also important for analysis of influ-
ence, when one participant is answering directly to another one citing in many cases the post which 
she answered to. 
We plan to use the results obtained in this study for analysis of discussions related to other highly 
debated health care policies.  One future possibility is to construct a Markov model for the sentiment 
sequences. However, in any online discussion there are random shifts and alternations in discourse 
which complicate application of the Markov model. 
In the future, we aim to annotate more text, enhance and refine HealthAffect, and use it to achieve 
reliable automated sentiment recognition across a spectrum of health-related issues. 
References
Balicco, L., C. Paganelli. 2011. Access to health information: going from professional to public practices, In-
formation Systems and Economic Intelligence: 4th International Conference - SIIE'2011.
Bobicev, V., M, Sokolova, Y. Jaffer, D. Schramm. 2012. Learning Sentiments from Tweets with Personal Health 
Information. Proceedings of Canadian AI 2012, p.p. 37?48, Springer.
Chee, B., R. Berlin, B. Schatz. 2009. Measuring Population Health Using Personal Health Messages. Proceed-
ings of AMIA Symposium, 92 - 96.
Chew, C. and G. Eysenbach. 2010. Pandemics in the Age of Twitter: Content Analysis of Tweets during the 2009 
H1N1 Outbreak. PLoS One, 5(11).
Chmiel, A., J. Sienkiewicz, M. Thelwall, G. Paltoglou, K. Buckley, A. Kappas, J. Holyst. 2011.  Collective Emo-
tions Online and Their Influence on Community Life. PLoS one.
Goeuriot, L., J. Na, W. Kyaing, C. Khoo,Y. Chang, Y. Theng and J. Kim. 2012. Sentiment lexicons for health-
related opinion mining. Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium, 
p.p. 219 ? 225, ACM.
Hassan, A., A. Abu-Jbara, D. Radev. 2012. Detecting subgroups in online discussions by modeling positive and 
negative relations among participants. In Proceedings of the 2012 Joint Conference on Empirical Methods in 
Natural Language Processing and Computational Natural Language Learning (pp. 59-70). 
Malik S. and N. Coulson. 2010. Coping with infertility online: an examination of self-help mechanisms in an 
online infertility support group. Patient Educ Couns, vol. 81, no. 2, pp. 315?318
Nichols, T., P. Wisner, G. Cripe, and L. Gulabchand. 2010. Putting the Kappa Statistic to Use. Qual Assur Jour-
nal, 13, p.p. 57-61.
Osman, D., J. Yearwood, P. Vamplew. 2010. Automated opinion detection: Implications of the level of agree-
ment between human raters. Information Processing and Management, 46, 331-342. 
Paul, M. and M. Dredze. 2011. You Are What You Tweet: Analyzing Twitter for Public Health. Proceedings of 
ICWSM.
Sokolova, M. and V. Bobicev. 2013.  What Sentiments Can Be Found in Medical Forums? Recent Advances in 
Natural Language Processing, 633-639
Tan, C., L. Lee , J. Tang , L. Jiang , M. Zhou, P. Li, 2011. User-level sentiment analysis incorporating social 
networks, Proceedings of the 17th ACM SIGKDD international conference on KDDM.
Turney, P.D. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of 
reviews. Proceedings of ACL'02, Philadelphia, Pennsylvania, pp. 417-424. 
Zafarani, R., W. Cole, and H. Liu. 2010. Sentiment Propagation in Social Networks: A Case Study in 
LiveJournal. Advances in Social Computing (SBP 2010), pp. 413?420, Springer.
49
