Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 439?447, Prague, June 2007. c?2007 Association for Computational Linguistics
Morphological Disambiguation of Hebrew:
A Case Study in Classifier Combination
Danny Shacham
Department of Computer Science
University of Haifa
Haifa, Israel
dannysh@gmail.com
Shuly Wintner
Department of Computer Science
University of Haifa
Haifa, Israel
shuly@cs.haifa.ac.il
Abstract
Morphological analysis and disambiguation
are crucial stages in a variety of natural
language processing applications, especially
when languages with complex morphology
are concerned. We present a system which
disambiguates the output of a morphologi-
cal analyzer for Hebrew. It consists of sev-
eral simple classifiers and a module which
combines them under linguistically moti-
vated constraints. We investigate a number
of techniques for combining the predictions
of the classifiers. Our best result, 91.44% ac-
curacy, reflects a 25% reduction in error rate
compared with the previous state of the art.
1 Introduction
Morphological analysis and disambiguation are cru-
cial pre-processing steps for a variety of natural lan-
guage processing applications, from search and in-
formation extraction to machine translation. For
languages with complex morphology these are non-
trivial processes. This paper presents a morphologi-
cal disambiguation module for Hebrew which uses
a sophisticated combination of classifiers to rank
the analyses produced by a morphological analyzer.
This work has a twofold contribution: first, our sys-
tem achieves over 91% accuracy on the full disam-
biguation task, reducing the error rate of the pre-
vious state of the art by 25%. More generally, we
explore several ways for combining the predictions
of simple classifiers under constraints; the insight
gained from these experiments will be useful for
other applications of machine learning to complex
(morphological and other) problems.
In the remainder of this section we discuss the
complexity of Hebrew morphology, the challenge
of morphological disambiguation and related work.
We describe our methodology in Section 2: we use
basic, na??ve classifiers (Section 3) to predict some
components of the analysis, and then combine them
in several ways (Section 4) to predict a consistent re-
sult. We analyze the errors of the system in Section 5
and conclude with suggestions for future work.
1.1 Linguistic background
Hebrew morphology is rich and complex.1 The ma-
jor word formation machinery is root-and-pattern,
and inflectional morphology is highly productive
and consists of prefixes, suffixes and circumfixes.
Nouns, adjectives and numerals inflect for number
(singular, plural and, in rare cases, also dual) and
gender (masculine or feminine). In addition, all
these three types of nominals have two phonologi-
cally and morphologically distinct forms, known as
the absolute and construct states. In the standard
orthography approximately half of the nominals ap-
pear to have identical forms in both states, a fact
which substantially increases the ambiguity. In ad-
dition, nominals take possessive pronominal suffixes
which inflect for number, gender and person.
Verbs inflect for number, gender and person (first,
second and third) and also for a combination of tense
and aspect/mood, referred to simply as ?tense? be-
low. Verbs can also take pronominal suffixes, which
are interpreted as direct objects, and in some cases
can also take nominative pronominal suffixes. A pe-
culiarity of Hebrew verbs is that the participle form
1To facilitate readability we use a straight-forward translit-
eration of Hebrew using ASCII characters, where the characters
(in Hebrew alphabetic order) are: abgdhwzxviklmnsypcqr$t.
439
can be used as present tense, but also as a noun or an
adjective.
These matters are complicated further due to two
sources: first, the standard Hebrew orthography
leaves most of the vowels unspecified. On top of
that, the script dictates that many particles, includ-
ing four of the most frequent prepositions, the def-
inite article, the coordinating conjunction and some
subordinating conjunctions, all attach to the words
which immediately follow them. When the definite
article h is prefixed by one of the prepositions b, k
or l, it is assimilated with the preposition and the
resulting form becomes ambiguous as to whether
or not it is definite. For example, bth can be read
either as b+th ?in tea? or as b+h+th ?in the tea?.
Thus, the form $bth can be read as an inflected stem
(the verb ?capture?, third person singular feminine
past), as $+bth ?that+field?, $+b+th ?that+in+tea?,
$+b+h+th ?that in the tea?, $bt+h ?her sitting? or
even as $+bt+h ?that her daughter?.
An added complexity stems from the fact that
there are two main standards for the Hebrew script:
one in which vocalization diacritics, known as
niqqud ?dots?, decorate the words, and another in
which the dots are missing, and other characters rep-
resent some, but not all of the vowels. Most of the
texts in Hebrew are of the latter kind; unfortunately,
different authors use different conventions for the
undotted script. Thus, the same word can be writ-
ten in more than one way, sometimes even within
the same document. This fact adds significantly to
the degree of ambiguity.
Our departure point in this work is HAMSAH
(Yona and Wintner, 2007), a wide coverage, lin-
guistically motivated morphological analyzer of He-
brew, which was recently re-implemented in Java
and made available from the Knowledge Cen-
ter for Processing Hebrew (http://mila.cs.
technion.ac.il/). The output that HAMSAH
produces for the form $bth is illustrated in Table 1.
In general, it includes the part of speech (POS)
as well as sub-category, where applicable, along
with several POS-dependent features such as num-
ber, gender, tense, nominal state, definitness, etc.
1.2 The challenge of disambiguation
Identifying the correct morphological analysis of a
given word in a given context is an important and
non-trivial task. Unlike POS tagging, the task does
not involve assigning an analysis to words which the
analyzer does not recognize. However, selecting an
analysis immediately induces a POS tagging for the
target word (by projecting the analysis on the POS
coordinate). Our main contribution in this work is a
system that solves this problem with high accuracy.
Compared with POS tagging of English, morpho-
logical disambiguation of Hebrew is a much more
complex endeavor due to the following factors:
Segmentation A single token in Hebrew can ac-
tually be a sequence of more than one lexi-
cal item. For example, analysis 4 of Table 1
($+b+h+th ?that+in+the+tea?) corresponds to
the tag sequence IN+IN+DT+NN.
Large tagset The number of different tags in a lan-
guage such as Hebrew (where the POS, mor-
phological features and prefix and suffix parti-
cles are considered) is huge. HAMSAH pro-
duces 22 different parts of speech, some with
subcategories; 6 values for the number feature
(including disjunctions of values), 4 for gender,
5 for person, 7 for tense and 3 for nominal state.
Possessive pronominal suffixes can have 15 dif-
ferent values, and prefix particle sequences can
theoretically have hundreds of different forms.
While not all the combinations of these values
are possible, we estimate the number of possi-
ble analyses to be in the thousands.
Ambiguity Hebrew is highly ambiguous: HAM-
SAH outputs on average approximately 2.64
analyses per word token. Oftentimes two or
more alternative analyses share the same part
of speech, and in some cases two or more anal-
yses are completely identical, except for their
lexeme (see analyses 7 and 8 in Table 1). Mor-
phological disambiguation of Hebrew is hence
closer to the problem of word sense disam-
biguation than to standard POS tagging.
Anchors, which are often function words, are al-
most always morphologically ambiguous in
Hebrew. These include most of the high-
frequency forms. Many of the function words
which help boost the performance of English
POS tagging are actually prefix particles which
add to the ambiguity in Hebrew.
440
# Lexical ID lexeme POS Num Gen Per Ten Stat Def Pref Suf
1 17280 $bt noun sing fem N/A N/A abs no h
2 1379 bt noun sing fem N/A N/A abs no $ h
3 19130 bth noun sing fem N/A N/A abs no $
4 19804 th noun sing masc N/A N/A abs yes $+b+h
5 19804 th noun sing masc N/A N/A abs no $+b
6 19804 th noun sing masc N/A N/A cons no $+b
7 1541 $bh verb sing fem 3 past N/A N/A
8 9430 $bt verb sing fem 3 past N/A N/A
Table 1: The analyses of the form $bth
Word order in Hebrew is freer than in English.
1.3 Related work
The idea of using short context for morphological
disambiguation dates back to Choueka and Lusig-
nan (1985). Levinger et al (1995) were the first
to apply it to Hebrew, but their work was ham-
pered by the lack of annotated corpora for training
and evaluation. The first work which uses stochas-
tic contextual information for morphological disam-
biguation in Hebrew is Segal (1999): texts are an-
alyzed using the morphological analyzer of Segal
(1997); then, each word in a text is assigned its
most likely analysis, defined by probabilities com-
puted from a small tagged corpus. In the next phase
the system corrects its own decisions by using short
context (one word to the left and one to the right
of the target word). The corrections are also au-
tomatically learned from the tagged corpus (using
transformation-based learning). In the last phase,
the analysis is corrected by the results of a syntac-
tic analysis of the sentence. The reported results
are excellent: 96.2% accuracy. More reliable tests,
however, reveal accuracy of 85.5% only (Lember-
ski, 2003, page 85). Furthermore, the performance
of the program is unacceptable (the reported running
time on ?two papers? is thirty minutes).
Bar-Haim et al (2005) use Hidden Markov Mod-
els (HMMs) to implement a segmenter and a tag-
ger for Hebrew. The main innovation of this work is
that it models word-segments (morphemes: prefixes,
stem and suffixes), rather than full words. The accu-
racy of this system is 90.51% for POS tagging (a
tagset of 21 POS tags is used) and 96.74% for seg-
mentation (which is defined as identifying all pre-
fixes, including a possibly assimilated definite arti-
cle). As noted above, POS tagging does not amount
to full morphological disambiguation.
Recently, Adler and Elhadad (2006) presented an
unsupervised, HMM-based model for Hebrew mor-
phological disambiguation, using a morphological
analyzer as the only resource. A morpheme-based
model learns both segmentation and tagging in par-
allel from a large (6M words) un-annotated corpus.
Reported results are 92.32% for POS tagging and
88.5% for full morphological disambiguation. We
refer to this result as the state of the art and use the
same data for evaluation.
A supervised approach to morphological disam-
biguation of Arabic is given by Habash and Rambow
(2005), who use two corpora of 120K words each
to train several classifiers. Each morphological fea-
ture is predicted separately and then combined into a
full disambiguation result. The accuracy of the dis-
ambiguator is 94.8%-96.2% (depending on the test
corpus). Note, however, the high baseline of each
classifier (96.6%-99.9%, depending on the classi-
fier) and the full disambiguation task (87.3%-92.1%,
depending on the corpus). We use a very similar ap-
proach below, but we experiment with more sophis-
ticated methods for combining simple classifiers to
induce a coherent prediction.
2 Methodology
For training and evaluation, we use a corpus of
approximately 90,000 word tokens, consisting of
newspaper texts, which was automatically analyzed
using HAMSAH and then manually annotated (El-
hadad et al, 2005). Annotation consists simply of
selecting the correct analysis produced by the an-
alyzer, or an indication that no such analysis ex-
441
ists. When the analyzer does not produce the cor-
rect analysis, it is added manually. This is the exact
setup of the experiments reported by Adler and El-
hadad (2006).
Table 2 lists some statistics of the corpus, and a
histogram of analyses is given in Table 3. Table 4
lists the distribution of POS in the corpus.
Tokens 89347
Types 23947
Tokens with no correct analysis 8218
Tokens with no analysis 130
Degree of ambiguity 2.64
Table 2: Statistics of training corpus
# analyses # tokens # analyses # tokens
1 38468 7 1977
2 15480 8 1309
3 11194 9 785
4 9934 10 622
5 5341 11 238
6 3472 >12 397
Table 3: Histogram of analyses
In all the experiments described in this paper we
use SNoW (Roth, 1998) as the learning environ-
ment, with winnow as the update rule (using per-
ceptron yielded very similar results). SNoW is a
multi-class classifier that is specifically tailored for
learning in domains in which the potential number
of information sources (features) taking part in de-
cisions is very large, of which NLP is a principal
example. It works by learning a sparse network of
linear functions over the feature space. SNoW has
already been used successfully as the learning vehi-
cle in a large collection of natural language related
tasks and compared favorably with other classifiers
(Punyakanok and Roth, 2001; Florian, 2002). Typi-
cally, SNoW is used as a classifier, and predicts us-
ing a winner-take-all mechanism over the activation
values of the target classes. However, in addition to
the prediction, it provides a reliable confidence level
in the prediction, which enables its use in an infer-
ence algorithm that combines predictors to produce
a coherent inference.
Following Daya et al (2004) and Habash and
POS # tokens % tokens
Noun 25836 28.92
Punctuation 13793 15.44
Proper Noun 7238 8.10
Verb 7192 8.05
Preposition 7164 8.02
Adjective 5855 6.55
Participle 3213 3.60
Pronoun 2688 3.01
Adverb 2226 2.49
Conjunction 2021 2.26
Numeral 1972 2.21
Quantifier 951 1.06
Negation 848 0.95
Interrogative 80 0.09
Prefix 29 0.03
Interjection 12 0.01
Foreign 6 0.01
Modal 5 0.01
Table 4: POS frequencies
Rambow (2005), we approach the problem of mor-
phological disambiguation as a complex classifica-
tion task. We train a classifier for each of the at-
tributes that can contribute to the disambiguation
of the analyses produced by HAMSAH (e.g., POS,
tense, state). Each classifier predicts a small set of
possible values and hence can be highly accurate.
In particular, the basic classifiers do not suffer from
problems of data sparseness. Of course, each sim-
ple classifier cannot fully disambiguate the output
of HAMSAH, but it does induce a ranking on the
analyses (see Table 6 below for the level of ambigu-
ity which remains after each simple classifier is ap-
plied). Then, we combine the outcomes of the sim-
ple classifiers to produce a consistent ranking which
induces a linear order on the analyses.
For evaluation we consider only the words that
have at least one correct analysis in the annotated
corpus. Accuracy is defined as the ratio between the
number of words classified correctly and the total
number of words in the test corpus that have a cor-
rect analysis. The remaining level of ambiguity is
defined as the average number of analyses per word
whose score is equal to the score of the top ranked
analysis. This is greater than 1 only for the simple
442
classifiers, where more than one analysis can have
the same tag. In all the experiments we perform 10-
fold cross-validation runs and report the average of
the 10 runs, both on the entire corpus and on a subset
of the corpus in which we only test on words which
do not occur in the training corpus.
The baseline tag of the token wi is the most
prominent tag of all the occurrences of wi in the
corpus. The baseline for the combination is the most
prominent analysis of all the occurrences ofwi in the
corpus. If wi does not occur in the corpus, we back
off and select the most prominent tag in the corpus
independently of the word wi. For the combination
baseline, we select the analysis of the most promi-
nent lexical ID, chosen from the list of all possible
lexical IDs of wi. If there is more than one possible
value, one top-ranking value is chosen at random.
3 Basic Classifiers
The simple classifiers are all built in the same way.
They are trained on feature vectors that are gener-
ated from the output of the morphological analyzer,
and tested on a clean output of the same analyzer.
We defined several classifiers for the attributes of
the morphological analyses. Since some attributes
do not apply to all the analyses, we add a value of
?N/A? for the inapplicable attributes. An annotated
corpus was needed in all those classifiers for train-
ing. We list the basic classifiers below.
POS 22 values (only 18 in our corpus), see Table 4.
Gender ?Masculine?, ?Feminine?, ?Masculine and
feminine?, ?N/A?.
Number ?Singular?, ?Plural?, ?Dual?, ?N/A?.
Person ?First?, ?Second?, ?Third?, ?N/A?.
Tense ?Past?, ?Present?, ?Participle?, ?Future?, ?Im-
perative?, ?Infinitive?, ?Bare Infinitive?, ?N/A?.
Definite Article ?Def?, ?indef?, ?N/A?. Identifies
also implicit (assimilated) definiteness.
Status ?Absolute?, ?Construct? and ?N/A?.
Segmentation Predicts the number of letters which
are prefix particles. Possible values are [0-6], 6
being the length of longest possible prefix se-
quence. Does not identify implicit definiteness.
Has properties A binary classifier which distin-
guishes between atomic POS categories (e.g.,
conjunction or negation) and categories whose
words have attributes (such as nouns or verbs).
Each word in the training corpus induces features
that are generated for itself and its immediate neigh-
bors, using the output of the morphological ana-
lyzer. For each word in the window, we generate
the following features: POS, number, gender, per-
son, tense, state, definiteness, prefixes (where each
possible prefix is a binary feature), suffix (binary: is
there word suffixed?), number/gender/person of suf-
fix, surface form, lemma, conjunction of the surface
form and the POS, conjunction of the POS and the
POS of prefixes and suffixes, and some disjunctions
of POS. The total number of features for each exam-
ple is huge (millions), but feature vectors are very
sparse.
The simple classifiers can be configured in several
ways. First, the size of the window around the target
word had to be determined, and we experimented
with several sizes, up to ?3 words. Another issue
is feature generation. It is straight-forward during
training, but during evaluation and testing the fea-
ture extractor is presented only with the set of anal-
yses produced by HAMSAH for each word, and has
no access to the correct analysis. We experimented
with two methods for tackling this problem: produce
the union of all possible values for each feature; or
select a single analysis, the baseline one, for each
word, and generate only the features induced by this
analysis. While this problem is manifested only dur-
ing testing, it impacts also the training procedure,
and so we experimented with feature generation at
training using the correct analysis, the union of the
analyses or the baseline analysis. The results of the
experiments for the POS classifier are shown in Ta-
ble 5. The best configuration uses a window of two
words before and one word after the target word. For
both testing and training we generate features using
the baseline analysis.
With this setup, the accuracy of all the classifiers
is shown in Table 6. We report results on two tasks:
the entire test corpus; and words in the test corpus
which do not occur in the training corpus, a much
harder task. We list the accuracy, remaining level
of ambiguity and reduction in error rate ERR, com-
443
Training Testing 1 - 2 2 - 1 2 - 2 1 - 3 3 - 1 2 - 3 3 - 2 3 - 3
correct baseline 91.37 91.53 91.69 91.55 91.69 91.83 91.75 92.01
correct all 79.15 79.55 80.53 80.07 80.13 80.75 81.00 82.07
all baseline 93.41 93.38 93.22 93.42 93.53 93.59 93.51 93.61
all all 93.37 93.42 93.28 93.2 93.61 93.05 93.48 93.15
baseline baseline 94.93 94.97 94.8 94.86 94.84 94.72 94.67 94.61
baseline all 84.48 84.78 84.82 85.65 84.97 85.13 85.03 85.45
Table 5: Architectural configurations of the POS classifier: columns reflect the window size, rows refer to
training and testing feature generation
All words Unseen words
baseline classifier baseline classifier
accuracy accuracy ambiguity ERR accuracy accuracy ERR
POS 93.01 94.97 1.46 28.04 84.67 88.65 25.96
Gender 96.34 96.74 1.86 10.93 92.15 94.38 28.41
Number 96.79 97.92 1.91 35.20 92.35 95.91 46.54
Person 98.14 98.62 2.25 25.81 94.04 96.50 41.28
Tense 98.40 98.69 2.21 18.12 94.80 96.37 30.19
Definite Article 93.90 95.76 1.83 30.49 85.38 91.77 43.71
Status 92.73 95.06 1.57 32.05 84.46 89.85 34.68
Segmentation 99.12 97.80 2.25 ? 97.67 97.66 ?
Has properties 97.63 98.11 2.26 20.25 95.91 95.97 1.47
Table 6: Accuracy of the simple classifiers: ERR is reduction in error rate, compared with the baseline
pared with the baseline.
4 Combination of Classifiers
Given a set of simple classifiers, we now investi-
gate various ways for combining their predictions.
These predictions may be contradicting (for exam-
ple, the POS classifier can predict ?noun? while the
tense classifier predicts ?past?), and we use the con-
straints imposed by the morphological analyzer to
enforce a consistent analysis.
First, we define a na??ve combination along the
lines of Habash and Rambow (2005). The scores
assigned by the simple classifiers (except segmenta-
tion, for which we use the baseline) to each analysis
are accumulated, and the score of the complete anal-
ysis is their sum (experiments with different weights
to the various classifiers proved futile). Even after
the combination, the remaining level of ambiguity
is 1.05; in ambiguous cases back off to the baseline
analysis, and then choose at random one of the top-
ranking analyses. The result of the combination is
shown in Table 7.
baseline classifier ERR
All words 86.11 90.26 29.88
Unseen words 67.53 78.52 33.85
Table 7: Results of the na??ve combination
Next, we define a hierarchical combination in
which we try to incorporate more linguistic knowl-
edge pertaining to the dependencies between the
classifiers. As a pre-processing step we classify the
target word to one of two groups, using the has prop-
erties classifier. Then, we predict the main POS of
the target word, and take this prediction to be true;
we then apply only the subset of the other classifiers
that are relevant to the main POS.
The results of the hierarchical combination are
shown in Table 8. As can be seen, the hierarchical
combination performs worse than the na??ve one. We
conjecture that this is because the hierarchical com-
bination does not fully disambiguate, and a random
top-ranking analysis is chosen more often than in the
case of the na??ve combination.
444
na??ve hierarchical ERR
All words 90.26 89.61 ?
Unseen words 78.52 78.08 ?
Table 8: Results of the hierarchial combination
The combination of independent classifiers un-
der the constraints imposed by the possible mor-
phological analyses is intended to capture context-
dependent constraints on possible sequences of anal-
yses. Such constraints are stochastic in nature, but
linguistic theory tells us that several hard (determin-
istic) constraints also exist which rule out certain se-
quences of otherwise possible analyses. We now ex-
plore the utility of implementing such constraints to
filter out linguistically impossible sequences.
Using several linguistic sources, we defined a set
of constraints, each of which is a linguistically im-
possible sequence of analyses (all sequences are of
length 2, although in principle longer ones could
have been defined). We then checked the annotated
corpus for violations of these constraints; we used
the corpus to either verify the correctness of a con-
straint or further refine it (or abandon it altogether,
in some cases). We then re-iterated the process with
the new set of constraints.
The result was a small set of six constraints which
are not violated in our annotated corpus. We used
the constraints to rule out some of the paths de-
fined by the possible outcomes of the morphologi-
cal analyzer on a sequence of words. Each of the
constraints below contributes a non-zero reduction
in the error rate of the disambiguation module.The
(slightly simplified) constraints are:
1. A verb in any tense but present cannot be fol-
lowed by the genitive preposition ?$l? (of).
2. A preposition with no attached pronomial suf-
fix must be followed by a nominal phrase. This
rule is relaxed for some prepositions which can
be followed by the prefix ?$?.
3. The preposition ?at? must be followed by a def-
inite nominal phrase.
4. Construct-state words must be followed by a
nominal phrase.
5. A sequence of two verbs is only allowed if: one
of them is the verb ?hih? (be); one of them has
a prefix; the second is infinitival; or the first is
imperative and the second is in future tense.
6. A non-numeral quantifier must be followed by
either a nominal phrase or a punctuation.
Imposing the linguistically motivated constraints
on the classifier combination improved the results to
some extent, as depicted in Table 9. The best results
are obtained when the constraints are applied to the
hierarchical combination.
5 Error analysis
We conducted extensive error analysis of both the
simple classifiers and the combination module. The
analysis was performed over one fold of the anno-
tated corpus (8933 tokens). Table 10 depicts, for
some classifiers, a subset of the confusion matrix:
it lists the correct tag, the chosen, or predicted, tag,
the number of occurrences of the specific error and
the total number of errors made by the classifier.
classifier correct chosen # total
has props yes no 110 167
no yes 57
segmentation 1 0 160 176
state const abs 154 412
definiteness def indef 98 300
Table 10: Simple classifiers, confusion matrix
Several patterns can be observed in Table 10. The
?has properties? classifier is biased towards predict-
ing ?yes? instead of ?no?. The ?segmentation? clas-
sifier, which predicts the length of the prefix, also
displays a clear bias. In almost 90% of its errors it
predicts no prefix instead of a prefix of length one.
?Status? and ?definiteness? are among the weakest
classifiers, biased towards the default.
Other classifiers make more sporadic types of er-
rors. Of particular interest is the POS classifier.
Here, when adjectives are mis-predicted, they are
predicted as nouns. This can be explained by the
morphological similarity of the two categories, and
in particular by the similar syntactic contexts in
which they occur. Similarly, almost 90% of mis-
predicted verbs are predicted to be either nouns
445
na??ve na??ve + consts ERR hier. hier. + cons ERR
All words 90.26 90.90 6.57 89.61 91.44 17.61
Unseen words 78.52 79.56 4.84 78.08 81.74 16.70
Table 9: Accuracy results of various combination architectures. ERR is reduction in error rate due to the
hard constraints. The best results are obtained using the hierarchical combination with hard constraints.
or adjectives, probably resulting from present-tense
verbs in the training corpus which, in Hebrew, have
similar distribution to nouns and adjectives.
The analysis of errors in the combination is more
interesting. On the entire corpus, the disambigua-
tor makes 7927 errors. Of those, 1476 (19%) are
errors in which the correct analysis differs from the
chosen one only in the value of the ?state? feature.
Furthermore, in 1341 of the errors (17%) the system
picks the correct analysis up to the value of ?definite-
ness?; of those, 1275 (16% of the errors) are words
in which the definite article is assimilated in a prepo-
sition. In sum, many of the errors seem to be in the
real tough cases.
6 Conclusions
Morphological disambiguation of Hebrew is a dif-
ficult task which involves, in theory, thousands of
possible tags. We reconfirm the results of Daya
et al (2004) and Habash and Rambow (2005),
which show that decoupling complex morphologi-
cal tasks into several simple tasks improves the ac-
curacy of classification. Our best result, 91.44%
accuracy, reflects a reduction of 25% in error rate
compared to the previous state of the art (Adler
and Elhadad, 2006), and almost 40% compared
to the baseline. We also show that imposing
few context-dependent constraints on possible se-
quences of analyses improves the accuracy of the
disambiguation. The disambiguation module will
be made available through the Knowledge Cen-
ter for Processing Hebrew (http://mila.cs.
technion.ac.il/).
We believe that these results can be further im-
proved in various ways. The basic classifiers can
benefit from more detailed feature engineering and
careful tuning of the parameters of the learning en-
vironment. There are various ways in which inter-
related classifiers can be combined; we only ex-
plored three here. Using other techniques, such as
inference-based training, in which the feature gen-
eration for training is done step by step, using infor-
mation inferred in the previous step, is likely to yield
better accuracy. We also believe that further linguis-
tic exploration, based on deeper error analysis, will
result in more hard constraints which can reduce the
error rate of the combination module. Finally, we
are puzzled by the differences between Hebrew and
Arabic (for which the baseline and the current state
of the art are significantly higher) on this task. We
intend to investigate the linguistic sources for this
puzzle in the future.
Acknowledgements
We are extremely grateful to Dan Roth for his con-
tinuing support and advise; to Meni Adler for pro-
viding the annotated corpus; to Dalia Bojan and
Alon Itai for the implementation of the morpholog-
ical analyzer; to Yariv Louck for the implementa-
tion of the deterministic constraints; to Nurit Melnik
for help with error analysis; and to Yuval Nardi his
help with statistical analysis. Thanks are due to Ido
Dagan, Alon Lavie and Michael Elhadad for useful
comments and advise. This research was supported
by THE ISRAEL SCIENCE FOUNDATION (grant
No. 137/06); by the Israel Internet Association; by
the Knowledge Center for Processing Hebrew; and
by the Caesarea Rothschild Institute for Interdisci-
plinary Application of Computer Science at the Uni-
versity of Haifa.
References
Meni Adler and Michael Elhadad. 2006. An unsuper-
vised morpheme-based hmm for hebrew morpholog-
ical disambiguation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 665?672, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Roy Bar-Haim, Khalil Sima?an, and Yoad Winter. 2005.
Choosing an optimal architecture for segmentation and
446
POS-tagging of Modern Hebrew. In Proceedings of
the ACL Workshop on Computational Approaches to
Semitic Languages, pages 39?46, Ann Arbor, Michi-
gan, June. Association for Computational Linguistics.
Yaacov Choueka and Serge Lusignan. 1985. Disam-
biguation by short context. Computers and the Hu-
manities, 19:147?157.
Ezra Daya, Dan Roth, and Shuly Wintner. 2004. Learn-
ing Hebrew roots: Machine learning with linguistic
constraints. In Proceedings of EMNLP?04, pages 357?
364, Barcelona, Spain, July.
Michael Elhadad, Yael Netzer, David Gabay, and Meni
Adler. 2005. Hebrew morphological tagging guide-
lines. Technical report, Department of Computer Sci-
ence, Ben Gurion University.
Radu Florian. 2002. Named entity recognition as a
house of cards: Classifier stacking. In Proceedings
of CoNLL-2002, pages 175?178. Taiwan.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 573?580, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
Gennadiy Lemberski. 2003. Named entity recognition
in Hebrew. Master?s thesis, Department of Computer
Science, Ben Gurion University, Beer Sheva, Israel,
March. In Hebrew.
Moshe Levinger, Uzzi Ornan, and Alon Itai. 1995.
Learning morpho-lexical probabilities from an un-
tagged corpus with an application to Hebrew. Com-
putational Linguistics, 21(3):383?404, September.
Vasin Punyakanok and Dan Roth. 2001. The use of clas-
sifiers in sequential inference. In NIPS-13; The 2000
Conference on Advances in Neural Information Pro-
cessing Systems 13, pages 995?1001. MIT Press.
Dan Roth. 1998. Learning to resolve natural language
ambiguities: A unified approach. In Proceedings of
AAAI-98 and IAAI-98, pages 806?813, Madison, Wis-
consin.
Erel Segal. 1997. Morphological analyzer for unvocal-
ized Hebrew words. Unpublished work.
Erel Segal. 1999. Hebrew morphological analyzer for
Hebrew undotted texts. Master?s thesis, Technion, Is-
rael Institute of Technology, Haifa, October. In He-
brew.
Shlomo Yona and Shuly Wintner. 2007. A finite-state
morphological grammar of Hebrew. Natural Lan-
guage Engineering. To appear.
447
Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 65?72,
Prague, 28 June 2007. c?2007 Association for Computational Linguistics
Cross Lingual and Semantic Retrieval for Cultural Heritage Appreciation
Idan Szpektor, Ido Dagan
Dept. of Computer Science
Bar Ilan University
szpekti@cs.biu.ac.il
Alon Lavie
Language Technologies Inst.
Carnegie Mellon University
alavie+@cs.cmu.edu
Danny Shacham, Shuly Wintner
Dept. of Computer Science
University of Haifa
shuly@cs.haifa.ac.il
Abstract
We describe a system which enhances the
experience of museum visits by providing
users with language-technology-based in-
formation retrieval capabilities. The sys-
tem consists of a cross-lingual search en-
gine, augmented by state of the art semantic
expansion technology, specifically designed
for the domain of the museum (history and
archaeology of Israel). We discuss the tech-
nology incorporated in the system, its adap-
tation to the specific domain and its contri-
bution to cultural heritage appreciation.
1 Introduction
Museum visits are enriching experiences: they pro-
vide stimulation to the senses, and through them to
the mind. But the experience does not have to end
when the visit ends: further exploration of the ar-
tifacts and their influence on the visitor is possible
after the visit, either on location or elsewhere. One
common means of exploration is Information Re-
trieval (IR) via a Search Engine. For example, a mu-
seum could implement a search engine over a col-
lection of documents relating to the topics exhibited
in the museum.
However, such document collections are usually
much smaller than general collections, in particular
the World Wide Web. Consequently, phenomena in-
herent to natural languages may severely hamper the
performance of human language technology when
applied to small collections. One such phenomenon
is the semantic variability of natural languages, the
ability to express a specific meaning in many dif-
ferent ways. For example, the expression ?Archae-
ologists found a new tomb? can be expressed also
by ?Archaeologists discovered a tomb? or ?A sar-
cophagus was dug up by Egyptian Researchers?. On
top of monolingual variability, the same information
can also be expressed in different languages. Ignor-
ing natural language variability may result in lower
recall of relevant documents for a given query, espe-
cially in small document collections.
This paper describes a system that attempts to
cope with semantic variability through the use of
state of the art human language technology. The
system provides both semantic expansion and cross
lingual IR (and presentation of information) in the
domain of archaeology and history of Israel. It
was specifically developed for the Hecht Museum
in Haifa, Israel, which contains a small but unique
collection of artifacts in this domain. The system
provides different users with different capabilities,
bridging over language divides; it addresses seman-
tic variation in novel ways; and it thereby comple-
ments the visit to the museum with long-lasting in-
stillation of information.
The main component of the system is a domain-
specific search engine that enables users to specify
queries and retrieve information pertaining to the do-
main of the museum. The engine is enriched by lin-
guistic capabilities which embody an array of means
for addressing semantic variation. Queries are ex-
panded using two main techniques: semantic expan-
sion based on textual entailment; and cross-lingual
expansion based on translation of Hebrew queries
to English and vice versa. Retrieved documents are
presented as links with associated snippets; the sys-
tem also translates snippets from Hebrew to English.
The main contribution of this work is, of course,
the system itself, which was recently demonstrated
65
successfully at the museum and which we believe
could be useful to a variety of museum visitor types,
from children to experts. For example, the system
provides Hebrew speakers access to English doc-
uments pertaining to the domain of the museum,
and vice versa, thereby expanding the availability
of multilingual material to museum visitors. More
generally, it is an instance of adaptation of state of
the art human language technology to the domain
of cultural heritage appreciation, demonstrating how
general resources and tools are adapted to a specific
domain, thereby improving their accuracy and us-
ability. Finally, it provides a test-bed for evaluating
the contribution of language technology in general,
as well as specific components and resources, to a
large-scale natural language processing system.
2 Background and Motivation
Internet search is hampered by the complexity of
natural languages. The two main characteristics of
this complexity are ambiguity and variability: the
former refers to the fact that a given text can be
interpreted in more than one way; the latter indi-
cates that the same meaning can be linguistically ex-
pressed in several ways. The two phenomena make
simple search techniques too weak for unsophisti-
cated users, as existing search engines perform only
direct keyword matching, with very limited linguis-
tic processing of the texts they retrieve.
Specifically, IR systems that do not address the
variability in languages may suffer from lower re-
call, especially in restricted domains and small doc-
ument locations. We next describe two prominent
types of variability that we think should be ad-
dressed in IR systems.
2.1 Textual Entailment and Entailment Rules
In many NLP applications, such as Question An-
swering (QA), Information Extraction (IE) and In-
formation Retrieval (IR), it is crucial to recognize
that a specific target meaning can be inferred from
different text variants. For example, a QA system
needs to induce that ?Mendelssohn wrote inciden-
tal music? can be inferred from ?Mendelssohn com-
posed incidental music? in order to answer the ques-
tion ?Who wrote incidental music??. This type of
reasoning has been identified as a core semantic in-
ference task by the generic textual entailment frame-
work (Dagan et al, 2006; Bar-Haim et al, 2006).
The typical way to address variability in IR is to
use lexical query expansion (Lytinen et al, 2000;
Zukerman and Raskutti, 2002). However, there are
variability patterns that cannot be described using
just constant phrase to phrase entailment. Another
important type of knowledge representation is en-
tailment rules and paraphrases. An entailment rule
is a directional relation between two templates, text
patterns with variables, e.g., ?X compose Y ?
X write Y ?. The left hand side is assumed to en-
tail the right hand side in certain contexts, under
the same variable instantiation. Paraphrases can be
viewed as bidirectional entailment rules. Such rules
capture basic inferences in the language, and are
used as building blocks for more complex entail-
ment inference. For example, given the above en-
tailment rule, a QA system can identify the answer
?Mendelssohn? in the above example. This need
sparked intensive research on automatic acquisition
of paraphrase and entailment rules.
Although knowledge-bases of entailment-rules
and paraphrases learned by acquisition algorithms
were used in other NLP applications, such as QA
(Lin and Pantel, 2001; Ravichandran and Hovy,
2002) and IE (Sudo et al, 2003; Romano et al,
2006), to the best of our knowledge the output of
such algorithms was never applied to IR before.
2.2 Cross Lingual Information Retrieval
The difficulties caused by variability are amplified
when the user is not a native speaker of the language
in which the retrieved texts are written. For exam-
ple, while most Israelis can read English documents,
fewer are comfortable with the specification of Eng-
lish queries. In a museum setting, some visitors may
be able to read Hebrew documents but still be rel-
atively poor at searching for them. Other visitors
may be unable to read Hebrew texts, but still benefit
from non-textual information that are contained in
Hebrew documents (e.g., pictures, maps, audio and
video files, external links, etc.)
This problem is addressed by the paradigm of
Cross-Lingual Information Retrieval (CLIR). This
paradigm has become a very active research area
in recent years, addressing the needs of multilingual
and non-English speaking communities, such as the
66
European Union, East-Asian nations and Spanish
speaking communities in the US (Hull and Grefen-
stette, 1996; Ballesteros and Croft, 1997; Carbonell
et al, 1997). The common approach for CLIR is
to translate a query in a source language to another
target language and then issue the translated query
to retrieve target language documents. As explained
above, CLIR research has to address various generic
problems caused by the variability and ambiguity of
natural languages, as well as specific problems re-
lated to the particular languages being addressed.
3 Coping with Semantic Variability in IR
We describe a search engine that is capable of per-
forming: (a) semantic English information retrieval;
and (b) cross-lingual (Hebrew-English and English-
Hebrew) information retrieval, allowing users to
pose queries in either of the two languages and re-
trieve documents in both. This is achieved by two
sub-processes of the search engine: first, the en-
gine performs shallow semantic linguistic inference
and supports the retrieval of documents which con-
tain phrases that imply the meaning of the translated
query, even when no exact match of the translated
keywords is found. This is enabled by automatic ac-
quisition of semantic variability patterns that are fre-
quent in the language, which extend traditional lexi-
cal query expansion techniques. Second, the engine
translates the original or expanded query to the tar-
get language, based on several linguistic processes
and a machine readable bilingual dictionary. The re-
sult is a semantic expansion of a given query to a va-
riety of alternative wordings in which an answer to
this query may be expressed in the target language
of the retrieved documents.
These enhancements are facilitated via a speci-
fication of the domain. As our system is specifi-
cally designed to work in the domain of the history
and archaeology, we could focus our attention on re-
sources and tools that are dedicated to this domain.
Thus, for example, lexicons and dictionaries, whose
preparation is always costly and time consuming,
were developed with the specific domain in mind;
and textual entailment and paraphrase patterns were
extracted for the specific domain. While the result-
ing system is focused on visiting the Hecht Museum,
the methodology which we used and discuss here
can be adapted to other areas of cultural heritage, as
well as to other narrow domains, in the same way.
3.1 Setting Up a Basic Retrieval Application
We created a basic retrieval system in two steps:
first, we collected relevant documents; then, we im-
plemented a search engine over the collected docu-
ments.
In order to construct a local corpus, an archae-
ology expert searched the Web for relevant sites
and pages. We then downloaded all the documents
linked from those pages using a crawler. The expert
looked for documents in both English and Hebrew.
In total, we collected a non-comparable bilingual
corpus for Archaeology containing several thousand
documents in English and Hebrew.
We implemented our enhanced retrieval modules
on top of the basic Jakarta Lucene indexing and
search engine1. All documents were indexed using
Lucene, but instead of inflected words, we indexed
the lemma of each word (see detailed description of
our Hebrew lemmatization in Section 3.3). In order
to match the indexed terms, query terms (either He-
brew or English) were also lemmatized before the
index was searched, in a manner similar to lemma-
tizing the documents.
3.2 Query Expansion Using Entailment Rules
As described in Section 2.1, entailment rules had not
been used as a knowledge resource for expanding IR
queries, prior to our work. In this paper we use this
resource instead of the typical lexical expansion in
order to test its benefit. Most entailment rules cap-
ture relations between different predicates. We thus
focus on documents retrieved for queries that con-
tain a predicate over one or two entities, which we
term here Relational IR. We would like to retrieve
only documents that describe an occurrence of that
predicate, but possibly in words different than the
ones used in the query. In this section we describe
in detail how we learn entailment rules and how we
apply them in query expansion.
Automatically Learning Entailment Rules from
the Web Many algorithms for automatically learn-
ing paraphrases and entailment rules have been
explored in recent years (Lin and Pantel, 2001;
1http://jakarta.apache.org/lucene/docs/index.html
67
Ravichandran and Hovy, 2002; Shinyama et al,
2002; Barzilay and Lee, 2003; Sudo et al, 2003;
Szpektor et al, 2004; Satoshi, 2005). In this pa-
per we use TEASE (Szpektor et al, 2004), a state-
of-the-art unsupervised acquisition algorithm for
lexical-syntactic entailment rules.
TEASE acquires entailment relations for a given
input template from the Web. It first retrieves from
the Web sentences that match the input template.
From these sentences it extracts the variable instan-
tiations, termed anchor-sets, which are identified as
being characteristic for the input template based on
statistical criteria.
Next, TEASE retrieves from the Web sentences
that contain the extracted anchor-sets. The retrieved
sentences are parsed and the anchors found in each
sentence are replaced with their corresponding vari-
ables. Finally, from this retrieved corpus of parsed
sentences, templates that are assumed to entail or
be entailed by the input template are learned. The
learned templates are ranked by the number of oc-
currences they were learned from.
Entailment Rules for Domain Specific Query Ex-
pansion Our goal is to use the knowledge-base of
entailment rules learned by TEASE in order to per-
form query expansion. The two subtasks that arise
are: (a) acquiring an appropriate knowledge-base
of rules; and (b) expanding a query given such a
knowledge-base.
TEASE learns entailment rules for a given input
template. As our document collection is domain
specific, a list of such relevant input templates can
be prepared. In our case, we used an archaeology
expert to generate a list of verbs and verb phrases
that relate to archaeology, such as: ?excavate?, ?in-
vade?, ?build?, ?reconstruct?, ?grow? and ?be located
in?. We then executed TEASE on each of the tem-
plates representing these verbs in order to learn from
the Web rules in which the input templates partici-
pate. An example for such rules is presented in Ta-
ble 1. We learned approximately 3900 rules for 80
input templates.
Since TEASE learns lexical-syntactic rules, we
need a syntactic representation of the query. We
parse each query using the Minipar dependency
parser (Lin, 1998). We next try to match the left
hand side template of every rule in the learned
knowledge-base. Since TEASE does not identify
the direction of the relation learned between two
templates, we try both directional rules that are in-
duced from a learned relation. Whenever a match
is found, a new query is generated, in which the
constant terms of the matched left hand side tem-
plate are replaced with the constant terms of the right
hand side template. For example, given the query
?excavations of Jerusalem by archaeologists? and a
learned rule ?excavation of Y by X ? X dig in Y ?,
a new query is generated, containing the terms ?ar-
chaeologists dig in Jerusalem?. Finally, we retrieve
all the documents that contain all the terms of at least
one of the expanded queries (including the original
query). The basic search engine provides a score for
each document. We re-score each document as the
sum of scores it obtained from the different queries
that it matched. Figure 1 shows an example of our
query expansion, where the first retrieved documents
do not contain the words used to describe the predi-
cate in the query, but other ways to describe it.
All the templates learned by TEASE contain two
variables, and thus the rules that are learned can only
be applied to queries that contain predicates over
two terms. In order to broaden the coverage of the
learned rules, we automatically generate also all the
partial templates of a learned template. These are
templates that contain just one of variables in the
original template. We then generate rules between
these partial templates that correspond to the origi-
nal rules. With partial templates/rules, expansion for
the query in Figure 1 becomes possible.
3.3 Cross-lingual IR
Until very recently, linguistic resources for Hebrew
were few and far between (Wintner, 2004). The last
few years, however, have seen a proliferation of re-
sources and tools for this language. In this work we
utilize a relatively large-scale lexicon of over 22,000
entries (Itai et al, 2006); a finite-state based mor-
phological analyzer of Hebrew that is directly linked
to the lexicon (Yona and Wintner, 2007); a medium-
size bilingual dictionary of some 24,000 word pairs;
and a rudimentary Hebrew to English machine trans-
lation system (Lavie et al, 2004). All these re-
sources had to be adapted to the domain of the Hecht
museum.
Cross-lingual language technology is utilized in
68
Figure 1: Semantic expansion example. Note that the expanded queries that were generated in the first two
retrieved texts (listed under ?matched query?) do not contain the original query.
three different components of the system: Hebrew
documents are morphologically processed to pro-
vide better indexing; query terms in English are
translated to Hebrew and vice versa; and Hebrew
snippets are translated to English. We discuss each
of these components in this section.
Linguistically-aware indexing The correct level
of indexing for morphologically-rich language has
been a matter of some debate in the information re-
trieval literature. When Arabic is concerned, Dar-
wish and Oard (2002) conclude that ?Character n-
grams or lightly stemmed words were found to
typically yield near-optimal retrieval effectiveness?.
Since Hebrew is even more morphologically (and
orthographically) ambiguous than Arabic, and espe-
cially in light of the various prefix particles which
can be attached to Hebrew words, we opted for full
morphological analysis of Hebrew documents be-
fore they are indexed, followed by indexing on the
lexeme.
We use the HAMSAH morphological analyzer
(Yona and Wintner, 2007), which was recently re-
written in Java and is therefore more portable and
efficient (Wintner, 2007). We processed the entire
domain specific corpus described above and used
the resulting lexemes to index documents. This pre-
processing brought to the foreground several omis-
sions of the analyzer, mostly due to domain-specific
terms missing in the lexicon. We selected the one
thousand most frequent words with no morphologi-
cal analysis and added their lexemes to the lexicon.
While we do not have quantitative evaluation met-
rics, the coverage of the system improved in a very
evident way.
Query translation When users submit a query in
one language they are provided with the option to re-
quest a translation of the query to the other language,
thereby retrieving documents in the other language.
The motivation behind this capability is that users
who may be able to read documents in a language
may find the specification of queries in that language
too challenging; also, retrieving documents in a for-
eign language may be useful due to the non-textual
information in the retrieved documents, especially in
a museum environment.
In order to support cross-lingual query specifica-
tion we capitalized on a medium-size bilingual dic-
tionary that was already used for Hebrew to Eng-
lish machine translation. Since the coverage of the
dictionary was rather limited, and many domain-
specific items were missing, we chose the one thou-
sand most frequent lexemes which had no transla-
69
Input Template Learned Template
X excavate Y X discover Y , X find Y ,
X uncover Y , X examine Y ,
X unearth Y , X explore Y
X construct Y X build Y , X develop Y ,
X create Y , X establish Y
X contribute to Y X cause Y , X linked to Y ,
X involve in Y
date X to Y X built in Y , X began in Y ,
X go back to Y
X cover Y X bury Y ,
X provide coverage for Y
X invade Y X occupy Y , X attack Y ,
X raid Y , X move into Y
X restore Y X protect Y , X preserve Y ,
X save Y , X conserve Y
Table 1: Examples for correct templates that were
learned by TEASE for input templates.
tions and translated them manually, augmenting the
lexicon with missing Hebrew lexemes where neces-
sary and expanding the bilingual dictionary to cover
this domain.
In order to translate query terms we use the He-
brew English dictionary also as an English-Hebrew
dictionary. While this is known to be sub-optimal,
our current results support such an adaptation in lieu
of dedicated directional bilingual dictionaries.
Translating a query from one language to another
may introduce ambiguity where none exists. For
example, the query term spinh ?vessel? is unam-
biguous in Hebrew, but once translated into English
will result in retrieving documents on both senses
of the English word. Usually, this problem is over-
come since users tend to specify multi-term queries,
and the terms disambiguate each other. However,
a more systematic solution can be offered since we
have access to semantic expansion capabilities (in a
single language). That is, expanding the query in
the source language will result in more query terms
which, when translated, are more likely to disam-
biguate the context. We leave such an extension for
future work.
Snippet translation When Hebrew documents are
retrieved, we augment the (Hebrew) snippet which
the system produces by an English translation. We
use an extended, improved version of a rudimentary
Hebrew to English MT system developed by Lavie
et al (2004). Extensions include an improved mor-
phological analysis of the input, an extended bilin-
gual dictionary and a revised set of transfer rules,
as well as a more modern transfer engine and a
much larger language model for generating the tar-
get (English) sentences.
The MT system is transfer based: it performs lin-
guistic pre-processing of the source language (in our
case, morphological analysis) and post-processing
of the target (generation of English word forms), and
uses a small set of transfer rules to translate local
structures from the source to the target and create
translation hypotheses, which are stored in a lattice.
A statistical language model is used to decode the
lattice and select the best hypotheses.
The benefit of this architecture is that domain spe-
cific adaptation of the system is relatively easy, and
does not require a domain specific parallel corpus
(which we do not have). The system has access
to our domain-specific lexicon and bilingual dictio-
nary, and we even refined some transfer rules due to
peculiarities of the domain. One advantage of the
transfer-based approach is that it enables us to treat
out-of-lexicon items in a unique way. We consider
such items proper names, and transfer rules process
them as such. As an example, Figure 2 depicts the
translation of a Hebrew snippet meaning A jar from
the early bronze period with seashells from the Nile.
The word nilws ?Nile? is missing from the lexicon,
but this does not prevent the system from producing
a legible translation, using the transliterated form
where an English equivalent is unavailable.
4 Conclusions
We described a system for cross-lingual and
semantically-enhanced retrieval of information in
the cultural heritage domain, obtained by adapting
existing state-of-the-art tools and resources to the
domain. The system enhances the experience of mu-
seum visits, using language technology as a vehi-
cle for long-lasting instillation of information. Due
to the novelty of this application and the dearth of
available multilingual annotated resources in this
domain, we are unable to provide a robust, quan-
70
Figure 2: Translation example
Query Without Expansion With Expansion
Relevant Total Relevant Total
in Top 10 Retrieved in Top 10 Retrieved
discovering boats 2 2 5 86
growing vineyards 0 0 6 8
Persian invasions 5 5 8 22
excavations of the Byzantine period 10 37 10 100
restoring mosaics 0 0 3 69
Table 2: Analysis of the number of relevant documents out of the top 10 and the total number of retrieved
documents (up to 100) for a sample of queries.
titative evaluation of the approach. A preliminary
analysis of a sample of queries is presented in Ta-
ble 2. It illustrates the potential of expansion for
document collections of narrow domain. In what
follows we provide some qualitative impressions.
We observed that the system was able to learn
many expansion rules that cannot be induced from
manually constructed lexical resources, such as the-
sauri or WordNet (Fellbaum, 1998). This is espe-
cially true for rules that are specific for a narrow do-
main, e.g. ?X restore Y ? X preserve Y ?. Fur-
thermore, the system learned lexical syntactic rules
that cannot be expressed by a mere lexical substitu-
tion, but include also a syntactic transformation. For
example, ?date X to Y ? X go back to Y ?.
In addition, since rules are acquired by searching
the Web, they are not necessarily restricted to learn-
ing from the target domain, but can be learned from
similar terminology in other domains. For example,
the rule ?X discover Y ? X find Y ? was learned
from contexts such as {X=?astronomers? ;Y =?new
planets?} and {X=?zoologists? ;Y =?new species?}.
The quality of the rules that were automatically
acquired is mediocre. We found that although many
rules were useful for expansion, they had to be
manually filtered in order to retain only rules that
achieved high precision.
Finally, we note that applying semantic query ex-
pansion (using entailment rules), followed by Eng-
lish to Hebrew query translation, results in query ex-
pansion for Hebrew using techniques that were so
far applicable only to resource-rich languages, such
as English.
Acknowledgements
This research was supported by the Israel Internet
Association; by THE ISRAEL SCIENCE FOUN-
DATION (grant No. 137/06 and grant No. 1095/05);
by the Caesarea Rothschild Institute for Interdisci-
plinary Application of Computer Science at the Uni-
versity of Haifa; by the ITC-irst/University of Haifa
collaboration; and by the US National Science Foun-
dation (grants IIS-0121631, IIS-0534217, and the
Office of International Science and Engineering).
71
We wish to thank the Hebrew Knowledge Center
at the Technion for providing resources for Hebrew.
We are grateful to Oliviero Stock, Martin Golumbic,
Alon Itai, Dalia Bojan, Erik Peterson, Nurit Mel-
nik, Yaniv Eytani and Noam Ordan for their help
and support.
References
Lisa Ballesteros and W. Bruce Croft. 1997. Phrasal
translation and query expansion techniques for cross-
language information retrieval. In ACM SIGIR Con-
ference on Research and Development in Information
Retrieval, pages 84?91.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second pascal recognising textual entail-
ment challenge. In Second PASCAL Challenge Work-
shop for Recognizing Textual Entailment.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL.
Jaime G. Carbonell, Yiming Yang, Robert E. Frederk-
ing, Ralf D. Brown, Yibing Geng, and Danny Lee.
1997. Translingual information retrieval: A compar-
ative evaluation. In IJCAI (1), pages 708?715.
Ido Dagan, Oren Glickman, and Bernardo. Magnini.
2006. The pascal recognising textual entailment chal-
lenge. In Lecture Notes in Computer Science, Volume
3944, volume 3944, pages 177?190.
Kareem Darwish and Douglas W. Oard. 2002. Term se-
lection for searching printed Arabic. In SIGIR ?02:
Proceedings of the 25th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, pages 261?268, New York, NY,
USA. ACM Press.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Language, Speech and Com-
munication. MIT Press.
D. A. Hull and G. Grefenstette. 1996. Querying across
languages. a dictionary-based approach to multilingual
information retrieval. In Proceedings of the 19th ACM
SIGIR Conference, pages 49?57.
Alon Itai, Shuly Wintner, and Shlomo Yona. 2006. A
computational lexicon of contemporary Hebrew. In
Proceedings of The fifth international conference on
Language Resources and Evaluation (LREC-2006).
Alon Lavie, Shuly Wintner, Yaniv Eytani, Erik Peterson,
and Katharina Probst. 2004. Rapid prototyping of a
transfer-based Hebrew-to-English machine translation
system. In Proceedings of TMI-2004: The 10th Inter-
national Conference on Theoretical and Methodolog-
ical Issues in Machine Translation, Baltimore, MD,
October.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. In Natural Lan-
guage Engineering, volume 7(4), pages 343?360.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalu-
ation of Parsing Systems at LREC.
S. Lytinen, N. Tomuro, and T. Repede. 2000. The use of
wordnet sense tagging in faqfinder. In Proceedings of
the AAAI00 Workshop on AI and Web Search.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Lorenza Romano, Milen Kouylekov, Idan Szpektor, Ido
Dagan, and Alberto Lavelli. 2006. Investigating a
generic paraphrase-based approach for relation extrac-
tion. In Proceedings of EACL.
Sekine Satoshi. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Yusuke Shinyama, Satoshi Sekine, Sudo Kiyoshi, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of HLT.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic ie pattern acquisition. In Pro-
ceedings of ACL.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP.
Shuly Wintner. 2004. Hebrew computational linguis-
tics: Past and future. Artificial Intelligence Review,
21(2):113?138.
Shuly Wintner. 2007. Finite-state technology as a pro-
gramming environment. In Alexander Gelbukh, edi-
tor, Proceedings of the Conference on Computational
Linguistics and Intelligent Text Processing (CICLing-
2007), volume 4394 of Lecture Notes in Computer Sci-
ence, pages 97?106, Berlin and Heidelberg, February.
Springer.
Shlomo Yona and Shuly Wintner. 2007. A finite-state
morphological grammar of Hebrew. Natural Lan-
guage Engineering. To appear.
Ingrid Zukerman and Bhavani Raskutti. 2002. Lexical
query paraphrasing for document retrieval. In Pro-
ceedings of ACL.
72
