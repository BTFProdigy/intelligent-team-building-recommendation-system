Proceedings of the ACL 2007 Demo and Poster Sessions, pages 133?136,
Prague, June 2007. c?2007 Association for Computational Linguistics
Building Emotion Lexicon from Weblog Corpora 
Changhua Yang        Kevin Hsin-Yih Lin        Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
#1 Roosevelt Rd. Sec. 4, Taipei, Taiwan 106 
{d91013, f93141, hhchen}@csie.ntu.edu.tw 
Abstract 
An emotion lexicon is an indispensable re-
source for emotion analysis.  This paper 
aims to mine the relationships between 
words and emotions using weblog corpora.  
A collocation model is proposed to learn 
emotion lexicons from weblog articles.  
Emotion classification at sentence level is 
experimented by using the mined lexicons 
to demonstrate their usefulness. 
1 Introduction 
Weblog (blog) is one of the most widely used cy-
bermedia in our internet lives that captures and 
shares moments of our day-to-day experiences, 
anytime and anywhere.  Blogs are web sites that 
timestamp posts from an individual or a group of 
people, called bloggers.  Bloggers may not follow 
formal writing styles to express emotional states.  
In some cases, they must post in pure text, so they 
add printable characters, such as ?:-)? (happy) and 
?:-(? (sad), to express their feelings.  In other cases, 
they type sentences with an internet messenger-
style interface, where they can attach a special set 
of graphic icons, or emoticons.  Different kinds of 
emoticons are introduced into text expressions to 
convey bloggers? emotions. 
Since thousands of blog articles are created eve-
ryday, emotional expressions can be collected to 
form a large-scale corpus which guides us to build 
vocabularies that are more emotionally expressive.  
Our approach can create an emotion lexicon free of 
laborious efforts of the experts who must be famil-
iar with both linguistic and psychological knowl-
edge. 
2 Related Works 
Some previous works considered emoticons from 
weblogs as categories for text classification.  
Mishne (2005), and Yang and Chen (2006) used 
emoticons as tags to train SVM (Cortes and Vap-
nik, 1995) classifiers at document or sentence level.  
In their studies, emoticons were taken as moods or 
emotion tags, and textual keywords were taken as 
features.  Wu et al (2006) proposed a sentence-
level emotion recognition method using dialogs as 
their corpus.  ?Happy, ?Unhappy?, or ?Neutral? 
was assigned to each sentence as its emotion cate-
gory.  Yang et al (2006) adopted Thayer?s model 
(1989) to classify music emotions.  Each music 
segment can be classified into four classes of 
moods.  In sentiment analysis research, Read (2005) 
used emoticons in newsgroup articles to extract 
instances relevant for training polarity classifiers. 
3 Training and Testing Blog Corpora 
We select Yahoo! Kimo Blog1 posts as our source 
of emotional expressions.  Yahoo! Kimo Blog 
service has 40 emoticons which are shown in Table 
1.  When an editing article, a blogger can insert an 
emoticon by either choosing it or typing in the 
corresponding codes.  However, not all articles 
contain emoticons.  That is, users can decide 
whether to insert emoticons into articles/sentences 
or not.  In this paper, we treat these icons as 
emotion categories and taggings on the 
corresponding text expressions. 
The dataset we adopt consists of 5,422,420 blog 
articles published at Yahoo! Kimo Blog from 
January to July, 2006, spanning a period of 212 
days.  In total, 336,161 bloggers? articles were col-
lected.  Each blogger posts 16 articles on average. 
We used the articles from January to June as the 
training set and the articles in July as the testing set.  
Table 2 shows the statistics of each set.  On aver-
age, 14.10% of the articles contain emotion-tagged 
expressions.  The average length of articles with 
tagged emotions, i.e., 272.58 characters, is shorter 
                                                 
1
 http://tw.blog.yahoo.com/ 
133
than that of articles without tagging, i.e., 465.37 
characters.  It seems that people tend to use emoti-
cons to replace certain amount of text expressions 
to make their articles more succinct. 
Figure 1 shows the three phases for the con-
struction and evaluation of emotion lexicons.  In 
phase 1, 1,185,131 sentences containing only one 
emoticon are extracted to form a training set to 
build emotion lexicons.  In phase 2, sentence-level 
emotion classifiers are constructed using the mined 
lexicons.  In phase 3, a testing set consisting of 
307,751 sentences is used to evaluate the classifi-
ers. 
4 Emotion Lexicon Construction 
The blog corpus contains a collection of bloggers? 
emotional expressions which can be analyzed to 
construct an emotion lexicon consisting of words 
that collocate with emoticons. We adopt a variation 
of pointwise mutual information (Manning and 
Sch?tze, 1999) to measure the collocation strength 
co(e,w) between an emotion e and a word w:  
)()(
),(log),(),o(
wPeP
weP
wecwec ?=  (1) 
where P(e,w)=c(e,w)/N, P(e)=c(e)/N, P(w)=c(w)/N, 
c(e)
 
and c(w) are the total occurrences of emoticon 
e and word w in a tagged corpus, respectively, 
c(e,w) is total co-occurrences of e and w, and N 
denotes the total word occurrences. 
A word entry of a lexicon may contain several 
emotion senses.  They are ordered by the colloca-
tion strength co.  Figure 2 shows two Chinese ex-
ample words, ??? ? (ha1ha1) and ??? ? 
(ke3wu4).  The former collocates with ?laughing? 
and ?big grin? emoticons with collocation strength 
25154.50 and 2667.11, respectively.  Similarly, the 
latter collocates with ?angry? and ?phbbbbt?.  
When all collocations (i.e., word-emotion pairs) 
are listed in a descending order of co, we can 
choose top n collocations to build an emotion lexi-
con.  In this paper, two lexicons (Lexicons A and B) 
are extracted by setting n to 25k and 50k.  Lexicon 
A contains 4,776 entries with 25,000 sense pairs 
and Lexicon B contains 11,243 entries and 50,000 
sense pairs. 
5 Emotion Classification 
Suppose a sentence S to be classified consists of n 
emotion words.  The emotion of S is derived by a 
mapping from a set of n emotion words to m emo-
tion categories as follows: 
 },...,{?},...,{ 11 m
tionclassifican
eeeewewS ???  
Table 1. Yahoo! Kimo Blog Emoticon Set. 
ID Emoticon Code Description ID Emoticon Code Description ID Emoticon Code Description ID Emoticon Code Description 
1 
 
:) happy 11 
 
:O surprise 21 
 
0:) angel 31 
 
(:| yawn 
2 
 
:( sad 12 
 
X-( angry 22 
 
:-B nerd 32 
 
=P~ drooling 
3 
 
;) winking 13 
 
:> smug 23 
 
=; 
talk to  
the hand 33  :-? thinking 
4 
 
:D big grin 14 
 
B-) cool 24 
 
I-) asleep 34 
 
;)) hee hee 
5 
 
;;) batting  eyelashes 15  :-S worried 25  8-) rolling eyes 35  =D> applause 
6 
 
:-/ confused 16 
 
>:) devil 26 
 
:-& sick 36 
 
[-o< praying 
7 
 
:x love struck 17 
 
:(( crying 27 
 
:-$ don't tell  anyone 37  :-< sigh 
8 
 
:?> blushing 18 
 
:)) laughing 28 
 
[-( not talking 38 
 
>:P phbbbbt 
9 
 
:p tongue 19 
 
:| straight face 29 
 
:o) clown 39 
 
@};- rose 
10 
 
:* kiss 20 
 
/:) raised  eyebrow 30  @-) hypnotized 40  :@) pig 
 
Table 2. Statistics of the Weblog Dataset. 
Dataset Article # Tagged # Percentage Tagged Len. Untagged L. 
Training 4,187,737 575,009 13.86% 269.77 chrs. 468.14 chrs. 
Testing 1,234,683 182,999 14.92% 281.42 chrs. 455.82 chrs. 
Total 5,422,420 764,788 14.10% 272.58 chrs. 465.37 chrs. 
 
Testing Set 
Figure 1. Emotion Lexicon Construction and Evaluation. 
Extraction 
Blog 
Articles 
Features 
Classifiers 
Evaluation 
Lexicon 
 Construction 
Training Set 
Phase 2
Phase 3 
Emotion 
 Lexicon 
Phase 1 
134
For each emotion word ewi, we may find several 
emotion senses with the corresponding collocation 
strength co by looking up the lexicon.  Three alter-
natives are proposed as follows to label a sentence 
S with an emotion: 
(a) Method 1 
(1)  Consider all senses of ewi as votes.  Label S 
with the emotion that receives the most votes. 
(2)  If more than two emotions get the same num-
ber of votes, then label S with the emotion that 
has the maximum co. 
(b) Method 2 
    Collect emotion senses from all ewi.  Label S 
with the emotion that has the maximum co. 
(c) Method 3 
The same as Method 1 except that each ewi v-
otes only one sense that has the maximum co. 
In past research, the approach used by Yang et 
al. (2006) was based on the Thayer?s model (1989), 
which divided emotions into 4 categories.  In sen-
timent analysis research, such as Read?s study 
(2006), a polarity classifier separated instances into 
positive and negative classes.  In our experiments, 
we not only adopt fine-grain classification, but also 
coarse-grain classification.  We first select 40 
emoticons as a category set, and also adopt the 
Thayer?s model to divide the emoticons into 4 
quadrants of the emotion space.  As shown in Fig-
ure 3, the top-right side collects the emotions that 
are more positive and energetic and the bottom-left 
side is more negative and silent.  A polarity classi-
fier uses the right side as positive and the left side 
as negative. 
6 Evaluation 
Table 3 shows the performance under various 
combinations of lexicons, emotion categories and 
classification methods.  ?Hit #? stands for the 
number of correctly-answered instances. The base-
line represents the precision of predicting the ma-
jority category, such as ?happy? or ?positive?, as 
the answer.  The baseline method?s precision in-
creases as the number of emotion classes decreases.  
The upper bound recall indicates the upper limit on 
the fraction of the 307,751 instances solvable by 
the corresponding method and thus reflects the 
limitation of the method.  The closer a method?s 
actual recall is to the upper bound recall, the better 
the method.  For example, at most 40,855 instances 
(14.90%) can be answered using Method 1 in 
combination with Lexicon A.  But the actual recall 
is 4.55% only, meaning that Method 1?s recall is 
more than 10% behind its upper bound.  Methods 
which have a larger set of candidate answers have 
higher upper bound recalls, because the probability 
that the correct answer is in their set of candidate 
answers is greater. 
Experiment results show that all methods utiliz-
ing Lexicon A have performance figures lower 
than the baseline, so Lexicon A is not useful.  In 
contrast, Lexicon B, which provides a larger col-
lection of vocabularies and emotion senses, outper-
forms Lexicon A and the baseline.  Although 
Method 3 has the smallest candidate answer set 
and thus has the smallest upper bound recall, it 
outperforms the other two methods in most cases.  
Method 2 achieves better precisions when using 
?? (ha1ha1) ?hah hah?  
Sense 1. (laughing) ? co: 25154.50 
e.g., ??...  ???????~ 
        ?hah hah?  I am getting lucky~? 
Sense 2. (big grin) ? co: 2667.11 
e.g., ??????????~??  
        ?I only memorized vowels today~ haha ? 
?? (ke3wu4) ?darn? 
Sense 1. (angry) ? co: 2797.82 
e.g., ??????...??  
        ?What's the hacker doing... darn it ? 
Sense 2. (phbbbbt) ? co: 619.24 
e.g., ???????  
        ?Damn those aliens ? 
Figure 2. Some Example Words in a Lexicon. 
 
Arousal (energetic) 
 
 
               
                       
                                                                  Valence 
(negative)                                            (positive) 
 
        
       
 
(silent) 
unassigned:  
Figure 3. Emoticons on Thayer?s model. 
135
Thayer?s emotion categories.  Method 1 treats the 
vote to every sense equally.  Hence, it loses some 
differentiation abilities.  Method 1 performs the 
best in the first case (Lexicon A, 40 classes). 
We can also apply machine learning to the data-
set to train a high-precision classification model.  
To experiment with this idea, we adopt LIBSVM 
(Fan et al, 2005) as the SVM kernel to deal with 
the binary polarity classification problem.  The 
SVM classifier chooses top k (k = 25, 50, 75, and 
100) emotion words as features.  Since the SVM 
classifier uses a small feature set, there are testing 
instances which do not contain any features seen 
previously by the SVM classifier.  To deal with 
this problem, we use the class prediction from 
Method 3 for any testing instances without any 
features that the SVM classifier can recognize.  In 
Table 4, the SVM classifier employing 25 features 
has the highest precision.  On the other hand, the 
SVM classifier employing 50 features has the 
highest F measure when used in conjunction with 
Method 3. 
7 Conclusion and Future Work 
Our methods for building an emotional lexicon 
utilize emoticons from blog articles collaboratively 
contributed by bloggers.  Since thousands of blog 
articles are created everyday, we expect the set of 
emotional expressions to keep expanding.  In the 
experiments, the method of employing each emo-
tion word to vote only one emotion category 
achieves the best performance in both fine-grain 
and coarse-grain classification. 
Acknowledgment 
Research of this paper was partially supported by 
Excellent Research Projects of National Taiwan 
University, under the contract of 95R0062-AE00-
02.  We thank Yahoo! Taiwan Inc. for providing 
the dataset for researches. 
References 
Corinna Cortes and V. Vapnik. 1995. Support-Vector 
Network. Machine Learning, 20:273?297. 
Rong-En Fan, Pai-Hsuen Chen and Chih-Jen Lin. 2005. 
Working Set Selection Using Second Order Informa-
tion for Training Support Vector Machines. Journal 
of Machine Learning Research, 6:1889?1918. 
Gilad Mishne. 2005. Experiments with Mood Classifi-
cation in Blog Posts. Proceedings of 1st Workshop on 
Stylistic Analysis of Text for Information Access. 
Jonathon Read. 2005. Using Emotions to Reduce De-
pendency in Machine Learning Techniques for Sen-
timent Classification. Proceedings of the ACL Stu-
dent Research Workshop, 43-48. 
Robert E. Thayer. 1989. The Biopsychology of Mood 
and Arousal, Oxford University Press. 
Changhua Yang and Hsin-Hsi Chen. 2006. A Study of 
Emotion Classification Using Blog Articles. Pro-
ceedings of Conference on Computational Linguistics 
and Speech Processing, 253-269. 
Yi-Hsuan Yang, Chia-Chu Liu, and Homer H. Chen. 
2006. Music Emotion Classification: A Fuzzy Ap-
proach. Proceedings of ACM Multimedia, 81-84. 
Chung-Hsien Wu, Ze-Jing Chuang, and Yu-Chung Lin. 
2006. Emotion Recognition from Text Using Seman-
tic Labels and Separable Mixture Models. ACM 
Transactions on Asian Language Information Proc-
essing, 5(2):165-182. 
Table 3. Evaluation Results. 
Method 1 (M1) Method 2 (M2) Method 3 (M3) 
 Baseline Upp. R. Hit # Prec. Reca. Upp. R. Hit # Prec. Reca. Upp. R. Hit # Prec. Reca. 
Lexicon A 
40 classes 8.04% 14.90% 14,009 4.86% 4.55% 14.90% 9,392 3.26% 3.05% 6.49% 13,929 4.83% 4.52% 
Lexicon A 
Thayer 38.38% 48.70% 90,332 32.46% 29.35% 48.70% 64,689 23.25% 21.02% 35.94% 93,285 33.53% 30.31% 
Lexicon A 
Polarity 63.49% 60.74% 150,946 54.25% 49.05% 60.74% 120,237 43.21% 39.07% 54.97% 153,292 55.09% 49.81% 
Lexicon B 
40 classes 8.04% 73.18% 45,075 15.65% 14.65% 73.18% 43,637 15.15% 14.18% 27.89% 45,604 15.83% 14.81% 
Lexicon B 
Thayer 38.38% 89.11% 104,094 37.40% 33.82% 89.11% 118,392 42.55% 38.47% 63.74% 110,904 39.86% 36.04% 
Lexicon B 
Polarity 63.49% 91.12% 192,653 69.24% 62.60% 91.12% 188,434 67.72% 61.23% 81.92% 195,190 70.15% 63.42% 
Upp. R. ? upper bound recall; Prec. ? precision; Reca. ? recall 
                          Table 4. SVM  Performance. 
Method Upp. R. Hit # Prec. Reca. F 
Lexicon B M3 81.92% 195,190 70.15% 63.42% 66.62% 
SVM 25 features 15.80% 38,651 79.49% 12.56% 21.69% 
SVM 50 features 26.27% 62,999 77.93% 20.47% 32.42% 
SVM 75 features 36.74% 84,638 74.86% 27.50% 40.23% 
SVM 100 features 45.49% 101,934 72.81% 33.12% 45.53% 
 (Svm-25 + M3) 90.41% 196,147 70.05% 63.73% 66.74% 
 (Svm-50 + M3) 90.41% 195,835 70.37% 63.64% 66.83% 
(Svm-75 + M3) 90.41% 195,229 70.16% 63.44% 66.63% 
 (Svm-100 + M3) 90.41% 195,054 70.01% 63.38% 66.53% 
                                F = 2?(Precision?Recall)/(Precision+Recall) 
136
Learning Formulation and Transformation Rules for  
Multilingual Named Entities 
Hsin-Hsi Chen Changhua Yang Ying Lin 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, TAIWAN, 106 
{hh_chen, d91013, b88034}@csie.ntu.edu.tw 
Abstract 
This paper investigates three multilingual 
named entity corpora, including named 
people, named locations and named 
organizations.  Frequency-based 
approaches with and without dictionary 
are proposed to extract formulation rules 
of named entities for individual languages, 
and transformation rules for mapping 
among languages.  We consider the issues 
of abbreviation and compound keyword at 
a distance.  Keywords specify not only the 
types of named entities, but also tell out 
which parts of a named entity should be 
meaning-translated and which part should 
be phoneme-transliterated.  An 
application of the results on cross 
language information retrieval is also 
shown. 
1 Introduction 
Named entities are major components of a 
document.  Capturing named entities is a 
fundamental task to understanding documents 
(MUC, 1998).  Several approaches have been 
proposed to recognize these types of terms.  For 
example, corpus-based methods are employed to 
extract Chinese personal names, and rule-based 
methods are used to extract Chinese date/time 
expressions and monetary and percentage 
expressions (Chen and Lee, 1996; Chen, Ding and 
Tsai, 1998).  In the past, named entity extraction 
mainly focuses on general domains and is 
employed to various applications such as 
information retrieval (Chen, Ding and Tsai, 1998), 
question-answering (Lin, et al, 2001), and so on.  
Recently, several attempts have been extended to 
mine knowledge from biomedical documents 
(Hirschman, et al, 2002). 
Most of the previous approaches dealt with 
monolingual named entity extraction.  Chen et al 
(1998) extended it to cross-language information 
retrieval.  A grapheme-based model was proposed 
to compute the similarity between Chinese 
transliteration name and English name.  Lin and 
Chen (2000) further classified the works into two 
directions ? say, forward transliteration (Wan and 
Verspoor, 1998) and backward transliteration 
(Chen et al, 1998; Knight and Graehl, 1998), and 
proposed a phoneme-based model.  Lin and Chen 
(2002) employed a machine learning approach to 
determine phonetic similarity scores for machine 
transliteration.  AI-Onaizan and Knight (2002) 
investigated the translation of Arabic named 
entities to English using monolingual and bilingual 
resources. 
The past works on multilingual named entities 
emphasizes on the transliteration issues.  However, 
the transformation between named entities in 
different languages is not transliteration only.  The 
mapping may be a combination of meaning 
translation and/or phoneme transliteration.  The 
following five English-Chinese examples show 
this issue.  The symbol A ? B denotes a foreign 
name A is translated and/or transliterated into a 
Chinese name B. 
 
(s1) Victoria Fall  
? ?????? (wei duo li ya pu bu) 
(s2) Little Rocky Mountains 
? ????? (xiao luo ji shan mo) 
(s3) Great Salt Lake ? ??? (da yan hu) 
(s4) Kenmare ? ??? (kang mei er) 
(s5) East Chicago ? ???? (dong zhi jia ge) 
 
Example (s1) shows a name part (i.e., Victoria) 
and a keyword part (i.e., Fall) of a named location 
are transliterated and translated into ?????? 
(wei duo li ya) and ???? (pu bu), respectively.  
In Example (s2), the keyword part (i.e., Mountains) 
is still translated, i.e., ???? (shan mo), however, 
some part of name is translated (i.e., Little ? ??? 
(xiao)) and another part is transliterated (i.e., 
Rocky ? ???? (luo ji)).  Example (s3) shows an 
extreme case.  All the three words are translated 
(i.e., Great ? ??? (da)), Salt ? ??? (yan), Lake 
? ??? (hu)).  Examples (s4) and (s5) show two 
location names without keywords.  The former is 
transliterated and the latter is a combination of 
transliteration and translation. 
Which part is translated and which part is 
transliterated depends on the type of named entities.  
For example, personal names tend to be 
transliterated.  For a location name, name part and 
keyword part are usually transliterated and 
translated, respectively.  The organization names 
are totally different.  Most of constituents are 
translated.  Besides the issue of the named entity 
types, different language pairs have different 
transformation rules.  German named entity has 
decompounding problem when it is 
translated/transliterated, e.g., Bundesbahn ? ??
????? (lian bang tie lu ju) and Bundesbank ? 
?????? (lian bang yin hang). 
This paper will study the issues of languages 
and named entity types on the choices of 
translation and transliteration.  We focus on three 
more challenging named entities only, i.e., named 
people, named locations and named organizations.  
Three phrase-aligned corpora will be adopted ? say, 
a multilingual personal name corpus and a 
multilingual organization name corpus compiled 
by Central News Agency (abbreviated CNA 
personal name and organization corpora hereafter), 
and a multilingual location name corpus compiled 
by National Institute for Compilation and 
Translation of Taiwan (abbreviated NICT location 
name corpus hereafter).  We will extract 
transliteration/translation rules from these 
multilingual named corpora.  This paper is 
organized as follows.  Section 2 introduces the 
corpora used.  Section 3 shows how to extract 
formulation rules and the transformation rules.  
Section 4 analyzes the results.  Section 5 
demonstrates the application of the extracted rules 
on cross language information retrieval.  Section 6 
concludes the remarks. 
2 Multilingual Named Entity Corpora 
NICT location name corpus which was developed 
by Ministry of Education of Taiwan in 1995 
collected 19,385 foreign location names.  Each 
entry consists of three parts, including foreign 
location name, Chinese transliteration/translation 
name, and country name, e.g., (Victoria Fall, ??
?????? (wei duo li ya pu bu), South Africa), 
(Little Rocky Mountains, ??????? (xiao luo 
ji shan mo), USA), etc.  The foreign location 
names are in English alphabet.  Some location 
names denoting the same city have more than one 
form like Firenze and Florence for a famous Italian 
city.  The former is an Italian name and the latter is 
its English name.  They correspond to two 
different transliterations in Chinese, respectively, 
i.e., ????? (fei leng cui) and ?????? (fo 
luo lun si).  The pronunciation of the foreign 
names in NICT corpus is based on Webster?s New 
Geographic Dictionary.  The foreign name itself 
may be a transliteration name.  A Japanese city is 
transliterated in English alphabet, but its 
corresponding translation name is in Kanji (Hanzi 
in Japanese).  It is hard to capture their 
relationships except dictionary lookup, so that 
Japanese location name is out of our discussion.  
We employ the country field to select the 
translation/transliteration pairs that we will deal 
with in this paper.  Table 1 summarizes the 
statistics of NICT corpus based on country tags. 
 
Table 1. Statistics of NICT Corpus 
Country Frequency Percentage Country Frequency Percentage
USA 3,012 15.5% Korea 574 3.0%
UK 1,073 5.5% Brazil 433 2.2%
Russia 961 5.0% German 395 2.0%
Japan 796 4.1% Italy 379 2.0%
Canada 692 3.6% Spain 370 1.9%
France 679 3.5% Mexico 324 1.7%
India 679 3.5% Others 8,413 43.5%
Australia 603 3.1% Total 19,385 100%
 
CNA personal name and organization corpora 
are used by news reporters to unify the name 
transliteration/translation in news stories.  There 
are 50,586 pairs of foreign personal names and 
Chinese transliteration/translation in persona name 
corpus.  Different from NICT corpus, there do not 
exist clear cues to identify the nationality of named 
people.  Thus, we could not exclude the Japanese 
names like ?Hayakawa? and the corresponding 
name ??? ? (zao chuan) from our discussion 
automatically.  There are 14,658 named 
organizations in CNA corpus.  Some organization 
names are tagged with the country names to which 
they belong.  For example, ?Aachen Technical 
University? ? ?????? (ya ken ji shu da 
xue) (Germany).  But not all the organization 
names have such country tags.  Comparatively, 
organization names are longer than the other two 
named entities.  Table 2 shows the statistics of 
NICT organization name corpus.  FL denotes the 
length of foreign names in words, CL denotes the 
length of Chinese names in characters, and Count 
denotes the number of foreign names of the 
specified length. 
3 Rule Mining 
3.1 Frequency-Based Approach with a 
Bilingual Dictionary 
We postulate that a transliterated term is usually an 
unknown word, i.e., not listed in a lexicon and a 
translated term often appears in a lexicon.  Under 
this postulation, a translated term occurs more 
often in a corpus, and comparatively, a 
transliterated term only appears very few. 
A simple frequency-based method will 
compute the frequencies of terms and use them to 
tell out the transliteration and translation parts in a 
named entity.  Because Chinese has segmentation 
problem, we start the frequency computation from 
the foreign name part in a multilingual named 
entity corpus.  The method is sketched as follows. 
(1) Compute the word frequencies of each 
word in the foreign name list. 
(2) Keep those words that appear more than a 
threshold and appear in a common foreign 
dictionary (e.g., an English dictionary).  These 
words form candidates of simple keywords. 
(3) Examine the foreign word list again.  
 
Table 2. Statistics of CNA Organization Corpus 
FL Count CL FL Count CL FL Count CL
1 1,773 4.73 7 425 9.94 13 10 14.20 
2 3,622 4.98 8 223 10.50 14 6 12.00 
3 3,751 6.30 9 122 10.98 15 5 17.00 
4 2,406 7.28 10 53 11.57 16 2 14.50 
5 1,434 8.27 11 32 13.41 18 1 9.00 
6 775 8.97 12 17 12.35 20 1 15.00 
 
Those word strings that are composed of simple 
keyword candidates are candidates of compound 
keywords.  We find out the compound keyword set 
by using collocation metric by selecting the most 
frequently occurring compounds through the well-
known elimination of prepositions. 
(4)Because the experimental corpus is aligned, 
we can cluster the Chinese name list based on 
foreign keywords.  For each Chinese name cluster, 
we try to identify the Chinese keyword sets.  Here 
a bilingual dictionary may be consulted. 
The above algorithm extracts foreign/Chinese 
keyword sets from a multilingual named entity 
corpus.  In the meantime, formulation rules for 
foreign names and Chinese counterparts are mined.  
A complete foreign name and a complete Chinese 
name are mapped into name-keyword combination.  
By the way, which method, translation or 
transliteration, is used is also determined. 
Take NICT location name corpus as an 
example.  The terms of frequencies greater than 20 
include River (?, he), Island (?, dao), Lake (?, 
hu), Mountain (?, shan), Bay (?, wan), Mountain 
(?, feng), Peak (?, feng), Islands (??, qun dao), 
Mountains (??, shan mo), Cape (?, jiao), City 
(?, cheng), Range (?, ling), Peninsula (??, ban 
dao), Point (?, jiao), Strait (??, hai xia), River 
(?, chuan), Gulf (?, wan), Cape (?, jia), Pass 
(?? , shan kou), Plateau (?? , gao yuan), 
Headland (?, jia), Harbor (?, gang), Sea (?, hai), 
Promontory (?, jia), and Hills (??, qui ling).  
On the one hand, a foreign location keyword, e.g., 
?Mountain?, may correspond to two Chinese 
location keywords, e.g., ?? ? (shan) and ?? ? 
(feng).  On the other hand, the same Chinese 
location keyword ??? (feng) can be translated into 
two English location keywords ?Mountain? and 
?Peak?. 
Similarly, suffix and prefix for organization 
names can be extracted from CNA organization 
name corpus.  Some high frequent keywords are 
shown as follows. 
(1) Suffix 
Party (?, dang), Association (??, xie 
hui), University (??, da xue), Co. (??, gong 
si), Committee (???, wei yuan hui), Company 
(??, gong si), Bank (??, yia hang), etc. 
(2) Prefix 
International (??, guo ji), World (??, 
shi jie), American (??, mei guo), National (??, 
quan guo), Japan (??, ri ben), National (??, 
guo jia), Asian (??, ya zhou), etc. 
3.2 Keyword Extraction without a Bilingual 
Dictionary 
At the step (4) of the algorithm in Section 3.1, a 
bilingual dictionary is required.  Because 
abbreviation is common adopted in translation, 
dictionary-based approach is hard to capture this 
phenomenon.  A named organization ?World 
Taiwanese Association? which is translated into 
????? (shi tai hui) is a typical example.  The 
term ?World? is translated into an abbreviated term 
??? (shi) rather than a complete term ???? (shi 
jie).  Here another approach without dictionary is 
proposed.  Suppose there are M pairs of (foreign 
name, Chinese name) in a multilingual named 
entity corpus.  The jth pair, 1 ? j ? M, is denoted by 
{Ej, Cj}, where Ej is a foreign named entity, and Cj 
is a Chinese named entity.  Then some Chinese 
segment c ? Cj should be associated with some 
foreign segment e ?  Ej.  Consider the following 
examples. 
 
(s6) Aletschhorn Mountain ? ?????? 
(s7) Catalan Mountain ? ????  
(s8) Cook Strait ? ????  
(s9) Dover, Strait of ?????  
 
We will align ??? (shan) and ???? (hai xia) to 
Mountain and Strait, respectively, from these 
examples. 
We further decompose the named entities.  If 
a named entity Ej comprises m words w1?w2?wm, 
then a candidate segment ep, q is defined as wp ? wq, 
where 1 ? p ? q ? m.  If a Chinese named entity Cj 
has n syllables s1?s2?sn, then a candidate segment 
cx, y is defined as sx ? sy, where 1 ? p ? q ? n.  
Theoretically, we can get 
2
)1(
2
)1( +?+ nnmm pairs of 
{ep, q, cx, y} from {Ej, Cj}.  We then group the pairs 
collected from the multilingual named entity list 
and count the frequency for each occurrence.  
Those pairs with higher frequency denote 
significant segment pairs.  In the above examples, 
both the two pairs {Mountain, ??? (shan)} and 
{Strait, ???? (hai xia)} appear twice, while the 
other pairs appear only once. 
All the pairs {e, c} whose frequency > 2 are 
kept.  Two issues have to be addressed.  The first is: 
redundancy which may exist in the pairs of 
segments should be eliminated carefully.  If a pair 
{e, s1 s2 ? st} occurs k times, then the frequency 
of t?(t+1)/2 substrings (1 ? u ? v ? t) is at least k.  
The second is: e may be translated to more than 
one synonym, which has the same prefix, suffix, or 
infix.  In examples (s10) and (s11), ?Association? 
may be translated into ???? (xie hui) and ???
?? (lian yi hui), where ??? (hui) is a common 
suffix of these two translation equivalents, so that 
its frequency is more than the translation 
equivalents. 
 
(s10) World Trade Association ? ?????? 
(s11) North Europe Chinese Association ?  
??????? 
 
These two issues may be mixed together to make 
this problem more challengeable. 
A metric to deal with the above issues is 
proposed.  The concept is borrowed from tf?idf 
scheme in information retrieval to measure the 
alignment of each foreign segment and the possible 
Chinese translation segments.  Assume there are N 
foreign segments.  Term frequency (tf) of a 
Chinese translation segment ci in e denotes the 
number of occurrences of ci in e.  Document 
frequency (df) of ci is the number of foreign 
segments that ci is translated to.  We prefer to the 
Chinese translation segment that occur frequently 
in a specific foreign segment, but rarely in the 
remainder of foreign segments.  Besides, we also 
prefer the longer Chinese segment, so that the 
length of a Chinese segment, i.e., |ci|, is also 
considered.   
=}),({ icescore  
)1|(|log)(}),({ 2 +?? iii ccidfcef   (1) 
}),{(max
}),({}),({
jj
i
i cetf
cetfcef =   (2) 
)
)(
(log)( 2
i
i cdf
Ncidf = ,   (3) 
For some e, the corresponding Chinese segment c 
is obtained by equation (4). 
}),({maxarg i
c
cescorec
i
=   (4) 
In this way, we can produce a ranking list of pairs 
of (foreign segment, Chinese segment), which 
form multilingual keyword pairs. 
3.3 Extraction of Transformation Rules 
We apply the keyword pairs extracted in the last 
section to the original named entity list.  In (s6)-
(s9), (mountain, ? (shan)) and (strait, ?? (hai 
xia)) are significant keyword pairs.  We replace the 
non-keywords of Ej and Cj with patterns ? and ?, 
respectively, get the following rules. 
 
(s6?) ? mountain ? ? ? 
(s7?) ? mountain ? ? ? 
(s8?) ? Strait ? ??? 
(s9?) ?, Strait of ? ??? 
 
(s6?) and (s7?) can be grouped into a rule.  As a 
result, a set of transformation rules can be 
formulated.  From these examples, Chinese 
location name keyword tends to be located in the 
rightmost and the remaining part is a transliterated 
name.  On the counterpart, foreign location name 
keyword tends to be either located in the rightmost, 
or permuted by some prepositions, comma, and the 
transliterating part. 
3.4 Extraction of Keywords at a Distance 
The algorithm proposed in Section 3.2 can deal 
with single keywords and connected compound 
keywords.  Now we will extend it to keywords at a 
distance.  Consider examples (s12)-(s15) at first. 
 
(s12) American Podiatric medical Association  
? ???????? 
(s13) American Public Health Association 
? ???????? 
(s14) American Society for Industrial Security 
? ???????? 
 (s15) American Society of Newspaper Editors  
? ????????? 
 
(s12) and (s13) show that an English compound 
keyword is separated and so is its corresponding 
Chinese counterpart.  In contrast, the English 
compound keyword is connected in (s14) and (s15), 
but the corresponding Chinese translation is 
separated.  The phenomenon appears quite often in 
the translation of organization names. 
We introduce a symbol ? to cope with the 
distance issue.  The original algorithm is modified 
as follows.  A candidate segment cp, q is defined as 
a string that begins with sp and ends with sq.  Each 
syllable from sp-1 to sq-1 can be replaced by ?.  
Therefore, both ep, q and cx, y are extended to 2(p-q-1), 
and 2(x-y-1) instances, respectively.  For example, 
the following shows some additional instances for 
?American Civil Liberties Union?. 
 
?American ? Liberties Union? 
?American Civil ? Union? 
?American ? Union? 
 
The scoring method, i.e., formulas (1)-(4), is still 
applicable for the new algorithm.  Nevertheless, 
the complexity is different.  The complexity of the 
original algorithm is O(m2n2), but the complexity 
of the algorithm here is O(2m2n), where m is the 
word count for a foreign named entity and n is the 
character count for a Chinese named entity. 
The mining procedure is performed only 
once, and the mined rules are employed in an 
application without being recomputed.  Thus, the 
running time is not the major concern of this paper.  
Besides, the N is bounded in a reasonable small 
number because the length of a named entity is 
always rather shorter than that of a sentence.  Table 
2 shows that 93.88% of foreign names in CNA 
organization name corpus consist of less than 7 
words. 
4 Experimental Results 
The algorithm in Section 3.2 was performed on 
NICT location name corpus, and CNA personal 
name and organization corpora.  With this 
algorithm, we can produce a ranking list of pairs of 
(foreign segment, Chinese segment), which form 
multilingual keyword pairs.  Individual foreign 
segments and Chinese segments are regarded as 
formulation rules for foreign languages and 
Chinese, respectively.  When both the two  
Table 3. Learning Statistics 
 NICT LOC CNA ORG CNA PER
# of records in corpus 18,922 14,658 50,586
# of records for learning 5,714 12,885 100 
Vocabulary size 18,220 11,542 50,315
# of keyword pairs 122 5,229 12 
# of transformation rules 230   
# of successful records 4,262   
 
segments are considered together, they form a 
transformation rule.  Table 3 summarizes the 
results using the frequency-based approach without 
dictionary.  For named locations, there are 18,922 
records, of which, only 5714 records consist of 
more than one foreign word.  In other words, 
13,208 named locations are single words, and they 
are unique, so that we cannot extract keywords 
from these words.  Total 122 keyword pairs are 
identified.  We classify these keyword pairs into 
the following types: 
 
(1) Meaning translation 
Total 69 keywords belong to this type.  It 
occupies 56.56%.  They are further 
classified into three subtypes. 
(a) common location keywords 
 Besides the English location 
keywords mentioned in Section 3.1, 
some location keywords in other 
languages are also captured, including 
Bir ? ? (jing), Ain ? ? (quan), 
Bahr ? ? (he), Cerro ? ? (shan), 
etc. 
(b) direction (e.g., Low  ? ?  (xia), 
Central ? ? (zhong), East  ? ? 
(dong), etc.), size (e.g., Big ? ? 
(da)), length (e.g, Long ? ? 
(zhang)), color (e.g., Black ? ? 
(hei), Blue ? ? (lan), etc.) 
(c) the specificity of place or area such as 
Crystal ? ?? (jie jing), Diamond 
? ?? (zuan shi), etc.  
(2) Phoneme transliteration keywords 
Some morphemes are transliterated such as 
el ? ? (la), Dera ? ?? (de la), Monte  
? ?? (meng te), Los ? ?? (luo si), 
Le ? ? (le), and so on.  Besides, some 
common transliteration names are also 
regarded as keywords, e.g., Elizabeth ? 
???? (yi li sha bai), Edward ? ??? 
(ai de hua), etc.  Total 39 terms belong to 
this type.  It occupies 31.97%. 
(3) Some keywords in type (1) are 
transliterated.  For example, Bay ? ? 
(Bay), Beach ? ?? (bi qi), mountain ? 
?? (meng tan), Little ? ?? (li te), etc.  
Total 14 keywords (11.48%) are extracted. 
Total 230 transformation rules are mined from 
the NICT location corpus.  On the average, a 
keyword pair corresponds to 1.89 transformation 
rules.  Consider a keyword pair mountain ? ? 
(shan) as an example.  Four transformation rules 
shown as follows are learned, where ? and ? 
denote keywords for foreign language and Chinese, 
respectively; ? is a Chinese transliteration of a 
foreign fragment ?; the number enclosed in 
parentheses denotes frequency the rule is applied. 
(1) ?? ? ?? (234) 
(2) ?, ? ? ?? (45) 
(3) ?, ?? ? ?? (1) 
(4) ??? ? ?? (1) 
When we apply the 230 transformation rules back 
to the 5,714 named locations, we can tell out which 
part is transliterated and which part is translated 
from 4,262 named locations.  It confirms our 
postulation that a named location is composed of 
two parts, i.e., one is translated and the other one is 
transliterated. 
Comparatively, there are 50,586 personal 
names in CNA personal names, but only 100 
named people are composed of more than one 
word.  The number of keywords extracted is only a 
few.  They are listed below. 
De ? ? (dai), La ? ? (la), De La ? ?? 
(dai la), Van Der ? ?? (fan de), Du ? ? (du), 
David ? ?? (da wei), Khan ? ? (han), Del ? 
? (dai), Le ? ? (le), Van Den ? ?? (fan 
deng), Di ? ? (di) 
It shows that personal names tend to be 
transliterated and the CNA personal name corpus 
is suitable for training the similarity scores among 
phonetic characters (Lin and Chen, 2002). 
Finally, we consider the named organizations. 
There are 14,658 records in CNA organization 
corpus.  Total 12,885 organization names are 
composed of more than one word.  The percentage, 
87.90%, is the highest among these three corpora.  
Besides that, 5,229 keyword pairs are extracted.  
Most of the keyword pairs are meaning translated.  
This set is also the largest among the three corpora.  
Thus, the keyword pairs are too small and too large 
to find suitable transformation rules for personal 
names and organization names, respectively.  
Although the original idea of our algorithm is 
universal for languages, it should be modified 
slightly for some specific languages.  The 
following takes German as examples.  German 
words have cases and genders.  Most of German 
words are compound.  Consider examples (s16)-
(s19). 
 
(s16) Neue Osnabruecker ? ???????
(s17) Neues Deutschland ? ??? 
(s18) Bundesbahn ? ????? 
(s19) Bundesbank ? ???? 
 
The first two examples show the German adjective 
Neu (New) has different suffixes such as ?-e? and 
?-es? according to the case and gender of the noun.  
The last two examples suggest that morphological 
analysis for decompounding the words into 
meaningful segments is necessary before our 
algorithm. 
 
5 Application on CLIR 
Cross language information retrieval (CLIR) 
facilitates using queries in one language to access 
documents in another.  Because named entities are 
key components of a document, they are usually 
targets that users are interested in.  Figure 1 shows 
an application of the extracted formulation rules 
and transformation rules on Chinese-Foreign CLIR.  
For each document in the Foreign collection, 
named entities are recognized and classified by 
using formulation rules.  They form important 
indices for the related documents.  When a Chinese 
query is issued, the system extracts the possible 
Chinese named entities according to Chinese 
formulation rules.  If keywords are specified in a 
query, we know the structure and the type of the 
named entity.  The lexical structure tells us which 
part is translated and which part is transliterated.   
The backward transliteration method proposed 
by Lin and Chen (2000, 2002) was followed to 
select the most similar English named entity and 
the related documents at the same time.  In Lin and 
Chen?s approach, both Chinese name and English 
candidates will be transformed into a canonical 
form in terms of International Phonetic Alphabets.  
Similarity computation among Chinese query term 
and English candidates are done on phoneme level.  
Figure 1.  A Chinese-Foreign CLIR System 
Foreign 
Document 
Collection 
Query 
Translation/ 
Transliteration 
Information 
Retrieval 
System 
Relevant
Documents
Named Entity
Extractor
Transliteration 
Knowledge 
Bilingual 
Dictionary 
Chinese Query
Chinese-Foreign
Transformation
Rules
Chinese
Formulation
Rules
Foreign
Formulation
Rules
Rule 
Miner 
Multi-Lingual 
Named Entity 
Corpora 
Named Entity
Extractor
That is an expensive operation.  Hopefully, the 
type of Chinese named entity will help to narrow 
down the number of candidate.   
6 Conclusion and Remarks 
This paper proposes corpus-based approaches to 
extract the formulation rules and the translation/ 
transliteration rules among multilingual named 
entities.  Simple frequency-based method identifies 
keywords of named entities for individual 
languages and their correspondence.  The modified 
tf?idf scheme deals with the issues of abbreviation 
and compound keyword at a distance. 
Since the corpora are already phrase-aligned, 
the mined rules cover at least a significant number 
of instances.  That is, they seem to be significant, 
but further evaluation is needed.  Two types of 
evaluation are being conducted, i.e., direct and 
indirect approaches.  In the former, we will 
partition the corpora into two parts, one for 
training and the other one for testing.  In the latter, 
we are integrating our method in a cross language 
information retrieval system.  Given a query 
consisting of Chinese named entity, the Chinese 
formulation rules will tell us its type and lexical 
structures.  The transformation rules show which 
parts should be translated and transliterated.  Our 
previous works on phoneme transliteration is 
integrated.  The transformation result may be 
submitted to an information retrieval system to 
access documents in another language.  In the 
ongoing evaluation, the test bed is supported by 
CLEF (2003).  The result will be reported in 
CLEF2003 after evaluation by CLEF organizer.  
Further applications will be explored in the future 
and the methodology will be extended to other 
types of named entities. 
 
References 
Al-Onaizan, Yaser and Knight, Kevin (2002) 
?Translating Named Entities Using Monolingual and 
Bilingual Resources,? Proceedings of 41st Annual 
Meeting of Association for Computational Linguistics, 
2002, pp. 400-408. 
Chen, Hsin-Hsi and Lee, Jen-Chang (1996) 
?Identification and Classification of Proper Nouns in 
Chinese Texts,? Proceedings of 16th International 
Conference on Computational Linguistics, 1996, pp. 
222-229. 
Chen, Hsin-Hsi; Ding, Yung-Wei and Tsai, Shih-Chung 
(1998) ?Named Entity Extraction for Information 
Retrieval,? Computer Processing of Oriental 
Languages, Special Issue on Information Retrieval 
on Oriental Languages, 12(1), 1998, pp. 75-85. 
Chen, Hsin-Hsi et al (1998) ?Proper Name Translation 
in Cross-Language Information Retrieval,? 
Proceedings of 17th COLING and 36th ACL, pp. 232-
236. 
CLEF (2003) Cross-Language Retrieval in Image 
Collections, Pilot Experiments, 2003. 
Hirschman, L.; Park, J.C.; Tsujii, J.; Wong, L. and Wu, 
C.H. (2002) ?Accomplishments and Challenges in 
Literature Data mining for Biology,? Bioinformatics, 
18(12), pp. 1553-1561. 
Knight, Kevin and Graehl, Jonathan (1998) ?Machine 
Transliteration,? Computational Linguistics, 24(4), 
pp. 599-612. 
Lin, Chuan-Jie; Chen, Hsin-Hsi; et al (2001) ?Open 
Domain Question Answering on Heterogeneous 
Data,? Proceedings of ACL Workshop on Human 
Language Technology and Knowledge Management, 
2001, pp. 79-85. 
Lin, Wei-Hao and Chen, Hsin-Hsi (2000) ?Similarity 
Measure in Backward Transliteration between 
Different Character Sets and Its Application to 
CLIR,? Proceedings of Research on Computational 
Linguistics Conference XIII, pp. 79-113. 
Lin, Wei-Hao and Chen, Hsin-Hsi (2002) ?Backward 
Machine Transliteration by Learning Phonetic 
Similarity,? Proceedings of 6th Conference on 
Natural Language Learning, 2002. 
MUC (1998) Proceedings of 7th Message 
Understanding Conference, 1998, 
http://www.itl.nist.gov/iaui/894.02/related_projects/
muc/index.html. 
Wan, Stephen and Verspoor, Cornelia Maria (1998) 
?Automatic English-Chinese Name Transliteration 
for Development of Multilingual Resources,? 
Proceedings of 17th COLING and 36th ACL, pp. 
1352-1356. 
 







Induction of Classification from Lexicon Expansion : 
Assigning Domain Tags to WordNet Entries 
 
Echa Chang*, Chu-Ren Huang**, Sue-Jin Ker***, Chang-Hua Yang*** 
*University of Waterloo, 200 University Ave. W., Waterloo, ON  N2L 3G1 Canada 
cecha@yahoo.com 
**Institute of Linguistics, Academia Sinica, Nankang, Taipei, 115, Taiwan 
churen@sinica.edu.tw 
***Soochow University 
ksj@sun.cis.scu.edu.tw, changhua@mail2000.com.tw 
 
Abstract 
We present in this paper a series of 
induced methods to assign domain tags to 
WordNet entries. Our prime objective is 
to enrich the contextual information in 
WordNet specific to each synset entry. By 
using the available lexical sources such as 
Far East Dictionary and the contextual 
information in WordNet itself, we can 
find a foundation upon which we can base 
our categorization. Next we further 
examine the similarity between common 
lexical taxonomy and the semantic 
hierarchy of WordNet. Based on this 
observation and the knowledge of other 
semantic relations we enlarge the 
coverage of our findings in a systematic 
way. Evaluation of the results shows that 
we achieved reasonable and satisfactory 
accuracy. We propose this as the first step 
of wordnet expansion into a bona fide 
semantic network linked to real-world 
knowledge. 
 
0. Introduction1 
WordNet is a lexicon comprising of nouns, 
verbs, adjectives and adverbs. Its basic 
                                                   
1
 This research is partially funded by  an IDLP project grant 
from the National Science Council of Taiwan, ROC. Work 
reported in this paper was carried out in summer 2001, 
during Chang's internship at Academia Sinica. We are 
indebted to two anonymous reviewers of SemaNet 2002, as 
well as from the First International WordNet Conference for 
their helpful comments. An earlier version of this paper was 
accepted by the first IWC but was not presented because of 
the authors' travelling difficulties at that time. We thank 
colleagues at Academia Sinica, especially Shu-Chuan Tseng, 
Keh-jiann Chen, and members of the WordNet group, for 
their input and help. 
organization is based on different semantic 
relations among the words.  Entries (or lemas) 
sharing the same meaning is grouped into a synset 
and assigned with a unique sense identification 
number for easy retrieval and tracking purposes.  
This unique offset number gives the information 
about the parts of speech and the hierarchy 
position to which a specific synset belongs.  For 
nouns and verbs the synsets are grouped into 
multiple lexical hierarchies; modifiers such as 
adjectives and adverbs are simply ?organized into 
clusters on the basis of binary opposition 
(antinomy).? [1]  This lexical hierarchy makes the 
lexical domain assigning task more 
straightforward because it coincides with a 
ontological taxonomy in many aspects.  The 
primary objective of our project is to enrich the 
WordNet knowledge content due to the fact that 
?WordNet lacks relations between related 
concepts.? [2] We adopt WordNet itself, together 
with other lexical resources to develop an 
integrated domain specific lexical resource. 
 
1. The Five Tagging Methods 
Starting with two lexical resources, we 
employed five steps to assign and expand domain 
tags. Basically, the explicit domain information 
from Far East Dictionary as well as WordNet's 
own hierarchy of semantic relation are used to 
extend the coverage of domain - assignment. 
 
1.1 Domain Data Lookup from Far East 
Dictionary 
The digital file of Far East Dictionary 
contains complete information for each word 
entry that can be found in an ordinary printed 
version. Most of all, it lists the domain 
information for each vocabulary wherever 
possible.  Thus we employ the available data from 
a text source file (each vocabulary entry is 
organized as one single row) and extract all the 
information by running a string manipulation 
program coded in Visual Basic.  During the 
extraction process we only take into account the 
part of speech of each word in Far East Dictionary. 
Next, we map the domains obtained from Far East 
Dictionary if the word and its part of speech 
coincides with the entries in our database which 
contains a complete list of synset.  Since  
WordNet collects only nouns, verbs, adjectives 
and adverbs, we only extract the domain data that 
falls into these four categories.  Later we group 
the information in a database table and extent the 
assigned domains of each word to its synset.  
Table 1 is an example of our database table which 
'contains all the adverbial uses of `aback.' 
 
id term domain 
00073303R aback aviation, 
00073386R aback aviation, 
Table 1 Example of The Far East Dictionary 
Domain Database Table 
 
In Table 1, it is shown that 'aback' has two 
adverbial senses. Since in Far East Dictionary 
?aback? is labeled with domain 'aviation,' extra 
work of expansion is necessary to further label all 
of its adverb synset with the same domain to 
maintain the integrity of the information. Because 
both the extraction and expansion method would 
produce ambiguities in domain assignment, 
manual verifications are required in the future. 
 
1.2 Extracting Domain Information from 
WordNet Sense Description 
Each WordNet entry (i.e. each synset) is 
followed by its sense. Although there is no 
specifically defined set of controlled vocabulary, 
the sense definition does specify the field the 
synset members are commonly used in that 
specific field of study, such as biology, physics or 
chemistry. This specification comes in a special 
format contained in a bracket for each WordNet 
entry so that extraction of data is possible and 
straightforward.  Due to the fact that each domain 
is directly extracted by its corresponding synset, 
there is simply no ambiguity in assigning the 
domain tags.  And if there is more than one lexical 
item in that synset, all will share the same domain 
tag. 
 
1.3 Establishment of a Common Domain 
Taxonomy for Nouns  
Each lexical resource uses a different domain 
taxonomy, which may be explicitly defined or 
implicitly assumed. Hence, when combining 
domain information from multiple sources, the 
establishment of a Common Domain Taxonomy 
(CDT) is crucial for both efficient representation 
as well as effective knowledge merging. Our 
survey of existing domain taxonomy, including 
LDOCE, HowNet, Tongyici Cilin, etc., show that 
there is quite a lot in common. Hence we decide to 
build a working CDT based on the two resources 
we have. Note that since our goal is to establish a 
domain taxonomy for wordnets (for English now 
and for Chinese in the future), the existing domain 
information in WordNet need to be assumed as 
defaults that can be over-ridden. Hence a model of 
CDT based on basic binary combination 
involving WordNet is necessary. 
After collecting all the domain tags from the 
two resources, we build our CDT. First, all 
common domain nodes are put in a hierarchy 
based on their relation. Second, inconsistent 
domain names are resolved. Last, when gaps 
appear after all domain tags are attached to the 
taxonomy, new domain categories are adopted to 
fill in the gaps and make a more complete CDT. 
Since top taxonomy presupposes a particular view 
on conceptual primacy and may differ in different 
lexical sources, we took a bottom-up approach to 
our CDT. That is, right now each taxonomy tree 
now stops at some broad-consensus level without 
being committed to a higher taxonomy. The 
following is a partial list of our current CDT. 
 
Humanity 
 
Linguistics   
 
Rhetorical Device   
 
Literature   
 
History   
 
Archeology   
? 
Social Science     
 
Sociology   
 
Statistics   
 
Economics   
 
Business   
 
Finance     
? 
Formal Science   
 
Mathematics   
 Geometry   
 
Algebra   
? 
Natural Science   
 
Physics   
 
Nuclear   
 
Chemistry   
 
Biology   
 
Palaeontology   
 
Botany   
 
Animal   
 
Fish   
 
Bird   
 ? 
Applied Science 
 
Medicine   
 
Anatomy   
 
Physiology   
 
Genetics   
 
Pharmacy   
 
Agriculture   
 ? 
Fine Arts 
 
Painting   
 
Sculpture   
 
Architecture   
 
Music   
 
Drama   
 ? 
Entertainment 
 
Sports   
 
Balls   
 
Track & Field   
 
Competition   
 
Game   
 
Board   
 
Card   
? 
Proper Noun 
 
Name   
 
Geographical Name   
 
Country   
 
Religion   
 
Trademark   
 ? 
Humanity 
 
Archaic   
 
Informal   
 
Slang   
 
Metaphor   
 
Formal   
 
Abbreviate   
 ? 
Lexical Sources 
 
Latin   
 
Greece   
 
Spanish   
 
French   
 
American   
 ? 
Please note that by induction and actual 
examples from the lexical organization in 
WordNet, it is found that a hyponym is very likely 
to belong to the same domain as its hypernym. 
Similar results are also found for wordnet based 
cross-lingual inference of lexical semantic 
relations [4]. For instance, under the term 
'mathematics,' all the hyponyms below are related 
to this field of study. To make us of this lexical 
semantic phenomenon, we make a table of all the 
domain terms and map them to their unique 
WordNet sense identification number.  Later we 
use the tree expansion method (discussed in more 
detail in Section 2.4) to trace down all the 
hyponyms.  For example, by using this method, 
the hyponyms of  Linguistics  are all labeled as 
'linguistics' and so forth. 
 
1.4 Lexical Hierarchy Expansion of 
Nominal Domain Assignment 
WordNet has is a lexical semantic hierarchy 
linking all synsets with lexical semantic relations. 
We convert all the relations to a database in a 
relational table, as shown in Table 2 [1]: 
 
Hypernym ID Hyponym ID Relation 
00001740A 04349777N = 
00001740A 00002062A ! 
00001740N 00002086N ~ 
? ? ? 
Table 2 Lexical Relation Table 
The relation symbols in Table 2 are adopted from 
the WordNet database files. These symbols are 
saved with each synset entry to indicate a specific 
semantic relation with other synsets. The 
implemented information allows us to trace and 
locate all the related synsets. 
 
 
 
 
WN Relation Symbol 
 
Antonym: ! 
Hyponym: ~ 
Hypernym: @ 
Meronym: # 
Holonym: % 
Attribute: = 
 
Table 3 Relations and Pointer Symbols 
 
  By manipulating Table 2 with SQL, all 
nouns can be traced to the eight unique beginners.  
 
Unique Beginners of Nouns In WordNet 
Entity,something 
Abstraction 
Act,human action,human activity 
State 
Event 
Group,grouping 
Phenomenon 
Possession 
Table 4. The Eight Unique Beginners for Nouns 
 
The general structure of tree expansion can be 
visualized as Figure1: 
 
 
1st 
 
 
 
 
? 
Figure 1. Example of Tree Expansion for Nouns 
 
This form of data presentation makes inspection 
and observation on the hierarchy among nouns 
more straightforward. After careful and 
systematic examination, domain assignment is 
trickled down to each synset level by level. The 
same task is performed up to the fifth level. A tree 
traversal program is executed to trace down the 
hyponyms and assign domain-tag based on its 
hypernyms. 
 
1.5  Relational Expansion of Other Parts 
of Speech 
The hierarchy expansion method based on 
taxonomy mainly applies to nouns.  For modifiers 
such as adjectives and adverbs this general 
observation does not produce a satisfactory result  
since ?[t]he semantic organization of modifiers in 
WordNet is unlike?the tree structures created by 
hyponymy for nouns or troponymy for verbs.? [1] 
However since adverbs/adjectives are often 
morphologically derived from other major 
categories, such information can be used to infer 
domain classification. For example, the adjective 
'stellar' is derived directly from the noun 'star.' 
The term, 'star' is mostly mentioned in an 
astronomical context.  Based on this relation, 
since 'star' is labeled with 'astronomy' based on 
Lexical Hierarchy, the adjective 'stellar' can be 
assigned with the same domain.   We combine  the 
tables on the left side and right side of Table 2 
Lexical Relation Table to obtain a table organized 
as follows: 
 
 
 
 
 
 
Figure 2. JOIN Method 
 
Later the recordsets that have the relation symbol 
as ?\?(denoted ?derived from,? refer to Table 2) 
are extracted and these derived adjectives and 
adverbs are further assigned with the same 
domain as the nouns they are derived from. 
 
Results 
There are 99,642 unique senses organized by 
WordNet. By expanding each specific vocabulary 
coupling with its specific senses, the number of 
these ?word & sense? unique pairs total up to  
173,941, which is the basis for all the results. 
 
Parts of Speech Percentage in Total 
Noun 66.87 % 
Adjective 17.18 % 
Verb 12.69 % 
Adverb     3.27 % 
Table 5. Percentage of Each Part of Speech in      
The 173,941 ?Word & Senses Pairs? Entries 
 
1.6 Far East Dictionary 
There are 20,126 senses that have been assigned 
with a domain tag with Far East Dictionary, which 
account for 20.20 % of the total senses (99,642 in 
total in WordNet).  However after expanding it to 
its synset the total 'word & sense' pairs, there are 
42,643 entries being tagged, which account for 
24.52 % of the 173,941 pairs in total. 
 
Entity 
 
cell 
object unit 
 
Domain 
Tagged 
Noun ID 
morpho-lo
gical 
Relation 
Un-tagged 
Adj/Adv 
ID 
2nd 
Parts of Speech Number Tagged Synset Coverage 
Noun 29,946 17.22 % 
Adjective   6,188  3.56 % 
Verb   6,160  3.54 % 
Adverb     349  0.20 % 
Table 6. Coverage by POS 
 
 	
