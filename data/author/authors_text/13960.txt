Linguistically enriched corpora for establishing variation in support verb
constructions
Begon?a Villada Moiro?n
Alfa-Informatica
University of Groningen
P.O.Box 716
9700 AS Groningen
M.B.Villada.Moiron@rug.nl
Abstract
Many NLP tasks that require syntactic
analysis necessitate an accurate de-
scription of the lexical components,
morpho-syntactic constraints and the
semantic idiosyncracies of fixed ex-
pressions. (Moon, 1998) and (Riehem-
ann, 2001) show that many fixed ex-
pressions and idioms allow limited vari-
ation and modification inside their com-
plementation.
This paper discusses to what extent a
corpus-based method can help us estab-
lish the variation and adjectival modi-
fication potential of Dutch support verb
constructions. We also discuss what
problems the data poses when apply-
ing an automated data-driven method to
solve the problem.
1 Introduction
We aim at finding methods that facilitate the de-
scription of the linguistic behavior of multiword
expressions. Empirical evidence and generaliza-
tions about the linguistic properties of multiword
expressions are required to further a theory of
fixed expressions (or multiword expressions) as
well as to expand the coverage of NLP lexical re-
sources and grammars.
This paper describes an attempt to develop
automated methods for induction of lexical in-
formation from a linguistically enriched corpus.
In particular, the paper discusses to what extent
can an automated corpus-based approach be use-
ful to establish the variation potential of support
verb constructions. The experimental work ap-
plies to Dutch expressions, however the issue is
widely relevant in the development of lexical re-
sources for other languages.
1.1 Partially lexicalized expressions
Corpus-based studies showed that certain fixed
expressions and idioms allow limited vari-
ation and adjectival modification (Moon, 1998;
Riehemann, 2001).1 Riehemann (2001) invest-
igated various types of multiword expressions in
English and observed that around 25% of idiom
occurrences in a corpus allow some variation. By
way of example, among the occurrences of the
idiom keep tabs on ?(fig.) watch?, variation affects
verb tense inflection, adjective modifiers (close,
better, regular, daily), noun number morpheme
(tab(s)) and the location of the on complement
phrase that may be separate from the object NP.
The above example is by no means an isolated
case.
Variation has an effect not only on the rep-
resentation of the syntactic structure but also on
the semantic interpretation of the multiword ex-
pression (Sag et al, 2001; Baldwin et al, to ap-
pear). The presence of variation in multiword ex-
pressions brings up two scenarios: (a) the loss of
the peculiar meaning or (b) the modification of
the original meaning. Returning to the example
above, modifiers of tabs affect the interpretation
of the event predicate as a whole. Thus, keep
1From now onwards, we use ?variation? to refer to mor-
phological productivity or alternation of specifi ers or pre-
nominal modifi ers.
63
close tabs on s.o. means ?watch s.o. closely?. A
different effect has been reported of some VERB
NP idioms in which the adjectival modification af-
fects only the complement NP (Nicolas, 1995).
For a correct interpretation, such idiomatic ex-
pressions require internal semantic structure.
These observations suggest that: (i) not all
fixed expressions and idioms are frozen word
combinations given that, parts of the expression
participate in syntactic operations; (ii) some lex-
emes (in ?fixed? expressions) are subject to mor-
phological processes; and (iii), some fixed ex-
pressions still preserve underlying semantic struc-
ture. A description that captures the previous
facts needs to allow variable slots so that the men-
tioned variants of the expression are licensed by
the grammar. In sum, variation is a property that
should not be neglected while deciding the lexical
representation of multiword expressions in com-
putational resources.
1.2 Support verb constructions
Support verb constructions are made up out of a
light verb (aka. support verb) and a complement
(e.g. take into account). The predicational com-
plement may be realized as a noun, an adjective or
a prepositional phrase. The light verb and its com-
plement form a complex predicate, in which the
complement itself supplies most of the semantic
load (Butt, 1995). The verb performs a ?sup-
port? function, i.e. it serves to ?further structure
or modulate the event described by the main pre-
dicator? (Butt, 1995). Most researchers agree that
the light verb adds aspect, tense and ?aktionsart?
information to the predicate. Since the support
verb?s meaning differs from the meaning of the
(main) verb lexeme, the meaning of the support
verb construction is not fully compositional. Due
to the similarities with other idiosyncratic expres-
sions, support verb constructions (LVCs) belong
to the group of lexicalized multiword expressions
(Sag et al, 2001).
We limit this study to support verb construc-
tions for two practical reasons. First, there seems
to be a group of core light verbs that exist cross-
linguistically. Thus, we can concentrate on a
small set of verbal lexemes. Second, these light
verbs are based on main verbs still in active use
in the language (Butt, 1995). Concerning Dutch,
nine verbs that can function as main but also as
light verbs are brengen ?bring?, doen ?do?, gaan
?go?, geven ?give?, hebben ?have?, komen ?come?,
krijgen ?get?, maken ?make?, nemen ?take? and
stellen ?state? (Hollebrandse, 1993). Establishing
the lexical properties of light verb predicates is
necessary so that parsers do not misanalyze main
verb and light verb uses.
Before we describe a corpus-based method to
extract evidence of variation from a syntactically
annotated corpus, we enumerate some research
assumptions and highlight the types of variation
and modification object of this study. Section 3
presents the automated method and the evaluation
of its merits. Section 4 describes a proposal of the
required lexical annotation drawn from a working
implementation. Our conclusions and further im-
provements are summarised in section 6.
2 Base form, variation and modification
In addition to a subject, some prepositional sup-
port verb constructions select an additional com-
plement. This may be realized by an accusat-
ive, dative or reflexive NP. Prior to applying the
corpus-based method described in section 3, we
partly ignore the lexical content within the PP
complement; this is also why we want to estab-
lish the variation potential within LVCs. For the
above two reasons, we assume that the minimum
required lexemes (i.e. common to all preposi-
tional LVCs) include the argument PP and the sup-
port verb and represent each expression as a triple
of the form [PREPOSITION NOUN VERB] (P N V).
(Thus, determiners and modifiers are left out).
Some further assumptions must be introduced,
namely, what we understand as a base form and as
a variant of a support verb construction. The base
form includes the mentioned triple and may in-
clude other lexicalized arguments. In expressions
that allow no morphosyntactic variation or modi-
fication within the required arguments, tense in-
flection is usually possible. The base form shows
the infinitive verb form. The base form of the ex-
pression voet bij stuk houden ?stick to one?s guns
(fig)? includes the noun voet, the PP bij stuk and
the verb houden; tense inflection is possible (1-b).
(1) a. VOET BIJ STUK HOUDEN
64
b. De
the
verzekeraars
insurers
hielden
kept
echter
really
voet
foot
bij
by
stuk.
piece
?The insurance companies really sticked to
their guns (fig.)?
Any instance of an LVC whose NP within the PP
argument differs from the NOUN lexeme is con-
sidered a variant. The expression uit zijn dak
gaan ?go crazy? has as base form (2-a) with the
noun dak allowing various possessive determiners
(2-b).
(2) a. UIT DAK GAAN
b. Het
the
publiek
audience
ging
went
uit
out
zijn
his
dak.
roof
?The audience went crazy.?
We study variation observed within the expres-
sion. We focus on two levels:
lexeme level productive inflectional and deriva-
tional morphology.
phrase level variability in specifiers and modifi-
ers.
The evidence we seek to extract is the follow-
ing: (a) use of diminutive in nominal lexemes; (b)
singular and plural alternation in nouns. Evid-
ence of derivational morphology, for example,
instances of compounding (another noun or an
acronym prefixed to the head noun) or a genit-
ive noun modifier; (c) alternation in specifiers.
Among the specifiers: zero determiner, definite,
indefinite, reciprocals, possessives, demonstrat-
ives and quantifiers; (d) NPs that are realized by
reflexives. Reflexives may instantiate either open
argument slots or an NP within complement PPs;
and (e), among modification, we explore pre-
nominal adjectives, past participles, gerunds and
other intervening material.
In addition, some expressions allow relative
clauses and PP post-nominal modifiers. Relat-
ive clauses are observed less often than PP post-
nominal modifiers. So far, we ignore these two
types of modification because we extract the evid-
ence from an automatically annotated corpus and
with automated means. It is well-known that dis-
ambiguating a syntactic attachment site, e.g. a
PP?attachment site, is one of the hardest problems
for present-day parsing technology. Needless to
say, the parser (Alpino) also encounters diffi-
culties with this problem. In this work, we did
not investigate syntactic flexibility at the sentence
level, that is, processes such as passive, topicaliz-
ation, control, clefting, coordination, etc.
3 A corpus-based method to infer
variation
With access to automatically parsed data, subcat-
egorization frames and a standard search query
language such as dt search, we can extract all
instances of an LVC that satisfy rather specific
morphosyntactic features and head-complement
dependencies; these requirements ? expressed
as dt search queries ? are applied to XML-
encoded syntactic dependency trees. For a more
detailed description of the corpus-based method
refer to (Villada Moiro?n, 2005).
3.1 Corpus annotation
A list of P N V triples was automatically acquired
from a syntactically annotated corpus using col-
location statistics and linguistic diagnostics (Vil-
lada Moiro?n, 2004). A P N V triple represents an
abstraction of a support verb construction (LVC).
For each automatically extracted triple, all sen-
tences containing the three component lexemes
found in the Twente Nieuws Corpus (TwNC) (Or-
delman, 2002) were collected in a subcorpus. For
example, for the expression uit zijn dak gaan ?go
crazy?, all sentences that include the preposition
uit ?out?, the noun dak ?roof? and the verb gaan
?go? or one of its inflectional variants are collec-
ted in a subcorpus.
The Alpino parser (van der Beek et al, 2002)
was used to annotate the subcorpora. This is a
wide-coverage parser for Dutch. Based on a lexic-
alist constraint-based grammar framework (Head-
Driven Phrase Structure Grammar) (Pollard and
Sag, 1994), the Alpino grammar licenses a wide
variety of syntactic constructions.
All parsed data is stored as XML-dependency
trees. To illustrate the annotation, the result of
parsing example (2-b) is the dependency structure
tree shown in figure 1.
Among the information contained in the parsed
trees, we use: (i) categorical information (phrasal
65
top
smain
su
np
det
det
het/[0,1]
hd
noun
publiek/[1,2]
hd
verb
ga/[2,3]
ld
pp
hd
prep
uit/[3,4]
obj1
np
det
det
zijn/[4,5]
hd
noun
dak/[5,6]
Het publiek ging uit zijn dak .
Figure 1: This syntactic dependency tree corres-
ponds to the parsed sentence in (2-b).
(np, pp) and lexical (det, noun)), (ii) syntactic
information (grammatical function or dependency
relation (subject su, direct object obj1, locative
or directive complement ld, head hd, determiner
det)) and (iii) lexical information (lexemes and
word forms). Dependency nodes are crucial in
stating daughter?ancestor relations between con-
stituents and sub-constituents in an LVC.
3.2 Extraction
dt search (Bouma and Kloosterman, 2002), a
treebank query tool based on XPATH,2 is used
to extract evidence from the annotated subcor-
pora. A dt search query applied on the corres-
ponding parsed subcorpus searches for all LVC in-
stances. Two types of queries are needed: narrow
search and wide search queries. Narrow search
queries seek instances of a head-dependent rela-
tion between a VERB and a PP sibbling, given
necessary lexical restrictions as input. Wide
searches state that the PP is embedded (some-
where) under a clausal node whose head is VERB.
Wide searches are needed because the parser may
wrongly attach the sought PP to a previous noun.
(Thus, in the annotated data the PP and VERB do
2Nevertheless, other XML-based query tools are also
freely available, e.g. XSLT or the TIGERSearch kit.
not satisfy a head-dependent relation). Finally,
the vaguest search states that a given PP needs to
occur within the same sentence as the verb. This
type of search is used in case the other two types
fail to retrieve any evidence. The query in figure 2
seeks NP-internal adjectival modification.
dt_search
?//node[@cat="np" and
./node[@cat="ap"] and
./node[@rel="hd" and
@root="gedachte"] and
../node[@rel="obj1"] and
../node[@rel="hd" and @word="op"
and
(../../../node[@rel="hd" and
@root="breng"] or
../../node[@rel="hd" and
@root="breng"] or
../node[@rel="hd" and
@root="breng"]) ] ]?
breng.opgedachten/*.xml
Figure 2: Query to extract adjectives in the ex-
pression iemand op gedachten brengen.
Among the constraints expressed in the search
queries there are: parent-child relations between
nodes, phrase category (@cat), dependency rela-
tion (@rel), word base form (@root) or surface
form (@word). Queries need to capture deeply
embedded LVCs. A verbal complement embedded
under several modal or auxiliary verbs is rather
common. To allow uncertainty about the location
of the PP argument node with respect to its head
verb, disjunctive constraints are introduced in the
queries (figure 2).
3.3 Retrieved corpus evidence
A search query retrieves each LVC realization that
satisfies the query requirements, as well as the
LVC frequency in the subcorpora.
Figure 3 gives an excerpt from the observed
adjectival modification in iemand op gedachten
brengen ?give s.o. the idea?. Op andere gedachten
brengen ?change s.o.?s idea? is the most frequent
realization with 634 out of a total of 682 occur-
rences. This suggests that the adjective andere is
almost frozen in the expression.
The method extracts evidence of morpholo-
gical productivity, variation of specifiers and ad-
jectival modification, i.e. positive and negative
evidence. A description of the positive evidence
66
1 aangename gedachten
1 amoureuze gedachten
634 andere gedachten
1 andere politieke gedachten
1 andere, redelijke gedachten
1 beeldende gedachten
1 bepaalde gedachten
2 betere gedachten
1 duivelse gedachten
1 heel andere gedachten over...
1 hitsige gedachten
1 hogere gedachten
1 kritische gedachten
1 meer poe?tische gedachten
Figure 3: Observed adjectival modification in the
LVC iemand op gedachten brengen.
follows. We investigated 107 Dutch LVCs: 94 ex-
pressions that require a PP argument among which
some show an NPacc open slot; lexical restrictions
affect the verb and the PP argument; in addition,
13 other expressions are made up of a (partially)
lexicalized NP and a PP argument.
LVCs fall in one of three groups: (a) totally
fixed, (b) semi-fixed and (c) flexible. Fixed
LVCs show no variation and no modification in
the lexicalized NP (if present) and PP constitu-
ent(s). 42% of the LVCs studied are fixed. Semi-
fixed LVCs show partially lexicalized constitu-
ent(s) (20.5% of the studied LVCs). Rarely, a sin-
gular noun appears in plural. Variation affects the
lexeme?s morphology and/or the specifiers slot.
Expressions whose lexicalized argument requires
a reflexive are included into this group. Flexible
LVCs allow adjectival modification (37.5% of the
studied LVCs). The data is rather varied. There are
LVCs that show: (i) non-productive morphology
and no specifier variation but they show a limited
number of adjectives and, (ii) specifier variation
(some show compounding) and limited adjectival
variation. Border-line cases exhibit no morpho-
logical productivity and either definite/possessive
determiner alternation or no specifier variation;
modification involves a unique adjective (e.g. in
(verzekerde) bewaring stellen ?put into custody?).
Negative evidence (noise) typically includes
sentences where the VERB and the PP occur
within the same clause but not in the LVC con-
text (in its literal use). Often, the PP is an adjunct
or a complement of another verb. The reason for
this noise can be attributed to the uncertainty in
the search queries or errors in the annotated data.
3.4 Discussion
We argue that the corpus-based method is effi-
cient in extracting the linguistic contexts where
variation and internal modification are found in-
side LVCs. Examining the evidence retrieved by
the corpus-based method, a researcher quickly
forms an impression about which expressions are
totally fixed and which expressions allow some
variation and/or modification. One also has direct
access to the realizations of the variable slots, the
LVC frequency and relevant examples in the cor-
pus. Next, we discuss some limitations posed by
the corpus annotation, extraction procedure and
the nature of the idiosyncratic data.
Finding specific constructions in corpora of
free word order languages such as Dutch is not
trivial. Corpus annotation enriched with gram-
matical functions and/or dependency relations fa-
cilitates the search task.3 Thus, we are able to
explore LVC occurrences in any syntactic struc-
ture (main or subordinate sentence, questions,
etc.) without stating linear precedence con-
straints. Furthermore, in most sentences, the an-
notation correctly identifies the clause containing
the LVC thus, granting access to all sibblings of
the head verb.
In general, knowledge of the grammar and the
lexicon used by the parser is helpful. In particu-
lar, knowing whether some LVCs or idiosyncratic
phrases are already annotated in the lexicon as
lexicalized phrases helps. In the event that an LVC
were described in the lexicon, the parser either
analyzes the expression as an LVC or as a regular
verb phrase. This uncertainty needs to be taken
into account in the extraction queries.
The corpus-based method requires information
about the subcategorization requirements of the
LVCs. This information was manually entered
for each expression. Once we have a list of
PREPOSITION NOUN VERB triples, methods de-
scribed in the literature on automatic acquisition
of subcategorization information might be suc-
cessful in finding out the remaining LVC syntactic
requirements. This is an open issue for future re-
3Preliminary experiments were done on chunked data. A
corpus-based method applied on phrasal chunks was imprac-
tical. A lot of noise needed to be manually discarded.
67
search, but a starting point would be the approach
by (Briscoe and Carroll, 1997).
The success of the search queries is dependent
on parsing accuracy. Sometimes extracted evid-
ence shows the specific PP we seek but misan-
alyzed as a dependent of another verb. Parsing
accuracy introduces another shortcoming: evid-
ence of relative clauses and PP post-nominal mod-
ifiers cannot be automatically retrieved. Because
of structural ambiguity, attachment decisions are
still a hard parsing problem. This led us to ignore
these two types of modification in our research.
Some limitations due to the nature of the
support verb constructions emerged. Specifier
changes or insertion of modification may destroy
the LVC reading. The queries could extract evid-
ence that looks like a variant of the LVC base
form; in practice, the LVC interpretation does not
apply. For example, in most of the instances of the
expression de hand boven het hoofd houden ?to
protect s.o.? (lit. the hand above the head hold),
hoofd is preceded by the definite determiner;
there are also a few instances with a reciprocal
elkaars ?each other?s? and some instances with
possessive determiners. The query results sug-
gest that all three specifiers are possible; however,
the instances with possessive determiners are lit-
eral uses. Occasionally, a PREPOSITION NOUN
VERB triple clusters homonymous expressions. A
search that specifies the triple base form IN HAND
HOUDEN could match any of the following: iets
in e?e?n hand houden ?to be the boss?, het heft in
handen houden ?remain in control?, de touwtjes in
handen houden, iets in handen houden ?have con-
trol over sth? or iets in de handen houden ?to hold
sth in one?s hands (lit.)?. Access to the subcat-
egorization requirements of the LVC use (that dif-
fers from those of the regular phrase) (e.g. iemand
van de straat houden ?keep s.o. off the street? vs.
van de straat houden ?to love the street?) would
solve some cases.
The corpus-based method cannot be fully auto-
mated; that is, extraction of variation and modi-
fication evidence cannot be done fully automat-
ically. Instead, the evidence retrieved needs to
be manually inspected. This brings up a last
limitation of the method. At least one instance
of each variation and modification type requires
manual inspection. The researcher needs to es-
tablish whether the LVC interpretation is present
or only a literal reading applies. Yet, all the tools
we used facilitated this process and they provide
plenty of relevant linguistic empirical evidence.
A last limitation affecting most corpus-based
research is that having found no evidence of vari-
ation and modification does not mean that it is not
possible in LVCs. Some LVCs are rare in the cor-
pus; LVCs that exhibit variation and/or modifica-
tion are even more infrequent. A larger corpus is
desirable.
4 Lexicon representation in Alpino
The Alpino lexicon entries specify (if applicable)
subcategorization frames enriched with depend-
ency relations and some lexical restrictions. Sup-
port verb constructions and idiomatic expressions
are treated similarly; neither of these expressions
constitute a lexical entry on their own (cf. (Breidt
et al, 1996)). We concentrate on the LVC annota-
tion in the remainder.
Support verb constructions are lexicalized
combinations of a support verb. Main verbs ex-
hibit the same form (lemma) as their related sup-
port verb. We distinguish between a main verb
and a support verb by specifying the distributional
context of the support verb. This context is cap-
tured as an extended subcategorization frame. 4
An extended subcategorization frame consists of
two parts: (a) list of syntactic dependents and
(b) syntactic operations that the LVC (dis)allows.
Among syntactic dependents, we include those
lexemes and/or phrases necessary to derive the
predicational content of the LVC. The syntactic
dependents may be realized by three types of
phrases: (i) fully lexicalized, (ii) partially lexic-
alized and (iii) variable argument slots. Next, the
description of the phrase types is supported with
expressions encountered earlier in the paper. 5
Fully lexicalized phrases exist as individual
lexical entries. No variation, modification nor ex-
traction out of these phrases is possible. A fully
4This working implementation assumes that the verb se-
lects the dependents of the LVC, thus, departing from other
proposals (Abeill?e, 1995) where the complement noun se-
lects the support verb. Although the semantics layer is left
out, this approach echoes lexicalist HPSG proposals such
as (Krenn and Erbach, 1994; Sailer, 2000).
5Each example displays the light verb followed by its
syntactic dependents given within ??. Subject is omitted.
68
lexicalized phrase is a string of lexemes ? each in
their surface form ? and is represented within ?[]?:
houden ? dat,[de,hand],[boven,het,hoofd] ?
houden ? refl, [van,de,domme] ?
Partially lexicalized phrases declare the type of
argument they introduce e.g. accusative, semi-
fixed prepositional phrase, predicative argument.
These phrases also specify lexical restrictions on
the head lexeme and, allow alternation of spe-
cifiers and morphological productivity in nouns.
Partially lexicalized PPs list the head preposition
and its object NP head.
houden ? acc(rekening), pc(met) ?
brengen ? acc, pp(op,gedachten) ?
Finally, open argument slots state what sort
of argument is required (e.g. acc(usative),
refl(exive), dat(ive)). No lexical restrictions are
declared.
stellen ? acc, pp(in,bewaring) ?
Concerning the syntactic behavior of LVCs,
Alpino currently only declares whether the ex-
pressions allow passive or not and the type of
passive. The current representation allows in-
tervening adjuncts and other material between
the syntactic dependents. No explicit constraints
are stated with regards to topicalization, wh-
extraction, coordination, clefting, etc.
5 Related work
Automatically annotated corpora have been used
before to identify (prepositional) support verb
constructions and to asses their variation and
modification potential. Led by (Krenn, 2000) and
continued by (Spranger, 2004) (among others),
most work focused on German support verb con-
structions and figurative expressions. Our use of
fully parsed corpora and the treebank query tool
to extract relevant evidence introduces a funda-
mental difference with the cited work.
Analytic techniques to annotate syntactically
flexible (but idiosyncratic) expressions in lexical
resources are discussed in (Breidt et al, 1996;
Sag et al, 2001) and (Odijk, 2004). Within a
similar line of work, (Sag et al, 2001) propose
lexical selection, inheritance hierarchies of con-
structions and the notion of idiomatic construc-
tion to formalize the syntax and semantics of truly
fixed, semi-fixed and syntactically flexible ex-
pressions. Assuming a regular syntactic behavior
and having checked that component lexemes sat-
isfy certain predicate-argument relationships, the
semantics layer assigns the idiomatic interpreta-
tion to syntactically flexible expressions. (Sag et
al., 2001) only mention light verb plus noun con-
structions. Supposedly, the Dutch prepositional
LVCs fall into the syntactically flexible group.
6 Conclusion and further improvements
The corpus-based method extracts evidence of
variation and modification within support verb
constructions. The method is sufficiently efficient
in extracting proof of morphological productivity,
specifier variation and adjectival modification in-
side LVCs, but at least one instance of each type
of variation needs to be manually assessed to de-
termine whether the LVC interpretation is present.
The evidence retrieved allows us to establish the
required syntactic structure, lexical restrictions
and furthermore, a preliminary classification of
LVCs. Our findings form the basis of the lexical
annotation of these expressions in Alpino.
A few ideas to enhance the method described in
order to improve the quality of the retrieved evid-
ence follow. During compilation of the raw sub-
corpus, we will adapt the method so that, for each
P N V triple, all verb and noun variant forms are
retrieved from an existing lexicon. This ensures
that the ?subcorpus compiler? collects all possible
variants from the TwNC. Given that the parsed
data includes dependency relations we are trying
different methods to infer the complete subcat-
egorization frame of each LVC. So far, an LVC
is represented as a P N V triple, but we need to
know other syntactic requirements of the predic-
ate. Access to subcategorization frames ought to
improve the extraction of variation evidence. Fi-
nally, the experiments described concentrate on
support verb constructions. It is sometimes dif-
ficult to distinguish a support verb construction
from an idiomatic expression. Thus, some of the
expressions might perfectly belong to the idioms
class, rather than the support verb construction
group. A related question is how to distinguish
69
the literal use of triples from the support verb
construction use automatically. This still needs
a solution.
Acknowledgements
I would like to thank Gertjan van Noord and the
three anonymous reviewers for their invaluable
input on this research. This research was suppor-
ted in part by the NWO PIONIER grant 220-70-001
and also the IRME STEVIN project.
References
Anne Abeill?e. 1995. The Flexibility of French
Idioms: a representation with lexicalized tree ad-
joining grammar. In Martin Everaert, Erik-Jan
van der Linden, Andre Schenk, and Rob Schreuder,
editors, Idioms: Structural & Psychological Per-
spectives. Lawrence Erlbaum Associates.
T. Baldwin, J. Beavers, L. van der Beek, F. Bond,
D. Flickinger, and I.A. Sag, to appear. In search
of a systematic treatment of Determinerless PPs.
Computational Linguistics Dimensions of Syntax
and Semantics of Prepositions. Kluwer Academic.
Gosse Bouma and Geert Kloosterman. 2002. Query-
ing dependency treebanks in XML. In Proceedings
of the 3rd International Conference on Language
Resources and Evaluation (LREC 2002), volume V,
pages 1686?1691, Las Palmas de Gran Canaria,
Spain.
Elisabeth Breidt, Frederique Segond, and Giuseppen
Valetto. 1996. Local grammars for the description
of multi-word lexemes and their automatic recogni-
tion in texts. In COMPLEX96, Budapest.
Ted Briscoe and John Carroll. 1997. Automatic ex-
traction of subcategorization from corpora. In Pro-
ceedings of the 5th ACL conference on applied Nat-
ural Language Processing, pages 356?363, Wash-
ington,D.C.
Miriam Butt. 1995. The structure of complex predic-
ates in Urdu. Ph.D. thesis, Stanford University.
Bart Hollebrandse. 1993. Dutch light verb construc-
tions. Master?s thesis, Tilburg University, the Neth-
erlands.
Brigitte Krenn and Gregor Erbach. 1994. Idioms
and support verb constructions. In John Nerbonne,
Klaus Netter, and Carl Pollard, editors, German
in Head-Driven Phrase Structure Grammar, pages
365?395. CSLI.
Brigitte Krenn. 2000. The Usual Suspects: Data Ori-
ented Models for the Identification and Representa-
tion of Lexical Collocations. Ph.D. thesis, DFKI &
Universitat des Saarlandes.
Rosamund Moon. 1998. Fixed expressions and
Idioms in English. A corpus-based approach. Clar-
endom Press, Oxford.
Tim Nicolas. 1995. Semantics of idiom modifi cation.
In Martin Everaert, Erik-Jan van der Linden, Andre
Schenk, and Rob Schreuder, editors, Idioms: Struc-
tural & Psychological Perspectives. Lawrence Erl-
baum Associates, New Jersey.
Jan Odijk. 2004. Reusable lexical representation for
idioms. In Proceedings of 4th International Confer-
ence on Language Resources and Evaluation 2004,
volume III, pages 903?906, Portugal.
R.J.F. Ordelman. 2002. Twente Nieuws Corpus
(TwNC), August. Parlevink Language Techono-
logy Group. University of Twente.
Carl Pollard and Ivan A. Sag. 1994. Head?Driven
Phrase Structure Grammar. The University of
Chicago Press, CSLI: Stanford.
Susanne Riehemann. 2001. A constructional ap-
proach to idioms and word formation. Ph.D. thesis,
Stanford University.
Ivan Sag, T. Baldwin, F. Bond, A. Copestake, and
D. Flickinger. 2001. Multiword expressions: a pain
in the neck for NLP. LinGO Working Paper No.
2001-03.
Manfred Sailer. 2000. Combinatorial Semantics
& Idiomatic Expressions in Head-Driven Phrase
Structure Grammar. Ph.D. thesis, University of
Tuebingen.
Kristina Spranger. 2004. Beyond subcategoriza-
tion acquisition. Multi-parameter extraction from
German text corpora. In Proceedings of the 11th
EURALEX International Congress, volume I, pages
171?177, France.
Leonoor van der Beek, Gosse Bouma, Jan Da-
ciuk, Tanja Gaustad, Robert Malouf, Gertjan
van Noord, Robbert Prins, and Begon?a Vil-
lada. 2002. Algorithms for Linguistic Pro-
cessing NWO PIONIER Progress Report. Avail-
able electronically at http://odur.let.rug.
nl/?vannoord/alp., Groningen.
Begon?a Villada Moir?on. 2004. Distinguishing prepos-
itional complements from fi xed arguments. In Pro-
ceedings of the 11th EURALEX International Con-
gress, volume III, pages 935?942, Lorient, France.
Begon?a Villada Moir?on. 2005. Data-driven Identi-
fication of fixed expressions and their modifiability.
Ph.D. thesis, University of Groningen.
70
Identifying idiomatic expressions using automatic word-alignment
Begon?a Villada Moiro?n and Jo?rg Tiedemann
Alfa Informatica, University of Groningen
Oude Kijk in ?t Jatstraat 26
9712 EK Groningen, The Netherlands
{M.B.Villada.Moiron,J.Tiedemann}@rug.nl
Abstract
For NLP applications that require some
sort of semantic interpretation it would be
helpful to know what expressions exhibit
an idiomatic meaning and what expres-
sions exhibit a literal meaning. We invest-
igate whether automatic word-alignment
in existing parallel corpora facilitates
the classification of candidate expressions
along a continuum ranging from literal and
transparent expressions to idiomatic and
opaque expressions. Our method relies on
two criteria: (i) meaning predictability that
is measured as semantic entropy and (ii),
the overlap between the meaning of an ex-
pression and the meaning of its compon-
ent words. We approximate the mentioned
overlap as the proportion of default align-
ments. We obtain a significant improve-
ment over the baseline with both meas-
ures.
1 Introduction
Knowing whether an expression receives a lit-
eral meaning or an idiomatic meaning is import-
ant for natural language processing applications
that require some sort of semantic interpretation.
Some applications that would benefit from know-
ing this distinction are machine translation (Im-
amura et al, 2003), finding paraphrases (Bannard
and Callison-Burch, 2005), (multilingual) inform-
ation retrieval (Melamed, 1997a), etc.
The purpose of this paper is to explore to what
extent word-alignment in parallel corpora can be
used to distinguish idiomatic multiword expres-
sions from more transparent multiword expres-
sions and fully productive expressions.
In the remainder of this section, we present our
characterization of idiomatic expressions, the mo-
tivation to use parallel corpora and related work.
Section 2 describes the materials required to ap-
ply our method. Section 3 portraits the routine to
extract a list of candidate expressions from auto-
matically annotated data. Experiments with differ-
ent word alignment types and metrics are shown
in section 4. Our results are discussed in section 5.
Finally, we draw some conclusions in section 6.
1.1 What are idiomatic expressions?
Idiomatic expressions constitute a subset of mul-
tiword expressions (Sag et al, 2001). We assume
that literal expressions can be distinguished from
idiomatic expressions provided we know how their
meaning is derived.1 The meaning of linguistic
expressions can be described within a scale that
ranges from fully transparent to opaque (in figur-
ative expressions).
(1) Wat
what
moeten
must
lidstaten
member states
ondernemen
do
om
to
aan
at
haar
her
eisen
demands
te
to
voldoen?
meet?
?What must EU member states do to meet her
demands??
(2) Deze
this
situatie
situation
brengt
brings
de
the
bestaande
existing
politieke
political
barrie`res
barriers
zeer
very
duidelijk
clearly
aan
in
het
the
licht.
light
?This situation brings the existing political
limitations to light very clearly.?
1Here, we ignore morpho-syntactic and pragmatic factors
that could help model the distinction.
33
(3) Wij
we
mogen
may
ons
us
hier
here
niet
not
bij
by
neerleggen,
agree,
maar
but
moeten
must
de
the
situatie
situation
publiekelijk
publicly
aan
op
de
the
kaak
cheek
stellen.
state
?We cannot agree but must denounce the situ-
ation openly.?
Literal and transparent meaning is associated
with high meaning predictability. The meaning of
an expression is fully predictable if it results from
combining the meaning of its individual words
when they occur in isolation (see (1)). When
the expression undergoes a process of metaphor-
ical interpretation its meaning is less predictable.
Moon (1998) considers a continuum of transpar-
ent, semi-transparent and opaque metaphors. The
more transparent metaphors have a rather predict-
able meaning (2); the more opaque have an un-
predictable meaning (3). In general, an unpredict-
able meaning results from the fact that the mean-
ing of the expression has been fossilized and con-
ventionalized. In an uninformative context, idio-
matic expressions have an unpredictable meaning
(3). Put differently, the meaning of an idiomatic
expression cannot be derived from the cumulative
meaning of its constituent parts when they appear
in isolation.
1.2 Why checking translations?
This paper addresses the task of distinguishing lit-
eral (transparent) expressions from idiomatic ex-
pressions. Deciding what sort of meaning an ex-
pression shows can be done in two ways:
? measuring how predictable the meaning of
the expression is and
? assessing the link between (a) the meaning of
the expression as a whole and (b) the cumu-
lative literal meanings of the components.
Fernando and Flavell (1981) observe that no
connection between (a) and (b) suggests the ex-
istence of opaque idioms and, a clear link between
(a) and (b) is observed in clearly perceived meta-
phors and literal expressions.
We believe we can approximate the meaning
of an expression by looking up the expressions?
translation in a foreign language. Thus, we are
interested in exploring to what extent parallel cor-
pora can help us to find out the type of meaning an
expression has.
For our approach we make the following as-
sumptions:
? regular words are translated (more or less)
consistently, i.e. there will be one or only
a few highly frequent translations whereas
translation alternatives will be infrequent;
? an expression has a (almost) literal meaning
if its translation(s) into a foreign language is
the result of combining each word?s transla-
tion(s) when they occur in isolation into a for-
eign language;
? an expression has a non-compositional mean-
ing if its translation(s) into a foreign language
does not result from a combination of the reg-
ular translations of its component words.
We also assume that an automatic word aligner
will get into trouble when trying to align non-
decomposable idiomatic expressions word by
word. We expect the aligner to produce a large
variety of links for each component word in such
expressions and that these links are different from
the default alignments found in the corpus other-
wise.
Bearing these assumptions in mind, our ap-
proach attempts to locate the translation of a MWE
in a target language. On the basis of all recon-
structed translations of a (potential) MWE, it is de-
cided whether the original expression (in source
language) is idiomatic or a more transparent one.
1.3 Related work
Melamed (1997b) measures the semantic entropy
of words using bitexts. Melamed computes the
translational distribution T of a word s in a source
language and uses it to measure the translational
entropy of the word H(T|s); this entropy approx-
imates the semantic entropy of the word that can
be interpreted either as (a) the semantic ambigu-
ity or (b) the inverse of reliability. Thus, a word
with high semantic entropy is potentially very am-
biguous and therefore, its translations are less re-
liable (or highly context-dependent). We also
use entropy to approximate meaning predictabil-
ity. Melamed (1997a) investigates various tech-
niques to identify non-compositional compounds
in parallel data. Non-compositional compounds
34
are those sequences of 2 or more words (adja-
cent or separate) that show a conventionalized
meaning. From English-French parallel corpora,
Melamed?s method induces and compares pairs of
translation models. Models that take into account
non-compositional compounds are highly accurate
in the identification task.
2 Data and resources
We base our investigations on the Europarl corpus
consisting of several years of proceedings from the
European Parliament (Koehn, 2003). We focus on
Dutch expressions and their translations into Eng-
lish, Spanish and German.2 Thus, we used the en-
tire sections of Europarl in these three languages.
The corpus has been tokenized and aligned at the
sentence level (Tiedemann and Nygaard, 2004).
The Dutch part contains about 29 million tokens
in about 1.2 million sentences. The English, Span-
ish and German counterparts are of similar size
between 28 and 30 million words in roughly the
same number of sentences.
Automatic word alignment has been done us-
ing GIZA++ (Och, 2003). We used standard set-
tings of the system to produce Viterbi alignments
of IBM model 4. Alignments have been produced
for both translation directions (source to target and
target to source) on tokenized plain text.3 We also
used a well-known heuristics for combining the
two directional alignments, the so-called refined
alignment (Och et al, 1999). Word-to-word align-
ments have been merged such that words are con-
nected with each other if they are linked to the
same target. In this way we obtained three differ-
ent word alignment files: source to target (src2trg)
with possible multi-word units in the source lan-
guage, target to source (trg2src) with possible
multi-word units in the target language, and re-
fined with possible multi-word units in both lan-
guages. We also created bilingual word type links
from the different word-aligned corpora. These
lists include alignment frequencies that we will
use later on for extracting default alignments for
individual words. Henceforth, we will call them
link lexica.
2This is only a restriction for our investigation but not for
the approach itself.
3Manual corrections and evaluations of the tokenization,
sentence and word alignment have not been done. We rely
entirely on the results of automatic processes.
3 Extracting candidates from corpora
The Dutch section from the Europarl corpus was
automatically parsed with Alpino, a Dutch wide-
coverage parser.4 1.25% of the sentences could
not be parsed by Alpino, given the fact that many
sentences are rather lengthy. We selected those
sentences in the Dutch Europarl section that con-
tain at least one of a group of verbs that can
function as main or support verbs. Support verbs
are prone to lexicalization or idiomatization along
with their complementation (Butt, 2003). The se-
lected verbs are: doen, gaan, geven, hebben, ko-
men, maken, nemen, brengen, houden, krijgen,
stellen and zitten.5
A fully parsed sentence is represented by the list
of its dependency triples. From the dependency
triples, each main verb is tallied with every de-
pendent prepositional phrase (PP). In this way, we
collected all the VERB PP tuples found in the selec-
ted documents. To avoid data sparseness, the NP
inside the PP is reduced to the head noun?s lemma
and verbs are lemmatized, too. Other potential
arguments under a verb phrase node are ignored.
A sample of more than 191,000 candidates types
(413,000 tokens) was collected. To ensure statist-
ical significance, the types that occur less than 50
times were ignored.
For each candidate triple, the log-likelihood
(Dunning, 1993) and salience (Kilgarriff and Tug-
well, 2001) scores were calculated. These scores
have been shown to perform reasonably well in
identifying collocations and other lexicalized ex-
pressions (Villada Moiro?n, 2005). In addition, the
head dependence between each PP in the candid-
ates dataset and its selecting verbs was measured.
Merlo and Leybold (2001) used the head depend-
ence as a diagnostic to determine the argument
(or adjunct) status of a PP. The head dependence
is measured as the amount of entropy observed
among the co-occurring verbs for a given PP as
suggested in (Merlo and Leybold, 2001; Bald-
win, 2005). Using the two association measures
and the head dependence heuristic, three different
rankings of the candidate triples were produced.
The three different ranks assigned to each triple
were uniformly combined to form the final rank-
ing. From this list, we selected the top 200 triples
4Available at http://www.let.rug.nl/
?vannoord/alp/Alpino.
5Butt (2003) maintains that the first 7 verbs are examples
of support verbs crosslinguistically. The other 5 have been
suggested for Dutch by (Hollebrandse, 1993).
35
which we considered a manageable size to test our
method.
4 Methodology
We examine how expressions in the source lan-
guage (Dutch) are conceptualized in a target lan-
guage. The translations in the target language en-
code the meaning of the expression in the source
language. Using the translation links in paral-
lel corpora, we attempt to establish what type of
meaning the expression in the source language
has. To accomplish this we make use of the three
word-aligned parallel corpora from Europarl as
described in section 2.
Once the translation links of each expression in
the source language have been collected, the en-
tropy observed among the translation links is com-
puted per expression. We also take into account
how often the translation of an expression is made
out of the default alignment for each triple com-
ponent. The default ?translation? is extracted from
the corresponding bilingual link lexicon.
4.1 Collecting alignments
For each triple in the source language (Dutch)
we collect its corresponding (hypothetical) trans-
lations in a target language. Thus, we have a list
of 200 VERB PP triples representing 200 potential
MWEs in Dutch. We selected all occurrences of
each triple in the source language and all aligned
sentences containing their corresponding transla-
tions into English, German and Spanish. We re-
stricted ourselves to instances found in 1:1 sen-
tence alignments. Other units contain many er-
rors in word and sentence alignment and, there-
fore, we discarded them. Relying on automated
word-alignment, we collect all translation links for
each verb, preposition and noun occurrence within
the triple context in the three target languages.
To capture the meaning of a source expression
(triple) S, we collect all the translation links of its
component words s in each target language. Thus,
for each triple, we gather three lists of transla-
tion links Ts. Let us see the example AAN LICHT
BRENG representing the MWE iets aan het licht
brengen ?reveal?. Table 1 shows some of the links
found for the triple AAN LICHT BRENG. If a word
in the source language has no link in the target lan-
guage (which is usually due to alignments to the
empty word), NO LINK is assigned.
Note that Dutch word order is more flexible than
Triple Links in English
aan NO LINK, to, of, in, for, from, on, into, at
licht NO LINK, light, revealed, exposed, highlight,
shown, shed light, clarify
breng NO LINK, brought, bring, highlighted,
has, is, makes
Table 1: Excerpt of the English links found for the
triple AAN LICHT BRENG ?bring to light?.
English word order and that, the PP argument in a
candidate expression may be separate from its se-
lecting verb by any number of constituents. This
introduces much noise during retrieving transla-
tion links. In addition, it is known that concepts
may be lexicalized very differently in different
languages. Because of this, words in the source
language may translate to nothing in a target lan-
guage. This introduces many mappings of a word
to NO LINK.
4.2 Measuring translational entropy
According to our intuition it is harder to align
words in idiomatic expressions than other words.
Thus, we expect a larger variety of links (includ-
ing erroneous alignments) for words in such ex-
pressions than for words taken from expressions
with a more literal meaning. For the latter, we
expect fewer alignment candidates, possibly with
only one dominant default translation. Entropy
is a good measure for the unpredictability of an
event. We like to use this measure for comparing
the alignment of our candidates and expect a high
average entropy for idiomatic expressions. In this
way we approximate a measure for meaning pre-
dictability.
For each word in a triple, we compute the en-
tropy of the aligned target words as shown in equa-
tion (1).
H(Ts|s) = ?
?
t?Ts
P (t|s)logP (t|s) (1)
This measure is equivalent to translational en-
tropy (Melamed, 1997b). P (t|s) is estimated as
the proportion of alignment t among all align-
ments of word s found in the corpus in the con-
text of the given triple.6 Finally, the translational
entropy of a triple is the average translational en-
tropy of its components. It is unclear how to
6Note that we also consider cases where s is part of an
aligned multi-word unit.
36
treat NO LINKS. Thus, we experiment with three
variants of entropy: (1) leaving out NO LINKS,
(2) counting NO LINKS as multiple types and (3)
counting all NO LINKS as one unique type.
4.3 Proportion of default alignments (pda)
If an expression has a literal meaning, we expect
the default alignments to be accurate literal trans-
lations. If an expression has idiomatic meaning,
the default alignments will be very different from
the links observed in the translations.
For each triple S, we count how often each of
its components s is linked to one of the default
alignments Ds. For the latter, we used the four
most frequent alignment types extracted from the
corresponding link lexicon as described in section
2. A large proportion of default alignments7 sug-
gests that the expression is very likely to have lit-
eral meaning; a low percentage is suggestive of
non-transparent meaning. Formally, pda is calcu-
lated in the following way:
pda(S) =
?
s?S
?
d?Ds align freq(s, d)
?
s?S
?
t?Ts align freq(s, t)
(2)
where align freq(s, t) is the alignment fre-
quency of word s to word t in the context of the
triple S.
5 Discussion of experiments and results
We experimented with the three word-alignment
types (src2trg, trg2src and refined) and the two
scoring methods (entropy and pda). The 200 can-
didate MWEs have been assessed and classified
into idiomatic or literal expressions by a human
expert. For assessing performance, standard pre-
cision and recall are not applicable in our case be-
cause we do not want to define an artificial cut-
off for our ranked list but evaluate the ranking it-
self. Instead, we measured the performance of
each alignment type and scoring method by ob-
taining another evaluation metric employed in in-
formation retrieval, uninterpolated average preci-
sion (uap), that aggregates precision points into
one evaluation figure. At each point c where a true
positive Sc in the retrieved list is found, the pre-
cision P (S1..Sc) is computed and, all precision
points are then averaged (Manning and Schu?tze,
1999).
7Note that we take NO LINKS into account when comput-
ing the proportions.
uap =
?
Sc P (S1..Sc)
|Sc|
(3)
We used the initial ranking of our candidates
as baseline. Our list of potential MWEs shows an
overall precision of 0.64 and an uap of 0.755.
5.1 Comparing word alignment types
Table 2 summarizes the results of using the en-
tropy measure (leaving out NO LINKS) with the
three alignment types for the NL-EN language
pair.8
Alignment uap
src2trg 0.864
trg2src 0.785
refined 0.765
baseline 0.755
Table 2: uap values of various alignments.
Using word alignments improves the ranking
of candidates in all three cases. Among them,
src2trg shows the best performance. This is
surprising because the quality of word-alignment
from English-to-Dutch (trg2src) in general is
higher due to differences in compounding in the
two languages. However, this is mainly an issue
for noun phrases which make up only one com-
ponent in the triples.
We assume that src2trg works better in our case
because in this alignment model we explicitly link
each word in the source language to exactly one
target word (or the empty word) whereas in the
trg2src model we often get multiple words (in the
target language) aligned to individual words in the
triple. Many errors are introduced in such align-
ment units. Table 3 illustrates this with an example
with links for the Dutch triple op prijs stel corres-
ponding to the expression iets op prijs stellen ?to
appreciate sth.?
src2trg trg2src
source target target source
gesteld appreciate NO LINK stellen
prijs appreciate much appreciate indeed prijs
op appreciate NO LINK op
gesteld be keenly appreciate stellen
prijs delighted fact prijs
op NO LINK NO LINK op
Table 3: Example src2trg and trg2src alignments
for the triple OP PRIJS STEL.
8The performance of the three alignment types remains
uniform across all chosen language pairs.
37
src2trg alignment proposes appreciate as a link
to all three triple components. This type of align-
ment is not possible in trg2src. Instead, trg2src in-
cludes two NO LINKS in the first example in table
3. Furthermore, we get several multiword-units in
the target language linked to the triple compon-
ents also because of alignment errors. This way,
we end up with many NO LINKS and many align-
ment alternatives in trg2src that influence our en-
tropy scores. This can be observed for idiomatic
expressions as well as for literal expressions which
makes translational entropy less reliable in trg2src
alignments for contrasting these two types of ex-
pressions.
The refined alignment model starts with the in-
tersection of the two directional models and adds
iteratively links if they meet some adjacency con-
straints. This results in many NO LINKS and also
alignments with multiple words on both sides.
This seems to have the same negative effect as in
the trg2src model.
5.2 Comparing scoring metrics
Table 4 offers a comparison of applying transla-
tional entropy and the pda across the three lan-
guage pairs. To produce these results, src2trg
alignment was used given that it reaches the best
performance (refer to Table 2).
Score NL-EN NL-ES NL-DE
entropy
- without NO LINKS 0.864 0.892 0.907
- NO LINKS=many 0.858 0.890 0.883
- NO LINKS=one 0.859 0.890 0.911
pda 0.891 0.894 0.894
baseline 0.755 0.755 0.755
Table 4: Translational entropy and the pda across
three language pairs. Alignment is src2trg.
All scores produce better rankings than the
baseline. In general, pda achieves a slightly better
accuracy than entropy except for the NL-DE lan-
guage pair. Nevertheless, the difference between
the metrics is hardly significant.
5.3 Further improvements
One problem in our data is that we deal with word-
form alignments and not with lemmatized ver-
sions. For Dutch, we know the lemma of each
word instance from our candidate set. However,
for the target languages, we only have access to
surface forms from the corpus. Naturally, inflec-
tional variations influence entropy scores (because
of the larger variety of alignment types) and also
the pda scores (where the exact wordforms have to
be matched with the default alignments instead of
lemmas). In order to test the effect of lemmatiz-
ation on different language pairs, we used CELEX
(Baayen et al, 1993) for English and German to
reduce wordforms in the alignments and in the link
lexicon to corresponding lemmas. We assigned the
most frequent lemma to ambiguous wordforms.
Table 5 shows the scores obtained from applying
lemmatization for the src2trg alignment using
entropy (without NO LINKS) and pda.
Setting NL-EN NL-ES NL-DE
using entropy scores
with prepositions
wordforms 0.864 0.892 0.907
lemmas 0.873 ? 0.906
without prepositions
wordforms 0.906 0.923 0.932
lemmas 0.910 ? 0.931
using pda scores
with prepositions
wordforms 0.891 0.894 0.894
lemmas 0.888 ? 0.903
without prepositions
wordforms 0.897 0.917 0.905
lemmas 0.900 ? 0.910
baseline 0.755 0.755 0.755
Table 5: Translational entropy and pda from
src2trg alignments across languages pairs with
different settings.
Surprisingly, lemmatization adds little or even
decreases the accuracy of the pda and entropy
scores. It is also surprising that lemmatization
does not affect the scores for morphologically
richer languages such as German (compared to
English). One possible reason for this is that
lemmatization discards morphological informa-
tion that is crucial to identify idiomatic expres-
sions. In fact, nouns in idiomatic expressions are
more fixed than nouns in literal expressions. By
contrast, verbs in idiomatic expressions often al-
low tense inflection. By clustering wordforms into
lemmas we lose this information. In future work,
we might lemmatize only the verb.
Another issue is the reliability of the word align-
ment that we base our investigation upon. We
want to make use of the fact that automatic word
alignment has problems with the alignment of in-
dividual words that belong to larger lexical units.
However, we believe that the alignment program
in general has problems with highly ambiguous
words such as prepositions. Therefore, preposi-
38
tions might blur the contrast between idiomatic ex-
pressions and literal translations when measured
on the alignment of individual words. Table 5
includes scores for ranking our candidate expres-
sions with and without prepositions. We observe
that there is a large improvement when leaving out
the alignments of prepositions. This is consistent
for all language pairs and the scores we used for
ranking.
rank pda entropy MWE triple
1 9.80 8.3585 ok breng tot stand ?create?
2 9.24 8.0923 ok breng naar voren ?bring up?
3 16.40 7.8741 ok kom in aanmerking ?qualify?
4 15.33 7.8426 ok kom tot stand ?come about?
5 8.70 7.4973 ok stel aan orde ?bring under discussion?
6 5.65 7.4661 ok ga te werk ?act unfairly?
7 17.46 7.4057 ok kom aan bod ?get a chance?
8 9.38 7.1762 ok ga van start ?proceed?
9 14.15 7.1009 ok stel aan kaak ?expose?
10 18.75 7.0321 ok breng op gang ?get going?
11 13.00 6.9304 ok kom ten goede ?benefit?
12 1.78 6.8715 ok neem voor rekening ?pay costs?
13 20.99 6.7411 ok kom tot uiting ?manifest?
14 1.41 6.7360 ok houd in stand ?preserve?
15 0.81 6.6426 ok breng in kaart ?chart?
16 16.71 6.5194 ok breng onder aandacht ?bring to attention?
17 10.25 6.4893 ok neem onder loep ?scrutinize?
18 7.83 6.4666 ok breng aan licht ?reveal?
19 5.99 6.4049 ok roep in leven ?set up?
20 15.89 6.3729 ok neem in aanmerking ?consider?
...
100 1.72 4.6940 ok leg aan band ?control?
101 14.91 4.6884 ok houd voor gek ?pull s.o.?s leg?
102 23.56 4.6865 ok kom te weten ?find out?
103 15.38 4.6713 ok neem in ontvangst ?receive?
104 31.57 4.6556 * ga om waar ?go about where?
105 35.95 4.6380 * houd met daar ?keep with there?
106 34.86 4.6215 * ga om zaak ?go about issue?
107 28.33 4.5846 ok kom tot overeenstemming ?come to terms?
108 6.06 4.5715 ok breng in handel ?launch?
109 35.62 4.5370 * ga om bedrag ?go about amount?
110 22.58 4.5089 * blijk uit feit ?seems from fact?
111 51.12 4.4063 ok ben van belang ?matter?
112 49.69 4.3921 * ga om kwestie ?go about issue?
113 23.61 4.3902 * voorzie in behoefte ?fill gap?
114 16.18 4.3568 ok geef aan oproep ?make appeal?
115 50.00 4.3254 * houd met aspect ?keep with aspect?
116 40.91 4.3006 * houd aan regel ?adhere to rule?
117 20.12 4.3002 * stel vast met voldoening ?settle with satisfaction?
118 36.90 4.2931 ok kom tot akkoord ?reach agreement?
119 36.49 4.2906 ok breng in stemming ?get in mood?
120 14.06 4.2873 ok sta op schroeven ?be unsettled?
...
180 70.53 2.7395 * voldoe aan criterium ?satisfy criterion?
181 52.33 2.7351 * beschik over informatie ?decide over information?
182 74.71 2.6896 * stem voor amendement ?vote for amending?
183 76.56 2.5883 * neem deel aan stemming ?participate in voting?
184 30.26 2.4484 ok kan op aan ?be able to trust?
185 68.89 2.3199 * zeg tegen heer ?tell a gentleman?
186 45.00 2.1113 * verwijs terug naar commissie ?refer to comission?
187 80.39 2.0992 * stem tegen amendement ?vote againsta amending?
188 78.04 2.0924 * onthoud van stemming ?withhold one?s vote?
189 77.63 1.9997 * feliciteer met werk ?congratulate with work?
190 82.21 1.9020 * stem voor verslag ?vote for report?
191 77.78 1.9016 * schep van werkgelegenheid ?set up of employment?
192 86.36 1.8775 * stem voor resolutie ?vote for resolution ?
193 73.33 1.8687 * bedank voor feit ?thank for fact?
194 39.13 1.8497 * was wit van geld ?wash money?
195 82.20 1.7944 * stem tegen verslag ?vote against report?
196 80.49 1.6443 * schep van baan ?set up of job?
197 86.17 1.4260 * stem tegen resolutie ?vote against resolution?
198 85.56 1.1779 * dank voor antwoord ?thank for reply?
199 90.55 1.0398 * ontvang overeenkomstig artikel ?receive similar article?
200 87.88 1.0258 * recht van vrouw ?right of woman?
Table 6: Rank (using entropy), entropy score, and
pda score of 60 candidate MWEs.
Table 6 provides an excerpt from the ranked
list of candidate triples. The ranking has been
done using src2trg alignments from Dutch to Ger-
man with the best setting (see table 5). The score
assigned by the pda metric is also shown. The
column labeled MWE states whether the expres-
sion is idiomatic (?ok?) or literal (?*?). One issue
that emerges is whether we can find a threshold
value that splits candidate expressions into idio-
matic and transparent ones. One should choose
such a threshold empirically however, it will de-
pend on what level of precision is desirable and
also on the final application of the list.
6 Conclusion and future work
In this paper we have shown that assessing auto-
matic word alignment can help to identify idio-
matic multi-word expressions. We ranked candid-
ates according to their link variability using trans-
lational entropy and their link consistency with
regards to default alignments. For our experi-
ments we used a set of 200 Dutch MWE candid-
ates and word-aligned parallel corpora from Dutch
to English, Spanish and German. The MWE can-
didates have been extracted using standard associ-
ation measures and a head dependence heuristic.
The word alignment has been done using standard
models derived from statistical machine transla-
tion. Two measures were tested to re-rank the can-
didates. Translational entropy measures the pre-
dictability of the translation of an expression by
looking at the links of its components to a target
language. Ranking our 200 MWE candidates us-
ing entropy on Dutch to German word alignments
improved the baseline of 75.5% to 93.2% uninter-
polated average precision (uap). The proportion of
default alignments among the links found for MWE
components is another score we explored for rank-
ing our MWE candidates. Here, the accuracy is
rather similar giving us 91.7% while using the res-
ults of a directional alignment model from Dutch
to Spanish. In general, we obtain slightly better
results when using word alignment from Dutch to
German and Spanish, compared to alignment from
Dutch to English.
There emerge several extensions of this work
that we wish to address in the future. Alignment
types and scoring metrics need to be tested in lar-
ger lists of randomly selected MWE candidates to
see if the results remain unaltered. We also want to
apply some weighting scheme by using the num-
39
ber of NO LINKS per expression. Our assump-
tion is that an expression with many NO LINKS is
harder to translate compositionally, and probably
an idiomatic or ambiguous expression. Altern-
atively, an expression with no NO LINKS is very
predictable, thus a literal expression. Finally, an-
other possible improvement is combining several
language pairs. There might be cases where idio-
matic expressions are conceptualized in a similar
way in two languages. For example, a Dutch idio-
matic expression with a cognate expression in Ger-
man might be conceptualized in a different way in
Spanish. By combining the entropy or pda scores
for NL-EN, NL-DE and NL-ES the accuracy might
improve.
Acknowledgments
This research was carried out as part of the re-
search programs for IMIX, financed by NWO and
the IRME STEVIN project. We would also like
to thank the three anonymous reviewers for their
comments on an earlier version of this paper.
References
R.H. Baayen, R. Piepenbrock, and H. van Rijn.
1993. The CELEX lexical database (CD-
ROM). Linguistic Data Consortium, University of
Pennsylvania,Philadelphia.
Timothy Baldwin. 2005. Looking for prepositional
verbs in corpus data. In Proc. of the 2nd ACL-
SIGSEM Workshop on the Linguistic Dimensions of
Prepositions and their use in computational linguist-
ics formalisms and applications, Colchester, UK.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43th Annual Meeting of the ACL,
pages 597?604, Ann Arbor. University of Michigan.
Miriam Butt. 2003. The light verb jungle.
http://ling.uni-konstanz.de/pages/
home/butt/harvard-work.pdf.
Ted Dunning. 1993. Accurate methods for the stat-
istics of surprise and coincidence. Computational
linguistics, 19(1):61?74.
Chitra Fernando and Roger Flavell. 1981. On idiom.
Critical views and perspectives, volume 5 of Exeter
Linguistic Studies. University of Exeter.
Bart Hollebrandse. 1993. Dutch light verb construc-
tions. Master?s thesis, Tilburg University, the Neth-
erlands.
K Imamura, E. Sumita, and Y. Matsumoto. 2003.
Automatic construction of machine translation
knowledge using translation literalness. In Proceed-
ings of the 10th EACL, pages 155?162, Budapest,
Hungary.
Adam Kilgarriff and David Tugwell. 2001. Word
sketch: Extraction & display of significant colloc-
ations for lexicography. In Proceedings of the 39th
ACL & 10th EACL -workshop ?Collocation: Com-
putational Extraction, Analysis and Explotation?,
pages 32?38, Toulouse.
Philipp Koehn. 2003. Europarl: A multilin-
gual corpus for evaluation of machine trans-
lation. unpublished draft, available from
http://people.csail.mit.edu/koehn/publications/europarl/.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. The MIT Press, Cambridge, Massachu-
setts.
I. Dan Melamed. 1997a. Automatic discovery of non-
compositional compounds in parallel data. In 2nd
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?97), Providence, RI.
I. Dan Melamed. 1997b. Measuring semantic entropy.
In ACL-SIGLEX Workshop Tagging Text with Lex-
ical Semantics: Why, What and How, pages 41?46,
Washington.
Paola Merlo and Matthias Leybold. 2001. Automatic
distinction of arguments and modifiers: the case of
prepositional phrases. In Procs of the Fifth Com-
putational Natural Language Learning Workshop
(CoNLL?2001), pages 121?128, Toulouse. France.
Rosamund Moon. 1998. Fixed expressions and Idioms
in English. A corpus-based approach. Clarendom
Press, Oxford.
Franz Josef Och, Christoph Tillmann, and Hermann
Ney. 1999. Improved alignment models for statist-
ical machine translation. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Nat-
ural Language Processing and Very Large Corpora
(EMNLP/VLC), pages 20?28, University of Mary-
land, MD, USA.
Franz Josef Och. 2003. GIZA++: Training of
statistical translation models. Available from
http://www.isi.edu/?och/GIZA++.html.
Ivan Sag, T. Baldwin, F. Bond, A. Copestake, and
D. Flickinger. 2001. Multiword expressions: a pain
in the neck for NLP. LinGO Working Paper No.
2001-03.
Jo?rg Tiedemann and Lars Nygaard. 2004. The OPUS
corpus - parallel & free. In Proceedings of the
Fourth International Conference on Language Re-
sources and Evaluation (LREC?04), Lisbon, Por-
tugal.
Begon?a Villada Moiro?n. 2005. Data-driven Identi-
fication of fixed expressions and their modifiability.
Ph.D. thesis, University of Groningen.
40
Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 25?32,
Prague, June 2007. c?2007 Association for Computational Linguistics
Semantics-based Multiword Expression Extraction
Tim Van de Cruys and Begon?a Villada Moiro?n
Alfa Informatica, University of Groningen
Oude Kijk in ?t Jatstraat 26
9712 EK Groningen, The Netherlands
{T.Van.de.Cruys|M.B.Villada.Moiron}@rug.nl
Abstract
This paper describes a fully unsupervised
and automated method for large-scale ex-
traction of multiword expressions (MWEs)
from large corpora. The method aims at cap-
turing the non-compositionality of MWEs;
the intuition is that a noun within a MWE
cannot easily be replaced by a semanti-
cally similar noun. To implement this intu-
ition, a noun clustering is automatically ex-
tracted (using distributional similarity mea-
sures), which gives us clusters of semanti-
cally related nouns. Next, a number of statis-
tical measures ? based on selectional prefer-
ences ? is developed that formalize the intu-
ition of non-compositionality. Our approach
has been tested on Dutch, and automatically
evaluated using Dutch lexical resources.
1 Introduction
MWEs are expressions whose linguistic behaviour is
not predictable from the linguistic behaviour of their
component words. Baldwin (2006) characterizes the
idiosyncratic behavior of MWEs as ?a lack of com-
positionality manifest at different levels of analysis,
namely, lexical, morphological, syntactic, seman-
tic, pragmatic and statistical?. Some MWEs show
productive morphology and/or syntactic flexibility.
Therefore, these two aspects are not sufficient con-
ditions to discriminate actual MWEs from productive
expressions. Nonetheless, the mentioned character-
istics are useful indicators to distinguish literal and
idiomatic expressions (Fazly and Stevenson, 2006).
One property that seems to affect MWEs the most
is semantic non-compositionality. MWEs are typi-
cally non-compositional. As a consequence, it is not
possible to replace the noun of a MWE by semanti-
cally related nouns. Take for example the expres-
sions in (1) and (2):
(1) a. break the vase
b. break the cup
c. break the dish
(2) a. break the ice
b. *break the snow
c. *break the hail
Expression (1-a) is fully compositional. Therefore,
vase can easily be replaced with semantically re-
lated nouns such as cup and dish. Expression (2-a),
on the contrary, is non-compositional; ice cannot be
replaced with semantically related words, such as
snow and hail without loss of the original meaning.
Due to the idiosyncratic behavior, current propos-
als argue that MWEs need to be described in the lexi-
con (Sag et al, 2002). In most languages, electronic
lexical resources (such as dictionaries, thesauri, on-
tologies) suffer from a limited coverage of MWEs.
To facilitate the update and expansion of language
resources, the NLP community would clearly bene-
fit from automated methods that extract MWEs from
large text collections. This is the main motivation to
pursue an automated and fully unsupervised MWE
extraction method.
25
2 Previous Work
Recent proposals that attempt to capture seman-
tic compositionality (or lack thereof) employ vari-
ous strategies. Approaches evaluated so far make
use of dictionaries with semantic annotation (Piao
et al, 2006), WordNet (Pearce, 2001), automati-
cally generated thesauri (Lin, 1999; McCarthy et
al., 2003; Fazly and Stevenson, 2006), vector-based
methods that measure semantic distance (Baldwin et
al., 2003; Katz and Giesbrecht, 2006), translations
extracted from parallel corpora (Villada Moiro?n
and Tiedemann, 2006) or hybrid methods that use
machine learning techniques informed by features
coded using some of the above methods (Venkata-
pathy and Joshi, 2005).
Pearce (2001) describes a method to extract collo-
cations from corpora by measuring semantic compo-
sitionality. The underlying assumption is that a fully
compositional expression allows synonym replace-
ment of its component words, whereas a collocation
does not. Pearce measures to what degree a collo-
cation candidate allows synonym replacement. The
measurement is used to rank candidates relative to
their compositionality.
Building on Lin (1998), McCarthy et al (2003)
measure the semantic similarity between expres-
sions (verb particles) as a whole and their compo-
nent words (verb). They exploit contextual features
and frequency information in order to assess mean-
ing overlap. They established that human composi-
tionality judgements correlate well with those mea-
sures that take into account the semantics of the par-
ticle. Contrary to these measures, standard associ-
ation measures poorly correlate with human judge-
ments.
A different approach proposed by Villada Moiro?n
and Tiedemann (2006) measures translational en-
tropy as a sign of meaning predictability, and there-
fore non-compositionality. The entropy observed
among word alignments of a potential MWE varies:
highly predictable alignments show less entropy and
probably correspond to compositional expressions.
Data sparseness and polysemy pose problems be-
cause the entropy cannot be accurately calculated.
Fazly and Stevenson (2006) use lexical and
syntactic fixedness as partial indicators of non-
compositionality. Their method uses Lin?s (1998)
automatically generated thesaurus to compute a met-
ric of lexical fixedness. Lexical fixedness mea-
sures the deviation between the pointwise mutual
information of a verb-object phrase and the aver-
age pointwise mutual information of the expres-
sions resulting from substituting the noun by its
synonyms in the original phrase. This measure is
similar to Lin?s (1999) proposal for finding non-
compositional phrases. Separately, a syntactic flexi-
bility score measures the probability of seeing a can-
didate in a set of pre-selected syntactic patterns. The
assumption is that non-compositional expressions
score high in idiomaticity, that is, a score resulting
from the combination of lexical fixedness and syn-
tactic flexibility. The authors report an 80% accu-
racy in distinguishing literal from idiomatic expres-
sions in a test set of 200 expressions. The perfor-
mance of both metrics is stable across all frequency
ranges.
In this study, we are interested in establishing
whether a fully unsupervised method can capture
the (partial or) non-compositionality of MWEs. The
method should not depend on the existence of large
(open domain) parallel corpora or sense tagged cor-
pora. Also, the method should not require numer-
ous adjustments when applied to new subclasses
of MWEs, for instance, when coding empirical at-
tributes of the candidates. Similar to Lin (1999),
McCarthy et al (2003) and Fazly and Stevenson
(2006), our method makes use of automatically gen-
erated thesauri; the technique used to compile the
thesauri differs from previous work. Aiming at find-
ing a method of general applicability, the measures
to capture non-compositionality differ from those
employed in earlier work.
3 Methodology
In the description and evaluation of our algorithm,
we focus on the extraction of verbal MWEs that con-
tain prepositional complements, although we believe
the method can be easily generalized to other kinds
of MWEs.
In our semantics-based approach, we want to for-
malize the intuition of non-compositionality, so that
MWE extraction can be done in a fully automated
way. A number of statistical measures are developed
that try to capture the MWE?s non-compositional
26
bond between a verb-preposition combination and
its noun by comparing the particular noun of a MWE
candidate to other semantically related nouns.
3.1 Data extraction
The MWE candidates (verb + prepositional phrase)
are automatically extracted from the Twente Nieuws
Corpus (Ordelman, 2002), a large corpus of Dutch
newspaper texts (500 million words), which has
been automatically parsed by the Dutch dependency
parser Alpino (van Noord, 2006). Next, a matrix is
created of the 5,000 most frequent verb-preposition
combinations by the 10,000 most frequent nouns,
containing the frequency of each MWE candidate.1
To this matrix, a number of statistical measures are
applied to determine the non-compositionality of the
candidate MWEs. These statistical measures are ex-
plained in 3.3.
3.2 Clustering
In order to compare a noun to its semantically re-
lated nouns, a noun clustering is created. These
clusters are automatically extracted using standard
distributional similarity techniques (Weeds, 2003;
van der Plas and Bouma, 2005). First, depen-
dency triples are extracted from the Twente Nieuws
Corpus. Next, feature vectors are created for each
noun, containing the frequency of the dependency
relations in which the noun occurs.2 This way, a
frequency matrix of 10K nouns by 100K depen-
dency relations is constructed. The cell frequencies
are replaced by pointwise mutual information scores
(Church et al, 1991), so that more informative fea-
tures get a higher weight. The noun vectors are then
clustered into 1,000 clusters using a simple K-means
clustering algorithm (MacQueen, 1967) with cosine
similarity. During development, several other clus-
tering algorithms and parameters have been tested,
but the settings described above gave us the best
EuroWordNet similarity score (using Wu and Palmer
(1994)).
Note that our clustering algorithm is a hard clus-
tering algorithm, which means that a certain noun
1The lowest frequency verb-preposition combination (with
regard to the 10,000 nouns) appears 3 times.
2e.g. dependency relations that qualify apple might be ?ob-
ject of eat? and ?adjective red?. This gives us dependency triples
like < apple, obj, eat >.
can only be assigned to one cluster. This may pose a
problem for polysemous nouns. On the other hand,
this makes the computation of our metrics straight-
forward, since we do not have to decide among var-
ious senses of a word.
3.3 Measures
The measures used to find MWEs are inspired by
Resnik?s method to find selectional preferences
(Resnik, 1993; Resnik, 1996). Resnik uses a number
of measures based on the Kullback-Leibler diver-
gence, to measure the difference between the prior
probability of a noun class p(c) and the probabil-
ity of the class given a verb p(c|v). We adopt the
method for particular nouns, and add a measure for
determining the ?unique preference? of a noun given
other nouns in the cluster, which, we claim, yields
a measure of non-compositionality. In total, 4 mea-
sures are used, the latter two being the symmetric
counterpart of the former two.
The first two measures, Av?n (equation 2) and
Rv?n (equation 3), formalize the unique prefer-
ence of the verb3 for the noun. Equation 1 gives
the Kullback-Leibler divergence between the overall
probability distribution of the nouns and the proba-
bility distribution of the nouns given a verb; it is used
as a normalization constant in equation 2. Equa-
tion 2 models the actual preference of the verb for
the noun.
Sv =
?
n
p(n | v) logp(n | v)p(n) (1)
Av?n =
p(n | v) log p(n|v)p(n)
Sv
(2)
When p(n|v) is 0, Av?n is undefined. In this
case, we assign a score of 0.
Equation 3 gives the ratio of the verb preference
for a particular noun, compared to the other nouns
that are present in the cluster.
Rv?n =
Av?n
?
n??C Av?n?
(3)
When Rv?n is more or less equally divided
among the different nouns in the cluster, there is no
3We will use ?verb? to designate a prepositional verb, i.e. a
combination of a verb and a preposition.
27
preference of the verb for a particular noun in the
cluster, whereas scores close to 1 indicate a ?unique?
preference of the verb for a particular noun in the
cluster. Candidates whose Rv?n value approaches
1 are likely to be non-compositional expressions.
In the latter two measures, An?v and Rn?v, the
direction of preference is changed: equations 4 and 5
are the symmetric counterparts of equations 2 and 3.
Instead of the preference of the verb for the noun,
the preference of the noun for the verb is modelled.
Except for the change of preference direction, the
characteristics of the former and the latter two mea-
sures are the same.
An?v =
p(v | n) log p(v|n)p(v)
Sn
(4)
Rn?v =
An?v
?
n??C An??v
(5)
Note that, despite their symmetry, the measures
for verb preference and the measures for noun pref-
erence are different in nature. It is possible that
a certain verb only selects a restricted number of
nouns, while the nouns themselves can co-occur
with many different verbs. This brings about differ-
ent probability distributions. In our evaluation, we
want to investigate the impact of both preferences.
3.4 Example
In this section, an elaborated example is presented,
to show how our method works. Take for example
the two MWE candidates in (3):
(3) a. in
in
de
the
smaak
taste
vallen
fall
to be appreciated
b. in
in
de
the
put
well
vallen
fall
to fall down the well
In the first expression, smaak cannot be replaced
with other semantically similar nouns, such as geur
?smell? and zicht ?sight?, whereas in the second ex-
pression, put can easily be replaced with other se-
mantically similar words, such as kuil ?hole? and
krater ?crater?.
The first step in the formalization of this intuition,
is the extraction of the clusters in which the words
smaak and put appear from our clustering database.
This gives us the clusters in (4).
(4) a. smaak: aroma ?aroma?, gehoor ?hear-
ing?, geur ?smell?, gezichtsvermogen
?sight?, reuk ?smell?, spraak ?speech?,
zicht ?sight?
b. put: afgrond ?abyss?, bouwput ?build-
ing excavation?, gaatje ?hole?, gat
?hole?, hiaat ?gap?, hol ?cave?, kloof
?gap?, krater ?crater?, kuil ?hole?, lacune
?lacuna?, leemte ?gap?, valkuil ?pitfall?
Next, the various measures described in section 3.3
are applied. Resulting scores are given in tables 1
and 2.
MWE candidate Av?n Rv?n An?v Rn?v
val#in smaak .12 1.00 .04 1.00
val#in geur .00 .00 .00 .00
val#in zicht .00 .00 .00 .00
Table 1: Scores for MWE candidate in de smaak
vallen and other nouns in the same cluster.
Table 1 gives the scores for the MWE in de smaak
vallen, together with some other nouns that are
present in the same cluster. Av?n shows that there
is a clear preference (.12) of the verb val in for the
noun smaak. Rv?n shows that there is a unique
preference of the verb for the particular noun smaak.
For the other nouns (geur, zicht, . . . ), the verb has no
preference whatsoever. Therefore, the ratio of verb
preference for smaak compared to the other nouns
in the cluster is 1.00.
An?v and Rn?v show similar behaviour. There
is a preference (.04) of the noun smaak for the verb
val in, and this preference is unique (1.00).
MWE candidate Av?n Rv?n An?v Rn?v
val#in put .00 .05 .00 .05
val#in kuil .01 .11 .02 .37
val#in kloof .00 .02 .00 .03
val#in gat .04 .71 .01 .24
Table 2: Scores for MWE candidate in de put vallen
and other nouns in same cluster.
28
Table 2 gives the scores for the instance in de put
vallen ? which is not a MWE ? together with other
nouns from the same cluster. The results are quite
different from the ones in table 1. Av?n ? the pref-
erence of the verb for the noun ? is quite low in most
cases, the highest score being a score of .04 for gat.
Furthermore, Rv?n does not show a unique pref-
erence of val in for put (a low ratio score of .05).
Instead, the preference mass is divided among the
various nouns in the cluster, the highest preference
of val in being assigned to the noun gat (.71).4
The other two scores show again a similar ten-
dency; An?v ? the preference of the noun for the
verb ? is low in all cases, and when all nouns in the
cluster are considered (Rn?v), there is no ?unique?
preference of one noun for the verb val in. Instead,
the preference mass is divided among all nouns in
the cluster.
4 Results & Evaluation
4.1 Quantitative evaluation
In this section, we quantitatively evaluate our
method, and compare it to the lexical and syntactic
fixedness measures proposed by Fazly and Steven-
son (2006). More information about Fazly and
Stevenson?s measures can be found in their paper.
The potential MWEs that are extracted with the
fully unsupervised method described above and with
Fazly and Stevenson?s (2006) method (FS from here
onwards) are automatically evaluated by compar-
ing the extracted list to handcrafted MWE databases.
Since we have extracted Dutch MWEs, we are us-
ing the two Dutch resources available: the Refer-
entie Bestand Nederlands (RBN, Martin and Maks
(2005)) and the Van Dale Lexicographical Informa-
tion System (VLIS) database. Evaluation scores are
calculated with regard to the MWEs that are present
in our evaluation resources. Among the MWEs in our
reference data, we consider only those expressions
that are present in our frequency matrix: if the verb
is not among the 5,000 most frequent verbs, or the
noun is not among the 10,000 most frequent nouns,
the frequency information is not present in our input
4The expression is ambiguous: it can be used in a lit-
eral sense (in een gat vallen, ?to fall down a hole?) and in a
metaphorical sense (in een zwart gat vallen, ?to get depressed
after a joyful or busy period?).
data. Consequently, our algorithm would never be
able to find those MWEs.
The first six rows of table 3 show precision, re-
call and f-measure for various parameter thresholds
with regard to the measures Av?n, Rv?n, An?v
and Rn?v, together with the number of candidates
found (n). The last 3 rows show the highest val-
ues we were able to reach by using FS?s fixedness
scores.
Using only two parameters ? Av?n and Rv?n ?
gives the highest f-measure (? 14%), with a pre-
cision and recall of about 17% and about 12% re-
spectively. Adding parameter Rn?v increases preci-
sion but degrades recall, and this tendency continues
when adding both parameters An?v and Rn?v. In
all cases, a higher threshold increases precision but
degrades recall. When using a high threshold for all
parameters, the algorithm is able to reach a precision
of ? 38%, but recall is low (? 4%).
Lexical fixedness reaches an f-measure of ? 12%
(threshold of 3.00). These scores show the best per-
formance that we reached using lexical fixedness.
Following FS, we evaluated the syntactic fixedness
scores of expressions falling above a frequency cut-
off. Since our corpus is much larger than that used
by FS, a frequency cutoff of 50 was chosen. The pre-
cision, recall and f-measure of the syntactic fixed-
ness measure (shown on table 3) are ? 10%, 41%
and 16% respectively, showing worse precision than
our method but much better recall and f-measure.
As shown by FS, syntactic fixedness performs better
than lexical fixedness; Fixednessoverall improves
on the syntactic fixedness results and also reaches
better overall performance than our method.
The compared methods show a different behav-
ior. FS?s method favours high recall whereas our
method prefers the best trade-off between precision
and recall. We wish to highlight that our method
reaches better precision than FS?s method while han-
dling many low frequency candidates (minimum fre-
quency is 3); this makes our method preferable in
some NLP tasks. It is possible that the two methods
are capturing different properties of MWEs; in future
work, we want to analyse whether the expressions
extracted by the two methods differ.
29
parameters precision recall f-measure
Av?n Rv?n An?v Rn?v n (%) (%) (%)
.10 .80 ? ? 3175 16.09 13.11 14.45
.10 .90 ? ? 2655 17.59 11.98 14.25
.10 .80 ? .80 2225 19.19 10.95 13.95
.10 .90 ? .90 1870 20.70 9.93 13.42
.10 .80 .01 .80 1859 20.33 9.69 13.13
.20 .99 .05 .99 404 38.12 3.95 7.16
Fixednesslex(v, n) 3.00 3899 15.14 9.92 11.99
Fixednesssyn(v, n) 50 15,630 10.20 40.90 16.33
Fixednessoverall(v, n) 50 7819 13.73 27.54 18.33
Table 3: Evaluation results compared to RBN & VLIS
4.2 Qualitative evaluation
Next, we elaborate upon advantages and disadvan-
tages of our semantics-based MWE extraction algo-
rithm by examining the output of the procedure, and
looking at the characteristics of the MWEs found and
the errors made by the algorithm.
First of all, our algorithm is able to filter out gram-
matical collocations that cause problems in tradi-
tional MWE extraction paradigms. An example is
given in (5).
(5) voldoen
meet
aan
to
eisen,
demands,
voorwaarden
conditions
meet the {demands, conditions}
In traditional MWE extraction algorithms, based on
collocations, highly frequent expressions like the
ones in (5) often get classified as a MWE, even
though they are fully compositional. Such algo-
rithms correctly identify a strong lexical affinity be-
tween two component words (voldoen, aan), which
make up a grammatical collocation; however, they
fail to capture the fact that the noun may be filled in
by a semantic class of nouns. Our algorithm filters
out those expressions, because semantic similarity
between nouns that fill in the object slot is taken into
account.
Our quantitative evaluation shows that the algo-
rithm reaches the best results (i.e. the highest f-
measures) when using only two parameters (Av?n
and Rv?n). Upon closer inspection of the output,
we noticed that An?v and Rn?v are often able to
filter out non-MWEs like the expressions b in (6)
and (7).
(6) a. verschijnen
appear
op
on
toneel
stage
to appear
b. zingen
sing
op
on
toneel
stage
to sing on the stage
(7) a. lig
lie
in
in
geheugen
memory
be in memory
b. lig
lie
in
in
ziekenhuis
hospital
lie in the hospital
It is only when the two other measures (a unique
preference of the noun for the verb) are taken into
account that the b expressions are filtered out ? ei-
ther because the noun preference for the verb is very
low, or because it is more evenly distributed among
the cluster. The b expressions, which are non-MWEs,
result from the combination of a verb with a highly
frequent PP. These PPs are typically locative, direc-
tional or predicative PPs, that may combine with nu-
merous verbs.
Also, expressions like the ones in (8), where the
fixedness of the expression lies not so much in the
verb-noun combination, but more in the noun part
(naar school, naar huis) are filtered out by the lat-
ter two measures. These preposition-noun combina-
tions seem to be institutionalized PPs, so-called de-
terminerless PPs.
30
(8) a. naar
to
school
school
willen
want
want to go to school
b. naar
to
huis
home
willen
want
want to go home
We will now look at some errors made by our algo-
rithm. First of all, our algorithm highly depends on
the quality of the noun clustering. If a noun appears
in a cluster with unrelated words, the measures will
overrate the semantic uniqueness of the expressions
in which the noun appears.
Secondly, syntax might play an important role.
Sometimes, there are syntactic restrictions between
the preposition and the noun. A noun like pagina
?page? can only appear with the preposition op ?on?,
as in lees op pagina ?read on page?. Other, semanti-
cally related nouns, such as hoofdstuk ?chapter?, pre-
fer in ?in?. Due to these restrictions, the measures
will again overrate the semantic uniqueness of the
noun (pagina in the example).
Finally, our hard clustering method does not take
polysemous nouns into account. A noun may only
occur in one cluster, ignoring other possible mean-
ings. Schaal, for example, means ?dish? as well as
?scale?. In our clustering, it only appears in a cluster
of dish-related nouns. Therefore, expressions like
maak gebruik op [grote] schaal ?make use of [sth.]
on a [large] scale?, receive again overrated measures
of semantic uniqueness, because the ?scale? sense of
the noun is compared to nouns related to the ?dish?
sense.
5 Conclusions and further work
Our algorithm based on non-compositionality ex-
plores a new approach aimed at large-scale MWE
extraction. Using only two parameters, Av?n and
Rv?n, yields the highest f-measure. Using the two
other parameters, An?v and Rn?v, increases preci-
sion but degrades recall. Due to the formalization of
the intuition of non-compositionality (using an auto-
matic noun clustering), our algorithm is able to rule
out various expressions that are coined MWEs by tra-
ditional algorithms.
Note that our algorithm has taken on a purely
semantics-based approach. ?Syntactic fixedness? of
the expressions is not taken into account. Combin-
ing our semantics-based approach with other extrac-
tion techniques such as the syntactic fixedness mea-
sure proposed by Fazly and Stevenson (2006) might
improve the results significantly.
We conclude with some issues saved for future
work. First of all, we would like to combine our
semantics-based method with other methods that are
used to find MWEs (especially syntax-based meth-
ods), and implement the method in general classifi-
cation models (decision tree classifier and maximum
entropy model). This includes the use of a more
principled (machine learning) framework in order to
establish the optimal threshold values.
Next, we would like to investigate a number of
topics to improve on our semantics-based method.
First of all, using the top k similar nouns for a certain
noun ? instead of the cluster in which a noun appears
? might be more beneficial to get a grasp of the com-
positionality of MWE candidates. Also, making use
of a verb clustering in addition to the noun clustering
might help in determining the non-compositionality
of expressions. Disambiguating among the various
senses of nouns should also be a useful improve-
ment. Furthermore, we would like to generalize our
method to other syntactic patterns (e.g. verb object
combinations), and test the approach for English.
One final issue is the realization of a manual eval-
uation of our semantics-based algorithm, by hav-
ing human judges decide whether a MWE candidate
found by our algorithm is an actual MWE. Our au-
tomated evaluation framework is error-prone due to
mistakes and incompleteness of our resources. Dur-
ing qualitative evaluation, we found many actual
MWEs found by our algorithm, that were not con-
sidered correct by our resources (e.g. [iemand] in
de gordijnen jagen ?to drive s.o. mad?, op het [ver-
keerde] paard gokken ?back the wrong horse?, [de
kat] uit de boom kijken ?wait to see which way the
wind blows?, uit het [goede] hout gesneden ?be a
trustworthy person?). Conversely, there were also
questionable MWE candidates that were described
as actual MWEs in our evaluation resources (val op
woensdag ?fall on a wednesday?, neem als voorzitter
?take as chairperson?, ruik naar haring ?smell like
herring?, ben voor [. . . ] procent ?to be . . . percent?).
A manual evaluation could overcome these difficul-
ties.
We believe that our method provides a genuine
31
and successful approach to get a grasp of the non-
compositionality of MWEs in a fully automated way.
We also believe that it is one of the first methods
able to extract MWEs based on non-compositionality
on a large scale, and that traditional MWE extrac-
tion algorithms will benefit from taking this non-
compositionality into account.
Acknowledgements
This research was carried out as part of the research
program IRME STEVIN project. We would also like
to thank Gertjan van Noord and the two anonymous
reviewers for their helpful comments on an earlier
version of this paper.
References
T. Baldwin, C. Bannard, T. Tanaka, and D. Widdows. 2003. An
Empirical Model of Multiword Expressions Decomposabil-
ity. In Proc. of the ACL-2003 Workshop on Multiword Ex-
pressions: Analysis, Acquisition and Treatment, pages 89?
96, Sapporo, Japan.
T. Baldwin. 2006. Compositionality and Multiword Expres-
sions: Six of One, Half a Dozen of the Other? Invited talk
given at the COLING/ACL?06 Workshop on Multiword Ex-
pressions: Identifying and Exploiting Underlying Properties,
July.
K. Church, W. Gale, P. Hanks, and D. Hindle. 1991. Using
statistics in lexical analysis. In Uri Zernik, editor, Lexical
Acquisition: Exploiting On-line resources to build a lexicon,
pages 115?164. Lawrence Erlbaum Associates, New Jersey.
A. Fazly and S. Stevenson. 2006. Automatically constructing
a lexicon of verb phrase idiomatic combinations. In Pro-
ceedings of the 11th Conference of the European Chapter of
the Association for Computational Linguistics (EACL-2006),
Trento, Italy.
G. Katz and E. Giesbrecht. 2006. Automatic identification of
non-compositional multi-word expressions using Latent Se-
mantic Analysis. In Proc. of the COLING/ACL?06 Work-
shop on Multiword Expressions: Identifying and Exploiting
Underlying Properties, pages 12?19, Sydney, Australia.
D. Lin. 1998. Automatic retrieval and clustering of simi-
lar words. In Proceedings of COLING/ACL 98, Montreal,
Canada.
D. Lin. 1999. Automatic identification of non-compositional
phrases. In Proceedings of ACL-99, pages 317?324. Univer-
sity of Maryland.
J. B. MacQueen. 1967. Some methods for classification and
analysis of multivariate observations. In Proceedings of 5-th
Berkeley Symposium on Mathematical Statistics and Prob-
ability, volume 1, pages 281?297, Berkeley. University of
California Press.
W. Martin and I. Maks, 2005. Referentie Bestand Nederlands.
Documentatie, April.
D. McCarthy, B. Keller, and J. Carroll. 2003. Detecting a Con-
tinuum of Compositionality in Phrasal Verbs. In Proc. of
the ACL-2003 Workshop on Multiword Expressions: Analy-
sis, Acquisition and Treatment, Sapporo, Japan.
R.J.F. Ordelman. 2002. Twente Nieuws Corpus (TwNC), Au-
gust. Parlevink Language Techonology Group. University of
Twente.
D. Pearce. 2001. Synonymy in collocation extraction. In Word-
Net and Other lexical resources: applications, extensions
& customizations (NAACL 2001), pages 41?46, Pittsburgh.
Carnegie Mellon University.
S. Piao, P. Rayson, O. Mudraya, A. Wilson, and R. Garside.
2006. Measuring mwe compositionality using semantic an-
notation. In Proceedings of the Workshop on Multiword
Expressions: Identifying and Exploiting Underlying Prop-
erties, pages 2?11, Sydney, Australia. Association for Com-
putational Linguistics.
P. Resnik. 1993. Selection and Information: A Class-Based
Approach to Lexical Relationships. PhD Thesis, University
of Pennsylvania.
P. Resnik. 1996. Selectional constraints: An information-
theoretic model and its computational realization. Cogni-
tion, 61:127?159.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger.
2002. Multiword Expressions: a pain in the neck for NLP.
In Proceedings of the Third International Conference on
Intelligent Text Processing and Computational Linguistics,
pages 1?15, Mexico City, Mexico.
L. van der Plas and G. Bouma. 2005. Syntactic contexts for
finding semantically similar words. Computational Linguis-
tics in the Netherlands 2004. Selected Papers from the Fif-
teenth CLIN Meeting, pages 173?184.
G. van Noord. 2006. At Last Parsing Is Now Operational.
In P. Mertens, C. Fairon, A. Dister, and P. Watrin, editors,
TALN06. Verbum Ex Machina. Actes de la 13e conference
sur le traitement automatique des langues naturelles, pages
20?42, Leuven.
S. Venkatapathy and A. Joshi. 2005. Measuring the relative
compositionality of verb-noun collocations by integrating
features. In Proceedings of the Human Language Technol-
ogy Conference and Conference on Empirical Methods in
Natural Language Processing, pages 899?906, Vancouver.
B. Villada Moiro?n and J. Tiedemann. 2006. Identifying id-
iomatic expressions using automatic word-alignment. In
Proceedings of the EACL 2006 Workshop on Multi-word-
expressions in a multilingual context?, pages 33?40, Trento,
Italy.
J. Weeds. 2003. Measures and Applications of Lexical Distri-
butional Similarity. PhD Thesis, University of Sussex.
Z. Wu and M. Palmer. 1994. Verb semantics and lexical selec-
tion. In 32nd. Annual Meeting of the Association for Com-
putational Linguistics, pages 133?138, New Mexico State
University, Las Cruces, New Mexico.
32
