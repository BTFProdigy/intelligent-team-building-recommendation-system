Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 351?354,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
KCDC: Word Sense Induction by Using Grammatical Dependencies and
Sentence Phrase Structure
Roman Kern
Know-Center
Graz, Austria
rkern@know-center.at
Markus Muhr
Know-Center
Graz, Austria
mmuhr@know-center.at
Michael Granitzer
Graz University of Technology,
Know-Center
Graz, Austria
mgrani@know-center.at
Abstract
Word sense induction and discrimination
(WSID) identifies the senses of an am-
biguous word and assigns instances of this
word to one of these senses. We have build
a WSID system that exploits syntactic and
semantic features based on the results of
a natural language parser component. To
achieve high robustness and good general-
ization capabilities, we designed our sys-
tem to work on a restricted, but grammat-
ically rich set of features. Based on the
results of the evaluations our system pro-
vides a promising performance and robust-
ness.
1 Introduction
The goal of the SemEval-2 word sense induc-
tion and discrimination task, see Manandhar et
al. (2010), is to identify the senses of ambiguous
nouns and verbs in an unsupervised manner and to
label unseen instances of these words with one of
the induced senses. The most common approach
towards this task is to apply clustering or graph
partitioning algorithms on a representation of the
words that surround an ambiguous target word, see
for example Niu et al (2007) and Pedersen (2007).
We followed this approach by employing a cluster-
ing algorithm to detect the individual senses, but
focused on generating feature sets different to the
mainstream approach. Our feature sets utilize the
output of a linguistic processing pipeline that cap-
tures the syntax and semantics of sentence parts
closely related with the target word.
2 System Overview
The base of our system is to apply a parser on the
sentence in which the target word occurs. Contex-
tual information, for example the sentences sur-
rounding the target sentence, are currently not
exploited by our system. To analyze the sen-
tences we applied the Stanford Parser (Version
1.6.2), which is based on lexicalized probabilis-
tic context free grammars, see Klein and Man-
ning (2003). This open-source parser not only ex-
tracts the phrase structure of a given sentence, but
also provides a list of so called grammatical rela-
tions (typed dependencies), see de Marneffe et al
(2006). These relations reflect the dependencies
between the words within the sentence, for exam-
ple the relationship between the verb and the sub-
ject. See Chen et al (2009) for an application of
grammatical dependencies for word sense disam-
biguation.
2.1 Feature Extraction
The phrase structure and the grammatical depen-
dencies are sources for the feature extraction stage.
To illustrate the result of the parser and feature ex-
traction stages we use an example sentence, where
the target word is the verb ?file?:
Afterward , I watched as a butt-ton of good , but
misguided people filed out of the theater , and
immediately lit up a smoke .
2.1.1 Grammatical Dependency Features
The Stanford Parser provides 55 different gram-
matical dependency types. Figure 2 depicts the list
of the grammatical dependencies identified by the
Stanford Parser for the example sentence. Only a
limited subset of these dependencies are selected
to build the grammatical feature set. This subset
has been defined based on preliminary tests on the
trial dataset. For verbs only dependencies that rep-
resent the association of a verb with prepositional
modifiers and phrasal verb particles are selected
(prep, prepc, prt). If the verb is not associated
with a preposition or particle, a synthetic ?miss-
ing? feature is added instead (!prep, !prt). For
nouns the selected dependencies are the preposi-
tions (for head nouns that are the object of a prepo-
sition) and noun compound modifiers (pobj, nn).
351
Figure 1: Phrase tree of the example sentence. The noun phrase ?misguided people? is connected to the
target word via the nsubj dependency and the phrase ?the theater? is associated with the target verb via
the prep and pobj dependencies.
Figure 2: List of grammatical dependencies as de-
tected by the Stanford Parser.
If the noun is associated with a verb the grammati-
cal dependencies of this verb are also added to the
feature set.
The name of the dependency and the word (i.e.
preposition or particle) are used to construct the
grammatical feature. The different features are
weighted. The weights have been derived from
their frequencies within the trial dataset and listed
in table 1. For the example sentence the extracted
grammatical features are:
?out?, ?of?, prep, prt
2.1.2 Phrase Term Features
The second set of features are generated from the
sentence phrase structure. In figure 1 the parse tree
for the example sentence is depicted.
Again we tried to keep the feature set as small
as possible. Starting with the target word only
phrases that are directly associated with the am-
biguous word are selected. To identify these
phrases the grammatical dependencies are ex-
ploited. For nouns as target words the associated
verb is searched at first. Given a verb the phrases
containing the head noun of a subject or object re-
lationship are identified. If the verb is accompa-
Feature Weight
prepc, prt, nn, pobj 0.9
prep 0.45
!prep, !prt 0.5
?prepositions?, ?particles? 0.97
Table 1: Weights of the grammatical features,
which were derived from their distribution within
the trial dataset.
nied by a preposition, the phrase carrying the ob-
ject of the preposition is also added. All nouns and
adjectives from these these phrases are then col-
lected. The phrase words together with the verb,
prepositions and particles are lemmatized using
tools also provided by the Stanford Parser project.
The weights of the phrase term features are
based on the frequency of the words within the
training dataset, where N is the total number of
sentences and N
f
is the number of sentences in
which the lemmatized phrase term occurs in:
weight
f
= log(
N
N
f
+ 1
) + 1 (1)
In our example sentence the extracted phrase
term features are:
of, misguided, file, theater, people, out
2.2 Phrase Term Expansion
The feature space of the phrase terms is expected
to be very sparse. Additionally different phrase
terms may have similar semantics. Therefore the
phrase terms are optionally expanded with asso-
ciated terms, where semantically similar terms
should be associated with the same terms.
To calculate the statistics for term expansion we
used the training dataset (although other datasets
352
would be more suitable for this purpose). The
dataset is split into sentences. Stopwords and
rarely used words, which occur in less than 3 sen-
tences, were removed. The remaining words were
finally lemmatized. For a given phrase term the
top 100 associated terms are used to build the
feature set. The association weight between two
terms is based on the Pointwise Mutual Informa-
tion:
weight
pmi
(t
i
, t
j
) =
log
2
(
P (t
i
|t
j
)
P (t
j
)
)
log
2
(
1
P (t
j
)
)
(2)
For example the top 10 associated terms for
theater are:
theater.n, movie.n, opera.n,
vaudeville.n, wnxt-abc.n, imax.n,
orpheum.n, pullulate.v, projector.n,
psychomania.n
2.3 Sense Induction
To detect the individual senses within the training
dataset we applied unsupervised machine learning
techniques. For each ambiguous word a matrix
- M
|Instances|?|Features|
- is created and a clus-
tering algorithm is applied, namely the Growing
k-Means, see Daszykowski et al (2002). This
algorithm needs the number of clusters and cen-
troids as initialization parameters, where the initial
centroids are calculated using a directed random
seed finder as described in Arthur and Vassilvitskii
(2007). We used the Jensen-Shannon Divergence
function for the grammatical dependency features
and the Cosine Similarity for the phrase term fea-
ture sets as relatedness function.
For each cluster number we re-run the clus-
tering with different random initial centroids (30
times) and for each run we calculate a cluster qual-
ity criterion. The overall cluster quality criterion is
the mean of all feature quality criteria, which are
calculated based on the set of clusters the feature
occurs in - C
f
- the number of instances of each
cluster - N
c
- and the number of instances within
a cluster where the feature occurs in - N
c,f
:
FQC
f
=
weight
f
|C
f
|
?
?
c?C
f
N
c,f
N
c
(3)
QC
run
= FQC
f
(4)
The cluster quality criterion is calculated for
each run and the combination of the mean and
standard deviations are then used to calculate a
stability criterion to detect the number of clusters,
which is based on the intuition that the correct
cluster count yields the lowest variation of QC
values:
SC
k
=
mean(QC)
stdev(QC)
(5)
Starting with two clusters the number of clusters
is incremented until the stability criterion starts to
decline. For the cluster number with the highest
stability criterion the run with the highest qual-
ity criterion is selected as final clustering solution.
The result of the sense induction processing is a
list of centroids for the identified clusters.
2.4 Sense Assignment
The final processing step is to assign an instance
of an ambiguous word to one of the pre-calculated
senses. The sentence with the target word is pro-
cessed exactly like the training sentences to gener-
ate a set of features. Finally the word is assigned
to the sense cluster with the maximum relatedness.
3 System Configurations & Results
Our system can be configured to use a combina-
tion of feature sets for the word sense induction
and discrimination calculations: a) KCDC-GD:
Grammatical dependency features, b) KCDC-PT:
Phrase terms features, c) KCDC-PC: Expanded
phrase term features, d) KCDC-PCGD: All train-
ing sentences are first processed by using the ex-
panded phrase term features and then by using
the grammatical dependency features with an ad-
ditional feature that encodes the cluster id found
by the phrase features.
In the evaluation we also submitted multiple
runs of the same configuration
1
to assess the in-
fluence of the random initialization of the cluster-
ing algorithm. Judging from the results the ran-
dom seeding has no pronounced impact and it in-
fluence should decrease when the number of clus-
tering runs for each cluster number is increased.
All configurations found on average about 3
senses for target words in the test set (2.8 for verbs,
3.3 for nouns), with exception of the KCDC-PT
configuration which identified only 1.5 senses on
average. In the gold standard the number of senses
for verbs is 3.12 and for nouns 4.46, which shows
that the stability criterion tends to underestimate
the number of senses slightly.
To compare the performance of the differ-
ent configurations, one can use the average rank
within the evaluation result lists. Judging from the
1
labeled KCDC-GD-2, KCDC-GDC for configuration ?a?
and KCDC-PC-2 for the configuration ?c?
353
rankings, the configurations that utilize the gram-
matical dependencies and the expanded phrase
terms provide similar performance. The config-
uration that takes the phrase terms directly as fea-
tures comes in last, which is expected due to the
sparse nature of the feature representation and the
low number of detected senses.
Comparing the performance of our system with
the two baselines shows that our system did out-
perform the random baseline in all evaluation runs
and the most frequent baseline (MFS) in all runs
with the exception of the F-Score based unsuper-
vised evaluation, where the MFS baseline has not
been beaten by any system. Although none of our
submitted configurations was ranked first in any of
the evaluations, their ranking was still better than
average, with the exception of the KCDC-PT con-
figuration.
Another observation that can be made is the dif-
ference in performance between nouns and verbs.
Our system, especially the grammatical depen-
dency based configurations, is tailored towards
verbs. Therefore the better performance of verbs
in the evaluation is in line with the expectations.
When looking at the results of the individual tar-
get words one can notice that for a set of words
the quality of the sense detection is above average.
For 16 of the 100 words a V-Measure of more than
30% in at least one configuration was achieved
(average: 7.8%)
2
. This can be seen as indicator
that our selection of features is effective for a spe-
cific group of words. For the remaining words an
according feature set has to be developed in future
work.
4 Conclusion
For the SemEval 2010 word sense induction and
discrimination task we have tried to build a system
that uses a minimal amount of information while
still providing a competitive performance. This
system contains a parser component to analyze the
phrase structure of a sentence and the grammat-
ical dependencies between words. The extracted
features are then clustered to detect the senses of
ambiguous words. In the evaluation runs our sys-
tem did demonstrate a satisfying performance for
a number of words.
The design of our system offers a wide range
of possible enhancements. For example the inte-
2
The best performing target words are: root.v,
presume.v, figure.v, weigh.v, cheat.v
gration of preposition disambiguation and noun-
phrase co-reference resolution could help to fur-
ther improve the word sense discrimination effec-
tiveness.
Acknowledgments
The Know-Center is funded within the Austrian COMET
Program - Competence Centers for Excellent Technologies -
under the auspices of the Austrian Federal Ministry of Trans-
port, Innovation and Technology, the Austrian Federal Min-
istry of Economy, Family and Youth and by the State of
Styria. COMET is managed by the Austrian Research Pro-
motion Agency FFG. Results are partially funded by the EU-
ROSTARS project 4811 MAKIN?IT.
References
D. Arthur and S. Vassilvitskii. 2007. k-means++:
The advantages of careful seeding. In Proceedings
of the eighteenth annual ACM-SIAM symposium on
Discrete algorithms, page 10271035. Society for In-
dustrial and Applied Mathematics Philadelphia, PA,
USA.
Ping Chen, Wei Ding, Chris Bowes, and David Brown.
2009. A fully unsupervised word sense disambigua-
tion method using dependency knowledge. Human
Language Technology Conference.
M Daszykowski, B Walczak, and D L Massart. 2002.
On the optimal partitioning of data with K-means,
growing K-means, neural gas, and growing neural
gas. Journal of chemical information and computer
sciences, 42(6):1378?89.
M.C. de Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC 2006.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics - ACL ?03, pages 423?430.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. SemEval-
2010 Task 14: Word Sense Induction & Disam-
biguation. In Proceedings of SemEval-2, Uppsala,
Sweden, ACL.
Zheng-yu Niu, Dong-hong Ji, and Chew-lim Tan.
2007. I2R: Three Systems for Word Sense Discrim-
ination, Chinese Word Sense Disambiguation, and
English Word Sense Disambiguation. In Proceed-
ings of the 4th International Workshop on Semantic
Evaluations. ACL.
T. Pedersen. 2007. Umnd2: Senseclusters applied to
the sense induction task of senseval-4. In Proceed-
ings of the 4th International Workshop on Semantic
Evaluations. ACL.
354
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 138?142, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
KnCe2013-CORE:Semantic Text Similarity by use of Knowledge Bases
Hermann Ziak
Know-Center GmbH
Graz University of Technology
Inffeldgasse 13/ 6. Stock
8010 Graz, Austria
hziak@know-center.at
Roman Kern
Know-Center GmbH
Graz University of Technology
Inffeldgasse 13/ 6. Stock
8010 Graz, Austria
rkern@know-center.at
Abstract
In this paper we describe KnCe2013-CORE,
a system to compute the semantic similarity
of two short text snippets. The system com-
putes a number of features which are gath-
ered from different knowledge bases, namely
WordNet, Wikipedia and Wiktionary. The
similarity scores derived from these features
are then fed into several multilayer perceptron
neuronal networks. Depending on the size
of the text snippets different parameters for
the neural networks are used. The final out-
put of the neural networks is compared to hu-
man judged data. In the evaluation our system
performed sufficiently well for text snippets
of equal length, but the performance dropped
considerably once the pairs of text snippets
differ in size.
1 Introduction
The task of the semantic sentence similarity is to as-
sign a score to a given pair of sentences. This score
should reflect the degree by which the two sentences
represent the same meaning. The semantic similar-
ity of two sentences could then be used in a num-
ber of different application scenarios, for example it
could help to improve the performance of informa-
tion retrieval systems.
In the past, systems based on regression mod-
els in combination with well chosen features have
demonstrated good performance on this topic[4] [6].
Therefore we took this approach as a starting point
to develop our semantic similarity system; addition-
ally, we integrated a number of existing knowledge
bases into our system. With it, trained with the data
discussed in the task specification of last year[1], we
participated in the shared task of SEM 2013.
Additionally, to the similarity based on the fea-
tures derived from the external knowledge bases, we
employ a neural network to compute the final simi-
larity score. The motivation to use a supervised ma-
chine learning algorithm has been the observation
that the semantic similarity is heavily influenced by
the context of the human evaluator. A financial ex-
pert for example would judge sentences with finan-
cial topics different to non financial experts, if oc-
curring numbers differ from each other.
The remainder of the paper is organised as fol-
lows: In Section 2 we described our system, the
main features and the neuronal network to combine
different feature sets. In Section 3 the calculation
method of our feature values is discribed. In Sec-
tion 4 we report the results of our system based on
our experiments and the submitted results of the test
data. In Section 5 and 6 we discuss the results and
the outcome of our work.
2 System Overview
2.1 Processing
Initially the system puts the sentence pairs of the
whole training set through our annotation pipeline.
After this process the sentence pairs are compared
to each other by our different feature scoring algo-
rithms. The result is a list of scores for each of these
pairs where every score represents a feature or part
of a feature. The processed sentences are now sep-
arated by their length and used to train the neuronal
138
network models for each length group. The testing
data is also grouped based on the sentence length
and the score for each pair is determined by a rele-
vant model.
2.2 Token Features
The first set of features are simply the tokens from
the two respective sentences. This feature set should
perform well, if exactly the same words are used
within the pair of sentences to be compared. But
as soon as words are replaced by their synonyms or
other semantically related words, this feature set will
not be able to capture the true similarity. Used with-
out other features it could even lead to false posi-
tive matches, for example given sentences with sim-
ilar content but containing antonyms. The tokenizer
used by our system was based on the OpenNLP
maximum entropy tokenizer, which detects token
boundaries based on probability model.
2.3 Wiktionary Features
While the collaboratively created encyclopedia
Wikipedia receives a lot of attention from the gen-
eral public, as well as the research community, the
free dictionary Wiktionary1 is far lesser known. The
Wiktionary dictionary stores the information in a
semi-structured way using Wikimedia syntax, where
a single page represents a single word or phrase.
Therefore we developed a parser to extract relevant
information. In our case we were especially inter-
ested in semantically related terms, where the se-
mantic relationship is:
Representations: Set of word forms for a spe-
cific term. These terms are expected to indicate the
highest semantic similarity. This includes all flex-
ions, for example the ?s? suffix for plural forms.
Synonyms: List of synonyms for the term.
Hyponyms: List of more specific terms.
Hypernym: Terms which represent more general
terms.
Antonym: List of terms, which represent an op-
posing sense.
Related Terms: Terms, with a semantic relation-
ship, which does not fall in the aforementioned cat-
egories. For example related terms for ?bank? are
1http://en.wiktionary.org
?bankrupt?. Related terms represent only a weak se-
mantic similarity.
Derived Terms: Terms, with overlapping word
forms, such as ?bank holiday?, ?bankroll? and ?data-
bank? for the term ?bank?. From all the semantic
relationship types, derived terms are the weakest in-
dicator for their similarities.
2.4 WordNet Features
The WordNet[5][2] features were generated identi-
cally to the Wiktionary features. We used the Word-
Net off line database and the provided library to get
a broader knowledge base. Therefore we extract the
semantically related terms of each token and saved
each class of relation. Where each dependency class
produced an one value in the final feature score list
of the sentence pairs.
2.5 Wikification Feature
We applied a Named Entity Recognition component,
which has been trained using Wikipedia categories
as input. Given a sentence it will annotate all found
concepts that match a Wikipedia article, together
with a confidence score. So for every found entry
by the annotator there is a list of possible associ-
ated topics. The confidence score can then be used
to score the topic information, in the final step the
evaluation values where calculated as follows:
scorewiki(s1, s2) =
|T1 ? T2|
norm(T1, T2)
where T1 and T2 are the set of topics of the two
sentences and norm is the mean of the confidence
scores of the topics.
2.6 Other Features
Although we mainly focused our approach on the
three core features above, others seemed to be useful
to improve the performance of the system of which
some are described below.
Numbers and Financial Expression Feature:
Some sentence pairs showed particular variations
between the main features and their actual score.
Many of these sentence pairs where quite similar
in their semantic topic but contained financial ex-
pressions or numbers that differed. Therefore these
expressions where extracted and compared against
each other with a descending score.
139
NGrams Feature: The ngram overlapping fea-
ture is based on a noun-phrase detection which re-
turns the noun-phrases in different ngrams. This
noun-phrase detection is a pos tagger pattern which
matches multiple nouns preceding adjectives and de-
terminers. In both sentences the ngrams where ex-
tracted and compared to each other returning only
the biggest overlapping. In the end, to produce the
evaluation values, the word-count of the overlapping
ngrams were taken.
3 Distance calculation
For the calculation of the distance of the different
features we chose a slightly modified version of the
Jacquard similarity coefficient.
Jsc(w, l) =
w
l
Where in this case w stands for the intersection of
the selected feature, and l for
la+lb
2 where la and lb
are the length of the sentences with or without stop-
words depending on the selected feature. The as-
sumption was that for some features the gap between
sentences where one has many stop-words and sen-
tences with none would have a crucial impact but for
others it would be detrimental. In regard to this we
used, depending on the feature, the words or words
excluding stop-words.
3.1 Scoring
One of the main issues at the beginning of our re-
search was how to signal the absence of features to
the neuronal network. As our feature scores depend
on the length of the sentence, the absence of a partic-
ular feature (e.g. financial values) and detected fea-
tures without intersections (e.g. none of the found
financial values in the sentences are intersecting) in
the sentence pairs would lead to the same result.
Therefore we applied two different similarity
scores based on the feature set. They differ in the
result they give, if there is no overlap between the
two feature sets.
For a simple term similarity we defined our simi-
larity score as
score(w, s, l) =
{
?1 : s = 0 or w = 0
Jsc(w, l) : w > 0
where w stands for the intersections and S for the
word-count of the sentences. The system returns the
similarity of -1 for no overlap, which signals no sim-
ilarity at all. For fully overlapping feature sets, the
score is 1.
For other features, where we did not expect them
to occur in every sentence, for example numbers or
financial terms, the similarity score was defined as
follows:
score(w, s, l) =
{
1 : s = 0 or w = 0
Jsc(w, l) : w > 0
In this case the score would yield 1 decreasing for
non overlapping feature sets and will drop to -1 the
more features differentiated. This redefines the nor-
mal state as equivalent to a total similarity of all
found features and only if features differ this value
drops.
3.2 Sentence Length Grouping
From tests with the training data we found that our
system performed very diversly with both long and
short sentences although our features where normal-
ized to the sentence length. To cover this problem
we separated the whole collection of training data
into different groups based on their length, each of
the groups were later used to train their own model.
Finally the testing data were also divided into this
groups and were applied on the group model.
3.3 Neural Network
We applyied multilayer perceptron neuronal net-
works on the individual sentence length groups. So
for each group of sentence length we computed sep-
arately the weights of the neural network. To model
the neural networks we used the open-source library
Neuroph.2. This network was defined with a 48-
input layer, which represented the extracted feature
scores, 4 hidden layers, and a 1-output layer which
represents the similarity score of the sentences. For
the runs referenced by table 1 and 2 we used 400000
iterations, which gave us the best results in our tests,
with a maximum error of 0.001 and a learning rate
of 0.001
2http://neuroph.sourceforge.net
140
4 Evaluation and Results
The following results of our system where produced
by our test-run after the challenge deadline. For
the first run we split each training set in halfe, self-
evident without the use of the datasets published af-
ter the challenge, and used the other half to validate
our system. See table 1 for result, which contain our
system.
MSRvid MSRpar SMTeuroparl
Grouping 0.69 0.55 0.50
Without Grouping 0.66 0.52 0.62
Table 1: Run with and without sentence length grouping
on the training set
For the validation the whole 2013 test set was
used as it wasnot used for training. In table 2 the
results of our system on the test-set are listed. When
using the sentence length grouping and without sen-
tence length grouping just using a single neural net-
work for all sentence similarities.
FNWN headlines OnWN SMT
Grouping 0.08 0.66 0.62 0.21
Without Grouping 0.38 0.62 0.39 0.25
Table 2: Results of our system with and without sentence
length grouping on the test set
Finally, we report the results from the original
evaluation of the STS-SharedTask in table 3.
FNWN headlines OnWN SMT
KnCe2013-all 0.11 0.35 0.35 0.16
KnCe2013-diff 0.13 0.40 0.35 0.18
KnCe2013-set 0.04 0.05 -0.15 -0.06
Table 3: The submission to the challenge
5 Discussion
Based on the results we can summarize that our
submitted system, worked well for data with very
short and simple sentences, such as the MSRvid;
however for the longer the sentences the perfor-
mance declined. The grouping based on the input
length worked well for sentences of similar length
when compared, as we used the average length of
both sentences to group them, but it seamed to fail
for sentences with very diverse lengths like in the
FNWN data set as shown in table 2. Comparing the
results of the official submission to the test runs of
our system it underperformed in all datasets. We as-
sume that the poor results in the submission run were
caused by badly chosen training settings.
6 Conclusion
In our system for semantic sentence similarity we
tried to integrate a number of external knowledge
bases to improve its performance. (Viz. WordNet,
Wikipedia, Wiktionary) Furthermore, we integrated
a neural network component to replicate the similar-
ity score assigned by human judges. We used dif-
ferent sets of neural networks, depending on the size
of the sentences. In the evaluation we found that
our system worked well for the most datasets. But
as soon as the pairs of sentences differed too much
in size, or the sentences were very long, the perfor-
mance decreased. In future work we will consider
to tackle this problem with partial matching[3] and
to introduces features to extract core statements of
short texts.
Acknowledgements
The Know-Center is funded within the Austrian
COMET Program - Competence Centers for Excel-
lent Technologies - under the auspices of the Aus-
trian Federal Ministry of Transport, Innovation and
Technology, the Austrian Federal Ministry of Econ-
omy, Family and Youth and by the State of Styria.
COMET is managed by the Austrian Research Pro-
motion Agency FFG.
References
[1] Eneko Agirre, Daniel Cer, Mona Diab, and
Aitor Gonza?lez. Semeval-2012 task 6: A pi-
lot on semantic textual similarity. In SEM
2012: The First Joint Conference on Lexical
and Computational Semantics (SemEval 2012),
Montreal, Canada, 2012.
[2] Christiane Fellbaum, editor. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cam-
bridge, MA, 1998.
141
[3] Prodromos Malakasiotis and Ion Androutsopou-
los. Learning textual entailment using svms and
string similarity measures.
[4] Nikos Malandrakis, Elias Iosif, and Alexan-
dros Potamianos. Deeppurple: estimating sen-
tence semantic similarity using n-gram regres-
sion models and web snippets. In Proceed-
ings of the First Joint Conference on Lexical
and Computational Semantics - Volume 1: Pro-
ceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth In-
ternational Workshop on Semantic Evaluation,
SemEval ?12, pages 565?570, Stroudsburg, PA,
USA, 2012. Association for Computational Lin-
guistics.
[5] George A. Miller. Wordnet: a lexical database
for english. Commun. ACM, 38(11):39?41,
November 1995.
[6] Frane S?aric?, Goran Glavas?, Mladen Karan, Jan
S?najder, and Bojana Dalbelo Bas?ic?. Takelab:
Systems for measuring semantic text similarity.
In Proceedings of the Sixth International Work-
shop on Semantic Evaluation (SemEval 2012),
pages 441?448, Montre?al, Canada, 7-8 June
2012. Association for Computational Linguis-
tics.
142
Proceedings of SADAATL 2014, pages 48?55,
Dublin, Ireland, August 24, 2014.
A Study of Scientific Writing:
Comparing Theoretical Guidelines with Practical Implementation
Mark Kr
?
oll
?
Know-Center GmbH
Graz, Austria
mkroell@know-center.at
Gunnar Schulze
?
Know-Center GmbH
Graz, Austria
gschulze@know-center.at
Roman Kern
Know-Center GmbH
Graz, Austria
rkern@know-center.at
Abstract
Good scientific writing is a skill researchers seek to acquire. Textbook literature provides guide-
lines to improve scientific writing, for instance, ?use active voice when describing your own
work?. In this paper we investigate to what extent researchers adhere to textbook principles in
their articles. In our analyses we examine a set of selected principles which (i) are general and
(ii) verifiable by applying text mining and natural language processing techniques. We develop
a framework to automatically analyse a large data set containing ?14.000 scientific articles re-
ceived from Mendeley and PubMed. We are interested in whether adhering to writing principles
is related to scientific quality, scientific domain or gender and whether these relations change
over time. Our results show (i) a clear relation between journal quality and scientific impreci-
sion, i.e. journals with low impact factors exhibit higher numbers of imprecision indicators such
as number of citation bunches and number of relativating words and (ii) that writing style partly
depends on domain characteristics and preferences.
1 Introduction
Writing good scientific articles is a skill. Researchers seek to acquire this skill for the purpose of succes-
fully disseminating their ideas to the scientific community. Learning to write good articles is a process
that for most of us starts at graduate level and keeps us company in the course of our careers. To advance
the learning process, there is (i) plenty of literature out there containing do?s and dont?s, (ii) seniors
administering doses of advice and (iii) entire lectures dedicated to this very subject.
In this paper, we investigate whether researchers do adhere to general writing principles taken from
textbook literature. We are interested in whether adhering to writing principles is related to the journal
quality, the scientific domain or gender and whether there is a change over time. Doing so allows us to
better understand which and to what extent theoretical guidelines are practically implemented. Devia-
tions from textbook literature could be indicators of good practice and if they occur frequently enough,
they might also be candidates for textbook updates.
Studying current trends in academic writing (cf. (Tas, 2010)) originates in the domains of pragmatics
and linguistics. In this research area we recognize two larger directions. The first one seeks to relate
an article?s content to scientific concepts, for instance, whether an article contains a theory or not (cf.
(Pettigrew et al., 2001)) or to scientific discourse elements, for instance, which paragraphs can be related
to categories such as Motivation or Experiment (cf. (Liakata et al., 2012)). The other direction focuses
more on organisation and structure including the analysis of entire scientific theses (cf. (Paltridge, 2002))
or the analysis of single structural elements such as the title (cf. (Soler, 2007), (Haggan, 2003)).
In contrast to previous work, we conduct our analyses at a larger scale. We thus develop a framework
to automatically analyze large amounts of scientific articles. In our experiments we select writing prin-
ciples which are on the one hand general and often recommended in textbook literature (cf. (Lebrun,
2007), (Alley, 1996)) and on the other hand automatically retrievable and verifiable by applying text
?
These two authors contributed equally to this work.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
48
mining and natural language processing techniques. To give an example, the principle ?use active voice
when describing your own work? is a popular one and can be verified by examining the verb types in the
article?s abstract, introduction and conclusion. In our study we analyze two data sets - one from Mende-
ley
1
, a popular reference management tool, - one from PubMed
2
, a free resource containing citations
for biomedical literature. We can observe relations between journal quality and textbook recommenda-
tions such as Avoid Imprecision and Engage the Reader. In addition, the results indicate writing style
preferences due to domain characteristics. Our findings show that theoretical guidelines partly concur
with practical implementation and thus contribute to better understand the extent to which theory guides
praxis and vice versa praxis might guide theory.
The remaining paper is organized as follows: Section 2 provides details on the used data sets as well
as the software framework to automate the analysis of scientific articles. Section 3 contains experimen-
tal results and discussions of analyzed writing principles. Related work is covered in Section 4 and
concluding remarks are presented in Section 5.
2 Experimental Setup
2.1 Data Sets
For our analyses we used scientific articles from two sources - Mendeley and PubMed - which also
provided us with meta data, e.g. name of the conference or journal. Most of the publication organs were
journals and we decided to select only journals which had a minimum of 10 articles and for which we
could find a respective 5-year impact factor
3
. We decided to conduct our analyses over a 10-year time
period from 2001 to 2010, since only in this period articles from both sources were available. In total we
experimented with 13866 scientific articles. Grouping them according to scientific quality, domain and
gender, we constructed three data sets described in the following:
- Quality: According to the impact factor (IF), we divided the scientific articles into three groups; low
IF ranging from 0 to 2.5 (2303 articles), middle IF ranging from 2.5 to 4 (5734 articles) and high
IF ranging from 4 to 35 (5829 articles). The ranges were chosen to reflect the journal quality while
containing an appropriate (not too small) number of scientific articles per category.
- Domain: We divided the scientific articles by their journal type into two groups: biomedical (7053
articles) and a technical (6813 articles) which contained mainly articles from physics and computer
science.
- Gender: We used two gazetteer lists to identify female
4
or male
5
first authors. Since only a part
of the authors? first names was unabbreviated, we used a subset of articles for these experiments:
number of articles with male first author = 1182, number of articles with female first author = 1990.
2.2 Framework
To automatically analyze large amounts of scientific articles, we designed a framework and embedded
our analysis algorithms in a Hadoop
6
environment. The environment allows parallelization of processes
and thus greatly reduces computation time. We stored the results in a PostgreSQL
7
database for quick
access and used various Python packages such as matplotlib
8
for creating graphical representations of
our results.
Our first pre-processing step encompassed the extraction of textual content from scientific publica-
tions. To automatically extract the content, we used a processing pipeline (cf. (Klampfl et al., 2013))
1
http://www.mendeley.com/
2
http://www.ncbi.nlm.nih.gov/pubmed
3
http://www.citefactor.org/impact-factor-list-2012.html
4
http://deron.meranda.us/data/census-dist-female-first.txt
5
http://deron.meranda.us/data/census-dist-male-first.txt
6
http://hadoop.apache.org/
7
http://www.postgresql.org/
8
http://matplotlib.org/
49
that applies various machine learning techniques in combination with heuristics to detect the logical
structure of a PDF document. Further processing steps included (i) tokenization, (ii) sentence splitting,
(iii) stemming, (iv) part-of-speech tagging and (v) chunking. We employed part-of-speech and chunking
information in our analyses (see Section 3) to distinguish verb phrases with respect to present vs. past
tense as well as active vs. passive voice.
3 Analysis of Scientific Literature
In this section we analyze a set of selected writing style principles with respect to Reader Engagement
and Imprecision. Each analysis contains (i) a motivating statement mostly taken from (Lebrun, 2007), (ii)
a visual representation of results and (iii) an interpretation of results. During our experiments we could
observe that most of the time there were no significant differences between articles written by male and
female first authors. We repeated the experiments with a majority criterion of authors, i.e. more female
first names or more male first names per article, resulting in similar findings. It appears that both genders
adhere to the same guidelines which were standardly used at the time.
3.1 Engaging the Reader
In this section we examine different means to engage the reader according to textbook literature including
(i) the title, (ii) figures & tables and (iii) a lively writing style based on using present tense and active
voice.
3.1.1 Title
The title represents the first point of contact with the reader (and the reviewer) and should ideally be
made catchy and standing out. We examine three means to do that: (i) usage of verbs to increase energy,
(ii) usage of acronyms to provide a reference shortcut for others and (iii) usage of questions to create a
hook. Figure 1 contains average numbers of article titles with respect to these means.
Figure 1: Illustration of (i) titles containting at least one verb (left), (ii) titles containing acronyms (mid-
dle) and (iii) titles which contain a question (right) over a ten-year time period. The upper row reflect
distinction by domain, the lower row by impact factor. The y-axis represents the average number of
article titles exhibiting the respective feature.
The upper left figure in Figure 1 tells us that using verbs in titles is more common among authors in
the biomedical domain than in the physics/computer science domain. The lower left figure indicates a
trend towards using more verbs in the title over the years independent of the journal quality. The upper,
middle figure of Figure 1 shows that using acronyms in titles is more common in the biomedical domain
50
and a possible trend using acronyms at the beginning of the century. The lower figure in the middle
indicates an up and down over the years across impact factors. The right figures in Figure 1 tell us that
only a low percentage of authors use questions in their titles independent of domain or journal quality.
The numbers corroborate textbook literatures? recommendation of using verbs in the titles as well as
using acronyms. A bit surprising is that questions in titles are rarely used, since according to literature
they create a mighty hook for the reader. In a next step we intend to relate the title to the content of the
abstract and the introduction to answer the question how well the title reflects the article?s content.
3.1.2 Figures & Tables
Visual representations of results in terms of figures and partly of tables help the reader to reduce reading
time. According to (Lebrun, 2007) they even represent visual information burgers which are easy to
digest. Figure 2 contains respective average figure and table counts.
Figure 2: Illustration of average figure and table counts over a ten-year period according to domain (left)
and impact factor (middle, right). The y-axis represents the average number of tables/figures per article.
The left figure in Figure 2 illustrates that authors from the biomedical domain use more tables and
figures than authors from the physics/computer science domain. The middle and right figure reflect
average counts according to impact factor. Journal articles with a high impact factor contain (i) fewer
tables than journals with middle or low impact factors and (ii) in general more figures.
From the results in Figure 2 we learn that usage of figures and tables appears to a certain degree
be dependent on the domain. In biomedicine, the usage of figures to convey information seems more
widespread than in technical domains. We assume even higher figure counts in domains such as chem-
istry where illustrations, for instance, of molecules are far more frequent. In addition, it seems that
authors of high impact journals prefer using figures to using tables probably because the information
content is more easily to grasp. Tables appear to be more suited to structure information. In a next step
we also intend to analyze figures? and tables? captions with respect to comprehensiveness, i.e. to what
extent are captions self-contained?
3.1.3 Lively Writing Style
Textbook literature advices authors to formulate their contributions in an active way using active voice
and the present tense. To learn more about present tense usage, we simply counted the occurrences of
the respective part-of-speech tags
9
, i.e. VB, VBP and VBZ. To count occurrences of active voice, we
inspected all identified verb chunks whether they contained auxiliary verbs as well as a past participle
part-of-speech tag. If they did, we considered them passive voice otherwise active voice. Figure 3
contains average fractions of verb phrases with respect to present tense and active voice.
The upper left figure in Figure 3 illustrates that authors of the physics/computer science domain use a
lot more present tense compared to authors from the biomedical domain. The upper right figure indicates
that the higher the journal?s impact factor the more present tense is used by the authors. The lower left
figure indicates that active and passive voice are almost evently distributed throughout article contents
with a bit passive predominance. The lower right figure shows no significant difference of using active
voice with respect to journal quality. There is but a trend towards using more active voice over the years.
9
http://www.cis.upenn.edu/
?
treebank/home.html
51
Figure 3: Illustration of average fractions of verb phrases with respect to present tense (upper figures) and
active voice (lower figures) over a ten-year period. The left figures correspond to analyses with respect
to domain and the right ones to analyses with respect to impact factor.
The observed high percentage of present tense and active voice verb phrases adheres to the textbook
principle of lively stating one?s own work. Yet, in this analysis we took into account the tense and the
voice for the entire article content. To address this issue in greater depth, we intend to solely analyze
abstract, introduction and conclusion in the near future.
3.2 Imprecision
In this section we examine indicators of scientific imprecision according to textbook literature including
(i) number of citation bunches and (ii) number of relativating words.
3.2.1 Citation Bunches
Statements such as ?many people have been working in this research area? + a sequence of citations in-
dicate lack of precision or insufficient dealing with the subject matter. We define a collection containing
more than 3 citations as citation bunch. Figure 4 contains average numbers of citation bunches per article.
The left figure in Figure 4 indicates that citation bunches occur more often in the biomedical domain
than in the physics/computer science domain. The right figure shows that citation bunches occur far more
often in journals with a low impact factor than in those with a middle or high one.
The findings indicate that journals with a higher impact factor contain fewer citation bunches - one
indicator of lack of precision. Concerning the higher numbers in the biomedical domain we intend to
examine the type of scientific articles in the future; for instance, we assume that survey articles contain
more citation bunches than others.
52
Figure 4: Illustration of averaged citation bunch counts according to domain (left) and impact factor
(right)) over a ten-year period. The y-axis corresponds to the average number of citation bunches (>3
citations) per scientific article.
3.2.2 Usage of Relativating Words
Overusage of relativating words
10
indicates lack of precision. Reviewers may doubt an author?s expertise
and assurance of results if relativating words occur too frequently. Figure 5 contains average numbers of
relativating words per article sentence.
Figure 5: Illustration of relativating words usage by domain (left) and by impact factor (right). The y-axis
represents the average number of relativating words per article sentence.
The left figure in Figure 5 shows that more relativating words are used in the biomedical domain than
in the physics/computer science domain. In the right figure journals with a high and a middle-ranged
impact factor exhibit fewer relativating words than journals with a low impact factor.
The higher usage of relativating words in the biomedical domain remains unclear and might be due
to domain characteristics. Regarding the journal quality a similar behavior as in Section 3.2.1 can be
observed: higher quality journals contain fewer relativating words - an indication for a higher scientific
preciseness and quality.
4 Related Work
The analysis of academic writing originates from research areas such as linguistics and pragmatics.
These areas are rather interested in studying how scientific articles are written instead of what kind of
knowledge they contain.
Discourse Analysis is a modern discipline that studies amongst other things language beyond the
level of a sentence taking into account the surrounding contexts as well. The detection of discourse
structure in scientific documents is important for a number of tasks such as information extraction or text
10
According to (Lebrun, 2007) relativating words include significantly, typically, generally, commonly, may/can, a number
of, the majority of, substantial, probably, several, less, various, frequent, many, others, more, often, most, a few, the main.
53
summarization. Elements of discourse include the statement of facts, claims and hypotheses as well as the
identification of methods and protocols. In this context (Liakata et al., 2012) automate the recognition of
11 categories including Hypothesis, Motivation and Result to access the scientific discourse of scientific
articles. In the Partridge system, (Ravenscroft et al., 2013) build upon the automated recognition to
automatically categorize articles according to their types such as Review or Case Study. (Teufel et al.,
2002) used discourse analysis to summarize scientific papers. She restored the discourse context by
adding the rhetorical status, for example, the scientific goal or criticism, to each sentence in an article.
In a similar way, (Liakata et al., 2013) take scientific discourse into account to generate a content model
for summarization purposes.
Besides analyzing the structure and organisation of entire publications (cf. (Paltridge, 2002)), there
is related literature dedicated to the analysis of single (structural) elements including (i) the title or (ii)
citations. The title is of particular importance often representing the first point of contact with the reader.
(Haggan, 2003) investigated whether titles of scientific articles could be regarded as headlines with a
clear role of informing and engaging the reader. In her work she pointed out the relation between title
formulation and information advertisement. (Soler, 2007) conducted title studies in two genres (review
and research papers) and in two fields (biological and social sciences). She statistically analyzed titles
with respect to word count, word frequency and title construction.
Citation analysis represents one of the most widely used methods of bibliometrics which aims to quan-
titatively analyze academic literature. Citation analysis (cf. (Garfield, 1979)) is an expression for simply
counting a scientific article?s citations which can be regarded as indicator for an article?s scientific im-
pact; the more often the article is cited, the higher its academic value (cf. (Garfield, 1972)). An important
part of citation analysis represents hedge detection (cf. (Lakoff, 1972)). Hedges are linguistic devices
which indicate that authors do not or cannot back up their statements with facts. Hedge detection, thus,
supports the distinction between facts and unreliable or uncertain information (cf. (Crompton, 1997)).
Facing the continuously growing amounts of scientific articles there has been an increased interest in
automating the process (cf. (Di Marco, 2006), (Farkas et al., 2007)).
5 Conclusion
Our paper?s contribution encompasses a comparison of theoretical guidelines, i.e. ?What the literature
recommends?? with their practical implementations, i.e. ?How authors actually write scientific arti-
cles??. We designed a framework to automatically analyze ?14.000 scientific articles with respect to a
selected set of writing principles.
To summarize the results: Section 3.2 shows a clear relation between journal quality and imprecision,
i.e. journals with low impact factors exhibit higher numbers of imprecision indicators such as number of
citation bunches and number of relativating words. In addition, the number of figures and the percentage
of verb phrases in present tense tend to be higher with higher quality journals (see Section 3.1).
In respect to the domain, the results indicate writing style preferences probably due to domain char-
acteristics, for instance, usage of more figures (see Section 3.1.2) and domain preferences, for instance,
lesser usage of present tense (see Section 3.1.3).
Other interesting observations include (i) that adhering to writing principles appears to be gender
independent and (ii) that using acronyms in titles is far more popular than using questions in the title (see
Section 3.1.1) independent of domain and impact factor.
Our findings show that theoretical guidelines partly concur with practical implementations and thus
contribute to better understand the extent to which theory guides praxis. A better understanding will
contribute (i) to confirm textbook principles and (ii) to update writing principles due to good practice.
In a next step we plan to extend the scale of our analyses to include several hundred thousand scientific
articles as well as the complexity of our analyses to investigate issues including (i) paper skeleton, for
instance, ?Is there a prefered heading structure?? and (ii) usage of synonyms which hampers clarity.
54
Acknowledgements
We thank Mendeley for providing the data set as well as Werner Klieber for crawling the PubMed data
set. The presented work was developed within the CODE project funded by the EU FP7 (grant no.
296150). The Know-Center is funded within the Austrian COMET Program - Competence Centers for
Excellent Technologies - under the auspices of the Austrian Federal Ministry of Transport, Innovation
and Technology, the Austrian Federal Ministry of Economy, Family and Youth and by the State of Styria.
COMET is managed by the Austrian Research Promotion Agency FFG.
References
[Alley1996] Alley, M. 1996. The Craft of Scientific Writing. Springer.
[Crompton1997] Crompton, P. 1997. Hedging in academic writing: Some theoretical problems. English for
Specific Purposes 16 (4).
[Di Marco2006] Di Marco, C., Kroon, F. and Mercer R. 2006. Using Hedges to Classify Citations in Scientific
Articles. Computing Attitude and Affect in Text: Theory and Applications. Springer Netherlands.
[Farkas et al.2007] Farkas, R., Vincze, V., Mora, G., Csirik, J. and Szarvas, G. 2010. The CoNLL-2010 shared task:
learning to detect hedges and their scope in natural language text. Proceedings of the Fourteenth Conference
on Computational Natural Language Learning.
[Garfield1972] Garfield, E. 1972. Citation analysis as a tool in journal evaluation. Science (178).
[Garfield1979] Garfield, E. 1979. Citation Indexing: Its Theory and Applications in Science, Technology, and
Humanities. John Wiley, New York, NY.
[Haggan2003] Haggan, M. 2003. Research paper titles in literature, linguistics and science: dimensions of attrac-
tion. Pragmatics 36 (2).
[Lakoff1972] Lakoff, G. 1972. Hedges: A study of meaning criteria and the logic of fuzzy concepts. Papers from
the Eighth Regional Meeting, Chicago Linguistics Society Papers.
[Lebrun2007] Lebrun, J. 2007. Scientific Writing. World Scientific Publishing Co Pte Ltd.
[Liakata et al.2012] Liakata, M., Saha, S., Dobnik, S., Batchelor, C. and Rebholz-Schuhmann, D. 2012. Automatic
recognition of conceptualization zones in scientific articles and two life science applications. Bioinformatics 28
(7).
[Liakata et al.2013] Liakata, M., Dobnik, S., Saha, S., Batchelor, C. and Rebholz-Schuhmann, D. 2013. A
discourse-driven content model for summarising scientific articles evaluated in a complex question answering
task. Proceedings of the Conference on Empirical Methods in Natural Language Processing.
[Klampfl et al.2013] Klampfl, S. and Kern, R. 2013. An Unsupervised Machine Learning Approach to Body Text
and Table of Contents Extraction from Digital Scientific Articles. Research and Advanced Technology for
Digital Libraries.
[Paltridge2002] Paltridge, B. 2002. Thesis and dissertation writing: an examination of published advice and actual
practice. English for Specific Purposes 21 (2).
[Pettigrew et al.2001] Pettigrew, K. and McKechnie, L. 2001. The use of theory in information science research.
American Society for Information Science and Technology, 52.
[Ravenscroft et al.2013] Ravenscroft, J., Liakata, M. and Clare, A. 2013. Partridge: An Effective System for
the Automatic Classification of the Types of Academic Papers. AI-2013: The Thirty-third SGAI International
Conference.
[Rubin2004] Rubin, R. 2004. Foundations of Library and Information Science. 2nd ed. New York: Neal-Schuman.
[Soler2007] Soler, V. 2007. Writing titles in science: An exploratory study. English for Specific Purposes 26 (1).
[Tas2010] Tas, E. 2010. ?In this paper I will discuss?: Current trends in academic writing. Procedia - Social and
Behavioral Sciences.
[Teufel et al.2002] Teufel, S. and Marc Moens, M. 2002. Summarizing scientific articles: experiments with rele-
vance and rhetorical status. Computational Linguistics 28 (4).
55
