Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 801?809, Prague, June 2007. c?2007 Association for Computational Linguistics
Finding Good Sequential Model Structures
using Output Transformations
Edward Loper
Department of Computer & Information Science
University of Pennsylvania
3330 Walnut Street
Philadelphia, PA 19104
edloper@cis.upenn.edu
Abstract
In Sequential Viterbi Models, such as
HMMs, MEMMs, and Linear Chain CRFs,
the type of patterns over output sequences
that can be learned by the model depend di-
rectly on the model?s structure: any pattern
that spans more output tags than are covered
by the models? order will be very difficult
to learn. However, increasing a model?s or-
der can lead to an increase in the number of
model parameters, making the model more
susceptible to sparse data problems.
This paper shows how the notion of output
transformation can be used to explore a va-
riety of alternative model structures. Us-
ing output transformations, we can selec-
tively increase the amount of contextual in-
formation available for some conditions, but
not for others, thus allowing us to capture
longer-distance consistencies while avoid-
ing unnecessary increases to the model?s pa-
rameter space. The appropriate output trans-
formation for a given task can be selected by
applying a hill-climbing approach to held-
out data. On the NP Chunking task, our
hill-climbing system finds a model structure
that outperforms both first-order and second-
order models with the same input feature set.
1 Sequence Prediction
A sequence prediction task is a task whose input is
a sequence and whose output is a corresponding se-
quence. Examples of sequence prediction tasks in-
clude part-of-speech tagging, where a sequence of
words is mapped to a sequence of part-of-speech
tags; and IOB noun phrase chunking, where a se-
quence of words is mapped to a sequence of labels,
I, O, and B, indicating whether each word is inside a
chunk, outside a chunk, or at the boundary between
two chunks, respectively.
In sequence prediction tasks, we are interested in
finding the most likely output sequence for a given
input. In order to be considered likely, an output
value must be consistent with the input value, but it
must also be internally consistent. For example, in
part-of-speech tagging, the sequence ?preposition-
verb? is highly unlikely; so we should reject an out-
put value that contains that sequence, even if the
individual tags are good candidates for describing
their respective words.
2 Sequential Viterbi Models
This intuition is captured in many sequence learning
models, including HiddenMarkovModels (HMMs),
Maximum Entropy Markov Models (MEMMs), and
Linear Chain Conditional Random Fields (LC-
CRFs), by including terms corresponding to pieces
of output structure in their scoring functions. (Sha
and Pereira, 2003; Sutton andMcCallum, 2006; Mc-
Callum et al, 2000; Alpaydin, 2004)
Each of these Sequential Viterbi Models defines
a set of scoring functions that evaluate fixed-size
pieces of the output sequence based on fixed-size
pieces of the input sequence.1 The overall score for
1For HMMs and MEMMs, the local scores are negative log
probabilities. For LC-CRFs, the local scores do not have any
direct probabilistic interpretation.
801
(a)
(b)
(c)
(d)
Figure 1: Common Model Structures. (a) Simple
first order. (b) Extended first order. (c) Simple sec-
ond order. (d) Extended second order.
an output value is then computed by summing the
scores for all its fixed-size pieces. Sequence predic-
tion models can differ from one another along two
dimensions:
1. Model Structure: The set of output pieces and
input pieces for which local scoring functions
are defined.
2. Model Type: The set of parametrized equa-
tions used to define those local scoring func-
tions, and the procedures used to determine
their parameters.
In this paper, we focus on model structure. In par-
ticular, we are interested in finding a suitable model
structure for a given task and training corpus.
2.1 Common Model Structures
The model structure used by classical HMMs is the
?simple first order? structure. This model structure
defines two local scoring functions. The first scoring
function evaluates an output value in the context of
the corresponding input value; and the second scor-
ing function evaluates adjacent pairs of output val-
ues. Simple LC-CRFs often extend this structure by
adding a third local scoring function, which evalu-
ates adjacent pairs of output values in the context of
the input value corresponding to one of those out-
puts. These model structures are illustrated in Fig-
ure 1.
Because these first order structures include scor-
ing functions for adjacent pairs of output items,
they can identify and reject output values that con-
tain improbable subsequences of length two. For
example, in part-of-speech tagging, the sequence
?preposition-verb? is highly unlikely; and such
models will easily learn to reject outputs contain-
ing that sequence. However, it is much more dif-
ficult for first order models to identify improbable
subsequences of length three or more. For example,
in English texts, the sequence ?verb-noun-verb? is
much less likely than one would predict based just
on the subsequences ?verb-noun? and ?noun-verb.?
But first order models are incapable of learning that
fact.
Thus, in order to improve performance, it is of-
ten necessary to include scoring functions that span
over larger sequences. In the ?simple second order?
model structure, the local scoring function for adja-
cent pairs of output values is replaced with a scoring
function for each triple of consecutive output values.
In extended versions of this structure typically used
by LC-CRFs, scoring functions are also added that
combine output value triples with an input value.
These model structures are illustrated in Figure 1.
Similarly, third order and and fourth order models
can be used to further increase the span over which
scoring functions are defined.
Moving to higher order model structures increases
the distance over which the model can check con-
sistency. However, it also increases the number of
parameters the model must learn, making the model
more susceptible to sparse data problems. Thus, the
usefulness of a model structure for a given task will
depend on the types of constraints that are important
for the task itself, and on the size and diversity of the
training corpus.
3 Searching for Good Model Structures
We can therefore use simple search methods to look
for a suitable model structure for a given task and
training corpus. In particular, we have performed
several experiments using hill-climbing methods to
search for an appropriate model structure for a given
task. In order to apply hill-climbing methods, we
need to define:
1. The search space. I.e., concrete representations
for the set of model structures we will consider.
2. A set of operations for moving through that
search space.
802
3. An evaluation metric.
In Section 4, we will define the search space us-
ing transformations on output values. This will al-
low us to consider a wide variety of model struc-
tures without needing to make any direct modifica-
tions to the underlying sequence modelling systems.
Output value transformations will be concretely rep-
resented using Finite State Transducers (FSTs). In
Section 5, we will define the set of operations for
moving through the search space as modification op-
erations on FSTs. For the evaluation metric, we sim-
ply train and test the model, using a given model
structure, on held-out data.
4 Representing Model Structure with
Reversible Output Transformations
The common model structures described in Sec-
tion 2.1 differ from one another in that they exam-
ine varying sizes of ?windows? on the output struc-
ture. Rather than varying the size of the window, we
can achieve the same effect by fixing the window
size, but transforming the output values. For exam-
ple, consider the effects of transforming the output
values by replacing individual output tags with pairs
of adjacent output tags:
y1, y2, . . . , yt ?
?START, y1?, ?y1, y2?, ?y2, y3?, . . . , ?yt?1, yt?
E.g.:
I O O I I B I ?
OI IO OO OI II IB BI
Training a first order model based on these trans-
formed values is equivalent to training a second or-
der model based on the original values, since in each
case the local scoring functions will be based on
pairs of adjacent output tags. Similarly, transform-
ing the output values by replacing individual output
tags with triples of adjacent output tags is equivalent
to training a third order model based on the original
output values.
Of course, when we apply a model trained on this
type of transformed output to new inputs, it will gen-
erate transformed output values. Thus, the transfor-
mation must be reversible, so that we can map the
output of the model back to an un-transformed out-
put value.
This transformational approach has the advantage
that we can explore different model structures us-
ing off-the-shelf learners, without modifying them.
In particular, we can apply the transformation corre-
sponding to a given model structure to the training
corpus, and then train the off-the-shelf learner based
on that transformed corpus. To predict the value for
a new input, we simply apply the learned model to
generate a corresponding transformed output value,
and then use the inverse transformation to map that
value back to an un-transformed value.
Output encoding transformations can be used to
represent a large class of model structures, including
commonly used structures (first order, second order,
etc) as well as a number of ?hybrid? structures that
use different window sizes depending on the content
of the output tags.
Output encoding transformations can also be used
to represent a wide variety of other model struc-
tures. For example, there has been some debate
about the relative merits of different output encod-
ings for the chunking task (Tjong Kim Sang and
Veenstra, 1999; Tjong Kim Sang, 2000; Shen and
Sarkar, 2005). These encodings differ in whether
they define special tags for the beginning of chunks,
for the ends of chunks, and for boundaries between
chunks. The output transformation procedure de-
scribed here is capable of capturing all of the output
encodings used for chunking. Thus, this transforma-
tional method provides a unified framework for con-
sidering both the type of information that should be
encoded by individual tags (i.e., the encoding) and
the distance over which that information should be
evaluated (i.e., the order of the model). Under this
framework, we can use simple search procedures to
find an appropriate transformation for a given task.
4.1 Representing Transformations as FSTs
Finite State Transducers (FSTs) provide a natural
formalism for representing output transformations.
FSTs are powerful enough to capture different or-
ders of model structure, including hybrid orders; and
to capture different output encodings, such as the
ones considered in (Shen and Sarkar, 2005). FSTs
are efficient, so they add very little overhead. Fi-
nally, there exist standard algorithms for inverting
803
O:O
I:?
I:I
B:E
O:IO
O:O
I:?
I:I
B:E
O:EO
IOE1
IOE2
I:I
B:B
IOB1
O:O
O:O
I:B
I:I
B:B
O:O
IOB2
IOBES
O:O
I
:
?
I
:
B
B
:
B
O
:
S
O
B:B
I:I
O
:
S
O
Figure 2: FSTs for Five Common Chunk Encod-
ings. Each transducer takes an IOB1-encoded string
for a given output value, and generates the corre-
sponding string for the same output value, using a
new encoding. Note that the IOB1 FST is an iden-
tity transducer; and note that the transducers that
make use of the E tag must use -output edges to
delay the decision of which tag should be used until
enough information is available.
and determinizing FSTs. 2
4.1.1 Necessary Properties for
Output-Transformation FSTs
In order for an FST to be used to transform output
values, it must have the following three properties:
1. The FST?s inverse should be deterministic.3
Otherwise, we will be unable to convert
the model?s (transformed) output into an un-
transformed output value.
2. The FST should recognize exactly the set of
valid output values. If it does not recognize
some valid output value, then it won?t be able
to transform that value. If it recognizes some
invalid output value, then there exists an trans-
formed output value that would map back to an
invalid output value.
3. The FST should not modify the length of the
output sequence. Otherwise, it will not be pos-
2Note that we are not attempting to learn a transducer
that generates the output values from input values, as is done
in e.g. (Oncina et al, 1993) and (Stolcke and Omohundro,
1993). Rather, we we are interested in finding a transducer from
one output encoding to another output encoding that will be
more amenable to learning by the underlying Sequential Viterbi
Model.
3Or at least determinizable.
sible to align the output values with input val-
ues when running the model.
In addition, it seems desirable for the FST to have
the following two properties:
4. The FST should be deterministic. Otherwise, a
single training example?s output could be en-
coded in multiple ways, which would make
training the individual base decision classifiers
difficult.
5. The FST should generate every output string.
Otherwise, there would be some possible sys-
tem output that we are unable to map back to
an un-transformed output.
Unfortunately, these two properties, when taken to-
gether with the first three, are problematic. To see
why, assume an FST with an output alphabet of
size k. Property (5) requires that all possible out-
put strings be generated, and property (1) requires
that no string is generated for two input strings,
so the number of strings generated for an input
of length n must be exactly kn. But the number
of possible chunkings for an input of length n is
3n ? 3n?1 ? 3n?2; and there is no integer k such
that kn = 3n ? 3n?1 ? 3n?2.4
We must therefore relax at least one of these two
properties. Relaxing the property 4 (deterministic
FSTs) will make training harder; and relaxing the
property 5 (complete FSTs) will make testing harder.
In the experiments presented here, we chose to relax
the second property.
4.1.2 Inverting the Transformation
Recall that the motivation behind property 5 is
that we need a way to map any output generated
by the machine learning system back to an un-
transformed output value.
As an alternative to requiring that the FST gener-
ate every output string, we can define an extended
inversion function, that includes the inverted FST,
but also generates output values for transformed val-
ues that are not generated by the FST. In particular,
4To see why the number of possible chunkings is 3n ?
3n?1 ? 3n?2, consider the IOB1 encoding: it generates all
chunkings, and is valid for any of the 3n strings except those
that start with B (of which there are 3n?1) and those that in-
clude the sequence OB (of which there are 3n?2).
804
in cases where the transformed value is not gener-
ated by the FST, we can assume that one or more
of the transformed tags was chosen incorrectly; and
make the minimal set of changes to those tags that
results in a string that is generated by the FST. Thus,
we can compute the optimal un-transformed output
value corresponding to each transformed output us-
ing the following procedure:
1. Invert the original FST. I.e., replace each arc
?S ? Q[? : ?]? with an arc ?S ? Q[? : ?]?.
2. Normalize the FST such that each arc has ex-
actly one input symbol.
3. Convert the FST to a weighted FST by as-
signing a weight of zero to all arcs. This
weighted FST uses non-negative real-valued
weights, and the weight of a path is the sum
of the weights of all edges in that path.
4. For each arc ?S ? Q[x : ?]?, and each y 6= x,
add a new arc ?S ? Q[y : ?]? with a weight
one.
5. Determinize the resulting FST, using a vari-
ant of the algorithm presented in (Mohri,
1997). This determinization algorithm will
prune paths that have non-optimal weights.
In cases where determinization algorithm has
not completed by the time it creates 10,000
states, the candidate FST is assumed to be non-
determinizable, and the original FST is rejected
as a candidate.
The resulting FST will accept all sequences of
transformed tags, and will generate for each trans-
formed tag the un-transformed output value that is
generated with the fewest number of ?repairs? made
to the transformed tags.
5 FST Modification Operations
In order to search the space of output-transforming
FSTs, we must define a set of modification oper-
ations, that generate a new FST from a previous
FST. In order to support a hill-climbing search
strategy, these modification operations should make
small incremental changes to the FSTs. The selec-
tion of appropriate modification operations is impor-
tant, since it will significantly impact the efficiency
of the search process. In this section, I describe the
set of FST modification operations that are used for
the experiments described in this paper. These oper-
ations were chosen based our intuitions about what
modifications would support efficient hill-climbing
search. In future experiments, we plan to examine
alternative modification operations.
5.1 New Output Tag
The new output tag operation replaces an arc ?S ?
Q[? : ?x?]? with an arc ?S ? Q[? : ?y?]?, where
y is a new output tag that is not used anywhere else
in the transducer. When a single output tag appears
on multiple arcs, this operation effectively splits that
tag in two. For example, when applied to the identity
transducer for the IOB1 encoding shown in Figure 2,
this operation can be used to distinguish O tags that
follow other O tags from O tags that follow I or B
tags ? effectively increasing the order of the model
structure for just O tags.
5.2 Specialize Output Tag5
The specialize output tag operation is similar to the
new output tag operation, but rather than replacing
the output tag with a new tag, we ?subdivide? the
tag. When the model is trained, features will be in-
cluded for both the subdivided tag and the original
(undivided) tag.
5.3 Loop Unrolling
The loop unrolling operation acts on a single self-
loop arc e at a state S, and makes the following
changes to the FST:
1. Create a new state S?.
2. For each outgoing arc e1 = ?S ? Q[? : ?]? 6=
e, add add an arc e2 = ?S? ? Q[? : ?]?. Note
that if e1 was a self-loop arc (i.e., S = Q), then
e2 will point from S? to S.
3. Change the destination of loop arc e from S to
S?.
By itself, the loop unrolling operation just mod-
ifies the structure of the FST, but does not change
5This operation requires the use of a model where features
are defined over (input,output) pairs, such as MEMMs or LC-
CRFs.
805
the actual transduction performed by the FST. It is
therefore always immediately followed by applying
the new output tag operation or the specialize output
tag operation to the loop arc e.
5.4 Copy Tag Forward
The copy tag forward operation splits an existing
state in two, directing all incoming edges that gen-
erate a designated output tag to one copy, and all
remaining incoming edges to the other copy. The
outgoing edges of these two states are then distin-
guished from one another, using either the specialize
output tag operation (if available) or the new output
tag operation.
This modification operation creates separate
edges for different output histories, effectively in-
creasing the ?window size? of tags that pass through
the state.
5.5 Copy State Forward
The copy state forward operation is similar to the
copy tag forward operation; but rather than redirect-
ing incoming edges based on what output tags they
generate, it redirects incoming edges based on what
state they originate from. This modification opera-
tion allows the FST to encode information about the
history of states in the transformational FST as part
of the model structure.
5.6 Copy Feature Forward
The copy feature forward operation is similar to the
copy tag forward operation; but rather than redirect-
ing incoming edges based on what output tags they
generate, it redirects incoming edges based on a fea-
ture of the current input value. This modification op-
eration allows the transformation to subdivide out-
put tags based on features of the input value.
6 Hill Climbing System
Having defined a search space, a set of transforma-
tions to explore that space, and an evaluation met-
ric, we can use a hill-climbing system to search for
a good model structure. This approach starts with
a simple initial FST, and makes incremental local
changes to that FST until a locally optimal FST is
found. In order to help avoid sub-optimal local max-
ima, we use a fixed-size beam search. To increase
the search speed, we used a 12-machine cluster to
evaluate candidate FSTs in parallel. The hill climb-
ing system iteratively performs the following proce-
dure:
1. Initialize candidates to be the singleton set
containing the identity transducer.
2. Repeat ...
(a) Generate a new FST, by applying a ran-
dom modification operation to a randomly
selected member of the candidates
set.
(b) Evaluate the new FSTs, and test its perfor-
mance on the held-out data set. (This is
done in parallel.)
(c) Once the FST has been evaluated, add it to
the candidates set.
(d) Sort the candidates set by their score
on the held-out data, and discard all but
the 10 highest-scoring candidates.
... until no improvement is made for twenty
consecutive iterations.
3. Return the candidate FST with the highest
score.
7 Noun Phrase Chunking Experiment
In order to test this approach to finding a good model
structure, we applied our hill-climbing system to the
task of noun phrase chunking. The base system
was a Linear Chain CRF, implemented using Mal-
let (McCallum, 2002). The set of features used are
listed in Figure 1. Training and testing were per-
formed using the noun phrase chunking corpus de-
scribed in Ramshaw & Marcus (1995) (Ramshaw
and Marcus, 1995). A randomly selected 10% of the
original training corpus was used as held-out data,
to provide feedback to the hill-climbing system.
7.1 NP Chunking Experiment: Results
Over 100 iterations, the hill-climbing system in-
creased chunking performance on the held-out data
from a F-score of 94.93 to an F-score of 95.32.
This increase was reflected in an improvement on
the test data from an F-score of 92.48 to an F-score
806
Feature Description
yi The current output tag.
yi, wi+n A tuple of the current output tag and
the i + nth word, ?2 ? n ? 2.
yi, wi, wi?1 A tuple of the current output tag, the
current word, and the previous word.
yi, wi, wi+1 A tuple of the current output tag, the
current word, and the next word.
yi, ti+n A tuple of the current output tag and
the part of speech tag of the i + nth
word, ?2 ? n ? 2.
yi, ti+n,
ti+n+1
A tuple of the current output tag and
the two consecutive part of speech
tags starting at word i + n, ?2 ?
n ? 1.
yi+n?1, ti+n,
ti+n+1
A tuple of the current output tag, and
three consecutive part of speech tags
centered on word i+n,?1 ? n ? 1.
Table 1: Feature Set for the CRF NP Chunker. yi
is the ith output tag; wi is the ith word; and ti is the
part-of-speech tag for the ith word.
System F1 (Held-out) F1 (Test)
Baseline (first order) 94.93 92.48
Second order 95.14 92.63
Learned structure 95.32 92.80
Table 2: Results for NP Chunking Experiment.
of 92.80.6 As a point of comparison, a simple sec-
ond order model achieves an intermediate F-score of
92.63 on the test data. Thus, the model learned by
the hill-climbing system outperforms both the sim-
ple first-order model and the simple second-order
model.
Figure 3 shows how the scores of FSTs on held-
out data changed as the hill-climbing system ran.
Figure 4 shows the search tree explored by the hill-
climbing system.
6The reason that held-out scores are significantly higher than
test scores is that held-out data was taken from the same sec-
tions of the original corpus as the training data; but test data was
taken from new sections. Thus, there was more lexical overlap
between the training data and the held-out data than between
the training data and the testing data.
...
Figure 3: Performance on Heldout Data for NP
Chunking Experiment. In this graph, each point
corresponds to a single transducer generated by the
hill-climbing system. The height of each trans-
ducer?s point indicates its score on held-out data.
The line indicates the highest score that has been
achieved on the held-out data by any transducer.
Figure 4: Hill Climbing Search Tree for NP
Chunking Experiment This tree shows the ?an-
cestry? of each transducer tried by the hill climb-
ing system. Lighter colors indicate higher scores
on the held-out data. After one hundred iterations,
the five highest scoring transducers were fst047,
fst058, fst083, fst102, and fst089.
807
S0
S
1
S
2
O:O
1
O
:
O
1
O:O
1
B:B
1
B
:
B
2
I
:
I
1
I
+
<
t
[
-
2
;
-
1
]
=
N
N
P
;
,
>
:
I
2
I
-
<
t
[
-
2
;
-
1
]
=
N
N
P
;
,
>
:
I
3
I
+<t[-2;-1]=NNP;,>
:I
2
I
-<t[-2;-1]=NNP;,>
:I
3
Figure 5: Final FST. The highest-scoring FST gen-
erated by the hill-climbing algorithm, after a run of
100 iterations. For a discussion of this transducer,
see Section 7.1.1.
7.1.1 NP Chunking Experiment: The Selected
Transformation
Figure 5 shows the FST for the best output trans-
formation found after 100 iterations of the hill-
climbing algorithm. Inspection of this FST reveals
that it transforms the original set of three tags (I , O,
and B) to six new tags: I1, I2, I3, O, B1, and B2.
The first three of these tags are used at the begin-
ning of a chunk: I1 is used if the preceding tag was
O; B1 is used if the preceding tag was B; and B2
is used if the preceding tag was I . This is similar to
a second order model, in that it records information
about both the current tag and the previous tag.
The next tag, O, is used for all words outside of
chunks. Thus, the hill-climbing system found that
increasing the window size used for O chunks does
not help to learn any useful constraints with neigh-
boring tags.
Finally, two tags are used for words that are inside
a chunk, but not at the beginning of the chunk: I2
and I3. The choice of which tag should be used de-
pends on the input feature that tests whether the cur-
rent word is a comma, and the previous word was a
proper noun (NNP). At first, this might seem like an
odd feature to distinguish. But note that in the Wall
Street Journal, it is quite common for proper nouns
to include internal commas; but for other nouns, it is
fairly uncommon. By dividing the I tag in two based
on this feature, the model can use separate distribu-
tions for these two cases. Thus, the model avoids
conflating two contexts that are significantly differ-
ent from one another for the task at hand.
8 Discussion
Sequential Viterbi Models are capable of learning to
model the probability of local patterns on the out-
put structure. But the distance that these patterns
can span is limited by the model?s structure. This
distance can be lengthened by moving to higher or-
der model structures, but only at the expense of an
increase in the number of model parameters, along
with the data sparsity issues that can arise from that
increase. Therefore, it makes sense to be more selec-
tive about how we extend the model structure. Using
reversible output transformations, it is possible to
define model structures that extend the reach of the
model only where necessary. And as we have shown
here, it is possible to find a suitable output transfor-
mation for a given task by using simple search pro-
cedures.
9 Acknowledgements
We gratefully acknowledge the support of the
National Science Foundation Grant NSF-0415923,
Word Sense Disambiguation, the DTO-AQUAINT
NBCHC040036 grant under the University of
Illinois subcontract to University of Penn-
sylvania 2003-07911-01, NSF-ITR-0325646:
Domain-Independent Semantic Interpretation, and
Defense Advanced Research Projects Agency
(DARPA/IPTO) under the GALE program,
DARPA/CMO Contract No. HR0011-06-C-0022.
References
Ethem Alpaydin, 2004. Introduction to Machine Learn-
ing, chapter 7. The MIT Press.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy Markov models
for information extraction and segmentation. In Proc.
17th International Conf. on Machine Learning, pages
591?598. Morgan Kaufmann, San Francisco, CA.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
808
Mehryar Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Linguis-
tics, 23(2):269?311.
Jose? Oncina, Pedro Garc??a, and Enrique Vidal. 1993.
Learning subsequential transducers for pattern recog-
nition tasks. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 15:448?458, May.
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In David
Yarowsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora, pages
82?94, Somerset, New Jersey. Association for Compu-
tational Linguistics.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings of
HLT-NAACL, pages 134?141.
Hong Shen and Anoop Sarkar. 2005. Voting between
multiple data representations for text chunking. In Ad-
vances in Artificial Intelligence: 18th Conference of
the Canadian Society for Computational Studies of In-
telligence, May.
Andreas Stolcke and Stephen Omohundro. 1993. Hidden
Markov Model induction by Bayesian model merging.
In C. L. Giles, S. J. Hanson, and J. D. Cowan, editors,
Advances in Neural Information Processing Systems 5.
Morgan Kaufman, San Mateo, Ca.
Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press. To appear.
Erik Tjong Kim Sang and Jorn Veenstra. 1999. Rep-
resenting text chunks. In Proceedings of EACL?99,
Bergen. Association for Computational Linguistics.
Erik Tjong Kim Sang. 2000. Noun phrase recognition
by system combination. In Proceedings of BNAIC,
Tilburg, The Netherlands.
809
Proceedings of NAACL HLT 2007, pages 548?555,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Can Semantic Roles Generalize Across Genres?
Szu-ting Yi
Dept of Computer Science
University of Pennsylvania
Philadelphia, PA 19104
Edward Loper
Dept of Computer Science
University of Pennsylvania
Philadelphia, PA 19104
Martha Palmer
Dept of Computer Science
University of Colorado at Boulder
Boulder, CO 80309
Abstract
PropBank has been widely used as train-
ing data for Semantic Role Labeling.
However, because this training data is
taken from the WSJ, the resulting machine
learning models tend to overfit on idiosyn-
crasies of that text?s style, and do not port
well to other genres. In addition, since
PropBank was designed on a verb-by-verb
basis, the argument labels Arg2 - Arg5 get
used for very diverse argument roles with
inconsistent training instances. For exam-
ple, the verb ?make? uses Arg2 for the
?Material? argument; but the verb ?multi-
ply? uses Arg2 for the ?Extent? argument.
As a result, it can be difficult for auto-
matic classifiers to learn to distinguish ar-
guments Arg2-Arg5. We have created a
mapping between PropBank and VerbNet
that provides a VerbNet thematic role la-
bel for each verb-specific PropBank label.
Since VerbNet uses argument labels that
are more consistent across verbs, we are
able to demonstrate that these new labels
are easier to learn.
1 Introduction
Correctly identifying semantic entities and success-
fully disambiguating the relations between them and
their predicates is an important and necessary step
for successful natural language processing applica-
tions, such as text summarization, question answer-
ing, and machine translation. For example, in or-
der to determine that question (1a) is answered by
sentence (1b), but not by sentence (1c), we must de-
termine the relationships between the relevant verbs
(eat and feed) and their arguments.
(1) a. What do lobsters like to eat?
b. Recent studies have shown that lobsters pri-
marily feed on live fish, dig for clams, sea
urchins, and feed on algae and eel-grass.
c. In the early 20th century, Mainers would
only eat lobsters because the fish they
caught was too valuable to eat themselves.
An important part of this task is Semantic Role
Labeling (SRL), where the goal is to locate the con-
stituents which are arguments of a given verb, and to
assign them appropriate semantic roles that describe
how they relate to the verb. Many researchers have
investigated applying machine learning to corpus
specifically annotated with this task in mind, Prop-
Bank, since 2000 (Chen and Rambow, 2003; Gildea
and Hockenmaier, 2003; Hacioglu et al, 2003; Mos-
chitti, 2004; Yi and Palmer, 2004; Pradhan et al,
2005b; Punyakanok et al, 2005; Toutanova et al,
2005). For two years, the CoNLL workshop has
made this problem the shared task (Carreras and
Ma?rquez, 2005). However, there is still little con-
sensus in the linguistic and NLP communities about
what set of role labels are most appropriate. The
Proposition Bank (PropBank) corpus (Palmer et al,
2005) avoids this issue by using theory-agnostic la-
bels (Arg0, Arg1, . . . , Arg5), and by defining those
labels to have verb-specific meanings. Under this
scheme, PropBank can avoid making any claims
548
about how any one verb?s arguments relate to other
verbs? arguments, or about general distinctions be-
tween verb arguments and adjuncts.
However, there are several limitations to this ap-
proach. The first is that it can be difficult to make
inferences and generalizations based on role labels
that are only meaningful with respect to a single
verb. Since each role label is verb-specific, we can
not confidently determine when two different verbs?
arguments have the same role; and since no encoded
meaning is associated with each tag, we can not
make generalizations across verb classes. In con-
trast, the use of a shared set of role labels, such as
thematic roles, would facilitate both inferencing and
generalization.
The second issue with PropBank?s verb-specific
approach is that it can make training automatic se-
mantic role labeling (SRL) systems more difficult.
A vast amount of data would be needed to train the
verb-specific models that are theoretically mandated
by PropBank?s design. Instead, researchers typically
build a single model for the numbered arguments
(Arg0, Arg1, . . . , Arg5). This approach works sur-
prisingly well, mainly because an explicit effort was
made to use arguments Arg0 and Arg1 consistently
across different verbs; and because those two argu-
ment labels account for 85% of all arguments. How-
ever, this approach causes the system to conflate
different argument types, especially with the highly
overloaded arguments Arg2-Arg5. As a result, these
argument labels are quite difficult to learn.
A final difficulty with PropBank?s current ap-
proach is that it limits SRL system robustness in
the face of verb senses, verbs or verb constructions
that were not included in the training data, and the
training data is all Wall Street Journal corpora. If
a PropBank-trained SRL system encounters a novel
verb or verb usage, then there is no way for it to
know which role labels are used for which argument
types, since role labels are defined so specifically.
This is especially problematic for Arg2-5. Similarly,
PropBank-trained SRL systems can have difficulty
generalizing when a known verb is encountered in
a novel construction. These problems can happen
quite frequently if the training data comes from a
different genre than the test data. This issue is re-
flected in the relatively poor performance of most
state-of-the-art SRL systems when tested on a novel
genre, the Brown corpus, during CoNLL 2005. For
example, the SRL system described in (Pradhan et
al., 2005b; Pradhan et al, 2005a) achieves an F-
score of 81% when tested on the same genre as it
is trained on (WSJ); but that score drops to 68.5%
when the same system is tested on a different genre
(the Brown corpus). DARPA-GALE is funding an
ongoing effort to PropBank additional genres, but
better techniques for generalizing the semantic role
labeling task are still needed.
In this paper, we demonstrate an increase in the
generality of our semantic role labeling based on a
mapping that has been developed between PropBank
and another lexical resource, VerbNet. By taking ad-
vantage of VerbNet?s more consistent set of labels,
we can generate more useful role label annotations
with a resulting improvement in SRL performance
on novel genres.
2 Background
2.1 PropBank
PropBank (Palmer et al, 2005) is an annotation of
one million words of the Wall Street Journal por-
tion of the Penn Treebank II (Marcus et al, 1994)
with predicate-argument structures for verbs, using
semantic role labels for each verb argument. In or-
der to remain theory neutral, and to increase anno-
tation speed, role labels were defined on a per-verb-
sense basis. Although the same tags were used for
all verbs, (namely Arg0, Arg1, ..., Arg5), these tags
are meant to have a verb-specific meaning.
Thus, the use of a given argument label should
be consistent across different uses of that verb, in-
cluding syntactic alternations. For example, the
Arg1 (underlined) in ?John broke the window? is the
same window that is annotated as the Arg1 in ?The
window broke?, even though it is the syntactic sub-
ject in one sentence and the syntactic object in the
other. However, there is no guarantee that an argu-
ment label will be used consistently across different
verbs. For example, the Arg2 label is used to des-
ignate the destination of the verb ?bring;? but the
extent of the verb ?rise.? Generally, the arguments
are simply listed in the order of their prominence
for each verb. However, an explicit effort was made
when PropBank was created to use Arg0 for argu-
ments that fulfill Dowty?s criteria for ?prototypical
549
agent,? and Arg1 for arguments that fulfill the cri-
teria for ?prototypical patient.? (Dowty, 1991) As
a result, these two argument labels are significantly
more consistent across verbs than the other three.
But nevertheless, there are still some inter-verb in-
consistencies for even Arg0 and Arg1.
2.2 VerbNet
VerbNet (Schuler, 2005) consists of hierarchically
arranged verb classes, inspired by and extended
from classes of Levin 1993 (Levin, 1993). Each
class and subclass is characterized extensionally by
its set of verbs, and intensionally by a list of the
arguments of those verbs and syntactic and seman-
tic information about the verbs. The argument list
consists of thematic roles (23 in total) and pos-
sible selectional restrictions on the arguments ex-
pressed using binary predicates. The syntactic infor-
mation maps the list of thematic arguments to deep-
syntactic arguments (i.e., normalized for voice alter-
nations, and transformations). The semantic predi-
cates describe the participants during various stages
of the event described by the syntactic frame.
The same thematic role can occur in different
classes, where it will appear in different predicates,
providing a class-specific interpretation of the role.
VerbNet has been extended from the original Levin
classes, and now covers 4526 senses for 3769 verbs.
A primary emphasis for VerbNet is the grouping of
verbs into classes that have a coherent syntactic and
semantic characterization, that will eventually facil-
itate the acquisition of new class members based on
observable syntactic and semantic behavior. The hi-
erarchical structure and small number of thematic
roles is aimed at supporting generalizations.
2.3 Mapping PropBank to VerbNet
Because PropBank includes a large corpus of man-
ually annotated predicate-argument data, it can be
used to train supervised machine learning algo-
rithms, which can in turn provide PropBank-style
annotations for novel or unseen text. However, as
we discussed in the introduction, PropBank?s verb-
specific role labels are somewhat problematic. Fur-
thermore, PropBank lacks much of the information
that is contained in VerbNet, including information
about selectional restrictions, verb semantics, and
inter-verb relationships.
We have therefore created a mapping between
VerbNet and PropBank (Loper et al, 2007), which
will allow us to use the machine learning tech-
niques that have been developed for PropBank anno-
tations to generate more semantically abstract Verb-
Net representations. Additionally, the mapping can
be used to translate PropBank-style numbered ar-
guments (Arg0. . .Arg5) to VerbNet thematic roles
(Agent, Patient, Theme, etc.), which should allow us
to overcome the verb-specific nature of PropBank.
The mapping between VerbNet and PropBank
consists of two parts: a lexical mapping and an in-
stance classifier. The lexical mapping is responsible
for specifying the potential mappings between Prop-
Bank and VerbNet for a given word; but it does not
specify which of those mappings should be used for
any given occurrence of the word. That is the job
of the instance classifier, which looks at the word
in context, and decides which of the mappings is
most appropriate. In essence, the instance classi-
fier is performing word sense disambiguation, de-
ciding which lexeme from each database is correct
for a given occurrence of a word. In order to train
the instance classifier, we semi-automatically anno-
tated each verb in the PropBank corpus with Verb-
Net class information.1 This mapped corpus was
then used to build the instance classifier. More de-
tails about the mapping, and how it was created, can
be found in (Loper et al, 2007).
3 Analysis of the Mapping
In order to confirm our belief that PropBank roles
Arg0 and Arg1 are relatively coherent, while roles
Arg2-5 are much more overloaded, we performed
a preliminary analysis of how argument roles were
mapped. Figure 1 shows how often each PropBank
role was mapped to each VerbNet thematic role, cal-
culated as a fraction of instances in the mapped cor-
pus. From this figure, we can see that Arg0 maps to
agent-like roles, such as ?agent? and ?experiencer,?
over 94% of the time; and Arg1 maps to patient-
like roles, including ?theme,? ?topic,? and ?patient,?
over 82% of the time. In contrast, arguments Arg2-5
get mapped to a much broader variety of roles. It is
also worth noting that the sample size for arguments
1Excepting verbs whose senses are not present in VerbNet
(24.5% of instances).
550
Arg3-5 is quite small in comparison with arguments
Arg0-2, suggesting that any automatically built clas-
sifier for arguments Arg3-5 will suffer severe sparse
data problems for those arguments.
4 Training a SRL system with VerbNet
Roles to Achieve Robustness
An important issue for state-of-the-art automatic
SRL systems is robustness: although they receive
high performance scores when tested on the Wall
Street Journal (WSJ) corpus, that performance drops
significantly when the same systems are tested on a
corpus from another genre. This performance drop
reflects the fact that the WSJ corpus is highly spe-
cialized, and tends to use genre-specific word senses
for many verbs. The 2005 CoNLL shared task has
addressed this issue of robustness by evaluating par-
ticipating systems on a test set extracted from the
Brown corpus, which is very different from the WSJ
corpus that was used for training. The results sug-
gest that there is much work to be done in order to
improve system robustness.
One of the reasons that current SRL systems have
difficulty deciding which role label to assign to a
given argument is that role labels are defined on a
per-verb basis. This is less problematic for Arg0
and Arg1, where a conscious effort was made to be
consistent across verbs; but is a significant problem
for Args[2-5], which tend to have very verb-specific
meanings. This problem is exacerbated even fur-
ther on novel genres, where SRL systems are more
likely to encounter unseen verbs and uses of argu-
ments that were not encountered in the training data.
4.1 Addressing Current SRL Problems via
Lexical Mappings
By exploiting the mapping between PropBank and
VerbNet, we can transform the data to make it more
consistent, and to expand the size and variety of the
training data. In particular, we can use the map-
ping to transform the verb-specific PropBank role
labels into the more general thematic role labels that
are used by VerbNet. Unlike the PropBank labels,
the VerbNet labels are defined consistently across
verbs; and therefore it should be easier for statisti-
cal SRL systems to model them. Furthermore, since
the VerbNet role labels are significantly less verb-
Arg0 (45,579)
Agent 85.4%
Experiencer 7.2%
Theme 2.1%
Cause 1.9%
Actor1 1.8%
Theme1 0.8%
Patient1 0.2%
Location 0.2%
Theme2 0.2%
Product 0.1%
Patient 0.0%
Attribute 0.0%
Arg1 (59,884)
Theme 47.0%
Topic 23.0%
Patient 10.8%
Product 2.9%
Predicate 2.5%
Patient1 2.4%
Stimulus 2.0%
Experiencer 1.9%
Cause 1.8%
Destination 0.9%
Theme2 0.7%
Location 0.7%
Source 0.7%
Theme1 0.6%
Actor2 0.6%
Recipient 0.5%
Agent 0.4%
Attribute 0.2%
Asset 0.2%
Patient2 0.2%
Material 0.2%
Beneficiary 0.0%
Arg2 (11,077)
Recipient 22.3%
Extent 14.7%
Predicate 13.4%
Destination 8.6%
Attribute 7.6%
Location 6.5%
Theme 5.5%
Patient2 5.3%
Source 5.2%
Topic 3.1%
Theme2 2.5%
Product 1.5%
Cause 1.2%
Material 0.8%
Instrument 0.6%
Beneficiary 0.5%
Experiencer 0.3%
Actor2 0.2%
Asset 0.0%
Theme1 0.0%
Arg3 (609)
Asset 38.6%
Source 25.1%
Beneficiary 10.7%
Cause 9.7%
Predicate 9.0%
Location 2.0%
Material 1.8%
Theme1 1.6%
Theme 0.8%
Destination 0.3%
Instrument 0.3%
Arg4 (18)
Beneficiary 61.1%
Product 33.3%
Location 5.6%
Arg5 (17)
Location 100.0%
Figure 1: The frequency with which each PropBank
numbered argument is mapped to each VerbNet the-
matic role in the mapped corpus. The numbers
next to each PropBank argument reflects the num-
ber of occurrences of that numbered argument in the
mapped corpus.
551
dependent than the PropBank roles, the SRL?s mod-
els should generalize better to novel verbs, and to
novel uses of known verbs.
5 SRL Experiments on Linked Lexical
Resources
In order to verify the feasibility of performing se-
mantic role labeling with VerbNet thematic roles, we
re-trained our existing SRL system, which originally
used PropBank role labels, with a new label set that
makes use of VerbNet thematic role information.
5.1 The SRL System
Our SRL system is a Maximum Entropy based
pipelined system which consists of four compo-
nents: Pre-processing, Argument Identification, Ar-
gument Classification, and Post Processing. The
Pre-processing component pipes a sentence through
a syntactic parser and filters out constituents which
are unlikely to be semantic arguments based on a
constituents location in the parse tree. The Argu-
ment Identification component is a binary MaxEnt
classifier, which tags candidate constituents as ar-
guments or non-arguments. The Argument Classifi-
cation component is a multi-class MaxEnt classifier
which assigns a semantic role to each constituent.
The Post Processing component further selects the
final arguments based on global constraints. Our ex-
periments mainly focused on changes to the Argu-
ment Classification stage of the SRL pipeline, and
in particular, on changes to the set of output tags.
For more information on our SRL system, see (Yi
and Palmer, 2004; Yi and Palmer, 2005).
The evaluation of SRL systems is typically ex-
pressed by precision, recall and the F1-measure.
Precision is the number of correct arguments pre-
dicted by a system divided by the total number of
arguments proposed. Recall is the number of cor-
rect arguments divided by the number of the total
number of arguments in the Gold Standard Data. F1
computes the harmonic mean of precision and recall.
5.2 SRL Experiments on Mapped VerbNet
Thematic Roles
Since PropBank arguments Arg0 and Arg1 are al-
ready quite coherent, we left them as-is in the new
label set. But since arguments Arg2-Arg5 are highly
Group 1 Group 2 Group 3 Group 4 Group 5
Recipient Extent Predicate Patient2 Instrument
Destination Asset Attribute Product Cause
Location Theme Experiencer
Source Theme1 Actor2
Material Theme2
Beneficiary Topic
Figure 2: Thematic Role Groupings for the exper-
iments on linked lexical resources; and for Arg2 in
the experiments on arguments with different verb in-
dependency.
overloaded, we replaced them by mapping them
to their corresponding VerbNet thematic role. We
found that mapping directly to individual role labels
created a significant sparse data problem, since the
number of output tags was increased from 6 to 23.
We therefore grouped the VerbNet thematic roles
into five coherent groups of similar thematic roles,
shown in Figure 2.2 Our new tag set therefore in-
cluded the following tags: Arg0 (agent); Arg1 (pa-
tient); Group1 (goal); Group2 (extent); Group3
(predicate/attrib); Group4 (product); and Group5
(instrument/cause).
Training our SRL system using these thematic
role groups, we obtained performance similar to the
original SRL system. However, it is important to
note that these performance figures are not directly
comparable, since the two systems are performing
different tasks: The Original system labels Arg0-
5,ArgA and ArgM and the Mapped system labels
Arg0, Arg1, ArgA, ArgM and Group1-5. In partic-
ular, the role labels generated by the original system
are verb-specific, while the role labels generated by
the new system are less verb-dependent.
5.2.1 Results
For our testing and training, we used the portion
of Penn Treebank II that is covered by the mapping,
and where at least one of Arg2-5 is used. Training
was performed using sections 2-21 of the Treebank
(10,783 instances of argument); and testing was per-
formed on section 23 (859 instances). Table 1 dis-
plays the performance score for the SRL system us-
ing the augmented tag set (?Mapped?). The per-
formance score of the original system (?Original?)
is also listed, for reference; however, as was dis-
2Karin Kipper assisted in creating the groupings.
552
System Precision Recall F1
Original 90.65 85.43 87.97
Mapped 88.85 84.56 86.65
Table 1: Overall SRL System performance using the
PropBank tag set (?Original?) and the augmented
tag set (?Mapped?)
System Precision Recall F1
Original 97.60 83.67 90.10
Mapped 91.70 82.86 87.06
Table 2: SRL System performance evaluated on only
Arg2-5 (Original) or Group1-5 (Mapped).
cussed above, these results are not directly compara-
ble because the two systems are performing different
tasks.
The results indicate that the performance drops
when we train on the new argument labels, espe-
cially on precision when we evaluate the systems
on only Arg2-5/Group1-5 (see Table 2). However,
it is premature to conclude that there is no benefit
from the VerbNet thematic role labels. Firstly, we
have very few mapped Arg3-5 instances (less than
1,000 instances); secondly, we lack test data gen-
erated from a genre other than WSJ to allow us to
evaluate the robustness (generality) of SRL trained
on the new argument labels.
We therefore redesigned our experiments by lim-
iting the scope to mapped instances of Arg1 and
Arg2. By doing this, we should be able to accom-
plish the following: 1) we can map new argument la-
bels back to the original PropBank labels; therefore
we can directly compare results; 2) With the ability
of testing our systems on other test data, we can eval-
uate the influence of the mapping on SRL robust-
ness; 3) We can validate our original hypothesis that
the behavior of Arg1 is primarily verb-independent
while Arg2 is more verb-specific.
5.3 SRL Experiments on Arguments with
Different Verb Independency
We conducted two further sets of experiments: one
to test the effect of the mapping on learning Arg2;
and one to test the effect on learning Arg1. Since
Arg2 is used in very verb-dependent ways, we ex-
pect that mapping it to VerbNet role labels will in-
Group 1 Group 2 Group 3 Group 4 Group 5
Theme Source Patient Agent Topic
Theme1 Location Product Actor2
Theme2 Destination Patient1 Experiencer Group 6
Predicate Recipient Patient2 Cause Asset
Stimulus Beneficiary
Attribute Material
Figure 3: Thematic Role Groupings for Arg1 in the
experiments on arguments with different verb inde-
pendency.
crease our performance. However, since a conscious
effort was made to keep the meaning of Arg1 consis-
tent across verbs, we expect that mapping it to Verb-
Net labels will provide less of an improvement.
Each experiment compares two SRL systems: one
trained using the original PropBank role labels; the
other trained with the argument role under consid-
eration (Arg1 or Arg2) subdivided based on which
VerbNet role label it maps to. In order to prevent
the training data from these subdivided labels from
becoming too sparse (which would impair system
performance) we grouped similar thematic roles to-
gether. For Arg2, we used the same groupings as the
previous experiment, shown in Figure 2. The argu-
ment role groupings we used for Arg1 are shown in
Figure 3.
The training data for both experiments is the por-
tion of Penn Treebank II (sections 02-21) that is cov-
ered by the mapping. We evaluated each experi-
mental system using two test sets: section 23 of the
Penn Treebank II, which represents the same genre
as the training data; and the PropBank-ed portion of
the Brown corpus, which represents a very different
genre.
5.3.1 Results and Discussion
Table 3 describes the results of SRL overall per-
formance tested on the WSJ corpus Section 23; Ta-
ble 4 demonstrates the SRL overall system perfor-
mance tested on the Brown corpus. Systems Arg1-
Original and Arg2-Original are trained using the
original PropBank labels, and show the baseline
performance of our SRL system. Systems Arg1-
Mapped and Arg2-Mapped are trained using Prop-
Bank labels augmented with VerbNet thematic role
groups. In order to allow comparison between the
system using the original PropBank labels and the
systems that augmented those labels with VerbNet
553
System Precision Recall F1
Arg1-Original 89.24 77.32 82.85
Arg1-Mapped 90.00 76.35 82.61
Arg2-Original 73.04 57.44 64.31
Arg2-Mapped 84.11 60.55 70.41
Table 3: SRL System Performance on Arg1 Map-
ping and Arg2 Mapping, tested using the WSJ cor-
pus (section 23). This represents performance on the
same genre as the training corpus.
System Precision Recall F1
Arg1-Original 86.01 71.46 78.07
Arg1-Mapped 88.24 71.15 78.78
Arg2-Original 66.74 52.22 58.59
Arg2-Mapped 81.45 58.45 68.06
Table 4: SRL System Performance on Arg1 Map-
ping and Arg2 Mapping, tested using the PropBank-
ed Brown corpus. This represents performance on a
different genre from the training corpus.
thematic role groups, system performance was eval-
uated based solely on the PropBank role label that
was assigned.
We had hypothesized that with the use of thematic
roles, we would be able to create a more consis-
tent training data set which would result in an im-
provement in system performance. In addition, the
thematic roles would behave more consistently than
the overloaded Args[2-5] across verbs, which should
enhance robustness. However, since in practice we
are also increasing the number of argument labels
an SRL system needs to tag, the system might suf-
fer from data sparseness. Our hope is that the en-
hancement gained from the mapping will outweigh
the loss due to data sparseness.
From Table 3 and Table 4 we see the F1 scores of
Arg1-Original and Arg1-Mapped are statistically in-
different both on the WSJ corpus and the Brown cor-
pus. These results confirm the observation that Arg1
in the PropBank behaves fairly verb-independently
so that the VerbNet mapping does not provide much
benefit. The increase of precision due to a more co-
herent training data set is compensated for by the
loss of recall due to data sparseness.
The results of the Arg2 experiments tell a differ-
Confusion ARG2-Original
Matrix ARG1 ARG2 ARGM
ARG2- ARG0 53 50 -
Mapped ARG1 - 716 -
ARG2 1 - 2
ARG3 - 1 -
ARGM 1 482 -
233 ARG2-Mapped arguments are not labeled by ARG2-
Original
Table 5: Confusion matrix on the 1,539 instances
which ARG2-Mapped tags correctly and ARG2-
Original fails to predict.
ent story. Both precision and recall are improved
significantly, which demonstrates that the Arg2 label
in the PropBank is quite overloaded. The Arg2 map-
ping improves the overall results (F1) on the WSJ
by 6% and on the Brown corpus by almost 10%. As
a more diverse corpus, the Brown corpus provides
many more opportunities for generalizing to new us-
ages. Our new SRL system handles these cases more
robustly, demonstrating the consistency and useful-
ness of the thematic role categories.
5.4 Improved Argument Distinction via
Mapping
The ARG2-Mapped system generalizes well both
on the WSJ corpus and the Brown corpus. In or-
der to explore the improved robustness brought by
the mapping, we extracted and observed the 1,539
instances to which the system ARG2-Mapped as-
signed the correct semantic role label, but which the
system ARG2-Original failed to predict. From the
confusion matrix depicted in Table 5, we discover
the following:
The mapping makes ARG2 more clearly defined,
and as a result there is a better distinction be-
tween ARG2 and other argument labels: Among
the 1,539 instances that ARG2-Original didn?t tag
correctly, 233 instances are not assigned an argu-
ment label, and 1,252 instances ARG2-Original con-
fuse the ARG2 label with another argument label:
the system ARG2-Original assigned the ARG2 la-
bel to 50 ARG0?s, 716 ARG1?s, 1 ARG3 and 482
ARGM?s, and assigned other argument labels to 3
ARG2?s.
554
6 Conclusions
In conclusion, we have described a mapping from
the annotated PropBank corpus to VerbNet verb
classes with associated thematic role labels. We hy-
pothesized that these labels would be more verb-
independent and less overloaded than the PropBank
Args2-5, and would therefore provide more consis-
tent training instances which would generalize better
to new genres. Our preliminary experiments confirm
this hypothesis, with a 6% performance improve-
ment on the WSJ and a 10% performance improve-
ment on the Brown corpus for Arg2.
In future work, we will map the PropBank-ed
Brown corpus to VerbNet as well, which will allow
much more thorough testing of our hypothesis. We
will also examine back-off to verb class membership
as a technique for improving performance on out of
vocabulary verbs. Finally, we plan to explore the ef-
fect of different thematic role groupings on system
performance.
References
Xavier Carreras and Llu??s Ma?rquez. 2005. Introduction
to the conll-2005 shared task: Semantic role labeling.
In Proceedings of CoNLL.
John Chen and Owen Rambow. 2003. Use of deep lin-
guistic features for the recognition and labeling of se-
mantic arguments. In Proceedings of EMNLP-2003,
Sapporo, Japan.
D. R. Dowty. 1991. Thematic proto-roles and argument
selection. Language, 67:574?619.
Daniel Gildea and Julia Hockenmaier. 2003. Identifying
semantic roles using Combinatory Categorial Gram-
mar. In 2003 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 57?64,
Sapporo, Japan.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James H.
Martin, and Daniel Jurafsky. 2003. Shallow semantic
parsing using support vector machines. Technical re-
port, The Center for Spoken Language Research at the
University of Colorado (CSLR).
Beth Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. The University of
Chicago Press.
Edward Loper, Szu-ting Yi, and Martha Palmer. 2007.
Empirical evidence for useful semantic role categories.
In Proceedings of the International Workshop on Com-
putational Linguistics.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre,
A. Bies, M. Ferguson, K. Katz, and B. Schasberger.
1994. The Penn treebank: Annotating predicate argu-
ment structure.
Alessandro Moschitti. 2004. A study on convolution
kernel for shallow semantic parsing. In Proceedings
of the 42-th Conference on Association for Computa-
tional Linguistic (ACL-2004), Barcelona, Spain.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: A corpus annotated with
semantic roles. Computational Linguistics, 31(1):71?
106.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, H. Mar-
tin, James, and Daniel Jurafsky. 2005a. Semantic role
chunking combining complementary syntactic views.
In Proceedings of CoNLL-2005.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2005b. Semantic role la-
beling using different syntactic views. In Proceedings
of the Association for Computational Linguistics 43rd
annual meeting (ACL-2005), Ann Arbor, MI.
V. Punyakanok, D. Roth, and W. Yih. 2005. The ne-
cessity of syntactic parsing for semantic role labeling.
In Proceedings of the 19th International Joint Confer-
ence on Artificial Intelligence (IJCAI-05).
Karin Kipper Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylvania.
Kristina Toutanova, Aria Haghighi, and Christopher D.
2005. Joint learning improves semantic role labeling.
In Proceedings of the Association for Computational
Linguistics 43rd annual meeting (ACL-2005), Ann Ar-
bor, MI.
Szu-ting Yi and Martha Palmer. 2004. Pushing the
boundaries of semantic role labeling with svm. In Pro-
ceedings of the International Conference on Natural
Language Processing.
Szu-ting Yi and Martha Palmer. 2005. The integration of
syntactic parsing and semantic role labeling. In Pro-
ceedings of CoNLL-2005.
555
NLTK: The Natural Language Toolkit
Steven Bird
Department of Computer Science
and Software Engineering
University of Melbourne
Victoria 3010, Australia
sb@csse.unimelb.edu.au
Edward Loper
Department of Computer
and Information Science
University of Pennsylvania
Philadelphia PA 19104-6389, USA
edloper@gradient.cis.upenn.edu
Abstract
The Natural Language Toolkit is a suite of program mod-
ules, data sets, tutorials and exercises, covering symbolic
and statistical natural language processing. NLTK is
written in Python and distributed under the GPL open
source license. Over the past three years, NLTK has
become popular in teaching and research. We describe
the toolkit and report on its current state of development.
1 Introduction
The Natural Language Toolkit (NLTK) was
developed in conjunction with a computational
linguistics course at the University of Pennsylvania
in 2001 (Loper and Bird, 2002). It was designed
with three pedagogical applications in mind:
assignments, demonstrations, and projects.
Assignments. NLTK supports assignments of
varying difficulty and scope. In the simplest assign-
ments, students experiment with existing compo-
nents to perform a wide variety of NLP tasks. As
students become more familiar with the toolkit, they
can be asked to modify existing components, or
to create complete systems out of existing compo-
nents.
Demonstrations. NLTK?s interactive graphical
demonstrations have proven to be very useful
for students learning NLP concepts. The
demonstrations give a step-by-step execution
of important algorithms, displaying the current
state of key data structures. A screenshot of the
chart parsing demonstration is shown in Figure 1.
Projects. NLTK provides students with a flexible
framework for advanced projects. Typical projects
might involve implementing a new algorithm,
developing a new component, or implementing a
new task.
We chose Python because it has a shallow learn-
ing curve, its syntax and semantics are transparent,
and it has good string-handling functionality. As
an interpreted language, Python facilitates interac-
tive exploration. As an object-oriented language,
Python permits data and methods to be encapsulated
and re-used easily. Python comes with an extensive
standard library, including tools for graphical pro-
gramming and numerical processing. The recently
added generator syntax makes it easy to create inter-
active implementations of algorithms (Loper, 2004;
Rossum, 2003a; Rossum, 2003b).
Figure 1: Interactive Chart Parsing Demonstration
2 Design
NLTK is implemented as a large collection of
minimally interdependent modules, organized
into a shallow hierarchy. A set of core modules
defines basic data types that are used throughout the
toolkit. The remaining modules are task modules,
each devoted to an individual natural language
processing task. For example, the nltk.parser
module encompasses to the task of parsing, or
deriving the syntactic structure of a sentence;
and the nltk.tokenizer module is devoted to
the task of tokenizing, or dividing a text into its
constituent parts.
2.1 Tokens and other core data types
To maximize interoperability between modules, we
use a single class to encode information about nat-
ural language texts ? the Token class. Each Token
instance represents a unit of text such as a word,
sentence, or document, and is defined by a (partial)
mapping from property names to values. For exam-
ple, the TEXT property is used to encode a token?s
text content:1
>>> from nltk.token import *
>>> Token(TEXT="Hello World!")
<Hello World!>
The TAG property is used to encode a token?s part-
of-speech tag:
>>> Token(TEXT="python", TAG="NN")
<python/NN>
The SUBTOKENS property is used to store a tok-
enized text:
>>> from nltk.tokenizer import *
>>> tok = Token(TEXT="Hello World!")
>>> WhitespaceTokenizer().tokenize(tok)
>>> print tok[?SUBTOKENS?])
[<Hello>, <World!>]
In a similar fashion, other language processing tasks
such as word-sense disambiguation, chunking and
parsing all add properties to the Token data struc-
ture.
In general, language processing tasks are formu-
lated as annotations and transformations involving
Tokens. In particular, each processing task takes
a token and extends it to include new information.
These modifications are typically monotonic; new
information is added but existing information is not
deleted or modified. Thus, tokens serve as a black-
board, where information about a piece of text is
collated. This architecture contrasts with the more
typical pipeline architecture where each processing
task?s output discards its input information. We
chose the blackboard approach over the pipeline
approach because it allows more flexibility when
combining tasks into a single system.
In addition to the Token class and its derivatives,
NLTK defines a variety of other data types. For
instance, the probability module defines classes
for probability distributions and statistical smooth-
ing techniques; and the cfg module defines classes
for encoding context free grammars and probabilis-
tic context free grammars.
1Some code samples are specific to NLTK version 1.4.
2.2 The corpus module
Many language processing tasks must be developed
and tested using annotated data sets or corpora.
Several such corpora are distributed with NLTK,
as listed in Table 1. The corpus module defines
classes for reading and processing many of these
corpora. The following code fragment illustrates
how the Brown Corpus is accessed.
>>> from nltk.corpus import brown
>>> brown.groups()
[?skill and hobbies?, ?popular lore?,
?humor?, ?fiction: mystery?, ...]
>>> brown.items(?humor?)
(?cr01?, ?cr02?, ?cr03?, ?cr04?, ?cr05?,
?cr06?, ?cr07?, ?cr08?, ?cr09?)
>>> brown.tokenize(?cr01?)
<[<It/pps>, <was/bedz>, <among/in>,
<these/dts>, <that/cs>, <Hinkle/np>,
<identified/vbd>, <a/at>, ...]>
A selection of 5% of the Penn Treebank corpus is
included with NLTK, and it is accessed as follows:
>>> from nltk.corpus import treebank
>>> treebank.groups()
(?raw?, ?tagged?, ?parsed?, ?merged?)
>>> treebank.items(?parsed?)
[?wsj_0001.prd?, ?wsj_0002.prd?, ...]
>>> item = ?parsed/wsj_0001.prd?
>>> sentences = treebank.tokenize(item)
>>> for sent in sentences[?SUBTOKENS?]:
... print sent.pp() # pretty-print
(S:
(NP-SBJ:
(NP: <Pierre> <Vinken>)
(ADJP:
(NP: <61> <years>)
<old>
)
...
2.3 Processing modules
Each language processing algorithm is implemented
as a class. For example, the ChartParser and
RecursiveDescentParser classes each define
a single algorithm for parsing a text. We imple-
ment language processing algorithms using classes
instead of functions for three reasons. First, all
algorithm-specific options can be passed to the con-
structor, allowing a consistent interface for applying
the algorithms. Second, a number of algorithms
need to have their state initialized before they can
be used. For example, the NthOrderTagger class
Corpus Contents and Wordcount Example Application
20 Newsgroups (selection) 3 newsgroups, 4000 posts, 780kw text classification
Brown Corpus 15 genres, 1.15Mw, tagged training & testing taggers, text classification
CoNLL 2000 Chunking Data 270kw, tagged and chunked training & testing chunk parsers
Project Gutenberg (selection) 14 texts, 1.7Mw text classification, language modelling
NIST 1999 IEER (selection) 63kw, named-entity markup training & testing named-entity recognizers
Levin Verb Index 3k verbs with Levin classes parser development
Names Corpus 8k male & female names text classification
PP Attachment Corpus 28k prepositional phrases, tagged parser development
Roget?s Thesaurus 200kw, formatted text word-sense disambiguation
SEMCOR 880kw, POS & sense tagged word-sense disambiguation
SENSEVAL 2 Corpus 600kw, POS & sense tagged word-sense disambiguation
Stopwords Corpus 2,400 stopwords for 11 lgs text retrieval
Penn Treebank (sample) 40kw, tagged & parsed parser development
Wordnet 1.7 180kw in a semantic network WSD, NL understanding
Wordlist Corpus 960kw and 20k affixes for 8 lgs spell checking
Table 1: Corpora and Corpus Samples Distributed with NLTK
must be initialized by training on a tagged corpus
before it can be used. Third, subclassing can be used
to create specialized versions of a given algorithm.
Each processing module defines an interface
for its task. Interface classes are distinguished by
naming them with a trailing capital ?I,? such as
ParserI. Each interface defines a single action
method which performs the task defined by the
interface. For example, the ParserI interface
defines the parse method and the Tokenizer
interface defines the tokenize method. When
appropriate, an interface defines extended action
methods, which provide variations on the basic
action method. For example, the ParserI interface
defines the parse n method which finds at most n
parses for a given sentence; and the TokenizerI
interface defines the xtokenize method, which
outputs an iterator over subtokens instead of a list
of subtokens.
NLTK includes the following modules:
cfg, corpus, draw (cfg, chart, corpus,
featurestruct, fsa, graph, plot, rdparser,
srparser, tree), eval, featurestruct,
parser (chart, chunk, probabilistic),
probability, sense, set, stemmer (porter),
tagger, test, token, tokenizer, tree, and
util. Please see the online documentation for
details.
2.4 Documentation
Three different types of documentation are avail-
able. Tutorials explain how to use the toolkit, with
detailed worked examples. The API documentation
describes every module, interface, class, method,
function, and variable in the toolkit. Technical
reports explain and justify the toolkit?s design and
implementation. All are available from http://
nltk.sf.net/docs.html.
3 Installing NLTK
NLTK is available from nltk.sf.net, and is
packaged for easy installation under Unix, Mac
OS X and Windows. The full distribution consists
of four packages: the Python source code (nltk);
the corpora (nltk-data); the documentation
(nltk-docs); and third-party contributions
(nltk-contrib). Before installing NLTK, it is
necessary to install Python version 2.3 or later,
available from www.python.org. Full installation
instructions and a quick start guide are available
from the NLTK homepage.
As soon as NLTK is installed, users can run the
demonstrations. On Windows, the demonstrations
can be run by double-clicking on their Python
source files. Alternatively, from the Python
interpreter, this can be done as follows:
>>> import nltk.draw.rdparser
>>> nltk.draw.rdparser.demo()
>>> nltk.draw.srparser.demo()
>>> nltk.draw.chart.demo()
4 Using and contributing to NLTK
NLTK has been used at the University of Pennsylva-
nia since 2001, and has subsequently been adopted
by several NLP courses at other universities, includ-
ing those listed in Table 2.
Third party contributions to NLTK include:
Brill tagger (Chris Maloof), hidden Markov model
tagger (Trevor Cohn, Phil Blunsom), GPSG-style
feature-based grammar and parser (Rob Speer, Bob
Berwick), finite-state morphological analyzer (Carl
de Marcken, Beracah Yankama, Bob Berwick),
decision list and decision tree classifiers (Trevor
Cohn), and Discourse Representation Theory
implementation (Edward Ivanovic).
NLTK is an open source project, and we wel-
come any contributions. There are several ways
to contribute: users can report bugs, suggest fea-
tures, or contribute patches on Sourceforge; users
can participate in discussions on the NLTK-Devel
mailing list2 or in the NLTK public forums; and
users can submit their own NLTK-based projects
for inclusion in the nltk contrib directory. New
code modules that are relevant, substantial, orig-
inal and well-documented will be considered for
inclusion in NLTK proper. All source code is dis-
tributed under the GNU General Public License, and
all documentation is distributed under a Creative
Commons non-commercial license. Thus, poten-
tial contributors can be confident that their work
will remain freely available to all. Further infor-
mation about contributing to NLTK is available at
http://nltk.sf.net/contrib.html.
5 Conclusion
NLTK is a broad-coverage natural language toolkit
that provides a simple, extensible, uniform frame-
work for assignments, demonstrations and projects.
It is thoroughly documented, easy to learn, and sim-
ple to use. NLTK is now widely used in research
and teaching. Readers who would like to receive
occasional announcements about NLTK are encour-
aged to sign up for the low-volume, moderated mail-
ing list NLTK-Announce.3
6 Acknowledgements
We are indebted to our students and colleagues for
feedback on the toolkit, and to many contributors
listed on the NLTK website.
2http://lists.sourceforge.net/
lists/listinfo/nltk-devel
3http://lists.sourceforge.net/
lists/listinfo/nltk-announce
Graz University of Technology, Austria
Information Search and Retrieval
Macquarie University, Australia
Intelligent Text Processing
Massachusetts Institute of Technology, USA
Natural Language Processing
National Autonomous University of Mexico, Mexico
Introduction to Natural Language Processing
in Python
Ohio State University, USA
Statistical Natural Language Processing
University of Amsterdam, Netherlands
Language Processing and Information Access
University of Colorado, USA
Natural Language Processing
University of Edinburgh, UK
Introduction to Computational Linguistics
University of Magdeburg, Germany
Natural Language Systems
University of Malta, Malta
Natural Language Algorithms
University of Melbourne, Australia
Human Language Technology
University of Pennsylvania, USA
Introduction to Computational Linguistics
University of Pittsburgh, USA
Artificial Intelligence Application Development
Simon Fraser University, Canada
Computational Linguistics
Table 2: University Courses using NLTK
References
Edward Loper and Steven Bird. 2002. NLTK:
The Natural Language Toolkit. In Proceedings
of the ACL Workshop on Effective Tools and
Methodologies for Teaching Natural Language
Processing and Computational Linguistics, pages
62?69. Somerset, NJ: Association for Computa-
tional Linguistics. http://arXiv.org/abs/
cs/0205028.
Edward Loper. 2004. NLTK: Building a pedagogi-
cal toolkit in Python. In PyCon DC 2004. Python
Software Foundation. http://www.python.
org/pycon/dc2004/papers/.
Guido Van Rossum. 2003a. An Introduction to
Python. Network Theory Ltd.
Guido Van Rossum. 2003b. The Python Language
Reference. Network Theory Ltd.
NLTK: The Natural Language Toolkit
Edward Loper and Steven Bird
Department of Computer and Information Science
University of Pennsylvania, Philadelphia, PA 19104-6389, USA
Abstract
NLTK, the Natural Language Toolkit,
is a suite of open source program
modules, tutorials and problem sets,
providing ready-to-use computational
linguistics courseware. NLTK covers
symbolic and statistical natural lan-
guage processing, and is interfaced to
annotated corpora. Students augment
and replace existing components, learn
structured programming by example,
and manipulate sophisticated models
from the outset.
1 Introduction
Teachers of introductory courses on compu-
tational linguistics are often faced with the
challenge of setting up a practical programming
component for student assignments and
projects. This is a difficult task because
different computational linguistics domains
require a variety of different data structures
and functions, and because a diverse range of
topics may need to be included in the syllabus.
A widespread practice is to employ multiple
programming languages, where each language
provides native data structures and functions
that are a good fit for the task at hand. For
example, a course might use Prolog for pars-
ing, Perl for corpus processing, and a finite-state
toolkit for morphological analysis. By relying
on the built-in features of various languages, the
teacher avoids having to develop a lot of software
infrastructure.
An unfortunate consequence is that a
significant part of such courses must be devoted
to teaching programming languages. Further,
many interesting projects span a variety of
domains, and would require that multiple
languages be bridged. For example, a student
project that involved syntactic parsing of corpus
data from a morphologically rich language might
involve all three of the languages mentioned
above: Perl for string processing; a finite state
toolkit for morphological analysis; and Prolog
for parsing. It is clear that these considerable
overheads and shortcomings warrant a fresh
approach.
Apart from the practical component, compu-
tational linguistics courses may also depend on
software for in-class demonstrations. This con-
text calls for highly interactive graphical user
interfaces, making it possible to view program
state (e.g. the chart of a chart parser), observe
program execution step-by-step (e.g. execu-
tion of a finite-state machine), and even make
minor modifications to programs in response to
?what if? questions from the class. Because
of these difficulties it is common to avoid live
demonstrations, and keep classes for theoreti-
cal presentations only. Apart from being dull,
this approach leaves students to solve important
practical problems on their own, or to deal with
them less efficiently in office hours.
In this paper we introduce a new approach to
the above challenges, a streamlined and flexible
way of organizing the practical component
of an introductory computational linguistics
course. We describe NLTK, the Natural
Language Toolkit, which we have developed in
conjunction with a course we have taught at
the University of Pennsylvania.
The Natural Language Toolkit is avail-
able under an open source license from
http://nltk.sf.net/. NLTK runs on all
platforms supported by Python, including
Windows, OS X, Linux, and Unix.
2 Choice of Programming Language
The most basic step in setting up a practical
component is choosing a suitable programming
language. A number of considerations
influenced our choice. First, the language must
have a shallow learning curve, so that novice
programmers get immediate rewards for their
efforts. Second, the language must support
rapid prototyping and a short develop/test
cycle; an obligatory compilation step is a
serious detraction. Third, the code should be
self-documenting, with a transparent syntax and
semantics. Fourth, it should be easy to write
structured programs, ideally object-oriented but
without the burden associated with languages
like C++. Finally, the language must have
an easy-to-use graphics library to support the
development of graphical user interfaces.
In surveying the available languages, we
believe that Python offers an especially good
fit to the above requirements. Python is an
object-oriented scripting language developed
by Guido van Rossum and available on all
platforms (www.python.org). Python offers
a shallow learning curve; it was designed to
be easily learnt by children (van Rossum,
1999). As an interpreted language, Python is
suitable for rapid prototyping. Python code is
exceptionally readable, and it has been praised
as ?executable pseudocode.? Python is an
object-oriented language, but not punitively
so, and it is easy to encapsulate data and
methods inside Python classes. Finally, Python
has an interface to the Tk graphics toolkit
(Lundh, 1999), and writing graphical interfaces
is straightforward.
3 Design Criteria
Several criteria were considered in the design
and implementation of the toolkit. These design
criteria are listed in the order of their impor-
tance. It was also important to decide what
goals the toolkit would not attempt to accom-
plish; we therefore include an explicit set of non-
requirements, which the toolkit is not expected
to satisfy.
3.1 Requirements
Ease of Use. The primary purpose of the
toolkit is to allow students to concentrate on
building natural language processing (NLP) sys-
tems. The more time students must spend learn-
ing to use the toolkit, the less useful it is.
Consistency. The toolkit should use consis-
tent data structures and interfaces.
Extensibility. The toolkit should easily
accommodate new components, whether those
components replicate or extend the toolkit?s
existing functionality. The toolkit should
be structured in such a way that it is obvious
where new extensions would fit into the toolkit?s
infrastructure.
Documentation. The toolkit, its data
structures, and its implementation all need to
be carefully and thoroughly documented. All
nomenclature must be carefully chosen and
consistently used.
Simplicity. The toolkit should structure the
complexities of building NLP systems, not hide
them. Therefore, each class defined by the
toolkit should be simple enough that a student
could implement it by the time they finish an
introductory course in computational linguis-
tics.
Modularity. The interaction between differ-
ent components of the toolkit should be kept
to a minimum, using simple, well-defined inter-
faces. In particular, it should be possible to
complete individual projects using small parts
of the toolkit, without worrying about how they
interact with the rest of the toolkit. This allows
students to learn how to use the toolkit incre-
mentally throughout a course. Modularity also
makes it easier to change and extend the toolkit.
3.2 Non-Requirements
Comprehensiveness. The toolkit is not
intended to provide a comprehensive set of
tools. Indeed, there should be a wide variety of
ways in which students can extend the toolkit.
Efficiency. The toolkit does not need to
be highly optimized for runtime performance.
However, it should be efficient enough that
students can use their NLP systems to perform
real tasks.
Cleverness. Clear designs and implementa-
tions are far preferable to ingenious yet inde-
cipherable ones.
4 Modules
The toolkit is implemented as a collection of
independent modules, each of which defines a
specific data structure or task.
A set of core modules defines basic data
types and processing systems that are used
throughout the toolkit. The token module
provides basic classes for processing individual
elements of text, such as words or sentences.
The tree module defines data structures for
representing tree structures over text, such
as syntax trees and morphological trees. The
probability module implements classes that
encode frequency distributions and probability
distributions, including a variety of statistical
smoothing techniques.
The remaining modules define data structures
and interfaces for performing specific NLP tasks.
This list of modules will grow over time, as we
add new tasks and algorithms to the toolkit.
Parsing Modules
The parser module defines a high-level inter-
face for producing trees that represent the struc-
tures of texts. The chunkparser module defines
a sub-interface for parsers that identify non-
overlapping linguistic groups (such as base noun
phrases) in unrestricted text.
Four modules provide implementations
for these abstract interfaces. The srparser
module implements a simple shift-reduce
parser. The chartparser module defines a
flexible parser that uses a chart to record
hypotheses about syntactic constituents. The
pcfgparser module provides a variety of
different parsers for probabilistic grammars.
And the rechunkparser module defines a
transformational regular-expression based
implementation of the chunk parser interface.
Tagging Modules
The tagger module defines a standard interface
for augmenting each token of a text with supple-
mentary information, such as its part of speech
or its WordNet synset tag; and provides several
different implementations for this interface.
Finite State Automata
The fsa module defines a data type for encod-
ing finite state automata; and an interface for
creating automata from regular expressions.
Type Checking
Debugging time is an important factor in the
toolkit?s ease of use. To reduce the amount of
time students must spend debugging their code,
we provide a type checking module, which can
be used to ensure that functions are given valid
arguments. The type checking module is used
by all of the basic data types and processing
classes.
Since type checking is done explicitly, it can
slow the toolkit down. However, when efficiency
is an issue, type checking can be easily turned
off; and with type checking is disabled, there is
no performance penalty.
Visualization
Visualization modules define graphical
interfaces for viewing and manipulating
data structures, and graphical tools for
experimenting with NLP tasks. The draw.tree
module provides a simple graphical inter-
face for displaying tree structures. The
draw.tree edit module provides an interface
for building and modifying tree structures.
The draw.plot graph module can be used to
graph mathematical functions. The draw.fsa
module provides a graphical tool for displaying
and simulating finite state automata. The
draw.chart module provides an interactive
graphical tool for experimenting with chart
parsers.
The visualization modules provide interfaces
for interaction and experimentation; they do
not directly implement NLP data structures or
tasks. Simplicity of implementation is therefore
less of an issue for the visualization modules
than it is for the rest of the toolkit.
Text Classification
The classifier module defines a standard
interface for classifying texts into categories.
This interface is currently implemented by two
modules. The classifier.naivebayes module
defines a text classifier based on the Naive Bayes
assumption. The classifier.maxent module
defines the maximum entropy model for text
classification, and implements two algorithms
for training the model: Generalized Iterative
Scaling and Improved Iterative Scaling.
The classifier.feature module provides
a standard encoding for the information that
is used to make decisions for a particular
classification task. This standard encoding
allows students to experiment with the
differences between different text classification
algorithms, using identical feature sets.
The classifier.featureselection module
defines a standard interface for choosing which
features are relevant for a particular classifica-
tion task. Good feature selection can signifi-
cantly improve classification performance.
5 Documentation
The toolkit is accompanied by extensive
documentation that explains the toolkit, and
describes how to use and extend it. This
documentation is divided into three primary
categories:
Tutorials teach students how to use the
toolkit, in the context of performing specific
tasks. Each tutorial focuses on a single domain,
such as tagging, probabilistic systems, or text
classification. The tutorials include a high-level
discussion that explains and motivates the
domain, followed by a detailed walk-through
that uses examples to show how NLTK can be
used to perform specific tasks.
Reference Documentation provides precise
definitions for every module, interface, class,
method, function, and variable in the toolkit. It
is automatically extracted from docstring com-
ments in the Python source code, using Epydoc
(Loper, 2002).
Technical Reports explain and justify the
toolkit?s design and implementation. They are
used by the developers of the toolkit to guide
and document the toolkit?s construction. Stu-
dents can also consult these reports if they would
like further information about how the toolkit is
designed, and why it is designed that way.
6 Uses of NLTK
6.1 Assignments
NLTK can be used to create student assign-
ments of varying difficulty and scope. In the
simplest assignments, students experiment with
an existing module. The wide variety of existing
modules provide many opportunities for creat-
ing these simple assignments. Once students
become more familiar with the toolkit, they can
be asked to make minor changes or extensions to
an existing module. A more challenging task is
to develop a new module. Here, NLTK provides
some useful starting points: predefined inter-
faces and data structures, and existing modules
that implement the same interface.
Example: Chunk Parsing
As an example of a moderately difficult
assignment, we asked students to construct
a chunk parser that correctly identifies base
noun phrase chunks in a given text, by
defining a cascade of transformational chunking
rules. The NLTK rechunkparser module
provides a variety of regular-expression
based rule types, which the students can
instantiate to construct complete rules.
For example, ChunkRule(?<NN.*>?) builds
chunks from sequences of consecutive nouns;
ChinkRule(?<VB.>?) excises verbs from
existing chunks; SplitRule(?<NN>?, ?<DT>?)
splits any existing chunk that contains a
singular noun followed by determiner into
two pieces; and MergeRule(?<JJ>?, ?<JJ>?)
combines two adjacent chunks where the first
chunk ends and the second chunk starts with
adjectives.
The chunking tutorial motivates chunk pars-
ing, describes each rule type, and provides all
the necessary code for the assignment. The pro-
vided code is responsible for loading the chun-
ked, part-of-speech tagged text using an existing
tokenizer, creating an unchunked version of the
text, applying the chunk rules to the unchunked
text, and scoring the result. Students focus on
the NLP task only ? providing a rule set with
the best coverage.
In the remainder of this section we reproduce
some of the cascades created by the students.
The first example illustrates a combination of
several rule types:
cascade = [
ChunkRule(?<DT><NN.*><VB.><NN.*>?),
ChunkRule(?<DT><VB.><NN.*>?),
ChunkRule(?<.*>?),
UnChunkRule(?<IN|VB.*|CC|MD|RB.*>?),
UnChunkRule("<,|\\.|??|??>"),
MergeRule(?<NN.*|DT|JJ.*|CD>?,
?<NN.*|DT|JJ.*|CD>?),
SplitRule(?<NN.*>?, ?<DT|JJ>?)
]
The next example illustrates a brute-force sta-
tistical approach. The student calculated how
often each part-of-speech tag was included in
a noun phrase. They then constructed chunks
from any sequence of tags that occurred in a
noun phrase more than 50% of the time.
cascade = [
ChunkRule(?<\\$|CD|DT|EX|PDT
|PRP.*|WP.*|\\#|FW
|JJ.*|NN.*|POS|RBS|WDT>*?)
]
In the third example, the student constructed
a single chunk containing the entire text, and
then excised all elements that did not belong.
cascade = [
ChunkRule(?<.*>+?)
ChinkRule(?<VB.*|IN|CC|R.*|MD|WRB|TO|.|,>+?)
]
6.2 Class demonstrations
NLTK provides graphical tools that can be used
in class demonstrations to help explain basic
NLP concepts and algorithms. These interactive
tools can be used to display relevant data struc-
tures and to show the step-by-step execution of
algorithms. Both data structures and control
flow can be easily modified during the demon-
stration, in response to questions from the class.
Since these graphical tools are included with
the toolkit, they can also be used by students.
This allows students to experiment at home with
the algorithms that they have seen presented in
class.
Example: The Chart Parsing Tool
The chart parsing tool is an example of a
graphical tool provided by NLTK. This tool can
be used to explain the basic concepts behind
chart parsing, and to show how the algorithm
works. Chart parsing is a flexible parsing algo-
rithm that uses a data structure called a chart to
record hypotheses about syntactic constituents.
Each hypothesis is represented by a single edge
on the chart. A set of rules determine when new
edges can be added to the chart. This set of rules
controls the overall behavior of the parser (e.g.,
whether it parses top-down or bottom-up).
The chart parsing tool demonstrates the pro-
cess of parsing a single sentence, with a given
grammar and lexicon. Its display is divided into
three sections: the bottom section displays the
chart; the middle section displays the sentence;
and the top section displays the partial syntax
tree corresponding to the selected edge. But-
tons along the bottom of the window are used
to control the execution of the algorithm. The
main display window for the chart parsing tool
is shown in Figure 1.
This tool can be used to explain several dif-
ferent aspects of chart parsing. First, it can be
used to explain the basic chart data structure,
and to show how edges can represent hypothe-
ses about syntactic constituents. It can then
be used to demonstrate and explain the indi-
vidual rules that the chart parser uses to create
new edges. Finally, it can be used to show how
Figure 1: Chart Parsing Tool
these individual rules combine to find a complete
parse for a given sentence.
To reduce the overhead of setting up demon-
strations during lecture, the user can define a
list of preset charts. The tool can then be reset
to any one of these charts at any time.
The chart parsing tool allows for flexible con-
trol of the parsing algorithm. At each step of
the algorithm, the user can select which rule or
strategy they wish to apply. This allows the user
to experiment with mixing different strategies
(e.g., top-down and bottom-up). The user can
exercise fine-grained control over the algorithm
by selecting which edge they wish to apply a rule
to. This flexibility allows lecturers to use the
tool to respond to a wide variety of questions;
and allows students to experiment with different
variations on the chart parsing algorithm.
6.3 Advanced Projects
NLTK provides students with a flexible frame-
work for advanced projects. Typical projects
involve the development of entirely new func-
tionality for a previously unsupported NLP task,
or the development of a complete system out of
existing and new modules.
The toolkit?s broad coverage allows students
to explore a wide variety of topics. In our intro-
ductory computational linguistics course, topics
for student projects included text generation,
word sense disambiguation, collocation analysis,
and morphological analysis.
NLTK eliminates the tedious infrastructure-
building that is typically associated with
advanced student projects by providing
students with the basic data structures, tools,
and interfaces that they need. This allows the
students to concentrate on the problems that
interest them.
The collaborative, open-source nature of the
toolkit can provide students with a sense that
their projects are meaningful contributions, and
not just exercises. Several of the students in our
course have expressed interest in incorporating
their projects into the toolkit.
Finally, many of the modules included in the
toolkit provide students with good examples
of what projects should look like, with well
thought-out interfaces, clean code structure, and
thorough documentation.
Example: Probabilistic Parsing
The probabilistic parsing module was created
as a class project for a statistical NLP course.
The toolkit provided the basic data types and
interfaces for parsing. The project extended
these, adding a new probabilistic parsing inter-
face, and using subclasses to create a prob-
abilistic version of the context free grammar
data structure. These new components were
used in conjunction with several existing compo-
nents, such as the chart data structure, to define
two implementations of the probabilistic parsing
interface. Finally, a tutorial was written that
explained the basic motivations and concepts
behind probabilistic parsing, and described the
new interfaces, data structures, and parsers.
7 Evaluation
We used NLTK as a basis for the assignments
and student projects in CIS-530, an introduc-
tory computational linguistics class taught at
the University of Pennsylvania. CIS-530 is a
graduate level class, although some advanced
undergraduates were also enrolled. Most stu-
dents had a background in either computer sci-
ence or linguistics (and occasionally both). Stu-
dents were required to complete five assign-
ments, two exams, and a final project. All class
materials are available from the course website
http://www.cis.upenn.edu/~cis530/.
The experience of using NLTK was very pos-
itive, both for us and for the students. The
students liked the fact that they could do inter-
esting projects from the outset. They also liked
being able to run everything on their computer
at home. The students found the extensive doc-
umentation very helpful for learning to use the
toolkit. They found the interfaces defined by
NLTK intuitive, and appreciated the ease with
which they could combine different components
to create complete NLP systems.
We did encounter a few difficulties during the
semester. One problem was finding large clean
corpora that the students could use for their
assignments. Several of the students needed
assistance finding suitable corpora for their
final projects. Another issue was the fact that
we were actively developing NLTK during the
semester; some modules were only completed
one or two weeks before the students used
them. As a result, students who worked at
home needed to download new versions of the
toolkit several times throughout the semester.
Luckily, Python has extensive support for
installation scripts, which made these upgrades
simple. The students encountered a couple of
bugs in the toolkit, but none were serious, and
all were quickly corrected.
8 Other Approaches
The computational component of computational
linguistics courses takes many forms. In this sec-
tion we briefly review a selection of approaches,
classified according to the (original) target audi-
ence.
Linguistics Students. Various books intro-
duce programming or computing to linguists.
These are elementary on the computational side,
providing a gentle introduction to students hav-
ing no prior experience in computer science.
Examples of such books are: Using Computers
in Linguistics (Lawler and Dry, 1998), and Pro-
gramming for Linguistics: Java Technology for
Language Researchers (Hammond, 2002).
Grammar Developers. Infrastructure
for grammar development has a long history
in unification-based (or constraint-based)
grammar frameworks, from DCG (Pereira
and Warren, 1980) to HPSG (Pollard and
Sag, 1994). Recent work includes (Copestake,
2000; Baldridge et al, 2002a). A concurrent
development has been the finite state toolkits,
such as the Xerox toolkit (Beesley and
Karttunen, 2002). This work has found
widespread pedagogical application.
Other Researchers and Developers.
A variety of toolkits have been created for
research or R&D purposes. Examples include
the CMU-Cambridge Statistical Language
Modeling Toolkit (Clarkson and Rosenfeld,
1997), the EMU Speech Database System
(Harrington and Cassidy, 1999), the General
Architecture for Text Engineering (Bontcheva
et al, 2002), the Maxent Package for Maximum
Entropy Models (Baldridge et al, 2002b), and
the Annotation Graph Toolkit (Maeda et al,
2002). Although not originally motivated by
pedagogical needs, all of these toolkits have
pedagogical applications and many have already
been used in teaching.
9 Conclusions and Future Work
NLTK provides a simple, extensible, uniform
framework for assignments, projects, and class
demonstrations. It is well documented, easy to
learn, and simple to use. We hope that NLTK
will allow computational linguistics classes to
include more hands-on experience with using
and building NLP components and systems.
NLTK is unique in its combination of three
factors. First, it was deliberately designed as
courseware and gives pedagogical goals primary
status. Second, its target audience consists of
both linguists and computer scientists, and it
is accessible and challenging at many levels of
prior computational skill. Finally, it is based on
an object-oriented scripting language support-
ing rapid prototyping and literate programming.
We plan to continue extending the breadth
of materials covered by the toolkit. We are
currently working on NLTK modules for Hidden
Markov Models, language modeling, and tree
adjoining grammars. We also plan to increase
the number of algorithms implemented by some
existing modules, such as the text classification
module.
Finding suitable corpora is a prerequisite for
many student assignments and projects. We are
therefore putting together a collection of corpora
containing data appropriate for every module
defined by the toolkit.
NLTK is an open source project, and we wel-
come any contributions. Readers who are inter-
ested in contributing to NLTK, or who have
suggestions for improvements, are encouraged to
contact the authors.
10 Acknowledgments
We are indebted to our students for feedback
on the toolkit, and to anonymous reviewers, Jee
Bang, and the workshop organizers for com-
ments on an earlier version of this paper. We are
grateful to Mitch Marcus and the Department of
Computer and Information Science at the Uni-
versity of Pennsylvania for sponsoring the work
reported here.
References
Jason Baldridge, John Dowding, and Susana Early.
2002a. Leo: an architecture for sharing resources
for unification-based grammars. In Proceedings
of the Third Language Resources and Evaluation
Conference. Paris: European Language Resources
Association.
http://www.iccs.informatics.ed.ac.uk/
~jmb/leo-lrec.ps.gz.
Jason Baldridge, Thomas Morton, and Gann
Bierner. 2002b. The MaxEnt project.
http://maxent.sourceforge.net/.
Kenneth R. Beesley and Lauri Karttunen. 2002.
Finite-State Morphology: Xerox Tools and Tech-
niques. Studies in Natural Language Processing.
Cambridge University Press.
Kalina Bontcheva, Hamish Cunningham, Valentin
Tablan, Diana Maynard, and Oana Hamza. 2002.
Using GATE as an environment for teaching NLP.
In Proceedings of the ACL Workshop on Effective
Tools and Methodologies for Teaching NLP and
CL. Somerset, NJ: Association for Computational
Linguistics.
Philip R. Clarkson and Ronald Rosenfeld.
1997. Statistical language modeling using
the CMU-Cambridge Toolkit. In Proceedings
of the 5th European Conference on Speech
Communication and Technology (EUROSPEECH
?97). http://svr-www.eng.cam.ac.uk/~prc14/
eurospeech97.ps.
Ann Copestake. 2000. The (new) LKB system.
http://www-csli.stanford.edu/~aac/doc5-2.
pdf.
Michael Hammond. 2002. Programming for Linguis-
tics: Java Technology for Language Researchers.
Oxford: Blackwell. In press.
Jonathan Harrington and Steve Cassidy. 1999. Tech-
niques in Speech Acoustics. Kluwer.
John M. Lawler and Helen Aristar Dry, editors.
1998. Using Computers in Linguistics. London:
Routledge.
Edward Loper. 2002. Epydoc.
http://epydoc.sourceforge.net/.
Fredrik Lundh. 1999. An introduction to tkinter.
http://www.pythonware.com/library/
tkinter/introduction/index.htm.
Kazuaki Maeda, Steven Bird, Xiaoyi Ma, and Hae-
joong Lee. 2002. Creating annotation tools with
the annotation graph toolkit. In Proceedings of
the Third International Conference on Language
Resources and Evaluation. http://arXiv.org/
abs/cs/0204005.
Fernando C. N. Pereira and David H. D. Warren.
1980. Definite clause grammars for language anal-
ysis ? a survey of the formalism and a comparison
with augmented transition grammars. Artificial
Intelligence, 13:231?78.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. Chicago University
Press.
Guido van Rossum. 1999. Computer program-
ming for everybody. Technical report, Corpo-
ration for National Research Initiatives. http:
//www.python.org/doc/essays/cp4e.html.
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 87?92,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 17: English Lexical Sample, SRL and All Words
Sameer S. Pradhan
BBN Technologies,
Cambridge, MA 02138
Edward Loper
University of Pennsylvania,
Philadelphia, PA 19104
Dmitriy Dligach and Martha Palmer
University of Colorado,
Boulder, CO 80303
Abstract
This paper describes our experience in
preparing the data and evaluating the results
for three subtasks of SemEval-2007 Task-17
? Lexical Sample, Semantic Role Labeling
(SRL) and All-Words respectively. We tab-
ulate and analyze the results of participating
systems.
1 Introduction
Correctly disambiguating words (WSD), and cor-
rectly identifying the semantic relationships be-
tween those words (SRL), is an important step for
building successful natural language processing ap-
plications, such as text summarization, question an-
swering, and machine translation. SemEval-2007
Task-17 (English Lexical Sample, SRL and All-
Words) focuses on both of these challenges, WSD
and SRL, using annotated English text taken from
the Wall Street Journal and the Brown Corpus.
It includes three subtasks: i) the traditional All-
Words task comprising fine-grained word sense dis-
ambiguation using a 3,500 word section of the Wall
Street Journal, annotated with WordNet 2.1 sense
tags, ii) a Lexical Sample task for coarse-grained
word sense disambiguation on a selected set of lex-
emes, and iii) Semantic Role Labeling, using two
different types of arguments, on the same subset of
lexemes.
2 Word Sense Disambiguation
2.1 English fine-grained All-Words
In this task we measure the ability of systems to
identify the correct fine-grained WordNet 2.1 word
sense for all the verbs and head words of their argu-
ments.
2.1.1 Data Preparation
We began by selecting three articles
wsj 0105.mrg (on homelessness), wsj 0186.mrg
(about a book on corruption), and wsj 0239.mrg
(about hot-air ballooning) from a section of the WSJ
corpus that has been Treebanked and PropBanked.
All instances of verbs were identified using the
Treebank part-of-speech tags, and also the head-
words of their noun arguments (using the PropBank
and standard headword rules). The locations of the
sentences containing them as well as the locations
of the verbs and the nouns within these sentences
were recorded for subsequent sense-annotation. A
total of 465 lemmas were selected from about 3500
words of text.
We use a tool called STAMP written by Ben-
jamin Snyder for sense-annotation of these in-
stances. STAMP accepts a list of pointers to the in-
stances that need to be annotated. These pointers
consist of the name of the file where the instance
is located, the sentence number of the instance, and
finally, the word number of the ambiguous word
within that sentence. These pointers were obtained
as described in the previous paragraph. STAMP also
requires a sense inventory, which must be stored in
XML format. This sense inventory was obtained by
querying WordNet 2.1 and storing the output as a
87
set of XML files (one for each word to be anno-
tated) prior to tagging. STAMP works by displaying
to the user the sentence to be annotated with the tar-
get word highlighted along with the previous and the
following sentences and the senses from the sense
inventory. The user can select one of the senses and
move on to the next instance.
Two linguistics students annotated the words with
WordNet 2.1 senses. Our annotators examined each
instance upon which they disagreed and resolved
their disagreements. Finally, we converted the re-
sulting data to the Senseval format. For this dataset,
we got an inter-annotator agreement (ITA) of 72%
on verbs and 86% for nouns.
2.1.2 Results
A total of 14 systems were evaluated on the All
Words task. These results are shown in Table 1.
We used the standard Senseval scorer ? scorer21
to score the systems. All the F-scores2 in this table
as well as other tables in this paper are accompanied
by a 95% confidence interval calculated using the
bootstrap resampling procedure.
2.2 OntoNotes English Lexical Sample WSD
It is quite well accepted at this point that it is dif-
ficult to achieve high inter-annotator agreement on
the fine-grained WordNet style senses, and with-
out a corpus with high annotator agreement, auto-
matic learning methods cannot perform at a level
that would be acceptable for a downstream applica-
tion. OntoNotes (Hovy et al, 2006) is a project that
has annotated several layers of semantic information
? including word senses, at a high inter-annotator
agreement of over 90%. Therefore we decided to
use this data for the lexical sample task.
2.2.1 Data
All the data for this task comes from the 1M word
WSJ Treebank. For the convenience of the partici-
pants who wanted to use syntactic parse information
as features using an off-the-shelf syntactic parser,
we decided to compose the training data of Sections
02-21. For the test sets, we use data from Sections
1http://www.cse.unt.edu/?rada/senseval/senseval3/scoring/
2
scorer2 reports Precision and Recall scores for each system. For a sys-
tem that attempts all the words, both Precision and Recall are the same. Since a
few systems had missing answers, they got different Precision and Recall scores.
Therefore, for ranking purposes, we consolidated them into an F-score.
Train Test Total
Verb 8988 2292 11280
Noun 13293 2559 15852
Total 22281 4851
Table 2: The number of instances for Verbs and
Nouns in the Train and Test sets for the Lexical Sam-
ple WSD task.
01, 22, 23 and 24. Fortunately, the distribution of
words was amenable to an acceptable number of in-
stances for each lemma in the test set. We selected
a total of 100 lemmas (65 verbs and 35 nouns) con-
sidering the degree of polysemy and total instances
that were annotated. The average ITA for these is
over 90%.
The training and test set composition is described
in Table 2. The distribution across all the verbs and
nouns is displayed in Table 4
2.2.2 Results
A total of 13 systems were evaluated on the Lexi-
cal Sample task. Table 3 shows the Precision/Recall
for all these systems. The same scoring software was
used to score this task as well.
2.2.3 Discussion
For the all words task, the baseline performance
using the most frequent WordNet sense for the lem-
mas is 51.4. The top-performing system was a su-
pervised system that used a Maximum Entropy clas-
sifier, and got a Precision/Recall of 59.1% ? about 8
points higher than the baseline. Since the coarse and
fine-grained disambiguation tasks have been part of
the two previous Senseval competitions, and we hap-
pen to have access to that data, we can take this op-
portunity to look at the disambiguation performance
trend. Although different test sets were used for ev-
ery evaluation, we can get a rough indication of the
trend. For the fine-grained All Words sense tagging
task, which has always used WordNet, the system
performance has ranged from our 59% to 65.2 (Sen-
seval3, (Decadt et al, 2004)) to 69% (Seneval2,
(Chklovski and Mihalcea, 2002)). Because of time
constraints on the data preparation, this year?s task
has proportionally more verbs and fewer nouns than
previous All-Words English tasks, which may ac-
count for the lower scores.
As expected, the Lexical Sample task using coarse
88
Rank Participant System ID Classifier F
1 Stephen Tratz <stephen.tratz@pnl.gov> PNNL MaxEnt 59.1?4.5
2 Hwee Tou Ng <nght@comp.nus.edu.sg> NUS-PT SVM 58.7?4.5
3 Rada Mihalcea <rada@cs.unt.edu> UNT-Yahoo Memory-based 58.3?4.5
4 Cai Junfu <caijunfu@gmail.com> NUS-ML naive Bayes 57.6?4.5
5 Oier Lopez de Lacalle <jibloleo@si.ehu.es> UBC-ALM kNN 54.4?4.5
6 David Martinez <davidm@csse.unimelb.edu.au> UBC-UMB-2 kNN 54.0?4.5
7 Jonathan Chang <jcone@princeton.edu> PU-BCD Exponential Model 53.9?4.5
8 Radu ION <radu@racai.ro> RACAI Unsupervised 52.7?4.5
9 Most Frequent WordNet Sense Baseline N/A 51.4?4.5
10 Davide Buscaldi <dbuscaldi@dsic.upv.es> UPV-WSD Unsupervised 46.9?4.5
11 Sudip Kumar Naskar <sudip.naskar@gmail.com> JU-SKNSB Unsupervised 40.2?4.5
12 David Martinez <davidm@csse.unimelb.edu.au> UBC-UMB-1 Unsupervised 39.9?4.5
14 Rafael Berlanga <berlanga@uji.es> tkb-uo Unsupervised 32.5?4.5
15 Jordan Boyd-Graber <jbg@princeton.edu> PUTOP Unsupervised 13.2?4.5
Table 1: System Performance for the All-Words task.
Rank Participant System Classifier F
1 Cai Junfu <caijunfu@gmail.com> NUS-ML SVM 88.7?1.2
2 Oier Lopez de Lacalle <jibloleo@si.ehu.es> UBC-ALM SVD+kNN 86.9?1.2
3 Zheng-Yu Niu <niu zy@hotmail.com> I2R Supervised 86.4?1.2
4 Lucia Specia <lspecia@gmail.com> USP-IBM-2 SVM 85.7?1.2
5 Lucia Specia <lspecia@gmail.com> USP-IBM-1 ILP 85.1?1.2
5 Deniz Yuret <dyuret@ku.edu.tr> KU Semi-supervised 85.1?1.2
6 Saarikoski <harri.saarikoski@helsinki.fi> OE naive Bayes, SVM 83.8?1.2
7 University of Technology Brno VUTBR naive Bayes 80.3?1.2
8 Ana Zelaia <ana.zelaia@ehu.es> UBC-ZAS SVD+kNN 79.9?1.2
9 Carlo Strapparava <strappa@itc.it> ITC-irst SVM 79.6?1.2
10 Most frequent sense in training Baseline N/A 78.0?1.2
11 Toby Hawker <toby@it.usyd.edu.au> USYD SVM 74.3?1.2
12 Siddharth Patwardhan <sidd@cs.utah.edu> UMND1 Unsupervised 53.8?1.2
13 Saif Mohammad <smm@cs.toronto.edu> Tor Unsupervised 52.1?1.2
- Toby Hawker <toby@it.usyd.edu.au> USYD? SVM 89.1?1.2
- Carlo Strapparava <strappa@itc.it> ITC? SVM 89.1?1.2
Table 3: System Performance for the OntoNotes Lexical Sample task. Systems marked with an * were
post-competition bug-fix submissions.
grained senses provides consistently higher per-
formance than previous more fine-grained Lexical
Sample Tasks. The high scores here were foreshad-
owed in an evaluation involving a subset of the data
last summer (Chen et al, 2006). Note that the best
system performance is now closely approaching the
ITA for this data of over 90%. Table 4 shows the
performance of the top 8 systems on all the indi-
vidual verbs and nouns in the test set. Owing to
space constraints we have removed some lemmas
that have perfect or almost perfect accuracies. At the
right are mentioned the average, minimum and max-
imum performances of the teams per lemma, and at
the bottom are the average scores per lemma (with-
out considering the lemma frequencies) and broken
down by verbs and nouns. A gap of about 10 points
between the verb and noun performance seems to
indicate that in general the verbs were more difficult
than the nouns. However, this might just be owing
to this particular test sample having more verbs with
higher perplexities, and maybe even ones that are
indeed difficult to disambiguate ? in spite of high
human agreement. The hope is that better knowl-
edge sources can overcome the gap still existing be-
tween the system performance and human agree-
ment. Overall, however, this data indicates that the
approach suggested by (Palmer, 2000) and that is be-
ing adopted in the ongoing OntoNotes project (Hovy
et al, 2006) does result in higher system perfor-
mance. Whether or not the more coarse-grained
senses are effective in improving natural language
processing applications remains to be seen.
89
Lemma S s T t 1 2 3 4 5 6 7 8 Average Min Max
turn.v 13 8 340 62 58 61 40 55 52 53 27 44 49 27 61
go.v 12 6 244 61 64 69 38 66 43 46 31 39 49 31 69
come.v 10 9 186 43 49 46 56 60 37 23 23 49 43 23 60
set.v 9 5 174 42 62 50 52 57 50 57 36 50 52 36 62
hold.v 8 7 129 24 58 46 50 54 54 38 50 67 52 38 67
raise.v 7 6 147 34 50 44 29 26 44 26 24 12 32 12 50
work.v 7 5 230 43 74 65 65 65 72 67 46 65 65 46 74
keep.v 7 6 260 80 56 54 52 64 56 52 48 51 54 48 64
start.v 6 4 214 38 53 50 47 55 45 42 37 45 47 37 55
lead.v 6 6 165 39 69 69 85 69 51 69 36 46 62 36 85
see.v 6 5 158 54 56 54 46 54 57 52 48 48 52 46 57
ask.v 6 3 348 58 84 72 72 78 76 52 67 66 71 52 84
find.v 5 3 174 28 93 93 86 89 82 82 75 86 86 75 93
fix.v 5 3 32 2 50 50 50 50 50 0 0 50 38 0 50
buy.v 5 3 164 46 83 80 80 83 78 76 70 76 78 70 83
begin.v 4 2 114 48 83 65 75 69 79 56 50 56 67 50 83
kill.v 4 1 111 16 88 88 88 88 88 88 88 81 87 81 88
join.v 4 4 68 18 44 50 50 39 56 57 39 44 47 39 57
end.v 4 3 135 21 90 86 86 90 62 87 86 67 82 62 90
do.v 4 2 207 61 92 90 90 93 93 90 85 84 90 84 93
examine.v 3 2 26 3 100 100 67 100 100 67 100 33 83 33 100
report.v 3 2 128 35 89 91 91 91 91 91 91 86 90 86 91
regard.v 3 3 40 14 93 93 86 86 64 86 57 93 82 57 93
recall.v 3 1 49 15 100 100 87 87 93 87 87 87 91 87 100
prove.v 3 2 49 22 90 88 82 80 90 86 70 74 82 70 90
claim.v 3 2 54 15 67 73 80 80 80 80 80 87 78 67 87
build.v 3 3 119 46 74 67 74 61 54 74 61 72 67 54 74
feel.v 3 3 347 51 71 69 69 74 76 69 61 71 70 61 76
care.v 3 3 69 7 43 43 43 43 100 29 57 57 52 29 100
contribute.v 2 2 35 18 67 72 72 67 50 61 50 67 63 50 72
maintain.v 2 2 61 10 80 80 70 100 80 90 90 80 84 70 100
complain.v 2 1 32 14 93 86 86 86 86 86 86 79 86 79 93
propose.v 2 2 34 14 100 86 100 86 100 93 79 79 90 79 100
promise.v 2 2 50 8 88 88 75 88 75 75 62 88 80 62 88
produce.v 2 2 115 44 82 82 77 73 75 75 77 80 78 73 82
prepare.v 2 2 54 18 94 83 89 89 83 86 83 83 86 83 94
explain.v 2 2 85 18 94 89 94 89 94 89 89 94 92 89 94
believe.v 2 2 202 55 87 78 78 86 84 78 74 80 81 74 87
occur.v 2 2 47 22 86 73 91 96 86 96 86 82 87 73 96
grant.v 2 2 19 5 100 80 80 80 40 80 60 80 75 40 100
enjoy.v 2 2 56 14 50 57 57 50 64 57 50 57 55 50 64
need.v 2 2 195 56 89 82 86 89 86 78 70 70 81 70 89
disclose.v 1 1 55 14 93 93 93 93 93 93 93 93 93 93 93
point.n 9 6 469 150 91 91 89 91 92 87 84 79 88 79 92
position.n 7 6 268 45 78 78 78 53 56 65 58 64 66 53 78
defense.n 7 7 120 21 57 48 52 43 48 29 48 48 46 29 57
carrier.n 7 3 111 21 71 71 71 71 67 71 71 62 70 62 71
order.n 7 4 346 57 93 95 93 91 93 92 90 91 92 90 95
exchange.n 5 3 363 61 92 90 92 85 90 88 82 79 87 79 92
system.n 5 3 450 70 79 73 66 67 59 63 63 61 66 59 79
source.n 5 5 152 35 86 80 80 63 83 68 60 29 69 29 86
space.n 5 2 67 14 93 100 93 93 93 86 86 71 89 71 100
base.n 5 4 92 20 75 80 75 50 65 40 50 75 64 40 80
authority.n 4 3 90 21 86 86 81 62 71 33 71 81 71 33 86
people.n 4 4 754 115 96 96 95 96 95 90 91 91 94 90 96
chance.n 4 3 91 15 60 67 60 60 67 73 20 73 60 20 73
part.n 4 3 481 71 90 90 92 97 90 74 66 66 83 66 97
hour.n 4 2 187 48 83 85 92 83 77 90 58 92 83 58 92
development.n 3 3 180 29 100 79 86 79 76 62 79 62 78 62 100
president.n 3 3 879 177 98 97 98 97 93 96 97 85 95 85 98
network.n 3 3 152 55 91 87 98 89 84 88 87 82 88 82 98
future.n 3 3 350 146 97 96 94 97 83 98 89 85 92 83 98
effect.n 3 2 178 30 97 93 80 93 80 90 77 83 87 77 97
state.n 3 3 617 72 85 86 86 83 82 79 83 82 83 79 86
power.n 3 3 251 47 92 87 87 81 77 77 77 74 81 74 92
bill.n 3 3 404 102 98 99 98 96 90 96 96 22 87 22 99
area.n 3 3 326 37 89 73 65 68 84 70 68 65 73 65 89
job.n 3 3 188 39 85 80 77 90 80 82 69 82 80 69 90
management.n 2 2 284 45 89 78 87 73 98 76 67 64 79 64 98
condition.n 2 2 132 34 91 82 82 56 76 78 74 76 77 56 91
policy.n 2 2 331 39 95 97 97 87 95 97 90 64 90 64 97
rate.n 2 2 1009 145 90 88 92 81 92 89 88 91 89 81 92
drug.n 2 2 205 46 94 94 96 78 94 94 87 78 89 78 96
Average Overall 86 83 83 82 82 79 76 77
Verbs 78 75 73 76 73 70 65 70
Nouns 89 87 86 81 83 80 77 76
Table 4: All Supervised system performance per predicate. (Column legend ? S=number of senses in training; s=number senses appearing more than 3 times;
T=instances in training; t=instances in test.; The numbers indicate system ranks.)
90
3 Semantic Role Labeling
Subtask 2 evaluates Semantic Role Labeling (SRL)
systems, where the goal is to locate the constituents
which are arguments of a given verb, and to assign
them appropriate semantic roles that describe how
they relate to the verb. SRL systems are an impor-
tant building block for many larger semantic sys-
tems. For example, in order to determine that ques-
tion (1a) is answered by sentence (1b), but not by
sentence (1c), we must determine the relationships
between the relevant verbs (eat and feed) and their
arguments.
(1) a. What do lobsters like to eat?
b. Recent studies have shown that lobsters pri-
marily feed on live fish, dig for clams, sea
urchins, and feed on algae and eel-grass.
c. In the early 20th century, Mainers would
only eat lobsters because the fish they
caught was too valuable to eat themselves.
Traditionally, SRL systems have been trained on
either the PropBank corpus (Palmer et al, 2005)
? for two years, the CoNLL workshop (Carreras
and Ma`rquez, 2004; Carreras and Ma`rquez, 2005)
has made this their shared task, or the FrameNet
corpus ? Senseval-3 used this for their shared task
(Litkowski, 2004). However, there is still little con-
sensus in the linguistics and NLP communities about
what set of role labels are most appropriate. The
PropBank corpus avoids this issue by using theory-
agnostic labels (ARG0, ARG1, . . . , ARG5), and
by defining those labels to have only verb-specific
meanings. Under this scheme, PropBank can avoid
making any claims about how any one verb?s ar-
guments relate to other verbs? arguments, or about
general distinctions between verb arguments and ad-
juncts.
However, there are several limitations to this ap-
proach. The first is that it can be difficult to make
inferences and generalizations based on role labels
that are only meaningful with respect to a single
verb. Since each role label is verb-specific, we can
not confidently determine when two different verbs?
arguments have the same role; and since no encoded
meaning is associated with each tag, we can not
make generalizations across verb classes. In con-
trast, the use of a shared set of role labels, such
System Type Precision Recall F
UBC-UPC Open 84.51 82.24 83.36?0.5
UBC-UPC Closed 85.04 82.07 83.52?0.5
RTV Closed 81.82 70.37 75.66?0.6
Without ?say?
UBC-UPC Open 78.57 74.70 76.60?0.8
UBC-UPC Closed 78.67 73.94 76.23?0.8
RTV Closed 74.15 57.85 65.00?0.9
Table 5: System performance on PropBank argu-
ments.
as VerbNet roles, would facilitate both inferencing
and generalization. VerbNet has more traditional la-
bels such as Agent, Patient, Theme, Beneficiary, etc.
(Kipper et al, 2006).
Therefore, we chose to annotate the corpus us-
ing two different role label sets: the PropBank role
set and the VerbNet role set. VerbNet roles were
generated using the SemLink mapping (Loper et al,
2007), which provides a mapping between Prop-
Bank and VerbNet role labels. In a small number of
cases, no VerbNet role was available (e.g., because
VerbNet did not contain the appropriate sense of the
verb). In those cases, the PropBank role label was
used instead.
We proposed two levels of participation in this
task: i) Closed ? the systems could use only the an-
notated data provided and nothing else. ii) Open ?
where systems could use PropBank data from Sec-
tions 02-21, as well as any other resource for training
their labelers.
3.1 Data
We selected 50 verbs from the 65 in the lexical sam-
ple task for the SRL task. The partitioning into train
and test set was done in the same fashion as for the
lexical sample task. Since PropBank does not tag
any noun predicates, none of the 35 nouns from the
lexical sample task were part of this data.
3.2 Results
For each system, we calculated the precision, re-
call, and F-measure for both role label sets. Scores
were calculated using the srl-eval.pl script from
the CoNLL-2005 scoring package (Carreras and
Ma`rquez, 2005). Only two teams chose to perform
the SRL subtask. The performance of these two
teams is shown in Table 5 and Table 6.
91
System Type Precision Recall F
UBC-UPC Open 85.31 82.08 83.66?0.5
UBC-UPC Closed 85.31 82.08 83.66?0.5
RTV Closed 81.58 70.16 75.44?0.6
Without ?say?
UBC-UPC Open 79.23 73.88 76.46?0.8
UBC-UPC Closed 79.23 73.88 76.46?0.8
RTV Closed 73.63 57.44 64.53?0.9
Table 6: System performance on VerbNet roles.
3.3 Discussion
Given that only two systems participated in the task,
it is difficult to form any strong conclusions. It
should be noted that since there was no additional
VerbNet role data to be used by the Open system, the
performance of that on PropBank arguments as well
as VerbNet roles is exactly identical. It can be seen
that there is almost no difference between the perfor-
mance of the Open and Closed systems for tagging
PropBank arguments. The reason for this is the fact
that all the instances of the lemma under consider-
ation was selected from the Propbank corpus, and
probably the number of training instances for each
lemma as well as the fact that the predicate is such
an important feature combine to make the difference
negligible. We also realized that more than half of
the test instances were contributed by the predicate
?say? ? the performance over whose arguments is in
the high 90s. To remove the effect of ?say? we also
computed the performances after excluding exam-
ples of ?say? from the test set. These numbers are
shown in the bottom half of the two tables. These
results are not directly comparable to the CoNLL-
2005 shared task since: i) this test set comprises
Sections 01, 22, 23 and 24 as opposed to just Sec-
tion 23, and ii) this test set comprises data for only
50 predicates as opposed to all the verb predicates in
the CoNLL-2005 shared task.
4 Conclusions
The results in the previous discussion seem to con-
firm the hypothesis that there is a predictable corre-
lation between human annotator agreement and sys-
tem performance. Given high enough ITA rates we
can can hope to build sense disambiguation systems
that perform at a level that might be of use to a con-
suming natural language processing application. It
is also encouraging that the more informative Verb-
Net roles which have better/direct applicability in
downstream systems, can also be predicted with al-
most the same degree of accuracy as the PropBank
arguments from which they are mapped.
5 Acknowledgments
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency
(DARPA/IPTO) under the GALE program,
DARPA/CMO Contract No. HR0011-06-C-0022;
National Science Foundation Grant NSF-0415923,
Word Sense Disambiguation; the DTO-AQUAINT
NBCHC040036 grant under the University of
Illinois subcontract to University of Pennsylvania
2003-07911-01; and NSF-ITR-0325646:
Domain-Independent Semantic Interpretation.
References
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduction to the
CoNLL-2004 shared task: Semantic role labeling. In
Proceedings of CoNLL-2004.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling. In
Proceedings of CoNLL-2005.
Jinying Chen, Andrew Schein, Lyle Ungar, and Martha Palmer.
2006. An empirical study of the behavior of active learning for
word sense disambiguation. In Proceedings of HLT/NAACL.
Timothy Chklovski and Rada Mihalcea. 2002. Building a
sense tagged corpus with open mind word expert. In
Proceedings of ACL-02 Workshop on WSD.
Bart Decadt, Ve?ronique Hoste, Walter Daelemans, and Antal
Van den Bosch. 2004. GAMBL, genetic algorithm
optimization of memory-based wsd. In Senseval-3.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The 90%
solution. In Proceedings of HLT/NAACL, June.
Karin Kipper, Anna Korhonen, Neville Ryant, and Martha
Palmer. 2006. Extending VerbNet with novel verb classes. In
LREC-06.
Ken Litkowski. 2004. Senseval-3 task: Automatic labeling of
semantic roles. In Proceedings of Senseval-3.
Edward Loper, Szu ting Yi, and Martha Palmer. 2007.
Combining lexical resources: Mapping between propbank and
verbnet. In Proceedings of the IWCS-7.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The
proposition bank: A corpus annotated with semantic roles.
Computational Linguistics, 31(1):71?106.
Martha Palmer. 2000. Consistent criteria for sense
distinctions. Computers and the Humanities, 34(1-1):217?222.
92
Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 62?70,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Multidisciplinary Instruction with the Natural Language Toolkit
Steven Bird
Department of Computer Science
University of Melbourne
sb@csse.unimelb.edu.au
Ewan Klein
School of Informatics
University of Edinburgh
ewan@inf.ed.ac.uk
Edward Loper
Computer and Information Science
University of Pennsylvania
edloper@gradient.cis.upenn.edu
Jason Baldridge
Department of Linguistics
University of Texas at Austin
jbaldrid@mail.utexas.edu
Abstract
The Natural Language Toolkit (NLTK) is
widely used for teaching natural language
processing to students majoring in linguistics
or computer science. This paper describes
the design of NLTK, and reports on how
it has been used effectively in classes that
involve different mixes of linguistics and
computer science students. We focus on three
key issues: getting started with a course,
delivering interactive demonstrations in the
classroom, and organizing assignments and
projects. In each case, we report on practical
experience and make recommendations on
how to use NLTK to maximum effect.
1 Introduction
It is relatively easy to teach natural language pro-
cessing (NLP) in a single-disciplinary mode to a uni-
form cohort of students. Linguists can be taught to
program, leading to projects where students manip-
ulate their own linguistic data. Computer scientists
can be taught methods for automatic text processing,
leading to projects on text mining and chatbots. Yet
these approaches have almost nothing in common,
and it is a stretch to call either of these NLP: more
apt titles for such courses might be ?linguistic data
management? and ?text technologies.?
The Natural Language Toolkit, or NLTK, was
developed to give a broad range of students access
to the core knowledge and skills of NLP (Loper
and Bird, 2002). In particular, NLTK makes it
feasible to run a course that covers a substantial
amount of theory and practice with an audience
consisting of both linguists and computer scientists.
NLTK is a suite of Python modules distributed
under the GPL open source license via nltk.org.
NLTK comes with a large collection of corpora,
extensive documentation, and hundreds of exercises,
making NLTK unique in providing a comprehensive
framework for students to develop a computational
understanding of language. NLTK?s code base of
100,000 lines of Python code includes support
for corpus access, tokenizing, stemming, tagging,
chunking, parsing, clustering, classification,
language modeling, semantic interpretation,
unification, and much else besides. As a measure of
its impact, NLTK has been used in over 60 university
courses in 20 countries, listed on the NLTK website.
Since its inception in 2001, NLTK has undergone
considerable evolution, based on the experience
gained by teaching courses at several universities,
and based on feedback from many teachers and
students.1 Over this period, a series of practical
online tutorials about NLTK has grown up into a
comprehensive online book (Bird et al, 2008). The
book has been designed to stay in lock-step with the
NLTK library, and is intended to facilitate ?active
learning? (Bonwell and Eison, 1991).
This paper describes the main features of
NLTK, and reports on how it has been used
effectively in classes that involve a combination
of linguists and computer scientists. First we
discuss aspects of the design of the toolkit that
1(Bird and Loper, 2004; Loper, 2004; Bird, 2005; Hearst,
2005; Bird, 2006; Klein, 2006; Liddy and McCracken, 2005;
Madnani, 2007; Madnani and Dorr, 2008; Baldridge and Erk,
2008)
62
arose from our need to teach computational
linguistics to a multidisciplinary audience (?2). The
following sections cover three distinct challenges:
getting started with a course (?3); interactive
demonstrations (?4); and organizing assignments
and projects (?5).
2 Design Decisions Affecting Teaching
2.1 Python
We chose Python2 as the implementation language
for NLTK because it has a shallow learning curve, its
syntax and semantics are transparent, and it has good
string-handling functionality. As an interpreted
language, Python facilitates interactive exploration.
As an object-oriented language, Python permits
data and methods to be encapsulated and re-used
easily. Python comes with an extensive standard
library, including tools for graphical programming
and numerical processing, which means it can be
used for a wide range of non-trivial applications.
Python is ideal in a context serving newcomers and
experienced programmers (Shannon, 2003).
We have taken the step of incorporating a detailed
introduction to Python programming in the NLTK
book, taking care to motivate programming con-
structs with linguistic examples. Extensive feedback
from students has been humbling, and revealed that
for students with no prior programming experience,
it is almost impossible to over-explain. Despite the
difficulty of providing a self-contained introduction
to Python for linguists, we nevertheless have also
had very positive feedback, and in combination with
the teaching techniques described below, have man-
aged to bring a large group of non-programmer stu-
dents rapidly to a point where they could carry out
interesting and useful exercises in text processing.
In addition to the NLTK book, the code in the
NLTK core is richly documented, using Python doc-
strings and Epydoc3 support for API documenta-
tion.4 Access to the code documentation is available
using the Python help() command at the interac-
tive prompt, and this can be especially useful for
checking the parameters and return type of func-
tions.
2http://www.python.org/
3http://epydoc.sourceforge.net/
4http://nltk.org/doc/api/
Other Python libraries are useful in the NLP con-
text: NumPy provides optimized support for linear
algebra and sparse arrays (NumPy, 2008) and PyLab
provides sophisticated facilities for scientific visual-
ization (Matplotlib, 2008).
2.2 Coding Requirements
As discussed in Loper & Bird (2002), the priorities
for NLTK code focus on its teaching role. When code
is readable, a student who doesn?t understand the
maths of HMMs, smoothing, and so on may benefit
from looking at how an algorithm is implemented.
Thus consistency, simplicity, modularity are all vital
features of NLTK code. A similar importance is
placed on extensibility, since this helps to ensure that
the code grows as a coherent whole, rather than by
unpredictable and haphazard additions.
By contrast, although efficiency cannot be
ignored, it has always taken second place to
simplicity and clarity of coding. In a similar vein,
we have tried to avoid clever programming tricks,
since these typically hinder intelligibility of the
code. Finally, comprehensiveness of coverage has
never been an overriding concern of NLTK; this
leaves open many possibilities for student projects
and community involvement.
2.3 Naming
One issue which has absorbed a considerable
amount of attention is the naming of user-oriented
functions in NLTK. To a large extent, the system of
naming is the user interface to the toolkit, and it is
important that users should be able to guess what
action might be performed by a given function.
Consequently, naming conventions need to be
consistent and semantically transparent. At the same
time, there is a countervailing pressure for relatively
succinct names, since excessive verbosity can also
hinder comprehension and usability. An additional
complication is that adopting an object-oriented
style of programming may be well-motivated for
a number of reasons but nevertheless baffling to
the linguist student. For example, although it is
perfectly respectable to invoke an instance method
WordPunctTokenizer().tokenize(text)
(for some input string text), a simpler version is
also provided: wordpunct tokenize(text).
63
2.4 Corpus Access
The scope of exercises and projects that students
can perform is greatly increased by the inclusion
of a large collection of corpora, along with easy-to-
use corpus readers. This collection, which currently
stands at 45 corpora, includes parsed, POS-tagged,
plain text, categorized text, and lexicons.5
In designing the corpus readers, we emphasized
simplicity, consistency, and efficiency. Corpus
objects, such as nltk.corpus.brown and
nltk.corpus.treebank, define common
methods for reading the corpus contents, abstracting
away from idiosyncratic file formats to provide a
uniform interface. See Figure 1 for an example of
accessing POS-tagged data from different tagged
and parsed corpora.
The corpus objects provide methods for loading
corpus contents in various ways. Common meth-
ods include: raw(), for the raw contents of the
corpus; words(), for a list of tokenized words;
sents(), for the same list grouped into sentences;
tagged words(), for a list of (word, tag) pairs;
tagged sents(), for the same list grouped into
sentences; and parsed sents(), for a list of parse
trees. Optional parameters can be used to restrict
what portion of the corpus is returned, e.g., a partic-
ular section, or an individual corpus file.
Most corpus reader methods return a corpus view
which acts as a list of text objects, but maintains
responsiveness and memory efficiency by only load-
ing items from the file on an as-needed basis. Thus,
when we print a corpus view we only load the first
block of the corpus into memory, but when we pro-
cess this object we load the whole corpus:
>>> nltk.corpus.alpino.words()
[?De?, ?verzekeringsmaatschappijen?,
?verhelen?, ...]
>>> len(nltk.corpus.alpino.words())
139820
2.5 Accessing Shoebox Files
NLTK provides functionality for working with
?Shoebox? (or ?Toolbox?) data (Robinson et
al., 2007). Shoebox is a system used by many
documentary linguists to produce lexicons and
interlinear glossed text. The ability to work
5http://nltk.org/corpora.html
straightforwardly with Shoebox data has created a
new incentive for linguists to learn how to program.
As an example, in the Linguistics Department at
the University of Texas at Austin, a course has been
offered on Python programming and working with
corpora,6 but so far uptake from the target audience
of core linguistics students has been low. They usu-
ally have practical computational needs and many of
them are intimidated by the very idea of program-
ming. We believe that the appeal of this course can
be enhanced by designing a significant component
with the goal of helping documentary linguistics stu-
dents take control of their own Shoebox data. This
will give them skills that are useful for their research
and also transferable to other activities. Although
the NLTK Shoebox functionality was not originally
designed with instruction in mind, its relevance to
students of documentary linguistics is highly fortu-
itous and may prove appealing for similar linguistics
departments.
3 Getting Started
NLP is usually only available as an elective course,
and students will vote with their feet after attending
one or two classes. This initial period is important
for attracting and retaining students. In particular,
students need to get a sense of the richness of lan-
guage in general, and NLP in particular, while gain-
ing a realistic impression of what will be accom-
plished during the course and what skills they will
have by the end. During this time when rapport
needs to be rapidly established, it is easy for instruc-
tors to alienate students through the use of linguistic
or computational concepts and terminology that are
foreign to students, or to bore students by getting
bogged down in defining terms like ?noun phrase?
or ?function? which are basic to one audience and
new for the other. Thus, we believe it is crucial
for instructors to understand and shape the student?s
expectations, and to get off to a good start. The best
overall strategy that we have found is to use succinct
nuggets of NLTK code to stimulate students? interest
in both data and processing techniques.
6http://comp.ling.utexas.edu/courses/
2007/corpora07/
64
>>> nltk.corpus.treebank.tagged_words()
[(?Pierre?, ?NNP?), (?Vinken?, ?NNP?), (?,?, ?,?), ...]
>>> nltk.corpus.brown.tagged_words()
[(?The?, ?AT?), (?Fulton?, ?NP-TL?), ...]
>>> nltk.corpus.floresta.tagged_words()
[(?Um?, ?>N+art?), (?revivalismo?, ?H+n?), ...]
>>> nltk.corpus.cess_esp.tagged_words()
[(?El?, ?da0ms0?), (?grupo?, ?ncms000?), ...]
>>> nltk.corpus.alpino.tagged_words()
[(?De?, ?det?), (?verzekeringsmaatschappijen?, ?noun?), ...]
Figure 1: Accessing Different Corpora via a Uniform Interface
3.1 Student Expectations
Computer science students come to NLP expecting
to learn about NLP algorithms and data structures.
They typically have enough mathematical prepara-
tion to be confident in playing with abstract for-
mal systems (including systems of linguistic rules).
Moreover, they are already proficient in multiple
programming languages, and have little difficulty in
learning NLP algorithms by reading and manipulat-
ing the implementations provided with NLTK. At the
same time, they tend to be unfamiliar with the termi-
nology and concepts that linguists take for granted,
and may struggle to come up with reasonable lin-
guistic analyses of data.
Linguistics students, on the other hand, are
interested in understanding NLP algorithms and
data structures only insofar as it helps them to
use computational tools to perform analytic tasks
from ?core linguistics,? e.g. writing a set of CFG
productions to parse some sentences, or plugging
together NLP components in order to derive the
subcategorization requirements of verbs in a corpus.
They are usually not interested in reading significant
chunks of code; it isn?t what they care about and
they probably lack the confidence to poke around in
source files.
In a nutshell, the computer science students typ-
ically want to analyze the tools and synthesize new
implementations, while the linguists typically want
to use the tools to analyze language and synthe-
size new theories. There is a risk that the former
group never really gets to grips with natural lan-
guage, while the latter group never really gets to
grips with processing. Instead, computer science
students need to learn that NLP is not just an applica-
tion of techniques from formal language theory and
compiler construction, and linguistics students need
to understand that NLP is not just computer-based
housekeeping and a solution to the shortcomings of
office productivity software for managing their data.
In many courses, linguistics students or computer
science students will dominate the class numeri-
cally, simply because the course is only listed in
one department. In such cases it is usually enough
to provide additional support in the form of some
extra readings, tutorials, and exercises in the open-
ing stages of the course. In other cases, e.g. courses
we have taught at the universities of Edinburgh, Mel-
bourne, Pennsylvania, and Texas-Austin or in sum-
mer intensive programs in several countries, there is
more of an even split, and the challenge of serving
both cohorts of students becomes acute. It helps to
address this issue head-on, with an early discussion
of the goals of the course.
3.2 Articulating the Goals
Despite an instructor?s efforts to add a cross-
disciplinary angle, students easily ?revert to
type.? The pressure of assessment encourages
students to emphasize what they do well. Students?
desire to understand what is expected of them
encourages instructors to stick to familiar
assessment instruments. As a consequence,
the path of least resistance is for students to
remain firmly monolingual in their own discipline,
while acquiring a smattering of words from a
foreign language, at a level we might call ?survival
linguistics? or ?survival computer science.? If they
ever get to work in a multidisciplinary team they are
65
likely only to play a type-cast role.
Asking computer science students to write their
first essay in years, or asking linguistics students
to write their first ever program, leads to stressed
students who complain that they don?t know what
is expected of them. Nevertheless, students need
to confront the challenge of becoming bilingual, of
working hard to learn the basics of another disci-
pline. In parallel, instructors need to confront the
challenge of synthesizing material from linguistics
and computer science into a coherent whole, and
devising effective methods for teaching, learning,
and assessment.
3.3 Entry Points
It is possible to identify several distinct pathways
into the field of Computational Linguistics. Bird
(2008) identifies four; each of these are supported
by NLTK, as detailed below:
Text Processing First: NLTK supports variety of
approaches to tokenization, tagging, evaluation, and
language engineering more generally.
Programming First: NLTK is based on Python
and the documentation teaches the language and
provides many examples and exercises to test and
reinforce student learning.
Linguistics First: Here, students come with a
grounding in one or more areas of linguistics, and
focus on computational approaches to that area by
working with the relevant chapter of the NLTK book
in conjunction with learning how to program.
Algorithms First: Here, students come with a
grounding in one or more areas of computer sci-
ence, and can use, test and extend NLTK?S reference
implementations of standard NLP algorithms.
3.4 The First Lecture
It is important that the first lecture is effective at
motivating and exemplifying NLP to an audience
of computer science and linguistics students. They
need to get an accurate sense of the interesting
conceptual and technical challenges awaiting them.
Fortunately, the task is made easier by the simple
fact that language technologies, and language itself,
are intrinsically interesting and appealing to a wide
audience. Several opening topics appear to work
particularly well:
The holy grail: A long term challenge,
mythologized in science fiction movies, is to
build machines that understand human language.
Current technologies that exhibit some basic level
of natural language understanding include spoken
dialogue systems, question answering systems,
summarization systems, and machine translation
systems. These can be demonstrated in class
without too much difficulty. The Turing test is a
linguistic test, easily understood by all students, and
which helps the computer science students to see
NLP in relation to the field of Artificial Intelligence.
The evolution of programming languages has
brought them closer to natural language, helping
students see the essentially linguistic purpose of
this central development in computer science.
The corresponding holy grail in linguistics is full
understanding of the human language faculty;
writing programs and building machines surely
informs this quest too.
The riches of language: It is easy to find
examples of the creative richness of language in its
myriad uses. However, linguists will understand
that language contains hidden riches that can only
be uncovered by careful analysis of large quantities
of linguistically annotated data, work that benefits
from suitable computational tools. Moreover, the
computational needs for exploratory linguistic
research often go beyond the capabilities of the
current tools. Computer scientists will appreciate
the cognate problem of extracting information from
the web, and the economic riches associated with
state-of-the-art text mining technologies.
Formal approaches to language: Computer sci-
ence and linguistics have a shared history in the area
of philosophical logic and formal language theory.
Whether the language is natural or artificial, com-
puter scientists and linguists use similar logical for-
malisms for investigating the formal semantics of
languages, similar grammar formalisms for model-
ing the syntax of languages, and similar finite-state
methods for manipulating text. Both rely on the
recursive, compositional nature of natural and arti-
ficial languages.
3.5 First Assignment
The first coursework assignment can be a significant
step forwards in helping students get to grips with
66
the material, and is best given out early, perhaps
even in week 1. We have found it advisable for
this assignment to include both programming and
linguistics content. One example is to ask students
to carry out NP chunking of some data (e.g. a section
of the Brown Corpus). The nltk.RegexpParser
class is initialized with a set of chunking rules
expressed in a simple, regular expression-oriented
syntax, and the resulting chunk parser can be run
over POS-tagged input text. Given a Gold Standard
test set like the CoNLL-2000 data,7 precision
and recall of the chunk grammar can be easily
determined. Thus, if students are given an existing,
incomplete set of rules as their starting point, they
just have to modify and test their rules.
There are distinctive outcomes for each set of stu-
dents: linguistics students learn to write grammar
fragments that respect the literal-minded needs of
the computer, and also come to appreciate the noisi-
ness of typical NLP corpora (including automatically
annotated corpora like CoNLL-2000). Computer
science students become more familiar with parts
of speech and with typical syntactic structures in
English. Both groups learn the importance of formal
evaluation using precision and recall.
4 Interactive Demonstrations
4.1 Python Demonstrations
Python fosters a highly interactive style of teaching.
It is quite natural to build up moderately complex
programs in front of a class, with the less confi-
dent students transcribing it into a Python session
on their laptop to satisfy themselves it works (but
not necessarily understanding everything they enter
first time), while the stronger students quickly grasp
the theoretical concepts and algorithms. While both
groups can be served by the same presentation, they
tend to ask quite different questions. However, this
is addressed by dividing them into smaller clusters
and having teaching assistants visit them separately
to discuss issues arising from the content.
The NLTK book contains many examples, and
the instructor can present an interactive lecture that
includes running these examples and experiment-
ing with them in response to student questions. In
7http://www.cnts.ua.ac.be/conll2000/
chunking/
early classes, the focus will probably be on learning
Python. In later classes, the driver for such interac-
tive lessons can be an externally-motivated empiri-
cal or theoretical question.
As a practical matter, it is important to consider
low-level issues that may get in the way of students?
ability to capture the material covered in interactive
Python sessions. These include choice of appropri-
ate font size for screen display, avoiding the prob-
lem of output scrolling the command out of view,
and distributing a log of the instructor?s interactive
session for students to study in their own time.
4.2 NLTK Demonstrations
A significant fraction of any NLP syllabus covers
fundamental data structures and algorithms. These
are usually taught with the help of formal notations
and complex diagrams. Large trees and charts are
copied onto the board and edited in tedious slow
motion, or laboriously prepared for presentation
slides. It is more effective to use live demonstrations
in which those diagrams are generated and updated
automatically. NLTK provides interactive graphical
user interfaces, making it possible to view program
state and to study program execution step-by-step.
Most NLTK components have a demonstration
mode, and will perform an interesting task without
requiring any special input from the user. It is
even possible to make minor modifications to
programs in response to ?what if? questions. In this
way, students learn the mechanics of NLP quickly,
gain deeper insights into the data structures and
algorithms, and acquire new problem-solving skills.
An example of a particularly effective set
of demonstrations are those for shift-reduce
and recursive descent parsing. These make
the difference between the algorithms glaringly
obvious. More importantly, students get a concrete
sense of many issues that affect the design of
algorithms for tasks like parsing. The partial
analysis constructed by the recursive descent
parser bobs up and down as it steps forward and
backtracks, and students often go wide-eyed as the
parser retraces its steps and does ?dumb? things
like expanding N to man when it has already
tried the rule unsuccessfully (but is now trying
to match a bare NP rather than an NP with a PP
modifier). Linguistics students who are extremely
67
knowledgeable about context-free grammars and
thus understand the representations gain a new
appreciation for just how naive an algorithm can be.
This helps students grasp the need for techniques
like dynamic programming and motivates them to
learn how they can be used to solve such problems
much more efficiently.
Another highly useful aspect of NLTK is the abil-
ity to define a context-free grammar using a sim-
ple format and to display tree structures graphically.
This can be used to teach context-free grammars
interactively, where the instructor and the students
develop a grammar from scratch and check its cov-
erage against a testbed of grammatical and ungram-
matical sentences. Because it is so easy to modify
the grammar and check its behavior, students readily
participate and suggest various solutions. When the
grammar produces an analysis for an ungrammatical
sentence in the testbed, the tree structure can be dis-
played graphically and inspected to see what went
wrong. Conversely, the parse chart can be inspected
to see where the grammar failed on grammatical sen-
tences.
NLTK?s easy access to many corpora greatly facil-
itates classroom instruction. It is straightforward to
pull in different sections of corpora and build pro-
grams in class for many different tasks. This not
only makes it easier to experiment with ideas on the
fly, but also allows students to replicate the exer-
cises outside of class. Graphical displays that show
the dispersion of terms throughout a text also give
students excellent examples of how a few simple
statistics collected from a corpus can provide useful
and interesting views on a text?including seeing the
frequency with which various characters appear in a
novel. This can in turn be related to other resources
like Google Trends, which shows the frequency with
which a term has been referenced in news reports or
been used in search terms over several years.
5 Exercises, Assignments and Projects
5.1 Exercises
Copious exercises are provided with the NLTK book;
these have been graded for difficulty relative to the
concepts covered in the preceding sections of the
book. Exercises have the tremendous advantage of
building on the NLTK infrastructure, both code and
documentation. The exercises are intended to be
suitable both for self-paced learning and in formally
assigned coursework.
A mixed class of linguistics and computer sci-
ence students will have a diverse range of program-
ming experience, and students with no programming
experience will typically have different aptitudes for
programming (Barker and Unger, 1983; Caspersen
et al, 2007). A course which forces all students
to progress at the same rate will be too difficult for
some, and too dull for others, and will risk alien-
ating many students. Thus, course materials need
to accommodate self-paced learning. An effective
way to do this is to provide students with contexts
in which they can test and extend their knowledge at
their own rate.
One such context is provided by lecture or lab-
oratory sessions in which students have a machine
in front of them (or one between two), and where
there is time to work through a series of exercises to
consolidate what has just been taught from the front,
or read from a chapter of the book. When this can be
done at regular intervals, it is easier for students to
know which part of the materials to re-read. It also
encourages them to get into the habit of checking
their understanding of a concept by writing code.
When exercises are graded for difficulty, it is
easier for students to understand how much effort
is expected, and whether they even have time to
attempt an exercise. Graded exercises are also good
for supporting self-evaluation. If a student takes
20 minutes to write a solution, they also need to
have some idea of whether this was an appropriate
amount of time.
The exercises are also highly adaptable. It is com-
mon for instructors to take them as a starting point
in building homework assignments that are tailored
to their own students. Some instructors prefer to
include exercises that do not allow students to take
advantage of built-in NLTK functionality, e.g. using
a Python dictionary to count word frequencies in the
Brown corpus rather than NLTK?s FreqDist (see
Figure 2). This is an important part of building
facility with general text processing in Python, since
eventually students will have to work outside of
the NLTK sandbox. Nonetheless, students often use
NLTK functionality as part of their solutions, e.g.,
for managing frequencies and distributions. Again,
68
nltk.FreqDist(nltk.corpus.brown.words())
fd = nltk.FreqDist()
for filename in corpus_files:
text = open(filename).read()
for w in nltk.wordpunct_tokenize(text):
fd.inc(w)
counts = {}
for w in nltk.corpus.brown.words():
if w not in counts:
counts[w] = 0
counts[w] += 1
Figure 2: Three Ways to Build up a Frequency Distribu-
tion of Words in the Brown Corpus
this flexibility is a good thing: students learn to
work with resources they know how to use, and can
branch out to new exercises from that basis. When
course content includes discussion of Unix com-
mand line utilities for text processing, students can
furthermore gain a better appreciation of the pros
and cons of writing their own scripts versus using
an appropriate Unix pipeline.
5.2 Assignments
NLTK supports assignments of varying difficulty and
scope: experimenting with existing components to
see what happens for different inputs or parameter
settings; modifying existing components and
creating systems using existing components;
leveraging NLTK?s extensible architecture by
developing entirely new components; or employing
NLTK?s interfaces to other toolkits such as Weka
(Witten and Frank, 2005) and Prover9 (McCune,
2008).
5.3 Projects
Group projects involving a mixture of linguists
and computer science students have an initial
appeal, assuming that each kind of student can
learn from the other. However, there?s a complex
social dynamic in such groups, one effect of which
is that the linguistics students may opt out of the
programming aspects of the task, perhaps with
view that their contribution would only hurt the
chances of achieving a good overall project mark.
It is difficult to mandate significant collaboration
across disciplinary boundaries, with the more
likely outcome being, for example, that a parser is
developed by a computer science team member,
then thrown over the wall to a linguist who will
develop an appropriate grammar.
Instead, we believe that it is generally more pro-
ductive in the context of a single-semester introduc-
tory course to have students work individually on
their own projects. Distinct projects can be devised
for students depending on their background, or stu-
dents can be given a list of project topics,8 and
offered option of self-proposing other projects.
6 Conclusion
We have argued that the distinctive features of
NLTK make it an apt vehicle for teaching NLP
to mixed audiences of linguistic and computer
science students. On the one hand, complete
novices can quickly gain confidence in their ability
to do interesting and useful things with language
processing, while the transparency and consistency
of the implementation also makes it easy for
experienced programmers to learn about natural
language and to explore more challenging tasks.
The success of this recipe is borne out by the
wide uptake of the toolkit, not only within tertiary
education but more broadly by users who just want
try their hand at NLP. We also have encouraging
results in presenting NLTK in classrooms at the
secondary level, thereby trying to inspire the
computational linguists of the future!
Finally, we believe that NLTK has gained much
by participating in the Open Source software move-
ment, specifically from the infrastructure provided
by SourceForge.net and from the invaluable
contributions of a wide range of people, including
many students.
7 Acknowledgments
We are grateful to the members of the NLTK com-
munity for their helpful feedback on the toolkit and
their many contributions. We thank the anonymous
reviewers for their feedback on an earlier version of
this paper.
8http://nltk.org/projects.html
69
References
Jason Baldridge and Katrin Erk. 2008. Teaching com-
putational linguistics to a large, diverse student body:
courses, tools, and interdepartmental interaction. In
Proceedings of the Third Workshop on Issues in Teach-
ing Computational Linguistics. Association for Com-
putational Linguistics.
Ricky Barker and E. A. Unger. 1983. A predictor for
success in an introductory programming class based
upon abstract reasoning development. ACM SIGCSE
Bulletin, 15:154?158.
Steven Bird and Edward Loper. 2004. NLTK: The Nat-
ural Language Toolkit. In Companion Volume to the
Proceedings of 42st Annual Meeting of the Association
for Computational Linguistics, pages 214?217. Asso-
ciation for Computational Linguistics.
Steven Bird, Ewan Klein, and Edward Loper. 2008.
Natural Language Processing in Python. http://
nltk.org/book.html.
Steven Bird. 2005. NLTK-Lite: Efficient scripting
for natural language processing. In 4th International
Conference on Natural Language Processing, Kanpur,
India, pages 1?8.
Steven Bird. 2006. NLTK: The Natural Language
Toolkit. In Proceedings of the COLING/ACL 2006
Interactive Presentation Sessions, pages 69?72, Syd-
ney, Australia, July. Association for Computational
Linguistics.
Steven Bird. 2008. Defining a core body of knowledge
for the introductory computational linguistics curricu-
lum. In Proceedings of the Third Workshop on Issues
in Teaching Computational Linguistics. Association
for Computational Linguistics.
Charles C. Bonwell and James A. Eison. 1991. Active
Learning: Creating Excitement in the Classroom.
Washington, D.C.: Jossey-Bass.
Michael Caspersen, Kasper Larsen, and Jens Benned-
sen. 2007. Mental models and programming aptitude.
SIGCSE Bulletin, 39:206?210.
Marti Hearst. 2005. Teaching applied natural language
processing: Triumphs and tribulations. In Proceedings
of the Second ACL Workshop on Effective Tools and
Methodologies for Teaching NLP and CL, pages 1?8,
Ann Arbor, Michigan, June. Association for Compu-
tational Linguistics.
Ewan Klein. 2006. Computational semantics in the Nat-
ural Language Toolkit. In Proceedings of the Aus-
tralasian Language Technology Workshop, pages 26?
33.
Elizabeth Liddy and Nancy McCracken. 2005. Hands-on
NLP for an interdisciplinary audience. In Proceedings
of the Second ACL Workshop on Effective Tools and
Methodologies for Teaching NLP and CL, pages 62?
68, Ann Arbor, Michigan, June. Association for Com-
putational Linguistics.
Edward Loper and Steven Bird. 2002. NLTK: The Nat-
ural Language Toolkit. In Proceedings of the ACL
Workshop on Effective Tools and Methodologies for
Teaching Natural Language Processing and Computa-
tional Linguistics, pages 62?69. Association for Com-
putational Linguistics.
Edward Loper. 2004. NLTK: Building a pedagogical
toolkit in Python. In PyCon DC 2004. Python Soft-
ware Foundation.
Nitin Madnani and Bonnie Dorr. 2008. Combining
open-source with research to re-engineer a hands-on
introductory NLP course. In Proceedings of the Third
Workshop on Issues in Teaching Computational Lin-
guistics. Association for Computational Linguistics.
Nitin Madnani. 2007. Getting started on natural lan-
guage processing with Python. ACM Crossroads,
13(4).
Matplotlib. 2008. Matplotlib: Python 2D plotting
library. http://matplotlib.sourceforge.
net/.
William McCune. 2008. Prover9: Automated
theorem prover for first-order and equational logic.
http://www.cs.unm.edu/?mccune/mace4/
manual-examples.html.
NumPy. 2008. NumPy: Scientific computing with
Python. http://numpy.scipy.org/.
Stuart Robinson, Greg Aumann, and Steven Bird. 2007.
Managing fieldwork data with Toolbox and the Natu-
ral Language Toolkit. Language Documentation and
Conservation, 1:44?57.
Christine Shannon. 2003. Another breadth-first
approach to CS I using Python. In Proceedings of
the 34th SIGCSE Technical Symposium on Computer
Science Education, pages 248?251. ACM.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Morgan
Kaufmann.
70
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 61?69,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Empirical Studies in Learning to Read 
Marjorie Freedman, Edward Loper, Elizabeth Boschee, Ralph Weischedel 
BBN Raytheon Technologies 
10 Moulton St 
Cambridge, MA 02139 
{mfreedma, eloper, eboschee, weischedel}@bbn.com 
Abstract 
In this paper, we present empirical results on 
the challenge of learning to read. That is, giv-
en a handful of examples of the concepts and 
relations in an ontology and a large corpus, 
the system should learn to map from text to 
the concepts/relations of the ontology. In this 
paper, we report contrastive experiments on 
the recall, precision, and F-measure (F) of the 
mapping in the following conditions: (1) em-
ploying word-based patterns, employing se-
mantic structure, and combining the two; and 
(2) fully automatic learning versus allowing 
minimal questions of a human informant. 
1 Introduction 
This paper reports empirical results with an algo-
rithm that ?learns to read? text and map that text 
into concepts and relations in an ontology specified 
by the user. Our approach uses unsupervised and 
semi-supervised algorithms to harness the diversity 
and redundancy of the ways concepts and relations 
are expressed in document collections. Diversity 
can be used to automatically generate patterns and 
paraphrases for new concepts and relations to 
boost recall. Redundancy can be exploited to au-
tomatically check and improve the accuracy of 
those patterns, allowing for system learning with-
out human supervision.  
For example, the system learns how to recog-
nize a new relation (e.g. invent), starting from 5-20 
instances (e.g. Thomas Edison + the light bulb). 
The system iteratively searches a collection of 
documents to find sentences where those instances 
are expressed (e.g. ?Thomas Edison?s patent for 
the light bulb?), induces patterns over textual fea-
tures found in those instances (e.g. pa-
tent(possessive:A, for:B)), and repeats the cycle by 
applying the generated patterns to find additional 
instances followed by inducing more patterns from 
those instances. Unsupervised measures of redun-
dancy and coverage are used to estimate the relia-
bility of the induced patterns and learned instances; 
only the most reliable are added, which minimizes 
the amount of noise introduced at each step.  
There have been two approaches to evaluation 
of mapping text to concepts and relations: Auto-
matic Content Extraction (ACE)1 and Knowledge 
Base Population (KBP)2. In ACE, complete ma-
nual annotation for a small corpus (~25k words) 
was possible; thus, both recall and precision could 
be measured across every instance in the test set. 
This evaluation can be termed micro reading in 
that it evaluates every concept/relation mention in 
the corpus. In ACE, learning algorithms had 
roughly 300k words of training data.  
By contrast, in KBP, the corpus of documents 
in the test set was too large for a complete answer 
key. Rather than a complete answer key, relations 
were extracted for a list of entities; system output 
was pooled and judged manually. This type of 
reading has been termed macro reading3, since 
finding any instance of the relation in the 1.3M 
document corpus is measured success, rather than 
finding every instance. Only 118 queries were pro-
vided, though several hundred were created and 
distributed by participants.  
In the study in this paper, recall, precision, and 
F are measured for 11 relations under the following 
contrastive conditions 
                                                          
1 http://www.nist.gov/speech/tests/ace/ 
2 http://apl.jhu.edu/~paulmac/kbp.html  
3 See http://rtw.ml.cmu.edu/papers/mitchell-iswc09.pdf   
61
1. Patterns based on words vs. predicate-
argument structure vs. combining both. 
2. Fully automatic vs. a few periodic res-
ponses by humans to specific queries. 
Though many prior studies have focused on 
precision, e.g., to find any text justification to an-
swer a question, we focus equally on recall and 
report recall performance as well as precision. This 
addresses the challenge of finding information on 
rarely mentioned entities (no matter how challeng-
ing the expression). We believe the effect will be 
improved technology overall. We evaluate our sys-
tem in a micro-reading context on 11 relations. In a 
fully automatic configuration, the system achieves 
an F of .48 (Recall=.37, Precision=.68). With li-
mited human intervention, F rises to .58 (Re-
call=.49, Precision=.70). We see that patterns 
based on predicate-argument structure (text 
graphs) outperform patterns based on surface 
strings with respect to both precision and recall. 
Section 2 describes our approach; section 3, 
some challenges; section 4, the implementation; 
section 5, evaluation; section 6, empirical results 
on extraction type; section 7, the effect of periodic, 
limited human feedback; section 8, related work; 
and section 9, lessons learned and conclusions. 
2 Approach 
Our approach for learning patterns that can be used 
to detect relations is depicted in Figure 1. Initially, 
a few instances of the relation tuples are provided, 
along with a massive corpus, e.g., the web or the 
gigaword corpus from the Linguistic Data Consor-
tium (LDC). The diagram shows three inventor-
invention pairs, beginning with Thomas Edi-
son?light bulb. From these, we find candidate 
sentences in the massive corpus, e.g., Thomas Edi-
son invented the light bulb. Features extracted from 
the sentences retrieved, for example features of the 
text-graph (the predicate-argument structure con-
necting the two arguments), provide a training in-
stance for pattern induction. The induced patterns 
are added to the collection (database) of patterns.
Running the extended pattern collection over the 
corpus finds new, previously unseen relation 
tuples. From these new tuples, additional sentences 
which express those tuples can be retrieved, and 
the cycle of learning can continue. 
There is an analogous cycle of learning con-
cepts from instances and the large corpus; the ex-
periments in this paper do not report on that paral-
lel learning cycle. 
Figure 1: Approach to Learning Relations 
At the ith iteration, the steps are 
1. Given the set of hypothesized instances of the 
relation (triples HTi), find instances of such 
triples in the corpus. (On the first iteration, 
?hypothesized? triples are manually-generated 
seed examples.) 
2. Induce possible patterns. For each proposed 
pattern P: 
a. Apply pattern P to the corpus to generate a 
set of triples TP
b. Estimate precision as the confidence-
weighted average of the scores of the 
triples in TP. Reduce precision score by the 
percentage of triples in TP that violate us-
er-specified relation constraints (e.g. arity 
constraints described in 4.3)  
c. Estimate recall as the confidence-weighted 
percentage of triples in HTi found by the 
pattern 
3. Identify a set of high-confidence patterns HPi
using cutoffs automatically derived from rank-
based curves for precision, recall, and F-
measure (?=0.7) 
4. Apply high-confidence patterns to a Web-scale 
corpus to hypothesize new triples. For each 
proposed triple T 
a. Estimate score(T) as the expected proba-
bility that T is correct, calculated by com-
bining the respective precision and recall 
scores of all of the patterns that did or did 
not return it (using the Na?ve Bayes as-
sumption that all patterns are independent) 
b. Estimate confidence(T) as the percentage 
of patterns in HPi by which T was found 
5. Identify a set of high-confidence triples HTi+1

	














	


	 







	



	

	
	
 

	 
