Incorporating New Words Detection with Chinese Word Segmentation 
Hua-Ping ZHANG1   Jian GAO1  Qian MO 2  He-Yan HUANG1 
1
 Beijing Institute of Technology, Beijing, P.R.C 100081 
2
 Beijing Technology and Business University, Beijing, P.R.C 100048 
Email: kevinzhang@bit.edu.cn 
 
     
Abstract 
With development in Chinese words 
segmentation, in-vocabulary word 
segmentation and named entity 
recognition achieves state-of-art 
performance. However, new words 
become bottleneck to Chinese word 
segmentation. This paper presents the 
result from Beijing Institute of 
Technology (BIT) in the Sixth 
International Chinese Word 
Segmentation Bakeoff in 2010. Firstly, 
the author reviewed the problem caused 
by the new words in Chinese texts, then 
introduced the algorithm of new words 
detection. The final section provided 
the official evaluation result in this 
bakeoff and gave conclusions.  
1  Introduction 
With the rapid development of Internet with 
Chinese language, word segmentation received 
extensive attention. In-vocabulary word segmentation 
and named entity recognition have achieved state-of-art 
performance.  Chinese words are actually not well 
defined, and there is not a commonly accepted 
segmentation lexicon. It is hard to collect all 
possible new words, or predict new words occurred 
in the future. New words is the bottleneck to 
Chinese word segmentation. The problem became 
more severe with word segmentation on special 
domain texts, such as computer, medicine and 
finance. There are much specialized words which 
are difficult to be exported to the lexicon. So new 
words detection is very important, which would 
have more substantial impact on the performance 
of word segmentation than ambiguous 
segmentation.  
In this paper?we presented a method of new 
words detection, and then detailed the process of 
Chinese word segmentation incorporating new 
words detection. The last section provided the 
evaluation and gave our conclusions. 
2 Problem with new words 
In the process of Chinese word Segmentation, 
there are many mistakes because of new words. 
These new words are Out of vocabulary (OOV), so 
the system couldn?t distinguish them from original 
texts, and then impacted the results of word 
segmentation.  
We gave an example from Text C in medicine 
domain to explain and detect the new words. 
???????????????????
?????? 12???????? PAD???
????????????????????
??????ABI?????????????
????? 
The sentence should be segmented as follows? 
???  ?  ????  ??  ??  ?
?    ?  ??  ??????  ??  12  ?  
?  ?  ???  ??  PAD  ??  ?  ?  
?  ??  ??  ?  ??  ??  ??  ?  
?  ?  ??  ??  ?  ??  ??  ?    
ABI  ??  ??  ?  ??  ??  ????  
?  ??  ?  ? 
Here, both ?????? and ???????? 
are domain words, or new words beyond general 
segmentation lexicon. Therefore, new words from 
domain should be detected and added to 
segmentation lexicon before word segmentation.  
3 Word segmentation with new words 
detection 
3.1 Framework 
Word Segmentation With 
general lexicon+domain 
lexicon
Frequent String 
Detection
No
AV statistics, language 
modeling
Generate New Words
Domain 
Lexicon
Yes
New words 
threshold
Output 
words
Figure 1: The framework of Chinese word segmentation 
incorporating with new words detection
General 
Lexicon
Input Sentence
 
 
As illustrated in Figure 1, Chinese word 
segmentation with new words detection is a 
recursive process.  The process is given as 
follows: 
1. Making Chinese word segmentation with 
domain lexicon beyond general lexicon. 
2. Frequent string (over twice) finding with 
postfix tree algorithm, and taking them as 
new words candidate. 
3. Access Variety statistics [Haodi Feng etc. 
2004], and language modeling on word 
formation. [Hemin, 2006] 
4. Exporting new words to domain lexicon. 
5. Recursively, until no more new word 
detected. 
6. Output final word sequence. 
 
 
 
3.2 The process of new words detection 
Simple word segmentation is the first step of 
processing of Chinese language when we deal with 
a very long Chinese article. The method of word 
segmentation is based on HHMM, and Zhang and 
Liu (2003) have given detailed explanation about 
this. 
During the process of word segmentation in 
the first, the system records the words which occur 
frequently. We can set a threshold value of words? 
occurrence frequency. As long as the word 
occurrence frequency reaches this value, this word 
could be recorded in the system as frequent string.  
With the frequent strings detected, we can do 
the further analysis. For every frequent string, we 
check its left and right adjacent one in the original 
text segmented respectively. Through this step, we 
find the adjacent words which occur next to some 
frequent string detected. If the adjacent word also 
occurs very frequently, or even it occurs at the left 
or right of the frequent string every time, it?s great 
possibility that the string detected and the adjacent 
word could merge into one word.  
With the detection in above steps, we gain 
new words from Chinese texts. Then we import 
these new words into domain lexicon and our 
lexicon is updated. With the lexicon containing 
new words, we can do the next cycle recursively 
and revise continually. 
Then, we can see this is a recursive structure. 
Through the continued process of word 
segmentation and new words detection, the state of 
segmentation tends to be steady. The condition of 
steady state has several kinds such as no more new 
words detected or the latest result equal to the 
previous one. At this time, we can break the 
recursion and output the final result. 
This is an example. This sentence is from 
Text D in finance domain 
???????????????????
???????????? (?The financial market 
has been stable and the stock has rebounded in less 
than one year time after Lehman Brother 
Corporation went bankrupt.?) 
After word segmentation with original 
lexicon, this altered sentence is: 
??/?/??/??/??/?/?/?/?/?/?? 
??/??/??/?/??/?/?/??/?/? 
?????? is a new word as a organization 
name and it is hard to be collected. Like this kind 
of word, there are difficulties to add new words to 
update the lexicon in time. So it is normal to 
segment this word ?????? into three words. 
Through frequent string detection, we gain these 
three words ???, ???and ????. With the 
adjacent analysis, we find the word ??? occurs 6 
times, ??? 3 times and ???? 3 times.  
The character ??? occurs 3 times in the 
detected word ????? and 3 times at the left of 
the word ???. So we can consider the word ??
?? as a whole word. 
Then we can easily find the words ???? are 
always at the right of words ????. So it?s 
necessary to consider ?????? as a whole 
word.  
4  Evaluation 
 The performance of word segmentation is 
measured by test precision (P), test recall (R), F 
score (which is defined as 2PR/(P+R)) and the 
OOV recall rate. 
 In this competition, our test corpus involved 
literature, computer, medicine and Finance, totally 
425KB. We take 6 months data of The People's 
Daily to be the training corpus. From Table 1, we 
can see the official evaluation result. 
 Table 1. Official evaluation result 
 Our system got high Precision Rate and 
Recall Rate after testing the texts in four domains, 
especially Recall Rate is all over 95%. And we 
also could see that this system detected most new 
words through several measures of OOV, 
especially IV RR is all over 97.5%. This proved 
that the system could be able to get a nice result 
through processing professional articles in 
literature, computer, medicine and finance domains, 
and we believed it also could do well in other 
domains. This also proved that the method of new 
words detection with Chinese word segmentation 
was competitive. 
 
5Conclusion 
Through this competition, we?ve found a lot 
of problems needed to be solved in Chinese word 
segmentation and tried our best to improve the 
system. Finally, we proposed the method of new 
words detection in Chinese word segmentation. 
But we still had some shortage during the 
evaluation and need to improve in the future. 
 
References 
Lawrence. R.Rabiner.1989. A Tutorial on Hidden 
Markov Models and Selected Applications in Speech 
Recognition. Proceedings of IEEE 77(2): pp.257-286. 
Hua-Ping Zhang, Qun Liu. Model of Chinese Words 
Rough Segmentation Based on N-Shortest-Paths 
Method. Journal of Chinese information processing, 
2002,16(5):1-7 (in Chinese) 
ZHANG Hua-Ping, LIU Qun, Zhang Hao and Cheng 
Xue-Qi. 2002. Automatic Recognition of Chinese 
Unknown Words Recognition. Proc. of First SigHan 
attached on COLING 2002  
ZHANG Hua-Ping, LIU Qun, YU Hong-Kui, CHENG 
Xue-Qi, BAI Shuo.  Chinese Named Entity 
Recognition Using Role Model. International Journal of 
Computational Linguistics and Chinese language 
processing, 2003,Vol. 8 (2) 
Mao-yuan Zhang, Zheng-ding Lu, Chun-yan Zou. A 
Chinese word segmentation based on language 
situation in processing ambiguous words. Information 
Sciences 162 (2004) 275?285 
Gao, Jianfeng, Andi Wu, Mu Li, Chang-Ning 
Huang,Hongqiao Li, Xinsong Xia, and Haowei Qin. 
Adaptive Chinese word segmentation. ACL2004. July 
21-26. 
Haodi Feng, Kang Chen, Xiaotie Deng, Weimin Zheng 
Accessor Variety Criteria for Chinese Word 
Extraction, Computational Linguistics March 2004, 
Vol. 30, No. 1: 75?93. 
Hemin, Web-Oriented Chinese Meaningful String 
Mining, M.Sc Thesis of Graduate University of 
Chinese Academy of Scienses. 2006 
  R P F1 
OOV 
R 
OOV 
RR 
IV 
RR 
A-Literature 0.965 0.94 0.952 0.069 0.814 0.976 
B-Computer 0.951 0.926 0.938 0.152 0.775 0.982 
C-Medicine 0.953 0.913 0.933 0.11 0.704 0.984 
D-Finance 0.963 0.938 0.95 0.087 0.758 0.982 
Chinese Personal Name Disambiguation Based on Person Modeling 
Hua-Ping ZHANG1   Zhi-Hua LIU2   Qian MO3  He-Yan HUANG1 
1
 Beijing Institute of Technology, Beijing, P.R.C 100081 
2
 North China University of Technology, P.R.C 100041 
3
 Beijing Technology and Business University, Beijing, P.R.C 100048 
Email: kevinzhang@bit.edu.cn 
 
 
 
Abstract 
This document presents the bakeoff re-
sults of Chinese personal name in the 
First CIPS-SIGHAN Joint Conference 
on Chinese Language Processing. The 
authors introduce the frame of person 
disambiguation system LJPD, which 
uses a new person model. LJPD was 
built in short time, and it is not given 
enough training and adjustment. Evalua-
tion on LJPD shows that the precision is 
competitive, but the recall is very low. It 
has more space for further improvement. 
1 Introduction 
We participated in the First CIPS-SIGHAN Joint 
Conference on Chinese Language Processing. 
And have taken task 3: Chinese Personal Name 
disambiguation. 
Chinese personal name disambiguation in-
cludes two stages: words are segmented to rec-
ognize Chinese personal name, and documents 
are clustered to disambiguate different person 
with the same personal name.  
In our system, it involves the following 
steps: 
1) Segmenting words and tagging the part-of-
speech, and then recognizing Chinese personal 
name using ICTCLAS 2010 system1. 
2) Extracting personal feature to create the per-
son attribution model on each document. 
3) Generating initial clusters according to fea-
tures in person model, and then clustering the 
initial clusters until the stop criteria is reached. 
The processing flow is illustrated in figure 1. 
                                                 
1
 It can be downloaded from 
http://hi.baidu.com/drkevinzhang 
 
 
Figure 1 Step of Person Disambiguation 
 
As illustrated in figure 1, the whole system 
addresses four problems: personal name recogni-
tion, anaphora resolution of personal name, per-
son model creation and clustering. 
 
2 Personal Name Recognition 
Chinese personal name recognition is more dif-
ficult than English. Such difficulties usually 
combine with Chinese word segmentation. The 
set of Chinese personal name is infinite, and the 
rule of name construction is varied. Chinese per-
sonal name is often made up of a usual word, 
and has ambiguity with its context. 
To solve the difficulties mentioned above, 
Chinese personal name recognition based on role 
tagging is given in [Zhang etc., 2002]. The ap-
proach is: tokens after segmentation are tagged 
using Viterbi algorithm with different roles ac-
cording to their functions in the generation of 
Chinese personal name; the possible names are 
recognized after maximum pattern matching on 
the roles sequence [ZHANG, etc., 2002]. With 
this approach, the precision of ICTCLAS 
reaches 95.57% and the recall is 95.23% in an 
opening corpus which contains 1,108,049 words. 
In the corpus, the count of the personal name is 
15,888. And ICTCLAS is a Chinese lexical 
analysis system witch combines part-of-speech 
tagging, word segmentation, unknown words 
recognition. It can meet our requirements, so 
ICTCLAS provides personal name recognition 
in our system. 
3 Anaphora Resolution of Personal 
Name 
Anaphora is very common in natural language. 
Resolve this problem can help us get more in-
formation of the person from a document. 
Anaphora resolution of personal name is an 
important part of anaphora resolution. At present, 
much advancement in anaphora resolution have 
occurred [Saliha 1998]. Anaphora resolution of 
personal pronouns is an especially complicate 
problem in anaphora resolution of personal name. 
In our system, we don?t process this problem. 
The reason is that anaphora resolution of per-
sonal name will take side effect to personal 
name disambiguation unless its precision is defi-
nitely high. So we just process the anaphora of 
the first name or the second name. For example, 
?Jianmin Wang? in above context and ?Profes-
sor Wang? will be resolved in our system. 
4 Personal Model 
We propose a person model to represent the 
person in the document: 
Person = {N, P, Q, R} 
where: 
N is the collection of appellation of person, 
such as name, nickname, alias, and so on 
P is the collection of the basic attributes of 
person 
Q is the collection of the other attributes of 
person 
R is the collection of the terms co-
occurrence with person name, witch is called 
term field 
In the system, we focused on seven attrib-
utes such as sex, nationality, birthday, native 
place, address, profession, family members and 
personal name, co-occurrence terms. In these 
features, name?N, {sex, nationality, birthday, 
native place}?P, {address, profession, family 
members}?Q, {co-occurrence term}?R. Table 
1 is the examples of person model. 
In view of the co-occurrence personal name 
is especially important for person disambigua-
tion. We separate it as another field in R. 
4.1  Attributes Feature 
The components N, P and Q of person model are 
attributes feature. The dimension of these fea-
tures for a person is different. For example, the 
sex of a person is constant in life, while his or 
her address may be different in different time. 
Take DOM to represent the dimension of the 
attributes features. Then: 
 DOM(Ni) = 1; (1?i?n) 
 DOM(Pi) = 1; (1?i?k) 
 DOM(Qi) ? 1; (1?i?m) 
For a person, N and P are constant in life. If 
one attribute of N or P between two persons is 
different, they are not the same person. 
To get the attributes feature, we have three 
steps: First, segment word and tag part-of-speech 
for the input document. Second, we identify the 
triggering word which is defined as attributes 
value and the Max-Noun Phrase. The triggering 
words are identified by their POS and a hand-
built triggering word thesaurus.  At last, a classi-
fier determines the attribute belongs to the left 
personal name or the right to the attribute. The 
classifier is trained by the corpus which is hand-
tagged documents from internet.
 
Figure 2 Step of Person Attributes Extraction 
4.2 Term Field 
In person model, R is the collection of the terms 
co-occurrence within person. We adopt Vector 
Space Model to represent these terms. The i-th 
term is represented by ti, and its weight is repre-
sented by wi, and the weight shows the impor-
tance of the term for the person.  
R = (t1, w1; t2, w2; ? ; tH, wH) 
To get the person?s term field, we identify 
a scope witch these terms occurred. We con-
sider three kinds of scope for term field: the 
total document, the paragraph where the per-
sonal name is present, sentence where the per-
sonal name is present. And then segment words 
and tag part-of-speech for these fragments. Next, 
filter out the attribute terms and filter by part-
of-speech and leave only nouns, verb, adjective, 
adverb and name entry. Third, we make a stop 
word list, and filter out these stop terms. Last, 
according to the term?s DF, filter out high fre-
quency and low frequency terms, and only the 
terms witch DF is not lower than 2 and not 
higher than N/3(N is the total count of docu-
ments) are left. 
In collection R, we have separated term 
field to co-occurrence personal name vector and 
co-occurrence common term vector. Because 
the two vectors have different affect to person 
disambiguation. This difference manifests in the 
different method to compute these weight. The 
common term?s weight is computed by tf-idf 
algorithm: 
)1/log()1),(log(),( +?+= tnNdttfdtw  
where: 
),( dtw  is the weight of term t in document 
d  
),( dttf  is the frequency of occurrence of t 
in d  
N is the total count of documents 
nt is the count of documents which contain 
term t 
 sex nationality birthday Native place address 
Family 
members profession 
Co-occurrence 
personal name 
Co-occurrence 
terms field 
Name1 ? ? 1949  ??  ?? ? ?? 
Name2 ?   ??  ?? ?? ? ?? 
Name3 ? ?   ??  ?? ? ?? 
Table 1 Examples of Person Model
 
The co-occurrence personal name?s weight is 
computed below: 
)1/'log()1),(log(),( +?+= namenNpnamenfpnamew  
where: 
),( pnamew  is the weight of co-occurrence 
name name  
),( pnamenf  is the frequency of co-
occurrence of name  and person p  
name  is the count of the co-occurrence of 
name  and the other personal name 
The similarity of term field between two persons 
is calculated by the angle cosine: 
? ?
?
==
i i
ii
i
ii
yx
yx
YXYXSim
22
*
*
),cos(),(  
5 Clustering 
Person model ?Person = {N, P, Q, R}? is multi-
dimensional. First, we adopted two rules to gen-
erate original clusters: 
Rule 1: For two persons whose name is same, if 
one of the birthday (accurate to month) or rela-
tive is matched, these two persons are the same 
person. 
Rule 2: For two persons whose name is same, if 
one of the sex, nationality, native place or birth-
day is not matched, these two persons are differ-
ent. 
There are profession, co-occurrence per-
sonal name and co-occurrence common terms 
left. For two persons whose name is same, we 
apply rule 1 and 2 first. If both of the two rules 
are not activating, compute the similarity Simposi-
tion(X, Y), cosname(X, Y), costerm(X, Y). And then 
synthesize these three similarities. 
Assume the three factors profession, co-
occurrence personal name and co-occurrence 
common terms are independent, and adopt Stan-
ford certainty theory to synthesize the three 
similarities. The Stanford certainty theory cre-
ates confidence measures and some simple rules 
for combing these confidences. Assume E1, E2, 
E2 are the Stanford certainty factors of event B, 
and CF represent the confidence, then the confi-
dence of event B is : 
)3()2()1()3()2()3(
)1()2()1()3()2()1()(
ECFECFECFECFECFECF
ECFECFECFECFECFECFBCF
??+??
????++=
 
For example, if the confidence of the three 
factors for event B is respectively: 88%, 74%, 
66%, then the confidence for event B is 88??
74??66??88??74??88??66??76??
66??88??74??66??98.93?. 
To compute the confidence of the factors, 
we should get the threshold (represented by ui) 
of the similarity for factors. If the similarity of 
the factor reaches the threshold, its confidence is 
100%: 
i
E
i u
sim
ECF i=)(  ]1,0[)( ?iECF  
The training method is: clustering training 
data according to the single factor, select the 
threshold with which the recall is higher with the 
premise that the precision is not lower than 98%. 
We get three thresholds 3, 0.5, 0.25 respectively 
for factor profession, co-occurrence personal 
name and co-occurrence common terms.  
Overall, the algorithm takes two steps: 
1) Adopt rule 1 and 2 to group the persons to 
the original clusters 
2) Adopt agglomerative hierarchical cluster-
ing algorithm to clustering these original 
clusters. 
(1) Take each original cluster as a single 
cluster 
(2) Select two clusters which are most 
likelihood and merge to one cluster 
(3) If there is only one cluster or reaches 
stop criteria, exit. Else, go to step (2). 
In the process of merging the clusters, we 
should merge the fragment of person. For term 
field vector, we simply compute the average of 
the term weights. For attribute feature, we adopt 
rule method to merge two clusters. 
6 Task 
We would introduce the operation of some dif-
ferent track in this section. 
In formal test, we first get a query name 
and its all files. Then we segment these files and 
extract the related information of our person 
model and output to files. At last, we cluster 
these person models and output to result xml. 
In the diagnosis test, the basic process is 
same to the formal test. The difference is that the 
element of clustering is changed to the subfolder 
of a real name. When all the subfolders are clus-
tered for a query name, we merge the results to 
one xml file. 
 
B-Cubed P-IP  
precision recall F score P IP F score 
Formal test 80.2 68.75 68.4 86.12 76.37 77.54 
Diagnosis test 94.62 63.32 72.48 96.44 72.78 80.85 
Table 2 Evaluation result of Personal Disambiguation 
7 Conclusion 
Through the first bakeoff, we have learned 
much about the development in Chinese per-
sonal name recognition and person disambigua-
tion. At the same time, we really find our prob-
lems during the evaluation. The bakeoff is inter-
esting and helpful. We look forward to partici-
pate in forthcoming bakeoff. 
References 
 
ZHANG Hua-Ping, LIU Qun, YU Hong-Kui, 
CHENG Xue-Qi, BAI Shuo.  Chinese Named En-
tity Recognition Using Role Model. International 
Journal of Computational Linguistics and Chinese 
language processing, 2003,Vol. 8 (2) 
Azzam Saliha, Kevin Humphreys & Robert Gai-
zauskas. Coreference resolution in a multilingual 
information extraction. In the Proc. of the Work-
shop on Linguistic Coference. Granada, 
Spain.1998. 
Yu Manquan.  Research on Knowledge Mining in 
Person Tracking.  Ph.D.Thesis of GUCAS. 2006 
