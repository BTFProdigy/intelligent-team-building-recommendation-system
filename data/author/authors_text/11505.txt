Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 226?229,
Paris, October 2009. c?2009 Association for Computational Linguistics
Using Treebanking Discriminants as Parse Disambiguation Features
Md. Faisal Mahbub Chowdhury? and Yi Zhang? and Valia Kordoni?
? Dept of Computational Linguistics, Saarland University
? Dept of Computational Linguistics, Saarland University and DFKI GmbH, Germany
{chowd,yzhang,kordoni}@coli.uni-sb.de
Abstract
This paper presents a novel approach of in-
corporating fine-grained treebanking deci-
sions made by human annotators as dis-
criminative features for automatic parse
disambiguation. To our best knowledge,
this is the first work that exploits treebank-
ing decisions for this task. The advan-
tage of this approach is that use of human
judgements is made. The paper presents
comparative analyses of the performance
of discriminative models built using tree-
banking decisions and state-of-the-art fea-
tures. We also highlight how differently
these features scale when these models are
tested on out-of-domain data. We show
that, features extracted using treebanking
decisions are more efficient, informative
and robust compared to traditional fea-
tures.
1 Introduction
State-of-the-art parse disambiguation models are
trained on treebanks, which are either fully hand-
annotated or manually disambiguated from the
parse forest produced by the parser. While most
of the hand-annotated treebanks contain only gold
trees, treebanks constructed from parser outputs
include both preferred and non-preferred analy-
ses. Some treebanking environments (such as
the SRI Cambridge TreeBanker (Carter, 1997) or
[incr tsdb()] (Oepen, 2001)) even record
the treebanking decisions (see section 2) that the
annotators take during manual annotation. These
treebanking decisions are, usually, stored in the
database/log files and used later for dynamic prop-
agation if a newer version of the grammar on the
same corpus is available (Oepen et al, 2002). But
until now, to our best knowledge, no research has
been reported on exploiting these decisions for
building a parse disambiguation model.
Previous research has adopted two approaches
to use treebanks for disambiguation models. One
approach, known as generative, uses only the gold
parse trees (Ersan and Charniak, 1995; Charniak,
2000). The other approach, known as discrimi-
native, uses both preferred trees and non-preferred
trees (Johnson et al, 1999; Toutanova et al, 2005).
In this latter approach, features such as local con-
figurations (i.e., local sub-trees), grandparents, n-
grams, etc., are extracted from all the trees and
are utilized to build the model. Neither of the ap-
proaches considers cognitive aspects of treebank-
ing, i.e. the fine-grained decision-making process
of the human annotators.
In this paper, we present our ongoing study of
using treebanking decisions for building a parse
disambiguation model. We present comparative
analyses among the features extracted using tree-
banking decisions and the state-of-the-art feature
types. We highlight how differently these features
scale when they are tested on out-of-domain data.
Our results demonstrate that features extracted us-
ing treebanking decisions are more efficient, in-
formative and robust, despite the total number of
these features being much less than that of the tra-
ditional feature types.
The rest of this paper is organised as follows
? section 2 presents some motivation along with
definition of treebanking decisions. Section 3 de-
scribes the feature extraction templates that have
been used for treebanking decisions. Section 4 ex-
plains the experimental data, results and analyses.
Section 5 concludes the paper with an outline of
our future research.
2 Treebanking decisions
One of the defining characteristics of Redwoods-
style treebanks1 (Oepen et al, 2002) is that the
candidate trees are constructed automatically by
1More details available in http://redwoods.stanford.edu.
226
D1 SUBJH the dog || barks
D2 HSPEC the || dog barks
D3 FRAG_NP the dog barks
D4 HSPEC the || dog
D5 NOUN_N_CMPND dog || barks
. . . . . .
D6 PLUR_NOUN_ORULE barks
D7 v_-_le barks
D8 n_-_mc_le barks
Figure 1: Example forest and discriminants
the grammar, and then manually disambiguated by
human annotators. In doing so, linguistically rich
annotation is built efficiently with minimum man-
ual labor. In order to further improve the manual
disambiguation efficiency, systems like [incr
tsdb()] computes the difference between can-
didate analyses. Instead of looking at the huge
parse forest, the treebank annotators select or re-
ject the features that distinguish between different
parses, until only one parse remains. The number
of decisions for each sentence is normally around
log2(n) where n is the total number of candidate
trees. For a sentence with 5000 candidate read-
ings, only about 12 treebanking decisions are re-
quired for a complete disambiguation. A similar
method was also proposed in (Carter, 1997).
Formally, a feature that distinguishes between
different parses is called a discriminant. For
Redwoods-style treebanks, this is usually ex-
tracted from the syntactic derivation tree of the
Head-driven Phrase Structure Grammar (HPSG)
analyses. Figure 1 shows a set of example dis-
criminants based on the two candidate trees.
A choice (acceptance or rejection, either manu-
ally annotated or inferred by the system) made on
a discriminant is called a decision. In the above
example, suppose the annotator decides to accept
the binary structure the dog || barks as a subject-
head construction and assigns a value yes to dis-
criminant D1, the remaining discriminants will
also receive inferred values by deduction (no for
D2, no for D3, yes for D4, etc). These decisions
are stored and used for dynamic evolution of the
treebank along with the grammar development.
Treebank decisions (especially those made by
annotators) are of particular interest to our study
of parse disambiguation. The decisions record the
fine-grained human judgements in the manual dis-
ambiguation process. This is different from the
traditional use of treebanks to build parse selec-
tion models, where a marked gold tree is picked
from the parse forest without concerning detailed
selection steps. Recent study on double annotated
treebanks (Kordoni and Zhang, 2009) shows that
annotators tend to start with the decisions with the
most certainty, and delay the ?hard? decisions as
much as possible. As the decision process goes,
many of the ?hard? discriminants will receive an
inferred value from the certain decisions. This
greedy approach helps to guarantee high inter-
annotator agreement. Concerning the statistical
parse selection models, the discriminative nature
of these treebanking decisions suggests that they
are highly effective features, and if properly used,
they will contribute to an efficient disambiguation
model.
3 Treebanking Decisions as
Discriminative Disambiguation
Features
We use three types of feature templates for tree-
banking decisions for feature extraction. We refer
to the features extracted using these templates as
TDF (Treebanking Decision Feature) in the rest of
this paper. The feature templates are
T1: discriminant + lexical types of the yield
T2: discriminant + rule(left-child)2 + rule(right-child)
T3: instances of T2 + rule(parent) + rule(siblings)
TDFs of T1, T2 and T3 in combination are re-
ferred to as TDFC or TDFs with context. For
example in Figure 1, instance of T1 for the
discriminant D4 is ?HSPEC3 + le_type(the)4 +
le_type(dog)"; instance of T2 is ?HSPEC + rule(
DET) + rule(N) "; and instance of T3 is ?HSPEC +
rule(DET ) + rule(N) + rule(S) + rule(VP)".
A TDF represents partial information about the
right parse tree (as most usual features). But in
some way, it also indicates that it was a point of
a decision (point of ambiguity with respect to the
underlying pre-processing grammar), hence carry-
ing some extra bit of information. TDFs allow to
2rule(X) represents the HPSG rule, applied on X, ex-
tracted from the corresponding derivation tree.
3HSPEC is the head-specifier rule in HPSG
4le_type(X) denotes the abstract lexical type of word X
inside the grammar.
227
omit certain details inside the features by encod-
ing useful purposes of relationships between lexi-
cal types of the words and their distant grandpar-
ents without considering nodes in the intermediate
levels (allowing some kind of underspecification).
In contrast, state-of-the-art feature types contain
all the nodes in the corresponding branches of
the tree. While they encode ancestor information
(through grandparenting), but they ignore siblings.
TDFs include siblings along with ancestor. Unlike
traditional features, which are generated from all
possible matches (which is huge) of feature types
followed by some frequency cut-offs, the selection
of TDFs is directly restricted by the small num-
ber of treebanking decisions themselves and ex-
haustive search is not needed. It should be noted
that, we do not use treebanking decisions made for
the parse forest of one sentence to extract features
from the parse forest of another sentence. That is
why, the number of TDFs is much smaller than
that of traditional features. This also ensures that
TDFs are highly correlated to the corresponding
constructions and corresponding sentences from
where they are extracted.
4 Experiment
4.1 Data
We use a collection of 8593 English sentences
from the LOGON corpus (Oepen et al, 2004) for
our experiment. 874 of them are kept as test items
and the remaining 7719 items are used for train-
ing. The sentences have an average length of 14.68
and average number of 203.26 readings per sen-
tence. The out-of-domain data are a set of 531
English Wikipedia sentences from WeScience cor-
pus (Ytrest?l et al, 2009).
Previous studies (Toutanova et al, 2005; Os-
borne and Baldridge, 2004) have reported rela-
tively high exact match accuracy with earlier ver-
sions of ERG (Flickinger, 2000) on datasets with
very short sentences. With much higher structural
ambiguities in LOGON and WeScience sentences,
the overall disambiguation accuracy drops signifi-
cantly.
4.2 Experimental setup and evaluation
measures
The goal of our experiments is to compare var-
ious types of features (with TDF) in terms of
efficiency, informativeness, and robustness. To
compare among the feature types, we build log-
linear training models (Johnson et al, 1999) for
parse selection (which is standard for unification-
based grammars) for TDFC, local configurations,
n-grams and active edges5. For each model, we
calculate the following evaluation metrics ?
? Exact (match) accuracy: it is simply the percentage
of times that the top-ranked analysis for each test sen-
tences is identical with the gold analysis of the same
sentence.
? 5-best (match) accuracy: it is the percentage of times
that the five top-ranked analyses for each of the sen-
tences contain the gold analysis.
? Feature Hit Count (FHC): it is the total number of oc-
currences of the features (of a particular feature type)
inside all the syntactic analyses for all the test sen-
tences. So, for example, if a feature (of a particular
feature type) is observed 100 times, then these 100 oc-
currences are added to the total FHC.
? Feature Type Hit Count (FTHC): it is the total num-
ber of distinct features (of the corresponding feature
type) observed inside the syntactic analyses of all the
test sentences.
While exact and 5-best match measures show
relative informativeness and robustness of the fea-
ture types, FHC and FTHC provide a more com-
prehensive picture of relative efficiencies.
4.3 Results and discussion
As we can see in Table 1, local configurations
achieve highest accuracy among the traditional
feature types. They also use higher number of fea-
tures (almost 2.7 millions). TDFC do better than
both n-grams and active edges, even with a lower
number of features. Though, local configurations
gain more accuracy than TDFC, but they do so at
a cost of 50 times higher number of features. This
indicates that features extracted using treebanking
decisions are more informative.
For out-of-domain data (Table 1), there is a big
drop of accuracy for local configurations. Active
edges and TDFC also have some accuracy drop.
Surprisingly, n-grams do better with our out-of-
domain data than in-domain, but still that accuracy
is close to that of TDFC. Note that n-grams have
8 times higher number of features than TDFC.
Hence, according to these results, TDFC are more
robust, for out-of-domain data, than local config-
urations and active edges, and almost as good as
n-grams.
5Active edges correspond to the branches (i.e. one daugh-
ter in turn) of the local sub-trees.
228
Feature Total 5-best accuracy 5-best accuracy Exact accuracy Exact accuracy
template features (in-domain) (out-of-domain) (in-domain) (out-of-domain)
n-gram 438,844 68.19% 62.71% 41.30% 42.37%
local configuration 2,735,486 75.51% 64.22% 50.69% 44.44%
active edges 89,807 68.99% 61.77% 41.88% 39.92%
TDFC 53,362 70.94% 62.71% 43.59% 41.05%
Table 1: Accuracies obtained on both in-domain and out-of-domain data using n-grams (n=4), local
configurations (with grandparenting level 3), active edges and TDFC.
Feature FHC FTHC Active
template features
n-gram 18,245,558 32,425 7.39%
local config. 62,060,610 357,150 13.06%
active edges 22,902,404 27,540 30.67%
TDFC 21,719,698 17,818 33.39%
Table 2: FHC and FTHC calculated for in-domain
data.
The most important aspect of TDFC is that they
are more efficient than their traditional counter-
parts (Table 2). They have significantly higher
number of active features ( FTHCTotalFeature# ) than n-grams and local configurations.
5 Future work
The results of the experiments described in this pa-
per indicate a good prospect for utilizing treebank-
ing decisions, although, we think that the types of
feature templates that we are using for them are
not yet fully conveying cognitive knowledge of the
annotators, in which we are specifically interested
in. For instance, we expect to model human dis-
ambiguation process more accurately by focusing
only on human annotators? decisions (instead of
only inferred decisions). Such a model will not
only improve the performance of the parsing sys-
tem at hand, but can also be applied interactively
in treebanking projects to achieve better annota-
tion speed (e.g., by ranking the promising discrim-
inants higher to help annotators make correct de-
cisions). Future experiments will also investigate
whether any pattern of discriminant selection by
the humans can be learnt from these decisions.
References
David Carter. 1997. The treebanker: A tool for supervised
training of parsed corpora. In Proceedings of the Work-
shop on Computational Environments for Grammar De-
velopment and Linguistic Engineering, Madrid, Spain.
Eugene Charniak. 2000. A maximum entropy-based parser.
In Proceedings of the 1st Annual Meeting of the North
American Chapter of Association for Computational Lin-
guistics (NAACL 2000), pages 132?139, Seattle, USA.
Murat Ersan and Eugene Charniak. 1995. A statistical syn-
tactic disambiguation program and what it learns. pages
146?159.
Dan Flickinger. 2000. On building a more efficient grammar
by exploiting types. 6(1):15?28.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
unifcation-based grammars. In Proceedings of the 37th
Annual Meeting of the Association for Computational Lin-
guistics (ACL 1999), pages 535?541, Maryland, USA.
Valia Kordoni and Yi Zhang. 2009. Annotating wall street
journal texts using a hand-crafted deep linguistic gram-
mar. In Proceedings of The Third Linguistic Annotation
Workshop (LAW III), Singapore.
Stephan Oepen, Kristina Toutanova, Stuart Shieber, Christo-
pher Manning, Dan Flickinger, and Thorsten Brants.
2002. The LinGO Redwoods treebank: motivation and
preliminary applications. In Proceedings of COLING
2002: The 17th International Conference on Computa-
tional Linguistics: Project Notes, Taipei, Taiwan.
Stephan Oepen, Helge Dyvik, Jan Tore L?nning, Erik Vell-
dal, Dorothee Beermann, John Carroll, Dan Flickinger,
Lars Hellan, Janne Bondi Johannessen, Paul Meurer, Tor-
bj?rn Nordg?rd, and Victoria Ros?n. 2004. Som ? kapp-
ete med trollet? towards mrs-based norwegian-english
machine translation. In Proceedings of the 10th Interna-
tional Conference on Theoretical and Methodological Is-
sues in Machine Translation, pages 11?20, MD, USA.
Stephan Oepen. 2001. [incr tsdb()] ? competence and
performance laboratory. User manual. Technical report,
Computational Linguistics, Saarland University, Saar-
br?cken, Germany.
Miles Osborne and Jason Baldridge. 2004. Ensemble-based
active learning for parse selection. In HLT-NAACL 2004:
Main Proceedings, pages 89?96, Boston, USA.
Kristina Toutanova, Christoper D. Manning, Dan Flickinger,
and Stephan Oepen. 2005. Stochastic HPSG parse selec-
tion using the Redwoods corpus. Journal of Research on
Language and Computation, 3(1):83?105.
Gisle Ytrest?l, Stephan Oepen, and Daniel Flickinger. 2009.
Extracting and annotating wikipedia sub-domains. In Pro-
ceedings of the 7th International Workshop on Treebanks
and Linguistic Theories, pages 185?197, Groningen, the
Netherlands.
229
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 420?429,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Combining Tree Structures, Flat Features and Patterns
for Biomedical Relation Extraction
Md. Faisal Mahbub Chowdhury ? ? and Alberto Lavelli ?
? Fondazione Bruno Kessler (FBK-irst), Italy
? University of Trento, Italy
{chowdhury,lavelli}@fbk.eu
Abstract
Kernel based methods dominate the current
trend for various relation extraction tasks
including protein-protein interaction (PPI)
extraction. PPI information is critical in un-
derstanding biological processes. Despite
considerable efforts, previously reported
PPI extraction results show that none of the
approaches already known in the literature
is consistently better than other approaches
when evaluated on different benchmark PPI
corpora. In this paper, we propose a
novel hybrid kernel that combines (auto-
matically collected) dependency patterns,
trigger words, negative cues, walk fea-
tures and regular expression patterns along
with tree kernel and shallow linguistic ker-
nel. The proposed kernel outperforms the
exiting state-of-the-art approaches on the
BioInfer corpus, the largest PPI benchmark
corpus available. On the other four smaller
benchmark corpora, it performs either bet-
ter or almost as good as the existing ap-
proaches. Moreover, empirical results show
that the proposed hybrid kernel attains con-
siderably higher precision than the existing
approaches, which indicates its capability
of learning more accurate models. This also
demonstrates that the different types of in-
formation that we use are able to comple-
ment each other for relation extraction.
1 Introduction
Kernel methods are considered the most effective
techniques for various relation extraction (RE)
tasks on both general (e.g. newspaper text) and
specialized (e.g. biomedical text) domains. In
particular, as the importance of syntactic struc-
tures for deriving the relationships between en-
tities in text has been growing, several graph
and tree kernels have been designed and experi-
mented.
Early RE approaches more or less fall in one of
the following categories: (i) exploitation of statis-
tics about co-occurrences of entities, (ii) usage of
patterns and rules, and (iii) usage of flat features
to train machine learning (ML) classifiers. These
approaches have been studied for a long period
and have their own pros and cons. Exploitation
of co-occurrence statistics results in high recall
but low precision, while rule or pattern based ap-
proaches can increase precision but suffer from
low recall. Flat feature based ML approaches em-
ploy various kinds of linguistic, syntactic or con-
textual information and integrate them into the
feature space. They obtain relatively good results
but are hindered by drawbacks of limited feature
space and excessive feature engineering. Kernel
based approaches have become an attractive alter-
native solution, as they can exploit huge amount
of features without an explicit representation.
In this paper, we propose a new hybrid kernel
for RE. We apply the kernel to Protein?protein
interaction (PPI) extraction, the most widely re-
searched topic in biomedical relation extraction.
PPI1 information is very critical in understanding
biological processes. Considerable progress has
been made for this task. Nevertheless, empirical
results of previous studies show that none of the
approaches already known in the literature is con-
sistently better than other approaches when evalu-
ated on different benchmark PPI corpora (see Ta-
ble 4). This demands further study and innovation
1PPIs occur when two or more proteins bind together,
and are integral to virtually all cellular processes, such as
metabolism, signalling, regulation, and proliferation (Tikk
et al 2010).
420
of new approaches that are sensitive to the varia-
tions of complex linguistic constructions.
The proposed hybrid kernel is the composition
of one tree kernel and two feature based kernels
(one of them is already known in the literature
and the other is proposed in this paper for the first
time). The novelty of the newly proposed feature
based kernel is that it envisages to accommodate
the advantages of pattern based approaches. More
precisely:
1. We propose a new feature based kernel (de-
tails in Section 4.1) by using syntactic de-
pendency patterns, trigger words, negative
cues, regular expression (henceforth, regex)
patterns and walk features (i.e. e-walks and
v-walks)2.
2. The syntactic dependency patterns are au-
tomatically collected from a type of depen-
dency subgraph (we call it reduced graph,
more details in Section 4.1.1) during run-
time.
3. We only use the regex patterns, trigger words
and negative cues mentioned in the literature
(Ono et al 2001; Fundel et al 2007; Bui et
al., 2010). The objective is to verify whether
we can exploit knowledge which is already
known and used.
4. We propose a hybrid kernel by combin-
ing the proposed feature based kernel (out-
lined above) with the Shallow Linguistic
(SL) kernel (Giuliano et al 2006) and the
Path-enclosed Tree (PET) kernel (Moschitti,
2004).
The aim of our work is to take advantage of
different types of information (i.e., dependency
patterns, regex patterns, trigger words, negative
cues, syntactic dependencies among words and
constituent parse trees) and their different repre-
sentations (i.e. flat features, tree structures and
graphs) which can complement each other to learn
more accurate models.
2The syntactic dependencies of the words of a sentence
create a dependency graph. A v-walk feature consists of
(wordi ? dependency typei,i+1 ? wordi+1), and an e-
walk feature is composed of (dependency typei?1,i ?
wordi ? dependency typei,i+1). Note that, in a depen-
dency graph, the words are nodes while the dependency
types are edges.
The remainder of the paper is organized as fol-
lows. In Section 2, we briefly review previous
work. Section 3 lists the datasets. Then, in Sec-
tion 4, we define our proposed hybrid kernel and
describe its individual component kernels. Sec-
tion 5 outlines the experimental settings. Follow-
ing that, empirical results are discussed in Section
6. Finally, we conclude with a summary of our
study as well as suggestions for further improve-
ment of our approach.
2 Related Work
In this section, we briefly discuss some of the
recent work on PPI extraction. Several RE ap-
proaches have been reported to date for the PPI
task, most of which are kernel based methods.
Tikk et al(2010) reported a benchmark evalu-
ation of various kernels on PPI extraction. An
interesting finding is that the Shallow Linguis-
tic (SL) kernel (Giuliano et al 2006) (to be dis-
cussed in Section 4.2), despite its simplicity, is on
par with the best kernels in most of the evaluation
settings.
Kim et al(2010) proposed walk-weighted sub-
sequence kernel using e-walks, partial matches,
non-contiguous paths, and different weights for
different sub-structures (which are used to capture
structural similarities during kernel computation).
Miwa et al(2009a) proposed a hybrid kernel,
which combines the all-paths graph (APG) kernel
(Airola et al 2008), the bag-of-words kernel, and
the subset tree kernel (Moschitti, 2006) (applied
on the shortest dependency paths between target
protein pairs). They used multiple parser inputs.
The system is regarded as the current state-of-the-
art PPI extraction system because of its high re-
sults on different PPI corpora (see the results in
Table 4).
As an extension of their work, they boosted sys-
tem performance by training on multiple PPI cor-
pora instead of on a single corpus and adopting
a corpus weighting concept with support vector
machine (SVM) which they call SVM-CW (Miwa
et al 2009b). Since most of their results are re-
ported by training on the combination of multi-
ple corpora, it is not possible to compare them
directly with the results published in the other re-
lated works (that usually adopt 10-fold cross vali-
dation on a single PPI corpus). To be comparable
with the vast majority of the existing work, we
also report results using 10-fold cross validation
421
Corpus Sentences Positive pairs Negative pairs
BioInfer 1,100 2,534 7,132
AIMed 1,955 1,000 4,834
IEPA 486 335 482
HPRD50 145 163 270
LLL 77 164 166
Table 1: Basic statistics of the 5 benchmark PPI cor-
pora.
on single corpora.
Apart from the approaches described above,
there also exist other studies that used kernels for
PPI extraction (e.g. subsequence kernel (Bunescu
and Mooney, 2006)).
A notable exception is the work published by
Bui et al(2010). They proposed an approach that
consists of two phases. In the first phase, their
system categorizes the data into different groups
(i.e. subsets) based on various properties and pat-
terns. Later they classify candidate PPI pairs in-
side each of the groups using SVM trained with
features specific for the corresponding group.
3 Data
There are 5 benchmark corpora for the PPI task
that are frequently used: HPRD50 (Fundel et al
2007), IEPA (Ding et al 2002), LLL (Ne?dellec,
2005), BioInfer (Pyysalo et al 2007) and AIMed
(Bunescu et al 2005). These corpora adopt dif-
ferent PPI annotation formats. For a comparative
evaluation Pyysalo et al(2008) put all of them
in a common format which has become the stan-
dard evaluation format for the PPI task. In our
experiments, we use the versions of the corpora
converted to such format.
Table 1 shows various statistics regarding the 5
(converted) corpora.
4 Proposed Hybrid Kernel
The hybrid kernel that we propose is as follows:
KHybrid (R1, R2) = KTPWF (R1, R2)
+ KSL (R1, R2) + w * KPET (R1, R2)
where KTPWF stands for the new feature
based kernel (henceforth, TPWF kernel) com-
puted using flat features collected by exploiting
patterns, trigger words, negative cues and walk
features. KSL and KPET stand for the Shallow
Linguistic (SL) kernel and the Path-enclosed Tree
(PET) kernel respectively. w is a multiplicative
constant used for the PET kernel. It allows the
hybrid kernel to assign more (or less) weight to
the information obtained using tree structures de-
pending on the corpus. The proposed hybrid ker-
nel is valid according to the closure properties of
kernels.
Both the TPWF and SL kernels are linear ker-
nels, while PET kernel is computed using Unlex-
icalized Partial Tree (uPT) kernel (Severyn and
Moschitti, 2010). The following subsections ex-
plain each of the individual kernels in more detail.
4.1 Proposed TPWF Kernel
4.1.1 Reduced graph, trigger words,
negative cues and dependency patterns
For each of the candidate entity pairs, we
construct a type of subgraph from the depen-
dency graph formed by the syntactic dependen-
cies among the words of a sentence. We call it
?reduced graph? and define it in the follow-
ing way:
A reduced graph is a subgraph
of the dependency graph of a sentence
which includes:
? the two candidate entities and their
governor nodes up to their least
common governor (if exists).
? dependent nodes (if exist) of all the
nodes added in the previous step.
? the immediate governor(s) (if ex-
ists) of the least common governor.
Figure 1 shows an example of a reduced graph.
A reduced graph is an extension of the smallest
common subgraph of the dependency graph that
aims at overcoming its limitations. It is a known
issue that the smallest common subgraph (or sub-
tree) sometimes does not contain cue words. Pre-
viously, Chowdhury et al(2011a) proposed a lin-
guistically motivated extension of the minimal
(i.e. smallest) common subtree (which includes
the candidate entity pairs), known as Mildly Ex-
tended Dependency Tree (MEDT). However, the
rules used for MEDT are too constrained. Our ob-
jective in constructing the reduced graph is to in-
clude any potential modifier(s) or cue word(s) that
describes the relation between the given pair of
entities. Sometimes such modifiers or cue words
are not directly dependent (syntactically) on any
422
BioInfer AIMed IEPA HPRD50 LLL
P R F P R F P R F P R F P R F
Only walk features 51.8 71.2 60.0 48.7 63.2 55.0 61.0 75.2 67.4 60.2 65.0 62.5 64.6 87.8 74.4
Features: dep. patterns, 53.8 68.8 60.4 50.6 63.9 56.5 63.9 74.6 68.9 65.0 71.8 68.2 66.5 89.6 76.4
trigger, neg. cues, walks
Features: dep. patterns, 53.5 68.6 60.1 52.5 62.9 57.2 63.8 74.6 68.8 65.1 69.9 67.5 67.4 88.4 76.5
trigger, neg. cues, walks,
regex patterns
Table 2: Results of the proposed TPWF feature based kernel on 5 benchmark PPI corpora before and after adding
features collected using dependency patterns, regex patterns, trigger words and negative cues to the walk features.
The TPWF kernel is a component of the new hybrid kernel.
Figure 1: Dependency graph for the sentence ?A pVHL mutant containing a P154L substitution does not promote
degradation of HIF1-Alpha? generated by the Stanford parser. The edges with blue dots form the smallest
common subgraph for the candidate entity pair pVHL and HIF1-Alpha, while the edges with red dots form the
reduced graph for the pair.
of the entities (of the candidate pair). Rather they
are dependent on some other word(s) which is de-
pendent on one (or both) of the entities. The word
?not? in Figure 1 is one such example. The re-
duced graph aims to preserve these cue words.
The following types of features are collected
from the reduced graph of a candidate pair:
1. HasTriggerWord: whether the least common
governor(s) of the target entity pairs inside
the reduced graph matches any trigger word.
2. Trigger-X: whether the least common gov-
ernor(s) of the target entity pairs inside the
reduced graph matches the trigger word ?X?.
3. HasNegWord: whether the reduced graph
contains any negative word.
4. DepPattern-i: whether the reduced graph
contains all the syntactic dependencies of the
i-th pattern of dependency pattern list.
The dependency pattern list is automatically
constructed from the training data during the
learning phase. Each pattern is a set of syntactic
dependencies of the corresponding reduced graph
of a (positive or negative) entity pair in the train-
ing data. For example, the dependency pattern for
the reduced graph in Figure 1 is {det, amod, part-
mod, nsubj, aux, neg, dobj, prep of}. The same
dependency pattern might be constructed for mul-
tiple (positive or negative) entity pairs. However,
if it is constructed for both positive and negative
pairs, it has to be discarded from the pattern list.
The dependency patterns allow some kind of
underspecification as they do not contain the lex-
ical items (i.e. words) but contain the likely com-
bination of syntactic dependencies that a given re-
lated pair of entities would pose inside their re-
duced graph.
The list of trigger words contains 144 words
previously used by Bui et al(2010) and Fundel
et al(2007). The list of negative cues contain 18
words, most of which are mentioned in Fundel et
al. (2007).
4.1.2 Walk features
We extract e-walk and v-walk features from
the Mildly Extended Dependency Tree (MEDT)
(Chowdhury et al 2011a) of each candidate pair.
Reduced graphs sometimes include some unin-
423
BioInfer AIMed IEPA HPRD50 LLL
Pos. / Neg. 2,534 / 7,132 1,000 / 4,834 335 / 482 163 / 270 164 / 166
P R F P R F P R F P R F P R F
Proposed TPWF kernel 53.8 68.8 60.4 50.6 63.9 56.5 63.9 74.6 68.9 65.0 71.8 68.2 66.5 89.6 76.4
(without regex)
Proposed TPWF kernel 53.5 68.6 60.1 52.5 62.9 57.2 63.8 74.6 68.8 65.1 69.9 67.5 67.4 88.4 76.5
(with regex)
SL kernel 60.8 65.8 63.2 56.2 64.4 60.0 73.3 71.9 72.6 62.0 65.0 63.5 74.9 85.4 79.8
PET kernel 72.8 74.9 73.9 44.8 72.8 55.5 70.7 77.9 74.2 65.0 73.0 68.8 72.1 89.6 79.9
Proposed hybrid kernel 80.0 71.4 75.5 64.2 58.2 61.1 81.1 69.3 74.7 72.9 59.5 65.5 70.4 95.7 81.1
(PET + SL + TPWF
(without regex))
Proposed hybrid kernel 80.1 72.0 75.9 64.4 58.3 61.2 79.3 69.6 74.1 71.9 61.4 66.2 70.6 95.1 81.0
(PET + SL + TPWF
(with regex))
Table 3: Results of the proposed hybrid kernel and its individual components. Pos. and Neg. refer to number
positive and negative relations respectively. PET refers to the path-enclosed tree kernel, SL refers to the shallow
linguistic kernel, and TPWF refers to the kernel computed using trigger, pattern, negative cue and walk features.
formative words which produce uninformative
walk features. Hence, they are not suitable for
walk feature generation. MEDT suits better for
this purpose. The walk features extracted from
MEDTs have the following properties:
? The directionality of the edges (or nodes) in
an e-walk (or v-walk) is not considered. In
other words, e.g., pos(stimulatory)?amod?
pos(effects) and pos(effects) ? amod ?
pos(stimulatory) are treated as the same fea-
ture.
? The v-walk features are of the form (posi ?
dependency typei,i+1?posi+1). Here, posi is
the POS tag of wordi, i is the governor node
and i + 1 is the dependent node.
? The e-walk features are of the form
(dep. typei?1,i ? posi ? dep. typei,i+1) and
(dep. typei?1,i ? lemmai ? dep. typei,i+1).
Here, lemmai is the lemmatized form of
wordi.
? Usually, the e-walk features are con-
structed using dependency types be-
tween {governor of X, node X} and
{node X, dependent of X}. However,
we also extract e-walk features from
the dependency types between any two
dependents and their common governor
(i.e. {node X, dependent 1 of X} and
{node X, dependent 2 of X}).
Apart from the above types of features, we also
add features for lemmas of the immediate preced-
ing and following words of the candidate entities.
These feature names are augmented with -1 or +1
depending on whether the corresponding words
are preceded or followed by a candidate entity.
4.1.3 Regular expression patterns
We use a set of 22 regex patterns as binary
features. These patterns were previously used
by Ono et al(2001) and Bui et al(2010).
If there is a match for a pattern (e.g. ?En-
tity 1.*activates.*Entity 2? where Entity 1 and
Entity 2 form the candidate entity pair) in a given
sentence, value 1 is added for the feature (i.e., pat-
tern) inside the feature vector.
4.2 Shallow Linguistic (SL) Kernel
The Shallow Linguistic (SL) kernel was proposed
by Giuliano et al(2006). It is one of the best
performing kernels applied on different biomedi-
cal RE tasks such as PPI and DDI (drug-drug in-
teraction) extraction (Tikk et al 2010; Segura-
Bedmar et al 2011; Chowdhury and Lavelli,
2011b; Chowdhury et al 2011c). It is defined
as follows:
KSL (R1, R2) = KLC (R1, R2) + KGC
(R1, R2)
424
BioInfer AIMed IEPA HPRD50 LLL
Pos. / Neg. 2,534 / 7,132 1,000 / 4,834 335 / 482 163 / 270 164 / 166
P R F P R F P R F P R F P R F
SL kernel ? ? ? 60.9 57.2 59.0 ? ? ? ? ? ? ? ? ?
(Giuliano et al 2006)
APG kernel 56.7 67.2 61.3 52.9 61.8 56.4 69.6 82.7 75.1 64.3 65.8 63.4 72.5 87.2 76.8
(Airola et al 2008)
Hybrid kernel and 65.7 71.1 68.1 55.0 68.8 60.8 67.5 78.6 71.7 68.5 76.1 70.9 77.6 86.0 80.1
multiple parser input
(Miwa et al 2009a)
SVM-CW, multiple ? ? 67.6 ? ? 64.2 ? ? 74.4 ? ? 69.7 ? ? 80.5
parser input and graph,
walk and BOW features
(Miwa et al 2009b)
kBSPS kernel 49.9 61.8 55.1 50.1 41.4 44.6 58.8 89.7 70.5 62.2 87.1 71.0 69.3 93.2 78.1
(Tikk et al 2010)
Walk weighted 61.8 54.2 57.6 61.4 53.3 56.6 73.8 71.8 72.9 66.7 69.2 67.8 76.9 91.2 82.4
subsequence kernel
(Kim et al 2010)
2 phase extraction 61.7 57.5 60.0 55.3 68.5 61.2 ? ? ? ? ? ? ? ? ?
(Bui et al 2010)
Our proposed hybrid 80.0 71.4 75.5 64.2 58.2 61.1 81.1 69.3 74.7 72.9 59.5 65.5 70.4 95.7 81.1
kernel (PET + SL +
TPWF without regex)
Table 4: Comparison of the results on the 5 benchmark PPI corpora. Pos. and Neg. refer to number positive and
negative relations respectively. The underlined numbers indicate the best results for the corresponding corpus
reported by any of the existing state-of-the-art approaches. The results of Bui et al(2010) on LLL, HPRD50,
and IEPA are not reported since thy did not use all the positive and negative examples during cross validation.
Miwa et al(2009b) showed that better results can be obtained using multiple corpora for training. However,
we consider only those results of their experiments where they used single training corpus as it is the standard
evaluation approach adopted by all the other studies on PPI extraction for comparing results. All the results of
the previous approaches reported in this table are directly quoted from their respective original papers.
where KSL, KGC and KLC correspond to SL,
global context (GC) and local context (LC) ker-
nels respectively. The GC kernel exploits contex-
tual information of the words occurring before,
between and after the pair of entities (to be in-
vestigated for RE) in the corresponding sentence;
while the LC kernel exploits contextual informa-
tion surrounding individual entities.
4.3 Path-enclosed tree (PET) Kernel
The path-enclosed tree (PET) kernel3 was first
proposed by Moschitti (2004) for semantic role
labeling. It was later successfully adapted by
Zhang et al(2005) and other works for relation
extraction on general texts (such as newspaper do-
3Also known as shortest path-enclosed tree (SPT) kernel.
main). A PET is the smallest common subtree of a
phrase structure tree that includes the two entities
involved in a relation.
A tree kernel calculates the similarity between
two input trees by counting the number of com-
mon sub-structures. Different techniques have
been proposed to measure such similarity. We use
the Unlexicalized Partial Tree (uPT) kernel (Sev-
eryn and Moschitti, 2010) for the computation of
the PET kernel since a comparative evaluation by
Chowdhury et al(2011a) reported that uPT ker-
nels achieve better results for PPI extraction than
the other techniques used for tree kernel compu-
tation.
425
5 Experimental Settings
We have followed the same criteria commonly
used for the PPI extraction tasks, i.e. abstract-
wise 10-fold cross validation on individual corpus
and one-answer-per-occurrence criterion. In fact,
we have used exactly the same (abstract-wise)
fold splitting of the 5 benchmark (converted) cor-
pora used by Tikk et al(2010) for benchmarking
various kernel methods4.
The Charniak-Johnson reranking parser (Char-
niak and Johnson, 2005), along with a self-trained
biomedical parsing model (McClosky, 2010), has
been used for tokenization, POS-tagging and
parsing of the sentences. Before parsing the sen-
tences, all the entities are blinded by assigning
names as EntityX where X is the entity index.
In each example, the POS tags of the two can-
didate entities are changed to EntityX . The
parse trees produced by the Charniak-Johnson
reranking parser are then processed by the Stan-
ford parser5 (Klein and Manning, 2003) to obtain
syntactic dependencies according to the Stanford
Typed Dependency format.
The Stanford parser often skips some syntactic
dependencies in output. We use the following two
rules to add some of such dependencies:
? If there is a ?conj and? or ?conj or? depen-
dency between two words X and Y, then X
should be dependent on any word Z on which
Y is dependent and vice versa.
? If there are two verbs X and Y such that in-
side the corresponding sentence they have
only the word ?and? or ?or? between them,
then any word Z dependent on X should be
also dependent on Y and vice versa.
Our system exploits SVM-LIGHT-TK6 (Mos-
chitti, 2006; Joachims, 1999). We made minor
changes in the toolkit to compute the proposed
hybrid kernel. The ratio of negative and positive
examples has been used as the value of the cost-
ratio-factor parameter. We have done parameter
tuning following the approach described by Hsu
et al(2003).
4Downloaded from http://informatik.hu-
berlin.de/forschung /gebiete/wbi/ppi-benchmark .
5http://nlp.stanford.edu/software/lex-parser.shtml
6http://disi.unitn.it/moschitti/Tree-Kernel.htm
6 Results and Discussion
To measure the contribution of the features col-
lected from the reduced graphs (using dependency
patterns, trigger words and negative cues) and
regex patterns, we have applied the new TPWF
kernel on the 5 PPI corpora before and after using
these features. Results shown in Table 2 clearly
indicate that usage of these features improve the
performance. The improvement of performance
is primarily due to the usage of dependency pat-
terns which resulted in higher precision for all the
corpora.
We have tried to measure the contribution of
the regex patterns. However, from the empirical
results a clear trend does not emerge (see Table
2).
Table 3 shows a comparison among the re-
sults of the proposed hybrid kernel and its indi-
vidual components. As we can see, the overall
results of the hybrid kernel (with and without us-
ing regex pattern features) are better than those
by any of its individual component kernels. Inter-
estingly, precision achieved on the 4 benchmark
corpora (other than the smallest corpus LLL) is
much higher for the hybrid kernel than for the in-
dividual components. This strongly indicates that
these different types of information (i.e. depen-
dency patterns, regex patterns, triggers, negative
cues, syntactic dependencies among words and
constituent parse trees) and their different repre-
sentations (i.e. flat features, tree structures and
graphs) can complement each other to learn more
accurate models.
Table 4 shows a comparison of the PPI extrac-
tion results of our proposed hybrid kernel with
those of other state-of-the-art approaches. Since
the contribution of regex patterns in the perfor-
mance of the hybrid kernel was not relevant (as
Tables 2 and 3 show), we used the results of pro-
posed hybrid kernel without regex for the compar-
ison. As we can see, the proposed kernel achieves
significantly higher results on the BioInfer corpus,
the largest benchmark PPI corpus (2,534 positive
PPI pair annotations) available, than any of the
existing approaches. Moreover, the results of the
proposed hybrid kernel are on par with the state-
of-the-art results on the other smaller corpora.
Furthermore, empirical results show that the
proposed hybrid kernel attains considerably
higher precision than the existing approaches.
426
Since a dependency pattern, by construction,
contains all the syntactic dependencies inside the
corresponding reduced graph, it may happen that
some of the dependencies (e.g. det or determiner)
are not informative for classifying the label of the
corresponding class label (i.e., positive or nega-
tive relation) of the pattern. Their presence in-
side a pattern might make it unnecessarily rigid
and less general. So, we tried to identify and dis-
card such non informative dependencies by mea-
suring probabilities of the dependencies with re-
spect to the class label and then removing any of
them which has probability lower than a threshold
(we tried with different threshold values). But do-
ing so decreased the performance. This suggests
that the syntactic dependencies of a dependency
pattern are not independent of each other even if
some of them might have low probability (with
respect to the class label) individually. We plan to
further investigate whether there could be differ-
ent criteria for identifying non informative depen-
dencies. For the work reported in this paper, we
used the dependency patterns as they are initially
constructed.
We also did experiments to see whether collect-
ing features for trigger words from the whole re-
duced graph would help. But that also decreased
performance. This suggests that trigger words are
more likely to appear in the least common gover-
nors.
7 Conclusion
In this paper, we have proposed a new hybrid
kernel for RE that combines two vector based
kernels and a tree kernel. The proposed kernel
outperforms any of the exiting approaches by a
wide margin on the BioInfer corpus, the largest
PPI benchmark corpus available. On the other
four smaller benchmark corpora, it performs ei-
ther better or almost as good as the existing state-
of-the art approaches.
We have also proposed a novel feature based
kernel, called TPWF kernel, using (automatically
collected) dependency patterns, trigger words,
negative cues, walk features and regular expres-
sion patterns. The TPWF kernel is used as a com-
ponent of the new hybrid kernel.
Empirical results show that the proposed hy-
brid kernel achieves considerably higher precision
than the existing approaches, which indicates its
capability of learning more accurate models. This
also demonstrates that the different types of infor-
mation that we use are able to complement each
other for relation extraction.
We believe there are at least three ways to
further improve the proposed approach. First
of all, the 22 regular expression patterns (col-
lected from Ono et al(2001) and Bui et al
(2010)) are applied at the level of the sen-
tences and this sometimes produces unwanted
matches. For example, consider the sentence
?X activates Y and inhibits Z? where X, Y,
and Z are entities. The pattern ?Entity1. ?
activates. ?Entity2? matches both the X?Y and
X?Z pairs in the sentence. But only the X?Y pair
should be considered. So, the patterns should
be constrained to reduce the number of unwanted
matches. For example, they could be applied on
smaller linguistic units than full sentences. Sec-
ondly, different techniques could be used to iden-
tify less-informative syntactic dependencies in-
side dependency patterns to make them more ac-
curate and effective. Thirdly, usage of automati-
cally collected paraphrases of regular expression
patterns instead of the patterns directly could be
also helpful. Weakly supervised collection of
paraphrases for RE has been already investigated
(e.g. Romano et al(2006)) and, hence, can be
tried for improving the TPWF kernel (which is a
component of the proposed hybrid kernel).
Acknowledgments
This work was carried out in the context of the project
?eOnco - Pervasive knowledge and data management
in cancer care?. The authors are grateful to Alessan-
dro Moschitti for his help in the use of SVM-LIGHT-
TK. We also thank the anonymous reviewers for help-
ful suggestions.
References
Antti Airola, Sampo Pyysalo, Jari Bjorne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski. 2008.
All-paths graph kernel for protein-protein inter-
action extraction with evaluation of cross-corpus
learning. BMC Bioinformatics, 9(Suppl 11):S2.
Quoc-Chinh Bui, Sophia Katrenko, and Peter M.A.
Sloot. 2010. A hybrid approach to extract protein-
protein interactions. Bioinformatics.
Razvan Bunescu and Raymond J. Mooney. 2006.
Subsequence kernels for relation extraction. In Pro-
ceedings of NIPS 2006, pages 171?178.
427
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun Ku-
mar Ramani, and Yuk Wah Wong. 2005. Compara-
tive experiments on learning information extractors
for proteins and their interactions. Artificial Intelli-
gence in Medicine, 33(2):139?155.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL 2005.
Md. Faisal Mahbub Chowdhury and Alberto Lavelli.
2011b. Drug-drug interaction extraction using com-
posite kernels. In Proceedings of DDIExtrac-
tion2011: First Challenge Task: Drug-Drug In-
teraction Extraction, pages 27?33, Huelva, Spain,
September.
Md. Faisal Mahbub Chowdhury, Alberto Lavelli, and
Alessandro Moschitti. 2011a. A study on de-
pendency tree kernels for automatic extraction of
protein-protein interaction. In Proceedings of
BioNLP 2011 Workshop, pages 124?133, Portland,
Oregon, USA, June.
Md. Faisal Mahbub Chowdhury, Asma Ben Abacha,
Alberto Lavelli, and Pierre Zweigenbaum. 2011c.
Two dierent machine learning techniques for drug-
drug interaction extraction. In Proceedings of
DDIExtraction2011: First Challenge Task: Drug-
Drug Interaction Extraction, pages 19?26, Huelva,
Spain, September.
J. Ding, D. Berleant, D. Nettleton, and E. Wurtele.
2002. Mining MEDLINE: abstracts, sentences, or
phrases? Pacific Symposium on Biocomputing,
pages 326?337.
Katrin Fundel, Robert Ku?ffner, and Ralf Zimmer.
2007. Relex?relation extraction using dependency
parse trees. Bioinformatics, 23(3):365?371.
Claudio Giuliano, Alberto Lavelli, and Lorenza Ro-
mano. 2006. Exploiting shallow linguistic infor-
mation for relation extraction from biomedical lit-
erature. In Proceedings of EACL 2006, pages 401?
408.
CW Hsu, CC Chang, and CJ Lin, 2003. A practical
guide to support vector classification. Department
of Computer Science and Information Engineering,
National Taiwan University, Taipei, Taiwan.
Thorsten Joachims. 1999. Making large-scale sup-
port vector machine learning practical. In Advances
in kernel methods: support vector learning, pages
169?184. MIT Press, Cambridge, MA, USA.
Seonho Kim, Juntae Yoon, Jihoon Yang, and Seog
Park. 2010. Walk-weighted subsequence kernels
for protein-protein interaction extraction. BMC
Bioinformatics, 11(1).
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL
2003, pages 423?430, Sapporo, Japan.
David McClosky. 2010. Any Domain Parsing: Au-
tomatic Domain Adaptation for Natural Language
Parsing. Ph.D. thesis, Department of Computer
Science, Brown University.
Makoto Miwa, Rune S?tre, Yusuke Miyao, and
Jun?ichi Tsujii. 2009a. Protein-protein interac-
tion extraction by leveraging multiple kernels and
parsers. International Journal of Medical Informat-
ics, 78.
Makoto Miwa, Rune S?tre, Yusuke Miyao, and
Jun?ichi Tsujii. 2009b. A rich feature vector for
protein-protein interaction extraction from multiple
corpora. In Proceedings of EMNLP 2009, pages
121?130, Singapore.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In Proceed-
ings of ACL 2004, Barcelona, Spain.
Alessandro Moschitti. 2006. Making Tree Kernels
Practical for Natural Language Learning. In Pro-
ceedings of EACL 2006, Trento, Italy.
Claire Ne?dellec. 2005. Learning language in logic -
genic interaction extraction challenge. Proceedings
of the ICML 2005 workshop: Learning Language in
Logic (LLL05), pages 31?37.
Toshihide Ono, Haretsugu Hishigaki, Akira Tanigami,
and Toshihisa Takagi. 2001. Automated ex-
traction of information on protein?protein interac-
tions from the biological literature. Bioinformatics,
17(2):155?161.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Jarvinen, and Tapio
Salakoski. 2007. Bioinfer: a corpus for information
extraction in the biomedical domain. BMC Bioin-
formatics, 8(1):50.
Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari
Bjo?rne, Filip Ginter, and Tapio Salakoski. 2008.
Comparative analysis of five protein-protein in-
teraction corpora. BMC Bioinformatics, 9(Suppl
3):S6.
Lorenza Romano, Milen Kouylekov, Idan Szpektor,
Ido Dagan, and Alberto Lavelli. 2006. Investi-
gating a generic paraphrase?based approach for re-
lation extraction. In Proceedings of EACL 2006,
pages 409?416.
Isabel Segura-Bedmar, Paloma Mart??nez, and Cesar de
Pablo-Sa?nchez. 2011. Using a shallow linguistic
kernel for drug-drug interaction extraction. Jour-
nal of Biomedical Informatics, In Press, Corrected
Proof, Available online, 24 April.
Aliaksei Severyn and Alessandro Moschitti. 2010.
Fast cutting plane training for structural kernels. In
Proceedings of ECML-PKDD 2010.
Domonkos Tikk, Philippe Thomas, Peter Palaga,
Jo?rg Hakenberg, and Ulf Leser. 2010. A Compre-
hensive Benchmark of Kernel Methods to Extract
Protein-Protein Interactions from Literature. PLoS
Computational Biology, 6(7), July.
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou,
and Chew Lim Tan. 2005. Discovering relations
428
between named entities from a large raw corpus us-
ing tree similarity-based clustering. In Natural Lan-
guage Processing ? IJCNLP 2005, volume 3651 of
Lecture Notes in Computer Science, pages 378?389.
Springer Berlin / Heidelberg.
429
Proceedings of NAACL-HLT 2013, pages 765?771,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Exploiting the Scope of Negations and Heterogeneous Features
for Relation Extraction: A Case Study for Drug-Drug Interaction Extraction
Md. Faisal Mahbub Chowdhury ? ? and Alberto Lavelli ?
? Fondazione Bruno Kessler (FBK-irst), Italy
? University of Trento, Italy
fmchowdhury@gmail.com, lavelli@fbk.eu
Abstract
This paper presents an approach that exploits
the scope of negation cues for relation extrac-
tion (RE) without the need of using any specif-
ically annotated dataset for building a separate
negation scope detection classifier. New fea-
tures are proposed which are used in two dif-
ferent stages. These also include non-target
entity specific features. The proposed RE ap-
proach outperforms the previous state of the
art for drug-drug interaction (DDI) extraction.
1 Introduction
Negation is a linguistic phenomenon where a nega-
tion cue (e.g. not) can alter the meaning of a partic-
ular text segment or of a fact. This text segment (or
fact) is said to be inside the scope of that negation
(cue). In the context of RE, there is not much work
that aims to exploit the scope of negations.1 The
only work on RE that we are aware of is Sanchez-
Graillet and Poesio (2007) where they used various
heuristics to extract negative protein interaction.
Despite the recent interest on automatically de-
tecting the scope of negation2 till now there seems
to be no empirical evidence supporting its exploita-
tion for the purpose of RE. Even if we could man-
age to obtain highly accurate automatically detected
1In the context of event extraction (a closely related task of
RE), there have been efforts in BioNLP shared tasks of 2009 and
2011 for (non-mandatory sub-task of) event negation detection
(3 participants in 2009; 2 in 2011) (Kim et al, 2009; Kim et al,
2011). The participants approached the sub-task using either
pre-defined patterns or some heuristics.
2This task is popularized by various recently held shared
tasks (Farkas et al, 2010; Morante and Blanco, 2012).
negation scopes, it is not clear how to feed this infor-
mation inside the RE approach. Simply considering
whether a pair of candidate mentions falls under the
scope of a negation cue might not be helpful.
In this paper, we propose that the scope of nega-
tions can be exploited at two different levels. Firstly,
the system would check whether all the target en-
tity3 mentions inside a sentence along with possible
relation clues (or trigger words), if any, fall (directly
or indirectly) under the scope of a negation cue. If
such a sentence is found, then it should be discarded
(i.e. candidate mention pairs4 inside that sentence
would not be considered). Secondly, for each of the
remaining pairs of candidate mentions, the system
should exploit features related to the scope of nega-
tion (rather than simply adding a feature for negation
cue, approach adopted in various RE systems) that
can provide indication (if any such evidence exists)
that the corresponding relation of interest actually
does not hold in that particular context.
In the subsequent sections, we describe our ap-
proach. The RE task considered is drug-drug in-
teraction (DDI) extraction. The task has signifi-
cant importance for public health safety.5 We used
3The target entities, for example, for DDI extraction and for
EMP-ORG relation extraction would be {DRUG} and {PER,
GPE, ORG} respectively. Any entity other than the target enti-
ties (w.r.t. the particular RE task) belongs to non-target entities.
4Candidate mention pairs for RE are taken from target entity
mentions.
5After the death of pop star Michael Jackson, allegedly due
to DDI, it was reported that about 2.2 million people in USA,
age 57 to 85, were taking potentially dangerous combinations of
drugs (Landau, 2009). An earlier report mentioned that deaths
from accidental drug interactions rose 68 percent between 1999
and 2004 (Payne, 2007).
765
the DDIExtraction-2011 challenge corpus (Segura-
Bedmar et al, 2011). The official training and test
data of the corpus contain 4,267 and 1,539 sen-
tences, and 2,402 and 755 DDI annotations respec-
tively.
2 Proposed Approach
2.1 Stage 1: Exploiting scope of negation to
filter out sentences
We propose a two stage RE approach. In the first
stage, our goal is to exploit the scope of negations
to reduce the number of candidate mention pairs by
discarding sentences. For this purpose, we propose
the following features to train a binary classifier:
? has2TM: If the sentence has exactly 2 target entity
mentions (i.e. drug mentions for DDI extraction).
? has3OrMoreTM: Whether the sentence has more
than 2 target entity mentions.
? allTMonRight: Whether all target entity mentions
inside the sentence appear after the negation cue.
? neitherAllTMonLeftOrRight: Whether some but not
all target entity mentions appear after the negation
cue.
? negCue: The negation cue itself.
? immediateGovernor: The word on which the cue is
directly syntactically dependent.
? nearestVerbGovernor: The nearest verb in the de-
pendency graph on which the cue is syntactically
dependent.
? isVerbGovernorRoot: Whether the nearestVerb-
Governor is root of the dependency graph of the
sentence.
? allTMdependentOnNVG: Whether all target en-
tity mentions are syntactically dependent (di-
rectly/indirectly) on the nearestVerbGovernor.
? allButOneTMdependentOnNVG: Whether all but
one target entity mentions are syntactically depen-
dent on the nearestVerbGovernor.
? although*PrecedeCue: Whether the syntactic
clause containing the negation cue begins with ?al-
though / though / despite / in spite?.
? commaBeforeNextTM: Whether there is a comma in
the text between the negation cue and the next target
entity mention after the cue.
? commaAfterPrevTM: Whether there is a comma in
the text between the previous target entity mention
before the negation cue and the cue itself.
? sentHasBut: Whether the sentence contains the
word ?but?.
The objective of the classifier is to decide whether
all of the target entity mentions (i.e. drugs) as well as
any possible evidence of the relation of interest (for
which we assume the immediate and the nearest verb
governors of the negation cue would be good candi-
dates) inside the corresponding sentence fall under
the scope of a negation cue in such a way that the
sentence is unlikely to contain a DDI.
At present, we limit our focus only on the first
occurrence of the following negation cues: ?no?,
?n?t? or ?not?.6 In the Stage 1, any sentence that
contains at least one DDI is considered by the clas-
sifier as a positive (training/test) instance. Other sen-
tences are considered as negative instances. We rule
out any sentence (i.e. we do not consider as train-
ing/test instance for the classifier that filters less in-
formative sentences) during both training and testing
if any of the following conditions holds:
? The sentence contains less than two target entity
mentions (such sentence would not contain the re-
lation of interest anyway).
? It has any of the following phrases ? ?not recom-
mended?, ?should not be? or ?must not be?.7
? There is no ?no?, ?n?t? or ?not? in the sentence.
? No target entity mention appears in the sentence af-
ter ?no?, ?n?t? or ?not?.
To assess the effectiveness of the proposed Stage
1 classifier, we defined a baseline classifier that fil-
ters any sentence that contains ?no?, ?n?t? or ?not?.
2.2 Stage 2
Once the sentences which are likely to have no DDI
are identified and removed, the next step is to ap-
ply a state-of-the-art RE approach on the remaining
sentences. In this section, we propose a new hybrid
kernel, KHybrid, for this purpose. It is defined as
follows:
KHybrid (R1, R2) = KHF (R1, R2) + KSL
(R1, R2) + w * KPET (R1, R2)
6These cues usually occur more frequently and generally
have larger negation scope than other negation cues.
7These expressions often provide clues that one of the bio-
entity mentions negatively influences the level of activity of the
other.
766
Here, KHF stands for a new feature based kernel
(proposed in this paper) that uses a heterogeneous
set of features. KSL stands for the Shallow Linguis-
tic (SL) kernel proposed by Giuliano et al (2006).
KPET stands for the Path-enclosed Tree (PET) ker-
nel (Moschitti, 2004). w is a multiplicative constant
used for the PET kernel. It allows the hybrid kernel
to assign more (or less) weight to the information
obtained using tree structures depending on the cor-
pus.
The proposed kernel composition is valid accord-
ing to the closure properties of kernels. We ex-
ploit the SVM-Light-TK toolkit (Moschitti, 2006;
Joachims, 1999) for kernel computation. In Stage
2, each candidate drug mention pair represents an
instance.
2.2.1 Proposed KHF kernel
As mentioned earlier, this proposed kernel uses
heterogeneous features. The first version of the het-
erogeneous feature set (henceforth, HF v1) com-
bines features proposed by two previous RE works.
The former is Zhou et al (2005), which uses 51 dif-
ferent features. We select the following 27 of their
features for our feature set:
WBNULL, WBFL, WBF, WBL, WBO,
BM1F, BM1L, AM2F, AM2L, #MB, #WB,
CPHBNULL, CPHBFL, CPHBF, CPHBL,
CPHBO, CPHBM1F, CPHBM1L, CPHAM2F,
CPHAM2F, CPP, CPPH, ET12SameNP,
ET12SamePP, ET12SameVP, PTP, PTPH
The latter is the TPWF kernel (Chowdhury and
Lavelli, 2012a) from which we use following fea-
tures:
HasTriggerWord, Trigger-X, DepPattern-i, e-
walk, v-walk
The TPWF kernel extracts the HasTriggerWord,
Trigger-X and DepPattern-i features from a sub-
graph called reduced graph. We also follow this ap-
proach with one minor difference. Unlike Chowd-
hury and Lavelli (2012a), we look for trigger words
in the whole reduced graph instead of using only the
root of the sub-graph.
Due to space limitation we refer the readers to
the corresponding papers for the description of the
above mentioned features and the definition of re-
duced graph.
In addition, HF v1 also includes surrounding to-
kens within the window of {-2,+2} for each candi-
date mention. We are unaware of any available list
of trigger words for drug-drug interaction. So, we
created such a list.8
We extend the heterogeneous feature set by
adding features related to the scope of negation
(henceforth, HF v2). We use a list of 13 negation
cues9 to search inside the reduced graph of a candi-
date pair. If the reduced graph contains any of the
negation cues or their morphological variants then
we add the following features:
? negCue: The corresponding negation cue.
? immediateNegatedWord: If the word following the
negation cue is neither a preposition nor a ?be verb?,
then that word, otherwise the word after the next
word.10
Furthermore, if the corresponding matched nega-
tion cue is either ?no?, ?n?t? or ?not?, then we add
additional features related to negation scope:
? bothEntDependOnImmediateGovernor: Whether
the immediate governor (if any) of the negation cue
is also governor of a dependency sub-tree (of the de-
pendency graph of the corresponding sentence) that
includes both of the candidate mentions.
? immediateGovernorIsVerbGovernor: Whether the
immediate governor of the negation cue is a verb.
? nearestVerbGovernor: The closest verb governor
(i.e. parent or grandparent inside the dependency
graph), if any, of the negation cue.
We further extend the heterogeneous feature set
by adding features related to relevant non-target en-
tities (with respect to the relation of interest; hence-
forth, HF v3). For the purpose of DDI extrac-
tion, we deem the presence of DISEASE mentions
(which might result as a consequence of a DDI)
can provide some clues. So, we use a publicly
available state-of-the-art disease NER system called
BioEnEx (Chowdhury and Lavelli, 2010) to anno-
tate the DDIExtraction-2011 challenge corpus. For
8The RE system developed for this work and the cre-
ated list of trigger words for DDI can be downloaded from
https://github.com/fmchowdhury/HyREX .
9No, not, neither, without, lack, fail, unable, abrogate, ab-
sence, prevent, unlikely, unchanged, rarely.
10For example, ?interested? from ?... not interested ...?, and
?confused? from ?... not to be confused ...?.
767
each candidate (drug) mention pair, we add the fol-
lowing features in HF v3:
? NTEMinsideSentence: Whether the corresponding
sentence contains important non-target entity men-
tion(s) (e.g. disease for DDI).
? immediateGovernorIsVerbGovernorOfNTEM: The
immediate governor (if any) of the non-target entity
mention, only if such governor is also governing a
dependency sub-tree that includes both of the target
candidate entity mentions.
? nearestVerbGovernorOfNTEM: The closest verb
governor (if any) of the non-target entity mention,
only if it also governs the candidate entity mentions.
? immediateGovernorIsVerbGovernorOfNTEM:
Whether the immediate governor is a verb.
3 Results and Discussion
We train a linear SVM classifier in Stage 1 and
tune the hyper-parameters (by doing 5-fold cross-
validation) for obtaining maximum possible recall.
In this way we minimize the number of false neg-
atives (i.e. sentences that contain DDIs but are
wrongly identified as not having any).
During the cross-validation experiments on the
training data, 334 sentences (7.83% of the total sen-
tences) containing at least 2 drug mentions were
identified by our proposed classifier (in Section 2.1)
as unlikely to have any DDI and hence are candi-
dates for discarding. Only 19 of these sentences
were incorrectly identified. When we trained on
the training data and tested on the official test data
of DDIExtraction-2011 challenge corpus, 121 sen-
tences (7.86% of the total test sentences) were iden-
tified by the classifier as candidates for discarding.
Only 5 of them were incorrectly identified.
Unlike Stage 1, in Stage 2 where we train the hy-
brid kernel based RE classifier and use it for RE (i.e.
DDI extraction) from the test data, sentences are not
the RE training/test instances. Instead, a RE instance
corresponds to a candidate mention pair.
All the DDIs (i.e. positive RE instances) of the
incorrectly identified sentences in Stage 1 (i.e. the
sentences which are incorrectly labelled as not hav-
ing any DDI and filtered) are automatically consid-
ered as false negatives during the calculation of DDI
extraction results in Stage 2.
To verify whether our proposed hybrid kernel
achieves state-of-the-art results without taking ben-
efits of the output of Stage 1, we did some experi-
ments without discarding any sentence. These ex-
periments are done using Zhou et al (2005), TPWF
kernel, SL kernel, different versions of proposed
KHF kernel and KHybrid kernel. Table 1 shows
the results of 5-fold cross-validation experiments
(hyper-parameters are tuned for obtaining maximum
F-score). As the results show, there is a gain +0.9
points in F-score (mainly due to the boost in re-
call) after the addition of features related to negation
scope. There is also some minor improvement due
to the proposed non-target entity specific features.
We also performed (5-fold cross validation) ex-
periments by combining the Stage 1 classifier with
each of the Zhou et al (2005), TPWF kernel, SL
kernel, PET kernel, KHF kernel and KHybrid kernel
separately (only the results of KHybrid are reported
in Table 1 due to space limitation). In each case,
there were improvements in precision, recall and F-
score. The gain in F-score ranged from 1.0 to 1.4
points.
P / R / F-score
Using SL kernel (Giuliano et al, 2006) 51.3 / 64.7 / 57.3
Using (Zhou et al, 2005) 58.7 / 37.1 / 45.5
Using PET kernel (Moschitti, 2004) 46.8 / 602 / 52.7
TPWF (Chowdhury and Lavelli, 2012a) 43.7 / 60.7 / 50.8
Proposed approaches
Proposed KHF v1 53.4 / 51.5 / 52.4
KHF v2 (i.e. + neg scope feat.) 53.9 / 52.6 / 53.3 (+0.9)
KHF v3 (i.e. + non-target entity feat.) 53.6 / 53.5 / 53.6 (+0.3)
Proposed KHybrid 56.3 / 68.5 / 61.8
Proposed KHybrid with Stage 1 57.3 / 69.4 / 62.8 (+1.0)
Table 1: 5-fold cross-validation results on training data.
Table 2 reports the results of the previously pub-
lished studies that used the same corpus. Our pro-
posed KHybrid kernel obtains an F-score that is
higher than that of the previous state of the art.
When the Stage 1 classifier (based on negation
scope features) is exploited before using the KHybrid
kernel, the F-score reaches up to 67.4. This is
+1.0 points higher than without exploiting the Stage
1 classifier and +1.7 higher than previous state of
768
the art. We did separate experiments (also reported
in Table 2) to assess the performance improvement
when the output of Stage 1 is used to filter sentences
from either training or test data only. The results
remain the same when only training sentences are
filtered; while there are some improvements when
only test sentences are filtered. Filtering both train-
ing and test sentences provides the larger gain which
is statistically significant.
Usually, the number of negative instances in a
corpus is much higher than that of the positive in-
stances. In a recent work, Chowdhury and Lavelli
(2012b) showed that by removing less informative
(negative) instances (henceforth, LIIs), not only the
skewness in instance distribution could be reduced
but it also leads to a better result. The proposed
Stage 1 classifier, presented in this work, also re-
duces skewness in instance distribution. This is be-
cause we are only removing those sentences that are
unlikely to contain any positive instance. So, in prin-
ciple, the Stage 1 classifier is focused on removing
only negative instances (although the classifier mis-
takenly discards few positive instances, too).
We wanted to study how the Stage 1 classifier
would contribute if we use it on top of the tech-
niques that were proposed in Chowdhury and Lavelli
(2012b) to remove LIIs. As Table 2 shows, by using
the Stage 1 classifier along with LLI filtering, we
could further improve the results (+3.2 points differ-
ence in F-score with the previous state of the art).
4 Conclusion
A major flexibility in the proposed approach is that
it does not require a separate dataset (which needs
to match the genre of the text to be used for RE)
annotated with negation scopes. Instead, the pro-
posed Stage 1 classifier uses the RE training data
(which do not have negation scope annotations) to
self-supervise itself. Various new features have been
exploited (both in stages 1 and 2) that can provide
strong indications of the scope of negation cues with
respect to the relation to be extracted. The only thing
needed is the list of possible negation cues (Morante
(2010) includes such a comprehensive list).
Our proposed kernel, which has a component that
exploits a heterogeneous set of features including
negation scope and presence of non-target entities,
already obtains better results than previous studies.
P R F-score
(Thomas et al, 2011) 60.5 71.9 65.7
(Chowdhury et al, 2011) 58.6 70.5 64.0
(Chowdhury and Lavelli, 2011) 58.4 70.1 63.7
(Bjorne et al, 2011) 58.0 68.9 63.0
Proposed KHybrid 60.0 74.3 66.4
KHybrid + Stage 1 baseline 61.8 68.9 65.1
KHybrid + proposed Stage 1 60.0 74.2 66.4
(only training sentences are filtered)
KHybrid + proposed Stage 1 61.4 73.8 67.0
(only test sentences are filtered)
KHybrid + proposed Stage 1 62.1 73.8 67.4 stat. sig.
(both training and test sentences are filtered)
Proposed KHybrid + LII filtering 61.1 75.1 67.4 stat. sig.
Proposed KHybrid + LII filtering 63.5 75.2 68.9 stat. sig.
+ proposed Stage 1
Table 2: Results obtained on the official test set of the
2011 DDI Extraction challenge. LII filtering refers to the
techniques proposed in Chowdhury and Lavelli (2012b)
for reducing skewness in RE data distribution. stat. sig. in-
dicates that the improvement of F-score, due to usage of
Stage 1 classifier, is statistically significant (verified using
Approximate Randomization Procedure (Noreen, 1989);
number of iterations = 1,000, confidence level = 0.01).
The results considerably improve when possible ir-
relevant sentences from both training and test data
are filtered by exploiting features related to the scope
of negations.
In future, we would like to exploit the scope of
more negation cues, apart from the three cues that
are used in this study. We believe our approach
would help to improve RE in other genres of text
(such as newspaper) as well.
Acknowledgement
This work was carried out in the context of the
project ?eOnco - Pervasive knowledge and data
management in cancer care?.
References
J Bjorne, A Airola, T Pahikkala, and T Salakoski. 2011.
Drug-drug interaction extraction with RLS and SVM
classifiers. In Proceedings of the 1st Challenge task
on Drug-Drug Interaction Extraction (DDIExtraction
2011), pages 35?42, Huelva, Spain, September.
769
MFM Chowdhury and A Lavelli. 2010. Disease mention
recognition with specific features. In Proceedings of
the 2010 Workshop on Biomedical Natural Language
Processing, pages 83?90, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
MFM Chowdhury and A Lavelli. 2011. Drug-drug inter-
action extraction using composite kernels. In Proceed-
ings of the 1st Challenge task on Drug-Drug Interac-
tion Extraction (DDIExtraction 2011), pages 27?33,
Huelva, Spain, September.
MFM Chowdhury and A Lavelli. 2012a. Combining tree
structures, flat features and patterns for biomedical re-
lation extraction. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL 2012), pages 420?
429, Avignon, France, April. Association for Compu-
tational Linguistics.
MFM Chowdhury and A Lavelli. 2012b. Impact of Less
Skewed Distributions on Efficiency and Effectiveness
of Biomedical Relation Extraction. In Proceedings of
the 24th International Conference on Computational
Linguistics (COLING 2012) : Posters, pages 205?216,
Mumbai, India, December.
MFM Chowdhury, AB Abacha, A Lavelli, and
P Zweigenbaum. 2011. Two different machine learn-
ing techniques for drug-drug interaction extraction. In
Proceedings of the 1st Challenge task on Drug-Drug
Interaction Extraction (DDIExtraction 2011), pages
19?26, Huelva, Spain, September.
R Farkas, V Vincze, G Mo?ra, J Csirik, and G Szarvas.
2010. The CoNLL-2010 shared task: Learning to de-
tect hedges and their scope in natural language text.
In Proceedings of the Fourteenth Conference on Com-
putational Natural Language Learning, pages 1?12,
Uppsala, Sweden, July. Association for Computational
Linguistics.
C Giuliano, A Lavelli, and L Romano. 2006. Exploit-
ing shallow linguistic information for relation extrac-
tion from biomedical literature. In Proceedings of the
11th Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL 2006),
pages 401?408.
T Joachims. 1999. Making large-scale support vec-
tor machine learning practical. In Advances in ker-
nel methods: support vector learning, pages 169?184.
MIT Press, Cambridge, MA, USA.
JD Kim, T Ohta, S Pyysalo, Y Kano, and J Tsujii. 2009.
Overview of BioNLP?09 shared task on event extrac-
tion. In Proceedings of the BioNLP 2009 Workshop
Companion Volume for Shared Task, pages 1?9, Boul-
der, Colorado, June. Association for Computational
Linguistics.
JD Kim, Y Wang, T Takagi, and A Yonezawa. 2011.
Overview of Genia event task in BioNLP shared task
2011. In Proceedings of BioNLP Shared Task 2011
Workshop, pages 7?15, Portland, Oregon, USA, June.
Association for Computational Linguistics.
E Landau. 2009. Jackson?s death raises questions about
drug interactions [Published in CNN; June 26, 2009].
http://articles.cnn.com/2009-06-
26/health/jackson.drug.interaction.
caution_1_drug-interactions-heart-
rhythms-antidepressants?_s=PM:
HEALTH.
R Morante and E Blanco. 2012. *SEM 2012 shared task:
Resolving the scope and focus of negation. In *SEM
2012: The First Joint Conference on Lexical and Com-
putational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 265?274,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
R Morante. 2010. Descriptive Analysis of Negation
Cue in Biomedical Texts. In Proceedings of the 7th
International Conference on Language Resources and
Evaluation (LREC 2010), Malta.
A Moschitti. 2004. A study on convolution kernels for
shallow semantic parsing. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics (ACL 2004), Barcelona, Spain.
A Moschitti. 2006. Making Tree Kernels Practical
for Natural Language Learning. In Proceedings of
11th Conference of the European Chapter of the As-
sociation for computational Linguistics (EACL 2006),
Trento, Italy.
EW Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses : An Introduction. Wiley-
Interscience, April.
JW Payne. 2007. A Dangerous Mix [Published
in The Washington Post; February 27, 2007].
http://www.washingtonpost.com/wp-
dyn/content/article/2007/02/23/
AR2007022301780.html.
O Sanchez-Graillet and M Poesio. 2007. Negation of
protein-protein interactions: analysis and extraction.
Bioinformatics, 23(13):i424?i432.
I Segura-Bedmar, P Mart??nez, and CD Pablo-Sa?nchez.
2011. The 1st DDIExtraction-2011 challenge task:
Extraction of Drug-Drug Interactions from biomedi-
cal texts. In Proceedings of the 1st Challenge task
on Drug-Drug Interaction Extraction (DDIExtraction
2011), pages 1?9, Huelva, Spain, September.
P Thomas, M Neves, I Solt, D Tikk, and U Leser.
2011. Relation extraction for drug-drug interactions
using ensemble learning. In Proceedings of the 1st
Challenge task on Drug-Drug Interaction Extraction
770
(DDIExtraction 2011), pages 11?18, Huelva, Spain,
September.
GD Zhou, J Su, J Zhang, and M Zhang. 2005. Ex-
ploring various knowledge in relation extraction. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics (ACL 2005), pages
427?434, Ann Arbor, Michigan, USA.
771
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 340?346,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
FBK: Exploiting Phrasal and Contextual Clues
for Negation Scope Detection
Md. Faisal Mahbub Chowdhury ? ?
? Fondazione Bruno Kessler (FBK-irst), Trento, Italy
? University of Trento, Italy
chowdhury@fbk.eu
Abstract
Automatic detection of negation cues along
with their scope and corresponding negated
events is an important task that could bene-
fit other natural language processing (NLP)
tasks such as extraction of factual information
from text, sentiment analysis, etc. This paper
presents a system for this task that exploits
phrasal and contextual clues apart from vari-
ous token specific features. The system was
developed for the participation in the Task 1
(closed track) of the *SEM 2012 Shared Task
(Resolving the Scope and Focus of Negation),
where it is ranked 3rd among the participating
teams while attaining the highest F1 score for
negation cue detection.
1 Introduction
Negation is a linguistic phenomenon that can al-
ter the meaning of a textual segment. While auto-
matic detection of negation expressions (i.e. cues)
in free text has been a subject of research interest
for quite some time (e.g. Chapman et al (2001),
Elkin et al (2005) etc), automatic detection of full
scope of negation is a relatively new topic (Morante
and Daelemans, 2009; Councill et al, 2010). Detec-
tion of negation cues, their scope and corresponding
negated events in free text could improve accuracy in
other natural language processing (NLP) tasks such
as extraction of factual information from text, senti-
ment analysis, etc (Jia et al, 2009; Councill et al,
2010).
In this paper, we present a system that was de-
veloped for the participation in the Scope Detection
task of the *SEM 2012 Shared Task1. The proposed
system exploits phrasal and contextual clues apart
from various token specific features. Exploitation
of phrasal clues is not new for negation scope de-
tection. But the way we encode this information
(i.e. the features for phrasal clues) is novel and dif-
fers completely from the previous work (Councill et
al., 2010; Morante and Daelemans, 2009). More-
over, the total number of features that we use is also
comparatively lower. Furthermore, to the best of our
knowledge, automatic negated event/property iden-
tification has not been explored prior to the *SEM
2012 Shared Task. So, our proposed approach for
this particular sub-task is another contribution of this
paper.
The remainder of this paper is organised as fol-
lows. First, we describe the scope detection task
as well as the accompanying datasets in Section 2.
Then in Section 3, we present how we approach the
task. Following that, in Section 4, various empiri-
cal results and corresponding analyses are discussed.
Finally, we summarize our work and discuss how the
system can be further improved in Section 5.
2 Task Description: Scope Detection
The Scope Detection task (Task 1) of *SEM 2012
Shared Task deals with intra-sentential (i.e. con-
text is single sentence) negations. According to
the guidelines of the task (Morante and Daelemans,
2012; Morante et al, 2011), the scope of a nega-
tion cue(s) is composed of all negated concepts and
negated event/property, if any. Negation cue(s) is
1http://www.clips.ua.ac.be/sem2012-st-neg/
340
Training Development Test
Total sentence 3644 787 1089
Negation sentences 848 144 235
Negation cues 984 173 264
Cues with scopes 887 168 249
Tokens in scopes 6929 1348 1805
Negated events 616 122 173
Table 1: Various statistics of the training, development
and test datasets.
not considered as part of the scope. Cues and scopes
may be discontinuous.
The organisers provided three sets of data ? train-
ing, development and test datasets, all consisting of
stories by Conan Doyle. The training dataset con-
tains Chapters 1-14 from The Hound of the
Baskervilles. While development dataset
contains The Adventures of Wisteria
Lodge. For testing, two other stories, The
Adventure of the Red Circle and The
Adventure of the Cardboard Box, were
released during the evaluation period of the shared
task. Table 1 shows various statistics regarding the
datasets.
In the training and development data, all occur-
rences of negation are annotated. For each negation
cue, the cue and corresponding scope are marked,
as well as the negated event/property, if any. The
data is provided in CoNLL-2005 Shared Task for-
mat. Table 2 shows an example of annotated data
where ?un? is the negation cue, ?his own conven-
tional appearance? is the scope, and ?conventional?
is the negated property.
The test data has a format similar to the training
data except that only the Columns 1?7 (as shown in
Table 2) are provided. Participating systems have to
output the remaining column(s).
During a random checking we have found at least
2 missing annotations2 in the development data. So,
there might be few wrong/missing annotations in the
other datasets, too.
There were two tracks in the task. For the closed
2Annotations for the following negation cues (and their cor-
responding scope/negated events) in the development data are
missing ? {cue: ?no?, token no.: 8, sentence no.: 237, chap-
ter: wisteria01} and {cue: ?never?, token no.: 3, sentence no.:
358, chapter: wisteria02}.
track, systems have to be built strictly with infor-
mation contained in the given training corpus. This
includes the automatic annotations that the organiz-
ers provide for different levels of analysis (POS tags,
lemmas and parse trees). For the open track, sys-
tems can be developed making use of any kind of
external tools and resources.
We participated in the closed track of the scope
detection task.
3 Our Approach
We approach the subtasks (i.e. cue, scope and
negated event detection) of the Task 1 as sequence
identification problems and train three different 1st
order Conditional Random Field (CRF) classifiers
(i.e. one for each of them) using the MALLET ma-
chine learning toolkit (McCallum, 2002). All these
classifiers use ONLY the information available in-
side the training corpus (i.e. training and develop-
ment datasets) as provided by the task organisers,
which is the requirement of the closed track.
3.1 Negation Cue Detection
At first, our system automatically collects a vocab-
ulary of all the positive tokens (i.e. those which are
not negation cues) of length greater than 3 charac-
ters, after excluding negation cue affixes (if any),
from the training data and uses them to extract fea-
tures that could be useful to identify potential nega-
tion cues which are subtokens (e.g. *un*able). We
also create a list of highly probable negation ex-
pressions (henceforth, NegExpList) from the train-
ing data based on frequencies. The list consists of
the following terms ? nor, neither, without, nobody,
none, nothing, never, not, no, nowhere, and non.
Negation cue subtokens are identified if the token
itself is predicted as a negation cue by the classi-
fier and has one of the following affixes that are col-
lected from the training data ? less, un, dis, im, in,
non, ir.
Lemmas are converted to lower case inside the
feature set. Additional post-processing is done to
annotate some obvious negation expressions that are
seen inside the training data but sometimes missed
by the classifier during prediction on the develop-
ment data. These expressions include neither, no-
body, save for, save upon, and by no means. A spe-
341
wisteria01 60 0 Our Our PRP$ (S(NP*
wisteria01 60 1 client client NN *)
wisteria01 60 2 looked look VBD (VP*
wisteria01 60 3 down down RB (ADVP*)
wisteria01 60 4 with with IN (PP*
wisteria01 60 5 a a DT (NP(NP*
wisteria01 60 6 rueful rueful JJ *
wisteria01 60 7 face face NN *)
wisteria01 60 8 at at IN (PP*
wisteria01 60 9 his his PRP$ (NP* his
wisteria01 60 10 own own JJ * own
wisteria01 60 11 unconventional unconventional JJ * un conventional conventional
wisteria01 60 12 appearance appearance NN *))))) appearance
Table 2: Example of the data provided for *SEM 2012 Shared Task.
Feature name Description
POSi Part-of-speech of tokeni
Lemmai Lemma form of tokeni
Lemmai?1 Lemma form of tokeni?1
hasNegPrefix If tokeni has a negation
prefix and is found inside the
automatically created vocabulary
hasNegSuffix If tokeni has a negation
suffix and is found inside the
automatically created vocabulary
matchesNegExp If tokeni is found in NegExpList
Table 3: Feature set for negation cue classifier
cial check is done for the phrase ?none the less?
which is marked as a non-negation expression inside
the training data.
Finally, a CRF model is trained using the col-
lected features (see Table 3) and used to predict
negation cue on test instance.
3.2 Scope and Negated Event Detection
Once the negation cues are identified, the next tasks
are to detect scopes of the cues and negated events
which are approached independently using separate
classifiers. If a sentence has multiple negation cues,
we create separate training/test instance of the sen-
tence for each of the cues.
Tables 4 and 5 show the feature sets that are used
to train classifiers. Both the feature sets exclusively
use various phrasal clues, e.g. whether the (clos-
est) NP, VP, S or SBAR containing the token un-
der consideration (i.e. tokeni) and that of the nega-
tion cue are different. Further phrasal clues that are
exploited include whether the least common phrase
of tokeni has no other phrase as child, and also
list of the counts of different common phrasal cat-
egories (starting from the root of the parse tree) that
contain tokeni and the cue. These latter two types
of phrasal clue features are found effective for the
negated event detection but not for scope detection.
We also use various token specific features (e.g.
lemma, POS, etc) and contextual features (e.g.
lemma of the 1st word of the corresponding sen-
tence, position of the token with respect to the cue,
presence of conjunction and special characters be-
tween tokeni and the cue, etc). Finally, new fea-
tures are created by combining different features of
the neighbouring tokens within a certain range of the
tokeni. The range values are selected empirically.
Once scopes and negated events are identified
(separately), the prediction output of all the three
classifiers are merged to produce the full negation
scope.
Initially, a number of features is chosen by doing
manual inspection (randomly) of the scopes/negated
events in the training data as well analysing syntac-
tic structures of the corresponding sentences. Some
of those features (e.g. POS of previous token for
scope detection) which are found (empirically) as
not useful for performance improvement have been
discarded.
342
Feature name: Description
Lemma1 Lemma of the 1st word
of the sentence
POSi Part-of-speech of tokeni
Lemmai Lemma of tokeni
Lemmai?1 Lemma of tokeni?1
isCue If tokeni is negation cue
isCueSubToken If a subtoken of tokeni
is negation cue
isCcBetCueAndCurTok If there is a conjunction
between tokeni and cue
isSpecCharBetCueAndCurTok If there is a
non-alphanumeric token
between tokeni and cue
Position Position of tokeni : before,
after or same w.r.t. the cue
isCueAndCurTokInDiffNP If tokeni and cue
belong to different NPs
isCueAndCurTokInDiffVP If tokeni and cue
belong to different VPs
isCueAndCurTokInDiffSorSBAR If tokeni and cue belong
to different S or SBAR
FeatureConjunctions New features by combining
those of tokeni?2 to tokeni+2
Table 4: Feature set for negation scope classifier. Bold
features are the phrasal clue features.
We left behind two verifications unintentionally
which should have been included. One of them is
to take into account whether a sentence is a fac-
tual statement or a question before negated event de-
tection. The other is to check whether a predicted
negated event is found inside the predicted scope of
the corresponding negation cue.
4 Results and Discussions
In this section, we discuss various empirical re-
sults on the development data and test data. De-
tails regarding the evaluation criteria are described
in Morante and Blanco (2012).
4.1 Results on the Development Dataset
Our feature sets are selected after doing a number of
experiments by combining various potential feature
types. In these experiments, the system is trained
on the training data and tested on development data.
Feature name Description
Lemma1 Lemma of the 1st word
of the sentence
POSi Part-of-speech of tokeni
Lemmai Lemma of tokeni
POSi?1 POS of tokeni?1
isCue If tokeni is negation cue
isCueSubToken If a subtoken of tokeni
is negation cue
isSpecCharBetCueAndCurTok If there is a
non-alphanumeric token
between tokeni and cue
IsModal If POS of tokeni is MD
IsDT If POS of tokeni is DT
isCueAndCurTokInDiffNP If tokeni and cue
belong to different NPs
isCueAndCurTokInDiffVP If tokeni and cue
belong to different VPs
isCueAndCurTokInDiffSorSBAR If tokeni and cue belong
to different S or SBAR
belongToSamePhrase If the least common phrase of
tokeni and cue do not
contain other phrase
CPcatBetCueAndCurTok All common phrase categories
(and their counts) that
contain tokeni and cue
FeatureConjunctions New features by combining
those of tokeni?3 to tokeni+1
Table 5: Feature set for negated event classifier. Bold
features are the phrasal clue features.
Due to time limitation we could not do parameter
tuning for CRF model training which we assume
could further improve the results.
Table 8 shows the results3 on the development
data using the feature sets described in Section 3.
There are two noticeable things in these results.
Firstly, there is a very high F1 score (93.29%) ob-
tained for negation cue identification. And secondly,
the precision obtained for scope detection (97.92%)
is very high as well.
Table 6 shows the results (of negated event iden-
3All the results reported in this paper, apart from the ones
on test data which are directly obtained from the organisers,
reported in this paper are computed using the official evaluation
script provided by the organisers.
343
TP FP FN Prec. Rec. F1
Using only 71 16 46 81.61 60.68 69.61
contextual and token
specific features
After adding phrasal 81 17 34 82.65 70.43 76.05
clue features
Table 6: Negated event detection results on development
data with and without the 5 phrasal clue feature types.
The results are obtained using gold annotation of nega-
tion cues. Note that, TP+FN is not the same. However, since these
results are computed using the official evaluation script, we are not sure
why there is this mismatch.
Using negation cues annotated by our system
TP FP FN Prec. Rec. F1
Scope detection 94 2 74 97.92 55.95 71.21
Event detection 63 19 51 76.83 55.26 64.28
Using gold annotations of negation cues
TP FP FN Prec. Rec. F1
Scope detection 103 0 65 100.00 61.31 76.02
Event detection 81 17 34 82.65 70.43 76.05
Table 7: Scope and negated event detection results on
development data with and without gold annotations of
negation cues. Note that, for negated events, TP+FN is not the same.
However, since these results are computed using the official evaluation
script, we are not sure why there is this mismatch.
tification) obtained before and after the usage of our
proposed 5 phrasal clue feature types (using gold an-
notation of negation cues). As we can see, there is a
significant improvement in recall (almost 10 points)
due to the usage of phrasal clues which ultimately
leads to a considerable increase (almost 6.5 points)
of F1 score.
4.2 Results on the Official Test Dataset
Table 9 shows official results of our system in the
*SEM 2012 Shared Task (closed track) of scope de-
tection, as provided by the organisers. It should be
noted that the test dataset is almost 1.5 times bigger
than the combined training corpus (i.e. training +
development data). Despite this fact, the results of
cue and scope detection on the test data are almost
similar as those on the development data. How-
ever, there is a sharp drop (almost 4 points lower F1
score) in negated event identification, primarily due
to lower precision. This resulted in a lower F1 score
(almost 4.5 points) for full negation identification.
4.3 Further Analyses of the Results and
Feature Sets
Our analyses of the empirical results (conducted
on the development data) suggest that negation cue
identification largely depends on the token itself
rather than its surrounding syntactic construction.
Although context (i.e. immediate neighbouring to-
kens) are also important, the significance of a vo-
cabulary of positive tokens (for the identification of
negation cue subtokens) and the list of negation cue
expressions is quite obvious. In a recently published
study, Morante (2010) listed a number of negation
cues and argued that their total number are actually
not exhaustive. We refrained from using the cues
listed in that paper (instead we built a list automati-
cally from the training data) since additional knowl-
edge/resource outside the training data was not al-
lowed for the closed track. But we speculate that
usage of such list of expressions as well as an exter-
nal dictionary of (positive) words can further boost
the high performance that we already achieved.
Since scope and negation event detection are de-
pendent on the correct identification of cues, we
have done separate evaluation on the development
data using the gold cues (instead of predicting the
cues first). As the results in Table 7 show, there is a
considerable increment in the results for both scope
and event detection if the correct annotation of cues
are available.
The general trend of errors that we have observed
in scope detection is that the more distant a token is
from the negation cue in the phrase structure tree (of
the corresponding sentence) the harder it becomes
for the classifier to predict whether the token should
be included in the scope or not. For example, in the
sentence ?I am not aware that in my whole life such
a thing has ever happened before.? of the devel-
opment data, the negation cue ?not? has scope over
the whole sentence. But the scope classifier fails to
include the last 4 words in the scope. Perhaps syn-
tactic dependency can provide complementary infor-
mation in such cases.
As for the negated event identification errors, the
majority of the prediction errors (on the develop-
ment data) occurred for verb and noun tokens which
are mostly immediately preceded by the negation
cue. Information of syntactic dependency should be
344
Gold System TP FP FN Prec. (%) Rec. (%) F1 (%)
Cues: 173 156 153 2 20 98.71 88.44 93.29
Scopes (cue match): 168 150 94 2 74 97.92 55.95 71.21
Scopes (no cue match): 168 150 94 2 74 97.92 55.95 71.21
Scope tokens (no cue match): 1348 1132 1024 108 324 90.46 75.96 82.58
Negated (no cue match): 122 90 63 19 51 76.83 55.26 64.28
Full negation: 173 156 67 2 106 97.10 38.73 55.37
Cues B: 173 156 153 2 20 98.08 88.44 93.01
Scopes B (cue match): 168 150 94 2 74 62.67 55.95 59.12
Scopes B (no cue match): 168 150 94 2 74 62.67 55.95 59.12
Negated B (no cue match): 122 90 63 19 51 70.00 55.26 61.76
Full negation B: 173 156 67 2 106 42.95 38.73 40.73
# Sentences: 787 # Negation sentences: 144 # Negation sentences with errors: 97
% Correct sentences: 87.55 % Correct negation sentences: 32.64
Table 8: Results on the development data. In the ?B? variant of the results, Precision = TP / System, instead of
Precision = TP / (TP + FP).
helpful to reduce such errors, too.
5 Conclusions
In this paper, we presented our approach for nega-
tion cue, scope and negated event detection task
(closed track) of *SEM 2012 Shared Task, where
our system ranked 3rd among the participating
teams for full negation detection while obtaining the
best F1 score for negation cue detection. Interest-
ingly, according to the results provided by the organ-
isers, our system performs better than all the systems
of the open track except one (details of these results
are described in (Morante and Blanco, 2012)).
The features exploited by our system include
phrasal and contextual clues as well as token spe-
cific information. Empirical results show that the
system achieves very high precision for scope de-
tection. The results also imply that the novel phrasal
clue features exploited by our system improve iden-
tification of negated events significantly.
We believe the system can be further improved
in a number of ways. Firstly, this can be done by
incorporating linguistic knowledge as described in
Morante (2010). Secondly, we did not take into ac-
count whether a sentence is a factual statement or
a question before negated event detection. We also
did not check whether a predicted negated event is
found inside the predicted scope of the correspond-
ing negation cue. These verifications should in-
crease the results more. Finally, previous work re-
ported that usage of syntactic dependency informa-
tion helps in scope detection (Councill et al, 2010).
Hence, this could be another possible direction for
improvement.
Acknowledgments
The author would like to thank Alberto Lavelli and
the anonymous reviewers for various useful feed-
back regarding the manuscript.
References
WW Chapman, W Bridewell, P Hanbury, GF Cooper,
and BG Buchanan. 2001. A Simple Algorithm for
Identifying Negated Findings and Diseases in Dis-
charge Summaries. Journal of Biomedical Informat-
ics, 34(5):301?10.
I Councill, R McDonald, and L Velikovich. 2010. Whats
Great and Whats Not: Learning to Classify the Scope
of Negation for Improved Sentiment Analysis. In Pro-
ceedings of the Workshop on Negation and Speculation
in Natural Language Processing, pages 51?59, Upp-
sala, Sweden.
P Elkin, S Brown, B Bauer, C Husser, W Carruth,
L Bergstrom, and D Wahner-Roedler. 2005. A con-
trolled trial of automated classification of negation
from clinical notes. BMC Medical Informatics and
Decision Making, 5(1):13.
L Jia, C Yu, and W Meng. 2009. The Effect of Negation
on Sentiment Analysis and Retrieval Effectiveness. In
345
Gold System TP FP FN Prec. (%) Rec. (%) F1 (%)
Cues: 264 263 241 17 23 93.41 91.29 92.34
Scopes (cue match): 249 249 145 18 104 88.96 58.23 70.39
Scopes (no cue match): 249 249 145 18 104 88.96 58.23 70.39
Scope tokens (no cue match): 1805 1825 1488 337 317 81.53 82.44 81.98
Negated (no cue match): 173 154 93 52 71 64.14 56.71 60.20
Full negation: 264 263 96 17 168 84.96 36.36 50.93
Cues B: 264 263 241 17 23 91.63 91.29 91.46
Scopes B (cue match): 249 249 145 18 104 58.23 58.23 58.23
Scopes B (no cue match): 249 249 145 18 104 58.23 58.23 58.23
Negated B (no cue match): 173 154 93 52 71 60.39 56.71 58.49
Full negation B: 264 263 96 17 168 36.50 36.36 36.43
# Sentences: 1089 # Negation sentences: 235 # Negation sentences with errors: 151
% Correct sentences: 84.94 % Correct negation sentences: 35.74
Table 9: Results on the *SEM 2012 Shared Task (closed track) test data provided by the organisers. In the ?B? variant
of the results, Precision = TP / System, instead of Precision = TP / (TP + FP).
Proceedings of the 18th ACM Conference on Informa-
tion and Knowledge Management (CIKM 2009), pages
1827?1830, Hong Kong, China.
AK McCallum. 2002. MALLET: A machine learning
for language toolkit. http://mallet.cs.umass.edu,.
R Morante and E Blanco. 2012. *SEM 2012 Shared
Task: Resolving the Scope and Focus of Negation.
In Proceedings of the First Joint Conference on Lexi-
cal and Computational Semantics (*SEM 2012), Mon-
treal, Canada.
R Morante and W Daelemans. 2009. A Metalearning
Approach to Processing the Scope of Negation. In
Proceedings of CoNLL 2009, pages 28?36, Boulder,
Colorado, USA.
R Morante and W Daelemans. 2012. ConanDoyle-neg:
Annotation of Negation in Conan Doyle Stories. In
Proceedings of the 8th International Conference on
Language Resources and Evaluation (LREC 2012), Is-
tanbul, Turkey.
R Morante, S Schrauwen, and W Daelemans. 2011. An-
notation of Negation Cues and Their Scope Guidelines
v1.0. Technical Report CLiPS Technical Report 3,
CLiPS, Antwerp, Belgium.
R Morante. 2010. Descriptive Analysis of Negation
Cue in Biomedical Texts. In Proceedings of the 7th
International Conference on Language Resources and
Evaluation (LREC 2010), Malta.
346
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 351?355, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
FBK-irst : A Multi-Phase Kernel Based Approach for Drug-Drug
Interaction Detection and Classification that Exploits Linguistic Information
Md. Faisal Mahbub Chowdhury ? ? and Alberto Lavelli ?
? Fondazione Bruno Kessler (FBK-irst), Italy
? University of Trento, Italy
fmchowdhury@gmail.com, lavelli@fbk.eu
Abstract
This paper presents the multi-phase relation
extraction (RE) approach which was used for
the DDI Extraction task of SemEval 2013. As
a preliminary step, the proposed approach in-
directly (and automatically) exploits the scope
of negation cues and the semantic roles of in-
volved entities for reducing the skewness in
the training data as well as discarding possible
negative instances from the test data. Then, a
state-of-the-art hybrid kernel is used to train
a classifier which is later applied on the in-
stances of the test data not filtered out by the
previous step. The official results of the task
show that our approach yields an F-score of
0.80 for DDI detection and an F-score of 0.65
for DDI detection and classification. Our sys-
tem obtained significantly higher results than
all the other participating teams in this shared
task and has been ranked 1st.
1 Introduction
Drug-drug interaction (DDI) is a condition when one
drug influences the level or activity of another. The
extraction of DDIs has significant importance for
public health safety. It was reported that about 2.2
million people in USA, age 57 to 85, were taking
potentially dangerous combinations of drugs (Lan-
dau, 2009). Another report mentioned that deaths
from accidental drug interactions rose by 68 percent
between 1999 and 2004 (Payne, 2007). The DDIEx-
traction 2011 and DDIExtraction 2013 shared tasks
underline the importance of DDI extraction.
The DDIExtraction 2013 task concerns the recog-
nition of drugs and the extraction of drug-drug in-
teractions from biomedical literature. The dataset of
the shared task is composed by texts from the Drug-
Bank database as well as MedLine abstracts in or-
der to deal with different type of texts and language
styles. Participants were asked to not only extract
DDIs but also classify them into one of four pre-
defined classes: advise, effect, mechanism and int.
A detailed description of the task settings and data
can be found in Segura-Bedmar et al (2013).
The system that we used in this shared task
combines various techniques proposed in our re-
cent research activities for relation extraction (RE)
(Chowdhury and Lavelli, 2012a; Chowdhury and
Lavelli, 2012b; Chowdhury and Lavelli, 2013).1
2 DDI Detection
Our system performs DDI detection and classifica-
tion in two separate steps. In this section, we explain
how DDI detection (i.e. whether two drug mentions
participate in a DDI) is accomplished. DDI classifi-
cation will be described in Section 3.
There are three phases for DDI detection: (i) dis-
card less informative sentences, (ii) discard less in-
formative instances, and (iii) train the system (a sin-
gle model regardless of DDI types) on the remaining
training instances and identify possible DDIs from
the remaining test instances. These phases are de-
scribed below.
2.1 Exploiting the scope of negations for
sentence filtering
Negation is a linguistic phenomenon where a nega-
tion cue (e.g. not) can alter the meaning of a partic-
1Available in https://github.com/fmchowdhury/HyREX.
351
ular text segment or of a fact. This text segment (or
fact) is said to be inside the scope of such negation
(cue). In one of our recent papers (Chowdhury and
Lavelli, 2013), we proposed how to exploit the scope
of negations for RE. We hypothesize that a classi-
fier trained solely on features related to the scope of
negations can be used to pro-actively filter groups
of instances which are less informative and mostly
negative.
To be more precise, we propose to train a classi-
fier (which will be applied before using the kernel
based RE classifier mentioned in Section 2.3) that
would check whether all the target entity mentions
inside a sentence along with possible relation clues
(or trigger words), if any, fall (directly or indirectly)
under the scope of a negation cue. If such a sentence
is found, then it would be identified as less informa-
tive and discarded (i.e. the candidate mention pairs
inside such sentence would not be considered). Dur-
ing training (and testing), we group the instances by
sentences. Any sentence that contains at least one
relation of interest is considered by the less infor-
mative sentence (LIS) classifier as a positive (train-
ing/test) instance. The remaining sentences are con-
sidered as negative instances.
We use a number of features related to negation
scopes to train a binary SVM classifier that filters out
less informative sentences. These features are basi-
cally contextual and shallow linguistic features. Due
to space limitation, we do not report these features
here. Interested readers are referred to Chowdhury
and Lavelli (2013).
The objective of the classifier is to decide whether
all target entity mentions as well as any possible ev-
idence inside the corresponding sentence fall under
the scope of a negation cue in such a way that the
sentence is unlikely to contain the relation of in-
terest (e.g. DDI). If the classifier finds such a sen-
tence, then it is assigned the negative class label. At
present, we focus only on the first occurrence of the
negation cues ?no?, ?n?t? or ?not?. These cues usu-
ally occur more frequently and generally have larger
negation scope than other negation cues.
The LIS classifier is trained using a linear SVM
classifier. Its hyper-parameters are tuned during
training for obtaining maximum recall. In this way
we minimize the number of false negatives (i.e. sen-
tences that contain relations but are wrongly filtered
out). Once the classifier is trained using the training
data, we apply it on both the training and test data.
However, if the recall of the LIS classifier is found
to be below a threshold value (we set it to 70.0) dur-
ing cross validation on the training data of a corpus,
it is not used for sentence filtering on such corpus.
Any (training/test) sentence that is classified as
negative is considered as a less informative sentence
and is filtered out. In other words, such a sentence is
not considered for RE. However, it should be noted
that, if such a sentence is a test sentence and it con-
tains positive RE instances, then all these filtered
positive RE instances are automatically considered
as false negatives during the calculation of RE per-
formance.
We rule out sentences (i.e. we consider them nei-
ther positive nor negative instances for training the
classifier that filters less informative sentences) dur-
ing both training and testing if any of the following
conditions holds:
? The sentence contains less than two target en-
tity mentions (such sentence would not contain
the relation of interest anyway).
? It has any of the following phrases ? ?not
recommended?, ?should not be? or ?must not
be?.2
? There is no ?no?, ?n?t? or ?not? in the sentence.
? No target entity mention appears in the sen-
tence after ?no?, ?n?t? or ?not?.
2.2 Discarding instances using semantic roles
and contextual evidence
For identifying less informative negative instances,
we exploit static (i.e. already known, heuristically
motivated) and dynamic (i.e. automatically col-
lected from the data) knowledge which has been
proposed in Chowdhury and Lavelli (2012b). This
knowledge is described by the following criteria:
? C1: If each of the two entity mentions (of a
candidate pair) has anti-positive governors (see
Section 2.2.1) with respect to the type of the
relation, then they are not likely to be in a given
relation.
2These expressions often provide clues that one of the drug
entity mentions negatively influences the level of activity of the
other.
352
? C2: If two entity mentions in a sentence refer
to the same entity, then it is unlikely that they
would have a relation between themselves.
? C3: If a mention is the abbreviation of another
mention (i.e. they refer to the same entity), then
they are unlikely to be in a relation.
Criteria C2 and C3 (static knowledge) are quite
intuitive. For criterion C1, we construct on the fly a
list of anti-positive governors (dynamic knowledge)
taken from the training data and use them for de-
tecting pairs that are unlikely to be in relation. As
for criterion C2, we simply check whether two men-
tions have the same name and there is more than one
character between them. For criterion C3, we look
for any expression of the form ?Entity1 (Entity2)?
and consider ?Entity2? as an abbreviation or alias of
?Entity1?.
The above criteria are used to filter instances from
both training and test data. Any positive test instance
filtered out by these criteria is automatically consid-
ered as a false negative during the calculation of RE
performance.
2.2.1 Anti-positive governors
The semantic roles of the entity mentions may in-
directly contribute either to relate or not to relate
them in a particular relation type (e.g. PPI) in the
corresponding context. To put it differently, the se-
mantic roles of two mentions in the same context
could provide an indication whether the relation of
interest does not hold between them. Interestingly,
the word on which a certain entity mention is (syn-
tactically) dependent (along with the dependency
type) could often provide a clue of the semantic role
of such mention in the corresponding sentence.
Our goal is to automatically identify the words
(if any) that tend to prevent mentions, which are di-
rectly dependent on those words, from participating
in a certain relation of interest with any other men-
tion in the same sentence. We call such words anti-
positive governors and assume that they could be ex-
ploited to identify negative instances (i.e. negative
entity mention pairs) in advance. Interested readers
are referred to Chowdhury and Lavelli (2012b) for
example and description of how anti-positive gov-
ernors are automatically collected from the training
data.
2.3 Hybrid Kernel based RE Classifier
As RE classifier we use the following hybrid kernel
that has been proposed in Chowdhury and Lavelli
(2013). It is defined as follows:
KHybrid (R1, R2) = KHF (R1, R2) + KSL
(R1, R2) + w * KPET (R1, R2)
where KHF is a feature based kernel (Chowdhury
and Lavelli, 2013) that uses a heterogeneous set
of features, KSL is the Shallow Linguistic (SL)
kernel proposed by Giuliano et al (2006), and
KPET stands for the Path-enclosed Tree (PET) ker-
nel (Moschitti, 2004). w is a multiplicative constant
that allows the hybrid kernel to assign more (or less)
weight to the information obtained using tree struc-
tures depending on the corpus. We exploit the SVM-
Light-TK toolkit (Moschitti, 2006; Joachims, 1999)
for kernel computation. The parameters are tuned
by doing 5-fold cross validation on the training data.
3 DDI Type Classification
The next step is to classify the extracted DDIs into
different categories. We train 4 separate models for
each of the DDI types (one Vs all) to predict the
class label of the extracted DDIs. During this train-
ing, all the negative instances from the training data
are removed. The filtering techniques described in
Sections 2.1 and 2.2 are not used in this stage.
The extracted DDIs are assigned a default DDI
class label. Once the above models are trained, they
are applied on the extracted DDIs from the test data.
The class label of the model which has the highest
confidence score for an extracted DDI instance is as-
signed to such instance.
4 Data Pre-processing and Experimental
Settings
The Charniak-Johnson reranking parser (Charniak
and Johnson, 2005), along with a self-trained
biomedical parsing model (McClosky, 2010), has
been used for tokenization, POS-tagging and pars-
ing of the sentences. Then the parse trees are pro-
cessed by the Stanford parser (Klein and Manning,
2003) to obtain syntactic dependencies. The Stan-
ford parser often skips some syntactic dependencies
in output. We use the rules proposed in Chowdhury
353
and Lavelli (2012a) to recover some of such depen-
dencies. We use the same techniques for unknown
characters (if any) as described in Chowdhury and
Lavelli (2011).
Our system uses the SVM-Light-TK toolkit3
(Moschitti, 2006; Joachims, 1999) for computation
of the hybrid kernels. The ratio of negative and posi-
tive examples has been used as the value of the cost-
ratio-factor parameter. The SL kernel is computed
using the jSRE tool4.
The KHF kernel can exploit non-target entities
to extract important clues (Chowdhury and Lavelli,
2013). So, we use a publicly available state-of-the-
art NER system called BioEnEx (Chowdhury and
Lavelli, 2010) to automatically annotate both the
training and the test data with disease mentions.
The DDIExtraction 2013 shared task data include
two types of texts: texts taken from the DrugBank
database and texts taken from MedLine abstracts.
During training we used both types together.
5 Experimental Results
Table 1 shows the results of 5-fold cross validation
for DDI detection on the training data. As we can
see, the usage of the LIS and LII filtering techniques
improves both precision and recall.
We submitted three runs for the DDIExtraction
2013 shared task. The only difference between the
three runs concerns the default class label (i.e. the
class chosen when none of the separate models as-
signs a class label to a predicted DDI). Such default
class label is ?int?, ?effect? and ?mechanism? for
run 1, 2 and 3 respectively. According to the offi-
cial results provided by the task organisers, our best
result was obtained by run 2 (shown in Table 2).
According to the official results, the performance
for ?advise? is very low (F1 0.29) in MedLine texts,
while the performance for ?int? is comparatively
much higher (F1 0.57) with respect to the one of the
other DDI types. In comparison, the performance
for ?int? is much lower (F1 0.55) in DrugBank texts
with respect to the one of the other DDI types.
In MedLine test data, the number of ?effect? (62)
and ?mechanism? (24) DDIs is much higher than
that of ?advise? (7) and ?int? (2). On the other
3http://disi.unitn.it/moschitti/Tree-Kernel.htm
4http://hlt.fbk.eu/en/technology/jSRE
P R F1
KHybrid 0.66 0.80 0.72
LIS filtering + KHybrid 0.67 0.80 0.73
LIS filtering + LII filtering 0.68 0.82 0.74
+ KHybrid
Table 1: Comparison of results for DDI detection on the
training data using 5-fold cross validation. Parameter tun-
ing is not done during these experiments.
P R F1
All text
DDI detection only 0.79 0.81 0.80
Detection and Classification 0.65 0.66 0.65
DrugBank text
DDI detection only 0.82 0.84 0.83
Detection and Classification 0.67 0.69 0.68
MedLine text
DDI detection only 0.56 0.51 0.53
Detection and Classification 0.42 0.38 0.40
Table 2: Official results of the best run (run 2) of our
system in the DDIExtraction 2013 shared task.
hand, in DrugBank test data, the different DDIs are
more evenly distributed ? ?effect? (298), ?mecha-
nism? (278), ?advise? (214) and ?int? (94).
Initially, it was not clear to us why our system (as
well as other participants) achieves so much higher
results on the DrugBank sentences in comparison to
MedLine sentences. Statistics of the average num-
ber of words show that the length of the two types
of training sentences are substantially similar (Drug-
Bank : 21.2, MedLine : 22.3). It is true that the num-
ber of the training sentences for the former is almost
5.3 times higher than the latter. But it could not be
the main reason for such high discrepancies.
So, we turned our attention to the presence of the
cue words. In the 4,683 sentences of the DrugBank
training set (which have at least one drug mention),
we found that the words ?increase? and ?decrease?
are present in 721 and 319 sentences respectively.
While in the 877 sentences of the MedLine train-
ing set (which have at least one drug mention), we
found that the same words are present in only 67
and 40 sentences respectively. In other words, the
presence of these two important cue words in the
354
DrugBank sentences is twice more likely than that
in the MedLine sentences. We assume similar obser-
vations might be also possible for other cue words.
Hence, this is probably the main reason why the re-
sults are so much better on the DrugBank sentences.
6 Conclusion
In this paper, we have described a novel multi-phase
RE approach that outperformed all the other partic-
ipating teams in the DDI Detection and Classifica-
tion task at SemEval 2013. The central component
of the proposed approach is a state-of-the-art hybrid
kernel. Our approach also indirectly (and automat-
ically) exploits the scope of negation cues and the
semantic roles of the involved entities.
Acknowledgments
This work is supported by the project ?eOnco - Pervasive
knowledge and data management in cancer care?. The
authors would like to thank Alessandro Moschitti for his
help in the use of SVM-Light-TK.
References
E Charniak and M Johnson. 2005. Coarse-to-fine n-best
parsing and MaxEnt discriminative reranking. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2005).
MFM Chowdhury and A Lavelli. 2010. Disease mention
recognition with specific features. In Proceedings of
the 2010 Workshop on Biomedical Natural Language
Processing, pages 83?90, Uppsala, Sweden, July.
MFM Chowdhury and A Lavelli. 2011. Drug-drug inter-
action extraction using composite kernels. In Proceed-
ings of the 1st Challenge task on Drug-Drug Interac-
tion Extraction (DDIExtraction 2011), pages 27?33,
Huelva, Spain, September.
MFM Chowdhury and A Lavelli. 2012a. Combining tree
structures, flat features and patterns for biomedical re-
lation extraction. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL 2012), pages 420?
429, Avignon, France, April.
MFM Chowdhury and A Lavelli. 2012b. Impact of Less
Skewed Distributions on Efficiency and Effectiveness
of Biomedical Relation Extraction. In Proceedings of
the 24th International Conference on Computational
Linguistics (COLING 2012), Mumbai, India, Decem-
ber.
MFM Chowdhury and A Lavelli. 2013. Exploiting the
Scope of Negations and Heterogeneous Features for
Relation Extraction: A Case Study for Drug-Drug In-
teraction Extraction. In Proceedings of the 2013 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technology (NAACL 2013), Atlanta, USA, June.
C Giuliano, A Lavelli, and L Romano. 2006. Exploit-
ing shallow linguistic information for relation extrac-
tion from biomedical literature. In Proceedings of the
11th Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL 2006),
pages 401?408.
T Joachims. 1999. Making large-scale support vec-
tor machine learning practical. In Advances in ker-
nel methods: support vector learning, pages 169?184.
MIT Press, Cambridge, MA, USA.
D Klein and C Manning. 2003. Accurate unlexicalized
parsing. In Proceedings of the 41st Annual Meeting
of the Association for Computational Linguistics (ACL
2003), pages 423?430, Sapporo, Japan.
E Landau. 2009. Jackson?s death raises ques-
tions about drug interactions [Published in CNN;
June 26, 2009]. http://edition.cnn.
com/2009/HEALTH/06/26/jackson.drug.
interaction.caution/index.html.
D McClosky. 2010. Any Domain Parsing: Automatic
Domain Adaptation for Natural Language Parsing.
Ph.D. thesis, Department of Computer Science, Brown
University.
A Moschitti. 2004. A study on convolution kernels for
shallow semantic parsing. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics, ACL ?04, Barcelona, Spain.
A Moschitti. 2006. Making tree kernels practical for nat-
ural language learning. In Proceedings of 11th Confer-
ence of the European Chapter of the Association for
computational Linguistics (EACL 2006), pages 113?
120, Trento, Italy.
JW Payne. 2007. A Dangerous Mix [Published
in The Washington Post; February 27, 2007].
http://www.washingtonpost.com/
wp-dyn/content/article/2007/02/23/
AR2007022301780.html.
I Segura-Bedmar, P Mart??nez, and M Herrero-Zazo.
2013. SemEval-2013 task 9: Extraction of drug-drug
interactions from biomedical texts. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion (SemEval 2013), Atlanta, USA, June.
355
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 466?470, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
FBK: Sentiment Analysis in Twitter with Tweetsted
Md. Faisal Mahbub Chowdhury
FBK and University of Trento, Italy
fmchowdhury@gmail.com
Marco Guerini
Trento RISE, Italy
marco.guerini@trentorise.eu
Sara Tonelli
FBK, Trento, Italy
satonelli@fbk.eu
Alberto Lavelli
FBK, Trento, Italy
lavelli@fbk.eu
Abstract
This paper presents the Tweetsted system im-
plemented for the SemEval 2013 task on Sen-
timent Analysis in Twitter. In particular, we
participated in Task B on Message Polar-
ity Classification in the Constrained setting.
The approach is based on the exploitation of
various resources such as SentiWordNet and
LIWC. Official results show that our approach
yields a F-score of 0.5976 for Twitter mes-
sages (11th out of 35) and a F-score of 0.5487
for SMS messages (8th out of 28 participants).
1 Introduction
Microblogging is currently a very popular commu-
nication tool where millions of users share opinions
on different aspects of life. For this reason it is a
valuable source of data for opinion mining and sen-
timent analysis.
Working with such type of texts presents chal-
lenges for NLP beyond those typically encountered
when dealing with more traditional texts, such as
newswire data. Tweets are short, the language used
is very informal, with creative spelling and punctua-
tion, misspellings, slang, new words, URLs, genre-
specific terminology and abbreviations, and #hash-
tags. These characteristics need to be handled with
specific approaches.
This paper presents the approach adopted for the
SemEval 2013 task on Sentiment Analysis in Twit-
ter, in particular Task B on Message Polarity Clas-
sification in the Constrained setting (i.e., using the
provided training data only).
The goal of Task B on Message Polarity Classi-
fication is the following: given a message, decide
whether it expresses a positive, negative, or neutral
sentiment. For messages conveying both a positive
and a negative sentiment, whichever is the stronger
sentiment should be chosen.
Two modalities are possible: (1) Constrained (us-
ing the provided training data only; other resources,
such as lexica, are allowed; however, it is not al-
lowed to use additional tweets/SMS messages or ad-
ditional sentences with sentiment annotations); and
(2) Unconstrained (using additional data for train-
ing, e.g., additional tweets/SMS messages or addi-
tional sentences annotated for sentiment). We par-
ticipated in the Constrained modality.
We adopted a supervised machine learning (ML)
approach based on various contextual and seman-
tic features. In particular, we exploited resources
such as SentiWordNet (Esuli and Sebastiani, 2006),
LIWC (Pennebaker and Francis, 2001), and the lex-
icons described in Mohammad et al (2009).
Critical features include: whether the mes-
sage contains intensifiers, adjectives, interjections,
presence of positive or negative emoticons, pos-
sible message polarity based on SentiWordNet
scores (Esuli and Sebastiani, 2006; Gatti and
Guerini, 2012), scores based on LIWC cate-
gories (Pennebaker and Francis, 2001), negated
words, etc.
2 System Description
Our supervised ML-based approach relies on Sup-
port Vector Machines (SVMs). The SVM imple-
mentation used in the system is LIBSVM (Chang
466
and Lin, 2001) for training SVM models and test-
ing. Moreover, in the preprocessing phase we used
TweetNLP (Owoputi et al, 2013), a POS tagger ex-
plicitly tailored for working on tweets.
We adopted a 2 stage approach: (1) during stage
1, we performed a binary classification of messages
according to the classes neutral vs subjective; (2)
in stage 2, we performed a binary classification of
subjective messages according to the classes positive
vs negative. We performed various experiments on
the training and development sets exploring the use
of different features (see Section 2.1) to find the best
configurations for the official submission.
2.1 Feature list
We implement several features divided into three
groups: contextual features, semantic features from
context and semantic features from external re-
sources. The complete list is reported in Table 1.
Contextual features are features computed by
considering only the tokens in the tweets/SMS and
the associated part of speech.
Semantic Features from Context are features
based on words polarity. Emoticons were recog-
nized through a list of emoticons extracted from
Wikipedia1 and then manually labeled as positive or
negative. Negated words (feature n. 18) are any to-
ken occurring between n?t, not, no and a comma, ex-
cluding those tagged as function words. Feature n.
19 captures tokens (or sequences of tokens) labeled
with a positive or negative polarity in the resource
described in Mohammad et al (2009). The intensi-
fiers considered for Feature n. 20 have been identi-
fied by implementing a simple algorithm that detects
tokens containing anomalously repeated characters
(e.g. happyyyyy). Feature n. 21 was computed by
training the system on the training data and predict-
ing labels for the test data, and then using these la-
bels as new features to train the system again.
Semantic Features from external resources in-
clude word classes from the Linguistic Inquiry
and Word Count (LIWC), a tool that calculates
the degree to which people use different cate-
gories of words related to psycholinguistic pro-
cesses (Pennebaker and Francis, 2001). LIWC in-
1http://en.wikipedia.org/wiki/List_of_
emoticons
cludes about 2,200 words and stems grouped into 70
broad categories relevant to psychological processes
(e.g., EMOTION, COGNITION). Sample words are
shown in Table 2.
For each non-zero valued LIWC category of a cor-
responding tweet/SMS, we added a feature for that
category and used the category score as the value
of that feature. We call this LWIC string feature.
Alternatively, we also added a separate feature for
each non-zero valued LIWC category and set 1 as
the value of that feature. This feature is called LWIC
boolean.
We also used words prior polarity - i.e. if a word
out of context evokes something positive or nega-
tive. For this, we relied on SentiWordNet, a broad-
coverage resource that provides polarities for (al-
most) every word. Since words can have multi-
ple senses, we compute the prior polarity of a word
starting from the polarity of each sense and returning
its polarity strength as an index between -1 and 1.
We tested 14 formulae that combine posterior polar-
ities in different ways to obtain a word prior polarity,
as reported in (Gatti and Guerini, 2012).
For the SWNscoresMaximum feature, we select
the prior polarity of the word in a tweet/SMS hav-
ing the maximum absolute score among all words
(of that tweet/SMS). For SWNscoresPolarityCount,
we select the polarity (positive, negative or neutral)
that is assigned to the majority of the words. As
for SWNscoresSum, it corresponds to the sum of
the prior polarities associated with all words in the
tweet/SMS.
3 Experimental Setup
In order to select the best performing feature set,
we carried out several 5-fold cross validation ex-
periments on the training data. We report in Table
3 the best performing feature set. In particular, we
adopted a 2 stage approach:
1. during the first stage we performed a binary
classification of messages according to the
classes neutral vs subjective;
2. in the second stage, we performed a binary
classification of subjective messages according
to the classes positive vs negative.
We opted for a two stage binary classification ap-
proach, since we observed that it produces slightly
467
Contextual Features
1. noOfAdjectives num
2. adjective list string
3. interjection list string
4. firstInterj string
5. lastInterj string
6. bigramList string
7. beginsWithRT boolean
8. hasRTinMiddle boolean
9. endsWithLink boolean
10. endsWithHashtag boolean
11. hasQuestion boolean
Semantic Features from Context
12. noOfPositiveEmoticons num
13. noOfNegativeEmoticons num
14. beginsWithPosEmoticon boolean
15. beginsWithNegEmoticon boolean
16. endsWithPosEmoticon boolean
17. endsWithNegEmoticon boolean
18. negatedWords string
19. indexOfChunksWithPolarity string
20. containsIntensifier boolean
21. labelPredictedBySystem pos./neg./neut.
Semantic Features from External Resources
22. LIWC string string
23. LIWC boolean string
24. SWNscoresMaximum pos./neg./neut.
25. SWNscoresPolarityCount pos./neg./neut.
26. SWNscoresSum pos./neg./neut.
Table 1: Complete feature list.
LABEL Sample words
CERTAIN all, very, fact*, exact*, certain*, completely
DISCREP but, if, expect*, should
TENTAT or, some, may, possib*, probab*
SENSES observ*, discuss*, shows, appears
SELF we, our, I, us
SOCIAL discuss*, interact*, suggest*, argu*
OPTIM best, easy*, enthus*, hope, pride
ANGER hate, kill, annoyed
INHIB block, constrain, stop
Table 2: Word categories along with sample words
better results than a single stage multi-class ap-
proach (i.e. neutral vs positive vs negative).2 Dif-
ferent combinations of classifiers were explored ob-
taining comparable results. Here we will report only
2The average F-scores (pos and neg) for two stage and single
stage approaches obtained using the official scorer, by training
on the training data and testing on the development data, are
0.5682 and 0.5611 respectively.
the best results.
STAGE 1. The best result for stage (1), neutral vs
subjective, obtained with 5-fold cross validation on
training set only, accounts for an accuracy of 69.6%.
Instead, the best result for stage (1), obtained with
training on training data and testing on development
data, accounts for an accuracy of 72.67%.
The list of best features is reported in Table 3.
Feature selection was performed by starting from a
small set of basic features, and then by adding the
remaining features incrementally.
Contextual Features
2. adjective list string
3. interjection list string
5. lastInterj string
Semantic Features from Context
12. noOfPositiveEmoticons num
13. noOfNegativeEmoticons num
18. negatedWords string
19. indexOfChunksWithPolarity string
20. containsIntensifier boolean
Semantic Features from external resources
23. LIWC boolean string
24. SWNscoresMaximum posi./neg./neut.
Table 3: Best performing feature set.
STAGE 2. In stage (2), positive vs negative, we
started from the best feature set obtained from stage
(1) and added the remaining features one by one in-
crementally. In this case, we kept SWNscoresMaxi-
mum without testing again other formulae; in partic-
ular, to compute words prior polarity, we also kept
the first sense approach, that assigns to every word
the SWN score of its most frequent sense and proved
to be the most discriminative in the first stage neutral
vs. subjective. We found that none of the feature sets
produced better results than that obtained using the
best feature set selected from stage (1). So, the best
feature set for stage (2) is unchanged. We trained
the system on the training data and tested it on the
development data, achieving an accuracy of 80.67%.
4 Evaluation
The SemEval task organizers (Wilson et al, 2013)
provided two test sets on which the systems were
to be evaluated: one included Twitter messages, i.e.
the same type of texts included in the training set,
468
while the other comprised SMS messages, i.e. texts
having more or less the same length as the Twitter
data but (supposedly) a different style. We applied
the same model, trained both on the training and the
development set, on the two types of data, without
any specific adaptation.
The Twitter test set was composed of 3,813
tweets. Official results show that our approach
yields an F-score of 0.5976 for Twitter messages
(11th out of 35), while the best performing system
obtained an F-score of 0.6902. The confusion ma-
trix is reported in Table 4, while the score details
in Table 5. The latter table shows that our system
achieves the lowest results on negative tweets, both
in terms of precision and of recall.
gs/pred positive negative neutral
positive 946 101 525
negative 90 274 237
neutral 210 70 1360
Table 4: Confusion matrix for Twitter task
class prec recall F-score
positive 0.7592 0.6018 0.6714
negative 0.6157 0.4559 0.5239
neutral 0.6409 0.8293 0.7230
average(pos and neg) 0.5976
Table 5: Detailed results for Twitter task
The SMS test set for the competition was com-
posed of 2,094 SMS. Official results provided by the
task organizers show that our approach yields an F-
score of 0.5487 for SMS messages (8th out of 28
participants), while the best performing system ob-
tained an F-score of 0.6846. The confusion matrix
is reported in Table 6, while the score details in Ta-
ble 7. Also in this case the recognition of negative
messages achieves by far the poorest performance.
A comparison of the results on the two test sets
shows that, as expected, our system performs bet-
ter on tweets than on SMS. However, precision
achieved by the system on neutral SMS is 0.12
points better on text messages than on tweets.
Interestingly, it appears from the results in Ta-
bles 5 and 7 (and from the distribution of the classes
in the data sets) that there may be a correlation be-
tween the number of tweets/SMS for a particular
class and the performance obtained for such class.
We plan to further investigate this issue.
gs/pred positive negative neutral
positive 320 44 128
negative 66 171 157
neutral 208 64 936
Table 6: Confusion matrix for SMS task
class prec recall F-score
positive 0.5387 0.6504 0.5893
negative 0.6129 0.4340 0.5082
neutral 0.7666 0.7748 0.7707
average(pos and neg) 0.5487
Table 7: Detailed results for SMS task
5 Conclusions
In this paper, we presented Tweetsted, the system de-
veloped by FBK for the SemEval 2013 task on Sen-
timent Analysis. We trained a classifier performing
a two-step binary classification, i.e. first neutral vs.
subjective data, and then positive vs. negative ones.
We implemented a set of features including contex-
tual and semantic ones. We also integrated in our
feature representation external knowledge from Sen-
tiWordNet, LIWC and the resource by Mohammad
et al (2009). On both test sets (i.e., Twitter mes-
sages and SMS) of the constrained modality of the
challenge, we achieved a good performance, being
among the top 30% of the competing systems. In
the near future, we plan to perform an error analysis
of the wrongly classified data to investigate possible
classification issues, in particular the lower perfor-
mance on negative tweets and SMS.
Acknowledgments
This work is supported by ?eOnco - Pervasive knowledge
and data management in cancer care? and ?Trento RISE
PerTe? projects.
References
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
469
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A
publicly available lexical resource for opinion min-
ing. In Proceedings of the 5th International Confer-
ence on Language Resources and Evaluation (LREC
2006), Genoa, Italy.
Lorenzo Gatti and Marco Guerini. 2012. Assessing sen-
timent strength in words prior polarities. In Proceed-
ings of COLING 2012: Posters, pages 361?370, Mum-
bai, India, December. The COLING 2012 Organizing
Committee.
Saif Mohammad, Bonnie Dorr, and Cody Dunne. 2009.
Generating High-Coverage Semantic Orientation Lex-
icons From Overtly Marked Words and a Thesaurus.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conver-
sational text with word clusters. In Proceedings of
NAACL 2013, Atlanta, Georgia, June.
J. Pennebaker and M. Francis. 2001. Linguistic inquiry
and word count: LIWC. Erlbaum Publishers.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ?13, June.
470
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 83?90,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Disease Mention Recognition with Specific Features
Md. Faisal Mahbub Chowdhury ? ? and Alberto Lavelli ?
?Human Language Technology Research Unit, Fondazione Bruno Kessler, Trento, Italy
? ICT Doctoral School, University of Trento, Italy
{chowdhury,lavelli}@fbk.eu
Abstract
Despite an increasing amount of research
on biomedical named entity recognition,
there has been not enough work done on
disease mention recognition. Difficulty of
obtaining adequate corpora is one of the
key reasons which hindered this particu-
lar research. Previous studies argue that
correct identification of disease mentions
is the key issue for further improvement
of the disease-centric knowledge extrac-
tion tasks. In this paper, we present a ma-
chine learning based approach that uses
a feature set tailored for disease mention
recognition and outperforms the state-of-
the-art results. The paper also discusses
why a feature set for the well studied
gene/protein mention recognition task is
not necessarily equally effective for other
biomedical semantic types such as dis-
eases.
1 Introduction
The massive growth of biomedical literature vol-
ume has made the development of biomedical text
mining solutions indispensable. One of the essen-
tial requirements for a text mining application is
the ability to identify relevant entities, i.e. named
entity recognition. Previous work on biomedi-
cal named entity recognition (BNER) has been
mostly focused on gene/protein mention recogni-
tion. Machine learning (ML) based approaches
for gene/protein mention recognition have already
achieved a sufficient level of maturity (Torii et
al., 2009). However, the lack of availability of
adequately annotated corpora has hindered the
progress of BNER research for other semantic
types such as diseases (Jimeno et al, 2008; Lea-
man et al, 2009).
Correct identification of diseases is crucial for
various disease-centric knowledge extraction tasks
(e.g. drug discovery (Agarwal and Searls, 2008)).
Previous studies argue that the most promising
candidate for the improvement of disease related
relation extraction (e.g. disease-gene) is the cor-
rect identification of concept mentions including
diseases (Bundschus et al, 2008).
In this paper, we present a BNER system which
uses a feature set specifically tailored for disease
mention recognition. The system1 outperforms
other approaches evaluated on the Arizona Dis-
ease Corpus (AZDC) (more details in Section 5.1).
One of the key differences between our approach
and previous approaches is that we put more em-
phasis on the contextual features. We exploit syn-
tactic dependency relations as well. Apart from
the experimental results, we also discuss why the
choice of effective features for recognition of dis-
ease mentions is different from that for the well
studied gene/protein mentions.
The remaining of the paper is organized as fol-
lows. Section 2 presents a brief description of pre-
vious work on BNER for disease mention recog-
nition. Then, Section 3 describes our system and
Section 4 the feature set of the system. After that,
Section 5 explains the experimental data, results
and analyses. Section 6 describes the differences
for the choice of feature set between diseases and
genes/proteins. Finally, Section 7 concludes the
paper with an outline of our future research.
2 Related Work
Named entity recognition (NER) is the task of lo-
cating boundaries of the entity mentions in a text
and tagging them with their corresponding seman-
tic types (e.g. person, location, gene and so on).
Although several disease annotated corpora have
been released in the last few years, they have been
annotated primarily to serve the purpose of re-
lation extraction and, for different reasons, they
1The source code of our system is available for download
at http://hlt.fbk.eu/people/chowdhury/research
83
are not suitable for the development of ML based
disease mention recognition systems (Leaman et
al., 2009). For example, the BioText (Rosario
and Hearst, 2004) corpus has no specific anno-
tation guideline and contains several inconsisten-
cies, while PennBioIE (Kulick et al, 2004) is very
specific to a particular sub-domain of diseases.
Among other disease annotated corpora, EBI dis-
ease corpus (Jimeno et al, 2008) is not annotated
with disease mention boundaries which makes it
unsuitable for BNER evaluation for diseases. Re-
cently, an annotated corpus, named as Arizona
Disease Corpus (AZDC) (Leaman et al, 2009),
has been released which has adequate and suitable
annotation of disease mentions following specific
annotation guidelines.
There has been some work on identifying dis-
eases in clinical texts, especially in the context
of CMC Medical NLP Challenge2 and i2b2 Chal-
lenge3. However, as noted by Meystre et al
(2008), there are a number of reasons that make
clinical texts different from texts of biomedical
literature, e.g. composition of short, telegraphic
phrases, use of implicit templates and pseudo-
tables and so on. Hence, the strategies adopted
for NER on clinical texts are not the same as the
ones practiced for NER on biomedical literature.
As mentioned before, most of the work to
date on BNER is focused on gene/protein men-
tion recognition. State-of-the-art BNER systems
are based on ML techniques such as conditional
random fields (CRFs), support vector machines
(SVMs) etc (Dai et al, 2009). These systems use
either gene/protein specific features (e.g. Greek
alphabet matching) or post-processing rules (e.g.
extension of the identified mention boundaries to
the left when a single letter with a hyphen precedes
them (Torii et al, 2009)) which might not be as
effective for other semantic type identification as
they are for genes/proteins. There is a substantial
agreement in the feature set that these systems use
(most of which are actually various orthographical
and morphological features).
Bundschus et al (2008) have used a CRF
based approach that uses typical features for
gene/protein mention recognition (i.e. no feature
tailoring for disease recognition) for disease, gene
and treatement recognition. The work has been
evaluated on two corpora which have been anno-
2http://www.computationalmedicine.org/challenge/index.php
3https://www.i2b2.org/NLP/Relations/Main.php
tated with those entities that participate in disease-
gene and disease-treatment relations. The reported
results show F-measure for recognition of all the
entities that participate in the relations and do
not indicate which F-measure has been achieved
specifically for disease recognition. Hence, the re-
ported results are not applicable for comparison.
To the best of our knowledge, the only sys-
tematic experimental results reported for disease
mention recognition in biomedical literature using
ML based approaches are published by Leaman
and Gonzalez (2008) and Leaman et al (2009).4
They have used a CRF based BNER system named
BANNER which basically uses a set of ortho-
graphic, morphological and shallow syntactic fea-
tures (Leaman and Gonzalez, 2008). The system
achieves an F-score of 86.43 on the BioCreative
II GM corpus5 which is one of the best results for
gene mention recognition task on that corpus.
BANNER achieves an F-score of 54.84 for dis-
ease mention recognition on the BioText corpus
(Leaman and Gonzalez, 2008). However, as said
above, the BioText corpus contains annotation in-
consistencies6. So, the corpus is not ideal for com-
paring system performances. The AZDC corpus
is much more suitable as it is annotated specifi-
cally for benchmarking of disease mention recog-
nition systems. An improved version of BAN-
NER achieves an F-score of 77.9 on AZDC cor-
pus, which is the state of the art on ML based dis-
ease mention recognition in biomedical literature
(Leaman et al, 2009).
3 Description of Our System
There are basically three stages in our approach ?
pre-processing, feature extraction and model train-
ing, and post-processing.
3.1 Pre-processing
At first, the system uses GeniaTagger7 to tokenize
texts and provide PoS tagging. After that, it cor-
rects some common inconsistencies introduced by
GeniaTagger inside the tokenized data (e.g. Ge-
niaTagger replaces double inverted commas with
4However, there are some work on disease recognition in
biomedical literature using other techniques such as morpho-
syntactic heuristic based approach (e.g. MetaMap (Aronson,
2001)), dictionary look-up method and statistical approach
(Ne?ve?ol et al, 2009; Jimeno et al, 2008; Leaman et al,
2009).
5As mentioned in http://banner.sourceforge.net/
6http://biotext.berkeley.edu/data/dis treat data.html
7http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/
84
two single inverted commas). These PoS tagged
tokized data are parsed using Stanford parser8.
The dependency relations provided as output by
the parser are used later as features. The tokens
are further processed using the following general-
ization and normalization steps:
? each number (both integer and real) inside a
token is replaced with ?9?
? each token is further tokenized if it contains
either punctuation characters or both digits
and alphabetic characters
? all letters are changed to lower case
? all Greek letters (e.g. alpha) are replaced with
G and Roman numbers (e.g. iv) with R
? each token is normalized using SPECIALIST
lexicon tool9 to avoid spelling variations
3.2 Feature extraction and model training
The features used by our system can be catego-
rized into the following groups:
? general linguistic features (Table 1)
? orthographic features (Table 2)
? contextual features (Table 3)
? syntactic dependency features (Table 4)
? dictionary lookup features (see Section 4)
During dictionary lookup feature extraction, we
ignored punctuation characters while matching
dictionary entries inside sentences. If a sequence
of tokens in a sentence matches an entry in the dic-
tionary, the leftmost token of that sequence is la-
beled with B-DB and the remaining tokens of the
sequence are labeled with I-DB. The label B-DB
indicates the beginning of a dictionary match. If a
token belongs to several dictionary matches, then
all the other dictionary matches except the longest
one are discarded.
The syntactic dependency features are extracted
from the output of the parser while the general lin-
guistic features are extracted directly from the pre-
processed tokens. To collect the orthographic fea-
tures, the original tokens inside the corresponding
sentences are considered. The contextual features
8http://nlp.stanford.edu/software/lex-parser.shtml
9http://lexsrv3.nlm.nih.gov/SPECIALIST/index.html
are derived using other extracted features and the
original tokens.
Tokens are labeled with the corresponding dis-
ease annotations according to the IOB2 format.
Our system uses Mallet (McCallum, 2002) to train
a first-order CRF model. CRF is a state-of-the-
art ML technique applied to a variety of text
processing tasks including named entity recogni-
tion (Klinger and Tomanek, 2007) and has been
successfully used by many other BNER systems
(Smith et al, 2008).
3.3 Post-processing
Once the disease mentions are identified using
the learned model, the following post-processing
techniques are applied to reduce the number of
wrong identifications:
? Bracket mismatch correction: If there is a
mismatch of brackets in the identified men-
tion, then the immediate following (or pre-
ceding) character of the corresponding men-
tion is checked and included inside the men-
tion if that character is the missing bracket.
Otherwise, all the characters from the index
where the mismatched bracket exists inside
the identified mention are discarded from the
corresponding mention.
? One sense per discourse: If any instance of
a character sequence is identified as a disease
mention, then all the other instances of that
character sequence inside the same sentence
are also annotated as disease mentions.
? Short/long form annotation: Using the algo-
rithm of Schwartz and Hearst (2003), ?long
form (short form)? instances are detected in-
side sentences. If the short form is annotated
as disease mention, then the long form is also
annotated and vice versa.
? Ungrammatical conjunction structure cor-
rection: If an annotated mention contains
comma (,) but there is no ?and? in the fol-
lowing character sequence (from the charac-
ter index of that comma) of that mention, then
the annotation is splitted into two parts (at the
index of the comma). Annotation of the origi-
nal mention is removed and the splitted parts
are annotated as two separate disease men-
tions.
85
? Short and long form separation: If both short
and long forms are annotated in the same
mention, then the original mention is dis-
carded and the corresponding short and long
forms are annotated separately.
4 Features for Disease Recognition
There are compelling reasons to believe that vari-
ous issues regarding the well studied gene/protein
mention recognition would not apply to the other
semantic types. For example, Jimeno et al (2008)
argue that the use of disease terms in biomedical
literature is well standardized, which is quite op-
posite for the gene terms (Smith et al, 2008).
After a thorough study and extensive experi-
ments on various features and their possible com-
binations, we have selected a feature set specific
to the disease mention identification which com-
prises features shown in Tables 1, 2, 4 and 3, and
dictionary lookup features.
Feature name Description
PoS Part-of-speech tag
NormWord Normalized token
(see Section 3.1)
Lemma Lemmatized form
charNgram 3 and 4 character n-grams
Suffix 2-4 character suffixes
Prefix 2-4 character prefixes
Table 1: General linguistic features for tokeni
Feature name Description
InitCap Is initial letter capital
AllCap Are all letters capital
MixCase Does contain mixed case letters
SingLow Is a single lower case letter
SingUp Is a single upper case letter
Num Is a number
PuncChar Punctuation character
(if tokeni is
a punctuation character)
PrevCharAN Is previous character
alphanumeric
Table 2: Orthographic features for tokeni
Like Leaman et al (2009), we have created
a dictionary with the instances of the following
nine of the twelve UMLS semantic types from
Feature name Description
Bi-gramk,k+1 Bi-grams of
for i? 2 ? k < i + 2 normalized tokens
Tri-gramk,k+1,k+2 Tri-grams of
for i? 2 ? k < i + 2 normalized tokens
CtxPoSk,k+1 Bi-grams of
for i ? k < i + 2 token PoS
CtxLemmak,k+1 Bi-grams of
for i ? k < i + 2 lemmatized tokens
CtxWordk,k+1 Bi-grams of
for i? 2 ? k < i + 2 original tokens
Offset conjunctions Extracted by Mallet
from features
in the range from
tokeni?1 to tokeni+1
Table 3: Contextual features for tokeni
Feature name Description
dobj Target token(s) to which tokeni
is a direct object
iobj Target token(s) to which tokeni
is an indirect object
nsubj Target token(s) to which tokeni
is an active nominal subject
nsubjpass Target token(s) to which tokeni
is a passive nominal subject
nn Target token(s) to which tokeni
is a noun compound modifier
Table 4: Syntactic dependency features for tokeni.
For example, in the sentence ?Clinton defeated
Dole?, ?Clinton? is the nsubj of the target token
?defeated?.
the semantic group ?DISORDER?10 from UMLS
Metathesaurus (Bodenreider, 2004): (i) disease or
syndrome, (ii) neoplastic process, (iii) congenital
abnormality, (iv) acquired abnormality, (v) exper-
imental model of disease, (vi) injury or poison-
ing, (vii) mental or behavioral dysfunction, (viii)
pathological function and (ix) sign or symptom.
We have not considered the other three semantic
types (findings, anatomical abnormality and cell
or molecular Dysfunction) since these three types
have not been used during the annotation of Ari-
zona Disease Corpus (AZDC) which we have used
in our experiments.
Previous studies have shown that dictionary
lookup features, i.e. name matching against a
10http://semanticnetwork.nlm.nih.gov/SemGroups/
86
dictionary of terms, often increase recall (Torii
et al, 2009; Leaman et al, 2009). However,
an unprocessed dictionary usually does not boost
overall performance (Zweigenbaum et al, 2007).
So, to reduce uninformative lexical differences or
spelling variations, we generalize and normalize
the dictionary entries using exactly the same steps
followed for the pre-processing of sentences (see
Section 3.1).
To reduce chances of false and unlikely
matches, any entry inside the dictionary having
less than 3 characters or more than 10 tokens is
discarded.
5 Experiments
5.1 Data
We have done experiments on the recently re-
leased Arizona Disease Corpus (AZDC)11 (Lea-
man et al, 2009). The corpus has detailed annota-
tions of diseases including UMLS codes, UMLS
concept names, possible alternative codes, and
start and end points of disease mentions inside
the corresponding sentences. These detailed an-
notations make this corpus a valuable resource
for evaluating and benchmarking text mining so-
lutions for disease recognition. Table 5 shows var-
ious characteristics of the corpus.
Item name Total count
Abstracts 793
Sentences 2,783
Total disease mentions 3,455
Disease mentions without overlaps 3,093
Disease mentions with overlaps 362
Table 5: Various characteristics of AZDC.
For the overlapping annotations, (e.g. ?endome-
trial and ovarian cancers? and ?ovarian cancers?)
we have considered only the larger annotations
in our experiments. There remain 3,224 disease
mentions after resolving overlaps according to the
aforementioned criterion. We have observed mi-
nor differences in some statistics of the AZDC re-
ported by Leaman et al (2009) with the statistics
of the downloadable version12 (Table 5). How-
11Downloaded from http://diego.asu.edu/downloads/AZDC/
at 5-Feb-2009
12Note that ?Disease mentions (total)? in the paper of Lea-
man et al (2009) actually refers to the total disease mentions
after overlap resolving (Robert Leaman, personal communi-
cation). One other thing is, Leaman et al (2009) mention 794
ever, these differences can be considered negligi-
ble.
5.2 Results
We follow an experimental setting similar to the
one in Leaman et al (2009) so that we can com-
pare our results with that of the BANNER system.
We performed 10-fold cross validation on AZDC
in such a way that all sentences of the same ab-
stract are included in the same fold. The results of
all folds are averaged to obtain the final outcome.
Table 6 shows the results of the experiments with
different features using the exact matching crite-
rion.
As we can see, our approach achieves signif-
icantly higher result than that of BANNER. Ini-
tially, with only the general linguistic and or-
thographic features the performance is not high.
However, once the contextual features are used,
there is a substantial improvement in the result.
Note that BANNER does not use contextual fea-
tures. In fact, the use of contextual features is also
quite limited in other BNER systems that achieve
high performance for gene/protein identification
(Smith et al, 2008).
Dictionary lookup features provide a very good
contribution in the outcome. This supports the ar-
gument of Jimeno et al (2008) that the use of dis-
ease terms in biomedical literature is well stan-
dardized. Post-processing and syntactic depen-
dency features also increase some performance.
We have done statistical significance tests for
the last four experimental results shown in Table 6.
For each of such four experiments, the immediate
previous experiment is considered as the baseline.
The tests have been performed using the approx-
imate randomization procedure (Noreen, 1989).
We have set the number of iterations to 1,000 and
the confidence level to 0.01. According to the
tests, the contributions of contextual features and
dictionary lookup features are statistically signif-
icant. However, we have found that the contri-
butions of post-processing rules and syntactic de-
pendency features are statistically significant only
when the confidence level is 0.2 or more. Since
AZDC consists of only 2,783 sentences, we can
assume that the impact of post-processing rules
abstracts, 2,784 sentences and 3,228 (overlap resolved) dis-
ease mentions in the AZDC. But in our downloaded version
of AZDC, there is 1 abstract missing (i.e. total 793 abstracts
instead of 794). As a result, there is 1 less sentence and 4
less (overlap resolved) disease mentions than the originally
reported numbers.
87
and syntactic dependency features has been not so
significant despite of some performance improve-
ment.
5.3 Error analysis
One of the sources of errors is the annotations
having conjunction structures. There are 94 dis-
ease mentions in the data which contain the word
?and?. The boundaries of 11 of them have been
wrongly identified during experiments, while 39
of them have been totally missed out by our sys-
tem. Our system also has not performed well
for disease annotations that have some specific
types of prepositional phrase structures. For ex-
ample, there are 80 disease annotations having the
word ?of? (e.g. ?deficient activity of acid beta-
glucosidase GBA?). Only 28 of them are correctly
annotated by our system. The major source of er-
rors, however, concerns abbreviated disease names
(e.g. ?PNH?). We believe one way to reduce this
specific error type is to generate a list of possi-
ble abbreviated disease names from the long forms
of disease names available in databases such as
UMLS Metathesaurus.
6 Why Features for Diseases and
Genes/Proteins are not the Same
Many of the existing BNER systems, which are
mainly tuned for gene/protein identification, use
features such as token shape (also known as word
class and brief word class (Settles, 2004)), Greek
alphabet matching, Roman number matching and
so forth. As mentioned earlier, we have done ex-
tensive experiments with various feature combina-
tions for the selection of disease specific features.
We have observed that many of the features used
for gene/protein identification are not equally ef-
fective for disease identification. Table 7 shows
some of the results of those experiments.
This observation is reasonable because
gene/protein names are much more complex than
entities such as diseases. For example, they often
contain punctuation characters (such as paren-
theses or hyphen), Greek alphabets and digits
which are unlikely in disease names. Ideally,
the ML algorithm itself should be able to utilize
information from only the useful features and
ignore the others in the feature set. But practically,
having non-informative features often mislead
the model learning. In fact, several surveys have
argued that the choice of features matter at least
as much as the choice of the algorithm if not more
(Nadeau and Sekine, 2007; Zweigenbaum et al,
2007).
One of the interesting trends in gene/protein
mention identification is to not utilize syntactic
dependency relations (with the exception of Vla-
chos (2007)). Gene/protein names in biomedi-
cal literature are often combined (i.e. without
being separated by space characters) with other
characters which do not belong to the correspond-
ing mentions (e.g. p53-mediated). Moreover,
as mentioned before, gene/protein mentions com-
monly have very complex structures (e.g. PKR(1-
551)K64E/K296R or RXRalphaF318A). So, it is a
common practice to tokenize gene/proten names
adopting an approach that split tokens as much as
possible to extract effective features (Torii et al,
2009; Smith et al, 2008). But while the extensive
tokenization boosts performance, it is often diffi-
cult to correctly detect dependency relations for
the tokens of the gene/protein names in the sen-
tences where they appear. As a result, use of the
syntactic dependency relations is not beneficial in
such approaches.13 In comparison, disease men-
tions are less complex. So, the identified depen-
dencies for disease mentions are more reliable and
hence may be usable as potential features (refer to
our experimental results in Table 6).
The above mentioned issues are some of the
reasons why a feature set for the well studied
gene/protein focused BNER approaches is not
necessarily suitable for other biomedical semantic
types such as diseases.
7 Conclusion
In this paper, we have presented a single CRF clas-
sifier based BNER approach for disease mention
identification. The feature set is constructed us-
ing disease-specific contextual, orthographic, gen-
eral linguistic, syntactic dependency and dictio-
nary lookup features. We have evaluated our ap-
proach on AZDC corpus. Our approach achieves
significantly higher result than BANNER which is
the current state-of-the-art ML based approach for
disease mention recognition. We have also ex-
plained why the choice of features for the well
studied gene/protein does not apply for other se-
mantic types such as diseases.
13We have done some experiments on Biocreative II GM
corpus with syntactic dependency relations of the tokens,
which are not reported in this paper, and the results support
our argument.
88
System Note Precision Recall F-score
BANNER (Leaman et al, 2009) 80.9 75.1 77.9
Our system Using general linguistic and orthographic features 74.90 71.01 72.90
Our system After adding contextual features 82.15 75.81 78.85
Our system After adding post-processing 81.57 76.61 79.01
Our system After adding syntactic dependency features 82.07 76.66 79.27
Our system After adding dictionary lookup features 83.21 79.06 81.08
Table 6: 10-fold cross validation results using exact matching criteria on AZDC.
Experiment Note Precision Recall F-score
(i) Using general linguistic, orthographic 82.15 75.81 78.85
and contextual features
(ii) After adding WC and BWC features in (i) 82.08 75.57 78.69
(iii) After adding IsGreekAlphabet, HasGreekAlphabet 82.10 75.69 78.76
and IsRomanNumber features in (i)
Table 7: Experimental results of our system after using some of the gene/protein specific features for
disease mention recognition on AZDC. Here, WC and BWC refer to the ?word class? and ?brief word
class? respectively.
Future work includes implementation of disease
mention normalization (i.e. associating a unique
identifier for each disease mention). We also
plan to improve our current approach by includ-
ing more contextual features and post-processing
rules.
Acknowledgments
This work was carried out in the context of the
project ?eOnco - Pervasive knowledge and data
management in cancer care?. The authors would
like to thank Robert Leaman for sharing the set-
tings of his experiments on AZDC.
References
Agarwal, P., Searls, D. 2008. Literature mining in sup-
port of drug discovery. Brief Bioinform, 9(6):479?
492.
Aronson, A. 2001. Effective mapping of biomedical
text to the UMLS Metathesaurus: the MetaMap pro-
gram. In Proceedings AMIA Symposium, pages 17?
21.
Bodenreider, O. 2004. The Unified Medical Language
System (UMLS): integrating biomedical terminol-
ogy. Nucleic Acids Research, 32(suppl 1):D267?
270, January.
Bundschus, M., Dejori, M., Stetter, M., Tresp, V.,
Kriegel, H. 2008. Extraction of semantic biomed-
ical relations from text using conditional random
fields. BMC Bioinformatics, 9:207.
Dai, H., Chang, Y., Tsai, R., Hsu, W. 2009. New
challenges for biological text-mining in the next
decade. Journal of Computer Science and Technol-
ogy, 25(1):169?179.
Jimeno, A., Jimnez-Ruiz, E., Lee, V., Gaudan, S.,
Berlanga, R., Rebholz-Schuhmann, D. 2008. As-
sessment of disease named entity recognition on a
corpus of annotated sentences. BMC Bioinformat-
ics, 9(S-3).
Klinger, R., Tomanek, K. 2007. Classical Probabilistic
Models and Conditional Random Fields. Technical
Report TR07-2-013, Department of Computer Sci-
ence, Dortmund University of Technology, Decem-
ber.
Kulick, S., Bies, A., Liberman, M., Mandel, M., Mc-
Donald, R., Palmer, M., Schein, A., Ungar, L. 2004.
Integrated annotation for biomedical information ex-
traction. In Proceedings of HLT/NAACL 2004 Bi-
oLink Workshop, pages 61?68.
Leaman, R., Gonzalez, G. 2008. Banner: An exe-
cutable survey of advances in biomedical named en-
tity recognition. In Proceedings of Pacific Sympo-
sium on Biocomputing, volume 13, pages 652?663.
Leaman, R., Miller, C., Gonzalez, G. 2009. Enabling
recognition of diseases in biomedical text with ma-
chine learning: Corpus and benchmark. In Proceed-
ings of the 3rd International Symposium on Lan-
guages in Biology and Medicine, pages 82?89.
McCallum, A. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu,.
Meystre, S., Savova, G., Kipper-Schuler, K., Hurdle,
J. 2008. Extracting information from textual doc-
uments in the electronic health record: a review of
89
recent research. IMIA Yearbook of Medical Infor-
matics, pages 128?44.
Ne?ve?ol, A., Kim, W., Wilbur, W., Lu, Z. 2009. Explor-
ing two biomedical text genres for disease recogni-
tion. In Proceedings of the BioNLP 2009 Workshop,
pages 144?152, June.
Nadeau, D., Sekine, S. 2007. A survey of named entity
recognition and classification. Linguisticae Investi-
gationes, 30(1):3?26.
Noreen, E.W. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience.
Rosario, B., Hearst, M. 2004. Classifying semantic
relations in bioscience texts. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL?04).
Schwartz, A., Hearst, M. 2003. A simple algorithm
for identifying abbreviation definitions in biomedi-
cal text. In Proceedings of Pacific Symposium on
Biocomputing, pages 451?62.
Settles, B. 2004. Biomedical named entity recognition
using conditional random fields and rich feature sets.
In Proceedings of the International Joint Workshop
on Natural Language Processing in Biomedicine
and its Applications, pages 104?107.
Smith, L., Tanabe, L., Ando, R., Kuo, C., et al 2008.
Overview of BioCreative II gene mention recogni-
tion. Genome Biology, 9(Suppl 2).
Torii, M., Hu, Z., Wu, C., Liu, H. 2009. Biotagger-
GM: a gene/protein name recognition system. Jour-
nal of the American Medical Informatics Associa-
tion : JAMIA, 16:247?255.
Vlachos, A. 2007. Tackling the BioCreative2 gene
mention task with conditional random fields and
syntactic parsing. In Proceedings of the 2nd BioCre-
ative Challenge Evaluation Workshop, pages 85?87.
Zweigenbaum, P., Demner-Fushman, D., Yu, H., Co-
hen, K. 2007. Frontiers of biomedical text mining:
current progress. Brief Bioinform, 8(5):358?375.
90
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 124?133,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
A Study on Dependency Tree Kernels
for Automatic Extraction of Protein-Protein Interaction
Md. Faisal Mahbub Chowdhury ? ? and Alberto Lavelli ? and Alessandro Moschitti ?
? Department of Information Engineering and Computer Science, University of Trento, Italy
? Human Language Technology Research Unit, Fondazione Bruno Kessler, Trento, Italy
{chowdhury,lavelli}@fbk.eu, moschitti@disi.unitn.it
Abstract
Kernel methods are considered the most ef-
fective techniques for various relation extrac-
tion (RE) tasks as they provide higher accu-
racy than other approaches. In this paper,
we introduce new dependency tree (DT) ker-
nels for RE by improving on previously pro-
posed dependency tree structures. These are
further enhanced to design more effective ap-
proaches that we call mildly extended depen-
dency tree (MEDT) kernels. The empirical re-
sults on the protein-protein interaction (PPI)
extraction task on the AIMed corpus show that
tree kernels based on our proposed DT struc-
tures achieve higher accuracy than previously
proposed DT and phrase structure tree (PST)
kernels.
1 Introduction
Relation extraction (RE) aims at identifying in-
stances of pre-defined relation types in text as for
example the extraction of protein-protein interaction
(PPI) from the following sentence:
?Native C8 also formed a heterodimer
with C5, and low concentrations of
polyionic ligands such as protamine and
suramin inhibited the interaction.?
After identification of the relevant named entities
(NE, in this case proteins) C8 and C5, the RE task
determines whether there is a PPI relationship be-
tween the entities above (which is true in the exam-
ple).
Kernel based approaches for RE have drawn a lot
of interest in recent years since they can exploit a
huge amount of features without an explicit repre-
sentation. Some of these approaches are structure
kernels (e.g. tree kernels), which carry out struc-
tural similarities between instances of relations, rep-
resented as phrase structures or dependency trees,
in terms of common substructures. Other kernels
simply use techniques such as bag-of-words, subse-
quences, etc. to map the syntactic and contextual
information to flat features, and later compute simi-
larity.
One variation of tree kernels is the dependency
tree (DT) kernel (Culotta and Sorensen, 2004;
Nguyen et al, 2009). A DT kernel (DTK) is a
tree kernel that is computed on a dependency tree
(or subtree). A dependency tree encodes grammati-
cal relations between words in a sentence where the
words are nodes, and dependency types (i.e. gram-
matical functions of children nodes with respect to
their parents) are edges. The main advantage of a
DT in comparison with phrase structure tree (PST)
is that the former allows for relating two words di-
rectly (and in more compact substructures than PST)
even if they are far apart in the corresponding sen-
tence according to their lexical word order.
Several kernel approaches exploit syntactic de-
pendencies among words for PPI extraction from
biomedical text in the form of dependency graphs or
dependency paths (e.g. Kim et al (2010) or Airola
et al (2008)). However, to the best of our knowl-
edge, there are only few works on the use of DT
kernels for this task. Therefore, exploring the po-
tential of DTKs applied to different structures is a
worthwhile research direction. A DTK, pioneered
by Culotta and Sorensen (2004), is typically applied
to the minimal or smallest common subtree that in-
cludes a target pair of entities. Such subtree reduces
124
Figure 1: Part of the DT for the sentence ?The binding
epitopes of BMP-2 for BMPR-IA was characterized using
BMP-2 mutant proteins?. The dotted area indicates the
minimal subtree.
unnecessary information by placing word(s) closer
to its dependent(s) inside the tree and emphasizes
local features of relations. Nevertheless, there are
cases where a minimal subtree might not contain im-
portant cue words or predicates. For example, con-
sider the following sentence where a PPI relation
holds between BMP-2 and BMPR-IA, but the mini-
mal subtree does not contain the cue word ?binding?
as shown in Figure 1:
The binding epitopes of BMP-2 for
BMPR-IA was characterized using BMP-
2 mutant proteins.
In this paper we investigate two assumptions. The
first is that a DTK based on a mild extension of
minimal subtrees would produce better results than
the DTK on minimal subtrees. The second is that
previously proposed DT structures can be further
improved by introducing simplified representation
of the entities as well as augmenting nodes in the
DT tree structure with relevant features. This paper
presents an evaluation of the above assumptions.
More specifically, the contributions of this paper
are the following:
? We propose the use of new DT structures,
which are improvement on the structures de-
fined in Nguyen et al (2009) with the most gen-
eral (in terms of substructures) DTK, i.e. Par-
tial Tree Kernel (PTK) (Moschitti, 2006).
? We firstly propose the use of the Unlexicalized
PTK (Severyn and Moschitti, 2010) with our
dependency structures, which significantly im-
proves PTK.
? We compare the performance of the proposed
DTKs on PPI with the one of PST kernels and
show that, on biomedical text, DT kernels per-
form better.
? Finally, we introduce a novel approach (called
mildly extended dependency tree (MEDT) ker-
nel1, which achieves the best performance
among various (both DT and PST) tree kernels.
The remainder of the paper is organized as fol-
lows. In Section 2, we introduce tree kernels and re-
lation extraction and we also review previous work.
Section 3 describes the unlexicalized PTK (uPTK).
Then, in Section 4, we define our proposed DT struc-
tures including MEDT. Section 5 describes the ex-
perimental results on the AIMed corpus (Bunescu et
al., 2005) and discusses their outcomes. Finally, we
conclude with a summary of our study as well as
plans for future work.
2 Background and Related Work
The main stream work for Relation Extraction uses
kernel methods. In particular, as the syntactic struc-
ture is very important to derive the relationships be-
tween entities in text, several tree kernels have been
designed and experimented. In this section, we in-
troduce such kernels, the problem of relation extrac-
tion and we also focus on the biomedical domain.
2.1 Tree Kernel types
The objective behind the use of tree kernels is
to compute the similarity between two instances
through counting similarities of their sub-structures.
Among the different proposed methods, two of the
most effective approaches are Subset Tree (SST)
kernel (Collins and Duffy, 2001) and Partial Tree
Kernel (PTK) (Moschitti, 2006).
The SST kernel generalizes the subtree ker-
nel (Vishwanathan and Smola, 2002), which consid-
ers all common subtrees in the tree representation of
two compared sentences. In other words, two sub-
trees are identical if the node labels and order of chil-
dren are identical for all nodes. The SST kernel re-
laxes the constraint that requires leaves to be always
included in the sub-structures. In SST, for a given
node, either none or all of its children have to be in-
cluded in the resulting subset tree. An extension of
1We defined new structures, which as it is well known it
corresponds to define a new kernel.
125
the SST kernel is the SST+bow (bag-of-words) ker-
nel (Zhang and Lee, 2003; Moschitti, 2006a), which
considers individual leaves as sub-structures as well.
The PT kernel (Moschitti, 2006) is more flexi-
ble than SST by virtually allowing any tree sub-
structure; the only constraint is that the order of child
nodes must be identical. Both SST and PT kernels
are convolution tree kernels2.
The PT kernel is the most complete in terms of
structures. However, the massive presence of child
node subsequences and single child nodes, which in
a DT often correspond to words, may cause overfit-
ting. Thus we propose the use of the unlexicalized
(i.e. PT kernel without leaves) tree kernel (uPTK)
(Severyn and Moschitti, 2010), in which structures
composed by only one lexical element, i.e. single
nodes, are removed from the feature space (see Sec-
tion 3).
2.2 Relation Extraction using Tree Kernels
A first version of dependency tree kernels (DTKs)
was proposed by Culotta and Sorensen (2004). In
their approach, they find the smallest common sub-
tree in the DT that includes a given pair of enti-
ties. Then, each node of the subtree is represented
as a feature vector. Finally, these vectors are used
to compute similarity. However, the tree kernel they
defined is not a convolution kernel, and hence it gen-
erates a much lower number of sub-structures result-
ing in lower performance.
For any two entities e1 and e2 in a DT, Nguyen
et al (2009) defined the following three dependency
structures to be exploited by convolution tree ker-
nels:
? Dependency Words (DW) tree: a DW tree is
the minimal subtree of a DT, which includes e1
and e2. An extra node is inserted as parent of
the corresponding NE, labeled with the NE cat-
egory. Only words are considered in this tree.
? Grammatical Relation (GR) tree: a GR tree
is similar to a DW tree except that words are
replaced by their grammatical functions, e.g.
prep, nsubj, etc.
2Convolution kernels aim to capture structural information
in term of sub-structures, providing a viable alternative to flat
features (Moschitti, 2004).
? Grammatical Relation and Words (GRW) tree:
a GRW tree is the minimal subtree that uses
both words and grammatical functions, where
the latter are inserted as parent nodes of the for-
mer.
Using PTK for the above dependency tree struc-
tures, the authors achieved an F-measure of 56.3 (for
DW), 60.2 (for GR) and 58.5 (for GRW) on the ACE
2004 corpus3.
Moschitti (2004) proposed the so called path-
enclosed tree (PET)4 of a PST for Semantic Role
Labeling. This was later adapted by Zhang et al
(2005) for relation extraction. A PET is the smallest
common subtree of a PST, which includes the two
entities involved in a relation.
Zhou et al (2007) proposed the so called context-
sensitive tree kernel approach based on PST, which
expands PET to include necessary contextual in-
formation. The expansion is carried out by some
heuristics tuned on the target RE task.
Nguyen et al (2009) improved the PET represen-
tation by inserting extra nodes for denoting the NE
category of the entities inside the subtree. They also
used sequence kernels from tree paths, which pro-
vided higher accuracy.
2.3 Relation Extraction in the biomedical
domain
There are several benchmarks for the PPI task,
which adopt different PPI annotations. Conse-
quently the experimental results obtained by dif-
ferent approaches are often difficult to compare.
Pyysalo et al (2008) put together these corpora (in-
cluding the AIMed corpus used in this paper) in a
common format for comparative evaluation. Each
of these corpora is known as converted corpus of the
corresponding original corpus.
Several kernel-based RE approaches have been
reported to date for the PPI task. These are based on
various methods such as subsequence kernel (Lodhi
et al, 2002; Bunescu and Mooney, 2006), depen-
dency graph kernel (Bunescu and Mooney, 2005),
etc. Different work exploited dependency analy-
ses with different kernel approaches such as bag-of-
3http://projects.ldc.upenn.edu/ace/
4Also known as shortest path-enclosed tree or SPT (Zhou et
al., 2007).
126
words kernel (e.g. Miwa et al (2009)), graph based
kernel (e.g. Kim et al (2010)), etc. However, there
are only few researches that attempted the exploita-
tion of tree kernels on dependency tree structures.
S?tre et al (2007) used DT kernels on AIMed
corpus and achieved an F-score of 37.1. The re-
sults were far better when they combined the out-
put of the dependency parser with that of a Head-
driven Phrase Structure Grammar (HPSG) parser,
and applied tree kernel on it. Miwa et al (2009) also
proposed a hybrid kernel 5, which is a composition
of all-dependency-paths kernel (Airola et al, 2008),
bag-of-words kernel and SST kernel. They used
multiple parser inputs. Their system is the current
state-of-the-art for PPI extraction on several bench-
marks. Interestingly, they applied SST kernel on the
shortest dependency paths between pairs of proteins
and achieved a relatively high F-score of 55.1. How-
ever, the trees they constructed from the shortest de-
pendency paths are actually not dependency trees. In
a dependency tree, there is only one node for each
individual word whereas in their constructed trees
(please refer to Fig. 6 of Miwa et al (2009)), a word
(that belongs to the shortest path) has as many node
representations as the number of dependency rela-
tions with other words (those belonging to the short-
est path). Perhaps, this redundancy of information
might be the reason their approach achieved higher
result. In addition to work on PPI pair extraction,
there has been some approaches that exploited de-
pendency parse analyses along with kernel methods
for identifying sentences that might contain PPI pairs
(e.g. Erkan et al (2007)).
In this paper, we focus on finding the best repre-
sentation based on a single structure. We speculate
that this can be helpful to improve the state-of-the-
art using several combinations of structures and fea-
tures. As a first step, we decided to use uPTK, which
is more robust to overfitting as the description in the
next section unveil.
5The term ?hybrid kernel? is identical to ?combined kernel?.
It refers to those kernels that combine multiple types of kernels
(e.g., tree kernels, graph kernels, etc)
3 Unlexicalized Partial Tree Kernel
(uPTK)
The uPTK was firstly proposed in (Severyn and
Moschitti, 2010) and experimented with semantic
role labeling (SRL). The results showed no improve-
ment for such task but it is well known that in SRL
lexical information is essential (so in that case it
could have been inappropriate). The uPTK defini-
tion follows the general setting of tree kernels.
A tree kernel function over two trees, T1 and T2,
is defined as
TK(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2),
where NT1 and NT2 are the sets of nodes in T1 and
T2, respectively, and
?(n1, n2) =
|F|?
i=1
?i(n1)?i(n2).
The ? function is equal to the number of common
fragments rooted in nodes n1 and n2 and thus de-
pends on the fragment type.
The algorithm for the uPTK computation straight-
forwardly follows from the definition of the ? func-
tion of PTK provided in (Moschitti, 2006). Given
two nodes n1 and n2 in the corresponding two trees
T1 and T2, ? is evaluated as follows:
1. if the node labels of n1 and n2 are different then
?(n1, n2) = 0;
2. else ?(n1, n2) = ?
(
?2 +
?
~I1,~I2,l(~I1)=l(~I2)
?d(
~I1)+d(~I2)
l(~I1)?
j=1
?(cn1(~I1j), cn2(~I2j))
)
,
where:
1. ~I1 = ?h1, h2, h3, ..? and ~I2 = ?k1, k2, k3, ..?
are index sequences associated with the ordered
child sequences cn1 of n1 and cn2 of n2, respec-
tively;
2. ~I1j and ~I2j point to the j-th child in the corre-
sponding sequence;
3. l(?) returns the sequence length, i.e. the number
of children;
127
4. d(~I1) = ~I1l(~I1)?
~I11 + 1 and d(~I2) = ~I2l(~I2)?
~I21 + 1; and
5. ? and ? are two decay factors for the size of
the tree and for the length of the child subse-
quences with respect to the original sequence,
i.e. we account for gaps.
The uPTK can be obtained by removing ?2 from
the equation in step 2. An efficient algorithm for the
computation of PTK is given in (Moschitti, 2006).
This evaluates ? by summing the contribution of
tree structures coming from different types of se-
quences, e.g. those composed by p children such
as:
?(n1, n2) = ?
(
?2 +
?lm
p=1 ?p(cn1 , cn2)
)
, (1)
where ?p evaluates the number of common subtrees
rooted in subsequences of exactly p children (of n1
and n2) and lm = min{l(cn1), l(cn2)}. It is easy to
verify that we can use the recursive computation of
?p by simply removing ?2 from Eq. 1.
4 Proposed dependency structures and
MEDT kernel
Our objective is twofold: (a) the definition of im-
proved DT structures and (b) the design of new DT
kernels to include important words residing outside
of the shortest dependency tree, which are neglected
in current approaches. For achieving point (a), we
modify the DW, GR and GRW structures, previously
proposed by Nguyen et al (2009). The new pro-
posed structures are the following:
? Grammatical Relation and lemma (GRL) tree:
A GRL tree is similar to a GRW tree except
that words are replaced by their corresponding
lemmas.
? Grammatical Relation, PoS and lemma
(GRPL) tree: A GRPL tree is an extension of a
GRL tree, where the part-of-speech (PoS) tag
of each of the corresponding words is inserted
as a new node between its grammatical func-
tion and its lemma, i.e. the new node becomes
the parent node of the node containing the
lemma.
Figure 2: Part of the DT for the sentence ?Interaction
was identified between BMP-2 and BMPR-IA?. The dot-
ted area indicates the minimal subtree.
Figure 3: Part of the DT for the sentence ?Phe93 forms
extensive contacts with a peptide ligand in the crystal
structure of the EBP bound to an EMP1?. The dotted
area indicates the minimal subtree.
? Ordered GRL (OGRL) or ordered GRW
(OGRW) tree: in a GRW (or GRL) tree, the
node containing the grammatical function of
a word is inserted as the parent node of such
word. So, if the word has a parent node con-
taining its NE category, the newly inserted node
with grammatical function becomes the child
node of the node containing NE category, i.e.
the order of the nodes is the following ? ?NE
category ? grammatical relation ? word (or
lemma)?. However, in OGRW (or OGRL), this
ordering is modified as follows ? ?grammatical
relation? NE category? word (or lemma)?.
? Ordered GRPL (OGRPL) tree: this is similar
to the OGRL tree except for the order of the
nodes, which is the following ? ?grammatical
relation? NE category? PoS? lemma?.
? Simplified (S) tree: any tree structure would
become an S tree if it contains simplified repre-
sentations of the entity types, where all its parts
except the head word of a multi-word entity are
not considered in the minimal subtree.
The second objective is to extend DTKs to include
important cue words or predicates that are missing
128
in the minimal subtree. We do so by mildly expand-
ing the minimal subtree, i.e. we define the mildly
extended DT (MEDT) kernel. We propose three dif-
ferent expansion rules for three versions of MEDT
as follows:
? Expansion rule for MEDT-1 kernel: If the root
of the minimal subtree is not a modifier (e.g.
adjective) or a verb, then look for such node in
its children or in its parent (in the original DT
tree) to extend the subtree.
The following example shows a sentence where
this rule would be applicable:
The binding epitopes of BMP-2
for BMPR-IA was characterized us-
ing BMP-2 mutant proteins.
Here, the cue word is ?binding?, the root of the
minimal subtree is ?epitopes? and the target en-
tities are BMP-2 and BMPR-IA. However, as
shown in Figure 1, the minimal subtree does
not contain the cue word.
? Expansion rule for MEDT-2 kernel: If the root
of the minimal subtree is a verb and its subject
(or passive subject) in the original DT tree is
not included in the subtree, then include it.
Consider the following sentence:
Interaction was identified be-
tween BMP-2 and BMPR-IA.
Here, the cue word is ?Interaction?, the root
is ?identified? and the entities are BMP-2 and
BMPR-IA. The passive subject ?Interaction?
does not belong to the minimal subtree (see
Figure 2).
? Expansion rule for MEDT-3 kernel: If the root
of the minimal subtree is the head word of one
of the interacting entities, then add the parent
node (in the original DT tree) of the root node
as the new root of the subtree.
This is an example sentence where this rule is
applicable (see Figure 3):
Phe93 forms extensive contacts
with a peptide ligand in the crystal
structure of the EBP bound to an
EMP1.
5 Experiments and results
We carried out several experiments with different
dependency structures and tree kernels. Most im-
portantly, we tested tree kernels on PST and our im-
proved representations for DT.
5.1 Data and experimental setup
We used the AIMed corpus (Bunescu et al, 2005)
converted using the software provided by Pyysalo et
al. (2008). AIMed is the largest benchmark corpus
(in terms of number of sentences) for the PPI task.
It contains 1,955 sentences, in which are annotated
1,000 positive PPI and 4,834 negative pairs.
We use the Stanford parser6 for parsing the data.7
The SPECIALIST lexicon tool8 is used to normalize
words to avoid spelling variations and also to pro-
vide lemmas. For training and evaluating tree ker-
nels, we use the SVM-LIGHT-TK toolkit9 (Mos-
chitti, 2006; Joachims, 1999). We tuned the param-
eters ?, ? and c following the approach described by
Hsu et al (2003), and used biased hyperplane.10 All
the other parameters are left as their default values.
Our experiments are evaluated with 10-fold cross
validation using the same split of the AIMed corpus
used by Bunescu et al (2005).
5.2 Results and Discussion
The results of different tree kernels applied to dif-
ferent structures are shown in Tables 1 and 2. All
the tree structures are tested with four different tree
kernel types: SST, SST+bow, PTK and uPTK.
According to the empirical outcome, our new DT
structures perform better than the existing tree struc-
tures. The highest result (F: 46.26) is obtained by
applying uPTK to MEDT-3 (SOGRL). This is 6.68
higher than the best F-measure obtained by previous
DT structures proposed in Nguyen et al (2009), and
0.36 higher than the best F-measure obtained using
PST (PET).
6http://nlp.stanford.edu/software/lex-parser.shtml
7For some of the positive PPI pairs, the connecting depen-
dency tree could not be constructed due to parsing errors for
the corresponding sentences. Such pairs are considered as false
negative (FN) during precision and recall measurements.
8http://lexsrv3.nlm.nih.gov/SPECIALIST/index.html
9http://disi.unitn.it/moschitti/Tree-Kernel.htm
10Please refer to http://svmlight.joachims.org/ and
http://disi.unitn.it/moschitti/Tree-Kernel.htm for details
about parameters of the respective tools
129
DT DT DT DT DT DT DT DT DT
(GR) (SGR) (DW) (SDW) (GRW) (SGRW) (SGRL) (SGRPL) (OGRPL)
SST P: 55.29 P: 54.22 P: 31.87 P: 30.74 P: 52.76 P: 52.47 P: 56.09 P: 56.03 P: 57.85
R: 23.5 R: 24.4 R: 27.5 R: 27.3 R: 33.4 R: 30.8 R: 33.6 R: 33.0 R: 31.7
F: 32.98 F: 33.66 F: 29.52 F: 28.92 F: 40.9 F: 38.82 F: 42.03 F: 41.54 F: 40.96
SST P: 57.87 P: 54.91 P: 30.71 P: 29.98 P: 52.98 P: 51.06 P: 51.99 P: 56.8 P: 61.73
+ R: 21.7 R: 23.5 R: 26.9 R: 25.9 R: 32.0 R: 31.3 R: 31.4 R: 28.8 R: 29.2
bow F: 31.56 F: 32.91 F: 28.68 F: 27.79 F: 39.9 F: 38.81 F: 39.15 F: 38.22 F: 39.65
PT P: 60.0 P: 57.84 P: 40.44 P: 42.2 P: 53.35 P: 53.41 P: 51.29 P: 52.88 P: 53.55
R: 15.9 R: 16.6 R: 23.9 R: 26.5 R: 34.2 R: 36.0 R: 37.9 R: 33.0 R: 33.2
F: 25.14 F: 25.8 F: 30.04 F: 32.56 F: 41.68 F: 43.01 F: 43.59 F: 40.64 F: 40.99
uPT P: 58.77 P: 59.5 P: 29.21 P: 29.52 P: 51.86 P: 52.17 P: 52.1 P: 54.64 P: 56.43
R: 23.8 R: 26.0 R: 30.2 R: 31.5 R: 32.0 R: 33.7 R: 36.0 R: 31.2 R: 30.7
F: 33.88 F: 36.19 F: 29.7 F: 30.48 F: 39.58 F: 40.95 F: 42.58 F: 39.72 F: 39.77
Table 1: Performance of DT (GR), DT (DW) and DT (GRW) (proposed by (Nguyen et al, 2009)) and their modified
and improved versions on the converted AIMed corpus.
RE experiments carried out on newspaper text
corpora (such as ACE 2004) have indicated that ker-
nels based on PST obtain better results than kernels
based on DT. Interestingly, our experiments on a
biomedical text corpus indicate an opposite trend.
Intuitively, this might be due to the different na-
ture of the PPI task. PPI can be often identified by
spotting cue words such as interaction, binding, etc,
since the interacting entities (i.e. proteins) usually
have direct syntactic dependency relation on such
cue words. This might have allowed kernels based
on DT to be more accurate.
Although tree kernels applied on DT and PST
structures have produced high performance on cor-
pora of news text (Zhou et al, 2007; Nguyen et al,
2009), in case of biomedical text the results that we
obtained are relatively low. This may be due to the
fact that biomedical texts are different from newspa-
per texts: more variation in vocabulary, more com-
plex naming of (bio) entities, more diversity of the
valency of verbs and so on.
One important finding of our experiments is the
effectiveness of the mild extension of DT struc-
tures. MEDT-3 achieves the best result for all ker-
nels (SST, SST+bow, PTK and uPTK). However, the
other two versions of MEDT appear to be less effec-
tive.
In general, the empirical outcome suggests that
uPTK can better exploit our proposed DT structures
as well as PST. The superiority of uPTK on PTK
demonstrates that single lexical features (i.e. fea-
tures with flat structure) tend to overfit.
Finally, we have performed statistical tests to as-
sess the significance of our results. For each kernel
(i.e. SST, SST+bow, PTK, uPTK), the PPI predic-
tions using the best structure (i.e. MEDT-3 applied
to SOGRL) are compared against the predictions of
the other structures. The tests were performed using
the approximate randomization procedure (Noreen,
1989). We set the number of iterations to 1,000 and
the confidence level to 0.01. According to the tests,
for each kernel, our best structure produces signifi-
cantly better results.
5.3 Comparison with previous work
To the best of our knowledge, the only work on tree
kernel applied on dependency trees that we can di-
rectly compare to ours is reported by S?tre et al
(2007). Their DT kernel achieved an F-score of
37.1 on AIMed corpus which is lower than our best
results. As discussed earlier, Miwa et al (2009))
also used tree kernel on dependency analyses and
achieved a much higher result. However, the tree
structure they used contains multiple nodes for a sin-
gle word and this does not comply with the con-
straints usually applied to dependency tree structures
(refer to Section 2.3). It would be interesting to ex-
amine why such type of tree representation leads to
130
DT DT DT DT MEDT-1 MEDT-2 MEDT-3 PST
(SOGRPL) (OGRL) (SOGRW) (SOGRL) (SOGRL) (SOGRL) (SOGRPL) (PET)
SST P: 57.59 P: 54.38 P: 51.49 P: 54.08 P: 58.15 P: 54.46 P: 59.55 P: 52.72
R: 33.0 R: 33.5 R: 31.2 R: 33.8 R: 34.6 R: 33.6 R: 37.1 R: 35.9
F: 41.96 F: 41.46 F: 38.86 F: 41.6 F: 43.39 F: 41.56 F: 45.72 F: 42.71
SST P: 60.31 P: 53.22 P: 50.08 P: 53.26 P: 58.84 P: 52.87 P: 59.35 P: 52.88
+ R: 30.7 R: 33.1 R: 30.9 R: 32.7 R: 32.6 R: 32.2 R: 34.9 R: 37.7
bow F: 40.69 F: 40.82 F: 38.22 F: 40.52 F: 41.96 F: 40.02 F: 43.95 F: 44.02
PT P: 55.45 P: 49.78 P: 51.05 P: 51.61 P: 52.94 P: 50.89 P: 54.1 P: 58.39
R: 34.6 R: 34.6 R: 34.1 R: 36.9 R: 36.0 R: 37.0 R: 38.9 R: 36.9
F: 42.61 F: 40.82 F: 40.89 F: 43.03 F: 42.86 F: 42.85 F: 45.26 F: 45.22
uPT P: 56.2 P: 50.87 P: 50.0 P: 52.74 P: 55.0 P: 52.17 P: 56.85 P: 56.6
R: 32.2 R: 35.0 R: 33.0 R: 35.6 R: 34.1 R: 34.8 R: 39.0 R: 38.6
F: 40.94 F: 41.47 F: 39.76 F: 42.51 F: 42.1 F: 41.75 F: 46.26 F: 45.9
Table 2: Performance of the other improved versions of DT kernel structures (including MEDT kernels) as well as
PST (PET) kernel (Moschitti, 2004; Nguyen et al, 2009) on the converted AIMed corpus.
a better result.
In this work, we compare the performance of tree
kernels applied of DT with that of PST. Previously,
Tikk et al (2010) applied similar kernels on PST for
exactly the same task and data set. They reported
that SST and PTK (on PST) achieved F-scores of
26.2 and 34.6, respectively on the converted AIMed
corpus (refer to Table 2 in their paper). Such results
do not match our figures obtained with the same
kernels on PST. We obtain much higher results for
those kernels. It is difficult to understand the rea-
son for such differences between our and their re-
sults. A possible explanation could be related to pa-
rameter settings. Another source of uncertainty is
given by the tool for tree kernel computation, which
in their case is not mentioned. Moreover, their de-
scription of PT and SST (in Figure 1 of their paper)
appears to be imprecise: for example, in (partial or
complete) phrase structure trees, words can only ap-
pear as leaves but in their figure they appear as non-
terminal nodes.
The comparison with other kernel approaches (i.e.
not necessarily tree kernels on DT or PST) shows
that there are model achieving higher results (e.g.
Giuliano et al (2006), Kim et al (2010), Airola et
al. (2008), etc). State-of-the-art results on most of
the PPI data sets are obtained by the hybrid kernel
presented in Miwa et al (2009). As noted earlier,
our work focuses on the design of an effective DTK
for PPI that can be combined with others and that
can hopefully be used to design state-of-the-art hy-
brid kernels.
6 Conclusion
In this paper, we have proposed a study of PPI ex-
traction from specific biomedical data based on tree
kernels. We have modeled and experimented with
new kernels and DT structures, which can be ex-
ploited for RE tasks in other domains too.
More specifically, we applied four different tree
kernels on existing and newly proposed DT and PST
structures. We have introduced some extensions of
DT kernel structures which are linguistically moti-
vated. We call these as mildly extended DT kernels.
We have also shown that in PPI extraction lexical
information can lead to overfitting as uPTK outper-
forms PTK. In general, the empirical results show
that our DT structures perform better than the previ-
ously proposed PST and DT structures.
The ultimate objective of our work is to improve
tree kernels applied to DT and then combine them
with other types of kernels and data to produce more
accurate models.
Acknowledgments
This work was carried out in the context of the project
?eOnco - Pervasive knowledge and data management in
cancer care?. The authors would like to thank the anony-
mous reviewers for providing excellent feedback.
131
References
A Airola, S Pyysalo, J Bj?orne, T Pahikkala, F Gin-
ter, and T Salakoski. 2008. A graph kernel for
protein-protein interaction extraction. In Proceedings
of BioNLP 2008, pages 1?9, Columbus, USA.
R Bunescu and R Mooney. 2005. A shortest path depen-
dency kernel for relation extraction. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 724?731, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
R Bunescu and RJ Mooney. 2006. Subsequence ker-
nels for relation extraction. In Proceedings of the
19th Conference on Neural Information Processing
Systems, pages 171?178.
R Bunescu, R Ge, RJ Kate, EM Marcotte, RJ Mooney,
AK Ramani, and YW Wong. 2005. Compara-
tive experiments on learning information extractors
for proteins and their interactions. Artificial Intelli-
gence in Medicine (Special Issue on Summarization
and Information Extraction from Medical Documents),
33(2):139?155.
M Collins and N Duffy. 2001. Convolution kernels for
natural language. In Proceedings of Neural Informa-
tion Processing Systems (NIPS?2001).
A Culotta and J Sorensen. 2004. Dependency tree ker-
nels for relation extraction. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics, Barcelona, Spain.
G Erkan, A Ozgur, and DR Radev. 2007. Semi-
Supervised Classification for Extracting Protein Inter-
action Sentences using Dependency Parsing. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-CoNLL
2007), pages 228?237.
C Giuliano, A Lavelli, and L Romano. 2006. Exploit-
ing shallow linguistic information for relation extrac-
tion from biomedical literature. In Proceedings of the
11th Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL?2006),
pages 401?408, Trento, Italy.
CW Hsu, CC Chang, and CJ Lin, 2003. A practical guide
to support vector classification. Department of Com-
puter Science and Information Engineering, National
Taiwan University, Taipei, Taiwan.
T Joachims. 1999. Making large-scale support vec-
tor machine learning practical. In Advances in ker-
nel methods: support vector learning, pages 169?184.
MIT Press, Cambridge, MA, USA.
S Kim, J Yoon, J Yang, and S Park. 2010. Walk-weighted
subsequence kernels for protein-protein interaction ex-
traction. BMC Bioinformatics, 11(1).
H Lodhi, C Saunders, J Shawe-Taylor, N Cristianini, and
C Watkins. 2002. Text classification using string ker-
nels. Journal of Machine Learning Research, 2:419?
444, March.
M Miwa, R S?tre, Y Miyao, T Ohta, and J Tsujii. 2009.
Protein-protein interaction extraction by leveraging
multiple kernels and parsers. International Journal of
Medical Informatics, 78.
A Moschitti. 2004. A study on convolution kernels for
shallow semantic parsing. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics, ACL ?04, Barcelona, Spain.
A Moschitti. 2006. Efficient convolution kernels for de-
pendency and constituent syntactic trees. In Johannes
Fu?rnkranz, Tobias Scheffer, and Myra Spiliopoulou,
editors, Machine Learning: ECML 2006, volume 4212
of Lecture Notes in Computer Science, pages 318?329.
Springer Berlin / Heidelberg.
A Moschitti. 2006a. Making Tree Kernels Practical for
Natural Language Learning. In Proceedings of the
11th Conference of the European Chapter of the As-
sociation for Computational Linguistics, Trento, Italy.
TT Nguyen, A Moschitti, and G Riccardi. 2009. Con-
volution kernels on constituent, dependency and se-
quential structures for relation extraction. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing (EMNLP?2009), pages
1378?1387, Singapore, August.
EW Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience.
S Pyysalo, A Airola, J Heimonen, J Bjo?rne, F Ginter,
and T Salakoski. 2008. Comparative analysis of five
protein-protein interaction corpora. BMC Bioinfor-
matics, 9(Suppl 3):S6.
R S?tre, K Sagae, and J Tsujii. 2007. Syntactic features
for protein-protein interaction extraction. In Proceed-
ings of the Second International Symposium on Lan-
guages in Biology and Medicine (LBM 2007), pages
6.1?6.14, Singapore.
A Severyn and A Moschitti. 2010. Fast cutting plane
training for structural kernels. In Proceedings of
ECML-PKDD.
D Tikk, P Thomas, P Palaga, J Hakenberg, and U Leser.
2010. A Comprehensive Benchmark of Kernel Meth-
ods to Extract Protein-Protein Interactions from Liter-
ature. PLoS Computational Biology, 6(7), July.
SVN Vishwanathan and AJ Smola. 2002. Fast kernels on
strings and trees. In Proceedings of Neural Informa-
tion Processing Systems (NIPS?2002), pages 569?576,
Vancouver, British Columbia, Canada.
D Zhang and WS Lee. 2003. Question classification us-
ing support vector machines. In Proceedings of the
132
26th annual international ACM SIGIR conference on
Research and development in information retrieval,
SIGIR ?03, pages 26?32, Toronto, Canada.
M Zhang, J Su, D Wang, G Zhou, and CL Tan. 2005.
Discovering relations between named entities from a
large raw corpus using tree similarity-based clustering.
In Robert Dale, Kam-Fai Wong, Jian Su, and Oi Yee
Kwong, editors, Natural Language Processing IJC-
NLP 2005, volume 3651 of Lecture Notes in Computer
Science, pages 378?389. Springer Berlin / Heidelberg.
GD Zhou, M Zhang, DH Ji, and QM Zhu. 2007. Tree
kernel-based relation extraction with context-sensitive
structured parse tree information. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
728?736, June.
133
Proceedings of the Fifth Law Workshop (LAW V), pages 101?109,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Assessing the Practical Usability
of an Automatically Annotated Corpus
Md. Faisal Mahbub Chowdhury ? ? and Alberto Lavelli ?
? Human Language Technology Research Unit, Fondazione Bruno Kessler, Trento, Italy
? Department of Information Engineering and Computer Science, University of Trento, Italy
{chowdhury,lavelli}@fbk.eu
Abstract
The creation of a gold standard corpus (GSC)
is a very laborious and costly process. Silver
standard corpus (SSC) annotation is a very re-
cent direction of corpus development which
relies on multiple systems instead of human
annotators. In this paper, we investigate the
practical usability of an SSC when a machine
learning system is trained on it and tested on
an unseen benchmark GSC. The main focus of
this paper is how an SSC can be maximally ex-
ploited. In this process, we inspect several hy-
potheses which might have influenced the idea
of SSC creation. Empirical results suggest that
some of the hypotheses (e.g. a positive impact
of a large SSC despite of having wrong and
missing annotations) are not fully correct. We
show that it is possible to automatically im-
prove the quality and the quantity of the SSC
annotations. We also observe that considering
only those sentences of SSC which contain an-
notations rather than the full SSC results in a
performance boost.
1 Introduction
The creation of a gold standard corpus (GSC) is
not only a very laborious task due to the manual ef-
fort involved but also a costly and time consuming
process. However, the importance of the GSC to ef-
fectively train machine learning (ML) systems can-
not be underestimated. Researchers have been trying
for years to find alternatives or at least some com-
promise. As a result, self-training, co-training and
unsupervised approaches targeted for specific tasks
(such as word sense disambiguation, syntactic pars-
ing, etc) have emerged. In the process of these re-
searches, it became clear that the size of the (manu-
ally annotated) training corpus has an impact on the
final outcome.
Recently an initiative is ongoing in the context of
the European project CALBC1 which aims to create
a large, so called silver standard corpus (SSC) us-
ing harmonized annotations automatically produced
by multiple systems (Rebholz-Schuhmann et al,
2010; Rebholz-Schuhmann et al, 2010a; Rebholz-
Schuhmann et al, 2010b). The basic idea is that
independent biomedical named entity recognition
(BNER) systems annotate a large corpus of biomed-
ical articles without any restriction on the methodol-
ogy or external resources to be exploited. The differ-
ent annotations are automatically harmonized using
some criteria (e.g. minimum number of systems to
agree on a certain annotation) to yield a consensus
based corpus. This consensus based corpus is called
silver standard corpus because, differently from a
GSC, it is not created exclusively by human anno-
tators. Several factors can influence the quantity and
quality of the annotations during SSC development.
These include varying performance, methodology,
annotation guidelines and resources of the SSC an-
notation systems (henceforth annotation systems).
The annotation of SSC in the framework of the
CALBC project is focused on (bio) entity mentions
(a specific application of the named entity recogni-
tion (NER)2 task). However, the idea of SSC cre-
ation might also be applied to other types of anno-
tations, e.g. annotation of relations among entities,
annotation of treebanks and so on. Hence, if it can be
1http://www.ebi.ac.uk/Rebholz-srv/CALBC/project.html
2Named entity recognition is the task of locating boundaries
of the entity mentions in a text and tagging them with their cor-
responding semantic types (e.g. person, location, disease and
so on).
101
shown that an SSC is a useful resource for the NER
task, similar resources can be developed for anno-
tation of information other than entities and utilized
for the relevant natural language processing (NLP)
tasks.
The primary objective of SSC annotation is to
compensate the cost, time and manual effort re-
quired for a GSC. The procedure of SSC develop-
ment is inexpensive, fast and yet capable of yielding
huge amount of annotated data. These advantages
trigger several hypotheses. For example:
? The size of annotated training corpus always
plays a crucial role in the performance of ML
systems. If the annotation systems have very
high precision and somewhat moderate recall,
they would be also able to annotate automat-
ically a huge SSC which would have a good
quality of annotations. So, one might assume
that, even if such an SSC may contain wrong
and missing annotations, a relatively 15 or 20
times bigger SSC than a smaller GSC should
allow an ML based system to ameliorate the ad-
verse effects of the erroneous annotations.
? Rebholz-Schuhmann et al (2010) hypothesized
that an SSC might serve as an approximation of
a GSC.
? In the absence of a GSC, it is expected that
ML systems would be able to exploit the har-
monised annotations of an SSC to annotate un-
seen text with reasonable accuracy.
? An SSC could be used to semi-automate the an-
notations of a GSC. However, in that case, it
is expected that the annotation systems would
have very high recall. One can assume that
converting an SSC into a GSC would be less
time consuming and less costly than develop-
ing a GSC from scratch.
All these hypotheses are yet to be verified. Nev-
ertheless, once we have an SSC annotated with cer-
tain type of information, the main question would be
how this corpus can be maximally exploited given
the fact that it might be created by annotation sys-
tems that used different resources and possibly not
the same annotation guidelines. This question is di-
rectly related to the practical usability of an SSC,
which is the focus of this paper.
Taking the aforementioned hypotheses into ac-
count, our goal is to investigate the following re-
search questions which are fundamental to the max-
imum exploitation of an SSC:
1. How can the annotation quality of an SSC be
improved automatically?
2. How would a system trained on an SSC per-
form if tested on an unseen benchmark GSC?
3. Can an SSC combined with a GSC produce a
better trained system?
4. What would be the impact on system perfor-
mance if unannotated sentences3 are removed
from an SSC?
5. What would be the effects of the variation in
the size of an SSC on precision and recall?
Our goal is not to judge the procedure of SSC cre-
ation, rather our objective is to examine how an SSC
can be exploited automatically and maximally for a
specific task. Perhaps this would provide useful in-
sights to re-evaluate the approach of SSC creation.
For our experiments, we use a benchmark GSC
called the BioCreAtIvE II GM corpus (Smith et
al., 2008) and the CALBC SSC-I corpus (Rebholz-
Schuhmann et al, 2010a). Both of these corpora
are annotated with genes. Our motivation behind the
choice of a gene annotated GSC for the SSC evalu-
ation is that ML based BNER for genes has already
achieved a sufficient level of maturity. This is not
the case for other important bio-entity types, primar-
ily due to the absence of training GSC of adequate
size. In fact, for many bio-entity types there exist no
GSC. If we can achieve a reasonably good baseline
for gene mention identification by maximizing the
exploitation of SSC, we might be able to apply al-
most similar strategies to exploit SSC for other bio-
entity types, too.
The remaining of this paper is organised as fol-
lows. Section 2 includes brief discussion of the re-
lated work. Apart from mentioning the related liter-
ature, this section also underlines the difference of
3For the specific SSC that we use in this work, unannotated
sentences correspond to those sentences that contain no gene
annotation.
102
SSC development with respect to approaches such
as self-training and co-training. Then in Section 3,
we describe the data used in our experiments and the
experimental settings. Following that, in Section 4,
empirical results are presented and discussed. Fi-
nally, we conclude with a description of what we
learned from this work in Section 5.
2 Related Work
As mentioned, the concept of SSC has been initi-
ated by the CALBC project (Rebholz-Schuhmann et
al., 2010a; Rebholz-Schuhmann et al, 2010). So far,
two versions of SSC have been released as part of the
project. The CALBC SSC-I has been harmonised
from the annotations of the systems provided by
the four project partners. Three of them are dictio-
nary based systems while the other is an ML based
system. The systems utilized different types of re-
sources such as GENIA corpus (Kim et al, 2003),
Entrez Genes4, Uniprot5, etc. The CALBC SSC-
II corpus has been harmonised from the annotations
done by the 11 participants of the first CALBC chal-
lenge and the project partners.6 Some of the par-
ticipants have used the CALBC SSC-I versions for
training while others used various gene databases or
benchmark GSCs such as the BioCreAtIvE II GM
corpus.
One of the key questions regarding an SSC would
be how close its annotation quality is to a corre-
sponding GSC. On the one hand, every GSC con-
tains its special view of the correct annotation of a
given corpus. On the other hand, an SSC is created
by systems that might be trained with resources hav-
ing different annotation standards. So, it is possible
that the annotations of an SSC significantly differ
with respect to a manually annotated (i.e., gold stan-
dard) version of the same corpus. This is because
human experts are asked to follow specific annota-
tion guidelines.
Rebholz-Schuhmann and Hahn (2010c) did an in-
trinsic evaluation of the SSC where they created an
4http://jura.wi.mit.edu/entrez gene/
5http://www.uniprot.org/
6See proceedings of the 1st CALBC Work-
shop, 2010, Editors: Dietrich Rebholz-Schuhmann
and Udo Hahn (http://www.ebi.ac.uk/Rebholz-
srv/CALBC/docs/FirstProceedings.pdf) for details.
SSC and a GSC on a dataset of 3,236 Medline7 ab-
stracts. They were not able to make any specific con-
clusion whether the SSC is approaching to the GSC.
They were of the opinion that SSC annotations are
more similar to terminological resources.
Hahn et al (2010) proposed a policy where sil-
ver standards can be dynamically optimized and cus-
tomized on demand (given a specific goal function)
using a gold standard as an oracle. The gold stan-
dard is used for optimization only, not for training
for the purpose of SSC annotation. They argued that
the nature of diverging tasks to be solved, the lev-
els of specificity to be reached, the sort of guide-
lines being preferred, etc should allow prospective
users of an SSC to customize one on their own and
not stick to something that is already prefabricated
without concrete application in mind.
Self-training and co-training are two of the exist-
ing approaches that have been used for compensat-
ing the lack of a training GSC with adequate size
in several different tasks such as word sense disam-
biguation, semantic role labelling, parsing, etc (Ng
and Cardie, 2003; Pierce and Cardie, 2004; Mc-
Closky et al, 2006; He and Gildea, 2006). Accord-
ing to Ng and Cardie (2003), self-training is the pro-
cedure where a committee of classifiers are trained
on the (gold) annotated examples to tag unannotated
examples independently. Only those new annota-
tions to which all the classifiers agree are added to
the training set and classifiers are retrained. This
procedure repeats until a stop condition is met. Ac-
cording to Clark et al (2003), self-training is a pro-
cedure in which ?a tagger is retrained on its own la-
beled cache at each round?. In other words, a sin-
gle classifier is trained on the initially (gold) anno-
tated data and then applied on a set of unannotated
data. Those examples meeting a selection criterion
are added to the annotated dataset and the classifier
is retrained on this new data set. This procedure can
continue for several rounds as required.
Co-training is another weakly supervised ap-
proach (Blum and Mitchell, 1998). It applies for
those tasks where each of the two (or more) sets of
features from the initially (gold) annotated training
data is sufficient to classify/annotate the unannotated
data (Pierce and Cardie, 2001; Pierce and Cardie,
7http://www.nlm.nih.gov/databases/databases medline.html
103
2004; He and Gildea, 2006). As with SSC annota-
tion and self-training, it also attempts to increase the
amount of annotated data by making use of unanno-
tated data. The main idea of co-training is to repre-
sent the initially annotated data using two (or more)
separate feature sets, each called a ?view?. Then,
two (or more) classifiers are trained on those views
of the data which are then used to tag new unanno-
tated data. From this newly annotated data, the most
confident predictions are added to the previously an-
notated data. This whole process may continue for
several iterations. It should be noted that, by limit-
ing the number of views to one, co-training becomes
self-training.
Like the SSC, the multiple classifier approach
of self-training and co-training, as described above,
adopts the same vision of utilizing automatic sys-
tems for producing the annotation. Apart from that,
SSC annotation is completely different from both
self-training and co-training. For example, classi-
fiers in self-training and co-training utilizes the same
(manually annotated) resource for their initial train-
ing. But SSC annotation systems do not necessar-
ily use the same resource. Both self-training and
co-training are weakly supervised approaches where
the classifiers are based on supervised ML tech-
niques. In the case of SSC annotation, the annota-
tion systems can be dictionary based or rule based.
This attractive flexibility allows SSC annotation to
be a completely unsupervised approach since the
annotation systems do not necessarily need to be
trained.
3 Experimental settings
We use the BioCreAtIvE II GM corpus (henceforth,
only the GSC) for evaluation of an SSC. The training
corpus in the GSC has in total 18,265 gene annota-
tions in 15,000 sentences. The GSC test data has
6,331 annotations in 5,000 sentences.
Some of the CALBC challenge participants have
used the BioCreAtIvE II GM corpus for training to
annotate gene/protein in the CALBC SSC-II corpus.
We wanted our benchmark corpus and benchmark
corpus annotation to be totally unseen by the sys-
tems that annotated the SSC to be used in our experi-
ments so that there is no bias in our empirical results.
SSC-I satisfies this criteria. So, we use the SSC-I
(henceforth, we would refer the CALBC SSC-I as
simply the SSC) in our experiments despite the fact
that it is almost 3 times smaller than the SSC-II.
The SSC has in total 137,610 gene annotations in
316,869 sentences of 50,000 abstracts.
Generally, using a customized dictionary of en-
tity names along with annotated corpus boosts NER
performance. However, since our objective is to ob-
serve to what extent a ML system can learn from
SSC, we avoid the use of any dictionary. We use
an open source ML based BNER system named
BioEnEx8 (Chowdhury and Lavelli, 2010). The
system uses conditional random fields (CRFs), and
achieves comparable results (F1 score of 86.22% on
the BioCreAtIvE II GM test corpus) to that of the
other state-of-the-art systems without using any dic-
tionary or lexicon.
One of the complex issues in NER is to come to an
agreement regarding the boundaries of entity men-
tions. Different annotation guidelines have different
preferences. There may be tasks where a longer en-
tity mention such as ?human IL-7 protein? may be
appropriate, while for another task a short one such
as ?IL-7? is adequate (Hahn et al, 2010).
However, usually evaluation on BNER corpora
(e.g., the BioCreAtIvE II GM corpus) is performed
adopting exact boundary match. Given that we have
used the official evaluation script of the BioCre-
AtIvE II GM corpus, we have been forced to
adopt exact boundary match. Considering a relaxed
boundary matching (i.e. the annotations might dif-
fer in uninformative terms such as the, a, acute, etc.)
rather than exact boundary matching might provide
a slightly different picture of the effectiveness of the
SSC usage.
4 Results and analyses
4.1 Automatically improving SSC quality
The CALBC SSC-I corpus has a negligible num-
ber of overlapping gene annotations (in fact, only 6).
For those overlapping annotations, we kept only the
longest ones. Our hypothesis is that a certain token
in the same context can refer to (or be part of) only
one concept name (i.e. annotation) of a certain se-
mantic group (i.e. entity type). After removing these
few overlaps, the SSC has 137,604 annotations. We
8Freely available at http://hlt.fbk.eu/en/people/chowdhury/research
104
will refer to this version of the SSC as the initial
SSC (ISSC).
We construct a list9 using the lemmatized form
of 132 frequently used words that appear in gene
names. These words cannot constitute a gene name
themselves. If (the lemmatized form of) all the
words in a gene name belong to this list then that
gene annotation should be discarded. We use this list
to remove erroneous annotations in the ISSC. After
this purification step, the total number of annotations
is reduced to 133,707. We would refer to this version
as the filtered SSC (FSSC).
Then, we use the post-processing module of
BioEnEx, first to further filter out possible wrong
gene annotations in the FSSC and then to automati-
cally include potential gene mentions which are not
annotated. It has been observed that some of the
annotated mentions in the SSC-I span only part of
the corresponding token10. For example, in the to-
ken ?IL-2R?, only ?IL-? is annotated. We extend
the post-processing module of BioEnEx to automat-
ically identify all such types of annotations and ex-
pand their boundaries when their neighbouring char-
acters are alphanumeric.
Following that, the extended post-processing
module of BioEnEx is used to check in every sen-
tence whether there exist any potential unannotated
mentions11 which differ from any of the annotated
mentions (in the same sentence) by a single charac-
ter (e.g. ?IL-2L? and ?IL-2R?), number (e.g. ?IL-
2R? and ?IL-345R?) or Greek letter (e.g. ?IFN-
alpha? and ?IFN-beta?). After this step, the total
number of gene annotations is 144,375. This means
that we were able to remove/correct some specific
types of errors and then further expand the total
number of annotations (by including entities not an-
notated in the original SSC) up to 4.92% with re-
spect to the ISSC. We will refer to this expanded
version of the SSC as the processed SSC (PSSC).
When BioEnEx is trained on the above versions
9The words are collected from
http://pir.georgetown.edu/pirwww/iprolink/general name
and the annotation guideline of GENETAG (Tanabe et al,
2005).
10By token we mean a sequence of consecutive non-
whitespace characters.
11Any token or sequence of tokens is considered to verify
whether it should be annotated or not, if its length is more than
2 characters excluding digits and Greek letters.
TP FP FN P R F1
ISSC 2,396 594 3,935 80.13 37.85 51.41
FSSC 2,518 557 3,813 81.89 39.77 53.54
PSSC 2,606 631 3,725 80.51 41.16 54.47
Table 1: The results of experiments when trained with
different versions of the SSC and tested on the GSC test
data.
of the SSC and tested on the GSC test data, we ob-
served an increase of more than 3% of F1 score be-
cause of the filtering and expansion (see Table 1).
One noticeable characteristic in the results is that the
number of annotations obtained (i.e. TP+FP12) by
training on any of the versions of the SSC is almost
half of the actual number annotations of the GSC test
data. This has resulted in a low recall. There could
be mainly two reasons behind this outcome:
? First of all, it might be the case that a consid-
erable number of gene names are not annotated
inside the SSC versions. As a result, the fea-
tures shared by the annotated gene names (i.e.
TP) and unnannotated gene names (i.e. FN)
might not have enough influence.
? There might be a considerable number of
wrong annotations which are actually not genes
(i.e. FP). Consequently, a number of bad fea-
tures might be collected from those wrong an-
notations which are misleading the training
process.
To verify the above conditions, it would be re-
quired to annotate the huge CALBC SSC manually.
This would be not feasible because of the cost of
human labour and time. Nevertheless, we can try to
measure the state of the above conditions roughly by
using only annotated sentences (i.e. sentences con-
taining at least one annotation) and varying the size
of the corpus, which are the subjects of our next ex-
periments.
12TP (true positive) = corresponding annotation done by the
system is correct, FP (false positive) = corresponding anno-
tation done by the system is incorrect, FN (false negative) =
corresponding annotation is correct but it is not annotated by
the system.
105
Figure 1: Graphical representation of the experimental
results with varying size of the CSSC.
4.2 Impact of annotated sentences and
different sizes of the SSC
We observe that only 77,117 out of the 316,869
sentences in the PSSC contain gene annotations.
We will refer to the sentences having at least one
gene annotation collectively as the condensed SSC
(CSSC). Table 2 and Figure 1 show the results when
we used different portions of the CSSC for training.
There are four immediate observations on the
above results:
? Using the full PSSC, we obtain total (i.e.
TP+FP) 3,237 annotations on the GSC test
data. But when we use only annotated sen-
tences of the PSSC (i.e. the CSSC), the total
number of annotations is 4,562, i.e. there is an
increment of 40.93%.
? Although we have a boost in F1 score due to the
increase in recall using the CSSC in place of the
PSSC, there is a considerable drop in precision.
? The number of FP is almost the same for the
usage of 10-75% of the CSSC.
? The number of FN kept decreasing (and TP
kept increasing) for 10-75% of the CSSC.
These observations can be interpreted as follows:
? Unannotated sentences inside the SSC in real-
ity contain many gene annotations; so the in-
clusion of such sentences misleads the training
process of the ML system.
? Some of the unannotated sentences actually
do not contain any gene names, while others
would contain such names but the automatic
annotations missed them. As a consequence,
the former sentences contain true negative ex-
amples which could provide useful features that
can be exploited during training so that less FPs
are produced (with a precision drop using the
CSSC). So, instead of simply discarding all the
unannotated sentences, we could adopt a filter-
ing strategy that tries to distinguish between the
two classes of sentences above.
? The experimental results with the increasing
size of the CSSC show a decrease in both pre-
cision (74.55 vs 76.17) and recall (53.72 vs
54.04). We plan to run again these experiments
with different randomized splits to better assess
the performance.
? Even using only 10% of the whole CSSC does
not produce a drastic difference with the results
when the full CSSC is used. This indicates that
perhaps the more CSSC data is fed, the more
the system tends to overfit.
? It is evident that the more the size of the CSSC
increases, the lower the improvement of F1
score, if the total number of annotations in
the newly added sentences and the accuracy of
the annotations are not considerably higher. It
might be not surprising if, after the addition of
more sentences in the CSSC, the F1 score drops
further rather than increasing. The assumption
that having a huge SSC would be beneficiary
might not be completely correct. There might
be some optimal limit of the SSC (depending
on the task) that can provide maximum bene-
fits.
4.3 Training with the GSC and the SSC
together
Our final experiments were focused on whether it is
possible to improve performance by simply merg-
ing the GSC training data with the PSSC and the
CSSC. The PSSC has almost 24 times the num-
ber of sentences and almost 8 times the number of
gene annotations than the GSC. There is a possibility
that, when we do a simple merge, the weight of the
106
Total tokens in the corpus No of annotated genes TP FP FN P R F1
PSSC 6,955,662 144,375 2,606 631 3,725 80.51 41.16 54.47
100% of CSSC 1,983,113 144,375 3,401 1,161 2,930 74.55 53.72 62.44
75% of CSSC 1,487,823 108,213 3,421 1,070 2,910 76.17 54.04 63.22
50% of CSSC 992,392 72,316 3,265 1,095 3,066 74.89 51.57 61.08
25% of CSSC 494,249 35,984 3,179 1,048 3,152 75.21 50.21 60.22
10% of CSSC 196,522 14,189 2,988 1,097 3,343 73.15 47.20 57.37
Table 2: The results of SSC experiments with varying size of the CSSC = condensed SSC (i.e. sentences containing
at least one annotation). SSC size = 316,869 sentences. CSSC size = 77,117.
TP FP FN P R F1
GSC 5,373 759 958 87.62 84.87 86.22
PSSC +
GSC 3,745 634 2,586 85.52 59.15 69.93
PSSC +
GSC * 8 4,163 606 2,168 87.29 65.76 75.01
CSSC +
GSC * 8 4,507 814 1,824 84.70 71.19 77.36
Table 3: The results of experiments by training on the
GSC training data merged with the PSSC and the CSSC.
gold annotations would be underestimated. So, apart
from doing a simple merge, we also try to balance
the annotations of the two corpora. There are two
options to do this ? (i) by duplicating the GSC train-
ing corpus 8 times to make its total number of anno-
tations equal to that of the PSSC, or (ii) by choos-
ing randomly a portion of the PSSC that would have
almost similar amount of annotations as that of the
GSC. We choose the 1st option.
Unfortunately, when an SSC (i.e. the PSSC or the
CSSC) is combined with the GSC, the result is far
below than that of using the GSC only (see Table 3).
Again, low recall is the main issue partly due to the
lower number of annotations (i.e. TP+FP) done by
the system trained on an SSC and the GSC instead of
the GSC only. As we know, a GSC is manually an-
notated following precise guidelines, while an SSC
is annotated with automatic systems that do not nec-
essarily follow the same guidelines as a GSC. So,
it would not have been surprising if the number of
annotations were high (since we have much bigger
training corpus due to SSC) but precision were low.
But in practice, precision obtained by combining an
SSC and the GSC is almost as high as the precision
achieved using the GSC.
One reason for the lower number of annotations
might be the errors that have been propagated in-
side the SSC. Some of the systems that have been
used for the annotation of the SSC might have low
recall. As a result, during harmonization of their an-
notations several valid gene mentions might not have
been included13.
One other possible reason could be the difference
in the entity name boundaries in the GSC and an
SSC. We have checked some of the SSC annotations
randomly. It appears that in those annotated entity
names some relevant (neighbouring) words (in the
corresponding sentences) are not included. It is most
likely that the SSC annotation systems had disagree-
ments on those words.
When the annotations of the GSC were given
higher preference (by duplicating), there is a sub-
stantial improvement in the F1 score, although still
lower than the result with the GSC only.
5 Conclusions
The idea of SSC development is simple and yet at-
tractive. Obtaining better results on a test dataset
by combining output of multiple (accurate and di-
verse14) systems is not new (Torii et al, 2009; Smith
et al, 2008). But adopting this strategy for cor-
13There can be two reasons for this ? (i) when a certain valid
gene name is not annotated by any of the annotation systems,
and (ii) when only a few of those systems have annotated the
valid name but the total number of such systems is below than
the minimum required number of agreements, and hence the
gene name is not considered as an SSC annotation.
14A system is said to be accurate if its classification perfor-
mance is better than a random classification. Two systems are
considered diverse if they do not make the same classification
mistakes. (Torii et al, 2009)
107
pus development is a novel and unconventional ap-
proach. Some natural language processing tasks (es-
pecially the new ones) lack adequate GSCs to be
used for the training of ML based systems. For such
tasks, domain experts can provide patterns or rules
to build systems that can be used to annotate an ini-
tial version of SSC. Such systems might lack high
recall but are expected to have high precision. Al-
ready available task specific lexicons or dictionaries
can also be utilized for SSC annotation. Such an
initial version of SSC can be later enriched using
automatic process which would utilize existing an-
notations in the SSC.
With this vision in mind, we pose ourselves sev-
eral questions (see Section 1) regarding the practi-
cal usability and exploitation of an SSC. Our experi-
ments are conducted on a publicly available biomed-
ical SSC developed for the training of biomedical
NER systems. For the evaluation of a state-of-the-
art ML system trained on such an SSC, we use a
widely used benchmark biomedical GSC.
In the search of answers for our questions, we ac-
cumulate several important empirical observations.
We have been able to automatically reduce the num-
ber of erroneous annotations from the SSC and in-
clude unannotated potential entity mentions simply
using the annotations that the SSC already provides.
Our techniques have been effective for improving
the annotation quality as there is a considerable in-
crement of F1 score (almost 11% higher when we
use CSSC instead of using ISSC; see Table 1 and 2).
We also observe that it is possible to obtain more
than 80% of precision using the SSC. But recall re-
mains quite low, partly due to the low number of
annotations provided by the system trained with the
SSC. Perhaps, the entity names in the SSC that are
missed by the annotation systems is one of the rea-
sons for that.
Perhaps, the most interesting outcome of this
study is that, if only annotated sentences (which
we call condensed corpus) are considered, then the
number of annotations as well as the performance
increases significantly. This indicates that many
unannotated sentences contain annotations missed
by the automatic annotation systems. However, it
appears that correctly unannotated sentences influ-
ence the achievement of high precision. Maybe a
more sophisticated approach should be adopted in-
stead of completely discarding the unannotated sen-
tences, e.g. devising a filter able to distinguish
between relevant unannotated sentences (i.e., those
that should contain annotations) from non-relevant
ones (i.e., those that correctly do not contain any an-
notation). Measuring lexical similarity between an-
notated and unannotated sentences might help in this
case.
We notice the size of an SSC affects performance,
but increasing it above a certain limit does not
always guarantee an improvement of performance
(see Figure 1). This rejects the hypothesis that hav-
ing a much larger SSC should allow an ML based
system to ameliorate the effect of having erroneous
annotations inside the SSC.
Our empirical results show that combining GSC
and SSC do not improve results for the particular
task of NER, even if GSC annotations are given
higher weights (through duplication). We assume
that this is partly due to the variations in the guide-
lines of entity name boundaries15. These impact the
learning of the ML algorithm. For other NLP tasks
where the possible outcome is boolean (e.g. relation
extraction, i.e. whether a particular relation holds
between two entities or not), we speculate the results
of such combination might be better.
We use a CRF based ML system for our exper-
iments. It would be interesting to see whether the
observations are similar if a system with a different
ML algorithm is used.
To conclude, this study suggests that an automat-
ically pre-processed SSC might already contain an-
notations with reasonable quality and quantity, since
using it we are able to reach more than 62% of F1
score. This is encouraging since in the absence of
a GSC, an ML system would be able to exploit an
SSC to annotate unseen text with a moderate (if not
high) accuracy. Hence, SSC development might be
a good option to semi-automate the annotation of a
GSC.
Acknowledgments
This work was carried out in the context of the project
?eOnco - Pervasive knowledge and data management in
cancer care?. The authors would like to thank Pierre
Zweigenbaum for useful discussion, and the anonymous
reviewers for valuable feedback.
15For example, ?human IL-7 protein? vs ?IL-7?.
108
References
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the 11th Annual Conference on Computational
learning theory (COLT?98), pages 92?100.
Md. Faisal Mahbub Chowdhury and Alberto Lavelli.
2010. Disease mention recognition with specific fea-
tures. In Proceedings of the Workshop on Biomedical
Natural Language Processing (BioNLP 2010), 48th
Annual Meeting of the Association for Computational
Linguistics, pages 83?90, Uppsala, Sweden, July.
Stephen Clark, James R. Curran, and Miles Osborne.
2003. Bootstrapping POS taggers using unlabelled
data. In Proceedings of the 7th Conference on Natural
Language Learning (CoNLL-2003), pages 49?55.
Udo Hahn, Katrin Tomanek, Elena Beisswanger, and Erik
Faessler. 2010. A proposal for a configurable silver
standard. In Proceedings of the 4th Linguistic Anno-
tation Workshop, 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 235?242,
Uppsala, Sweden, July.
Shan He and Daniel Gildea. 2006. Self-training and
co-training for semantic role labeling: Primary report.
Technical report, University of Rochester.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun?ichi
Tsujii. 2003. Genia corpus - semantically annotated
corpus for bio-textmining. Bioinformatics, 19(Suppl
1):i180?182.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In Proceedings of the 21st International Con-
ference on Computational Linguistics, pages 337?344,
Sydney, Australia.
Vincent Ng and Claire Cardie. 2003. Weakly supervised
natural language learning without redundant views.
In Proceedings of the 2003 Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL-2003), pages 173?180.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2001), pages 1?9.
David Pierce and Claire Cardie. 2004. Co-training and
self-training for word sense disambiguation. In Pro-
ceedings of the 8th Conference on Computational Nat-
ural Language Learning (CoNLL-2004), pages 33?40.
Dietrich Rebholz-Schuhmann and Udo Hahn. 2010c.
Silver standard corpus vs. gold standard corpus. In
Proceedings of the 1st CALBC Workshop, Cambridge,
U.K., June.
Dietrich Rebholz-Schuhmann, Antonio Jimeno, Chen Li,
Senay Kafkas, Ian Lewin, Ning Kang, Peter Corbett,
David Milward, Ekaterina Buyko, Elena Beisswanger,
Kerstin Hornbostel, Alexandre Kouznetsov, Rene
Witte, Jonas B Laurila, Christopher JO Baker, Chen-Ju
Kuo, Simon Clematide, Fabio Rinaldi, Richrd Farkas,
Gyrgy Mra, Kazuo Hara, Laura Furlong, Michael
Rautschka, Mariana Lara Neves, Alberto Pascual-
Montano, Qi Wei, Nigel Collier, Md. Faisal Mah-
bub Chowdhury, Alberto Lavelli, Rafael Berlanga,
Roser Morante, Vincent Van Asch, Walter Daele-
mans, Jose? Lu??s Marina, Erik van Mulligen, Jan Kors,
and Udo Hahn. 2010. Assessment of NER solu-
tions against the first and second CALBC silver stan-
dard corpus. In Proceedings of the fourth Interna-
tional Symposium on Semantic Mining in Biomedicine
(SMBM?2010), October.
Dietrich Rebholz-Schuhmann, Antonio Jose? Jimeno-
Yepes, Erik van Mulligen, Ning Kang, Jan Kors, David
Milward, Peter Corbett, Ekaterina Buyko, Elena Beis-
swanger, and Udo Hahn. 2010a. CALBC silver stan-
dard corpus. Journal of Bioinformatics and Computa-
tional Biology, 8:163?179.
Dietrich Rebholz-Schuhmann, Antonio Jose? Jimeno-
Yepes, Erik van Mulligen, Ning Kang, Jan Kors, David
Milward, Peter Corbett, Ekaterina Buyko, Katrin
Tomanek, Elena Beisswanger, and Udo Hahn. 2010b.
The CALBC silver standard corpus for biomedical
named entities ? a study in harmonizing the contri-
butions from four independent named entity taggers.
In Proceedings of the 7th International conference on
Language Resources and Evaluation (LREC?10), Val-
letta, Malta, May.
Larry Smith, Lorraine Tanabe, Rie Ando, Cheng-
Ju Kuo, I-Fang Chung, Chun-Nan Hsu, Yu-Shi
Lin, Roman Klinger, Christoph Friedrich, Kuzman
Ganchev, Manabu Torii, Hongfang Liu, Barry Had-
dow, Craig Struble, Richard Povinelli, Andreas Vla-
chos, William Baumgartner, Lawrence Hunter, Bob
Carpenter, Richard Tsai, Hong-Jie Dai, Feng Liu,
Yifei Chen, Chengjie Sun, Sophia Katrenko, Pieter
Adriaans, Christian Blaschke, Rafael Torres, Mariana
Neves, Preslav Nakov, Anna Divoli, Manuel Mana-
Lopez, Jacinto Mata, and W John Wilbur. 2008.
Overview of BioCreAtIvE II gene mention recogni-
tion. Genome Biology, 9(Suppl 2):S2.
Lorraine Tanabe, Natalie Xie, Lynne Thom, Wayne Mat-
ten, and W John Wilbur. 2005. GENETAG: a
tagged corpus for gene/protein named entity recogni-
tion. BMC Bioinformatics, 6(Suppl 1):S3.
Manabu Torii, Zhangzhi Hu, Cathy H Wu, and Hong-
fang Liu. 2009. Biotagger-GM: a gene/protein name
recognition system. Journal of the American Medical
Informatics Association : JAMIA, 16:247?255.
109
