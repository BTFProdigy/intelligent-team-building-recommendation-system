Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 665?668,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Reranking the Berkeley and Brown Parsers?
Mark Johnson
Department of Computing
Macquarie University
Sydney, Australia
mjohnson@science.mq.edu.au
Ahmet Engin Ural
Cognitive and Linguistic Sciences
Brown University
Providence, RI, USA
aeural@gmail.com
Abstract
The Brown and the Berkeley parsers are two
state-of-the-art generative parsers. Since both
parsers produce n-best lists, it is possible to
apply reranking techniques to the output of
both of these parsers, and to their union. We
note that the standard reranker feature set dis-
tributed with the Brown parser does not do
well with the Berkeley parser, and propose an
extended set that does better. An ablation ex-
periment shows that different parsers benefit
from different reranker features.
1 Introduction
Syntactic parsing is the task of identifying the
phrases and clauses in natural language sentences.
It has been intensively studied primarily because it
is generally believed that identifying syntactic struc-
ture is a first step towards semantic interpretation.
This paper focuses on parsing the Wall Street Jour-
nal (WSJ) section of the University of Pennsylva-
nia treebank corpus (Marcus et al, 1993). There
are a large number of different approaches to this
task. For simplicity we focus on two popular gener-
ative statistical parsing models: Charniak?s ?Maxi-
mum Entropy Inspired? parser (Charniak and John-
son, 2005) and Petrov?s ?split-merge? parser (Petrov
et al, 2006). We follow conventional informal usage
and refer to these as the ?Brown? and the ?Berkeley?
parsers respectively.
? We would like to thank Eugene Charniak and the other
members of BLLIP for their helpful advice on this work. Natu-
rally all errors remain our own.
Briefly, the Berkeley parser is a smoothed PCFG
whose non-terminals are refinements of the original
treebank grammar obtained by an automatic split-
merge procedure, while the Brown parser is effec-
tively a smoothed PCFG whose non-terminals en-
code a wide variety of manually chosen condition-
ing information, such as heads, governors, etc. The
Berkeley parser is usually viewed as unlexicalized
(although the preterminals may be split so finely
that they may be viewed as identifying lexical clus-
ters), while essentially every distribution used in
the Brown parser conditions on lexical information.
Even from this cursory description it is clear that
these parsers parsers extract generalizations from the
training data in different ways.
This paper applies reranking (Collins and Koo,
2005) to the n-best output of both parsers individ-
ually, as well as to an n-best list consisting of the
union of the outputs of both parsers. We are inter-
ested to see whether the same kinds of features im-
prove the performance of both the Berkeley and the
Brown parsers, or whether successful reranking re-
quires features that are specially tuned to the parser
it is applied to. Finally, we are interested in the
performance of the reranker trained on the union n-
best lists. Combining the output of multiple parsers
in other more complex ways has been previously
demonstrated to improve overall accuracy, so it is in-
teresting to see if the relatively simple method used
here improves parsing accuracy as well.
The approach of Zhang et al (2009) is closest to
the work described here. They combine n-best lists
produced by the same parsers as we do, but use only
a relatively small set of features (the log probabil-
665
Trees Reranker features
standard extended
Berkeley 91.6 91.7
Brown 91.8 91.6
Combined 91.8 91.9
Table 1: The f-scores on section 22 of rerankers trained
on folds 1?18 by minimizing a regularized ?MaxEnt? ob-
jective (negative log likelihood with a Gaussian regular-
izer) using L-BFGS. The weight of the regularizer was
tuned to optimize f-score on folds 19?20.
ity of the parses plus a constituent overlap feature),
while we investigate models with millions of fea-
tures here. They report a higher f-score than we do
when they replace the generative Brown parser with
the the self-trained discriminatively-reranked parser
of McClosky et al (2006), but with inputs provided
by the generative Berkeley and Brown n-best parsers
they report an f-score of 91.43 on section 23, which
is consistent with the results reported here.
2 Experimental setup
We ran both parsers in 50-best mode, and con-
structed 20-fold cross-validated training data as de-
scribed in Collins and Koo (2005) and Charniak
and Johnson (2005), i.e., the trees in sections 2?21
of the WSJ treebank were divided into 20 equal-
sized folds, and the parses for each fold were gen-
erated by a parser trained on the trees in the other
folds. Then sections 22, 23 and 24 were parsed us-
ing the standard ?out-of-the-box? parser. Follow-
ing the suggestion in Collins and Koo (2005), in or-
der to avoid over-training on section 23 all rerank-
ing experiments reported here (except the final one)
used folds 1?18 as training data, used folds 19?20
as development data and used section 22 as test data.
(The averaged perceptron algorithm does not require
development data, so the experiments using that al-
gorithm report averages over folds 19?20 and sec-
tion 22).
The Berkeley parser can be run in many modes;
in order to produce the 20-fold training data we ran
the Berkeley trainer with 6 splits, and ran the re-
sulting parsers in ?accurate? mode. It failed to pro-
duce any parses for 12 sentences in sections 2?21
and one sentence in section 24. The Brown parser
was trained using the ?out-of-the-box? settings, and
produced parses for all sentences.
Using the reranker features distributed with the
Brown reranker (Charniak and Johnson, 2005),
which we call the ?standard? set below, we ob-
tained no overall improvement in f-score when ei-
ther reranking the Berkeley parser n-best lists alone,
or when the Berkeley parses were combined with the
Brown parses.
However, it is possible that these results reflect
the fact that the features used by the reranker were
chosen because they improve the Brown parser, i.e.,
they are the result of feature selection based on
reranking the Brown parser?s n-best lists. In order
to determine if this is the case, we developed an ?ex-
tended? feature set that incorporates a wider set of
features, specifically including features that capture
global properties of the tree that might be harder for
the Berkeley parser to learn.
Our extended feature set consists of
4,256,553 features, which are instances of
162 feature classes, which in turn are grouped
into 20 feature ?super-classes?. By contrast, the
standard feature set contains 1,333,950 features in
90 feature classes, grouped into 14 super-classes.
A brief description of the extended feature set
super-classes follows:
Parser: an indicator feature indicating which parsers
generated this parse,
RelLogP: the log probability of this parse according to
each parser,
InterpLogCondP: an indicator feature based on the
binned log conditional probability according to
each parser,
RightBranch: an indicator function of each node that
lies on the right-most branch of the parse tree,
Heavy: an indicator function based on the size and lo-
cation of each nonterminal (designed to identify the
locations of ?heavy? phrases),
LeftBranchLength: an indicator function of the binned
length of each left-branching chain,
RightBranchLength: an indicator function of the
binned length of each right-branching chain,
Rule: an indicator function of parent and children cate-
gories, optionally with head POS annotations,
NNGram: and indicator function of parent and n-gram
sequences of children categories, optionally head
666
 Pa
rse
r
 Re
lLo
gP
 Int
erp
Lo
gC
ond
P
 Ri
ght
Bra
nch
 He
avy
 Le
ftB
ran
chL
eng
th
 Ri
ght
Bra
nch
Le
ngt
h
 Ru
le
 NN
Gra
m
 He
ads
 Sy
nSe
mH
ead
s
 RB
Co
nte
xt
 Su
bjV
erb
Ag
r
 Co
Par
 Co
Le
nPa
r
 W
ord
 W
Pro
j
 W
Ed
ges
 NG
ram
Tre
e
 He
adT
ree
-0.3
-0.2
-0.1
0.0
0.1
Berkeley
Brown
Combined
Figure 1: The average change in f-score on folds 19?20 and section 22 caused by removing a feature superclass
from the extended feature set and retraining. Difference in scores less that 0.1% are probably not significant. In this
experiment all rerankers were trained using the averaged perceptron algorithm. With the full extended feature set,
rerankers trained with the averaged perceptron algorithm achieve f-scores of 91.2% on both the Berkeley and Brown
parses, and 91.6% on the combined parses.
annotated, inspired by the n-gram rule features de-
scribed in Collins and Koo (2005),
Heads: an indicator function of ?head-to-head? depen-
dencies,
SynSemHeads: an indicator function of the pair of syn-
tactic (i.e., functional) and semantic (i.e., lexical)
heads of each non-terminal,
RBContext: an indicator function of how much each
subtree deviates from from right-branching,
SubjVerbAgr: an indicator function of whether subject-
verb agreement is violated,
CoPar: an indicator function that fires when conjoined
phrases in a coordinate structure have approxi-
mately parallel syntactic structure,
CoLenPar: an indicator function that fires when con-
joined phrases in a coordinate structure have ap-
proximately the same length,
Word: an indicator function that identifies words and
their preterminals,
WProj: an indicator function that identifies words and
their phrasal projections up to their maximal pro-
jection,
WEdges: an indicator function that identifies the words
and POS tags appearing at the edges of each nonter-
minal,
NGramTree: an indicator function of the subtree con-
sisting of nodes connecting each pair of adjacent
words in the parse tree, and
HeadTree: a tree fragment consisting of a head word
and its projection up to its maximal projection, plus
all of the siblings of each node in this sequence (this
is like an auxiliary tree in a TAG).
The InterpLogCondP features were designed to
capture non-linearities in the way that the Berke-
ley and Brown parsers assign probabilities to trees.
We deliberately added features that incorporated lin-
guistic notions such as head, governor and maximal
projection, as the Berkeley parser does not explic-
itly condition on such information (in contrast to the
Brown parser, which does).
In fact, as the reader can verify the differences in
f-scores between rerankers containing the extended
features and the standard features is minimal. In
order to better study the importance of the various
features we conducted an ablation study, in which
we trained rerankers which were missing one feature
superclass from the 20 superclasses of the extended
feature set. In order to speed training time we used
the averaged perceptron algorithm (Collins, 2002)
(it converges an order of magnitude faster than the L-
BFGS algorithm we used in the other experiments,
but the f-score of the model estimated with the av-
eraged perceptron is approximately 0.1% lower than
when using L-BFGS). The results from this experi-
ment are shown in Figure 1. The averaged percep-
tron algorithm does not rely on the development data
667
(folds 19?20), so the results we report are average f-
scores on the development data and on section 22
(we did this because the differences are small, so
a larger evaluation set may be able to detect differ-
ences more reliably).
It is interesting that linguistically-informed fea-
tures (such as Heads, SynSemHeads and HeadTree)
seem to be much more important when reranking
the combined n-best lists than when reranking the
output of either parser alone. This suggests that the
log probability scores from both parsers are inter-
nally consistent, but need to be recalibrated when
the parses are combined. The log probability scores
from the parsers themselves (in the form of the In-
terpLogCondP feature) are also supplying useful in-
formation that the reranker features on their own are
not providing. Finally, the WEdges feature, which
identifies the words and POS at the left and right
boundaries of each nonterminal, also provides ex-
tremely useful information, especially for reranking
the Berkeley parser.
3 Conclusion
Reranking is a straight-forward method for improv-
ing the accuracy of n-best parsers. While one
might have hoped that reranking the n-best output
of the Berkeley parser, or the union of the outputs
of the Berkeley and Brown parsers, would dramat-
ically improve overall f-score, this seems not to be
the case. It?s possible that the features of current
rerankers have been implicitly designed to work well
with parsers like the Brown parser, but a reranker
with a dramatically enlarged feature set performs
only marginally better. This result was confirmed
by training a reranker with the extended features on
the union of the output of the Berkeley and Brown
parsers on sections 2?21 and testing on section 23
(i.e., the standard WSJ parsing evaluation), which
achieved an f-score of 91.49%; approximately 0.1%
higher than a reranker with the standard feature set
trained on the output of the Brown parser alone.
Acknowledgments
This research was funded by NSF awards 0530118,
0544127 and 0631667.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
173?180, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25?70.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1?8. Association for
Computational Linguistics.
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, Main Conference, pages 152?159, New
York City, USA, June. Association for Computational
Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433?440, Sydney, Aus-
tralia. Association for Computational Linguistics.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1552?1560, Singapore. Association for Computational
Linguistics.
668
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 223?227
Manchester, August 2008
Discriminative vs. Generative Approaches in Semantic Role Labeling
Deniz Yuret
Koc? University
dyuret@ku.edu.tr
Mehmet Ali Yatbaz
Koc? University
myatbaz@ku.edu.tr
Ahmet Engin Ural
Koc? University
aural@ku.edu.tr
Abstract
This paper describes the two algorithms
we developed for the CoNLL 2008 Shared
Task ?Joint learning of syntactic and se-
mantic dependencies?. Both algorithms
start parsing the sentence using the same
syntactic parser. The first algorithm
uses machine learning methods to identify
the semantic dependencies in four stages:
identification and labeling of predicates,
identification and labeling of arguments.
The second algorithm uses a generative
probabilistic model, choosing the seman-
tic dependencies that maximize the proba-
bility with respect to the model. A hybrid
algorithm combining the best stages of
the two algorithms attains 86.62% labeled
syntactic attachment accuracy, 73.24% la-
beled semantic dependency F1 and 79.93%
labeled macro F1 score for the combined
WSJ and Brown test sets1.
1 Introduction
In this paper we describe the system we developed
for the CoNLL 2008 Shared Task (Surdeanu et al,
2008). Section 2 describes our approach for iden-
tifying syntactic dependencies. For semantic role
labeling (SRL), we pursued two independent ap-
proaches. Section 3 describes our first approach,
where we treated predicate identification and la-
beling, and argument identification and labeling as
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1These numbers are slightly higher than the official results
due to a small bug in our submission.
four separate machine learning problems. The fi-
nal program consists of four stages, each stage tak-
ing the answers from the previous stage as given
and performing its own identification or labeling
task based on a model generated from the train-
ing set. Section 4 describes our second approach
where we used a generative model based on the
joint distribution of the predicate, the arguments,
their labels and the syntactic dependencies con-
necting them. Section 5 summarizes our results
and suggests possible improvements.
2 Syntactic dependencies
We used a non-projective dependency parser based
on spanning tree algorithms. The parameters were
determined based on the experimental results of
the English task in (McDonald et al, 2005), i.e. we
used projective parsing and a first order feature set
during training. Due to the new representation of
hyphenated words in both training and testing data
of our shared task and the absence of the gold part
of speech (GPOS) column in the test data, the for-
mat of the CoNLL08 shared task is slightly differ-
ent from the format of the CoNLL05 shared task,
which is supported by the McDonald?s parser. We
reformatted the data accordingly. The resulting la-
beled attachment score on the test set is 87.39% for
WSJ and 80.46% for Brown.
3 The 4-stage discriminative approach
Our first approach to SRL consists of four distinct
stages: (1) predicate identification, (2) predicate
labeling, (3) argument identification, and (4) argu-
ment labeling.
A discriminative machine learning algorithm is
trained for each stage using the gold input and out-
put values from the training set. The following sec-
223
tions describe the machine learning algorithm, the
nature of its input/output, and the feature selection
process for each stage. The performance of each
stage is compared to a most frequent class base-
line and analyzed separately for the two test sets
and for nouns and verbs. In addition we look at the
performance given the input from the gold data vs.
the input from the previous stage.
3.1 Predicate identification
The task of this stage is to determine whether a
given word is a nominal or a verb predicate using
the dependency-parsed input. As potential predi-
cates we only consider words that appear as a pred-
icate in the training data or have a corresponding
PropBank or NomBank XML file. The method
constructs feature vectors for each occurrence of
a target word in the training and test data. It as-
signs class labels to the target words in the training
data depending on whether a target word is a pred-
icate or not, and finally classifies the test data. We
experimented with combinations of the following
features for each word in a 2k + 1 word window
around the target: (1) POS(W): the part of speech
of the word, (2) DEP(W, HEAD(W)): the syntac-
tic dependency of the word, (3) LEMMA(W): the
lemma of the word, (4) POS(HEAD(W)): the part
of speech of the syntactic head.
We empirically selected the combination that
gives the highest accuracy in terms of the precision
and recall scores on the development data. The
method achieved its highest score when we used
features 1-3 for the target word and features 1-2 for
the neighbors in a [-3 +3] word window. TiMBL
(Daelemans et al, 2004) was used as the learning
algorithm.
Table 1 (4-stage, All1) shows the results of our
learning method on the WSJ and Brown test data.
The noun and verb results are given separately
(Verb1, Noun1). To distinguish the mistakes com-
ing from parsing we also give the results of our
method after the gold parse (4-stage-gold). Our re-
sults are significantly above the most frequent class
baseline which gives 72.3% on WSJ and 65.3% on
Brown.
3.2 Predicate labeling
The task of the second stage is deciding the correct
frame for a word given that the word is a predicate.
The input of the stage is 11-column data, where the
columns contain part of speech, lemma and syn-
tactic dependency for each word. The first stage?s
decision for the frame is indicated by a string in
the predicate column. The output of the stage is
simply the replacement of that string with the cho-
sen frame of the word. The chosen frame of the
word may be word.X, where X is a valid number
in PropBank or NomBank.
The statistics of the training data show that by
picking the most frequent frame, the system can
pick the correct frame in a large percent of the
cases. Thus we decided to use the most frequent
frame baseline for this stage. If the word is never
seen in the training, first frame of the word is
picked as default.
In the test phase, the results are as the follow-
ing; in the Brown data, assuming that the stage 1
is gold, the score is 80.8%, noting that 11% of the
predicates are not seen in the training phase. In
WSJ, the score based on gold input is 88.3%, and
only 5% of the predicates are not seen in the train-
ing phase. Table 1 gives the full results for Stage 2
(4-stage, Verb2, Noun2, All2).
3.3 Argument identification
The input data at this stage contains the syntac-
tic dependencies, predicates and their frames. We
look at the whole sentence for each predicate and
decide whether each word should be an argument
of that predicate or not. We mark the words we
choose as arguments indicating which predicate
they belong to and leave the labeling of the ar-
gument type to the next stage. Thus, for each
predicate-word pair we have a yes/no decision to
make.
As input to the learning algorithm we experi-
mented with representations of the syntactic de-
pendency chain between the predicate and the
argument at various levels of granularity. We
identified the syntactic dependency chain between
the predicate and each potential argument using
breadth-first-search on the dependency tree. We
tried to represent the chain using various subsets
of the following elements: the argument lemma
and part-of-speech, the predicate frame and part-
of-speech, the parts-of-speech and syntactic de-
pendencies of the intermediate words linking the
argument to the predicate.
The syntactic dependencies leading from the ar-
gument to the predicate can be in the head-modifier
or the modifier-head direction. We marked the di-
rection associated with each dependency relation
in the chain description. We also experimented
224
with using fine-grained and coarse-grained parts of
speech. The coarse-grained part of speech consists
of the first two characters of the Penn Treebank
part of speech given in the training set.
We used a simple learning algorithm: choose
the answer that is correct for the majority of the
instances with the same chain description from
the training set. Not having enough detail in the
chain description leaves crucial information out
that would help with the decision process, whereas
having too much detail results in bad classifica-
tions due to sparse data. In the end, neither the ar-
gument lemma, nor the predicate frame improved
the performance. The best results were achieved
with a chain description including the coarse parts
of speech and syntactic dependencies of each word
leading from the argument to the predicate. The
results are summarized in Table 1 (4-stage, Verb3,
Noun3, All3).
3.4 Argument labeling
The task of this stage is choosing the correct argu-
ment tag for a modifier given that it is modifying
a particular predicate. Input data format has ad-
ditional columns indicating which words are argu-
ments for which predicates. There are 54 possible
values for a labeled argument. As a baseline we
take the most frequent argument label in the train-
ing data (All1) which gives 37.8% on the WSJ test
set and 33.8% on the Brown test set.
The features to determine the correct label of an
argument are either lexical or syntactic. In a few
cases, they are combined. The following list gives
the set we have used. Link is the type of the syntac-
tic dependency. Direction is left or right, depend-
ing the location of the head and the modifier in the
sentence. LastLink is the type of the dependency
at the end of the dependency chain and firstLink
is type of the dependency at the beginning of the
dependency chain.
Feature1 : modifierStem + headStem
Feature2 : modifierStem + coarsePosModifier +
headStem + coarsePosHead + direction
Feature3 : coarsePosModifier + headPos +
firstLink + lastLink + direction
Feature4: modifierStem + coarsePosModifier
The training phase includes building simple his-
tograms based on four features. Feature1 and Fea-
ture2 are sparser than the other two features and
are better features as they include lexical informa-
tion. Last two features are less sparse, covering
most of the development data, i.e. their histograms
give non-zero values in the development phase. In
order to match all the instances in the development
and use the semantic information, a cascade of the
features is implemented similar to the one done by
Gildea and Jurafsky(2002), although no weighting
and a kind of back-off smoothing is used. First,
a match is searched in the histogram of the first
feature, if not found it is searched in the following
histogram. After a match, the most frequent argu-
ment with that match is returned. Table 1 gives the
performance (4-stage, Verb4, Noun4, All4).
4 The generative approach
One problem with the four-stage approach is that
the later stages provide no feedback to the earlier
ones. Thus, a frame chosen because of its high
prior probability will not get corrected when we
fail to find appropriate arguments for it. A gen-
erative model, on the other hand, does not suffer
from this problem. The probability of the whole
assignment, including predicates, arguments, and
their labels, is evaluated together and the highest
probability combination is chosen.
4.1 The generative model
Figure 1: The graphical model depicting the con-
ditional independence assumptions.
Our generative model specifies the distribution
of the following random variables: P is the lemma
(stem+pos) of a candidate predicate. F is the
frame chosen for the predicate (could be null). A
i
is the argument label of word i with respect to a
given predicate (could be null). W
i
is the lemma
(stem+pos) of word i. L
i
is the syntactic depen-
dency chain leading from word i to the given pred-
icate (similar to Section 3.3).
We consider each word in the sentence as a can-
didate predicate and use the joint distribution of the
above variables to find the maximum probability F
225
WSJ Verb1 Verb2 Verb3 Verb4 Noun1 Noun2 Noun3 Noun4 All1 All2 All3 All4
4-stage 97.1 85.5 85.7 71.7 84.6 78.4 61.1 49.4 90.6 81.8 76.6 63.5
generative 96.1 88.4 83.4 74.0 82.8 79.5 69.8 63.2 89.0 83.6 77.4 69.2
4-stage-gold 97.4 88.3 95.2 82.7 85.2 92.7 70.5 81.9 91.1 90.5 86.0 82.4
generative-gold 96.3 92.6 91.1 88.0 83.4 95.5 80.7 86.9 89.4 94.0 86.7 87.5
hybrid 97.1 89.3 85.7 74.7 84.6 80.9 70.9 64.0 90.6 84.9 79.5 70.2
Brown Verb1 Verb2 Verb3 Verb4 Noun1 Noun2 Noun3 Noun4 All1 All2 All3 All4
4-stage 93.0 74.5 78.9 59.0 74.4 58.6 52.3 38.8 86.0 68.6 72.8 54.3
generative 91.4 71.7 76.1 60.0 70.8 59.3 54.0 45.3 83.1 66.6 69.6 55.7
4-stage-gold 93.0 80.8 93.7 73.2 75.7 80.3 70.1 70.5 86.5 80.8 88.2 72.4
generative-gold 91.6 80.6 85.8 78.05 71.2 85.9 70.5 75.1 83.5 82.6 81.8 77.1
hybrid 93.0 73.3 78.9 60.4 74.4 62.9 57.6 47.5 86.0 69.3 73.4 57.0
Table 1: The F1 scores for different datasets, models, stages, and predicate parts of speech. The ?Verb?
in the column heading indicates verbal predicates, ?Noun? indicates nominal predicates, ?All? indicates
all predicates. The numbers 1-4 in column headings indicate the 4 stages: (1) predicate identification, (2)
predicate labeling, (3) argument identification, (4) argument labeling. The gold results assume perfect
output from the previous stages. The highest number in each column is marked with boldface.
and A
i
labels given P , W
i
, and L
i
. The graphical
model in Figure 1 specifies the conditional inde-
pendence assumptions we make. Equivalently, we
take the following to be proportional to the joint
probability of a particular assignment:
Pr(F |P )
?
i
Pr(A
i
|F ) Pr(W
i
|FA
i
) Pr(L
i
|FA
i
)
4.2 Parameter estimation
To estimate the parameters of the generative model
we used the following methodology:
For Pr(F |P ) we use the maximum likelihood
estimate from the training data. As a consequence,
frames that were never observed in the training
data have zero probability. One exception is lem-
mas which have not been observed in the training
data, for which each frame is considered equally
likely.
For Pr(A
i
|F ) we also use the maximum like-
lihood estimate and normalize it using sentence
length. For a given argument label we find the
expected number of words in a sentence with that
label for frame F . We divide this expected num-
ber with the length of the given sentence to find
Pr(A
i
|F ) for a single word. Any leftover prob-
ability is given to the null label. If the sentence
length is shorter than the expected number of ar-
guments, all probabilities are scaled down propor-
tionally.
For the remaining two terms Pr(L
i
|F,A
i
) and
Pr(W
i
|F,A
i
) using the maximum likelihood esti-
mate is not effective because of data sparseness.
The arguments in the million word training data
contain about 16,000 unique words and 25,000
unique dependency chains. To handle the sparse-
ness problem we smoothed these two estimates us-
ing the part-of-speech argument distribution, i.e.
Pr(L
i
|POS, A
i
) and Pr(W
i
|POS, A
i
), where POS
represents the coarse part of speech of the predi-
cate.
5 Results and Analysis
Table 1 gives the F1 scores for the two models
(4-stage and generative), presented separately for
noun and verb predicates and the four stages of
predicate identification/labeling, argument identi-
fication/labeling. In order to isolate the perfor-
mance of each stage we also give their scores with
gold input. The rest of this section analyzes these
results and suggests possible improvements.
A hybrid algorithm: A comparison of the two
algorithms show that the 4-stage approach is su-
perior in predicate and verbal-argument identifica-
tion and the generative algorithm is superior in the
labeling of predicates and arguments and nominal-
argument identification. This suggests a hybrid al-
gorithm where we restrict the generative model to
take the answers for the better stages from the 4-
stage algorithm (Noun1, Verb1, Verb3) as given.
Tables 1 and 2 present the results for the hybrid
algorithm compared to the 4-stage and generative
models.
Parsing performance: In order to see the effect
of syntactic parsing performance, we ran the hy-
brid algorithm starting with the gold parse. The
labeled semantic score went up to 78.84 for WSJ
and 67.20 for Brown, showing that better parsing
226
Data/algorithm Unlabeled Labeled
WSJ 4-stage 81.15 69.44
WSJ generative 81.01 73.66
WSJ hybrid 82.94 74.74
Brown 4-stage 76.91 58.76
Brown generative 73.76 59.05
Brown hybrid 77.22 60.80
Table 2: Semantic scores for the 4-stage, genera-
tive, and hybrid algorithms
can add about 4-6% to the overall performance.
Syntactic vs lexical features: Our algorithms
use two broad classes of features: information
from the dependency parse provides syntactic ev-
idence, and the word pairs themselves provide se-
mantic evidence for a possible relation. To iden-
tify their relative contributions, we experimented
with two modifications of the generative algo-
rithm: gen-l does not use the Pr(W
i
|FA
i
) term
and gen-w does not use the Pr(L
i
|FA
i
) term. gen-
l, using only syntactic information and the pred-
icate, gets a labeled semantic score of 70.97 for
WSJ and 58.83 for Brown, a relatively small de-
crease. In contrast gen-w, using only lexical infor-
mation gets 43.06 for WSJ and 33.17 for Brown
causing almost a 40% decrease in performance.
On the other hand, we find that the lexical fea-
tures are essential for certain tasks. In labeling the
arguments of nominal predicates, finding an exact
match for the lexical pair guarantees a 90% accu-
racy. If there is no exact match, the 4-stage algo-
rithm falls back on a syntactic match, which only
gives a 75% accuracy.
Future work: The hybrid algorithm shows the
strengths and weaknesses of our two approaches.
The generative algorithm allows feedback from the
later stages to the earlier stages and the 4-stage ma-
chine learning approach allows the use of better
features. One way to improve the system could be
by adding feedback to the 4-stage algorithm (later
stages can veto input coming from previous ones),
or adding more features to the generative model
(e.g. information about neighbor words when pre-
dicting F ). More importantly, there is no feedback
between the syntactic parser and the semantic role
labeling in our systems. Treating both problems
under the same framework may lead to better re-
sults.
Another property of both models is the indepen-
dence of the argument label assignments from each
other. Even though we try to control the number of
arguments of a particular type by adjusting the pa-
rameters, there are cases when we end up with no
assignments for a mandatory argument or multiple
assignments where only one is allowed. A more
strict enforcement of valence constraints needs to
be studied.
The use of smoothing in the generative model
was critical, it added about 20% to our final F1
score. This raises the question of finding more
effective smoothing techniques. In particular, the
jump from specific frames to coarse parts of speech
is probably not optimal. There may be interme-
diate groups of noun and verb predicates which
share similar semantic or syntactic argument dis-
tributions. Identifying and using such groups will
be considered in future work.
References
Daelemans, W., J. Zavrel, K. van der Sloot, and
A. van den Bosch. 2004. TiMBL: Tilburg memory-
Based Learner. Tilburg University.
Gildea, D. and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245 288.
McDonald, R., K. Crammer, and F. Pereira. 2005. On-
line Large-Margin Training of Dependency Parsers.
Ann Arbor, 100.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008).
227
