Tuning Support Vector Machines for Biomedical Named Entity Recognition
Jun?ichi Kazama? Takaki Makino? Yoshihiro Ohta? Jun?ichi Tsujii? ?
? Department of Computer Science, Graduate School of Information Science and Technology,
University of Tokyo, Bunkyo-ku, Tokyo 113-0033, Japan
? Department of Complexity Science and Engineering, Graduate School of Frontier Sciences,
University of Tokyo, Bunkyo-ku, Tokyo 113-0033, Japan
? Central Research Laboratory, Hitachi, Ltd., Kokubunji, Tokyo 185-8601, Japan
? CREST, JST (Japan Science and Technology Corporation)
Abstract
We explore the use of Support Vector Ma-
chines (SVMs) for biomedical named en-
tity recognition. To make the SVM train-
ing with the available largest corpus ? the
GENIA corpus ? tractable, we propose to
split the non-entity class into sub-classes,
using part-of-speech information. In ad-
dition, we explore new features such as
word cache and the states of an HMM
trained by unsupervised learning. Experi-
ments on the GENIA corpus show that our
class splitting technique not only enables
the training with the GENIA corpus but
also improves the accuracy. The proposed
new features also contribute to improve
the accuracy. We compare our SVM-
based recognition system with a system
using Maximum Entropy tagging method.
1 Introduction
Application of natural language processing (NLP) is
now a key research topic in bioinformatics. Since
it is practically impossible for a researcher to grasp
all of the huge amount of knowledge provided in
the form of natural language, e.g., journal papers,
there is a strong demand for biomedical information
extraction (IE), which extracts knowledge automati-
cally from biomedical papers using NLP techniques
(Ohta et al, 1997; Proux et al, 2000; Yakushiji et
al., 2001).
The process called named entity recognition,
which finds entities that fill the information slots,
e.g., proteins, DNAs, RNAs, cells etc., in the
biomedical context, is an important building block in
such biomedical IE systems. Conceptually, named
entity recognition consists of two tasks: identifica-
tion, which finds the region of a named entity in
a text, and classification, which determines the se-
mantic class of that named entity. The following il-
lustrates biomedical named entity recognition.
?Thus, CIITAPROTEIN not only acti-
vates the expression of class II genesDNA
but recruits another B cell-specific
coactivator to increase transcriptional
activity of class II promotersDNA in
B cellsCELLTYPE.?
Machine learning approach has been applied to
biomedical named entity recognition (Nobata et al,
1999; Collier et al, 2000; Yamada et al, 2000;
Shimpuku, 2002). However, no work has achieved
sufficient recognition accuracy. One reason is the
lack of annotated corpora for training as is often
the case of a new domain. Nobata et al (1999) and
Collier et al (2000) trained their model with only
100 annotated paper abstracts from the MEDLINE
database (National Library of Medicine, 1999), and
Yamada et al (2000) used only 77 annotated paper
abstracts. In addition, it is difficult to compare the
techniques used in each study because they used a
closed and different corpus.
To overcome such a situation, the GENIA cor-
pus (Ohta et al, 2002) has been developed, and at
this time it is the largest biomedical annotated cor-
pus available to public, containing 670 annotated ab-
stracts of the MEDLINE database.
Another reason for low accuracies is that biomed-
ical named entities are essentially hard to recognize
using standard feature sets compared with the named
entities in newswire articles (Nobata et al, 2000).
Thus, we need to employ powerful machine learning
techniques which can incorporate various and com-
plex features in a consistent way.
Support Vector Machines (SVMs) (Vapnik, 1995)
and Maximum Entropy (ME) method (Berger et al,
1996) are powerful learning methods that satisfy
such requirements, and are applied successfully to
other NLP tasks (Kudo and Matsumoto, 2000; Nak-
agawa et al, 2001; Ratnaparkhi, 1996). In this pa-
per, we apply Support Vector Machines to biomed-
ical named entity recognition and train them with
                                            Association for Computational Linguistics.
                              the Biomedical Domain, Philadelphia, July 2002, pp. 1-8.
                         Proceedings of the Workshop on Natural Language Processing in
the GENIA corpus. We formulate the named entity
recognition as the classification of each word with
context to one of the classes that represent region
and named entity?s semantic class. Although there
is a previous work that applied SVMs to biomedi-
cal named entity task in this formulation (Yamada et
al., 2000), their method to construct a classifier us-
ing SVMs, one-vs-rest, fails to train a classifier with
entire GENIA corpus, since the cost of SVM train-
ing is super-linear to the size of training samples.
Even with a more feasible method, pairwise (Kre?el,
1998), which is employed in (Kudo and Matsumoto,
2000), we cannot train a classifier in a reasonable
time, because we have a large number of samples
that belong to the non-entity class in this formula-
tion. To solve this problem, we propose to split the
non-entity class to several sub-classes, using part-of-
speech information. We show that this technique not
only enables the training feasible but also improves
the accuracy.
In addition, we explore new features such as word
cache and the states of an unsupervised HMM for
named entity recognition using SVMs. In the exper-
iments, we show the effect of using these features
and compare the overall performance of our SVM-
based recognition system with a system using the
Maximum Entropy method, which is an alternative
to the SVM method.
2 The GENIA Corpus
The GENIA corpus is an annotated corpus of pa-
per abstracts taken from the MEDLINE database.
Currently, 670 abstracts are annotated with named
entity tags by biomedical experts and made avail-
able to public (Ver. 1.1).1 These 670 abstracts are a
subset of more than 5,000 abstracts obtained by the
query ?human AND blood cell AND transcription
factor? to the MEDLINE database. Table 1 shows
basic statistics of the GENIA corpus. Since the GE-
NIA corpus is intended to be extensive, there exist
24 distinct named entity classes in the corpus.2 Our
task is to find a named entity region in a paper ab-
stract and correctly select its class out of these 24
classes. This number of classes is relatively large
compared with other corpora used in previous stud-
ies, and compared with the named entity task for
newswire articles. This indicates that the task with
the GENIA corpus is hard, apart from the difficulty
of the biomedical domain itself.
1Available via http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/
2The GENIA corpus also has annotations for conjunc-
tive/disjunctive named entity expressions such as ?human B- or
T-cell lines? (Kim et al, 2001). In this paper we ignore such
expressions and consider that constituents in such expressions
are annotated as a dummy class ?temp?.
Table 1: Basic statistics of the GENIA corpus
# of sentences 5,109
# of words 152,216
# of named entities 23,793
# of words in NEs 50,229
# of words not in NEs 101,987
Av. length of NEs (?) 2.11 (1.40)
3 Named Entity Recognition Using SVMs
3.1 Named Entity Recognition as Classification
We formulate the named entity task as the classi-
fication of each word with context to one of the
classes that represent region information and named
entity?s semantic class. Several representations to
encode region information are proposed and exam-
ined (Ramshaw and Marcus, 1995; Uchimoto et al,
2000; Kudo and Matsumoto, 2001). In this paper,
we employ the simplest BIO representation, which
is also used in (Yamada et al, 2000). We modify
this representation in Section 5.1 in order to acceler-
ate the SVM training.
In the BIO representation, the region information
is represented as the class prefixes ?B-? and ?I-?, and
a class ?O?. B- means that the current word is at the
beginning of a named entity, I- means that the cur-
rent word is in a named entity (but not at the be-
ginning), and O means the word is not in a named
entity. For each named entity class C, class B-C and
I-C are produced. Therefore, if we have N named
entity classes, the BIO representation yields 2N + 1
classes, which will be the targets of a classifier. For
instance, the following corresponds to the annota-
tion ?Number of glucocorticoid receptorsPROTEIN in
lymphocytesCELLTYPE and ...?.
Number of glucocorticoid receptors
O O B-PROTEIN I-PROTEIN
in lymphocytes and ...
O B-CELLTYPE O ...
3.2 Support Vector Machines
Support Vector Machines (SVMs) (Cortes and Vap-
nik, 1995) are powerful methods for learning a clas-
sifier, which have been applied successfully to many
NLP tasks such as base phrase chunking (Kudo and
Matsumoto, 2000) and part-of-speech tagging (Nak-
agawa et al, 2001).
The SVM constructs a binary classifier that out-
puts +1 or ?1 given a sample vector x ? Rn. The de-
cision is based on the separating hyperplane as fol-
lows.
c(x) =
?????
+1 if w ? x + b > 0, w ? Rn, b ? R,
?1 otherwise
The class for an input x, c(x), is determined by see-
ing which side of the space separated by the hyper-
plane, w ? x + b = 0, the input lies on.
Given a set of labeled training samples
{(y1, x1), ? ? ? , (yL, xL)}, xi ? Rn, yi ? {+1,?1},
the SVM training tries to find the optimal hy-
perplane, i.e., the hyperplane with the maximum
margin. Margin is defined as the distance between
the hyperplane and the training samples nearest
to the hyperplane. Maximizing the margin insists
that these nearest samples (support vectors) exist
on both sides of the separating hyperplane and the
hyperplane lies exactly at the midpoint of these
support vectors. This margin maximization tightly
relates to the fine generalization power of SVMs.
Assuming that |w?xi+b| = 1 at the support vectors
without loss of generality, the SVM training can be
formulated as the following optimization problem.3
minimize 1
2
||w||2
subject to yi(w ? xi + b) ? 1, i = 1, ? ? ? , L.
The solution of this problem is known to be written
as follows, using only support vectors and weights
for them.
f (x) = w ? x + b=
?
i?S V s
yi?ix ? xi + b (1)
In the SVM learning, we can use a function k(xi, x j)
called a kernel function instead of the inner prod-
uct in the above equation. Introducing a kernel
function means mapping an original input x using
?(x), s.t. ?(xi) ??(x j) = k(xi, x j) to another, usually
a higher dimensional, feature space. We construct
the optimal hyperplane in that space. By using ker-
nel functions, we can construct a non-linear separat-
ing surface in the original feature space. Fortunately,
such non-linear training does not increase the com-
putational cost if the calculation of the kernel func-
tion is as cheap as the inner product. A polynomial
function defined as (sxi ? x j + r)d is popular in ap-
plications of SVMs to NLPs (Kudo and Matsumoto,
2000; Yamada et al, 2000; Kudo and Matsumoto,
2001), because it has an intuitively sound interpre-
tation that each dimension of the mapped space is a
3For many real-world problems where the samples may be
inseparable, we allow the constraints are broken with some
penalty. In the experiments, we use so-called 1-norm soft mar-
gin formulation described as:
minimize 1
2
||w||2 + C
L?
i=1
?i
subject to yi(w ? xi + b) ? 1 ? ?i, i = 1, ? ? ? , L,
?i ? 0, i = 1, ? ? ? , L.
(weighted) conjunction of d features in the original
sample.
3.3 Multi-Class SVMs
As described above, the standard SVM learning con-
structs a binary classifier. To make a named entity
recognition system based on the BIO representation,
we require a multi-class classifier. Among several
methods for constructing a multi-class SVM (Hsu
and Lin, 2002), we use a pairwise method proposed
by Kre?el (1998) instead of the one-vs-rest method
used in (Yamada et al, 2000), and extend the BIO
representation to enable the training with the entire
GENIA corpus. Here we describe the one-vs-rest
method and the pairwise method to show the neces-
sity of our extension.
Both one-vs-rest and pairwise methods construct
a multi-class classifier by combining many binary
SVMs. In the following explanation, K denotes the
number of the target classes.
one-vs-rest Construct K binary SVMs, each of
which determines whether the sample should
be classified as class i or as the other classes.
The output is the class with the maximum f (x)
in Equation 1.
pairwise Construct K(K ? 1)/2 binary SVMs, each
of which determines whether the sample should
be classified as class i or as class j. Each binary
SVM has one vote, and the output is the class
with the maximum votes.
Because the SVM training is a quadratic optimiza-
tion program, its cost is super-linear to the size of the
training samples even with the tailored techniques
such as SMO (Platt, 1998) and kernel evaluation
caching (Joachims, 1998). Let L be the number of
the training samples, then the one-vs-rest method
takes time in K ? OS V M(L). The BIO formula-
tion produces one training sample per word, and
the training with the GENIA corpus involves over
100,000 training samples as can be seen from Ta-
ble 1. Therefore, it is apparent that the one-vs-
rest method is impractical with the GENIA corpus.
On the other hand, if target classes are equally dis-
tributed, the pairwise method will take time in K(K?
1)/2?OS V M(2L/K). This method is worthwhile be-
cause each training is much faster, though it requires
the training of (K ? 1)/2 times more classifiers. It
is also reported that the pairwise method achieves
higher accuracy than other methods in some bench-
marks (Kre?el, 1998; Hsu and Lin, 2002).
3.4 Input Features
An input x to an SVM classifier is a feature repre-
sentation of the word to be classified and its context.
We use a bit-vector representation, each dimension
of which indicates whether the input matches with
a certain feature. The following illustrates the well-
used features for the named entity recognition task.
wk,i =
???????????
1 if a word at k,Wk, is the ith word
in the vocabularyV
0 otherwise (word feature)
posk,i =
???????????
1 if Wk is assigned the ith POS tag
in the POS tag list POS
0 otherwise (part-of-speech feature)
prek,i =
???????????
1 if Wk starts with the ith prefix
in the prefix list P
0 otherwise (prefix feature)
suf k,i =
???????????
1 if Wk starts with the ith suffix
in the suffix list S
0 otherwise (suffix feature)
subk,i =
???????????
1 if Wk contains the ith substring
in the substring list SB
0 otherwise (substring feature)
pck,i =
?????
1 if Wk(k < 0) was assigned ith class
0 otherwise (preceding class feature)
In the above definitions, k is a relative word position
from the word to be classified. A negative value rep-
resents a preceding word?s position, and a positive
value represents a following word?s position. Note
that we assume that the classification proceeds left
to right as can be seen in the definition of the pre-
ceding class feature. For the SVM classification, we
does not use a dynamic argmax-type classification
such as the Viterbi algorithm, since it is difficult to
define a good comparable value for the confidence of
a prediction such as probability. The consequences
of this limitation will be discussed with the experi-
mental results.
Features usually form a group with some vari-
ables such as the position unspecified. In this paper,
we instantiate all features, i.e., instantiate for all i,
for a group and a position. Then, it is convenient to
denote a set of features for a group g and a position
k as gk (e.g., wk and posk). Using this notation, we
write a feature set as {w?1,w0, pre?1, pre0, pc?1}.4
This feature description derives the following input
vector.5
x = {w?1,1,w?1,2, ? ? ? ,w?1,|V|,w0,1, ? ? ? ,w0,|V|,
pre?1,1, ? ? ? , pre0,|P|, pc?1,1, ? ? ? , pc?1,K}
4We will further compress this as {?w, pre?[?1,0], pc?1}.
5Although a huge number of features are instantiated, only
a few features have value one for a given g and k pair.
4 Named Entity Recognition Using ME
Model
The Maximum Entropy method, with which we
compare our SVM-based method, defines the prob-
ability that the class is c given an input vector x as
follows.
P(c|x) = 1
Z(x)
?
i
? fi(c,x)i ,
where Z(x) is a normalization constant, and fi(c, x)
is a feature function. A feature function is defined
in the same way as the features in the SVM learn-
ing, except that it includes c in it like f (c, x) =
(c is the jth class) ? wi,k(x). If x contains pre-
viously assigned classes, then the most probable
class sequence, c?T1 = argmaxc1,??? ,cT
?T
t=1 P(ct|xt) is
searched by using the Viterbi-type algorithm. We
use the maximum entropy tagging method described
in (Kazama et al, 2001) for the experiments, which
is a variant of (Ratnaparkhi, 1996) modified to use
HMM state features.
5 Tuning of SVMs for Biomedical NE Task
5.1 Class Splitting Technique
In Section 3.3, we described that if target classes are
equally distributed, the pairwise method will reduce
the training cost. In our case, however, we have a
very unbalanced class distribution with a large num-
ber of samples belonging to the class ?O? (see Table
1). This leads to the same situation with the one-vs-
rest method, i.e., if LO is the number of the samples
belonging to the class ?O?, then the most dominant
part of the training takes time in K ? OS V M(LO).
One solution to this unbalanced class distribution
problem is to split the class ?O? into several sub-
classes effectively. This will reduce the training cost
for the same reason that the pairwise method works.
In this paper, we propose to split the non-entity
class according to part-of-speech (POS) informa-
tion of the word. That is, given a part-of-speech
tag set POS, we produce new |POS| classes, ?O-
p? p ? POS. Since we use a POS tagger that out-
puts 45 Penn Treebank?s POS tags in this paper, we
have new 45 sub-classes which correspond to non-
entity regions such as ?O-NNS? (plural nouns), ?O-
JJ? (adjectives), and ?O-DT? (determiners).
Splitting by POS information seems useful for im-
proving the system accuracy as well, because in the
named entity recognition we must discriminate be-
tween nouns in named entities and nouns in ordi-
nal noun phrases. In the experiments, we show this
class splitting technique not only enables the feasi-
ble training but also improves the accuracy.
5.2 Word Cache and HMM Features
In addition to the standard features, we explore word
cache feature and HMM state feature, mainly to
solve the data sparseness problem.
Although the GENIA corpus is the largest anno-
tated corpus for the biomedical domain, it is still
small compared with other linguistic annotated cor-
pora such as the Penn Treebank. Thus, the data
sparseness problem is severe, and must be treated
carefully. Usually, the data sparseness is prevented
by using more general features that apply to a
broader set of instances (e.g., disjunctions). While
polynomial kernels in the SVM learning can effec-
tively generate feature conjunctions, kernel func-
tions that can effectively generate feature disjunc-
tions are not known. Thus, we should explicitly add
dimensions for such general features.
The word cache feature is defined as the disjunc-
tion of several word features as:
wck{k1,??? ,kn},i ? ?k?kwk,i
We intend that the word cache feature captures the
similarities of the patterns with a common key word
such as follows.
(a) ?human W?2 W?1 W0? and ?human W?1 W0?
(b) ?W0 gene? and ?W0 W1 gene?
We use a left word cache defined as lwck,i ?
wc{?k,??? ,0},i, and a right word cache defined as
rwck,i ? wc{1,??? ,k},i for patterns like (a) and (b) in
the above example respectively.
Kazama et al (2001) proposed to use as features
the Viterbi state sequence of a hidden Markov model
(HMM) to prevent the data sparseness problem in
the maximum entropy tagging model. An HMM is
trained with a large number of unannotated texts by
using an unsupervised learning method. Because
the number of states of the HMM is usually made
smaller than |V|, the Viterbi states give smoothed
but maximally informative representations of word
patterns tuned for the domain, from which the raw
texts are taken.
The HMM feature is defined in the same way as
the word feature as follows.
hmmk,i =
???????????
1 if the Viterbi state for Wk is
the ith state in the HMM?s statesH
0 otherwise (HMM feature)
In the experiments, we train an HMM using raw
MEDLINE abstracts in the GENIA corpus, and
show that the HMM state feature can improve the
accuracy.
5.3 Implementation Issues
Towards practical named entity recognition using
SVMs, we have tackled the following implementa-
tion issues. It would be impossible to carry out the
experiments in a reasonable time without such ef-
forts.
Parallel Training: The training of pairwise SVMs
has trivial parallelism, i.e., each SVM can be trained
separately. Since computers with two or more CPUs
are not expensive these days, parallelization is very
practical solution to accelerate the training of pair-
wise SVMs.
Fast Winner Finding: Although the pairwise
method reduces the cost of training, it greatly in-
creases the number of classifications needed to de-
termine the class of one sample. For example, for
our experiments using the GENIA corpus, the BIO
representation with class splitting yields more than
4,000 classification pairs. Fortunately, we can stop
classifications when a class gets K ? 1 votes and this
stopping greatly saves classification time (Kre?el,
1998). Moreover, we can stop classifications when
the current votes of a class is greater than the others?
possible votes.
Support Vector Caching: In the pairwise method,
though we have a large number of classifiers, each
classifier shares some support vectors with other
classifiers. By storing the bodies of all support vec-
tors together and letting each classifier have only the
weights, we can greatly reduce the size of the clas-
sifier. The sharing of support vectors also can be
exploited to accelerate the classification by caching
the value of the kernel function between a support
vector and a classifiee sample.
6 Experiments
To conduct experiments, we divided 670 abstracts
of the GENIA corpus (Ver. 1.1) into the train-
ing part (590 abstracts; 4,487 sentences; 133,915
words) and the test part (80 abstracts; 622 sen-
tences; 18,211 words).6 Texts are tokenized by us-
ing Penn Treebank?s tokenizer. An HMM for the
HMM state features was trained with raw abstracts
of the GENIA corpus (39,116 sentences).7 The
number of states is 160. The vocabulary for the
word feature is constructed by taking the most fre-
quent 10,000 words from the above raw abstracts,
the prefix/suffix/prefix list by taking the most fre-
quent 10,000 prefixes/suffixes/substrings.8
The performance is measured by precision, recall,
and F-score, which are the standard measures for the
6Randomly selected set used in (Shimpuku, 2002). We do
not use paper titles, while he used.
7These do not include the sentences in the test part.
8These are constructed using the training part to make the
comparison with the ME method fair.
Table 2: Training time and accuracy with/without
the class splitting technique. The number of training
samples includes SOS and EOS (special words for
the start/end of a sentence).
no splitting splitting
training time acc. time acc.
samples (sec.) (F-score) (sec.) (F-
score)
16,000 2,809 37.04 5,581 36.82
32,000 13,614 40.65 9,175 41.36
48,000 21,174 42.44 9,709 42.49
64,000 40,869 42.52 12,502 44.34
96,000 - - 21,922 44.93
128,000 - - 36,846 45.99
named entity recognition. Systems based on the BIO
representation may produce an inconsistent class se-
quence such as ?O B-DNA I-RNA O?. We interpret
such outputs as follows: once a named entity starts
with ?B-C? then we interpret that the named entity
with class ?C? ends only when we see another ?B-?
or ?O-? tag.
We have implemented SMO algorithm (Platt,
1998) and techniques described in (Joachims, 1998)
for soft margin SVMs in C++ programming lan-
guage, and implemented support codes for pairwise
classification and parallel training in Java program-
ming language. To obtain POS information required
for features and class splitting, we used an English
POS tagger described in (Kazama et al, 2001).
6.1 Class Splitting Technique
First, we show the effect of the class splitting
described in Section 5.1. Varying the size of
training data, we compared the change in the
training time and the accuracy with and with-
out the class splitting. We used a feature set
{?w, pre, suf , sub, pos?[?2,??? ,2], pc[?2,?1]} and the in-
ner product kernel.9 The training time was mea-
sured on a machine with four 700MHz PentiumIIIs
and 16GB RAM. Table 2 shows the results of the
experiments. Figure 1 shows the results graphi-
cally. We can see that without splitting we soon suf-
fer from super-linearity of the SVM training, while
with splitting we can handle the training with over
100,000 samples in a reasonable time. It is very im-
portant that the splitting technique does not sacrifice
the accuracy for speed, rather improves the accuracy.
6.2 Word Cache and HMM State Features
In this experiment, we see the effect of the word
cache feature and the HMM state feature described
in Section 3.4. The effect is assessed by the
accuracy gain observed by adding each feature
set to a base feature set and the accuracy degra-
dation observed by subtracting it from a (com-
9Soft margin constant C is 1.0 throughout the experiments.
Table 3: Effect of each feature set assessed by
adding/subtracting (F-score). Changes in bold face
means positive effect.
feature set (A) adding (B) sub. (k=2) (C) sub. (k=3)
Base 42.86 47.82 49.27
Left cache 43.25 (+0.39) 47.77 (-0.05) 49.02 (-0.25)
Right cache 42.34 (-0.52) 47.81 (-0.01) 49.07 (-0.20)
HMM state 44.70 (+1.84) 47.25 (-0.57) 48.03 (-1.24)
POS 44.82 (+1.96) 48.29 (+0.47) 48.75 (-0.52)
Prec. class 44.58 (+1.72) 43.32 (-4.50) 43.84 (-5.43)
Prefix 42.77 (-0.09) 48.11 (+0.29) 48.73 (-0.54)
Suffix 45.88 (+3.02) 47.07 (-0.75) 48.48 (-0.79)
Substring 42.16 (-0.70) 48.38 (+0.56) 50.23 (+0.96)
plete) base set. The first column (A) in Ta-
ble 3 shows an adding case where the base fea-
ture set is {w[?2,??? ,2]}. The columns (B) and
(C) show subtracting cases where the base feature
set is {?w, pre, suf , sub, pos, hmm?[?k,??? ,k], lwck, rwck,
pc[?2,?1]} with k = 2 and k = 3 respectively. The
kernel function is the inner product. We can see that
word cache and HMM state features surely improve
the recognition accuracy. In the table, we also in-
cluded the accuracy change for other standard fea-
tures. Preceeding classes and suffixes are definitely
helpful. On the other hand, the substring feature is
not effective in our setting. Although the effects of
part-of-speech tags and prefixes are not so definite,
it can be said that they are practically effective since
they show positive effects in the case of the maxi-
mum performance.
6.3 Comparison with the ME Method
In this set of experiments, we compare our
SVM-based system with a named entity recog-
nition system based on the Maximum Entropy
method. For the SVM system, we used the fea-
ture set {?w, pre, suf , pos, hmm?[?3,??? ,3], lwc3, rwc3,
pc[?2,?1]}, which is shown to be the best in the pre-
vious experiment. The compared system is a max-
imum entropy tagging model described in (Kazama
et al, 2001). Though it supports several character
type features such as number and hyphen and some
conjunctive features such as word n-gram, we do not
use these features to compare the performance un-
der as close a condition as possible. The feature set
used in the maximum entropy system is expressed
as {?w, pre, suf , pos, hmm?[?2,??? ,2], pc[?2,?1]}.10 Both
systems use the BIO representation with splitting.
Table 4 shows the accuracies of both systems. For
the SVM system, we show the results with the inner
product kernel and several polynomial kernels. The
row ?All (id)? shows the accuracy from the view-
10When the width becomes [?3, ? ? ? , 3], the accuracy de-
grades (53.72 to 51.73 in F-score).
 0
 5000
 10000
 15000
 20000
 25000
 30000
 35000
 40000
 45000
 0  20000  40000  60000  80000  100000  120000  140000
Tra
inin
g T
ime
 (se
cond
s)
Number of training samples
No splitSplit
(a) Training size vs. time
 0.36
 0.37
 0.38
 0.39
 0.4
 0.41
 0.42
 0.43
 0.44
 0.45
 0.46
 0  5000  10000  15000  20000  25000  30000  35000  40000  45000
Ter
m A
ccu
rac
y (F
-Sco
re)
Training Time (seconds)
No splitSplit
(b) Training time vs. accuracy
Figure 1: Effect of the class splitting technique.
point of the identification task, which only finds the
named entity regions. The accuracies for several ma-
jor entity classes are also shown. The SVM system
with the 2-dimensional polynomial kernel achieves
the highest accuracy. This comparison may be un-
fair since a polynomial kernel has the effect of us-
ing conjunctive features, while the ME system does
not use such conjunctive features. Nevertheless, the
facts: we can introduce the polynomial kernel very
easily; there are very few parameters to be tuned;11
we could achieve the higher accuracy; show an ad-
vantage of the SVM system.
It will be interesting to discuss why the SVM sys-
tems with the inner product kernel (and the polyno-
mial kernel with d = 1) are outperformed by the ME
system. We here discuss two possible reasons. The
first is that the SVM system does not use a dynamic
decision such as the Viterbi algorithm, while the ME
system uses it. To see this, we degrade the ME sys-
tem so that it predicts the classes deterministically
without using the Viterbi algorithm. We found that
this system only marks 51.54 in F-score. Thus, it can
be said that a dynamic decision is important for this
named entity task. However, although a method to
convert the outputs of a binary SVM to probabilistic
values is proposed (Platt, 1999), the way to obtain
meaningful probabilistic values needed in Viterbi-
type algorithms from the outputs of a multi-class
SVM is unknown. Solving this problem is certainly
a part of the future work. The second possible rea-
son is that the SVM system in this paper does not
use any cut-off or feature truncation method to re-
move data noise, while the ME system uses a sim-
ple feature cut-off method.12 We observed that the
ME system without the cut-off only marks 49.11 in
11C, s, r, and d
12Features that occur less than 10 times are removed.
F-score. Thus, such a noise reduction method is
also important. However, the cut-off method for the
ME method cannot be applied without modification
since, as described in Section 3.4, the definition of
the features are different in the two approaches. It
can be said the features in the ME method is ?finer?
than those in SVMs. In this sense, the ME method
allows us more flexible feature selection. This is an
advantage of the ME method.
The accuracies achieved by both systems can be
said high compared with those of the previous meth-
ods if we consider that we have 24 named entity
classes. However, the accuracies are not sufficient
for a practical use. Though higher accuracy will be
achieved with a larger annotated corpus, we should
also explore more effective features and find effec-
tive feature combination methods to exploit such a
large corpus maximally.
7 Conclusion
We have described the use of Support Vector Ma-
chines for the biomedical named entity recognition
task. To make the training of SVMs with the GE-
NIA corpus practical, we proposed to split the non-
entity class by using POS information. In addition,
we explored the new types of features, word cache
and HMM states, to avoid the data sparseness prob-
lem. In the experiments, we have shown that the
class splitting technique not only makes training fea-
sible but also improves the accuracy. We have also
shown that the proposed new features also improve
the accuracy and the SVM system with the polyno-
mial kernel function outperforms the ME-based sys-
tem.
Acknowledgements
We would like to thank Dr. Jin-Dong Kim for pro-
viding us easy-to-use preprocessed training data.
Table 4: Comparison: The SVM-based system and the ME-based system. (precision/recall/F-score)
SVM ME
inner product polynomial (s = 0.01, r = 1.0))
type # d = 1 d = 2 d = 3
All (2,782) 50.7 /49.8 /50.2 54.6 /48.8 /51.5 56.2 /52.8 /54.4 55.1 /51.5 /53.2 53.4 /53.0 /53.2
All(id) 71.8 /70.4 /71.1 75.0 /67.1 /70.8 75.9 /71.4 /73.6 75.3 /70.3 /72.7 73.5 /72.9 /73.2
protein (709) 47.2 /55.2 /50.8 45.7 /64.9 /53.6 49.2 /66.4 /56.5 48.7 /64.7 /55.6 49.1 /62.1 /54.8
DNA (460) 39.9 /37.6 /38.7 48.2 /31.5 /38.1 49.6 /37.0 /42.3 47.9 /37.4 /42.0 47.3 /39.6 /43.1
cell line (121) 54.8 /47.1 /50.7 61.2 /43.0 /50.5 60.2 /46.3 /52.3 62.2 /46.3 /53.1 58.0 /53.7 /55.8
cell type (199) 67.6 /74.4 /70.8 67.4 /74.9 /71.0 70.0 /75.4 /72.6 68.6 /72.4 /70.4 69.9 /72.4 /71.1
lipid (109) 77.0 /61.5 /68.4 83.3 /50.5 /62.9 82.7 /61.5 /70.5 79.2 /56.0 /65.6 68.9 /65.1 /67.0
other names (590) 52.5 /53.9 /53.2 60.2 /55.9 /58.0 59.3 /58.0 /58.6 58.9 /57.8 /58.3 59.0 /61.7 /60.3
References
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A
maximum entropy approach to natural language processing.
Computational Linguistics, 22(1):39?71.
N. Collier, C. Nobata, and J. Tsujii. 2000. Extracting the names
of genes and gene products with a hidden Markov model. In
Proc. of COLING 2000, pages 201?207.
C. Cortes and V. Vapnik. 1995. Support vector networks. Ma-
chine Learning, 20:273?297.
C. Hsu and C. Lin. 2002. A comparison of methods for multi-
class Support Vector Machines. In IEEE Transactions on
Neural Networks. to appear.
T. Joachims. 1998. Making large-scale support vector machine
learning practical. In Advances in Kernel Methods, pages
169?184. The MIT Press.
J. Kazama, Y. Miyao, and J. Tsujii. 2001. A maximum entropy
tagger with unsupervised hidden markov models. In Proc. of
the 6th NLPRS, pages 333?340.
J. Kim, T. Ohta, Y. Tateisi, H. Mima, and J. Tsujii. 2001. XML-
based linguistic annotation of corpus. In Proc. of the First
NLP and XML Workshop.
U. Kre?el. 1998. Pairwise classification and support vector
machines. In Advances in Kernel Methods, pages 255?268.
The MIT Press.
T. Kudo and Y. Matsumoto. 2000. Use of support vector learn-
ing for chunk identification. In Proc. of CoNLL-2000 and
LLL-2000.
T. Kudo and Y. Matsumoto. 2001. Chunking with Support
Vector Machines. In Proc. of NAACL 2001, pages 192?199.
T. Nakagawa, T. Kudoh, and Y. Matsumoto. 2001. Unknown
word guessing and part-of-speech tagging using support vec-
tor machines. In Proc. of the 6th NLPRS, pages 325?331.
National Library of Medicine. 1999. MEDLINE. available at
http://www.ncbi.nlm.nih.gov/.
C. Nobata, N. Collier, and J. Tsujii. 1999. Automatic term
identification and classification in biology texts. In Proc. of
the 5th NLPRS, pages 369?374.
C. Nobata, N. Collier, and J. Tsujii. 2000. Comparison between
tagged corpora for the named entity task. In Proc. of the
Workshop on Comparing Corpora (at ACL?2000), pages 20?
27.
Y. Ohta, Y. Yamamoto, T. Okazaki, I. Uchiyama, and T. Tak-
agi. 1997. Automatic construction of knowledge base from
biological papers. In Proc. of the 5th ISMB, pages 218?225.
T. Ohta, Y. Tateisi, J. Kim, H. Mima, and Tsujii J. 2002. The
GENIA corpus: An annotated research abstract corpus in
molecular biology domain. In Proc. of HLT 2002.
J. C. Platt. 1998. Fast training of support vector machines us-
ing sequential minimal optimization. In Advances in Kernel
Methods, pages 185?208. The MIT Press.
J. C. Platt. 1999. Probabilistic outputs for support vector ma-
chines and comparisons to regularized likelihood methods.
Advances in Large Margin Classifiers.
D. Proux, F. Prechenmann, and L. Julliard. 2000. A pragmatic
information extraction strategy for gathering data on genetic
interactions. In Proc. of the 8th ISMB, pages 279?285.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunking us-
ing transformation-based learning. In Proc. of the 3rd ACL
Workshop on Very Large Corpora.
A. Ratnaparkhi. 1996. A maximum entropy model for part-
of-speech tagging. In Proc. of the Conference on Empirical
Methods in Natural Language Processing, pages 133?142.
S. Shimpuku. 2002. A medical/biological term recognizer with
a term hidden Markov model incorporating multiple infor-
mation sources. A master thesis. University of Tokyo.
K. Uchimoto, M. Murata, Q. Ma, H. Ozaku, and H. Isahara.
2000. Named entity extraction based on a maximum entropy
model and transformation rules. In Proc. of the 38th ACL,
pages 326?335.
V. Vapnik. 1995. The Nature of Statistical Learning Theory.
Springer Verlag.
A. Yakushiji, Y. Tateisi, Y. Miyao, and J. Tsujii. 2001. Event
extraction from biomedical papers using a full parser. In
Proc. of PSB 2001, pages 408?419.
H. Yamada, T. Kudo, and Y. Matsumoto. 2000. Using sub-
strings for technical term extraction and classification. IPSJ
SIGNotes, (NL-140):77?84. (in Japanese).
Evaluation and Extension of Maximum Entropy Models
with Inequality Constraints
Jun?ichi Kazama?
kazama@is.s.u-tokyo.ac.jp
?Department of Computer Science
University of Tokyo
Hongo 7-3-1, Bunkyo-ku,
Tokyo 113-0033, Japan
Jun?ichi Tsujii??
tsujii@is.s.u-tokyo.ac.jp
?CREST, JST
(Japan Science and Technology Corporation)
Honcho 4-1-8, Kawaguchi-shi,
Saitama 332-0012, Japan
Abstract
A maximum entropy (ME) model is usu-
ally estimated so that it conforms to equal-
ity constraints on feature expectations.
However, the equality constraint is inap-
propriate for sparse and therefore unre-
liable features. This study explores an
ME model with box-type inequality con-
straints, where the equality can be vio-
lated to reflect this unreliability. We eval-
uate the inequality ME model using text
categorization datasets. We also propose
an extension of the inequality ME model,
which results in a natural integration with
the Gaussian MAP estimation. Experi-
mental results demonstrate the advantage
of the inequality models and the proposed
extension.
1 Introduction
The maximum entropy model (Berger et al, 1996;
Pietra et al, 1997) has attained great popularity in
the NLP field due to its power, robustness, and suc-
cessful performance in various NLP tasks (Ratna-
parkhi, 1996; Nigam et al, 1999; Borthwick, 1999).
In the ME estimation, an event is decomposed
into features, which indicate the strength of certain
aspects in the event, and the most uniform model
among the models that satisfy:
Ep?[fi] = Ep[fi], (1)
for each feature. Ep?[fi] represents the expectation
of feature fi in the training data (empirical expec-
tation), and Ep[fi] is the expectation with respect
to the model being estimated. A powerful and ro-
bust estimation is possible since the features can be
as specific or general as required and does not need
to be independent of each other, and since the most
uniform model avoids overfitting the training data.
In spite of these advantages, the ME model still
suffers from a lack of data as long as it imposes the
equality constraint (1), since the empirical expecta-
tion calculated from the training data of limited size
is inevitably unreliable. A careful treatment is re-
quired especially in NLP applications since the fea-
tures are usually very sparse. In this study, text cat-
egorization is used as an example of such tasks with
sparse features.
Previous work on NLP proposed several solutions
for this unreliability such as the cut-off, which sim-
ply omits rare features, the MAP estimation with
the Gaussian prior (Chen and Rosenfeld, 2000), the
fuzzy maximum entropy model (Lau, 1994), and fat
constraints (Khudanpur, 1995; Newman, 1977).
Currently, the Gaussian MAP estimation (com-
bined with the cut-off) seems to be the most promis-
ing method from the empirical results. It succeeded
in language modeling (Chen and Rosenfeld, 2000)
and text categorization (Nigam et al, 1999). As
described later, it relaxes constraints like Ep?[fi] ?
Ep[fi] =
?
i
?2
, where ?i is the model?s parameter.
This study follows this line, but explores the fol-
lowing box-type inequality constraints:
Ai ? Ep?[fi] ? Ep[fi] ? ?Bi, Ai, Bi > 0. (2)
Here, the equality can be violated by the widths Ai
and Bi. We refer to the ME model with the above
inequality constraints as the inequality ME model.
This inequality constraint falls into a type of fat con-
straints, ai ? Ep[fi] ? bi, as suggested by (Khudan-
pur, 1995). However, as noted in (Chen and Rosen-
feld, 2000), this type of constraint has not yet been
applied nor evaluated for NLPs.
The inequality ME model differs from the Gaus-
sian MAP estimation in that its solution becomes
sparse (i.e., many parameters become zero) as a re-
sult of optimization with inequality constraints. The
features with a zero parameter can be removed from
the model without changing its prediction behavior.
Therefore, we can consider that the inequality ME
model embeds feature selection in its estimation.
Recently, the sparseness of the solution has been rec-
ognized as an important concept in constructing ro-
bust classifiers such as SVMs (Vapnik, 1995). We
believe that the sparse solution improves the robust-
ness of the ME model as well.
We also extend the inequality ME model so that
the constraint widths can move using slack vari-
ables. If we penalize the slack variables by their 2-
norm, we obtain a natural integration of the inequal-
ity ME model and the Gaussian MAP estimation.
While it incorporates the quadratic stabilization of
the parameters as in the Gaussian MAP estimation,
the sparseness of the solution is preserved.
We evaluate the inequality ME models empiri-
cally, using two text categorization datasets. The
results show that the inequality ME models outper-
form the cut-off and the Gaussian MAP estimation.
Such high accuracies are achieved with a fairly small
number of active features, indicating that the sparse
solution can effectively enhance the performance. In
addition, the 2-norm extended model is shown to be
more robust in several situations.
2 The Maximum Entropy Model
The ME estimation of a conditional model p(y|x)
from the training examples {(xi, yi)} is formulated
as the following optimization problem.1
maximize
p
H(p) =
?
x
p?(x)
?
y
p(y|x) log p(y|x)
subject to Ep?[fi]? Ep[fi] = 0 1 ? i ? F. (3)
1To be precise, we have also the constraints
P
y
p(y|x) ?
1 = 0 x ? X . Note that although we explain using a condi-
tional model throughout the paper, the discussion can be applied
easily to a joint model by considering the condition x is fixed.
The empirical expectations and model expectations
in the equality constraints are defined as follows.
Ep?[fi] =
?
x p?(x)
?
y p?(y|x)fi(x, y), (4)
Ep[fi] =
?
x p?(x)
?
y p(y|x)fi(x, y), (5)
p?(x) = c(x)/L, p?(y|x) = c(x, y)/c(x), (6)
where c(?) indicates the number of times ? occurred
in the training data, and L is the number of training
examples.
By the Lagrange method, p(y|x) is found to have
the following parametric form:
p?(y|x) =
1
Z(x)
exp(
?
i
?ifi(x, y)), (7)
where Z(x) =
?
y exp(
?
i ?ifi(x, y)). The dual
objective function becomes:
L(?) =
?
x p?(x)
?
y p?(y|x)
?
i ?ifi(x, y) (8)
?
?
x p?(x) log
?
y exp(
?
i ?ifi(x, y)).
The ME estimation becomes the maximization of
L(?). And it is equivalent to the maximization of the
log-likelihood: LL(?) = log
?
x,y p?(y|x)
p?(x,y)
.
This optimization can be solved using algo-
rithms such as the GIS algorithm (Darroch and Rat-
cliff, 1972) and the IIS algorithm (Pietra et al,
1997). In addition, gradient-based algorithms can
be applied since the objective function is concave.
Malouf (2002) compares several algorithms for the
ME estimation including GIS, IIS, and the limited-
memory variable metric (LMVM) method, which is
a gradient-based method, and shows that the LMVM
method requires much less time to converge for real
NLP datasets. We also observed that the LMVM
method converges very quickly for the text catego-
rization datasets with an improvement in accuracy.
Therefore, we use the LMVM method (and its vari-
ant for the inequality models) throughout the exper-
iments. Thus, we only show the gradient when men-
tioning the training. The gradient of the objective
function (8) is computed as:
?L(?)
??
i
= Ep?[fi]? Ep[fi]. (9)
3 The Inequality ME Model
The maximum entropy model with the box-type in-
equality constraints (2) can be formulated as the fol-
lowing optimization problem:
maximize
p
?
x
p?(x)
?
y
p(y|x) log p(y|x),
subject to Ep?[fi]? Ep[fi]? Ai ? 0, (10)
Ep[fi]? Ep?[fi]? Bi ? 0. (11)
By using the Lagrange method for optimization
problems with inequality constraints, the following
parametric form is derived.
p?,?(y|x) =
1
Z(x)
exp(
?
i
(?i ? ?i)fi(x, y)),
?i ? 0, ?i ? 0, (12)
where parameters ?i and ?i are the Lagrange mul-
tipliers corresponding to constraints (10) and (11).
The Karush-Kuhn-Tucker conditions state that, at
the optimal point,
?i(Ep?[fi]? Ep[fi]? Ai) = 0,
?i(Ep[fi]? Ep?[fi]? Bi) = 0.
These conditions mean that the equality constraint is
maximally violated when the parameter is non-zero,
and if the violation is strictly within the widths, the
parameter becomes zero. We call a feature upper
active when ?i > 0, and lower active when ?i > 0.
When ?i??i = 0, we call that feature active.2 Inac-
tive features can be removed from the model without
changing its behavior. Since Ai >0 and Bi >0, any
feature should not be upper active and lower active
at the same time.3
The inequality constraints together with the con-
straints
?
y p(y|x)? 1 = 0 define the feasible re-
gion in the original probability space, on which the
entropy varies and can be maximized. The larger
the widths, the more the feasible region is enlarged.
Therefore, it can be implied that the possibility of a
feature becoming inactive (the global maximal point
is strictly within the feasible region with respect
to that feature?s constraints) increases if the corre-
sponding widths become large.
2The term ?active? may be confusing since in the ME re-
search, a feature is called active when f
i
(x, y) > 0 for an
event. However, we follow the terminology in the constrained
optimization.
3This is only achieved with some tolerance in practice.
The solution for the inequality ME model would
become sparse if the optimization determines many
features as inactive with given widths. The relation
between the widths and the sparseness of the solu-
tion is shown in the experiment.
The dual objective function becomes:
L(?, ?) =
?
x p?(x)
?
y p?(y|x)
?
i(?i ? ?i)fi(x, y)
?
?
x p?(x) log
?
y exp(
?
i(?i ? ?i)fi(x, y))
?
?
i ?iAi ?
?
i ?iBi. (13)
Thus, the estimation is formulated as:
maximize
?
i
?0,?
i
?0
L(?, ?).
Unlike the optimization in the standard maximum
entropy estimation, we now have bound constraints
on parameters which state that parameters must be
non-negative. In addition, maximizing L(?, ?) is no
longer equivalent to maximizing the log-likelihood
LL(?, ?). Instead, we maximize:
LL(?, ?) ?
?
i ?iAi ?
?
i ?iBi. (14)
Although we can use many optimization algorithms
to solve this dual problem since the objective func-
tion is still concave, a method that supports bounded
parameters must be used. In this study, we use the
BLMVM algorithm (Benson and More?, ), a variant
of the limited-memory variable metric (LMVM) al-
gorithm, which supports bound constraints.4
The gradient of the objective function is:
?L(?,?)
??
i
= Ep?[fi] ? Ep[fi] ? Ai,
?L(?,?)
??
i
= Ep[fi] ? Ep?[fi] ? Bi. (15)
4 Soft Width Extension
In this section, we present an extension of the in-
equality ME model, which we call soft width. The
soft width allows the widths to move as Ai + ?i
and ?Bi ? ?i using slack variables, but with some
penalties in the objective function. This soft width
extension is analogous to the soft margin extension
of the SVMs, and in fact, the mathematical discus-
sion is similar. If we penalize the slack variables
4Although we consider only the gradient-based method here
as noted earlier, an extension of GIS or IIS to support bounded
parameters would also be possible.
by their 2-norm, we obtain a natural combination of
the inequality ME model and the Gaussian MAP es-
timation. We refer to this extension using 2-norm
penalty as the 2-norm inequality ME model. As the
Gaussian MAP estimation has been shown to be suc-
cessful in several tasks, it should be interesting em-
pirically, as well as theoretically, to incorporate the
Gaussian MAP estimation into the inequality model.
We first review the Gaussian MAP estimation in the
following, and then we describe our extension.
4.1 The Gaussian MAP estimation
In the Gaussian MAP ME estimation (Chen and
Rosenfeld, 2000), the objective function is:
LL(?) ?
?
i(
1
2?2
i
)?2i , (16)
which is derived as a consequence of maximizing
the log-likelihood of the posterior probability, using
a Gaussian distribution centered around zero with
the variance ?2i as a prior on parameters. The gra-
dient becomes:
?L(?)
??
i
= Ep?[fi]? Ep[fi]?
?
i
?2
i
. (17)
At the optimal point, Ep?[fi] ? Ep[fi] ? ?i?2
i
= 0.
Therefore, the Gaussian MAP estimation can also be
considered as relaxing the equality constraints. The
significant difference between the inequality ME
model and the Gaussian MAP estimation is that the
parameters are stabilized quadratically in the Gaus-
sian MAP estimation (16), while they are stabilized
linearly in the inequality ME model (14).
4.2 2-norm penalty extension
Our 2-norm extension to the inequality ME model is
as follows.5
maximize
p,?,?
H(p)? C
1
?
i ?i
2
? C
2
?
i ?
2
i ,
subject to Ep?[fi] ? Ep[fi] ? Ai ? ?i, (18)
Ep[fi] ? Ep?[fi] ? Bi ? ?i, (19)
5It is also possible to impose 1-norm penalties in the objec-
tive function. It yields an optimization problem which is iden-
tical to the inequality ME model except that the parameters are
upper-bounded as 0 ? ?
i
? C
1
and 0 ? ?
i
? C
2
. We will not
investigate this 1-norm extension in this paper and leave it for
future research.
where C
1
and C
2
is the penalty constants. The para-
metric form is identical to the inequality ME model
(12). However, the dual objective function becomes:
LL(?, ?) ?
?
i
(
?iAi +
?2
i
4C
1
)
?
?
i
(
?iBi +
?2
i
4C
2
)
.
Accordingly, the gradient becomes:
?L(?,?)
??
i
= Ep?[fi] ? Ep[fi] ?
(
Ai +
?
i
2C
1
)
,
?L(?,?)
??
i
= Ep[fi]? Ep?[fi]?
(
Bi +
?
i
2C
2
)
. (20)
It can be seen that this model is a natural combina-
tion of the inequality ME model and the Gaussian
MAP estimation. It is important to note that the so-
lution sparseness is preserved in the above model.
5 Calculation of the Constraint Width
The widths, Ai and Bi, in the inequality constraints
are desirably widened according to the unreliability
of the feature (i.e., the unreliability of the calculated
empirical expectation). In this paper, we examine
two methods to determine the widths.
The first is to use a common width for all features
fixed by the following formula.
Ai = Bi = W ?
1
L
, (21)
where W is a constant, width factor, to control the
widths. This method can only capture the global re-
liability of all the features. That is, only the reli-
ability of the training examples as a whole can be
captured. We call this method single.
The second, which we call bayes, is a method that
determines the widths based on the Bayesian frame-
work to differentiate between the features depending
on their reliabilities.
For many NLP applications including text catego-
rization, we use the following type of features.
fj,i(x, y) = hi(x) if y = yj, 0 otherwise. (22)
In this case, if we assume the approximation,
p?(y|x) ? p?(y|hi(x) > 0), the empirical expectation
can be interpreted as follows.6
Ep?[fj,i]=
?
x: h
i
(x)>0
p?(x)p?(y = yj|hi(x)>0)hi(x).
6This is only for estimating the unreliability, and is not used
to calculate the actual empirical expectations in the constraints.
Here, a source of unreliability is p?(y|hi(x)>0). We
consider p?(y|hi(x) > 0) as the parameter ? of the
Bernoulli trials. That is, p(y|hi(x) > 0) = ? and
p(y?|hi(x)>0) = 1 ? ?. Then, we estimate the pos-
terior distribution of ? from the training examples
by Bayesian estimation and utilize the variance of
the distribution. With the uniform distribution as the
prior, k times out of n trials give the posterior distri-
bution: p(?) = Be(1+k, 1+n?k), where Be(?, ?)
is the beta distribution. The variance is calculated as
follows.
V [?] =
(1+k)(1+n?k)
(2+n)2(n+3)
. (23)
Letting k = c(fj,i(x, y)>0) and n = c(hi(x)>0),
we obtain fine-grained variances narrowed accord-
ing to c(hi(x) > 0) instead of a single value, which
just captures the global reliability. Assuming the in-
dependence of training examples, the variance of the
empirical expectation becomes:
V
[
Ep?[fj,i]
]
=
[
?
x: h
i
(x)>0 {p?(x)hi(x)}
2
]
V [?j,i].
Then, we calculate the widths as follows:
Ai = Bi = W ?
?
V
[
Ep?[fj,i]
]
. (24)
6 Experiments
For the evaluation, we use the ?Reuters-21578, Dis-
tribution 1.0? dataset and the ?OHSUMED? dataset.
The Reuters dataset developed by David D. Lewis
is a collection of labeled newswire articles.7 We
adopted ?ModApte? split to split the collection,
and we obtained 7, 048 documents for training, and
2, 991 documents for testing. We used 112 ?TOP-
ICS? that actually occurred in the training set as the
target categories.
The OHSUMED dataset (Hersh et al, 1994) is a
collection of clinical paper abstracts from the MED-
LINE database. Each abstract is manually assigned
MeSH terms. We simplified a MeSH term, like
?A/B/C ? A?, and used the most frequent 100
simplified terms as the target categories. We ex-
tracted 9, 947 abstracts for training, and 9, 948 ab-
stracts for testing from the file ?ohsumed.91.?
A documents is converted to a bag-of-words vec-
tor representation with TFIDF values, after the stop
7Available from http://www.daviddlewis.com/resources/
words are removed and all the words are downcased.
Since the text categorization task requires that mul-
tiple categories are assigned if appropriate, we con-
structed a binary categorizer, pc(y ? {+1,?1}|d),
for each category c. If the probability pc(+1|d) is
greater than 0.5, the category is assigned. To con-
struct a conditional maximum entropy model, we
used the feature function of the form (22), where
hi(d) returns the TFIDF value of the i-th word of
the document vector.
We implemented the estimation algorithms as an
extension of an ME estimation tool, Amis,8 using
the Toolkit for Advanced Optimization (TAO) (Ben-
son et al, 2002), which provides the LMVM and the
BLMVM optimization modules. For the inequal-
ity ME estimation, we added a hook that checks the
KKT conditions after the normal convergence test.9
We compared the following models:
? ME models only with cut-off (cut-off ),
? ME models with cut-off and the Gaussian MAP
estimation (gaussian),
? Inequality ME models (ineq),
? Inequality ME models with 2-norm extension
described in Section 4 (2-norm),10
For the inequality ME models, we compared the two
methods to determine the widths, single and bayes,
as described in Section 5. Although the Gaussian
MAP estimation can use different ?i for each fea-
ture, we used a common variance ? for gaussian.
Thus, gaussian roughly corresponds to single in the
way of dealing with the unreliability of features.
Note that, for inequality models, we started with
all possible features and rely on their ability to re-
move unnecessary features automatically by solu-
tion sparseness. The average maximum number of
features in a categorizer is 63, 150.0 for the Reuters
dataset and 116, 452.0 for the OHSUMED dataset.
8Developed by Yusuke Miyao so as to support various
ME estimations such as the efficient estimation with compli-
cated event structures (Miyao and Tsujii, 2002). Available at
http://www-tsujii.is.s.u-tokyo.ac.jp/
?yusuke/amis
9The tolerance for the normal convergence test (relative im-
provement) and the KKT check is 10?4. We stop the training if
the KKT check has been failed many times and the ratio of the
bad (upper and lower active) features among the active features
is lower than 0.01.
10Here, we fix the penalty constants C
1
= C
2
= 10
16
.
 0.8
 0.805
 0.81
 0.815
 0.82
 0.825
 0.83
 0.835
 0.84
 0.845
 0.85
 1e-16 1e-14 1e-12 1e-10 1e-08 1e-06 1e-04 0.01 1
Ac
cu
ra
cy
 (F
-sc
ore
)
Width Factor
A
B
CD
A: ineq + single
B: 2-norm + single
C: ineq + bayes
D: 2-norm + bayes
cut-off best
gaussian best
(a) Reuters
 0.54
 0.55
 0.56
 0.57
 0.58
 0.59
 0.6
 0.61
 0.62
 1e-16 1e-14 1e-12 1e-10 1e-08 1e-06 1e-04 0.01 1 100
Ac
cu
ra
cy
 (F
-sc
ore
)
Width Factor
A
B
C
D
A: ineq + single
B: 2-norm + single
C: ineq + bayes
D: 2-norm + bayes
cut-off best
gaussian best
(b) OHSUMED
Figure 1: Accuracies as a function of the width factor W for the development sets.
 0
 10000
 20000
 30000
 40000
 50000
 60000
 70000
 1e-16 1e-14 1e-12 1e-10 1e-08 1e-06 1e-04 0.01 1
# 
of
 A
ct
ive
 F
ea
tu
re
s
Width Factor
A
B
C
D
A: ineq + single
B: 2-norm + single
C: ineq + bayes
D: 2-norm + bayes
(a) Reuters
 0
 20000
 40000
 60000
 80000
 100000
 120000
 1e-16 1e-14 1e-12 1e-10 1e-08 1e-06 1e-04 0.01 1 100
# 
of
 A
ct
ive
 F
ea
tu
re
s
Width Factor
A
B
C
D
A: ineq + single
B: 2-norm + single
C: ineq + bayes
D: 2-norm + bayes
(b) OHSUMED
Figure 2: The average number of active features as a function of width factor W .
6.1 Results
We first found the best values for the control param-
eters of each model, W , ?, and the cut-off threshold,
by using the development set. We show that the in-
equality models outperform the other methods in the
development set. We then show that these values are
valid for the evaluation set. We used the first half of
the test set as the development set, and the second
half as the evaluation set.
Figure 1 shows the accuracies of the inequality
ME models for various width factors. The accura-
cies are presented by the ?micro averaged? F-score.
The horizontal lines show the highest accuracies of
cut-off and gaussian models found by exhaustive
search. For cut-off, we varied the cut-off thresh-
old and found the best threshold. For gaussian, we
varied ? with each cut-off threshold, and found the
best ? and cut-off combination. We can see that
the inequality models outperform the cut-off method
and the Gaussian MAP estimation with an appro-
priate value for W in both datasets. Although the
OHSUMED dataset seems harder than the Reuters
dataset, the improvement in the OHSUMED dataset
is greater than that in the Reuters dataset. This may
be because the OHSUMED dataset is more sparse
than the Reuters dataset. The 2-norm extension
boosts the accuracies, especially for bayes, at the
moderate W s (i.e., with the moderate numbers of
active features). However, we can not observe the
apparent advantage of the 2-norm extension in terms
of the highest accuracy here.
Figure 2 shows the average number of active fea-
tures of each inequality ME model for various width
factors. We can see that active features increase
 0.79
 0.8
 0.81
 0.82
 0.83
 0.84
 0.85
 100  1000  10000
Ac
cu
ra
cy
 (F
-sc
ore
)
# of Active Features
B
D
F
E
B: 2-norm + single
D: 2-norm + bayes
E: cut-off
F: gaussian
(a) Reuters
 0.54
 0.55
 0.56
 0.57
 0.58
 0.59
 0.6
 0.61
 0.62
 1000  10000  100000
Ac
cu
ra
cy
 (F
-sc
ore
)
# of Active Features
B
D
F E
B: 2-norm + single
D: 2-norm + bayes
E: cut-off
F: gaussian
(b) OHSUMED
Figure 3: Accuracies as a function of the average number of active features for the development sets. For
gaussian, the accuracy with the best ? found by exhaustive search is shown for each cut-off threshold.
when the widths become small as expected.
Figure 3 shows the accuracy of each model as a
function of the number of active features. We can
see that the inequality ME models achieve the high-
est accuracy with a fairly small number of active fea-
tures, removing unnecessary features on their own.
Besides, they consistently achieve much higher ac-
curacies than the cut-off and the Gaussian MAP es-
timation with a small number of features.
Table 1 summarizes the above results including
the best control parameters for the development set,
and shows how well each method performs for the
evaluation set with these parameters. We can see that
the best parameters are valid for the evaluation sets,
and the inequality ME models outperform the other
methods in the evaluation set as well. This means
that the inequality ME model is generally superior
to the cut-off method and the Gaussian MAP estima-
tion. At this point, the 2-norm extension shows the
advantage of being robust, especially for the Reuters
dataset. That is, the 2-norm models outperform the
normal inequality models in the evaluation set. To
see the reason for this, we show the average cross
entropy of each inequality model as a function of
the width factor in Figure 4. The average cross en-
tropy was calculated as ? 1
C
?
c
1
L
?
i log pc(yi|di),
where C is the number of categories. The cross en-
tropy of the 2-norm model is consistently more sta-
ble than that of the normal inequality model. Al-
though there is no simple relation between the abso-
lute accuracy and the cross entropy, this consistent
difference can be one explanation for the advantage
of the 2-norm extension. Besides, it is possible that
the effect of 2-norm extension appears more clearly
in the Reuters dataset because the robustness is more
important in the Reuters dataset since the develop-
ment set is rather small and easy to overfit.
Lastly, we could not observe the advantage of
bayes method in these experiments. However, since
our method is still in development, it is premature
to conclude that the idea of using different widths
according to its unreliability is not successful. It is
possible that the uncertainty of p?(x), which were not
concerned about, is needed to be modeled, or the
Bernoulli trial assumption is inappropriate. Further
investigation on these points must be done.
7 Conclusion and Future Work
We have shown that the inequality ME models
outperform the cut-off method and the Gaussian
MAP estimation, using the two text categoriza-
tion datasets. Besides, the inequality ME models
achieved high accuracies with a small number of
features due to the sparseness of the solution. How-
ever, it is an open question how the inequality ME
model differs from other sophisticated methods of
feature selection based on other criteria.
Future work will investigate the details of the in-
equality model including the effect of the penalty
constants of the 2-norm extension. Evaluations on
other NLP tasks are also planned. In addition, we
need to analyze the inequality ME model further to
Table 1: The summary of the experiments.
Reuters OHSUMED
best setting # active feats acc (dev) acc (eval) best setting # active feats acc (dev) acc (eval)
cut-off cthr=2 16, 961.9 83.24 86.38 cthr=0 116, 452.0 58.83 58.35
gaussian cthr=3, ?=4.22E3 12, 326.6 84.01 87.04 cthr=8, ?=2.55E3 10, 154.7 59.53 59.08
ineq+single W =1.78E?11 9, 479.9 84.47 87.41 W =4.22E?2 1, 375.5 61.23 61.10
2-norm+single W =5.62E?11 6, 611.1 84.35 87.59 W =4.50E?2 1, 316.5 61.26 61.23
ineq+bayes W =3.16E?15 63, 150.0 84.21 87.37 W =9.46 1, 136.6 60.65 60.31
2-norm+bayes W =3.16E?9 10, 022.3 84.01 87.57 W =9.46 1, 154.5 60.67 60.32
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 0.16
 0.18
 1e-16 1e-14 1e-12 1e-10 1e-08 1e-06 1e-04 0.01 1
Av
g.
 E
nt
ro
py
Width Factor
A
B
C
D
A: ineq + single
B: 2-norm + single
C: ineq + bayes
D: 2-norm + bayes
(a) Reuters
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1.8
 2
 1e-16 1e-14 1e-12 1e-10 1e-08 1e-06 1e-04 0.01 1 100
Av
g.
 E
nt
ro
py
Width Factor
A
B
C
D
A: ineq + single
B: 2-norm + single
C: ineq + bayes
D: 2-norm + bayes
(b) OHSUMED
Figure 4: W vs. the average cross entropy for the development sets.
clarify the reasons for its success.
Acknowledgments We would like to thank
Yusuke Miyao, Yoshimasa Tsuruoka, and the
anonymous reviewers for many helpful comments.
References
S. J. Benson and J. J. More?. A limited memory variable metric
method for bound constraint minimization. Technical Re-
port ANL/MCS-P909-0901, Argonne National Laboratory.
S. Benson, L. C. McInnes, J. J. More?, and J. Sarich. 2002.
TAO users manual. Technical Report ANL/MCS-TM-242-
Revision 1.4, Argonne National Laboratory.
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A
maximum entropy approach to natural language processing.
Computational Linguistics, 22(1):39?71.
A. Borthwick. 1999. A maximum entropy approach to named
entity recognition. Ph.D. Thesis. New York University.
S. F. Chen and R. Rosenfeld. 2000. A survey of smoothing
techniques for ME models. IEEE Trans. on Speech and Au-
dio Processing, 8(1):37?50.
J. N. Darroch and D. Ratcliff. 1972. Generalized iterative
scaling for log-linear models. The Annals of Mathematical
Statistics, 43:1470?1480.
W. Hersh, C. Buckley, T.J. Leone, and D. Hickam. 1994.
OHSUMED: An interactive retrieval evaluation and new
large test collection for research. In Proc. of the 17th An-
nual ACM SIGIR Conference, pages 192?201.
S. Khudanpur. 1995. A method of ME estimation with re-
laxed constraints. In Johns Hopkins Univ. Language Model-
ing Workshop, pages 1?17.
R. Lau. 1994. Adaptive statistical language modeling. A Mas-
ter?s Thesis. MIT.
R. Malouf. 2002. A comparison of algorithms for maximum
entropy parameter estimation. In Proc. of the sixth CoNLL.
Y. Miyao and J. Tsujii. 2002. Maximum entropy estimation for
feature forests. In Proc. of HLT 2002.
W. Newman. 1977. Extension to the ME method. In IEEE
Trans. on Information Theory, volume IT-23, pages 89?93.
K. Nigam, J. Lafferty, and A. McCallum. 1999. Using maxi-
mum entropy for text classification. In IJCAI-99 Workshop
on Machine Learning for Information Filtering, pages 61?
67.
S. Pietra, V. Pietra, and J. Lafferty. 1997. Inducing features of
random fields. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 19(4):380?393.
A. Ratnaparkhi. 1996. A maximum entropy model for part-of-
speech tagging. In Proc. of the EMNLP, pages 133?142.
V. Vapnik. 1995. The Nature of Statistical Learning Theory.
Springer Verlag.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 185?192
Manchester, August 2008
Looking for Trouble
Stijn De Saeger Kentaro Torisawa
Language Infrastructure Group
National Institute of Information
and Communications Technology
{stijn,torisawa}@nict.go.jp
Jun?ichi Kazama
School of Information Science
Japan Advanced Institute
of Science and Technology
kazama@jaist.ac.jp
Abstract
This paper presents a method for mining
potential troubles or obstacles related to
the use of a given object. Some exam-
ple instances of this relation are ?medicine,
side effect? and ?amusement park, height
restriction?. Our acquisition method con-
sists of three steps. First, we use an un-
supervised method to collect training sam-
ples from Web documents. Second, a set
of expressions generally referring to trou-
bles is acquired by a supervised learning
method. Finally, the acquired troubles
are associated with objects so that each
of the resulting pairs consists of an ob-
ject and a trouble or obstacle in using that
object. To show the effectiveness of our
method we conducted experiments using
a large collection of Japanese Web doc-
uments for acquisition. Experimental re-
sults show an 85.5% precision for the top
10,000 acquired troubles, and a 74% pre-
cision for the top 10% of over 60,000 ac-
quired object-trouble pairs.
1 Introduction
The Stanford Encyclopedia of Philosophy defines
an artifact as ?. . . an object that has been inten-
tionally made or produced for a certain purpose?.
Because of this purpose-orientedness, most human
actions relating to an object or artifact fall into
two broad categories ? actions relating to its in-
tended use (e.g. reading a book), and the prepa-
rations necessary therefore (like buying the book).
Information concerning potential obstacles, harm-
ful effects or troubles that interfere with this in-
tended use is therefore highly relevant to the user.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
While some such troubles are self-evident, others
represent a genuine obstacle whose existence was
thusfar unknown to the user. For example, in early
2008 a food poisoning case caused a big media stir
in Japan when dozens of people fell ill after eating
Chinese-imported frozen food products containing
residual traces of toxic pesticides. While suppos-
edly the presence of toxic chemicals in imported
frozen foods had already been established on sev-
eral occasions before, until the recent incidents
public awareness of these facts remained low. In
retrospect, a publicly available system suggesting
?residual agrichemicals? as a potential danger with
the consumption of ?frozen foods? based on in-
formation mined from a large collection of Web
documents might have led to earlier detection of
this crisis. From the viewpoint of manufacturers
as well, regularly monitoring the Internet for prod-
uct names and associated troubles may allow them
to find out about perceived flaws in their products
sooner and avoid large scale recalls and damage to
their brand.
For a less dramatic example, searching for
?Tokyo Disneyland? on the Internet typically
yields many commercial sites offering travel deals,
but little or no information about potential ob-
stacles such as ?height restrictions? (constraints
on who can enjoy a given attraction
1
) and ?traf-
fic jams? (a necessary preparation for enjoying a
theme park is actually getting there in time). Ofter
users have no way of finding out about this until
they actually go there.
These examples demonstrate the importance of
a highly accurate automatic method for acquir-
ing what we will call ?object-trouble? relations ?
pairs ?e
o
, e
t
? in which the thing referred to by e
t
constitutes an (actual or potential) trouble, obsta-
cle or risk in the context of use of an object e
o
.
1
For example, one has to be over 3 ft. tall to get on the
Splash Mountain.
185
Large scale acquisition of this type of contextual
knowledge has not been thoroughly studied so far.
In this paper, we propose a method for automati-
cally acquiring Japanese noun phrases referring to
troubles, (henceforth referred to as trouble expres-
sions), and associating them with expressions de-
noting artifacts, objects or facilities.
Our acquisition method consists of three steps.
As a first step, we use an unsupervised method for
efficiently collecting training data from a Web cor-
pus. Then, a set of expressions denoting troubles is
acquired by a supervised learning method ? Sup-
port Vector Machines (Vapnik, 1998) ? trained on
this data. Finally, the acquired trouble expressions
are paired with noun phrases referring to objects,
using a combination of pairwise mutual informa-
tion and a verb-noun dependency filter based on
statistics in a Web corpus.
A broad focus on noun-verb dependencies ?
and in particular the distinction between depen-
dency relations with negated versus non-negated
verbs ? is the main characteristic of our method.
While this distinction did not prove useful for im-
proving the supervised classifier?s performance in
step 2, it forms the basis underlying the unsuper-
vised method for training sample selection in the
first step, and the final filtering mechanism in the
third step.
The rest of this paper is organized as follows.
Section 2 points out related work. Section 3 ex-
amines the notion of trouble expressions and their
evidences. Section 4 describes our method, whose
experimental results are discussed in Section 5.
2 Related Work
Our goal of automatically acquiring object-trouble
pairs from Web documents is perhaps best viewed
as a problem of semantic relation extraction. Re-
cently the Automatic Content Extraction (ACE)
program (Doddington et al, 2004) is a well-
known benchmark task concerned with the au-
tomatic recognition of semantic relations from
unstructured text. Typical target relations in-
clude ?Reaction? and ?Production? (Pantel and
Pennacchiootti, 2006), ?person-affiliation? and
?organization-location? (Zelenko et al, 2002),
?part-whole? (Berland and Charniak, 1999; Girju
et al, 2006) and temporal precedence relations be-
tween events (Chklovski and Pantel, 2004; Tori-
sawa, 2006). Our current task of acquiring ?object-
trouble? relations is new and object-trouble rela-
tions are inherently more abstract and indirect than
relations like ?person-affiliation? ? they crucially
depend on additional knowledge about whether
and how a given object?s use might be hampered
by a specific trouble.
Another line of research closely related to our
work is the recognition of semantic orientation and
sentiment analysis (Turney, 2002; Takamura et al,
2006; Kaji and Kitsuregawa, 2006). Clearly trou-
bles should be associated with a negative orien-
tation of an expression, but studies on the acqui-
sition of semantic orientation traditionally do not
bother with the context of evaluation. While re-
cent work on sentiment analysis has started to as-
sociate sentiment-related attribute-evaluation pairs
to objects (Kobayashi et al, 2007), these attributes
usually concern intrinsic properties of the objects,
such as a digital camera?s colors ? they do not
extend to sentiment-related factors external to the
object like ?traffic jams? for theme parks. The ac-
quisition method proposed in this work addresses
both these matters.
Finally, our task of acquiring trouble expres-
sions can be regarded as hyponymy acquisition,
where target expressions are hyponyms of the
word ?trouble?. Although we used the classical
lexico-syntactic patterns for hyponymy acquisition
(Hearst, 1992; Imasumi, 2001; Ando et al, 2003)
to reflect this intuition, our experiments show we
were unable to attain satisfactory performance us-
ing lexico-syntactic patterns alone. Thus, we also
use verb-noun dependencies as evidence in learn-
ing (Pantel and Ravichandran, 2004; Shinzato and
Torisawa, 2004). We treat the evidences uniformly
as elements in a feature vector given to a super-
vised learning method, which allowed us to ex-
tract a considerably larger number of trouble ex-
pressions than could be acquired by sparse lexico-
syntactic patterns alone, while still keeping decent
precision. What kind of hyponymy relations can
be acquired by noun-verb dependencies is still an
open question in NLP. In this work we show that
at least trouble expressions can successfully be ac-
quired based on noun-verb dependency informa-
tion alone.
3 Trouble Expressions and Features for
Their Acquisition
In section 1 we have characterized trouble expres-
sions as a kind of ?trouble? that occurs in the spe-
cific context of using some object, in other words:
186
1. hyponym ni nita hypernym
(hyponym similar to hypernym)
2. hyponym to yobareru hypernym
(hypernym called hyponym)
3. hyponym igai no hypernym
(hypernym other than hyponym)
4. hyponym no youna hypernym
(hypernym like hyponym)
5. hyponym to iu hypernym
(hypernym called hyponym)
6. hyponym nado(no|,) hypernym
(hypernym such as hyponym)
Table 1: Japanese lexico-syntactic patterns for hy-
ponymy relations
as hyponyms of ?trouble?. Hence one source of ev-
idence for acquisition are hyponymy relations with
?trouble? or its synonyms. Another characteriza-
tion of trouble expressions is to think of them as
obstacles in a broad sense: things that prevent cer-
tain actions from being undertaken properly. In
this sense traffic jams and sickness are troubles
since they prevent people from going places and
doing things. This assumption underlies a second
important class of evidences for learning.
More precisely, the evidence used for learning is
classified into three categories: (i) lexico-syntactic
patterns for hyponymy relations, (ii) dependency
relations between expressions and negated verbs,
and (iii) dependency relations between expres-
sions and non-negated verbs. The first two cat-
egories are assumed to contain positive evidence
of trouble expressions, while we assumed the third
to function mostly as negative evidence. Our ex-
periments show that (i) turns out to be less useful
than expected, while the combination of (ii) and
(iii) alone already gave quite reasonable precision
in acquiring trouble expressions. Each category of
evidence is described further below.
3.1 Lexico-syntactic patterns for hyponymy
Since trouble expressions are hyponyms of ?trou-
ble?, one obvious way of acquiring trouble expres-
sions is to use classical lexico-syntactic patterns
for hyponymy acquisition (Hearst, 1992). Table
1 lists some of the patterns proposed in studies
on hyponymy acquisition for Japanese (Ando et
al., 2003; Imasumi, 2001) that are utilized in this
work.
In actual acquisition, we instantiated the hy-
pernym positions in the patterns by Japanese
translations of ?trouble? and its synonyms,
namely toraburu (troubles), sainan (acci-
dents), saigai (disasters) and shougai (obsta-
cles or handicaps), and used the instantiated pat-
terns as evidence. Hereafter, we call these pat-
terns LSPHs (Lexico-Syntactic Patterns for Hy-
ponymy).
3.2 Dependency relations with Verbs
We expect expressions that frequently refer to
troubles to have a distinct dependency profile, by
which we mean a specific set of dependency rela-
tions with verbs (i.e. occurrences in specific argu-
ment positions). If T is a trouble expression, then
given a sufficiently large corpus one would expect
to find a reasonable number of instantiations of
patterns like the following:
? T kept X from doing Y .
? X didn?t enjoy Y because of T .
Similarly, ?X enjoyed T ? would present neg-
ative evidence for T being a trouble expression.
Rather than single out a set of particular depen-
dency relations suspected to be indicative of trou-
ble expressions, we let a supervised classifier learn
an appropriate weight for each feature in a large
vector of dependency relations. Two classes of de-
pendency relations proved to be especially benefi-
cial in determining trouble candidates in an unsu-
pervised manner, so we discuss them in more detail
below.
Dependency relations with negated verbs Fol-
lowing our characterization of troubles as things
that prevent specific actions from taking place, we
expect a good deal of trouble expressions to appear
in patterns like the following.
? X cannot go to Y because of T .
? X did not enjoy Y because of T .
The important points in the above are (i) the
negated verbs and (ii) the mention of T as the rea-
son for not verb-ing. The following are Japanese
translations of the above patterns. Here P denotes
postpositions (Japanese case markers), V stands
for verbs and the phrase ?because of? is translated
as the postposition de.
?
T de Y ni ikenai.
P P V (cannot go)
?
T de X ga tanoshikunakatta.
P P V (did not enjoy)
187
We refer to the following dependency relations
between expressions marked with the postposition
de and negated verbs in these patterns as DNVs
(Dependencies to Negated Verbs).
T de ? negated verb (1)
We allow any verb to be the negated verb, expect-
ing that inappropriate verbs will be less weighted
by machine learning techniques. For instance,
the dependency relations to negated verbs with an
originally negative orientation such as ?suffer? and
?die? will not work as positive examples for trou-
ble expressions.
Unfortunately, these patterns still present only
weak evidence for trouble expressions. The pre-
cision of the trouble expressions collected using
DNV patterns is extremely low ? around 6.5%.
This is due to the postposition de?s ambiguity ?
besides ?because of? relations it also functions as a
marker for location, time and instrument relations,
among others. As a result, non-trouble expressions
such as ?by car? (instrument) and ?in Tokyo? (lo-
cation) are marked by the postposition de as well.
We consider a second class of dependency rela-
tions, acting mostly as a counter to the noisy ex-
pressions introduced by the ambiguity of the post-
position de.
Dependency relations with non-negated verbs
The final type of evidence is formulated as the fol-
lowing dependency relation.
T de ? non-negated verb
We call this type of relation DAVs (Dependen-
cies to Affirmative Verbs). The use of these pat-
terns is motivated by the intuition that noisy ex-
pressions found with DNVs, such as expressions
about locations or instruments, will also frequently
appear with non-negated verbs. That is, if you ob-
serve ?cannot go to Y (by / because of) X? and X
is not a trouble expression, then you can expect to
find ?can go to Y (by / because of) X? as well.
Our initial expectation was that the DNV and
DAV evidences observed with the postposition de
alone would contain sufficient information to ob-
tain an accurate classifier, but this expectation was
not borne out by our early experiments. As it
turns out, using dependency relations to verbs in
all argument positions as features to the SVM re-
sulted roughly in a 10?15% increase in precision.
Therefore in our final experiments we let the DNV
and DAV evidence consist of dependencies with
four additional postpositions (ha, ga, wo and ni),
which are used to indicate topicalization, subject,
object and indirect object. We found that the SVM
was quite successful in learning a dependency pro-
file for trouble expressions based on this informa-
tion.
Nonetheless, the DNV/DAV patterns proved to
be useful besides as evidence for supervised learn-
ing, for instance in gathering sufficient trouble can-
didates and sample selection when preparing train-
ing data
2
.
4 Method
As mentioned, our method for finding troubles
in using some objects consists of three steps, de-
scribed in more detail below.
Step 1 Gather training data with a sufficient
amount of positive samples using an unsuper-
vised method to reduce the workload of man-
ual annotation.
Step 2 Collect expressions commonly perceived
as troubles by using the evidences described
in the previous section.
Step 3 Identify pairs of trouble expressions and
objects such that the trouble expressions rep-
resent an obstacle in using the objects.
4.1 Step 1: Gathering Training Data
We considered noun phrases observed with the
LSPH and DNV evidences as candidate trouble ex-
pressions. However, we still found only 7% of
the samples observed with these evidences to be
real troubles. Because of the diversity of our ev-
idences (dependencies with verbs) we need a rea-
sonable amount of positive samples in order to ob-
tain an accurate classifier. Without some sample
selection scheme, we would have to manually an-
notate about 8000 samples in order to obtain only
560 positive samples in the training data. For this
reason we used the following scoring function as
an unsupervised method for sample selection.
Score(e) =
f
LSPH
(e) + f
DNV
(e)
f
LSPH
(e) + f
DNV
(e) + f
DAV
(e)
(2)
Here, f
LSPH
(e), f
DNV
(e) and f
DAV
(e) are the fre-
quencies that expression e appears with the re-
spective evidences. Intuitively, this function gives
2
We will discuss yet another use of the DNV evidence in
step 2 of our acquisition method.
188
a large score to expressions that occur frequently
with the positive evidences for trouble expressions
(LSPHs and DNVs), or those that appear rarely
with the negative evidences (DAVs). In prepar-
ing training data we ranked all candidates accord-
ing to the above score, and annotated N elements
from the top and bottom of the ranking as train-
ing data. In our experiments, the top elements in-
cluded a reasonable number of positive samples
(25.8%) while there were almost none in the worst
elements.
4.2 Step 2: Finding Trouble Expressions
In this step our aim is to acquire expressions of-
ten associated with troubles. We use a super-
vised classifier, namely Support Vector Machines
(SVMs) (Vapnik, 1998) for distinguishing trou-
bles from non-troubles, based on the evidences de-
scribed above. Each dimension of the feature vec-
tor presented to the SVM corresponds to the obser-
vation of a particular evidence (i.e., these are bi-
nary features). We tried using frequencies instead
of binary feature values but could not find any sig-
nificant improvement in performance. After learn-
ing we sort the candidate trouble expressions ac-
cording to their distance to the hyperplane learned
by the SVM, and consider the top N expressions
in the sorted list as true trouble expressions.
4.3 Step 2: Identifying Object-Trouble Pairs
In this third stage we rank possible combinations
of objects and trouble expressions acquired in the
previous step according to their degree of associ-
ation and apply a filter using negated verbs to the
top pairs in the ranking. The final output of our
method is the top N pairs that survived the filter-
ing. We describe each step below.
Generating Object-Trouble Pairs To generate
and rank object-trouble pairs we use a variant of
pairwise mutual information that scores an object-
trouble pair ?e
o
, e
t
? based on the observed fre-
quency of the following pattern.
e
o
no e
t
P
(3)
The postposition no is a genitive case marker, and
the whole pattern can be translated as ?e
t
of / in
e
o
?. We assume that appearance of expression e
t
in this pattern refers to a trouble in using the object
e
o
.
More precisely, we generate all possible combi-
nations of trouble expression and objects and rank
them according to the following score.
I(e
o
, e
t
) =
f(?e
o
no e
t
?)
f(?e
o
?)f(?e
t
?)
(4)
where f(e) denotes an expression e?s frequency.
This score is large when the pattern ?e
o
no e
t
?
is observed more frequently than can be expected
from e
o
and e
t
?s individual frequencies. Frequency
data for all noun phrases was precomputed for the
whole Web corpus.
Filtering Object-Trouble Pairs The filtering in
the second step is based on the following assump-
tion.
Assumption If a trouble expression e
t
refers to a
trouble in using an object e
o
, there is a verb v
such that v frequently co-occurs with e
o
and
v has the following dependency relation with
e
t
.
e
t
de ? negated v
The intuition behind this assumption can be ex-
plained as follows. First, if e
o
denotes an object or
artifact then its frequently co-occurring verbs are
likely to be related to a use of e
o
. Second, if e
t
is a
trouble in using e
o
, there is some action associated
with e
o
that e
t
prevents or hinders, implying that e
t
should be observed with its negation. For instance,
if ?traffic jam? is a trouble in using an amusement
park, then we can expect the following pattern to
appear also in a corpus.
?
juutai de yuuenchi ni ikenai.
traffic jam P theme park P V (cannot go)
cannot go to a theme park because of a traffic jam
The verb ?to go? co-occurs often with the noun
?theme park? and the above pattern contains the
dependency relation ?traffic jam de? cannot go?.
Substituting v in the hypothesis for ?to go?, the as-
sumption becomes valid. Because of data sparse-
ness the above pattern may not actually appear in
the corpus, but even so the dependency relation
?traffic jam de ? cannot go? may be observed
with other facilities, and thus making the assump-
tion hold anyway.
As a final filtering procedure, we gathered K
verbs most frequently co-occurring with each ob-
ject and checked if the trouble expression in the
pair has dependency relations with the K verbs in
189
negated form and the postposition de. If none of
the K verbs has such a dependency with the trou-
ble expression, the pair is discarded. Otherwise, it
is produced as the final output of our method.
5 Experimental Results
5.1 Finding Trouble Expressions
We extracted noun phrases observed in LSPH,
DNV and DAV patterns from 6? 10
9
sentences in
about 10
8
crawled Japanese Web documents, and
used the LSPH and DNV data
3
as candidate trou-
ble expressions. After restricting the noun phrases
to those observed more than 10 times in the evi-
dences, we had 136,212 noun phrases. We denote
this set asD. Extracting 200 random samples from
D we found the ratio of troubles to non-troubles
was around 7% and thus expected to find about
10, 000 real trouble expressions in D.
4
Using the sample selection method described in
Section 4.2 we prepared 6,500 annotated samples
taken from D as training data. The top 3,500 sam-
ples included 912 positive samples and the worst
3,000 had just 9 positives, thereby confirming the
effectiveness of the scoring function for selecting
a reasonable amount of positive samples. Our final
training data thus contained 14% positives.
For the feature vectors we included dependen-
cies with all verbs occurring more than 30 times
in our Web corpus. Besides the LSPH, DNV and
DAV evidences discussed previously, we also in-
cluded 10 additional binary features indicating for
each of the five postpositions whether the expres-
sion was observed with DNV or DAV evidence at
all, and found that including this information im-
proved performance.
We trained a classifier with a polynomial kernel
of degree 1 on these evidences using the software
TinySVM
5
, and evaluated the results obtained by
the supervised acquisition method by asking three
human raters whether a randomly selected sample
expression denotes a kind of ?trouble? in general
situations. More specifically, we asked whether
the expression is a kind of toraburu (trou-
ble), sainan (accident), saigai (disaster) or
shougai (obstacle or handicap).
6
For various
3
We restricted noun phrases from the DNV data to those
found with the postposition de, as these are most likely to
refer to troubles.
4
Thus, in the experiments we evaluated the top 10,000
samples output by our method.
5
http://chasen.org/?taku/software/TinySVM/
6
Actually one of the raters is a co-author of this paper, but
 0
 20
 40
 60
 80
 100
 0  20  40  60  80  100
Pr
ec
isio
n (%
)
Number of Samples (%)
random
LSPH
Score
full
w/o LSPH
w/o DAV
w/o DNV
w/o sum DAV/DNV
Figure 1: Performance of trouble expression ac-
quisition (all 3 raters)
combinations of evidences (described below), we
presented 200 randomly sampled expressions from
the top 10,000 expressions ranked according to the
distance to the hyperplane learned by the SVM.
Samples of all the compared methods are merged
and shuffled before evaluation. The kappa statistic
for assessing the inter-rater agreement was 0.78,
indicating substantial agreement according to Lan-
dis and Koch, 1977.
7
We made no effort to remove
samples used in training from the experiment, and
found that the samples scored by the raters (1281
in total, after removal of duplicates) contained 67
training samples. The 200 samples from the ?full?
classifier contained 12 of these.
Fig. 1 shows the precision of the acquired trou-
ble expressions compared to the samples labeled
as troubles by all three raters. We sorted the sam-
ples according to their distance to the SVM hyper-
plane and plotted the precision of the top N sam-
ples. The best overall precision (85.5%) was ob-
tained by a classifier trained on the full combina-
tion of evidences (labeled ?full? in Fig. 1), main-
taining over 90% precision for the top 70% of the
200 samples.
The remaining results show the relative con-
tributions of the evidences. They were obtained
by retraining the ?full? classifier with a particu-
lar set of evidences removed, respectively LSPH
evidences (labeled ?w/o LSPH?), DNV evidences
(?w/o DNV?), DAV evidences (?w/o DAV?) and
the 10 features indicating the observation of
he had no knowledge of the experimental setting nor had seen
the acquired data prior to the experiment.
7
This kappa value was calculated over the sum total of
samples presented to the raters for scoring (duplicates re-
moved).
190
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  20  40  60  80  100
Pr
ec
isio
n (%
)
Number of Samples (%)
random
top 10% MI
top 50% MI
top 10% proposed
top 50% proposed
Figure 2: Performance of object-trouble pair ac-
quisition (3 raters)
DAV/DNV evidence per postposition (?w/o sum
DAV/DNV?).
As Fig. 1 shows, leaving out DNV and even
LSPH evidences did not affect performance as
much as we expected, while leaving out the DAV
dependencies gave more than 20% worse results.
Of further interest is the importance of the binary
features for DAV/DNV presence per postposition
(?w/o sum DAV/DNV?). The absence of these 10
binary features accounts for a 10% precision loss
compared to the full feature set (75%).
We also compared it with a baseline method us-
ing only lexico-syntactic patterns. We extracted
100 random noun phrases from the LSPH evidence
in D for evaluation (?LSPH? in Fig. 1). The pre-
cision for this method was 31%, confirming that
lexico-syntactic patterns for hyponymy constitute
fairly weak evidence for predicting trouble expres-
sions when used alone. ?Score? shows the preci-
sion of the top 100 samples output by our Score
function from section 4. Finally, ?random? (drawn
as a straight line) denotes 100 random samples
from D and roughly corresponds to our estimate
of 7% true positives.
5.2 Identifying Object-Trouble Pairs
For the second step, we assumed the top 10,000 ex-
pressions obtained by our best-scoring supervised
learning method (?full? in the previous experi-
ments) to be trouble expressions, and proceeded
to combine them with terms denoting artifacts or
facilities.
We randomly picked 2,500 words that ap-
peared as direct objects of the verbs kau (?to
buy?), tsukau (?to use?), tsukuru (?to make?),
taberu (?to eat?) and tanoshimu (?to enjoy?)
rank
/raters object trouble expressions
1/3 kousoku douro sakeyoi unten
(highway) (drunk driving)
7/3 kouseibushitsu ranyou
(antibiotics) (abuse)
8/3 suidousui suiatsu teika
(tap water) (drop in water pressure)
21/3 nouyaku zanryuubushitsu
(agrichemicals) (residue)
98/2 kikai gohandan
(machine) (judgement error)
136/3 zaisan souzoku funsou
(estate) (succession dispute)
Figure 3: Examples of acquired object-trouble
pairs
more than 500 times in our Web corpus, assuming
that this would yield a representative set of noun
phrases denoting objects or artifacts.
8
Combining
this set of objects with the acquired trouble expres-
sions gave a list of 61,873 object-trouble pairs (all
pairs ?e
o
, e
t
? with at least one occurrence of the
pattern ?e
o
no e
t
?). Of this list, 58,570 pairs sur-
vived the DNV filtering step and form the final out-
put of our method. For the DNV filtering, we used
the top 30 verbs most frequently co-occurring with
the object.
We again evaluated the resulting object-trouble
pairs by asking three human raters whether the pre-
sented pairs consist of an object and an expression
referring to an actual or potential trouble in using
the object. The kappa statistic was 0.60, indicating
moderate inter-rater agreement.
Fig. 2 shows the precision of the acquired pairs
when comparing with what are considered true
object-trouble relations by all three raters. Some
examples of the pairs obtained by our method are
listed in table 3 along with their ranking and the
number of raters who judged the pair to be correct.
The precision for our proposed method when
considering the top 10% of pairs ranked by the I
score and filtered by the method described in sec-
tion 4.3 is 71.5% (?top 10% proposed? in Fig. 2),
which is actually worse than the results obtained
without the final DNV filtering (?top 10% MI?,
74%). For the first half of all samples however, we
do observe some performance increase by the fil-
tering, though both methods appear to converge in
the second half of the graph. This tendency is mir-
rored closely when considering the results for the
top 50% of all pairs (respectively ?top 50% pro-
posed? and ?top 50% MI? in Fig. 2). The 15%
decrease in precision compared to top 10% results
8
We manually removed pronouns from this set.
191
indicates that performance drops gradually when
moving to the lower ranked pairs.
6 Concluding Remarks and Future Work
We have presented an automatic method for find-
ing potential troubles in using objects, mainly ar-
tifacts and facilities. Our method acquired 10,000
trouble expressions with 85.5% precision, and over
6000 pairs of objects and trouble expressions with
74% precision.
Currently, we are developing an Internet search
engine frontend that issues warnings about poten-
tial troubles related to search keywords. Although
we were able to acquire object-trouble pairs with
reasonable precision, we plan to make a large-scale
highly precise list of troubles by manually check-
ing the output of our method. We expect such a list
to lead to even more acurate object-trouble pair ac-
quisition.
References
Ando, M., S. Sekine, and S. Ishizaki. 2003. Automatic
extraction of hyponyms from newspaper using lexi-
cosyntactic patterns. In IPSJ SIG Technical Report
2003-NL-157, pages 77?82. in Japanese.
Berland, M. and E. Charniak. 1999. Finding parts in
very large corpora. In Proc. of ACL-1999, pages 57?
64.
Chklovski, T. and P. Pantel. 2004. Verbocean: Mining
the web for fine-grained semantic verb relations. In
Proc. of EMNLP-04.
Doddington, G., A. Mitchell, M. Przybocki,
L. Ramshaw, S. Strassel, and R. Weischedel.
2004. The Automatic Content Extraction (ACE)
Program?Tasks, Data, and Evaluation. Proceedings
of LREC 2004, pages 837?840.
Girju, R., A. Badulescu, and D. Moldvan. 2006. Au-
tomatic discovery of part-whole relations. Computa-
tional Linguistics, 32(1):83?135.
Hearst, M. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of COLING?92,
pages 539?545.
Imasumi, K. 2001. Automatic acqusition of hyponymy
relations from coordinated noun phrases and apposi-
tions. Master?s thesis, Kyushu Institute of Technol-
ogy.
Kaji, N. and M. Kitsuregawa. 2006. Automatic con-
struction of polarity-tagged corpus from html docu-
ments. In Proc. of COLING/ACL 2006, pages 452?
459. (poster session).
Kobayashi, N., K. Inui, and Y. Matsumoto. 2007. Ex-
tracting aspect-evaluation and aspect-of relations in
opinion mining. In Proc. of EMNLP-CoNLL 2007,
pages 1065?1074.
Pantel, P. and M. Pennacchiootti. 2006. Espresso:
Leveranging generic patterns for automatically
harvesting semantic relations. In Proc. of
COLING/ACL-06, pages 113?120.
Pantel, P. and D. Ravichandran. 2004. Automatically
labelling semantic classes. In Proc. of HLT/NAACL-
04, pages 321?328.
Shinzato, K. and K. Torisawa. 2004. Acquir-
ing hyponymy relations from web documents. In
HLT/NAACL-04, pages 73?80.
Takamura, H., T. Inui, and M. Okumura. 2006. Latent
variable models for semantic orientation of phrases.
In Proc. of EACL 2006, pages 201?208.
Torisawa, K. 2006. Acquiring inference rules with
temporal constraints by using japanese coordinated
sentences and noun-verb co-occurrences. In Moore,
R.C., J.A. Bilmes, J. Chu-Carroll, and M. Sanderson,
editors, HLT-NAACL. The Association for Computa-
tional Linguistics.
Turney, P. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classification
of reviews. In Proc. of ACL?02, pages 417?424.
Vapnik, Vladimir N. 1998. Statistical Learning The-
ory. Wiley-Interscience.
Zelenko, D., C. Aone, and A. Richardella. 2002. Ker-
nel methods for relation extraction. In EMNLP ?02:
Proceedings of the ACL-02 conference on Empirical
methods in natural language processing, pages 71?
78, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
192
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 315?324, Prague, June 2007. c?2007 Association for Computational Linguistics
A New Perceptron Algorithm for
Sequence Labeling with Non-local Features
Jun?ichi Kazama and Kentaro Torisawa
Japan Advanced Institute of Science and Technology (JAIST)
Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan
{kazama, torisawa}@jaist.ac.jp
Abstract
We cannot use non-local features with cur-
rent major methods of sequence labeling
such as CRFs due to concerns about com-
plexity. We propose a new perceptron algo-
rithm that can use non-local features. Our
algorithm allows the use of all types of
non-local features whose values are deter-
mined from the sequence and the labels. The
weights of local and non-local features are
learned together in the training process with
guaranteed convergence. We present experi-
mental results from the CoNLL 2003 named
entity recognition (NER) task to demon-
strate the performance of the proposed algo-
rithm.
1 Introduction
Many NLP tasks such as POS tagging and named
entity recognition have recently been solved as se-
quence labeling. Discriminative methods such as
Conditional Random Fields (CRFs) (Lafferty et al,
2001), Semi-Markov Random Fields (Sarawagi and
Cohen, 2004), and perceptrons (Collins, 2002a)
have been popular approaches for sequence label-
ing because of their excellent performance, which is
mainly due to their ability to incorporate many kinds
of overlapping and non-independent features.
However, the common limitation of these meth-
ods is that the features are limited to ?local? fea-
tures, which only depend on a very small number
of labels (usually two: the previous and the current).
Although this limitation makes training and infer-
ence tractable, it also excludes the use of possibly
useful ?non-local? features that are accessible after
all labels are determined. For example, non-local
features such as ?same phrases in a document do not
have different entity classes? were shown to be use-
ful in named entity recognition (Sutton and McCal-
lum, 2004; Bunescu and Mooney, 2004; Finkel et
al., 2005; Krishnan and Manning, 2006).
We propose a new perceptron algorithm in this pa-
per that can use non-local features along with lo-
cal features. Although several methods have al-
ready been proposed to incorporate non-local fea-
tures (Sutton and McCallum, 2004; Bunescu and
Mooney, 2004; Finkel et al, 2005; Roth and Yih,
2005; Krishnan and Manning, 2006; Nakagawa and
Matsumoto, 2006), these present a problem that
the types of non-local features are somewhat con-
strained. For example, Finkel et al (2005) enabled
the use of non-local features by using Gibbs sam-
pling. However, it is unclear how to apply their
method of determining the parameters of a non-local
model to other types of non-local features, which
they did not used. Roth and Yih (2005) enabled
the use of hard constraints on labels by using inte-
ger linear programming. However, this is equivalent
to only allowing non-local features whose weights
are fixed to negative infinity. Krishnan and Manning
(2006) divided the model into two CRFs, where the
second model uses the output of the first as a kind of
non-local information. However, it is not possible
to use non-local features that depend on the labels
of the very candidate to be scored. Nakagawa and
Matsumoto (2006) used a Bolzmann distribution to
model the correlation of the POS of words having
the same lexical form in a document. However, their
method can only be applied when there are conve-
nient links such as the same lexical form.
Since non-local features have not yet been exten-
sively investigated, it is possible for us to find new
useful non-local features. Therefore, our objective
in this study was to establish a framework, where all
315
types of non-local features are allowed.
With non-local features, we cannot use efficient
procedures such as forward-backward procedures
and the Viterbi algorithm that are required in train-
ing CRFs (Lafferty et al, 2001) and perceptrons
(Collins, 2002a). Recently, several methods (Collins
and Roark, 2004; Daume? III and Marcu, 2005; Mc-
Donald and Pereira, 2006) have been proposed with
similar motivation to ours. These methods allevi-
ate this problem by using some approximation in
perceptron-type learning.
In this paper, we follow this line of research and
try to solve the problem by extending Collins? per-
ceptron algorithm (Collins, 2002a). We exploited
the not-so-familiar fact that we can design a per-
ceptron algorithm with guaranteed convergence if
we can find at least one wrong labeling candidate
even if we cannot perform exact inference. We first
ran the A* search only using local features to gen-
erate n-best candidates (this can be efficiently per-
formed), and then we only calculated the true score
with non-local features for these candidates to find
a wrong labeling candidate. The second key idea
was to update the weights of local features during
training if this was necessary to generate sufficiently
good candidates. The proposed algorithm combined
these ideas to achieve guaranteed convergence and
effective learning with non-local features.
The remainder of the paper is organized as fol-
lows. Section 2 introduces the Collins? perceptron
algorithm. Although this algorithm is the starting
point for our algorithm, its baseline performance is
not outstanding. Therefore, we present a margin ex-
tension to the Collins? perceptron in Section 3. This
margin perceptron became the direct basis of our al-
gorithm. We then explain our algorithm for non-
local features in Section 4. We report the experi-
mental results using the CoNLL 2003 shared task
dataset in Section 6.
2 Perceptron Algorithm for Sequence
Labeling
Collins (2002a) proposed an extension of the per-
ceptron algorithm (Rosenblatt, 1958) to sequence
labeling. Our aim in sequence labeling is to as-
sign label yi ? Y to each word xi ? X in a
sequence. We denote sequence x1, . . . , xT as x
and the corresponding labels as y. We assume
weight vector ? ? Rd and feature mapping ?
that maps each (x,y) to feature vector ?(x,y) =
(?1(x,y), ? ? ? ,?d(x,y)) ? Rd. The model deter-
mines the labels by:
y? = argmaxy?Y|x|?(x,y) ??,
where ? denotes the inner product. The aim
of the learning algorithm is to obtain an ap-
propriate weight vector, ?, given training set
{(x1,y?1), ? ? ? , (xL,y?L)}.
The learning algorithm, which is illustrated in
Collins (2002a), proceeds as follows. The weight
vector is initialized to zero. The algorithm passes
over the training examples, and each sequence is de-
coded using the current weights. If y? is not the cor-
rect answer y?, the weights are updated according to
the following rule.
?new = ?+ ?(x,y?)? ?(x,y?).
This algorithm is proved to converge (i.e., there are
no more updates) in the separable case (Collins,
2002a).1 That is, if there exist weight vectorU (with
||U || = 1), ? (> 0), and R (> 0) that satisfy:
?i,?y ? Y |xi| ?(xi,yi?) ?U ? ?(xi,y) ?U ? ?,
?i,?y ? Y |xi| ||?(xi,yi?)? ?(xi,y)|| ? R,
the number of updates is at most R2/?2.
The perceptron algorithm only requires one can-
didate y? for each sequence xi, unlike the training of
CRFs where all possible candidates need to be con-
sidered. This inherent property is the key to train-
ing with non-local features. However, note that the
tractability of learning and inference relies on how
efficiently y? can be found. In practice, we can find
y? efficiently using a Viterbi-type algorithm only
when the features are all local, i.e., ?s(x,y) can be
written as the sum of (two label) local features ?s as
?s(x,y) =
?T
i ?s(x, yi?1, yi). This locality con-
straint is also required to make the training of CRFs
tractable (Lafferty et al, 2001).
One problem with the perceptron algorithm de-
scribed so far is that it offers no treatment for over-
fitting. Thus, Collins (2002a) also proposed an av-
eraged perceptron, where the final weight vector is
1Collins (2002a) also provided proof that guaranteed ?good?
learning for the non-separable case. However, we have only
considered the separable case throughout the paper.
316
Algorithm 3.1: Perceptron with margin for
sequence labeling (parameters: C)
? ? 0
until no more updates do
for i ? 1 to L do
8
>
>
>
<
>
>
>
:
y? = argmaxy?(xi,y) ? ?
y?? = 2nd-besty?(xi,y) ? ?
if y? ?= y?i then
? = ? + ?(xi,y?i )? ?(xi,y?)
else if ?(xi,y?i ) ? ? ? ?(xi,y??) ? ? ? C then
? = ? + ?(xi,y?i )? ?(xi,y??)
the average of all weight vectors during training.
Howerver, we found in our experiments that the av-
eraged perceptron performed poorly in our setting.
We therefore tried to make the perceptron algorithm
more robust to overfitting. We will describe our ex-
tension to the perceptron algorithm in the next sec-
tion.
3 Margin Perceptron Algorithm for
Sequence Labeling
We extended a perceptron with a margin (Krauth and
Me?zard, 1987) to sequence labeling in this study, as
Collins (2002a) extended the perceptron algorithm
to sequence labeling.
In the case of sequence labeling, the margin is de-
fined as:
?(?) = minxi
miny ?=y?i
?(xi,yi?) ??? ?(xi,y) ??
||?||
Assuming that the best candidate, y?, equals the cor-
rect answer, y?, the margin can be re-written as:
= minxi
?(xi,yi?) ??? ?(xi,y??) ??
||?|| ,
where y?? = 2nd-besty?(xi,y) ??. Using this rela-
tion, the resulting algorithm becomes Algorithm 3.1.
The algorithm tries to enlarge the margin as much as
possible, as well as make the best scoring candidate
equal the correct answer.
Constant C in Algorithm 3.1 is a tunable param-
eter, which controls the trade-off between the mar-
gin and convergence time. Based on the proofs
in Collins (2002a) and Li et al (2002), we can
prove that the algorithm converges within (2C +
R2)/?2 updates and that ?(?) ? ?C/(2C + R2) =
(?/2)(1 ? (R2/(2C + R2))) after training. As can
be seen, the margin approaches at least half of true
margin ? (at the cost of infinite training time), as
C ? ?.
Note that if the features are all local, the second-
best candidate (generally n-best candidates) can also
be found efficiently by using an A* search that uses
the best scores calculated during a Viterbi search as
the heuristic estimation (Soong and Huang, 1991).
There are other methods for improving robustness
by making margin larger for the structural output
problem. Such methods include ALMA (Gentile,
2001) used in (Daume? III and Marcu, 2005)2, MIRA
(Crammer et al, 2006) used in (McDonald et al,
2005), and Max-Margin Markov Networks (Taskar
et al, 2003). However, to the best of our knowledge,
there has been no prior work that has applied a per-
ceptron with a margin (Krauth and Me?zard, 1987)
to structured output.3 Our method described in this
section is one of the easiest to implement, while
guaranteeing a large margin. We found in the experi-
ments that our method outperformed the Collins? av-
eraged perceptron by a large margin.
4 Algorithm
4.1 Definition and Basic Idea
Having described the basic perceptron algorithms,
we will know explain our algorithm that learns the
weights of local and non-local features in a unified
way.
Assume that we have local features and non-
local features. We use the superscript, l, for
local features as ?li(x,y) and g for non-local
features as ?gi (x,y). Then, feature mapping is
written as ?a(x,y) = ?l(x,y) + ?g(x,y) =
(?l1(x,y), ? ? ? ,?ln(x,y),?
g
n+1(x,y), ? ? ? ,?
g
d(x,y)).
Here, we define:
?l(x,y) = (?l1(x,y), ? ? ? ,?ln(x,y), 0, ? ? ? , 0)
?g(x,y) = (0, ? ? ? , 0,?gn+1(x,y), ? ? ? ,?
g
d(x,y))
Ideally, we want to determine the labels using the
whole feature set as:
y? = argmaxy?Y|x|?
a(x,y) ??.
2(Daume? III and Marcu, 2005) also presents the method us-
ing the averaged perceptron (Collins, 2002a)
3For re-ranking problems, Shen and Joshi (2004) proposed
a perceptron algorithm that also uses margins. The difference is
that our algorithm trains the sequence labeler itself and is much
simpler because it only aims at labeling.
317
Algorithm 4.1: Candidate algorithm (parameters:
n, C)
? ? 0
until no more updates do
for i ? 1 to L do
8
>
>
>
>
>
<
>
>
>
>
>
:
{yn} = n-besty?l(xi,y) ? ?
y? = argmaxy?{yn}?a(xi,y) ? ?
y?? = 2nd-besty?{yn}?a(xi,y) ? ?
if y? ?= yi?
& ?a(xi,y?i ) ? ? ? ?a(xi,y?) ? ? ? C then
? = ? + ?a(xi,y?i )? ?a(xi,y?)
else if ?a(xi,y?i ) ?? ? ?a(xi,y??) ? ? ? C then
? = ? + ?a(xi,y?i )? ?a(xi,y??)
However, if there are non-local features, it is impos-
sible to find the highest scoring candidate efficiently,
since we cannot use the Viterbi algorithm. Thus,
we cannot use the perceptron algorithms described
in the previous sections. The training of CRFs is
also intractable for the same reason.
To deal with this problem, we first relaxed our ob-
jective. The modified objective was to find a good
model from those with the form:
{yn} = n-besty?l(x,y) ??
y? = argmaxy?{yn}?a(x,y) ??, (1)
That is, we first generate n-best candidates {yn}
under the local model, ?l(x,y) ? ?. This can be
done efficiently using the A* algorithm. We then
find the best scoring candidate under the total model,
?a(x,y) ??, only from these n-best candidates. If n
is moderately small, this can also be done in a prac-
tical amount of time.
This resembles the re-ranking approach (Collins
and Duffy, 2002; Collins, 2002b). However, unlike
the re-ranking approach, the local model, ?l(x,y) ?
?, and the total model, ?a(x,y) ??, correlate since
they share a part of the vector and are trained at
the same time in our algorithm. The re-ranking ap-
proach has the disadvantage that it is necessary to
use different training corpora for the first model and
for the second, or to use cross validation type train-
ing, to make the training for the second meaning-
ful. This reduces the effective size of training data
or increases training time substantially. On the other
hand, our algorithm has no such disadvantage.
However, we are no longer able to find the high-
est scoring candidate under ?a(x,y) ? ? exactly
with this approach. We cannot thus use the percep-
tron algorithms directly. However, by examining the
Algorithm 4.2: Perceptron with local and
non-local features (parameters: n, Ca, Cl)
? ? 0
until no more updates do
for i ? 1 to L do
8
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
:
{yn} = n-besty?l(xi,y) ? ?
y? = argmaxy?{yn}?a(xi,y) ? ?
y?? = 2nd-besty?{yn}?a(xi,y) ? ?
if y? ?= y?i
& ?a(xi,y?i ) ? ? ? ?a(xi,y?) ? ? ? Ca then
? = ? + ?a(xi,y?i )? ?a(xi,y?) (A)
else if ?a(xi,y?i ) ?? ? ?a(xi,y??) ? ? ? Ca then
? = ? + ?a(xi,y?i )? ?a(xi,y??) (A)
else
(B)
8
>
<
>
:
if y1 ?= yi? then (y1 represents the best in {yn})
? = ? + ?l(xi,y?i )? ?l(xi,y1)
else if ?l(xi,y?i ) ? ? ? ?l(xi,y2) ? ? ? Cl then
? = ? + ?l(xi,y?i )? ?l(xi,y2)
proofs in Collins (2002a), we can see that the essen-
tial condition for convergence is that the weights are
always updated using some y (?= y?) that satisfies:
?(xi,y?i ) ??? ?(xi,y) ?? ? 0
(? C in the case of a perceptron with a margin). (2)
That is, y does not necessarily need to be the exact
best candidate or the exact second-best candidate.
The algorithm also converges in a finite number of
iterations even with Eq. (1) as long as Eq. (2) is
satisfied.
4.2 Candidate Algorithm
The algorithm we came up with first based on the
above idea, is Algorithm 4.1. We first find the n-
best candidates using the local model, ?l(x,y) ? ?.
At this point, we can determine the value of the non-
local features, ?g(x,y), to form the whole feature
vector, ?a(x,y), for the n-best candidates. Next,
we re-score and sort them using the total model,
?a(x,y) ? ?, to find a candidate that violates the
margin condition. We call this algorithm the ?can-
didate algorithm?. After the training has finished,
?a(xi,y?i ) ? ? ? ?a(xi,y) ? ? > C is guaran-
teed for all (xi,y) where y ? {yn},y ?= y?.
At first glance, this seems sufficient condition for
good models. However, this is not true because if
y? ?? {yn}, the inference defined by Eq. (1) is not
guaranteed to find the correct answer, y?. In fact,
this algorithm does not work well with non-local
features as we found in the experiments.
318
4.3 Final Algorithm
Our idea for improving the above algorithm is that
the local model,?l(x,y)??, must at least be so good
that y? ? {yn}. To achieve this, we added a modi-
fication term that was intended to improve the local
model when the local model was not good enough
even when the total model was good enough.
The final algorithm resulted in Algorithm 4.2. As
can be seen, the part marked (B) has been added. We
call this algorithm the ?proposed algorithm?. Note
that the algorithm prioritizes the update of the to-
tal model, (A), over that of the local model, (B), al-
though the opposite is also possible. Also note that
the update of the local model in (B) is ?aggressive?
since it updates the weights until the best candidate
output by the local model becomes the correct an-
swer and satisfies the margin condition. A ?conser-
vative? updating, where we cease the update when
the n-best candidates contain the correct answer, is
also possible from our idea above. We made these
choices since they worked better than the other al-
ternatives.
The tunable parameters are the local margin pa-
rameter, C l, the total margin parameter, Ca, and n
for the n-best search. We used C = C l = Ca in this
study to reduce the search space.
We can prove that the algorithm in Algorithm 4.2
also converges in a finite number of iterations. It
converges within (2C + R2)/?2 updates, assuming
that there exist weight vector U l (with ||U l|| = 1
and U li = 0 (n+1 ? i ? d)), ? (> 0), and R (> 0)
that satisfy:
?i,?y ? Y |xi| ?l(xi,yi?)?U l??l(xi,y)?U l ? ?,
?i,?y ? Y |xi| ||?a(xi,yi?)? ?a(xi,y)|| ? R.
In addition, we can prove that ??(?) ? ?C/(2C +
R2) for the margin after convergence, where ??(?)
is defined as:
minxi
min
y?{yn},?=y?i
?a(xi,yi?) ??? ?a(xi,y) ??
||?||
See Appendix A for the proofs.
We also incorporated the idea behind Bayes point
machines (BPMs) (Herbrich and Graepel, 2000) to
improve the robustness of our method further. BPMs
try to cancel out overfitting caused by the order of
examples, by training several models by shuffling
the training examples.4 However, it is very time
consuming to run the complete training process sev-
eral times. We thus ran the training in only one pass
over the shuffled examples several times, and used
the averaged output weight vectors as a new initial
weight vector, because we thought that the early part
of training would be more seriously affected by the
order of examples. We call this ?BPM initializa-
tion?. 5
5 Named Entity Recognition and
Non-Local Features
We evaluated the performance of the proposed algo-
rithm using the named entity recognition task. We
adopted IOB (IOB2) labeling (Ramshaw and Mar-
cus, 1995), where the first word of an entity of class
?C? is labeled ?B-C?, the words in the entity are la-
beled ?I-C?, and other words are labeled ?O?.
We used non-local features based on Finkel et al
(2005). These features are based on observations
such as ?same phrases in a document tend to have
the same entity class? (phrase consistency) and ?a
sub-phrase of a phrase tends to have the same entity
class as the phrase? (sub-phrase consistency). We
also implemented the ?majority? version of these
features as used in Krishnan and Manning (2006).
In addition, we used non-local features, which are
based on the observation that ?entities tend to have
the same entity class if they are in the same con-
junctive or disjunctive expression? as in ?? ? ? in U.S.,
EU, and Japan? (conjunction consistency). This type
of non-local feature was not used by Finkel et al
(2005) or Krishnan and Manning (2006).
6 Experiments
6.1 Data and Setting
We used the English dataset of the CoNLL 2003
named entity shared task (Tjong et al, 2003) for
the experiments. It is a corpus of English newspa-
per articles, where four entity classes, PER, LOC,
ORG, and MISC are annotated. It consists of train-
ing, development, and testing sets (14,987, 3,466,
4The results for the perceptron algorithms generally depend
on the order of the training examples.
5Note that we can prove that the perceptron algorithms con-
verge even though the weight vector is not initialized as ? = 0.
319
and 3,684 sentences, respectively). Automatically
assigned POS tags and chunk tags are also provided.
The CoNLL 2003 dataset contains document bound-
ary markers. We concatenated the sentences in the
same document according to these markers.6 This
generated 964 documents for the training set, 216
documents for the development set, and 231 docu-
ments for the testing set. The documents generated
as above become the sequence, x, in the learning
algorithms.
We first evaluated the baseline performance of
a CRF model, the Collins? perceptron, and the
Collins? averaged perceptron, as well as the margin
perceptron, with only local features. We next eval-
uated the performance of our perceptron algorithm
proposed for non-local features.
We used the local features summarized in Table
1, which are similar to those used in other studies
on named entity recognition. We omitted features
whose surface part listed in Table 1 occurred less
than twice in the training corpus.
We used CRF++ (ver. 0.44)7 as the basis of our
implementation. We implemented scaling, which
is similar to that for HMMs (see such as (Rabiner,
1989)), in the forward-backward phase of CRF train-
ing to deal with very long sequences due to sentence
concatenation.8
We used Gaussian regularization (Chen and
Rosenfeld, 2000) for CRF training to avoid overfit-
ting. The parameter of the Gaussian, ?2, was tuned
using the development set. We also tuned the margin
parameter, C, for the margin perceptron algorithm.9
The convergence of CRF training was determined by
checking the log-likelihood of the model. The con-
vergence of perceptron algorithms was determined
by checking the per-word labeling error, since the
6We used sentence concatenation even when only using lo-
cal features, since we found it does not degrade accuracy (rather
we observed a slight increase).
7http://chasen.org/?taku/software/CRF++
8We also replaced the optimization module in the original
package with that used in the Amis maximum entropy estima-
tor (http://www-tsujii.is.s.u-tokyo.ac.jp/amis) since we encoun-
tered problems with the provided module in some cases.
9For the Gaussian parameter, we tested {13, 25, 50, 100,
200, 400, 800} (the accuracy did not change drastically among
these values and it seems that there is no accuracy hump even
if we use smaller values). We tested {500, 1000, 1414, 2000,
2828, 4000, 5657, 8000, 11313, 16000, 32000} for the margin
parameters.
Table 1: Local features used. The value of a node
feature is determined from the current label, y0, and
a surface feature determined only from x. The value
of an edge feature is determined by the previous la-
bel, y?1, the current label, y0, and a surface feature.
Used surface features are the word (w), the down-
cased word (wl), the POS tag (pos), the chunk tag
(chk), the prefix of the word of length n (pn), the
suffix (sn), the word form features: 2d - cp (these are
based on (Bikel et al, 1999)), and the gazetteer fea-
tures: go for ORG, gp for PER, and gm for MISC.
These represent the (longest) match with an entry in
the gazetteer by using IOB2 tags.
Node features:
{??, x?2, x?1, x0, x+1, x+2} ? y0
x =, w, wl, pos, chk, p1, p2, p3, p4, s1, s2, s3,
s4, 2d, 4d, d&a, d&-, d&/, d&,, d&., n, ic, ac,
l, cp, go, gp, gm
Edge features:
{??, x?2, x?1, x0, x+1, x+2} ? y?1 ? y0
x =, w, wl, pos, chk, p1, p2, p3, p4, s1, s2, s3,
s4, 2d, 4d, d&a, d&-, d&/, d&,, d&., n, ic, ac,
l, cp, go, gp, gm
Bigram node features:
{x?2x?1, x?1x0, x0x+1} ? y0
x = wl, pos, chk, go, gp, gm
Bigram edge features:
{x?2x?1, x?1x0, x0x+1} ? y?1 ? y0
x = wl, pos, chk, go, gp, gm
number of updates was not zero even after a large
number of iterations in practice. We stopped train-
ing when the relative change in these values became
less than a pre-defined threshold (0.0001) for at least
three iterations.
We used n = 20 (n of the n-best) for training
since we could not use too a large n because it would
have slowed down training. However, we could ex-
amine a larger n during testing, since the testing time
did not dominate the time for the experiment. We
found an interesting property for n in our prelimi-
nary experiment. We found that an even larger n in
testing (written as n?) achieved higher accuracy, al-
though it is natural to assume that the same n that
was used in training would also be appropriate for
testing. We thus used n? = 100 to evaluate perfor-
mance during parameter tuning. After finding the
best C with n? = 100, we varied n? to investigate its
320
Table 2: Summary of performance (F1).
Method dev test C (or ?2)
local features
CRF 91.10 86.26 100
Perceptron 89.01 84.03 -
Averaged perceptron 89.32 84.08 -
Margin perceptron 90.98 85.64 11313
+ non-local features
Candidate (n? = 100) 90.71 84.90 4000
Proposed (n? = 100) 91.95 86.30 5657
Table 3: Effect of n?.
Method dev test C
Proposed (n? = 20) 91.76 86.19 5657
Proposed (n? = 100) 91.95 86.30 5657
Proposed (n? = 400) 92.13 86.39 5657
Proposed (n? = 800) 92.09 86.39 5657
Proposed (n? = 1600) 92.13 86.46 5657
Proposed (n? = 6400) 92.19 86.38 5657
effects further.
6.2 Results
Table 2 compares the results. CRF outperformed
the perceptron by a large margin. Although the av-
eraged perceptron outperformed the perceptron, the
improvement was slight. However, the margin per-
ceptron greatly outperformed compared to the aver-
aged perceptron. Yet, CRF still had the best baseline
performance with only local features.
The proposed algorithm with non-local features
improved the performance on the test set by 0.66
points over that of the margin perceptron without
non-local features. The row ?Candidate? refers to
the candidate algorithm (Algorithm 4.1). From the
results for the candidate algorithm, we can see that
the modification part, (B), in Algorithm 4.2 was es-
sential to make learning with non-local features ef-
fective.
We next examined the effect of n?. As can be
seen from Table 3, an n? larger than that for train-
ing yields higher performance. The highest perfor-
mance with the proposed algorithm was achieved
when n? = 6400, where the improvement due to
non-local features became 0.74 points.
The performance of the related work (Finkel et
al., 2005; Krishnan and Manning, 2006) is listed in
Table 4. We can see that the final performance of our
algorithm was worse than that of the related work.
We changed the experimental setting slightly
to investigate our algorithm further. Instead of
Table 4: The performance of the related work.
Method dev test
Finkel et al, 2005 (Finkel et al, 2005)
baseline CRF - 85.51
+ non-local features - 86.86
Krishnan and Manning, 2006 (Krishnan and Manning, 2006)
baseline CRF - 85.29
+ non-local features - 87.24
Table 5: Summary of performance with POS/chunk
tags by TagChunk.
Method dev test C (or ?2)
local features
CRF 91.39 86.30 200
Perceptron 89.36 84.35 -
Averaged perceptron 89.76 84.50 -
Margin perceptron 91.06 86.24 32000
+ non-local features
Proposed (n? = 100) 92.23 87.04 5657
Proposed (n? = 6400) 92.54 87.17 5657
the POS/chunk tags provided in the CoNLL 2003
dataset, we used the tags assigned by TagChunk
(Daume? III and Marcu, 2005)10 with the intention
of using more accurate tags. The results with this
setting are summarized in Table 5. Performance was
better than that in the previous experiment for all al-
gorithms. We think this was due to the quality of
the POS/chunk tags. It is interesting that the ef-
fect of non-local features rose to 0.93 points with
n? = 6400, even though the baseline performance
was also improved. The resulting performance of
the proposed algorithm with non-local features is
higher than that of Finkel et al (2005) and compara-
ble with that of Krishnan and Manning (2006). This
comparison, of course, is not fair because the setting
was different. However, we think the results demon-
strate a potential of our new algorithm.
The effect of BPM initialization was also exam-
ined. The number of BPM runs was 10 in this
experiment. The performance of the proposed al-
gorithm dropped from 91.95/86.30 to 91.89/86.03
without BPM initialization as expected in the set-
ting of the experiment of Table 2. The perfor-
mance of the margin perceptron, on the other hand,
changed from 90.98/85.64 to 90.98/85.90 without
BPM initialization. This result was unexpected from
the result of our preliminary experiment. However,
the performance was changed from 91.06/86.24 to
10http://www.cs.utah.edu/?hal/TagChunk/
321
Table 6: Comparison with re-ranking approach.
Method dev test C
local features
Margin Perceptron 91.06 86.24 32000
+ non-local features
Re-ranking 1 (n? = 100) 91.62 86.57 4000
Re-ranking 1 (n? = 80) 91.71 86.58 4000
Re-ranking 2 (n? = 100) 92.08 86.86 16000
Re-ranking 2 (n? = 800) 92.26 86.95 16000
Proposed (n? = 100) 92.23 87.04 5657
Proposed (n? = 6400) 92.54 87.17 5657
Table 7: Comparison of training time (C = 5657).
Method dev test time (sec.)
local features
Margin Perceptron 91.04 86.28 15,977
+ non-local features
Re-ranking 1 (n? = 100) 91.48 86.53 86,742
Re-ranking 2 (n? = 100) 92.02 86.85 112,138
Proposed (n? = 100) 92.23 87.04 28,880
91.17/86.08 (i.e., dropped for the evaluation set as
expected), in the setting of the experiment of Table
5. Since the effect of BPM initialization is not con-
clusive only from these results, we need more exper-
iments on this.
6.3 Comparison with re-ranking approach
Finally, we compared our algorithm with the re-
ranking approach (Collins and Duffy, 2002; Collins,
2002b), where we first generate the n-best candi-
dates using a model with only local features (the
first model) and then re-rank the candidates using
a model with non-local features (the second model).
We implemented two re-ranking models, ?re-
ranking 1? and ?re-ranking 2?. These models dif-
fer in how to incorporate the local information in the
second model. ?re-ranking 1? uses the score of the
first model as a feature in addition to the non-local
features as in Collins (2002b). ?re-ranking 2? uses
the same local features as the first model11 in addi-
tion to the non-local features. The first models were
trained using the margin perceptron algorithm in Al-
gorithm 3.1. The second models were trained using
the algorithm, which is obtained by replacing {yn}
with the n-best candidates by the first model. The
first model used to generate n-best candidates for the
development set and the test set was trained using
the whole training data. However, CRFs or percep-
trons generally have nearly zero error on the train-
ing data, although the first model should mis-label
11The weights were re-trained for the second model.
to some extent to make the training of the second
model meaningful. To avoid this problem, we adopt
cross-validation training as used in Collins (2002b).
We split the training data into 5 sets. We then trained
five first models using 4/5 of the data, each of which
was used to generate n-best candidates for the re-
maining 1/5 of the data.
As in the previous experiments, we tuned C using
the development set with n? = 100 and then tested
other values for n?. Table 6 shows the results. As can
be seen, re-ranking models were outperformed by
our proposed algorithm, although they also outper-
formed the margin perceptron with only local fea-
tures (?re-ranking 2? seems better than ?re-ranking
1?). Table 7 shows the training time of each algo-
rithm.12 Our algorithm is much faster than the re-
ranking approach that uses cross-validation training,
while achieving the same or higher level of perfor-
mance.
7 Discussion
As we mentioned, there are some algorithms simi-
lar to ours (Collins and Roark, 2004; Daume? III and
Marcu, 2005; McDonald and Pereira, 2006; Liang
et al, 2006). The differences of our algorithm from
these algorithms are as follows.
Daume? III and Marcu (2005) presented the
method called LaSO (Learning as Search Optimiza-
tion), in which intractable exact inference is approx-
imated by optimizing the behavior of the search pro-
cess. The method can access non-local features
at each search point, if their values can be deter-
mined from the search decisions already made. They
provided robust training algorithms with guaranteed
convergence for this framework. However, a differ-
ence is that our method can use non-local features
whose value depends on all labels throughout train-
ing, and it is unclear whether the features whose val-
ues can only be determined at the end of the search
(e.g., majority features) can be learned effectively
with such an incremental manner of LaSO.
The algorithm proposed by McDonald and
Pereira (2006) is also similar to ours. Their tar-
get was non-projective dependency parsing, where
exact inference is intractable. Instead of using
12Training time was measured on a machine with 2.33 GHz
QuadCore Intel Xeons and 8 GB of memory. C was fixed to
5657.
322
n-best/re-scoring approach as ours, their method
modifies the single best projective parse, which
can be found efficiently, to find a candidate with
higher score under non-local features. Liang et al
(2006) used n candidates of a beam search in the
Collins? perceptron algorithm for machine transla-
tion. Collins and Roark (2004) proposed an approxi-
mate incremental method for parsing. Their method
can be used for sequence labeling as well. These
studies, however, did not explain the validity of their
updating methods in terms of convergence.
To achieve robust training, Daume? III and Marcu
(2005) employed the averaged perceptron (Collins,
2002a) and ALMA (Gentile, 2001). Collins and
Roark (2004) used the averaged perceptron (Collins,
2002a). McDonald and Pereira (2006) used MIRA
(Crammer et al, 2006). On the other hand, we em-
ployed the margin perceptron (Krauth and Me?zard,
1987), extending it to sequence labeling. We demon-
strated that this greatly improved robustness.
With regard to the local update, (B), in Algo-
rithm 4.2, ?early updates? (Collins and Roark, 2004)
and ?y-good? requirement in (Daume? III and Marcu,
2005) resemble our local update in that they tried to
avoid the situation where the correct answer cannot
be output. Considering such commonality, the way
of combining the local update and the non-local up-
date might be one important key for further improve-
ment.
It is still open whether these differences are ad-
vantages or disadvantages. However, we think our
algorithm can be a contribution to the study for in-
corporating non-local features. The convergence
guarantee is important for the confidence in the
training results, although it does not mean high per-
formance directly. Our algorithm could at least im-
prove the accuracy of NER with non-local features
and it was indicated that our algorithm was supe-
rior to the re-ranking approach in terms of accu-
racy and training cost. However, the achieved accu-
racy was not better than that of related work (Finkel
et al, 2005; Krishnan and Manning, 2006) based
on CRFs. Although this might indicate the limita-
tion of perceptron-based methods, it has also been
shown that there is still room for improvement in
perceptron-based algorithms as our margin percep-
tron algorithm demonstrated.
8 Conclusion
In this paper, we presented a new perceptron algo-
rithm for learning with non-local features. We think
the proposed algorithm is an important step towards
achieving our final objective. We would like to in-
vestigate various types of new non-local features us-
ing the proposed algorithm in future work.
Appendix A: Convergence of Algorithm 4.2
Let ?k be a weight vector before the kth update and
?k be a variable that takes 1 when the kth update is
done in (A) and 0 when done in (B). The update rule
can then be written as ?k+1 = ?k + ?k(?a???a +
(1? ?k)(?l? ? ?l).13 First, we obtain
?k+1 ?U l = ?k ?U l + ?k(?a? ?U l ? ?a ?U l)
+(1? ?k)(?l? ?U l ? ?l ?U l)
? ?k ?U l + ?k? + (1? ?k)?
= ?k ?U l + ? ? ?1 ?U l + k? = k?
Therefore, (k?)2 ? (?k+1 ? U l)2 ?
(||?k+1||||U l||)2 = ||?k+1||2 ? (1). On the
other hand, we also obtain
||?k+1||2 ? ||?k||2 + 2?k?k(?a? ? ?a)
+2(1? ?k)?k(?l? ? ?l)
+{?k(?a? ? ?a) + (1? ?k)(?l? ? ?l)}2
? ||?k||2 + 2C + R2
? ||?1||2 + k(R2 + 2C) = k(R2 + 2C)? (2)
We used ?k(?a? ? ?a) ? Ca, ?k(?l? ? ?l) ?
C l and C l = Ca = C to derive 2C in the second
inequality. We used ||?l???l|| ? ||?a???a|| ? R
to derive R2.
Combining (1) and (2), we obtain k ? (R2 +
2C)/?2. Substituting this into (2) gives ||?k|| ?
(R2+2C)/?. Since y? = y? and?a? ????a?? ?? >
C after convergence, we obtain
??(?) = minxi
?a? ??? ?a?? ??
||?|| ? C?/(2C + R
2).
13We use the shorthand ?a? = ?a(xi,y?i ), ?a =
?a(xi,y), ?l? = ?l(xi,y?i ), and ?l = ?l(xi,y) where y
represents the candidate used to update (y? , y?? , y1, or y2).
323
References
D. M. Bikel, R. L. Schwartz, and R. M. Weischedel.
1999. An algorithm that learns what?s in a name. Ma-
chine Learning, 34(1-3):211?231.
R. Bunescu and R. J. Mooney. 2004. Collective infor-
mation extraction with relational markov networks. In
ACL 2004.
S. F. Chen and R. Rosenfeld. 2000. A survey of smooth-
ing techniques for ME models. IEEE Transactions on
Speech and Audio Processing, 8(1):37?50.
M. Collins and N. Duffy. 2002. New ranking algorithms
for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In ACL 2002, pages
263?270.
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In ACL 2004.
M. Collins. 2002a. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In EMNLP 2002.
M. Collins. 2002b. Ranking algorithms for named-entity
extraction: Boosting and the voted perceptron. In ACL
2002.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research.
H. Daume? III and D. Marcu. 2005. Learning as search
optimization: Approximate large margin methods for
structured prediction. In ICML 2005.
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by Gibbs sampling. In ACL 2005.
C. Gentile. 2001. A new approximate maximal margin
classification algorithm. JMLR, 3.
R. Herbrich and T. Graepel. 2000. Large scale Bayes
point machines. In NIPS 2000.
W. Krauth and M. Me?zard. 1987. Learning algorithms
with optimal stability in neural networks. Journal of
Physics A 20, pages 745?752.
V. Krishnan and C. D. Manning. 2006. An effective two-
stage model for exploiting non-local dependencies in
named entity recognitioin. In ACL-COLING 2006.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML 2001,
pages 282?289.
Y. Li, H. Zaragoza, R. Herbrich, J. Shawe-Taylor, and
J. Kandola. 2002. The perceptron algorithm with un-
even margins. In ICML 2002.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In ACL-COLING 2006.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In EACL
2006.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In ACL
2005.
T. Nakagawa and Y. Matsumoto. 2006. Guessing parts-
of-speech of unknown words using global information.
In ACL-COLING 2006.
L. R. Rabiner. 1989. A tutorial on hidden Markov mod-
els and selected applications in speech recognition.
Proceedings of the IEEE, 77(2):257?286.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunk-
ing using transformation-based learning. In third ACL
Workshop on very large corpora.
F. Rosenblatt. 1958. The perceptron: A probabilistic
model for information storage and organization in the
brain. Psycological Review, pages 386?407.
D. Roth and W. Yih. 2005. Integer linear program-
ming inference for conditional random fields. In ICML
2005.
S. Sarawagi and W. W. Cohen. 2004. Semi-Markov ran-
dom fields for information extraction. In NIPS 2004.
L. Shen and A. K. Joshi. 2004. Flexible margin selection
for reranking with full pairwise samples. In IJCNLP
2004.
F. K. Soong and E. Huang. 1991. A tree-trellis based
fast search for finding the n best sentence hypotheses
in continuous speech recognition. In ICASSP-91.
C. Sutton and A. McCallum. 2004. Collective segme-
nation and labeling of distant entitites in information
extraction. University of Massachusetts Rechnical Re-
port TR 04-49.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In NIPS 2003.
E. F. Tjong, K. Sang, and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In CoNLL
2003.
324
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 698?707, Prague, June 2007. c?2007 Association for Computational Linguistics
Exploiting Wikipedia as External Knowledge for Named Entity Recognition
Jun?ichi Kazama and Kentaro Torisawa
Japan Advanced Institute of Science and Technology (JAIST)
Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan
{kazama, torisawa}@jaist.ac.jp
Abstract
We explore the use of Wikipedia as external
knowledge to improve named entity recog-
nition (NER). Our method retrieves the cor-
responding Wikipedia entry for each can-
didate word sequence and extracts a cate-
gory label from the first sentence of the en-
try, which can be thought of as a definition
part. These category labels are used as fea-
tures in a CRF-based NE tagger. We demon-
strate using the CoNLL 2003 dataset that the
Wikipedia category labels extracted by such
a simple method actually improve the accu-
racy of NER.
1 Introduction
It has been known that Gazetteers, or entity dic-
tionaries, are important for improving the perfor-
mance of named entity recognition. However, build-
ing and maintaining high-quality gazetteers is very
time consuming. Manymethods have been proposed
for solving this problem by automatically extracting
gazetteers from large amounts of texts (Riloff and
Jones, 1999; Thelen and Riloff, 2002; Etzioni et al,
2005; Shinzato et al, 2006; Talukdar et al, 2006;
Nadeau et al, 2006). However, these methods re-
quire complicated induction of patterns or statistical
methods to extract high-quality gazetteers.
We have recently seen a rapid and successful
growth of Wikipedia (http://www.wikipedia.org),
which is an open, collaborative encyclopedia on
the Web. Wikipedia has now more than 1,700,000
articles on the English version (March 2007) and
the number is still increasing. Since Wikipedia
aims to be an encyclopedia, most articles are about
named entities and they are more structured than raw
texts. Although it cannot be used as gazetteers di-
rectly since it is not intended as a machine readable
resource, extracting knowledge such as gazetteers
from Wikipedia will be much easier than from raw
texts or from usual Web texts because of its struc-
ture. It is also important that Wikipedia is up-
dated every day and therefore new named entities are
added constantly. We think that extracting knowl-
edge from Wikipedia for natural language process-
ing is one of the promising ways towards enabling
large-scale, real-life applications. In fact, many
studies that try to exploit Wikipedia as a knowl-
edge source have recently emerged (Bunescu and
Pas?ca, 2006; Toral and Mun?oz, 2006; Ruiz-Casado
et al, 2006; Ponzetto and Strube, 2006; Strube and
Ponzetto, 2006; Zesch et al, 2007).
As a first step towards such approach, we demon-
strate in this paper that category labels extracted
from the first sentence of a Wikipedia article, which
can be thought of as the definition of the entity de-
scribed in the article, are really useful to improve the
accuracy of NER. For example, ?Franz Fischler? has
the article with the first sentence, ?Franz Fischler
(born September 23, 1946) is an Austrian politi-
cian.? We extract ?politician? from this sentence
as the category label for ?Franz Fischler?. We use
such category labels as well as matching informa-
tion as features of a CRF-based NE tagger. In our
experiments using the CoNLL 2003 NER dataset
(Tjong et al, 2003), we demonstrate that we can
improve performance by using the Wikipedia fea-
tures by 1.58 points in F-measure from the baseline,
and by 1.21 points from the model that only uses
the gazetteers provided in the CoNLL 2003 dataset.
Our final model incorporating all features achieved
88.02 in F-measure, which means a 3.03 point im-
provement over the baseline, which does not use any
698
gazetteer-type feature.
The studies most relevant to ours are Bunescu and
Pas?ca (2006) and Toral and Mun?oz (2006).
Bunescu and Pas?ca (2006) presented a method of
disambiguating ambiguous entities exploiting inter-
nal links in Wikipedia as training examples. The
difference however is that our method tries to use
Wikipedia features for NER, not for disambiguation
which assumes that entity regions are already found.
They also did not focus on the first sentence of an
article. Also, our method does not disambiguate
ambiguous entities, since accurate disambiguation
is difficult and possibly introduces noise. There are
two popular ways for presenting ambiguous entities
in Wikipedia. The first is to redirect users to a dis-
ambiguation page, and the second is to redirect users
to one of the articles. We only focused on the second
case and did not utilize disambiguation pages in this
study. This method is simple but works well because
the article presented in the second case represents in
many cases the major meaning of the ambiguous en-
tities and therefore that meaning frequently appears
in a corpus.
Toral and Mun?oz (2006) tried to extract gazetteers
from Wikipedia by focusing on the first sentences.
However, their way of using the first sentence is
slightly different. We focus on the first noun phrase
after be in the first sentence, while they used all the
nouns in the sentence. By using these nouns and
WordNet, they tried to map Wikipedia entities to ab-
stract categories (e.g., LOC, PER ORG, MISC) used
in usual NER datasets. We on the other hand use the
obtained category labels directly as features, since
we think the mapping performed automatically by
a CRF model is more precise than the mapping by
heuristic methods. Finally, they did not demonstrate
the usefulness of the extracted gazetteers in actual
NER systems.
The rest of the paper is organized as follows. We
first explain the structure of Wikipedia in Section
2. Next, we introduce our method of extracting and
using category labels in Section 3. We then show
the experimental results on the CoNLL 2003 NER
dataset in Section 4. Finally, we discuss the pos-
sibility of further improvement and future work in
Section 5.
2 Wikipedia
2.1 Basic structure
An article in Wikipedia is identified by a unique
name, which can be obtained by concatenating the
words in the article title with underscore ? ?. For ex-
ample, the unique name for the article, ?David Beck-
ham?, is David Beckham. We call these unique
names ?entity names? in this paper.
Wikipedia articles have many useful structures for
knowledge extraction such as headings, lists, inter-
nal links, categories, and tables. These are marked
up by using the Wikipedia syntax in source files,
which authors edit. See the Wikipedia entry iden-
tified by How to edit a page for the details of the
markup language.
We describe two important structures, redirec-
tions and disabiguation pages, in the following sec-
tions.
2.2 Redirection
Some entity names in Wikipedia do not have a sub-
stantive article and are only redirected to an arti-
cle with another entity name. This mechanism is
called ?redirection?. Redirections are marked up
as ?#REDIRECT [[A B C]]? in source files, where
?[[...]]? is a syntax for a link to another article in
Wikipedia (internal links). If the source file has such
a description, users are automatically redirected to
the article specified by the entity name in the brackes
(A B C for the above example). Redirections are
used for several purposes regarding ambiguity. For
example, they are used for spelling resolution such
as from ?Apples? to ?Apple? and abbreviation res-
olution such as from ?MIT? to ?Massachusetts In-
stitute of Technology?. They are also used in the
context of more difficult disambiguations described
in the next section.
2.3 Disambiguation pages
Some authors make a ?disambiguation? page for an
ambiguous entity name.1 A disambiguation page
typically enumerates possible articles for that name.
For example, the page for ?Beckham? enumerates
?David Beckham (English footballer)?, ?Victoria
1We mean by ?ambiguous? the case where a name can
be used to refer to several difference entities (i.e., articles in
Wikipedia).
699
Beckham (English celebrity and wife of David)?,
?Brice Beckham (American actor)?, and so on.
Most, but not all, disambiguation pages have a name
like Beckham (disambiguation) and are some-
times used with redirection. For example, Beck-
ham is redirected to Beckham (disambiguation)
in the above example. However, it is also possible
that Beckham redirects to one of the articles (e.g,
David Beckham). As we mentioned, we did not
utilize the disambiguation pages and relied on the
above case in this study.
2.4 Data
Snapshots of the entire contents of Wikipedia are
provided in XML format for each language version.
We used the English version at the point of Febru-
ary 2007, which includes 4,030,604 pages.2 We im-
ported the data into a text search engine3 and used it
for the research.
3 Method
In this section, we describe our method of extracting
category labels fromWikipedia and how to use those
labels in a CRF-based NER model.
3.1 Generating search candidates
Our purpose here is to find the corresponding en-
tity in Wikipedia for each word sequence in a sen-
tence. For example, given the sentence, ?Rare Jimi
Hendrix song draft sells for almost $17,000?, we
would like to know that ?Jimi Hendrix? is described
in Wikipedia and extract the category label, ?mu-
sician?, from the article. However, considering all
possible word sequences is costly. We thus restricted
the candidates to be searched to the word sequences
of no more than eight words that start with a word
containing at least one capitalized letter.4
3.2 Finding category labels
We converted a candidate word sequence to a
Wikipedia entity name by concatenating the words
with underscore. For example, a word sequence
2The number of article pages is 2,954,255 including redirec-
tion pages
3We used HyperEstraier available at
http://hyperestraier.sourceforge.net/index.html
4Words such as ?It? and ?He? are not considered as capital-
ized words here (we made a small list of stop words).
?Jimi Hendrix? is converted to Jimi Hendrix. Next,
we retrieved the article corresponding to the entity
name.5 If the page for the entity name is a redirec-
tion page, we followed redirection until we find a
non-redirection page.
Although there is no strict formatting rule in
Wikipedia, the convention is to start an article with
a short sentence defining the entity the article de-
scribes. For example, the article for Jimi Hendrix
starts with the sentence, ?Jimi Hendrix (November
27, 1942, Seattle, Washington - September 18, 1970,
London, England) was an American guitarist, singer
and songwriter.? Most of the time, the head noun of
the noun phrase just after be is a good category la-
bel. We thus tried to extract such head nouns from
the articles.
First, we eliminated unnecessary markup such
as italics, bold face, and internal links from the
article. We also converted the markup for inter-
nal links like [[Jimi Hendrix|Hendrix]] to
Hendrix, since the part after |, if it exists, rep-
resents the form to be displayed in the page. We
also eliminated template markup, which is enclosed
by {{ and }}, because template markup sometimes
comes at the beginning of the article and makes
the extraction of the first sentence impossible.6 We
then divided the article into lines according to the
new line code, \n, <br> HTML tags, and a very
simple sentence segmentation rule for period (.).
Next, we removed lines that match regular expres-
sion /?\s*:/ to eliminate the lines such as:
This article is about the tree and its fruit.
For the consumer electronics corporation,
see Apple Inc.
These sentences are not the content of the article but
often placed at the beginning of an article. Fortu-
nately, they are usually marked up using :, which is
for indentation.
After the preprocessing described above, we ex-
tracted the first line in the remaining lines as the first
sentence from which we extract a category label.
5There are pages for other than usual articles in the
Wikipedia data. They are distinguished by a namespace at-
tribute. To retrieve articles, we only searched in namespace 0,
which is for usual articles.
6Templates are used for example to generate profile tables
for persons.
700
We then performed POS tagging and phrase chunk-
ing. TagChunk (Daume? III and Marcu, 2005)7 was
used as a POS/chunk tagger. Next, we extracted the
first noun phrase after the first ?is?, ?was?, ?are?, or
?were? in the sentence. Basically, we extracted the
last word in the noun phrase as the category label.
However, we used the second noun phrase when the
first noun phrase ended with ?one?, ?kind?, ?sort?,
or ?type?, or it ended with ?name? followed by ?of?.
These rules were for treating examples like:
Jazz is [a kind]NP [of]PP [music]NP characterized
by swung and blue notes.
In these cases, we would like to extract the head
noun of the noun phrase after ?of? (e.g., ?music?
in instead of ?kind? for the above example). How-
ever, we would like to extract ?name? itself when the
sentence was like ?Ichiro is a Japanese given name?.
We did not utilize Wikipedia?s ?Category? sec-
tions in this study, since a Wikipedia article can have
more than one category, and many of them are not
clean hypernyms of the entity as far as we observed.
We will need to select an appropriate category from
the listed categories in order to utilize the Category
section. We left this task for future research.
3.3 Using category labels as features
If we could find the category label for the candidate
word sequence, we annotated it using IOB2 tags in
the same way as we represent named entities. In
IOB2 tagging, we use ?B-X?, ?I-X?, and ?O? tags,
where ?B?, ?I?, and ?O? means the beginning of an
entity, the inside of an entity, and the outside of en-
tities respectively. Suffix X represents the category
of an entity.8 In this case, we used the extracted cat-
egory label as the suffix. For example, if we found
that ?Jimi Hendrix? was in Wikipedia and extracted
?guitarist? as the category label, we annotated the
sentence, ?Rare Jimi Hendrix song draft sells for al-
most $17,000?, as:
RareO JimiB-guitarist HendrixI-guitarist songO draftO
forO almostO $17,000O .O
Note that we adopted the leftmost longest match if
there were several possible matchings. These IOB2
tags were used in the same way as other features
7http://www.cs.utah.edu/?hal/TagChunk/
8We use bare ?B?, ?I?, and ?O? tags if we want to represent
only the matching information.
in our NE tagger using Conditional Random Fields
(CRFs) (Lafferty et al, 2001). For example, we used
a feature such as ?the Wikipedia tag is B-guitarist
and the NE tag is B-PER?.
4 Experiments
In this section, we demonstrate the usefulness of the
extracted category labels for NER.
4.1 Data and setting
We used the English dataset of the CoNLL 2003
shared task (Tjong et al, 2003). It is a corpus of
English newspaper articles, where four entity cate-
gories, PER, LOC, ORG, and MISC are annotated.
It consists of training, development, and testing sets
(14,987, 3,466, and 3,684 sentences, respectively).
We concatenated the sentences in the same docu-
ment according to the document boundary markers
provided in the dataset.9 This generated 964 doc-
uments for the training set, 216 documents for the
development set, and 231 documents for the test-
ing set. Although automatically assigned POS and
chunk tags are also provided in the dataset, we used
TagChunk (Daume? III and Marcu, 2005)10 to assign
POS and chunk tags, since we observed that accu-
racy could be improved, presumably due to the qual-
ity of the tags.11
We used the features summarized in Table 1 as the
baseline feature set. These are similar to those used
in other studies on NER. We omitted features whose
surface part described in Table 1 occurred less than
twice in the training corpus.
Gazetteer files for the four categories, PER
(37,831 entries), LOC (10,069 entries), ORG (3,439
entries), and MISC (3,045 entries), are also provided
in the dataset. We compiled these files into one
gazetteer, where each entry has its entity category,
and used it in the same way as the Wikipedia feature
described in Section 3.3. We will compare features
using this gazetteer with those using Wikipedia in
the following experiments.
9We used sentence concatenation because we found it im-
proves the accuracy in another study (Kazama and Torisawa,
2007).
10http://www.cs.utah.edu/?hal/TagChunk/
11This is not because TagChunk overfits the CoNLL 2003
dataset (TagChunk is trained on the Penn Treebank (Wall Street
Journal), while the CoNLL 2003 data are taken from the Reuters
corpus).
701
Table 1: Baseline features. The value of a node fea-
ture is determined from the current label, y0, and a
surface feature determined only from x. The value
of an edge feature is determined by the previous la-
bel, y?1, the current label, y0, and a surface feature.
Used surface features are the word (w), the down-
cased word (wl), the POS tag (pos), the chunk tag
(chk), the prefix of the word of length n (pn), the
suffix (sn), the word form features: 2d - cp (these
are based on (Bikel et al, 1999))
Node features:
{??, x?2, x?1, x0, x+1, x+2} ? y0
x = w, wl, pos, chk, p1, p2, p3, p4, s1, s2, s3, s4, 2d,
4d, d&a, d&-, d&/, d&,, d&., n, ic, ac, l, cp
Edge features:
{??, x?2, x?1, x0, x+1, x+2} ? y?1 ? y0
x = w, wl, pos, chk, p1, p2, p3, p4, s1, s2, s3, s4, 2d,
4d, d&a, d&-, d&/, d&,, d&., n, ic, ac, l, cp
Bigram node features:
{x?2x?1, x?1x0, x0x+1} ? y0
x = wl, pos, chk
Bigram edge features:
{x?2x?1, x?1x0, x0x+1} ? y?1 ? y0
x = wl, pos, chk
We used CRF++ (ver. 0.44)12 as the basis of our
implementation of CRFs. We implemented scaling,
which is similar to that for HMMs (see for instance
(Rabiner, 1989)), in the forward-backward phase of
CRF training to deal with long sequences due to
sentence concatenation.13 We used Gaussian reg-
ularization to avoid overfitting. The parameter of
the Gaussian, ?2, was tuned using the development
set.14 We stopped training when the relative change
in the log-likelihood became less than a pre-defined
threshold, 0.0001, for at least three iterations.
4.2 Category label finding
Table 2 summarizes the statistics of category label
finding for the training set. Table 3 lists examples
of the extracted categories. As can be seen, we
could extract more than 1,200 distinct category la-
bels. These category labels seem to be useful, al-
12http://chasen.org/?taku/software/CRF++
13We also replaced the optimization module in the original
package with that used in the Amis maximum entropy estima-
tor (http://www-tsujii.is.s.u-tokyo.ac.jp/amis) since we encoun-
tered problems with the provided module in some cases. Al-
though this Amis module implements BLMVM (Benson and
More?, 2001), which supports the bounding of weights, we did
not use this feature in this study (i.e., we just used it as the re-
placement for the L-BFGS optimizer in CRF++).
14We tested 15 points: {0.01, 0.02, 0.04, . . . , 163.84, 327.68}.
Table 2: Statistics of category label finding.
search candidates (including duplication) 256,418
candidates having Wikipedia article 39,258
(articles found by redirection) 9,587
first sentence found 38,949
category label extracted 23,885
(skipped ?one?) 544
(skipped ?kind?) 14
(skipped ?sort?) 1
(skipped ?type?) 41
(skipped ?name of?) 463
distinct category labels 1,248
Table 3: Examples of category labels (top 20).
category frequency # distinct entities
country 2598 152
city 1436 284
name 1270 281
player 578 250
day 564 131
month 554 15
club 537 167
surname 515 185
capital 454 79
state 416 60
term 369 78
form 344 40
town 287 97
cricketer 276 97
adjective 260 6
golfer 229 88
world 221 24
team 220 52
organization 214 38
second 212 1
though there is no guarantee that the extracted cate-
gory label is correct for each candidate.
4.3 Feature comparison
We compared the following features in this experi-
ment.
Gazetteer Match (gaz m) This feature represents
the matching with a gazetteer entry by using
?B?, ?I?, and ?O? tags. That is, this is the
gazetteer version of wp m below.
Gazetteer Category Label (gaz c) This feature
represents the matching with a gazetteer entry
and its category by using ?B-X?, ?I-X?, and
?O? tags, where X is one of ?PER?, ?LOC?,
?ORG?, and ?MISC?. That is, this is the
gazetteer version of wp c below.
Wikipedia Match (wp m) This feature represents
the matching with a Wikipedia entity by using
?B?, ?I?, and ?O? tags.
702
Table 4: Statistics of gazetteer and Wikipedia fea-
tures. Rows ?NEs (%)? show the number of matches
that also matched the regions of the named entities in
the training data, and the percentage of such named
entities (there were 23,499 named entities in total in
the training data).
Gazetteer Match (gaz m)
matches 12,397
NEs (%) 6,415 (27.30%)
Wikipedia Match (wp m)
matches 27,779
NEs (%) 16,600 (70.64%)
Wikipedia Category Label (wp c)
matches 18,617
NEs (%) 11,645 (49.56%)
common with gazetteer match 5,664
Wikipedia Category Label (wp c) This feature
represents the matching with a Wikipedia
entity and its category in the way described
Section in 3.3. Note that this feature only
fires when the category label is successfully
extracted from the Wikipedia article.
For these gaz m, gaz c, wp m, and wp c, we gener-
ate the node features, the edge features, the bigram
node features, and the bigram edge features, as de-
scribed in Table 1.
Table 4 shows how many matches (the leftmost
longest matches that were actually output) were
found for gaz m, wp m, and wp c. We omit-
ted the numbers for gaz c, since they are same
as gaz m. We can see that Wikipedia had more
matches than the gazetteer, and covers more named
entities (more than 70% of the NEs in the training
corpus). The overlap between the gazetteer matches
and the Wikipedia matches was moderate as the last
row indicates (5,664 out of 18,617 matches). This
indicates that Wikipedia has many entities that are
not listed in the gazetteer.
We then compared the baseline model (baseline),
which uses the feature set in Table 1, with the fol-
lowing models to see the effect of the gazetteer fea-
tures and the Wikipedia features.
(A): + gaz m This uses gaz m in addition to the
features in baseline.
(B): + gaz m, gaz c This uses gaz m and gaz c in
addition to the features in baseline.
(C): + wp m This uses wp m in addition to the fea-
tures in baseline.
(D): + wp m, wp c This uses wp m and wp c in
addition to the features in baseline.
(E): + gaz m, gaz c, wp m, wp c This uses
gaz m, gaz c, wp m, and wp c in addition to
the features in baseline.
(F): + gaz m, gaz c, wp m, wp c (word comb.)
This model uses the combination of words
(wl) and gaz m, gaz c, wp m, or wp c,
in addition to the features of model (E).
More specifically, these features are the node
feature, wl0 ? x0 ? y0, the edge feature,
wl0 ? x0 ? y?1 ? y0, the bigram node feature,
wl?1 ? wl0 ? x?1 ? x0 ? y0, and the bigram
edge feature, wl?1?wl0?x?1?x0?y?1?y0,
where x is one of gaz m, gaz c, wp m, and
wp c. We tested this model because we thought
these combination features could alleviate the
problem by incorrectly extracted categories
in some cases, if there is a characteristic
correlation between words and incorrectly
extracted categories.
Table 5 shows the performance of these mod-
els. The results for (A) and (C) indicate that the
matching information alone does not improve ac-
curacy. This is because entity regions can be iden-
tified fairly correctly if models are trained using a
sufficient amount of training data. The category la-
bels, on the other hand, are actually important for
improvement as the results for (B) and (D) indicate.
The gazetteer model, (B), improved F-measure by
1.47 points from the baseline. TheWikipedia model,
(D), improved F-measure by 1.58 points from the
baseline. The effect of the gazetteer feature, gaz c,
and the Wikipedia features, wp c, did not differ
much. However, it is notable that the Wikipedia fea-
ture, which is obtained by our very simple method,
achieved such an improvement easily.
The results for model (E) show that we can im-
prove accuracy further, by using the gazetteer fea-
tures and the Wikipedia features together. Model (E)
achieved 87.67 in F-measure, which is better than
those of (B) and (D). This result coincides with the
fact that the overlap between the gazetteer feature
703
Table 5: Effect of gazetteer and Wikipedia features.
dev eval
model (best ?2) category P R F P R F
baseline (20.48)
PER 90.29 92.89 91.57 87.19 91.34 89.22
LOC 93.32 92.81 93.07 88.14 88.25 88.20
ORG 85.36 83.07 84.20 82.25 78.93 80.55
MISC 92.21 84.71 88.30 79.58 75.50 77.49
ALL 90.42 89.38 89.90 85.17 84.81 84.99
(A): + gaz m (81.92)
PER 90.60 92.56 91.57 87.90 90.72 89.29
LOC 92.84 93.20 93.02 88.26 88.37 88.32
ORG 85.54 82.92 84.21 82.37 79.05 80.68
MISC 92.15 85.25 88.56 78.73 75.93 77.30
ALL 90.41 89.45 89.92 85.33 84.76 85.04
(B): + gaz m, gaz c (163.84)
PER 92.45 94.41 93.42 90.78 91.96 91.37
LOC 94.43 94.07 94.25 89.98 89.33 89.65
ORG 86.68 85.38 86.03 82.43 81.34 81.88
MISC 92.47 85.25 88.71 79.50 76.78 78.12
ALL 91.77 90.84 91.31 86.74 86.17 86.46
(C): + wp m (163.84)
PER 90.84 92.56 91.69 87.77 90.11 88.92
LOC 92.63 93.03 92.83 87.23 88.07 87.65
ORG 86.19 83.74 84.95 81.77 79.65 80.70
MISC 91.69 84.92 88.18 79.04 75.21 77.08
ALL 90.49 89.53 90.01 84.85 84.58 84.71
(D): + wp m, wp c (163.84)
PER 91.57 94.41 92.97 90.13 92.02 91.06
LOC 94.78 93.96 94.37 89.41 89.63 89.52
ORG 87.36 85.01 86.17 82.70 82.00 82.35
MISC 91.87 84.60 88.09 81.34 76.35 78.77
ALL 91.68 90.63 91.15 86.71 86.42 86.57
(E): + gaz m, gaz c, wp m,
wp c (40.96)
PER 93.32 95.49 94.39 92.28 93.14 92.71
LOC 94.91 94.39 94.65 90.69 90.47 90.58
ORG 88.27 86.95 87.60 83.08 83.68 83.38
MISC 93.14 85.36 89.08 81.33 76.92 79.06
ALL 92.65 91.65 92.15 87.79 87.55 87.67
(F): + gaz m, gaz c, wp m,
wp c (word comb.) (5.12)
PER 93.38 95.66 94.50 92.52 93.26 92.89
LOC 94.88 94.77 94.83 91.25 90.71 90.98
ORG 88.67 86.95 87.80 83.61 84.17 83.89
MISC 93.56 85.03 89.09 81.63 77.21 79.36
ALL 92.82 91.77 92.29 88.21 87.84 88.02
704
 70
 72
 74
 76
 78
 80
 82
 84
 86
 88
 100  200  300  400  500  600  700  800  900  1000
F
training size (documents)
baseline
+wp_m
+wp_m, wp_c
Figure 1: Relation between the training size and the
accuracy.
and the Wikipedia feature was not so large. If we
consider model (B) a practical baseline, we can say
that the Wikipedia features improved the accuracy in
F-measure by 1.21 points.
We can also see that the effect of the gazetteer
features and the Wikipedia features were consistent
irrespective of categories (i.e., PER, LOC, ORG, or
MISC) and performance measures (i.e., precision,
recall, or F-measure). This indicates that gazetteer-
type features are reliable as features for NER.
The final model, (F), achieved 88.02 in F-
measure. This is greater than that of the baseline by
3.03 points, showing the usefulness of the gazetteer
type features.
4.4 Effect of training size
We observed in the previous experiment that the
matching information alone was not useful. How-
ever, the situation may change if the size of the train-
ing data becomes small. We thus observed the effect
of the training size for the Wikipedia features wp m
and wp c (we used ?2 = 10.24). Figure 1 shows
the result. As can be seen, the matching information
had a slight positive effect when the size of training
data was small. For example, it improved F-measure
by 0.8 points from the baseline at 200 documents.
However, the superiority of category labels over the
matching information did not change. The effect of
category labels became greater as the training size
became smaller. Its effect compared with the match-
ing information alone was 3.01 points at 200 docu-
ments, while 1.91 points at 964 documents (i.e., the
whole training data).
Table 6: Breakdown of improvements and errors.
(B) ? (E) num. g? ? w? g? ? w g ? w? g ? w
inc ? inc 442 219 123 32 68
inc ? cor 102 28 56 3 15
cor? inc 56 28 13 7 8
cor? cor 5,342 1,320 1,662 723 1,637
4.5 Improvement and error analysis
We analyze the improvements and the errors caused
by using the Wikipedia features in this section.
We compared the output of (B) and (E) for the de-
velopment set. There were 5,942 named entities in
the development set. We assessed how the labeling
for these entities changed between (B) and (E). Note
that the labeling for 199 sentences out of total 3,466
sentences was changed. Table 6 shows the break-
down of the improvements and the errors. ?inc? in
the table means that the model could not label the
entity correctly, i.e., the model could not find the en-
tity region at all, or it assigned an incorrect category
to the entity. ?cor? means that the model could label
the entity correctly. The column, ?inc ? cor?, for
example, has the numbers for the entities that were
labeled incorrectly by (B) but labeled correctly by
(E). We can see from the column, ?num?, that the
number of improvements by (E) exceeded the num-
ber of errors introduced by (E) (102 vs. 56). Table
6 also shows how the gazetteer feature, gaz c, and
the Wikipedia feature, wp c, fired in each case. We
mean that the gazetteer feature fired by using ?g?,
and that the Wikipedia feature fired by using ?w?.
?g?? and ?w?? mean that the feature did not fire. As
is the case for other machine learning methods, it
is difficult to find a clear reason for each improve-
ment or error. However, we can see that the number
of g? ? w exceeded those of other cases in the case
of ?inc ? cor?, meaning that the Wikipedia feature
contributed the most.
Finally, we show an example of case inc ?
cor in Figure 2. We can see that ?Gazzetta dello
Sport? in the sentence was correctly labeled as an
entity of ?ORG? category by model (E), because the
Wikipedia feature identified it as a newspaper en-
tity.15
15Note that the category label, ?character?, for ?Atalanta? in
the sentence was not correct in this context, which is an example
where disambiguation is required. The final recognition was
correct in this case presumably because of the information from
gaz c feature.
705
O O O O B-LOC
Sentence No. 584
UEFA came down heavily on Belgian club Standard Liege on Friday for " disgraceful behaviour " in an Intertoto final match against Karlsruhe of Germany .
B-
ORG
O O O O O O B-ORG O O O O O O O O O O O O O O B-LOC O O O
B-
body
O O O O
B-
country
O B-club I-club O O O O O O O O O
B-
competition
O O O B-city O
B-
country
O
B-
ORG
O O O O B-MISC O B-ORG
I-
ORG
O O O O O O O O O B-MISC O O O B-ORG O B-LOC O
B-
ORG
O O O O B-MISC O B-ORG
I-
ORG
O O O O O O O O O B-LOC O O O B-ORG O B-LOC O
B-
ORG
O O O O B-MISC O B-ORG
I-
ORG
O O O O O O O O O B-MISC O O O B-ORG O B-LOC O
Sentence No. 591
ATHLETICS - HARRISON , EDWARDS TO MEET IN SARAJEVO .
O O O O O O O O O O
O O O O O O O O O O
O O B-PER O B-PER O O O B-LOC O
O O B-PER O B-LOC O O O B-LOC O
O O B-PER O B-ORG O O O B-LOC O
Sentence No. 596
Edwards was quoted as saying : " What type of character do we show by going to the IAAF Grand Prix Final in Milan where there is a lot of money to make but refusing to make the trip to Sarajevo as a humanitarian gesture ? "
B-PER O O O O O O O O O O O O O O O O O
B-
ORG
B-
MISC
I-
MISC
O O
B-
PER
O O O O O O O O O O O O O O O O B-LOC O O O O O O
O O O O O O O O O O O O O O O O O O
B-
2003
I-
2003
I-
2003
I-
2003
O
B-
city
O O O O O O O O O O O O O O O O B-city O O O O O O
B-PER O O O O O O O O O O O O O O O O O
B-
MISC
I-
MISC
I-
MISC
I-
MISC
O
B-
LOC
O O O O O O O O O O O O O O O O B-LOC O O O O O O
B-PER O O O O O O O O O O O O O O O O O
B-
ORG
I-
ORG
I-
ORG
I-
ORG
I-
ORG
I-
ORG
O O O O O O O O O O O O O O O O B-LOC O O O O O O
B-PER O O O O O O O O O O O O O O O O O
B-
ORG
I-
ORG
I-
ORG
I-
ORG
O
B-
LOC
O O O O O O O O O O O O O O O O B-LOC O O O O O O
Sentence No. 604
SOCCER - MILAN 'S LENTINI MOVES TO ATALANTA .
O O O O O O O O O
O O B-missile O O O O O O
O O B-ORG O B-PER O O B-ORG O
O O B-PER O B-PER O O B-ORG O
O O B-ORG O B-PER O O B-ORG O
Sentence No. 607
The Gazzetta dello Sport said the deal would cost Atalanta around $ 600,000 .
O O O B-ORG O O O O O B-ORG O O O O
O B-newspaper I-newspaper I-newspaper O O O O O B-character O O O O
O B-ORG I-ORG I-ORG O O O O O B-ORG O O O O
O B-LOC O B-ORG O O O O O B-ORG O O O O
O B-ORG I-ORG I-ORG O O O O O B-ORG O O O O
Sentence No. 610
The move to
Bergamo-
based
Atalanta reunites Lentini , who fell out with
ex-
Milan
coach Fabio Capello last season , with his former coach at Torino , Emiliano Mondonico .
O O O O B-ORG O B-PER O O O O O O O B-PER O O O O O O O O O B-LOC O B-PER O O
O O O O
B-
character
O
B-
town
O O O O O O O
B-
coaches
I-
coaches
O O O O O O O O
B-
business
O
B-
manager
I-manager O
O O O B-MISC B-ORG O B-PER O O O O O
B-
MISC
O B-PER I-PER O O O O O O O O B-ORG O B-PER I-PER O
O O O B-MISC B-ORG O B-PER O O O O O O O B-PER I-PER O O O O O O O O B-LOC O B-PER I-PER O
O O O B-MISC B-ORG O
B-
ORG
O O O O O O O B-PER I-PER O O O O O O O O B-LOC O B-PER I-PER O
Sentence No. 653
Did not bat : Dharmasena , Vaas , Muralitharan .
O O O O O O O O O O
O O O O O O B-cricketer O B-cricketer O
O O O O B-PER O B-PER O B-PER O
O O O O B-PER O B-LOC O B-LOC O
- gaz_c
- wp_c
- correct
- (B)
- (C)
-  gaz_c
- wp_c
-  correct
-  (B)
-  (E)
Figure 2: An example of improvement caused by Wikipedia feature.
5 Discussion and Future Work
We have empirically shown that even category la-
bels extracted from Wikipedia by a simple method
such as ours really improves the accuracy of a
NER model. The results indicate that structures
in Wikipedia are suited for knowledge extraction.
However, the results also indicate that there is room
for improvement, considering that the effects of
gaz c and wp c were similar, while the matching
rate was greater for wp c. An issue, which we
should treat, is the disambiguation of ambiguous
entities. Our method worked well although it was
very simple, presumably because of the following
reason. (1) If a retrieved page is a disambiguation
page, we cannot extract a category label and critical
noise is not introduced. (2) If a retrieved page is not
a disambiguation page, it will be the page describ-
ing the major meaning determined by the agreement
of many authors. The extracted categories are use-
ful for improving accuracy because the major mean-
ing will be used frequently in the corpus. How-
ever, it is clear that disambiguation techniques are
required to achieve further improvements. In ad-
dition, if Wikipedia grows at the current rate, it is
possible that almost all entities become ambiguous
and a retrieved page is a disambiguation page most
of the time. We will need a method for finding the
most suitable article from the articles listed in a dis-
ambiguation page.
An interesting point in our results is that
Wikipedia category labels improved accuracy, al-
though they were much more specific (more than
1,200 categories) than the four categories of the
CoNLL 2003 dataset. The correlation between a
Wikipedia category label and a category label of
NER (e.g., ?musician? to ?PER?) was probably
learned by a CRF tagger. However, the merit of
using such specific Wikipedia labels will be much
greater when we aim at developing NER systems for
more fine-grained NE categories such as proposed
in Sekine et al (2002) or Shinzato et al (2006).
We thus would like to inv tig te the effect of the
Wikipedia feature for NER with such fine-grained
categories as well. Disambiguation techniques will
be important again in that case. Although the impact
of ambiguity will be small as long as the target cat-
egories are abstract and an incorrectly extracted cat-
egory is in the same abstract category as the correct
one (e.g., extracting ?footballer? instead of ?crick-
eter?), such mis-categorization is critical if it is nec-
essary to distinguish footballers from cricketers.
6 Conclusion
We tried to exploit Wikipedia as external knowledge
to improve NER. We extracted a category label from
the first sentence of a Wikipedia article and used it
as a feature of a CRF-based NE tagger. The experi-
ments using the CoNLL 2003 NER dataset demon-
strated that category labels extracted by such a sim-
ple method really improved accuracy. However, dis-
ambiguation techniques will become more impor-
tant as Wikipedia grows or if we aim at more fine-
grained NER. We thus would like to incorporate a
disambiguation technique into our method in future
work. Exploiting Wikipedia structures such as dis-
ambiguation pages and link structures will be the
key in that case as well.
References
S. J. Benson and J. J. More?. 2001. A limited mem-
ory variable metric method for bound constraint min-
imization. Technical Report ANL/MCS-P909-0901,
Argonne National Laboratory.
D. M. Bikel, R. L. Schwartz, and R. M. Weischedel.
1999. An algorithm that learns what?s in a name. Ma-
chine Learning, 34(1-3):211?231.
706
R. Bunescu and M. Pas?ca. 2006. Using encyclopedic
knowledge for named entity disambiguation. In EACL
2006.
H. Daume? III and D. Marcu. 2005. Learning as search
optimization: Approximate large margin methods for
structured prediction. In ICML 2005.
O. Etzioni, M. Cafarella, D. Downey, A. M. Popescu,
T. Shaked, S. Soderland, D. S. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web ? an experimental study. Artificial Intelligence
Journal.
J. Kazama and K. Torisawa. 2007. A new perceptron al-
gorithm for sequence labeling with non-local features.
In EMNLP-CoNLL 2007.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML 2001,
pages 282?289.
D. Nadeau, Peter D. Turney, and Stan Matwin. 2006.
Unsupervised named-entity recognition: Generating
gazetteers and resolving ambiguity. In 19th Canadian
Conference on Artificial Intelligence.
S. P. Ponzetto and M. Strube. 2006. Exploiting semantic
role lebeling, WordNet and Wikipedia for coreference
resolution. In NAACL 2006.
L. R. Rabiner. 1989. A tutorial on hidden Markov mod-
els and selected applications in speech recognition.
Proceedings of the IEEE, 77(2):257?286.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In 16th National Conference on Artificial Intelligence
(AAAI-99).
M. Ruiz-Casado, E. Alfonseca, and P. Castells. 2006.
From Wikipedia to semantic relationships: a semi-
automated annotation approach. In Third European
Semantic Web Conference (ESWC 2006).
S. Sekine, K. Sudo, and C. Nobata. 2002. Extended
named entity hierarchy. In LREC ?02.
K. Shinzato, S. Sekine, N. Yoshinaga, and K. Tori-
sawa. 2006. Constructing dictionaries for named en-
tity recognition on specific domains from the Web. In
Web Content Mining with Human Language Technolo-
gies Workshop on the 5th International Semantic Web.
M. Strube and S. P. Ponzetto. 2006. WikiRelate! com-
puting semantic relatedness using Wikipedia. In AAAI
2006.
P. P. Talukdar, T. Brants, M. Liberman, and F. Pereira.
2006. A context pattern induction method for named
entity extraction. In CoNLL 2006.
M. Thelen and E. Riloff. 2002. A bootstrapping method
for learning semantic lexicons using extraction pattern
context. In EMNLP 2002.
E. F. Tjong, K. Sang, and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In CoNLL
2003.
A. Toral and R. Mun?oz. 2006. A proposal to automat-
ically build and maintain gazetteers for named entity
recognition by using Wikipedia. In EACL 2006.
T. Zesch, I. Gurevych, and M. Mo?hlha?user. 2007. Ana-
lyzing and accessing Wikipedia as a lexical semantic
resource. In Biannual Conference of the Society for
Computational Linguistics and Language Technology.
707
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 137?144, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Speeding up Training with Tree Kernels for Node Relation Labeling
Jun?ichi Kazama and Kentaro Torisawa
Japan Advanced Institute of Science and Technology (JAIST)
Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan
{kazama, torisawa}@jaist.ac.jp
Abstract
We present a method for speeding up the
calculation of tree kernels during train-
ing. The calculation of tree kernels is still
heavy even with efficient dynamic pro-
gramming (DP) procedures. Our method
maps trees into a small feature space
where the inner product, which can be cal-
culated much faster, yields the same value
as the tree kernel for most tree pairs. The
training is sped up by using the DP pro-
cedure only for the exceptional pairs. We
describe an algorithm that detects such ex-
ceptional pairs and converts trees into vec-
tors in a feature space. We propose tree
kernels on marked labeled ordered trees
and show that the training of SVMs for
semantic role labeling using these kernels
can be sped up by a factor of several tens.
1 Introduction
Many NLP tasks such as parse selection and tag-
ging can be posed as the classification of labeled
ordered trees. Several tree kernels have been pro-
posed for building accurate kernel-based classifiers
(Collins and Duffy, 2001; Kashima and Koyanagi,
2002). They have the following form in common.
K(T1, T2) =
?
Si
W (Si) ?#Si(T1) ?#Si(T2), (1)
where Si is a possible subtree, #Si(Tj) is the num-
ber of times Si is included in Tj , and W (Si) is
the weight of Si. That is, tree kernels are inner
products in a subtree feature space where a tree is
mapped to vector V (Tj) =
(?
W (Si)#Si(Tj)
)
i
.
With tree kernels we can take global structures into
account, while alleviating overfitting with kernel-
based learning methods such as support vector ma-
chines (SVMs) (Vapnik, 1995).
Previous studies (Collins and Duffy, 2001;
Kashima and Koyanagi, 2002) showed that although
it is difficult to explicitly calculate the inner product
in Eq. (1) because we need to consider an exponen-
tial number of possible subtrees, the tree kernels can
be computed in O(|T1||T2|) time by using dynamic
programming (DP) procedures. However, these DP
procedures are time-consuming in practice.
In this paper, we present a method for speeding
up the training with tree kernels. Our target ap-
plication is node relation labeling, which includes
NLP tasks such as semantic role labeling (SRL)
(Gildea and Jurafsky, 2002; Moschitti, 2004; Ha-
cioglu et al, 2004). For this purpose, we designed
kernels on marked labeled ordered trees and derived
O(|T1||T2|) procedures. However, the lengthy train-
ing due to the cost of kernel calculation prevented us
from assessing the performance of these kernels and
motivated us to make the training practically fast.
Our speed-up method is based on the observation
that very few pairs in the training set have a great
many common subtrees (we call such pairs mali-
cious pairs) and most pairs have a very small number
of common subtrees. This leads to a drastic vari-
ance in kernel values, e.g., when W (Si) = 1. We
thus call this property of data unbalanced similarity.
Fast calculation based on the inner product is possi-
ble for non-malicious pairs since we can convert the
trees into vectors in a space of a small subset of all
subtrees. We can speed up the training by using the
DP procedure only for the rare malicious pairs.
We developed the FREQTM algorithm, a modifi-
cation of the FREQT algorithm (Asai et al, 2002),
to detect the malicious pairs and efficiently convert
trees into vectors by enumerating only the subtrees
actually needed (feature subtrees). The experiments
demonstrated that our method makes the training of
SVMs for the SRL task faster by a factor of several
tens, and that it enables the performance of the ker-
nels to be assessed in detail.
137
2 Kernels for Labeled Ordered Trees
The tree kernels proposed so far differ in how sub-
tree inclusion is defined. For instance, Kashima and
Koyanagi (2002) used the following definition.
DEFINITION 2.1 S is included in T iff there exists
a one-to-one function ? from a node of S to a node
of T such that (i) pa(?(ni)) = ?(pa(ni)) (pa(ni)
returns the parent of node ni), (ii) ?(ni) ? ?(nj) iff
ni ? nj (ni ? nj means that ni is an elder sibling
of nj), and (iii) l(?(ni)) = l(ni) (l(ni) returns the
label of ni).
We refer to the tree kernel based on this definition as
Klo. Collins and Duffy (2001) used a more restric-
tive definition where the preservation of CFG pro-
ductions, i.e., nc(?(ni)) = nc(ni) if nc(ni) > 0
(nc(ni) is the number of children of ni), is required
in addition to the requirements in Definition 2.1. We
refer to the tree kernel based on this definition as Kc.
It is pointed that extremely unbalanced kernel val-
ues cause overfitting. Therefore, Collins and Duffy
(2001) used W (Si) = ?(# of productions in Si),
and Kashima and Koyanagi (2002) used W (Si) =
?|Si|, where ? (0 ? ? ? 1) is a factor to alleviate
the unbalance by penalizing large subtrees.
To calculate the tree kernels efficiently, Collins
and Duffy (2001) presented an O(|T1||T2|) DP pro-
cedure for Kc. Kashima and Koyanagi (2002) pre-
sented one for Klo. The point of these procedures is
that Eq. (1) can be transformed:
K(T1, T2) =
?
n1?T1
?
n2?T2
C(n1, n2),
C(n1, n2)?PSi W (Si) ?#Si(T1 M n1) ?#Si(T2 M n2),
where #Si(Tj M nk) is the number of times Si is
included in Tj with ?(root(Si)) = nk. C(n1, n2)
can then be calculated recursively from those of the
children of n1 and n2.
3 Kernels for Marked Labeled Ordered
Trees for Node Relation Labeling
3.1 Node Relation Labeling
The node relation labeling finds relations among
nodes in a tree. Figure 1 illustrates the concept of
node relation labeling with the SRL task as an ex-
ample. A0, A1, and AM-LOC are the semantic roles
 
   
  

      
 	 

   
	    
   
 
   
  
  
Figure 1: Node relation labeling.
 
   
  

      
 	 

   
	    
   
 
   
  
  
 
   
  

      
 	 

   
	    
   
 
   
  
  
Figure 2: Semantic roles encoded by marked labeled
ordered trees.
of the arguments of the verb ?see (saw)?. We repre-
sent an argument by the node that is the highest in
the parse tree among the nodes that exactly cover
the words in the argument. The node for the verb
is determined similarly. For example, the node la-
beled ?PP? represents the AM-LOC argument ?in
the sky?, and the node labeled ?VBD? represents the
verb ?see (saw)?. We assume that there is a two-
node relation labeled with the semantic role (repre-
sented by the arrow in the figure) between the verb
node and the argument node.
3.2 Kernels on Marked Labeled Ordered Trees
We define a marked labeled ordered tree as a labeled
ordered tree in which each node has a mark in ad-
dition to a label. We use m(ni) to denote the mark
of node ni. If ni has no mark, m(ni) returns the
special mark no-mark. We also use the function
marked(ni), which returns true iff m(ni) is not
no-mark. We can encode a k-node relation by using
k distinct marks. Figure 2 shows how the semantic
roles illustrated in Figure 1 can be encoded using
marked labeled ordered trees. We used the mark *1
to represent the verb node and *2 to represent the
argument node.
The node relation labeling task can be posed as
the classification of marked trees that returns +1
when the marks encode the correct relation and ?1
138
Algorithm 3.1: KERNELLOMARK(T1, T2)
(nodes are ordered by the post-order traversal)
for n1 ? 1 to |T1| do
for n2 ? 1 to |T2| do ?????????????(A)8
>>>>>>>>>>>>>>>>>><
>>>>>>>>>>>>>>>>>>:
if lm(n1) 6= lm(n2) then
C(n1, n2) ? 0 Cr(n1, n2) ? 0
else if n1 and n2 are leaf nodes then
C(n1, n2) ? ?
if marked(n1) and marked(n2) then
Cr(n1, n2) ? ? else Cr(n1, n2) ? 0
else
S(0, j) ? 1 S(i, 0) ? 1
if marked(n1) and marked(n2) then
Sr(0, j) ? 1 Sr(i, 0) ? 1
else Sr(0, j) ? 0 Sr(i, 0) ? 0
for i ? 1 to nc(n1) do
for j ? 1 to nc(n2) do
S(i, j) ?
S(i? 1, j) + S(i, j ? 1)? S(i? 1, j ? 1)
+S(i? 1, j ? 1) ? C(chi(n1), chj(n2))
Sr(i, j) ? ??????????(B)
Sr(i? 1, j)+Sr(i, j ? 1)?Sr(i? 1, j ? 1)
+Sr(i? 1, j ? 1) ? C(chi(n1), chj(n2))
+S(i? 1, j ? 1) ? Cr(chi(n1), chj(n2))
?Sr(i? 1, j ? 1) ? Cr(chi(n1), chj(n2))
C(n1, n2) ? ? ? S(nc(n1), nc(n2))
Cr(n1, n2) ? ? ? Sr(nc(n1), nc(n2))
return (P|T1|n1=1
P|T2|
n2=1 C
r(n1, n2))
otherwise. To enable such classification, we need
tree kernels that take into account the node marks.
We thus propose mark-aware tree kernels formu-
lated as follows.
K(T1, T2) =
?
Si:marked(Si)
W (Si)?#Si(T1)?#Si(T2),
where marked(Si) returns true iff marked(ni) =
true for at least one node in tree Si. In these ker-
nels, we require m(?(ni)) = m(ni) in addition to
l(?(ni)) = l(ni) for subtree Si to be regarded as in-
cluded in tree Tj . In other words, these kernels treat
lm(ni) ? (l(ni),m(ni)) as the new label of node
ni and sum only over subtrees that have at least one
marked node. We refer to the marked version of Klo
as Krlo and the marked version of Kc as Krc .
We can derive O(|T1||T2|) DP procedures for the
above kernels as well. Algorithm 3.1 shows the DP
procedure for Krlo, which is derived by extending
the DP procedure for Klo (Kashima and Koyanagi,
2002). The key is the use of Cr(n1, n2), which
stores the sum over only marked subtrees, and its re-
cursive calculation using C(n1, n2) and Cr(n1, n2)
(B). An O(|T1||T2|) procedure for Krc can also be
derived by extending (Collins and Duffy, 2001).
Table 1: Malicious and non-malicious pairs in the
1k data (3,136 trees) used in Sec. 5.2. We used
K(Ti, Tj) = 104 with ? = 1 as the threshold for
maliciousness. (A): pairs (i, i). (B): pairs from the
same sentence except (i, i). (C): pairs from different
sentences. Some malicious pairs are from different
but similar sentences, which are difficult to detect.
Krlo Krc
# pairs avg. K(Ti, Tj) # of pairs avg. K(Ti, Tj)
?
104
(A) 3,121 1.17? 1052 3,052 2.49? 1032
(B) 7,548 7.24? 1048 876 1.26? 1032
(C) 6,510 6.80? 109 28 1.82? 104
<
104
(A) 15 4.19? 103 84 3.06? 103
(B) 4,864 2.90? 102 11,536 1.27? 102
(C) 9,812,438 1.82? 101 9,818,920 1.84? 10?1
4 Fast Training with Tree Kernels
4.1 Basic Idea
As mentioned, we define two types of tree pairs: ma-
licious and non-malicious pairs. Table 1 shows how
these two types of pairs are distributed in an actual
training set. There is a clear distinction between ma-
licious and non-malicious pairs, and we can exploit
this property to speed up the training.
We define subset F = {Fi} (feature subtrees),
which includes only the subtrees that appear as
a common included subtree in the non-malicious
pairs. We convert a tree to feature vector V (Tj) =(?
W (Fi)#Fi(Tj)
)
i
using only F . Then we use a
procedure that chooses the DP procedure or the in-
ner product procedure depending on maliciousness:
K(Ti, Tj)=
{
K(Ti, Tj) (DP) if (i, j) is malicious.
?V (Ti), V (Tj)? otherwise
This procedure returns the same value as the origi-
nal calculation. Naively, if |V (Ti)| (the number of
feature subtrees such that #Fi(Ti) 6= 0) is small
enough, we can expect a speed-up because the cost
of calculating the inner product is O(|V (Ti)| +
|V (Tj)|). However, since |V (Ti)| might increase as
the training set becomes larger, we need a way to
scale the speed-up to large data. In most kernel-
based methods, such as SVMs, we actually need
to calculate the kernel values with all the train-
ing examples for a given example Ti: KS(Ti) =
{K(Ti, T1), . . . ,K(Ti, TL)}, where L is the num-
ber of training examples. Using occurrence pat-
tern OP (Fi) = {(k,#Fi(Tk))|#Fi(Tk) 6= 0} pre-
139
Algorithm 4.1: CALCULATEKS(Ti)
for each F such that #F (Ti) 6= 0 do
for each (j,#F (Tj)) ? OP (F ) do
KS(j) ? KS(j) +W (F ) ?#F (Ti) ?#F (Tj) (A)
for j = 1 to L do
if (i, j) is malicious then KS(j) ? K(Ti, Tj) (DP)
pared beforehand, we can calculate KS(Ti) effi-
ciently (Algorithm 4.1). A similar technique was
used in (Kudo and Matsumoto, 2003a) to speed up
the calculation of inner products.
We can show that the per-pair cost of Algorithm
4.1 is O(c1Q + rmc2|Ti||Tj |), where Q is the av-
erage number of common feature subtrees in a tree
pair, rm is the rate of malicious pairs, c1 and c2 are
the constant factors for vector operations and DP op-
erations. This cost is independent of the number of
training examples. We expect from our observations
that both Q and rm are very small and that c1 ? c2.
4.2 Feature Subtree Enumeration with
Malicious Pair Detection
To detect malicious pairs and enumerate feature sub-
trees F (and to convert each tree to a feature vector),
we developed an algorithm based on the FREQT al-
gorithm (Asai et al, 2002). The FREQT algorithm
can efficiently enumerate subtrees that are included
(Definition 2.1) in more than a pre-specified number
of trees in the training examples by generating can-
didate subtrees using right most expansions (RMEs).
FREQT-based algorithms have recently been used
in methods that treat subtrees as features (Kudo and
Matsumoto, 2004; Kudo and Matsumoto, 2003b).
To develop the algorithm, we made the defini-
tion of maliciousness more search-oriented since it
is costly to check for maliciousness based on the ex-
act number of common subtrees or the kernel values
(i.e., by using the DP procedure for all L2 pairs).
Whatever definition we use, the correctness is pre-
served as long as we do not fail to enumerate the
subtrees that appear in the pairs we consider non-
malicious. First, we consider pairs (i, i) to always
be malicious. Then, we use a FREQT search that
enumerates the subtrees that are included in at least
two trees as a basis. Next, we modify FREQT so that
it stops the search if candidate subtree Fi is too large
(larger than size D, e.g., 20), and we regard the pairs
of the trees where Fi appears as malicious because
having a large subtree in common implies having a
Algorithm 4.2: FREQTM(D,R)
procedure GENERATECANDIDATE(Fi)
for each (j, n) ? occ(Fi) do
for each (Fk, nr) ? RME(Fi, Tj , n) do
S ? S ? {Fk}; occ(Fk) ? occ(Fk) ? (j, nr)
if |occ(Fk)|/|sup(Fi)| > R then
return ((?, false ))????????????(R)
return (({Fk|Fk ? S, |sup(Fk)| ? 2}, true ))
procedure SEARCH(Fi, precheck)
if |Fi| ? D then REGISTERMAL(Fi) return ( false )?(P)
(C, suc) ? GENERATECANDIDATE(Fi)
if not suc then REGISTERMAL(Fi) return ( false )?(S)
for each Fk ? C do
if malicious(Fk) then goto next Fk ?????-(P2)
suc ?SEARCH(Fk, precheck)
if not suc and |sup(Fi)| = |sup(Fk)| then
return ( false )???????????????(P1)
if not precheck and marked(Fi) then
REGISTERSUBTREE(Fi)????????????(F)
return ( true )
main
M? ? (a set of malicious pairs)
F1 ? {Fi||Fi| = 1 and |sup(Fi)| ? 2}
for each Fi ? F1 do SEARCH(Fi, true )?????-(PC)
for each Fi ? F1 do SEARCH(Fi, false )
M?M? {(i, i)|1 ? i ? l}
return (M, {V (Ti)}, {W (fi)})
Table 2: Functions in FREQTM.
? occ(Fi) returns occurrence list of Fi whose element
(j, n) indicates that Fi appears in Tj and that n (of Tj)
is the node added to generated Fi in Tj by the RME (n
works as the position of Fi in Tj).
? sup(Fi) returns the IDs of distinct trees in occ(Fi).
? malicious(Fi) returns true iff all pairs in sup(Fi) are
already registered in the set of malicious pairs, M. (Cur-
rently, this returns false if |sup(Fi)| > M where M is the maximum
support size of the malicious subtrees so far. We will remove this check
since we found that it did not affect efficiency so much.)
? RME(Fi, Tj , n) is a set of subtrees generated by RMEs
of Fi in Tj (permitted when previously expanded node to
generate Fi is n).
possibly exponential number of subtrees of that sub-
tree in common. Although this test is heuristic and
conservative in that it ignores the shape and marks
of a tree, it works fine empirically.
Algorithm 4.2 is our algorithm, which we call
FREQTM. The differences from FREQT are under-
lined. Table 2 summarizes the functions used. To
make the search efficient, pruning is performed as
follows (see also Figure 3). The basic idea behind is
that if malicious(Fi) is true then malicious(Fk)
is also true for Fk that is expanded from Fi by an
140
RME since sup(Fk) ? sup(Fi). This means we do
not need to enumerate Fi nor any descendant of Fi.
? (P) Once |Fi| ? D and the malicious pairs are
registered, we stop searching further.
? (P1) If the search from Fk (expanded from Fi)
found a malicious subtree and if |sup(Fi)| =
|sup(Fk)|, we stop the search from any other
subtree Fm (expanded from Fi) since we can
prove that malicious(Fm) = true without ac-
tually testing it (proof omitted).
? (P2) If malicious(Fk) = true, we prune
the search from Fk. To prune even when
malicious(Fk) becomes true as a result of
succeeding searches, we first run a search only
for detecting malicious pairs (see (PC)).
? (S) We stop searching when the occurrence
list becomes too long (larger than threshold R)
since it causes a severe search slowdown.
Note that we use a depth-first version of FREQT as
a basis to first find the largest subtrees and to detect
malicious pairs at early points in the search. Enu-
meration of unnecessary subtrees is avoided because
the registration of subtrees is performed at the post-
order position (F). The conversion to vectors is per-
formed by assigning an ID to subtree Fi when regis-
tering it at (F) and distributing the ID to all the exam-
ples in occ(Fi). Finally, D should be large enough
to make rm sufficiently small but should not be so
large that too many feature subtrees are enumerated.
We expect that the cost of FREQTM is offset by
the faster training, especially when training on the
same data is repeatedly performed as in the tuning
of hyperparameters.
For Krc , we use a similar search procedure. How-
ever, the RME is modified so that all the children of
a CFG production are expanded at once. Although
the modification is not trivial, we omit the explana-
tion due to space limitations.
4.3 Feature Compression
Additionally, we use a simple but effective feature
compression technique to boost speed-up. The idea
is simple: feature subtrees Fi and Fj can be treated
as one feature fk, with weight W (fk) = W (Fi) +
W (Fj) if OP (Fi) = OP (Fj). This drastically re-
duces the number of features. Although this is sim-
sup = {1, 2, 3, 4}sup = {2, 3} (2, 3) /?M
(1, 2) (1, 3) (2, 3)
{1, 2, 3}
{1, 2, 3}
{1, 2, 3}
{1, 3} {2, 4}
> D
  
Figure 3: Pruning in FREQTM.
ilar to finding closed and maximal subtrees (Chi et
al., 2004), it is easy to implement since we need only
the occurrence pattern, OP (Fi), which is easily ob-
tained from occ(Fi) in the FREQTM search.
4.4 Alternative Methods
Vishwanathan and Smola (2004) presented the
O(|T1| + |T2|) procedure that exploits suffix trees
to speed up the calculation of tree kernels. However,
it can be applied to only a few types of subtrees that
can be represented as a contiguous part in a string
representation of a tree. Therefore, neither Krlo nor
Krc can be sped up by using this procedure.
Another method is to change an inner loop, such
as (B) in Algorithm 3.1, so that it iterates only over
nodes in T2 that have l(n1). We use this as the base-
line for comparison, since we found that this is about
two times faster than the standard implementation. 1
4.5 Remaining Problem
Note that the method described here cannot speed up
classification, since the converted vectors are valid
only for calculating the kernels between trees in the
training set. However, when we classify the same
trees repeatedly, we can convert the trees in the train-
ing set and the classified trees at the same time and
use the obtained vectors for classification.
5 Evaluation
We first evaluated the speed-up by our method for
the semantic role labeling (SRL) task. We then
demonstrated that the speed-up method enables a de-
tailed comparison of Krlo and Krc for the SRL task.
1For Krc , it might be possible to speed up comparisons in
the algorithm by assigning IDs for CFG rules. We leave this for
future work since it complicates implementation.
141
Table 3: Conversion statistics and speed-up for semantic role A2.
Krlo Krc
size (# positive examples) 1,000 2,000 4,000 8,000 12,000 1,000 2,000 4,000 8,000 12,000
# examples 3,136 6,246 12,521 25,034 34,632 3,136 6,246 12,521 25,034 34,632
# feature subtrees (?104) 804.4 2,427.3 6,542.9 16,750.1 26,146. 5 3.473 9.009 21.867 52.179 78.440
# features (compressed) (?104) 20.7 67.3 207.2 585.9 977.0 0.580 1.437 3.426 8.128 12.001
avg. |V | (compressed) 468.0 866.5 1,517.3 2,460.5 3,278.3 10.5 14.0 17.9 23.1 25.9
rate of malicious pairs rm (%) 0.845 0.711 0.598 0.575 1.24 0.161 0.0891 0.0541 0.0370 0.0361
conversion time (sec.) 208.0 629.2 1,921.1 6,519.8 14,824.9 3.8 8.7 20.4 46.5 70.4
SVM time (DP+lookup) (sec.) 487.9 1,716.2 4,526.4 79,800.7 92,542.2 360.7 1,263.5 5,893.3 53,055.5 47,089.2
SVM time (proposed) (sec.) 17.5 68.6 186.4 1,721.7 2,531.8 4.9 25.7 119.5 982.8 699.1
speed-up factor 27.8 25.0 24.3 46.4 36.6 73.3 49.1 49.3 53.98 67.35
5.1 Setting
We used the data set provided for the CoNLL05 SRL
shared task (Carreras and Ma`rquez, 2005). We used
only the training part and divided it into our training,
development, and testing sets (23,899, 7,966, and
7,967 sentences, respectively). As the tree structure,
we used the output of Collins? parser (with WSJ-
style non-terminals) provided with the data set. We
also used POS tags by inserting the nodes labeled by
POS tags above the word nodes. The average num-
ber of nodes in a tree was about 82. We ignored any
arguments (and verbs) that did not match any node
in the tree (the rate of such cases was about 3.5%). 2
The words were lowercased.
We used TinySVM3 as the implementation of
SVM and added our tree kernels, Krlo and Krc . We
implemented FREQTM based on the implementa-
tion of FREQT by Kudo.4 We normalized the kernel
values: K(Ti, Tj)/
?K(Ti, Ti)?K(Tj , Tj). Note
that this normalization barely affected the training
time since we can calculate K(Ti, Ti) beforehand.
We assumed two-step labeling where we first find
the argument node and then we determine the label
by using a binary classifier for each semantic role. In
this experiment, we focused on the performance for
the classifiers in the latter step. We used the marked
labeled ordered tree that encoded the target role as
a positive example and the trees that encoded other
roles of the verb in the same sentence as negative
examples. We trained and evaluated the classifiers
using the examples generated as above. 5
2This was caused by parse errors, which can be solved by us-
ing more accurate parsers, and by bracketing inconsistencies be-
tween parser outputs and SRL annotations (e.g., phrasal verbs),
many of which can be avoided by using heuristic transformers.
3http://chasen.org/?taku/software/TinySVM
4http://chasen.org/?taku/software/freqt
5The evaluation is slightly easier since the classifier for role
5.2 Training Speed-up
We calculated the statistics for the conversion by
FREQTM and measured the speed-up in SVM train-
ing for semantic role A2, for various numbers of
training examples. For FREQTM, we used D = 20
and R = 20. For SVM training, we used conver-
gence tolerance 0.001 (-e option in TinySVM), soft
margin cost C = 1.0 ? 103 (-c), maximum num-
ber of iterations 105, kernel cache size 512 MB (-
m), and decay factor ? = 0.2 for the weight of
each subtree. We compared the time with our fast
method (Algorithm 4.1) with that with the DP pro-
cedure with the node lookup described in Section
4.4. Note that these two methods yield almost iden-
tical SVM models (there are very slight differences
due to the numerical computation). The time was
measured using a computer with 2.4-GHz Opterons.
Table 3 shows the results for Krlo and Krc . The
proposed method made the SVM training substan-
tially faster for both Krlo and Krc . As we expected,
the speed-up factor did not decrease even though |V |
increased as the amount of data increased. Note
that FREQTM actually detected non-trivial mali-
cious pairs such as those from very similar sentences
in addition to trivial ones, e.g., (i, i). FREQTM con-
version was much faster and the converted feature
vectors were much shorter for Krc , presumably be-
cause Krc restricts the subtrees more.
The compression technique described in Section
4.3 greatly reduced the number of features. Without
this compression, the storage requirement would be
impractical. It also boosted the speed-up. For ex-
ample, the training time with Krlo for the size 1,000
data in Table 3 was 86.32 seconds without compres-
sion. This means that the compression boosted the
X is evaluated only on the examples generated from the sen-
tences that contain a verb that has X as a role.
142
100
101
102
103
104
105
103 104
Time
 (sec.)
Number of examples
conversionSVM (DP+lookup)SVM (proposed)
100
101
102
103
104
105
103 104
Time
 (sec.)
Number of examples
conversionSVM (DP+lookup)SVM (proposed)
Figure 4: Scaling of conversion time and SVM train-
ing time. Left: Krlo. Right: Krc
 0 2
 4 6
 8 10
 12 14
 5  10  15  20  25  30  0
 0.2 0.4
 0.6 0.8
 1
Time 
(? 103  s
ec.)
Malic
ious P
air Ra
te (r m)
D
conversionSVM (proposed)rm
 0 0.2
 0.4 0.6
 0.8 1
 5  10  15  20  25  30  0
 0.2 0.4
 0.6 0.8
 1
Time 
( ? 103  
sec.
)
Malic
ious P
air Ra
te (r m)
D
conversionSVM (proposed)rm
Figure 5: Relation between D and conversion time,
SVM training time, and rm. Left: Krlo. Right: Krc
speed-up by a factor of more than 5.
The cost of FREQTM is much smaller than that
of SVM training with DP. Therefore, our method is
beneficial even if we train the SVM only once.
To see how our method scales to large amounts
of data, we plotted the time for the conversion and
the SVM training w.r.t. data size on a log-log scale.
As shown in Figure 4, the scaling factor was about
1.7 for the conversion time, 2.1 for SVM training
with DP, and 2.0 for the proposed SVM training for
Krlo. For Krc , the factors were about 1.3, 2.1, and
2.0, respectively. Regardless of the method, the cost
of SVM training was about O(L2), as reported in
the literature. Although FREQTM also has a super-
linear cost, it is smaller than that of SVM training.
Therefore, the cost of SVM training will become a
problem before the cost of FREQTM does.
As we mentioned, the choice of D is a trade-off.
Figure 5 shows the relationships between D and the
time of conversion by FREQTM, the time of SVM
training using the converted vectors, and the rate of
malicious pairs, rm. We can see that the choice of D
is more important in the case of Klo and that D = 20
used in our evaluation is not a bad choice.
5.3 Semantic Role Labeling
We assessed the performance of Krlo and Krc for se-
mantic roles A1, A2, AM-ADV, and AM-LOC us-
ing our fast training method. We tuned soft mar-
gin cost C and ? by using the development set (we
used the technique described in Section 4.5 to en-
able fast classification of the development set). We
experimented with two training set sizes (4,000 and
8,000). For each ? (0.1, 0.15, 0.2, 0.25, and 0.30),
we tested 40 different values of C (C ? [2 . . . 103]
for size 4,000 and C ? [0.5 . . . 103] for size 8,000),
and we evaluated the accuracy of the best setting for
the test set.6 Fast training is crucial since the per-
formance differs substantially depending on the val-
ues of these hyperparameters. Table 4 shows the re-
sults. The accuracies are shown by F1. We can see
that Krlo outperformed Krc in all cases, presumably
because Krc allows only too restrictive subtrees and
therefore causes data sparseness. In addition, as one
would expect, larger training sets are beneficial.
6 Discussion
The proposed speed-up method can also be applied
to labeled ordered trees (e.g., for parse selection).
However, the speed-up might be smaller since with-
out node marks the number of subtrees increases
while the DP procedure becomes simpler. On the
other hand, the FREQTM conversion for marked la-
beled ordered trees might be made faster by exploit-
ing the mark information for pruning. Although our
method is not a complete solution in a classification
setting, it might be in a clustering setting (in a sense
it is training only). However, it is an open question
whether unbalanced similarity, which is the key to
our speed-up, is ubiquitous in NLP tasks and under
what conditions our method scales better than the
SVMs or other kernel-based methods.
Several studies claim that learning using tree ker-
nels and other convolution kernels tends to overfit
and propose selecting or restricting features (Cumby
and Roth, 2003; Suzuki et al, 2004; Kudo and Mat-
sumoto, 2004). Sometimes, the classification be-
comes faster as a result (Suzuki et al, 2004; Kudo
and Matsumoto, 2004). We do not disagree with
these studies. The fact that small ? values resulted in
the highest accuracy in our experiment implies that
too large subtrees are not so useful. However, since
this tendency depends on the task, we need to assess
the performance of full tree kernels for comparison.
In this sense, our method is of great importance.
Node relation labeling is a generalization of node
6We used 106 as the maximum number of iterations.
143
Table 4: Comparison between Krlo and Krc .
training set size = 4,000 training set size = 8,000
best setting F1 (dev) F1 (test) best setting F1 (dev) F1 (test)
A1 K
r
lo ? = 0.2, C = 13.95 87.89 87.90 ? = 0.25, C = 8.647 89.80 89.81
Krc ? = 0.15, C = 3.947 85.36 85.56 ? = 0.2, C = 17.63 87.93 87.96
A2 K
r
lo ? = 0.20, C = 13.95 85.65 84.70 ? = 0.20, C = 57.82 87.94 87.26
Krc ? = 0.10, C = 7.788 84.79 83.51 ? = 0.15, C = 1.0? 103 87.37 86.23
AM-ADV K
r
lo ? = 0.25, C = 8.647 86.20 86.64 ? = 0.15, C = 45.60 86.91 87.01
Krc ? = 0.20, C = 3.344 83.58 83.72 ? = 0.20, C = 2.371 84.34 84.08
AM-LOC K
r
lo ? = 0.15, C = 20.57 91.11 92.92 N/A
Krc ? = 0.15, C = 13.95 89.59 91.32 AM-LOC does not have more than 4,000 positive examples.
marking where we determine the mark (tag) of a
node. Kashima and Koyanagi (2002) dealt with this
task by inserting the node representing the mark
above the node to be tagged and classifying the
transformed tree using SVMs with tree kernels such
as Klo. For the SRL task, Moschitti (2004) applied
the tree kernel (Kc) to tree fragments that are heuris-
tically extracted to reflect the role of interest. For re-
lation extraction, Culotta and Sorensen (2004) pro-
posed a tree kernel that operates on only the smallest
tree fragment including two entities for which a re-
lation is assigned. Our kernels on marked labeled
ordered trees differ in what subtrees are permitted.
Although comparisons are needed, we think our ker-
nels are intuitive and general.
There are many possible structures for which tree
kernels can be defined. Shen et al (2003) proposed
a tree kernel for LTAG derivation trees to focus only
on linguistically meaningful structures. Culotta and
Sorensen (2004) proposed a tree kernel for depen-
dency trees. An important future task is to find suit-
able structures for each task (the SRL task in our
case). Our speed-up method will be beneficial as
long as there is unbalanced similarity.
7 Conclusion
We have presented a method for speeding up the
training with tree kernels. Using the SRL task, we
demonstrated that our speed-up method made the
training substantially faster.
References
T. Asai, K. Abe, S. Kawasoe, H. Arimura, H. Sakamoto,
and S. Arikawa. 2002. Efficient substructure discov-
ery from large semi-structured data. In SIAM SDM?02.
X. Carreras and L. Ma`rquez. 2005. Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In
CoNLL 2005.
Y. Chi, Y. Yang, Y. Xia, and R. R. Muntz. 2004.
CMTreeMiner: Mining both closed and maximal fre-
quent subtrees. In PAKDD 2004.
M. Collins and N. Duffy. 2001. Convolution kernels for
natural language. In NIPS 2001.
A. Culotta and J. Sorensen. 2004. Dependency tree ker-
nels for relation extraction. In ACL 2004.
C. Cumby and D. Roth. 2003. On kernel methods for
relational learning. In ICML 2003.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3).
K. Hacioglu, S. Pradhan, W. Ward, J. H. Martin, and
D. Jurafsky. 2004. Semantic role labeling by tagging
syntactic chunks. In CoNLL 2004.
H. Kashima and T. Koyanagi. 2002. Kernels for semi-
structured data. In ICML 2002, pages 291?298.
T. Kudo and Y. Matsumoto. 2003a. Fast methods for
kernel-based text analysis. In ACL 2003.
T. Kudo and Y. Matsumoto. 2003b. Subtree-based
Markov random fields and its application to natural
language analysis (in Japanese). IPSJ, NL-157.
T. Kudo and Y. Matsumoto. 2004. A boosting algorithm
for classification of semi-structured text. In EMNLP
2004, pages 301?308.
A. Moschitti. 2004. A study on convolution kernels for
shallow semantic parsing. In ACL 2004.
L. Shen, A. Sarkar, and A. K. Joshi. 2003. Using LTAG
based features in parse reranking. In EMNLP 2003.
J. Suzuki, H. Isozaki, and E. Maeda. 2004. Convolu-
tion kernels with feature selection for natural language
processing tasks. In ACL 2004, pages 119?126.
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer Verlag.
S. V. N. Vishwanathan and A. J. Smola. 2004. Fast ker-
nels for string and tree matching. Kernels and Bioin-
formatics.
144
Proceedings of ACL-08: HLT, pages 407?415,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Inducing Gazetteers for Named Entity Recognition
by Large-scale Clustering of Dependency Relations
Jun?ichi Kazama
Japan Advanced Institute of
Science and Technology (JAIST),
Asahidai 1-1, Nomi,
Ishikawa, 923-1292 Japan
kazama@jaist.ac.jp
Kentaro Torisawa
National Institute of Information and
Communications Technology (NICT),
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto, 619-0289 Japan
torisawa@nict.go.jp
Abstract
We propose using large-scale clustering of de-
pendency relations between verbs and multi-
word nouns (MNs) to construct a gazetteer for
named entity recognition (NER). Since depen-
dency relations capture the semantics of MNs
well, the MN clusters constructed by using
dependency relations should serve as a good
gazetteer. However, the high level of computa-
tional cost has prevented the use of clustering
for constructing gazetteers. We parallelized
a clustering algorithm based on expectation-
maximization (EM) and thus enabled the con-
struction of large-scale MN clusters. We
demonstrated with the IREX dataset for the
Japanese NER that using the constructed clus-
ters as a gazetteer (cluster gazetteer) is a effec-
tive way of improving the accuracy of NER.
Moreover, we demonstrate that the combina-
tion of the cluster gazetteer and a gazetteer ex-
tracted from Wikipedia, which is also useful
for NER, can further improve the accuracy in
several cases.
1 Introduction
Gazetteers, or entity dictionaries, are important for
performing named entity recognition (NER) accu-
rately. Since building and maintaining high-quality
gazetteers by hand is very expensive, many meth-
ods have been proposed for automatic extraction of
gazetteers from texts (Riloff and Jones, 1999; The-
len and Riloff, 2002; Etzioni et al, 2005; Shinzato et
al., 2006; Talukdar et al, 2006; Nadeau et al, 2006).
Most studies using gazetteers for NER are based
on the assumption that a gazetteer is a mapping
from a multi-word noun (MN)1 to named en-
tity categories such as ?Tokyo Stock Exchange ?
{ORGANIZATION}?.2 However, since the corre-
spondence between the labels and the NE categories
can be learned by tagging models, a gazetteer will be
useful as long as it returns consistent labels even if
those returned are not the NE categories. By chang-
ing the perspective in such a way, we can explore
more broad classes of gazetteers. For example, we
can use automatically extracted hyponymy relations
(Hearst, 1992; Shinzato and Torisawa, 2004), or au-
tomatically induced MN clusters (Rooth et al, 1999;
Torisawa, 2001).
For instance, Kazama and Torisawa (2007) used
the hyponymy relations extracted from Wikipedia
for the English NER, and reported improved accu-
racies with such a gazetteer.
We focused on the automatically induced clus-
ters of multi-word nouns (MNs) as the source of
gazetteers. We call the constructed gazetteers clus-
ter gazetteers. In the context of tagging, there are
several studies that utilized word clusters to prevent
the data sparseness problem (Kazama et al, 2001;
Miller et al, 2004). However, these methods cannot
produce the MN clusters required for constructing
gazetteers. In addition, the clustering methods used,
such as HMMs and Brown?s algorithm (Brown et
al., 1992), seem unable to adequately capture the se-
mantics of MNs since they are based only on the
information of adjacent words. We utilized richer
1We used the term, ?multi-word?, to emphasize that a
gazetteer includes not only one-word expressions but also
multi-word expressions.
2Although several categories can be associated in general,
we assume that only one category is associated.
407
syntactic/semantic structures, i.e., verb-MN depen-
dencies to make clean MN clusters. Rooth et al
(1999) and Torisawa (2001) showed that the EM-
based clustering using verb-MN dependencies can
produce semantically clean MN clusters. However,
the clustering algorithms, especially the EM-based
algorithms, are computationally expensive. There-
fore, performing the clustering with a vocabulary
that is large enough to cover the many named entities
required to improve the accuracy of NER is difficult.
We enabled such large-scale clustering by paralleliz-
ing the clustering algorithm, and we demonstrate the
usefulness of the gazetteer constructed.
We parallelized the algorithm of (Torisawa, 2001)
using the Message Passing Interface (MPI), with the
prime goal being to distribute parameters and thus
enable clustering with a large vocabulary. Apply-
ing the parallelized clustering to a large set of de-
pendencies collected from Web documents enabled
us to construct gazetteers with up to 500,000 entries
and 3,000 classes.
In our experiments, we used the IREX dataset
(Sekine and Isahara, 2000) to demonstrate the use-
fulness of cluster gazetteers. We also compared
the cluster gazetteers with the Wikipedia gazetteer
constructed by following the method of (Kazama
and Torisawa, 2007). The improvement was larger
for the cluster gazetteer than for the Wikipedia
gazetteer. We also investigated whether these
gazetteers improve the accuracies further when they
are used in combination. The experimental results
indicated that the accuracy improved further in sev-
eral cases and showed that these gazetteers comple-
ment each other.
The paper is organized as follows. In Section 2,
we explain the construction of cluster gazetteers and
its parallelization, along with a brief explanation of
the construction of the Wikipedia gazetteer. In Sec-
tion 3, we explain how to use these gazetteers as fea-
tures in an NE tagger. Our experimental results are
reported in Section 4.
2 Gazetteer Induction
2.1 Induction by MN Clustering
Assume we have a probabilistic model of a multi-
word noun (MN) and its class: p(n, c) =
p(n|c)p(c), where n ? N is an MN and c ? C is a
class. We can use this model to construct a gazetteer
in several ways. The method we used in this study
constructs a gazetteer: n ? argmax
c
p(c|n). This
computation can be re-written by the Bayes rule as
argmax
c
p(n|c)p(c) using p(n|c) and p(c).
Note that we do not exclude non-NEs when we
construct the gazetteer. We expect that tagging
models (CRFs in our case) can learn an appropri-
ate weight for each gazetteer match regardless of
whether it is an NE or not.
2.2 EM-based Clustering using Dependency
Relations
To learn p(n|c) and p(c) for Japanese, we use the
EM-based clustering method presented by Torisawa
(2001). This method assumes a probabilistic model
of verb-MN dependencies with hidden semantic
classes:3
p(v, r, n) =
?
c
p(?v, r?|c)p(n|c)p(c), (1)
where v ? V is a verb and n ? N is an MN that
depends on verb v with relation r. A relation, r,
is represented by Japanese postpositions attached to
n. For example, from the following Japanese sen-
tence, we extract the following dependency: v =
?? (drink), r = ? (?wo? postposition), n =
??? (beer).
??? (beer)? (wo)?? (drink) (? drink beer)
In the following, we let vt ? ?v, r? ? VT for the
simplicity of explanation.
To be precise, we attach various auxiliary verb
suffixes, such as ??? (reru)?, which is for pas-
sivization, into v, since these greatly change the type
of n in the dependent position. In addition, we also
treated the MN-MN expressions, ?MN1 ? MN2?
(? ?MN2 of MN1?), as dependencies v = MN2,
r = ?, n = MN1, since these expressions also
characterize the dependent MNs well.
Given L training examples of verb-MN depen-
dencies {(vti, ni, fi)}Li=1, where fi is the number
of dependency (vti, ni) in a corpus, the EM-based
clustering tries to find p(vt|c), p(n|c), and p(c) that
maximize the (log)-likelihood of the training exam-
ples:
LL(p) =
?
i
fi log(
?
c
p(vti|c)p(ni|c)p(c)). (2)
3This formulation is based on the formulation presented in
Rooth et al (1999) for English.
408
We iteratively update the probabilities using the EM
algorithm. For the update procedures used, see Tori-
sawa (2001).
The corpus we used for collecting dependencies
was a large set (76 million) of Web documents,
that were processed by a dependency parser, KNP
(Kurohashi and Kawahara, 2005).4 From this cor-
pus, we extracted about 380 million dependencies
of the form {(vti, ni, fi)}Li .
2.3 Parallelization for Large-scale Data
The disadvantage of the clustering algorithm de-
scribed above is the computational costs. The space
requirements are O(|VT ||C|+ |N ||C|+ |C|) for stor-
ing the parameters, p(vt|c), p(n|c), and p(c)5, plus
O(L) for storing the training examples. The time
complexity is mainly O(L ? |C| ? I), where I is
the number of update iterations. The space require-
ments are the main limiting factor. Assume that a
floating-point number consumes 8 bytes. With the
setting, |N | = 500, 000, |VT | = 500, 000, and
|C| = 3, 000, the algorithm requires more than 44
GB for the parameters and 4 GB of memory for the
training examples. A machine with more than 48
GB of memory is not widely available even today.
Therefore, we parallelized the clustering algo-
rithm, to make it suitable for running on a cluster
of PCs with a moderate amount of memory (e.g., 8
GB). First, we decided to store the training examples
on a file since otherwise each node would need to
store all the examples when we use the data splitting
described below, and having every node consume 4
GB of memory is memory-consuming. Since the ac-
cess to the training data is sequential, this does not
slow down the execution when we use a buffering
technique appropriately.6
We then split the matrix for the model parameters,
p(n|c) and p(vt|c), along with the class coordinate.
That is, each cluster node is responsible for storing
only a part of classes Cl, i.e., 1/|P | of the parame-
ter matrix, where P is the number of cluster nodes.
This data splitting enables linear scalability of mem-
ory sizes. However, doing so complicates the update
procedure and, in terms of execution speed, may
4Acknowledgements: This corpus was provided by Dr.
Daisuke Kawahara of NICT.
5To be precise, we need two copies of these.
6Each node has a copy of the training data on a local disk.
Algorithm 2.1: Compute p(cl|vti, ni)
localZ = 0, Z = 0
for cl ? Cl do
?
?
?
d = p(vti|c)p(ni|c)p(c)
p(cl|vti, ni) = d
localZ += d
MPI Allreduce( localZ, Z, 1, MPI DOUBLE,
MPI SUM, MPI COMM WORLD)
for cl ? Cl do p(cl|vti, ni) /= Z
Figure 1: Parallelized inner-most routine of EM cluster-
ing algorithm. Each node executes this code in parallel.
offset the advantage of parallelization because each
node needs to receive information about the classes
that are not on the node in the inner-most routine of
the update procedure.
The inner-most routine should compute:
p(c|vti, ni) = p(vti|c)p(ni|c)p(c)/Z, (3)
for each class c, where Z =
?
c p(vti|c)p(ni|c)p(c)
is a normalizing constant. However, Z cannot be
calculated without knowing the results of other clus-
ter nodes. Thus, if we use MPI for parallelization,
the parallelized version of this routine should re-
semble the algorithm shown in Figure 1. This rou-
tine first computes p(vti|cl)p(ni|cl)p(cl) for each
cl ? Cl, and stores the sum of these values as localZ.
The routine uses an MPI function, MPI Allreduce,
to sum up localZ of the all cluster nodes and to
set Z with the resulting sum. We can compute
p(cl|vti, ni) by using this Z to normalize the value.
Although the above is the essence of our paralleliza-
tion, invoking MPI Allreduce in the inner-most loop
is very expensive because the communication setup
is not so cheap. Therefore, our implementation cal-
culates p(cl|vti, ni) in batches of B examples and
calls MPI Allreduce at every B examples.7 We used
a value of B = 4, 096 in this study.
By using this parallelization, we successfully per-
formed the clustering with |N | = 500, 000, |VT | =
500, 000, |C| = 3, 000, and I = 150, on 8 clus-
ter nodes with a 2.6 GHz Opteron processor and 8
GB of memory. This clustering took about a week.
To our knowledge, no one else has performed EM-
based clustering of this type on this scale. The re-
sulting MN clusters are shown in Figure 2. In terms
of speed, our experiments are still at a preliminary
7MPI Allreduce can also take array arguments and apply the
operation to each element of the array in one call.
409
Class 791 Class 2760
??? ? ? ? ?
(WINDOM)
???/????? ?????
(Chiba Marine Stadium [abb.])
? ? ? ? ? ? ?
(CAMRY)
??/??? ????????
(Osaka Dome)
??? ? ? ? ?
(DIAMANTE)
??/? ????????
(Nagoya Dome [abb.])
? ??? ? ? ?
(ODYSSEY)
??/????????? ??
(Fukuoka Dome)
????????
(INSPIRE)
??/?? ?????????
(Osaka Stadium)
? ? ? ? ? ?
(SWIFT)
??/???????????
(Yokohama Stadium [abb.])
Figure 2: Clean MN clusters with named entity entries
(Left: car brand names. Right: stadium names). Names
are sorted on the basis of p(c|n). Stadium names are
examples of multi-word nouns (word boundaries are in-
dicated by ?/?) and also include abbreviated expressions
(marked by [abb.]) .
stage. We have observed 5 times faster execution,
when using 8 cluster nodes with a relatively small
setting, |N | = |VT | = 50, 000, |C| = 2, 000.
2.4 Induction from Wikipedia
Defining sentences in a dictionary or an encyclope-
dia have long been used as a source of hyponymy re-
lations (Tsurumaru et al, 1991; Herbelot and Copes-
take, 2006).
Kazama and Torisawa (2007) extracted hy-
ponymy relations from the first sentences (i.e., defin-
ing sentences) of Wikipedia articles and then used
them as a gazetteer for NER. We used this method
to construct the Wikipedia gazetteer.
The method described by Kazama and Torisawa
(2007) is to first extract the first (base) noun phrase
after the first ?is?, ?was?, ?are?, or ?were? in the first
sentence of a Wikipedia article. The last word in the
noun phase is then extracted and becomes the hyper-
nym of the entity described by the article. For exam-
ple, from the following defining sentence, it extracts
?guitarist? as the hypernym for ?Jimi Hendrix?.
Jimi Hendrix (November 27, 1942) was an Ameri-
can guitarist, singer and songwriter.
The second noun phrase is used when the first noun
phrase ends with ?one?, ?kind?, ?sort?, or ?type?,
or it ended with ?name? followed by ?of?. This
rule is for treating expressions like ?... is one of
the landlocked countries.? By applying this method
of extraction to all the articles in Wikipedia, we
# instances
page titles processed 550,832
articles found 547,779
(found by redirection) (189,222)
first sentences found 545,577
hypernyms extracted 482,599
Table 1: Wikipedia gazetteer extraction
construct a gazetteer that maps an MN (a title of a
Wikipedia article) to its hypernym.8 When the hy-
pernym extraction failed, a special hypernym sym-
bol, e.g., ?UNK?, was used.
We modified this method for Japanese. After pre-
processing the first sentence of an article using a
morphological analyzer, MeCab9, we extracted the
last noun after the appearance of Japanese postpo-
sition ?? (wa)? (? ?is?). As in the English case,
we also refrained from extracting expressions corre-
sponding to ?one of? and so on.
From the Japanese Wikipedia entries of April
10, 2007, we extracted 550,832 gazetteer entries
(482,599 entries have hypernyms other than UNK).
Various statistics for this extraction are shown in
Table 1. The number of distinct hypernyms in
the gazetteer was 12,786. Although this Wikipedia
gazetteer is much smaller than the English version
used by Kazama and Torisawa (2007) that has over
2,000,000 entries, it is the largest gazetteer that can
be freely used for Japanese NER. Our experimen-
tal results show that this Wikipedia gazetteer can be
used to improve the accuracy of Japanese NER.
3 Using Gazetteers as Features of NER
Since Japanese has no spaces between words, there
are several choices for the token unit used in NER.
Asahara and Motsumoto (2003) proposed using
characters instead of morphemes as the unit to alle-
viate the effect of segmentation errors in morpholog-
ical analysis and we also used their character-based
method. The NER task is then treated as a tagging
task, which assigns IOB tags to each character in
a sentence.10 We use Conditional Random Fields
(CRFs) (Lafferty et al, 2001) to perform this tag-
ging.
The information of a gazetteer is incorporated
8They handled ?redirections? as well by following redirec-
tion links and extracting a hypernym from the article reached.
9http://mecab.sourceforge.net
10Precisely, we use IOB2 tags.
410
ch ? ? ? ? ? ? ? ? ? ?
match O B I I O O O ? ? ?
(w/ class) O B-?? I-?? I-?? O O O ? ? ?
Figure 3: Gazetteer features for Japanese NER. Here, ??
??? means ?SONY?, ???? means ?company?, and ?
??? means ?to develop?.
as features in a CRF-based NE tagger. We follow
the method used by Kazama and Torisawa (2007),
which encodes the matching with a gazetteer entity
using IOB tags, with the modification for Japanese.
They describe using two types of gazetteer features.
The first is a matching-only feature, which uses
bare IOB tags to encode only matching information.
The second uses IOB tags that are augmented with
classes (e.g., B-country and I-country).11 When
there are several possibilities for making a match,
the left-most longest match is selected. The small
differences from their work are: (1) We used char-
acters as the unit as we described above, (2) While
Kazama and Torisawa (2007) checked only the word
sequences that start with a capitalized word and thus
exploited the characteristics of English language, we
checked the matching at every character, (3) We
used a TRIE to make the look-up efficient.
The output of gazetteer features for Japanese NER
are thus as those shown in Figure 3. These annotated
IOB tags can be used in the same way as other fea-
tures in a CRF tagger.
4 Experiments
4.1 Data
We used the CRL NE dataset provided in the
IREX competition (Sekine and Isahara, 2000). In
the dataset, 1,174 newspaper articles are annotated
with 8 NE categories: ARTIFACT, DATE, LO-
CATION, MONEY, ORGANIZATION, PERCENT,
PERSON, and TIME.12 We converted the data into
the CoNLL 2003 format, i.e., each row corresponds
to a character in this case. We obtained 11,892 sen-
tences13 with 18,677 named entities. We split this
data into the training set (9,000 sentences), the de-
11Here, we call the value returned by a gazetteer a ?class?.
Features are not output when the returned class is UNK in the
case of the Wikipedia gazetteer. We did not observe any signif-
icant change if we also used UNK.
12We ignored OPTIONAL category.
13This number includes the number of -DOCSTART- tokens
in CoNLL 2003 format.
Name Description
ch character itself
ct character type: uppercase alphabet, lower-
case alphabet, katakana, hiragana, Chinese
characters, numbers, numbers in Chinese
characters, and spaces
m mo bare IOB tag indicating boundaries of mor-
phemes
m mm IOB tag augmented by morpheme string,
indicating boundaries and morphemes
m mp IOB tag augmented by morpheme type, in-
dicating boundaries and morpheme types
(POSs)
bm bare IOB tag indicating ?bunsetsu? bound-
aries (Bunsetsu is a basic unit in Japanese
and usually contains content words fol-
lowed by function words such as postpo-
sitions)
bi bunsetsu-inner feature. See (Nakano and
Hirai, 2004).
bp adjacent-bunsetsu feature. See (Nakano
and Hirai, 2004).
bh head-of-bunsetsu features. See (Nakano
and Hirai, 2004).
Table 2: Atomic features used in baseline model.
velopment set (1,446 sentences), and the testing set
(1,446 sentences).
4.2 Baseline Model
We extracted the atomic features listed in Table 2
at each character for our baseline model. Though
there may be slight differences, these features are
based on the standard ones proposed and used in
previous studies on Japanese NER such as those by
Asahara and Motsumoto (2003), Nakano and Hirai
(2004), and Yamada (2007). We used MeCab as a
morphological analyzer and CaboCha14 (Kudo and
Matsumoto, 2002) as the dependency parser to find
the boundaries of the bunsetsu. We generated the
node and the edge features of a CRF model as de-
scribed in Table 3 using these atomic features.
4.3 Training
To train CRF models, we used Taku Kudo?s CRF++
(ver. 0.44) 15 with some modifications.16 We
14http://chasen.org/?taku/software/
CaboCha
15http://chasen.org/?taku/software/CRF++
16We implemented scaling, which is similar to that for
HMMs (Rabiner, 1989), in the forward-backward phase and re-
placed the optimization module in the original package with the
411
Node features:
{??, x?2, x?1, x0, x+1, x+2} ? y0
where x = ch, ct, m mm, m mo, m mp, bi,
bp, and bh
Edge features:
{??, x?1, x0, x+1} ? y?1 ? y0
where x = ch, ct, and m mp
Bigram node features:
{x?2x?1, x?1x0, x0x+1} ? y0
x = ch, ct, m mo, m mp, bm, bi, bp, and bh
Table 3: Baseline features. Value of node feature is deter-
mined from current tag, y0, and surface feature (combina-
tion of atomic features in Table 2). Value of edge feature
is determined by previous tag, y?1, current tag, y0, and
surface feature. Subscripts indicate relative position from
current character.
used Gaussian regularization to prevent overfitting.
The parameter of the Gaussian, ?2, was tuned us-
ing the development set. We tested 10 points:
{0.64, 1.28, 2.56, 5.12, . . . , 163.84, 327.68}. We
stopped training when the relative change in the log-
likelihood became less than a pre-defined threshold,
0.0001. Throughout the experiments, we omitted
the features whose surface part described in Table
3 occurred less than twice in the training corpus.
4.4 Effect of Gazetteer Features
We investigated the effect of the cluster gazetteer de-
scribed in Section 2.1 and the Wikipedia gazetteer
described in Section 2.4, by adding each gazetteer
to the baseline model. We added the matching-
only and the class-augmented features, and we gen-
erated the node and the edge features in Table 3.17
For the cluster gazetteer, we made several gazetteers
that had different vocabulary sizes and numbers of
classes. The number of clustering iterations was 150
and the initial parameters were set randomly with a
Dirichlet distribution (?i = 1.0).
The statistics of each gazetteer are summarized
in Table 4. The number of entries in a gazetteer is
given by ?# entries?, and ?# matches? is the number
of matches that were output for the training set. We
define ?# e-matches? as the number of matches that
also match a boundary of a named entity in the train-
ing set, and ?# optimal? as the optimal number of ?#
e-matches? that can be achieved when we know the
LMVM optimizer of TAO (version 1.9) (Benson et al, 2007)
17Bigram node features were not used for gazetteer features.
oracle of entity boundaries. Note that this cannot
be realized because our matching uses the left-most
longest heuristics. We define ?pre.? as the precision
of the output matches (i.e., # e-matches/# matches),
and ?rec.? as the recall (i.e., # e-matches/# NEs).
Here, # NEs = 14, 056. Finally, ?opt.? is the op-
timal recall (i.e., # optimal/# NEs). ?# classes? is
the number of distinct classes in a gazetteer, and
?# used? is the number of classes that were out-
put for the training set. Gazetteers are as follows:
?wikip(m)? is the Wikipedia gazetteer (matching
only), and ?wikip(c)? is the Wikipedia gazetteer
(with class-augmentation). A cluster gazetteer,
which is constructed by the clustering with |N | =
|VT | = X ? 1, 000 and |C| = Y ? 1, 000, is indi-
cated by ?cXk-Y k?. Note that ?# entries? is slightly
smaller than the vocabulary size since we removed
some duplications during the conversion to a TRIE.
These gazetteers cover 40 - 50% of the named en-
tities, and the cluster gazetteers have relatively wider
coverage than the Wikipedia gazetteer has. The pre-
cisions are very low because there are many erro-
neous matches, e.g., with a entries for a hiragana
character.18 Although this seems to be a serious
problem, removing such one-character entries does
not affect the accuracy, and in fact, makes it worsen
slightly. We think this shows one of the strengths
of machine learning methods such as CRFs. We can
also see that our current matching method is not an
optimal one. For example, 16% of the matches were
lost as a result of using our left-most longest heuris-
tics for the case of the c500k-2k gazetteer.
A comparison of the effect of these gazetteers is
shown in Table 5. The performance is measured
by the F-measure. First, the Wikipedia gazetteer
improved the accuracy as expected, i.e., it repro-
duced the result of Kazama and Torisawa (2007)
for Japanese NER. The improvement for the test-
ing set was 1.08 points. Second, all the tested clus-
ter gazetteers improved the accuracy. The largest
improvement was 1.55 points with the c300k-3k
gazetteer. This was larger than that of the Wikipedia
gazetteer. The results for c300k-Y k gazetteers show
a peak of the improvement at some number of clus-
ters. In this case, |C| = 3, 000 achieved the best
improvement. The results of cXk-2k gazetteers in-
18Wikipedia contains articles explaining each hiragana char-
acter, e.g., ?? is a hiragana character?.
412
Name # entries # matches # e-matches # optimal pre. (%) rec. (%) opt. rec. (%) # classes # used
wikip(m) 550,054 225,607 6,804 7,602 3.02 48.4 54.1 N/A N/A
wikip(c) 550,054 189,029 5,441 6,064 2.88 38.7 43.1 12,786 1,708
c100k-2k 99,671 193,897 6,822 8,233 3.52 48.5 58.6 2,000 1,910
c300k-2k 295,695 178,220 7,377 9,436 4.14 52.5 67.1 2,000 1,973
c300k-1k ? ? ? ? ? ? ? 1,000 982
c300k-3k ? ? ? ? ? ? ? 3,000 2,848
c300k-4k ? ? ? ? ? ? ? 4,000 3,681
c500k-2k 497,101 174,482 7,470 9,798 4.28 53.1 69.7 2,000 1,951
c500k-3k ? ? ? ? ? ? ? 3,000 2,854
Table 4: Statistics of various gazetteers.
Model F (dev.) F (test.) best ?2
baseline 87.23 87.42 20.48
+wikip 87.60 88.50 2.56
+c300k-1k 88.74 87.98 40.96
+c300k-2k 88.75 88.01 163.84
+c300k-3k 89.12 88.97 20.48
+c300k-4k 88.99 88.40 327.68
+c100k-2k 88.15 88.06 20.48
+c500k-2k 88.80 88.12 40.96
+c500k-3k 88.75 88.03 20.48
Table 5: Comparison of gazetteer features.
Model F (dev.) F (test.) best ?2
+wikip+c300k-1k 88.65 *89.32 0.64
+wikip+c300k-2k *89.22 *89.13 10.24
+wikip+c300k-3k 88.69 *89.62 40.96
+wikip+c300k-4k 88.67 *89.19 40.96
+wikip+c500k-2k *89.26 *89.19 2.56
+wikip+c500k-3k *88.80 *88.60 10.24
Table 6: Effect of combination. Figures with * mean that
accuracy was improved by combining gazetteers.
dicate that the larger a gazetteer is, the larger the im-
provement. However, the accuracies of the c300k-3k
and c500k-3k gazetteers seem to contradict this ten-
dency. It might be caused by the accidental low qual-
ity of the clustering that results from random initial-
ization. We need to investigate this further.
4.5 Effect of Combining the Cluster and the
Wikipedia Gazetteers
We have observed that using the cluster gazetteer
and the Wikipedia one improves the accuracy of
Japanese NER. The next question is whether these
gazetteers improve the accuracy further when they
are used together. The accuracies of models that
use the Wikipedia gazetteer and one of the cluster
gazetteers at the same time are shown in Table 6.
The accuracy was improved in most cases. How-
Model F
(Asahara and Motsumoto, 2003) 87.21
(Nakano and Hirai, 2004) 89.03
(Yamada, 2007) 88.33
(Sasano and Kurohashi, 2008) 89.40
proposed (baseline) 87.62
proposed (+wikip) 88.14
proposed (+c300k-3k) 88.45
proposed (+c500k-2k) 88.41
proposed (+wikip+c300k-3k) 88.93
proposed (+wikip+c500k-2k) 88.71
Table 7: Comparison with previous studies
ever, there were some cases where the accuracy for
the development set was degraded. Therefore, we
should state at this point that while the benefit of
combining these gazetteers is not consistent in a
strict sense, it seems to exist. The best performance,
F = 89.26 (dev.) / 89.19 (test.), was achieved when
we combined the Wikipedia gazetteer and the clus-
ter gazetteer, c500k-2k. This means that there was
a 1.77-point improvement from the baseline for the
testing set.
5 Comparison with Previous Studies
Since many previous studies on Japanese NER used
5-fold cross validation for the IREX dataset, we
also performed it for some our models that had the
best ?2 found in the previous experiments. The re-
sults are listed in Table 7 with references to the re-
sults of recent studies. These results not only re-
confirmed the effects of the gazetteer features shown
in the previous experiments, but they also showed
that our best model is comparable to the state-of-the-
art models. The system recently proposed by Sasano
and Kurohashi (2008) is currently the best system
for the IREX dataset. It uses many structural fea-
tures that are not used in our model. Incorporating
413
such features might improve our model further.
6 Related Work and Discussion
There are several studies that used automatically ex-
tracted gazetteers for NER (Shinzato et al, 2006;
Talukdar et al, 2006; Nadeau et al, 2006; Kazama
and Torisawa, 2007). Most of the methods (Shin-
zato et al, 2006; Talukdar et al, 2006; Nadeau et
al., 2006) are oriented at the NE category. They
extracted a gazetteer for each NE category and uti-
lized it in a NE tagger. On the other hand, Kazama
and Torisawa (2007) extracted hyponymy relations,
which are independent of the NE categories, from
Wikipedia and utilized it as a gazetteer. The ef-
fectiveness of this method was demonstrated for
Japanese NER as well by this study.
Inducing features for taggers by clustering has
been tried by several researchers (Kazama et al,
2001; Miller et al, 2004). They constructed word
clusters by using HMMs or Brown?s clustering algo-
rithm (Brown et al, 1992), which utilize only infor-
mation from neighboring words. This study, on the
other hand, utilized MN clustering based on verb-
MN dependencies (Rooth et al, 1999; Torisawa,
2001). We showed that gazetteers created by using
such richer semantic/syntactic structures improves
the accuracy for NER.
The size of the gazetteers is also a novel point of
this study. The previous studies, with the excep-
tion of Kazama and Torisawa (2007), used smaller
gazetteers than ours. Shinzato et al (2006) con-
structed gazetteers with about 100,000 entries in
total for the ?restaurant? domain; Talukdar et al
(2006) used gazetteers with about 120,000 entries
in total, and Nadeau et al (2006) used gazetteers
with about 85,000 entries in total. By paralleliz-
ing the clustering algorithm, we successfully con-
structed a cluster gazetteer with up to 500,000 en-
tries from a large amount of dependency relations
in Web documents. To our knowledge, no one else
has performed this type of clustering on such a large
scale. Wikipedia also produced a large gazetteer
of more than 550,000 entries. However, compar-
ing these gazetteers and ours precisely is difficult at
this point because the detailed information such as
the precision and the recall of these gazetteers were
not reported.19 Recently, Inui et al (2007) investi-
19Shinzato et al (2006) reported some useful statistics about
gated the relation between the size and the quality of
a gazetteer and its effect. We think this is one of the
important directions of future research.
Parallelization has recently regained attention in
the machine learning community because of the
need for learning from very large sets of data. Chu
et al (2006) presented the MapReduce framework
for a wide range of machine learning algorithms, in-
cluding the EM algorithm. Newman et al (2007)
presented parallelized Latent Dirichlet Allocation
(LDA). However, these studies focus on the distri-
bution of the training examples and relevant com-
putation, and ignore the need that we found for the
distribution of model parameters. The exception,
which we noticed recently, is a study by Wolfe et
al. (2007), which describes how each node stores
only those parameters relevant to the training data
on each node. However, some parameters need to
be duplicated and thus their method is less efficient
than ours in terms of memory usage.
We used the left-most longest heuristics to find
the matching gazetteer entries. However, as shown
in Table 4 this is not an optimal method. We need
more sophisticated matching methods that can han-
dle multiple matching possibilities. Using models
such as Semi-Markov CRFs (Sarawagi and Cohen,
2004), which handle the features on overlapping re-
gions, is one possible direction. However, even if
we utilize the current gazetteers optimally, the cov-
erage is upper bounded at 70%. To cover most of
the named entities in the data, we need much larger
gazetteers. A straightforward approach is to increase
the number ofWeb documents used for theMN clus-
tering and to use larger vocabularies.
7 Conclusion
We demonstrated that a gazetteer obtained by clus-
tering verb-MN dependencies is a useful feature
for a Japanese NER. In addition, we demonstrated
that using the cluster gazetteer and the gazetteer ex-
tracted from Wikipedia (also shown to be useful)
can together further improves the accuracy in sev-
eral cases. Future work will be to refine the match-
ing method and to construct even larger gazetteers.
their gazetteers.
414
References
M. Asahara and Y. Motsumoto. 2003. Japanese named
entity extraction with redundant morphological analy-
sis.
S. Benson, L. C. McInnes, J. More?, T. Munson, and
J. Sarich. 2007. TAO user manual (revision 1.9).
Technical Report ANL/MCS-TM-242, Mathematics
and Computer Science Division, Argonne National
Laboratory. http://www.mcs.anl.gov/tao.
P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai,
and R. L. Mercer. 1992. Class-based n-gram mod-
els of natural language. Computational Linguistics,
18(4):467?479.
C.-T. Chu, S. K. Kim, Y.-A. Lin, Y. Yu, G. Bradski, A. Y.
Ng, and K. Olukotun. 2006. Map-reduce for machine
learning on multicore. In NIPS 2006.
O. Etzioni, M. Cafarella, D. Downey, A. M. Popescu,
T. Shaked, S. Soderland, D. S. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
Web ? an experimental study. Artificial Intelligence
Journal.
M. A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 14th In-
ternational Conference on Computational Linguistics,
pages 539?545.
A. Herbelot and A. Copestake. 2006. Acquiring onto-
logical relationships from Wikipedia using RMRS. In
Workshop on Web Content Mining with Human Lan-
guage Technologies ISWC06.
T. Inui, K. Murakami, T. Hashimoto, K. Utsumi, and
M. Ishikawa. 2007. A study on using gazetteers for
organization name recognition. In IPSJ SIG Technical
Report 2007-NL-182 (in Japanese).
J. Kazama and K. Torisawa. 2007. Exploiting Wikipedia
as external knowledge for named entity recognition.
In EMNLP-CoNLL 2007.
J. Kazama, Y. Miyao, and J. Tsujii. 2001. A maxi-
mum entropy tagger with unsupervised hiddenMarkov
models. In NLPRS 2001.
T. Kudo and Y. Matsumoto. 2002. Japanese dependency
analysis using cascaded chunking. In CoNLL 2002.
S. Kurohashi and D. Kawahara. 2005. KNP (Kurohashi-
Nagao parser) 2.0 users manual.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML 2001.
S. Miller, J. Guinness, and A. Zamanian. 2004. Name
tagging with word clusters and discriminative training.
In HLT-NAACL04.
D. Nadeau, Peter D. Turney, and Stan Matwin. 2006.
Unsupervised named-entity recognition: Generating
gazetteers and resolving ambiguity. In 19th Canadian
Conference on Artificial Intelligence.
K. Nakano and Y. Hirai. 2004. Japanese named entity
extraction with bunsetsu features. IPSJ Journal (in
Japanese).
D. Newman, A. Asuncion, P. Smyth, and M. Welling.
2007. Distributed inference for latent dirichlet alo-
cation. In NIPS 2007.
L. R. Rabiner. 1989. A tutorial on hidden Markov mod-
els and selected applications in speech recognition.
Proceedings of the IEEE, 77(2):257?286.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In 16th National Conference on Artificial Intelligence
(AAAI-99).
M. Rooth, S. Riezler, D. Presher, G. Carroll, and F. Beil.
1999. Inducing a semantically annotated lexicon via
EM-based clustering.
S. Sarawagi and W. W. Cohen. 2004. Semi-Markov ran-
dom fields for information extraction. In NIPS 2004.
R. Sasano and S. Kurohashi. 2008. Japanese named en-
tity recognition using structural natural language pro-
cessing. In IJCNLP 2008.
S. Sekine and H. Isahara. 2000. IREX: IR and IE evalu-
ation project in Japanese. In IREX 2000.
K. Shinzato and K. Torisawa. 2004. Acquiring hy-
ponymy relations from Web documents. In HLT-
NAACL 2004.
K. Shinzato, S. Sekine, N. Yoshinaga, and K. Tori-
sawa. 2006. Constructing dictionaries for named en-
tity recognition on specific domains from the Web. In
Web Content Mining with Human Language Technolo-
gies Workshop on the 5th International Semantic Web.
P. P. Talukdar, T. Brants, M. Liberman, and F. Pereira.
2006. A context pattern induction method for named
entity extraction. In CoNLL 2006.
M. Thelen and E. Riloff. 2002. A bootstrapping method
for learning semantic lexicons using extraction pattern
context. In EMNLP 2002.
K. Torisawa. 2001. An unsupervised method for canoni-
calization of Japanese postpositions. In NLPRS 2001.
H. Tsurumaru, K. Takeshita, K. Iami, T. Yanagawa, and
S. Yoshida. 1991. An approach to thesaurus construc-
tion from Japanese language dictionary. In IPSJ SIG
Notes Natural Language vol.83-16, (in Japanese).
J. Wolfe, A. Haghighi, and D. Klein. 2007. Fully dis-
tributed EM for very large datasets. In NIPS Workshop
on Efficient Machine Learning.
H. Yamada. 2007. Shift-reduce chunking for Japanese
named entity extraction. In ISPJ SIG Technical Report
2007-NL-179.
415
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 53?60, New York City, June 2006. c?2006 Association for Computational Linguistics
Semantic Role Recognition using Kernels on Weighted Marked Ordered
Labeled Trees
Jun?ichi Kazama and Kentaro Torisawa
Japan Advanced Institute of Science and Technology (JAIST)
Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan
{kazama, torisawa}@jaist.ac.jp
Abstract
We present a method for recognizing se-
mantic role arguments using a kernel on
weighted marked ordered labeled trees
(the WMOLT kernel). We extend the
kernels on marked ordered labeled trees
(Kazama and Torisawa, 2005) so that the
mark can be weighted according to its im-
portance. We improve the accuracy by
giving more weights on subtrees that con-
tain the predicate and the argument nodes
with this ability. Although Kazama and
Torisawa (2005) presented fast training
with tree kernels, the slow classification
during runtime remained to be solved. In
this paper, we give a solution that uses an
efficient DP updating procedure applica-
ble in argument recognition. We demon-
strate that the WMOLT kernel improves
the accuracy, and our speed-up method
makes the recognition more than 40 times
faster than the naive classification.
1 Introduction
Semantic role labeling (SRL) is a task that recog-
nizes the arguments of a predicate (verb) in a sen-
tence and assigns the correct role to each argument.
As this task is recognized as an important step after
(or the last step of) syntactic analysis, many stud-
ies have been conducted to achieve accurate seman-
tic role labeling (Gildea and Jurafsky, 2002; Mos-
chitti, 2004; Hacioglu et al, 2004; Punyakanok et
al., 2004; Pradhan et al, 2005a; Pradhan et al,
2005b; Toutanova et al, 2005).
Most of the studies have focused on machine
learning because of the availability of standard
datasets, such as PropBank (Kingsbury and Palmer,
2002). Naturally, the usefulness of parse trees in
this task can be anticipated. For example, the recent
CoNLL 2005 shared task (Carreras and Ma`rquez,
2005) provided parse trees for use and their useful-
ness was ensured. Most of the methods heuristically
extract features from parse trees, and from other
sources, and use them in machine learning methods
based on feature vector representation. As a result,
these methods depend on feature engineering, which
is time-consuming.
Tree kernels (Collins and Duffy, 2001; Kashima
and Koyanagi, 2002) have been proposed to directly
handle trees in kernel-based methods, such as SVMs
(Vapnik, 1995). Tree kernels calculate the similar-
ity between trees, taking into consideration all of the
subtrees, and, therefore there is no need for such fea-
ture engineering.
Moschitti and Bejan (2004) extensively studied
tree kernels for semantic role labeling. However,
they reported that they could not successfully build
an accurate argument recognizer, although the role
assignment was improved. Although Moschitti et al
(2005) reported on argument recognition using tree
kernels, it was a preliminary evaluation because they
used oracle parse trees.
Kazama and Torisawa (2005) proposed a new tree
kernel for node relation labeling, as which SRL can
be cast. This kernel is defined on marked ordered la-
beled trees, where a node can have a mark to indicate
the existence of a relation. We refer to this kernel
as the MOLT kernel. Compared to (Moschitti and
Bejan, 2004) where tree fragments are heuristically
extracted before applying tree kernels, the MOLT
kernel is general and desirable since it does not re-
quire such fragment extraction. However, the eval-
uation conducted by Kazama and Torisawa (2005)
was limited to preliminary experiments for role as-
signment. In this study, we first evaluated the per-
formance of the MOLT kernel for argument recogni-
tion, and found that theMOLT kernel cannot achieve
a high accuracy if used in its original form.
53
a catI saw the parkin
DT NNPRP VBD DT NNIN
NP
S
NP VP
NP
PP
(a)
a catI saw the parkin
DT NNPRP VBD DT NNIN
NP
S
NP VP
NP
PP
(b)
a catI saw the parkin
DT NNPRP VBD DT NNIN
NP
S
NP VP
NP
PP
(c)
a catI saw the parkin
DT NNPRP VBD DT NNIN
NP
S
NP VP
NP
PP
(a')
*0
*1
Figure 1: (a)-(c): Argument recognition as node relation recognition. (a?): relation (a) represented as marked
ordered tree.
Therefore, in this paper we propose a modifica-
tion of the MOLT kernel, which greatly improves
the accuracy. The problem with the original MOLT
kernel is that it treats subtrees with one mark, i.e.,
those including only the argument or the predicate
node, and subtrees with two marks, i.e., those in-
cluding both the argument and the predicate nodes
equally, although the latter is likely to be more im-
portant for distinguishing difficult arguments. Thus,
we modified the MOLT kernel so that the marks can
be weighted in order to give large weights to the sub-
trees with many marks. We call the modified kernel
the WMOLT kernel (the kernel on weighted marked
ordered labeled trees). We show that this modifica-
tion greatly improves the accuracy when the weights
for marks are properly tuned.
One of the issues that arises when using tree ker-
nels is time complexity. In general, tree kernels can
be calculated in O(|T1||T2|) time, where |Ti| is the
number of nodes in tree Ti, using dynamic program-
ming (DP) procedures (Collins and Duffy, 2001;
Kashima and Koyanagi, 2002). However, this cost
is not negligible in practice. Kazama and Torisawa
(2005) proposed a method that drastically speeds up
the calculation during training by converting trees
into efficient vectors using a tree mining algorithm.
However, the slow classification during runtime re-
mained an open problem.
We propose a method for speeding up the runtime
classification for argument recognition. In argument
recognition, we determine whether a node is an ar-
gument or not for all the nodes in a tree . This
requires a series of calculations between a support
vector tree and a tree with slightly different mark-
ing. By exploiting this property, we can efficiently
update DP cells to obtain the kernel value with less
computational cost.
In the experiments, we demonstrated that the
WMOLT kernel drastically improved the accuracy
and that our speed-up method enabled more than
40 times faster argument recognition. Despite these
successes, the performance of our current system is
F1 = 78.22 on the CoNLL 2005 evaluation set when
using the Charniak parse trees, which is far worse
than the state-of-the-art system. We will present
possible reasons and future directions.
2 Semantic Role Labeling
Semantic role labeling (SRL) recognizes the argu-
ments of a given predicate and assigns the correct
role to each argument. For example, the sentence ?I
saw a cat in the park? will be labeled as follows with
respect to the predicate ?see?.
[A0 I] [V saw] [A1 a cat] [AM-LOC in the park]
In the example, A0, A1, and AM-LOC are the roles
assigned to the arguments. In the CoNLL 2005
dataset, there are the numbered arguments (AX)
whose semantics are predicate dependent, the ad-
juncts (AM-X), and the references (R-X) for rel-
ative clauses.
Many previous studies employed two-step SRL
methods, where (1) we first recognize the argu-
ments, and then (2) classify the argument to the cor-
rect role. We also assume this two-step processing
and focus on the argument recognition.
Given a parse tree, argument recognition can be
cast as the classification of tree nodes into two
classes, ?ARG? and ?NO-ARG?. Then, we consider
the words (a phrase) that are the descendants of an
?ARG? node to be an argument. Since arguments
are defined for a given predicate, this classification
is the recognition of a relation between the predicate
and tree nodes. Thus, we want to build a binary clas-
sifier that returns a +1 for correct relations and a -1
for incorrect relations. For the above example, the
classifier will output a +1 for the relations indicated
by (a), (b), and (c) in Figure 1 and a -1 for the rela-
tions between the predicate node and other nodes.
54
Since the task is the classification of trees with
node relations, tree kernels for usual ordered la-
beled trees, such as those proposed by Collins and
Duffy (2001) and Kashima and Koyanagi (2002),
are not useful. Kazama and Torisawa (2005) pro-
posed to represent a node relation in a tree as a
marked ordered labeled tree and presented a kernel
for it (MOLT kernel). We adopted the MOLT kernel
and extend it for accurate argument recognition.
3 Kernels for Argument Recognition
3.1 Kernel-based classification
Kernel-based methods, such as support vector ma-
chines (SVMs) (Vapnik, 1995), consider a mapping
?(x) that maps the object x into a, (usually high-
dimensional), feature space and learn a classifier in
this space. A kernel function K(xi, xj) is a function
that calculates the inner product ??(xi),?(xj)? in
the feature space without explicitly computing?(x),
which is sometimes intractable. Then, any classifier
that is represented by using only the inner products
between the vectors in a feature space can be re-
written using the kernel function. For example, an
SVM classifier has the form:
f(x) =
?
i
?iK(xi, x) + b,
where ?i and b are the parameters learned in the
training. With kernel-based methods, we can con-
struct a powerful classifier in a high-dimensional
feature space. In addition, objects x do not need
to be vectors as long as a kernel function is defined
(e.g., x can be strings, trees, or graphs).
3.2 MOLT kernel
A marked ordered labeled tree (Kazama and Tori-
sawa, 2005) is an ordered labeled tree in which each
node can have a mark in addition to a label. We can
encode a k-node relation by using k distinct marks.
In this study, we determine an argument node with-
out considering other arguments of the same pred-
icate, i.e., we represent an argument relation as a
two-node relation using two marks. For example,
the relation (a) in Figure 1 can be represented as the
marked ordered labeled tree (a?).1
1Note that we use mark *0 for the predicate node and mark
*1 for the argument node.
Table 1: Notations for MOLT kernel.
? ni denotes a node of a tree. In this paper, ni is an ID assigned in the
post-order traversal.
? |Ti| denotes the number of nodes in tree Ti.
? l(ni) returns the label of node ni.
? m(ni) returns the mark of node ni. If ni has no mark, m(ni)
returns the special mark no-mark.
? marked(ni) returns true iff m(ni) is not no-mark.
? nc(ni) is the number of children of node ni.
? chk(ni) is the k-th child of node ni.
? pa(ni) is the parent of node ni.
? root(Ti) is the root node of Ti
? ni ? nj means that ni is an elder sister of nj .
Kazama and Torisawa (2005) presented a kernel
on marked ordered trees (the MOLT kernel), which
is defined as:2
K(T1, T2) =
E
?
i=1
W (Si) ?#Si(T1) ?#Si(T2),
where Si is a possible subtree and #Si(Tj) is
the number of times Si is included in Tj . The
mapping corresponding to this kernel is ?(T ) =
(
?
W (S1)#S1(T ), ? ? ? ,
?
W (SE)#SE (T )), which
maps the tree into the feature space of all the possi-
ble subtrees.
The tree inclusion is defined in many ways. For
example, Kashima and Koyanagi (2002) presented
the following type of inclusion.
1 DEFINITION S is included in T iff there exists a
one-to-one function ? from a node of S to a node
of T , such that (i) pa(?(ni)) = ?(pa(ni)), (ii)
?(ni) ? ?(nj) iff ni ? nj , , and (iii) l(?(ni)) =
l(ni) (and m(?(ni)) = m(ni) in the MOLT kernel).
See Table 1 for the meaning of each function. This
definition means that any subtrees preserving the
parent-child relation, the sibling relation, and label-
marks, are allowed. In this paper, we employ this
definition, since Kazama and Torisawa (2005) re-
ported that the MOLT kernel with this definition has
a higher accuracy than one with the definition pre-
sented by Collins and Duffy (2001).
W (Si) is the weight of subtree Si. The weight-
ing in Kazama and Torisawa (2005) is written as fol-
2This notation is slightly different from (Kazama and Tori-
sawa, 2005).
55
Table 2: Example of subtree inclusion and sub-
tree weights. The last row shows the weights for
WMOLT kernel.
T included subtrees
W (Si) 0 ? ? ?2 ?2 ?3
W (Si) 0 ?? ?? ?2? ?2?2 ?3?2
lows.
W (Si) =
{
?|Si| if marked(Si),
0 otherwise,
(1)
where marked(Si) returns true iff marked(ni) =
true for at least one node in tree Si. By this weight-
ing, only the subtrees with at least one mark are con-
sidered. The idea behind this is that subtrees having
no marks are not useful for relation recognition or
labeling. ? (0 ? ? ? 1) is a factor to prevent the ker-
nel values from becoming too large, which has been
used in previous studies (Collins and Duffy, 2001;
Kashima and Koyanagi, 2002).
Table 2 shows an example of subtree inclusion
and the weights given to each included subtree. Note
that the subtrees are treated differently when the
markings are different, even if the labels are the
same.
Although the dimension of the feature space
is exponential, tree kernels can be calculated in
O(|T1||T2|) time using dynamic programming (DP)
procedures (Collins and Duffy, 2001; Kashima and
Koyanagi, 2002). The MOLT kernel also has an
O(|T1||T2|) DP procedure (Kazama and Torisawa,
2005).
3.3 WMOLT kernel
Although Kazama and Torisawa (2005) evaluated
the MOLT kernel for SRL, the evaluation was only
on the role assignment task and was preliminary. We
evaluated the MOLT kernel for argument recogni-
tion, and found that theMOLT kernel cannot achieve
a high accuracy for argument recognition.
The problem is that the MOLT kernel treats sub-
trees with one mark and subtrees with two marks
equally, although the latter seems to be more impor-
tant in distinguishing difficult arguments.
Consider the sentence, ?He said industry should
build plants?. For ?say?, we have the following la-
beling.
[A0 He] [V said] [A1 industry should build plants]
On the other hand, for ?build?, we have
He said [A0 industry] [AM-MOD should] [V build]
[A1 plants].
As can be seen, ?he? is the A0 argument of ?say?,
but not an argument of ?build?. Thus, our classifier
should return a +1 for the tree where ?he? is marked
when the predicate is ?say?, and a -1 when the pred-
icate is ?build?. Although the subtrees around the
node for ?say? and ?build? are different, the subtrees
around the node for ?he? are identical for both cases.
If ?he? is often the A0 argument in the corpus, it is
likely that the classifier returns a +1 even for ?build?.
Although the subtrees containing both the predicate
and the argument nodes are considered in the MOLT
kernel, they are given relatively small weights by Eq.
(1), since such subtrees are large.
Thus, we modify the MOLT kernel so that the
mark can be weighted according to its importance
and the more marks the subtrees contain, the more
weights they get. The modification is simple. We
change the definition of W (Si) as follows.
W (Si) =
{
?|Si|
?
ni?Si ?(m(ni)) if marked(Si),
0 otherwise,
where ?(m) (? 1) is the weight of mark m. We
call a kernel with this weight the WMOLT kernel.
In this study, we assume ?(no-mark) = 1 and
?(*0) = ?(*1) = ?. Then, the weight is simpli-
fied as follows.
W (Si) =
{
?|Si|?#m(Si) if marked(Si),
0 otherwise,
where #m(Si) is the number of marked nodes in
Si. The last row in Table 2 shows how the subtree
weights change by introducing this mark weighting.
For the WMOLT kernel, we can derive
O(|T1||T2|) DP procedure by slightly modify-
ing the procedure presented by Kazama and
Torisawa (2005). The method for speeding up
training described in Kazama and Torisawa (2005)
can also be applied with a slight modification.
56
Algorithm 3.1: WMOLT-KERNEL(T1, T2)
for n1 ? 1 to |T1| do // nodes are ordered by the post-order traversal
m ? marked(n1)
for n2 ? 1 to |T2| do // actually iterate only on n2 with l(n1) = l(n2)
(A)
8
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
:
if l(n1) ?= l(n2) or m(n1) ?= m(n2) then
C(n1, n2) ? 0 Cr(n1, n2) ? 0
else if n1 and n2 are leaf nodes then
if m then C(n1, n2) ? ? ? ?; Cr(n1, n2) ? ? ? ? else C(n1, n2) ? ?; Cr(n1, n2) ? 0
else
S(0, j) ? 1, S(i, 0) ? 1 (i ? [0, nc(n1)], j ? [0, nc(n2)])
if m then Sr(0, j) ? 1, Sr(i, 0) ? 1 else Sr(0, j) ? 0, Sr(i, 0) ? 0
for i ? 1 to nc(n1) do
for j ? 1 to nc(n2) do
S(i, j) ? S(i?1, j) + S(i, j?1)? S(i?1, j?1) + S(i?1, j?1) ? C(chi(n1), chj(n2))
Sr(i, j) ? Sr(i?1, j) + Sr(i, j?1)? Sr(i?1, j?1) + Sr(i?1, j?1) ? C(chi(n1), chj(n2))
+S(i?1, j?1) ? Cr(chi(n1), chj(n2))? Sr(i?1, j?1) ? Cr(chi(n1), chj(n2))
if m then C(n1, n2) ? ? ? ? ? S(nc(n1), nc(n2)) else C(n1, n2) ? ? ? S(nc(n1), nc(n2))
if m then Cr(n1, n2) ? ? ? ? ? Sr(nc(n1), nc(n2)) else Cr(n1, n2) ? ? ? Sr(nc(n1), nc(n2))
return (
P|T1|
n1=1
P|T2|
n2=1 C
r(n1, n2))
We describe this DP procedure in some detail.
The key is the use of two DP matrices of size
|T1| ? |T2|. The first is C(n1, n2) defined as:
C(n1, n2)?
P
Si W
?(Si) ?#Si(T1 ? n1) ?#Si(T2 ? n2),
where #Si(Tj ? nk) represents the number of times
subtree Si is included in tree Tj with ?(root(Si)) =
nk. W ?(Si) is defined as W ?(Si) = ?|Si|?#m(Si).
This means that this matrix records the values that
ignore whether marked(Si) = true or not. The
second is Cr(n1, n2) defined as:
Cr(n1, n2)?
P
Si W (Si) ?#Si(T1 ? n1) ?#Si(T2 ? n2).
With these matrices, the kernel is calculated as:
K(T1, T2) =
?
n1?T1
?
n2?T2
Cr(n1, n2).
C(n1, n2) and Cr(n1, n2) are calculated recur-
sively, starting from the leaves of the trees. The re-
cursive procedure is shown in Algorithm 3.1. See
also Table 1 for the meaning of the functions used.
4 Fast Argument Recognition
We use the SVMs for the classifiers in argument
recognition in this study and describe the fast clas-
sification method based on SVMs.3 We denote a
marked ordered labeled tree where node nk of an
ordered labeled tree U is marked by mark X , nl by
Y , and so on, by U@{nk = X,nl = Y, . . . }.
3The method can be applied to a wide range of kernel-based
methods that have the same structure as SVMs.
Algorithm 4.1: CALCULATE-T(U, Tj)
procedure FAST-UPDATE(nk)
diff ? 0, m(nk) ? *1, U ? ?
for n2 ? 1 to |Tj | do change(n2) ? true
n1 ? nk
while n1 ?= nil do
8
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
:
for n2 ? 1 to |Tj | do
// actually iterate only on n2 with l(pa(n1)) = l(n2)
nchange(n2) ? false
for n2 ? 1 to |Tj | do
// actually iterate only on n2 with l(n1) = l(n2)
if change(n2) then
pre ? Cr(n1, n2), U ? U ? (n1, n2)
update C(n1, n2) and Cr(n1, n2)
using (A) of Algorithm 3.1
diff += (Cr(n1, n2)? pre)
if pa(n2) ?= nil then nchange(pa(n2)) ? true
n1 ? pa(n1), change ? nchange
for (n1, n2) ? U do //restore DP cells
C(n1, n2) ? C?(n1, n2), Cr(n1, n2) ? Cr ?(n1, n2)
m(nk) ? no-mark
return (diff )
main
m(nv) ? ?0, k ? WMOLT-KERNEL(U, Tj)
C?(n1, n2) ? C(n1, n2), Cr ?(n1, n2) ? Cr(n1, n2)
for nk ? 1 to |U | do (nk ?= nv)
diff ? FAST-UPDATE(nk), t(nk) ? k + diff
Given a sentence represented by tree U and the
node for the target predicate nv, the argument recog-
nition requires the calculation of:
s(nk) =
?
Tj?SV
?jK(U@{nv=*0, nk=*1}, Tj)+b,
(2)
for all nk ? U (?= nv), where SV represents the
support vectors. Naively, this requires O(|U | ?
|SV| ? |U ||Tj |) time, which is rather costly in prac-
tice.
57
However, if we exploit the fact that U@{nv =
*0, nk = *1} is different from U@{nv = *0} at one
node, we can greatly speed up the above calculation.
At first, we calculate K(U@{nv = *0}, Tj) using
the DP procedure presented in the previous section,
and then calculate K(U@{nv = *0, nk = *1}, Tj)
using a more efficient DP that updates only the val-
ues of the necessary DP cells of the first DP. More
specifically, we only need to update the DP cells in-
volving the ancestor nodes of nk.
Here we show the procedure for calculating
t(nk) = K(U@{nv = *0, nk = *1}, Tj) for all
nk for a given support vector Tj , which will suf-
fice for calculating s(nk). Algorithm 4.1 shows the
procedure. For each nk, this procedure updates at
most (nk?s depth) ? |Tj | cells, which is much less
than |U | ? |Tj | cells. In addition, when updating
the cells for (n1, n2), we only need to update them
when the cells for any child of n2 have been updated
in the calculation of the cells for the children of n1.
To achieve this, change(n2) in the algorithm stores
whether the cells of any child of n2 have been up-
dated. This technique will also reduce the number
of updated cells.
5 Non-overlapping Constraint
Finally, in argument recognition, there is a strong
constraint that the arguments for a given predicate
do not overlap each other. To enforce this constraint,
we employ the approach presented by Toutanova
et al (2005). Given the local classification proba-
bility p(nk = Xk) (Xk ? {ARG,NO-ARG}),
this method finds the assignment that maximizes
?
k p(nk = Xk) while satisfying the above non-
overlapping constraint, by using a dynamic pro-
gramming procedure. Since the output of SVMs is
not a probability value, in this study we obtain the
probability value by converting the output from the
SVM, s(nk), using the sigmoid function:4
p(nk = ARG) = 1/(1 + exp(?s(nk))).
6 Evaluation
6.1 Setting
For our evaluation we used the dataset pro-
vided for the CoNLL 2005 SRL shared task
4Parameter fitting (Platt, 1999) is not performed.
(www.lsi.upc.edu/?srlconll). We used only the train-
ing part and divided it into our training, develop-
ment, and test sets (23,899, 7,966, and 7,967 sen-
tences, respectively). We used the outputs of the
Charniak parser provided with the dataset. We also
used POS tags, which were also provided, by insert-
ing the nodes labeled by POS tags above the word
nodes. The words were downcased.
We used TinySVM5 as the implementation of the
SVMs, adding the WMOLT kernel. We normalized
the kernel as: K(Ti, Tj)/
?
K(Ti, Ti)?K(Tj , Tj).
To train the classifiers, for a positive example we
used the marked ordered labeled tree that encodes
an argument in the training set. Although nodes
other than the argument nodes were potentially neg-
ative examples, we used 1/5 of these nodes that were
randomly-sampled, since the number of such nodes
is so large that the training cannot be performed in
practice. Note that we ignored the arguments that
do not match any node in the tree (the rate of such
arguments was about 3.5% in the training set).
6.2 Effect of mark weighting
We first evaluated the effect of the mark weight-
ing of the WMOLT kernel. For several fixed ?, we
tuned ? and the soft-margin constant of the SVM,C,
and evaluated the recognition accuracy. We tested
30 different values of C ? [0.1 . . . 500] for each
? ? [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]. The tuning was
performed using the method for speeding up the
training with tree kernels described by Kazama and
Torisawa (2005). We conducted the above experi-
ment for several training sizes.
Table 3 shows the results. This table shows the
best setting of ? and C, the performance on the de-
velopment set with the best setting, and the perfor-
mance on the test set. The performance is shown
in the F1 measure. Note that we treated the region
labeled C-k in the CoNLL 2005 dataset as an inde-
pendent argument.
We can see that the mark weighting greatly im-
proves the accuracy over the original MOLT kernel
(i.e., ? = 1). In addition, we can see that the best
setting for ? is somewhere around ? = 4, 000. In
this experiment, we could only test up to 1,000 sen-
tences due to the cost of SVM training, which were
5chasen.org/?taku/software/TinySVM
58
Table 3: Effect of ? in mark weighting of WMOLT kernel.
training size (No. of sentences)
250 500 700 1,000
setting dev test setting dev test setting dev test setting dev test
? (?,C) (F1) (F1) (?,C) (F1) (F1) (?,C) (F1) (F1) (?,C) (F1) (F1)
1 0.15, 20.50 63.66 65.13 0.2, 20.50 69.01 70.33 0.2, 20.50 72.11 73.57 0.25, 12.04 75.38 76.25
100 0.3, 12.04 80.13 80.85 0.3,500 82.25 82.98 0.3, 34.92 83.93 84.72 0.3, 3.18 85.09 85.85
1,000 0.2, 2.438 82.65 83.36 0.2, 2.438 84.80 85.45 0.2, 3.182 85.58 86.20 0.2, 7.071 86.40 86.80
2,000 0.2, 2.438 83.43 84.12 0.2, 2.438 85.56 86.24 0.2, 2.438 86.23 86.80 0.2, 12.04 86.61 87.18
4,000 0.2, 2.438 83.87 84.50 0.15, 4.15 84.94 85.61 0.15, 7.07 85.84 86.32 0.2, 12.04 86.82 87.31
4,000 (w/o) 80.81 81.41 80.71 81.51 81.86 82.33 84.27 84.63
empirically O(L2) where L is the number of train-
ing examples, regardless of the use of the speed-up
method (Kazama and Torisawa, 2005), However, we
can observe that the WMOLT kernel achieves a high
accuracy even when the training data is very small.
6.3 Effect of non-overlapping constraint
Additionally, we observed how the accuracy
changes when we do not use the method described
in Section 5 and instead consider the node to be an
argument when s(nk) > 0. The last row in Ta-
ble 3 shows the accuracy for the model obtained
with ? = 4, 000. We could observe that the non-
overlapping constraint also improves the accuracy.
6.4 Recognition speed-up
Next, we examined the method for fast argument
recognition described in Section 4. Using the clas-
sifiers with ? = 4, 000, we measured the time re-
quired for recognizing the arguments for 200 sen-
tences with the naive classification of Eq. (2) and
with the fast update procedure shown in Algorithm
4.1. The time was measured using a computer with
2.2-GHz dual-core Opterons and 8-GB of RAM.
Table 4 shows the results. We can see a constant
speed-up by a factor of more than 40, although the
time was increased for both methods as the size of
the training data increases (due to the increase in the
number of support vectors).
Table 4: Recognition time (sec.) with naive classifi-
cation and proposed fast update.
training size (No. of sentences)
250 500 750 1,000
naive 11,266 13,008 18,313 30,226
proposed 226 310 442 731
speed-up 49.84 41.96 41.43 41.34
6.5 Evaluation on CoNLL 2005 evaluation set
To compare the performance of our system with
other systems, we conducted the evaluation on the
official evaluation set of the CoNLL 2005 shared
task. We used a model trained using 2,000 sen-
tences (57,547 examples) with (? = 4, 000, ? =
0.2, C = 12.04), the best setting in the previous ex-
periments. This is the largest model we have suc-
cessfully trained so far, and has F1 = 88.00 on the
test set in the previous experiments.
The accuracy of this model on the official evalua-
tion set was F1 = 79.96 using the criterion from the
previous experiments where we treated a C-k argu-
ment as an independent argument. The official eval-
uation script returned F1 = 78.22. This difference
is caused because the official script takes C-k argu-
ments into consideration, while our system cannot
output C-k labels since it is just an argument rec-
ognizer. Therefore, the performance will become
slightly higher than F1 = 78.22 if we perform the
role assignment step. However, our current system
is worse than the systems reported in the CoNLL
2005 shared task in any case, since it is reported that
they had F1 = 79.92 to 83.78 argument recognition
accuracy (Carreras and Ma`rquez, 2005).
7 Discussion
Although we have improved the accuracy by intro-
ducing theWMOLT kernel, the accuracy for the offi-
cial evaluation set was not satisfactory. One possible
reason is the accuracy of the parser. Since the Char-
niak parser is trained on the same set with the train-
ing set of the CoNLL 2005 shared task, the pars-
ing accuracy is worse for the official evaluation set
than for the training set. For example, the rate of the
arguments that do not match any node of the parse
tree is 3.93% for the training set, but 8.16% for the
59
evaluation set. This, to some extent, explains why
our system, which achieved F1 = 88.00 for our test
set, could only achieved F1 = 79.96. To achieve a
higher accuracy, we need to make the system more
robust to parsing errors. Some of the non-matching
arguments are caused by incorrect treatment of quo-
tation marks and commas. These errors seem to be
solved by using simple pre-processing. Other major
non-matching arguments are caused by PP attach-
ment errors. To solve these errors, we need to ex-
plore more, such as using n-best parses and the use
of several syntactic views (Pradhan et al, 2005b).
Another reason for the low accuracy is the size of
the training data. In this study, we could train the
SVM with 2,000 sentences (this took more than 30
hours including the conversion of trees), but this is
a very small fraction of the entire training set. We
need to explore the methods for incorporating a large
training set within a reasonable training time. For
example, the combination of small SVMs (Shen et
al., 2003) is a possible direction.
The contribution of this study is not the accuracy
achieved. The first contribution is the demonstration
of the drastic effect of the mark weighting. We will
explore more accurate kernels based on theWMOLT
kernel. For example, we are planning to use dif-
ferent weights depending on the marks. The sec-
ond contribution is the method of speeding-up argu-
ment recognition. This is of great importance, since
the proposed method can be applied to other tasks
where all nodes in a tree should be classified. In ad-
dition, this method became possible because of the
WMOLT kernel, and it is hard to apply to Moschitti
and Bejan (2004) where the tree structure changes
during recognition. Thus, the architecture that uses
the WMOLT kernel is promising, if we assume fur-
ther progress is possible with the kernel design.
8 Conclusion
We proposed a method for recognizing semantic role
arguments using the WMOLT kernel. The mark
weighting introduced in the WMOLT kernel greatly
improved the accuracy. In addition, we presented
a method for speeding up the recognition, which re-
sulted in more than a 40 times faster recognition. Al-
though the accuracy of the current system is worse
than the state-of-the-art system, we expect to further
improve our system.
References
X. Carreras and L. Ma`rquez. 2005. Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In
CoNLL 2005.
M. Collins and N. Duffy. 2001. Convolution kernels for
natural language. In NIPS 2001.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3).
K. Hacioglu, S. Pradhan, W. Ward, J. H. Martin, and
D. Jurafsky. 2004. Semantic role labeling by tagging
syntactic chunks. In CoNLL 2004.
H. Kashima and T. Koyanagi. 2002. Kernels for semi-
structured data. In ICML 2002, pages 291?298.
J. Kazama and K. Torisawa. 2005. Speeding up training
with tree kernels for node relation labeling. In EMNLP
2005.
P. Kingsbury and M. Palmer. 2002. From treebank to
propbank. In LREC 02.
A. Moschitti and C. A. Bejan. 2004. A semantic kernels
for predicate argument classification. In CoNLL 2004.
A. Moschitti, B. Coppola, D. Pighin, and B. Basili. 2005.
Engineering of syntactic features for shallow semantic
parsing. In ACL 2005 Workshop on Feature Enginner-
ing for Machine Learning in Natural Language Pro-
cessing.
A. Moschitti. 2004. A study on convolution kernels for
shallow semantic parsing. In ACL 2004.
J. C. Platt. 1999. Probabilistic outputs for support vector
machines and comparisons to regularized likelihood
methods. Advances in Large Margin Classifiers.
S. Pradhan, K. Hacioglu, W. Ward, D. Jurafsky, and J. H.
Martin. 2005a. Support vector learning for semantic
argument classification. Machine Learning, 60(1).
S. Pradhan, W. Ward, K. Hacioglu, J. H. Martin, and
D. Jurafsky. 2005b. Semantic role labeling using dif-
ferent syntactic views. In ACL 2005.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004.
Semantic role labeling via integer linear programming
inference. In COLING 2004.
L. Shen, A. Sarkar, and A. K. Joshi. 2003. Using LTAG
based features in parse reranking. In EMNLP 2003.
K. Toutanova, A. Haghighi, and C. D. Manning. 2005.
Joint learning improves semantic role labeling. In ACL
2005.
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer Verlag.
60
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 570?579,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Improving Dependency Parsing with Subtrees from Auto-Parsed Data
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto, and Kentaro Torisawa
Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
{chenwl, kazama, uchimoto, torisawa}@nict.go.jp
Abstract
This paper presents a simple and effective
approach to improve dependency parsing
by using subtrees from auto-parsed data.
First, we use a baseline parser to parse
large-scale unannotated data. Then we ex-
tract subtrees from dependency parse trees
in the auto-parsed data. Finally, we con-
struct new subtree-based features for pars-
ing algorithms. To demonstrate the ef-
fectiveness of our proposed approach, we
present the experimental results on the En-
glish Penn Treebank and the Chinese Penn
Treebank. These results show that our ap-
proach significantly outperforms baseline
systems. And, it achieves the best accu-
racy for the Chinese data and an accuracy
which is competitive with the best known
systems for the English data.
1 Introduction
Dependency parsing, which attempts to build de-
pendency links between words in a sentence, has
experienced a surge of interest in recent times,
owing to its usefulness in such applications as
machine translation (Nakazawa et al, 2006) and
question answering (Cui et al, 2005). To ob-
tain dependency parsers with high accuracy, super-
vised techniques require a large amount of hand-
annotated data. While hand-annotated data are
very expensive, large-scale unannotated data can
be obtained easily. Therefore, the use of large-
scale unannotated data in training is an attractive
idea to improve dependency parsing performance.
In this paper, we present an approach that ex-
tracts subtrees from dependency trees in auto-
parsed data to improve dependency parsing. The
auto-parsed data are generated from large-scale
unannotated data by using a baseline parser. Then,
from dependency trees in the data, we extract dif-
ferent types of subtrees. Finally, we represent
subtree-based features on training data to train de-
pendency parsers.
The use of auto-parsed data is not new. How-
ever, unlike most of the previous studies (Sagae
and Tsujii, 2007; Steedman et al, 2003) that im-
proved the performance by using entire trees from
auto-parsed data, we exploit partial information
(i.e., subtrees) in auto-parsed data. In their ap-
proaches, they used entire auto-parsed trees as
newly labeled data to train the parsing models,
while we use subtree-based features and employ
the original gold-standard data to train the mod-
els. The use of subtrees instead of complete trees
can be justified by the fact that the accuracy of par-
tial dependencies is much higher than that of en-
tire dependency trees. Previous studies (McDon-
ald and Pereira, 2006; Yamada and Matsumoto,
2003; Zhang and Clark, 2008) show that the accu-
racies of complete trees are about 40% for English
and about 35% for Chinese, while the accuracies
of relations between two words are much higher:
about 90% for English and about 85% for Chinese.
From these observations, we may conjecture that
it is possible to conduct a more effective selection
by using subtrees as the unit of information.
The use of word pairs in auto-parsed data was
tried in van Noord (2007) and Chen et al (2008).
However, the information on word pairs is limited.
To provide richer information, we consider more
words besides word pairs. Specifically, we use
subtrees containing two or three words extracted
from dependency trees in the auto-parsed data. To
demonstrate the effectiveness of our proposed ap-
proach, we present experimental results on En-
570
glish and Chinese data. We show that this sim-
ple approach greatly improves the accuracy and
that the use of richer structures (i.e, word triples)
indeed gives additional improvement. We also
demonstrate that our approach and other improve-
ment techniques (Koo et al, 2008; Nivre and Mc-
Donald, 2008) are complementary and that we can
achieve very high accuracies when we combine
our method with other improvement techniques.
Specifically, we achieve the best accuracy for the
Chinese data.
The rest of this paper is as follows: Section 2
introduces the background of dependency parsing.
Section 3 proposes an approach for extracting sub-
trees and represents the subtree-based features for
dependency parsers. Section 4 explains the ex-
perimental results and Section 5 discusses related
work. Finally, in section 6 we draw conclusions.
2 Dependency parsing
Dependency parsing assigns head-dependent rela-
tions between the words in a sentence. A sim-
ple example is shown in Figure 1, where an arc
between two words indicates a dependency rela-
tion between them. For example, the arc between
?ate? and ?fish? indicates that ?ate? is the head of
?fish? and ?fish? is the dependent. The arc be-
tween ?ROOT? and ?ate? indicates that ?ate? is the
ROOT of the sentence.
ROOT    I    ate    the    fish    with    a    fork    .
Figure 1: Example for dependency structure
2.1 Parsing approach
For dependency parsing, there ar two main
types of parsing models (Nivre and McDonald,
2008): graph-based model and transition-based
model, which achieved state-of-the-art accuracy
for a wide range of languages as shown in recent
CoNLL shared tasks (Buchholz et al, 2006; Nivre
et al, 2007). Our subtree-based features can be
applied in both of the two parsing models.
In this paper, as the base parsing system, we
employ the graph-based MST parsing model pro-
posed by McDonald et al (2005) and McDonald
and Pereira (2006), which uses the idea of Max-
imum Spanning Trees of a graph and large mar-
gin structured learning algorithms. The details
of parsing model were presented in McDonald et
al. (2005) and McDonald and Pereira (2006).
2.2 Baseline Parser
In the MST parsing model, there are two well-used
modes: the first-order and the second-order. The
first-order model uses first-order features that are
defined over single graph edges and the second-
order model adds second-order features that are
defined on adjacent edges.
For the parsing of unannotated data, we use the
first-order MST parsing model, because we need
to parse a large number of sentences and the parser
must be fast. We call this parser the Baseline
Parser.
3 Our approach
In this section, we describe our approach of ex-
tracting subtrees from unannotated data. First,
we preprocess unannotated data using the Baseline
Parser and obtain auto-parsed data. Subsequently,
we extract the subtrees from dependency trees in
the auto-parsed data. Finally, we generate subtree-
based features for the parsing models.
3.1 Subtrees extraction
To ease explanation, we transform the dependency
structure into a more tree-like structure as shown
in Figure 2, the sentence is the same as the one in
Figure 1.
ate
I                             fish      with                    .
the                                       fork
ROOT
a
I       ate      the      fish      with      a      fork .
Figure 2: Example for dependency structure in
tree-format
Our task is to extract subtrees from dependency
trees. If a subtree contains two nodes, we call it a
bigram-subtree. If a subtree contains three nodes,
we call it a trigram-subtree.
3.2 List of subtrees
We extract subtrees from dependency trees and
store them in list L
st
. First, we extract bigram-
subtrees that contain two words. If two words have
571
a dependency relation in a tree, we add these two
words as a subtree into list L
st
. Similarly, we can
extract trigram-subtrees. Note that the dependency
direction and the order of the words in the original
sentence are important in the extraction. To enable
this, the subtrees are encoded in the string format
that is expressed as st = w : wid : hid(?w :
wid : hid)+
1
, where w refers to a word in the
subtree, wid refers to the ID (starting from 1) of
a word in the subtree (words are ordered accord-
ing to the positions of the original sentence)
2
, and
hid refers to an ID of the head of the word (hid=0
means that this word is the root of a subtree). For
example, ?ate? and ?fish? have a right dependency
arc in the sentence shown in Figure 2. So the
subtree is encoded as ?ate:1:0-fish:2:1?. Figure 3
shows all the subtrees extracted from the sentence
in Figure 2, where the subtrees in (a) are bigram-
subtrees and the ones in (b) are trigram-subtrees.
ateI I:1:1-ate:2:0atefish ate:1:0-fish:2:1
atefish  with ate:1:0-fish:2:1-with:3:1
atewith ate:1:0-with:2:1
ate
.
ate:1:0-.:2:1
fishthe the:1:1-fish:2:0
with fork with:1:0-fork:2:1forka a:1:1-fork:2:0
ate
with   . ate:1:0-with:2:1-.:3:1(b)
(a)
Figure 3: Examples of subtrees
Note that we only used the trigram-subtrees
containing a head, its dependent d1, and d1?s
leftmost right sibling
3
. We could not consider
the case where two children are on different
sides
4
of the head (for instance, ?I? and ?fish?
for ?ate? in Figure 2). We also do not use the
child-parent-grandparent type (grandparent-type
in short) trigram-subtrees. These are due to the
limitations of the parsing algorithm of (McDonald
and Pereira, 2006), which does not allow the fea-
tures defined on those types of trigram-subtrees.
We extract the subtrees from the auto-parsed
data, then merge the same subtrees into one en-
try, and count their frequency. We eliminate all
subtrees that occur only once in the data.
1
+ refers to matching the preceding element one or more
times and is the same as a regular expression in Perl.
2
So, wid is in fact redundant but we include it for ease of
understanding.
3
Note that the order of the siblings is based on the order
of the words in the original sentence.
4
Here, ?side? means the position of a word relative to the
head in the original sentence.
3.3 Subtree-based features
We represent new features based on the extracted
subtrees and call them subtree-based features. The
features based on bigram-subtrees correspond to
the first-order features in the MST parsing model
and those based on trigram-subtrees features cor-
respond to the second-order features.
We first group the extracted subtrees into dif-
ferent sets based on their frequencies. After ex-
periments with many different threshold settings
on development data sets, we chose the follow-
ing way. We group the subtrees into three sets
corresponding to three levels of frequency: ?high-
frequency (HF)?, ?middle-frequency (MF)?, and
?low-frequency (LF)?. HF, MF, and LF are used
as set IDs for the three sets. The following are the
settings: if a subtree is one of the TOP-10% most
frequent subtrees, it is in set HF; else if a subtree is
one of the TOP-20% subtrees, it is in set MF; else
it is in set LF. Note that we compute these levels
within a set of subtrees with the same number of
nodes. We store the set ID for every subtree in
L
st
. For example, if subtree ?ate:1:0-with:2:1? is
among the TOP-10%, its set ID is HF.
3.3.1 First-order subtree-based features
The first-order features are based on bigram-
subtrees that are related to word pairs. We gener-
ate new features for a head h and a dependent d in
the parsing process. Figure 4-(a)
5
shows the words
and their surrounding words, where h
?1
refers to
the word to the left of the head in the sentence,
h
+1
refers to the word to the right of the head, d
?1
refers to the word to the left of the dependent, and
d
+1
refers to the word to the right of the depen-
dent. Temporary bigram-subtrees are formed by
word pairs that are linked by dashed-lines in the
figure. Then we retrieve these subtrees in L
st
to
get their set IDs (if a subtree is not included in
L
st
, its set ID is ZERO. That is, we have four sets:
HF, MF, LF, and ZERO.).
Then we generate first-order subtree-based fea-
tures, consisting of indicator functions for set IDs
of the retrieved bigram-subtrees. When generating
subtree-based features, each dashed line in Figure
4-(a) triggers a different feature.
To demonstrate how to generate first-order
subtree-based features, we use an example that is
as follows. Suppose that we are going to parse the
sentence ?He ate the cake with a fork.? as shown
5
Please note that d could be before h.
572
? h
-1 h      h+1 ? d-1     d      d+1  ?
(a)
(b)
? h      ? d1 ? d2 ?
Figure 4: Word pairs and triple for feature repre-
sentation
in Figure 5, where h is ?ate? and d is ?with?.
We can generate the features for the pairs linked
by dashed-lines, such as h ? d, h ? d
+1
and so
on. Then we have the temporary bigram-subtrees
?ate:1:0-with:2:1? for h ? d and ?ate:1:0-a:2:1?
for h ? d
+1
, and so on. If we can find subtree
?ate:1:0-with:2:1? for h ? d from L
st
with set ID
HF, we generate the feature ?H-D:HF?, and if we
find subtree ?ate:1:0-a:2:1? for h?d
+1
with set ID
ZERO, we generate the feature ?H-D+1:ZERO?.
The other three features are also generated simi-
larly.
He    ate    the    cake    with    a    fork    .
h
-1 h       h+1 d-1 d      d+1
Figure 5: First-order subtree-based features
3.3.2 Second-order subtree-based features
The second-order features are based on trigram-
subtrees that are related to triples of words. We
generate features for a triple of a head h, its de-
pendent d1, and d1?s right-leftmost sibling d2.
The triple is shown in Figure 4-(b). A temporary
trigram-subtree is formed by the word forms of h,
d1, and d2. Then we retrieve the subtree in L
st
to
get its set ID. In addition, we consider the triples
of ?h-NULL?
6
, d1, and d2, which means that we
only check the words of sibling nodes without
checking the head word.
Then, we generate second-order subtree-based
features, consisting of indicator functions for set
IDs of the retrieved trigram-subtrees.
6
h-NULL is a dummy token
We also generate combined features involving
the set IDs and part-of-speech tags of heads, and
the set IDs and word forms of heads. Specifically,
for any feature related to word form, we remove
this feature if the word is not one of the Top-N
most frequent words in the training data. We used
N=1000 for the experiments in this paper. This
method can reduce the size of the feature sets.
In this paper, we only used bigram-subtrees and
the limited form of trigram-subtrees, though in
theory we can use k-gram-subtrees, which are lim-
ited in the same way as our trigram subtrees, in
(k-1)th-order MST parsing models mentioned in
McDonald and Pereira (2006) or use grandparent-
type trigram-subtrees in parsing models of Car-
reras (2007). Although the higher-order MST
parsing models will be slow with exact inference,
requiring O(n
k
) time (McDonald and Pereira,
2006), it might be possible to use higher-order k-
gram subtrees with approximated parsing model
in the future. Of course, our method can also be
easily extended to the labeled dependency case.
4 Experiments
In order to evaluate the effectiveness of the
subtree-based features, we conducted experiments
on English data and Chinese Data.
For English, we used the Penn Treebank (Mar-
cus et al, 1993) in our experiments and the tool
?Penn2Malt?
7
to convert the data into dependency
structures using a standard set of head rules (Ya-
mada and Matsumoto, 2003). To match previ-
ous work (McDonald et al, 2005; McDonald and
Pereira, 2006; Koo et al, 2008), we split the data
into a training set (sections 2-21), a development
set (Section 22), and a test set (section 23). Fol-
lowing the work of Koo et al (2008), we used
the MXPOST (Ratnaparkhi, 1996) tagger trained
on training data to provide part-of-speech tags for
the development and the test set, and we used 10-
way jackknifing to generate tags for the training
set. For the unannotated data, we used the BLLIP
corpus (Charniak et al, 2000) that contains about
43 million words of WSJ text.
8
We used the MX-
POST tagger trained on training data to assign
part-of-speech tags and used the Basic Parser to
process the sentences of the BLLIP corpus.
For Chinese, we used the Chinese Treebank
7
http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
8
We ensured that the text used for extracting subtrees did
not include the sentences of the Penn Treebank.
573
(CTB) version 4.0
9
in the experiments. We also
used the ?Penn2Malt? tool to convert the data and
created a data split: files 1-270 and files 400-931
for training, files 271-300 for testing, and files
301-325 for development. We used gold standard
segmentation and part-of-speech tags in the CTB.
The data partition and part-of-speech settings were
chosen to match previous work (Chen et al, 2008;
Yu et al, 2008). For the unannotated data, we
used the PFR corpus
10
, which has approximately
15 million words whose segmentation and POS
tags are given. We used its original segmentation
though there are differences in segmentation pol-
icy between CTB and this corpus. As for POS
tags, we discarded the original POS tags and as-
signed CTB style POS tags using a TNT-based
tagger (Brants, 2000) trained on the training data.
We used the Basic Parser to process all the sen-
tences of the PFR corpus.
We measured the parser quality by the unla-
beled attachment score (UAS), i.e., the percentage
of tokens (excluding all punctuation tokens) with
the correct HEAD. And we also evaluated on com-
plete dependency analysis.
4.1 Experimental Results
In our experiments, we used MSTParser, a
freely available implementation
11
of the first- and
second-order MST parsing models. For baseline
systems, we used the first- and second-order basic
features, which were the same as the features used
by McDonald and Pereira (2006), and we used
the default settings of MSTParser throughout the
paper: iters=10; training-k=1; decode-type=proj.
We implemented our systems based on the MST-
Parser by incorporating the subtree-based features.
4.1.1 Main results of English data
English
UAS Complete
Ord1 90.95 37.45
Ord1s 91.76(+0.81) 40.68
Ord2 91.71 42.88
Ord2s 92.51(+0.80) 46.19
Ord2b 92.28(+0.57) 45.44
Ord2t 92.06(+0.35) 42.96
Table 1: Dependency parsing results for English
9
http://www.cis.upenn.edu/?chinese/.
10
http://www.icl.pku.edu.
11
http://mstparser.sourceforge.net
The results are shown in Table 1, where
Ord1/Ord2 refers to a first-/second-order
MSTParser with basic features, Ord1s/Ord2s
refers to a first-/second-order MSTParser with
basic+subtree-based features, and the improve-
ments by the subtree-based features over the basic
features are shown in parentheses. Note that
we use both the bigram- and trigram- subtrees
in Ord2s. The parsers using the subtree-based
features consistently outperformed those using
the basic features. For the first-order parser,
we found that there is an absolute improvement
of 0.81 points (UAS) by adding subtree-based
features. For the second-order parser, we got an
absolute improvement of 0.8 points (UAS) by
including subtree-based features. The improve-
ments of parsing with subtree-based features were
significant in McNemar?s Test (p < 10
?6
).
We also checked the sole effect of bigram- and
trigram-subtrees. The results are also shown in
Table 1, where Ord2b/Ord2t refers to a second-
order MSTParser with bigram-/trigram-subtrees
only. The results showed that trigram-subtrees can
provide further improvement, although the effect
of the bigram-subtrees seemed larger.
4.1.2 Comparative results of English data
Table 2 shows the performance of the systems
that were compared, where Y&M2003 refers to
the parser of Yamada and Matsumoto (2003),
CO2006 refers to the parser of Corston-Oliver et
al. (2006), Hall2006 refers to the parser of Hall
et al (2006), Wang2007 refers to the parser of
Wang et al (2007), Z&C 2008 refers to the combi-
nation graph-based and transition-based system of
Zhang and Clark (2008), KOO08-dep1c/KOO08-
dep2c refers to a graph-based system with first-
/second-order cluster-based features by Koo et al
(2008), and Carreras2008 refers to the paper of
Carreras et al (2008). The results showed that
Ord2s performed better than the first five systems.
The second-order system of Koo et al (2008) per-
formed better than our systems. The reason may
be that the MSTParser only uses sibling interac-
tions for second-order, while Koo et al (2008)
uses both sibling and grandparent interactions, and
uses cluster-based features. Carreras et al (2008)
reported a very high accuracy using information of
constituent structure of the TAG grammar formal-
ism. In our systems, we did not use such knowl-
edge.
Our subtree-based features could be combined
574
with the techniques presented in other work,
such as the cluster-based features in Koo et al
(2008), the integrating methods of Zhang and
Clark (2008), and Nivre and McDonald (2008),
and the parsing methods of Carreras et al (2008).
English
UAS Complete
Y&M2003 90.3 38.4
CO2006 90.8 37.6
Hall2006 89.4 36.4
Wang2007 89.2 34.4
Z&C2008 92.1 45.4
KOO08-dep1c 92.23 ?
KOO08-dep2c 93.16 ?
Carreras2008 93.5 ?
Ord1 90.95 37.45
Ord1s 91.76 40.68
Ord1c 91.88 40.71
Ord1i 91.68 41.43
Ord1sc 92.20 42.98
Ord1sci 92.60 44.28
Ord2 91.71 42.88
Ord2s 92.51 46.19
Ord2c 92.40 44.08
Ord2i 92.12 44.37
Ord2sc 92.70 46.56
Ord2sci 93.16 47.15
Table 2: Dependency parsing results for English,
for our parsers and previous work
To demonstrate that our approach and other
work are complementary, we thus implemented
a system using all the techniques we had at hand
that used subtree- and cluster-based features
and applied the integrating method of Nivre and
McDonald (2008). We used the word clustering
tool
12
, which was used by Koo et al (2008), to
produce word clusters on the BLLIP corpus. The
cluster-based features were the same as the fea-
tures used by Koo et al (2008). For the integrating
method, we used the transition MaxEnt-based
parser of Zhao and Kit (2008) because it was
faster than the MaltParser. The results are shown
in the bottom part of Table 2, where Ord1c/Ord2c
refers to a first-/second-order MSTParser with
cluster-based features, Ord1i/Ordli refers to a first-
/second-order MSTParser with integrating-based
features, Ord1sc/Ord2sc refers to a first-/second-
order MSTParser with subtree-based+cluster-
based features, and Ord1sci/Ord2sci refers to
a first-/second-order MSTParser with subtree-
based+cluster-based+integrating-based features.
Ord1c/Ord2c was worse than KOO08-dep1c/-
dep2c, but Ord1sci outperformed KOO08-dep1c
12
http://www.cs.berkeley.edu/?pliang/software/brown-
cluster-1.2.zip
and Ord2sci performed similarly to KOO08-dep2c
by using all of the techniques we had. These
results indicated that subtree-based features can
provide different information and work well with
other techniques.
4.1.3 Main results of Chinese data
The results are shown in Table 3 where abbrevia-
tions are the same as in Table 1. As in the English
experiments, parsers with the subtree-based fea-
tures outperformed parsers with the basic features,
and second-order parsers outperformed first-order
parsers. For the first-order parser, the subtree-
based features provided 1.3 absolute points im-
provement. For the second-order parser, the
subtree-based features achieved an absolute im-
provement of 1.25 points. The improvements of
parsing with subtree-based features were signifi-
cant in McNemar?s Test (p < 10
?5
).
Chinese
UAS Complete
Ord1 86.38 40.80
Ord1s 87.68(+1.30) 42.24
Ord2 88.18 47.12
Ord2s 89.43(+1.25) 47.53
Ord2b 89.16(+0.98) 47.12
Ord2t 88.55(+0.37) 47.12
Table 3: Dependency parsing results for Chinese.
4.1.4 Comparative results of Chinese data
Table 4 shows the comparative results, where
Wang2007 refers to the parser of Wang et
al. (2007), Chen2008 refers to the parser of Chen
et al (2008), and Yu2008 refers to the parser of
Yu et al (2008) that is the best reported results
for this data set. And ?all words? refers to all the
sentences in test set and ?? 40 words?
13
refers to
the sentences with the length up to 40. The table
shows that our parsers outperformed previous sys-
tems.
We also implemented integrating systems for
Chinese data as well. When we applied the
cluster-based features, the performance dropped a
little. The reason may be that we are using gold-
POS tags for Chinese data
14
. Thus we did not
13
Wang et al (2007) and Chen et al (2008) reported the
scores on these sentences.
14
We tried to use the cluster-based features for Chinese
with the same setting of POS tags as English data, then the
cluster-based features did provide improvement.
575
use cluster-based features for the integrating sys-
tems. The results are shown in Table 4, where
Ord1si/Ord2si refers to the first-order/second-
order system with subtree-based+intergrating-
based features. We found that the integrating sys-
tems provided better results. Overall, we have
achieved a high accuracy, which is the best known
result for this dataset.
Zhang and Clark (2008) and Duan et al (2007)
reported results on a different data split of Penn
Chinese Treebank. We also ran our systems
(Ord2s) on their data and provided UAS 86.70
(for non-root words)/77.39 (for root words), better
than their results: 86.21/76.26 in Zhang and Clark
(2008) and 84.36/73.70 in Duan et al (2007).
Chinese
all words ? 40 words
UAS Complete UAS Complete
Wang2007 ? ? 86.6 28.4
Chen2008 86.52 ? 88.4 ?
Yu2008 87.26 ? ? ?
Ord1s 87.68 42.24 91.11 54.40
Ord1si 88.24 43.96 91.32 55.93
Ord2s 89.43 47.53 91.67 59.77
Ord2si 89.91 48.56 92.34 62.83
Table 4: Dependency parsing results for Chinese,
for our parsers and for previous work
4.1.5 Effect of different sizes of unannotated
data
Here, we consider the improvement relative to the
sizes of the unannotated data. Figure 6 shows the
results of first-order parsers with different num-
bers of words in the unannotated data. Please note
that the size of full English unannotated data is
43M and the size of full Chinese unannotated data
is 15M. From the figure, we found that the parser
obtained more benefits as we added more unanno-
tated data.
 86
 87
 88
 89
 90
 91
 92
4332168420
U
A
S
Size of unannotated data(M)
EnglishChinese
Figure 6: Results with different sizes of large-
scale unannotated data.
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
 0  1  2  3  4  5  6
P
e
r
c
e
n
t
a
g
e
 
(
s
m
o
o
t
h
e
d
)
Number of unknown words
BetterNoChangeWorse
Figure 7: Improvement relative to unknown words
for English
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  1  2  3  4  5  6
P
e
r
c
e
n
t
a
g
e
 
(
s
m
o
o
t
h
e
d
)
Number of unknown words
BetterNoChangeWorse
Figure 8: Improvement relative to unknown words
for Chinese
4.2 Additional Analysis
In this section, we investigated the results on
sentence level from different views. For Fig-
ures 7-12, we classified each sentence into one of
three classes: ?Better? for those where the pro-
posed parsers provided better results relative to
the parsers with basic features, ?Worse? for those
where the proposed parsers provided worse results
relative to the basic parsers, and ?NoChange? for
those where the accuracies remained the same.
4.2.1 Unknown words
Here, we consider the unknown word
15
problem,
which is an important issue for parsing. We cal-
culated the number of unknown words in one sen-
tence, and listed the changes of the sentences with
unknown words. Here, we compared the Ord1
system and the Ord1s system.
Figures 7 and 8 show the results, where the x
axis refers to the number of unknown words in one
sentence and the y axis shows the percentages of
the three classes. For example, for the sentences
having three unknown words in the Chinese data,
31.58% improved, 23.68% worsened, and 44.74%
were unchanged. We did not show the results of
15
An unknown word is a word that is not included in the
training data.
576
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
43210
P
e
r
c
e
n
t
a
g
e
 
(
s
m
o
o
t
h
e
d
)
Number of CCs
BetterNoChangeWorse
Figure 9: Improvement relative to number of
conjunctions for English
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
3210
P
e
r
c
e
n
t
a
g
e
 
(
s
m
o
o
t
h
e
d
)
Number of CCs
BetterNoChangeWorse
Figure 10: Improvement relative to number of
conjunctions for Chinese
the sentences with more than six unknown words
because their numbers were very small. The Bet-
ter and Worse curves showed that our approach al-
ways provided better results. The results indicated
that the improvements apparently became larger
when the sentences had more unknown words for
the Chinese data. And for the English data, the
graph also showed the similar trend, although the
improvements for the sentences have three and
four unknown words were slightly less than the
others.
4.2.2 Coordinating conjunctions
We analyzed our new parsers? behavior for coordi-
nating conjunction structures, which is a very dif-
ficult problem for parsing (Kawahara and Kuro-
hashi, 2008). Here, we compared the Ord2 system
with the Ord2s system.
Figures 9 and 10 show how the subtree-based
features affect accuracy as a function of the num-
ber of conjunctions, where the x axis refers to the
number of conjunctions in one sentence and the
y axis shows the percentages of the three classes.
The figures indicated that the subtree-based fea-
tures improved the coordinating conjunction prob-
lem. In the trigram-subtree list, many subtrees
are related to coordinating conjunctions, such as
?utilities:1:3 and:2:3 businesses:3:0? and ?pull:1:0
and:2:1 protect:3:1?. These subtrees can provide
additional information for parsing models.
4.2.3 PP attachment
We analyzed our new parsers? behavior for
preposition-phrase attachment, which is also a dif-
ficult task for parsing (Ratnaparkhi et al, 1994).
We compared the Ord2 system with the Ord2s sys-
tem. Figures 11 and 12 show how the subtree-
based features affect accuracy as a function of the
number of prepositions, where the x axis refers to
the number of prepositions in one sentence and the
y axis shows the percentages of the three classes.
The figures indicated that the subtree-based fea-
tures improved preposition-phrase attachment.
5 Related work
Our approach is to incorporate unannotated data
into parsing models for dependency parsing. Sev-
eral previous studies relevant to our approach have
been conducted.
Chen et al (2008) previously proposed an ap-
proach that used the information on short de-
pendency relations for Chinese dependency pars-
ing. They only used the word pairs within two
word distances for a transition-based parsing al-
gorithm. The approach in this paper differs in
that we use richer information on trigram-subtrees
besides bigram-subtrees that contain word pairs.
And our work is focused on graph-based parsing
models as opposed to transition-based models. Yu
et al (2008) constructed case structures from auto-
parsed data and utilized them in parsing. Com-
pared with their method, our method is much sim-
pler but has great effects.
Koo et al (2008) used the Brown algorithm to
produce word clusters on large-scale unannotated
data and represented new features based on the
clusters for parsing models. The cluster-based fea-
tures provided very impressive results. In addition,
they used the parsing model by Carreras (2007)
that applied second-order features on both sibling
and grandparent interactions. Note that our ap-
proach and their approach are complementary in
that we can use both subtree- and cluster-based
features for parsing models. The experimental re-
sults showed that we achieved better accuracy for
first-order models when we used both of these two
types of features.
Sagae and Tsujii (2007) presented an co-
training approach for dependency parsing adap-
577
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
 0  1  2  3  4  5  6  7
P
e
r
c
e
n
t
a
g
e
 
(
s
m
o
o
t
h
e
d
)
Number of prepositions
BetterNoChangeWorse
Figure 11: Improvement relative to number of
prepositions for English
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
3210
P
e
r
c
e
n
t
a
g
e
 
(
s
m
o
o
t
h
e
d
)
Number of prepositions
BetterNoChangeWorse
Figure 12: Improvement relative to number of
prepositions for Chinese
tation. They used two parsers to parse the sen-
tences in unannotated data and selected only iden-
tical results produced by the two parsers. Then,
they retrained a parser on newly parsed sentences
and the original labeled data. Our approach repre-
sents subtree-based features on the original gold-
standard data to retrain parsers. McClosky et
al. (2006) presented a self-training approach for
phrase structure parsing and the approach was
shown to be effective in practice. However,
their approach depends on a high-quality reranker,
while we simply augment the features of an ex-
isting parser. Moreover, we could use the output
of our systems for co-training/self-training tech-
niques.
6 Conclusions
We present a simple and effective approach to
improve dependency parsing using subtrees from
auto-parsed data. In our method, first we use a
baseline parser to parse large-scale unannotated
data, and then we extract subtrees from depen-
dency parsing trees in the auto-parsed data. Fi-
nally, we construct new subtree-based features for
parsing models. The results show that our ap-
proach significantly outperforms baseline systems.
We also show that our approach and other tech-
niques are complementary, and then achieve the
best reported accuracy for the Chinese data and an
accuracy that is competitive with the best known
systems for the English data.
References
T. Brants. 2000. TnT?a statistical part-of-speech tag-
ger. Proceedings of ANLP, pages 224?231.
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.
2006. CoNLL-X shared task on multilingual depen-
dency parsing. Proceedings of CoNLL-X.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. Tag, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of CoNLL 2008, pages 9?16, Manchester, Eng-
land, August. Coling 2008 Organizing Committee.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 957?961.
E. Charniak, D. Blaheta, N. Ge, K. Hall, J. Hale, and
M. Johnson. 2000. BLLIP 1987-89 WSJ Corpus
Release 1, LDC2000T43. Linguistic Data Consor-
tium.
WL. Chen, D. Kawahara, K. Uchimoto, YJ. Zhang, and
H. Isahara. 2008. Dependency parsing with short
dependency relations in unlabeled data. In Proceed-
ings of IJCNLP 2008.
S. Corston-Oliver, A. Aue, Kevin. Duh, and Eric Ring-
ger. 2006. Multilingual dependency parsing using
bayes point machines. In HLT-NAACL2006.
H. Cui, RX. Sun, KY. Li, MY. Kan, and TS. Chua.
2005. Question answering passage retrieval us-
ing dependency relations. In Proceedings of SIGIR
2005, pages 400?407, New York, NY, USA. ACM.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic models for action-based chinese dependency
parsing. In Proceedings of ECML/ECPPKDD, War-
saw, Poland.
Johan Hall, Joakim Nivre, and Jens Nilsson. 2006.
Discriminative classifiers for deterministic depen-
dency parsing. In In Proceedings of CoLING-ACL.
D. Kawahara and S. Kurohashi. 2008. Coordination
disambiguation without any similarities. In Pro-
ceedings of Coling 2008, pages 425?432, Manch-
ester, UK, August.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceed-
ings of ACL-08: HLT, Columbus, Ohio, June.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn Treebank. Computational Linguis-
ticss, 19(2):313?330.
578
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proceedings of Coling-ACL, pages 337?344.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proc. of EACL2006.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proc. of ACL 2005.
T. Nakazawa, K. Yu, D. Kawahara, and S. Kurohashi.
2006. Example-based machine translation based on
deeper nlp. In Proceedings of IWSLT 2006, pages
64?70, Kyoto, Japan.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL-08: HLT, Columbus, Ohio,
June.
J. Nivre, J. Hall, S. K?ubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, pages 915?932.
A. Ratnaparkhi, J. Reynar, and S. Roukos. 1994. A
maximum entropy model for prepositional phrase at-
tachment. In Proceedings of HLT, pages 250?255.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of EMNLP,
pages 133?142.
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with LR models and parser en-
sembles. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 1044?1050.
M. Steedman, M. Osborne, A. Sarkar, S. Clark,
R. Hwa, J. Hockenmaier, P. Ruhlen, S. Baker, and
J. Crim. 2003. Bootstrapping statistical parsers
from small datasets. In Proceedings of EACL 2003,
pages 331?338.
Gertjan van Noord. 2007. Using self-trained bilexical
preferences to improve disambiguation accuracy. In
Proceedings of IWPT-07, June.
Qin Iris Wang, Dekang Lin, and Dale Schuurmans.
2007. Simple training of dependency parsers via
structured boosting. In Proceedings of IJCAI2007.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proceedings of IWPT2003, pages 195?206.
K. Yu, D. Kawahara, and S. Kurohashi. 2008. Chi-
nese dependency parsing with large scale automat-
ically constructed case structures. In Proceedings
of Coling 2008, pages 1049?1056, Manchester, UK,
August.
Y. Zhang and S. Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of EMNLP 2008, pages 562?571, Hon-
olulu, Hawaii, October.
H. Zhao and CY. Kit. 2008. Parsing syntactic and
semantic dependencies with two single-stage max-
imum entropy models. In Proceedings of CoNLL
2008, pages 203?207, Manchester, England, Au-
gust.
579
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 929?937,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Hypernym Discovery Based on Distributional Similarity                     
and Hierarchical Structures 
Ichiro Yamada?, Kentaro Torisawa?, Jun?ichi Kazama?, Kow Kuroda?,  
Masaki Murata?, Stijn De Saeger?, Francis Bond? and Asuka Sumida? 
 
?National Institute of Information and Communications Technology 
3-5 Hikaridai, Keihannna Science City 619-0289, JAPAN 
{iyamada,torisawa,kazama,kuroda,murata,stijn,bond}@nict.go.jp
?Japan Advanced Institute of Science and Technology 
1-1 Asahidai, Nomi-shi, Ishikawa-ken 923-1211, JAPAN 
a-sumida@jaist.ac.jp 
 
Abstract 
This paper presents a new method of devel-
oping a large-scale hyponymy relation data-
base by combining Wikipedia and other Web 
documents. We attach new words to the hy-
ponymy database extracted from Wikipedia 
by using distributional similarity calculated 
from documents on the Web. For a given tar-
get word, our algorithm first finds k similar 
words from the Wikipedia database. Then, 
the hypernyms of these k similar words are 
assigned scores by considering the distribu-
tional similarities and hierarchical distances 
in the Wikipedia database. Finally, new hy-
ponymy relations are output according to the 
scores. In this paper, we tested two distribu-
tional similarities. One is based on raw verb-
noun dependencies (which we call ?RVD?), 
and the other is based on a large-scale clus-
tering of verb-noun dependencies (called 
?CVD?). Our method achieved an attachment 
accuracy of 91.0% for the top 10,000 rela-
tions, and an attachment accuracy of 74.5% 
for the top 100,000 relations when using 
CVD. This was a far better outcome com-
pared to the other baseline approaches. Ex-
cluding the region that had very high scores, 
CVD was found to be more effective than 
RVD. We also confirmed that most relations 
extracted by our method cannot be extracted 
merely by applying the well-known lexico-
syntactic patterns to Web documents. 
1 Introduction 
Large-scale taxonomies such as WordNet (Fell-
baum 1998) play an important role in informa-
tion extraction and question answering. However, 
extremely high costs are borne to manually en-
large and maintain such taxonomies. Thus, appli-
cations using these taxonomies tend to face the 
drawback of data sparseness. This paper presents 
a new method for discovering a large set of hy-
ponymy relations. Here, a word1 X is regarded as 
a hypernym of a word Y if Y is a kind of X or Y 
is an instance of X. We are able to generate 
large-scale hyponymy relations by attaching new 
words to the hyponymy database extracted from 
Wikipedia (referred to as ?Wikipedia relation 
database?) by using distributional similarity cal-
culated from Web documents. Relations ex-
tracted from Wikipedia are relatively clean. On 
the other hand, reliable distributional similarity 
can be calculated using a large number of docu-
ments on the Web. In this paper, we combine the 
advantages of these two resources.  
Using distributional similarity, our algorithm 
first computes k similar words for a target word. 
Then, each k similar word assigns a score to its 
ancestors in the hierarchical structures of the 
Wikipedia relation database. The hypernym that 
has the highest score for the target word is se-
lected as the hypernym of the target word. Figure 
1 is an overview of the proposed approach. 
In the experiment, we extracted hypernyms for 
approximately 670,000 target words that are not 
included in the Wikipedia relation database but 
are found on the Web. We tested two distribu-
tional similarities: one based on raw verb-noun 
dependencies (RVD) and the other based on a 
large-scale clustering of verb-noun dependencies 
(CVD). The experimental results showed that the 
proposed methods were more effective than the 
other baseline approaches. In addition, we con-
firmed that most of the relations extracted by our 
method could not be extracted using the lexico-
syntactic pattern-based method.  
In the remainder of this paper, we first intro-
                                                 
1 In this paper, we use the term ?word? for both ?a 
single-word word? and ?a multi-word word.? 
929
duce some related works in Section 2. Section 3 
describes the Wikipedia relation database. Sec-
tion 4 describes the distributional similarity cal-
culated by the two methods. In Section 5, we 
describe a method to discover an appropriate 
hypernym for each target word. The experimen-
tal results are presented in Section 6 before con-
cluding the paper in Section 7. 
2 Related Works 
Most previous researchers have relied on lex-
ico-syntactic patterns for hyponymy acquisition. 
Lexico-syntactic patterns were first used by 
Hearst (1992). The patterns used by her included 
?NP0 such as NP1,? in which NP0 is a hypernym 
of NP1. Using these patterns as seeds, Hearst dis-
covered new patterns by which to semi-
automatically extract hyponymy relations. Pantel 
et al (2004a) proposed a method to automatical-
ly discover the patterns using a minimal edit dis-
tance. Ando et al (2003) applied predefined lex-
ico-syntactic patterns to Japanese news articles. 
Snow et al (2005) generalized these lexico-
syntactic pattern-based methods by using depen-
dency path features for machine learning. Then, 
they extended the framework such that this me-
thod was capable of making use of heterogenous 
evidence (Snow et al 2006). These pattern-based 
methods require the co-occurrences of a target 
word and the hypernym in a document. It should 
be noted that the requirement of such co-
occurrences actually poses a problem when we 
extract a large set of hyponymy relations since 
they are not frequently observed (Shinzato et al 
2004, Pantel et al 2004b). 
Clustering-based methods have been proposed 
as another approach. Caraballo (1999), Pantel et 
al. (2004b), and Shinzato et al (2004) proposed a 
method to find a common hypernym for word 
classes, which are automatically constructed us-
ing some measures of word similarities or hierar-
chical structures in HTML documents. Etzioni et 
al. (2005) used both a pattern-based approach 
and a clustering-based approach. The required 
amount of co-occurrences is significantly re-
duced due to class-based generalization 
processes. Note that these clustering-based me-
thods obtain the same hypernym for all the words 
in a particular class. This causes a problem for 
selecting an appropriate hypernym for each word 
in the case when the granularity or the construc-
tion of the classes is incorrect. Figure 2 shows 
the drawbacks of the existing approaches. 
Ponzetto et al (2007) and Sumida et al (2008) 
proposed a method for acquiring hyponymy rela-
tions from Wikipedia. This Wikipedia-based ap-
proach can extract a large volume of hyponymy 
relations with high accuracy. However, it is also 
true that this approach does not account for many 
words that usually appear in Web documents; 
this could be because of the unbalanced topics in 
Wikipedia or merely because of the incomplete 
coverage of articles on Wikipedia. Our method 
can target words that frequently appear on the 
Web but are not included in the Wikipedia rela-
tion database, thus making the results of the Wi-
kipedia-based approach richer and more ba-
lanced. Our approach uses distributional similari-
Figure 1: Overview of the proposed approach. 
hypernym : 
Target word:  Selected from the Web 
: word
k similar words
No direct co-occurrences of 
hypernym and hyponym in 
corpora are needed.
Selected from hypernyms in the 
Wikipedia relation database.
A hypernym is selected for 
each word independently.
Wikipedia relation database
Wikipedia-based approach
(Ponzetto et al 2007 and 
Sumida et al 2008)
Hyponymy relations are 
extracted using the layout 
information of Wikipedia.
Wikipedia
Figure 2: Drawbacks in existing approaches for hypo-
nymy acquisition. 
Pattern-based method
(Hearst 1992, Pantel et al 
2004a, Ando et al 2003, 
Snow et al 2005, Snow et al 
2006, and Etzioni et al 2005)
Clustering-based method
(Caraballo 1999, Pantel et al 
2004b, Shinzato et al 2004, 
and Etzioni et al 2005)
DocumentsCorpus/documents
Co-occurrences 
in a pattern are 
needed 
hypernym such as word hypernym ..?   word
word
word
wordword
Word Class
word
The same hypernym 
is selected for all 
words in a class.
930
ty, which is computed based on the noun-verb 
dependency profiles on the Web. The use of dis-
tributional similarity resembles the clustering-
based approach; however, our method can select 
a hypernym for each word independently, and it 
does not suffer from class granularity mismatch 
or the low quality of classes. In addition, our ap-
proach exploits the hierarchical structures of the 
Wikipedia hypernym relations.  
3 Wikipedia Relation Database 
Our Wikipedia relation database is based on the 
extraction method of Sumida et al (2008). They 
proposed a method of automatically acquiring 
hyponymy relations by focusing on the hierar-
chical layout of articles on Wikipedia. By way of 
an example, Figure 3 shows part of the source 
code clipped from the article titled ?Penguin.? 
An article has hierarchical structures composed 
of titles, sections, itemizations, etc. The entire 
article is divided into sections titled ?Anatomy,? 
?Mating habits,? ?Systematics and evolution,? 
?Penguins in popular culture,? and so on. The 
section ?Systematics and evolution? has a sub-
section ?Systematics,? which is further divided 
into ?Aptenodytes,? ?Eudyptes,? and so on. 
Some of these section-subsection relations can be 
regarded as valid hyponymy relations. In this 
article, relations such as the one between ?Apte-
nodytes? and ?Emperor Penguin? and that be-
tween ?Book? and ?Penguins of the World? are 
valid hyponymy relations.  
First, Sumida et al (2008) extracted hypony-
my relation candidates from hierarchical struc-
tures on Wikipedia. Then, they selected proper 
hyponymy relations using a support vector ma-
chine classifier. They used several kinds of fea-
tures for the hyponymy relation candidate, such 
as a POS tag for each word, the appearance of 
morphemes of each word, the distance between 
two words in the hierarchical structures of Wiki-
pedia, and the last character of each word. As a 
result of their experiments, approximately 2.4 
million hyponymy relations in Japanese were 
extracted, with a precision rate of 90.1%.  
Compared to the traditional taxonomies, these 
extracted hyponymy relations have the following 
characteristics (Fellbaum 1998, Bond et al 2008). 
(a) The database includes a more extensive 
vocabulary. 
(b)  The database includes a large number of 
named entities. 
Popular Japanese taxonomies GoiTaikei (Ike-
hara et al 1997) and Bunrui-Goi-Hyo (1996) 
contain approximately 300,000 words and 
96,000 words, respectively. In contrast, the ex-
tracted hyponymy relations contain approximate-
ly 1.2 million hyponyms and are undoubtedly 
much larger than the existing taxonomies. 
Another difference is that since Wikipedia covers 
a large number of named entities, the extracted 
hyponymy relations also contain a large number 
of named entities.  
Note that the extracted relations have a hierar-
chical structure because one hypernym of a cer-
tain word may also be the hyponym of another 
hypernym. However, we observed that the depth 
of the hierarchy, on an average, is extremely 
shallow. To make the hierarchy appropriate for 
our method, we extended these into a deeper hie-
rarchical structure. The extracted relations in-
clude many compound nouns as hypernyms, and 
we decomposed a compound noun into a se-
quence of nouns using a morphological analyzer. 
Since Japanese is a head-final language, the suf-
fix of a noun sequence becomes the hypernym of 
the original compound noun if the suffix forms 
another valid compound noun. We extracted suf-
fixes of compound nouns and manually checked 
whether they were valid compound nouns; then, 
we constructed a hierarchy of compound nouns. 
The hierarchy can be extended such that it in-
cludes the hyponyms of the original hypernym 
and the resulting hierarchy constitutes a hierar-
chical taxonomy. We use this hierarchical tax-
onomy as a target for expansion.2  
                                                 
2  Note that this modification was performed as part of 
another project of ours aimed at constructing a large-scale 
and clean hypernym knowledge base by human annotation. 
We do not think this cost is directly relevant to the method 
proposed here. 
Figure 3: A part of source code clipped from the 
article ?Penguin? in Wikipedia. 
'''Penguins''' are a group of 
[[Aquatic animal|aquatic]], 
[[flightless bird]]s. 
== Anatomy == 
== Mating habits == 
==Systematics and evolution== 
===Systematics=== 
* Aptenodytes 
**[[Emperor Penguin]] 
** [[King Penguin]] 
* Eudyptes 
== Penguins in popular culture == 
== Book == 
* Penguins 
* Penguins of the World 
== Notes == 
* Penguinone 
* the [[Penguin missile]] 
[[Category:Penguins]] 
[[Category:Birds]]
931
4 Distributional Similarity 
The distributional hypothesis states that words 
that occur in similar contexts tend to be semanti-
cally similar (Harris 1985). In this section, we 
first introduce distributional similarity based on 
raw verb-noun dependencies (RVD). To avoid 
the sparseness problem of the co-occurrence of 
verb-noun dependencies, we also use distribu-
tional similarity based on a large-scale clustering 
of verb-noun dependencies (CVD). 
In the experiment mentioned in the following 
section, we used the TSUBAKI corpus (Shinzato 
et al 2008) to calculate distributional similarity. 
This corpus provides a collection of 100 million 
Japanese Web pages containing 6 ? 109
 
sentences. 
4.1 Distributional Similarity Based on RVD 
When calculating the distributional similarity 
based on RVD, we use the triple <v, rel, n>, 
where v is a verb, n is a noun phrase, and rel 
stands for the relation between v and n. In Japa-
nese, a relation rel is represented by postposi-
tions attached to n and the phrase composed of n 
and rel modifies v. Each triple is divided into two 
parts. The first is <v, rel> and the second is n. 
Then, we consider the conditional probability of 
occurrence of the pair <v, rel>: P(<v, rel>|n).  
P(<v, rel>|n) can be regarded as the distribution 
of the grammatical contexts of the noun phrase n. 
The distributional similarity can be defined as 
the distance between these distributions. There 
are several kinds of functions for evaluating the 
distance between two distributions (Lee 1999). 
Our method uses the Jensen-Shannon divergence. 
The Jensen-Shannon divergence between two 
probability distributions, )|( 1nP ?  and )|( 2nP ? , 
can be calculated as follows: 
 
)),
2
)|()|(
||)|((
)
2
)|()|(
||)|(((
2
1
))|(||)|((
21
2
21
1
21
nPnP
nPD
nPnP
nPD
nPnPD
KL
KL
JS
?+??+
?+??=
??
 
 
where DKL indicates the Kullback-Leibler diver-
gence and is defined as follows: 
 
.
)|(
)|(
log)|())|(||)|((
2
1
121 ? ???=?? nP nPnPnPnPDKL  
 
Finally, the distributional similarity between 
two words, n1 and n2, is defined as follows: 
 
)).|(||)|((1),( 2121 nPnPDnnsim JS ???=  
 
This similarity assumes a value from 0 to 1. If 
two words are similar, the value will be close to 
1; if two words have entirely different meanings, 
the value will be 0.
 
In the experiment, we used 1,000,000 noun 
phrases and 100,000 pairs of verbs and postposi-
tions to calculate the probability P(<v, rel>|n) 
from the dependency relations extracted from the 
above-mentioned Web corpus (Shinzato et al 
2008). The probabilities are computed using the 
following equation by modifying for the fre-
quency using the log function: 
 
?
>?<
+><
+><=><
Drelv
nrelvf
nrelvf
nrelvP
,
1),,(log(
1)),,(log(
)|,(
,0),,(if >>< nrelvf
  
where f(<v, rel, n>) is the frequency of a triple 
<v, rel, n> and D is the set defined as { <v, rel > | 
f(<v, rel, n>) > 0 }. In the case of f(<v, rel, n>) = 
0, P(<v, rel>|n) is set to 0.  
Instead of using the observed frequency di-
rectly as in the usual maximum likelihood esti-
mation, we modified it as above. Although this 
might seems strange, this kind of modification is 
common in information retrieval as a term 
weighing method (Manning et al 1999) and  it is 
also applied in some studies to yield better word 
similarities (Terada et al 2006, Kazama et al 
2009). We also adopted this idea in this study. 
4.2 Distributional Similarity Based on CVD 
Rooth et al (1999) and Torisawa (2001) showed 
that EM-based clustering using verb-noun de-
pendencies can produce semantically clean noun 
clusters. We exploit these EM-based clustering 
results as the smoothed contexts for noun n. In 
Torisawa?s model (2001), the probability of oc-
currence of the triple <v, rel, n> is defined as 
follows: 
 
,)()|()|,(
),,(
? ? ><=
><
Aadef aPanParelvP
nrelvP
 
 
where a denotes a hidden class of <v,rel> and n. 
In this equation, the probabilities P(<v,rel>|a), 
P(n|a), and P(a) cannot be calculated directly 
because class a is not observed in a given corpus. 
The EM-based clustering method estimates these 
probabilities using a given corpus. In the E-step, 
932
the probability P(a|<v,rel>) is calculated. In the 
M-step, the probabilities P(<v,rel>|a), P(n|a), 
and P(a) are updated to arrive at the maximum 
likelihood using the results of the E-step. From 
the results of estimation of this EM-based clus-
tering method, we can obtain the probabilities 
P(<v,rel>|a), P(n|a), and P(a) for each <v, rel>, n, 
and a. Then, P(a|n) is calculated by the following 
equation: 
 
.
)()|(
)()|(
)|( ? ?= Aa aPanP
aPanP
naP  
 
P(a|n) can be used to find the class of n. For 
example, the class that has the maximum P(a|n) 
can be regarded as the class to which n belongs. 
Noun phrases that occur with similar pairs 
<v,rel> tend to be classified in the same class. 
Kazama et al (2008) proposed the paralleliza-
tion of this EM-based clustering with the aim of 
enabling large-scale clustering and using the re-
sulting clusters in named entity recognition. Ka-
zama et al (2009) reported the calculation of 
distributional similarity using the clustering re-
sults. The distributional similarity was calculated 
by the Jensen-Shannon divergence, which was 
used in this paper. Similar to the case in Kazama 
et al, we performed word clustering using 
1,000,000 noun phrases and 2,000 classes. Note 
that the frequencies of dependencies were mod-
ified with the log function, as in RVD, described 
in the previous section. 
5 Discovering an Appropriate Hyper-
nym for a Target word 
In the Wikipedia relation database, there are 
about 95,000 hypernyms and about 1.2 million 
hyponyms. In both RVD and CVD, the words 
used were selected according to the number (the 
number of kinds, not the frequency) of <v, rel >s 
that n has dependencies in the data. As a result, 1 
million words were selected. The number of 
common words that are also included in the Wi-
kipedia relation database are as follows: 
 
Hypernyms     28,015 (common hypernyms) 
Hyponyms   175,022 (common hyponyms) 
 
These common hypernyms become candidates 
for hypernyms for a target word. On the other 
hand, the common hyponyms are used as clues 
for identifying appropriate hypernyms. 
In our task, the potential target words are 
about 810,000 in number and are not included in 
the Wikipedia relation database. These include 
some strange words or word phrases that are ex-
tracted due to the failure of morphological analy-
sis. We exclude these words using simple rules. 
Consequently, the number of target words for our 
process is reduced to about 670,000.  
In the following section, we outline the scor-
ing method that uses k similar words to discover 
an appropriate hypernym for a target word. We 
also explain several baseline approaches that use 
distributional similarity. 
5.1 Scoring with k similar Words 
In this approach, we first calculate the similari-
ties between the common hyponyms and a target 
word and select the k most similar common hy-
ponyms. Here, we use a similarity threshold val-
ue Smin to avoid the effect of words having lower 
similarities. If the similarity is less than the thre-
shold value, the word is excluded from the set of 
k similar words. Next, each k similar word votes 
a score to its ancestors in the hierarchical struc-
tures of the Wikipedia relation database. The 
score used to vote for a hypernym nhyper is as fol-
lows: 
 
,),(
)(
)()(
1),(?
??
? ?=
trghyperhypo
hypohyper
nksimilarnDescn
hypotrg
nnr
hyper
nnsimd
nscore
 
 
where ntrg is the target word, Desc(nhyper) is the 
descendant of the hypernym nhyper, ksimilar(ntrg) 
is the k similar word of ntrg, 
1),( ?hypohyper nnrd is a 
penalty that depends on the differences in the 
depth of hierarchy, d is a parameter for the penal-
ty value and has a value between 0 and 1, and 
r(ntrg, nhypo) is the difference in the depth of hie-
rarchy between ntrg and nhypo. sim(ntrg,nhypo) is a 
distributional similarity between ntrg and nhypo.  
As a result of scoring, each hypernym has a 
score for the target word. The hypernym that has 
the highest score for the target word is selected 
as its hypernym. The hyponymy relations thus 
produced are ranked according to the scores. 
Figure 4 shows an example of the scoring 
process. In this example, we use CitroenAX as the 
target word whose hypernym will be identified. 
First, the k similar words are extracted from the 
common hyponyms in the Wikipedia relation: 
Opel Astra, TVR Tuscan, Mitsubishi Minica, and 
Renault Lutecia are extracted. Next, each k simi-
lar word votes a score to its ancestors. The words 
Opel Astra, TVR Tuscan, and Renault Lutecia 
vote to their parent car and the word Mitsubishi 
933
Minica votes to its parent mini-vehicle and its 
grandparent car with a small penalty. Finally, the 
hypernym car, which has the highest score, is 
selected as the hypernym of the target word Ci-
troenAX. 
5.2 Baseline Approaches 
Using distributional similarity, we can also de-
velop the following baseline approaches to dis-
cover hyponymy relations. 
 
Selecting the hypernym of the most similar hy-
ponym (baseline approach 1) 
We use the heuristics that similar words tend to 
have the same hypernym. In this approach, we 
first calculate the similarities between the com-
mon hyponyms and the target word. The com-
mon hyponym most similar to the target word is 
extracted. Then, the parent of the extracted 
common hyponym is regarded as the hypernym 
of the target word. This approach outputs several 
hypernyms when the most similar hyponym has 
several hypernyms. This approach can be consi-
dered to be the same as the scoring method using 
k similar words when k = 1. We use the distribu-
tional similarity between the target word and the 
most similar hyponym in the Wikipedia relation 
database as the score for the appropriateness of 
the resulting hyponymy. 
 
Selecting the most similar hypernym (baseline 
approach 2) 
The distributional similarity between the com-
mon hypernym and the target word is calculated. 
Then, the hypernym that has the highest distribu-
tional similarity is regarded as the hypernym of 
the target word. The similarity is used as the 
score of the appropriateness of the produced hy-
ponymy. 
 
Scoring based on the average similarity of the 
hypernym?s children (baseline approach 3) 
This approach uses the probabilistic distributions 
of the hypernym?s children. We define the prob-
ability )|( hyperchild nP ? characterized by the children 
of the hypernym nhyper, as follows: 
 
,
)(
)()|(
)|(
)(
)(
?
?
?
?
?
=?
hyperhypo
hyperhypo
nChn
hypo
nChn
hypohypo
hyperchild nP
nPnP
nP  
 
where Ch(nhyper) is a set of all children of nhyper. 
Then, distributional similarities between a com-
mon hypernym nhyper and the target word nhypo are 
calculated. The hypernym that has the highest 
distributional similarity is selected as the hyper-
nym of the word. This distributional similarity is 
used as the score of the appropriateness of the 
produced hyponymy. 
If a hypernym has only a few children, the re-
liability of the probabilistic distribution of 
hypernym defined here will be low because the 
Wikipedia relation database includes some incor-
rect relations. For this reason, we use the hyper-
nym only if the number of children it has is more 
than a threshold value.  
6 Experiments 
We evaluated our proposed methods by using it 
in experiments to discover hypernyms from the 
Wikipedia relation database for the target words 
extracted from about 670,000 noun phrases.  
6.1 Parameter Estimation by Preliminary 
Experiments 
In the proposed methods, there are several para-
meters. We performed parameter optimization by 
randomly selecting 694 words as development 
data in our preliminary experiments. The hyper-
nyms of these words were determined manually. 
We adjusted the parameters so that each method 
achieved the best performance for this develop-
ment data. 
The parameters in the scoring method with k 
similar words were adjusted as follows3:  
 (RVD) 
Number of similar words:         k = 100. 
Similarity threshold:           Smin = 0.05. 
Penalty value for ancestors:    d = 0.6. 
                                                 
3 We tested the parameter values k = {100, 200, 300, 400, 
500, 600, 700, 800, 900, 1000}, Smin={0, 0.05, 0.1, 0.15, 0.2, 
0.25, 0.3, 0.35, 0.4} and d={0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 
0.8, 0.85, 0.9, 0.95, 1.0}. 
Figure 4: Overview of the scoring process.
car
CitroenAX
mini
vehicle
hybrid 
vehicle
Opel 
Astra
Renault 
Lutecia
Mitsubishi
Minica
k similar words
Each k-similar word
votes the score to its 
ancestors in the Wikipedia 
relation database.
Target word selected 
from the Web text (ntrg).
TVR 
Tuscan
: common hypernym(nhyper)
: k similar word &  
common hyponym(nhypo)
x d1
x d0
x d0
934
(CVD) 
Number of similar words:         k = 200. 
Similarity threshold:                Smin = 0.3. 
Penalty value for ancestors:    d = 0.6. 
 
The parameter in baseline approach 3 was ad-
justed as follows: 
Threshold for the number of children: 20. 
6.2 Evaluation of the Experimental Results 
on the Basis of Score Ranking 
Using the adjusted parameters, we conducted 
experiments to extract the hypernym of each tar-
get word with the help of the scoring method 
based on k similar words. In these experiments, 
two kinds of distributional similarity mentioned 
in Section 4 were exploited individually. The 
words that were used in the development data 
were excluded.  
We also conducted a comparative experiment 
in which the parameter value for the penalty of 
the hierarchal difference, d, was set to 0 to clari-
fy the ability of using hierarchal structures in the 
k similar words method. This means each k simi-
lar word votes only to their parent. 
We then judged the quality of each acquired 
hypernym. The evaluation data sets were sam-
pled from the top 1,000, 10,000, 100,000, and 
670,000 results that were ranked according to the 
score of each method. Then, against 200 samples 
that were randomly sampled from each set, one 
of the authors judged whether the hypernym ex-
tracted by each method for the target word was 
correct or not. In this evaluation, if the sentence 
?The target word is a kind of the hypernym? or 
?The target word is an instance of the hypernym? 
was consistent, the extracted hyponymy was 
judged as correct. It should be noted that the out-
puts of the compared methods are combined and 
shuffled to enable fair comparison. In addition, 
baseline approach 1 extracted several hypernyms 
for the target word. In this case, we judged the 
hypernym as correct when the case where one of 
the hypernyms was correct.  
The precision of each result is shown in Table 
1. The results of the k similar words method are 
far better than those of the other baseline me-
thods. In particular, the k similar words method 
with CVD outperformed the methods of the k 
similar words where the parameter value d was 
set to 0 and the method using RVD except for the 
top 1,000 results. This means that the use of hie-
rarchal structures and the clustering process for 
calculating distributional similarity are effective 
for this task. We confirmed the significant differ-
ences of the proposed method (CVD) as com-
pared with all the baseline approaches at the 1% 
significant level by the Fisher?s exact test (Hays 
1988). 
The precision of baseline approach 2 that se-
lected the most similar hypernym was the worst 
among all the methods. There were words that 
were similar to the target word among the hyper-
nyms extracted incorrectly. For example, the 
word semento-kojo (cement factory) was ex-
tracted for the hypernym of the word kuriningu-
kojo (dry cleaning plant). It is difficult to judge 
whether the word is a hypernym or just a similar 
word by using only the similarity measure. 
As for the results of baseline approach 1 using 
the most similar hyponym and baseline approach 
3 using the similarity of the set of hypernym?s 
children, the noise on the Wikipedia relation da-
tabase decreased the precision. Moreover, over-
specified hypernyms were extracted incorrectly 
by these methods. In contrast, the method of 
scoring based on the use of k similar words was 
robust against noise because it uses the voting 
approach for the similarities. Further, this me-
thod can extract hypernyms that are not over-
specific because it uses all descendants for scor-
ing.  
Table 2 shows some examples of relations ex-
tracted by the k similar words method using 
CVD. 
 
Table 1:  Precision of each approach based on the score ranking. CVD represents the method that uses the dis-
tributional similarity based on large-scale of clustering of verb-noun dependencies. RVD represents the 
one based on raw verb-noun dependencies. 
 k-similar words
(CVD) 
k-similar words
(RVD) 
k-similar words
(CVD, d = 0)
Baseline  
approach 1 
(CVD) 
Baseline  
approach 2 
(CVD) 
Baseline  
approach 3 
(CVD) 
1,000 0.940 1.000 0.850 0.730 0.290 0.630 
10,000 0.910 0.875 0.875 0.555 0.300 0.445 
100,000 0.745 0.710 0.730 0.500 0.280 0.435 
670,000 0.520 0.500 0.470 0.345 0.115 0.170 
935
6.3 Investigation of the Extracted Relation 
Overlap with a Conventional Method 
We randomly sampled 300 hyponymy rela-
tions that were extracted correctly using the k 
similar words method exploiting CVD and inves-
tigated whether or not these relations can be ex-
tracted by the conventional method based on the 
lexico-syntactic pattern. The possible hyponymy 
relations were extracted using the pattern-based 
method (Ando et al 2003) from the TSUBAKI 
corpus (Shinzato et al 2008). From a comparison 
of these relations, we found only 57 common 
hyponymy relations. That is, the remaining 243 
hyponymy relations were not included in the 
possible hyponymy relations. This result indi-
cates that our method can acquire the hyponymy 
relations that cannot be extracted by the conven-
tional pattern-based method. 
6.4 Discussions 
We investigated the reason for the errors gener-
ated by the method of scoring using k similar 
words exploiting CVD. We conducted experi-
ments on hypernym extraction targeting 694 
words in the development data mentioned in Sec-
tion 6.1. Among these, 286 relations were ex-
tracted incorrectly. In these relations, there were 
some frequent hypernyms. For example, the 
word sakuhin (work) appeared 28 times and hon 
(book) appeared 20 times. As shown in Table 2, 
hon (book) was also extracted for the target word 
meru-seminah (mail seminar). It is really diffi-
cult even for a human to identify whether the 
title is that of the book or the event. If we can 
identify these difficult hypernyms in advance, we 
can improve precision by excluding them from 
the target hypernyms. This will be one of the top-
ics for future study. 
7 Conclusion 
In this paper, we proposed a method for disco-
vering hyponymy relations between nouns by 
fusing the Wikipedia relation database and words 
from the Web. We demonstrated that the method 
using k similar words has high accuracy. The 
experimental results showed the effectiveness of 
using hierarchal structures and the clustering 
process for calculating distributional similarity 
for this task. The experimental results showed 
that our method could achieve 91.0% attachment 
accuracy for the top 10,000 hyponymy relations 
and 74.5% attachment accuracy for the top 
100,000 relations when using the clustering-
based similarity. We confirmed that most rela-
tions extracted by the proposed method could not 
be handled by the lexico-syntactic pattern-based 
method. Future work will be to filter out difficult 
hypernyms for hyponymy extraction process to 
achieve higher precision. 
References 
M. Ando, S. Sekine and S. Ishizaki. 2003. Automatic 
Extraction of Hyponyms from Newspaper Using 
Lexicosyntactic Patterns. IPSJ SIG Notes, 2003-
NL-157, pp. 77?82 (in Japanese). 
F. Bond, H. Isahara, K. Kanzaki and K. Uchimoto. 
2008. Boot-strapping a WordNet Using Multiple 
Existing WordNets. In the 6th International Confe-
rence on Language Resources and Evaluation 
(LREC), Marrakech.  
Bunruigoihyo. 1996. The National Language Re-
search Institute (in Japanese). 
S. A. Caraballo. 1999. Automatic Construction of a 
Hypernym-labeled Noun Hierarchy from Text. In 
Proceedings of the Conference of the Association 
for Computational Linguistics (ACL). 
O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. 
Shaked, S. Soderland, D. Weld and A. Yates. 2005. 
Unsupervised Named-Entity Extraction from the 
Web: An Experimental Study. Artificial Intelli-
gence, 165(1):91?134. 
C. Fellbaum. 1998. WordNet: An Electronic Lexical 
Table2:  Hypernym discovery results by the k-similar 
words based approach (CVD). The underline indi-
cates the hypernyms which are extracted incorrectly.
Score Target word Extracted hypernym 
58.6 INDIVI burando 
(fashion label)
54.3 kureome (Cleome) hana (flower)
34.4 UOKR  gemu (game)
21.7 Okido (Okido) machi (town)
20.5 Sumatofotsu 
(Smart fortwo) 
kuruma  
(car) 
15.6 Fukagawameshi 
(Fukagawa rice)
ryori (dish) 
8.9 John Barry sakkyokuka 
 (composer)
8.5 JVM sofuto-wea 
(software) 
6.6 metangasu 
(methane gas) 
genso 
(chemical element)
5.4 me-ru semina 
(mail seminar) 
Hon (book) 
3.9 gurometto 
(grommet) 
shohin 
(merchandise)
3.1 supuringubakku  
(spring back) 
gensho 
(phenomenon)
936
Database. Cambridge, MA: MIT Press. 
Z. Harris. 1985. Distributional Structure. In Katz, J. J. 
(ed.) The Philosophy of Linguistics, Oxford Uni-
versity Press, pp. 26?47. 
W. L. Hays. 1988. Statistics: Analyzing Qualitative 
Data, Rinehart and Winston, Inc., Ch. 18, pp. 769?
783. 
M. Hearst. 1992. Automatic Acquisition of Hypo-
nyms from Large Text Corpora. In Proceedings of 
the 14th Conference on Computational Linguistics 
(COLING), pp. 539?545.  
S. Ikehara, M. Miyazaki, S. Shirai, A. Yokoo, H. Na-
kaiwa, K. Ogura, Y. Ooyama and Y. Hayashi. 1997. 
Goi-Taikei A Japanese Lexicon, Iwanami Shoten. 
J. Kazama and K. Torisawa. 2008. Inducing Gazet-
teers for Named Entity Recognition by Large-scale 
Clustering of Dependency Relations. In Proceed-
ings of ACL-08: HLT, pp. 407?415. 
J. Kazama, Stijn De Saeger, K. Torisawa and M. Mu-
rata. 2009. Generating a Large-scale Analogy List 
Using a Probabilistic Clustering Based on Noun-
Verb Dependency Profiles. In 15th Annual Meeting 
of the Association for Natural Language 
Processing, C1?3 (in Japanese). 
L. Lee. 1999. Measures of Distributional Similarity. 
In Proceedings of the 37th Annual Meeting of the 
Association for Computational Linguistics, pp. 25?
32. 
C. D. Manning and H. Schutze. 1999. Foundations of 
Statistical Natural Language Processing. The MIT 
Press. 
P. Pantel, D. Ravichandran and E. Hovy. 2004a. To-
wards Terascale Knowledge Acquisition. In Pro-
ceedings of the 20th International Conference on 
Computational Linguistics. 
P. Pantel and D. Ravichandran. 2004b. Automatically 
Labeling Semantic Classes. In Proceedings of the 
Human Language Technology and North American 
Chapter of the Association for Computational Lin-
guistics Conference. 
S. P. Ponzetto, and M. Strube. 2007. Deriving a Large 
Scale Taxonomy from Wikipedia. In Proceedings 
of the 22nd National Conference on Artificial Intel-
ligence, pp. 1440?1445. 
M. Rooth, S. Riezler, D. Presher, G. Carroll and F. 
Beil. 1999. Inducing a Semantically Annotated 
Lexicon via EM-based Clustering. In Proceedings 
of the 37th annual meeting of the Association for 
Computational Linguistics, pp. 104?111. 
K. Shinzato and K. Torisawa. 2004. Acquiring Hypo-
nymy Relations from Web Documents. In Proceed-
ings of HLT-NAACL, pp. 73?80. 
K. Shinzato, D. Kawahara, C. Hashimoto and S. Ku-
rohashi. 2008. A Large-Scale Web Data Collection 
as A Natural Language Processing Infrastructure. 
In the 6th International Conference on Language 
Resources and Evaluation (LREC). 
R. Snow, D. Jurafsky and A. Y. Ng. 2005. Learning 
Syntactic Patterns for Automatic Hypernym Dis-
covery. NIPS 2005. 
R. Snow, D. Jurafsky, A. Y. Ng. 2006. Semantic Tax-
onomy Induction from Heterogenous Evidence. In 
Proceedings of the 21st International Conference 
on Computational Linguistics and the 44th annual 
meeting of the Association for Computational Lin-
guistics, pp. 801?808. 
A. Sumida, N. Yoshinaga and K. Torisawa. 2008. 
Boosting Precision and Recall of Hyponymy Rela-
tion Acquisition from Hierarchical Layouts in Wi-
kipedia. In the 6th International Conference on 
Language Resources and Evaluation (LREC). 
A. Terada, M. Yoshida, H. Nakagawa. 2006. A Tool 
for Constructing a Synonym Dictionary using con-
text Information. In proceedings of IPSJ SIG Tech-
nical Reports, vol.2006 No.124, pp. 87-94. (In Jap-
anese). 
K. Torisawa. 2001. An Unsupervised Method for Ca-
nonicalization of Japanese Postpositions. In Pro-
ceedings of the 6th Natural Language Processing 
Pacific Rim Symposium (NLPRS), pp. 211?218. 
K. Torisawa, Stijn De Saeger, Y. Kakizawa, J. Kaza-
ma, M. Murata, D. Noguchi and A. Sumida. 2008. 
TORISHIKI-KAI, An Autogenerated Web Search 
Directory. In Proceedings of the second interna-
tional symposium on universal communication, pp. 
179?186, 2008. 
937
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1172?1181,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Large-Scale Verb Entailment Acquisition from the Web
Chikara Hashimoto? Kentaro Torisawa? Kow Kuroda?
Stijn De Saeger? Masaki Murata? Jun?ichi Kazama?
National Institute of Information and Communications Technology
Sorakugun, Kyoto, 619-0289, JAPAN
{
? ch,? torisawa,? kuroda,? stijn,?murata,? kazama}@nict.go.jp
Abstract
Textual entailment recognition plays a
fundamental role in tasks that require in-
depth natural language understanding. In
order to use entailment recognition tech-
nologies for real-world applications, a
large-scale entailment knowledge base is
indispensable. This paper proposes a con-
ditional probability based directional sim-
ilarity measure to acquire verb entailment
pairs on a large scale. We targeted 52,562
verb types that were derived from 108
Japanese Web documents, without regard
for whether they were used in daily life
or only in specific fields. In an evaluation
of the top 20,000 verb entailment pairs ac-
quired by previous methods and ours, we
found that our similarity measure outper-
formed the previous ones. Our method
also worked well for the top 100,000 re-
sults.
1 Introduction
We all know that if you snored, you must have
been sleeping, that if you are divorced, you must
have been married, and that if you won a lawsuit,
you must have sued somebody. These relation-
ships between events where one is the logical con-
sequence of the other are called entailment. Such
knowledge plays a fundamental role in tasks that
require in-depth natural language understanding,
e.g., answering questions and using natural lan-
guage interfaces.
This paper proposes a novel method for verb
entailment acquisition. Using a Japanese Web
corpus (Kawahara and Kurohashi, 2006a) derived
from 108 Japanese Web documents, we automat-
ically acquired such verb pairs as snore ? sleep
and divorce ? marry, where entailment holds be-
tween the verbs in the pair.1 Our definition of ?en-
tailment? is the same as that in WordNet3.0; v
1
entails v
2
if v
1
cannot be done unless v
2
is, or has
been, done.2
Our method follows the distributional similar-
ity hypothesis, i.e., words that occur in the same
context tend to have similar meanings. Just as in
the methods of Lin and Pantel (2001) and Szpek-
tor and Dagan (2008), we regard the arguments
of verbs as the context in the hypothesis. How-
ever, unlike the previous methods, ours is based
on conditional probability and is augmented with
a simple trick that improves the accuracy of verb
entailment acquisition. In an evaluation of the top
20,000 verb entailment pairs acquired by the pre-
vious methods and ours, we found that our similar-
ity measure outperformed the previous ones. Our
method also worked well for the top 100,000 re-
sults,
Since the scope of Natural Language Process-
ing (NLP) has advanced from a formal writing
style to a colloquial style and from restricted to
open domains, it is necessary for the language re-
sources for NLP, including verb entailment knowl-
edge bases, to cover a broad range of expressions,
regardless of whether they are used in daily life
or only in specific fields that are highly techni-
cal. As we will discuss later, our method can ac-
quire, with reasonable accuracy, verb entailment
pairs that deal not only with common and familiar
verbs but also with technical and unfamiliar ones
like podcast ? download and jibe ? sail.
Note that previous researches on entailment ac-
quisition focused on templates with variables or
word-lattices (Lin and Pantel, 2001; Szpektor and
Dagan, 2008; Barzilay and Lee, 2003; Shinyama
1Verb entailment pairs are described as v
1
? v
2
(v
1
is
the entailing verb and v
2
is the entailed one) henceforth.
2WordNet3.0 provides entailment relationships between
synsets like divorce, split up?marry, get married, wed, con-
join, hook up with, get hitched with, espouse.
1172
et al, 2002). Certainly these templates or word
lattices are more useful in such NLP applications
as Q&A than simple entailment relations between
verbs. However, our contention is that entailment
certainly holds for some verb pairs (like snore ?
sleep) by themselves, and that such pairs consti-
tute the core of a future entailment rule database.
Although we focused on verb entailment, our
method can also acquire template-level entailment
pairs with a reasonable accuracy.
The rest of this paper is organized as follows.
In ?2, related works are described. ?3 presents our
proposed method. After this, an evaluation of our
method and the existing methods is presented in
Section 4. Finally, we conclude the paper in ?5.
2 Related Work
Previous studies on entailment, inference rules,
and paraphrase acquisition are roughly classi-
fied into those that require comparable corpora
(Shinyama et al, 2002; Barzilay and Lee, 2003;
Ibrahim et al, 2003) and those that do not (Lin
and Pantel, 2001; Weeds and Weir, 2003; Geffet
and Dagan, 2005; Pekar, 2006; Bhagat et al, 2007;
Szpektor and Dagan, 2008).
Shinyama et al (2002) regarded newspaper arti-
cles that describe the same event as a pool of para-
phrases, and acquired them by exploiting named
entity recognition. They assumed that named en-
tities are preserved across paraphrases, and that
text fragments in the articles that share several
comparable named entities should be paraphrases.
Barzilay and Lee (2003) also used newspaper ar-
ticles on the same event as comparable corpora
to acquire paraphrases. They induced paraphras-
ing patterns by sentence clustering. Ibrahim et al
(2003) relied on multiple English translations of
foreign novels and sentence alignment to acquire
paraphrases. We decided not to take this approach
since using comparable corpora limits the scale
of the acquired paraphrases or entailment knowl-
edge bases. Although obtaining comparable cor-
pora has been simplified by the recent explosion
of the Web, the availability of plain texts is incom-
parably better.
Entailment acquisition methods that do not re-
quire comparable corpora are mostly based on the
distributional similarity hypothesis and use plain
texts with a syntactic parser. Basically, they parse
texts to obtain pairs of predicate phrases and their
arguments, which are regarded as features of the
predicates with appropriately assigned weights.
Lin and Pantel (2001) proposed a paraphrase ac-
quisition method (non-directional similarity mea-
sure) called DIRT which acquires pairs of binary-
templates (predicate phrases with two argument
slots) that are paraphrases of each other. DIRT em-
ploys the following similarity measure proposed
by Lin (1998):
Lin(l, r) =
?
f?F
l
?F
r
[w
l
(f) + w
r
(f)]
?
f?F
l
w
l
(f) +
?
f?F
r
w
r
(f)
where l and r are the corresponding slots of two
binary templates, F
s
is s?s feature vector (argu-
ment nouns), and w
s
(f) is the weight of f ? F
s
(PMI between s and f ). The intuition behind this
is that the more nouns two templates share, the
more semantically similar they are. Since we ac-
quire verb entailment pairs based on unary tem-
plates (Szpektor and Dagan, 2008) we used the
Lin formula to acquire unary templates directly
rather than using the DIRT formula, which is the
arithmetic-geometric mean of Lin?s similarities for
two slots in a binary template.
Bhagat et al (2007) developed an algorithm
called LEDIR for learning the directionality of
non-directional inference rules like those pro-
duced by DIRT. LEDIR implements a Direction-
ality Hypothesis: when two binary semantic re-
lations tend to occur in similar contexts and the
first one occurs in significantly more contexts than
the second, then the second most likely implies the
first and not vice versa.
Weeds and Weir (2003) proposed a general
framework for distributional similarity that mainly
consists of the notions of what they call Precision
(defined below) and Recall:
Precision(l, r) =
?
f?F
l
?F
r
w
l
(f)
?
f?F
l
w
l
(f)
where l and r are the targets of a similarity mea-
surement, F
s
is s?s feature vector, and w
s
(f) is the
weight of f ? F
s
. The best performing weight is
PMI. Precision is a directional similarity measure
that examines the coverage of l?s features by those
of r?s, with more coverage indicating more simi-
larity.
Szpektor and Dagan (2008) proposed a direc-
tional similarity measure called BInc (Balanced-
Inclusion) that consists of Lin and Precision, as
BInc(l, r) =
?
Lin(l, r) ? Precision(l, r)
1173
where l and r are the target templates. For weight-
ing features, they used PMI. Szpektor and Dagan
(2008) also proposed a unary template, which is
defined as a template consisting of one argument
slot and one predicate phrase. For example, X take
a nap ? X sleep is an entailment pair consisting
of two unary templates. Note that the slot X must
be shared between templates. Though most of the
previous entailment acquisition studies focused on
binary templates, unary templates have an obvi-
ous advantage over binary ones; they can handle
intransitive predicate phrases and those that have
omitted arguments. The Japanese language, which
we deal with here, often omits arguments, and thus
the advantage of unary templates is obvious.
As shown in ?4, our method outperforms Lin,
Precision, and BInc in accuracy.
Szpector et al (2004) addressed broad coverage
entailment acquisition. But their method requires
an existing lexicon to start, while ours does not.
Apart from the dichotomy of the compara-
ble corpora and the distributional similarity ap-
proaches, Torisawa (2006) exploited the structure
of Japanese coordinated sentences to acquire verb
entailment pairs. Pekar (2006) used the local
structure of coherent text by identifying related
clauses within a local discourse. Zanzotto et al
(2006) exploited agentive nouns. For example,
they acquired win ? play from ?the player wins.?
Geffet and Dagan (2005) proposed the Distribu-
tional Inclusion Hypotheses, which claimed that if
a word v entails another word w, then all the char-
acteristic features of v are expected to appear with
w, and vice versa. They applied this to noun en-
tailment pair acquisition, rather than verb pairs.
3 Proposed Method
This section presents our method of verb entail-
ment acquisition. First, the basics of Japanese are
described. Then, we present the directional sim-
ilarity measure that we developed in ?3.2. ?3.3
describes the structure and acquisition of the web-
based data from which entailment pairs are de-
rived. Finally, we show how we acquire verb en-
tailment pairs using our proposed similarity mea-
sure and the web-based data in ?3.4.
3.1 Basics of Japanese
Japanese explicitly marks arguments including the
subject and object by postpositions, and is a head-
final language. Thus, a verb phrase consisting of
an object hon (book) and a verb yomu (read), for
example, is expressed as hon-wo yomu (book-ACC
read) ?read a book? with the accusative postpo-
sition wo marking the object.3 Accordingly, we
refer to a unary template as ?p, v? hereafter, with
p and v referring to the postposition and a verb.
Also, we abbreviate a template-level entailment
?p
l
, v
l
? ? ?p
r
, v
r
? as l ? r for simplicity. We
define a unary template as a template consisting
of one argument slot and one predicate, following
Szpektor and Dagan (2008).
3.2 Directional Similarity Measure based on
Conditional Probability
The directional similarity measure that we devel-
oped and called Score is defined as follows:
Score(l, r) = Score
base
(l, r) ? Score
trick
(l, r)
where l and r are unary templates, and Score in-
dicates the probability of l ? r. Score
base
, which
is the base of Score, is defined as follows:
Score
base
(l, r) =
?
f?F
l
?F
r
P (r|f)P (f |l)
where F
s
is s?s feature vector (nouns including
compounds). The intention behind the definition
of Score
base
is to emulate the conditional proba-
bility P (v
r
|v
l
)
4 in a distributional similarity style
function. Note that P (v
r
|v
l
) should be 1 when en-
tailment v
l
? v
r
holds (i.e., v
r
is observed when-
ever v
l
is observed) and we have reliable proba-
bility values. Then, if we can directly estimate
P (v
r
|v
l
), it is reasonable to assume v
l
? v
r
if
P (v
r
|v
l
) is large enough. However, we cannot es-
timate P (v
r
|v
l
) directly since it is unlikely that we
will observe the verbs v
r
and v
l
at the same time.
(People do not usually repeat v
r
and v
l
in the same
document to avoid redundancy.) Thus, instead of
a direct estimation, we substitute Score
base
(l, r)
as defined above. In other words, we assume
P (v
r
|v
l
) ? P (r|l) ? ?
f?F
l
?F
r
P (f |l)P (r|f).
Actually, Score
base
originally had another mo-
tivation, inspired by Torisawa (2005), for which no
postposition but the instrumental postposition de
was relevant. In this discussion, all of the nouns
(fs) that are marked by the instrumental postposi-
tion are seen as ?tools,? and P (f |l) is interpreted
3ACC represents an accusative postposition in Japanese.
Likewise, NOM, DAT, INS, and TOP are the symbols for the
nominative, dative, instrumental, and topic postpositions.
4Remember that v
l
and v
r
are the verbs of unary tem-
plates l and r.
1174
as a measure of how typically the tool f is used
to perform the action denoted by (the v
l
of) l; if
P (f |l) is large enough, f is a typical tool used in
l. On the other hand, P (r|f) indicates the proba-
bility of (the v
r
of) r being the purpose for using
the tool f . See (1) for an example.
(1) konro-de chouri-suru
cooking.stove-INS cook
?cook (something) using a cooking stove.?
The purpose of using a cooking stove is to cook.
Torisawa (2005) has pointed out that when r ex-
presses the purpose of using a tool f , P (r|f) tends
to be large. This predicts that P (r|cooking stove)
is large, where r is ?de, cook?.
According to this observation, if f is a single
purpose tool and P (f |l), the probability of f be-
ing the tool by which l is performed, and P (r|f),
the probability of r being the purpose of using the
tool f , are large enough, then the typical perfor-
mance of the action v
l
should contain some ac-
tions that can be described by v
r
, i.e., the pur-
pose of using f . Moreover, if all the typical tools
(fs) used in v
l
are also used for v
r
, most perfor-
mances of the action v
l
should contain a part de-
scribed by the action v
r
. In summary, this means
that when ?
f?F
l
?F
r
P (r|f)P (f |l), Score
base
, has
a large value, we can expect v
l
? v
r
.
For example, let v
l
be deep-fry and v
r
be cook.
Note that v
l
? v
r
holds for this example. There
are many tools that are used for deep-frying,
such as cooking stove, pot, or pan. This means
that P (cooking stove|l), P (pot|l), or P (pan|l) are
large. On the other hand, the purpose of using all
of these tools is cooking, based on common sense.
Thus, probabilities such as P (r|cooking stove)
and P (r|pan) should have large values. Accord-
ingly, ?
f?F
l
?F
r
P (f |l)P (r|f), Score
base
, should
be relatively large for deep-fry ? cook,
Actually, we defined Score
base
based on the
above assumption However, through a series of
preliminary experiments, we found that the same
score could be applied without losing the preci-
sion to the other postpositions. Thus, we gener-
alized the framework so that it could deal with
most postpositions, namely ga (NOM), wo (ACC),
ni (DAT), de (INS), and wa (TOP). Note that this
is a variation of the distributional inclusion hy-
pothesis (Geffet and Dagan, 2005), but that we do
not use mutual information as in previous works,
based on the hypothesis discussed above. Actu-
ally, as shown in ?4, our conditional probability
based method outperformed the mutual informa-
tion based metrics in our experiments.
On the other hand, Score
trick
implements an-
other assumption that if only one feature con-
tributes to Score
base
and the contribution of the
other nouns is negligible, if any, the similarity is
unreliable. Accordingly, for Score
trick
, we uni-
formly ignore the contribution of the most domi-
nant feature from the similarity measurement.
Score
trick
(l, r)
= Score
base
(l, r) ? max
f?F
l
?F
r
P (r|f)P (f |l)
As shown in ?4, this trick actually improved the
entailment acquisition accuracy.
We used maximum likelihood estimation to ob-
tain P (r|f) and P (f |l) in the above discussion.
Bannard and Callison-Burch (2005) and Fujita
and Sato (2008) also proposed directional simi-
larity measures based on conditional probability,
which are very similar to Score
base
, although ei-
ther their method?s prerequisites or the targets of
the similarity measurements were different from
ours. The method of Bannard and Callison-Burch
(2005) requires bilingual parallel corpora, and
uses the translations of expressions as its feature.
Fujita and Sato (2008) dealt with productive pred-
icate phrases, while our target is non-productive
lexical units, i.e., verbs. Thus, this is the first
attempt to apply a conditional probability based
similarity measure to verb entailment acquisition.
In addition, the trick implemented in Score
trick
is
novel.
3.3 Preparing Template-Feature Tuples
Our method starts from a dataset called template-
feature tuples, which was derived from the Web
in the following way: 1) Parse the Japanese Web
corpus (Kawahara and Kurohashi, 2006a) derived
from 108 Japanese Web documents with Japanese
dependency parser KNP (Kawahara and Kuro-
hashi, 2006b). 2) Extract triples ?n, p, v? consist-
ing of nouns (n), postpositions (p), and verbs (v),
where an n marked by a p depends on a v from
the parsed Web text. 3) From the triple database,
construct template-feature tuples ?n, ?p, v?? by re-
garding ?p, v? as a unary template and n as one of
its features. 4) Convert the verbs into their canon-
ical forms as defined by KNP. 5) Filter out tuples
that fall into one of the following categories: 5-
1) Freq(?p, v?) < 20. 5-2) Its verb is passivized,
1175
causativized, or negated. 5-3) Its verb is semanti-
cally vague like be, do, or become. 5-4) Its post-
position is something other than ga (NOM), wo
(ACC), ni (DAT), de (INS), or wa (TOP).
The resulting unary template-feature tuples in-
cluded 127,808 kinds of templates that consisted
of 52,562 verb types and five kinds of postpo-
sitions. The verbs included compound words
like bosi-kansen-suru (mother.to.child-infection-
do) ?infect from mothers to infants.?
3.4 Acquiring Entailment Pairs
We acquired verb entailment pairs using the fol-
lowing procedure: i) From the template-feature
tuples mentioned in ?3.3, acquire unary template
pairs that exhibit an entailment relation between
them using the directional similarity measure in
?3.2. ii) Convert the acquired unary templates
?p, v? into naked verbs v by stripping the postpo-
sitions p. iii) Remove the duplicated verb pairs
resulting from stripping ps. To be precise, when
we removed the duplicated pairs, we left the high-
est ranked one. iv) Retrieve N-best verb pairs as
the final output from the result of iii). That is, we
first acquired unary template pairs and then trans-
formed them into verb pairs.
Although this paper focuses on verb entailment
acquisition, we also evaluated the accuracy of
template-level entailment acquisition, in order to
show that our similarity measure works well, not
only for verb entailment acquisition, but also for
template entailment acquisition (See ?4.4). we
created two kinds of unary templates: the ?Scoring
Slots? template and the ?Nom(inative) Slots? tem-
plate. The first is simply the result of the procedure
i); all of the templates have slots that are used for
similarity scoring. The second one was obtained
in the following way: 1) Only templates whose p
is not a nominative are sampled from the result of
the procedure i). 2) Their ps are all changed to a
nominative. Templates of the second kind are used
to show that the corresponding slots between tem-
plates (nominative, in this case) that are not used
for similarity scoring can be incorporated to re-
sulting template-level entailment pairs if the scor-
ing function really captures the semantic similarity
between templates.
Note that, for unary template entailment pairs
like (2) to be well-formed, the two unary slots (X-
wo) between templates must share the same noun
as the index i indicates. This is relevant in ?4.4.
(2) X
i
-wo musaborikuu ? X
i
-wo taberu
X
i
-ACC gobble X
i
-ACC eat
4 Evaluation
We compare the accuracy of our method with that
of the alternative methods in ?4.1. ?4.2 shows
the effectiveness of the trick. We examine the en-
tailment acquisition accuracy for frequent verbs in
?4.3, and evaluate the performance of our method
when applied to template-level entailment acquisi-
tion in ?4.4. Finally, by showing the accuracy for
verb pairs obtained from the top 100,000 results,
we claim that our method provides a good start-
ing point from which a large-scale verb entailment
resource can be constructed in ?4.5.
For the evaluation, three human annotators (not
the authors) checked whether each acquired entail-
ment pair was correct. The average of the three
Kappa values for each annotator pair was 0.579
for verb entailment pairs and 0.568 for template
entailment pairs, both of which indicate the mid-
dling stability of this evaluation annotation.
4.1 Experiment 1: Verb Pairs
We applied Score, BInc, Lin, and Precision to the
template-feature tuples (?3.3), obtained template
entailment pairs, and finally obtained verb entail-
ment pairs by removing the postpositions from the
templates as described in ?3. As a baseline, we
created pairs from randomly chosen verbs.
Since we targeted all of the verbs that ap-
peared on the Web (under the condition of
Freq(?p, v?) ? 20), the annotators were con-
fronted with technical terms and slang that they
did not know. In such cases, they consulted dic-
tionaries (either printed or machine readable ones)
and the Web. If they still could not find the mean-
ing of a verb, they labeled the pair containing the
unknown verb as incorrect.
We used the accuracy = # of correct pairs# of acquired pairs as
an evaluation measure. We regarded a pair as cor-
rect if it was judged correct by one (Accuracy-1),
two (Accuracy-2), or three (Accuracy-3) annota-
tors.
We evaluated 200 entailment pairs sampled
from the top 20,000 for each method (# of ac-
quired pairs = 200). For fairness, the evaluation
samples for each method were shuffled and placed
in one file from which the annotators worked. In
this way, they were unable to know which entail-
ment pair came from which method.
1176
Note that the verb entailment pairs produced
by Lin do not provide the directionality of en-
tailment. Thus, the annotators decided the direc-
tionality of these entailment pairs as follows: i)
Copy 200 original samples and reverse the order
of v
1
and v
2
. ii) Shuffle the 400 Lin samples
(the original and reversed samples) with the other
ones. iii) Evaluate all of the shuffled pairs. Each
Lin pair was regarded as correct if either direction
was judged correct. In other words, we evaluated
the upper bound performance of the LEDIR algo-
rithm.
Table 1 shows the accuracy of the acquired
verb entailment pairs for each method. Figure 1
Method Acc-1 Acc-2 Acc-3
Score 0.770 0.660 0.460
BInc 0.450 0.255 0.125
Precision 0.725 0.545 0.385
Lin 0.590 0.370 0.160
Random 0.050 0.010 0.005
Table 1: Accuracy of verb entailment pairs.
shows the accuracy figures for the N-best entail-
ment pairs for each method, with N being 1,000,
2,000, . . ., or 20,000. We observed the following
points from the results. First, Score outperformed
all the other methods. Second, Score and Pre-
cision, which are directional similarity measures,
worked well, while Lin, which is a symmetric one,
performed poorly even though the directionality of
its output was determined manually.
Looking at the evaluated samples, Score suc-
cessfully acquired pairs in which the entailed
verbs generalized entailing verbs that were techni-
cal terms. (3) shows examples of Score?s outputs.
(3) a. RSS-haisin-suru ? todokeru
RSS-feed-do deliver
?feed the RSS data?
b. middosippu-maunto-suru ? tumu
midship-mounting-do mount
?have (engine) midship-mounted?
The errors made by DIRT (4) and BInc (5) in-
cluded pairs consisting of technical terms.
(4) kurakkingu-suru
software.cracking-do
?crack a (security) system?
? koutiku-hosyu-suru
building-maintenance-do
?build and maintain a system?
Accuracy-1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
BInc
Precision
Lin
Accuracy-2
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
BInc
Precision
Lin
Accuracy-3
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
BInc
Precision
Lin
Figure 1: Accuracy of verb entailment pairs.
(5) suisou-siiku-suru
tank-raising-do
?raise (fish) in a tank?
? siken-houryuu-suru
test-discharge-do
?stock (with fish) experimentally?
These terms are related in some sense, but they
are not entailment pairs.
4.2 Experiment 2: Effectiveness of the Trick
Next, we investigated the effectiveness of the trick
described in ?3. We evaluated Score, Score
trick
,
and Score
base
. Table 2 shows the accuracy figures
for each method. Figure 2 shows the accuracy fig-
ures for the N-best outputs for each method. The
1177
Method Acc-1 Acc-2 Acc-3
Score 0.770 0.660 0.460
Score
trick
0.725 0.610 0.395
Score
base
0.590 0.465 0.315
Table 2: Effectiveness of the trick.
results illustrate that introducing the trick signif-
icantly improved the performance of Score
base
,
and so did multiplying Score
trick
and Score
base
,
which is our proposal Score.
(6) shows an example of Score
base
?s errors.
(6) gazou-sakusei-suru ? henkou-suru
image-making-do change-do
?make an image? ?change?
This pair has only two shared nouns (f ? F
l
?F
r
),
and more than 99.99% of the pair?s similarity re-
flects only one of the two. Clearly, the trick would
have prevented the pair from being highly ranked.
4.3 Experiment 3: Pairs of Frequent Verbs
We found that the errors made by Lin and BInc
in Experiment 1 were mostly pairs of infrequent
verbs such as technical terms. Thus, we con-
ducted the acquisition of entailment pairs targeting
more frequent verbs to see how their performance
changed. The experimental conditions were the
same as in Experiment 1, except that the templates
(?p, v?) used were all Freq(?p, v?) ? 200.
Table 3 shows the accuracy figures for each
method with the changes in accuracy from those
of the original methods in parentheses. The re-
Method Acc-1 Acc-2 Acc-3
Score
0.690 0.520 0.335
(?0.080) (?0.140) (?0.125)
BInc 0.455 0.295 0.160(+0.005) (+0.040) (+0.035)
Precision 0.450 0.355 0.205(?0.275) (?0.190) (?0.180)
Lin 0.635 0.385 0.205(+0.045) (+0.015) (+0.045)
Table 3: Accuracy of frequent verb pairs.
sults show that the accuracies of Score and Pre-
cision (the two best methods in Experiment 1) de-
graded, while the other two improved a little. We
suspect that the performance difference between
these methods would get smaller if we further re-
stricted the target verbs to more frequent ones.
Accuracy-1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
ScoretrickScorebase
Accuracy-2
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
ScoretrickScorebase
Accuracy-3
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
ScoretrickScorebase
Figure 2: Accuracy of verb entailment pairs ac-
quired by Score, Score
trick
, and Score
base
.
However, we believe that dealing with verbs com-
prehensively, including infrequent ones, is impor-
tant, since, in the era of information explosion, the
impact on applications is determined not only by
frequent verbs but also infrequent ones that consti-
tute the long tail of a verb-frequency graph. Thus,
this tendency does not matter for our purpose.
4.4 Experiment 4: Template Pairs
This section presents the entailment acquisition
accuracy for template pairs to show that our
method can also perform the entailment acqui-
sition of unary templates. We presented pairs
of unary templates, obtained by the procedure in
1178
?3.4, to the annotators. In doing so, we restricted
the correct entailment pairs to those for which en-
tailment always held regardless of what argument
filled the two unary slots, and the two slots had to
be filled with the same argument, as exemplified
in (2). We evaluated Score and Precision.
Table 4 shows the accuracy of the acquired pairs
of unary templates. Compared to verb entailment
Method Acc-1 Acc-2 Acc-3
Score
0.655 0.510 0.300
Scoring (?0.115) (?0.150) (?0.160)
Slots Precision 0.565 0.430 0.265(?0.160) (?0.115) (?0.120)
Score
0.665 0.515 0.315
Nom (?0.105) (?0.145) (?0.145)
Slots Precision 0.490 0.325 0.215(?0.235) (?0.220) (?0.170)
Table 4: Accuracy of entailment pairs of templates
whose slots were used for scoring.
acquisition, the accuracy of both methods dropped
by about 10%. This was mainly due to the evalua-
tion restriction exemplified in (2) which was not
introduced in the previous experiments; the an-
notators ignored the argument correspondence be-
tween the verb pairs in Experiment 1. Also note
that Score outperformed Precision in this experi-
ment, too.
(7) and (8) are examples of the Scoring Slots
template entailment pairs and (9) is that of the
Nom Slots acquired by our method.
(7) X-wo tatigui-suru ? X-wo taberu
X-ACC standing.up.eating-do X-ACC eat
?eat X standing up? ?eat X?
(8) X-de marineedo-suru ? X-wo ireru
X-INS marinade-do X-ACC pour
?marinate with X? ?pour X?
(9) X-ga NBA-iri-suru ? ? ? (was X-de (INS))
X-NOM NBA-entering-do
?X joins an NBA team?
? X-ga nyuudan-suru ? ? ? (was X-de)
X-NOM enrollment-do
?X joins a team?
4.5 Experiment 5: Verb Pairs form the Top
100,000
Finally, we examined the accuracy of the top
100,000 verb pairs acquired by Score and Preci-
sion. As Table 5 shows, Score outperformed Pre-
Method Acc-1 Acc-2 Acc-3
Score 0.610 0.480 0.300
Precision 0.470 0.295 0.190
Table 5: Accuracy of the top 100,000 verb pairs.
cision. Note also that Score kept a reasonable ac-
curacy for the top 100,000 results (Acc-2: 48%).
The accuracy is encouraging enough to consider
human annotation for the top 100,000 results to
produce a language resource for verb entailment,
which we actually plan to do.
Below are correct verb entailment examples
from the top 100,000 results of our method.
(10) The 121th pair
kaado-kessai-suru ? siharau
card-payment-do pay
?pay by card? ?pay?
(11) The 6,081th pair
saitei-suru ? sadameru
adjudicate-do settle
?adjudicate? ?settle?
(12) The 15,464th pair
eraa-syuuryou-suru ? jikkou-suru
error-termination-do perform-do
?abend? ?execute?
(13) The 30,044th pair
ribuuto-suru ? kidou-suru
reboot-do start-do
?reboot? ?boot?
(14) The 57,653th pair
rinin-suru ? syuunin-suru
resignation-do accession-do
?resign? ?accede?
(15) The 70,103th pair
sijou-tounyuu-suru ? happyou-suru
market-input-do publication-do
?bring to the market? ?publicize?
Below are examples of erroneous pairs from our
results. (16) is a causal relation but not an entail-
ment. (17) is a contradictory pair.
(16) The 5,475th pair
juken-suru ? goukaku-suru
take.an.exam-do acceptance-do
?take an exam? ?gain admission?
1179
(17) The 40,504th pair
ketujou-suru ? syutujou-suru
not.take.part-do take.part-do
?not take part? ?take part?
5 Conclusion
This paper addressed verb entailment acquisition
from the Web, and proposed a novel directional
similarity measure Score. Through a series of ex-
periments, we showed i) that Score outperforms
the previously proposed measures, Lin, Precision,
and BInc in large scale verb entailment acquisi-
tion, ii) that our proposed trick implemented in
Score
trick
significantly improves the accuracy of
verb entailment acquisition despite its simplicity,
iii) that Score worked better than the others even
when we restricted the target verbs to more fre-
quent ones, iv) that our method is also moder-
ately successful at producing template-level en-
tailment pairs, and v) that our method maintained
reasonable accuracy (in terms of human annota-
tion) for the top 100,000 results. As examples of
the acquired verb entailment pairs illustrated, our
method can acquire from an ocean of information,
namely the Web, a variety of verb entailment pairs
ranging from those that are used in daily life to
those that are used in very specific fields.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL2005),
pages 597?604.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In Proceedings of
HLT-NAACL 2003, pages 16?23.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
Ledir: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP2007), pages 161?170.
Atsushi Fujita and Satoshi Sato. 2008. A probabilis-
tic model for measuring grammaticality and similar-
ity of automatically generated paraphrases of pred-
icate phrases. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(COLING2008), pages 225?232.
Maayan Geffet and Ido Dagan. 2005. The dis-
tributional inclusion hypotheses and lexical entail-
ment. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL2005), pages 107?114.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Ex-
tracting structural paraphrases from aligned mono-
lingual corpora. In Proceedings of the 2nd Interna-
tional Workshop on Paraphrasing (IWP2003), pages
57?64.
Daisuke Kawahara and Sadao Kurohashi. 2006a.
Case Frame Compilation from the Web using High-
Performance Computing. In Proceedings of The 5th
International Conference on Language Resources
and Evaluation (LREC-06), pages 1344?1347.
Daisuke Kawahara and Sadao Kurohashi. 2006b. A
Fully-Lexicalized Probabilistic Model for Japanese
Syntactic and Case Structure Analysis. In Pro-
ceedings of the Human Language Technology Con-
ference of the North American Chapter of the
Association for Computational Linguistics (HLT-
NAACL2006), pages 176?183.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics (COLING-ACL1998), pages
768?774.
Viktor Pekar. 2006. Acquisition of verb entailment
from text. In Proceedings of the main confer-
ence on Human Language Technology Conference
of the North American Chapter of the Association
of Computational Linguistics (HLT-NAACL2006),
pages 49?56.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news
articles. In Proceedings of the 2nd international
Conference on Human Language Technology Re-
search (HLT2002), pages 313?318.
Idan Szpector, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP2004), pages 41?48.
Idan Szpektor and Ido Dagan. 2008. Learning en-
tailment rules for unary template. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics (COLING2008), pages 849?856.
Kentaro Torisawa. 2005. Automatic acquisition of ex-
pressions representing preparation and utilization of
an object. In Proceedings of the Recent Advances
in Natural Language Processing (RANLP05), pages
556?560.
1180
Kentaro Torisawa. 2006. Acquiring inference rules
with temporal constraints by using japanese cood-
inated sentences and noun-verb co-occurences. In
Proceedings of the Human Language Technology
Conference of the Norh American Chapter of the
ACL (HLT-NAACL2006), pages 57?64.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP2003), pages 81?88.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using se-
lectional preferences. In Proceedings of the 44th
Annual Meeting of the Association for Computa-
tional Linguistics and 21th InternationalConference
on Computational Linguistics (COLING-ACL2006),
pages 849?856.
1181
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 513?521,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
An Error-Driven Word-Character Hybrid Model
for Joint Chinese Word Segmentation and POS Tagging
Canasai Kruengkrai?? and Kiyotaka Uchimoto? and Jun?ichi Kazama?
Yiou Wang? and Kentaro Torisawa? and Hitoshi Isahara??
?Graduate School of Engineering, Kobe University
1-1 Rokkodai-cho, Nada-ku, Kobe 657-8501 Japan
?National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289 Japan
{canasai,uchimoto,kazama,wangyiou,torisawa,isahara}@nict.go.jp
Abstract
In this paper, we present a discriminative
word-character hybrid model for joint Chi-
nese word segmentation and POS tagging.
Our word-character hybrid model offers
high performance since it can handle both
known and unknown words. We describe
our strategies that yield good balance for
learning the characteristics of known and
unknown words and propose an error-
driven policy that delivers such balance
by acquiring examples of unknown words
from particular errors in a training cor-
pus. We describe an efficient framework
for training our model based on the Mar-
gin Infused Relaxed Algorithm (MIRA),
evaluate our approach on the Penn Chinese
Treebank, and show that it achieves supe-
rior performance compared to the state-of-
the-art approaches reported in the litera-
ture.
1 Introduction
In Chinese, word segmentation and part-of-speech
(POS) tagging are indispensable steps for higher-
level NLP tasks. Word segmentation and POS tag-
ging results are required as inputs to other NLP
tasks, such as phrase chunking, dependency pars-
ing, and machine translation. Word segmenta-
tion and POS tagging in a joint process have re-
ceived much attention in recent research and have
shown improvements over a pipelined fashion (Ng
and Low, 2004; Nakagawa and Uchimoto, 2007;
Zhang and Clark, 2008; Jiang et al, 2008a; Jiang
et al, 2008b).
In joint word segmentation and the POS tag-
ging process, one serious problem is caused by
unknown words, which are defined as words that
are not found in a training corpus or in a sys-
tem?s word dictionary1. The word boundaries and
the POS tags of unknown words, which are very
difficult to identify, cause numerous errors. The
word-character hybrid model proposed by Naka-
gawa and Uchimoto (Nakagawa, 2004; Nakagawa
and Uchimoto, 2007) shows promising properties
for solving this problem. However, it suffers from
structural complexity. Nakagawa (2004) described
a training method based on a word-based Markov
model and a character-based maximum entropy
model that can be completed in a reasonable time.
However, this training method is limited by the
generatively-trained Markov model in which in-
formative features are hard to exploit.
In this paper, we overcome such limitations
concerning both efficiency and effectiveness. We
propose a new framework for training the word-
character hybrid model based on the Margin
Infused Relaxed Algorithm (MIRA) (Crammer,
2004; Crammer et al, 2005; McDonald, 2006).
We describe k-best decoding for our hybrid model
and design its loss function and the features appro-
priate for our task.
In our word-character hybrid model, allowing
the model to learn the characteristics of both
known and unknown words is crucial to achieve
optimal performance. Here, we describe our
strategies that yield good balance for learning
these two characteristics. We propose an error-
driven policy that delivers this balance by acquir-
ing examples of unknown words from particular
errors in a training corpus. We conducted our ex-
periments on Penn Chinese Treebank (Xia et al,
2000) and compared our approach with the best
previous approaches reported in the literature. Ex-
perimental results indicate that our approach can
achieve state-of-the-art performance.
1A system?s word dictionary usually consists of a word
list, and each word in the list has its own POS category. In
this paper, we constructed the system?s word dictionary from
a training corpus.
513
Figure 1: Lattice used in word-character hybrid model.
Tag Description
B Beginning character in a multi-character word
I Intermediate character in a multi-character word
E End character in a multi-character word
S Single-character word
Table 1: Position-of-character (POC) tags.
The paper proceeds as follows: Section 2 gives
background on the word-character hybrid model,
Section 3 describes our policies for correct path
selection, Section 4 presents our training method
based on MIRA, Section 5 shows our experimen-
tal results, Section 6 discusses related work, and
Section 7 concludes the paper.
2 Background
2.1 Problem formation
In joint word segmentation and the POS tag-
ging process, the task is to predict a path
of word hypotheses y = (y1, . . . , y#y) =
(?w1, p1?, . . . , ?w#y, p#y?) for a given character
sequence x = (c1, . . . , c#x), where w is a word,
p is its POS tag, and a ?#? symbol denotes the
number of elements in each variable. The goal of
our learning algorithm is to learn a mapping from
inputs (unsegmented sentences) x ? X to outputs
(segmented paths) y ? Y based on training sam-
ples of input-output pairs S = {(xt, yt)}Tt=1.
2.2 Search space representation
We represent the search space with a lattice based
on the word-character hybrid model (Nakagawa
and Uchimoto, 2007). In the hybrid model,
given an input sentence, a lattice that consists
of word-level and character-level nodes is con-
structed. Word-level nodes, which correspond to
words found in the system?s word dictionary, have
regular POS tags. Character-level nodes have spe-
cial tags where position-of-character (POC) and
POS tags are combined (Asahara, 2003; Naka-
gawa, 2004). POC tags indicate the word-internal
positions of the characters, as described in Table 1.
Figure 1 shows an example of a lattice for a Chi-
nese sentence: ? ? (Chongming is
China?s third largest island). Note that some nodes
and state transitions are not allowed. For example,
I and E nodes cannot occur at the beginning of the
lattice (marked with dashed boxes), and the transi-
tions from I to B nodes are also forbidden. These
nodes and transitions are ignored during the lattice
construction processing.
In the training phase, since several paths
(marked in bold) can correspond to the correct
analysis in the annotated corpus, we need to se-
lect one correct path yt as a reference for training.2
The next section describes our strategies for deal-
ing with this issue.
With this search space representation, we
can consistently handle unknown words with
character-level nodes. In other words, we use
word-level nodes to identify known words and
character-level nodes to identify unknown words.
In the testing phase, we can use a dynamic pro-
gramming algorithm to search for the most likely
path out of all candidate paths.
2A machine learning problem exists called structured
multi-label classification that allows training from multiple
correct paths. However, in this paper we limit our considera-
tion to structured single-label classification, which is simple
yet provides great performance.
514
3 Policies for correct path selection
In this section, we describe our strategies for se-
lecting the correct path yt in the training phase.
As shown in Figure 1, the paths marked in bold
can represent the correct annotation of the seg-
mented sentence. Ideally, we need to build a word-
character hybrid model that effectively learns the
characteristics of unknown words (with character-
level nodes) as well as those of known words (with
word-level nodes).
We can directly estimate the statistics of known
words from an annotated corpus where a sentence
is already segmented into words and assigned POS
tags. If we select the correct path yt that corre-
sponds to the annotated sentence, it will only con-
sist of word-level nodes that do not allow learning
for unknown words. We therefore need to choose
character-level nodes as correct nodes instead of
word-level nodes for some words. We expect that
those words could reflect unknown words in the
future.
Baayen and Sproat (1996) proposed that the
characteristics of infrequent words in a training
corpus resemble those of unknown words. Their
idea has proven effective for estimating the statis-
tics of unknown words in previous studies (Ratna-
parkhi, 1996; Nagata, 1999; Nakagawa, 2004).
We adopt Baayen and Sproat?s approach as
the baseline policy in our word-character hybrid
model. In the baseline policy, we first count the
frequencies of words3 in the training corpus. We
then collect infrequent words that appear less than
or equal to r times.4 If these infrequent words are
in the correct path, we use character-level nodes
to represent them, and hence the characteristics of
unknown words can be learned. For example, in
Figure 1 we select the character-level nodes of the
word ? ? (Chongming) as the correct nodes. As
a result, the correct path yt can contain both word-
level and character-level nodes (marked with as-
terisks (*)).
To discover more statistics of unknown words,
one might consider just increasing the threshold
value r to obtain more artificial unknown words.
However, our experimental results indicate that
our word-character hybrid model requires an ap-
propriate balance between known and artificial un-
3We consider a word and its POS tag a single entry.
4In our experiments, the optimal threshold value r is se-
lected by evaluating the performance of joint word segmen-
tation and POS tagging on the development set.
known words to achieve optimal performance.
We now describe our new approach to lever-
age additional examples of unknown words. In-
tuition suggests that even though the system can
handle some unknown words, many unidentified
unknown words remain that cannot be recovered
by the system; we wish to learn the characteristics
of such unidentified unknown words. We propose
the following simple scheme:
? Divide the training corpus into ten equal sets
and perform 10-fold cross validation to find
the errors.
? For each trial, train the word-character hybrid
model with the baseline policy (r = 1) us-
ing nine sets and estimate errors using the re-
maining validation set.
? Collect unidentified unknown words from
each validation set.
Several types of errors are produced by the
baseline model, but we only focus on those caused
by unidentified unknown words, which can be eas-
ily collected in the evaluation process. As de-
scribed later in Section 5.2, we measure the recall
on out-of-vocabulary (OOV) words. Here, we de-
fine unidentified unknown words as OOV words
in each validation set that cannot be recovered by
the system. After ten cross validation runs, we
get a list of the unidentified unknown words de-
rived from the whole training corpus. Note that
the unidentified unknown words in the cross val-
idation are not necessary to be infrequent words,
but some overlap may exist. Finally, we obtain the
artificial unknown words that combine the uniden-
tified unknown words in cross validation and in-
frequent words for learning unknown words. We
refer to this approach as the error-driven policy.
4 Training method
4.1 Discriminative online learning
Let Yt = {y1t , . . . , yKt } be a lattice consisting of
candidate paths for a given sentence xt. In the
word-character hybrid model, the lattice Yt can
contain more than 1000 nodes, depending on the
length of the sentence xt and the number of POS
tags in the corpus. Therefore, we require a learn-
ing algorithm that can efficiently handle large and
complex lattice structures.
Online learning is an attractive method for
the hybrid model since it quickly converges
515
Algorithm 1 Generic Online Learning Algorithm
Input: Training set S = {(xt, yt)}Tt=1
Output: Model weight vector w
1: w(0) = 0;v = 0; i = 0
2: for iter = 1 to N do
3: for t = 1 to T do
4: w(i+1) = update w(i) according to (xt, yt)
5: v = v +w(i+1)
6: i = i+ 1
7: end for
8: end for
9: w = v/(N ? T )
within a few iterations (McDonald, 2006). Algo-
rithm 1 outlines the generic online learning algo-
rithm (McDonald, 2006) used in our framework.
4.2 k-best MIRA
We focus on an online learning algorithm called
MIRA (Crammer, 2004), which has the de-
sired accuracy and scalability properties. MIRA
combines the advantages of margin-based and
perceptron-style learning with an optimization
scheme. In particular, we use a generalized ver-
sion of MIRA (Crammer et al, 2005; McDonald,
2006) that can incorporate k-best decoding in the
update procedure. To understand the concept of k-
best MIRA, we begin with a linear score function:
s(x, y;w) = ?w, f(x, y)? , (1)
where w is a weight vector and f is a feature rep-
resentation of an input x and an output y.
Learning a mapping between an input-output
pair corresponds to finding a weight vector w such
that the best scoring path of a given sentence is
the same as (or close to) the correct path. Given
a training example (xt, yt), MIRA tries to estab-
lish a margin between the score of the correct path
s(xt, yt;w) and the score of the best candidate
path s(xt, y?;w) based on the current weight vector
w that is proportional to a loss function L(yt, y?).
In each iteration, MIRA updates the weight vec-
tor w by keeping the norm of the change in the
weight vector as small as possible. With this
framework, we can formulate the optimization
problem as follows (McDonald, 2006):
w(i+1) = argminw?w ?w(i)? (2)
s.t. ?y? ? bestk(xt;w(i)) :
s(xt, yt;w)? s(xt, y?;w) ? L(yt, y?) ,
where bestk(xt;w(i)) ? Yt represents a set of top
k-best paths given the weight vector w(i). The
above quadratic programming (QP) problem can
be solved using Hildreth?s algorithm (Yair Cen-
sor, 1997). Replacing Eq. (2) into line 4 of Al-
gorithm 1, we obtain k-best MIRA.
The next question is how to efficiently gener-
ate bestk(xt;w(i)). In this paper, we apply a dy-
namic programming search (Nagata, 1994) to k-
best MIRA. The algorithm has two main search
steps: forward and backward. For the forward
search, we use Viterbi-style decoding to find the
best partial path and its score up to each node in
the lattice. For the backward search, we use A?-
style decoding to generate the top k-best paths. A
complete path is found when the backward search
reaches the beginning node of the lattice, and the
algorithm terminates when the number of gener-
ated paths equals k.
In summary, we use k-best MIRA to iteratively
update w(i). The final weight vector w is the av-
erage of the weight vectors after each iteration.
As reported in (Collins, 2002; McDonald et al,
2005), parameter averaging can effectively avoid
overfitting. For inference, we can use Viterbi-style
decoding to search for the most likely path y? for
a given sentence x where:
y? = argmax
y?Y
s(x, y;w) . (3)
4.3 Loss function
In conventional sequence labeling where the ob-
servation sequence (word) boundaries are fixed,
one can use the 0/1 loss to measure the errors of
a predicted path with respect to the correct path.
However, in our model, word boundaries vary
based on the considered path, resulting in a dif-
ferent numbers of output tokens. As a result, we
cannot directly use the 0/1 loss.
We instead compute the loss function through
false positives (FP ) and false negatives (FN ).
Here, FP means the number of output nodes that
are not in the correct path, and FN means the
number of nodes in the correct path that cannot
be recognized by the system. We define the loss
function by:
L(yt, y?) = FP + FN . (4)
This loss function can reflect how bad the pre-
dicted path y? is compared to the correct path yt.
A weighted loss function based on FP and FN
can be found in (Ganchev et al, 2007).
516
ID Template Condition
W0 ?w0? for word-level
W1 ?p0? nodes
W2 ?w0, p0?
W3 ?Length(w0), p0?
A0 ?AS(w0)? if w0 is a single-
A1 ?AS(w0), p0? character word
A2 ?AB(w0)? for word-level
A3 ?AB(w0), p0? nodes
A4 ?AE(w0)?
A5 ?AE(w0), p0?
A6 ?AB(w0), AE(w0)?
A7 ?AB(w0), AE(w0), p0?
T0 ?TS(w0)? if w0 is a single-
T1 ?TS(w0), p0? character word
T2 ?TB(w0)? for word-level
T3 ?TB(w0), p0? nodes
T4 ?TE(w0)?
T5 ?TE(w0), p0?
T6 ?TB(w0), TE(w0)?
T7 ?TB(w0), TE(w0), p0?
C0 ?cj?, j ? [?2, 2] ? p0 for character-
C1 ?cj , cj+1?, j ? [?2, 1] ? p0 level nodes
C2 ?c?1, c1? ? p0
C3 ?T (cj)?, j ? [?2, 2] ? p0
C4 ?T (cj), T (cj+1)?, j ? [?2, 1] ? p0
C5 ?T (c?1), T (c1)? ? p0
C6 ?c0, T (c0)? ? p0
Table 2: Unigram features.
4.4 Features
This section discusses the structure of f(x, y). We
broadly classify features into two categories: uni-
gram and bigram features. We design our feature
templates to capture various levels of information
about words and POS tags. Let us introduce some
notation. We write w?1 and w0 for the surface
forms of words, where subscripts ?1 and 0 in-
dicate the previous and current positions, respec-
tively. POS tags p?1 and p0 can be interpreted in
the same way. We denote the characters by cj .
Unigram features: Table 2 shows our unigram
features. Templates W0?W3 are basic word-level
unigram features, where Length(w0) denotes the
length of the word w0. Using just the surface
forms can overfit the training data and lead to poor
predictions on the test data. To alleviate this prob-
lem, we use two generalized features of the sur-
face forms. The first is the beginning and end
characters of the surface (A0?A7). For example,
?AB(w0)? denotes the beginning character of the
current word w0, and ?AB(w0), AE(w0)? denotes
the beginning and end characters in the word. The
second is the types of beginning and end charac-
ters of the surface (T0?T7). We define a set of
general character types, as shown in Table 4.
Templates C0?C6 are basic character-level un-
ID Template Condition
B0 ?w?1, w0? if w?1 and w0
B1 ?p?1, p0? are word-level
B2 ?w?1, p0? nodes
B3 ?p?1, w0?
B4 ?w?1, w0, p0?
B5 ?p?1, w0, p0?
B6 ?w?1, p?1, w0?
B7 ?w?1, p?1, p0?
B8 ?w?1, p?1, w0, p0?
B9 ?Length(w?1), p0?
TB0 ?TE(w?1)?
TB1 ?TE(w?1), p0?
TB2 ?TE(w?1), p?1, p0?
TB3 ?TE(w?1), TB(w0)?
TB4 ?TE(w?1), TB(w0), p0?
TB5 ?TE(w?1), p?1, TB(w0)?
TB6 ?TE(w?1), p?1, TB(w0), p0?
CB0 ?p?1, p0? otherwise
Table 3: Bigram features.
Character type Description
Space Space
Numeral Arabic and Chinese numerals
Symbol Symbols
Alphabet Alphabets
Chinese Chinese characters
Other Others
Table 4: Character types.
igram features taken from (Nakagawa, 2004).
These templates operate over a window of ?2
characters. The features include characters (C0),
pairs of characters (C1?C2), character types (C3),
and pairs of character types (C4?C5). In addi-
tion, we add pairs of characters and character types
(C6).
Bigram features: Table 3 shows our bigram
features. Templates B0-B9 are basic word-
level bigram features. These features aim to
capture all the possible combinations of word
and POS bigrams. Templates TB0-TB6 are the
types of characters for bigrams. For example,
?TE(w?1), TB(w0)? captures the change of char-
acter types from the end character in the previ-
ous word to the beginning character in the current
word.
Note that if one of the adjacent nodes is a
character-level node, we use the template CB0 that
represents POS bigrams. In our preliminary ex-
periments, we found that if we add more features
to non-word-level bigrams, the number of features
grows rapidly due to the dense connections be-
tween non-word-level nodes. However, these fea-
tures only slightly improve performance over us-
ing simple POS bigrams.
517
(a) Experiments on small training corpus
Data set CTB chap. IDs # of sent. # of words
Training 1-270 3,046 75,169
Development 301-325 350 6,821
Test 271-300 348 8,008
# of POS tags 32
OOV (word) 0.0987 (790/8,008)
OOV (word & POS) 0.1140 (913/8,008)
(b) Experiments on large training corpus
Data set CTB chap. IDs # of sent. # of words
Training 1-270, 18,089 493,939
400-931,
1001-1151
Development 301-325 350 6,821
Test 271-300 348 8,008
# of POS tags 35
OOV (word) 0.0347 (278/8,008)
OOV (word & POS) 0.0420 (336/8,008)
Table 5: Training, development, and test data
statistics on CTB 5.0 used in our experiments.
5 Experiments
5.1 Data sets
Previous studies on joint Chinese word segmen-
tation and POS tagging have used Penn Chinese
Treebank (CTB) (Xia et al, 2000) in experiments.
However, versions of CTB and experimental set-
tings vary across different studies.
In this paper, we used CTB 5.0 (LDC2005T01)
as our main corpus, defined the training, develop-
ment and test sets according to (Jiang et al, 2008a;
Jiang et al, 2008b), and designed our experiments
to explore the impact of the training corpus size on
our approach. Table 5 provides the statistics of our
experimental settings on the small and large train-
ing data. The out-of-vocabulary (OOV) is defined
as tokens in the test set that are not in the train-
ing set (Sproat and Emerson, 2003). Note that the
development set was only used for evaluating the
trained model to obtain the optimal values of tun-
able parameters.
5.2 Evaluation
We evaluated both word segmentation (Seg) and
joint word segmentation and POS tagging (Seg
& Tag). We used recall (R), precision (P ), and
F1 as evaluation metrics. Following (Sproat and
Emerson, 2003), we also measured the recall on
OOV (ROOV) tokens and in-vocabulary (RIV) to-
kens. These performance measures can be calcu-
lated as follows:
Recall (R) = # of correct tokens# of tokens in test data
Precision (P ) = # of correct tokens# of tokens in system output
F1 = 2 ?R ? PR+ P
ROOV = # of correct OOV tokens# of OOV tokens in test data
RIV = # of correct IV tokens# of IV tokens in test data
For Seg, a token is considered to be a cor-
rect one if the word boundary is correctly iden-
tified. For Seg & Tag, both the word boundary and
its POS tag have to be correctly identified to be
counted as a correct token.
5.3 Parameter estimation
Our model has three tunable parameters: the num-
ber of training iterations N ; the number of top
k-best paths; and the threshold r for infrequent
words. Since we were interested in finding an
optimal combination of word-level and character-
level nodes for training, we focused on tuning r.
We fixed N = 10 and k = 5 for all experiments.
For the baseline policy, we varied r in the range
of [1, 5] and found that setting r = 3 yielded the
best performance on the development set for both
the small and large training corpus experiments.
For the error-driven policy, we collected unidenti-
fied unknown words using 10-fold cross validation
on the training set, as previously described in Sec-
tion 3.
5.4 Impact of policies for correct path
selection
Table 6 shows the results of our word-character
hybrid model using the error-driven and baseline
policies. The third and fourth columns indicate the
numbers of known and artificial unknown words
in the training phase. The total number of words
is the same, but the different policies yield differ-
ent balances between the known and artificial un-
known words for learning the hybrid model. Op-
timal balances were selected using the develop-
ment set. The error-driven policy provides addi-
tional artificial unknown words in the training set.
The error-driven policy can improve ROOV as well
as maintain good RIV, resulting in overall F1 im-
provements.
518
(a) Experiments on small training corpus
# of words in training (75,169)
Eval type Policy kwn. art. unk. R P F1 ROOV RIV
Seg error-driven 63,997 11,172 0.9587 0.9509 0.9548 0.7557 0.9809baseline 64,999 10,170 0.9572 0.9489 0.9530 0.7304 0.9820
Seg & Tag error-driven 63,997 11,172 0.8929 0.8857 0.8892 0.5444 0.9377baseline 64,999 10,170 0.8897 0.8820 0.8859 0.5246 0.9367
(b) Experiments on large training corpus
# of words in training (493,939)
Eval Type Policy kwn. art. unk. R P F1 ROOV RIV
Seg error-driven 442,423 51,516 0.9829 0.9746 0.9787 0.7698 0.9906baseline 449,679 44,260 0.9821 0.9736 0.9779 0.7590 0.9902
Seg & Tag error-driven 442,423 51,516 0.9407 0.9328 0.9367 0.5982 0.9557baseline 449,679 44,260 0.9401 0.9319 0.9360 0.5952 0.9552
Table 6: Results of our word-character hybrid model using error-driven and baseline policies.
Method Seg Seg & Tag
Ours (error-driven) 0.9787 0.9367
Ours (baseline) 0.9779 0.9360
Jiang08a 0.9785 0.9341
Jiang08b 0.9774 0.9337
N&U07 0.9783 0.9332
Table 7: Comparison of F1 results with previous
studies on CTB 5.0.
Seg Seg & Tag
N&U07 Z&C08 Ours N&U07 Z&C08 Ours
Trial (base.) (base.)
1 0.9701 0.9721 0.9732 0.9262 0.9346 0.9358
2 0.9738 0.9762 0.9752 0.9318 0.9385 0.9380
3 0.9571 0.9594 0.9578 0.9023 0.9086 0.9067
4 0.9629 0.9592 0.9655 0.9132 0.9160 0.9223
5 0.9597 0.9606 0.9617 0.9132 0.9172 0.9187
6 0.9473 0.9456 0.9460 0.8823 0.8883 0.8885
7 0.9528 0.9500 0.9562 0.9003 0.9051 0.9076
8 0.9519 0.9512 0.9528 0.9002 0.9030 0.9062
9 0.9566 0.9479 0.9575 0.8996 0.9033 0.9052
10 0.9631 0.9645 0.9659 0.9154 0.9196 0.9225
Avg. 0.9595 0.9590 0.9611 0.9085 0.9134 0.9152
Table 8: Comparison of F1 results of our baseline
model with Nakagawa and Uchimoto (2007) and
Zhang and Clark (2008) on CTB 3.0.
Method Seg Seg & Tag
Ours (baseline) 0.9611 0.9152
Z&C08 0.9590 0.9134
N&U07 0.9595 0.9085
N&L04 0.9520 -
Table 9: Comparison of averaged F1 results (by
10-fold cross validation) with previous studies on
CTB 3.0.
5.5 Comparison with best prior approaches
In this section, we attempt to make meaning-
ful comparison with the best prior approaches re-
ported in the literature. Although most previous
studies used CTB, their versions of CTB and ex-
perimental settings are different, which compli-
cates comparison.
Ng and Low (2004) (N&L04) used CTB 3.0.
However, they just showed POS tagging results
on a per character basis, not on a per word basis.
Zhang and Clark (2008) (Z&C08) generated CTB
3.0 from CTB 4.0. Jiang et al (2008a; 2008b)
(Jiang08a, Jiang08b) used CTB 5.0. Shi and
Wang (2007) used CTB that was distributed in the
SIGHAN Bakeoff. Besides CTB, they also used
HowNet (Dong and Dong, 2006) to obtain seman-
tic class features. Zhang and Clark (2008) indi-
cated that their results cannot directly compare to
the results of Shi and Wang (2007) due to different
experimental settings.
We decided to follow the experimental settings
of Jiang et al (2008a; 2008b) on CTB 5.0 and
Zhang and Clark (2008) on CTB 4.0 since they
reported the best performances on joint word seg-
mentation and POS tagging using the training ma-
terials only derived from the corpora. The perfor-
mance scores of previous studies are directly taken
from their papers. We also conducted experiments
using the system implemented by Nakagawa and
Uchimoto (2007) (N&U07) for comparison.
Our experiment on the large training corpus is
identical to that of Jiang et al (Jiang et al, 2008a;
Jiang et al, 2008b). Table 7 compares the F1 re-
sults with previous studies on CTB 5.0. The result
of our error-driven model is superior to previous
reported results for both Seg and Seg & Tag, and
the result of our baseline model compares favor-
ably to the others.
Following Zhang and Clark (2008), we first
generated CTB 3.0 from CTB 4.0 using sentence
IDs 1?10364. We then divided CTB 3.0 into
ten equal sets and conducted 10-fold cross vali-
519
dation. Unfortunately, Zhang and Clark?s exper-
imental setting did not allow us to use our error-
driven policy since performing 10-fold cross val-
idation again on each main cross validation trial
is computationally too expensive. Therefore, we
used our baseline policy in this setting and fixed
r = 3 for all cross validation runs. Table 8 com-
pares the F1 results of our baseline model with
Nakagawa and Uchimoto (2007) and Zhang and
Clark (2008) on CTB 3.0. Table 9 shows a sum-
mary of averaged F1 results on CTB 3.0. Our
baseline model outperforms all prior approaches
for both Seg and Seg & Tag, and we hope that
our error-driven model can further improve perfor-
mance.
6 Related work
In this section, we discuss related approaches
based on several aspects of learning algorithms
and search space representation methods. Max-
imum entropy models are widely used for word
segmentation and POS tagging tasks (Uchimoto
et al, 2001; Ng and Low, 2004; Nakagawa,
2004; Nakagawa and Uchimoto, 2007) since they
only need moderate training times while they pro-
vide reasonable performance. Conditional random
fields (CRFs) (Lafferty et al, 2001) further im-
prove the performance (Kudo et al, 2004; Shi
and Wang, 2007) by performing whole-sequence
normalization to avoid label-bias and length-bias
problems. However, CRF-based algorithms typ-
ically require longer training times, and we ob-
served an infeasible convergence time for our hy-
brid model.
Online learning has recently gained popularity
for many NLP tasks since it performs comparably
or better than batch learning using shorter train-
ing times (McDonald, 2006). For example, a per-
ceptron algorithm is used for joint Chinese word
segmentation and POS tagging (Zhang and Clark,
2008; Jiang et al, 2008a; Jiang et al, 2008b).
Another potential algorithm is MIRA, which in-
tegrates the notion of the large-margin classifier
(Crammer, 2004). In this paper, we first intro-
duce MIRA to joint word segmentation and POS
tagging and show very encouraging results. With
regard to error-driven learning, Brill (1995) pro-
posed a transformation-based approach that ac-
quires a set of error-correcting rules by comparing
the outputs of an initial tagger with the correct an-
notations on a training corpus. Our approach does
not learn the error-correcting rules. We only aim to
capture the characteristics of unknown words and
augment their representatives.
As for search space representation, Ng and
Low (2004) found that for Chinese, the character-
based model yields better results than the word-
based model. Nakagawa and Uchimoto (2007)
provided empirical evidence that the character-
based model is not always better than the word-
based model. They proposed a hybrid approach
that exploits both the word-based and character-
based models. Our approach overcomes the limi-
tation of the original hybrid model by a discrimi-
native online learning algorithm for training.
7 Conclusion
In this paper, we presented a discriminative word-
character hybrid model for joint Chinese word
segmentation and POS tagging. Our approach
has two important advantages. The first is ro-
bust search space representation based on a hy-
brid model in which word-level and character-
level nodes are used to identify known and un-
known words, respectively. We introduced a sim-
ple scheme based on the error-driven concept to
effectively learn the characteristics of known and
unknown words from the training corpus. The sec-
ond is a discriminative online learning algorithm
based on MIRA that enables us to incorporate ar-
bitrary features to our hybrid model. Based on ex-
tensive comparisons, we showed that our approach
is superior to the existing approaches reported in
the literature. In future work, we plan to apply
our framework to other Asian languages, includ-
ing Thai and Japanese.
Acknowledgments
We would like to thank Tetsuji Nakagawa for his
helpful suggestions about the word-character hy-
brid model, Chen Wenliang for his technical assis-
tance with the Chinese processing, and the anony-
mous reviewers for their insightful comments.
References
Masayuki Asahara. 2003. Corpus-based Japanese
morphological analysis. Nara Institute of Science
and Technology, Doctor?s Thesis.
Harald Baayen and Richard Sproat. 1996. Estimat-
ing lexical priors for low-frequency morphologi-
cally ambiguous forms. Computational Linguistics,
22(2):155?166.
520
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543?565.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1?8.
Koby Crammer, Ryan McDonald, and Fernando
Pereira. 2005. Scalable large-margin online learn-
ing for structured classification. In NIPS Workshop
on Learning With Structured Outputs.
Koby Crammer. 2004. Online Learning of Com-
plex Categorial Problems. Hebrew Univeristy of
Jerusalem, PhD Thesis.
Zhendong Dong and Qiang Dong. 2006. Hownet and
the Computation of Meaning. World Scientific.
Kuzman Ganchev, Koby Crammer, Fernando Pereira,
Gideon Mann, Kedar Bellare, Andrew McCallum,
Steven Carroll, Yang Jin, and Peter White. 2007.
Penn/umass/chop biocreative ii systems. In Pro-
ceedings of the Second BioCreative Challenge Eval-
uation Workshop.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.
2008a. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word
lattice reranking for chinese word segmentation and
part-of-speech tagging. In Proceedings of COLING.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
japanese morphological analysis. In Proceedings of
EMNLP, pages 230?237.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML, pages 282?
289.
Ryan McDonald, Femando Pereira, Kiril Ribarow, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT/EMNLP, pages 523?530.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
University of Pennsylvania, PhD Thesis.
Masaki Nagata. 1994. A stochastic japanese mor-
phological analyzer using a forward-DP backward-
A* n-best search algorithm. In Proceedings of
the 15th International Conference on Computational
Linguistics, pages 201?207.
Masaki Nagata. 1999. A part of speech estimation
method for japanese unknown words using a statis-
tical model of morphology and context. In Proceed-
ings of ACL, pages 277?284.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A
hybrid approach to word segmentation and pos tag-
ging. In Proceedings of ACL Demo and Poster Ses-
sions.
Tetsuji Nakagawa. 2004. Chinese and japanese word
segmentation using word-level and character-level
information. In Proceedings of COLING, pages
466?472.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
EMNLP, pages 277?284.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP, pages 133?142.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
crfs based joint decoding method for cascaded seg-
mentation and labeling tasks. In Proceedings of IJ-
CAI.
Richard Sproat and Thomas Emerson. 2003. The first
international chinese word segmentation bakeoff. In
Proceedings of the 2nd SIGHAN Workshop on Chi-
nese Language Processing, pages 133?143.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-
hara. 2001. The unknown word problem: a morpho-
logical analysis of japanese using maximum entropy
aided by a dictionary. In Proceedings of EMNLP,
pages 91?99.
Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen
Okurowski, John Kovarik, Fu dong Chiou, and
Shizhe Huang. 2000. Developing guidelines and
ensuring consistency for chinese text annotation. In
Proceedings of LREC.
Stavros A. Zenios Yair Censor. 1997. Parallel Op-
timization: Theory, Algorithms, and Applications.
Oxford University Press.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and pos tagging on a single perceptron. In
Proceedings of ACL.
521
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 61?66,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multilingual Dependency Learning:
Exploiting Rich Features for Tagging Syntactic and Semantic Dependencies
Hai Zhao(??)?, Wenliang Chen(???)?,
Jun?ichi Kazama?, Kiyotaka Uchimoto?, and Kentaro Torisawa?
?Department of Chinese, Translation and Linguistics
City University of Hong Kong
83 Tat Chee Avenue, Kowloon, Hong Kong, China
?Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
haizhao@cityu.edu.hk, chenwl@nict.go.jp
Abstract
This paper describes our system about mul-
tilingual syntactic and semantic dependency
parsing for our participation in the joint task
of CoNLL-2009 shared tasks. Our system
uses rich features and incorporates various in-
tegration technologies. The system is evalu-
ated on in-domain and out-of-domain evalu-
ation data of closed challenge of joint task.
For in-domain evaluation, our system ranks
the second for the average macro labeled F1 of
all seven languages, 82.52% (only about 0.1%
worse than the best system), and the first for
English with macro labeled F1 87.69%. And
for out-of-domain evaluation, our system also
achieves the second for average score of all
three languages.
1 Introduction
This paper describes the system of National In-
stitute of Information and Communications Tech-
nology (NICT) and City University of Hong Kong
(CityU) for the joint learning task of CoNLL-2009
shared task (Hajic? et al, 2009)1. The system is ba-
sically a pipeline of syntactic parser and semantic
parser. We use a syntactic parser that uses very rich
features and integrates graph- and transition-based
methods. As for the semantic parser, a group of well
selected feature templates are used with n-best syn-
tactic features.
1Our thanks give to the following corpus providers, (Taule?
et al, 2008; Palmer and Xue, 2009; Hajic? et al, 2006; Surdeanu
et al, 2008; Burchardt et al, 2006) and (Kawahara et al, 2002).
The rest of the paper is organized as follows. The
next section presents the technical details of our syn-
tactic dependency parsing. Section 3 describes the
details of the semantic dependency parsing. Section
4 shows the evaluation results. Section 5 looks into a
few issues concerning our forthcoming work for this
shared task, and Section 6 concludes the paper.
2 Syntactic Dependency Parsing
Basically, we build our syntactic dependency parsers
based on the MSTParser, a freely available imple-
mentation2, whose details are presented in the paper
of McDonald and Pereira (2006). Moreover, we ex-
ploit rich features for the parsers. We represent fea-
tures by following the work of Chen et al (2008) and
Koo et al (2008) and use features based on depen-
dency relations predicted by transition-based parsers
(Nivre and McDonald, 2008). Chen et al (2008) and
Koo et al (2008) proposed the methods to obtain
new features from large-scale unlabeled data. In our
system, we perform their methods on training data
because the closed challenge does not allow to use
unlabeled data. In this paper, we call these new ad-
ditional features rich features.
2.1 Basic Features
Firstly, we use all the features presented by McDon-
ald et al (2006), if they are available in data. Then
we add new features for the languages having FEAT
information (Hajic? et al, 2009). FEAT is a set of
morphological-features, e.g. more detailed part of
speech, number, gender, etc. We try to align differ-
ent types of morphological-features. For example,
2http://mstparser.sourceforge.net
61
we can obtain a sequence of gender tags of all words
from a head h to its dependent d. Then we represent
the features based on the obtained sequences.
Based on the results of development data, we per-
form non-projective parsing for Czech and German
and perform projective parsing for Catalan, Chinese,
English, Japanese, and Spanish.
2.2 Features Based on Dependency Pairs
I    see    a    beautiful    bird    .
Figure 1: Example dependency graph.
Chen et al (2008) presented a method of extract-
ing short dependency pairs from large-scale auto-
parsed data. Here, we extract all dependency pairs
rather than short dependency pairs from training
data because we believe that training data are reli-
able. In a parsed sentence, if two words have de-
pendency relation, we add this word pair into a list
named L and count its frequency. We consider the
direction. For example, in figure 1, a and bird have
dependency relation in the sentence ?I see a beauti-
ful bird.?. Then we add word pair ?a-bird:HEAD?3
into list L and accumulate its frequency.
We remove the pairs which occur only once in
training data. According to frequency, we then
group word pairs into different buckets, with bucket
LOW for frequencies 2-7, bucket MID for frequen-
cies 8-14, and bucket HIGH for frequencies 15+.
We set these threshold values by following the set-
ting of Chen et al (2008). For example, the fre-
quency of pair ?a-bird:HEAD? is 5. Then it is
grouped into bucket ?LOW?. We also add a vir-
tual bucket ?ZERO? to represent the pairs that are
not included in the list. So we have four buckets.
?ZERO?, ?LOW?, ?MID?, and ?HIGH? are used as
bucket IDs.
Based on the buckets, we represent new features
for a head h and its dependent d. We check word
pairs surrounding h and d. Table 1 shows the word
pairs, where h-word refers to the head word, d-word
refers to the dependent word, h-word-1 refers to
3HEAD means that bird is the head of the pair.
the word to the left of the head in the sentence, h-
word+1 refers to the word to the right of the head,
d-word-1 refers to the word to the left of the depen-
dent, and d-word+1 refers the word to the right of
the dependent. Then we obtain the bucket IDs of
these word pairs from L.
We generate new features consisting of indicator
functions for bucket IDs of word pairs. We call these
features word-pair-based features. We also generate
combined features involving bucket IDs and part-of-
speech tags of heads.
h-word, d-word
h-word-1, d-word
h-word+1, d-word
h-word, d-word-1
h-word, d-word+1
Table 1: Word pairs for feature representation
2.3 Features Based on Word Clusters
Koo et al (2008) presented new features based on
word clusters obtained from large-scale unlabeled
data and achieved large improvement for English
and Czech. Here, word clusters are generated only
from the training data for all the languages. We per-
form word clustering by using the clustering tool4,
which also was used by Koo et al (2008). The
cluster-based features are the same as the ones used
by Koo et al (2008).
2.4 Features Based on Predicted Relations
Nivre and McDonald (2008) presented an integrat-
ing method to provide additional information for
graph-based and transition-based parsers. Here, we
represent features based on dependency relations
predicted by transition-based parsers for graph-
based parser. Based on the results on development
data, we choose the MaltParser for Catalan, Czech,
German, and Spanish, and choose another MaxEnt-
based parser for Chinese, English, and Japanese.
2.4.1 A Transition-based Parser: MaltParser
For Catalan, Czech, German, and Spanish, we
use the MaltParser, a freely available implementa-
4http://www.cs.berkeley.edu/?pliang/software/brown-
cluster-1.2.zip
62
tion5, whose details are presented in the paper of
Nivre (2003). More information about the parser can
be available in the paper (Nivre, 2003).
Due to computational cost, we do not select new
feature templates for the MaltParser. Following the
features settings of Hall et al (2007), we use their
Czech feature file and Catalan feature file. To sim-
ply, we apply Czech feature file for German too, and
apply Catalan feature file for Spanish.
2.4.2 Another Transition-based Parser:
MaxEnt-based Parser
In three highly projective language, Chinese,
English and Japanese, we use the maximum en-
tropy syntactic dependency parser as in Zhao and
Kit (2008). We still use the similar feature notations
of that work. We use the same greedy feature selec-
tion of Zhao et al (2009) to determine an optimal
feature template set for each language. Full feature
sets for the three languages can be found at website,
http://bcmi.sjtu.edu.cn/?zhaohai.
2.4.3 Feature Representation
For training data, we use 2-way jackknifing to
generate predicted dependency parsing trees by two
transition-based parsers. Following the features of
Nivre and McDonald (2008), we define features for
a head h and its dependent d with label l as shown in
table 2, where GTran refers to dependency parsing
trees generated by the MaltParser or MaxEnt-base
Parser and ? refers to any label. All features are
conjoined with the part-of-speech tags of the words
involved in the dependency.
Is (h, d, ?) in GTran?
Is (h, d, l) in GTran?
Is (h, d, ?) not in GTran?
Is (h, d, l) not in GTran?
Table 2: Features set based on predicted labels
3 n-best Syntactic Features for Semantic
Dependency Parsing
Due to the limited computational resource that we
have, we used the the similar learning framework as
our participant in semantic-only task (Zhao et al,
5http://w3.msi.vxu.se/?nivre/research/MaltParser.html
Normal n-best Matched
Ca 53 54 50
Ch 75 65 55
En 73 70 63
Table 3: Feature template sets:n-best vs. non-n-best
2009). Namely, three languages, a single maximum
entropy model is used for all identification and clas-
sification tasks of predicate senses or argument la-
bels in four languages, Catalan, Czech, Japanese, or
Spanish. For the rest three languages, an individual
sense classifier still using maximum entropy is ad-
ditionally used to output the predicate sense previ-
ously. More details about argument candidate prun-
ing strategies and feature template set selection are
described in Zhao et al (2009).
The same feature template sets as the semantic-
only task are used for three languages, Czech, Ger-
man and Japanese. For the rest four languages, we
further use n-best syntactic features to strengthen
semantic dependency parsing upon those automati-
cally discovered feature template sets. However, we
cannot obtain an obvious performance improvement
in Spanish by using n-best syntactic features. There-
fore, only Catalan, Chinese and English semantic
parsing adopted these types of features at last.
Our work about n-best syntactic features still
starts from the feature template set that is originally
selected for the semantic-only task. The original fea-
ture template set is hereafter referred to ?the normal?
or ?non-n-best?. In practice, only 2nd-best syntactic
outputs are actually adopted by our system for the
joint task.
To generate helpful feature templates from the
2nd-best syntactic tree, we simply let al feature tem-
plates in the normal feature set that are based on
the 1st-best syntactic tree now turn to the 2nd-best
one. Using the same notations for feature template
representation as in Zhao et al (2009), we take an
example to show how the original n-best features
are produced. Assuming a.children.dprel.bag is
one of syntactic feature templates in the normal
set, this feature means that all syntactic children of
the argument candidate (a) are chosen, and their
dependant labels are collected, the duplicated la-
bels are removed and then sorted, finally all these
strings are concatenated as a feature. The cor-
63
Language Features
Catalan p:2.lm.dprel
a.lemma + a:2.h.form
a.lemma + a:2.pphead.form
(a:2:p:2|dpPath.dprel.seq) + p.FEAT1
Chinese a:2.h.pos
a:2.children.pos.seq + p:2.children.pos.seq
a:2:p:2|dpPath.dprel.bag
a:2:p:2|dpPathPred.form.seq
a:2:p:2|dpPath.pos.bag
(a:2:p:2|dpTreeRelation) + p.pos
(a:2:p:2|dpPath.dprel.seq) + a.pos
English a:2:p:2|dpPathPred.lemma.bag
a:2:p:2|dpPathPred.pos.bag
a:2:p:2|dpTreeRelation
a:2:p:2|dpPath.dprel.seq
a:2:p:2|dpPathPred.dprel.seq
a.lemma + a:2.dprel + a:2.h.lemma
(a:2:p:2|dpTreeRelation) + p.pos
Table 4: Features for n-best syntactic tree
responding 2nd-best syntactic feature will be a :
2.children.dprel.bag. As all operations to gener-
ate the feature for a.children.dprel.bag is within
the 1st-best syntactic tree, while those for a :
2.children.dprel.bag is within the 2nd-best one. As
all these 2nd-best syntactic features are generated,
we use the same greedy feature selection procedure
as in Zhao et al (2009) to determine the best fit fea-
ture template set according to the evaluation results
in the development set.
For Catalan, Chinese and English, three opti-
mal n-best feature sets are obtained, respectively.
Though dozens of n-best features are initially gen-
erated for selection, only few of them survive af-
ter the greedy selection. A feature number statis-
tics is in Table 3, and those additionally selected
n-best features for three languages are in Table
4. Full feature lists and their explanation for
all languages will be available at the website,
http://bcmi.sjtu.edu.cn/?zhaohai.
4 Evaluation Results
Two tracks (closed and open challenges) are pro-
vided for joint task of CoNLL2009 shared task.
We participated in the closed challenge and evalu-
ated our system on the in-domain and out-of-domain
evaluation data.
avg. Cz En Gr
Syntactic (LAS) 77.96 75.58 82.38 75.93
Semantic (Labeled F1) 75.01 82.66 74.58 67.78
Joint (Macro F1) 76.51 79.12 78.51 71.89
Table 6: The official results of our submission for out-of-
domain task(%)
Test Dev
Basic ALL Basic ALL
Catalan 82.91 85.88 83.15 85.98
Chinese 74.28 75.67 73.36 75.64
Czech 77.21 79.70 77.91 80.22
English 88.63 89.19 86.35 87.40
German 84.61 86.24 83.99 85.44
Japanese 92.31 92.32 92.01 92.85
Spanish 83.59 86.29 83.73 86.22
Average 83.32 85.04 82.92 84.82
(+1.72) (+1.90)
Table 7: The effect of rich features for syntactic depen-
dency parsing
4.1 Official Results
The official results for the joint task are in Table 5,
and the out-of-domain task in Table 6, where num-
bers in bold stand for the best performances for the
specific language. For out-of-domain (OOD) eval-
uation, we did not perform any domain adaptation.
For both in-domain and out-of-domain evaluation,
our system achieved the second best performance
for the average Macro F1 scores of all the languages.
And our system provided the first best performance
for the average Semantic Labeled F1 score and the
forth for the average Labeled Syntactic Accuracy
score for in-domain evaluation.
4.2 Further results
At first, we check the effect of rich features for syn-
tactic dependency parsing. Table 7 shows the com-
parative results of basic features and all features on
test and development data, where ?Basic? refers to
the system with basic features and ?ALL? refers to
the system with basic features plus rich features. We
found that the additional features provided improve-
ment of 1.72% for test data and 1.90% for develop-
ment data.
Then we investigate the effect of different train-
ing data size for semantic parsing. The learning
64
average Catalan Chinese Czech English German Japanese Spanish
Syntactic (LAS) 85.04 85.88 75.67 79.70 89.19 86.24 92.32 86.29
Semantic (Labeled F1) 79.96 80.10 76.77 82.04 86.15 76.19 78.17 80.29
Joint (Macro F1) 82.52 83.01 76.23 80.87 87.69 81.22 85.28 83.31
Table 5: The official results of our joint submission (%)
Data Czech Chinese English
normal n-best normal n-best
25% 80.71 75.12 75.24 82.02 82.06
50% 81.52 76.50 76.59 83.52 83.42
75% 81.90 76.92 77.01 84.21 84.30
100% 82.24 77.35 77.34 84.73 84.80
Table 8: The performance in development set (semantic
labeled F1) vs. training corpus size
curves are drawn for Czech, Chinese and English.
We use 25%, 50% and 75% training corpus, respec-
tively. The results in development sets are given in
Table 8. Note that in this table the differences be-
tween normal and n-best feature template sets are
also given for Chinese and English. The results
in the table show that n-best features help improve
Chinese semantic parsing as the training corpus is
smaller, while it works for English as the training
corpus is larger.
5 Discussion
This work shows our further endeavor in syntactic
and semantic dependency parsing, based on our pre-
vious work (Chen et al, 2008; Zhao and Kit, 2008).
Chen et al (Chen et al, 2008) and Koo et al (Koo
et al, 2008) used large-scale unlabeled data to im-
prove syntactic dependency parsing performance.
Here, we just performed their method on training
data. From the results, we found that the new fea-
tures provided better performance. In future work,
we can try these methods on large-scale unlabeled
data for other languages besides Chinese and En-
glish.
In Zhao and Kit (2008), we addressed that seman-
tic parsing should benefit from cross-validated train-
ing corpus and n-best syntactic output. These two
issues have been implemented during this shared
task. Though existing work show that re-ranking for
semantic-only or syntactic-semantic joint tasks may
bring higher performance, the limited computational
resources does not permit us to do this for multiple
languages.
To analyze the advantage and the weakness of our
system, the ranks for every languages of our sys-
tem?s outputs are given in Table 9, and the perfor-
mance differences between our system and the best
one in Table 106. The comparisons in these two ta-
bles indicate that our system is slightly weaker in the
syntactic parsing part, this may be due to the reason
that syntactic parsing in our system does not ben-
efit from semantic parsing as the other joint learn-
ing systems. However, considering that the seman-
tic parsing in our system simply follows the output
of the syntactic parsing and the semantic part of our
system still ranks the first for the average score, the
semantic part of our system does output robust and
stable results. It is worth noting that semantic la-
beled F1 in Czech given by our system is 4.47%
worse than the best one. This forby gap in this lan-
guage further indicates the advantage of our system
in the other six languages and some latent bugs or
learning framework misuse in Czech semantic pars-
ing.
6 Conclusion
We describe the system that uses rich features and
incorporates integrating technology for joint learn-
ing task of syntactic and semantic dependency pars-
ing in multiple languages. The evaluation results
show that our system is good at both syntactic and
semantic parsing, which suggests that a feature-
oriented method is effective in multiple language
processing.
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
6The difference for Chinese in the latter table is actually
computed between ours and the second best system.
65
average Catalan Chinese Czech English German Japanese Spanish
Syntactic (LAS) 4 4 4 4 2 3 3 4
Semantic (Labeled F1) 1 1 3 4 1 2 2 1
Joint (Macro F1) 2 1 3 4 1 3 2 1
Table 9: Our system?s rank within the joint task according to three main measures
average Catalan Chinese Czech English German Japanese Spanish
Syntactic (LAS) 0.73 1.98 0.84 0.68 0.69 1.24 0.25 1.35
Semantic (Labeled F1) - - 0.38 4.47 - 2.42 0.09 -
Joint (Macro F1) 0.12 - 0.15 2.40 - 1.22 0.37 -
Table 10: The performance differences between our system and the best one within the joint task according to three
main measures
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of LREC-2006,
Genoa, Italy.
Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchimoto,
Yujie Zhang, and Hitoshi Isahara. 2008. Dependency
parsing with short dependency relations in unlabeled
data. In Proceedings of IJCNLP-2008, Hyderabad, In-
dia, January 8-10.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of CoNLL-
2009, Boulder, Colorado, USA.
Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen Eryig?it,
Bea?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? a study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
933?939, Prague, Czech, June.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of LREC-2002, pages 2008?
2013, Las Palmas, Canary Islands.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-08: HLT, pages 595?603, Columbus,
Ohio, USA, June.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL-2006, pages 81?88,
Trento, Italy, April.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of CoNLL-
X, New York City, June.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT, pages 950?958,
Columbus, Ohio, June.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies (IWPT
03), pages 149?160, Nancy, France, April 23-25.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the CoNLL-
2008.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the LREC-
2008, Marrakesh, Morroco.
Hai Zhao and Chunyu Kit. 2008. Parsing syntactic and
semantic dependencies with two single-stage maxi-
mum entropy models. In Proceedings of CoNLL-2008,
pages 203?207, Manchester, UK, August 16-17.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009. Multilingual dependency learning: A
huge feature engineering method to semantic depen-
dency parsing. In Proceedings of CoNLL-2009, Boul-
der, Colorado, USA.
66
Automatic Discovery of Attribute Words from
Web Documents
Kosuke Tokunaga, Jun?ichi Kazama, and Kentaro Torisawa
Japan Advanced Institute of Science and Technology (JAIST),
Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan
{kosuke-t, kazama, torisawa}@jaist.ac.jp
Abstract. We propose a method of acquiring attribute words for a wide
range of objects from Japanese Web documents. The method is a simple
unsupervised method that utilizes the statistics of words, lexico-syntactic
patterns, and HTML tags. To evaluate the attribute words, we also es-
tablish criteria and a procedure based on question-answerability about
the candidate word.
1 Introduction
Knowledge about how we recognize objects is of great practical importance for
many NLP tasks. Knowledge about attributes, which tells us from what view-
points objects are usually understood or described, is one of such type of knowl-
edge. For example, the attributes of car objects will be weight, engine, steering
wheel, driving feel, and manufacturer. In other words, attributes are items whose
values we want to know when we want to know about the object. More analyti-
cally, we tend to regard A as an attribute for objects of class C when A works
as if function v = A(o), o ? C where v is necessary to us to identify o (especially
to distinguish o from o?(= o) ? C). Therefore, obvious applications of attributes
are ones such as summarization [1,2] and question-answering [3]. Moreover, they
can be useful as features in word clustering [4] or machine learning. Although
the knowledge base for attributes can be prepared manually (e.g., WordNet [5]),
problems are cost and coverage. To overcome these, we propose a method that
automatically acquires attribute knowledge from the Web.
To acquire the attributes for a given class, C (e.g., car), the proposed method
first downloads documents that contain class label C (e.g., ?car?) from the Web.1
We extract the candidates of attribute words from these documents and score
them according to the statistics of words, lexico-syntactic patterns, and HTML
tags. Highly scored words are output as attributes for the class. Lexico-syntactic
patterns and other statistics have been used in other lexical knowledge acquisi-
tion systems [3,4,6,7,8]. We specifically used lexico-syntactic patterns involving
the Japanese postposition ?no? as used in [8] such as ?C no A? where A is an
attribute word, which is almost equivalent to pattern ?A of C? used in [7] to
1 We use C to denote both the class and its class label (the word representing the
class). We also use A to denote both the attribute and the word representing it.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 106?118, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Automatic Discovery of Attribute Words from Web Documents 107
find part-whole relations. Novel features of our method are its use of Web search
engines to focus on documents highly relevant to the class and its use of statistics
concerning attribute words and surrounding HTML tags.
One of the difficulties in studying attribute knowledge is that there are no
standard definitions of attributes, or criteria for evaluating obtained attributes.
In this paper, we propose a simple but effective definition of attributes that
matches our motivation and applications, i.e., whether we can ask a question
about the attribute and whether there is an answer to that question (question
answerability). For example, one can ask as ?Who is the manufacturer of this
car??, and someone might answer ?Honda?, because we want to know the manu-
facturer when we concerned about cars. We designed a procedure for evaluating
attributes based on this idea. As the literature points out [9,10], attributes can
include many types of relations such as property (e.g., weight), part-of (e.g.,
engine), telic (e.g., driving feel), and agentive (e.g., manufacturer). However, we
ignored type distinctions in this study. First, because attributes are useful even
if the type is not known, and second, because defining attributes as one of these
types and evaluating them only complicates the evaluation process, making the
results unstable. The use of linguistic tests to define attributes is not that new.
Woods [11] devised a test on whether we can say ?The A of o is v.? Although we
followed this procedure, we focused more on attributes that are important for
our understanding of an object by using question-answerability as our criterion.
2 Acquisition Method
2.1 Basic Observations on Attributes
Our method is based on the following three observations.
1. Attributes tend to occur in documents that contain the class label and not
in other documents.
2. Attributes tend to be emphasized by the use of certain HTML tags or occur
as items in HTML itemizations or tables in Web documents.
3. Attributes tend to co-occur with the class label in specific lexico-syntactic
patterns involving the postposition ?no.?
2.2 Extraction of Candidate Words
To acquire the attributes of class C, we first download documents that contain
class label C using a Web search engine, according to the first observation. We
refer to this set of documents as a local document set (LD(C)). All the nouns
appearing in the local document set are regarded as candidates of attribute
words. Here, the nouns are words tagged as ?proper nouns?, ?sahen nouns?
(nouns that can become a verb with the suffix ?suru?), ?location?, or ?unknown?
(e.g., words written in katakana) by a Japanese morphological analyzer, JUMAN
[12]. Note that we restricted ourselves to single word attributes in this study.
The obtained candidate words are scored in the next step.
108 K. Tokunaga, J. Kazama, and K. Torisawa
Table 1. Lexico-syntactic patterns for attribute acquisition. (We added possible En-
glish translations for the patterns in parenthesis).
C no A ha (A of C [verb]) C no A de (by A of C) C no A e (to A of C)
C no A ga (A of C [verb]) C no A made (even/until A of C) C no AA(A of C,)
C no A wo ([verb] A of C) C no A kara (from A of C)
C no A ni (at/in A of C) C no A yori (from/than A of C)
2.3 Ranking of Candidate Words
We rank the candidate words according to a score that reflects the observations
described in Sect. 2.1. The overall score takes the following form.
V (C, A) = n(C, A) ? f(C, A) ? t(C, A) ? dfidf(C, A), (1)
where A is the candidate word to be scored and C is the class. n(C, A) and
f(C, A) are scores concerning lexico-syntactic patterns. t(C, A) is a score con-
cerning the statistics of HTML tags to reflect the second observation. Finally,
dfidf(C, A) is the score related to word statistics. This reflects the first obser-
vation. By multiplying these sub-scores, we expect that they will complement
each other. We will explain the details on these sub-scores in the following.
As previously mentioned, we use lexico-syntactic patterns including the Japa-
nese postposition ?no? as clues. The patterns take the form ?C no A POST ?
where POST is a Japanese postposition or a punctuation mark.2 The actual
patterns used are listed in Table 1. Score n(C, A) is the number of times C and
A co-occur in these patterns in the local document set LD(C).
Score f(C, A) requires more explanation. Roughly, f(C, A) is the number of
times C and A co-occur in the patterns without the last postposition (i.e., pat-
tern ?C no A?) collected from 33 years of parsed newspaper articles.3 Note that
pattern matching was done against the parsed dependency structures.4 The rea-
son this score was used in addition to n(C, A) was to obtain more reliable scores
by increasing the number of documents to be matched. This may sound contra-
dictory to the fact that the Web is the largest corpus in the world. However,
we found that we could not obtain all the documents that contained the class
label because existing commercial Web search engines return URLs for a very
small fraction of matched documents (usually up to about 1,000 documents).
Although we could use hit counts for the patterns, we did not do this to avoid
overloading the search engine (each class has about 20,000 candidate words).
Score t(C, A) is the number of times A appears in LD(C) surrounded by
HTML tags. More precisely, we count the number of times A appears in the
form: ?<tag1>A<tag2>? where the number of characters between HTML tags
2 Note that there are actually no spaces between words in Japanese. The spaces are
for easier understanding.
3 Yomiuri newspaper 1987?2001, Mainichi newspaper 1991?1999, and Nikkei newspa-
per 1983?1990; 3.01 GB in total. We used a Japanese dependency parser [13].
4 The differences from n(C, A) were introduced to reuse the existing parsed corpus.
Automatic Discovery of Attribute Words from Web Documents 109
<B>???????</B><BR>??<BR>?? 400g??? 2????????? 2????
?????? 1.5<BR>???? 1.5??????? 1?????????????<P>??
??<BR>??????????<P>???<BR><OL><LI>???????????????
Fig. 1. Example HTML document
(i.e., the length of A) is 20 at maximum. The tags (<tag1> and <tag2>) can be
either a start tag (e.g., <A>) or an end tag (e.g., </A>). This score is intended
to give high values for words that are emphasized or occur in itemizations or
tables. For example, in the HTML document in Fig. 1, the words ??????
?? (Thai-curry)?, ??? (ingredient)?, ????? (spice)?, ?????????
?? (coriander, cumin)?, and ???? (recipe)? are counted.
Finally, dfidf(C, A), which reflects the first observation, is calculated as:
dfidf(C, A) = df(A, LD(C)) ? idf(A), idf(A) = log |G|df(A,G) .
df(A, X) denotes the number of documents where A appears in documents X .
G is a large set of randomly collected Web documents, which we call the global
document set. We derived this score from a similar score, which was used in [14]
to measure the association between a hypernym and hyponyms.
3 Evaluation Criteria
This section presents the evaluation criteria based on question-answerability (QA
tests). Based on the criteria, we designed an evaluation procedure where the
evaluators were asked to answer either by yes or no to four tests at maximum,
i.e., a hyponymy test (Sect. 3.4), a QA test (Sect. 3.1) and a suffix augmented
QA test (Sect. 3.2) followed by a generality test (Sect. 3.3).
3.1 Question-Answerability Test
By definitions we used, attributes are what we want to know about the object.
Therefore, if A is an attribute of objects of class C, we can arrange questions
(consisting of A and C) that require the values for A as the answer. Then someone
should be able to answer the questions. For example, we can ask ?Who is the
director of this movie?? because director is an attribute of movie. The answer
might be someone such as ?Stanley Kubrick.? We designed the QA test shown in
Fig. 2 to assess the correctness of attribute A for class C based on this criterion.
Several points should be noted. First, since the value for the attribute is actually
defined for the object instance (i.e., v = A(o), o ? C), we should qualify class
label C using ?kono (this)? to refer to an object instance of class C.
Second, since we cannot know what question is possible for A beforehand,
we generate all the question types listed in Fig. 2 and ask whether any of them
are acceptable.
110 K. Tokunaga, J. Kazama, and K. Torisawa
Are any of the following questions grammatically correct, natural, and answerable?
1. ?? C ? A??? (kono C no A ha nani?/What is the A of this C?)
2. ?? C ? A??? (kono C no A ha dare?/Who is the A of this C?)
3. ?? C ? A???? (kono C no A ha itu?/When is the A of this C?)
4. ?? C ? A???? (kono C no A ha doko?/Where is the A of this C?)
5. ?? C ? A???? (kono C no A ha dore?/Which is the A of this C?)
6. ?? C ? A????? (kono C no A ha ikutu?/How many is the A of this C?)
7. ?? C ? A???? (kono C no A ha dou?/How much is the A of this C?)
Fig. 2. Question-answerability Test
Third, the question should be natural as well as grammatically correct. Nat-
uralness was explained to the evaluators as positively determining whether the
question can be their first choice in usual conversations. In our point of view, at-
tributes should be important items for people in describing objects. We assumed
that attributes that conformed to the naturalness criterion would be such impor-
tant attributes. For example, stapler is not an attribute of company in our sense,
although almost all companies own staplers. Our naturalness criterion can re-
flect this observation since the question ?What is the stapler of this company??
is unnatural as a first question when talking about a company, and therefore
we can successfully conclude that stapler is not an attribute. Note that Woods?
linguistic test [11] (i.e., whether ?the attribute of an object is a value? can be
stated or not) cannot reject stapler since it does not have the naturalness re-
quirement (e.g., we can say ?the stapler of [used by] SONY is Stapler-X?).5 In
addition, note that such importances can be assessed more easily in the QA test,
since questioners basically ask what they think is important at least at the time
of utterance. However, we cannot expect such an implication even though the
declarative sentence is acceptable.
Finally, the answer to the question does not necessarily need to be written in
language. For example, values for attributes such as map, picture, and blueprint
cannot be written as language expressions but can be represented by other media.
Such attributes are not rare since we obtain attributes from the Web.
3.2 Suffix Augmented QA Test
Some attributes that are obtained can fail the QA test even if they are correct,
especially when the surface form is different from the one they actually mean.
This often occurs since Japanese is very elliptic and our method is restricted to
single word attributes. For example, the word seito (students) can be used to
represent the attribute seito suu (number of students) as in the sentence below.
kono gakko no seito ha 500 nin
this school of students is 500 NUM
(The number of students of this school is 500.)
5 Stapler might be an important attribute of companies for stationery sellers. However,
we focus on attributes that are important for most people in most situations.
Automatic Discovery of Attribute Words from Web Documents 111
? ? (number of)??? (method for) ? (name of)?? (-er)
? ??? ([amount of] time of) ?? (time of)???? (period of) ?? (location of)
? ?? (amount of money for) ?? (degree of)??? (state of)
? ??? (nominalized adjectives e.g., ?height of? ?prettiness of?)
Fig. 3. Allowed augmentation
These attributes whose parts are elided (e.g., seito representing seito suu) are
also useful since they are actually used in sentences as in the above example.
Therefore, they should be assessed as correct attributes in some way. Although
the most appropriate question for seito representing seito suu is (6) in Fig. 2,
it is unfortunately ungrammatical since ikutu cannot be used for the number of
persons. Therefore, seito representing seito suu will fail the QA test.6
In Japanese, most of the elided parts can be restored by adding appropriate
suffixes (as ?suu? (number of) in the previous example) or by adding ?no? +
nominalized adjectives. Thus, when the attribute word failed the first QA test,
we asked the evaluators to re-do the QA test by choosing an appropriate suffix
or a nominalized adjective from the list of allowed augmentations and adding it
to the end of the evaluated word. Figure 3 lists the allowed augmentations.7,8
3.3 Generality Test
Although our primal aim was to acquire the attributes for a given class, i.e., ,
to find attributes that are common to all the instances of the class, we found,
in preliminary experiments, that some uncommon (but interesting) attributes
were assessed as correct according to the QA test depending on the evaluator.
An example is subtitle for the class movie. Strictly speaking, subtitle is not
an attribute of all movies, since all movies do not necessarily have subtitles.
For example, only foreign films have subtitles in Japan. However, we think this
attribute is also useful in practice for people who have a keen interest in foreign
films. Thus, the evaluators were asked whether the attribute was common for
most instances of the class when the attribute was judged to be correct in the
QA test. We call attributes that passed this generality test general attributes,
and those that failed but passed the QA test relaxed attributes (note that general
attributes is a subset of relaxed attributes). We compare the accuracies for the
relaxed and general attributes in the experiments.
6 Seito (representing students) might pass the QA test with question type (2) in Fig.
2. However, this is not always the case since some evaluators will judge the question
to be unnatural.
7 Postposition ?no (of)? before the suffix is also allowed to be added if it makes the
question more natural.
8 The problem here might not occur if we used many more question types in the first
QA test. However, we did not do this to keep the first QA test simple. With the same
motivation, we kept the list of allowed suffixes short (only general and important
suffixes). The uncovered cases were treated by adding nominalized adjectives.
112 K. Tokunaga, J. Kazama, and K. Torisawa
3.4 Hyponymy Test
Finally, we should note that we designed the evaluation procedure so that the
evaluators could be asked whether candidate A is a hyponym of C before the QA
tests. If A is a hyponym of C, we can skip all subsequent tests since A cannot
be an attribute of C. We added this test because the output of the system often
contains hyponyms and these tend to cause confusion in the QA tests since
expression ?C no A? is natural even when A is a hyponym of C (e.g., ?anime no
Dragon Ball (Dragon Ball [of/the] anime)?).
4 Experiments
4.1 Experimental Setting
We first selected 32 word classes from 1,589 classes acquired from the Web with
an automatic hypernym-hyponym acquisition method [14]. Here, we regarded the
hypernym as the class label. Since our purpose was just to evaluate our method
for classes from the Web, we selected classes that were obtained successfully. We
randomly chose the 22 classes listed in Table 2 for human evaluation from these
32 classes.9 The hyponyms were used to help the evaluators to disambiguate the
meaning of class labels (if ambiguity existed).
To collect LD(C), we used the Web search engine goo (http://www.goo.ne.jp).
The size of LD(C) was 857 documents (URLs) on class average. There were
about 20, 000 candidate words on class average. As global document set G re-
quired for the calculation of dfidf(C, A), we used 1.0?106 randomly downloaded
Web documents.
Table 2. Classes used in evaluation
?? (city), ??? (museum), ?? (national holiday), ?? (police), ?? (facility), ?? (university),
?? (newspaper), ?? (garbage), ?? (shrine), ? (bird), ?? (hospital), ?? (plant), ? (river), ?
?? (elementary school), ? (music tune), ??? (library), ?? (branch office), ??? (web site), ?
(town), ???? (sensor), ?? (training), ??? (car)
We output the top 50 attributes for each class ranked with our proposed
method and with alternative methods that were used for comparison. We gath-
ered outputs for all the methods, removing duplication (i.e., taking the set union)
to achieve efficient evaluation, and re-sorted them randomly to ensure that the
assessment was unbiased. Four human evaluators assessed these gathered at-
tributes class-by-class in four days using a GUI tool implementing the evaluation
procedure described in Sect. 3. There were a total of 3, 678 evaluated attributes.
Using the evaluation results, we re-constructed the evaluations for the top 50 for
each method. The kappa value [15], which indicates inter-evaluator agreement,
was 0.533 for the general attribute case and 0.593 for the relaxed attribute case.
According to [15], these kappa values indicate ?moderate? agreement.
9 This selection was due to time/cost limitations.
Automatic Discovery of Attribute Words from Web Documents 113
4.2 Accuracy of Proposed Method
Figure 4 has accuracy graphs for the proposed method for relaxed attributes.
The graph on the left shows per-evaluator precision when the top n (repre-
sented by x axis) attributes were output. The precision is the average over all
classes. Although we cannot calculate the actual recall, the x axis corresponds
to approximate recall. We can see that ranking with the proposed method has
a positive correlation with human evaluation, although the assessments varied
greatly depending on the evaluator. The graph on the right shows curves for
average (with standard deviation), 3-consensus, and 4-consensus precision. 3-
consensus (4-consensus) is precision where the attribute is considered correct by
at least three (four) evaluators. Figure 5 has graphs for the general attribute
case the same as for the relaxed case. Although there is a positive correlation
between ranking with the proposed method and human evaluators, the precision
was, not surprisingly, lower than that for the relaxed case. In addition, the lower
kappa value (0.533 compared to 0.593 for the relaxed case) indicated that the
generality test was harder than the QA tests.
The accuracy of the proposed method was encouraging. Although we cannot
easily determine which indicator is appropriate, if we use the majority rule (3-
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 5  10  15  20  25  30  35  40  45  50
Pr
ec
is
io
n
Rank (recall)
Evaluator1
Evaluator2
Evaluator3
Evaluator4
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 5  10  15  20  25  30  35  40  45  50
Pr
ec
is
io
n
Rank (recall)
Average
3-consensus
4-consensus
Fig. 4. Accuracy of relaxed attributes
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 5  10  15  20  25  30  35  40  45  50
Pr
ec
is
io
n
Rank (recall)
Evaluator1
Evaluator2
Evaluator3
Evaluator4
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 5  10  15  20  25  30  35  40  45  50
Pr
ec
is
io
n
Rank (recall)
Average
3-consensus
4-consensus
Fig. 5. Accuracy of general attributes
114 K. Tokunaga, J. Kazama, and K. Torisawa
Table 3. Top 20 attributes of several classes obtained by proposed method
Classes Attributes
?????
(bird)
?? (picture)[4/4] ?? (name)[4/2] ?? (sort)[4/4] ???? (illustration)[3/3] ?? (characteristics)[4/4] ?
? (disease)[4/2] ?? (life)[4/4] ?? (topic)[3/2] ?? (relation)[0/0] ???? (image)[4/4] ? (nest)[4/4] ?
?? (song)[4/4] ? (shape)[4/4] ?? (info.)[4/4] ?? (world)[0/0] ? (song)[4/4] ?? (animal)[0/0] ???
(page)[3/2] ?? (ecology)[4/4] ? (wing)[4/4]
??????
(hospital)
?????? (home page)[4/1] ?? (facility)[3/3] ?? (info.)[4/4] ?? (intro.)[4/4] ?? (info. desk)[4/4] ??
(authorization)[3/3] ?? (name)[4/2] ?? (doctor)[4/4] ??? (psychiatry)[4/2] ?? (reputation)[4/4] ??
(handling)[4/4] ?? (phone)[2/2] ?? (medical care)[4/4] ?? (treatment)[4/4] ?? (medical service)[3/3] ?
? (function)[3/3] ?? (director)[4/4] ?? (valuation)[4/4] ?? (medical examination)[4/4] ??? (page)[2/2]
?? (admin.)[4/3] ?? (part)[1/1]
??????
(plant)
?? (name)[4/2] ?? (species)[4/4] ?? (picture)[4/4] ?? (seed)[4/4] ?? (cultivation)[4/3] ?? (observa-
tion)[4/3] ?? (characteristics)[4/4] ?? (explanation)[4/4] ?? (image)[4/4] ?? (surveillance)[4/3] ??
? (data)[4/4] ?? (evolution)[3/3] ?? (description)[4/4] ??? (list)[2/2] ? (leaf)[4/3] ?? (preserva-
tion)[2/2] ???? (design)[1/1] ?? (growth)[4/4]
?????
(river)
?? (water level)[4/4] ?? (upstream)[4/4] ?? (name)[4/2] ?? (environment)[4/4] ?? (water qual-
ity)[4/4] ?? (history)[4/4] ?? (head stream)[4/4] ?? (picture)[4/4] ? (water)[4/4] ?? (surface)[4/4] ?
? (location)[4/4] ?? (current)[4/4] ?? (waterside)[4/4] ?? (river head)[4/4] ?? (four seasons)[3/3] ?
? (characteristics)[4/4] ? (inside)[1/1] ??? (streamside)[4/4] ?? (nature)[4/4] ???? (babbling)[4/4]
?????
(elementary
school)
?? (activity)[4/4] ???? (efforts)[4/3] ??? (athletic meeting)[4/4] ??? (child)[4/4] ?????? (home
page)[4/0] ?? (head teacher)[4/4] ?? (classroom)[4/4] ?? (school song)[4/4] ?? (student)[4/4] ??
(school building)[4/4] ?? (event)[4/4] ?? (learning)[3/3] ?? (feeding service)[4/3] ??? (page)[2/2] ?
?? (gym)[4/4] ?? (class)[3/3] ??? (mail)[0/0] ?? (grade)[1/1] ??? (opening ceremony)[4/4] ??
(music)[2/2]
?????
(music tune)
?? (lyrics)[4/1] ???? (title)[4/2] ?? (performance)[4/4] ??? (list)[0/0] ???? (image)[4/4] ??
(lyrics writing)[4/1] ?? (musical score)[4/4] ?? (name)[4/2] ?? (content)[3/3] ???? (genre)[4/4] ??
(info.)[4/4] ???? (point)[4/4] ?? (world)[1/1] ????? (melody)[4/4] ?? (end)[3/2] ?? (title)[4/2]
? (inside)[0/0] ?? (composition)[4/4] ??? (theme)[4/4] ??? (data)[4/2]
?????
(library)
?? (source material)[4/4] ?????? (home page)[4/2] ??? (page)[3/1] ?? (history)[4/4] ?? (establish-
ment)[4/4] ???? (system)[4/4] ?? (book stock)[4/4] ??? (copy)[2/2] ? (book)[4/4] ?? (location)[4/4]
?? (use)[4/4] ???? (service)[4/4] ?????? (database)[4/3] ?? (book)[4/4] ?? (newspaper)[4/4] ?
? (close)[4/4] ?? (catalog)[3/3] ?? (display)[4/2] ?? (facility)[2/2] ?? (info.)[4/4]
?????
(town)
?? (population)[4/4]?? (history)[4/4]?????? (home page)[4/0]?? (sightseeing)[4/4]?? (info.)[3/3]
?? (finance)[4/4] ?? (facility)[4/4] ??? (heritage)[4/2] ?? (environment)[4/4] ?? (hot spring)[3/1]
?? (topic)[3/2] ?? (four seasons)[3/3] ???? (event)[4/3] ??? (library)[4/3] ?? (culture)[4/4] ??
(landscape)[4/4] ???? (symbol)[4/3] ?? (industry)[4/3] ?? (agriculture)[4/2] ?? (town council)[3/3]
??????
(sensor)
?? (info.)[4/4] ?? (sensitivity)[4/3] ?? (sort)[4/3] ?? (position)[4/4] ???? (install)[4/4] ?? (devel-
opment)[4/4] ?? (accuracy)[4/4] ??? (size)[4/4] ?? (specification)[4/4] ?? (temperature)[2/1] ???
(data)[4/4] ??? (set)[4/4] ?? (install)[4/4] ?? (function)[4/4] ?? (technology)[4/4] ?? (feature)[4/4]
??? (page)[3/3] ?? (height)[3/2] ?? (adoption)[3/3] ?? (application)[4/4]
??????
(training)
?? (content)[4/4] ?? (purpose)[4/4] ?? (practice)[4/4] ??? (theme)[4/3] ????? (program)[4/4] ?
? (lecturer)[4/4] ?? (plan)[4/4] ?? (name)[4/2] ???? (menu)[4/4] ?? (report)[4/4] ?? (target)[4/4]
?? (outcome)[4/4] ?? (satisfaction)[2/2] ? (place/atmosphere)[3/3] ??? (state of existence)[2/2] ??
(detail)[4/4] ?? (opportunity)[1/1] ?? (capacity)[4/4] ?? (participation)[4/4] ?? (other)[0/0]
consensus in our case) employed in [7], the proposed method obtained relaxed
attributes with 0.852 precision and general attributes with 0.727 precision for the
top 20 outputs. Table 3 lists the top 20 attributes obtained with the proposed
method for several classes. The numeral before (after) ?/? is the number of
evaluators who judged the attribute as correct as a relaxed (general) attribute.
We can see that many interesting attributes were obtained.
4.3 Effect of Scores
In this analysis, we assessed the effect that sub-scores in Eq. (1) had on the
acquisition accuracy by observing the decrease in precision when we removed
each score from Eq. (1). First, we could observe a positive effect for most scores
in terms of the precision averaged over evaluators. Moreover, interestingly, the
tendency of the effect was very similar for all evaluators, even though the as-
sessments varied greatly depending on the evaluator as the previous experiment
showed. Due to space limitations, we will only present the latter analysis here.
Automatic Discovery of Attribute Words from Web Documents 115
-0.2
-0.15
-0.1
-0.05
 0
 0.05
 0.1
 5  10  15  20  25  30  35  40  45  50
D
iff
er
en
ce
Rank (recall)
Proposed - pattern (web)
Proposed - pattern (news)
Proposed - tag
Proposed - dfidf
-0.3
-0.25
-0.2
-0.15
-0.1
-0.05
 0
 0.05
 0.1
 5  10  15  20  25  30  35  40  45  50
D
iff
er
en
ce
Rank (recall)
Proposed - pattern (web)
Proposed - pattern (news)
Proposed - tag
Proposed - dfidf
Fig. 6. Effect of scores. Left: relaxed attribute. Right: general attribute.
We calculated the change in precision ?per evaluator?, and then calculated the
averaged change, i.e., the change averaged over evaluators. Figure 6 plots the
averaged change and standard deviations. The effect of n(C, A) is represented
by ?Proposed - pattern (web)?, that of f(C, A) by ?Proposed - pattern (news)?,
that of t(C, A) by ?Proposed - tag?, and that of dfidf(C, A) by ?Proposed -
dfidf?. In the relaxed attribute case, we can see that most of the scores were ef-
fective at almost all ranks regardless of the evaluator (negative difference means
positive effect). The effect of f(C, A) and t(C, A) was especially remarkable. Al-
though n(C, A) has a similar curve to f(C, A), the effect is weaker. This may
be caused by the difference in the number of documents available (As we previ-
ously described, we currently cannot obtain a large number of documents from
the Web). The effect dfidf(C, A) had was two-fold. This contributed positively
at lower ranks but it contributed negatively at higher ranks (around the top
1-5). In the general attribute case, the positive effect became harder to observe
although the tendency was similar to the relaxed case. However, we can see that
f(C, A) still contributed greatly even in this case. The effect of t(C, A), on the
other hand, seems to have weakened greatly.
4.4 Effect of Hypernym
If we have a hypernym-hyponym knowledge base, we can also collect the local
document set by using the hyponyms in the class as the keywords for the search
engine instead of using the class label (hypernym). In this experiment, we com-
pared the proposed method with this alternative. We collected about the same
number of documents for the alternative method as for the proposed method to
focus on the quality of collected documents. We used hyponyms with the alter-
native method instead of class label C in patterns for n(C, A) (thus n(Hs, A) to
be precise). f(C, A) was unchanged. Figure 7 plots the results in the same way
as for the previous analysis (i.e., difference from the proposed method). We can
see that the class label is better than hyponyms for collecting local documents
at least in the current setting.
116 K. Tokunaga, J. Kazama, and K. Torisawa
-0.1
-0.08
-0.06
-0.04
-0.02
 0
 0.02
 0.04
 5  10  15  20  25  30  35  40  45  50
D
iff
er
en
ce
Rank (recall)
Hyponym
-0.1
-0.08
-0.06
-0.04
-0.02
 0
 0.02
 0.04
 5  10  15  20  25  30  35  40  45  50
D
iff
er
en
ce
Rank (recall)
Hyponym
Fig. 7. Effect of hypernyms. Left: relaxed case. Right: general case.
5 Discussion
5.1 Related Work
Several studies have attempted to acquire attributes or attribute-value pairs
[1,3,7,8,16]. Yoshida [1] proposed a method of integrating tables on the Web.
Although his method consequently acquired attributes, he did not evaluate the
accuracy of attributes. Yoshida et al [16] proposed a method of identifying
attribute-value pairs in Web documents. However, since this method only iden-
tified the attributes obtained with the method in [1], the coverage might be
bounded by the coverage of tables for attributes. Moreover, these methods did
not utilize the statistics for words or lexico-syntactic patterns as ours did. Taka-
hashi et al [8] extracted triples (object, attribute, value) from newspaper articles
using lexico-syntactic patterns and statistical scores. However, they focused only
on proper nouns and selected the attribute candidates manually. Freishmann et
al. [3] extracted attribute-value pairs with a high degree of precision by filtering
the candidates extracted with lexico-syntactic patterns by using a model learned
with supervised learning. Although this approach is promising, their method was
limited to person names and we must prepare training data to apply the method
to other types of objects.
5.2 Future Directions
Clues based on QA tests. The current ranking, Eq. (1), does not exploit the
observation behind the criteria in Sect. 3. Only the lexico-syntactic patterns ?C
no A? slightly reflect the criteria. Higher accuracy might be achieved by using
patterns that directly reflect the QA tests, e.g., statistics from FAQ lists. The
hyponym tests in Sect. 3.4 can also be reflected if we use a hyponymy database.
In addition, it is not surprising that the proposed method was not efficient at
acquiring general attributes since the score was not meant for that (although
the use of class labels might be a contributing factor, ambiguous class labels
Automatic Discovery of Attribute Words from Web Documents 117
cause problems at the same time). The hyponym database might be exploited
to measure the generality of attributes.
Full use of the Web. The current method cannot use all Web documents
due to limitations with search engines. The more Web documents we have, the
more useful the score n(C, A). We are currently planning to prepare our own
non-restricted Web repository. Using this, we would also like to elaborate on the
comparison described in Sect. 4.4 between the use of hypernyms (class labels)
and hyponyms (instance words) in collecting the local document set.
Assessment of Coverage. Currently, the actual recall with the proposed
method is unknown. It will be important to estimate how many attributes are
needed for practical applications, e.g., by manually analyzing the use of pattern
?C no A? exhaustively for a certain class, C. In addition, since we selected classes
that were successfully obtained with a hyponymy acquisition method, we cannot
deny the possibility that the proposed method has been evaluated for the classes
for which reliable statistics can easily be obtained. Thus, the evaluation of more
difficult (e.g., more infrequent) classes will be an important future work.
Type Acquisition. What types of questions and what types of suffix augmen-
tations are possible for a given attribute (i.e., the type of attribute value) might
also be useful, e.g., in value extraction and in determining type of the attribute
(in the sense of ?property or part-of?). This was left for the evaluators to chose
arbitrarily in this study. We would like to extract such knowledge from the Web
using similar techniques such as word statistics and lexico-syntactic patterns.
6 Conclusion
We presented a method of acquiring attributes that utilizes statistics on words,
lexico-syntactic patterns, and HTML tags. We also proposed criteria and an
evaluation procedure based on question-answerability. Using the procedure, we
conducted experiments with four human evaluators. The results revealed that
our method could obtain attributes with a high degree of precision.
References
1. Yoshida, M.: Extracting attributes and their values from web pages. In: Proc. of
the ACL 2002 Student Research Workshop. (2002) 72?77
2. Yoshida, M., Torisawa, K., Tsujii, J.: Integrating tables on the world wide web.
Transactions of the Japanese Society for Artificial Intelligence 19 (2004) 548?560
3. Fleischman, M., Hovy, E., Echihabi, A.: Offline strategies for online question
answering: Answering questions before they are asked. In: Proc. of ACL 2003.
(2003) 1?7
4. Almuhareb, A., Poesio, M.: Attribute-based and value-based clustering: An eval-
uation. In: Proc. of EMNLP 2004. (2004) 158?165
5. Fellbaum, C., ed.: WordNet: An electronic lexical database. The MIT Press (1998)
118 K. Tokunaga, J. Kazama, and K. Torisawa
6. Hearst, M.A.: Automatic acquisition of hyponyms from large text corpora. In:
Proc. of COLING ?92. (1992) 539?545
7. Berland, M., Charniak, E.: Finding parts in very large corpora. In: Proc. of ACL
?99. (1999)
8. Takahashi, T., Inui, K., Matsumoto, Y.: Automatic extraction of attribute relations
from text (in Japanese). IPSJ, SIG-NLP. NL-164 (2004) 19?24
9. Guarino, N.: Concepts, attributes and arbitrary relations: some linguistic and on-
tological criteria for structuring knowledge base. Data and Knowledge Engineering
(1992) 249?261
10. Pustejovsky, J.: The Generative Lexicon. The MIT Press (1995)
11. Woods, W.A.: What?s in a Link: Foundations for Semantic Networks. In: Repre-
sentation and Understanding: Studies in Cognitive Science. Academic Press (1975)
12. Kurohashi, S., Nagao, M.: Japanese morphological analysis system JUMAN version
3.61 manual (1999)
13. Kanayama, H., Torisawa, K., Mitsuishi, Y., Tsujii, J.: A hybrid Japanese parser
with hand-crafted grammar and statistics. In: Proc. of COLING 2000. (2000)
411?417
14. Shinzato, K., Torisawa, K.: Acquiring hyponymy relations from web documents.
In: Proc. of HLT-NAACL04. (2004) 73?80
15. Landis, J.R., Koch, G.G.: The measurement of observer agreement for categorial
data. Biometrics 33 (1977) 159?174
16. Yoshida, M., Torisawa, K., Tsujii, J.: Chapter 10 (Extracting Attributes and Their
Values from Web Pages). In: Web Document Analysis. World Scientific (2003)
Coling 2010: Poster Volume, pages 126?134,
Beijing, August 2010
Improving Graph-based Dependency Parsing with Decision History
Wenliang Chen?, Jun?ichi Kazama?, Yoshimasa Tsuruoka?? and Kentaro Torisawa?
?Language Infrastructure Group, MASTAR Project, NICT
{chenwl, kazama, torisawa}@nict.go.jp
?School of Information Science, JAIST
tsuruoka@jaist.ac.jp
Abstract
This paper proposes an approach to im-
prove graph-based dependency parsing by
using decision history. We introduce a
mechanism that considers short dependen-
cies computed in the earlier stages of pars-
ing to improve the accuracy of long de-
pendencies in the later stages. This re-
lies on the fact that short dependencies are
generally more accurate than long depen-
dencies in graph-based models and may
be used as features to help parse long de-
pendencies. The mechanism can easily
be implemented by modifying a graph-
based parsing model and introducing a set
of new features. The experimental results
show that our system achieves state-of-
the-art accuracy on the standard PTB test
set for English and the standard Penn Chi-
nese Treebank (CTB) test set for Chinese.
1 Introduction
Dependency parsing is an approach to syntactic
analysis inspired by dependency grammar. In re-
cent years, interest in this approach has surged due
to its usefulness in such applications as machine
translation (Nakazawa et al, 2006), information
extraction (Culotta and Sorensen, 2004).
Graph-based parsing models (McDonald and
Pereira, 2006; Carreras, 2007) have achieved
state-of-the-art accuracy for a wide range of lan-
guages as shown in recent CoNLL shared tasks
(Buchholz et al, 2006; Nivre et al, 2007). How-
ever, to make parsing tractable, these models are
forced to restrict features over a very limited his-
tory of parsing decisions (McDonald and Pereira,
2006; McDonald and Nivre, 2007). Previous
work showed that rich features over a wide range
of decision history can lead to significant im-
provements in accuracy for transition-based mod-
els (Yamada and Matsumoto, 2003a; Nivre et al,
2004).
In this paper, we propose an approach to im-
prove graph-based dependency parsing by using
decision history. Here, we make an assumption:
the dependency relations between words with a
short distance are more reliable than ones between
words with a long distance. This is supported by
the fact that the accuracy of short dependencies
is in general greater than that of long dependen-
cies as reported in McDonald and Nivre (2007)
for graph-based models. Our idea is to use deci-
sion history, which is made in previous scans in a
bottom-up procedure, to help parse other words in
later scans. In the bottom-up procedure, short de-
pendencies are parsed earlier than long dependen-
cies. Thus, we introduce a mechanism in which
we treat short dependencies built earlier as deci-
sion history to help parse long dependencies in
later stages. It can easily be implemented by mod-
ifying a graph-based parsing model and designing
a set of features for the decision history.
To demonstrate the effectiveness of the pro-
posed approach, we present experimental results
on English and Chinese data. The results indi-
cate that the approach greatly improves the accu-
racy and that richer history-based features indeed
make large contributions. The experimental re-
sults show that our system achieves state-of-the-
art accuracy on the data.
2 Motivation
In this section, we present an example to show
the idea of using decision history in a dependency
parsing procedure.
Suppose we have two sentences in Chinese, as
shown in Figures 1 and 2, where the correct de-
pendencies are represented by the directed links.
For example, in Figure 1 the directed link from
126
w3:?(bought) to w5:?(books) mean that w3 is
the head and w5 is the dependent. In Chinese,
the relationship between clauses is often not made
explicit and two clauses may simply be put to-
gether with only a comma (Li and Thompson,
1997). This makes it hard to parse Chinese sen-
tences with several clauses.
ROOT
?? ? ? ? ? ??? ? ? ? ? ?(last year) (I) (bought) (NULL) (books) (,) (this year) (he) (also) (bought) (NULL) (books) w1         w2   w3         w4      w5      w6     w7        w8    w9     w10      w11      w12(Last year I bought some books and this year he also bought some books.)
Figure 1: Example A
ROOT
?? ? ? ? ? ??? ? ? ? ?(last year) (I) (bought) (NULL) (books) (,) (this year) (also) (bought) (NULL) (books) w1         w2   w3        w4        w5      w6     w7     w8        w9       w10        w11(Last year I bought some books and this year too)      
Figure 2: Example B
If we employ a graph-based parsing model,
such as the model of (McDonald and Pereira,
2006; Carreras, 2007), it is difficult to assign the
relations between w3 and w10 in Example A and
between w3 and w9 in Example B. For simplicity,
we use wAi to refer to wi of Example A and wBi to
refer to wi of Example B in what follows.
The key point is whether the second clauses are
independent in the sentences. The two sentences
are similar except that the second clause of Exam-
ple A is an independent clause but that of Exam-
ple B is not. wA10 is the root of the second clause
of Example A with subject wA8 , while wB9 is the
root of the second clause of Example B, but the
clause does not have a subject. These mean that
the correct decisions are to assign wA10 as the head
of wA3 and wB3 as the head of wB9 , as shown by the
dash-dot-lines in Figures 1 and 2.
However, the model can use very limited infor-
mation. Figures 3-(a) and 4-(a) show the right
dependency relation cases and Figures 3-(b) and
4-(b) show the left direction cases. For the right
direction case of Example A, the model has the
information about wA3 ?s rightmost child wA5 and
wA10?s leftmost child wA6 inside wA3 and wA10, but it
does not have information about the other children
?? ? ? ? ? ? ?? ? ? ? ? ?(last year) (I) (bought) (NULL) (books) (,) (this year) (he) (also) (bought) (NULL) (books) w1         w2   w3        w4        w5      w6     w7     w8    w9     w10        w11      w12(a)
?? ? ? ? ? ? ?? ? ? ? ? ?(last year) (I) (bought) (NULL) (books) (,) (this year) (he) (also) (bought) (NULL) (books)
(b)w1         w2   w3        w4        w5      w6     w7     w8    w9     w10        w11      w12
Figure 3: Example A: two directions
?? ? ? ? ? ??? ? ? ? ?(last year) (I) (bought) (NULL) (books) (,) (this year) (also) (bought) (NULL) (books) w1         w2   w3         w4    w5      w6  w7        w8     w9         w10        w11      (a)
?? ? ? ? ? ??? ? ? ? ?(last year) (I) (bought) (NULL) (books) ( ) (this year) (also) (bought) (NULL) (books)
(b)
,w1         w2   w3         w4    w5      w6  w7        w8     w9         w10        w11      
Figure 4: Example B: two directions
(such as wA8 ) of wA3 and wA10, which may be useful
for judging the relation between wA3 and wA10. The
parsing model can not find the difference between
the syntactic structures of two sentences for pairs
(wA3 ,wA10) and (wB3 ,wB9 ). If we can provide the in-
formation about the other children of wA3 and wA10
to the model, it becomes easier to find the correct
direction between wA3 and wA10.
Next, we show how to use decision history to
help parse wA3 and wA10 of Example A.
In a bottom up procedure, the relations between
the words inside [wA3 , wA10] are built as follows
before the decision for wA3 and wA10. In the first
round, we build relations for neighboring words
(word distance1=1), such as the relations between
wA3 and wA4 and between wA4 and wA5 . In the sec-
ond round, we build relations for words of dis-
tance 2, and then for longer distance words until
all the possible relations between the inside words
are built. Figure 5 shows all the possible relations
inside [wA3 , wA10] that we can build. To simplify,
we use undirected links to refer to both directions
1Word distance between wi and wj is |j ? i|.
127
of dependency relations between words in the fig-
ure.
?? ? ? ? ? ? ?? ? ? ? ? ?(last year) (I) (bought) (NULL) (books)   (,) (this year) (he) (also) (bought) (NULL) (books) w1         w2   w3         w4 w5        w6  w7        w8   w9     w10     w11      w12
Figure 5: Example A: first step
Then given those inside relations, we choose
the inside structure with the highest score for each
direction of the dependency relation between wA3
and wA10. Figure 6 shows the chosen structures.
Note that the chosen structures for two directions
could either be identical or different. In Figure
6-(a) and -(b), they are different.
?? ? ? ? ? ? ?? ? ? ? ? ?(last year) (I) (bought) (NULL) (books) (,) (this year) (he) (also) (bought) (NULL) (books) w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 w11 w12(a)       
(b)
?? ? ? ? ? ? ?? ? ? ? ? ?(last year) (I) (bought) (NULL) (books) (,) (this year) (he) (also) (bought) (NULL) (books) w1         w2   w3        w4        w5      w6     w7     w8    w9     w10        w11      w12
Figure 6: Example A: second step
Finally, we use the chosen structures as deci-
sion history to help parse wA3 and wA10. For ex-
ample, the fact that wA8 is a dependent of wA10 is
a clue that suggests that the second clause may be
independent. This results in wA10 being the head of
wA3 .
This simple example shows how to use the de-
cision history to help parse the long distance de-
pendencies.
3 Background: graph-based parsing
models
Before we describe our method, we briefly intro-
duce the graph-based parsing models. We denote
input sentence w by w = (w0, w1, ..., wn), where
w0 = ROOT is an artificial root token inserted at
the beginning of the sentence and does not depend
on any other token in w and wi refers to a word.
We employ the second-order projective graph-
based parsing model of Carreras (2007), which is
an extension of the projective parsing algorithm of
Eisner (1996).
The parsing algorithms used in Carreras (2007)
independently find the left and right dependents of
a word and then combine them later in a bottom-
up style based on Eisner (1996). A subtree that
spans the words in [s, t] (and roots at s or t) is
represented by chart item [s, t, right/left, C/I],
where right (left) indicates that the root of the sub-
tree is s (t) and C means that the item is complete
while I means that the item is incomplete (Mc-
Donald, 2006). Here, complete item in the right
(left) direction means that the words other than s
(t) cannot have dependents outside [s, t] and in-
complete item in the right (left) direction, on the
other hand, means that t (s) may have dependents
outside [s, t]. In addition, t (s) is the direct depen-
dent of s (t) in the incomplete item with the right
(left) direction.
Larger chart items are created from pairs of
smaller chart items by the bottom-up procedure.
Figure 7 illustrates the cubic parsing actions of the
Eisner?s parsing algorithm (Eisner, 1996) in the
right direction, where s, r, and t refer to the start
and end indices of the chart items. In Figure 7-
(a), all the items on the left side are complete and
represented by triangles, where the triangle of [s,
r] is complete item [s, r,?, C] and the triangle of
[r + 1, t] is complete item [r + 1, t,?, C]. Then
the algorithm creates incomplete item [s, t,?, I]
(trapezoid on the right side of Figure 7-(a)) by
combining the chart items on the left side. This
action builds the dependency from s to t. In Fig-
ure 7-(b), the item of [s, r] is incomplete and
the item of [r, t] is complete. Then the algo-
rithm creates complete item [s, t,?, C]. For the
left direction case, the actions are similar. Note
that only the actions of creating the incomplete
chart items build new dependency relations be-
tween words, while the ones of creating the com-
plete items merge the existing structures without
building new relations.
Once the parser has considered the dependency
relations between words of distance 1, it goes on
128
to dependency relations between words of dis-
tance 2, and so on by the parsing actions. For
words of distance 2 and greater, it considers ev-
ery possible partition of the structures into two
parts and chooses the one with the highest score
for each direction. The score is the sum of the fea-
ture weights of the chart items. The features are
designed over edges of dependency trees and the
weights are given by model parameters (McDon-
ald and Pereira, 2006; Carreras, 2007). We store
the obtained chart items in a table. The chart item
includes the information on the optimal splitting
point of itself. Thus, by looking up the table, we
can obtain the best tree structure (with the highest
score) of any chart item.
s         r     r+1    t            s                   t
(a)
s         r     r t               s                 t
(b)
Figure 7: Cubic parsing actions of Eisner (1996)
4 Parsing with decision history
As mentioned above, the actions for creating
the incomplete items build the relations between
words. In this study, we only consider using his-
tory information when creating incomplete items.
4.1 Decision history
Suppose we are going to compute the scores of
the relations between ws and wt. There are two
possible directions for them.
By using the bottom-up style algorithm, the
scores of the structures between words with dis-
tance < |s?t| are computed in previous scans and
the structures are stored in the table. We divide
the decision history into two types: history-inside
and history-outside. The history-inside type is the
decision history made inside [s,t] and the history-
outside type is the history made outside [s,t].
4.1.1 History-inside
We obtain the structure with the highest score
for each direction of the dependency between ws
and wt. Figure 8-(b) shows the best solution (with
the highest score) of the left direction, where the
structure is split into two parts, [s, r1,?, C] and
[r1 + 1, t,?, C]. Figure 8-(c) shows the best so-
lution of the right case, where the structure is split
into two parts, [s, r2,?, C] and [r2 + 1, t,?, C].
s          r1 r1+1               t
ws ?                 wt (b)(a)
s r r +1 t2 2(c)
Figure 8: History-inside
By looking up the table, we have a subtree that
roots at ws on the right side of ws and a subtree
that roots at wt on the left side of wt. We use these
structures as the information on history-inside.
4.1.2 History-outside
For history-outside, we try to obtain the sub-
tree that roots at ws on the left side of ws and
the one that roots at wt on the right side of wt.
However, compared to history-inside, obtaining
history-outside is more complicated because we
do not know the boundaries and the proper struc-
tures of the subtrees. Here, we use an simple
heuristic method to find a subtree whose root is
at ws on the left side of ws and one whose root is
at wt on the right side of wt.
We introduce two assumptions: 1) The struc-
ture within a sub-sentence 2 is more reliable than
the one that goes across from sub-sentences. 2)
More context (more words) can result in a better
solution for determining subtree structures.
2To simplify, we split one sentence into sub-sentences
with punctuation marks.
129
Algorithm 1 Searching for history-outside
boundaries
1: Input: w, s, t
2: for k = s? 1 to 1 do
3: if(isPunct(wk)) break;
4: if(s? k >= t? s? 1) break
5: end for
6: bs = k
7: for k = t + 1 to |w| do
8: if(isPunct(wk)) break;
9: if(k ? t >= t? s? 1) break
10: end for
11: bt = k
12: Output: bs, bt
Under these two assumptions, Algorithm 1
shows the procedure for searching for history-
outside boundaries, where bs is the boundary for
for the descendants on the left side of ws , bt
is the boundary for searching the descendants on
the right side of wt, and isPunct is the function
that checks if the word is a punctuation mark. bs
should be in the same sub-sentence with s and
|s? bs| should be less than |t? s|. bt should be in
the same sub-sentence with t and |bt ? t| should
be less than |t? s|.
Next we try to find the subtree structures. First,
we collect the part-of-speech (POS) tags of the
heads of all the POS tags in training data and
remove the tags that occur fewer than 10 times.
Then, we determine the directions of the relations
by looking up the collected list. For bs and s, we
check if the POS tag of ws could be the head tag
of the POS tag of wbs by looking up the list. If
so, the direction d is ?. Otherwise, we check if
the POS tag of wbs could be the head tag of the
POS tag of ws. If so, d is ?, else d is ?. Fi-
nally, we obtain the subtree of ws from chart item
[bs, s, d, I]. Similarly, we obtain the subtree of wt.
Figure 9 shows the history-outside information for
ws and wt, where the relation between wbs and ws
and the relation between wbt and wt will be de-
termined by the above method. We have subtree
[rs, s, left, C] that roots at ws on the left side of
ws and subtree [t, rt, right, C] that roots at wt on
the right side of wt in Figure 9-(b) and (c).
4.2 Parsing algorithm
Then, we explain how to use these decision his-
tory in the parsing algorithm. We use Lst to rep-
bs rs s        t         rt bt(b)ws ?                 wt(a)
b r s t r b(c)s s t t
Figure 9: History-outside
resent the scores of basic features for the left di-
rection and Rst for the right case. Then we design
history-based features (described in Section 4.3)
based on the history-inside and history-outside in-
formation, as mentioned above. Finally, we up-
date the scores with the ones of the history-based
features by the following equations:
L+st = Lst + Ldfst (1)
R+st = Rst + Rdfst (2)
where L+st and R+st refer to the updated scores, Ldfst
and Rdfst refer to the scores of the history-based
features.
Algorithm 2 Parsing algorithm
1: Initialization: V [s, s, dir, I/C] = 0.0 ?s, dir
2: for k = 1 to n do
3: for s = 0 to n? k do
4: t = s + k
5: % Create incomplete items
6: Lst=V [s, t,?, I]= maxs?r<tV I(r);
7: Rst=V [s, t,?, I]= maxs?r<tV I(r);
8: Calculate Ldfst and Rdfst ;9: % Update the scores of incomplete chart items
10: V [s, t,?, I]=L+st=Lst + Ldfst
11: V [s, t,?, I]=R+st=Rst + Rdfst12: % Create complete items
13: V [s, t,?, C]= maxs?r<tV C(r);
14: V [s, t,?, C]= maxs<r?tV C(r);
15: end for
16: end for
Algorithm 2 is the parsing algorithm with
the history-based features, where V [s, t, dir, I/C]
refers to the score of chart item [s, t, dir, I/C],
V I(r) is a function to search for the optimal
sibling and grandchild nodes for the incomplete
items (line 6 and 7) (Carreras, 2007) given the
130
splitting point r and return the score of the struc-
ture, and V C(r) is a function to search for the op-
timal grandchild node for the complete items (line
13 and 14). Compared with the parsing algorithms
of Carreras (2007), Algorithm 2 uses history in-
formation by adding line 8, 10, and 11.
In Algorithm 2, it first creates chart items with
distance 1, then goes on to chart items with dis-
tance 2, and so on. In each round, it searches for
the structures with the highest scores for incom-
plete items shown at line 6 and 7 of Algorithm 2.
Then we update the scores with the history-based
features by Equation 1 and Equation 2 at line 10
and 11 of Algorithm 2. However, note that we can
not guarantee to find the candidate with the high-
est score with Algorithm 2 because new features
violate the assumptions of dynamic programming.
4.3 History-based features
In this section, we design features that capture the
history information in the recorded decisions.
For a dependency between two words, say s and
t, there are four subtrees that root at s or t. We de-
sign the features by combining s, twith each child
of s and t in the subtrees. The feature templates
are shown as follows: (In the following, c means
one of the children of s and t, and the nodes in the
templates are expanded to their lexical form and
POS tags to obtain actual features.):
C+Dir this feature template is a 2-tuple con-
sisting of (1) a c node and (2) the direction of the
dependency.
C+Dir+S/C+Dir+T this feature template is a 3-
tuple consisting of (1) a c node, (2) the direction
of the dependency, and (3) a s or t node.
C+Dir+S+T this feature template is a 4-tuple
consisting of (1) a c node, (2) the direction of the
dependency, (3) a s node, and (4) a t node.
s     csi r1 r1+1 cti tr2 cso cto r3
Figure 10: Structure of decision history
We use SHI to represent the subtree of s in
the history-inside, THI to represent the one of t
in the history-inside, SHO to represent the one
of s in the history-outside, and THO to represent
the one of t in the history-outside. Based on the
subtree types, the features are divided into four
sets: FSHI , FTHI , FSHO, and FTHO refer to the
features related to the children that are in subtrees
SHI , THI , SHO, and THO respectively.
Figure 10 shows the structure of decision his-
tory of a left dependency (between s and t) re-
lation. For the right case, the structure is simi-
lar. In the figure, SHI is chart item [s, r1,?, C],
THI is chart item [r1 + 1, t,?, C], SHO is
chart item [r2, s,?, C], and THO is chart item
[t, r3,?, C]. We use csi, cti, cso, and cto to repre-
sent a child of s/t in subtrees SHI , THI , SHO,
and THO respectively. The lexical form features
of FSHI and FSHO are listed as examples in Table
1, where ?L? refers to the left direction. We can
also expand the nodes in the templates to the POS
tags. Compared with the algorithm of Carreras
(2007) that only considers the furthest children of
s and t, Algorithm 2 considers all the children.
Table 1: Lexical form features of FSHI and FSHO
template FSHI FSHO
C+DIR word-csi+L word-cso+L
C+DIR+S word-csi+L+word-s word-cso+L+word-s
C+DIR+T word-csi+L+word-t word-cso+L+word-t
C+DIR word-csi+L word-cso+L
+S+T +word-s+word-t +word-s+word-t
4.4 Policy of using history
In practice, we define several policies to use the
history information for different word pairs as fol-
lows:
? All: Use the history-based features for all the
word pairs without any restriction.
? Sub-sentences: use the history-based fea-
tures only for the relation of two words from
sub-sentences. Here, we use punctuation
marks to split sentences into sub-sentences.
? Distance: use the history-based features for
the relation of two words within a predefined
distance. We set the thresholds to 3, 5, and
10.
131
5 Experimental results
In order to evaluate the effectiveness of the
history-based features, we conducted experiments
on Chinese and English data.
For English, we used the Penn Treebank (Mar-
cus et al, 1993) in our experiments and the tool
?Penn2Malt?3 to convert the data into dependency
structures using a standard set of head rules (Ya-
mada and Matsumoto, 2003a). To match previous
work (McDonald and Pereira, 2006; Koo et al,
2008), we split the data into a training set (sec-
tions 2-21), a development set (Section 22), and a
test set (section 23). Following the work of Koo
et al (2008), we used the MXPOST (Ratnaparkhi,
1996) tagger trained on training data to provide
part-of-speech tags for the development and the
test set, and we used 10-way jackknifing to gener-
ate tags for the training set.
For Chinese, we used the Chinese Treebank
(CTB) version 4.04 in the experiments. We also
used the ?Penn2Malt? tool to convert the data and
created a data split: files 1-270 and files 400-931
for training, files 271-300 for testing, and files
301-325 for development. We used gold stan-
dard segmentation and part-of-speech tags in the
CTB. The data partition and part-of-speech set-
tings were chosen to match previous work (Chen
et al, 2008; Yu et al, 2008).
We measured the parser quality by the unla-
beled attachment score (UAS), i.e., the percentage
of tokens with the correct HEAD 5. And we also
evaluated on complete dependency analysis.
In our experiments, we implemented our sys-
tems on the MSTParser6 and extended with
the parent-child-grandchild structures (McDonald
and Pereira, 2006; Carreras, 2007). For the base-
line systems, we used the first- and second-order
(parent-sibling) features that were used in Mc-
Donald and Pereira (2006) and other second-order
features (parent-child-grandchild) that were used
in Carreras (2007). In the following sections, we
call the second-order baseline systems Baseline
3http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
4http://www.cis.upenn.edu/?chinese/.
5As in previous work, English evaluation ignores any to-
ken whose gold-standard POS tag is one of {?? `` : , .} and
Chinese evaluation ignores any token whose tag is ?PU?.
6http://mstparser.sourceforge.net
and our new systems OURS.
5.1 Results with different feature settings
In this section, we test our systems with different
settings on the development data.
Table 2: Results with different policies
Chinese English
Baseline 89.04 92.43
D1 88.73 92.27
D3 88.90 92.36
D5 89.10 92.59
D10 89.32 92.57
Dsub 89.57 92.63
Table 2 shows the parsing results when we used
different policies defined in Section 4.4 with all
the types of features, where Dsub refers to apply-
ing the policy: sub-sentence, D1 refers to apply-
ing the policy: all, and D3|5|10 refers to applying
the policy: distance with the predefined distance
3, 5, or 10. The results indicated that the accu-
racies of our systems decreased if we used the
history information for short distance words. The
system with Dsub performed the best.
Table 3: Results with different types of Features
Chinese English
Baseline 89.04 92.43
+FSHI 89.14 92.53
+FTHI 89.33 92.35
+FSHO 89.25 92.47
+FTHO 88.99 92.54
Then we investigated the effect of different
types of the history-based features. Table 3 shows
the results with policy Dsub. From the table, we
found that FTHI provided the largest improve-
ment for Chinese and FTHO performed the best
for English.
In what follows, we used Dsub as the policy for
all the languages, the features FSHI + FTHI +
FSHO for Chinese, and the features FSHI +
FSHO + FTHO for English.
5.2 Main results
The main results are shown in the upper parts of
Tables 4 and 5, where the improvements by OURS
over the Baselines are shown in parentheses. The
results show that OURS provided better perfor-
mance over the Baselines by 1.02 points for Chi-
132
Table 4: Results for Chinese
UAS Complete
Baseline 88.41 48.85
OURS 89.43(+1.02) 50.86
OURS+STACK 89.53 49.42
Zhao2009 87.0 ?
Yu2008 87.26 ?
STACK 88.95 49.42
Chen2009 89.91 48.56
nese and 0.29 points for English. The improve-
ments of (OURS) were significant in McNemar?s
Test with p < 10?4 for Chinese and p < 10?3 for
English.
5.3 Comparative results
Table 4 shows the comparative results for Chinese,
where Zhao2009 refers to the result of (Zhao et
al., 2009), Yu2008 refers to the result of Yu et
al. (2008), Chen2009 refers to the result of Chen
et al (2009) that is the best reported result on
this data, and STACK refers to our implementa-
tion of the combination parser of Nivre and Mc-
Donald (2008) using our baseline system and the
MALTParser7. The results indicated that OURS
performed better than Zhao2009, Yu2008, and
STACK, but worse than Chen2009 that used large-
scale unlabeled data (Chen et al, 2009). We also
implemented the combination system of OURS
and the MALTParser, referred as OURS+STACK
in Table 4. The new system achieved further im-
provement. In future work, we can combine our
approach with the parser of Chen et al (2009).
Table 5 shows the comparative results for En-
glish, where Y&M2003 refers to the parser of Ya-
mada and Matsumoto (2003b), CO2006 refers to
the parser of Corston-Oliver et al (2006), Z&C
2008 refers to the combination system of Zhang
and Clark (2008), STACK refers to our implemen-
tation of the combination parser of Nivre and Mc-
Donald (2008), KOO2008 refers to the parser of
Koo et al (2008), Chen2009 refers to the parser
of Chen et al (2009), and Suzuki2009 refers to
the parser of Suzuki et al (2009) that is the best
reported result for this data. The results shows
that OURS outperformed the first two systems that
were based on single models. Z&C 2008 and
STACK were the combination systems of graph-
7http://www.maltparser.org/
Table 5: Results for English
UAS Complete
Baseline 91.92 44.28
OURS 92.21 (+0.29) 45.24
Y&M2003 90.3 38.4
CO2006 90.8 37.6
Z&C2008 92.1 45.4
STACK 92.53 47.06
KOO2008 93.16 ?
Chen2009 93.16 47.15
Suzuki2009 93.79 ?
based and transition-based models. OURS per-
formed better than Z&C 2008, but worse than
STACK. The last three systems that used large-
scale unlabeled data performed better than OURS.
6 Related work
There are several studies that tried to overcome
the limited feature scope of graph-based depen-
dency parsing models .
Nakagawa (2007) proposed a method to deal
with the intractable inference problem in a graph-
based model by introducing the Gibbs sampling
algorithm. Compared with their approach, our ap-
proach is much simpler yet effective. Hall (2007)
used a re-ranking scheme to provide global fea-
tures while we simply augment the features of an
existing parser.
Nivre and McDonald (2008) and Zhang and
Clark (2008) proposed stacking methods to com-
bine graph-based parsers with transition-based
parsers. One parser uses dependency predictions
made by another parser. Our results show that our
approach can be used in the stacking frameworks
to achieve higher accuracy.
7 Conclusions
This paper proposes an approach for improving
graph-based dependency parsing by using the de-
cision history. For the graph-based model, we
design a set of features over short dependen-
cies computed in the earlier stages to improve
the accuracy of long dependencies in the later
stages. The results demonstrate that our proposed
approach outperforms baseline systems by 1.02
points for Chinese and 0.29 points for English.
133
References
Buchholz, S., E. Marsi, A. Dubey, and Y. Kry-
molowski. 2006. CoNLL-X shared task on
multilingual dependency parsing. Proceedings of
CoNLL-X.
Carreras, X. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 957?961.
Chen, WL., D. Kawahara, K. Uchimoto, YJ. Zhang,
and H. Isahara. 2008. Dependency parsing with
short dependency relations in unlabeled data. In
Proceedings of IJCNLP 2008.
Chen, WL., J. Kazama, K. Uchimoto, and K. Torisawa.
2009. Improving dependency parsing with subtrees
from auto-parsed data. In Proceedings of EMNLP
2009, pages 570?579, Singapore, August.
Corston-Oliver, S., A. Aue, Kevin. Duh, and Eric Ring-
ger. 2006. Multilingual dependency parsing using
bayes point machines. In HLT-NAACL2006.
Culotta, A. and J. Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proceedings of
ACL 2004, pages 423?429.
Eisner, J. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proc. of
COLING 1996, pages 340?345.
Hall, Keith. 2007. K-best spanning tree parsing. In
Proc. of ACL 2007, pages 392?399, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Koo, T., X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proceed-
ings of ACL-08: HLT, Columbus, Ohio, June.
Li, Charles N. and Sandra A. Thompson. 1997. Man-
darin Chinese - A Functional Reference Grammar.
University of California Press.
Marcus, M., B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn Treebank. Computational Linguis-
ticss, 19(2):313?330.
McDonald, R. and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models.
In Proceedings of EMNLP-CoNLL, pages 122?131.
McDonald, R. and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proc. of EACL2006.
McDonald, Ryan. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Nakagawa, Tetsuji. 2007. Multilingual dependency
parsing using global features. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 952?956.
Nakazawa, T., K. Yu, D. Kawahara, and S. Kurohashi.
2006. Example-based machine translation based on
deeper NLP. In Proceedings of IWSLT 2006, pages
64?70, Kyoto, Japan.
Nivre, J. and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL-08: HLT, Columbus, Ohio,
June.
Nivre, J., J. Hall, and J. Nilsson. 2004. Memory-
based dependency parsing. In Proc. of CoNLL
2004, pages 49?56.
Nivre, J., J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, pages 915?932.
Ratnaparkhi, A. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of EMNLP,
pages 133?142.
Suzuki, Jun, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An empirical study of semi-
supervised structured conditional models for depen-
dency parsing. In Proc. of EMNLP 2009, pages
551?560, Singapore, August. Association for Com-
putational Linguistics.
Yamada, H. and Y. Matsumoto. 2003a. Statistical de-
pendency analysis with support vector machines. In
Proceedings of IWPT2003, pages 195?206.
Yamada, H. and Y. Matsumoto. 2003b. Statistical de-
pendency analysis with support vector machines. In
Proceedings of IWPT2003, pages 195?206.
Yu, K., D. Kawahara, and S. Kurohashi. 2008. Chi-
nese dependency parsing with large scale automati-
cally constructed case structures. In Proceedings of
Coling 2008, pages 1049?1056, Manchester, UK,
August.
Zhang, Y. and S. Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of EMNLP 2008, pages 562?571, Hon-
olulu, Hawaii, October.
Zhao, Hai, Yan Song, Chunyu Kit, and Guodong
Zhou. 2009. Cross language dependency parsing
using a bilingual lexicon. In Proceedings of ACL-
IJCNLP2009, pages 55?63, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
134
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 73?83,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
SMT Helps Bitext Dependency Parsing
Wenliang Chen??, Jun?ichi Kazama?, Min Zhang?, Yoshimasa Tsuruoka??,
Yujie Zhang??, Yiou Wang?, Kentaro Torisawa? and Haizhou Li?
?Human Language Technology, Institute for Infocomm Research, Singapore
?National Institute of Information and Communications Technology (NICT), Japan
?School of Information Science, JAIST, Japan
?Beijing Jiaotong University, China
{wechen, mzhang, hli}@i2r.a-star.edu.sg
{kazama, torisawa, yujie, wangyiou}@nict.go.jp
tsuruoka@jaist.ac.jp
Abstract
We propose a method to improve the accuracy
of parsing bilingual texts (bitexts) with the
help of statistical machine translation (SMT)
systems. Previous bitext parsing methods use
human-annotated bilingual treebanks that are
hard to obtain. Instead, our approach uses an
auto-generated bilingual treebank to produce
bilingual constraints. However, because the
auto-generated bilingual treebank contains er-
rors, the bilingual constraints are noisy. To
overcome this problem, we use large-scale
unannotated data to verify the constraints and
design a set of effective bilingual features for
parsing models based on the verified results.
The experimental results show that our new
parsers significantly outperform state-of-the-
art baselines. Moreover, our approach is still
able to provide improvement when we use a
larger monolingual treebank that results in a
much stronger baseline. Especially notable
is that our approach can be used in a purely
monolingual setting with the help of SMT.
1 Introduction
Recently there have been several studies aiming to
improve the performance of parsing bilingual texts
(bitexts) (Smith and Smith, 2004; Burkett and Klein,
2008; Huang et al, 2009; Zhao et al, 2009; Chen
et al, 2010). In bitext parsing, we can use the in-
formation based on ?bilingual constraints? (Burkett
and Klein, 2008), which do not exist in monolingual
sentences. More accurate bitext parsing results can
be effectively used in the training of syntax-based
machine translation systems (Liu and Huang, 2010).
Most previous studies rely on bilingual treebanks
to provide bilingual constraints for bitext parsing.
Burkett and Klein (2008) proposed joint models on
bitexts to improve the performance on either or both
sides. Their method uses bilingual treebanks that
have human-annotated tree structures on both sides.
Huang et al (2009) presented a method to train a
source-language parser by using the reordering in-
formation on words between the sentences on two
sides. It uses another type of bilingual treebanks
that have tree structures on the source sentences and
their human-translated sentences. Chen et al (2010)
also used bilingual treebanks and made use of tree
structures on the target side. However, the bilingual
treebanks are hard to obtain, partly because of the
high cost of human translation. Thus, in their experi-
ments, they applied their methods to a small data set,
the manually translated portion of the Chinese Tree-
bank (CTB) which contains only about 3,000 sen-
tences. On the other hand, many large-scale mono-
lingual treebanks exist, such as the Penn English
Treebank (PTB) (Marcus et al, 1993) (about 40,000
sentences in Version 3) and the latest version of CTB
(over 50,000 sentences in Version 7).
In this paper, we propose a bitext parsing ap-
proach in which we produce the bilingual constraints
on existing monolingual treebanks with the help of
SMT systems. In other words, we aim to improve
source-language parsing with the help of automatic
translations.
In our approach, we first use an SMT system
to translate the sentences of a source monolingual
treebank into the target language. Then, the target
sentences are parsed by a parser trained on a tar-
get monolingual treebank. We then obtain a bilin-
gual treebank that has human annotated trees on the
source side and auto-generated trees on the target
side. Although the sentences and parse trees on the
73
target side are not perfect, we expect that we can
improve bitext parsing performance by using this
newly auto-generated bilingual treebank. We build
word alignment links automatically using a word
alignment tool. Then we can produce a set of bilin-
gual constraints between the two sides.
Because the translation, parsing, and word align-
ment are done automatically, the constraints are not
reliable. To overcome this problem, we verify the
constraints by using large-scale unannotated mono-
lingual sentences and bilingual sentence pairs. Fi-
nally, we design a set of bilingual features based on
the verified results for parsing models.
Our approach uses existing resources including
monolingual treebanks to train monolingual parsers
on both sides, bilingual unannotated data to train
SMT systems and to extract bilingual subtrees,
and target monolingual unannotated data to extract
monolingual subtrees. In summary, we make the fol-
lowing contributions:
? We propose an approach that uses an auto-
generated bilingual treebank rather than
human-annotated bilingual treebanks used in
previous studies (Burkett and Klein, 2008;
Huang et al, 2009; Chen et al, 2010). The
auto-generated bilingual treebank is built with
the help of SMT systems.
? We verify the unreliable constraints by using
the existing large-scale unannotated data and
design a set of effective bilingual features over
the verified results. Compared to Chen et al
(2010) that also used tree structures on the tar-
get side, our approach defines the features on
the auto-translated sentences and auto-parsed
trees, while theirs generates the features by
some rules on the human-translated sentences.
? Our parser significantly outperforms state-of-
the-art baseline systems on the standard test
data of CTB containing about 3,000 sentences.
Moreover, our approach continues to achieve
improvement when we build our system us-
ing the latest version of CTB (over 50,000 sen-
tences) that results in a much stronger baseline.
? We show the possibility that we can improve
the performance even if the test set has no hu-
man translation. This means that our proposed
approach can be used in a purely monolingual
setting with the help of SMT. To our knowl-
edge, this paper is the first one that demon-
strates this widened applicability, unlike the
previous studies that assumed that the parser is
applied only on the bitexts made by humans.
Throughout this paper, we use Chinese as the
source language and English as the target language.
The rest of this paper is organized as follows. Sec-
tion 2 introduces the motivation of this work. Sec-
tion 3 briefly introduces the parsing model used in
the experiments. Section 4 describes a set of bilin-
gual features based on the bilingual constraints and
Section 5 describes how to use large-scale unanno-
tated data to verify the bilingual constraints and de-
fine another set of bilingual features based on the
verified results. Section 6 explains the experimental
results. Finally, in Section 7 we draw conclusions.
2 Motivation
Here, bitext parsing is the task of parsing source sen-
tences with the help of their corresponding transla-
tions. Figure 1-(a) shows an example of the input
of bitext parsing, where ROOT is an artificial root
token inserted at the beginning and does not depend
on any other token in the sentence, the dashed undi-
rected links are word alignment links, and the di-
rected links between words indicate that they have
a dependency relation. Given such inputs, we build
dependency trees for the source sentences. Figure
1-(b) shows the output of bitext parsing for the ex-
ample in 1-(a).
ROOT!? ?? ?? ? ? ?? ?? ? ?? ??ta gaodu pingjia le yu lipeng zongli de huitan jieguo!! !!!
ROOT H hi hl d d h l f h f i h P Li!! e! g y!commen e !t e!resu ts!!o !!!t e!con erence!!!!w t !! eng
(a)
ROOT!? ?? ?? ? ? ?? ?? ? ?? ??ta gaodu pingjia le yu lipeng zongli de huitan jieguo!! !!(b)Figure 1: Input and output of our approach
In bitext parsing, some ambiguities exist on the
source side, but they may be unambiguous on the
74
target side. These differences are expected to help
improve source-side parsing.
Suppose we have a Chinese sentence shown in
Figure 2-(a). In this sentence, there is a nomi-
nalization case (Li and Thompson, 1997) in which
the particle ??(de)/nominalizer? is placed after the
verb compound ???(peiyu)??(qilai)/cultivate?
to modify ???(jiqiao)/skill?. This nominaliza-
tion is a relative clause, but does not have a clue
about its boundary. That is, it is very hard to deter-
mine which word is the head of ???(jiqiao)/skill?.
The head may be ???(fahui)/demonstrate? or ??
?(peiyu)/cultivate?, as shown in Figure 2-(b) and
-(c), where (b) is correct.
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiaoPN!!!!!!!VV!!!!!!!!!DT!!!!!!!!!!!!!!!NN!!!!!!!!!!!!!!!AD!!!!!!!!!!!!!!VV!!!!!!AD!!!!!!!VV!!!!VV DEC NN!!!!CC!!!NN(a)
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao(b)
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao(c)Figure 2: Example of an ambiguity on the Chinese side
In its English translation (Figure 3), word ?that? is
a clue indicating the relative clause which shows the
relation between ?skill? and ?cultivate?, as shown in
Figure 3. The figure shows that the translation can
provide useful bilingual constraints. From the de-
pendency tree on the target side, we find that the
word ?skill? corresponding to ???(jiqiao)/skill?
depends on the word ?demonstrate? corresponding
to ???(fahui)/demonstrate?, while the word ?cul-
tivate? corresponding to ???(peiyu)/cultivate? is a
grandchild of ?skill?. This is a positive evidence for
supporting ???(fahui)/demonstrate? as being the
head of ???(jiqiao)/skill?.
The above case uses the human translation on
the target side. However, there are few human-
annotated bilingual treebanks and the existing bilin-
gual treebanks are usually small. In contrast, there
are large-scale monolingual treebanks, e.g., the PTB
and the latest version of CTB. So we want to use
existing resources to generate a bilingual treebank
with the help of SMT systems. We hope to improve
source side parsing by using this newly built bilin-
gual treebank.
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
He!hoped!that!all!the!athletes!would!!fully!demonstrate!the!strength!and!skill!that!they!cultivate!daily
Figure 3: Example of human translation
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 4: Example of Moses translation
Figure 4 shows an example of a translation us-
ing a Moses-based system, where the target sen-
tence is parsed by a monolingual target parser. The
translation contains some errors, but it does contain
some correct parts that can be used for disambigua-
tion. In the figure, the word ?skills? corresponding
to ???(jiqiao)/skill? is a grandchild of the word
?play? corresponding to ???(fahui)/demonstrate?.
This is a positive evidence for supporting ??
?(fahui)/demonstrate? as being the head of ??
?(jiqiao)/skill?.
From this example, although the sentences and
parse trees on the target side are not perfect, we
still can explore useful information to improve bitext
parsing. In this paper, we focus on how to design
a method to verify such unreliable bilingual con-
straints.
3 Parsing model
In this paper, we implement our approach based
on graph-based parsing models (McDonald and
Pereira, 2006; Carreras, 2007). Note that our ap-
proach can also be applied to transition-based pars-
ing models (Nivre, 2003; Yamada and Matsumoto,
2003).
The graph-based parsing model is to search for
the maximum spanning tree (MST) in a graph (Mc-
Donald and Pereira, 2006). The formulation defines
the score of a dependency tree to be the sum of edge
scores,
75
s(x, y) =
?
g?y
score(w, x, g) =
?
g?y
w ?f(x, g) (1)
where x is an input sentence, y is a dependency
tree for x, and g is a spanning subgraph of y. f(x, g)
can be based on arbitrary features of the subgraph
and the input sequence x and the feature weight
vector w are the parameters to be learned by using
MIRA (Crammer and Singer, 2003) during training.
In our approach, we use two types of features
for the parsing model. One is monolingual fea-
tures based on the source sentences. The mono-
lingual features include the first- and second- order
features presented in McDonald and Pereira (2006)
and the parent-child-grandchild features used in Car-
reras (2007). The other one is bilingual features (de-
scribed in Sections 4 and 5) that consider the bilin-
gual constraints.
We call the parser with the monolingual features
on the source side Parsers, and the parser with the
monolingual features on the target side Parsert.
4 Original bilingual features
In this paper, we generate two types of bilingual fea-
tures, original and verified bilingual features. The
original bilingual features (described in this section)
are based on the bilingual constraints without being
verified by large-scale unannotated data. And the
verified bilingual features (described in Section 5)
are based on the bilingual constraints verified by us-
ing large-scale unannotated data.
4.1 Auto-generated bilingual treebank
Assuming that we have monolingual treebanks on
the source side, an SMT system that can translate
the source sentences into the target language, and a
Parsert trained on the target monolingual treebank.
We first translate the sentences of the source
monolingual treebank into the target language using
the SMT system. Usually, SMT systems can output
the word alignment links directly. If they can not, we
perform word alignment using some publicly avail-
able tools, such as Giza++ (Och and Ney, 2003) or
Berkeley Aligner (Liang et al, 2006; DeNero and
Klein, 2007). The translated sentences are parsed by
the Parsert. Then, we have a newly auto-generated
bilingual treebank.?
4.2 Bilingual constraint functions
In this paper, we focus on the first- and second-
order graph models (McDonald and Pereira, 2006;
Carreras, 2007). Thus we produce the constraints
for bigram (a single edge) and trigram (adjacent
edges) dependencies in the graph model. For the tri-
gram dependencies, we consider the parent-sibling
and parent-child-grandchild structures described in
McDonald and Pereira (2006) and Carreras (2007).
We leave the third-order models (Koo and Collins,
2010) for a future study.
Suppose that we have a (candidate) dependency
relation rs that can be a bigram or trigram de-
pendency. We examine whether the corresponding
words of the source words of rs have a dependency
relation rt in the target trees. We also consider the
direction of the dependency relation. The corre-
sponding word of the head should also be the head
in rt. We define a binary function for this bilingual
constraint: Fbn(rsn : rtk), where n and k refers to
the types of the dependencies (2 for bigram and 3 for
trigram). For example, in rs2 : rt3, rs2 is a bigram
dependency on the source side and rt3 is a trigram
dependency on the target side.
4.2.1 Bigram constraint function: Fb2
For rs2, we consider two types of bilingual con-
straints. The first constraint function, denoted as
Fb2(rs2 : rt2), checks if the corresponding words
also have a direct dependency relation rt2. Figure
5 shows an example, where the source word ??
?(quanti)? depends on ????(yundongyuan)?
and word ?all? corresponding to ???(quanti)? de-
pends on word ?athletes? corresponding to ???
?(yundongyuan)?. In this case, Fb2(rs2 : rt2) =
+. However, when the source words are ??(ta)?
and ???(xiwang)?, this time their corresponding
words ?He? and ?hope? do not have a direct depen-
dency relation. In this case, Fb2(rs2 : rt2)=?.
The second constraint function, denoted as
Fb2(rs2 : rt3), checks if the corresponding words
form a parent-child-grandchild relation that often
occurs in translation (Koehn et al, 2003). Figure 6
shows an example. The source word ???(jiqiao)?
depends on ???(fahui)? while its corresponding
word ?skills? indirectly depends on ?play? which
corresponds to ???(fahui)? via ?to?. In this case,
Fb2(rs2 : rt3)=+.
76
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 5: Example of bilingual constraints (2to2)
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 6: Example of bilingual constraints (2to3)
4.2.2 Trigram constraint function: Fb3
For a second-order relation on the source side,
we consider one type of constraint. We have three
source words that form a second-order relation and
all of them have the corresponding words. We
define function Fb3(rs3 : rt3) for this constraint.
The function checks if the corresponding words
form a trigram dependencies structure. An exam-
ple is shown in Figure 7. The source words ??
?(liliang)?, ??(he)?, and ???(jiqiao)? form a
parent-sibling structure, while their corresponding
words ?strength?, ?and?, and ?skills? also form a
parent-sibling structure on the target side. In this
case, function Fb3(rs3 : rt3)=+.
? ?? ?? ??? ?? ?? ?? ?? ?? ? ?? ? ??ta xiwang quanti yundongyuan chongfeng fahui pingshi peiyu qilai de!!liliang he!jiqiao
he!expressed!the!hope!that!all!athletes!used!to!give!full!play!to!the!country!'s!strength!and!skills!
Figure 7: Example of bilingual constraints (3to3)
4.3 Bilingual reordering function: Fro
Huang et al (2009) proposed features based on
reordering between languages for a shift-reduce
parser. They define the features based on word-
alignment information to verify whether the corre-
sponding words form a contiguous span to resolve
shift-reduce conflicts. We also implement similar
features in our system. For example, in Figure 1-
(a) the source span is [??(huitan), ??(jieguo)],
which maps onto [results, conference]. Because no
word within this target span is aligned to a source
word outside of the source span, this span is a con-
tiguous span. In this case, function Fro =+, other-
wise Fro=?.
4.4 Original bilingual features
We define original bilingual features based on the
bilingual constraint functions and the bilingual re-
ordering function.
Table 1 lists the original features, where Dir
refers to the directions1 of the source-side dependen-
cies, Fb2 can be Fb2(rs2 : rt2) and Fb2(rs2 : rt3),
and Fb3 is Fb3(rs3 : rt3). Each line of the table
defines a feature template that is a combination of
functions.
First-order features Second-order features
?Fro?
?Fb2, Dir? ?Fb3, Dir?
?Fb2, Dir, Fro? ?Fb3, Dir, Fro?
Table 1: Original bilingual features
We use an example to show how to generate the
original bilingual features in practice. In Figure 4,
we want to define the bilingual features for the bi-
gram dependency (rs2) between ???(fahui)? and
???(jiqiao)?. The corresponding words form a tri-
gram relation rt3 in the target dependency tree. The
direction of the bigram dependency is right. Then
we have feature ??Fb2(rs2 : rt3)=+, RIGHT ?? for
the second first-order feature template in Table 1.
5 Verified bilingual features
However, because the bilingual treebank is gener-
ated automatically, using the bilingual constraints
alone is not reliable. Therefore, in this section we
verify the constraints by using large-scale unanno-
tated data to overcome this problem. More specifi-
cally, rtk of the constraint is verified by checking a
list of target monolingual subtrees and rsn : rtk is
verified by checking a list of bilingual subtrees. The
subtrees are extracted from the large-scale unanno-
tated data. The basic idea is as follows: if the de-
pendency structures of a bilingual constraint can be
found in the list of the target monolingual subtrees
1For the second order features, Dir is the combination of
the directions of two dependencies.
77
or bilingual subtrees, this constraint will probably be
reliable.
We first parse the large-scale unannotated mono-
lingual and bilingual data. Subsequently, we ex-
tract the monolingual and bilingual subtrees from
the parsed data. We then verify the bilingual con-
straints using the extracted subtrees. Finally, we
generate the bilingual features based on the verified
results for the parsing models.
5.1 Verified constraint functions
5.1.1 Monolingual target subtrees
Chen et al (2009) proposed a simple method to
extract subtrees from large-scale monolingual data
and used them as features to improve monolingual
parsing. Following their method, we parse large
unannotated data with the Parsert and obtain the sub-
tree list (STt) on the target side. We extract two
types of subtrees: bigram (two words) subtree and
trigram (three words) subtree.
H b ht b h b k
ROOT!!He!!!!!bought!!!!!a!!!!book
e!!!!! oug oug t!! oo !
a book b ht b k!!!!!
(a) (b)
oug !!!a!!!!! oo !
Figure 8: Example of monolingual subtree extraction
From the dependency tree in Figure 8-(a), we ob-
tain the subtrees shown in Figure 8-(b) where the
first three are bigram subtrees and the last one is
a trigram subtree. After extraction, we obtain the
subtree list STt that includes two sets, one for bi-
gram subtrees, and the other one for trigram sub-
trees. We remove the subtrees occurring only once
in the data. For each set, we assign labels to the
extracted subtrees according to their frequencies by
using the same method as that of Chen et al (2009).
If the frequency of a subtree is in the top 10% in the
corresponding set, it is labeled HF. If the frequency
is between the top 20% and 30%, it is labeled MF.
We assign the label LF to the remaining subtrees.
We use Type(stt) to refer to the label of a subtree,
stt.
5.1.2 Verified target constraint function:
Fvt(rtk)
We use the extracted target subtrees to verify the
rtk of the bilingual constraints. In fact, rtk is a can-
didate subtree. If the rtk is included in STt, func-
tion Fvt(rtk) = Type(rtk), otherwise Fvt(rtk) =
ZERO. For example, in Figure 5 the bigram struc-
ture of ?all? and ?athletes? can form a bigram sub-
tree that is included STt and its label is HF. In this
case, Fvt(rt2)= HF .
5.1.3 Bilingual subtrees
We extract bilingual subtrees from a bilingual
corpus, which is parsed by the Parsers and Parsert
on both sides. We extract three types of bilingual
subtrees: bigram-bigram (stbi22), bigram-trigram
(stbi23), and trigram-trigram (stbi33) subtrees. For
example, stbi22 consists of a bigram subtree on the
source side and a bigram subtree on the target side.
? ? ? ?? ? ? ? ??ROOT! ?ta shi yi ming xuesheng
ROOT!!He!!!!!is!!!!!a!!!!!student He!!!!!is is!!!!!student
(a) (b)
Figure 9: Example of bilingual subtree extraction
From the dependency tree in Figure 9-(a), we
obtain the bilingual subtrees shown in Figure 9-
(b). Figure 9-(b) shows the extracted bigram-bigram
bilingual subtrees. After extraction, we obtain the
bilingual subtrees STbi. We remove the subtrees oc-
curring only once in the data.
5.1.4 Verified bilingual constraint function:
Fvb(rbink)
We use the extracted bilingual subtrees to verify
the rsn : rtk (rbink in short) of the bilingual con-
straints. rsn and rtk form a candidate bilingual sub-
tree stbink. If the stbink is included in STbi, function
Fvb(rbink)=+, otherwise Fvb(rbink)=?.
5.2 Verified bilingual features
Then, we define another set of bilingual features by
combining the verified constraint functions. We call
these bilingual features ?verified bilingual features?.
78
Table 2 lists the verified bilingual features used in
our experiments, where each line defines a feature
template that is a combination of functions.
We use an example to show how to generate the
verified bilingual features in practice. In Figure 4,
we want to define the verified features for the bi-
gram dependency (rs2) between ???(fahui)? and
???(jiqiao)?. The corresponding words form a
trigram relation rt3. The direction of the bigram
dependency is right. Suppose we can find rt3 in
STt with label MF and can not find the candidate
bilingual subtree in STbi. Then we have feature
??Fb2(rs2 : rt3) = +, Fvt(rt3) = MF,RIGHT ??
for the third first-order feature template and feature
??Fb2(rs2 : rt3)=+, Fvb(rbi23)=?, RIGHT ?? for
the fifth in Table 2.
First-order features Second-order features
?Fro?
?Fb2, Fvt(rtk)? ?Fb3, Fvt(rtk)?
?Fb2, Fvt(rtk), Dir? ?Fb3, Fvt(rtk), Dir?
?Fb2, Fvb(rbink)? ?Fb3, Fvb(rbink)?
?Fb2, Fvb(rbink), Dir? ?Fb3, Fvb(rbink), Dir?
?Fb2, Fro, Fvb(rbink)?
Table 2: Verified bilingual features
6 Experiments
We evaluated the proposed method on the translated
portion of the Chinese Treebank V2 (referred to as
CTB2tp) (Bies et al, 2007), articles 1-325 of CTB,
which have English translations with gold-standard
parse trees. The tool ?Penn2Malt?2 was used to con-
vert the data into dependency structures. Following
the studies of Burkett and Klein (2008), Huang et
al. (2009) and Chen et al (2010), we used the ex-
act same data split: 1-270 for training, 301-325 for
development, and 271-300 for testing. Note that we
did not use human translation on the English side
of this bilingual treebank to train our new parsers.
For testing, we used two settings: a test with hu-
man translation and another with auto-translation.
To process unannotated data, we trained a first-order
Parsers on the training data.
To prove that the proposed method can work on
larger monolingual treebanks, we also tested our
2http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
methods on the CTB7 (LDC2010T07) that includes
much more sentences than CTB2tp. We used arti-
cles 301-325 for development, 271-300 for testing,
and the other articles for training. That is, we eval-
uated the systems on the same test data as CTB2tp.
Table 3 shows the statistical information on the data
sets.
Train Dev Test
CTB2tp 2,745 273 290
CTB7 50,747 273 290
Table 3: Number of sentences of data sets used
We built Chinese-to-English SMT systems based
on Moses3. Minimum error rate training (MERT)
with respect to BLEU score was used to tune the de-
coder?s parameters. The translation model was cre-
ated from the FBIS corpus (LDC2003E14). We used
SRILM4 to train a 5-gram language model. The lan-
guage model was trained on the target side of the
FBIS corpus and the Xinhua news in English Gi-
gaword corpus (LDC2009T13). The development
and test sets were from NIST MT08 evaluation cam-
paign5. We then used the SMT systems to translate
the training data of CTB2tp and CTB7.
To directly compare with the results of Huang
et al (2009) and Chen et al (2010), we also used
the same word alignment tool, Berkeley Aligner
(Liang et al, 2006; DeNero and Klein, 2007), to
perform word alignment for CTB2tp and CTB7.
We trained a Berkeley Aligner on the FBIS corpus
(LDC2003E14). We removed notoriously bad links
in {a, an, the}?{?(de),?(le)} following the work
of Huang et al (2009).
To train an English parser, we used the PTB
(Marcus et al, 1993) in our experiments and the
tool ?Penn2Malt? to convert the data. We split the
data into a training set (sections 2-21), a develop-
ment set (section 22), and a test set (section 23).
We trained first-order and second-order Parsert on
the training data. The unlabeled attachment score
(UAS) of the second-order Parsert was 91.92, in-
dicating state-of-the-art accuracy on the test data.
We used the second-order Parsert to parse the auto-
translated/human-made target sentences in the CTB
3http://www.statmt.org/moses/
4http://www.speech.sri.com/projects/srilm/download.html
5http://www.itl.nist.gov/iad/mig//tests/mt/2008/
79
data.
To extract English subtrees, we used the BLLIP
corpus (Charniak et al, 2000) that contains about
43 million words of WSJ texts. We used the MX-
POST tagger (Ratnaparkhi, 1996) trained on train-
ing data to assign POS tags and used the first-order
Parsert to process the sentences of the BLLIP cor-
pus. To extract bilingual subtrees, we used the FBIS
corpus and an additional bilingual corpus contain-
ing 800,000 sentence pairs from the training data of
NIST MT08 evaluation campaign. On the Chinese
side, we used the morphological analyzer described
in (Kruengkrai et al, 2009) trained on the training
data of CTBtp to perform word segmentation and
POS tagging and used the first-order Parsers to parse
all the sentences in the data. On the English side, we
used the same procedure as we did for the BLLIP
corpus. Word alignment was performed using the
Berkeley Aligner.
We reported the parser quality by the UAS, i.e.,
the percentage of tokens (excluding all punctuation
tokens) with correct HEADs.
6.1 Experimental settings
For baseline systems, we used the monolingual fea-
tures mentioned in Section 3. We called these fea-
tures basic features. To compare the results of (Bur-
kett and Klein, 2008; Huang et al, 2009; Chen et
al., 2010), we used the test data with human trans-
lation in the following three experiments. The tar-
get sentences were parsed by using the second-order
Parsert. We used PAG to refer to our parsers trained
on the auto-generated bilingual treebank.
6.2 Training with CTB2tp
Order-1 Order-2
Baseline 84.35 87.20
PAGo 84.71(+0.36) 87.85(+0.65)
PAG 85.37(+1.02) 88.49(+1.29)
ORACLE 85.79(+1.44) 88.87(+1.67)
Table 4: Results of training with CTB2tp
First, we conducted the experiments on the stan-
dard data set of CTB2tp, which was also used in
other studies (Burkett and Klein, 2008; Huang et al,
2009; Chen et al, 2010). The results are given in
Table 4, where Baseline refers to the system with
the basic features, PAGo refers to that after adding
the original bilingual features of Table 1 to Baseline,
PAG refers to that after adding the verified bilingual
features of Table 2 to Baseline, and ORACLE6 refers
to using human-translation for training data with
adding the features of Table 1. We obtained an ab-
solute improvement of 1.02 points for the first-order
model and 1.29 points for the second-order model by
adding the verified bilingual features. The improve-
ments of the final systems (PAG) over the Baselines
were significant in McNemar?s Test (p < 0.001 for
the first-order model and p < 0.0001 for the second-
order model). If we used the original bilingual fea-
tures (PAGo), the system dropped 0.66 points for the
first-order and 0.64 points for the second-order com-
pared with system PAG. This indicated that the ver-
ified bilingual constraints did provide useful infor-
mation for the parsing models.
We also found that PAG was about 0.3 points
lower than ORACLE. The reason is mainly due
to the imperfect translations, although we used
the large-scale subtree lists to help verify the con-
straints. We tried adding the features of Table 2 to
the ORACLE system, but the results were worse.
These facts indicated that our approach obtained the
benefits from the verified constraints, while using
the bilingual constraints alone was enough for OR-
ACLE.
6.3 Training with CTB7
 0.83
 0.84
 0.85
 0.86
 0.87
 0.88
 0.89
 0.9
 0.91
 0.92
 5  10  20  30  40  50
U
A
S
Amount of training data (K)
Baseline1PAG1Baseline2PAG2
Figure 10: Results of using different sizes of training data
Here, we demonstrate that our approach is still
able to provide improvement, even if we use larger
6Note that we also used the tool to perform the word align-
ment automatically.
80
Baseline D10 D20 D50 D100 GTran
BLEU n/a 14.71 15.84 16.92 17.95 n/a
UAS 87.20 87.63 87.67 88.20 88.49 88.58
Table 5: Results of using different translations
training data that result in strong baseline systems.
We incrementally increased the training sentences
from the CTB7. Figure 10 shows the results of us-
ing different sizes of CTB7 training data, where the
numbers of the x-axis refer to the sentence numbers
of training data used, Baseline1 and Baseline2 re-
fer to the first- and second-order baseline systems,
and PAG1 and PAG2 refer to our first- and second-
order systems. The figure indicated that our sys-
tem always outperformed the baseline systems. For
small data sizes, our system performed much better
than the baselines. For example, when using 5,000
sentences, our second-order system provided a 1.26
points improvement over the second-order baseline.
Finally, when we used all of the CTB7 training
data, our system achieved 91.66 for the second-order
model, while the baseline achieved 91.10.
6.4 With different settings of SMT systems
We investigated the effects of different settings of
SMT systems. We randomly selected 10%, 20%,
and 50% of FBIS to train the Moses systems and
used them to translate CTB2tp. The results are in
Table 5, where D10, D20, D50, and D100 refer to
the system with 10%, 20%, 50%, and 100% data re-
spectively. For reference, we also used the Google-
translate online system7, indicated as GTran in the
table, to translate the CTB2tp.
From the table, we found that our system outper-
formed the Baseline even if we used only 10% of the
FBIS corpus. The BLEU and UAS scores became
higher, when we used more data of the FBIS corpus.
And the gaps among the results of D50, D100, and
GTran were small. This indicated that our approach
was very robust to the noise produced by the SMT
systems.
6.5 Testing with auto-translation
We also translated the test data into English using
the Moses system and tested the parsers on the new
7http://translate.google.com/
test data. Table 6 shows the results. The results
showed that PAG outperformed the baseline systems
for both the first- and second-order models. This
indicated that our approach can provide improve-
ment in a purely monolingual setting with the help
of SMT.
Order-1 Order-2
Baseline 84.35 87.20
PAG 84.88(+0.53) 87.89(+0.69)
Table 6: Results of testing with auto-translation (training
with CTB2tp)
6.6 Comparison results
With CTB2tp With CTB7
Type System UAS System UAS
M Baseline 87.20 Baseline 91.10
HA
Huang2009 86.3 n/a
Chen2010BI 88.56
Chen2010ALL 90.13
AG PAG 88.49 PAG 91.66PAG+STs 89.75
Table 7: Comparison of our results with other pre-
vious reported systems. Type M denotes training on
monolingual treebank. Types HA and AG denote training
on human-annotated and auto-generated bilingual tree-
banks respectively.
We compared our results with the results reported
previously for the same data. Table 7 lists the re-
sults, where Huang2009 refers to the result of Huang
et al (2009), Chen2010BI refers to the result of
using bilingual features in Chen et al (2010), and
Chen2010ALL refers to the result of using all of
the features in Chen et al (2010). The results
showed that our new parser achieved better accuracy
than Huang2009 and comparable to Chen2010BI .
To achieve higher performance, we also added the
source subtree features (Chen et al, 2009) to our
system: PAG+STs. The new result is close to
Chen2010ALL. Compared with the approaches of
81
Huang et al (2009) and Chen et al (2010), our
approach used an auto-generated bilingual treebank
while theirs used a human-annotated bilingual tree-
bank. By using all of the training data of CTB7, we
obtained a more powerful baseline that performed
much better than the previous reported results. Our
parser achieved 91.66, much higher accuracy than
the others.
7 Conclusion
We have presented a simple yet effective approach
to improve bitext parsing with the help of SMT sys-
tems. Although we trained our parser on an auto-
generated bilingual treebank, we achieved an accu-
racy comparable to the systems trained on human-
annotated bilingual treebanks on the standard test
data. Moreover, our approach continued to pro-
vide improvement over the baseline systems when
we used a much larger monolingual treebank (over
50,000 sentences) where target human translations
are not available and very hard to construct. We also
demonstrated that the proposed approach can be ef-
fective in a purely monolingual setting with the help
of SMT.
Acknowledgments
This study was started when Wenliang Chen, Yu-
jie Zhang, and Yoshimasa Tsuruoka were members
of Language Infrastructure Group, National Insti-
tute of Information and Communications Technol-
ogy (NICT), Japan. We would also thank the anony-
mous reviewers for their detailed comments, which
have helped us to improve the quality of this work.
References
Ann Bies, Martha Palmer, Justin Mott, and Colin Warner.
2007. English Chinese Translation Treebank V 1.0,
LDC2007T02. Linguistic Data Consortium.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of EMNLP 2008, pages 877?886, Honolulu, Hawaii,
October. Association for Computational Linguistics.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 957?961, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson. 2000. BLLIP 1987-
89 WSJ Corpus Release 1, LDC2000T43. Linguistic
Data Consortium.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving dependency
parsing with subtrees from auto-parsed data. In Pro-
ceedings of EMNLP 2009, pages 570?579, Singapore,
August.
Wenliang Chen, Jun?ichi Kazama, and Kentaro Torisawa.
2010. Bitext dependency parsing with bilingual sub-
tree constraints. In Proceedings of ACL 2010, pages
21?29, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of ACL 2007, pages 17?24, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP 2009, pages 1222?
1231, Singapore, August. Association for Computa-
tional Linguistics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
NAACL 2003, pages 48?54. Association for Computa-
tional Linguistics.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL
2010, pages 1?11, Uppsala, Sweden, July. Association
for Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and POS
tagging. In Proceedings of ACL-IJCNLP2009, pages
513?521, Suntec, Singapore, August. Association for
Computational Linguistics.
Charles N. Li and Sandra A. Thompson. 1997. Man-
darin Chinese - A Functional Reference Grammar.
University of California Press.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of NAACL 2006,
pages 104?111, New York City, USA, June. Associa-
tion for Computational Linguistics.
Yang Liu and Liang Huang. 2010. Tree-based and forest-
based translation. In Tutorial Abstracts of ACL 2010,
page 2, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
82
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguisticss, 19(2):313?330.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL 2006, pages 81?88.
Joakim Nivre. 2003. An efficient algorithm for
projective dependency parsing. In Proceedings of
IWPT2003, pages 149?160.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996, pages 133?142.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proceedings of EMNLP 2004, pages
49?56.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT 2003, pages 195?206.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing us-
ing a bilingual lexicon. In Proceedings of ACL-
IJCNLP2009, pages 55?63, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
83
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 825?835,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Relation Acquisition using Word Classes and Partial Patterns
Stijn De Saeger?? Kentaro Torisawa? Masaaki Tsuchida? Jun?ichi Kazama?
Chikara Hashimoto? Ichiro Yamada? Jong Hoon Oh? Istva?n Varga? Yulan Yan?
? Information Analysis Laboratory, National Institute of
Information and Communications Technology, 619-0289 Kyoto, Japan
{stijn,torisawa,kazama,ch,rovellia,istvan,yulan}@nict.go.jp
? Information and Media Processing Laboratories, NEC Corporation, 630-0101 Nara, Japan
m-tsuchida@cq.jp.nec.com
? Human & Information Science Research Division,
NHK Science & Technology Research Laboratories, 157-8510 Tokyo, Japan
yamada.i-hy@nhk.or.jp
Abstract
This paper proposes a semi-supervised rela-
tion acquisition method that does not rely on
extraction patterns (e.g. ?X causes Y? for
causal relations) but instead learns a combi-
nation of indirect evidence for the target re-
lation ? semantic word classes and partial
patterns. This method can extract long tail
instances of semantic relations like causality
from rare and complex expressions in a large
JapaneseWeb corpus? in extreme cases, pat-
terns that occur only once in the entire cor-
pus. Such patterns are beyond the reach of cur-
rent pattern based methods. We show that our
method performs on par with state-of-the-art
pattern based methods, and maintains a rea-
sonable level of accuracy even for instances
acquired from infrequent patterns. This abil-
ity to acquire long tail instances is crucial for
risk management and innovation, where an ex-
haustive database of high-level semantic rela-
tions like causation is of vital importance.
1 Introduction
Pattern based relation acquisition methods rely on
lexico-syntactic patterns (Hearst, 1992) for extract-
ing relation instances. These are templates of natu-
ral language expressions such as ?X causes Y ? that
signal an instance of some semantic relation (i.e.,
causality). Pattern based methods (Agichtein and
Gravano, 2000; Pantel and Pennacchiotti, 2006b;
Pas?ca et al, 2006; De Saeger et al, 2009) learn many
? This work was done when all authors were at the National
Institute of Information and Communications Technology.
such patterns to extract new instances (word pairs)
from the corpus.
However, since extraction patterns are learned us-
ing statistical methods that require a certain fre-
quency of observations, pattern based methods fail
to capture relations from complex expressions in
which the pattern connecting the two words is rarely
observed. Consider the following sentence:
?Curing hypertension alleviates the deteriora-
tion speed of the renal function, thereby lower-
ing the risk of causing intracranial bleeding?
Humans can infer that this sentence expresses a
causal relation between the underlined noun phrases.
But the actual pattern connecting them, i.e., ?Cur-
ing X alleviates the deterioration speed of the re-
nal function, thereby lowering the risk of causing
Y ?, is rarely observed more than once even in a 108
page Web corpus. In the sense that the term pat-
tern implies a recurring event, this expression con-
tains no pattern for detecting the causal relation be-
tween hypertension and intracranial bleeding. This
is what we mean by ?long tail instances? ? words
that co-occur infrequently, and only in sparse extrac-
tion contexts.
Yet an important application of relation extraction
is mining the Web for so-called unknown unknowns
? in the words of D. Rumsfeld, ?things we don?t
know we don?t know? (Torisawa et al, 2010). In
knowledge discovery applications like risk manage-
ment and innovation, the usefulness of relation ex-
traction lies in its ability to find many unexpected
remedies for diseases, causes of social problems,
and so on. To give an example, our relation extrac-
825
tion system found a blog post mentioning Japanese
automaker Toyota as a hidden cause of Japan?s de-
flation. Several months later the same connection
was made in an article published in an authoritative
economic magazine.
We propose a semi-supervised relation extraction
method that does not rely on direct pattern evidence
connecting the two words in a sentence. We argue
that the role of binary patterns can be replaced by a
combination of two types of indirect evidence: se-
mantic class information about the target relation
and partial patterns, which are fragments or sub-
patterns of binary patterns. The intuition is this: if
a sentence like the example sentence above contains
some wordX belonging to the class of medical con-
ditions and another word Y from the class of trau-
mas, and X matches the partial pattern ?. . . causing
X?, there is a decent chance that this sentence ex-
presses a causal relation between X and Y . We
show that just using this combination of indirect
evidence we can pick up semantic relations with
roughly 50% precision, regardless of the complexity
or frequency of the expression in which the words
co-occur. Furthermore, by combining this idea with
a straightforward machine learning approach, the
overall performance of our method is on par with
state-of-the-art pattern based methods. However,
our method manages to extract a large number of
instances from sentences that contain no pattern that
can be learned by pattern induction methods.
Our method is a two-stage system. Figure 1
presents an overview. In Stage 1 we apply a state-
of-the-art pattern based relation extractor to a Web
corpus to obtain an initial batch of relation instances.
In Stage 2 a supervised classifier is built from vari-
ous components obtained from the output of Stage
1. Given the output of Stage 1 and access to a
Web corpus, the Stage 2 extractor is completely
self-sufficient, and the whole method requires no
supervision other than a handful of seed patterns
to start the first stage extractor. The whole proce-
dure is therefore minimally supervised. Semantic
word classes and partial patterns play a crucial role
throughout all steps of the process.
We evaluate our method on three relation acqui-
sition tasks (causation, prevention and material re-
lations) using a 600 million Japanese Web page cor-
Figure 1: Proposed method: data flow.
pus (Shinzato et al, 2008) and show that our sys-
tem can successfully acquire relations from both
frequent and infrequent patterns. Our system ex-
tracted 100,000 causal relations with 84.6% preci-
sion, 50,000 prevention relations with 58.4% preci-
sion and 25,000 material relations with 76.1% preci-
sion. In the extreme case, we acquired several thou-
sand word pairs co-occurring only in patterns that
appear once in the entire corpus. We call such pat-
terns single occurrence (SO) patterns. Word pairs
that co-occur only with SO patterns represent the
theoretical limiting case of relations that cannot be
acquired using existing pattern based methods. In
this sense our method can be seen as complemen-
tary with pattern based approaches, and merging our
method?s output with that of a pattern based method
may be beneficial.
2 Stage 1 Extractor
This section introduces our Stage 1 extractor: the
pattern based method from (De Saeger et al, 2009),
which we call CDP for ?class dependent patterns?.
We give a brief overview below, and refer the reader
to the original paper for a more comprehensive ex-
planation.
CDP takes a set of seed patterns as input, and au-
tomatically learns new class dependent patterns as
paraphrases of the seed patterns. Class dependent
patterns are semantic class restricted versions of or-
dinary lexico-syntactic patterns. Existing methods
use class independent patterns such as ?X causes
Y ? to learn causal relations betweenX and Y . Class
dependent patterns however place semantic class re-
826
strictions on the noun pairs they may extract, like
?Yaccidents causes Xincidents?. The accidents and
incidents subscripts specify the semantic class of the
X and Y slot fillers.
These class restrictions make it possible to distin-
guish between multiple senses of highly ambiguous
patterns (so-called ?generic? patterns). For instance,
given the generic pattern ?Y by X?, if we restrict
Y and X in to the semantic classes of injuries and
accidents (as in ?death by drowning?), the class de-
pendent pattern ?Yinjuries by Xaccidents? becomes a
valid paraphrase of ?X causes Y ? and can safely be
used to extract causal relations, whereas other class
dependent versions of the same generic pattern (e.g.,
?Yproducts byXcompanies?, as in ?iPhone by Apple?)
may not.
CDP ranks each noun pair in the corpus accord-
ing to a score that reflects its likelihood of being
a proper instance of the target relation, by calcu-
lating the semantic similarity of a set of seed pat-
terns to the class dependent patterns this noun pair
co-occurs with. The output of CDP is a list of noun
pairs ranked by score, together with the highest scor-
ing class dependent pattern each noun pair co-occurs
with. This list becomes the input to Stage 2 of our
method, as shown in Figure 1. We adopted CDP as
Stage 1 extractor because, besides having generally
good performance, the class dependent patterns pro-
vide the two fundamental ingredients for Stage 2 of
our method ? the target semantic word classes for a
given relation (in the form of the semantic class re-
strictions attached to patterns), and partial patterns.
To obtain fine-grained semantic word classes we
used the large scale word clustering algorithm from
(Kazama and Torisawa, 2008), which uses the EM
algorithm to compute the probability that a word w
belongs to class c, i.e., P (c|w). Probabilistic cluster-
ing defines no discrete boundary between members
and non-members of a semantic class, so we simply
assume w belongs to c whenever P (c|w) ? 0.2. For
this work we clustered 106 nouns into 500 classes.
Finally, we adopt the structural representation of
patterns introduced in (Lin and Pantel, 2001). All
sentences in our corpus are dependency parsed, and
patterns consist of words on the path of dependency
relations connecting two nouns.
3 Stage 2 Extractor
We use CDP as our Stage 1 extractor, and the top
N noun pairs along with the class dependent pat-
terns that extract them are given as input to Stage 2,
which represents the main contribution of this work.
As shown in Figure 1, Stage 2 consists of three mod-
ules: a candidate generator, a training data gener-
ator and a supervised classifier. The training data
generator builds training data for the classifier from
the top N output of CDP and sentences retrieved
from the Web corpus. This classifier then scores and
ranks the candidate relations generated by the can-
didate relation generator. We introduce each module
below.
Candidate Generator This module generates
sentences containing candidate word pairs for the
target relation from the corpus. It does so using the
semantic class restrictions and partial patterns ob-
tained from the output of CDP. The set of all seman-
tic class pairs obtained from the class dependent pat-
terns that extracted the topN results become the tar-
get semantic class pairs from which new candidate
instances are generated. We extract all sentences
containing a word pair belonging to one of the target
class pairs from the corpus.
From these sentences we keep only those that con-
tain a trace of evidence for the target semantic re-
lation. For this we decompose the class dependent
patterns from the Stage 1 extractor into partial pat-
terns. As mentioned previously, patterns consist of
words on the path of dependency relations connect-
ing the two target words in a syntactic tree. To obtain
partial patterns we split this dependency path into its
two constituent branches, each one leading from the
leaf word (i.e. variable) to the syntactic head of the
pattern. For example, ?X subj?? causes obj?? Y ? is
split into two partial patterns ?X subj?? causes? and
?causes obj?? Y ?. These partial patterns capture the
predicate structures in binary patterns.1 We discard
partial patterns with syntactic heads other than verbs
or adjectives.
The candidate genarator retrieves all sentences
from the corpus in which two nouns belonging to
one of the target semantic classes co-occur and
1 In Japanese, case information is encoded in post-positions
attached to the noun.
827
where at least one of the nouns matches a partial pat-
tern. As shown in Figure 1, these sentences and the
candidate noun pairs they contain (called (noun pair,
sentence) triples hereafter) are submitted to the clas-
sifier for scoring. Restricting candidate noun pairs
by this combination of semantic word classes and
partial pattern matching proved to be quite powerful.
For instance, in the case of causal relations we found
that close to 60% of the (noun pair, sentence) triples
produced by the candidate generator were correct
(Figure 6).
Training Data Generator As shown in Figure 1,
the (noun pair, sentence) triples used as training data
for the SVM classifier were generated from the top
results of the Stage 1 extractor and the corpus. We
consider the noun pairs in the top N output of the
Stage 1 extractor as true instances of the target re-
lation (even though they may contain erroneous ex-
tractions), and retrieve from the corpus all sentences
in which these noun pairs co-occur and that match
one of the partial patterns mentioned above. In our
experiments we set N to 25, 000. We randomly se-
lect positive training samples from this set of (noun
pair, sentence) triples.
Negative training samples are also selected ran-
domly, as follows. If one member of the target noun
pair in the positive samples above matches a partial
pattern but the other does not, we randomly replace
the latter by another noun found in the same sen-
tence, and generate this new (noun pair, sentence)
triple as a negative training sample. In the causal
relation experiments this approach had about 5%
chance of generating false negatives ? noun pairs
contained in the top N results of the Stage 1 extrac-
tor. Such samples were discarded. Our experimen-
tal results show that this scheme works quite well in
practice. We randomly sample M positive and neg-
ative samples from the autogenerated training data
to train the SVM. M was empirically set to 50,000
in our experiments.
SVM Classifier We used a straightforward fea-
ture set for training the SVM classifier. Because
our classifier will be faced with sentences contain-
ing long and infrequent patterns where the distance
between the two target nouns may be quite large,
we did not try to represent lexico-syntactic patterns
as features but deliberately restricted the feature set
to local context features of the candidate noun pair
in the target sentence. Concretely, we looked at bi-
grams and unigrams surrounding both nouns of the
candidate relation, as the local context around the
target words may contain many telling expressions
like ?increase in X? or ?X deficiency? which are use-
ful clues for causal relations. Also, in Japanese case
information is encoded in post-positions attached to
the noun, which is captured by the unigram features.
In addition to these base features, we include the
semantic classes to which the candidate noun pair
belongs, the partial patterns they match in this sen-
tence, and the infix words inbetween the target noun
pair. Note that this feature set is not intended to
be optimal beyond the actual claims of this paper,
and we have deliberately avoided exhaustive fea-
ture engineering so as not to obscure the contribu-
tion of semantic classes and partial pattern to our
approach. Clearly an optimal classifier will incorpo-
rate many more advanced features (see GuoDong et
al. (2005) for a comprehensive overview), but even
without sophisticated feature engineering our clas-
sifier achieved sufficient performance levels to sup-
port our claims. An overview of the feature set is
given in Table 1. The relative contribution of each
type of features is discussed in section 4. In prelim-
inary experiments we found a polynomial kernel of
degree 3 gave the best results, which suggests the ef-
fectiveness of combining different types of indirect
evidence.
The SVM classifier outputs (noun pair, sentence)
triples, ranked by SVM score. To obtain the final
output of our method we assign each unique noun
pair the maximum score from all (noun pair, sen-
tence) triples it occurs in, and discard all other sen-
tences for this noun pair. In section 4 below we eval-
uate the acquired noun pairs in the context of the
sentence that maximizes their score.
4 Evaluation
We demonstrate the effectiveness of semantic word
classes and partial pattern matching for relation ex-
traction by showing that the method proposed in this
paper performs at the level of other state-of-the-art
relation acquisition methods. In addition we demon-
strate that our method can successfully extract re-
lation instances from infrequent patterns, and we
828
Feature type Description Number of features
Morpheme features Unigram and bigram morphemes surrounding both target words. 554,395
POS features Coarse- and fine-grained POS tags of the noun pair and morpheme features. 2,411
Semantic features Semantic word classes of the target noun pair. 1000 (500 classes ?2)
Infix word features Morphemes found inbetween the target noun pair. 94,448
Partial patterns Partial patterns matching the target noun pair. 86
Table 1: Feature set used in the Stage 2 classifier, and their number for the causal relation experiments.
explore several criteria for what constitutes an in-
frequent pattern ? including the theoretical limit-
ing case of patterns observed only once in the en-
tire corpus. These instances are impossible to ac-
quire by pattern based methods. The ability to ac-
quire relations from extremely infrequent expres-
sions with decent accuracy demonstrates the utility
of combining semantic word classes with partial pat-
tern matching.
4.1 Experimental Setting
We evaluate our method on three semantic relation
acquisition tasks: causality, prevention and mate-
rial. Two concepts stand in a causal relation when
the source concept (the ?cause?) is directly or indi-
rectly responsible for the subsequent occurrence of
the target concept (its ?effect?). In a prevention rela-
tion the source concept directly or indirectly acts to
avoid the occurrence of the target concept, and in a
material relation the source concept is a material or
ingredient of the target concept.
For our experiments we used the latest version
of the TSUBAKI corpus (Shinzato et al, 2008),
a collection of 600 million Japanese Web pages
dependency parsed by the Japanese dependency
parser KNP2. In our implementation of CDP, lexico-
syntactic patterns consist of words on the path con-
necting two nouns in a dependency parse tree. We
discard patterns from dependency paths longer than
8 constituent nodes. Furthermore, we estimated pat-
tern frequencies in a subset of the corpus (50 million
pages, or 1/12th of the entire corpus) and discarded
patterns that co-occur with less than 10 unique noun
pairs in this smaller corpus. These restrictions do
not apply to the proposed method, which can extract
noun pairs connected by patterns of arbitrary length,
even if found only once in the corpus. For our pur-
2 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html
pose we treat dependency paths whose observed fre-
quency is below this threshold as insufficiently fre-
quent to be considered as ?patterns?. This threshold
is of course arbitrary, but in section 4 we show that
our results are not affected by these implementation
details.
We asked three human judges to evaluate ran-
dom (noun pair, sentence) triples, i.e. candidate
noun pairs in the context of some corpus sentence
in which they co-occur. If the judges find the sen-
tence contains sufficient evidence that the target re-
lation holds between the candidate nouns, they mark
the noun pair correct. To evaluate the performance
of each method we use two evaluation criteria: strict
(all judges must agree the candidate relation is cor-
rect) and lenient (decided by the judges? majority
vote). Over all experiments the interrater agreement
(Kappa) ranged between 0.57 and 0.82 with an aver-
age of 0.72, indicating substantial agreement (Lan-
dis and Koch, 1977).
4.1.1 Methods Compared
We compare our results to two pattern based
methods: CDP (the Stage 1 extractor) and Espresso
(Pantel and Pennacchiotti, 2006a).
Espresso is a popular bootstrapping based method
that uses a set of seed instances to induce extraction
patterns for the target relation and then acquire new
instances in an iterative bootstrapping process. In
each iteration Espresso performs pattern induction,
pattern ranking and selection using previously ac-
quired instances, and uses the newly acquired pat-
terns to extraction new instances. Espresso com-
putes a reliability score for both instances and pat-
terns based on their pointwise mutual information
(PMI) with the top-scoring patterns and instances
from the previous iteration.3 We refer to (Pantel and
3 In our implementation of Espresso we found that, despite
the many parameters for controlling the bootstrapping process,
829
30%
40%
50%
60%
70%
80%
90%
100%
0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K
Proposed (L)
Proposed (S)
Prop. w/o CDP (L)
Prop. w/o CDP (S)
Prop. w/o pattern (L)
Prop. w/o pattern (S)
Espresso (L)
Espresso (S)CDP (L)CDP (S)
Figure 2: Precision of acquired relations (causality). L
and S denote lenient and strict evaluation.
Pennacchiotti, 2006a) for a more detailed descrip-
tion.
For all methods compared we rank the acquired
noun pairs by their score and evaluated 500 random
samples from the top 100,000 results. For noun pairs
acquired by CDP and Espresso we select the pattern
that extracted this noun pair (in the case of Espresso,
the pattern with the highest PMI for this noun pair),
and randomly select a sentence in which the noun
pair co-occurs with that pattern from our corpus. For
the proposed method we evaluate noun pairs in the
context of the (noun pair, sentence) triple with the
highest SVM score.
4.2 Results and Discussion
The performance of each method on the causality,
prevention and material relations are shown in Fig-
ures 2, 3 and 4 respectively. In the causality exper-
iments (Figure 2) the proposed method performs on
par with CDP for the top 25,000 results, both achiev-
ing close to 90% precision. But whereas CDP?s per-
it remains very difficult to prevent semantic drift (Komachi et
al., 2008) from occurring. One small adjustment to the al-
gorithm stabilized the bootstrapping process considerably and
gave overall better results. In the pattern induction step (sec-
tion 3.2 in (Pantel and Pennacchiotti, 2006a)), Espresso com-
putes a reliability score for each candidate pattern based on the
weighted PMI of the pattern with all instances extracted so far.
As the number of extracted instances increases this dispropor-
tionally favours high frequency (i.e. generic) patterns, so in-
stead of using all instances for computing pattern reliability we
only use the m most reliable instances from the previous iter-
ation, which were used to extract the candidate patterns of the
current iteration (m = 200, like the original).
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K
Proposed (L)
Proposed (S)
Prop. w/o CDP (L)
Prop. w/o CDP (S)
Prop. w/o pattern (L)
Prop. w/o pattern (S)
Espresso (L)
Espresso (S)CDP (L)CDP (S)
Figure 3: Precision of acquired relations (prevention). L
and S denote lenient and strict evaluation.
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K
Proposed (L)
Proposed (S)
Prop. w/o CDP (L)
Prop. w/o CDP (S)
Prop. w/o pattern (L)
Prop. w/o pattern (S)
Espresso (L)
Espresso (S)CDP (L)CDP (S)
Figure 4: Precision of acquired relations (material). L
and S denote lenient and strict evaluation.
formance drops from there our method maintains
the same high precision throughout (84.6%, lenient).
Both our method and CDP outperform Espresso by
a large margin.
For the prevention relation (Figure 3), precision
is considerably lower for all methods except the top
10,000 of CDP (82% precision, lenient). The pro-
posed method peaks at around 20,000 results (67%
precision, lenient) and performance remains more or
less constant from there on. The proposed method
overtakes CDP?s performance around the top 45,000
mark, which suggests that combining the results of
both methods may be beneficial.
In the material relations the proposed method
slightly outperforms both pattern based methods
in the top 10,000 results (92% precision, lenient).
830
However for this relation our method produced only
35,409 instances. The reason is that the top 25,000
results of CDP were all extracted by just 12 patterns,
and these contained many patterns whose syntactic
head is not a verb or adjective (like ?Y rich in X? or
?Y containing X?). Only 12 partial patterns were ob-
tained, which greatly reduced the output of the pro-
posed method. Taking into account the high perfor-
mance of CDP for material relations, this suggests
that for some relations our method?s N and M pa-
rameters could use some tuning. In conclusion, in
all three relations our method performs at a level
comparable to state-of-the-art pattern based meth-
ods, which is remarkable given that it only uses in-
direct evidence.
Dealing with Difficult Extractions How does our
method handle noun pairs that are difficult to ac-
quire by pattern based methods? The graphs marked
?Prop. w/o CDP? (Proposed without CDP) in Fig-
ures 2 , 3 and 4 show the number and precision of
evaluated samples from the proposed method that do
not co-occur in our corpus with any of the patterns
that extracted the top N results of the first stage ex-
tractor. These graphs show that our method is not
simply regenerating CDP?s top results but actually
extracts many noun pairs that do not co-occur in pat-
terns that are easily learned. Figure 2 shows that
roughly two thirds of the evaluated samples are in
this category, and that their performance is not sig-
nificantly worse than the overall result. The same
conclusion holds for the prevention results (Figure
3), where over 80% of the proposed method?s sam-
ples are noun pairs that do not co-occur with eas-
ily learnable patterns. Their precision is about 5%
worse than all samples from the proposed method.
For material relations (Figure 4) about half of all
evaluated samples are in this category, but their pre-
cision is markedly worse compared to all results.
For genuinely infrequent patterns, the graphs
marked ?Prop. w/o pattern? (Proposed without pat-
tern) in Figures 2 , 3 and 4 show the number and
precision of noun pairs evaluated for the proposed
method that were acquired from sentences without
any discernible pattern. As explained in section 4
above, these constitute noun pairs co-occurring in a
sentence in which the path of dependency relations
connecting them is either longer than 8 nodes or can
 0
 5
 10
 15
 20
1 2 32 1024 32768 1.05x106 3.36x107
% o
f al
l sa
mp
les
# of noun pairs co-occurring with patterns
Pattern frequency, CDPPattern frequency, ProposedPattern frequency, Espresso
Figure 5: Frequencies of patterns in the evaluation data
(causation).
extract fewer than 10 noun pairs in 50 million Web
pages. Note that in theory it is possible that these
noun pairs could not be acquired by pattern based
methods due to this threshold ? patterns must be
able to extract more than 10 different noun pairs in
a subset of our corpus, while the proposed method
does not have this constraint. So at least in the-
ory, pattern based methods might be able to acquire
all noun pairs obtained by our method by lowering
this threshold. To see that this is unlikely to be the
case, consider Figure 5, which shows the pattern fre-
quency of the patterns induced by CDP and Espresso
for the causality experiment. The x-axis represents
pattern frequency in terms of the number of unique
noun pairs a pattern co-occurs with in our corpus (on
a log scale), and the y-axis shows the percentage of
samples that was extracted by patterns of a given fre-
quency.4 Figure 5 shows that for the pattern based
methods, the large majority of noun pairs was ex-
tracted by patterns that co-occur with several thou-
sand different noun pairs. Extrapolating the original
frequency threshold of 10 nounpairs to the size of
our entire corpus roughly corresponds to about 120
distinct noun pairs (10 times in 1/12th of the entire
corpus). In Figure 5, the histograms for the pattern
based methods CDP and Espresso start around 1000
noun pairs, which is far above this new lowerbound.
4 In the case of CDP we ignore semantic class restrictions
on the patterns when comparing frequencies. For Espresso, the
most frequent pattern (?Y by X? at the 24,889,329 data point
on the x-axis) extracted up to 53.8% of the results, but the graph
was cut at 20% for readability.
831
Cau
sali
ty
??????? ??????????????????????????????????????????[????]??????
Because ?catecholamine? causes a rapid increase of heart rate, the change of circulation inside the blood vessels leads to blood vessel
disorders and promotes [thrombus generation].
????? ??????????????????????????????????? [????]?????????????
When we injected Xylocaine during a ?tachycardia seizure?, the patient suddenly lost consciousness and fell into a fit of [convulsions].
???????? ????????? ????????? [???]???????????????
(. . . ) The reason is that by taking a lot of ?animal proteins? the causative agents of [tragomaschalia] increase.
*???????????? ?????? ?????????????????????? [???]?
* [Radon] heightens the (body?s) antioxidative function and is effective for eliminating activated oxygen, which is a cause of aging and
?lifestyle-related? diseases.
Pre
ven
tion
???????????????????? ???? ??????????????[???]?????????????
Because the fatty meat of tuna contains DHA and ?EPA? in abundance, it is effective for preventing [neuralgia].
??????? ????? ??????? [????]?????????
If you use ?nitrogen gas? instead of air you may prevent [dust explosions].
??????????? ??????? ???????????????????????? [???]???????????
In ancient Europe ?orthosiphon aristatus? tea was called a ?diet tea?, and supposedly it helps preventing triglycerides and [adult diseases].
* ?? ???????????????????????? [????]????????
* (It) is something that prevents [scratches] on the screen if the ?calash? gets stuck between the screens during storage.
Table 2: Causality and Prevention relations acquired from Single Occurrence (SO) patterns. ?X? and [Y] indicate the
relation instance?s source and target words, and ?*? indicates erroneous extractions.
Thus, pattern based methods naturally tend to induce
patterns that are much more frequent than the range
of patterns our method can capture, and it is unlikely
that this is a result of implementation details like pat-
tern frequency threshold.
The precision of noun pairs in the category ?Prop.
w/o pattern? is clearly lower than the overall re-
sults, but the graphs demonstrate that our method
still handles these difficult cases reasonably well.
The 500 samples evaluated contained 155 such in-
stances for causality, 403 for prevention and 276 for
material. For prevention, the high ratio of these noun
pairs helps explain why the overall performance was
lower than for the other relations.
Finally, the theoretical limiting case for pattern
based algorithms consists of patterns that only co-
occur with a single noun pair in the entire corpus
(single occurrence or SO patterns). Pattern based
methods learn new patterns that share many noun
pairs with a set of reliable patterns in order to extract
new relation instances. If a noun pair that co-occurs
with a SO pattern also co-occurs with more reliable
patterns there is no need to learn the SO pattern. If
that same noun pair does not co-occur with any other
reliable pattern, the SO pattern is beyond the reach
of any pattern induction method. Thus, SO patterns
are effectively useless for pattern based methods.
For the 500 samples evaluated from the causality
and prevention relations acquired by our method we
found 7 causal noun pairs that co-occur only in SO
patterns and 29 such noun pairs for prevention. The
precision of these instances was 42.9% and 51.7%
respectively. In total we found 8,716 causal noun
pairs and 7,369 prevention noun pairs that co-occur
only with SO patterns. Table 2 shows some example
relations from our causality and prevention experi-
ments that were extracted from SO patterns. To con-
clude, our method is able to acquire correct relations
even from the most extreme infrequent expressions.
Semantic Classes, Partial Patterns or Both? In
the remainder of this section we look at how the
combination of semantic word classes and partial
patterns benefits our method. For each relation we
evaluated 1000 random (noun pair, sentence) triples
satisfying the two conditions from section 3 ?
matching semantic class pairs and partial patterns.
Surprisingly, the precision of these samples was
59% for causality, 40% for prevention and 50.4%
for material, showing just how compelling these two
types of indirect evidence are in combination.
To estimate the relative contribution of each
heuristic we compared our candidate generation
method against two baselines. The first baseline
evaluates the precision of random noun pairs from
832
 50
 60
 70
 80
 90
 100
 0  200  400  600  800  1000
pre
cis
ion
 (%
)
(noun pair, sentence) triples ranked by score
Base features onlyAll minus semantic classesAll minus infix wordsAll minus partial patternsAll features
Figure 6: Contribution of feature sets (causality).
 30
 40
 50
 60
 70
 80
 90
 100
 0  200  400  600  800  1000
pre
cis
ion
 (%
)
(noun pair, sentence) triples ranked by score
Base features onlyAll minus semantic classesAll minus infix wordsAll minus partial patternsAll features
Figure 7: Contribution of feature sets (prevention).
the target semantic classes that co-occur in a sen-
tence. The second baseline does the same for the
second heuristic, selecting random sentences con-
taining a noun pair that matches some partial pat-
tern. Evaluating 100 samples for causality and pre-
vention, we found the precision of the semantic class
baseline was 16% for causality and 5% for preven-
tion. The pattern fragment baseline gave 9% for
causality and 22% for prevention. This is consid-
erably lower than the precision of random samples
that satisfy both the semantic class and partial pat-
tern conditions, showing that the combination of se-
mantic classes and partial patterns is more effective
than either one individually.
Finally, we investigated the effect of the various
feature sets used in the classifier. Figures 6, 7 and
8 show the results for the respective semantic re-
lations. The ?Base features? graph shows the per-
 30
 40
 50
 60
 70
 80
 90
 100
 0  200  400  600  800  1000
pre
cis
ion
 (%
)
(noun pair, sentence) triples ranked by score
Base features onlyAll minus semantic classesAll minus infix wordsAll minus partial patternsAll features
Figure 8: Contribution of feature sets (material).
formance the unigram, bigram and part-of-speech
features. ?All features? uses all features in Table
1. The other graphs show the effect of removing
one type of features. These graphs suggest that the
contribution of the individual feature types (seman-
tic class information, partial patterns or infix words)
to the classification performance is relatively minor,
but in combination they do give a marked improve-
ment over the base features, at least for some rela-
tions like causation and material. In other words,
the main contribution of semantic word classes and
partial patterns to our method?s performance lies not
in the final classification step but seems to occur at
earlier stages of the process, in the candidate and
training data generation steps.
5 Related Work
Using lexico-syntactic patterns to extract semantic
relations was first explored by Hearst (Hearst, 1992),
and has inspired a large body of work on semi-
supervised relation acquisition methods (Berland
and Charniak, 1999; Agichtein and Gravano, 2000;
Etzioni et al, 2004; Pantel and Pennacchiotti,
2006b; Pas?ca et al, 2006; De Saeger et al, 2009),
two of which were used in this work.
Some researchers have addressed the sparse-
ness problems inherent in pattern based methods.
Downey et al (2007) starts from the output of
the unsupervised information extraction system Tex-
tRunner (Banko and Etzioni, 2008), and uses lan-
guage modeling techniques to estimate the reliabil-
ity of sparse extractions. Pas?ca et al (2006) alle-
833
viates pattern sparseness by using infix patterns that
are generalized using classes of distributionally sim-
ilar words. In addition, their method employs clus-
tering based semantic similarities to filter newly ex-
tracted instances in each iteration of the bootstrap-
ping process. A comparison with our method would
have been instructive, but we were unable to imple-
ment their method because the original paper con-
tains insufficient detail to allow replication.
There is a large body of research in the super-
vised tradition that does not use explicit pattern rep-
resentations ? kernel based methods (Zelenko et
al., 2003; Culotta, 2004; Bunescu and Mooney,
2005) and CRF based methods (Culotta et al, 2006).
These approaches are all fully supervised, whereas
in our work the automatic generation of candi-
dates and training data is an integral part of the
method. An interesting alternative is distant super-
vision (Mintz et al, 2009), which trains a classi-
fier using an existing database (Freebase) containing
thousands of semantic relations, with millions of in-
stances. We believe our method is more general, as
depending on external resources like a database of
semantic relations limits both the range of seman-
tic relations (i.e., Freebase contains only relations
between named entities, and none of the relations
in this work) and languages (i.e., no resource com-
parable to Freebase exists for Japanese) to which
the technology can be applied. Furthermore, it is
unclear whether distant supervision can deal with
noisy input such as automatically acquired relation
instances.
Finally, inference based methods (Carlson et al,
2010; Schoenmackers et al, 2010; Tsuchida et al,
2010) are another attempt at relation acquisition that
goes beyond pattern matching. Carlson et al (2010)
proposed a method based on inductive logic pro-
gramming (Quinlan, 1990). Schoenmackers et al
(2010) takes relation instances produced by Tex-
tRunner (Banko and Etzioni, 2008) as input and in-
duces first-order Horn clauses, and new instances are
infered using a Markov Logic Network (Richardson
and Domingo, 2006; Huynh and Mooney, 2008).
Tsuchida et al (2010) generated new relation hy-
potheses by substituting words in seed instances
with distributionally similar words. The difference
between these works and ours lies in the treatment
of evidence. While the above methods learn infer-
ence rules to acquire new relation instances from in-
dependent information sources scattered across dif-
ferent Web pages, our method takes the other option
of working with all the clues and indirect evidence a
single sentence can provide. In the future, a combi-
nation of both approaches may prove beneficial.
6 Conclusion
We have proposed a relation acquisition method that
is able to acquire semantic relations from infrequent
expressions by focusing on the evidence provided by
semantic word classes and partial pattern matching
instead of direct extraction patterns. We experimen-
tally demonstrated the effectiveness of this approach
on three relation acquisition tasks, causality, preven-
tion and material relations. In addition we showed
our method could acquire a significant number of
relation instances that are found in extremely infre-
quent expressions, the most extreme case of which
are single occurrence patterns, which are beyond
the reach of existing pattern based methods. We be-
lieve this ability is of crucial importance for acquir-
ing valuable long tail instances. In future work we
will investigate whether the current framework can
be extended to acquire inter-sentential relations.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In Proc. of the fifth ACM conference on Digital li-
braries, pages 85?94.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proc. of the 46th ACL-08:HLT, pages 28?36.
Matthew Berland and Eugene Charniak. 1999. Find-
ing parts in very large corpora. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics, pages 57?64, College Park, Mary-
land, USA, June.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of the Conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing (HLT ?05), pages 724?731.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for neverend-
ing language learning. In Proc of the 24th AAAI, pages
1306?1313.
834
Aron Culotta, Andrew McCallum, and Jonathan Betz.
2006. Integrating probabilistic extraction models and
data mining to discover relations and patterns in text.
In Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics (HLT/NAACL), pages 296?303.
Aron Culotta. 2004. Dependency tree kernels for rela-
tion extraction. In In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL-04, pages 423?429.
Stijn De Saeger, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large Scale
Relation Acquisition Using Class Dependent Patterns.
In Proc. of the 9th International Conference on Data
Mining (ICDM), pages 764?769.
Doug Downey, Stefan Schoenmackers, and Oren Etzioni.
2007. Sparse information extraction: Unsupervised
language models to the rescue. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL2007).
Oren Etzioni, Michael Cafarella, Doug Downey, Stanley
Kok, Ana-Maria Popescu, Tal Shaked, Stephen Soder-
land, Daniel Weld, and Alexander Yates. 2004. Web-
scale information extraction in KnowItAll. In Proc. of
the 13th international conference on World Wide Web
(WWW04), pages 100?110.
Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation extrac-
tion. In Proc. of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, ACL ?05, pages
419?444.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 14th In-
ternational Conference on Computational Linguistics
(COLING?92), pages 539?545.
Tuyen N. Huynh and Raymond J. Mooney. 2008.
Discriminative structure and parameter learning for
markov logic networks. In Proc. of the 25th ICML,
pages 416?423.
Jun?ichi Kazama and Kentaro Torisawa. 2008. Inducing
gazetteers for named entity recognition by large-scale
clustering of dependency relations. In Proc. of the
46th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL-08: HLT), pages 407?415.
Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of se-
mantic drift in espresso-like bootstrapping algorithms.
In Proc. of EMNLP?08. Honolulu, USA, pages 1011?
1020.
Dekang Lin and Patrick Pantel. 2001. Dirt - discovery
of inference rules from text. In Proc. of the ACM
SIGKDD Conference on Knowledge Discovery and
Data Mining, pages 323?328.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proc. of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 1003?1011.
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Names and Similarities on
the Web: Fact Extraction in the Fast Lane. In Proc. of
the COLING-ACL06, pages 809?816.
Patrick Pantel and Marco Pennacchiotti. 2006a.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proc. of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics (COLING-ACL-06,
pages 113?120.
Patrick Pantel and Pennacchiotti Pennacchiotti, Marco.
2006b. Espresso: Leveraging generic patterns for au-
tomatically harvesting semantic relations. In Proc. of
the COLING-ACL06, pages 113?120.
J. R. Quinlan. 1990. Learning logical definitions from
relations. Machine Learning, 5(3):239?266.
Matthew Richardson and Pedro Domingo. 2006.
Markov logic networks. Machine Learning, 26:107?
136.
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,
and Jesse Davis. 2010. Learning first-order horn
clauses from web text. In Proc. of EMNLP2010, pages
1088?1098.
Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,
Chikara Hashimoto, and Sadao Kurohashi. 2008.
TSUBAKI: An open search engine infrastructure for
developing new information access. In Proc. of IJC-
NLP, pages 189?196.
Kentaro Torisawa, Stijn De Saeger, Jun?ichi Kazama,
Asuka Sumida, Daisuke Noguchi, Yasunari Kakizawa,
Masaaki Murata, Kow Kuroda, and Ichiro Yamada.
2010. Organizing the web?s information explosion to
discover unknown unknowns. New Generation Com-
puting, 28(3):217?236.
Masaaki Tsuchida, Stijn De Saeger, Kentaro Torisawa,
Masaki Murata, Jun?ichi Kazama, Kow Kuroda, and
Hayato Ohwada. 2010. Large scale similarity-based
relation expansion. In Proc of the 4th IUCS, pages
140?147.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. Journal of Machine Learning Research,
pages 1083?1106.
835
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 368?378, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Why Question Answering using Sentiment Analysis and Word Classes
Jong-Hoon Oh? Kentaro Torisawa? Chikara Hashimoto ?
Takuya Kawada? Stijn De Saeger? Jun?ichi Kazama? Yiou Wang??
Information Analysis Laboratory
Universal Communication Research Institute
National Institute of Information and Communications Technology (NICT)
{? rovellia,? torisawa,? ch,? tkawada,?stijn,? kazama,??wangyiou}@nict.go.jp
Abstract
In this paper we explore the utility of sen-
timent analysis and semantic word classes
for improving why-question answering on a
large-scale web corpus. Our work is moti-
vated by the observation that a why-question
and its answer often follow the pattern that if
something undesirable happens, the reason is
also often something undesirable, and if some-
thing desirable happens, the reason is also of-
ten something desirable. To the best of our
knowledge, this is the first work that intro-
duces sentiment analysis to non-factoid ques-
tion answering. We combine this simple idea
with semantic word classes for ranking an-
swers to why-questions and show that on a set
of 850 why-questions our method gains 15.2%
improvement in precision at the top-1 answer
over a baseline state-of-the-art QA system that
achieved the best performance in a shared task
of Japanese non-factoid QA in NTCIR-6.
1 Introduction
Question Answering (QA) research for factoid ques-
tions has recently achieved great success as demon-
strated by IBM?s Watson at Jeopardy: its accuracy
has been reported to be around 85% on factoid ques-
tions (Ferrucci et al2010). Although recent shared
QA tasks (Voorhees, 2004; Pe?as et al2011; Fuku-
moto et al2007) have stimulated the research com-
munity to move beyond factoid QA, comparatively
little attention has been paid to QA for non-factoid
questions such as why questions and how to ques-
tions, and the performance of the state-of-art non-
factoid QA systems reported in the literature (Mu-
rata et al2007; Surdeanu et al2011; Verberne et
al., 2010) remains considerably lower than that of
factoid QA (i.e., 34% in MRR at top-150 results on
why-questions (Verberne et al2010)).
In this paper we explore the utility of sentiment
analysis (Pang et al2002; Turney, 2002; Nakagawa
et al2010) and semantic word classes for improv-
ing why-question answering (why-QA) on a large-
scale web corpus. The inspiration behind this work
is the observation that why-questions and their an-
swers often have the following tendency:
? if something undesirable happens, the reason is
often also something undesirable, and
? if something desirable happens, its reason is of-
ten also desirable.
Consider the following question Q1, and its an-
swer candidates A1-1 and A1-2.
? Q1: Why does cancer occur?
? A1-1: Carcinogens such as nitrosamine and
benzopyrene may increase the risk of cancer by
altering DNA in cells.
? A1-2: Maintaining a healthy weight may lower
the risk of various types of cancer.
Here A1-1 describes an undesirable event related to
cancer, while A1-2 suggests a desirable action for
its prevention. Our hypothesis suggests that A1-1
is more appropriate for answering Q1. If this hy-
pothesis holds, we can obtain a significant improve-
ment in performance on why-QA tasks by exploiting
the sentiment orientation1 of expressions obtainable
1 In this paper we denote the desirable/undesirable polar-
ity of an expression by the term ?sentiment orientation? instead
of ?semantic orientation? to avoid confusion with our different
notion of ?semantic word classes.?
368
by automatic sentiment analysis of questions and an-
swers.
A second observation motivating this work is that
there are often significant associations between the
lexico-semantic classes of words in a question and
those in its answer sentence. For instance, questions
concerning diseases like Q1 often have answers that
include references to specific semantic word classes
such as chemicals (like A1-1), viruses, body parts,
and so on. Capturing such statistical correlations be-
tween diseases and harmful substances may lead to
higher why-QA performance. For this purpose we
use classes of semantically similar words that were
automatically acquired from a large web corpus us-
ing an EM-based clustering method (Kazama and
Torisawa, 2008).
Another issue is that simply introducing the sen-
timent orientation of words or phrases in question
and answer sentences in a naive way is insufficient,
since answer candidate sentences may contain mul-
tiple sentiment expressions with different polarities
in answer candidates (i.e., about 33% of correct an-
swers had such multiple sentiment expressions with
different polarities in our test set). For example, if
A1-2 contained a second sentiment expression with
negative polarity like the example below,
?Trusting a specific food is not effective
for preventing cancer, but maintaining a
healthy weight may help lower the risk of
various types of cancer.?
both A1-1 and A1-2 would contain sentiment ex-
pressions with the same polarity as that of Q1. Thus,
it is difficult to expect that the sentiment orientation
alone will work well for recognizing A1-1 as a cor-
rect answer to Q1. To address this problem, we con-
sider the combination of sentiment polarity and the
contents of sentiment expressions associated with
the polarity in questions and their answer candidates
as well. To deal with the data sparseness problem
arising in using the content of sentiment expressions,
we developed a feature set that combines the polar-
ity and the semantic word classes effectively.
We exploit these two main ideas (concerned with
the sentiment orientation and the semantic classes
described so far) for training a supervised classi-
fier to rank answer candidates to why-questions.
Through a series of experiments on 850 Japanese
why-questions, we showed that the proposed seman-
tic features were effective in identifying correct an-
swers, and our proposed method obtained more than
15% improvement in precision of its top answer
(P@1) over our baseline, which achieved the best
performance in the non-factoid QA task in NTCIR-
6 (Murata et al2007). We also show that our
method can potentially perform with high precision
(64.8% in P@1) when answer candidates containing
at least one correct answer are given to our re-ranker.
2 Approach
Our proposed method is composed of answer re-
trieval and answer re-ranking. The first step, an-
swer retrieval, extracts a set of answer candidates to
a why-question from 600 million Japanese Web cor-
pus. The answer retrieval is our implementation of
the state-of-art method that has shown the best per-
formance in the shared task of Japanese non-factoid
QA in NTCIR-6 (Murata et al2007; Fukumoto et
al., 2007). The second step, answer re-ranking, is
the focus of this work.
2.1 Answer Retrieval
We use Solr2 to retrieve documents from a 600 mil-
lion Japanese Web page corpus3for a given why-
question. Let a set of content words in a why-
question be T = {t1, ? ? ? , tn}. Two boolean queries
for a why-question, ?t1 AND ? ? ? AND tn? and ?t1
OR ? ? ? OR tn,? are given to Solr and top-300 doc-
uments for each query are retrieved. Note that re-
trieved documents by each query have different cov-
erage and relevance to a given why-question. To
keep balance between the coverage and relevance of
retrieved documents, we use a set of retrieved doc-
uments by these two queries for obtaining answer
candidates. Each document in the result of docu-
ment retrieval is split into a set of answer candi-
dates consisting of five subsequent sentences4. Sub-
sequent answer candidates can share up to two sen-
tences to avoid errors due to wrong document seg-
mentation.
2 http://lucene.apache.org/solr
3 To the best of our knowledge, few Japanese non-factoid
QA systems in the literature have used such a large-scale cor-
pus.
4 The length of acceptable answer candidates for why-
QA in the literature ranges from one sentence to two para-
graphs (Fukumoto et al2007; Murata et al2007; Higashinaka
and Isozaki, 2008; Verberne et al2007; Verberne et al2010).
369
Answer candidate ac for question q is ranked
according to scoring function S(q, ac) given in
Eq. (1) (Murata et al2007). Murata et al2007)?s
method uses text search to look for answer candi-
dates containing terms from the question with ad-
ditional clue terms referring to ?reason? or ?cause.?
Following the original method we used riyuu (rea-
son), genin (cause) and youin (cause) as clue terms.
The top-20 answer candidates for each question are
passed on to the next step, which is answer re-
ranking. S(q, ac) assigns a score to answer candi-
dates like tf -idf , where 1/dist(t1, t2) functions like
tf and 1/df(t2) is idf for given terms t1 and t2 that
are shared by q and ac.
S(q, ac) = maxt1?T
?
t2?T
?? log(ts(t1, t2)) (1)
ts(t1, t2) =
N
2? dist(t1, t2)? df(t2)
Here T is a set of terms including nouns, verbs, and
adjectives in question q that appear in answer can-
didate ac. Note that the clue terms are added to T
if they exist in ac. N is the total number of docu-
ments (600 million), dist(t1, t2) represents the dis-
tance (the number of characters) between t1 and t2
in answer candidate ac, df(t) is the document fre-
quency of term t, and ? ? {0, 1} is an indicator,
where ? = 1 if ts(t1, t2) > 1, ? = 0 otherwise.
2.2 Answer Re-ranking
Our re-ranker is a supervised classifier (SVMs)
(Vapnik, 1995) that uses three types of feature
sets: features expressing morphological and syn-
tactic analysis of questions and answer candidates,
features representing semantic word classes appear-
ing in questions and answer candidates, and features
from sentiment analysis. All answer candidates of a
question are ranked in a descending order of their
score given by SVMs. We trained and tested the
re-ranker using 10-fold cross validation on a cor-
pus composed of 850 why-questions and their top-
20 answer candidates provided by the answer re-
trieval procedure in Section 2.1. The answer candi-
dates were manually annotated by three human an-
notators (not by the authors). Our corpus construc-
tion method is described in more detail in Section 4.
3 Features for Answer Re-ranking
This section describes our feature sets for answer
re-ranking: features expressing morphological and
syntactic analysis (MSA), features representing se-
mantic word class (SWC), and features indicat-
ing sentiment analysis (SA). MSA, which has been
widely used for re-ranking answers in the literature,
is used to identify associations between questions
and answers at the morpheme, word phrase, and syn-
tactic dependency levels. The other two feature sets
are proposed in this paper. SWC is devised for iden-
tifying semantic word class associations between
questions and answers. SA is used for identify-
ing sentiment orientation associations between ques-
tions and answers as well as expressing the combi-
nation of each sentiment expression and its polarity.
Table 1 summarizes the respective feature sets, each
of which is described in detail below.
3.1 Morphological and Syntactic Analysis
MSA including n-grams of morphemes, words, and
syntactic dependencies has been widely used for re-
ranking answers in non-factoid QA (Higashinaka
and Isozaki, 2008; Surdeanu et al2011; Verberne
et al2007; Verberne et al2010). We use MSA as
a baseline feature set in this work.
We represent all sentences in a question and
its answer candidate in three ways: morphemes,
word phrases (bunsetsu5) and syntactic dependency
chains. These are obtained using a morphological
analyzer6 and a dependency parser7. From each
question and answer candidate we extract n-grams
of morphemes, word phrases, and syntactic depen-
dencies, where n ranges from 1 to 3. Syntactic de-
pendency n-grams are defined as a syntactic depen-
dency chain containing n word phrases. Syntactic
dependency 1-grams coincide with word phrase 1-
grams, so they are ignored.
Table 1 defines four types of MSA (MSA1 to
MSA4). MSA1 is n-gram features from all sen-
tences in a question and its answer candidates and
distinguishes an n-gram feature found in a ques-
tion from that same feature found in answer candi-
dates. MSA2 contains n-grams found in the answer
5 A bunsetsu is a syntactic constituent composed of a content
word and several function words such as post-positions and case
markers. It is the smallest unit of syntactic analysis in Japanese.
6 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
7 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP
370
MSA1 Morpheme n-grams, word phrase n-grams, and syntactic dependency n-grams in a question and its answer candidate, where n ranges
from 1 to 3. n-grams in a question and those in an answer candidate are distinguished.
MSA2 MSA1?s n-grams in an answer candidate that contain a question term.
MSA3 MSA1?s n-grams that contain a clue term including riyuu (reason), genin (cause) and youin (cause). These n-grams in a question and
those in an answer candidate are distinguished.
MSA4 The ratio of the number of question terms in an answer candidate to the total number of question terms.
SWC1 Word class n-grams in a question and its answer candidate. These n-grams in a question and those in an answer candidate are distin-
guished.
SWC2 SWC1?s n-grams in an answer candidate whose original MSA1?s n-grams contain any question term.
SA@W1 Word polarity n-grams in a question and its answer candidate. These n-grams in a question and those in an answer candidate are
distinguished.
SA@W2 SA@W1?s n-grams in an answer candidate whose original MSA1 n-grams contain any question term.
SA@W3 Joint class-polarity n-grams in a question and its answer candidate. These n-grams in a question and those in an answer candidate are
distinguished.
SA@W4 SA@W3?s n-grams in an answer candidates whose original MSA1 n-grams contain any question term.
SA@P1 The indicator for polarity agreement between sentiment phrases, one in a question and the other in an answer candidate: 1 if any pair of
such sentiment phrases has polarity in agreement, 0 otherwise.
SA@P2 The phrase-polarity, positive or negative, of a pair of sentiment phrases for which the indicator in SA@P1 is 1.
SA@P3 Morpheme n-grams, word phrase n-grams, and syntactic dependency n-grams in sentiment phrases are coupled with their phrase-polarity,
where n ranges from 1 to 3. These n-grams in a question and those in an answer candidate are distinguished.
SA@P4 SA@P3?s n-grams in an answer candidates that contain a question term.
SA@P5 The ratio of the number of question terms in sentences that have sentiment phrases in answer candidates to the total number of question
terms.
SA@P6 Word class n-grams in sentiment phrases are coupled with phrase-polarity. These n-grams in a question and those in an answer candidate
are distinguished.
SA@P7 SA@P6?s n-grams in an answer candidates, whose original MSA1?s n-grams include any question term.
SA@P8 Joint class-polarity n-grams in sentiment phrases of a question and its answer candidate are coupled with phrase-polarity of the sentiment
phrases. These n-grams in a question and those in an answer candidate are distinguished.
SA@P9 SA@P8?s n-grams in an answer candidates, whose original MSA1?s n-grams include any question term.
SA@P10 A pair of SA@P6?s n-grams, one from sentiment phrases in a question and the other from those in an answer candidate, where the two
sentiment phrases should have the same sentiment orientation.
Table 1: Features used in training our re-ranker
candidates that themselves contain a term from the
question (e.g., ?types of cancer? in example A1-2).
MSA3 is the n-gram feature that contains one of the
clue terms used for answer retrieval (riyuu (reason),
genin (cause) or youin (cause)). Here too, n-grams
obtained from the questions and answer candidates
are distinguished. Finally, MSA4 is the percentage
of the question terms found in an answer candidate.
3.2 Semantic Word Class
Semantic word classes are sets of semantically simi-
lar words. We construct these semantic word classes
by using the noun clustering algorithm proposed in
Kazama and Torisawa (2008). The algorithm fol-
lows the distributional hypothesis, which states that
semantically similar words tend to appear in simi-
lar contexts (Harris, 1954). By treating syntactic de-
pendency relations between words as ?contexts,? the
clustering method defines a probabilistic model of
noun-verb dependencies with hidden classes as:
p(n, v, r) =
?
c
p(n|c)p(?v, r?|c)p(c) (2)
Here, n is a noun, v is a verb or noun on which n de-
pends via a grammatical relation r (post-positions in
Japanese), and c is a hidden class. Dependency rela-
tion frequencies were obtained from our 600-million
page web corpus, and model parameters p(n|c),
p(?v, r?|c) and p(c) were estimated using the EM
algorithm (Hofmann, 1999). We successfully clus-
tered 5.5 million nouns into 500 classes. For each
noun n, EM clustering estimates a probability dis-
tribution over hidden variables representing seman-
tic classes. From this distribution we obtained dis-
crete semantic word classes by assigning each noun
n to semantic class c = argmaxc? p(c?|n). The
resulting classes actually form clean semantic cat-
egories such as chemicals, nutrients, diseases and
conditions, in our examples of Q1 and Q2. The fol-
lowing are the top-10 words (English translation) ac-
cording to p(c|n) for these classes.
chemicals: acetylene, hydrogenation product,
phosphoric monoester, methacrylate, levoglu-
cosan, ammonium salt, halogenated organic
compound, benzonitrile, alkyne, nitrosamine
371
nutrients: glucide, carbonhydrate, mineral, salt,
sugar, water, fat, vitamin, nutrients, protein
diseases: pneumonia, neuritis, cancer, oral leuko-
plakia, pachymeningitis, acidosis, encephalitis,
abdominal injury, valvulitis, gingivitis
conditions: proficiency, decrepitude, deficiency,
impurity, abnormalities, floated, crisis, dis-
placement, condition, shortage
Semantic word class (SWC) features are used to
capture associations between semantic classes of
words in the question and those in the answer candi-
dates. For example:
? Q2: Why does rickets (Wdisease) occur in chil-
dren?
? A2: Deficiency (Wcondition) of vitamin D
(Wnutrients) can cause rickets (Wdisease).
Wcondition, Wdisease and Wnutrients represent se-
mantic word classes of conditions, diseases and nu-
trients, respectively. If this question-answer pair is
given to the classifier as a positive training sample,
we expect it to learn that if a disease name appears
in a question then, everything else being equal, an-
swers including nutrient names are more likely to be
correct. Note that in principle the same association
could be learned between word pairs, i.e., rickets and
vitamin D. However, we found that word level asso-
ciations are often too specific, and because of data
sparseness this knowledge cannot easily be general-
ized to unseen questions. This is our main motiva-
tion for introducing broad coverage semantic word
classes into the feature set.
We call the feature set with the word classes SWC
and use two types of SWC, as shown in Table 1. To
obtain the first type (SWC1), we convert all nouns
in the MSA1 n-grams into their respective word
classes, and keep all n-grams that contain at least
one word class. We call these features word class
n-grams. Again, word class n-grams obtained from
questions are distinguished from the ones in answer
candidates. For example, we extract ?Wdisease oc-
cur? as a word class 2-gram from Q2.
The second type of SWC, SWC2, represents word
class n-grams in an answer candidate, in which
question terms are replaced by their respective se-
mantic word classes. For example, Wdisease in word
class 2-gram ?cause Wdisease? from A2 is the se-
mantic word class of rickets, one of the question
terms. These features capture the correspondence
between semantic word classes in the question and
answer candidates.
3.3 Sentiment Analysis
Sentiment analysis (SA) features are classified into
word-polarity and phrase-polarity features. We use
opinion extraction tool8 and sentiment orientation
lexicon in the tool for these features.
3.3.1 Opinion Extraction Tool
Opinion extraction tool is a software, the im-
plementation of Nakagawa et al2010). It ex-
tracts linguistic expressions representing opinions
(henceforth, we call them sentiment phrases) from
a Japanese sentence and then identifies the polarity
of these sentiment phrases using machine learning
techniques. For example, rickets occur in Q2 and
Deficiency of vitamin D can cause rickets in A2 can
be identified as sentiment phrases with a negative
polarity. The tool identifies sentiment phrases and
their polarity by using polarities of words and de-
pendency subtrees as evidence, where these polari-
ties are given in a word polarity dictionary.
In this paper, we use a trained model and a word
polarity dictionary (containing about 35,000 entries)
distributed via the ALAGIN forum9 for our sen-
timent analysis. Table 2 shows the performance
of opinion extraction tool, precision (P), recall (R)
and F-value (F), in this setting (reported in the
Japanese homepage of this tool). In the evaluation of
sentiment-phrase extraction, an extracted sentiment
phrase is determined as correct if its head word is
the same as one in the gold standard. Polarity clas-
sification is evaluated under the condition that all of
the sentiment phrases are correctly extracted.
P R F
Sentiment-phrase extraction 0.602 0.408 0.486
Polarity classification (pos.) 0.873 0.893 0.883
Polarity classification (neg.) 0.866 0.842 0.854
Table 2: The performance of opinion extraction tool
3.3.2 Word Polarity (SA@W)
Polarities of words are identified by simply look-
ing up the word polarity dictionary of opinion ex-
8 Available at http://alaginrc.nict.go.jp/opinion/index_e.html
9 http://www.alagin.jp/index-e.html. Only the members of
the ALAGIN forum can access these resources.
372
traction tool. Word polarity features are used
for identifying associations between the polarity of
words in a question and that in a correct answer. For
example:
? Q2: Why does rickets (W?) occur in children?
? A2: Deficiency (W?) of vitamin D can cause
rickets (W?).
Here, W? represents negative word polarities. We
expect our classifier to learn from this question and
answer pair that if a word with negative polarity ap-
pears in a question then its correct answer is likely
to contain a negative polarity word as well.
SA@W1 and SA@W2 in Table 1 are sentiment
analysis features from word polarity n-grams, which
contain at least one word that has word polarities.
We obtain these n-grams by converting all nouns in
MSA1 n-grams into their word polarities through
dictionary lookup. For example, from Q2 in the
above example we extract ?W? occur? as a word
polarity 2-gram. SA@W1 is concerned with all
word polarity n-grams in questions and answer can-
didates. For SA@W2, we restrict word polarity
n-grams from SA@W1 to those whose original n-
gram include a question term.
Furthermore, word polarities are coupled with se-
mantic word classes so that our classifier can iden-
tify meaningful combinations of both. For example,
deficiency in A2 can be represented asW?condition by
its respective semantic word class and word polar-
ity, which allows for the representation of undesir-
able conditions. This in turn lets our system learn
meaningful correlations between words expressing
these kind of negative conditions and their connec-
tion to questions asking about diseases. SA@W3
and SA@W4 are features from this combination.
They are defined in the same way as SA@W1 and
SA@W2 except that word polarities are replaced
with the combination of semantic word classes and
word polarities. We call n-grams in SA@W3 and
SA@W4 joint (word) class-polarity n-grams.
3.3.3 Phrase Polarity (SA@P)
Opinion extraction tool is applied to question and
its answer candidate to identify sentiment phrases
and their phrase-polarities. In preliminary tests we
found that sentiment phrases do not help to iden-
tify correct answers if answer sentences including
the sentiment phrases do not have any term from the
question. So we restrict the target sentiment phrases
to those acquired from sentences containing at least
one question term. From these sentiment phrases we
extract three categories of features.
First, SA@P1 and SA@P2 are features concerned
with phrase-polarity agreement between sentiment
phrases in a question and its answer candidate. We
consider all possible pairs of sentiment phrases from
the question and answer. If any such pair agrees
in phrase-polarity, an indicator for the agreement
and its polarity in the agreement become features
SA@P1 and SA@P2, respectively.
Secondly, following the original hypothesis un-
derlying this paper, we assume that sentiment
phrases often represent the core part of the cor-
rect answer (e.g., A2 above) and it is important
to express the content of the sentiment phrases in
features. SA@P3 and SA@P4 were devised for
this purpose. SA@P3 represents this sentiment
phrase contents as n-grams of morphemes, words,
and syntactic dependencies of sentiment phrases,
together with their phrase-polarity. Furthermore,
SA@P4 is the subset of SA@P3 n-grams restricted
to those that include terms found in the question,
and SA@P5 indicates the percentage of sentiment
n-grams from the question that are found in a given
answer candidate.
Finally, features SA@P6 through SA@P9 use se-
mantic word classes to generalize the content fea-
tures mentioned above. These features consist of
word class n-grams and joint class-polarity n-grams
taken from sentiment phrases, together with their
phrase polarity. Similar to the definition of SA@P4,
for SA@P7 and SA@P9 we restrict ourselves to n-
grams containing a question term. SA@P10 repre-
sents the semantic content of two sentiment phrases
with the same sentiment orientation (one from a
question and the other from an answer candidate)
using word class n-grams, together with the phrase-
polarity in agreement.
4 Test Set
We prepared three sets of why-questions (QS1, QS2
and QS3) and used these questions to build two test
sets for our experiments.
Why-questions in QS1 are taken from the
Japanese version of Yahoo! Answers (called Ya-
hoo! Chiebukuro)10. We automatically extracted
10 We used ?Yahoo! Chiebukuro Data (2nd edition)? which is
373
questions consisting of a single sentence and con-
taining the interrogative naze (why), and our anno-
tators verified that these questions are meaningful
without further context. For example, they discarded
questions like ?Why doesn?t the WBC (world box-
ing council) make an objection to the WBC (World
baseball classic)?? (the object of the objection is
unclear) and ?Why do minors trade at the auction
even though it is disallowed by the rules? (informa-
tion about which auction is not provided).
Because questions in Yahoo! Answers are aimed
at human readers, users often ?set the stage? by giv-
ing lots of background information about their ques-
tion. This often leads to large stylistic differences
between the questions in Yahoo! Answers and those
typically posed to a QA system. We therefore cre-
ated a second set of why-questions, QS2, whose
style should be more appropriate for a QA system
(examples showing these differences are given in the
supplementary materials of this paper). Six human
annotators (not the authors) were asked to create
why-questions in their own words, keeping in mind
that the questions they create are for a QA system. In
addition, the annotators were asked to verify on the
Web that the questions they created ask about some
real event or phenomena. For example, a question
like ?Why does Mars appear blue?? is disallowed in
QS2 because ?Mars appears blue? is false. Note that
the correct answer to these questions does not have
to be either in our target corpus or in real-world Web
texts. These two sets of why-questions, QS1 and
QS2, are used to build a test set for evaluating our
proposed method.
Finally, QS3 contains why-questions that have at
least one answer in our target corpus (600 million
Japanese Web page corpus). For creating such why-
questions, four human annotators (not the authors)
were given a text passage composed of three contin-
uous sentences and asked to locate the reasons for
some event as described in this passage. Then they
created a why-question for which the description is a
correct answer. Because randomly selected passages
from our target corpus have little chance of generat-
ing good why-questions we extracted passages from
our target corpus that include at least one of the clue
terms used in our answer retrieval step (i.e. riyuu
(reason), genin (cause), or youin (cause)). This set-
provided by Yahoo Japan Corporation and contains 16 million
questions asked from April, 2004 to April 2009.
ting may not necessarily reflect a ?real world? dis-
tribution of why-questions, in which ideally a wide
range of people ask questions that may or may not
have an answer in our corpus. However, QS3 al-
lows us to evaluate our method under the idealized
conditions where we have a perfect answer retrieval
module whose answer candidates always contain at
least one correct answer (the source passage used
for creating the why-question). This setting allows
us to estimate the ideal-case performance of our
method. Under these circumstances we found that
our method achieves almost 65% precision in P@1,
which suggests that it can potentially perform with
high precision if the answer candidates given by the
answer retrieval module contain at least one correct
answer. This is the main purpose of QS3. Addition-
ally, we use QS3 for building training data, to check
whether questions that do not reflect the real-world
distribution of why-questions are useful for improv-
ing the system?s performance on ?real-world? ques-
tions (see Section 5.1).
In addition, we checked QS1, QS2 and QS3 for
questions having the same topic, to avoid the pos-
sibility that the distribution of questions is biased
towards certain topics. We manually extracted the
questions? topic words and randomly selected a sin-
gle representative question from all questions with
the same topic. For example, ?Why does Twitter
only allow 140 characters?? and ?Why is Twitter
so popular?? both have as topic Twitter. In the end
we obtained 250 questions in QS1, 250 questions in
QS2 and 350 questions in QS3.
For evaluation we prepared two test sets, Set1 and
Set2. Set1 contains question-answer pairs whose
questions are taken from QS1 and QS2. In our ex-
periment, we evaluate systems with 10-fold cross
validation on Set1. Set2 has question-answer pairs
whose questions are from QS3. Set2 is mainly used
for estimating estimate the ideal-case performance
of our method with a perfect answer retrieval mod-
ule. Furthermore Set2 is used as additional training
data in evaluating systems with 10-fold cross vali-
dation on Set1. We used our answer retrieval sys-
tem to obtain the top-20 answer candidates for each
question, and all question-answer (candidate) pairs
were checked by three annotators, where their inter-
rater agreement (Fleiss? kappa) was 0.634, indicat-
ing substantial agreement. Finally, correct answers
to each question were determined by majority vote.
374
Q1:???????????????????????????????????????????
(Why does the increase of greenhouse gases such as carbon dioxide in the atmosphere lead to a rise of ocean level?)
A1: .. ????????????????????????????????????????????????????????
??????????????????????????????????????? ... ???????????????????
???????????????????????
(The burning of fossil fuels contributes to the increase of atmospheric concentrations of greenhouse gases and this makes the atmosphere absorb more
thermal radiation. As a result, Earth?s average surface temperature increases. This is global warming. ... There are warnings that the increase of sea
water and melting of polar ice due to the global warming may cause sea-surface height to rise by 9?88 cm on average.
Q2:?????????????????????????????
(Why does hemoglobin deficiency cause lack of oxygen in the human body?)
A2:... ????????????????????????????????????????????????????????
?????????????????????????????????????????????????????????..
(... Hemoglobin has an important role in the human body of carrying oxygen to the organs and transferring carbon dioxide back to the lungs, to be
dispensed from the organism. If the amount of hemoglobin produced by the body is insufficient due to iron deficiency, the amount of oxygen delivered
throughout the body decreases, causing oxygen deficiency. ... )
Table 3: Correct question-answer pairs in our test set
Table 3 shows a sample of correct question-answer
pairs in our test set. Please see the supplementary
materials of this paper for more examples.
Note that word and phrase polarities are not con-
sidered by the annotators in building our test sets
and these polarities are automatically identified us-
ing a word polarity dictionary and opinion extraction
tool. We confirmed that about 35% of questions and
40% of answer candidates had at least one sentiment
phrase by opinion extraction tool, and about 45% of
questions and 85% of answer candidates contained
at least one word having polarity by a word polarity
dictionary.
5 Experiments
We use TinySVM11 with a linear kernel for training
our re-ranker. Evaluation was done by P@1 (Pre-
cision of the top answer) and MAP (Mean Average
Precision). P@1 measures how many questions have
a correct top answer candidate. MAP, widely used in
evaluation of IR systems, measures the overall qual-
ity of the top-n answer candidates (n=20 in this ex-
periment) using the formula:
MAP =
1
|Q|
?
q?Q
?n
k=1(Prec(k)? rel(k))
|Aq|
(3)
Here Q is a set of why-questions, Aq is a set of cor-
rect answers to why-question q ? Q, Prec(k) is the
precision at cut-off k in the top-n answer candidates,
rel(k) is an indicator, 1 if the item at rank k is a cor-
rect answer in Aq, 0 otherwise.
We evaluated all systems using 10-fold cross val-
idation in two ways. In the first setting we per-
formed 10-fold cross validation on Set1. Set1 con-
11 http://chasen.org/?taku/software/TinySVM/
sists of 10,000 question-answer pairs (500 questions
with their 20 answer candidates), and was parti-
tioned into 10 subsamples such that the questions
in one subsample do not overlap with those of the
other subsamples. 9 subsamples (9,000 question-
answer pairs) were used as training data and the
remaining subsample (1,000 question-answer pairs)
was retained as test data. This experiment is called
CV(Set1). It shows the effect of answer re-ranking
when evaluating our proposed method with train-
ing data built with real world why-questions alone.
In the second setting, we used the same 10 sub-
samples of Set1 as in CV(Set1) and exploited Set2
(composed of 7,000 question-answer pairs) as ad-
ditional training data for 10-fold cross validation.
As a result, in each fold 16,000 question-answer
pairs (9,000 from Set1 and 7,000 from Set2) were
used as training data for re-rankers, and all systems
were evaluated on the remaining 1,000 question-
answer pair subsample from Set1. We call this set-
ting CV(Set1+Set2). It verifies whether training
data that does not necessarily reflect a real-world
distribution of why-questions can improve why-QA
performance on real-world questions.
5.1 Results
Table 4 shows the evaluation results of six different
systems. For each system, we represent the perfor-
mance in P@1 and MAP. B-QA is a system of our
answer retrieval and the other five re-rank top-20 an-
swer candidates using their own re-ranker.
B-QA: our answer retrieval system, our implemen-
tation of Murata et al2007).
B-Ranker: a system that has a re-ranker trained
with morphological and syntactic analysis
(MSA) features alone.
375
System CV(Set1) CV(Set1+Set2)P@1 MAP P@1 MAP
B-QA 0.222 (0.368) 0.270 (0.447) 0.222 (0.368) 0.270 (0.447)
B-Ranker 0.256 (0.424) 0.319 (0.528) 0.274 (0.454) 0.323 (0.535)
B-Ranker+CR 0.262 (0.434) 0.319 (0.528) 0.278 (0.460) 0.325 (0.538)
B-Ranker+WN 0.257 (0.425) 0.320 (0.530) 0.275 (0.455) 0.325 (0.538)
Proposed 0.336 (0.56) 0.377 (0.624) 0.374 (0.619) 0.391 (0.647)
UpperBound 0.604 (1) 0.604 (1) 0.604 (1) 0.604 (1)
Table 4: Comparison of systems
B-Ranker+CR: a system has a re-ranker trained
with our MSA features and the causal relation
(CR) features used in Higashinaka and Isozaki
(2008). The CR features include binary fea-
tures indicating whether an answer candidate
contains a causal relation pattern, which causal
relation pattern the answer candidate has, and
whether the question-answer pair contains a
causal relation instance ? cause in the answer,
effect in the question). We acquired causal
relation instances from our target corpus us-
ing the method from (De Saeger et al2009),
and exploited the top-100,000 causal relation
instances and the patterns that extracted them
for CR features. Note that these CR features
are introduced only for comparing our semantic
features with ones in Higashinaka and Isozaki
(2008) and they are not a part of our method.
B-Ranker+WN: its re-ranker is trained with our
MSA features and the WordNet features in Ver-
berne et al2010). The WordNet features in-
clude the percentage of the question terms and
their synonyms in WordNet synsets found in
an answer candidate and the semantic related-
ness score between a question and its answer
candidate, the average of the concept similar-
ity between each question term and all of the
answer terms by WordNet::Similarity (Peder-
sen et al2004). We used the Japanese Word-
Net 1.1 (Bond et al2009) for these WordNet
features. Note that the Japanese WordNet 1.1
has 93,834 Japanese words linked to 57,238
WordNet synsets, while the English WordNet
3.0 covers 155,287 words linked to 117,659
synsets. Due to this lower coverage, the Word-
Net features in Japanese may have a less power
for finding a correct answer than those in En-
glish used in Verberne et al2010).
Proposed: our proposed method. All of the MSA,
SWC and SA features are used for training our
re-ranker.
UpperBound: a system that ranks all n correct an-
swers as the top n results of the 20 answer can-
didates if there are any. This indicates the per-
formance upperbound in this experiment. The
relative performance of each system compared
to UpperBound is shown in parentheses.
The proposed method achieved the best perfor-
mance both in CV(Set1) and CV(Set1+Set2). Our
method shows a significant improvement (11.4?
15.2% in P@1 and 10.7?12.1% in MAP) over our
answer retrieval method, B-QA. Its improvement
over B-Ranker, B-Ranker+CR and B-Ranker+WN
(7.6?10% in P@1 and 5.7?6.6% in MAP) shows
the effectiveness of our proposed feature set over
the features used in previous works. Both B-
Ranker+CR and B-Ranker+WN did not show signif-
icant performance improvement over B-Ranker. At
least in our setting, the causal relation and WordNet
features did not prove effective. The performance
gap between B-Ranker and B-QA (3.4?5.2% in P@1
and 4.9?5.3% in MAP) suggests the effectiveness
of re-ranking. All systems consistently show better
performance in CV(Set1+Set2) than CV(Set1). This
suggests that training data built with why-questions
that does not reflect real-world distribution of why-
questions is useful in training re-rankers.
We investigate the contribution of each type of
features to the performance by removing one fea-
ture set from the all feature sets in training our re-
ranker. In this experiment, we split SA into SA@W
(features expressing words and their polarity) and
SA@P (features expressing phrases and their po-
larity) to investigate their contribution either. The
results are summarized in Table 5.
In Table 5, MSA+SWC+SA represents our pro-
posed method using all feature sets. The perfor-
mance gap between MSA+SWC+SA and the others
confirms that all the features contributed to a higher
376
System CV(Set1) CV(Set1+Set2)P@1 MAP P@1 MAP
SWC+SA 0.302 0.324 0.314 0.332
MSA+SWC 0.308 0.349 0.318 0.358
MSA+SA 0.300 0.352 0.314 0.364
MSA+SWC+SA@W 0.312 0.358 0.325 0.365
MSA+SWC+SA@P 0.323 0.369 0.358 0.384
MSA+SWC+SA 0.336 0.377 0.374 0.391
UpperBound 0.604 0.604 0.604 0.604
Table 5: Evaluation with different combination of feature
sets used in training our re-ranker
performance. The significant performance improve-
ment by SA (features from sentiment analysis) and
SWC (features from semantic word classes) (The
gap between MSA+SWC+SA and MSA+SWC was
2.8?6% and that between MSA+SWC+SA and
MSA+SA was 3.6%?6% in P@1) supports the hy-
pothesis for sentiment analysis and semantic word
classes in this paper.
Though the performance gap between
MSA+SWC+SA and MSA+SWC+SA@P
(1.3%?1.6% in P@1) shows that SA@W is
useful in training our re-ranker, we found that
MSA+SWC+SA@W made only 0.4?0.7% im-
provement over MSA+SWC. We believe that this
is mainly because SA@W and SWC are based on
semantic and sentiment information at the word
level, and these often capture a similar type of
information. For instance, disease names that are
grouped together into one class in SWC are typi-
cally classified as negative in SA@W. Therefore the
similarity in the information provided by SA@W
and SWC causes a classifier trained with both of
these features to obtain only a minor improvement
over a classifier using only one of the features.
To estimate the ideal-case performance of our
proposed method, we made another experiment by
using Set1 as training data for our re-ranker and
Set2 as test data for evaluating our proposed method.
Here, we assume a perfect answer retrieval module
that adds the source passage that was used for gener-
ating the original why-question in Set2 as a correct
answer to the set of existing answer candidates, giv-
ing 21 answer candidates. The performance of our
method in this setting was 64.8% in P@1 and 66.6%
in MAP. This evaluation result suggests that our re-
ranker can potentially perform with high precision
when at least one correct answer in answer candi-
dates is given by the answer retrieval module.
6 Related Work
In the QA literature, Higashinaka and Isozaki
(2008), Verberne et al2010), and Surdeanu et al
(2011) are closest to our work. The first two deal
with why-questions, the last with how-questions.
Similar to our method, they use machine learn-
ing techniques to re-rank answer candidates to non-
factoid questions based on various combinations of
syntactic, semantic and other statistical features such
as the density and frequency of question terms in the
answer candidates and patterns for causal relations
in the answer candidates. Especially for why-QA,
Higashinaka and Isozaki (2008) used causal relation
features and Verberne et al2010) exploited Word-
Net features as a kind of semantic features for train-
ing their re-ranker, where we used these features, re-
spectively, for B-Ranker+CR and B-Ranker+WN in
our experiment.
Our work differs from the above approaches in
that we propose semantic word classes and senti-
ment analysis as a new type of semantic features,
and show their usefulness in why-QA. Sentiment
analysis has been used before on the slightly un-
usual task of opinion question answering, where the
system is asked to answer subjective opinion ques-
tions (Stoyanov et al2005; Dang, 2008; Li et al
2009). To the best of our knowledge though, no pre-
vious work has systematically explored the use of
sentiment analysis in a general QA setting beyond
opinion questions.
7 Conclusion
In this paper, we have explored the utility of senti-
ment analysis and semantic word classes for ranking
answer candidates to why-questions. We proposed a
set of semantic features that exploit sentiment anal-
ysis and semantic word classes obtained from large-
scale noun clustering, and used them to train an an-
swer candidate re-ranker. Through a series of exper-
iments on 850 why-questions, we showed that the
proposed semantic features were effective in identi-
fying correct answers, and our proposed method ob-
tained more than 15% improvement in precision of
its top answer (P@1) over our baseline, a state-of-
the-art IR based QA system. We plan to use new se-
mantic knowledge such as semantic orientation, ex-
citatory or inhibitory, proposed in Hashimoto et al
(2012) for improving why-QA.
377
References
Francis Bond, Hitoshi Isahara, Sanae Fujita, Kiyotaka
Uchimoto, Takayuki Kuribayashi, and Kyoko Kan-
zaki. 2009. Enhancing the japanese wordnet. In Pro-
ceedings of the 7th Workshop on Asian Language Re-
sources, pages 1?8.
Hoa Tran Dang. 2008. Overview of the TAC 2008 opin-
ion question answering and summarization tasks. In
Proc. TAC 2008.
Stijn De Saeger, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large scale
relation acquisition using class dependent patterns. In
Proc. of ICDM 2009, pages 764?769.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John M.
Prager, Nico Schlaefer, and Christopher A. Welty.
2010. Building Watson: An overview of the DeepQA
project. AI Magazine, 31(3):59?79.
Junichi Fukumoto, Tsuneaki Kato, Fumito Masui, and
Tsunenori Mori. 2007. An overview of the 4th ques-
tion answering challenge (QAC-4) at NTCIR work-
shop 6. In Proc. of NTCIR-6.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger,
Jong-Hoon Oh, and Jun?ichi Kazama. 2012. Excita-
tory or inhibitory: A new semantic orientation extracts
contradiction and causality from the web. In Proceed-
ings of EMNLP-CoNLL 2012.
Ryuichiro Higashinaka and Hideki Isozaki. 2008.
Corpus-based question answering for why-questions.
In Proc. of IJCNLP, pages 418?425.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proc. of the 22nd annual international
ACM SIGIR conference on Research and development
in information retrieval, SIGIR ?99, pages 50?57.
Jun?ichi Kazama and Kentaro Torisawa. 2008. Inducing
gazetteers for named entity recognition by large-scale
clustering of dependency relations. In Proc. of ACL-
08: HLT, pages 407?415.
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan
Zhu. 2009. Answering opinion questions with ran-
dom walks on graphs. In Proc. of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 2 - Volume 2, pages
737?745.
Masaki Murata, Sachiyo Tsukawaki, Toshiyuki Kana-
maru, Qing Ma, and Hitoshi Isahara. 2007. A system
for answering non-factoid Japanese questions by using
passage retrieval weighted based on type of answer. In
Proc. of NTCIR-6.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classification
using CRFs with hidden variables. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 786?794, Los An-
geles, California, June. Association for Computational
Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proc. of the 2002 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 79?86.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity: measuring the
relatedness of concepts. In Demonstration Papers
at HLT-NAACL 2004, HLT-NAACL?Demonstrations
?04, pages 38?41.
Anselmo Pe?as, Eduard H. Hovy, Pamela Forner, ?lvaro
Rodrigo, Richard F. E. Sutcliffe, Corina Forascu, and
Caroline Sporleder. 2011. Overview of QA4MRE at
CLEF 2011: Question answering for machine reading
evaluation. In CLEF.
Veselin Stoyanov, Claire Cardie, and Janyce Wiebe.
2005. Multi-perspective question answering using the
opqa corpus. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods in
Natural Language Processing, HLT ?05, pages 923?
930.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-
factoid questions from web collections. Computa-
tional Linguistics, 37(2):351?383.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proc. of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 417?424.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc., New
York, NY, USA.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-
Arno Coppen. 2007. Evaluating discourse-based an-
swer extraction for why-question answering. In SIGIR,
pages 735?736.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-
Arno Coppen. 2010. What is not in the bag of words
for why-QA? Computational Linguistics, 36:229?
245.
Ellen M. Voorhees. 2004. Overview of the TREC 2004
question answering track. In TREC.
378
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 619?630, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Excitatory or Inhibitory: A New Semantic Orientation Extracts
Contradiction and Causality from the Web
Chikara Hashimoto? Kentaro Torisawa? Stijn De Saeger?
Jong-Hoon Oh? Jun?ichi Kazama?
National Institute of Information and Communications Technology
Kyoto, 619-0289, JAPAN
{? ch, ? torisawa, ? stijn, ? rovellia, ?kazama}@nict.go.jp
Abstract
We propose a new semantic orientation, Ex-
citation, and its automatic acquisition method.
Excitation is a semantic property of predicates
that classifies them into excitatory, inhibitory
and neutral. We show that Excitation is useful
for extracting contradiction pairs (e.g., destroy
cancer ? develop cancer) and causality pairs
(e.g., increase in crime ? heighten anxiety).
Our experiments show that with automatically
acquired Excitation knowledge we can extract
one million contradiction pairs and 500,000
causality pairs with about 70% precision from
a 600 million page Web corpus. Furthermore,
by combining these extracted causality and
contradiction pairs, we can generate one mil-
lion plausible causality hypotheses that are not
written in any single sentence in our corpus
with reasonable precision.
1 Introduction
Recognizing semantic relations between events in
texts is crucial for such NLP tasks as question an-
swering (QA). For example, to answer the question
?What ruined the crops in Japan?? a QA system
must recognize that the sentence ?the Fukushima
nuclear power plant caused radioactive pollution
and contaminated the crops in Japan? contains a
causal relation and that contaminate crops entails
ruin crops but contradicts preserve crops.
To facilitate the acquisition of causality, contra-
diction, paraphrase and entailment relations between
events we propose a new semantic orientation, Ex-
citation, that classifies unary predicates (templates,
hereafter) into excitatory, inhibitory and neutral. An
excitatory template entails that the main function or
effect of the referent of its argument is activated or
enhanced (e.g., cause X, preserve X), while an in-
hibitory template entails that it is deactivated or sup-
pressed (e.g., ruin X, contaminate X, prevent X).
Excitation is useful for extracting contradiction;
if two templates with similar distributional profiles
have opposite Excitation polarities, they tend to be
contradictions (e.g., contaminate crops and preserve
crops). With extracted contradictions we can distin-
guish paraphrases from contradictions among distri-
butionally similar phrases. Furthermore, contradic-
tion in itself is important knowledge for Recogniz-
ing Textual Entailment (RTE) (Voorhees, 2008).
Excitation is also a powerful indicator of causal-
ity. In the physical world, the activation or de-
activation of one thing often causes the activation
or deactivation of another. Two excitatory or in-
hibitory templates that co-occur in some temporal
or logical order in the same narrative often describe
a causal chain of events, like ?the Fukushima nu-
clear power plant caused radioactive pollution and
contaminated crops in Japan?.
In this paper we propose both the concept of Ex-
citation and an automatic method for its acquisition.
Our method acquires Excitation templates based on
certain natural, language independent constraints on
narrative structures found in text. We also propose
acquisition methods for contradiction and causal-
ity relations based on Excitation. Our methods ex-
tract one million contradiction pairs with over 70%
precision, and 500,000 causality pairs with about
70% precision from a 600 million page Web corpus.
Moreover, by combining these extracted causality
pairs and contradiction pairs, we generated one mil-
lion plausible causality hypotheses that were not
619
written in any single sentence in our corpus with rea-
sonable precision. For example, a causality hypoth-
esis prevent radioactive pollution ? preserve crops
can be generated from an extracted causality cause
radioactive pollution ? contaminate crops.
We target the Japanese language in this paper.
2 What is Excitation?
Excitation classifies templates into excitatory, in-
hibitory, and neutral, as explained below.
excitatory templates entail that the function, ef-
fect, purpose or role of their argument?s refer-
ent is activated or enhanced. (e.g., cause X, buy
X, produce X, import X, increase X, enable X)
inhibitory templates entail that the function, ef-
fect, purpose or role of their argument?s refer-
ent is deactivated or suppressed. (e.g., prevent
X, discard X, remedy X, decrease X, disable X)
neutral templates are neither excitatory nor in-
hibitory. (e.g., consider X, proportional to X,
related to X, evaluate X, close to X)
For example, when fire fills the X slot of cause X,
it suggests that the effect of fire is activated. If pre-
vent X?s slot is filled with flu, the effect of flu is sup-
pressed. In this study, we aim to acquire excitatory
and inhibitory templates that are useful for extract-
ing contradiction and causality, though neutral tem-
plates are the most frequent in our data (See Section
5.1). Collectively we call excitatory and inhibitory
templates Excitation templates, and excitatory and
inhibitory two opposite polarities.
Excitation is independent of the good/bad seman-
tic orientation. (Hatzivassiloglou and McKeown,
1997; Turney, 2002; Rao and Ravichandran, 2009).
For example, sophisticate X and complicate X are
both excitatory, but only the former has a positive
connotation. Similarly, remedy X and degrade X are
both inhibitory but only the latter is negative.
General Inquirer (Stone et al1966) deals with
semantic factors some of which were proposed by
Osgood et al1957). Their ?activity? factor involves
binary opposition between ?active? and ?passive.?
Notice that activity and Excitation are independent.
In General Inquirer, both accelerate X and abolish
X are active, but only the former is excitatory. Both
accept X and abate X are passive, but only the lat-
ter is inhibitory. Pustejovsky (1995) proposed telic
and agentive roles, which inspired our excitatory no-
tion, but they have no corresponding notion of in-
hibitory. Andreevskaia and Bergler (2006) acquired
the increase/decrease semantic orientation, which is
a subclass of Excitation.
Excitation is inverted if a template?s predicate is
negated. For example, preserve X is excitatory,
while don?t preserve X is inhibitory. We acknowl-
edge that this may seem somewhat counter-intuitive
and will address this issue in future work.
3 Excitation Template Acquisition
This section presents our acquisition method of Ex-
citation templates. We introduce constraints in the
co-occurrence of templates in text that seem both ro-
bust and language independent in Section 3.1. Our
method exploits these constraints for the acquisition
of Excitation templates. First we construct a tem-
plate network where nodes are templates and links
represent that two connected templates have either
SAME or OPPOSITE polarities. Given 46 manually
prepared seed templates we calculate the Excitation
value of each template, a value in range [?1, 1] that
is positive if the template is excitatory and negative
if it is inhibitory. Technically, our method treats all
templates as excitatory or inhibitory, and, upon com-
pletion, regards templates with small absolute Exci-
tation values as neutral.
The whole method is a bootstrapping process.
Each iteration expands the network and the Excita-
tion value of each template is (re-)calculated.
3.1 Characteristics of Excitation Templates
Our method exploits natural discourse constraints on
the possible combinations of (a) the polarity of co-
occurring templates, (b) the nouns that fill their ar-
gument slots and (c) the connectives that link the
templates in a given sentence. Table 1 shows the
constraints and Figure 1 shows examples that will
be explained shortly. Though our target is Japanese
we believe these constraints are universal discourse
principles, and as such not language dependent. Ex-
amples are given in English for ease of explanation.
We first identify two categories of connectives
in our target sentences: AND/THUS-type (e.g., and,
thus and since) and BUT-type (e.g., but and though).
Both types suggest a sort of consistency or inconsis-
tency between predicates. We manually classified
169 frequently used connectives into AND/THUS-
620
(1) He smoked cigarettes, AND/THUS he suffered lung
cancer. (Both smoke X and suffer X are excitatory.)
(2) He quit cigarettes, AND/THUS was immune from lung
cancer. (quit X and immune from X are inhibitory.)
(3) He smoked cigarettes, BUT didn?t suffer lung cancer.
(smoke X is excitatory, not suffer X is inhibitory.)
(4) He quit cigarettes, BUT he suffered lung cancer. (quit
X is inhibitory, but suffer X is excitatory.)
(5) He underwent cancer treatment, AND/THUS he could
cure the cancer. (undergo X is excitatory, cure X is
inhibitory.)
(6) He underwent cancer treatment, BUT still had cancer.
(Both undergo X and have X are excitatory.)
(7) Unnatural: He smoked cigarettes, BUT he suffered
lung cancer. (smoke X and suffer X are excitatory.)
Figure 1: Examples of constraints: (cigarettes, lung can-
cer) is PNP and (cancer treatment, cancer) is NNP.
PNPs NNPs others
AND/THUS SAME OPPOSITE N/A
BUT OPPOSITE SAME N/A
Table 1: Constraint matrix.
and BUT-type (See supplementary materials).
Next we extract sentences from the Web in which
two templates co-occur and are joined by one of
these connectives, and then classify the noun pairs
filling the templates? argument slots into ?positively-
associated? and ?negatively-associated? noun pairs
(PNPs and NNPs). Mirroring our definition of Excita-
tion, PNPs are noun pairs in which the referent of the
first noun facilitates the emergence of the referent
of the second noun. PNPs can range from causally
related noun pairs like (cigarettes, lung cancer) to
?material-product? relation pairs like (semiconduc-
tor, electronic circuit). We found that PNPs only
fill the argument slots of (a) same Excitation polar-
ity templates connected by AND/THUS-type connec-
tives (examples 1 and 2 in Figure 1), or (b) opposite
Excitation polarity templates connected by a BUT-
type connectives (examples 3 and 4). Violating such
constraints (example 7) seems unnatural. Similarly,
NNPs are noun pairs in which the referent of one
noun suppresses the emergence of the referent of the
other noun. Examples include such ?inverse causal-
ity? pairs as (cancer treatment, cancer). NNPs only
fill the argument slots of (a) opposite Excitation po-
larity templates connected by AND/THUS-type con-
nectives (example 5), or (b) same polarity templates
connected by a BUT-type connective (example 6).
All these constraints are summarized in Table 1,
which we will call the constraint matrix. Accord-
ing to the constraint matrix, we can know whether
two templates? polarities are the same or opposite if
we know whether a noun pair filling the two tem-
plates? slots is PNP or NNP. Conversely, we can
know whether a noun pair is PNP or NNP if we know
whether two templates whose slots are filled with
the noun pair have the same or opposite polarities.
We believe these constraints capture certain univer-
sal principles of discourse, since it is difficult in any
language to produce natural sounding sentences that
violate these constraints. We empirically confirm
their validity for Japanese in Section 5.1.
3.2 Bootstrapping Approach to Excitation
Template Acquisition
To calculate the Excitation values for the templates,
we construct a template network where templates
are connected by links indicating polarity agreement
between two connected templates (either SAME or
OPPOSITE polarity), as determined by the constraint
matrix. Excitation values are determined by spread-
ing activation applied to the network, given a small
number of manually prepared seed templates.
However, we cannot construct the network unless
we know whether each noun pair is PNP or NNP, due
to the configuration of the constraint matrix, and cur-
rently we have no feasible method to classify all of
them into PNPs and NNPs in advance. We therefore
adopt a bootstrapping method (Figure 2) that starts
from manually prepared excitatory and inhibitory
seed templates (Step 1 in Figure 2). Our method
begins by extracting noun pairs from the Web that
co-occur with two seed templates connected by a
AND/THUS- or BUT-type connective, and classifies
these noun pairs into PNPs and NNPs based on the
constraint matrix (Steps 2 and 3). Next, we automat-
ically extract additional (non-seed) template pairs
from the Web that co-occur with these PNPs and
NNPs. Links (either SAME or OPPOSITE) between
all template pairs are determined by the constraint
matrix (Step 4), and we construct a template network
from both seed and non-seed template pairs (Step 5).
Our method calculates the Excitation values for
all the templates in the network by first assign-
ing Excitation values +1 and ?1 to the excitatory
and inhibitory seed templates, and applies a spread-
ing activation method proposed by Takamura et al
(2005) (Step 6) to the network. This method calcu-
621
1. Prepare initial seed templates with fixed excitation values (either
+1 or ?1).
2. Make seed template pairs that are combinations of two seed tem-
plates and a connective (either AND/THUS-type or BUT-type).
3. Extract noun pairs that co-occur with one of the seed template
pairs from the Web. Classify the noun pairs into PNPs and NNPs
based on the constraints matrix. Filter out those noun pairs that
appear as both PNP and NNP on the Web or those whose occur-
rence frequency is less than or equal to F, which is set to 5.
4. Extract additional (non-seed) template pairs that are filled by one
of the PNPs or NNPs from the Web. Determine the link type
(SAME or OPPOSITE) for each template pair based on the con-
straint matrix. If a template pair appears on the Web as having
both link types, we determine its link type by majority vote.
5. Construct the template network from all the template pairs. Re-
move from the network those templates whose number of linked
templates is less than D, which is set to 5.
6. Apply Takamura et al method to the network and fix the Exci-
tation value of each template.
7. Extract the top- and bottom-ranked N ? i templates from the
result of Takamura et al method. N is a constant, which is
set to 30. i is the iteration number. They are used as additional
seed templates for the next iteration. The top-ranked templates
are given Excitation value +1 and the bottom-ranked templates
are assigned ?1. Go to Step 2.
Figure 2: Bootstrapping for template acquisition.
lates all templates? excitation values by solving the
network constraints imposed by the SAME and OP-
POSITE links, and the Excitation values of the seed
templates (This method is detailed in Section 3.3).
In each iteration i, our method selects the N ? i top-
ranked and bottom-ranked templates as additional
seed templates for the next iteration (N is set to 30)
(Step 7). Our method then constructs a new tem-
plate network using the augmented seed templates
and restarts the calculation process. Figure 2 sum-
marizes our bootstrapping process.
Bootstrapping stops after M iterations, with M
set to 7 based on our preliminary experiments.
To prepare the initial seed templates we con-
structed a maximal template network that could in
theory be created by our bootstrapping method. This
maximal network consists of any two templates that
co-occur in a sentence with any connective, regard-
less of their arguments. We manually selected 36
excitatory and 10 inhibitory seed templates from
among 114 templates with the most links in the net-
work (See supplementary materials).
3.3 Determining Excitation in the Network
This section details Step 6 of our bootstrapping
method, i.e., how Takamura et al method calcu-
lates the Excitation value of each template. Their
method is based on the spin model in physics, where
each electron has a spin of either up or down. We
chose this method due to the straightforward parallel
between the spin model and our Excitation template
model. Both models capture the spreading of acti-
vation (either spin direction or excitation polarity)
between neighboring objects in a network. Deter-
mining the optimal algorithm for this task is beyond
our current scope, but for the purpose of our experi-
ments we found that Takamura et al method gave
satisfactory results.
The spin model defines an energy function on a
spin network, and each electron?s spin can be esti-
mated by minimizing this function:
E(x,W ) = ?1/2? ?ijwijxixj
Here, xi, xj ? x are spins of electrons i and j, and
matrix W = {wij} assigns weights to links between
electrons. We regard templates as electrons and Ex-
citation polarities as their spins (up and down corre-
spond to excitatory and inhibitory). We define the
weight wij of the link between templates i and j as:
wij =
{
1/
?
d(i)d(j) if SAME(i, j)
?1/
?
d(i)d(j) if OPPOSITE(i, j)
Here, d(i) denotes the number of templates linked
to i. SAME(i, j) (OPPOSITE(i, j)) indicates a SAME
(OPPOSITE) link exists between i and j. We obtain
excitation values by minimizing the above energy
function. Note that after minimizing E, xi and xj
tend to get the same polarity when wij is positive.
When wij is negative, xi and xj tend to have op-
posite polarities. Initially seed templates are given
values +1 or ?1 depending on whether they are ex-
citatory or inhibitory, and others are given 0.
We used SUPPIN (http://www.lr.pi.titech.
ac.jp/?takamura/pubs/SUPPIN-0.01.tar.gz),
an implementation of Takamura et al method. Its
parameter ? is set to the default value (0.75).
4 Knowledge Acquisition by Excitation
This section shows how the concept of Excitation
can be used for automatic knowledge acquisition.
4.1 Contradiction Extraction
Our first knowledge acquisition method extracts
contradiction pairs like destroy cancer ? develop
cancer, based on our assumption that they often con-
sist of distributionally similar templates that have a
sharp contrast in Excitation value. Concretely, we
622
extract two phrases as a contradiction pair if (a)
their templates have opposite Excitation polarities,
(b) they share the same argument noun, and (c) the
part-of-speech of their predicates is the same. Then
the contradiction pairs are ranked by Ct:
Ct(p1, p2) = |s1| ? |s2| ? sim(t1, t2)
Here p1 and p2 are two phrases that satisfy condi-
tions (a), (b) and (c) above, t1 and t2 are their re-
spective templates, and |s1| and |s2| are the absolute
values of t1 and t2?s excitation values. sim(t1, t2) is
the distributional similarity proposed by Lin (1998).
Note that ?contradiction? here includes what we
call ?quasi-contradiction.? This consists of two
phrases such that, if the tendencies of the events they
describe get stronger, they eventually become con-
tradictions. For example, the pair emit smells ? re-
duce smells is not logically contradictory since the
two events can happen at the same time. However,
they become almost contradictory when their ten-
dencies get stronger (i.e., emit smells more strongly
? thoroughly reduce smells). We believe quasi-
contradictions are useful for NLP tasks.
4.2 Causality Extraction
Our second knowledge acquisition method extracts
causality pairs like increase in crime ? heighten
anxiety that co-occur with AND/THUS-type connec-
tives in a sentence. The assumption is that if two
templates (t1 and t2) with a strong Excitation ten-
dency are connected by an AND/THUS-type connec-
tive in a sentence, the event described by t1 and its
argument n1 tends to be a cause of the event de-
scribed by t2 and its argument n2. Here, Excitation
strength is expressed by absolute Excitation values.
The intuition is that, if the referent of n1 is strongly
activated or suppressed, it tends to have some causal
effect on the referent of n2 in the same sentence.
We focus on extracting causality pairs that co-
occur with only ?non-causal connectives? like and,
which are AND/THUS-type connectives that do NOT
explicitly signal causality, since causal connectives
like thus can mask the effectiveness of Excitation.
We prepared 139 non-causal connectives (See sup-
plementary materials). We extract two templates
such as increase in X and heighten Y co-occurring
with only non-causal connectives, as well as the
noun pair that fills the two templates? slots (e.g.,
(crime, anxiety)) to obtain causal phrase pairs. In
Japanese, the temporal order between events is usu-
ally determined by precedence in the sentence. Cs
ranks the obtained causality pairs:
Cs(p1, p2) = |s1| ? |s2|
Here p1 and p2 are the phrases of causality pair, and
|s1| and |s2| are absolute Excitation values of p1?s
and p2?s templates. As is common in the literature,
this notion of causality should be interpreted prob-
abilistically rather than logically, i.e., we interpret
causality A ? B as ?if A happens, the probability of
B increases?. This interpretation is often more use-
ful for NLP tasks than a strict logical interpretation.
4.3 Causality Hypothesis Generation
Our third knowledge acquisition method generates
plausible causality hypotheses that are not written in
any single sentence using the previously extracted
contradiction and causality pairs. We assume that if
a causal relation (e.g., increase in crime ? heighten
anxiety ) is valid, its inverse (e.g., decrease in crime
? diminish anxiety ) is often valid as well. From
a logical definition of causation, taking the inverse
of an implication obviously does not preserve valid-
ity. However, at least under our probabilistic inter-
pretation, taking the inverse of a given causality pair
using the extracted contradiction pairs proves to be
a viable strategy for generating non-trivial causality
hypotheses, as our experiments in Section 5.4 show.
For an extracted causality pair, we generate its
inverse as a causality hypothesis by replacing both
phrases in the original pair with their contradiction
counterparts. For instance, a causality hypothesis
decrease in crime ? diminish anxiety is generated
from a causality increase in crime ? heighten anxi-
ety by two contradictions, decrease in crime ? in-
crease in crime and diminish anxiety ? heighten
anxiety. Since we are interested in finding new
causal hypotheses, we filter out hypotheses whose
phrase pair co-occurs in a sentence in our corpus.
Remaining causality hypotheses are ranked by Hp.
Hp(q1, q2) = Ct(p1, q1)? Ct(p2, q2)? Cs?(p1, p2)
Here, q1 and q2 are two phrases of a causality hy-
pothesis. p1 and p2 are two phrases of a hypothesis?s
original causality. That is, p1 ? q1 and p2 ? q2 are
contradiction pairs, and Ct(p1, q1) and Ct(p2, q2)
are their contradiction scores. Cs?(p1, p2) is the
original causality?s causality score. Cs? can be Cs
623
from Section 4.2, but based on preliminary experi-
ments we found the following score works better:
Cs?(p1, p2) = |s1| ? |s2| ? npfreq(n1, n2)
|s1| and |s2| are absolute Excitation values of p1?s
and p2?s templates, whose slots are filled with n1 and
n2. npfreq(n1, n2) is the co-occurrence frequency
of (n1, n2) with polarity-identical template pairs (if
(n1, n2) is PNP) or with polarity-opposite template
pairs (if (n1, n2) is NNP). Thus, npfreq indicates a
sort of association strength between two nouns.
5 Experiments
This section shows that our template acquisition
method acquired many Excitation templates. More-
over, using only the acquired templates we extracted
one million contradiction pairs with more than 70%
precision, and 500,000 causality pairs with about
70% precision. Further, using only these extracted
contradiction and causality pairs we generated one
million causality hypotheses with 57% precision.
In our experiments we removed evaluation sam-
ples containing the initial seed templates and exam-
ples used for annotation instruction from the evalua-
tion data. Three annotators (not the authors) marked
all evaluation samples, which were randomly shuf-
fled so that they could not identify which sample was
produced by which method. Information about the
predicted labels or ranks was also removed from the
evaluation data. Final judgments were made by ma-
jority vote between the annotators. They were non-
experts without formal training in linguistics or se-
mantics. See supplementary materials for our anno-
tation manuals (translated into English).
We used 600 million Japanese Web pages
(Akamine et al2010) parsed by KNP (Kawahara
and Kurohashi, 2006) as a corpus. We restricted
the argument positions of templates to ha (topic),
ga (nominative), wo (accusative), ni (dative), and de
(instrumental). We discarded templates appearing
fewer than 20 times in compound sentences (regard-
less of connectives) in our corpus.
5.1 Excitation Template Acquisition
We show that our proposed method for template ex-
traction (PROPtmp) successfully acquired many Ex-
citation templates from which we obtained a huge
number of contradiction and causality pairs, and that
Excitation is a reasonably comprehensible notion
even for non-experts. We also show that PROPtmp
outperformed two baselines by a large margin.
The template network constructed by PROPtmp
contained 10,825 templates. Among these, the boot-
strapping process classified 8,685 templates as exci-
tatory and 2,140 as inhibitory. Note that these can-
didates in fact also contain neutral templates, as ex-
plained at the beginning of Section 3.
Baselines The baseline methods are ALLEXC and
SIM. ALLEXC regards all templates that are ran-
domly extracted from the Web as excitatory, since in
our data excitatory templates outnumber inhibitory
ones. Actually, in our data neutral templates rep-
resent the most frequent class, but since our objec-
tive is to acquire excitatory and inhibitory templates,
a baseline marking all templates as neutral would
make little sense. SIM is a distributional similarity
baseline that takes as input the same 10,825 tem-
plates of PROPtmp above, constructs a network by
connecting two templates whose distributional simi-
larity is greater than zero, and regards two connected
templates as having the same polarity. The weight
of the links between templates is set to their distri-
butional similarity based on Lin (1998). Then SIM
is given the same initial seed templates as PROPtmp,
by which it calculates the Excitation values of tem-
plates using Takamura et al method. As a result,
SIM assigned positive Excitation values to all tem-
plates, and except for the 10 inhibitory initial seed
templates no templates were regarded inhibitory.
Evaluation scheme We randomly sampled 100
templates each from PROPtmp?s 8,685 excitatory
candidates, PROPtmp?s 2,140 inhibitory candidates,
all the ALLEXC?s templates, and all the SIM?s tem-
plates, i.e., 400 templates in total. To make the an-
notators? judgements easier, we randomly filled the
argument slot of each template with a noun filling its
argument slot in our Web corpus. Three annotators
labeled each sample (a combination of a template
and a noun) as ?excitatory,? ?inhibitory,? ?neutral,? or
?undecided? if they were not sure about its label.
Results for excitatory In the top graph in Fig-
ure 3, ?Proposed? shows PROPtmp?s precision curve.
The curve is drawn from its 100 samples whose X-
axis positions represent their ranks. We plot a dot for
every 5 samples. Among the 100 samples, 37 were
judged as excitatory, 6 as inhibitory, 45 as neutral,
and 6 as ?undecided?. For the remaining 6 samples,
624
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2000  4000  6000  8000  10000
Pre
cisio
n
?Proposed?
?Sim?
?Allexc?
 0.4
 0.6
 0.8
 1
 0  500  1000  1500  2000  2500
Pre
cisio
n
Top-N
?Proposed?
Figure 3: Precision of template acquisition: excitatory
(top) and inhibitory (bottom).
the three annotators gave three different labels and
the label was not fixed (?split-votes? hereafter). For
calculating precision, only the 37 samples labeled
excitatory were regarded as correct. PROPtmp out-
performed all baselines by a large margin, with an
estimated 70% precision for the top 2,000 templates.
?Allexc? and ?Sim? in Figure 3 denote ALLEXC and
SIM. Among ALLEXC?s 100 samples, 19 were
judged as excitatory, 5 as inhibitory, 74 as neutral,
and 2 as ?undecided?. SIM?s low performance re-
flects the fact that templates with opposite polarities
are sometimes distributionally similar, and as a re-
sult get connected by SAME links.
Results for inhibitory ?Proposed? in the bottom
graph in Figure 3 shows the precision curve drawn
from the 100 samples of PROPtmp?s inhibitory can-
didates. Among the 100 samples, 41 were judged as
inhibitory, 15 as excitatory, 32 as neutral, 4 as ?unde-
cided?, and 8 as ?split-votes?. Only the 41 inhibitory
samples were regarded as correct. From the curve
we estimate that PROPtmp achieved about 70% pre-
cision for the top 500. Note that SIM could not ac-
quire any inhibitory templates, yet we can think of
no other reasonable baseline for this task.
Inter-annotator agreement The Fleiss? kappa
(Fleiss, 1971) of annotator judgements was 0.48
(moderate agreement (Landis and Koch, 1977)). For
training, the annotators were given a one-page anno-
tation manual (see supplementary materials), which
basically described the same contents in Section 2,
in addition to 14 examples of excitatory, 14 exam-
ples of inhibitory, and 6 examples of neutral tem-
plates that were manually prepared by the authors.
Using the manual and the examples, we instructed
all the annotators face-to-face for a few hours. We
also made sure the evaluation data did not contain
any examples used during instruction.
Observations about argument positions Among
the 200 evaluation samples of PROPtmp (for both ex-
citatory and inhibitory evaluations), 52 were judged
as excitatory, 47 as inhibitory, and 77 as neutral. For
the excitatory templates, the numbers of nominative,
topic, accusative, dative, and instrumental argument
positions are 15, 11, 10, 8, and 8, respectively. For
the inhibitory templates, the numbers are 17, 11, 16,
3, and 0. For the neutral templates, the numbers are
8, 23, 17, 21, and 8. Accordingly, we found no no-
ticeable bias with regard to their numbers. Likewise,
we found no noticeable bias regarding their useful-
ness for contradiction and causality acquisition re-
ported shortly, too.
Summary PROPtmp works well, as it outperforms
the baselines. Its performance demonstrates the va-
lidity of our constraint matrix in Table 1. Besides,
since our annotators were non-experts but showed
moderate agreement, we conclude that Excitation is
a reasonably comprehensible notion.
5.2 Contradiction Extraction
This section shows that our proposed method for
contradiction extraction (PROPcont) extracted one
million contradiction pairs with more than 70% pre-
cision, and that Excitation values are useful for con-
tradiction ranking. As input for PROPcont we took
the top 2,000 excitatory and the top 500 inhibitory
templates from the previous experiment (i.e., the
other templates were regarded as neutral).
Baselines Our baseline methods are RANDcont
and PROPcont-NE. RANDcont randomly combines
two phrases, each consisting of a template and a
noun that they share. It does not rank its output.
PROPcont-NE is the same as PROPcont except that it
does not use Excitation values; ranking is based only
on sim(t1, t2). PROPcont-NE does combine phrases
with opposite template polarities, just like PROPcont.
Evaluation scheme We randomly sampled 200
phrase pairs from the top one million results of each
625
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  200000  400000  600000  800000  1e+06
Pre
cisio
n
Top-N
?Proposed?
?Proposed-ne?
?Random?
Figure 4: Precision of contradiction extraction.
PROPcont and PROPcont-NE, and 100 samples from
the output of RANDcont?s output, giving 500 sam-
ples. Three annotators labeled whether the samples
are contradictions. Fleiss? kappa was 0.78 (substan-
tial agreement).
Results ?Proposed? in Figure 4 shows the preci-
sion curve of PROPcont. PROPcont achieved an esti-
mated 70% precision for its top one million results.
Readers might wonder whether PROPcont?s output
consists of a small number of template pairs that are
filled with many different nouns. If this were the
case, PROPcont?s performance would be somewhat
misleading. However, we found that PROPcont?s 200
samples contained 194 different template pairs, sug-
gesting that our method can acquire a large variety
of contradiction phrases. ?Proposed-ne? is the pre-
cision curve for PROPcont-NE. Its precision is more
than 10% lower than PROPcont at the top one million
results. ?Random? shows that RANDcont?s precision
is only 4%. Table 2 shows examples of PROPcont?s
outputs and their English translation. The labels
?Cont,? ?Quasi? and ?6? denote whether a pair is con-
tradictory, quasi-contradictory, or not contradictory.
Among PROPcont?s 145 samples judged by the an-
notators as contradiction, 46 were judged as quasi-
contradictory by one of the authors. The first 6
case in Table 2 was caused by the template, X??
??? (improve X). It is tricky since it is excitatory
when taking arguments like function, while it is in-
hibitory when taking arguments like disorder. How-
ever, PROPtmp currently cannot distinguish these us-
ages and judged it as inhibitory in our experiments
in Section 5.1, though it must be interpreted as ex-
citatory for the 6 case. The second 6 case was due
to PROPtmp?s error; it incorrectly judged the neutral
template, X????? (related to X), as inhibitory.
Rank Contradiction Pairs Label
8,767 ??????????? ????????????? Cont
repair imbalance ? become imbalanced
103,581 ?????? ??????? Cont
assist the driver ? disturb the driver
151,338 ????????? ?????? Quasi
calm tension ? feel tension
184,014 ??????? ??????? 6
improve function ? boost function
316,881 ?????? ???????? Cont
yen depreciation stops ? yen depreciation develops
317,028 ???????? ???????? Cont
noise gets worse ? noise abates
334,642 ????? ??????? Cont
a sour taste is augmented ? a sour taste is lost
487,496 ??????? ??????? Quasi
feel pain ? reduce pain
529,173 ???????? ?????????? Cont
access occurs ? curb access
555,049 ?????? ??????? Cont
lose nuclear plants ? augment nuclear plants
608,895 ????????? ??????? Quasi
radioactivity is released ? radioactivity is reduced
638,092 ???????? ????????? Cont
Euro falls ? Euro gets strong
757,423 ??????? ????????? Quasi
have share (in market) ? share decreases
833,941 ?????????? ?????????? 6
generate active oxygen ? related to active oxygen
848,331 ??????? ????????? Cont
destroy cancer ? develop cancer
982,980 ????????? ??????????? Cont
virus becomes extinct ? virus is activated
Table 2: Examples of PROPcont?s outputs.
Summary PROPcont is a low cost but high perfor-
mance method, since it acquired one million con-
tradiction pairs with over 70% precision from only
the 46 initial seed templates. Besides, Excitation
contributes to contradiction ranking since PROPcont
outperformed PROPcont-NE by a 10% margin for the
top one million results. Thus we conclude that our
assumption on contradiction extraction is valid.
5.3 Causality Extraction
We show that our method for causality extraction
(PROPcaus) extracted 500,000 causality pairs with
about 70% precision, and that Excitation values con-
tribute to the ranking of causal pairs. PROPcaus took
as input all 10,825 templates classified by PROPtmp.
Baselines RANDcaus randomly extracts two
phrases that co-occur in a sentence with one of the
AND/THUS-type connectives, i.e., it uses not only
non-causal connectives but also causal ones like
thus. FREQ is the same as PROPcaus except that it
ranks its output by the phrase pair co-occurrence
frequency rather than Excitation values.
626
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  200000  400000  600000  800000  1e+06
Pre
cisio
n
Top-N
?Proposed?
?Freq?
?Random?
Figure 5: Precision of causality extraction.
Evaluation scheme We randomly sampled 100
pairs each from the top one million results of
PROPcaus and FREQ, and all RANDcaus?s output.
The annotators were shown the original sentences
from which the samples were extracted. Fleiss?
kappa was 0.68 (substantial agreement).
Results ?Proposed? in Figure 5 is the precision
curve for PROPcaus. From this curve the estimated
precision of PROPcaus is about 70% around the top
500,000. Note that PROPcaus outperformed FREQ
by a large margin, and extracted a large variety of
causal pairs since its 100 samples contained 91 dif-
ferent template pairs. Table 3 shows examples of
PROPcaus?s output along with English translations.
The labels ?4? and ?6? denote whether a pair is
causality or not. The 6 cases in Table 3 were
exceptions to our assumption described in Section
4.2; even if two Excitation templates co-occur in a
sentence with an AND/THUS-type connective, they
sometimes do not constitute causality. Actually, the
first 6 case consists of two phrases that co-occurred
in a sentence with a (non-causal) AND/THUS-type
connective but described two events that happen as
the effects of introducing the RAID storage system;
both are caused by the third event. In the second 6
case, the two phrases co-occurred in a sentence with
a (non-causal) AND/THUS-type connective but just
described two opposing events.
Summary PROPcaus performs well since it ex-
tracted 500,000 causality pairs with about 70%
precision. Moreover, Excitation values contribute
to causality ranking since PROPcaus outperformed
FREQ by a large margin. Then we conclude that our
assumption on causality extraction is confirmed.
Rank Causality Pairs Label
1,036 ?????????????????? 4
increase basal metabolism ? enhance fat-burning ability
2,128 ?????????????????? 4
increase desire to learn ? facilitate self-learning
6,471 ?????????????? 6
improve reliability ? increase capacity
29,638 ???????????????????????? 4
circulating thyroid hormone level increases ? improves metabolism
56,868 ??????????????? 4
exports increase ? GDP grows
267,364 ???????????????? 4
promote blood circulation ? improve metabolism
268,670 ???????????????? 4
BSE outbreak occurs ? import ban (on beef) is issued
290,846 ????????????????? 4
improve the view ? improve the efficiency of work
322,121 ??????????????????? 4
giant earthquake occurs ? meltdown is triggered
532,106 ??????????????? 4
good at thermal efficiency ? enhance heating efficiency
563,462 ???????????????? 4
promote inflation (in Japan) ? yen depreciation develops
591,175 ???????????????? 6
bring profit ? bring detriment
657,676 ?????????????? 4
physical strength declines ? immune system weakens
676,902 ?????????????????? 4
sharp fall in government bond futures occurs ? interest rates increase
914,101 ?????????????? 4
have a margin of error ? cause trouble
Table 3: Examples of PROPcaus?s outputs.
5.4 Causality Hypothesis Generation
Here we show that our causality hypothesis genera-
tion method in Section 4.3 (PROPhyp) extracted one
million hypotheses with about 57% precision.
This experiment took the top 100,000 results of
PROPcaus as input, generated hypotheses from them,
and randomly selected 100 samples from the top one
million hypotheses. We evaluated only PROPcaus,
since we could not think of any reasonable baseline
for this task. Randomly coupling two phrases might
be a baseline, but it would perform so poorly that it
could not be a reasonable baseline.
The annotators judged each sample in the same
way as Section 5.3, except that we presented them
with source causality pairs from which hypotheses
were generated, as well as the original sentences of
these source pairs. Fleiss? kappa was 0.51 (moderate
agreement).
As a result, PROPhyp generated one million hy-
potheses with 57% precision. It generated various
kinds of hypotheses, since these 100 samples con-
tained 99 different template pairs. Table 4 shows
some causal hypotheses generated by PROPhyp. The
source causal pair is shown in parentheses. The la-
627
bels ?4? and ?6? denote whether a pair is causality
or not. The first 6 case was due to an error made by
Rank Causality Hypotheses (and their Origin) Label
18,886 ?????????????????? 4
(???????????????) 4
alleviate stress ? remedy insomnia
(increase stress ? continue to have insomnia)
93,781 ???????????????? 4
(????????????) 4
halt deflation ? tax revenue increases
(deflation is promoted ? tax revenes declines)
121,163 ?????????????????? 4
(???????????????) 4
enjoyment increases ? stress decreases
(enjoyment decreases ? stress grows)
205,486 ?????????????? 4
(??????????????) 4
decrease in crime ? diminish anxiety
(increase in crime ? heighten anxiety)
253,531 ????????????????? 4
(????????????????????) 4
reduce chlorine ? bacteria grow
(generate chlorine ? bacteria extinct)
450,353 ???????????????? 4
(????????????) 4
expand demand ? decrease unemployment rate
(decrease demand ? increase unemployment rate)
464,546 ??????????????????? 6
(??????????????????) 6
(ability of) digestion deteriorates ? cholesterol increases
(aid digestion ? decrease cholesterol)
538,310 ??????????????? 4
(?????????????) 4
relieve fatigue ? improve immunity
(feel fatigued ? immunity is weakened)
789,481 ??????????????? 4
(????????????????) 4
conditions improve ? prevent troubles
(conditions become bad ? cause troubles)
837,850 ????????????????? 6
(????????????????) 4
control economic conditions ? accompany problems
(economic conditions improve ? problems are solved)
Table 4: Examples of causality hypotheses.
our causality extraction method PROPcaus; the case
was erroneous since its original causality was erro-
neous. The second 6 case was due to the fact that
one of the contradiction phrase pairs used to gener-
ate the hypothesis was in fact not contradictory (?
?????????? 6? ??????? ?con-
trol economic conditions 6? economic conditions im-
prove?).
From these results, we conclude that our assump-
tion on causality hypothesis generation is valid.
6 Related Work
While the semantic orientation involving good/bad
(or desirable/undesirable) has been extensively stud-
ied (Hatzivassiloglou and McKeown, 1997; Turney,
2002; Rao and Ravichandran, 2009; Velikovich et
al., 2010), we believe Excitation represents a gen-
uinely new semantic orientation.
Most previous methods of contradiction extrac-
tion require either thesauri like Roget?s or WordNet
(Harabagiu et al2006; Mohammad et al2008; de
Marneffe et al2008) or large training data for su-
pervision (Turney, 2008). In contrast, our method
requires only a few seed templates. Lin et al2003)
used a few ?incompatibility? patterns to acquire
antonyms, but they did not report their method?s per-
formance on the incompatibility identification task.
Many methods for extracting causality or script-
like knowledge between events exist (Girju, 2003;
Torisawa, 2005; Torisawa, 2006; Abe et al2008;
Chambers and Jurafsky, 2009; Do et al2011; Shi-
bata and Kurohashi, 2011), but none uses a notion
similar to Excitation. As we have shown, we expect
that Excitation will improve their performance.
Regarding the acquisition of semantic knowledge
that is not explicitly written in corpora, Tsuchida et
al. (2011) proposed a novel method to generate se-
mantic relation instances as hypotheses using auto-
matically discovered inference rules. We think that
automatically generating plausible semantic knowl-
edge that is not written (explicitly) in corpora as hy-
potheses and augmenting semantic knowledge base
is important for the discovery of so-called ?unknown
unknowns? (Torisawa et al2010), among others.
7 Conclusion
We proposed a new semantic orientation, Excitation,
and its acquisition method. Our experiments showed
that Excitation allows to acquire one million con-
tradiction pairs with over 70% precision, as well as
causality pairs and causality hypotheses of the same
volume with reasonable precision from the Web. We
plan to make all our acquired knowledge resources
available to the research community soon (Visit
http://www.alagin.jp/index-e.html).
We will investigate additional applications of Ex-
citation in future work. For instance, we expect that
Excitation and its related semantic knowledge ac-
quired in this study will improve the performance
of Why-QA system like the one proposed by Oh et
al. (2012).
628
References
Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008.
Two-phrased event relation acquisition: Coupling the
relation-oriented and argument-oriented approaches.
In Proceedings of the 22nd International Conference
on Computational Linguistics (COLING 2008), pages
1?8.
Susumu Akamine, Daisuke Kawahara, Yoshikiyo Kato,
Tetsuji Nakagawa, Yutaka I. Leon-Suematsu, Takuya
Kawada, Kentaro Inui, Sadao Kurohashi, and Yutaka
Kidawara. 2010. Organizing information on the web
to support user judgments on information credibil-
ity. In Proceedings of 2010 4th International Uni-
versal Communication Symposium Proceedings (IUCS
2010), pages 122?129.
Alina Andreevskaia and Sabine Bergler. 2006. Semantic
tag extraction from wordnet glosses. In Proceedings
of the 5th International Conference on Language Re-
sources and Evaluation (LREC 2006).
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In Proceedings of the 47th Annual Meeting
of the ACL and the 4th IJCNLP of the AFNLP (ACL-
IJCNLP 2009), pages 602?610.
Marie-Catherine de Marneffe, Anna Rafferty, and
Christopher D. Manning. 2008. Finding contradiction
in text. In Proceedings of the 48th Annual Meeting of
the Association of Computational Linguistics: Human
Language Technologies (ACL-08: HLT), pages 1039?
1047.
Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011.
Minimally supervised event causality identification.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2011), pages 294?303.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
Roxana Girju. 2003. Automatic detection of causal re-
lations for question answering. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2003), Workshop on Multi-
lingual Summarization and Question Answering - Ma-
chine Learning and Beyond, pages 76?83.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast and contradiction in text pro-
cessing. In Proceedings of the 21st National Confer-
ence on Artificial Intelligence (AAAI-06), pages 755?
762.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35 Annual Meeting of
the Association for Computational Linguistics and the
8the Conference of the European Chapter of the Asso-
ciation of Computational Linguistics, pages 174?181.
Daisuke Kawahara and Sadao Kurohashi. 2006. A fully-
lexicalized probabilistic model for Japanese syntactic
and case structure analysis. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL (HLT-NAACL2006),
pages 176?183.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159?174.
Dekang Lin, Shaojun Zhao Lijuan Qin, and Ming Zhou.
2003. Identifying synonyms among distributionally
similar words. In Proceedings of the 18th Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-03), pages 1492?1493.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics (COLING-ACL1998), pages 768?
774.
Saif Mohammad, Bonnie Dorr, and Greame Hirst. 2008.
Computing word-pair antonymy. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 982?991.
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Takuya Kawada, Stijn De Saeger, Junichi Kazama, and
Yiou Wang. 2012. Why question answering using
sentiment analysis and word classes. In Proceedings
of EMNLP-CoNLL 2012: Conference on Empirical
Methods in Natural Language Processing and Natural
Language Learning.
Charles E. Osgood, George J. Suci, and Percy H. Tannen-
baum. 1957. The measurement of meaning. Univer-
sity of Illinois Press.
James Pustejovsky. 1995. The Generative Lexicon. MIT
Press.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceedings
of the 12th Conference of the European Chapter of the
ACL, pages 675?682.
Tomohide Shibata and Sadao Kurohashi. 2011. Acquir-
ing strongly-related events using predicate-argument
co-occurring statistics and case frames. In Proceed-
ings of the 5th International Joint Conference on Natu-
ral Language Processing (IJCNLP 2011), pages 1028?
1036.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. MIT
Press.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientation of words using
629
spin model. In Proceedings of the 43rd Annual Meet-
ing of the ACL, pages 133?140.
Kentaro Torisawa, Stijn De Saeger, Jun?ichi Kazama,
Asuka Sumida, Daisuke Noguchi, Yasunari Kakizawa,
Masaki Murata, Kow Kuroda, and Ichiro Yamada.
2010. Organizing the web?s information explosion
to discover unknown unknowns. New Generation
Computing (Special Issue on Information Explosion),
28(3):217?236.
Kentaro Torisawa. 2005. Automatic acquisition of ex-
pressions representing preparation and utilization of
an object. In Proceedings of the Recent Advances
in Natural Language Processing (RANLP05), pages
556?560.
Kentaro Torisawa. 2006. Acquiring inference rules
with temporal constraints by using japanese coordi-
nated sentences and noun-verb co-occurrences. In
Proceedings of the Human Language Technology Con-
ference of the North American Chapter of the ACL
(HLT-NAACL2006), pages 57?64.
Masaaki Tsuchida, Kentaro Torisawa, Stijn De Saeger,
Jong-Hoon Oh, Jun?ichi Kazama, Chikara Hashimoto,
and Hayato Ohwada. 2011. Toward finding semantic
relations not written in a single sentence: An inference
method using auto-discovered rules. In Proceedings
of the 5th International Joint Conference on Natural
Language Processing (IJCNLP 2011), pages 902?910.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL 2002), pages 417?424.
Peter Turney. 2008. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics (COLING 2008), pages 905?912.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and RyanMcDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the ACL, pages 777?785.
Ellen M. Voorhees. 2008. Contradictions and justifica-
tions: Extensions to the textual entailment task. In
Proceedings of the 48th Annual Meeting of the Associ-
ation of Computational Linguistics: Human Language
Technologies (ACL-08: HLT), pages 63?71.
630
Proceedings of NAACL-HLT 2013, pages 63?73,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Minimally Supervised Method for Multilingual Paraphrase Extraction
from Definition Sentences on the Web
Yulan Yan? Chikara Hashimoto? Kentaro Torisawa?
Takao Kawai? Jun?ichi Kazama? Stijn De Saeger??
? ? ? ?? ?? Information Analysis Laboratory
Universal Communication Research Institute
National Institute of Information and Communications Technology (NICT)
{? yulan, ? ch, ? torisawa, ??stijn}@nict.go.jp
Abstract
We propose a minimally supervised method
for multilingual paraphrase extraction from
definition sentences on the Web. Hashimoto
et al (2011) extracted paraphrases from
Japanese definition sentences on the Web, as-
suming that definition sentences defining the
same concept tend to contain paraphrases.
However, their method requires manually an-
notated data and is language dependent. We
extend their framework and develop a mini-
mally supervised method applicable to multi-
ple languages. Our experiments show that our
method is comparable to Hashimoto et al?s
for Japanese and outperforms previous unsu-
pervised methods for English, Japanese, and
Chinese, and that our method extracts 10,000
paraphrases with 92% precision for English,
82.5% precision for Japanese, and 82% preci-
sion for Chinese.
1 Introduction
Automatic paraphrasing has been recognized as an
important component for NLP systems, and many
methods have been proposed to acquire paraphrase
knowledge (Lin and Pantel, 2001; Barzilay and
McKeown, 2001; Shinyama et al, 2002; Barzilay
and Lee, 2003; Dolan et al, 2004; Callison-Burch,
2008; Hashimoto et al, 2011; Fujita et al, 2012).
We propose a minimally supervised method for
multilingual paraphrase extraction. Hashimoto et al
(2011) developed a method to extract paraphrases
from definition sentences on the Web, based on
their observation that definition sentences defining
the same concept tend to contain many paraphrases.
Their method consists of two steps; they extract def-
inition sentences from the Web, and extract phrasal
(1) a. Paraphrasing is the use of your own words to express the au-
thor?s ideas without changing the meaning.
b. Paraphrasing is defined as a process of transforming an expres-
sion into another while keeping its meaning intact.
(2) a. ?????????????????????????
???????????????? (Paraphrasing refers to
the replacement of an expression into another without changing
the semantic content.)
b. ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
???????????????????????
????????? (Paraphrasing is a process of trans-
forming an expression into another of the same language while
preserving the meaning and content as much as possible.)
(3) a. ????????????????????????
??????? (Paraphrasing refers to the transformation
of sentence structure by the translator without changing the
meaning of original text.)
b. ?????????????????????????
(Paraphrasing is a translation method of keeping the content of
original text but not keeping the expression.)
Figure 1: Multilingual definition pairs on ?paraphrasing.?
paraphrases from the definition sentences. Both
steps require supervised classifiers trained by manu-
ally annotated data, and heavily depend on their tar-
get language. However, the basic idea is actually
language-independent. Figure 1 gives examples of
definition sentences on the Web that define the same
concept in English, Japanese, and Chinese (with En-
glish translation). As indicated by underlines, each
definition pair has a phrasal paraphrase.
We aim at extending Hashimoto et al?s method
to a minimally supervised method, thereby enabling
acquisition of phrasal paraphrases within one lan-
guage, but in different languages without manually
annotated data. The first contribution of our work
is to develop a minimally supervised method for
multilingual definition extraction that uses a clas-
sifier distinguishing definition from non-definition.
The classifier is learnt from the first sentences in
63
Defini?n	 ?sentences	 Defini?n	 ?pairs	 ? Paraphrase	 ?candidates	 ?
Ranked	 ?paraphrase	 ?candidates	 ?Classifier	Web	
Defini?on	 ?Extrac?on	 ?(Sec?on	 ?2.1)	 Paraphrase	 ?Extrac?on	 ?(Sec?on	 ?2.2)	
Ranking	 ?by	 ?Score	Automa?cally	 ?constructed	 ?training	 ?data	
Web	 Wikipedia	
Figure 2: Overall picture of our method.
Wikipedia articles, which can be regarded as the def-
inition of the title of Wikipedia article (Kazama and
Torisawa, 2007) and hence can be used as positive
examples. Our method relies on a POS tagger, a de-
pendency parser, a NER tool, noun phrase chunking
rules, and frequency thresholds for each language,
in addition to Wikipedia articles, which can be seen
as a manually annotated knowledge base. How-
ever, our method needs no additional manual anno-
tation particularly for this task and thus we catego-
rize our method as a minimally supervised method.
On the other hand, Hashimoto et al?s method heav-
ily depends on the properties of Japanese like the
assumption that characteristic expressions of defini-
tion sentences tend to appear at the end of sentence
in Japanese. We show that our method is applica-
ble to English, Japanese, and Chinese, and that its
performance is comparable to state-of-the-art super-
vised methods (Navigli and Velardi, 2010). Since
the three languages are very different we believe that
our definition extraction method is applicable to any
language as long as Wikipedia articles of the lan-
guage exist.
The second contribution of our work is to de-
velop a minimally supervised method for multi-
lingual paraphrase extraction from definition sen-
tences. Again, Hashimoto et al?s method utilizes
a supervised classifier trained with annotated data
particularly prepared for this task. We eliminate the
need for annotation and instead introduce a method
that uses a novel similarity measure considering
the occurrence of phrase fragments in global con-
texts. Our paraphrase extraction method is mostly
language-independent and, through experiments for
the three languages, we show that it outperforms
unsupervised methods (Pas?ca and Dienes, 2005;
Koehn et al, 2007) and is comparable to Hashimoto
et al?s supervised method for Japanese.
Previous methods for paraphrase (and entailment)
extraction can be classified into a distributional sim-
ilarity based approach (Lin and Pantel, 2001; Gef-
fet and Dagan, 2005; Bhagat et al, 2007; Szpek-
tor and Dagan, 2008; Hashimoto et al, 2009) and a
parallel corpus based approach (Barzilay and McK-
eown, 2001; Shinyama et al, 2002; Barzilay and
Lee, 2003; Dolan et al, 2004; Callison-Burch,
2008). The former can exploit large scale monolin-
gual corpora, but is known to be unable to distin-
guish paraphrase pairs from antonymous pairs (Lin
et al, 2003). The latter rarely mistakes antonymous
pairs for paraphrases, but preparing parallel corpora
is expensive. As with Hashimoto et al (2011), our
method is a kind of parallel corpus approach in that it
uses definition pairs as a parallel corpus. However,
our method does not suffer from a high labor cost
of preparing parallel corpora, since it can automati-
cally collect definition pairs from the Web on a large
scale. The difference between ours and Hashimoto
et al?s is that our method requires no manual label-
ing of data and is mostly language-independent.
2 Proposed Method
Our method first extracts definition sentences from
the Web, and then extracts paraphrases from the def-
inition sentences, as illustrated in Figure 2.
2.1 Definition Extraction
2.1.1 Automatic Construction of Training Data
Our method learns a classifier that classifies sen-
tences into definition and non-definition using auto-
matically constructed training data, TrDat. TrDat?s
positive examples, Pos, are the first sentences of
Wikipedia articles and the negative examples, Neg,
are randomly sampled Web sentences. The former
can be seen as definition, while the chance that the
sentences in the latter are definition is quite small.
Our definition extraction not only distinguishes
definition from non-definition but also identities the
defined term of a definition sentence, and in the
paraphrase extraction step our method couples two
definition sentences if their defined terms are identi-
cal. For example, the defined terms of (1a) and (1b)
in Figure 1 are both ?Paraphrasing? and thus the two
definition sentences are coupled. For Pos, we mark
up the title of Wikipedia article as the defined term.
For Neg, we randomly select a noun phrase in a sen-
64
(A)
N-gram definition pattern N-gram non-definition pattern
?[term] is the [term] may be
[term] is a type of [term] is not
(B)
Subsequence definition pattern Subsequence non-definition pattern
[term] is * which is located you may * [term]
[term] is a * in the was [term] * , who is
(C)
Subtree definition pattern Subtree non-definition pattern
[term] is defined as the NP [term] will not be
Table 1: Examples of English patterns.
tence and mark it up as a (false) defined term. Any
marked term is uniformly replaced with [term].
2.1.2 Feature Extraction and Learning
As features, we use patterns that are characteristic
of definition (definition patterns) and those that are
unlikely to be a part of definition (non-definition pat-
terns). Patterns are either N-grams, subsequences, or
dependency subtrees, and are mined automatically
from TrDat. Table 1 shows examples of patterns
mined by our method. In (A) of Table 1, ??? is
a symbol representing the beginning of a sentence.
In (B), ?*? represents a wildcard that matches any
number of arbitrary words. Patterns are represented
by either their words? surface form, base form, or
POS. (Chinese words do not inflect and thus we do
not use the base form for Chinese.)
We assume that definition patterns are fre-
quent in Pos but are infrequent in Neg, and
non-definition patterns are frequent in Neg but
are infrequent in Pos. To see if a given pat-
tern ? is likely to be a definition pattern, we
measure ??s probability rate Rate(?). If the
probability rate of ? is large, ? tends to be a
definition pattern. The probability rate of ? is:
Rate(?) =
freq(?,Pos)/|Pos|
freq(?,Neg)/|Neg|
, iffreq(?,Neg) 6= 0.
Here, freq(?,Pos) = |{s ? Pos : ? ? s}| and
freq(?,Neg) = |{s ? Neg : ? ? s}|. We write ? ? s
if sentence s contains ?. If freq(?,Neg) = 0,
Rate(?) is set to the largest value of all the patterns?
Rate values. Only patterns whose Rate is more
than or equal to a Rate threshold ?pos and whose
freq(?,Pos) is more than or equal to a frequency
threshold are regarded as definition patterns. Simi-
larly, we check if ? is likely to be a non-definition
pattern. Only patterns whose Rate is less or equal
English Japanese Chinese
Type Representation Pos Neg Pos Neg Pos Neg
N-gram
Surface 120 400 30 100 20 100
Base 120 400 30 100 ? ?
POS 2,000 4,000 500 500 100 400
Subsequence
Surface 120 400 30 100 20 40
Base 120 400 30 100 ? ?
POS 2,000 2,000 500 500 200 400
Subtree
Surface 5 10 5 10 5 5
Base 5 10 5 10 ? ?
POS 25 50 25 50 25 50
Table 2: Values of frequency threshold.
to a Rate threshold ?neg and whose freq(?,Neg)
is more than or equal to a frequency threshold are
regarded as non-definition patterns. The probability
rate is based on the growth rate (Dong and Li,
1999).
?pos and ?neg are set to 2 and 0.5, while the fre-
quency threshold is set differently according to lan-
guages, pattern types (N-gram, subsequence, and
subtree), representation (surface, base, and POS),
and data (Pos and Neg), as in Table 2. The thresholds
in Table 2 were determined manually, but not really
arbitrarily. Basically they were determined accord-
ing to the frequency of each pattern in our data (e.g.
how frequently the surface N-gram of English ap-
pears in English positive training samples (Pos)).
Below, we detail how patterns are acquired. First,
we acquire N-gram patterns. Then, subsequence
patterns are acquired using the N-gram patterns as
input. Finally, subtree patterns are acquired using
the subsequence patterns as input.
N-gram patterns We collect N-gram patterns
from TrDat with N ranging from 2 to 6. We filter
out N-grams using thresholds on the Rate and fre-
quency, and regard those that are kept as definition
or non-definition N-grams.
Subsequence patterns We generate subsequence
patterns as ordered combinations of N-grams with
the wild card ?*? inserted between them (we use
two or three N-grams for a subsequence). Then, we
check each of the generated subsequences and keep
it if there exists a sentence in TrDat that contains the
subsequence and whose root node is contained in the
subsequence. For example, subsequence ?[term]
is a * in the? is kept if a term-marked sentence like
?[term] is a baseball player in the Dominican Re-
public.? exists in TrDat. Then, patterns are filtered
65
out using thresholds on the Rate and frequency as
we did for N-grams.
Subtree patterns For each definition and non-
definition subsequence, we retrieve all the term-
marked sentences that contain the subsequence from
TrDat, and extract a minimal dependency subtree
that covers all the words of the subsequence from
each retrieved sentence. For example, assume that
we retrieve a term-marked sentence ?[term] is
usually defined as the way of life of a group of peo-
ple.? for subsequence ?[term] is * defined as the?.
Then we extract from the sentence the minimal de-
pendency subtree in the left side of (C) of Table 1.
Note that all the words of the subsequence are con-
tained in the subtree, and that in the subtree a node
(?way?) that is not a part of the subsequence is re-
placed with its dependency label (?NP?) assigned by
the dependency parser. The patterns are filtered out
using thresholds on the Rate and frequency.
We train a SVM classifier1 with a linear kernel,
using binary features that indicate the occurrence of
the patterns described above in a target sentence.
In theory, we could feed all the features to the
SVM classifier and let the classifier pick informa-
tive features. But we restricted the feature set for
practical reasons: the number of features would be-
come tremendously large. There are two reasons for
this. First, the number of sentences in our automati-
cally acquired training data is huge (2,439,257 posi-
tive sentences plus 5,000,000 negative sentences for
English, 703,208 positive sentences plus 1,400,000
negative sentences for Japanese and 310,072 posi-
tive sentences plus 600,000 negative sentences for
Chinese). Second, since each subsequence pattern
is generated as a combination of two or three N-
gram patterns and one subsequence pattern can gen-
erate one or more subtree patterns, using all possi-
ble features leads to a combinatorial explosion of
features. Moreover, since the feature vector will be
highly sparse with a huge number of infrequent fea-
tures, SVM learning becomes very time consuming.
In preliminary experiments we observed that when
using all possible features the learning process took
more than one week for each language. We there-
fore introduced the current feature selection method,
in which the learning process finished in one day but
1http://svmlight.joachims.org.
Original Web sentence: Albert Pujols is a baseball player.
Term-marked sentence 1: [term] is a baseball player.
Term-marked sentence 2: Albert Pujols is a [term].
Figure 3: Term-marked sentences from a Web sentence.
still obtains good results.
2.1.3 Definition Extraction from the Web
We extract a large amount of definition sen-
tences by applying this classifier to sentences in our
Web archive. Because our classifier requires term-
marked sentences (sentences in which the term be-
ing defined is marked) as input, we first have to iden-
tify all such defined term candidates for each sen-
tence. For example, Figure 3 shows a case where a
Web sentence has two NPs (two candidates of de-
fined term). Basically we pick up NPs in a sen-
tence by simple heuristic rules. For English, NPs are
identified using TreeTagger (Schmid, 1995) and two
NPs are merged into one when they are connected by
?for? or ?of?. After applying this procedure recur-
sively, the longest NPs are regarded as candidates of
defined terms and term-marked sentences are gener-
ated. For Japanese, we first identify nouns that are
optionally modified by adjectives as NPs, and allow
two NPs connected by ??? (of ), if any, to form
a larger NP. For Chinese, nouns that are optionally
modified by adjectives are considered as NPs.
Then, each term-marked sentence is given a fea-
ture vector and classified by the classifier. The term-
marked sentence whose SVM score (the distance
from the hyperplane) is the largest among those from
the same original Web sentence is chosen as the final
classification result for the original Web sentence.
2.2 Paraphrase Extraction
We use all the Web sentences classified as defini-
tion and all the sentences in Pos for paraphrase ex-
traction. First, we couple two definition sentences
whose defined term is the same. We filter out defini-
tion sentence pairs whose cosine similarity of con-
tent word vectors is less than or equal to threshold
C, which is set to 0.1. Then, we extract phrases
from each definition sentence, and generate all pos-
sible phrase pairs from the coupled sentences. In
this study, phrases are restricted to predicate phrases
that consist of at least one dependency relation and
in which all the constituents are consecutive in a
66
f1
The ratio of the number of words shared between two can-
didate phrases to the number of all of the words in the two
phrases. Words are represented by either their surface form
(f1,1), base form (f1,2) or POS (f1,3).
f2
The identity of the leftmost word (surface form (f2,1), base
form (f2,2) or POS (f2,3)) between two candidate phrases.
f3
The same as f2 except that we use the rightmost word.
There are three corresponding subfunctions (f3,1 to f3,3).
f4
The ratio of the number of words that appear in a candidate
phrase segment of a definition sentence s1 and in a segment
that is NOT a part of the candidate phrase of another def-
inition sentence s2 to the number of all the words of s1?s
candidate phrase. Words are in their base form (f4,1).
f5 The reversed (s1 ? s2) version of f4,1 (f5,1).
f6
The ratio of the number of words (the surface form) of a
shorter candidate phrase to that of a longer one (f6,1).
f7
Cosine similarity between two definition sentences from
which two candidate phrases are extracted. Only content
words in the base form are used (f7,1).
f8
The ratio of the number of parent dependency subtrees that
are shared by two candidate phrases to the number of all the
parent dependency subtrees. The parent dependency sub-
trees are adjacent to the candidate phrases and represented
by their surface form (f8,1), base form (f8,2), or POS (f8,3).
f9
The same as f8 except that we use child dependency sub-
trees. There are 3 subfunctions (f9,1 to f9,3) of f9 type.
f10
The ratio of the number of context N-grams that are shared
by two candidate phrases to the number of all the context N-
grams of both candidate phrases. The context N-grams are
adjacent to the candidate phrases and represented by either
the surface form, the base form, or POS. The N ranges from
1 to 3, and the context is either left-side or right-side. Thus,
there are 18 subfunctions (3? 3? 2).
Table 3: Local similarity subfunctions, f1,1 to f10,18.
sentence. Accordingly, if two definition sentences
that are coupled have three such predicate phrases
respectively, we get nine phrase pairs, for instance.
A phrase pair extracted from a definition pair is a
paraphrase candidate and is given a score that indi-
cates the likelihood of being a paraphrase, Score. It
consists of two similarity measures, local similarity
and global similarity, which are detailed below.
Local similarity Following Hashimoto et al, we
assume that two candidate phrases (p1, p2) tend to
be a paraphrase if they are similar enough and/or
their surrounding contexts are sufficiently similar.
Then, we calculate the local similarity (localSim) of
(p1, p2) as the weighted sum of 37 similarity sub-
functions that are grouped into 10 types (Table 3.)
For example, the f1 type consists of three subfunc-
tions, f1,1, f1,2, and f1,3. The 37 subfunctions are
inspired by Hashimoto et al?s features. Then, local-
Sim is defined as:
localSim(p1, p2) = max
(dl,dm)?DP (p1,p2)
ls(p1, p2, dl, dm).
Here, ls(p1, p2, dl, dm) =
?10
i=1
?ki
j=1
wi,j?fi,j(p1,p2,dl,dm)
ki .
DP (p1, p2) is the set of all definition sentence pairs
that contain (p1, p2). (dl, dm) is a definition sen-
tence pair containing (p1, p2). ki is the number
of subfunctions of fi type. wi,j is the weight for
fi,j . wi,j is uniformly set to 1 except for f4,1
and f5,1, whose weight is set to ?1 since they
indicate the unlikelihood of (p1, p2)?s being a
paraphrase. As the formula indicates, if there is
more than one definition sentence pair that contains
(p1, p2), localSim is calculated from the definition
sentence pair that gives the maximum value of
ls(p1, p2, dl, dm). localSim is local in the sense that
it is calculated based on only one definition pair
from which (p1, p2) are extracted.
Global similarity The global similarity (global-
Sim) is our novel similarity function. We decompose
a candidate phrase pair (p1, p2) into Comm, the com-
mon part between p1 and p2, and Diff , the difference
between the two. For example, Comm and Diff of
(?keep the meaning intact?, ?preserve the meaning?)
is (?the meaning?) and (?keep, intact?, ?preserve?).
globalSim measures the semantic similarity of
the Diff of a phrase pair. It is proposed based on
the following intuition: phrase pair (p1, p2) tend
to be a paraphrase if their surface difference (i.e.
Diff ) have the same meaning. For example, if
?keep, intact? and ?preserve? mean the same, then
(?keep the meaning intact?, ?preserve the meaning?)
is a paraphrase.
globalSim considers the occurrence of Diff in
global contexts (i.e., all the paraphrase candidates
from all the definition pairs). The globalSim of a
given phrase pair (p1, p2) is measured by basically
counting how many times the Diff of (p1, p2) ap-
pears in all the candidate phrase pairs from all the
definition pairs. The assumption is that Diff tends to
share the same meaning if it appears repeatedly in
paraphrase candidates from all definition sentence
pairs, i.e., our parallel corpus. Each occurrence of
Diff is weighted by the localSim of the phrase pair
in which Diff occurs. Precisely, globalSim is defined
as:
67
Threshold The frequency threshold of Table 2 (Section 2.1.2).
NP rule Rules for identifying NPs in sentences (Section 2.1.3).
POS list The list of content words? POS (Section 2.2).
Tagger/parser POS taggers, dependency parsers and NER tools.
Table 4: Language-dependent components.
globalSim(p1, p2) =
?
(pi,pj)?PP (p1,p2)
localSim(pi, pj)
M
.
PP (p1, p2) is the set of candidate phrase pairs
whose Diff is the same as (p1, p2).2 M is the num-
ber of similarity subfunction types whose weight is
1, i.e. M = 8 (all the subfunction types except f4
and f5). It is used to normalize the value of each
occurrence of Diff to [0, 1].3 globalSim is global
in the sense that it considers all the definition pairs
that have a phrase pair with the same Diff as a target
candidate phrase pair (p1, p2).
The final score for a candidate phrase pair is:
Score(p1, p2) = localSim(p1, p2) + ln globalSim(p1, p2).
The way of combining the two similarity functions
has been determined empirically after testing several
other ways of combining them. This ranks all the
candidate phrase pairs.
Finally, we summarize language-dependent com-
ponents that we fix manually in Table 4.
3 Experiments
3.1 Experiments of Definition Extraction
We show that our unsupervised definition extrac-
tion method is competitive with state-of-the-art su-
pervised methods for English (Navigli and Velardi,
2010), and that it extracts a large number of defini-
tions reasonably accurately for English (3,216,121
definitions with 70% precision), Japanese (651,293
definitions with 62.5% precision), and Chinese
(682,661 definitions with 67% precision).
2If there are more than one (pi, pj) in a definition pair, we
use only one of them that has the largest localSim value.
3Although we claim that our idea of using globalSim is ef-
fective, we do not claim that the above formula for calculating
is the optimal way to implement the idea. Currently we are in-
vestigating a more mathematically well-motivated model.
3.1.1 Preparing Corpora
First we describe Pos, Neg, and the Web corpus
from which definition sentences are extracted. As
the source of Pos, we used the English Wikipedia
of April 2011 (3,620,149 articles), the Japanese
Wikipedia of October 2011 (830,417 articles), and
the Chinese Wikipedia of August 2011 (365,545 ar-
ticles). We removed category articles, template ar-
ticles, list articles and so on from them. Then the
number of sentences of Pos was 2,439,257 for En-
glish, 703,208 for Japanese, and 310,072 for Chi-
nese. We verified our assumption that Wikipedia
first sentences can mostly be seen as definition by
manually checking 200 random samples from Pos.
96.5% of English Pos, 100% of Japanese Pos, and
99.5% of Chinese Pos were definitions.
As the source of Neg, we used 600 million
Japanese Web pages (Akamine et al, 2010) and
the ClueWeb09 corpus for English (about 504 mil-
lion pages) and Chinese (about 177 million pages).4
From each Web corpus, we collected the sentences
satisfying following conditions: 1) they contain 5
to 50 words and at least one verb, 2) less than half
of their words are numbers, and 3) they end with a
period. Then we randomly sampled sentences from
the collected sentences as Neg so that |Neg| was
about twice as large as |Pos|: 5,000,000 for English,
1,400,000 for Japanese, and 600,000 for Chinese.
In Section 3.1.3, we use 10% of the Web corpus as
the input to the definition classifier. The number of
sentences are 294,844,141 for English, 245,537,860
for Japanese, and 68,653,130 for Chinese.
All the sentences were POS-tagged and parsed.
We used TreeTagger and MSTParser (McDonald
et al, 2006) for English, JUMAN (Kurohashi and
Kawahara, 2009a) and KNP (Kurohashi and Kawa-
hara, 2009b) for Japanese, MMA (Kruengkrai et al,
2009) and CNP (Chen et al, 2009) for Chinese.
3.1.2 Comparison with Previous Methods
We compared our method with the state-of-the-
art supervised methods proposed by Navigli and Ve-
lardi (2010), using their WCL datasets v1.0 (http:
//lcl.uniroma1.it/wcl/), definition and non-
definition datasets for English (Navigli et al, 2010).
Specifically, we used its training data (TrDatwcl,
hereafter), which consisted of 1,908 definition and
4http://lemurproject.org/clueweb09.php/
68
Method Precision Recall F1 Accuracy
Proposeddef 86.79 86.97 86.88 89.18
WCL-1 99.88 42.09 59.22 76.06
WCL-3 98.81 60.74 75.23 83.48
Table 5: Definition classification results on TrDatwcl.
2,711 non-definition sentences, and compared the
following three methods. WCL-1 and WCL-3 are
methods proposed by Navigli and Velardi (2010).
They were trained and tested with 10 fold cross vali-
dation using TrDatwcl. Proposeddef is our method,
which used TrDat for acquiring patterns (Section
2.1.2) and training. We tested Proposeddef on each
of TrDatwcl?s 10 folds and averaged the results.
Note that, for Proposeddef , we removed sentences
in TrDatwcl from TrDat in advance for fairness.
Table 5 shows the results. The numbers for WCL-
1 and WCL-3 are taken from Navigli and Velardi
(2010). Proposeddef outperformed both methods in
terms of recall, F1, and accuracy. Thus, we conclude
that Proposeddef is comparable to WCL-1/WCL-3.
We conducted ablation tests of our method to in-
vestigate the effectiveness of each type of pattern.
When using only N-grams, F1 was 85.41. When
using N-grams and subsequences, F1 was 86.61.
When using N-grams and subtrees, F1 was 86.85.
When using all the features, F1 was 86.88. The re-
sults show that each type of patterns contribute to the
performance, but the contributions of subsequence
patterns and subtree patterns do not seem very sig-
nificant.
3.1.3 Experiments of Definition Extraction
We extracted definitions from 10% of the Web
corpus. We applied Proposeddef to the cor-
pus of each language, and the state-of-the-art su-
pervised method for Japanese (Hashimoto et al,
2011) (Hashidef , hereafter) to the Japanese corpus.
Hashidef was trained on their training data that con-
sisted of 2,911 sentences, 61.1% of which were def-
initions. Note that we removed sentences in TrDat
from 10% of the Web corpus in advance, while we
did not remove Hashimoto et al?s training data from
the corpus. This means that, for Hashidef , the train-
ing data is included in the test data.
For each method, we filtered out its positive out-
puts whose defined term appeared more than 1,000
times in 10% of the Web corpus, since those terms
tend to be too vague to be a defined term or re-
fer to an entity outside the definition sentence. For
example, if ?the college? appears more than 1,000
times in 10% of the corpus, we filter out sen-
tences like ?The college is one of three colleges
in the Coast Community College District and was
founded in 1947.? For Proposeddef , the number of
remaining positive outputs is 3,216,121 for English,
651,293 for Japanese, and 682,661 for Chinese. For
Hashidef , the number of positive outputs is 523,882.
For Proposeddef of each language, we randomly
sampled 200 sentences from the remaining positive
outputs. For Hashidef , we first sorted its output by
the SVM score in descending order and then ran-
domly sampled 200 from the top 651,293, i.e., the
same number as the remaining positive outputs of
Proposeddef of Japanese, out of all the remaining
sentences of Hashidef .
For each language, after shuffling all the samples,
two human annotators evaluated each sample. The
annotators for English and Japanese were not the au-
thors, while one of the Chinese annotators was one
of the authors. We regarded a sample as a defini-
tion if it was regarded as a definition by both an-
notators. Cohen?s kappa (Cohen, 1960) was 0.55
for English (moderate agreement (Landis and Koch,
1977)), 0.73 for Japanese (substantial agreement),
and 0.69 for Chinese (substantial agreement).
For English, Proposeddef achieved 70% precision
for the 200 samples. For Japanese, Proposeddef
achieved 62.5% precision for the 200 samples, while
Hashidef achieved 70% precision for the 200 sam-
ples. For Chinese, Proposeddef achieved 67% pre-
cision for the 200 samples. From these results, we
conclude that Proposeddef can extract a large num-
ber of definition sentences from the Web moderately
well for the three languages.
Although the precision is not very high, our ex-
periments in the next section show that we can still
extract a large number of paraphrases with high pre-
cision from these definition sentences, due mainly to
our similarity measures, localSim and globalSim.
3.2 Experiments of Paraphrase Extraction
We show (1) that our paraphrase extraction method
outperforms unsupervised methods for the three lan-
guages, (2) that globalSim is effective, and (3) that
our method is comparable to the state-of-the-art su-
69
ProposedScore: Our method. Outputs are ranked by Score.
Proposedlocal: This is the same as ProposedScore except that it ranks
outputs by localSim. The performance drop from ProposedScore
shows globalSim?s effectiveness.
Hashisup: Hashimoto et al?s supervised method. Training data is the
same as Hashimoto et al Outputs are ranked by the SVM score
(the distance from the hyperplane). This is for Japanese only.
Hashiuns: The unsupervised version of Hashisup. Outputs are
ranked by the sum of feature values. Japanese only.
SMT: The phrase table construction method of Moses (Koehn et al,
2007). We assume that Moses should extract a set of two phrases
that are paraphrases of each other, if we input monolingual par-
allel sentence pairs like our definition pairs. We used default
values for all the parameters. Outputs are ranked by the product
of two phrase translation probabilities of both directions.
P&D: The distributional similarity based method by Pas?ca and Di-
enes (2005) (their ?N-gram-Only? method). Outputs are ranked
by the number of contexts two phrases share. Following Pas?ca
and Dienes (2005), we used the parameters LC = 3 and
MaxP = 4, while MinP , which was 1 in Pas?ca and Dienes
(2005), was set to 2 since our target was phrasal paraphrases.
Table 6: Evaluated paraphrase extraction methods.
pervised method for Japanese.
3.2.1 Experimental Setting
We extracted paraphrases from definition sen-
tences in Pos and those extracted by Proposeddef in
Section 3.1.3. First we coupled two definition sen-
tences whose defined term was the same. The num-
ber of definition pairs was 3,208,086 for English,
742,306 for Japanese, and 457,233 for Chinese.
Then we evaluated six methods in Table 6.5 All
the methods except P&D took the same definition
pairs as input, while P&D?s input was 10% of the
Web corpus. The input can be seen as the same for
all the methods, since the definition pairs were de-
rived from that 10% of the Web corpus. In our ex-
periments Exp1 and Exp2 below, all evaluation sam-
ples were shuffled so that human annotators could
not know which sample was from which method.
Annotators were the same as those who conducted
the evaluation in Section 3.1.3. Cohen?s kappa (Co-
hen, 1960) was 0.83 for English, 0.88 for Japanese,
5We filtered out phrase pairs in which one phrase contained a
named entity but the other did not contain the named entity from
the output of ProposedScore, Proposedlocal, SMT , and P&D,
since most of them were not paraphrases. We used Stanford
NER (Finkel et al, 2005) for English named entity recognition
(NER), KNP for Japanese NER, and BaseNER (Zhao and Kit,
2008) for Chinese NER. Hashisup and Hashiuns did the named
entity filtering of the same kind (footnote 3 of Hashimoto et al
(2011)), and thus we did not apply the filter to them any further.
and 0.85 for Chinese, all of which indicated reason-
ably good (Landis and Koch, 1977). We regarded a
candidate phrase pair as a paraphrase if both annota-
tors regarded it as a paraphrase.
Exp1 We compared the methods that take def-
inition pairs as input, i.e. ProposedScore, Pro-
posedlocal, Hashisup, Hashiuns, and SMT . We ran-
domly sampled 200 phrase pairs from the top 10,000
for each method for evaluation. The evaluation of
each candidate phrase pair (p1, p2) was based on
bidirectional checking of entailment relation, p1 ?
p2 and p2 ? p1, with p1 and p2 embedded in con-
texts, as Hashimoto et al (2011) did. Entailment
relation of both directions hold if (p1, p2) is a para-
phrase. We used definition pairs from which candi-
date phrase pairs were extracted as contexts.
Exp2 We compared ProposedScore and P&D.
Since P&D restricted its output to phrase pairs in
which each phrase consists of two to four words,
we restricted the output of ProposedScore to 2-to-4-
words phrase pairs, too. We randomly sampled 200
from the top 3,000 phrase pairs from each method
for evaluation, and the annotators checked entail-
ment relation of both directions between two phrases
using Web sentence pairs that contained the two
phrases as contexts.
3.2.2 Results
From Exp1, we obtained precision curves in the
upper half of Figure 4. The curves were drawn from
the 200 samples that were sorted in descending order
by their score, and we plotted a dot for every 5 sam-
ples. ProposedScore outperformed Proposedlocal for
the three languages, and thus globalSim was effec-
tive. ProposedScore outperformed Hashisup. How-
ever, we observed that ProposedScore acquired many
candidate phrase pairs (p1, p2) for which p1 and p2
consisted of the same content words like ?send a
postcard to the author? and ?send the author a post-
card,? while the other methods tended to acquire
more content word variations like ?have a chance?
and ?have an opportunity.? Then we evaluated all
the methods in terms of how many paraphrases with
content word variations were extracted. We ex-
tracted from the evaluation samples only candidate
phrase pairs whose Diff contained a content word
(content word variation pairs), to see how many
70
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200
Prec
ision
(A) Top N (#Samples)  0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200(B) Top N (#Samples)  0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200(C) Top N (#Samples)
?Proposed_score?
?Proposed_local?
?SMT?
?Hashi_sup?
?Hashi_uns?
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200
Prec
ision
(a) Top N (#Samples)  0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200(b) Top N (#Samples)  0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200(c) Top N (#Samples)
?Proposed_score_cwv?
?Proposed_local_cwv?
?SMT_cwv?
?Hashi_sup_cwv?
?Hashi_uns_cwv?
Figure 4: Precision curves of Exp1: English (A)(a), Chinese (B)(b), and Japanese (C)(c).
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200
Prec
ision
(A) Top N (#Samples)  0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200(B) Top N (#Samples)  0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200(C) Top N (#Samples)
?Proposed_score?
?Proposed_score_cwv?
?Pasca?
?Pasca_cwv?
Figure 5: Precision curves of Exp2: English (A), Chinese (B), and Japanese (C).
of them were paraphrases. The lower half of Fig-
ure 4 shows the results (curves labeled with cwv).
The number of samples for ProposedScore reduced
drastically compared to the others for English and
Japanese, though precision was kept at a high level.
It is due mainly to the globalSim; the Diff of the
non-content word variation pairs appears frequently
in paraphrase candidates, and thus their globalSim
scores are high.
From Exp2, precision curves in Figure 5 were
obtained. P&D acquired more content word varia-
tion pairs as the curves labeled by cwv indicates.
However, ProposedScore?s precision outperformed
P&D?s by a large margin for the three languages.
From all of these results, we conclude (1) that our
paraphrase extraction method outperforms unsuper-
vised methods for the three languages, (2) that glob-
alSim is effective, and (3) that our method is com-
parable to the state-of-the-art supervised method for
Japanese, though our method tends to extract fewer
content word variation pairs than the others.
Table 7 shows examples of English paraphrases
extracted by ProposedScore.
is based in Halifax = is headquartered in Halifax
used for treating HIV = used to treat HIV
is a rare form = is an uncommon type
is a set = is an unordered collection
has an important role = plays a key role
Table 7: Examples of extracted English paraphrases.
4 Conclusion
We proposed a minimally supervised method for
multilingual paraphrase extraction. Our experiments
showed that our paraphrase extraction method out-
performs unsupervised methods (Pas?ca and Dienes,
2005; Koehn et al, 2007; Hashimoto et al, 2011)
for English, Japanese, and Chinese, and is compara-
ble to the state-of-the-art language dependent super-
vised method for Japanese (Hashimoto et al, 2011).
71
References
Susumu Akamine, Daisuke Kawahara, Yoshikiyo Kato,
Tetsuji Nakagawa, Yutaka I. Leon-Suematsu, Takuya
Kawada, Kentaro Inui, Sadao Kurohashi, and Yutaka
Kidawara. 2010. Organizing information on the web
to support user judgments on information credibil-
ity. In Proceedings of 2010 4th International Uni-
versal Communication Symposium Proceedings (IUCS
2010), pages 122?129.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL
2003, pages 16?23.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting of the ACL joint
with the 10th Meeting of the European Chapter of the
ACL (ACL/EACL 2001), pages 50?57.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
Ledir: An unsupervised algorithm for learning direc-
tionality of inference rules. In Proceedings of Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP2007), pages 161?170.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 196?205.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving dependency
parsing with subtrees from auto-parsed data. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?09,
pages 570?579, Singapore. Association for Computa-
tional Linguistics.
Jacob Cohen. 1960. Coefficient of agreement for nom-
inal scales. In Educational and Psychological Mea-
surement, pages 37?46.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of the 20th international conference on Com-
putational Linguistics (COLING 2004), pages 350?
356, Geneva, Switzerland, Aug 23?Aug 27.
Guozhu Dong and Jinyan Li. 1999. Efficient mining of
emerging patterns: discovering trends and differences.
In Proceedings of the fifth ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD ?99, pages 43?52, San Diego, California, United
States.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL 2005),
pages 363?370.
Atsushi Fujita, Pierre Isabelle, and Roland Kuhn. 2012.
Enlarging paraphrase collections through generaliza-
tion and instantiation. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL 2012), pages 631?
642.
Maayan Geffet and Ido Dagan. 2005. The distributional
inclusion hypotheses and lexical entailment. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2005), pages
107?114.
Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda,
Stijn De Saeger, Masaki Murata, and Jun?ichi Kazama.
2009. Large-scale verb entailment acquisition from
the web. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2009), pages 1172?1181.
Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger,
Jun?ichi Kazama, and Sadao Kurohashi. 2011. Ex-
tracting paraphrases from definition sentences on the
web. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1087?1097, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Exploit-
ing Wikipedia as external knowledge for named entity
recognition. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 698?707, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint chinese word segmentation and pos
tagging. In Proceedings of the Joint Conference of the
72
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 513?521, Suntec, Singapore,
August. Association for Computational Linguistics.
Sadao Kurohashi and Daisuke Kawahara. 2009a.
Japanese morphological analyzer system ju-
man version 6.0 (in japanese). Kyoto University,
http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN.
Sadao Kurohashi and Daisuke Kawahara. 2009b.
Japanese syntax and case analyzer knp version 3.0
(in japanese). Kyoto University, http://nlp.ist.i.kyoto-
u.ac.jp/EN/index.php?KNP.
J. Richard Landis and Gary G. Koch. 1977. Measure-
ment of observer agreement for categorical data. Bio-
metrics, 33(1):159?174.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Dekang Lin, Shaojun Zhao Lijuan Qin, and Ming Zhou.
2003. Identifying synonyms among distributionally
similar words. In Proceedings of the 18th Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-03), pages 1492?1493.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning, CoNLL-X ?06, pages 216?220, New
York City, New York.
Roberto Navigli and Paola Velardi. 2010. Learning
word-class lattices for definition and hypernym extrac-
tion. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1318?1327, Uppsala, Sweden, July. Association for
Computational Linguistics.
Roberto Navigli, Paola Velardi, and Juana Mar??a Ruiz-
Mart??nez. 2010. An annotated dataset for extracting
definitions and hypernyms from the web. In Proceed-
ings of LREC 2010, pages 3716?3722.
Marius Pas?ca and Pe?ter Dienes. 2005. Aligning needles
in a haystack: paraphrase acquisition across the web.
In Proceedings of the Second international joint con-
ference on Natural Language Processing, IJCNLP?05,
pages 119?130, Jeju Island, Korea.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to german. In Proceedings
of the ACL SIGDAT-Workshop, pages 47?50.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proceedings of the 2nd international Con-
ference on Human Language Technology Research
(HLT2002), pages 313?318.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary template. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING2008), pages 849?856.
Hai Zhao and Chunyu Kit. 2008. Unsupervised seg-
mentation helps supervised learning of character tag-
ging for word segmentation and named entity recog-
nition. In Proceedings of the Sixth SIGHAN Workshop
on Chinese Language Processing, pages 106?111, Hy-
derabad, India.
73
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 21?29,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Bitext Dependency Parsing with Bilingual Subtree Constraints
Wenliang Chen, Jun?ichi Kazama and Kentaro Torisawa
Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
{chenwl, kazama, torisawa}@nict.go.jp
Abstract
This paper proposes a dependency parsing
method that uses bilingual constraints to
improve the accuracy of parsing bilingual
texts (bitexts). In our method, a target-
side tree fragment that corresponds to a
source-side tree fragment is identified via
word alignment and mapping rules that
are automatically learned. Then it is ver-
ified by checking the subtree list that is
collected from large scale automatically
parsed data on the target side. Our method,
thus, requires gold standard trees only on
the source side of a bilingual corpus in
the training phase, unlike the joint parsing
model, which requires gold standard trees
on the both sides. Compared to the re-
ordering constraint model, which requires
the same training data as ours, our method
achieved higher accuracy because of richer
bilingual constraints. Experiments on the
translated portion of the Chinese Treebank
show that our system outperforms mono-
lingual parsers by 2.93 points for Chinese
and 1.64 points for English.
1 Introduction
Parsing bilingual texts (bitexts) is crucial for train-
ing machine translation systems that rely on syn-
tactic structures on either the source side or the
target side, or the both (Ding and Palmer, 2005;
Nakazawa et al, 2006). Bitexts could provide
more information, which is useful in parsing, than
a usual monolingual texts that can be called ?bilin-
gual constraints?, and we expect to obtain more
accurate parsing results that can be effectively
used in the training of MT systems. With this mo-
tivation, there are several studies aiming at highly
accurate bitext parsing (Smith and Smith, 2004;
Burkett and Klein, 2008; Huang et al, 2009).
This paper proposes a dependency parsing
method, which uses the bilingual constraints that
we call bilingual subtree constraints and statistics
concerning the constraints estimated from large
unlabeled monolingual corpora. Basically, a (can-
didate) dependency subtree in a source-language
sentence is mapped to a subtree in the correspond-
ing target-language sentence by using word align-
ment and mapping rules that are automatically
learned. The target subtree is verified by check-
ing the subtree list that is collected from unla-
beled sentences in the target language parsed by
a usual monolingual parser. The result is used as
additional features for the source side dependency
parser. In this paper, our task is to improve the
source side parser with the help of the translations
on the target side.
Many researchers have investigated the use
of bilingual constraints for parsing (Burkett and
Klein, 2008; Zhao et al, 2009; Huang et al,
2009). For example, Burkett and Klein (2008)
show that parsing with joint models on bitexts im-
proves performance on either or both sides. How-
ever, their methods require that the training data
have tree structures on both sides, which are hard
to obtain. Our method only requires dependency
annotation on the source side and is much sim-
pler and faster. Huang et al (2009) proposes a
method, bilingual-constrained monolingual pars-
ing, in which a source-language parser is extended
to use the re-ordering of words between two sides?
sentences as additional information. The input of
their method is the source trees with their trans-
lation on the target side as ours, which is much
easier to obtain than trees on both sides. However,
their method does not use any tree structures on
21
the target side that might be useful for ambiguity
resolution. Our method achieves much greater im-
provement because it uses the richer subtree con-
straints.
Our approach takes the same input as Huang
et al (2009) and exploits the subtree structure on
the target side to provide the bilingual constraints.
The subtrees are extracted from large-scale auto-
parsed monolingual data on the target side. The
main problem to be addressed is mapping words
on the source side to the target subtree because
there are many to many mappings and reordering
problems that often occur in translation (Koehn et
al., 2003). We use an automatic way for generat-
ing mapping rules to solve the problems. Based
on the mapping rules, we design a set of features
for parsing models. The basic idea is as follows: if
the words form a subtree on one side, their corre-
sponding words on the another side will also prob-
ably form a subtree.
Experiments on the translated portion of the
Chinese Treebank (Xue et al, 2002; Bies et al,
2007) show that our system outperforms state-of-
the-art monolingual parsers by 2.93 points for Chi-
nese and 1.64 points for English. The results also
show that our system provides higher accuracies
than the parser of Huang et al (2009).
The rest of the paper is organized as follows:
Section 2 introduces the motivation of our idea.
Section 3 introduces the background of depen-
dency parsing. Section 4 proposes an approach
of constructing bilingual subtree constraints. Sec-
tion 5 explains the experimental results. Finally, in
Section 6 we draw conclusions and discuss future
work.
2 Motivation
In this section, we use an example to show the
idea of using the bilingual subtree constraints to
improve parsing performance.
Suppose that we have an input sentence pair as
shown in Figure 1, where the source sentence is in
English, the target is in Chinese, the dashed undi-
rected links are word alignment links, and the di-
rected links between words indicate that they have
a (candidate) dependency relation.
In the English side, it is difficult for a parser to
determine the head of word ?with? because there
is a PP-attachment problem. However, in Chinese
it is unambiguous. Therefore, we can use the in-
formation on the Chinese side to help disambigua-
He  ate    the    meat with     a    fork    .
?(He) ?(use) ??(fork) ?(eat) ?(meat) ?(.)
Figure 1: Example for disambiguation
tion.
There are two candidates ?ate? and ?meat? to be
the head of ?with? as the dashed directed links in
Figure 1 show. By adding ?fork?, we have two
possible dependency relations, ?meat-with-fork?
and ?ate-with-fork?, to be verified.
First, we check the possible relation of ?meat?,
?with?, and ?fork?. We obtain their corresponding
words ??(meat)?, ??(use)?, and ???(fork)? in
Chinese via the word alignment links. We ver-
ify that the corresponding words form a subtree
by looking up a subtree list in Chinese (described
in Section 4.1). But we can not find a subtree for
them.
Next, we check the possible relation of ?ate?,
?with?, and ?fork?. We obtain their correspond-
ing words ??(ate)?, ??(use)?, and ???(fork)?.
Then we verify that the words form a subtree by
looking up the subtree list. This time we can find
the subtree as shown in Figure 2.
?(use) ??(fork) ?(eat)
Figure 2: Example for a searched subtree
Finally, the parser may assign ?ate? to be the
head of ?with? based on the verification results.
This simple example shows how to use the subtree
information on the target side.
3 Dependency parsing
For dependency parsing, there are two main types
of parsing models (Nivre and McDonald, 2008;
Nivre and Kubler, 2006): transition-based (Nivre,
2003; Yamada and Matsumoto, 2003) and graph-
based (McDonald et al, 2005; Carreras, 2007).
Our approach can be applied to both parsing mod-
els.
In this paper, we employ the graph-based MST
parsing model proposed by McDonald and Pereira
22
(2006), which is an extension of the projec-
tive parsing algorithm of Eisner (1996). To use
richer second-order information, we also imple-
ment parent-child-grandchild features (Carreras,
2007) in the MST parsing algorithm.
3.1 Parsing with monolingual features
Figure 3 shows an example of dependency pars-
ing. In the graph-based parsing model, features are
represented for all the possible relations on single
edges (two words) or adjacent edges (three words).
The parsing algorithm chooses the tree with the
highest score in a bottom-up fashion.
ROOT     He  ate    the    meat   with     a    fork    .
Figure 3: Example of dependency tree
In our systems, the monolingual features in-
clude the first- and second- order features pre-
sented in (McDonald et al, 2005; McDonald
and Pereira, 2006) and the parent-child-grandchild
features used in (Carreras, 2007). We call the
parser with the monolingual features monolingual
parser.
3.2 Parsing with bilingual features
In this paper, we parse source sentences with the
help of their translations. A set of bilingual fea-
tures are designed for the parsing model.
3.2.1 Bilingual subtree features
We design bilingual subtree features, as described
in Section 4, based on the constraints between the
source subtrees and the target subtrees that are ver-
ified by the subtree list on the target side. The
source subtrees are from the possible dependency
relations.
3.2.2 Bilingual reordering feature
Huang et al (2009) propose features based on
reordering between languages for a shift-reduce
parser. They define the features based on word-
alignment information to verify that the corre-
sponding words form a contiguous span for resolv-
ing shift-reduce conflicts. We also implement sim-
ilar features in our system.
4 Bilingual subtree constraints
In this section, we propose an approach that uses
the bilingual subtree constraints to help parse
source sentences that have translations on the tar-
get side.
We use large-scale auto-parsed data to obtain
subtrees on the target side. Then we generate the
mapping rules to map the source subtrees onto the
extracted target subtrees. Finally, we design the
bilingual subtree features based on the mapping
rules for the parsing model. These features in-
dicate the information of the constraints between
bilingual subtrees, that are called bilingual subtree
constraints.
4.1 Subtree extraction
Chen et al (2009) propose a simple method to ex-
tract subtrees from large-scale monolingual data
and use them as features to improve monolingual
parsing. Following their method, we parse large
unannotated data with a monolingual parser and
obtain a set of subtrees (STt) in the target lan-
guage.
We encode the subtrees into string format that is
expressed as st = w : hid(?w : hid)+1, where w
refers to a word in the subtree and hid refers to the
word ID of the word?s head (hid=0 means that this
word is the root of a subtree). Here, word ID refers
to the ID (starting from 1) of a word in the subtree
(words are ordered based on the positions of the
original sentence). For example, ?He? and ?ate?
have a left dependency arc in the sentence shown
in Figure 3. The subtree is encoded as ?He:2-
ate:0?. There is also a parent-child-grandchild re-
lation among ?ate?, ?with?, and ?fork?. So the
subtree is encoded as ?ate:0-with:1-fork:2?. If a
subtree contains two nodes, we call it a bigram-
subtree. If a subtree contains three nodes, we call
it a trigram-subtree.
From the dependency tree of Figure 3, we ob-
tain the subtrees, as shown in Figure 4 and Figure
5. Figure 4 shows the extracted bigram-subtrees
and Figure 5 shows the extracted trigram-subtrees.
After extraction, we obtain a set of subtrees. We
remove the subtrees occurring only once in the
data. Following Chen et al (2009), we also group
the subtrees into different sets based on their fre-
quencies.
1+ refers to matching the preceding element one or more
times and is the same as a regular expression in Perl.
23
ate
He He:1:2-ate:2:0ate
meat ate:1:0-meat:2:1
ate
with ate:1:0-with:2:1
meat
the the:1:2-meat:2:0
with
fork with:1:0-fork:2:1fork
a a:1:2-fork:2:0
Figure 4: Examples of bigram-subtrees
atemeat  with ate:1:0-meat:2:1-with:3:1 atewith   . ate:1:0-with:2:1-.:3:1
(a)
He:1:3-NULL:2:3-ate:3:0ateHe  NULL ateNULL  meat ate:1:0-NULL:2:1-meat:3:1the:1:3-NULL:2:3-meat:3:0a:1:3-NULL:2:3-fork:3:0 with:1:0-NULL:2:1-fork:3:1
ate:1:0-the:2:3-meat:3:1 ate:1:0-with:2:1-fork:3:2with:1:0-a:2:3-fork:3:1 NULL:1:2-He:2:3-ate:3:0He:1:3-NULL:2:1-ate:3:0 ate:1:0-meat:2:1-NULL:3:2ate:1:0-NULL:2:3-with:3:1 with:1:0-fork:2:1-NULL:3:2NULL:1:2-a:2:3-fork:3:0 a:1:3-NULL:2:1-fork:3:0ate:1:0-NULL:2:3-.:3:1 ate:1:0-.:2:1-NULL:3:2
(b)NULL:1:2-the:2:3-meat:3:0 the:1:3-NULL:2:1-meat:3:0
Figure 5: Examples of trigram-subtrees
4.2 Mapping rules
To provide bilingual subtree constraints, we need
to find the characteristics of subtree mapping for
the two given languages. However, subtree map-
ping is not easy. There are two main problems:
MtoN (words) mapping and reordering, which of-
ten occur in translation. MtoN (words) map-
ping means that a source subtree with M words
is mapped onto a target subtree with N words. For
example, 2to3 means that a source bigram-subtree
is mapped onto a target trigram-subtree.
Due to the limitations of the parsing algo-
rithm (McDonald and Pereira, 2006; Carreras,
2007), we only use bigram- and trigram-subtrees
in our approach. We generate the mapping rules
for the 2to2, 2to3, 3to3, and 3to2 cases. For
trigram-subtrees, we only consider the parent-
child-grandchild type. As for the use of other
types of trigram-subtrees, we leave it for future
work.
We first show the MtoN and reordering prob-
lems by using an example in Chinese-English
translation. Then we propose a method to auto-
matically generate mapping rules.
4.2.1 Reordering and MtoN mapping in
translation
Both Chinese and English are classified as SVO
languages because verbs precede objects in simple
sentences. However, Chinese has many character-
istics of such SOV languages as Japanese. The
typical cases are listed below:
1) Prepositional phrases modifying a verb pre-
cede the verb. Figure 6 shows an example. In En-
glish the prepositional phrase ?at the ceremony?
follows the verb ?said?, while its corresponding
prepositional phrase ??(NULL)??(ceremony)
?(at)? precedes the verb ??(say)? in Chinese.
? ?? ? ?
Said at the ceremony
Figure 6: Example for prepositional phrases mod-
ifying a verb
2) Relative clauses precede head noun. Fig-
ure 7 shows an example. In Chinese the relative
clause ???(today) ??(signed)? precedes the
head noun ???(project)?, while its correspond-
ing clause ?signed today? follows the head noun
?projects? in English.
?? ?? ? ? ? ??
The 3 projects signed today
Figure 7: Example for relative clauses preceding
the head noun
3) Genitive constructions precede head noun.
For example, ???(car) ??(wheel)? can be
translated as ?the wheel of the car?.
4) Postposition in many constructions rather
than prepositions. For example, ???(table)
?(on)? can be translated as ?on the table?.
24
We can find the MtoN mapping problem occur-
ring in the above cases. For example, in Figure 6,
trigram-subtree ??(NULL):3-?(at):1-?(say):0?
is mapped onto bigram-subtree ?said:0-at:1?.
Since asking linguists to define the mapping
rules is very expensive, we propose a simple
method to easily obtain the mapping rules.
4.2.2 Bilingual subtree mapping
To solve the mapping problems, we use a bilingual
corpus, which includes sentence pairs, to automat-
ically generate the mapping rules. First, the sen-
tence pairs are parsed by monolingual parsers on
both sides. Then we perform word alignment us-
ing a word-level aligner (Liang et al, 2006; DeN-
ero and Klein, 2007). Figure 8 shows an example
of a processed sentence pair that has tree structures
on both sides and word alignment links.
ROOT    ?? ?? ?? ?? ?
ROOT    They   are   on   the   fringes   of   society   .
Figure 8: Example of auto-parsed bilingual sen-
tence pair
From these sentence pairs, we obtain subtree
pairs. First, we extract a subtree (sts) from a
source sentence. Then through word alignment
links, we obtain the corresponding words of the
words of sts. Because of the MtoN problem, some
words lack of corresponding words in the target
sentence. Here, our approach requires that at least
two words of sts have corresponding words and
nouns and verbs need corresponding words. If not,
it fails to find a subtree pair for sts. If the corre-
sponding words form a subtree (stt) in the target
sentence, sts and stt are a subtree pair. We also
keep the word alignment information in the tar-
get subtree. For example, we extract subtree ??
?(society):2-??(fringe):0? on the Chinese side
and get its corresponding subtree ?fringes(W 2):0-
of:1-society(W 1):2? on the English side, where
W 1 means that the target word is aligned to the
first word of the source subtree, and W 2 means
that the target word is aligned to the second word
of the source subtree. That is, we have a sub-
tree pair: ???(society):2-??(fringe):0? and
?fringe(W 2):0-of:1-society(W 1):2?.
The extracted subtree pairs indicate the trans-
lation characteristics between Chinese and En-
glish. For example, the pair ???(society):2-
? ?(fringe):0? and ?fringes:0-of:1-society:2?
is a case where ?Genitive constructions pre-
cede/follow the head noun?.
4.2.3 Generalized mapping rules
To increase the mapping coverage, we general-
ize the mapping rules from the extracted sub-
tree pairs by using the following procedure. The
rules are divided by ?=>? into two parts: source
(left) and target (right). The source part is
from the source subtree and the target part is
from the target subtree. For the source part,
we replace nouns and verbs using their POS
tags (coarse grained tags). For the target part,
we use the word alignment information to rep-
resent the target words that have correspond-
ing source words. For example, we have the
subtree pair: ???(society):2-??(fringe):0?
and ?fringes(W 2):0-of:1-society(W 1):2?, where
?of? does not have a corresponding word, the POS
tag of ???(society)? is N, and the POS tag of
???(fringe)? is N. The source part of the rule
becomes ?N:2-N:0? and the target part becomes
?W 2:0-of:1-W 1:2?.
Table 1 shows the top five mapping rules of
all four types ordered by their frequencies, where
W 1 means that the target word is aligned to the
first word of the source subtree, W 2 means that
the target word is aligned to the second word, and
W 3 means that the target word is aligned to the
third word. We remove the rules that occur less
than three times. Finally, we obtain 9,134 rules
for 2to2, 5,335 for 2to3, 7,450 for 3to3, and 1,244
for 3to2 from our data. After experiments with dif-
ferent threshold settings on the development data
sets, we use the top 20 rules for each type in our
experiments.
The generalized mapping rules might generate
incorrect target subtrees. However, as described in
Section 4.3.1, the generated subtrees are verified
by looking up list STt before they are used in the
parsing models.
4.3 Bilingual subtree features
Informally, if the words form a subtree on the
source side, then the corresponding words on the
target side will also probably form a subtree. For
25
# rules freq
2to2 mapping
1 N:2 N:0 =>W 1:2 W 2:0 92776
2 V:0 N:1 =>W 1:0 W 2:1 62437
3 V:0 V:1 =>W 1:0 W 2:1 49633
4 N:2 V:0 =>W 1:2 W 2:0 43999
5 ?:2 N:0 =>W 2:0 W 1:2 25301
2to3 mapping
1 N:2-N:0 =>W 2:0-of:1-W 1:2 10361
2 V:0-N:1 =>W 1:0-of:1-W 2:2 4521
3 V:0-N:1 =>W 1:0-to:1-W 2:2 2917
4 N:2-V:0 =>W 2:0-of:1-W 1:2 2578
5 N:2-N:0 =>W 1:2-?:3-W 2:0 2316
3to2 mapping
1 V:2-?/DEC:3-N:0 =>W 1:0-W 3:1 873
2 V:2-?/DEC:3-N:0 =>W 3:2-W 1:0 634
3 N:2-?/DEG:3-N:0 =>W 1:0-W 3:1 319
4 N:2-?/DEG:3-N:0 =>W 3:2-W 1:0 301
5 V:0-?/DEG:3-N:1 =>W 3:0-W 1:1 247
3to3 mapping
1 V:0-V:1-N:2 =>W 1:0-W 2:1-W 3:2 9580
2 N:2-?/DEG:3-N:0 =>W 3:0-W 2:1-W 1:2 7010
3 V:0-N:3-N:1 =>W 1:0-W 2:3-W 3:1 5642
4 V:0-V:1-V:2 =>W 1:0-W 2:1-W 3:2 4563
5 N:2-N:3-N:0 =>W 1:2-W 2:3-W 3:0 3570
Table 1: Top five mapping rules of 2to3 and 3to2
example, in Figure 8, words ???(they)? and
???(be on)? form a subtree , which is mapped
onto the words ?they? and ?are? on the target side.
These two target words form a subtree. We now
develop this idea as bilingual subtree features.
In the parsing process, we build relations for
two or three words on the source side. The con-
ditions of generating bilingual subtree features are
that at least two of these source words must have
corresponding words on the target side and nouns
and verbs must have corresponding words.
At first, we have a possible dependency relation
(represented as a source subtree) of words to be
verified. Then we obtain the corresponding target
subtree based on the mapping rules. Finally, we
verify that the target subtree is included in STt. If
yes, we activate a positive feature to encourage the
dependency relation.
? ? ?? ?? ? ? ? ??
Those are the 3 projects signed today
Figure 9: Example of features for parsing
We consider four types of features based on
2to2, 3to3, 3to2, and 2to3 mappings. In the 2to2,
3to3, and 3to2 cases, the target subtrees do not add
new words. We represent features in a direct way.
For the 2to3 case, we represent features using a
different strategy.
4.3.1 Features for 2to2, 3to3, and 3to2
We design the features based on the mapping
rules of 2to2, 3to3, and 3to2. For example, we
design features for a 3to2 case from Figure 9.
The possible relation to be verified forms source
subtree ???(signed)/VV:2-?(NULL)/DEC:3-
??(project)/NN:0? in which ???(project)?
is aligned to ?projects? and ???(signed)? is
aligned to ?signed? as shown in Figure 9. The
procedure of generating the features is shown in
Figure 10. We explain Steps (1), (2), (3), and (4)
as follows:
??/VV:2-?/DEC:3-??/NN:0
projects(W_3) signed(W_1) 
(1)
V:2-?/DEC:3-N:0
W_3:0-W_1:1W 3:2 W 1:0
(2)
_ - _
(3)
projects:0-signed:1projects:2-signed:0 STt
(4)
3to2:YES
Figure 10: Example of feature generation for 3to2
case
(1) Generate source part from the source
subtree. We obtain ?V:2-?/DEC:3-N:0? from
?? ?(signed)/VV:2-?(NULL)/DEC:3-?
?(project)/NN:0?.
(2) Obtain target parts based on the matched
mapping rules, whose source parts equal
?V:2-?/DEC:3-N:0?. The matched rules are
?V:2-?/DEC:3-N:0 =>W 3:0-W 1:1? and
?V:2-?/DEC:3-N:0 => W 3:2-W 1:0?. Thus,
we have two target parts ?W 3:0-W 1:1? and
?W 3:2-W 1:0?.
(3) Generate possible subtrees by consider-
26
ing the dependency relation indicated in the
target parts. We generate a possible subtree
?projects:0-signed:1? from the target part ?W 3:0-
W 1:1?, where ?projects? is aligned to ??
?(project)(W 3)? and ?signed? is aligned to ??
?(signed)(W 1)?. We also generate another pos-
sible subtree ?projects:2-signed:0? from ?W 3:2-
W 1:0?.
(4) Verify that at least one of the generated
possible subtrees is a target subtree, which is in-
cluded in STt. If yes, we activate this feature. In
the figure, ?projects:0-signed:1? is a target subtree
in STt. So we activate the feature ?3to2:YES?
to encourage dependency relations among ??
?(signed)?, ??(NULL)?, and ???(project)?.
4.3.2 Features for 2to3
In the 2to3 case, a new word is added on the target
side. The first two steps are identical as those in
the previous section. For example, a source part
?N:2-N:0? is generated from ???(car)/NN:2-?
?(wheel)/NN:0?. Then we obtain target parts
such as ?W 2:0-of/IN:1-W 1:2?, ?W 2:0-in/IN:1-
W 1:2?, and so on, according to the matched map-
ping rules.
The third step is different. In the target parts,
there is an added word. We first check if the added
word is in the span of the corresponding words,
which can be obtained through word alignment
links. We can find that ?of? is in the span ?wheel
of the car?, which is the span of the corresponding
words of ???(car)/NN:2-??(wheel)/NN:0?.
Then we choose the target part ?W 2:0-of/IN:1-
W 1:2? to generate a possible subtree. Finally,
we verify that the subtree is a target subtree in-
cluded in STt. If yes, we say feature ?2to3:YES?
to encourage a dependency relation between ??
?(car)? and ???(wheel)?.
4.4 Source subtree features
Chen et al (2009) shows that the source sub-
tree features (Fsrc?st) significantly improve per-
formance. The subtrees are obtained from the
auto-parsed data on the source side. Then they are
used to verify the possible dependency relations
among source words.
In our approach, we also use the same source
subtree features described in Chen et al (2009).
So the possible dependency relations are verified
by the source and target subtrees. Combining two
types of features together provides strong discrim-
ination power. If both types of features are ac-
tive, building relations is very likely among source
words. If both are inactive, this is a strong negative
signal for their relations.
5 Experiments
All the bilingual data were taken from the trans-
lated portion of the Chinese Treebank (CTB)
(Xue et al, 2002; Bies et al, 2007), articles
1-325 of CTB, which have English translations
with gold-standard parse trees. We used the tool
?Penn2Malt?2 to convert the data into dependency
structures. Following the study of Huang et al
(2009), we used the same split of this data: 1-270
for training, 301-325 for development, and 271-
300 for test. Note that some sentence pairs were
removed because they are not one-to-one aligned
at the sentence level (Burkett and Klein, 2008;
Huang et al, 2009). Word alignments were gen-
erated from the Berkeley Aligner (Liang et al,
2006; DeNero and Klein, 2007) trained on a bilin-
gual corpus having approximately 0.8M sentence
pairs. We removed notoriously bad links in {a,
an, the}?{?(DE),?(LE)} following the work of
Huang et al (2009).
For Chinese unannotated data, we used the
XIN CMN portion of Chinese Gigaword Version
2.0 (LDC2009T14) (Huang, 2009), which has ap-
proximately 311 million words whose segmenta-
tion and POS tags are given. To avoid unfair com-
parison, we excluded the sentences of the CTB
data from the Gigaword data. We discarded the an-
notations because there are differences in annota-
tion policy between CTB and this corpus. We used
the MMA system (Kruengkrai et al, 2009) trained
on the training data to perform word segmentation
and POS tagging and used the Baseline Parser to
parse all the sentences in the data. For English
unannotated data, we used the BLLIP corpus that
contains about 43 million words of WSJ text. The
POS tags were assigned by the MXPOST tagger
trained on training data. Then we used the Base-
line Parser to parse all the sentences in the data.
We reported the parser quality by the unlabeled
attachment score (UAS), i.e., the percentage of to-
kens (excluding all punctuation tokens) with cor-
rect HEADs.
5.1 Main results
The results on the Chinese-source side are shown
in Table 2, where ?Baseline? refers to the systems
2http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
27
with monolingual features, ?Baseline2? refers to
adding the reordering features to the Baseline,
?FBI? refers to adding all the bilingual subtree
features to ?Baseline2?, ?Fsrc?st? refers to the
monolingual parsing systems with source subtree
features, ?Order-1? refers to the first-order mod-
els, and ?Order-2? refers to the second-order mod-
els. The results showed that the reordering fea-
tures yielded an improvement of 0.53 and 0.58
points (UAS) for the first- and second-order mod-
els respectively. Then we added four types of
bilingual constraint features one by one to ?Base-
line2?. Note that the features based on 3to2 and
3to3 can not be applied to the first-order models,
because they only consider single dependencies
(bigram). That is, in the first model, FBI only in-
cludes the features based on 2to2 and 2to3. The
results showed that the systems performed better
and better. In total, we obtained an absolute im-
provement of 0.88 points (UAS) for the first-order
model and 1.36 points for the second-order model
by adding all the bilingual subtree features. Fi-
nally, the system with all the features (OURS) out-
performed the Baseline by an absolute improve-
ment of 3.12 points for the first-order model and
2.93 points for the second-order model. The im-
provements of the final systems (OURS) were sig-
nificant in McNemar?s Test (p < 10?4).
Order-1 Order-2
Baseline 84.35 87.20
Baseline2 84.88 87.78
+2to2 85.08 88.07
+2to3 85.23 88.14
+3to3 ? 88.29
+3to2 ? 88.56
FBI 85.23(+0.88) 88.56(+1.36)
Fsrc?st 86.54(+2.19) 89.49(+2.29)
OURS 87.47(+3.12) 90.13(+2.93)
Table 2: Dependency parsing results of Chinese-
source case
We also conducted experiments on the English-
source side. Table 3 shows the results, where ab-
breviations are the same as in Table 2. As in the
Chinese experiments, the parsers with bilingual
subtree features outperformed the Baselines. Fi-
nally, the systems (OURS) with all the features
outperformed the Baselines by 1.30 points for the
first-order model and 1.64 for the second-order
model. The improvements of the final systems
(OURS) were significant in McNemar?s Test (p <
10?3).
Order-1 Order-2
Baseline 86.41 87.37
Baseline2 86.86 87.66
+2to2 87.23 87.87
+2to3 87.35 87.96
+3to3 ? 88.25
+3to2 ? 88.37
FBI 87.35(+0.94) 88.37(+1.00)
Fsrc?st 87.25(+0.84) 88.57(+1.20)
OURS 87.71(+1.30) 89.01(+1.64)
Table 3: Dependency parsing results of English-
source case
5.2 Comparative results
Table 4 shows the performance of the system we
compared, where Huang2009 refers to the result of
Huang et al (2009). The results showed that our
system performed better than Huang2009. Com-
pared with the approach of Huang et al (2009),
our approach used additional large-scale auto-
parsed data. We did not compare our system with
the joint model of Burkett and Klein (2008) be-
cause they reported the results on phrase struc-
tures.
Chinese English
Huang2009 86.3 87.5
Baseline 87.20 87.37
OURS 90.13 89.01
Table 4: Comparative results
6 Conclusion
We presented an approach using large automati-
cally parsed monolingual data to provide bilingual
subtree constraints to improve bitexts parsing. Our
approach remains the efficiency of monolingual
parsing and exploits the subtree structure on the
target side. The experimental results show that the
proposed approach is simple yet still provides sig-
nificant improvements over the baselines in pars-
ing accuracy. The results also show that our sys-
tems outperform the system of previous work on
the same data.
There are many ways in which this research
could be continued. First, we may attempt to ap-
ply the bilingual subtree constraints to transition-
28
based parsing models (Nivre, 2003; Yamada and
Matsumoto, 2003). Here, we may design new fea-
tures for the models. Second, we may apply the
proposed method for other language pairs such as
Japanese-English and Chinese-Japanese. Third,
larger unannotated data can be used to improve the
performance further.
References
Ann Bies, Martha Palmer, Justin Mott, and Colin
Warner. 2007. English Chinese translation treebank
v 1.0. In LDC2007T02.
David Burkett and Dan Klein. 2008. Two languages
are better than one (for syntactic parsing). In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 877?
886, Honolulu, Hawaii, October. Association for
Computational Linguistics.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL
2007, pages 957?961.
WL. Chen, J. Kazama, K. Uchimoto, and K. Torisawa.
2009. Improving dependency parsing with subtrees
from auto-parsed data. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 570?579, Singapore, Au-
gust. Association for Computational Linguistics.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 17?24,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In ACL ?05: Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 541?548, Morristown, NJ,
USA. Association for Computational Linguistics.
J. Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proc. of
the 16th Intern. Conf. on Computational Linguistics
(COLING), pages 340?345.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1222?1231, Singapore, August. Associ-
ation for Computational Linguistics.
Chu-Ren Huang. 2009. Tagged Chinese Gigaword
Version 2.0, LDC2009T14. Linguistic Data Con-
sortium.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of NAACL,
page 54. Association for Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint Chinese word segmentation and
POS tagging. In Proceedings of ACL-IJCNLP2009,
pages 513?521, Suntec, Singapore, August. Associ-
ation for Computational Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 104?111, New York City,
USA, June. Association for Computational Linguis-
tics.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proc. of EACL2006.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proc. of ACL 2005.
T. Nakazawa, K. Yu, D. Kawahara, and S. Kurohashi.
2006. Example-based machine translation based on
deeper nlp. In Proceedings of IWSLT 2006, pages
64?70, Kyoto, Japan.
J. Nivre and S. Kubler. 2006. Dependency parsing:
Tutorial at Coling-ACL 2006. In CoLING-ACL.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL-08: HLT, Columbus, Ohio,
June.
J. Nivre. 2003. An efficient algorithm for projective
dependency parsing. In Proceedings of IWPT2003,
pages 149?160.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proceedings of EMNLP.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated Chinese cor-
pus. In Coling.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proceedings of IWPT2003, pages 195?206.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing us-
ing a bilingual lexicon. In Proceedings of ACL-
IJCNLP2009, pages 55?63, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
29
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 247?256,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Bayesian Method for Robust Estimation of Distributional Similarities
Jun?ichi Kazama Stijn De Saeger Kow Kuroda
Masaki Murata? Kentaro Torisawa
Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology (NICT)
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289 Japan
{kazama, stijn, kuroda, torisawa}@nict.go.jp
?Department of Information and Knowledge Engineering
Faculty/Graduate School of Engineering, Tottori University
4-101 Koyama-Minami, Tottori, 680-8550 Japan?
murata@ike.tottori-u.ac.jp
Abstract
Existing word similarity measures are not
robust to data sparseness since they rely
only on the point estimation of words?
context profiles obtained from a limited
amount of data. This paper proposes a
Bayesian method for robust distributional
word similarities. The method uses a dis-
tribution of context profiles obtained by
Bayesian estimation and takes the expec-
tation of a base similarity measure under
that distribution. When the context pro-
files are multinomial distributions, the pri-
ors are Dirichlet, and the base measure is
the Bhattacharyya coefficient, we can de-
rive an analytical form that allows efficient
calculation. For the task of word similar-
ity estimation using a large amount of Web
data in Japanese, we show that the pro-
posed measure gives better accuracies than
other well-known similarity measures.
1 Introduction
The semantic similarity of words is a long-
standing topic in computational linguistics be-
cause it is theoretically intriguing and has many
applications in the field. Many researchers have
conducted studies based on the distributional hy-
pothesis (Harris, 1954), which states that words
that occur in the same contexts tend to have similar
meanings. A number of semantic similarity mea-
sures have been proposed based on this hypothesis
(Hindle, 1990; Grefenstette, 1994; Dagan et al,
1994; Dagan et al, 1995; Lin, 1998; Dagan et al,
1999).
?The work was done while the author was at NICT.
In general, most semantic similarity measures
have the following form:
sim(w1, w2) = g(v(w1), v(w2)), (1)
where v(wi) is a vector that represents the con-
texts in which wi appears, which we call a context
profile of wi. The function g is a function on these
context profiles that is expected to produce good
similarities. Each dimension of the vector corre-
sponds to a context, fk, which is typically a neigh-
boring word or a word having dependency rela-
tions with wi in a corpus. Its value, vk(wi), is typ-
ically a co-occurrence frequency c(wi, fk), a con-
ditional probability p(fk|wi), or point-wise mu-
tual information (PMI) between wi and fk, which
are all calculated from a corpus. For g, various
works have used the cosine, the Jaccard coeffi-
cient, or the Jensen-Shannon divergence is uti-
lized, to name only a few measures.
Previous studies have focused on how to de-
vise good contexts and a good function g for se-
mantic similarities. On the other hand, our ap-
proach in this paper is to estimate context profiles
(v(wi)) robustly and thus to estimate the similarity
robustly. The problem here is that v(wi) is com-
puted from a corpus of limited size, and thus in-
evitably contains uncertainty and sparseness. The
guiding intuition behind our method is as follows.
All other things being equal, the similarity with
a more frequent word should be larger, since it
would be more reliable. For example, if p(fk|w1)
and p(fk|w2) for two given words w1 and w2 are
equal, but w1 is more frequent, we would expect
that sim(w0, w1) > sim(w0, w2).
In the NLP field, data sparseness has been rec-
ognized as a serious problem and tackled in the
context of language modeling and supervised ma-
chine learning. However, to our knowledge, there
247
has been no study that seriously dealt with data
sparseness in the context of semantic similarity
calculation. The data sparseness problem is usu-
ally solved by smoothing, regularization, margin
maximization and so on (Chen and Goodman,
1998; Chen and Rosenfeld, 2000; Cortes and Vap-
nik, 1995). Recently, the Bayesian approach has
emerged and achieved promising results with a
clearer formulation (Teh, 2006; Mochihashi et al,
2009).
In this paper, we apply the Bayesian framework
to the calculation of distributional similarity. The
method is straightforward: Instead of using the
point estimation of v(wi), we first estimate the
distribution of the context profile, p(v(wi)), by
Bayesian estimation and then take the expectation
of the original similarity under this distribution as
follows:
simb(w1, w2) (2)
= E[sim(w1, w2)]{p(v(w1)),p(v(w2))}
= E[g(v(w1), v(w2))]{p(v(w1)),p(v(w2))}.
The uncertainty due to data sparseness is repre-
sented by p(v(wi)), and taking the expectation en-
ables us to take this into account. The Bayesian
estimation usually gives diverging distributions for
infrequent observations and thus decreases the ex-
pectation value as expected.
The Bayesian estimation and the expectation
calculation in Eq. 2 are generally difficult and
usually require computationally expensive proce-
dures. Since our motivation for this research is to
calculate good semantic similarities for a large set
of words (e.g., one million nouns) and apply them
to a wide range of NLP tasks, such costs must be
minimized.
Our technical contribution in this paper is to
show that in the case where the context profiles are
multinomial distributions, the priors are Dirich-
let, and the base similarity measure is the Bhat-
tacharyya coefficient (Bhattacharyya, 1943), we
can derive an analytical form for Eq. 2, that en-
ables efficient calculation (with some implemen-
tation tricks).
In experiments, we estimate semantic similari-
ties using a large amount of Web data in Japanese
and show that the proposed measure gives bet-
ter word similarities than a non-Bayesian Bhat-
tacharyya coefficient or other well-known similar-
ity measures such as Jensen-Shannon divergence
and the cosine with PMI weights.
The rest of the paper is organized as follows. In
Section 2, we briefly introduce the Bayesian esti-
mation and the Bhattacharyya coefficient. Section
3 proposes our new Bayesian Bhattacharyya coef-
ficient for robust similarity calculation. Section 4
mentions some implementation issues and the so-
lutions. Then, Section 5 reports the experimental
results.
2 Background
2.1 Bayesian estimation with Dirichlet prior
Assume that we estimate a probabilistic model for
the observed data D, p(D|?), which is parame-
terized with parameters ?. In the maximum like-
lihood estimation (MLE), we find the point esti-
mation ?? = argmax?p(D|?). For example, we
estimate p(fk|wi) as follows with MLE:
p(fk|wi) = c(wi, fk)/
X
k
c(wi, fk). (3)
On the other hand, the objective of the Bayesian
estimation is to find the distribution of ? given
the observed data D, i.e., p(?|D), and use it in
later processes. Using Bayes? rule, this can also
be viewed as:
p(?|D) = p(D|?)pprior(?)p(D) . (4)
pprior(?) is a prior distribution that represents the
plausibility of each ? based on the prior knowl-
edge. In this paper, we consider the case where
? is a multinomial distribution, i.e.,
?
k ?k = 1,
that models the process of choosing one out of K
choices. Estimating a conditional probability dis-
tribution ?k = p(fk|wi) as a context profile for
each wi falls into this case. In this paper, we also
assume that the prior is the Dirichlet distribution,
Dir(?). The Dirichlet distribution is defined as
follows.
Dir(?|?) =
?(
PK
k=1 ?k)
QK
k=1 ?(?k)
K
Y
k=1
??k?1k . (5)
?(.) is the Gamma function. The Dirichlet distri-
bution is parametrized by hyperparameters ?k(>
0).
It is known that p(?|D) is also a Dirichlet dis-
tribution for this simplest case, and it can be ana-
lytically calculated as follows.
p(?|D) = Dir(?|{?k + c(k)}), (6)
where c(k) is the frequency of choice k in data D.
For example, c(k) = c(wi, fk) in the estimation
of p(fk|wi). This is very simple: we just need to
add the observed counts to the hyperparameters.
248
2.2 Bhattacharyya coefficient
When the context profiles are probability distribu-
tions, we usually utilize the measures on probabil-
ity distributions such as the Jensen-Shannon (JS)
divergence to calculate similarities (Dagan et al,
1994; Dagan et al, 1997). The JS divergence is
defined as follows.
JS(p1||p2) =
1
2(KL(p1||pavg) + KL(p2||pavg)),
where pavg = p1+p22 is a point-wise average of p1
and p2 and KL(.) is the Kullback-Leibler diver-
gence. Although we found that the JS divergence
is a good measure, it is difficult to derive an ef-
ficient calculation of Eq. 2, even in the Dirichlet
prior case.1
In this study, we employ the Bhattacharyya co-
efficient (Bhattacharyya, 1943) (BC for short),
which is defined as follows:
BC(p1, p2) =
K
X
k=1
?
p1k ? p2k.
The BC is also a similarity measure on probabil-
ity distributions and is suitable for our purposes as
we describe in the next section. Although BC has
not been explored well in the literature on distribu-
tional word similarities, it is also a good similarity
measure as the experiments show.
3 Method
In this section, we show that if our base similarity
measure is BC and the distributions under which
we take the expectation are Dirichlet distributions,
then Eq. 2 also has an analytical form, allowing
efficient calculation.
Here, we calculate the following value given
two Dirichlet distributions:
BCb(p1, p2) = E[BC(p1, p2)]{Dir(p1|?? ),Dir(p2|?? )}
=
ZZ
???
Dir(p1|?
?
)Dir(p2|?
?
)BC(p1, p2)dp1dp2.
After several derivation steps (see Appendix A),
we obtain the following analytical solution for the
above:
1A naive but general way might be to draw samples of
v(wi) from p(v(wi)) and approximate the expectation using
these samples. However, such a method will be slow.
= ?(?
?
0)?(?
?
0)
?(??0 + 12 )?(?
?
0 + 12 )
K
X
k=1
?(?
?
k + 12 )?(?
?
k + 12 )
?(??k)?(?
?
k)
, (7)
where ??0 =
?
k ?
?
k and ?
?
0 =
?
k ?
?
k. Note that
with the Dirichlet prior, ??k = ?k + c(w1, fk) and
??k = ?k + c(w2, fk), where ?k and ?k are the
hyperparameters of the priors of w1 and w2, re-
spectively.
To put it all together, we can obtain a new
Bayesian similarity measure on words, which can
be calculated only from the hyperparameters for
the Dirichlet prior, ? and ?, and the observed
counts c(wi, fk). It is written as follows.
BCb(w1, w2) = (8)
?(?0 + a0)?(?0 + b0)
?(?0 + a0 + 12 )?(?0 + b0 +
1
2 )
?
K
X
k=1
?(?k + c(w1, fk) + 12 )?(?k + c(w2, fk) +
1
2 )
?(?k + c(w1, fk))?(?k + c(w2, fk))
,
where a0 =
?
k c(w1, fk) and b0 =
?
k c(w2, fk). We call this new measure the
Bayesian Bhattacharyya coefficient (BCb for
short). For simplicity, we assume ?k = ?k = ? in
this paper.
We can see that BCb actually encodes our guid-
ing intuition. Consider four words, w0, w1, w2,
and w4, for which we have c(w0, f1) = 10,
c(w1, f1) = 2, c(w2, f1) = 10, and c(w3, f1) =
20. They have counts only for the first dimen-
sion, i.e., they have the same context profile:
p(f1|wi) = 1.0, when we employ MLE. When
K = 10, 000 and ?k = 1.0, the Bayesian similar-
ity between these words is calculated as
BCb(w0, w1) = 0.785368
BCb(w0, w2) = 0.785421
BCb(w0, w3) = 0.785463
We can see that similarities are different ac-
cording to the number of observations, as ex-
pected. Note that the non-Bayesian BC will re-
turn the same value, 1.0, for all cases. Note
also that BCb(w0, w0) = 0.78542 if we use Eq.
8, meaning that the self-similarity might not be
the maximum. This is conceptually strange, al-
though not a serious problem since we hardly use
sim(wi, wi) in practice. If we want to fix this,
we can use the special definition: BCb(wi, wi) ?
1. This is equivalent to using simb(wi, wi) =
E[sim(wi, wi)]{p(v(wi))} = 1 only for this case.
249
4 Implementation Issues
Although we have derived the analytical form
(Eq. 8), there are several problems in implement-
ing robust and efficient calculations.
First, the Gamma function in Eq. 8 overflows
when the argument is larger than 170. In such
cases, a commonly used way is to work in the log-
arithmic space. In this study, we utilize the ?log
Gamma? function: ln?(x), which returns the log-
arithm of the Gamma function directly without the
overflow problem.2
Second, the calculation of the log Gamma func-
tion is heavier than operations such as simple mul-
tiplication, which is used in existing measures.
In fact, the log Gamma function is implemented
using an iterative algorithm such as the Lanczos
method. In addition, according to Eq. 8, it seems
that we have to sum up the values for all k, be-
cause even if c(wi, fk) is zero the value inside the
summation will not be zero. In the existing mea-
sures, it is often the case that we only need to sum
up for k where c(wi, fk) > 0. Because c(wi, fk)
is usually sparse, that technique speeds up the cal-
culation of the existing measures drastically and
makes it practical.
In this study, the above problem is solved by
pre-computing the required log Gamma values, as-
suming that we calculate similarities for a large
set of words, and pre-computing default values for
cases where c(wi, fk) = 0. The following values
are pre-computed once at the start-up time.
For each word:
(A) ln?(?0 + a0) ? ln?(?0 + a0 + 12)
(B) ln?(?k+c(wi, fk))?ln?(?k+c(wi, fk)+ 12)
for all k where c(wi, fk) > 0
(C) ? exp(2(ln?(?k + 12) ? ln?(?k)))) +
exp(ln?(?k + c(wi, fk)) ? ln?(?k +
c(wi, fk) + 12) + ln?(?k +
1
2) ? ln?(?k))
for all k where c(wi, fk) > 0;
For each k:
(D): exp(2(ln?(?k + 12)).
In the calculation of BCb(w1, w2), we first as-
sume that all c(wi, fk) = 0 and set the output
variable to the default value. Then, we iterate
over the sparse vectors c(w1, fk) and c(w2, fk). If
2We used the GNU Scientific Library (GSL)
(www.gnu.org/software/gsl/), which implements this
function.
c(w1, fk) > 0 and c(w2, fk) = 0 (and vice versa),
we update the output variable just by adding (C).
If c(w1, fk) > 0 and c(w2, fk) > 0, we update
the output value using (B), (D) and one additional
exp(.) operation. With this implementation, we
can make the computation of BCb practically as
fast as using other measures.
5 Experiments
5.1 Evaluation setting
We evaluated our method in the calculation of sim-
ilarities between nouns in Japanese.
Because human evaluation of word similari-
ties is very difficult and costly, we conducted au-
tomatic evaluation in the set expansion setting,
following previous studies such as Pantel et al
(2009).
Given a word set, which is expected to con-
tain similar words, we assume that a good simi-
larity measure should output, for each word in the
set, the other words in the set as similar words.
For given word sets, we can construct input-and-
answers pairs, where the answers for each word
are the other words in the set the word appears in.
We output a ranked list of 500 similar words
for each word using a given similarity measure
and checked whether they are included in the an-
swers. This setting could be seen as document re-
trieval, and we can use an evaluation measure such
as the mean of the precision at top T (MP@T ) or
the mean average precision (MAP). For each input
word, P@T (precision at top T ) and AP (average
precision) are defined as follows.
P@T = 1T
T
X
i=1
?(wi ? ans),
AP = 1R
N
X
i=1
?(wi ? ans)P@i.
?(wi ? ans) returns 1 if the output word wi is
in the answers, and 0 otherwise. N is the number
of outputs and R is the number of the answers.
MP@T and MAP are the averages of these values
over all input words.
5.2 Collecting context profiles
Dependency relations are used as context profiles
as in Kazama and Torisawa (2008) and Kazama et
al. (2009). From a large corpus of Japanese Web
documents (Shinzato et al, 2008) (100 million
250
documents), where each sentence has a depen-
dency parse, we extracted noun-verb and noun-
noun dependencies with relation types and then
calculated their frequencies in the corpus. If a
noun, n, depends on a word, w, with a relation,
r, we collect a dependency pair, (n, ?w, r?). That
is, a context fk, is ?w, r? here.
For noun-verb dependencies, postpositions
in Japanese represent relation types. For
example, we extract a dependency relation
(???, ? ??,? ?) from the sentence below,
where a postposition ?? (wo)? is used to mark
the verb object.
??? (wine)? (wo)?? (buy) (? buy a wine)
Note that we leave various auxiliary verb suf-
fixes, such as ??? (reru),? which is for passiviza-
tion, as a part of w, since these greatly change the
type of n in the dependent position.
As for noun-noun dependencies, we considered
expressions of type ?n1 ? n2? (? ?n2 of n1?) as
dependencies (n1, ?n2,? ?).
We extracted about 470 million unique depen-
dencies from the corpus, containing 31 million
unique nouns (including compound nouns as de-
termined by our filters) and 22 million unique con-
texts, fk. We sorted the nouns according to the
number of unique co-occurring contexts and the
contexts according to the number of unique co-
occurring nouns, and then we selected the top one
million nouns and 100,000 contexts. We used only
260 million dependency pairs that contained both
the selected nouns and the selected contexts.
5.3 Test sets
We prepared three test sets as follows.
Set ?A? and ?B?: Thesaurus siblings We
considered that words having a common
hypernym (i.e., siblings) in a manually
constructed thesaurus could constitute a
similar word set. We extracted such sets
from a Japanese dictionary, EDR (V3.0)
(CRL, 2002), which contains concept hier-
archies and the mapping between words and
concepts. The dictionary contains 304,884
nouns. In all, 6,703 noun sibling sets were
extracted with the average set size of 45.96.
We randomly chose 200 sets each for sets
?A? and ?B.? Set ?A? is a development set to
tune the value of the hyperparameters and
?B? is for the validation of the parameter
tuning.
Set ?C?: Closed sets Murata et al (2004) con-
structed a dataset that contains several closed
word sets such as the names of countries,
rivers, sumo wrestlers, etc. We used all of
the 45 sets that are marked as ?complete? in
the data, containing 12,827 unique words in
total.
Note that we do not deal with ambiguities in the
construction of these sets as well as in the calcu-
lation of similarities. That is, a word can be con-
tained in several sets, and the answers for such a
word is the union of the words in the sets it belongs
to (excluding the word itself).
In addition, note that the words in these test sets
are different from those of our one-million-word
vocabulary. We filtered out the words that are not
included in our vocabulary and removed the sets
with size less than 2 after the filtering.
Set ?A? contained 3,740 words that are actually
evaluated, with about 115 answers on average, and
?B? contained 3,657 words with about 65 answers
on average. Set ?C? contained 8,853 words with
about 1,700 answers on average.
5.4 Compared similarity measures
We compared our Bayesian Bhattacharyya simi-
larity measure, BCb, with the following similarity
measures.
JS Jensen-Shannon divergence between p(fk|w1)
and p(fk|w2) (Dagan et al, 1994; Dagan et
al., 1999).
PMI-cos The cosine of the context profile vec-
tors, where the k-th dimension is the point-
wise mutual information (PMI) between
wi and fk defined as: PMI(wi, fk) =
log p(wi,fk)p(wi)p(fk) (Pantel and Lin, 2002; Pantel
et al, 2009).3
Cls-JS Kazama et al (2009) proposed using
the Jensen-Shannon divergence between hid-
den class distributions, p(c|w1) and p(c|w2),
which are obtained by using an EM-based
clustering of dependency relations with a
model p(wi, fk) =
?
c p(wi|c)p(fk|c)p(c)
(Kazama and Torisawa, 2008). In order to
3We did not use the discounting of the PMI values de-
scribed in Pantel and Lin (2002).
251
alleviate the effect of local minima of the EM
clustering, they proposed averaging the simi-
larities by several different clustering results,
which can be obtained by using different ini-
tial parameters. In this study, we combined
two clustering results (denoted as ?s1+s2? in
the results), each of which (?s1? and ?s2?)
has 2,000 hidden classes.4 We included this
method since clustering can be regarded as
another way of treating data sparseness.
BC The Bhattacharyya coefficient (Bhat-
tacharyya, 1943) between p(fk|w1) and
p(fk|w2). This is the baseline for BCb.
BCa The Bhattacharyya coefficient with absolute
discounting. In calculating p(fk|wi), we sub-
tract the discounting value, ?, from c(wi, fk)
and equally distribute the residual probabil-
ity mass to the contexts whose frequency is
zero. This is included as an example of naive
smoothing methods.
Since it is very costly to calculate the sim-
ilarities with all of the other words (one mil-
lion in our case), we used the following approx-
imation method that exploits the sparseness of
c(wi, fk). Similar methods were used in Pantel
and Lin (2002), Kazama et al (2009), and Pan-
tel et al (2009) as well. For a given word, wi,
we sort the contexts in descending order accord-
ing to c(wi, fk) and retrieve the top-L contexts.5
For each selected context, we sort the words in de-
scending order according to c(wi, fk) and retrieve
the top-M words (L = M = 1600).6 We merge
all of the words above as candidate words and cal-
culate the similarity only for the candidate words.
Finally, the top 500 similar words are output.
Note also that we used modified counts,
log(c(wi, fk)) + 1, instead of raw counts,
c(wi, fk), with the intention of alleviating the ef-
fect of strangely frequent dependencies, which can
be found in the Web data. In preliminary ex-
periments, we observed that this modification im-
proves the quality of the top 500 similar words as
reported in Terada et al (2004) and Kazama et al
(2009).
4In the case of EM clustering, the number of unique con-
texts, fk, was also set to one million instead of 100,000, fol-
lowing Kazama et al (2009).
5It is possible that the number of contexts with non-zero
counts is less than L. In that case, all of the contexts with
non-zero counts are used.
6Sorting is performed only once in the initialization step.
Table 1: Performance on siblings (Set A).
Measure MAP MP
@1 @5 @10 @20
JS 0.0299 0.197 0.122 0.0990 0.0792
PMI-cos 0.0332 0.195 0.124 0.0993 0.0798
Cls-JS (s1) 0.0319 0.195 0.122 0.0988 0.0796
Cls-JS (s2) 0.0295 0.198 0.122 0.0981 0.0786
Cls-JS (s1+s2) 0.0333 0.206 0.129 0.103 0.0841
BC 0.0334 0.211 0.131 0.106 0.0854
BCb (0.0002) 0.0345 0.223 0.138 0.109 0.0873
BCb (0.0016) 0.0356 0.242 0.148 0.119 0.0955
BCb (0.0032) 0.0325 0.223 0.137 0.111 0.0895
BCa (0.0016) 0.0337 0.212 0.133 0.107 0.0863
BCa (0.0362) 0.0345 0.221 0.136 0.110 0.0890
BCa (0.1) 0.0324 0.214 0.128 0.101 0.0825
without log(c(wi, fk)) + 1 modification
JS 0.0294 0.197 0.116 0.0912 0.0712
PMI-cos 0.0342 0.197 0.125 0.0987 0.0793
BC 0.0296 0.201 0.118 0.0915 0.0721
As for BCb, we assumed that all of the hyper-
parameters had the same value, i.e., ?k = ?. It
is apparent that an excessively large ? is not ap-
propriate because it means ignoring observations.
Therefore, ?must be tuned. The discounting value
of BCa is also tuned.
5.5 Results
Table 1 shows the results for Set A. The MAP and
the MPs at the top 1, 5, 10, and 20 are shown for
each similarity measure. As for BCb and BCa, the
results for the tuned and several other values for ?
are shown. Figure 1 shows the parameter tuning
for BCb with MAP as the y-axis (results for BCa
are shown as well). Figure 2 shows the same re-
sults with MPs as the y-axis. The MAP and MPs
showed a correlation here. From these results, we
can see that BCb surely improves upon BC, with
6.6% improvement in MAP and 14.7% improve-
ment in MP@1 when ? = 0.0016. BCb achieved
the best performance among the compared mea-
sures with this setting. The absolute discounting,
BCa, improved upon BC as well, but the improve-
ment was smaller than with BCb. Table 1 also
shows the results for the case where we did not
use the log-modified counts. We can see that this
modification gives improvements (though slight or
unclear for PMI-cos).
Because tuning hyperparameters involves the
possibility of overfitting, its robustness should be
assessed. We checked whether the tuned ? with
Set A works well for Set B. The results are shown
in Table 2. We can see that the best ? (= 0.0016)
found for Set A works well for Set B as well. That
is, the tuning of ? as above is not unrealistic in
252
 0.02
 0.022
 0.024
 0.026
 0.028
 0.03
 0.032
 0.034
 0.036
 1e-06  1e-05  0.0001  0.001  0.01  0.1  1
MA
P
? (log-scale)
BayesAbsolute Discounting
Figure 1: Tuning of ? (MAP). The dashed hori-
zontal line indicates the score of BC.
 0.04 0.06
 0.08 0.1
 0.12 0.14
 0.16 0.18
 0.2 0.22
 0.24 0.26
 1e-06  1e-05  0.0001  0.001  0.01
MP
? (log-scale)
 MP@1
 MP@5
 MP@10
 MP@20
 MP@30
 MP@40
Figure 2: Tuning of ? (MP).
practice because it seems that we can tune it ro-
bustly using a small subset of the vocabulary as
shown by this experiment.
Next, we evaluated the measures on Set C, i.e.,
the closed set data. The results are shown in Ta-
ble 3. For this set, we observed a tendency that
is different from Sets A and B. Cls-JS showed a
particularly good performance. BCb surely im-
proves upon BC. For example, the improvement
was 7.5% for MP@1. However, the improvement
in MAP was slight, and MAP did not correlate
well with MPs, unlike in the case of Sets A and
B.
We thought one possible reason is that the num-
ber of outputs, 500, for each word was not large
enough to assess MAP values correctly because
the average number of answers is 1,700 for this
dataset. In fact, we could output more than 500
words if we ignored the cost of storage. Therefore,
we also included the results for the case where
L = M = 3600 and N = 2, 000. Even with
this setting, however, MAP did not correlate well
with MPs.
Although Cls-JS showed very good perfor-
mance for Set C, note that the EM clustering
is very time-consuming (Kazama and Torisawa,
2008), and it took about one week with 24 CPU
cores to get one clustering result in our computing
environment. On the other hand, the preparation
Table 2: Performance on siblings (Set B).
Measure MAP MP
@1 @5 @10 @20
JS 0.0265 0.208 0.116 0.0855 0.0627
PMI-cos 0.0283 0.203 0.116 0.0871 0.0660
Cls-JS (s1+s2) 0.0274 0.194 0.115 0.0859 0.0643
BC 0.0295 0.223 0.124 0.0922 0.0693
BCb (0.0002) 0.0301 0.225 0.128 0.0958 0.0718
BCb (0.0016) 0.0313 0.246 0.135 0.103 0.0758
BCb (0.0032) 0.0279 0.228 0.127 0.0938 0.0698
BCa (0.0016) 0.0297 0.223 0.125 0.0934 0.0700
BCa (0.0362) 0.0298 0.223 0.125 0.0934 0.0705
BCa (0.01) 0.0300 0.224 0.126 0.0949 0.0710
Table 3: Performance on closed-sets (Set C).
Measure MAP MP
@1 @5 @10 @20
JS 0.127 0.607 0.582 0.566 0.544
PMI-cos 0.124 0.531 0.519 0.508 0.493
Cls-JS (s1) 0.125 0.589 0.566 0.548 0.525
Cls-JS (s2) 0.137 0.608 0.592 0.576 0.554
Cls-JS (s1+s2) 0.152 0.638 0.617 0.603 0.583
BC 0.131 0.602 0.579 0.565 0.545
BCb (0.0004) 0.133 0.636 0.605 0.587 0.563
BCb (0.0008) 0.131 0.647 0.615 0.594 0.568
BCb (0.0016) 0.126 0.644 0.615 0.593 0.564
BCb (0.0032) 0.107 0.573 0.556 0.529 0.496
L = M = 3200 and N = 2000
JS 0.165 0.605 0.580 0.564 0.543
PMI-cos 0.165 0.530 0.517 0.507 0.492
Cls-JS (s1+s2) 0.209 0.639 0.618 0.603 0.584
BC 0.168 0.600 0.577 0.562 0.542
BCb (0.0004) 0.170 0.635 0.604 0.586 0.562
BCb (0.0008) 0.168 0.647 0.615 0.594 0.568
BCb (0.0016) 0.161 0.644 0.615 0.593 0.564
BCb (0.0032) 0.140 0.573 0.556 0.529 0.496
for our method requires just an hour with a single
core.
6 Discussion
We should note that the improvement by using our
method is just ?on average,? as in many other NLP
tasks, and observing clear qualitative change is rel-
atively difficult, for example, by just showing ex-
amples of similar word lists here. Comparing the
results of BCb and BC, Table 4 lists the numbers
of improved, unchanged, and degraded words in
terms of MP@20 for each evaluation set. As can
be seen, there are a number of degraded words, al-
though they are fewer than the improved words.
Next, Figure 3 shows the averaged differences of
MP@20 in each 40,000 word-ID range.7 We can
observe that the advantage of BCb is lessened es-
7Word IDs are assigned in ascending order when we chose
the top one million words as described in Section 5.2, and
they roughly correlate with frequencies. So, frequent words
tend to have low-IDs.
253
Table 4: The numbers of improved, unchanged,
and degraded words in terms of MP@20 for each
evaluation set.
# improved # unchanged # degraded
Set A 755 2,585 400
Set B 643 2,610 404
Set C 3,153 3,962 1,738
-0.01 0
 0.01 0.02
 0.03 0.04
 0.05 0.06
 0  500000  1e+06Avg
. Diff. 
of MP
@20
ID range
-0.01 0
 0.01 0.02
 0.03 0.04
 0.05 0.06
 0  500000  1e+06Avg
. Diff. 
of MP
@20
ID range
-0.01 0 0.01
 0.02 0.03 0.04
 0.05 0.06 0.07
 0.08
 0  500000  1e+06Avg
. Diff. 
of MP
@20
ID range
Figure 3: Averaged Differences of MP@20 be-
tween BCb (0.0016) and BC within each 40,000
ID range (Left: Set A. Right: Set B. Bottom: Set
C).
pecially for low-ID words (as expected) with on-
average degradation.8 The improvement is ?on av-
erage? in this sense as well.
One might suspect that the answer words tended
to be low-ID words, and the proposed method is
simply biased towards low-ID words because of
its nature. Then, the observed improvement is a
trivial consequence. Table 5 lists some interest-
ing statistics about the IDs. We can see that BCb
surely outputs more low-ID words than BC, and
BC more than Cls-JS and JS.9 However, the av-
erage ID of the outputs of BC is already lower
than the average ID of the answer words. There-
fore, even if BCb preferred lower-ID words than
BC, it should not have the effect of improving
the accuracy. That is, the improvement by BCb
is not superficial. From BC/BCb, we can also see
that the IDs of the correct outputs did not become
smaller compared to the IDs of the system outputs.
Clearly, we need more analysis on what caused
the improvement by the proposed method and how
that affects the efficacy in real applications of sim-
ilarity measures.
The proposed Bayesian similarity measure out-
performed the baseline Bhattacharyya coefficient
8This suggests the use of different ?s depending on ID
ranges (e.g., smaller ? for low-ID words) in practice.
9The outputs of Cls-JS are well-balanced in the ID space.
Table 5: Statistics on IDs. (A): Avg. ID of an-
swers. (B): Avg. ID of system outputs. (C): Avg.
ID of correct system outputs.
Set A Set C
(A) 238,483 255,248
(B) (C) (B) (C)
Cls-JS (s1+s2) 282,098 176,706 273,768 232,796
JS 183,054 11,3442 211,671 201,214
BC 162,758 98,433 193,508 189,345
BCb(0.0016) 55,915 54,786 90,472 127,877
BC/BCb 2.91 1.80 2.14 1.48
and other well-known similarity measures. As
a smoothing method, it also outperformed a
naive absolute discounting. Of course, we can-
not say that the proposed method is better than
any other sophisticated smoothing method at this
point. However, as noted above, there has
been no serious attempt to assess the effect of
smoothing in the context of word similarity cal-
culation. Recent studies have pointed out that
the Bayesian framework derives state-of-the-art
smoothing methods such as Kneser-Ney smooth-
ing as a special case (Teh, 2006; Mochihashi et
al., 2009). Consequently, it is reasonable to re-
sort to the Bayesian framework. Conceptually,
our method is equivalent to modifying p(fk|wi)
as p(fk|wi) =
{
?(?0+a0)?(?k+c(wi,fk)+ 12 )
?(?0+a0+ 12 )?(?k+c(wi,fk))
}2
and
taking the Bhattacharyya coefficient. However,
the implication of this form has not yet been in-
vestigated, and so we leave it for future research.
Our method is the simplest one as a Bayesian
method. We did not employ any numerical opti-
mization or sampling iterations, as in a more com-
plete use of the Bayesian framework (Teh, 2006;
Mochihashi et al, 2009). Instead, we used the ob-
tained analytical form directly with the assump-
tion that ?k = ? and ? can be tuned directly by
using a simple grid search with a small subset of
the vocabulary as the development set. If substan-
tial additional costs are allowed, we can fine-tune
each ?k using more complete Bayesian methods.
We also leave this for future research.
In terms of calculation procedure, BCb has the
same form as other similarity measures, which is
basically the same as the inner product of sparse
vectors. Thus, it can be as fast as other similar-
ity measures with some effort as we described in
Section 4 when our aim is to calculate similarities
between words in a fixed large vocabulary. For ex-
ample, BCb took about 100 hours to calculate the
254
top 500 similar nouns for all of the one million
nouns (using 16 CPU cores), while JS took about
57 hours. We think this is an acceptable additional
cost.
The limitation of our method is that it can-
not be used efficiently with similarity measures
other than the Bhattacharyya coefficient, although
that choice seems good as shown in the experi-
ments. For example, it seems difficult to use the
Jensen-Shannon divergence as the base similar-
ity because the analytical form cannot be derived.
One way we are considering to give more flexi-
bility to our method is to adjust ?k depending on
external knowledge such as the importance of a
context (e.g., PMIs). In another direction, we will
be able to use a ?weighted? Bhattacharyya coeffi-
cient:
?
k ?(w1, fk)?(w2, fk)
?
p1k ? p2k, where
the weights, ?(wi, fk), do not depend on pik, as
the base similarity measure. The analytical form
for it will be a weighted version of BCb.
BCb can also be generalized to the case where
the base similarity is BCd(p1, p2) =
?K
k=1 pd1k ?
pd2k, where d > 0. The Bayesian analytical form
becomes as follows.
BCdb (w1, w2) =
?(?0 + a0)?(?0 + b0)
?(?0 + a0 + d)?(?0 + b0 + d)
?
K
X
k=1
?(?k + c(w1, fk) + d)?(?k + c(w2, fk) + d)
?(?k + c(w1, fk))?(?k + c(w2, fk))
.
See Appendix A for the derivation. However, we
restricted ourselves to the case of d = 12 in this
study.
Finally, note that our BCb is different from
the Bhattacharyya distance measure on Dirichlet
distributions of the following form described in
Rauber et al (2008) in its motivation and analyti-
cal form:
p
?(??0)?(?
?
0)
q
Q
k ?(?
?
k)
q
Q
k ?(?
?
k)
?
Q
k ?((?
?
k + ?
?
k)/2)
?( 12
PK
k (?
?
k + ?
?
k))
. (9)
Empirical and theoretical comparisons with this
measure also form one of the future directions.10
7 Conclusion
We proposed a Bayesian method for robust distri-
butional word similarities. Our method uses a dis-
tribution of context profiles obtained by Bayesian
10Our preliminary experiments show that calculating sim-
ilarity using Eq. 9 for the Dirichlet distributions obtained by
Eq. 6 does not produce meaningful similarity (i.e., the accu-
racy is very low).
estimation and takes the expectation of a base sim-
ilarity measure under that distribution. We showed
that, in the case where the context profiles are
multinomial distributions, the priors are Dirichlet,
and the base measure is the Bhattacharyya coeffi-
cient, we can derive an analytical form, permitting
efficient calculation. Experimental results show
that the proposed measure gives better word simi-
larities than a non-Bayesian Bhattacharyya coeffi-
cient, other well-known similarity measures such
as Jensen-Shannon divergence and the cosine with
PMI weights, and the Bhattacharyya coefficient
with absolute discounting.
Appendix A
Here, we give the analytical form for the general-
ized case (BCdb ) in Section 6. Recall the following
relation, which is used to derive the normalization
factor of the Dirichlet distribution:
Z
?
Y
k
??
?
k?1
k d? =
Q
k ?(?
?
k)
?(??0)
= Z(?
?
)?1. (10)
Then, BCdb (w1, w2)
=
ZZ
???
Dir(?1|?
?
)Dir(?2|?
?
)
X
k
?d1k?d2k d?1 d?2
= Z(?
?
)Z(?
?
)?
ZZ
???
Y
l
??
?
l?1
1l
Y
m
??
?
m?1
2m
X
k
?d1k?d2k d?1 d?2
| {z }
A
.
Using Eq. 10, A in the above can be calculated as
follows:
=
Z
?
Y
m
??
?
m?1
2m
2
4
X
k
?d2k
Z
?
??
?
k+d?1
1k
Y
l?=k
??
?
l?1
1l d?1
3
5 d?2
=
Z
?
Y
m
??
?
m?1
2m
"
X
k
?d2k
?(?
?
k + d)
Q
l?=k ?(?
?
l)
?(??0 + d)
#
d?2
=
X
k
?(?
?
k + d)
Q
l?=k ?(?
?
l)
?(??0 + d)
Z
?
??
?
k+d?1
2k
Y
m ?=k
??
?
m?1
2m d?2
=
X
k
?(?
?
k + d)
Q
l?=k ?(?
?
l)
?(??0 + d)
?(?
?
k + d)
Q
m?=k ?(?
?
m)
?(??0 + d)
=
Q
?(?
?
l)
Q
?(?
?
m)
?(??0 + d)?(?
?
0 + d)
X
k
?(?
?
k + d)
?(??k)
?(?
?
k + d)
?(??k)
.
This will give:
BCdb (w1, w2) =
?(?
?
0)?(?
?
0)
?(??0 + d)?(?
?
0 + d)
K
X
k=1
?(?
?
k + d)?(?
?
k + d)
?(??k)?(?
?
k)
.
255
References
A. Bhattacharyya. 1943. On a measure of divergence
between two statistical populations defined by their
probability distributions. Bull. Calcutta Math. Soc.,
49:214?224.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. TR-10-98, Computer Science Group,
Harvard University.
Stanley F. Chen and Ronald Rosenfeld. 2000. A
survey of smoothing techniques for ME models.
IEEE Transactions on Speech and Audio Process-
ing, 8(1):37?50.
Corinna Cortes and Vladimir Vapnik. 1995. Support
vector networks. Machine Learning, 20:273?297.
CRL. 2002. EDR electronic dictionary version 2.0
technical guide. Communications Research Labo-
ratory (CRL).
Ido Dagan, Fernando Pereira, and Lillian Lee. 1994.
Similarity-based estimation of word cooccurrence
probabilities. In Proceedings of ACL 94.
Ido Dagan, Shaul Marcus, and Shaul Markovitch.
1995. Contextual word similarity and estimation
from sparse data. Computer, Speech and Language,
9:123?152.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1997.
Similarity-based methods for word sense disam-
biguation. In Proceedings of ACL 97.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1999.
Similarity-based models of word cooccurrence
probabilities. Machine Learning, 34(1-3):43?69.
Gregory Grefenstette. 1994. Explorations In Auto-
matic Thesaurus Discovery. Kluwer Academic Pub-
lishers.
Zellig Harris. 1954. Distributional structure. Word,
pages 146?142.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In Proceedings of
ACL-90, pages 268?275.
Jun?ichi Kazama and Kentaro Torisawa. 2008. In-
ducing gazetteers for named entity recognition by
large-scale clustering of dependency relations. In
Proceedings of ACL-08: HLT.
Jun?ichi Kazama, Stijn De Saeger, Kentaro Torisawa,
and Masaki Murata. 2009. Generating a large-scale
analogy list using a probabilistic clustering based on
noun-verb dependency profiles. In Proceedings of
15th Annual Meeting of The Association for Natural
Language Processing (in Japanese).
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING/ACL-
98, pages 768?774.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested Pitman-Yor language modeling.
In Proceedings of ACL-IJCNLP 2009, pages 100?
108.
Masaki Murata, Qing Ma, Tamotsu Shirado, and Hi-
toshi Isahara. 2004. Database for evaluating ex-
tracted terms and tool for visualizing the terms. In
Proceedings of LREC 2004 Workshop: Computa-
tional and Computer-Assisted Terminology, pages
6?9.
Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 613?619.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP 2009, pages 938?947.
T. W. Rauber, T. Braun, and K. Berns. 2008. Proba-
bilistic distance measures of the Dirichlet and Beta
distributions. Pattern Recognition, 41:637?645.
Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,
Chikara Hashimoto, and Sadao Kurohashi. 2008.
Tsubaki: An open search engine infrastructure for
developing new information access. In Proceedings
of IJCNLP 2008.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proceedings of COLING-ACL 2006, pages 985?992.
Akira Terada, Minoru Yoshida, and Hiroshi Nakagawa.
2004. A tool for constructing a synonym dictionary
using context information. In IPSJ SIG Technical
Report (in Japanese), pages 87?94.
256
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1087?1097,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Extracting Paraphrases from Definition Sentences on the Web
Chikara Hashimoto? Kentaro Torisawa? Stijn De Saeger?
Jun?ichi Kazama? Sadao Kurohashi?
? ? ? ? National Institute of Information and Communications Technology
Kyoto, 619-0237, JAPAN
? ?Graduate School of Informatics, Kyoto University
Kyoto, 606-8501, JAPAN
{? ch,? torisawa, ? stijn,? kazama}@nict.go.jp
?kuro@i.kyoto-u.ac.jp
Abstract
We propose an automatic method of extracting
paraphrases from definition sentences, which
are also automatically acquired from the Web.
We observe that a huge number of concepts
are defined in Web documents, and that the
sentences that define the same concept tend
to convey mostly the same information using
different expressions and thus contain many
paraphrases. We show that a large number
of paraphrases can be automatically extracted
with high precision by regarding the sentences
that define the same concept as parallel cor-
pora. Experimental results indicated that with
our method it was possible to extract about
300,000 paraphrases from 6? 108 Web docu-
ments with a precision rate of about 94%.
1 Introduction
Natural language allows us to express the same in-
formation in many ways, which makes natural lan-
guage processing (NLP) a challenging area. Ac-
cordingly, many researchers have recognized that
automatic paraphrasing is an indispensable compo-
nent of intelligent NLP systems (Iordanskaja et al,
1991; McKeown et al, 2002; Lin and Pantel, 2001;
Ravichandran and Hovy, 2002; Kauchak and Barzi-
lay, 2006; Callison-Burch et al, 2006) and have tried
to acquire a large amount of paraphrase knowledge,
which is a key to achieving robust automatic para-
phrasing, from corpora (Lin and Pantel, 2001; Barzi-
lay and McKeown, 2001; Shinyama et al, 2002;
Barzilay and Lee, 2003).
We propose a method to extract phrasal para-
phrases from pairs of sentences that define the same
concept. The method is based on our observation
that two sentences defining the same concept can
be regarded as a parallel corpus since they largely
convey the same information using different expres-
sions. Such definition sentences abound on the Web.
This suggests that we may be able to extract a large
amount of phrasal paraphrase knowledge from the
definition sentences on the Web.
For instance, the following two sentences, both of
which define the same concept ?osteoporosis?, in-
clude two pairs of phrasal paraphrases, which are
indicated by underlines 1? and 2?, respectively.
(1) a. Osteoporosis is a disease that 1? decreases the
quantity of bone and 2? makes bones fragile.
b. Osteoporosis is a disease that 1? reduces bone
mass and 2? increases the risk of bone fracture.
We define paraphrase as a pair of expressions be-
tween which entailment relations of both directions
hold. (Androutsopoulos and Malakasiotis, 2010).
Our objective is to extract phrasal paraphrases
from pairs of sentences that define the same con-
cept. We propose a supervised method that exploits
various kinds of lexical similarity features and con-
textual features. Sentences defining certain concepts
are acquired automatically on a large scale from the
Web by applying a quite simple supervised method.
Previous methods most relevant to our work
used parallel corpora such as multiple translations
of the same source text (Barzilay and McKeown,
2001) or automatically acquired parallel news texts
(Shinyama et al, 2002; Barzilay and Lee, 2003;
Dolan et al, 2004). The former requires a large
amount of manual labor to translate the same texts
1087
in several ways. The latter would suffer from the
fact that it is not easy to automatically retrieve large
bodies of parallel news text with high accuracy. On
the contrary, recognizing definition sentences for
the same concept is quite an easy task at least for
Japanese, as we will show, and we were able to find
a huge amount of definition sentence pairs from nor-
mal Web texts. In our experiments, about 30 million
definition sentence pairs were extracted from 6?108
Web documents, and the estimated number of para-
phrases recognized in the definition sentences using
our method was about 300,000, for a precision rate
of about 94%. Also, our experimental results show
that our method is superior to well-known compet-
ing methods (Barzilay and McKeown, 2001; Koehn
et al, 2007) for extracting paraphrases from defini-
tion sentence pairs.
Our evaluation is based on bidirectional check-
ing of entailment relations between paraphrases that
considers the context dependence of a paraphrase.
Note that using definition sentences is only the
beginning of our research on paraphrase extraction.
We have a more general hypothesis that sentences
fulfilling the same pragmatic function (e.g. defini-
tion) for the same topic (e.g. osteoporosis) convey
mostly the same information using different expres-
sions. Such functions other than definition may in-
clude the usage of the same Linux command, the
recipe for the same cuisine, or the description of re-
lated work on the same research issue.
Section 2 describes related works. Section 3
presents our proposed method. Section 4 reports on
evaluation results. Section 5 concludes the paper.
2 Related Work
The existing work for paraphrase extraction is cat-
egorized into two groups. The first involves a dis-
tributional similarity approach pioneered by Lin and
Pantel (2001). Basically, this approach assumes that
two expressions that have a large distributional simi-
larity are paraphrases. There are also variants of this
approach that address entailment acquisition (Geffet
and Dagan, 2005; Bhagat et al, 2007; Szpektor and
Dagan, 2008; Hashimoto et al, 2009). These meth-
ods can be applied to a normal monolingual corpus,
and it has been shown that a large number of para-
phrases or entailment rules could be extracted. How-
ever, the precision of these methods has been rela-
tively low. This is due to the fact that the evidence,
i.e., distributional similarity, is just indirect evidence
of paraphrase/entailment. Accordingly, these meth-
ods occasionally mistake antonymous pairs for para-
phrases/entailment pairs, since an expression and its
antonymous counterpart are also likely to have a
large distributional similarity. Another limitation of
these methods is that they can find only paraphrases
consisting of frequently observed expressions since
they must have reliable distributional similarity val-
ues for expressions that constitute paraphrases.
The second category is a parallel corpus approach
(Barzilay and McKeown, 2001; Shinyama et al,
2002; Barzilay and Lee, 2003; Dolan et al, 2004).
Our method belongs to this category. This approach
aligns expressions between two sentences in par-
allel corpora, based on, for example, the overlap
of words/contexts. The aligned expressions are as-
sumed to be paraphrases. In this approach, the ex-
pressions do not need to appear frequently in the
corpora. Furthermore, the approach rarely mistakes
antonymous pairs for paraphrases/entailment pairs.
However, its limitation is the difficulty in preparing
a large amount of parallel corpora, as noted before.
We avoid this by using definition sentences, which
can be easily acquired on a large scale from theWeb,
as parallel corpora.
Murata et al (2004) used definition sentences in
two manually compiled dictionaries, which are con-
siderably fewer in the number of definition sen-
tences than those on the Web. Thus, the coverage of
their method should be quite limited. Furthermore,
the precision of their method is much poorer than
ours as we report in Section 4.
For a more extensive survey on paraphrasing
methods, see Androutsopoulos and Malakasiotis
(2010) and Madnani and Dorr (2010).
3 Proposed method
Our method, targeting the Japanese language, con-
sists of two steps: definition sentence acquisition
and paraphrase extraction. We describe them below.
3.1 Definition sentence acquisition
We acquire sentences that define a concept (defini-
tion sentences) as in Example (2), which defines ??
1088
???? (osteoporosis), from the 6?108 Web pages
(Akamine et al, 2010) and the Japanese Wikipedia.
(2) ??????????????????????
(Osteoporosis is a disease that makes bones fragile.)
Fujii and Ishikawa (2002) developed an unsuper-
vised method to find definition sentences from the
Web using 18 sentential templates and a language
model constructed from an encyclopedia. On the
other hand, we developed a supervised method to
achieve a higher precision.
We use one sentential template and an SVM clas-
sifier. Specifically, we first collect definition sen-
tence candidates by a template ??NP??.*?, where
? is the beginning of sentence and NP is the noun
phrase expressing the concept to be defined followed
by a particle sequence, ??? (comitative) and ???
(topic) (and optionally followed by comma), as ex-
emplified in (2). As a result, we collected 3,027,101
sentences. Although the particle sequence tends
to mark the topic of the definition sentence, it can
also appear in interrogative sentences and normal as-
sertive sentences in which a topic is strongly empha-
sized. To remove such non-definition sentences, we
classify the candidate sentences using an SVM clas-
sifier with a polynominal kernel (d = 2).1 Since
Japanese is a head-final language and we can judge
whether a sentence is interrogative or not from the
last words in the sentence, we included morpheme
N -grams and bag-of-words (with the window size
of N ) at the end of sentences in the feature set. The
features are also useful for confirming that the head
verb is in the present tense, which definition sen-
tences should be. Also, we added the morpheme
N -grams and bag-of-words right after the particle
sequence in the feature set since we observe that
non-definition sentences tend to have interrogative
related words like ??? (what) or ???? ((what) on
earth) right after the particle sequence. We chose 5
as N from our preliminary experiments.
Our training data was constructed from 2,911 sen-
tences randomly sampled from all of the collected
sentences. 61.1% of them were labeled as positive.
In the 10-fold cross validation, the classifier?s ac-
curacy, precision, recall, and F1 were 89.4, 90.7,
1We use SVMlight available at http://svmlight.
joachims.org/.
92.2, and 91.4, respectively. Using the classifier,
we acquired 1,925,052 positive sentences from all
of the collected sentences. After adding definition
sentences from Wikipedia articles, which are typi-
cally the first sentence of the body of each article
(Kazama and Torisawa, 2007), we obtained a total
of 2,141,878 definition sentence candidates, which
covered 867,321 concepts ranging from weapons to
rules of baseball. Then, we coupled two definition
sentences whose defined concepts were the same
and obtained 29,661,812 definition sentence pairs.
Obviously, our method is tailored to Japanese. For
a language-independent method of definition acqui-
sition, see Navigli and Velardi (2010) as an example.
3.2 Paraphrase extraction
Paraphrase extraction proceeds as follows. First,
each sentence in a pair is parsed by the depen-
dency parser KNP2 and dependency tree frag-
ments that constitute linguistically well-formed con-
stituents are extracted. The extracted dependency
tree fragments are called candidate phrases here-
after. We restricted candidate phrases to predicate
phrases that consist of at least one dependency re-
lation, do not contain demonstratives, and in which
all the leaf nodes are nominal and all of the con-
stituents are consecutive in the sentence. KNP indi-
cates whether each candidate phrase is a predicate
based on the POS of the head morpheme. Then,
we check all the pairs of candidate phrases between
two definition sentences to find paraphrase pairs.3
In (1), repeated in (3), candidate phrase pairs to be
checked include ( 1? decreases the quantity of bone,
1? reduces bone mass), ( 1? decreases the quantity
of bone, 2? increases the risk of bone fracture), ( 2?
makes bones fragile, 1? reduces bone mass), and ( 2?
makes bones fragile, 2? increases the risk of bone
fracture).
(3) a. Osteoporosis is a disease that 1? decreases the
quantity of bone and 2? makes bones fragile.
b. Osteoporosis is a disease that 1? reduces bone
mass and 2? increases the risk of bone fracture.
2http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp.html.
3Our method discards candidate phrase pairs in which one
subsumes the other in terms of their character string, or the dif-
ference is only one proper noun like ?toner cartridges that Ap-
ple Inc. made? and ?toner cartridges that Xerox made.? Proper
nouns are recognized by KNP.
1089
f1 The ratio of the number of morphemes shared between two candidate phrases to the number of all of the morphemes in the two phrases.
f2 The ratio of the number of a candidate phrase?s morphemes, for which there is a morpheme with small edit distance (1 in our experiment) in
another candidate phrase, to the number of all of the morphemes in the two phrases. Note that Japanese has many orthographical variations
and edit distance is useful for identifying them.
f3 The ratio of the number of a candidate phrase?s morphemes, for which there is a morpheme with the same pronunciation in another candidate
phrase, to the number of all of the morphemes in the two phrases. Pronunciation is also useful for identifying orthographic variations.
Pronunciation is given by KNP.
f4 The ratio of the number of morphemes of a shorter candidate phrase to that of a longer one.
f5 The identity of the inflected form of the head morpheme between two candidate phrases: 1 if they are identical, 0 otherwise.
f6 The identity of the POS of the head morpheme between two candidate phrases: 1 or 0.
f7 The identity of the inflection (conjugation) of the head morpheme between two candidate phrases: 1 or 0.
f8 The ratio of the number of morphemes that appear in a candidate phrase segment of a definition sentence s1 and in a segment that is NOT a
part of the candidate phrase of another definition sentence s2 to the number of all of the morphemes of s1?s candidate phrase, i.e. how many
extra morphemes are incorporated into s1?s candidate phrase.
f9 The reversed (s1 ? s2) version of f8.
f10 The ratio of the number of parent dependency tree fragments that are shared by two candidate phrases to the number of all of the parent de-
pendency tree fragments of the two phrases. Dependency tree fragments are represented by the pronunciation of their component morphemes.
f11 A variation of f10; tree fragments are represented by the base form of their component morphemes.
f12 A variation of f10; tree fragments are represented by the POS of their component morphemes.
f13 The ratio of the number of unigrams (morphemes) that appear in the child context of both candidate phrases to the number of all of the child
context morphemes of both candidate phrases. Unigrams are represented by the pronunciation of the morpheme.
f14 A variation of f13; unigrams are represented by the base form of the morpheme.
f15 A variation of f14; the numerator is the number of child context unigrams that are adjacent to both candidate phrases.
f16 The ratio of the number of trigrams that appear in the child context of both candidate phrases to the number of all of the child context
morphemes of both candidate phrases. Trigrams are represented by the pronunciation of the morpheme.
f17 Cosine similarity between two definition sentences from which a candidate phrase pair is extracted.
Table 1: Features used by paraphrase classifier.
The paraphrase checking of candidate phrase
pairs is performed by an SVM classifier with a linear
kernel that classifies each pair of candidate phrases
into a paraphrase or a non-paraphrase.4 Candidate
phrase pairs are ranked by their distance from the
SVM?s hyperplane. Features for the classifier are
based on our observation that two candidate phrases
tend to be paraphrases if the candidate phrases them-
selves are sufficiently similar and/or their surround-
ing contexts are sufficiently similar. Table 1 lists the
features used by the classifier.5 Basically, they rep-
resent either the similarity of candidate phrases (f1-
9) or that of their contexts (f10-17). We think that
they have various degrees of discriminative power,
and thus we use the SVM to adjust their weights.
Figure 1 illustrates features f8-12, for which you
may need supplemental remarks. English is used for
ease of explanation. In the figure, f8 has a positive
value since the candidate phrase of s1 contains mor-
phemes ?of bone?, which do not appear in the can-
4We use SVMperf available at http://svmlight.
joachims.org/svm perf.html.
5In the table, the parent context of a candidate phrase con-
sists of expressions that appear in ancestor nodes of the candi-
date phrase in terms of the dependency structure of the sentence.
Child contexts are defined similarly.
Figure 1: Illustration of features f8-12.
didate phrase of s2 but do appear in the other part
of s2, i.e. they are extra morphemes for s1?s candi-
date phrase. On the other hand, f9 is zero since there
is no such extra morpheme in s2?s candidate phrase.
Also, features f10-12 have positive values since the
two candidate phrases share two parent dependency
tree fragments, (that increases) and (of fracture).
We have also tried the following features, which
we do not detail due to space limitation: the sim-
ilarity of candidate phrases based on semantically
similar nouns (Kazama and Torisawa, 2008), entail-
ing/entailed verbs (Hashimoto et al, 2009), and the
identity of the pronunciation and base form of the
head morpheme; N -grams (N=1,2,3) of child and
parent contexts represented by either the inflected
form, base form, pronunciation, or POS of mor-
1090
Original definition sentence pair (s1, s2) Paraphrased definition sentence pair (s?1, s?2)
s1: Osteoporosis is a disease that reduces bonemass and makes bones
fragile.
s?1: Osteoporosis is a disease that decreases the quantity of bone and
makes bones fragile.
s2: Osteoporosis is a disease that decreases the quantity of bone and
increases the risk of bone fracture.
s?2: Osteoporosis is a disease that reduces bone mass and increases
the risk of bone fracture.
Figure 2: Bidirectional checking of entailment relation (?) of p1 ? p2 and p2 ? p1. p1 is ?reduces bone mass?
in s1 and p2 is ?decreases the quantity of bone? in s2. p1 and p2 are exchanged between s1 and s2 to generate
corresponding paraphrased sentences s?1 and s?2. p1 ? p2 (p2 ? p1) is verified if s1 ? s?1 (s2 ? s?2) holds. In this
case, both of them hold. English is used for ease of explanation.
pheme; parent/child dependency tree fragments rep-
resented by either the inflected form, base form, pro-
nunciation, or POS; adjacent versions (cf. f15) of
N -gram features and parent/child dependency tree
features. These amount to 78 features, but we even-
tually settled on the 17 features in Table 1 through
ablation tests to evaluate the discriminative power
of each feature.
The ablation tests were conducted using training
data that we prepared. In preparing the training data,
we faced the problem that the completely random
sampling of candidate paraphrase pairs provided us
with only a small number of positive examples.
Thus, we automatically collected candidate para-
phrase pairs that were expected to have a high like-
lihood of being positive as examples to be labeled.
The likelihood was calculated by simply summing
all of the 78 feature values that we have tried, since
they indicate the likelihood of a given candidate
paraphrase pair?s being a paraphrase. Note that val-
ues of the features f8 and f9 are weighted with ?1,
since they indicate the unlikelihood. Specifically,
we first randomly sampled 30,000 definition sen-
tence pairs from the 29,661,812 pairs, and collected
3,000 candidate phrase pairs that had the highest
likelihood from them. The manual labeling of each
candidate phrase pair (p1, p2) was based on bidirec-
tional checking of entailment relation, p1 ? p2 and
p2 ? p1, with p1 and p2 embedded in contexts.
This scheme is similar to the one proposed by
Szpektor et al (2007). We adopt this scheme since
paraphrase judgment might be unstable between an-
notators unless they are given a particular context
based on which they make a judgment. As de-
scribed below, we use definition sentences as con-
texts. We admit that annotators might be biased by
this in some unexpected way, but we believe that
this is a more stable method than that without con-
texts. The labeling process is as follows. First, from
each candidate phrase pair (p1, p2) and its source
definition sentence pair (s1, s2), we create two para-
phrase sentence pairs (s?1, s?2) by exchanging p1 and
p2 between s1 and s2. Then, annotators check if s1
entails s?1 and s2 entails s?2 so that entailment rela-
tions of both directions p1 ? p2 and p2 ? p1 are
checked. Figure 2 shows an example of bidirectional
checking. In this example, both entailment relations,
s1 ? s?1 and s2 ? s?2, hold, and thus the candidate
phrase pair (p1, p2) is judged as positive. We used
(p1, p2), for which entailment relations of both di-
rections held, as positive examples (1,092 pairs) and
the others as negative ones (1,872 pairs).6
We built the paraphrase classifier from the train-
ing data. As mentioned, candidate phrase pairs were
ranked by the distance from the SVM?s hyperplane.
4 Experiment
In this paper, our claims are twofold.
I. Definition sentences on the Web are a treasure
trove of paraphrase knowledge (Section 4.2).
II. Our method of paraphrase acquisition from
definition sentences is more accurate than well-
known competing methods (Section 4.1).
We first verify claim II by comparing our method
with that of Barzilay and McKeown (2001) (BM
method), Moses7 (Koehn et al, 2007) (SMT
method), and that of Murata et al (2004) (Mrt
method). The first two methods are well known for
accurately extracting semantically equivalent phrase
pairs from parallel corpora.8 Then, we verify claim
6The remaining 36 pairs were discarded as they contained
garbled characters of Japanese.
7http://www.statmt.org/moses/
8As anonymous reviewers pointed out, they are unsuper-
vised methods and thus unable to be adapted to definition sen-
1091
I by comparing definition sentence pairs with sen-
tence pairs that are acquired from the Web using Ya-
hoo!JAPANAPI9 as a paraphrase knowledge source.
In the latter data set, two sentences of each pair
are expected to be semantically similar regardless of
whether they are definition sentences. Both sets con-
tain 100,000 pairs.
Three annotators (not the authors) checked evalu-
ation samples. Fleiss? kappa (Fleiss, 1971) was 0.69
(substantial agreement (Landis and Koch, 1977)).
4.1 Our method vs. competing methods
In this experiment, paraphrase pairs are extracted
from 100,000 definition sentence pairs that are ran-
domly sampled from the 29,661,812 pairs. Before
reporting the experimental results, we briefly de-
scribe the BM, SMT, and Mrt methods.
BM method Given parallel sentences like multi-
ple translations of the same source text, the BM
method works iteratively as follows. First, it collects
from the parallel sentences identical word pairs and
their contexts (POS N -grams with indices indicat-
ing corresponding words between paired contexts)
as positive examples and those of different word
pairs as negative ones. Then, each context is ranked
based on the frequency with which it appears in pos-
itive (negative) examples. The most likely K posi-
tive (negative) contexts are used to extract positive
(negative) paraphrases from the parallel sentences.
Extracted positive (negative) paraphrases and their
morpho-syntactic patterns are used to collect addi-
tional positive (negative) contexts. All the positive
(negative) contexts are ranked, and additional para-
phrases and their morpho-syntactic patterns are ex-
tracted again. This iterative process finishes if no
further paraphrase is extracted or the number of iter-
ations reaches a predefined threshold T . In this ex-
periment, following Barzilay and McKeown (2001),
K is 10 and N is 1 to 3. The value of T is not given
in their paper. We chose 3 as its value based on our
preliminary experiments. Note that paraphrases ex-
tracted by this method are not ranked.
tences. Nevertheless, we believe that comparing these methods
with ours is very informative, since they are known to be accu-
rate and have been influential.
9http://developer.yahoo.co.jp/webapi/
SMT method Our SMT method uses Moses
(Koehn et al, 2007) and extracts a phrase table, a
set of two phrases that are translations of each other,
given a set of two sentences that are translations of
each other. If you give Moses monolingual parallel
sentence pairs, it should extract a set of two phrases
that are paraphrases of each other. In this experi-
ment, default values were used for all parameters.
To rank extracted phrase pairs, we assigned each of
them the product of two phrase translation probabil-
ities of both directions that were given by Moses.
For other SMT methods, see Quirk et al (2004) and
Bannard and Callison-Burch (2005) among others.
Mrt method Murata et al (2004) proposed a
method to extract paraphrases from two manually
compiled dictionaries. It simply regards a difference
between two definition sentences of the same word
as a paraphrase candidate. Paraphrase candidates are
ranked according to an unsupervised scoring scheme
that implements their assumption. They assume that
a paraphrase candidate tends to be a valid paraphrase
if it is surrounded by infrequent strings and/or if it
appears multiple times in the data.
In this experiment, we evaluated the unsupervised
version of our method in addition to the supervised
one described in Section 3.2, in order to compare
it fairly with the other methods. The unsupervised
method works in the same way as the supervised
one, except that it ranks candidate phrase pairs by
the sum of all 17 feature values, instead of the dis-
tance from the SVM?s hyperplane. In other words,
no supervised learning is used. All the feature val-
ues are weighted with 1, except for f8 and f9, which
are weighted with ?1 since they indicate the unlike-
lihood of a candidate phrase pair being paraphrases.
BM, SMT, Mrt, and the two versions of our method
were used to extract paraphrase pairs from the same
100,000 definition sentence pairs.
Evaluation scheme Evaluation of each para-
phrase pair (p1, p2) was based on bidirectional
checking of entailment relations p1 ? p2 and p2 ?
p1 in a way similar to the labeling of the training
data. The difference is that contexts for evaluation
are two sentences that are retrieved from the Web
and contain p1 and p2, instead of definition sen-
tences from which p1 and p2 are extracted. This
1092
is intended to check whether extracted paraphrases
are also valid for contexts other than those from
which they are extracted. The evaluation proceeds
as follows. For the top m paraphrase pairs of each
method (in the case of the BM method, randomly
sampled m pairs were used, since the method does
not rank paraphrase pairs), we retrieved a sentence
pair (s1, s2) for each paraphrase pair (p1, p2) from
the Web, such that s1 contains p1 and s2 contains p2.
In doing so, we make sure that neither s1 nor s2 are
the definition sentences from which p1 and p2 are
extracted. For each method, we randomly sample
n samples from all of the paraphrase pairs (p1, p2)
for which both s1 and s2 are retrieved. Then, from
each (p1, p2) and (s1, s2), we create two paraphrase
sentence pairs (s?1, s?2) by exchanging p1 and p2 be-
tween s1 and s2. All samples, each consisting of
(p1, p2), (s1, s2), and (s?1, s?2), are checked by three
human annotators to determine whether s1 entails
s?1 and s2 entails s?2 so that entailment relations of
both directions are verified. In advance of evaluation
annotation, all the evaluation samples are shuffled
so that the annotators cannot find out which sample
is given by which method for fairness. We regard
each paraphrase pair as correct if at least two annota-
tors judge that entailment relations of both directions
hold for it. You may wonder whether only one pair
of sentences (s1, s2) is enough for evaluation since a
correct (wrong) paraphrase pair might be judged as
wrong (correct) accidentally. Nevertheless, we sup-
pose that the final evaluation results are reliable if
the number of evaluation samples is sufficient. In
this experiment, m is 5,000 and n is 200. We use
Yahoo!JAPAN API to retrieve sentences.
Graph (a) in Figure 3 shows a precision curve
for each method. Sup and Uns respectively indi-
cate the supervised and unsupervised versions of our
method. The figure indicates that Sup outperforms
all the others and shows a high precision rate of
about 94% at the top 1,000. Remember that this
is the result of using 100,000 definition sentence
pairs. Thus, we estimate that Sup can extract about
300,000 paraphrase pairs with a precision rate of
about 94%, if we use all 29,661,812 definition sen-
tence pairs that we acquired.
Furthermore, we measured precision after trivial
paraphrase pairs were discarded from the evaluation
samples of each method. A candidate phrase pair
Definition sentence pairs Sup Uns BM SMT Mrt
with trivial 1,381,424 24,049 9,562 18,184
without trivial 1,377,573 23,490 7,256 18,139
Web sentence pairs Sup Uns BM SMT Mrt
with trivial 277,172 5,101 4,586 4,978
without trivial 274,720 4,399 2,342 4,958
Table 2: Number of extracted paraphrases.
(p1, p2) is regarded as trivial if the pronunciation is
the same between p1 and p2,10 or all of the con-
tent words contained in p1 are the same as those
of p2. Graph (b) gives a precision curve for each
method. Again, Sup outperforms the others too, and
maintains a precision rate of about 90% until the top
1,000. These results support our claim II.
The upper half of Table 2 shows the number of
extracted paraphrases with/without trivial pairs for
each method.11 Sup and Uns extracted many more
paraphrases. It is noteworthy that Sup performed the
best in terms of both precision rate and the number
of extracted paraphrases.
Table 3 shows examples of correct and incorrect
outputs of Sup. As the examples indicate, many of
the extracted paraphrases are not specific to defini-
tion sentences and seem very reusable. However,
there are few paraphrases involving metaphors or id-
ioms in the outputs due to the nature of definition
sentences. In this regard, we do not claim that our
method is almighty. We agree with Sekine (2005)
who claims that several different methods are re-
quired to discover a wider variety of paraphrases.
In graphs (a) and (b), the precision of the SMT
method goes up as rank goes down. This strange be-
havior is due to the scoring by Moses that worked
poorly for the data; it gave 1.0 to 82.5% of all the
samples, 38.8% of which were incorrect. We suspect
SMTmethods are poor at monolingual alignment for
paraphrasing or entailment tasks since, in the tasks,
data is much noisier than that used for SMT. See
MacCartney et al (2008) for similar discussion.
4.2 Definition pairs vs. Web sentence pairs
To collect Web sentence pairs, first, we randomly
sampled 1.8 million sentences from the Web corpus.
10There are many kinds of orthographic variants in Japanese,
which can be identified by their pronunciation.
11We set no threshold for candidate phrase pairs of each
method, and counted all the candidate phrase pairs in Table 2.
1093
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  1000  2000  3000  4000  5000
Pre
cisi
on
Top-N
?Sup_def?
?Uns_def?
?SMT_def?
?BM_def?
?Mrt_def?
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  1000  2000  3000  4000  5000
Pre
cisi
on
Top-N
?Sup_def_n?
?Uns_def_n?
?SMT_def_n?
?BM_def_n?
?Mrt_def_n?
(a) Definition sentence pairs with trivial paraphrases (b) Definition sentence pairs without trivial paraphrases
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  1000  2000  3000  4000  5000
Pre
cisi
on
Top-N
?Sup_www?
?Uns_www?
?SMT_www?
?BM_www?
?Mrt_www?
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  1000  2000  3000  4000  5000
Pre
cisi
on
Top-N
?Sup_www_n?
?Uns_www_n?
?SMT_www_n?
?BM_www_n?
?Mrt_www_n?
(c) Web sentence pairs with trivial paraphrases (d) Web sentence pairs without trivial paraphrases
Figure 3: Precision curves of paraphrase extraction.
Rank Paraphrase pair
Correct
13 ?????????????? (send a message to the e-mail address)????????????????? (send
an e-mail message to the e-mail address)
19 ????????? (requested by a customer)?????????? (commissioned by a customer)
70 ?????????? (describe the fiscal condition of company) ??????????? (indicate the fiscal state
of company)
112 ???????????? (get information)???????? (get news)
656 ???????? (it is a convention)????????? (it is a rule)
841 ??????????????? (represent the energy scale of earthquake)????????? (represent the scale
of earthquake)
929 ???????? (cause the oxidation of cells)????????? (cause cellular aging)
1,553 ??????? (remove dead skin cells)??????? (peel off dead skin cells)
2,243 ????????? (required for the development of fetus)??????????????? (indispensable for the
growth and development of fetus)
2,855 ??????? (correct eyesight)???????? (perform eyesight correction)
2,931 ????????? (call it even)?????????? (call it quits)
3,667 ?????????????? (accumulated on a hard disk)?????????????????? (stored on a
hard disk drive)
4,870 ????????? (excrete harmful substance)?????????? (discharge harmful toxin)
5,501 ????????????????????????? (mount two processor cores on one CPU)????????
????????????????? (build two processor cores into one package)
10,675 ??????? (trade foreign currencies)???????? (exchange one currency for another)
112,819 ??????????? (become a regular staff member of the company where (s)he has worked as a temp) ???
????????? (employed by the company where (s)he has worked as a temp)
193,553 ????????????? (access Web sites)??????????? (visit WWW sites)
Incorrect
903 ?????????? (send to a Web browser)??????????? (send to a PC)
2,530 ?????? (intend to balance)?????????? (intend to refresh)
3,008 ???????????? (unable to digest with digestive enzymes)???????????? (hard to digest with
digestive enzymes)
Table 3: Examples of correct and incorrect paraphrases extracted by our supervised method with their rank.
1094
We call them sampled sentences. Then, using Ya-
hoo!JAPANAPI, we retrieved up to 20 snippets rele-
vant to each sampled sentence using all of the nouns
in each sentence as a query. After that, each snippet
was split into sentences, which we call snippet sen-
tences. We paired a sampled sentence and a snippet
sentence that was the most similar to the sampled
sentence. Similarity is the number of nouns shared
by the two sentences. Finally, we randomly sampled
100,000 pairs from all the pairs.
Paraphrase pairs were extracted from the Web
sentence pairs by using BM, SMT, Mrt and the su-
pervised and unsupervised versions of our method.
The features used with our methods were selected
from all of the 78 features mentioned in Section 3.2
so that they performed well for Web sentence pairs.
Specifically, the features were selected by ablation
tests using training data that was tailored to Web
sentence pairs. The training data consisted of 2,741
sentence pairs that were collected in the same way as
the Web sentence pairs and was labeled in the same
way as described in Section 3.2.
Graph (c) of Figure 3 shows precision curves. We
also measured precision without trivial pairs in the
same way as the previous experiment. Graph (d)
shows the results. The lower half of Table 2 shows
the number of extracted paraphrases with/without
trivial pairs for each method.
Note that precision figures of our methods in
graphs (c) and (d) are lower than those of our meth-
ods in graphs (a) and (b). Additionally, none of the
methods achieved a precision rate of 90% using Web
sentence pairs.12 We think that a precision rate of
at least 90% would be necessary if you apply auto-
matically extracted paraphrases to NLP tasks with-
out manual annotation. Only the combination of Sup
and definition sentence pairs achieved that precision.
Also note that, for all of the methods, the numbers
of extracted paraphrases from Web sentence pairs
are fewer than those from definition sentence pairs.
From all of these results, we conclude that our
claim I is verified.
12Precision of SMT is unexpectedly good. We found some
Web sentence pairs consisting of two mostly identical sentences
on rare occasions. The method worked relatively well for them.
5 Conclusion
We proposed a method of extracting paraphrases
from definition sentences on the Web. From the ex-
perimental results, we conclude that the following
two claims of this paper are verified.
1. Definition sentences on the Web are a treasure
trove of paraphrase knowledge.
2. Our method extracts many paraphrases from
the definition sentences on the Web accurately;
it can extract about 300,000 paraphrases from
6 ? 108 Web documents with a precision rate
of about 94%.
Our future work is threefold. First, we will release
extracted paraphrases from all of the 29,661,812
definition sentence pairs that we acquired, after hu-
man annotators check their validity. The result will
be available through the ALAGIN forum.13
Second, we plan to induce paraphrase rules
from paraphrase instances. Though our method
can extract a variety of paraphrase instances on
a large scale, their coverage might be insufficient
for real NLP applications since some paraphrase
phenomena are highly productive. Therefore, we
need paraphrase rules in addition to paraphrase in-
stances. Barzilay and McKeown (2001) induced
simple POS-based paraphrase rules from paraphrase
instances, which can be a good starting point.
Finally, as mentioned in Section 1, the work in
this paper is only the beginning of our research on
paraphrase extraction. We are trying to extract far
more paraphrases from a set of sentences fulfilling
the same pragmatic function (e.g. definition) for the
same topic (e.g. osteoporosis) on the Web. Such
functions other than definition may include the us-
age of the same Linux command, the recipe for the
same cuisine, or the description of related work on
the same research issue.
Acknowledgments
We would like to thank Atsushi Fujita, Francis
Bond, and all of the members of the Information
Analysis Laboratory, Universal Communication Re-
search Institute at NICT.
13http://alagin.jp/
1095
References
Susumu Akamine, Daisuke Kawahara, Yoshikiyo Kato,
Tetsuji Nakagawa, Yutaka I. Leon-Suematsu, Takuya
Kawada, Kentaro Inui, Sadao Kurohashi, and Yutaka
Kidawara. 2010. Organizing information on the web
to support user judgments on information credibil-
ity. In Proceedings of 2010 4th International Uni-
versal Communication Symposium Proceedings (IUCS
2010), pages 122?129.
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entailment
methods. Journal of Artificial Intelligence Research,
38:135?187.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL-2005), pages 597?
604.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL
2003, pages 16?23.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting of the ACL joint
with the 10th Meeting of the European Chapter of the
ACL (ACL/EACL 2001), pages 50?57.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
Ledir: An unsupervised algorithm for learning direc-
tionality of inference rules. In Proceedings of Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP2007), pages 161?170.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of the 2006 Human
Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics (HLT-NAACL 2006), pages 17?24.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of the 20th international conference on Com-
putational Linguistics (COLING 2004), pages 350?
356.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
Atsushi Fujii and Tetsuya Ishikawa. 2002. Extraction
and organization of encyclopedic knowledge informa-
tion using the World Wide Web (written in Japanese).
Institute of Electronics, Information, and Communica-
tion Engineers, J85-D-II(2):300?307.
Maayan Geffet and Ido Dagan. 2005. The distributional
inclusion hypotheses and lexical entailment. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2005), pages
107?114.
Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda,
Stijn De Saeger, Masaki Murata, and Jun?ichi Kazama.
2009. Large-scale verb entailment acquisition from
the web. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2009), pages 1172?1181.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgue`re. 1991. Lexical selection and paraphrase in
a meaning-text generation model. In Ce?cile L. Paris,
William R. Swartout, and William C. Mann, editors,
Natural language generation in artificial intelligence
and computational linguistics, pages 293?312. Kluwer
Academic Press.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings of
the 2006 Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL 2006), pages
455?462.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Exploit-
ing Wikipedia as external knowledge for named entity
recognition. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL 2007), pages 698?707, June.
Jun?ichi Kazama and Kentaro Torisawa. 2008. Inducing
gazetteers for named entity recognition by large-scale
clustering of dependency relations. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (ACL-08: HLT), pages 407?415.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2007), pages
177?180.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159?174.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model for
natural language inference. In Proceedings of the 2008
1096
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2008), pages 802?811.
Nitin Madnani and Bonnie Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Computational Linguistics, 36(3).
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news
on a daily basis with columbia?s newsblaster. In Pro-
ceedings of the 2nd international conference on Hu-
man Language Technology Research, pages 280?285.
Masaki Murata, Toshiyuki Kanemaru, and Hitoshi Isa-
hara. 2004. Automatic paraphrase acquisition based
on matching of definition sentences in plural dictionar-
ies (written in Japanese). Journal of Natural Language
Processing, 11(5):135?149.
Roberto Navigli and Paola Velardi. 2010. Learning
word-class lattices for definition and hypernym extrac-
tion. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL
2010), pages 1318?1327.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2004), pages 142?149.
Deepak Ravichandran and Eduard H. Hovy. 2002.
Learning surface text patterns for a question answer-
ing system. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), pages 41?47.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of the Third International Workshop on
Paraphrasing (IWP-2005), pages 80?87.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proceedings of the 2nd international Con-
ference on Human Language Technology Research
(HLT2002), pages 313?318.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary template. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING2008), pages 849?856.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acquisi-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics (ACL 2007),
pages 456?463.
1097
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 40?49,
Beijing, August 2010
A Look inside the Distributionally Similar Terms
Kow Kuroda
kuroda@nict.go.jp
Jun?ichi Kazama
kazama@nict.go.jp
National Institute of Information and Communications Technology (NICT)
Kentaro Torisawa
torisawa@nict.go.jp
Abstract
We analyzed the details of aWeb-derived
distributional data of Japanese nominal
terms with two aims. One aim is to
examine if distributionally similar terms
can be in fact equated with ?semanti-
cally similar? terms, and if so to what
extent. The other is to investigate into
what kind of semantic relations con-
stitute (strongly) distributionally similar
terms. Our results show that over 85%
of the pairs of the terms derived from
the highly similar terms turned out to
be semantically similar in some way.
The ratio of ?classmate,? synonymous,
hypernym-hyponym, and meronymic re-
lations are about 62%, 17%, 8% and 1%
of the classified data, respectively.
1 Introduction
The explosion of online text allows us to enjoy
a broad variety of large-scale lexical resources
constructed from the texts in the Web in an un-
supervised fashion. This line of approach was
pioneered by researchers such as Hindle (1990),
Grefenstette (1993), Lee (1997) and Lin (1998).
At the heart of the approach is a crucial working
assumption called ?distributional hypothesis,? as
with Harris (1954). We now see an impressive
number of applications in natural language pro-
cessing (NLP) that benefit from lexical resources
directly or indirectly derived from this assump-
tion. It seems that most researchers are reason-
ably satisfied with the results obtained thus far.
Does this mean, however, that the distribu-
tional hypothesis was proved to be valid? Not
necessarily: while we have a great deal of con-
firmative results reported in a variety of research
areas, but we would rather say that the hypothe-
sis has never been fully ?validated? for two rea-
sons. First, it has yet to be tested under the pre-
cise definition of ?semantic similarity.? Second,
it has yet to be tested against results obtained at
a truly large scale.
One of serious problems is that we have seen
no agreement on what ?similar terms? mean and
should mean. This paper intends to cast light
on this unsolved problem through an investiga-
tion into the precise nature of lexical resources
constructed under the distributional hypothesis.
The crucial question to be asked is, Can distri-
butionally similar terms really be equated with
semantically similar terms or not? In our investi-
gation, we sought to recognize what types of se-
mantic relations can be found for pairs of terms
with high distributional similarity, and see where
the equation of distributional similarity with se-
mantic similarity fails. With this concern, this
paper tried to factor out as many components of
semantic similarity as possible. The effort of fac-
torization resulted in the 18 classes of semantic
(un)relatedness to be explained in ?2.3.1. Such
factorization is a necessary step for a full valida-
tion of the hypothesis. To meet the criterion of
testing the hypothesis at a very large scale, we
analyzed 300,000 pairs of distributionally simi-
lar terms. Details of the data we used are given
in ?2.2.
This paper is organized as follows. In ?2, we
present our method and data we used. In ?3, we
present the results and subsequent analysis. In
?4, we address a few remaining problems. In ?5,
we state tentative conclusions.
40
2 Method and Data
2.1 Method
The question we need to address is how many
subtypes of semantic relation we can identify in
the highly similar terms. We examined the ques-
tion in the following procedure:
(1) a. Select a set of ?base? terms B.
b. Use a similarity measure M to con-
struct a list of n terms T = [ti,1, ti,2,. . . ,
ti, j, . . . , ti,n] where ti, j denotes the j-
th most similarity term in T against
bi ? B. P(k) are pairs of bi and ti,k, i.e.,
the k-th most similar term to bi.
c. Human raters classify a portion Q of
the pairs in P(k) with reference to
a classification guideline prepared for
the task.
Note that the selection of base set B can be
independent of the selection of T . Note also that
T is indexed by terms in B. To encode this, we
write: T [bi] = [ti,1, ti,2,. . . , ti, j, . . . , ti,n].
2.2 Data
For T , we used Kazama?s nominal term cluster-
ing (Kazama and Torisawa, 2008; Kazama et al,
2009). In this data, base set B for T is one mil-
lion terms defined by the type counts of depen-
dency relations, which is roughly equated with
the ?frequencies? of the terms. Each base term
in B is associated with up to 500 of the most dis-
tributionally similar terms. This defines T .
For M, we used the Jensen-Shannon diver-
gence (JS-divergence) base on the probability
distributions derived by an EM-based soft clus-
tering (Kazama and Torisawa, 2008). For con-
venience, some relevant details of the data con-
struction are described in Appendix A, but in a
nutshell, we used dependency relations as dis-
tributional information. This makes our method
comparable to that used in Hindle (1990). The
statistics of the distributional data used were as
follows: roughly 920 million types of depen-
dency relations1) were automatically acquired
1)The 920 million types come in two kinds of context
triples: 590 million types of (t, p,v) and 320 million types
from a large-scale Japanese Web-corpus called
the Tsubaki corpus (Shinzato et al, 2008) which
consists of roughly 100 million Japanese pages
with six billion sentences. After excluding hapax
nouns, we had about 33 million types of nouns
(in terms of string) and 27 million types of verbs.
These nouns were ranked by type count of the
two context triples, i.e., (t, p,v) and (n?, p?, t). B
was determined by selecting the top one million
terms with the most variations of context triples.
2.2.1 Sample of T [b]
For illustration, we present examples of the
Web-derived distributional similar terms. (2)
shows the 10 most distributionally similar terms
(i.e., [t1070,1, t1070,2, . . . , t1070,10] in T (b1070))
where b1070 = ????? (piano) is the 1070-th
term in B. Likewise, (3) shows the 10 most dis-
tributionally similar terms [t38555,1, t38555,2, . . . ,
t38555,10] in T (b38555)) where b38555 = ??????
???? (Tchaikovsky) is the 38555-th term in B.
(2) 10 most similar to ?????
1. ?????? (Electone; electronic or-
gan) [-0.322]
2. ????? (violin) [-0.357]
3. ?????? (violin) [-0.358]
4. ??? (cello) [-0.358]
5. ?????? (trumpet) [-0.377]
6. ??? (shamisen) [-0.383]
7. ???? (saxophone) [-0.39]
8. ???? (organ) [-0.392]
9. ?????? (clarinet) [-0.394]
10. ?? (erh hu) (-0.396)
(3) 10 most similar to ??????????
1. ????? (Brahms) [-0.152]
2. ????? (Schumann) [-0.163]
3. ???????? (Mendelssohn) [-
0.166]
4. ????????? (Shostakovich) [-
0.178]
5. ????? (Sibelius) [-0.18]
of (t, p?,n?), where t denotes the target nominal term, p a
postposition, v a verb, and n? a nominal term that follows t
and p?, i.e., ?t-no? analogue to the English ?of t.?
41
6. ???? (Haydn) [-0.181]
7. ???? (Ha?ndel) [-0.181]
8. ???? (Ravel) [-0.182]
9. ?????? (Schubert) [-0.187]
10. ??????? (Beethoven) [-0.19]
For construction of P(k), we had the follow-
ing settings: i) k = 1,2; and ii) for each k, we
selected the 150,000 most frequent terms (out of
one million terms) with some filtering specified
below. Thus, Q was 300,000 pairs whose base
terms are roughly the most frequent 150,000
terms in B with filtering and targets are terms
k = 1 or k = 2.
2.2.2 Filtering of terms in B
For filtering, we excluded the terms of B with
one of the following properties: a) they are in an
invalid form that could have resulted from parse
errors; b) they have regular ending (e.g., -??
, -? [event], -? [time or when], -?? [thing or
person], -? [thing], -? [person]). The reason
for the second is two-fold. First, it was desir-
able to reduce the ratio of the class of ?class-
mates with common morpheme,? which is ex-
plained in ?2.3.2, whose dominance turned out to
be evident in the preliminary analysis. Second,
the semantic property of the terms in this class
is relatively predictable from their morphology.
That notwithstanding, this filtering might have
had an undesirable impact on our results, at least
in terms of representativeness. Despite of this,
we decided to place priority on collecting more
varieties of classes.
The crucial question is, again, whether dis-
tributionally similar terms can really be equated
with semantically similar terms. Put differently,
what kinds of terms can we find in the sets con-
structed using distributionally similarity? We
can confirm the hypothesis if the most of the
term pairs are proved to be semantically simi-
lar for most sets of terms constructed based on
the distributional hypothesis. To do this, how-
ever, we need to clarify what constitutes seman-
tic similarity. We will deal with this prerequisite.
2.3 Classification
2.3.1 Factoring out ?semantic similarity?
Building on lexicographic works like Fell-
baum (1998) and Murphy (2003), we assume
that the following are the four major classes
of semantic relation that contribute to semantic
similarity between two terms:
(4) a. ?synonymic? relation (one can substi-
tute for another on an identity basis).
Examples are (Microsoft, MS).
b. ?hypernym-hyponym? relation be-
tween two terms (one can substitute
for another on un underspecifica-
tion/abstraction basis). Examples are
(guys, players)
c. ?meronymic? (part-whole) relation be-
tween two terms (one term can be a
substitute for another on metonymic
basis). Examples are (bodies, players)
[cf. All the players have strong bodies]
d. ?classmate? relation between two
terms, t1 and t2, if and only if (i) they
are not synonymous and (ii) there is a
concrete enough class such that both t1
and t2 are instances (or subclasses).2)
For example, (China, South Korea)
[cf. (Both) China and South Korea
are countries in East Asia], (Ford, Toy-
ota) [cf. (Both) Ford and Toyota are
top-selling automotive companies] and
(tuna, cod) [cf. (Both) tuna and cod
are types of fish that are eaten in the
Europe] are classmates.
For the substitution, the classmate class behaves
somewhat differently. In this case, one term can-
not substitute for another for a pair of terms. It
is hard to find out the context in which pairs like
(China, South Korea), (Ford, Toyota) and (tuna,
cod) can substitute one another. On the other
hand, substitution is more or less possible in the
other three types. For example, a synonymic pair
of (MS, Microsoft) can substitute for one another
in contexts like Many people regularly complain
2)The proper definition of classmates is extremely hard
to form. The authors are aware of the incompleteness of
their definition, but decided not to be overly meticulous.
42
pair of forms
pair of 
meaningful 
terms
x: pair with a 
meaningless 
form
u: pair of terms 
in no conceivable 
semantic relation
r: pair of terms in 
a conceivable 
semantic relation
s:* synonymous 
pair in the 
broadest sense
a: acronymic 
pair
v: allographic 
pair
n: alias pair
e: erroneous 
pair
f: quasi-
erroneous pair
v*: notational 
variation of the 
same term
m: misuse pair
o: pair in other, 
unindentified 
relation
h: hypernym-
hyponym pair
k**: classmate 
in the broadest 
sense
k*: classmate 
without obvious 
contrastiveness
c*: contrastive 
pairs d: antonymic 
pair
c: contrastive 
pair without 
antonymity
p: meronymic 
pair
t: pair of terms 
with inherent 
temporal order
y: undecidable
k: classmate 
without shared 
morpheme
w: classmate 
with shared 
morpheme
s: synonymous 
pair of different 
terms
Figure 1: Classification tree for semantic relations used
about products { i. MS; ii. Microsoft }. A
hypernym-hyponym pair of (guys, players) can
substitute in contexts like We have dozens of ex-
cellent { i. guys; ii. players } on our team. A
meronymic pair of (bodies, players) can substi-
tute for each other in contexts like They had a few
of excellent { i. bodies; ii. players} last year.
2.3.2 Classification guidelines
The classification guidelines were specified
based on a preliminary analysis of 5,000 ran-
domly selected examples. We asked four annota-
tors to perform the task. The guidelines were fi-
nalized after several revisions. This revision pro-
cess resulted in a hierarchy of binary semantic
relations as illustrated in Figure 1, which sub-
sumes 18 types as specified in (5). The essen-
tial division is made at the fourth level where
we have s* (pairs of synonyms in the broadest
sense) with two subtypes, p (pairs of terms in
the ?part-whole? relation), h (pairs of terms in
the ?hypernym-hyponym? relation), k** (pairs
of terms in the ?classmate? relation), and o (pairs
of terms in any other relation). Note that this
system includes the four major types described
in (4). The following are some example pairs of
Japanese terms with or without English transla-
tions:
(5) s: synonymic pairs (subtype of s*) in
which the pair designates the same en-
tity, property, or relation. Examples
are: (??, ??) [both mean root], (?
?????,????) [(supporting mem-
ber, cooperating member)], (????
?, ?????) [(invoker of the pro-
cess, parent process)], (????????
?, ?????) [(venture business, ven-
ture)], (????, ???????) [(op-
posing hurler, opposing pitcher)], (?
?, ???) [(medical history, anamne-
ses)],
n: alias pairs (subtype of s*) in which
one term of the pair is the ?alias? of
the other term. Examples are (Steve
Jobs, founder of Apple, Inc.), (Barak
Obama, US President), (???,????
???), (???, ????)
43
a: acronymic pair (subtype of s*) in
which one term of the pair is the
acronym of of the other term. Ex-
amples are: (DEC, Digital Equip-
ment), (IBM, International Business
Machine) (Microsoft ?, MS ?), (??
?, ????), (????, ??),
v: allographic pairs (subtype of s*) in
which the pair is the pair of two forms
of the same term. Examples are:
(convention centre, convention cen-
ter), (colour terms, color terms), (??
???, ????), (????, ????),
(??????????, ????????
???), (??, ??), (????, ????
), (???, ??), (????, ?????),
(??, ??), (????, ????)
h: hypernym-hyponym pair in which one
term of the pair designates the ?class?
of the other term. Examples (or-
der is irrelevant) are: (thesaurus, Ro-
get?s), (?????, ?????) [(search
tool, search software)], (????, ??
??) [(unemployment measures, em-
ployment measures)], (??, ????
) [(business conditions, employment
conditions)], (???????, ???)
[(festival, music festival)], (???, ?
????) [(test agent, pregnancy test)],
(??????, ???) [(cymbidium, or-
chid)], (????,?????) [(company
logo, logo)], (????,????) [(mys-
tical experiences, near-death experi-
ences)]
p: meronymic pair in which one term of
the pair designates the ?part? of the
other term. Examples (order is ir-
relevant) are: (????, ??) [(earth,
sea)], (??, ??) [(affirmation, ad-
mission)], (??, ????) [(findings,
research progress)], (????????
?, ?????) [(solar circuit system,
exterior thermal insulation method)],
(?????, ??) [(Provence, South
France)],
k: classmates not obviously contrastive
without common morpheme (subtype
of k*). Examples are: (????, ???
?) [(self-culture, training)], (????
, ??) [(sub-organs, services)], (???
??,??????) [(Dongba alphabets,
hieroglyphs)], (Tom, Jerry)
w: classmates not obviously contrastive
with common morpheme (subtype of
k*). Examples are: (????, ????)
[(gas facilities, electric facilities)], (?
???,???) [(products of other com-
pany, aforementioned products)], (??
?, ???) [(affiliate station, local sta-
tion)], (???,????) [(Niigata City,
Wakayama City)], (?????, ???
??) [(Sinai Peninsula, Malay Penin-
sula)],
c: contrastive pairs without antonymity
(subtype of c*). Examples are: (???
??, ????) [(romanticism, natural-
ism)], (????????, ???????
????) [(mobile user, internet user)],
(???, PS2?), [(bootleg edition, PS2
edition)]
d: antonymic pairs = contrastive pairs
with antonymity (subtype of c*). Ex-
amples are: (??, ??) [(bond-
ing, disintegration)], (???, ???)
[(gravel road, pavement)], (??, ??
) [(west walls, east walls)], (???,
????) [(daughter and son-in-law,
son and daughter-in-law)], (??, ??
) [(tax-exclusive prices, tax-inclusive
prices)], (??????, ????????
) [(front brake, rear brake)], (????
??, ???????) [(tag-team match,
solo match)], (???, ???) [(wip-
ing with dry materials, wiping with
wet materials)], (??????, ??)
[(sleeveless, long-sleeved)]
t: pairs with inherent temporal order
(subtype of c*). Examples are: (??
?, ???) [(harvesting of rice, plant-
ing of rice)], (????, ????) [(day
of departure, day of arrival)], (???
?, ????) [(career decision, career
selection)], (???, ????) [(catnap,
stay up)], (??, ??) [(poaching, con-
44
traband trade)], (??, ??) [(surren-
der, dispatch of troops)], (???, ?
??) [(2nd-year student, 3rd-year stu-
dent)]
e: erroneous pairs are pairs in which
one term of the pair seems to suffer
from character-level input errors, i.e.
?mistypes.? Examples are: (???, ?
??), (???????, ???????),
(???, ???)
f: quasi-erroneous pair is a pair of terms
with status somewhat between v and e.
Examples (order is irrelevant) are: (?
???, ????) [(supoito, supoido)],
(??????, ??????) [(goru-
fubaggu, gorufugakku)], (?????,
?????) [(biggu ban, bikku ban)],
m: misuse pairs in which one term of the
pair seems to suffer from ?mistake? or
?bad memory? of a word (e is caused
by mistypes but m is not). Examples
(order is irrelevant) are: (???, ???
), (?????, ?????), (??, ??),
(???, ???), (??, ??)
o: pairs in other unidentified relation in
which the pair is in some semantic re-
lation other than s*, k**, p, h, and
u. Examples are: (??, ???) [(ul-
terior motives, possessive feeling)], (?
????,?????) [(theoretical back-
ground, basic concepts)], (?????
???, ????) [(Alexandria, Sira-
cusa)],
u: unrelated pairs in which the pair is in
no promptly conceivable semantic re-
lation. Examples are: (???, ????
) [(noncontact, high resolution)], (??
, ????) [(imitation, overinterpreta-
tion)],
x: nonsensical pairs in which either of the
pair is not a proper term of Japanese.
(but it can be a proper name with very
low familiarity). Examples are: (???
?, ???), (????, ??), (??, ??
?), (???, ??), (ma, ?????)
y: unclassifiable under the allowed time
limit.3) Examples are: (???, ???
???), (fj, ???), (??, ??),
Note that some relation types are symmetric
and others are asymmetric: a, n, h, p, and t (and
e, f, and m, too) are asymmetric types. This
means that the order of the pair is relevant, but it
was not taken into account during classification.
Annotators were asked to ignore the direction of
pairs in the classification task. In the finaliza-
tion, we need to reclassify these to get them in
the right order.
2.3.3 Notes on implicational relations
The overall implicational relation in the hier-
archy in Figure 1 is the following:
(6) a. s, k**, p, h, and o are supposed to be
mutually exclusive, but the distinction
is sometimes obscure.4)
b. k** has two subtypes: k* and c*.
c. k and w are two subtypes k*.
d. c, d and t three subtypes of c*.
To resolve the issue of ambiguity, priority was
set among the labels so that e, f < v < a < n <
p < h < s < t < d < c < w < k < m < o < u <
x < y, where the left label is more preferred over
the right. This guarantees preservation of the im-
plicational relationship among labels.
2.3.4 Notes on quality of classification
We would like to add a remark on the quality.
After a quick overview, we reclassified o and w,
because the first run of the final task ultimately
produced a resource of unsatisfactory quality.
Another note on inter-annotator agreement:
originally, the classification task was designed
and run as a part of a large-scale language re-
source development. Due to its overwhelming
size, we tried to make our development as effi-
cient as possible. In the final phase, we asked
3)We did not ask annotators to check for unknown terms.
4)To see this, consider pairs like (large bowel, bowel),
(small bowel, bowel). Are they instances of p or h? The
difficulty in the distinction between h and p becomes harder
in Japanese due to the lack of plurality marking: cases
like (Mars, heavenly body) (a case of h) and (Mars, heav-
enly bodies) (a p case) cannot be explicitly distinguished.
In fact, the Japanese term ?? can mean both ?heavenly
body? (singular) and ?heavenly bodies? (plural).
45
Table 1: Distribution of relation types
rank count ratio (%) cum. (%) class label
1 108,149 36.04 36.04 classmates without common morpheme k
2 67,089 22.35 58.39 classmates with common morpheme w
3 26,113 8.70 67.09 synonymic pairs s
4 24,599 8.20 75.29 hypernym-hyponym pairs h
5 20,766 6.92 82.21 allographic pairs v
6 18,950 6.31 88.52 pairs in other ?unidentified? relation o
7 12,383 4.13 92.65 unrelated pairs u
8 8,092 2.70 95.34 contrastive pairs without antonymity c
9 3,793 1.26 96.61 pairs with inherent temporal order t
10 3,038 1.01 97.62 antonymic pairs d
11 2,995 1.00 98.62 meronymic pairs p
12 1,855 0.62 99.23 acronymic pairs a
13 725 0.24 99.48 alias pairs n
14 715 0.24 99.71 erroneous pairs e
15 397 0.13 99.85 misuse pairs m
16 250 0.08 99.93 nonsensical pairs x
17 180 0.06 99.99 quasi-erroneous pairs f
18 33 0.01 100.00 unclassified y
17 annotators to classify the data with no over-
lap. Ultimately we obtained results that deserve
a detailed report. This history, however, brought
us to an undesirable situation: no inter-annotator
agreement is calculable because there was no
overlap in the task. This is why no inter-rater
agreement data is now available.
3 Results
Table 1 summarizes the distribution of relation
types with their respective ranks and proportions.
The statistics suggests that classes of e, f, m, x,
and y can be ignored without risk.
3.1 Observations
We noticed the following. Firstly, the largest
class is the class of classmates, narrowly defined
or broadly defined. The narrow definition of the
classmates is the conjunction of k and w, which
makes 58.39%. The broader definition of class-
mates, k**, is the union of k, w, c, d and t, which
makes 62.10%. This confirms the distributional
hypothesis.
The second largest class is the narrowly de-
fined synonymous pairs s. This is 8.7% of the
total, but the general class of synonymic pairs,
s* as the union of s, a, n, v, e, f, and m, makes
16.91%. This comes next to h and w. Notice
also that the union of k** and s* makes 79.01%.
The third largest is the class of terms in
hypernym-hyponym relations. This is 8.20% of
the total. We are not sure if this is large or small.
These results look reasonable and can be
seen as validation of the distributional hypothe-
sis. But there is something uncomfortable about
the the fourth and fifth largest classes, pairs in
?other? relation and ?unrelated? pairs, which
make 6.31% and 4.13% of the total, respectively.
Admittedly, 6.31% are 4.13% are not very large
numbers, but it does not guarantee that we can
ignore them safely. We need a closer examina-
tion of these classes and return to this in ?4.
3.2 Note on allography in Japanese
There are some additional notes: the rate of al-
lographic pairs [v] (6.92%) is rather high.5) We
suspect that this ratio is considerably higher than
the similar results that are to be expected in other
5)Admittedly, 6.92% is not a large number in an absolute
value, but it is quite large for the rate of allographic pairs.
46
languages. In fact, the range of notational varia-
tions in Japanese texts is notoriously large. Many
researchers in Japanese NLP became to be aware
of this, by experience, and claim that this is one
of the causes of Japanese NLP being less effi-
cient than NLP in other (typically ?segmented?)
languages. Our result revealed only the allogra-
phy ratio in nominal terms. It is not clear to what
extent this result is applied to the notional varia-
tions on predicates, but it is unlikely that predi-
cates have a lesser degree of notational variation
than nominals. At the least, informal analysis
suggests that the ratio of allography is more fre-
quent and has more severe impacts in predicates
than in nominals. So, it is very unlikely that we
had a unreasonably high rate of allography in our
data.
3.3 Summary of the results
Overall, we can say that the distributional hy-
pothesis was to a great extent positively con-
firmed to a large extent. Classes of classmates
and synonymous pairs are dominant. If the side
effects of filtering described in ?2.2.2 are ig-
nored, nearly 88% (all but o, u, m, x, and y)
of the pairs in the data turned out to be ?se-
mantically similar? in the sense they are clas-
sified into one of the regular semantic relations
defined in (5). While the status of the inclusion
of hypernym-hyponym pairs in classes of seman-
tically similar terms could be controversial, this
result cannot be seen as negative.
One aspect somewhat unclear in the results we
obtained, however, is that highly similar terms
in our data contain such a number of pairs in
unidentifiable relation. We will discuss this in
more detail in the following section.
4 Discussion
4.1 Limits induced by parameters
Our results have certain limits. We specify those
here.
First, our results are based on the case of
k = 1, 2 for P(k). This may be too small and
it is rather likely that we did not acquire results
with enough representativeness. For more com-
plete results, we need to compare the present re-
sults under larger k, say k = 4, 8, 16, . . .. We did
not do this, but we have a comparable result in
one of the preliminary studies. In the prepara-
tion stage, we classified samples of pairs whose
base term is at frequency ranks 13?172, 798?
1,422 and 12,673?15,172 where k = 1, 2, 3, . . . ,
9, 10.6) Table 2 shows the ratios of relation types
for this sample (k = 1, 2, 4, 8, 10).
Table 2: Similarity rank = 1, 2, 4, 8, 10
rank 1 2 4 8 10
v 18.13 10.48 3.92 2.51 1.04
o 17.08 21.24 26.93 28.24 29.56
w 13.65 13.33 14.30 12.19 12.75
s 11.74 9.14 7.05 4.64 4.06
u 11.07 16.48 17.63 20.79 20.87
h 10.50 10.29 11.17 12.96 10.20
k 7.82 8.38 7.84 7.74 8.22
d 2.58 2.00 1.57 1.16 0.85
p 2.00 1.14 1.08 1.35 1.79
c 1.43 1.05 1.27 1.35 1.89
a 1.05 1.33 0.88 0.39 0.57
x 1.05 1.14 1.27 1.64 2.08
t 0.29 0.19 0.20 0.39 0.47
f 0.10 0.10 0.00 0.10 0.09
m 0.00 0.10 0.20 0.00 0.19
#item 1,048 1,050 1,021 1,034 1,059
From Table 2, we notice that: as similarity
rank decreases, (i) the ratios of v, s, a, and d
decrease monotonically, and the ratios of v and s
decrease drastically; (ii) the ratios of o, u, and x
increases monotonically, and the ratio of o and u
increases considerably; and while (iii) the ratios
of h, k, p, w, m, and f seem to be constant. But
it is likely that the ratios of h, k, p, w, m, and f
change at larger k, say 128, 256.
Overall, however, this suggests that the differ-
ence in similarity rank has the greatest impact
on s* (recall that s and v are subtypes of s*),
o, and u, but not so much on others. Two ten-
dencies can be stated: first, terms at lower sim-
ilarity ranks become less synonymous. Second,
6)The frequency/rank in B was measured in terms of the
count of types of dependency relation.
47
the relationships among terms at lower similar-
ity ranks become more obscure. Both are quite
understandable.
There are, however, two caveats concerning
the data in Table 2, however. First, the 15 la-
bels used in this preliminary task are a subset of
the 18 labels used in the final task. Second, the
definitions of some labels are not completely the
same even if the same labels are used (this is why
we have this great of a ratio of o in Table 2. We
must admit, therefore, that no direct comparison
is possible between the data in Tables 1 and 2.
Second, it is not clear if we made the best
choices for clustering algorithm and distribu-
tional data. For the issue of algorithm, there
are too many clustering algorithms and it is hard
to reasonably select candidates for comparison.
We do, however, plan to extend our evaluation
method to other clustering algorithms. Cur-
rently, one of such options is Bayesian cluster-
ing. We are planning to perform some compar-
isons.
For the issue of what kind of distributional in-
formation to use, many kinds of distributional
data other than dependency relation are avail-
able. For example, simple co-occurrences within
a ?window? are a viable option. With a lack
of comparison, however, we cannot tell at the
present what will come about if another kind of
distributional data was used in the same cluster-
ing algorithm.
4.2 Possible overestimation of hypernyms
A closer look suggests that the ratio of
hypernym-hyponym pairs was somewhat overes-
timated. This is due to the algorithm used in our
data construction. It was often the case that head
nouns were extracted as bare nouns from com-
plex, much longer noun phrases, sometimes due
to the extraction algorithms or parse errors. This
resulted in accidental removal of modifiers be-
ing attached to head nouns in their original uses.
We have not yet checked how often this was the
case. We are aware that this could have resulted
in the overestimation of the ratio of hypernymic
relations in our data.
4.3 Remaining issues
As stated, the fourth largest class, roughly 6.31%
of the total, is that of the pairs in the ?other?
unidentified relation [o]. In our setting, ?other?
means that it is in none among the synonymous,
classmate, part-whole or hypernym-hyponym re-
lation. A closer look into some examples of
o suggest that they are pairs of terms with ex-
tremely vague association or contrast.
Admittedly, 6.31% is not a large number, but
its ratio is comparable with that of the allo-
graphic pairs [v], 6.92%. We have no explana-
tion why we have this much of an unindenfiable
kind of semantic relation distinguished from un-
related pairs [u]. All we can say now is that we
need further investigation into it.
u is not as large as o, but it has a status similar
to o. We need to know why this much amount of
this kind of pairs. A possible answer would be
that they are caused by parse errors, directly or
indirectly.
5 Conclusion
We analyzed the details of the Japanese nominal
terms automatically constructed under the ?dis-
tributional hypothesis,? as in Harris (1954). We
had two aims. One aim was to examine to see
if what we acquire under the hypothesis is ex-
actly what we expect, i.e., if distributional sim-
ilarity can be equated with semantic similarity.
The other aim was to see what kind of seman-
tic relations comprise a class of distributionally
similar terms.
For the first aim, we obtained a positive result:
nearly 88% of the pairs in the data turned out to
be semantically similar under the 18 criteria de-
fined in (5), which include hypernym-hyponym,
meronymic, contrastive, and synonymic rela-
tions. Though some term pairs we evaluated
were among none of these relations, the ratio of
o and u in sum is about 14% and within the ac-
ceptable range.
For the second aim, our result revealed that
the ratio of the classmates, synonymous, rela-
tion, hypernym-hyponym, and meronymic rela-
tions are respectively about 62%, 17%, 8% and
1% of the classified data.
48
Overall, these results suggest that automatic
acquisition of terms under the distributional hy-
pothesis give us reasonable results.
A Clustering of one million nominals
This appendix provides some details on how the
clustering of one million nominal terms was per-
formed.
To determine the similarity metric of a pair of
nominal terms (t1, t2), Kazama et al (2009) used
the Jensen-Shannon divergence (JS-divergence)
DJS(p||q) = 12D(p||M) + 12D(q||M), where pand q are probability distributions, and D =
?i p(i)log p(i)q(i) (Kullback-Leibler divergence, or
KL-divergence) of p and q, and M = 12(p+ q).We obtained p and q in the following way.
Instead of using raw distribution, Kazama et
al. (2009) applied smoothing using EM algo-
rithm (Rooth et al, 1999; Torisawa, 2001). In
Torisawa?s model (2001), the probability of the
occurrence of the dependency relation ?v,r,n? is
defined as:
P(?v,r, t?) =def ?
a?A
P(?v,r?|a)P(t|a)P(a),
where a denotes a hidden class of ?v,r? and term
t. In this equation, the probabilities P(?v,r?|a),
P(t|a), and P(a) cannot be calculated directly
because class a is not observed in a given depen-
dency data. The EM-based clustering method
estimates these probabilities using a given cor-
pus. In the E-step, the probability P(a|?v,r?)
is calculated. In the M-step, the probabilities
P(?v,r?|a), P(t|a), and P(a) are updated until
the likelihood is improved using the results of
the E-step. From the results of this EM-based
clustering method, we can obtain the probabili-
ties P(?v,r?|a), P(t|a), and P(a) for each ?v,r?, t,
and a. Then, P(a|t) is calculated by the follow-
ing equation:
P(a|t) = P(t|a)P(a)?a?AP(t|a)P(a) .
The distributional similarity between t1 and t2
was calculated by the JS divergence between
P(a|t1) and P(a|t2).
References
Fellbaum, C., ed. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Grefenstette, G. 1993. Automatic thesaurus gener-
ation from raw text using knowledge-poor tech-
niques. In In Making Sense of Words: The 9th
Annual Conference of the UW Centre for the New
OED and Text Research.
Harris, Z. S. 1954. Distributional structure. Word,
10(2-3):146?162. Reprinted in Fodor, J. A and
Katz, J. J. (eds.), Readings in the Philosophy
of Language, pp. 33?49. Englewood Cliffs, NJ:
Prentice-Hall.
Hindle, D. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL-90,
pp. 268?275, Pittsburgh, PA.
Kazama, J. and K. Torisawa. 2008. Inducing
gazetteers for named entity recognition by large-
scale clustering of dependency relations. In Pro-
ceedings of ACL-2008: HLT, pp. 407?415.
Kazama, J., S. De Saeger, K. Torisawa, and M. Mu-
rata. 2009. Generating a large-scale analogy list
using a probabilistic clustering based on noun-
verb dependency profiles. In Proceedings of the
15th Annual Meeting of the Association for Natu-
ral Language Processing. [in Japanese].
Lee, L. 1997. Similarity-Based Approaches to Natu-
ral Language Processing. Unpublished Ph.D. the-
sis, Harvard University.
Lin, D. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING/ACL-
98, Montreal, Canda, pages 768?774.
Murphy, M. L. 2003. Semantic Relations and the
Lexicon. Cambridge University Press, Cambridge,
UK.
Rooth, M., S. Riezler, D. Presher, G. Carroll, and
F. Beil. 1999. Inducing a semantically annotated
lexicon via em-based clustering. In Proceedings
of the 37th Annual Meeting of the Association for
Computational Linguistics, pp. 104?111.
Shinzato, K., T. Shibata, D. Kawahara, C. Hashimoto,
and S. Kurohashi. 2008. TSUBAKI: An open
search engine infrastructure for developing new
information access. In Proceedings of IJCNLP
2008.
Torisawa, K. 2001. An unsupervised method for
canonicalization of Japanese postpositions. In
Proceedings of the 6th Natural Language Process-
ing Pacific Rim Symposium (NLPRS), pp. 211?
218.
49
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 238?246,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Learning with Lookahead:
Can History-Based Models Rival Globally Optimized Models?
Yoshimasa Tsuruoka?? Yusuke Miyao?? Jun?ichi Kazama?
? Japan Advanced Institute of Science and Technology (JAIST), Japan
? National Institute of Informatics (NII), Japan
? National Institute of Information and Communications Technology (NICT), Japan
tsuruoka@jaist.ac.jp yusuke@nii.ac.jp kazama@nict.go.jp
Abstract
This paper shows that the performance of
history-based models can be significantly im-
proved by performing lookahead in the state
space when making each classification deci-
sion. Instead of simply using the best ac-
tion output by the classifier, we determine
the best action by looking into possible se-
quences of future actions and evaluating the
final states realized by those action sequences.
We present a perceptron-based parameter op-
timization method for this learning frame-
work and show its convergence properties.
The proposed framework is evaluated on part-
of-speech tagging, chunking, named entity
recognition and dependency parsing, using
standard data sets and features. Experimental
results demonstrate that history-based models
with lookahead are as competitive as globally
optimized models including conditional ran-
dom fields (CRFs) and structured perceptrons.
1 Introduction
History-based models have been a popular ap-
proach in a variety of natural language process-
ing (NLP) tasks including part-of-speech (POS) tag-
ging, named entity recognition, and syntactic pars-
ing (Ratnaparkhi, 1996; McCallum et al, 2000; Ya-
mada and Matsumoto, 2003; Nivre et al, 2004).
The idea is to decompose the complex structured
prediction problem into a series of simple classifi-
cation problems and use a machine learning-based
classifier to make each decision using the informa-
tion about the past decisions and partially completed
structures as features.
Although history-based models have many prac-
tical merits, their accuracy is often surpassed by
globally optimized models such as CRFs (Lafferty
et al, 2001) and structured perceptrons (Collins,
2002), mainly due to the label bias problem. To-
day, vanilla history-based models such as maximum
entropy Markov models (MEMMs) are probably not
the first choice for those who are looking for a ma-
chine learning model that can deliver the state-of-
the-art accuracy for their NLP task. Globally opti-
mized models, by contrast, are gaining popularity in
the community despite their relatively high compu-
tational cost.
In this paper, we argue that history-based mod-
els are not something that should be left behind
in research history, by demonstrating that their ac-
curacy can be significantly improved by incorpo-
rating a lookahead mechanism into their decision-
making process. It should be emphasized that we
use the word ?lookahead? differently from some lit-
erature on syntactic parsing in which lookahead sim-
ply means looking at the succeeding words to choose
the right parsing actions. In this paper, we use the
word to refer to the process of choosing the best ac-
tion by considering different sequences of future ac-
tions and evaluating the structures realized by those
sequences. In other words, we introduce a looka-
head mechanism that performs a search in the space
of future actions.
We present a perceptron-based training algorithm
that can work with the lookahead process, together
with a proof of convergence. The algorithm enables
us to tune the weight of the perceptron in such a way
that we can correctly choose the right action for the
238
State Operation Stack Queue
0 I saw a dog with eyebrows
1 shift I saw a dog with eyebrows
2 shift I saw a dog with eyebrows
3 reduceL saw(I) a dog with eyebrows
. . .
4 saw(I) dog(a) with eyebrows
5 shift saw(I) dog(a) with eyebrows
6 shift saw(I) dog(a) with eyebrows
7 reduceR saw(I) dog(a) with(eyebrows)
8 reduceR saw(I) dog(a, with(eyebrows))
5? reduceR saw(I, dog(a)) with eyebrows
6? shift saw(I, dog(a)) with eyebrows
7? shift saw(I, dog(a)) with eyebrows
8? reduce R saw(I, dog(a)) with(eyebrows)
9? reduce R saw(I, dog(a), with(eyebrows))
Figure 1: Shift-reduce dependency parsing
current state at each decision point, given the infor-
mation obtained from a search.
To answer the question of whether the history-
based models enhanced with lookahead can actually
compete with globally optimized models, we eval-
uate the proposed framework with a range of stan-
dard NLP tasks, namely, POS tagging, text chunking
(a.k.a. shallow parsing), named entity recognition,
and dependency parsing.
This paper is organized as follows. Section 2
presents the idea of lookahead with a motivating
example from dependency parsing. Section 3 de-
scribes our search algorithm for lookahead and a
perceptron-based training algorithm. Experimen-
tal results on POS tagging, chunking, named entity
recognition, and dependency parsing are presented
in Section 4. We discuss relationships between our
approach and some related work in Section 5. Sec-
tion 6 offers concluding remarks with some potential
research directions.
2 Motivation
This section describes an example of dependency
parsing that motivates the introduction of lookahead
in history-based models.
A well-known history-based approach to depen-
dency parsing is shift-reduce parsing. This al-
gorithm maintains two data structures, stack and
queue: A stack stores intermediate parsing results,
and a queue stores words to read. Two operations
(actions), shift and reduce, on these data structures
construct dependency relations one by one.
For example, assume that we are given the follow-
ing sentence.
I saw a dog with eyebrows.
In the beginning, we have an empty stack, and a
queue filled with a list of input words (State 0 in Fig-
ure 1). The shift operation moves the left-most ele-
ment of the queue to the stack. In this example, State
1 is obtained by applying shift to State 0. After the
two shift operations, we reach State 2, in which the
stack has two elements. When we have two or more
elements in the stack, we can apply the other opera-
tion, reduce, which merges the two stack elements
by creating a dependency relation between them.
When we apply reduceL, which means to have the
left element as a dependent of the right element, we
reach State 3: The word ?I? has disappeared from
the stack and instead it is attached to its head word
?saw?.1 In this way, the shift-reduce parsing con-
structs a dependency tree by reading words from the
queue and constructing dependency relations on the
stack.
1In Figure 1, H(D1, D2, . . .) indicates that D1, D2, . . . are
the dependents of the head H .
239
Let?s say we have now arrived at State 4 after sev-
eral operations. At this state, we cannot simply de-
termine whether we should shift or reduce. In such
cases, conventional methods rely on a multi-class
classifier to determine the next operation. That is,
a classifier is used to select the most plausible oper-
ation, by referring to the features about the current
state, such as surface forms and POSs of words in
the stack and the queue.
In the lookahead strategy, we make this decision
by referring to future states. For example, if we ap-
ply shift to State 4, we will reach State 8 in the end,
which indicates that ?with? attaches to ?dog?. The
other way, i.e., applying reduceR to State 4, eventu-
ally arrives at State 9?, indicating ?with? attaches to
?saw?. These future states indicate that we were im-
plicitly resolving PP-attachment ambiguity at State
4. While conventional methods attempt to resolve
such ambiguity using surrounding features at State
4, the lookahead approach resolves the same ambi-
guity by referring to the future states, for example,
State 8 and 9?. Because future states can provide ad-
ditional and valuable information for ambiguity res-
olution, improved accuracy is expected.
It should be noted that Figure 1 only shows one
sequence of operations for each choice of operation
at State 4. In general, however, the number of poten-
tial sequences grows exponentially with the looka-
head depth, so the lookahead approach requires us to
pay the price as the increase of computational cost.
The primary goal of this paper is to demonstrate that
the cost is actually worth it.
3 Learning with Lookahead
This section presents our framework for incorporat-
ing lookahead in history-based models. In this pa-
per, we focus on deterministic history-based models
although our method could be generalized to non-
deterministic cases.
We use the word ?state? to refer to a partially
completed analysis as well as the collection of his-
torical information available at each decision point
in deterministic history-based analysis. State transi-
tions are made by ?actions? that are defined at each
state. In the example of dependency parsing pre-
sented in Section 2, a state contains all the infor-
mation about past operations, stacks, and queues as
1: Input
2: d: remaining depth of search
3: S0: current state
4: Output
5: S: state of highest score
6: v: highest score
7:
8: function SEARCH(d, S0)
9: if d = 0 then
10: return (S0,w ? ?(S0))
11: (S, v)? (null,??)
12: for each a ? POSSIBLEACTIONS(S0)
13: S1 ? UPDATESTATE(S0, a)
14: (S?, v?)? SEARCH(d? 1, S1)
15: if v? > v then
16: (S, v)? (S?, v?)
17: return (S, v)
Figure 2: Search algorithm.
well as the observation (i.e. the words in the sen-
tence). The possible actions are shift, reduceR, and
reduceL. In the case of POS tagging, for example, a
state is the words and the POS tags assigned to the
words on the left side of the current target word (if
the tagging is conducted in the left-to-right manner),
and the possible actions are simply defined by the
POS tags in the annotation tag set.
3.1 Search
With lookahead, we choose the best action at each
decision point by considering possible sequences of
future actions and the states realized by those se-
quences. In other words, we need to perform a
search for each possible action.
Figure 2 describes our search algorithm in pseudo
code. The algorithm performs a depth-first search to
find the state of the highest score among the states in
its search space, which is determined by the search
depth d. This search process is implemented with
a recursive function, which receives the remaining
search depth and the current state as its input and
returns the state of the highest score together with
its score.
We assume a linear scoring model, i.e., the score
of each state S can be computed by taking the dot
product of the current weight vector w and ?(S),
the feature vector representation of the state. The
240
1: Input
2: C: perceptron margin
3: D: depth of lookahead search
4: S0: current state
5: ac: correct action
6:
7: procedure UPDATEWEIGHT(C,D, S0, ac)
8: (a?, S?, v)? (null, null,??)
9: for each a ? POSSIBLEACTIONS(S0)
10: S1 ? UPDATESTATE(S0, a)
11: (S?, v?)?SEARCH(D,S1)
12: if a = ac then
13: v? ? v? ? C
14: S?c ? S?
15: if v? > v then
16: (a?, S?, v)? (a, S?, v?)
17: if a? 6= ac then
18: w ? w + ?(S?c )? ?(S?)
Figure 3: Perceptron weight update
scores are computed at each leaf node of the search
tree and backed up to the root.2
Clearly, the time complexity of determinis-
tic tagging/parsing with this search algorithm is
O(nmD+1), where n is the number of actions
needed to process the sentence, m is the (average)
number of possible actions at each state, and D is
the search depth. It should be noted that the time
complexity of k-th order CRFs is O(nmk+1), so
a history-based model with k-depth lookahead is
comparable to k-th order CRFs in terms of train-
ing/testing time.
Unlike CRFs, our framework does not require the
locality of features since it is history-based, i.e., the
decisions can be conditioned on arbitrary features.
One interpretation of our learning framework is that
it trades off the global optimality of the learned pa-
rameters against the flexibility of features.
3.2 Training a margin perceptron
We adapt a learning algorithm for margin percep-
trons (Krauth and Mezard, 1987) to our purpose of
2In actual implementation, it is not efficient to compute the
score of a state from scratch at each leaf node. For most of
the standard features used in tagging and parsing, it is usually
straight-forward to compute the scores incrementally every time
the state is updated with an action.
optimizing the weight parameters for the lookahead
search. Like other large margin approaches such
as support vector machines, margin perceptrons are
known to produce accurate models compared to per-
ceptrons without a margin (Li et al, 2002).
Figure 3 shows our learning algorithm in pseudo
code. The algorithm is very similar to the standard
training algorithm for margin perceptrons, i.e., we
update the weight parameters with the difference of
two feature vectors (one corresponding to the cor-
rect action, and the other the action of the highest
score) when the perceptron makes a mistake. The
feature vector for the second best action is also used
when the margin is not large enough. Notice that the
feature vector for the second best action is automat-
ically selected by using a simple trick of subtracting
the margin parameter from the score for the correct
action (Line 13 in Figure 3).
The only difference between our algorithm and
the standard algorithm for margin perceptrons is that
we use the states and their scores obtained from
lookahead searches (Line 11 in Figure 3), which are
backed up from the leaves of the search trees. In Ap-
pendix A, we provide a proof of the convergence of
our training algorithm and show that the margin will
approach at least half the true margin (assuming that
the training data are linearly separable).
As in many studies using perceptrons, we average
the weight vector over the whole training iterations
at the end of the training (Collins, 2002).
4 Experiments
This section presents four sets of experimental re-
sults to show how the lookahead process improves
the accuracy of history-based models in common
NLP tasks.
4.1 Sequence prediction tasks
First, we evaluate our framework with three se-
quence prediction tasks: POS tagging, chunking,
and named entity recognition. We compare our
method with the CRF model, which is one of the de
facto standard machine learning models for such se-
quence prediction tasks. We trained L1-regularized
first-order CRF models using the efficient stochastic
gradient descent (SGD)-based training method pre-
sented in Tsuruoka et al (2009). Since our main in-
241
terest is not in achieving the state-of-the-art results
for those tasks, we did not conduct feature engineer-
ing to come up with elaborate features?we sim-
ply adopted the feature sets described in their paper
(with an exception being tag trigram features tested
in the POS tagging experiments). The experiments
for these sequence prediction tasks were carried out
using one core of a 3.33GHz Intel Xeon W5590 pro-
cessor.
The first set of experiments is about POS tagging.
The training and test data were created from theWall
Street Journal corpus of the Penn Treebank (Marcus
et al, 1994). Sections 0-18 were used as the training
data. Sections 19-21 were used for tuning the meta
parameters for learning (the number of iterations and
the margin C). Sections 22-24 were used for the
final accuracy reports.
The experimental results are shown in Table 1.
Note that the models in the top four rows use exactly
the same feature set. It is clearly seen that the looka-
head improves tagging accuracy, and our history-
based models with lookahead is as accurate as the
CRF model. We also created another set of models
by simply adding tag trigram features, which can-
not be employed by first-order CRF models. These
features have slightly improved the tagging accu-
racy, and the final accuracy achieved by a search
depth of 3 was comparable to some of the best re-
sults achieved by pure supervised learning in this
task (Shen et al, 2007; Lavergne et al, 2010).
The second set of experiments is about chunking.
We used the data set for the CoNLL 2000 shared
task, which contains 8,936 sentences where each to-
ken is annotated with the ?IOB? tags representing
text chunks. The experimental results are shown
in Table 2. Again, our history-based models with
lookahead were slightly more accurate than the CRF
model using exactly the same set of features. The
accuracy achieved by the lookahead model with a
search depth of 2 was comparable to the accuracy
achieved by a computationally heavy combination
of max-margin classifiers (Kudo and Matsumoto,
2001). We also tested the effectiveness of additional
features of tag trigrams using the development data,
but there was no improvement in the accuracy.
The third set of experiments is about named en-
tity recognition. We used the data provided for
the BioNLP/NLPBA 2004 shared task (Kim et al,
2004), which contains 18,546 sentences where each
token is annotated with the ?IOB? tags representing
biomedical named entities. We performed the tag-
ging in the right-to-left fashion because it is known
that backward tagging is more accurate than forward
tagging on this data set (Yoshida and Tsujii, 2007).
Table 3 shows the experimental results, together
with some previous performance reports achieved
by pure machine leaning methods (i.e. without rule-
based post processing or external resources such as
gazetteers). Our history-based model with no looka-
head was considerably worse than the CRF model
using the same set of features, but it was signifi-
cantly improved by the introduction of lookahead
and resulted in accuracy figures better than that of
the CRF model.
4.2 Dependency parsing
We also evaluate our method in dependency parsing.
We follow the most standard experimental setting
for English dependency parsing: The Wall Street
Journal portion of Penn Treebank is converted to de-
pendency trees by using the head rules of Yamada
and Matsumoto (2003).3 The data is split into train-
ing (section 02-21), development (section 22), and
test (section 23) sets. The parsing accuracy was eval-
uated with auto-POS data, i.e., we used our looka-
head POS tagger (depth = 2) presented in the previ-
ous subsection to assign the POS tags for the devel-
opment and test data. Unlabeled attachment scores
for all words excluding punctuations are reported.
The development set is used for tuning the meta pa-
rameters, while the test set is used for evaluating the
final accuracy.
The parsing algorithm is the ?arc-standard?
method (Nivre, 2004), which is briefly described in
Section 2. With this algorithm, state S corresponds
to a parser configuration, i.e., the stack and the
queue, and action a corresponds to shift, reduceL,
and reduceR. In this experiment, we use the same
set of feature templates as Huang and Sagae (2010).
Table 4 shows training time, test time, and parsing
accuracy. In this table, ?No lookahead (depth = 0)?
corresponds to a conventional shift-reduce parsing
method without any lookahead search. The results
3Penn2Malt is applied for this conversion, while depen-
dency labels are removed.
242
Training Time (sec) Test Time (sec) Accuracy
CRF (L1 regularization & SGD training) 847 3 97.11 %
No lookahead (depth = 0) 85 5 97.00 %
Lookahead (depth = 1) 294 9 97.19 %
Lookahead (depth = 2) 8,688 173 97.19 %
No lookahead (depth = 0) + tag trigram features 88 5 97.11 %
Lookahead (depth = 1) + tag trigram features 313 10 97.22 %
Lookahead (depth = 2) + tag trigram features 10,034 209 97.28 %
Structured perceptron (Collins, 2002) n/a n/a 97.11 %
Guided learning (Shen et al, 2007) n/a n/a 97.33 %
CRF with 4 billion features (Lavergne et al, 2010) n/a n/a 97.22 %
Table 1: Performance of English POS tagging (training times and accuracy scores on test data)
Training time (sec) Test time (sec) F-measure
CRF (L1 regularization & SGD training) 74 1 93.66
No lookahead (depth = 0) 22 1 93.53
Lookahead (depth = 1) 73 1 93.77
Lookahead (depth = 2) 1,113 9 93.81
Voting of 8 SVMs (Kudo and Matsumoto, 2001) n/a n/a 93.91
Table 2: Performance of text chunking (training times and accuracy scores on test data).
clearly demonstrate that the lookahead search boosts
parsing accuracy. As expected, training and test
speed decreases, almost by a factor of three, which
is the branching factor of the dependency parser.
The table also lists accuracy figures reported in
the literature on shift-reduce dependency parsing.
Most of the latest studies on shift-reduce depen-
dency parsing employ dynamic programing or beam
search, which implies that deterministic methods
were not as competitive as those methods. It should
also be noted that all of the listed studies learn struc-
tured perceptrons (Collins and Roark, 2004), while
our parser learns locally optimized perceptrons. In
this table, our parser without lookahead search (i.e.
depth = 0) resulted in significantly lower accuracy
than the previous studies. In fact, it is worse than the
deterministic parser of Huang et al (2009), which
uses (almost) the same set of features. This is pre-
sumably due to the difference between locally opti-
mized perceptrons and globally optimized structured
perceptrons. However, our parser with lookahead
search is significantly better than their determinis-
tic parser, and its accuracy is close to the levels of
the parsers with beam search.
5 Discussion
The reason why we introduced a lookahead mech-
anism into history-based models is that we wanted
the model to be able to avoid making such mistakes
that can be detected only in later stages. Probabilis-
tic history-based models such as MEMMs should be
able to avoid (at least some of) such mistakes by per-
forming a Viterbi search to find the highest proba-
bility path of the actions. However, as pointed out
by Lafferty et al (2001), the per-state normaliza-
tion of probabilities makes it difficult to give enough
penalty to such incorrect sequences of actions, and
that is primarily why MEMMs are outperformed by
CRFs.
Perhaps the most relevant to our work in terms
of learning is the general framework for search and
learning problems in history-based models proposed
by Daume? III and Marcu (2005). This framework,
called LaSO (Learning as Search Optimization), can
include many variations of search strategies such as
beam search and A* search as a special case. In-
deed, our lookahead framework could be regarded
as a special case in which each search node con-
243
Training time (sec) Test time (sec) F-measure
CRF (L1 regularization & SGD training) 235 4 71.63
No lookahead (depth = 0) 66 4 70.17
Lookahead (depth = 1) 91 4 72.28
Lookahead (depth = 2) 302 7 72.00
Lookahead (depth = 3) 2,419 33 72.21
Semi-Markov CRF (Okanohara et al, 2006) n/a n/a 71.48
Reranking (Yoshida and Tsujii, 2007) n/a n/a 72.65
Table 3: Performance of biomedical named entity recognition (training times and accuracy scores on test data).
Training time (sec) Test time (sec) Accuracy
No lookahead (depth = 0) 1,937 4 89.73
Lookahead (depth = 1) 4,907 13 91.00
Lookahead (depth = 2) 12,800 31 91.10
Lookahead (depth = 3) 31,684 79 91.24
Beam search (k = 64) (Zhang and Clark, 2008) n/a n/a 91.4
Deterministic (Huang et al, 2009) n/a n/a 90.2
Beam search (k = 16) (Huang et al, 2009) n/a n/a 91.3
Dynamic programming (Huang and Sagae, 2010) n/a n/a 92.1
Table 4: Performance of English dependency parsing (training times and accuracy scores on test data).
sists of the next and lookahead actions4, although
the weight updating procedure differs in several mi-
nor points. Daume? III and Marcu (2005) did not try
a lookahead search strategy, and to the best of our
knowledge, this paper is the first that demonstrates
that lookahead actually works well for various NLP
tasks.
Performing lookahead is a very common tech-
nique for a variety of decision-making problems
in the field of artificial intelligence. In computer
chess, for example, programs usually need to per-
form a very deep search in the game tree to find a
good move. Our decision-making problem is sim-
ilar to that of computer Chess in many ways, al-
though chess programs perform min-max searches
rather than the ?max? searches performed in our al-
gorithm. Automatic learning of evaluation functions
for chess programs can be seen as the training of
a machine learning model. In particular, our learn-
ing algorithm is similar to the supervised approach
4In addition, the size of the search queue is always truncated
to one for the deterministic decisions presented in this paper.
Note, however, that our lookahead framework can also be com-
bined with other search strategies such as beam search. In that
case, the search queue is not necessarily truncated.
(Tesauro, 2001; Hoki, 2006) in that the parameters
are optimized based on the differences of the feature
vectors realized by the correct and incorrect actions.
In history-based models, the order of actions is of-
ten very important. For example, backward tagging
is considerably more accurate than forward tagging
in biomedical named entity recognition. Our looka-
head method is orthogonal to more elaborate tech-
niques for determining the order of actions such as
easy-first tagging/parsing strategies (Tsuruoka and
Tsujii, 2005; Elhadad, 2010). We expect that incor-
porating such elaborate techniques in our framework
will lead to improved accuracy, but we leave it for
future work.
6 Conclusion
We have presented a simple and general framework
for incorporating a lookahead process in history-
based models and a perceptron-based training algo-
rithm for the framework. We have conducted ex-
periments using standard data sets for POS tagging,
chunking, named entity recognition and dependency
parsing, and obtained very promising results?the
accuracy achieved by the history-based models en-
244
hanced with lookahead was as competitive as glob-
ally optimized models including CRFs.
In most of the experimental results, steady im-
provement in accuracy has been observed as the
depth of the search is increased. Although it is
not very practical to perform deeper searches with
our current implementation?we naively explored
all possible sequences of actions, future work should
encompass extending the depths of search space
by introducing elaborate pruning/search extension
techniques.
In this work, we did not conduct extensive feature
engineering for improving the accuracy of individ-
ual tasks because our primary goal with this paper is
to present the learning framework itself. However,
one of the major merits of using history-based mod-
els is that we are allowed to define arbitrary features
on the partially completed structure. Another inter-
esting direction of future work is to see how much
we could improve the accuracy by performing ex-
tensive feature engineering in this particular learning
framework.
Appendix A: Convergence of the Learning
Procedure
Let {xi, aic}Ki=1 be the training examples where aic
is the correct first action for decision point xi, and
let Si be the set of all the states at the leaves of
the search trees for xi generated by the lookahead
searches and Sic be the set of all the states at the
leaves of the search tree for the correct action aic.
We also define Si = Si \ Sic. We write the weight
vector before the k-th update as wk. We define
S?c = argmax
S?Sic
w??(S) and S? = argmax
S?Si
w??(S)5.
Then the update rule can be interpreted as wk+1 =
wk +(?(S?c )??(S?)). Note that this update is per-
formed only when ?(Sc) ?wk?C < ?(S?) ?wk for
all Sc ? Sc since otherwise S? in the learning algo-
rithm cannot be a state with an incorrect first action.
In other words, ?(Sc) ?w ? ?(S?) ?w ? C for all
Sc ? Sc after convergence.
Given these definitions, we prove the convergence
for the separable case. That is, we assume the exis-
tence of a weight vector u (with ||u|| = 1), ? (> 0),
5S?c and S? depend on the weight vector at each point, but
we omit it from the notation for brevity.
and R (> 0) that satisfy:
?i,?Sc ? Sic,?S ? Si ?(Sc) ? u? ?(S) ? u ? ?,
?i,?Sc ? Sic,?S ? Si ||?(Sc)? ?(S)|| ? R.
The proof is basically an adaptation of the proofs
in Collins (2002) and Li et al (2002). First, we ob-
tain the following relation:
wk+1 ? u = wk ? u+ (?(S?c ) ? u? ?(S?) ? u)
= wk ? u+ ? ? w1 ? u+ k? = k?.
Therefore, ||wk+1 ? u||2 = ||wk+1||2 ? (k?)2 ?
(1). We assumed w1 = 0 but this is not an essential
assumption.
Next, we also obtain:
||wk+1||2 ? ||wk||2 + 2(?(S?c )? ?(S?)) ?wk
+||?(S?c )? ?(S?)||2
? ||wk||2 + 2C +R2
? ||w1||2 + k(R2 + 2C) = k(R2 + 2C)? (2)
Combining (1) and (2), we obtain k ? (R2 +
2C)/?2. That is, the number of updates is bounded
from above, meaning that the learning procedure
converges after a finite number of updates. Substi-
tuting this into (2) gives ||wk+1|| ? (R2 + 2C)/?
? (3).
Finally, we analyze the margin achieved by the
learning procedure after convergence. The margin,
?(w), is defined as follows in this case.
?(w) = min
xi
min
Sc?Sic,S?Si
?(Sc) ?w ? ?(S) ?w
||w||
= min
xi
min
Sc?Sic
?(Sc) ?w ? ?(S?) ?w
||w||
After convergence (i.e., w = wk+1), ?(Sc) ? w ?
?(S?)?w ? C for all Sc ? Sc as we noted. Together
with (3), we obtain the following bound:
?(w) ? min
xi
?C
2C +R2
= ?C
2C +R2
=
(
?
2
)(
1? R
2
2C +R2
)
As can be seen, the margin approaches at least half
the true margin, ?/2 as C ? ? (at the cost of infi-
nite number of updates).
245
References
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL, pages 111?118.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8.
Hal Daume? III and Daniel Marcu. 2005. Learning as
search optimization: Approximate large margin meth-
ods for structured prediction. In Proceedings of ICML,
pages 169?176.
Yoav Goldbergand Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of NAACL-HLT, pages
742?750.
Kunihito Hoki. 2006. Optimal control of minimax
search results to learn positional evaluation. In Pro-
ceedings of the 11th Game Programming Workshop
(GPW), pages 78?83 (in Japanese).
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of ACL, pages 1077?1086.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP, pages 1222?1231.
J.-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-
lier. 2004. Introduction to the bio-entity recogni-
tion task at JNLPBA. In Proceedings of the Interna-
tional Joint Workshop on Natural Language Process-
ing in Biomedicine and its Applications (JNLPBA),
pages 70?75.
W Krauth and M Mezard. 1987. Learning algorithms
with optimal stability in neural networks. Journal of
Phisics A, 20(11):L745?L752.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of NAACL.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML, pages 282?289.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings of ACL, pages 504?513.
Yaoyong Li, Hugo Zaragoza, Ralf Herbrich, John Shawe-
Taylor, and Jaz S. Kandola. 2002. The perceptron
algorithm with uneven margins. In Proceedings of
ICML, pages 379?386.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy markov models for
information extraction and segmentation. In Proceed-
ings of ICML, pages 591?598.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceedings
of CoNLL, pages 49?56.
Joakim Nivre. 2004. Incrementality in deterministic de-
pendency parsing. In ACL 2004 Workshop on Incre-
mental Parsing: Bringing Engineering and Cognition
Together, pages 50?57.
Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun?ichi Tsujii. 2006. Improving the scal-
ability of semi-markov conditional random fields for
named entity recognition. In Proceedings of COL-
ING/ACL, pages 465?472.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996, pages 133?142.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In Proceedings of ACL, pages 760?767.
Gerald Tesauro, 2001. Comparison training of chess
evaluation functions, pages 117?130. Nova Science
Publishers, Inc.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidirec-
tional inference with the easiest-first strategy for tag-
ging sequence data. In Proceedings of HLT/EMNLP
2005, pages 467?474.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumulative
penalty. In Proceedings of ACL-IJCNLP, pages 477?
485.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT, pages 195?206.
Kazuhiro Yoshida and Jun?ichi Tsujii. 2007. Reranking
for biomedical named-entity recognition. In Proceed-
ings of ACL Workshop on BioNLP, pages 209?216.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graphbased
and transition-based dependency parsing using beam-
search. In Proceedings of EMNLP, pages 562?571.
246
