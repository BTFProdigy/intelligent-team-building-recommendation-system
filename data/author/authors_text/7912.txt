Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 134?137,
Sydney, July 2006. c?2006 Association for Computational Linguistics
On Closed Task of Chinese Word Segmentation: An Improved CRF 
Model Coupled with Character Clustering and  
Automatically Generated Template Matching 
Richard Tzong-Han Tsai, Hsieh-Chuan Hung, Cheng-Lung Sung,  
Hong-Jie Dai, and Wen-Lian Hsu 
Intelligent Agent Systems Lab 
Institute of Information Science, Academia Sinica 
No. 128, Sec. 2, Academia Rd., 115 Nankang, Taipei, Taiwan, R.O.C. 
{thtsai,yabt,clsung,hongjie,hsu}@iis.sinica.edu.tw 
 
  
 
Abstract 
This paper addresses two major prob-
lems in closed task of Chinese word 
segmentation (CWS): tagging sentences 
interspersed with non-Chinese words, 
and long named entity (NE) identifica-
tion. To resolve the former, we apply K-
means clustering to identify non-Chinese 
characters, and then adopt a two-tagger 
architecture: one for Chinese text and the 
other for non-Chinese text. For the latter 
problem, we apply postprocessing to our 
CWS output using automatically gener-
ated templates. The experiment results 
show that, when non-Chinese characters 
are sparse in the training corpus, our 
two-tagger method significantly im-
proves the segmentation of sentences 
containing non-Chinese words. Identifi-
cation of long NEs and long words is 
also enhanced by template-based post-
processing. In the closed task of 
SIGHAN 2006 CWS, our system 
achieved F-scores of 0.957, 0.972, and 
0.955 on the CKIP, CTU, and MSR cor-
pora respectively.  
1 Introduction 
Unlike Western languages, Chinese does not 
have explicit word delimiters. Therefore, word 
segmentation (CWS) is essential for Chinese 
text processing or indexing. There are two main 
problems in the closed CWS task. The first is to 
identify and segment non-Chinese word se-
quences in Chinese documents, especially in a 
closed task (described later). A good CWS sys-
tem should be able to handle Chinese texts pep-
pered with non-Chinese words or phrases. Since 
non-Chinese language morphologies are quite 
different from that of Chinese, our approach 
must depend on how many non-Chinese words 
appear, whether they are connected with each 
other, and whether they are interleaved with 
Chinese words. If we can distinguish non-
Chinese characters automatically and apply dif-
ferent strategies, the segmentation performance 
can be improved. The second problem in closed 
CWS is to correctly identify longer NEs. Most 
ML-based CWS systems use a five-character 
context window to determine the current charac-
ter?s tag. In the majority of cases, given the con-
straints of computational resources, this com-
promise is acceptable. However, limited by the 
window size, these systems often handle long 
words poorly. 
In this paper, our goal is to construct a general 
CWS system that can deal with the above prob-
lems. We adopt CRF as our ML model. 
2 Chinese Word Segmentation System 
2.1 Conditional Random Fields 
Conditional random fields (CRFs) are undirected 
graphical models trained to maximize a condi-
tional probability (Lafferty et al, 2001). A lin-
ear-chain CRF with parameters ?={?1, ?2, ?} 
defines a conditional probability for a state se-
quence y = y1 ?yT , given that an input sequence 
x = x1 ?xT  is 
???
????
?= ??
=
??
T
t k
ttkk tyyfZ
P
1
1 ),,,(exp
1
)|( xxy
x
?  ,(1)                          
where Zx is the normalization factor that makes 
the probability of all state sequences sum to one; 
fk(yt-1, yt, x, t) is often a binary-valued feature 
function and ?k is its weight. The feature 
134
functions can measure any aspect of a state 
transition, yt-1?yt, and the entire observation 
sequence, x, centered at the current time step, t. 
For example, one feature function might have 
the value 1 when yt-1 is the state B, yt is the state 
I, and tx  is the character ???. 
2.2 Character Clustering 
In many cases, Chinese sentences may be inter-
spersed with non-Chinese words. In a closed 
task, there is no way of knowing how many lan-
guages there are in a given text. Our solution is 
to apply a clustering algorithm to find homoge-
neous characters belonging to the same character 
clusters. One general rule we adopted is that a 
language?s characters tend to appear together in 
tokens. In addition, character clusters exhibit 
certain distinct properties. The first property is 
that the order of characters in some pairs can be 
interchanged. This is referred to as exchange-
ability. The second property is that some charac-
ters, such as lowercase characters, can appear in 
any position of a word; while others, such as 
uppercase characters, cannot. This is referred to 
as location independence. According to the gen-
eral rule, we can calculate the pairing frequency 
of characters in tokens by checking all tokens in 
the corpus. Assuming the alphabet is ?, we first 
need to represent each character as a |?|-
dimensional vector. For each character ci, we use 
vj to represent its j-dimension value, which is 
calculated as follows: 
r
jiijj ffv )],)[min(1( ?? ?+= ?             (2), 
where fij denotes the frequency with which ci and 
cj appear in the same word when ci?s position 
precedes that of cj. We take the minimum value 
of fij and fji because even when ci and cj have a 
high co-occurrence frequency, if either fij or fji is 
low, then one order does not occur often, so vj?s 
value will be low. We use two parameters to 
normalize vj within the range 0 to 1; ? is used to 
enlarge the gap between non-zero and zero fre-
quencies, and ? is used to weaken the influence 
of very high frequencies. 
Next, we apply the K-means algorithm to 
generate candidate cluster sets composed of K 
clusters (Hartigan et al, 1979). Different K?s, 
??s, and ??s are used to generate possible charac-
ter cluster sets. Our K-means algorithm uses the 
cosine distance. 
After obtaining the K clusters, we need to se-
lect the N1 best character clusters among them. 
Assuming the angle between the cluster centroid 
vector and (1, 1, ... , 1) is ?, the cluster with the 
largest cosine ? will be removed. This is because 
characters whose co-occurrence frequencies are 
nearly all zero will be transformed into vectors 
very close to (?, ?, ... , ?); thus, their centroids 
will also be very close to (?, ?, ... , ?), leading to 
unreasonable clustering results. 
After removing these two types of clusters, 
for each character c in a cluster M, we calculate 
the inverse relative distance (IRDist) of c using 
(3): 
??
?
?
?
??
?
?
?
=
?
),cos(
),(cos
log)IRDist(
mc
mc
c
i
i   ,            (3) 
where mi stands for the centroid of cluster Mi, 
and m stands for the centroid of M.  
We then calculate the average inverse dis-
tance for each cluster M. The N1 best clusters are 
selected from the original K clusters.   
The above K-means clustering and character 
cluster selection steps are executed iteratively 
for each cluster set generated from K-means 
clustering with different K?s, ??s, and ??s.  
After selecting the N1 best clusters for each 
cluster set, we pool and rank them according to 
their inner ratios. Each cluster?s inner ratio is 
calculated by the following formula: 
?
?
?
?
= ?
ji
ji
cc
ji
Mcc
ji
cc
cc
M
,
,
),occurence(co
 ),occurence(co
)inner(
 ,   (4) 
where co-occurrence(ci, cj) denotes the fre-
quency with which characters  ci and cj co-occur 
in the same word. 
To ensure that we select a balanced mix of 
clusters, for each character in an incoming clus-
ter M, we use Algorithm 1 to check if the fre-
quency of each character in C?M is greater 
than a threshold ?. 
 
Algorithm 1 Balanced Cluster Selection 
Input: A set of character clusters P={M1 ,  . . .  , MK} 
          Number of selections N2, 
Output: A set of clusters Q={ '1M  ,  . . .  , 
'
2N
M }. 
 
1: C={} 
2: sort the clusters in P by their inner ratios; 
3: while |C|<=N2 do 
4:     pick the cluster M that has highest inner ratio; 
5:     for each character c in M do 
6:          if the frequency of c in C?M is over thresh-
old ? 
7:                 P?P?M; 
8:                 continue; 
9 :        else 
135
10:               C?C?M; 
11:               P?P?M; 
12:        end; 
13:   end; 
14: end 
 
The above algorithm yields the best N1 clus-
ters in terms of exchangeability. Next, we exe-
cute the above procedures again to select the 
best  N2 clusters based on their location inde-
pendence and exchangeability. However, for 
each character ci, we use vj to denote the value of 
its j-th dimension. We calculate vj as follows: 
r
jijiijijj ffffv )]',,',)[min(1(
' ?? ?+= ,      (5) 
where ijf  stands for the frequency with which ci 
and cj appear in the same word when ci is the 
first character; and f?ij stands for the frequency 
with which ci and cj co-occur in the same word 
when ci precedes cj  but not in the first position. 
We choose the minimum value from ijf , f?ij, jif , 
and f?ji  because if ci and cj both appear in the 
first position of a word and their order is ex-
changeable, the four frequency values, including 
the minimum value, will all be large enough. 
Type Cluster Inner (K, ?, ?) 
,.0123456789 0.94 (10, 0.60, 0.16)
EX 
 
-/ABCDEFGHIKLMNOPR 
STUVWabcdefghiklmnoprst 
uvwxy 
0.93 (10, 0.70, 0.16)
??ABCDEFGHIKLMNO 
PRSTUVWabcdefghiklmno 
prstvwxy 
0.84 (10, 0.50, 0.25)EL 
?????????? 0.76 (10, 0.50, 0.26)
Table 1. Clustering Results of the CTU corpus 
Our next goal is to create the best hybrid of 
the above two cluster sets. The set selected for 
exchangeability is referred to as the EX set, 
while the set selected for both exchangeability 
and location independence is referred to as the 
EL set. We create a development set and use the 
best first strategy to build the optimal cluster set 
from EX?EL. The EX and EL for the CTU 
corpus are shown in Table 1. 
2.3 Handling Non-Chinese Words 
Non-Chinese characters suffer from a serious 
data sparseness problem, since their frequencies 
are much lower than those of Chinese characters. 
In bigrams containing at least one non-Chinese 
character (referred as non-Chinese bigrams), the 
problem is more serious. Take the phrase ???  
20?? (about 20 years old) for example. ?2? is 
usually predicted as I, (i.e., ???? is connected 
with ?2?) resulting in incorrect segmentation, 
because the frequency of ?2? in the I class is 
much higher than that of ?2? in the B class, even 
though the feature C-2C-1=???? has a high 
weight for assigning ?2? to the B class. 
Traditional approaches to CWS only use one 
general tagger (referred as the G tagger) for 
segmentation. In our system, we use two CWS 
taggers. One is a general tagger, similar to the 
traditional approaches; the other is a specialized 
tagger designed to deal with non-Chinese words. 
We refer to the composite tagger (the general 
tagger plus the specialized tagger) as the GS 
tagger. 
Here, we refer to all characters in the selected 
clusters as non-Chinese characters. In the devel-
opment stage, the best-first feature selector de-
termines which clusters will be used. Then, we 
convert each sentence in the training data and 
test data into a normalized sentence. Each non-
Chinese character c is replaced by a cluster rep-
resentative symbol ?M, where c is in the cluster 
M. We refer to the string composed of all ?M as 
F. If the length of F is more than that of W, it 
will be shortened to W. The normalized sentence 
is then placed in one file, and the non-Chinese 
character sequence is placed in another. Next, 
we use the normalized training and test file for 
the general tagger, and the non-Chinese se-
quence training and test file for the specialized 
tagger. Finally, the results of these two taggers 
are combined. 
The advantage of this approach is that it re-
solves the data sparseness problem in non-
Chinese bigrams. Consider the previous example 
in which ? stands for the numeral cluster. Since 
there is a phrase ??? 8??  in the training data, 
C-1C0= ?? 8? is still an unknown bigram using 
the G tagger. By using the GS tagger, however, 
??? 20?? and ??? 8?? will be converted 
as ??? ???? and ??? ???, respectively. 
Therefore, the bigram feature C-1C0=?? ?? is no 
longer unknown. Also, since ? in ?? ?? is 
tagged as B, (i.e., ??? and ??? are separated), 
??? and ??? will be separated in  ??? ????. 
2.4 Generating and Applying Templates 
Template Generation 
We first extract all possible word candidates 
from the training set. Given a minimum word 
length L, we extract all words whose length is 
greater than or equal to L, after which we align 
all word pairs. For each pair, if more than fifty 
136
percent of the characters are identical, a template 
will be generated to match both words in the pair. 
Template Filtering 
We have two criteria for filtering the extracted 
templates. First, we test the matching accuracy 
of each template t on the development set. This 
is calculated by the following formula: 
strings matched all of #
separators no with strings matched of #
)( =tA . 
In our system, templates whose accuracy is 
lower than the threshold ?1 are discarded. For the 
remaining templates, we apply two different 
strategies. According to our observations of the 
development set, most templates whose accu-
racy is less than ?2 are ineffective. To refine such 
templates, we employ the character class infor-
mation generated by character clustering to im-
pose a class limitation on certain template slots. 
This regulates the potential input and improves 
the precision. Consider a template with one or 
more wildcard slots. If any string matched with 
these wildcard slots contains characters in dif-
ferent clusters, this template is also discarded.  
Template-Based Post-Processing (TBPP) 
After the generated templates have been filtered, 
they are used to match our CWS output and 
check if the matched tokens can be combined 
into complete words. If a template?s accuracy is 
greater than ?2, then all separators within the 
matched strings will be eliminated; otherwise, 
for a template t with accuracy between ?1 and ?2, 
we eliminate all separators in its matched string 
if no substring matched with t?s wildcard slots 
contains characters in different clusters. Resul-
tant words of less than three characters in length 
are discarded because CRF performs well with 
such words. 
3 Experiment 
3.1 Dataset 
We use the three larger corpora in SIGHAN 
Bakeoff 2006: a Simplified Chinese corpus pro-
vided by Microsoft Research Beijing, and two 
Traditional Chinese corpora provided by Aca-
demia Sinica in Taiwan and the City University 
of Hong Kong respectively. Details of each cor-
pus are listed in Table 2. 
Training Size Test SizeCorpus 
Types Words Types Words
CKIP 141 K 5.45 M 19 K 122 K
City University (CTU) 69 K 1.46 M 9 K 41 K
Microsoft Research (MSR) 88 K 2.37 M 13 K 107 K
Table 2. Corpora Information 
3.2 Results 
Table 3 lists the best combination of n-gram fea-
tures used in the G tagger. 
Uni-gram Bigram  
C-2, C-1, C0, C1 C-2C-1, C-1C0, C0C1, C-3C-1, C-2C0, C-1C1
Table 3. Best Combination of N-gram Features 
Table 4 compares the baseline G tagger and the 
enhanced GST tagger. We observe that the GST 
tagger outperforms the G tagger on all three cor-
pora. 
Conf R P F ROOV RIV 
CKIP-g 0.958 0.949 0.954 0.690 0.969 
CKIP-gst 0.961 0.953 0.957 0.658 0.974 
CTU-g 0.966 0.967 0.966 0.786 0.973 
CTU-gst 0.973 0.972 0.972 0.787 0.981 
MSR-g 0.949 0.957 0.953 0.673 0.959 
MSR-gst 0.953 0.956 0.955 0.574 0.966 
Table 4 Performance Comparison of the G Tag-
ger and the GST Tagger  
4 Conclusion  
The contribution of this paper is two fold. First, 
we successfully apply the K-means algorithm to 
character clustering and develop several cluster 
set selection algorithms for our GS tagger. This 
significantly improves the handling of sentences 
containing non-Chinese words as well as the 
overall performance. Second, we develop a post-
processing method that compensates for the 
weakness of ML-based CWS on longer words. 
References 
Hartigan, J. A., & Wong, M. A. (1979). A K-means 
Clustering Algorithm. Applied Statistics, 28, 100-
108. 
Lafferty, J., McCallum, A., & Pereira, F. (2001). 
Conditional Random Fields: Probabilistic Models 
for Segmenting and Labeling Sequence Data. Pa-
per presented at the ICML-01. 
 
 
137
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 57?64,
New York City, June 2006. c?2006 Association for Computational Linguistics
BIOSMILE: Adapting Semantic Role Labeling for Biomedical Verbs: 
An Exponential Model Coupled with 
Automatically Generated Template Features 
 
 
Richard Tzong-Han Tsai1,2, Wen-Chi Chou1, Yu-Chun Lin1,2, Cheng-Lung Sung1,  
Wei Ku1,3, Ying-Shan Su1,4, Ting-Yi Sung1 and Wen-Lian Hsu1 
1Institute of Information Science, Academia Sinica 
2Dept. of Computer Science and Information Engineering, National Taiwan University 
3Institute of Molecular Medicine, National Taiwan University  
4Dept. of Biochemical Science and Technology, National Taiwan University 
{thtsai,jacky957,sbb,clsung,wilmaku,qnn,tsung,hsu}@iis.sinica.edu.tw 
 
 
 
 
Abstract 
In this paper, we construct a biomedical 
semantic role labeling (SRL) system that 
can be used to facilitate relation extraction. 
First, we construct a proposition bank on 
top of the popular biomedical GENIA 
treebank following the PropBank annota-
tion scheme. We only annotate the predi-
cate-argument structures (PAS?s) of thirty 
frequently used biomedical predicates and 
their corresponding arguments. Second, 
we use our proposition bank to train a 
biomedical SRL system, which uses a 
maximum entropy (ME) model. Thirdly, 
we automatically generate argument-type 
templates which can be used to improve 
classification of biomedical argument 
types. Our experimental results show that 
a newswire SRL system that achieves an 
F-score of 86.29% in the newswire do-
main can maintain an F-score of 64.64% 
when ported to the biomedical domain. 
By using our annotated biomedical corpus, 
we can increase that F-score by 22.9%. 
Adding automatically generated template 
features further increases overall F-score 
by 0.47% and adjunct arguments (AM) F-
score by 1.57%, respectively. 
1 Introduction 
The volume of biomedical literature available has 
experienced unprecedented growth in recent years. 
The ability to automatically process this literature 
would be an invaluable tool for both the design and 
interpretation of large-scale experiments. To this 
end, more and more information extraction (IE) 
systems using natural language processing (NLP) 
have been developed for use in the biomedical 
field. A key IE task in the biomedical field is ex-
traction of relations, such as protein-protein and 
gene-gene interactions. 
Currently, most biomedical relation-extraction 
systems fall under one of the following three ap-
proaches: cooccurence-based (Leroy et al, 2005), 
pattern-based (Huang et al, 2004), and machine-
learning-based. All three, however, share the same 
limitation when extracting relations from complex 
natural language. They only extract the relation 
targets (e.g., proteins, genes) and the verbs repre-
senting those relations, overlooking the many ad-
verbial and prepositional phrases and words that 
describe location, manner, timing, condition, and 
extent. The information in such phrases may be 
important for precise definition and clarification of 
complex biological relations. 
The above problem can be tackled by using se-
mantic role labeling (SRL) because it not only rec-
ognizes main roles, such as agents and objects, but 
also extracts adjunct roles such as location, manner, 
57
timing, condition, and extent. The goal of SRL is 
to group sequences of words together and classify 
them with semantic labels. In the newswire domain, 
Morarescu et al (2005) have demonstrated that 
full-parsing and SRL can improve the performance 
of relation extraction, resulting in an F-score in-
crease of 15% (from 67% to 82%). This significant 
result leads us to surmise that SRL may also have 
potential for relation extraction in the biomedical 
domain. Unfortunately, no SRL system for the 
biomedical domain exists. 
In this paper, we aim to build such a biomedical 
SRL system. To achieve this goal we roughly im-
plement the following three steps as proposed by 
Wattarujeekrit et al, (2004): (1) create semantic 
roles for each biomedical verb; (2) construct a 
biomedical corpus annotated with verbs and their 
corresponding semantic roles (following defini-
tions created in (1) as a reference resource;) (3) 
build an automatic semantic interpretation model 
using the annotated text as a training corpus for 
machine learning. In the first step, we adopt the 
definitions found in PropBank (Palmer et al, 2005), 
defining our own framesets for verbs not in Prop-
Bank, such as ?phosphorylate?. In the second step, 
we first use an SRL system (Tsai et al, 2005) 
trained on the Wall Street Journal (WSJ) to auto-
matically tag our corpus. We then have the results 
double-checked by human annotators. Finally, we 
add automatically-generated template features to 
our SRL system to identify adjunct (modifier) ar-
guments, especially those highly relevant to the 
biomedical domain. 
2 Biomedical Proposition Bank  
As proposition banks are semantically annotated 
versions of a Penn-style treebank, they provide 
consistent semantic role labels across different syn-
tactic realizations of the same verb (Palmer et al, 
2005). The annotation captures predicate-argument 
structures based on the sense tags of polysemous 
verbs (called framesets) and semantic role labels 
for each argument of the verb. Figure 1 shows the 
annotation of semantic roles, exemplified by the 
following sentence: ?IL4 and IL13 receptors acti-
vate STAT6, STAT3 and STAT5 proteins in the 
human B cells.? The chosen predicate is the word 
?activate?; its arguments and their associated word 
groups are illustrated in the figure. 
 
 
Figure 1. A Treebank Annotated with Semantic 
Role Labels 
Since proposition banks are annotated on top of 
a Penn-style treebank, we selected a biomedical 
corpus that has a Penn-style treebank as our corpus. 
We chose the GENIA corpus (Kim et al, 2003), a 
collection of MEDLINE abstracts selected from 
the search results with the following keywords: 
human, blood cells, and transcription factors. In the 
GENIA corpus, the abstracts are encoded in XML 
format, where each abstract also contains a 
MEDLINE UID, and the title and content of the 
abstract. The text of the title and content is seg-
mented into sentences, in which biological terms 
are annotated with their semantic classes. The 
GENIA corpus is also annotated with part-of-
speech (POS) tags (Tateisi et al, 2004), and co-
references (Yang et al, 2004). 
The Penn-style treebank for GENIA, created by 
Tateisi et al (2005), currently contains 500 ab-
stracts. The annotation scheme of the GENIA 
Treebank (GTB), which basically follows the Penn 
Treebank II (PTB) scheme (Bies et al, 1995), is 
encoded in XML. However, in contrast to the WSJ 
corpus, GENIA lacks a proposition bank. We 
therefore use its 500 abstracts with GTB as our 
corpus. To develop our biomedical proposition 
bank, BioProp, we add the proposition bank anno-
tation on top of the GTB annotation. 
2.1 Important Argument Types 
In the biomedical domain, relations are often de-
pendent upon locative and temporal factors 
(Kholodenko, 2006). Therefore, locative (AM-
LOC) and temporal modifiers (AM-TMP) are par-
ticularly important as they tell us where and when 
biomedical events take place. Additionally, nega-
58
tive modifiers (AM-NEG) are also vital to cor-
rectly extracting relations. Without AM-NEG, we 
may interpret a negative relation as a positive one 
or vice versa. In total, we use thirteen modifiers in 
our biomedical proposition bank. 
2.2 Verb Selection 
We select 30 frequently used verbs from the mo-
lecular biology domain given in Table 1. 
express trigger encode 
associate repress enhance 
interact signal increase 
suppress activate induce 
prevent alter Inhibit 
modulate affect Mediate 
phosphorylate bind Mutated 
transactivate block Reduce 
transform decrease Regulate 
differentiated promote Stimulate 
Table 1. 30 Frequently Biomedical Verbs 
Let us examine a representative verb, ?activate?. 
Its most frequent usage in molecular biology is the 
same as that in newswire. Generally speaking, ?ac-
tivate? means, ?to start a process? or ?to turn on.? 
Many instances of this verb express the action of 
waking genes, proteins, or cells up. The following 
sentence shows a typical usage of the verb ?acti-
vate.?  
[NF-kappaB
 Arg1
] is [not
 AM-NEG
] [activated
predicate
] [upon tetra-
cycline removal
AM-TMP
] [in the NIH3T3 cell line
AM-LOC
]. 
3 Semantic Role Labeling on BioProp 
In this section, we introduce our BIOmedical Se-
MantIc roLe labEler, BIOSMILE. Like POS tag-
ging, chunking, and named entity recognition, SRL 
can be formulated as a sentence tagging problem. 
A sentence can be represented by a sequence of 
words, a sequence of phrases, or a parsing tree; the 
basic units of a sentence are words, phrases, and 
constituents arranged in the above representations, 
respectively. Hacioglu et al (2004) showed that 
tagging phrase by phrase (P-by-P) is better than 
word by word (W-by-W). Punyakanok et al, (2004) 
further showed that constituent-by-constituent (C-
by-C) tagging is better than P-by-P. Therefore, we 
choose C-by-C tagging for SRL. The gold standard 
SRL corpus, PropBank, was designed as an addi-
tional layer of annotation on top of the syntactic 
structures of the Penn Treebank. 
SRL can be broken into two steps. First, we 
must identify all the predicates. This can be easily 
accomplished by finding all instances of verbs of 
interest and checking their POS?s. 
Second, for each predicate, we need to label all 
arguments corresponding to the predicate. It is a 
complicated problem since the number of argu-
ments and their positions vary depending on a 
verb?s voice (active/passive) and sense, along with 
many other factors.  
In this section, we first describe the maximum 
entropy model used for argument classification. 
Then, we illustrate basic features as well as spe-
cialized features such as biomedical named entities 
and argument templates.  
3.1 Maximum Entropy Model 
The maximum entropy model (ME) is a flexible 
statistical model that assigns an outcome for each 
instance based on the instance?s history, which is 
all the conditioning data that enables one to assign 
probabilities to the space of all outcomes. In SRL, 
a history can be viewed as all the information re-
lated to the current token that is derivable from the 
training corpus. ME computes the probability, 
p(o|h), for any o from the space of all possible out-
comes, O, and for every h from the space of all 
possible histories, H. 
The computation of p(o|h) in ME depends on a 
set of binary features, which are helpful in making 
predictions about the outcome. For instance, the 
node in question ends in ?cell?, it is likely to be 
AM-LOC. Formally, we can represent this feature 
as follows: 
??
??
?
=
=
=
otherwise :0
LOC-AMo and    
 true)(s_in_cellde_endcurrent_no if :1
),(
h
ohf
Here, current_node_ends_in_cell(h) is a binary 
function that returns a true value if the current 
node in the history, h, ends in ?cell?. Given a set of 
features and a training corpus, the ME estimation 
process produces a model in which every feature f i 
has a weight ?i. Following Bies et al (1995), we 
can compute the conditional probability as: 
?=
i
ohf
i
i
hZ
hop ),(
)(
1
)|( ?  
??=
o i
ohf
i
ihZ ),()( ?  
59
The probability is calculated by multiplying the 
weights of the active features (i.e., those of f i (h,o) 
= 1).  ?i is estimated by a procedure called Gener-
alized Iterative Scaling (GIS) (Darroch et al, 
1972). The ME estimation technique guarantees 
that, for every feature, f i, the expected value of ?i 
equals the empirical expectation of ?i in the train-
ing corpus. We use Zhang?s MaxEnt toolkit and 
the L-BFGS (Nocedal et al, 1999) method of pa-
rameter estimation for our ME model. 
BASIC FEATURES 
z Predicate ? The predicate lemma 
z Path ? The syntactic path through the parsing tree from 
the parse constituent be-ing classified to the predicate 
z Constituent type 
z Position ? Whether the phrase is located before or after 
the predicate 
z Voice ? passive: if the predicate has a POS tag VBN, 
and its chunk is not a VP, or it is preceded by a form of 
?to be? or ?to get? within its chunk; otherwise, it is ac-
tive 
z Head word ? calculated using the head word table de-
scribed by (Collins, 1999) 
z Head POS ? The POS of the Head Word 
z Sub-categorization ? The phrase structure rule that ex-
pands the predicate?s parent node in the parsing tree 
z First and last Word and their POS tags 
z Level ? The level in the parsing tree 
PREDICATE FEATURES 
z Predicate?s verb class 
z Predicate POS tag 
z Predicate frequency 
z Predicate?s context POS 
z Number of predicates 
FULL PARSING FEATURES 
z Parent?s, left sibling?s, and right sibling?s paths, con-
stituent types, positions, head words and head POS 
tags 
z Head of PP parent ? If the parent is a PP, then the head 
of this PP is also used as a feature 
COMBINATION FEATURES 
z Predicate distance combination 
z Predicate phrase type combination 
z Head word and predicate combination 
z Voice position combination 
OTHERS 
z Syntactic frame of predicate/NP 
z Headword suffixes of lengths 2, 3, and 4 
z Number of words in the phrase 
z Context words & POS tags 
Table 2. The Features Used in the Baseline Argu-
ment Classification Model 
3.2 Basic Features 
Table 2 shows the features that are used in our 
baseline argument classification model. Their ef-
fectiveness has been previously shown by (Pradhan 
et al, 2004; Surdeanu et al, 2003; Xue et al, 
2004). Detailed descriptions of these features can 
be found in (Tsai et al, 2005). 
3.3 Named Entity Features 
In the newswire domain, Surdeanu et al (2003) 
used named entity (NE) features that indicate 
whether a constituent contains NEs, such as per-
sonal names, organization names, location names, 
time expressions, and quantities of money. Using 
these NE features, they increased their system?s F-
score by 2.12%. However, because NEs in the 
biomedical domain are quite different from news-
wire NEs, we create bio-specific NE features using 
the five primary NE categories found in the 
GENIA ontology1: protein, nucleotide, other or-
ganic compounds, source and others. Table 3 illus-
trates the definitions of these five categories. When 
a constituent exactly matches an NE, the corre-
sponding NE feature is enabled.  
 NE Definition 
Protein Proteins include protein groups, families, molecules, complexes, and substructures.  
Nucleotide A nucleic acid molecule or the compounds that consist of nucleic acids. 
Other organic 
compounds 
Organic compounds exclude protein and 
nucleotide. 
Source 
Sources are biological locations where 
substances are found and their reactions 
take place.  
Others 
The terms that are not categorized as 
sources or substances may be marked up, 
with 
Table 3. Five GENIA Ontology NE Categories 
3.4 Biomedical Template Features 
Although a few NEs tend to belong almost exclu-
sively to certain argument types (such as ??cell? 
being mainly AM-LOC), this information alone is 
not sufficient for argument-type classification. For 
one, most NEs appear in a variety of argument 
types. For another, many appear in more than one 
constituent (node in a parsing tree) in the same 
sentence. Take the sentence ?IL4 and IL13 recep-
tors activate STAT6, STAT3 and STAT5 proteins 
in the human B cells,? for example. The NE ?the 
human B cells? is found in two constituents (?the 
                                                          
1 http://www-tsujii.is.s.u-tokyo.ac.jp/~genia/topics/Corpus/ 
genia-ontology.html  
60
human B cells? and ?in the human B cells?) as 
shown in figure 1. Yet only ?in the human B cells? 
is an AM-LOC because here ?human B cells? is 
preceded by the preposition ?in? and the deter-
miner ?the?. Another way to express this would be 
as a template?<prep> the <cell>.? We believe 
such templates composed of NEs, real words, and 
POS tags may be helpful in identifying constitu-
ents? argument types. In this section, we first de-
scribe our template generation algorithm, and then 
explain how we use the generated templates to im-
prove SRL performance. 
Template Generation (TG) 
Our template generation (TG) algorithm extracts 
general patterns for all argument types using the 
local alignment algorithm. We begin by pairing all 
arguments belonging to the same type according to 
their similarity. Closely matching pairs are then 
aligned word by word and a template that fits both 
is created. Each slot in the template is given con-
straint information in the form of either a word, NE 
type, or POS. The hierarchy of this constraint in-
formation is word > NE type > POS. If the argu-
ments share nothing in common for a given slot, 
the TG algorithm will put a wildcard in that posi-
tion. Figure 2 shows an aligned pair arguments. 
For this pair, the TG algorithm generated the tem-
plate ?AP-1 CC PTN? (PTN: protein name) be-
cause in the first position, both arguments have 
?AP-1;? in the second position, they have the same 
POS ?CC;? and in the third position, they share a 
common NE type, ?PTN.? The complete TG algo-
rithm is described in Algorithm 1. 
AP-1/PTN/NN and/O/CC NF-AT/PTN/NN 
AP-1/PTN/NN or/O/CC NFIL-2A/PTN/NN 
Figure 2. Aligned Argument Pair 
Applying Generated Templates 
The generated templates may match exactly or par-
tially with constituents. According to our observa-
tions, the former is more useful for argument 
classification. For example, constituents that per-
fectly match the template ?IN a * <cell>? are 
overwhelmingly AM-LOCs. Therefore, we only 
accept exact template matches. That is, if a con-
stituent exactly matches a template t, then the fea-
ture corresponding to t will be enabled. 
Algorithm 1 Template Generation 
Input: Sentences set S = {s1, . . . , sn}, 
Output: A set of template T = {t1, . . . , tk}. 
 
1: T = {}; 
2: for each sentence si from s1 to sn-1 do 
3:    for each sentence sj from si to sn do 
4:        perform alignment on si and sj, then 
5:          pair arguments according to similarity; 
6:        generate common template t from argument pairs; 
7:        T?T?t; 
8:    end; 
9: end; 
10: return T; 
4 Experiments 
4.1 Datasets 
In this paper, we extracted all our datasets from 
two corpora, the Wall Street Journal (WSJ) corpus 
and the BioProp, which respectively represent the 
newswire and biomedical domains. The Wall 
Street Journal corpus has 39,892 sentences, and 
950,028 words. It contains full-parsing information, 
first annotated by Marcus et al (1997), and is the 
most famous treebank (WSJ treebank). In addition 
to these syntactic structures, it was also annotated 
with predicate-argument structures (WSJ proposi-
tion bank) by Palmer et al (2005).  
In biomedical domain, there is one available 
treebank for GENIA, created by Yuka Tateshi et al 
(2005), who has so far added full-parsing informa-
tion to 500 abstracts. In contrast to WSJ, however, 
GENIA lacks any proposition bank. 
Since predicate-argument annotation is essential 
for training and evaluating statistical SRL systems, 
to make up for GENIA?s lack of a proposition 
bank, we constructed BioProp. Two biologists with 
masters degrees in our laboratory undertook the 
annotation task after receiving computational lin-
guistic training for approximately three months.  
We adopted a semi-automatic strategy to anno-
tate BioProp. First, we used the PropBank to train 
a statistical SRL system which achieves an F-score 
of over 86% on section 24 of the PropBank. Next, 
we used this SRL system to annotate the GENIA 
treebank automatically. Table 4 shows the amounts 
of all adjunct argument types (AMs) in BioProp. 
The detail description of can be found in (Babko-
Malaya, 2005).  
 
61
Type Description # Type Description # 
NEG negation 
marker 
103 ADV general  
purpose 
307
LOC location 389 PNC purpose 3
TMP time 145 CAU cause 15
MNR manner 489 DIR direction 22
EXT extent 23 DIS discourse 
connectives 
179
   MOD modal verb 121
Table 4. Subtypes of the AM Modifier Tag 
4.2 Experiment Design 
Experiment 1: Portability 
Ideally, an SRL system should be adaptable to the 
task of information extraction in various domains 
with minimal effort. That is, we should be able to 
port it from one domain to another. In this experi-
ment, we evaluate the cross-domain portability of 
our SRL system. We use Sections 2 to 21 of the 
PropBank to train our SRL system. Then, we use 
our system to annotate Section 24 of the PropBank 
(denoted by Exp 1a) and all of BioProp (denoted 
by Exp 1b). 
Experiment 2: The Necessity of BioProp 
To compare the effects of using biomedical train-
ing data vs. using newswire data, we train our SRL 
system on 30 randomly selected training sets from 
BioProp (g1,.., g30) and 30 from PropBank (w1,.., 
w30), each having 1200 training PAS?s. We then 
test our system on 30 400-PAS test sets from Bio-
Prop, with g1 and w1 being tested on test set 1, g2 
and w2 on set 2, and so on. Then we add up the 
scores for w1-w30 and g1-g30, and compare their 
averages. 
Experiment 3: The Effect of Using Biomedical-
Specific Features 
In order to improve SRL performance, we add do-
main specific features. In Experiment 3, we inves-
tigate the effects of adding biomedical NE features 
and argument template features composed of 
words, NEs, and POSs. The dataset selection pro-
cedure is the same as in Experiment 2. 
5 Results and Discussion 
All experimental results are summarized in Table 5. 
For argument classification, we report the preci-
sion (P), recall (R) and F-scores (F). The details 
are illustrated in the following paragraphs. 
Configuration Training Test P R F 
Exp 1a PropBank PropBank 90.47 82.48 86.29
Exp 1b PropBank BioProp 75.28 56.64 64.64
Exp 2a PropBank BioProp 74.78 56.25 64.20
Exp 2b BioProp BioProp 88.65 85.61 87.10
Exp 3a BioProp BioProp 88.67 85.59 87.11
Exp 3b BioProp BioProp 89.13 86.07 87.57
Table 5. Summary of All Experiments 
Exp 1a Exp 1b Role 
P R F P R F 
+/-(%)
Overall 90.47 82.48 86.29 75.28 56.64 64.64 -21.65
ArgX 91.46 86.39 88.85 78.92 67.82 72.95 -15.90
Arg0 86.36 78.01 81.97 85.56 64.41 73.49   -8.48
Arg1 95.52 92.11 93.78 82.56 75.75 79.01 -14.77
Arg2 87.19 84.53 85.84 32.76 31.59 32.16 -53.68
AM 86.76 70.02 77.50 62.70 32.98 43.22 -34.28
-ADV 73.44 52.32 61.11 39.27 26.34 31.53 -29.58
-DIS 81.71 48.18 60.62 67.12 48.18 56.09 -4.53
-LOC 89.19 57.02 69.57 68.54 2.67 5.14 -64.43
-MNR 67.93 57.86 62.49 46.55 22.97 30.76 -31.73
-MOD 99.42 92.5 95.84 99.05 88.01 93.2 -2.64
-NEG 100 91.21 95.40 99.61 80.13 88.81 -6.59
-TMP 88.15 72.83 79.76 70.97 60.36 65.24 -14.52
Table 6. Performance of Exp 1a and Exp 1b 
Experiment 1 
Table 6 shows the results of Experiment 1. The 
SRL system trained on the WSJ corpus obtains an 
F-score of 64.64% when used in the biomedical 
domain. Compared to traditional rule-based or 
template-based approaches, our approach suffers 
acceptable decrease in overall performance when 
recognizing ArgX arguments. However, Table 6 
also shows significant decreases in F-scores from 
other argument types. AM-LOC drops 64.43% and 
AM-MNR falls 31.73%. This may be due to the 
fact that the head words in PropBank are quite dif-
ferent from those in BioProp. Therefore, to achieve 
better performance, we believe it will be necessary 
to annotate biomedical corpora for training bio-
medical SRL systems. 
Experiment 2 
Table 7 shows the results of Experiment 2. When 
tested on BioProp, BIOSMILE (Exp 2b) outper-
forms the newswire SRL system (Exp 2a) by 
22.9% since the two systems are trained on differ-
ent domains. This result is statistically significant. 
Furthermore, Table 7 shows that BIOSMILE 
outperforms the newswire SRL system in most 
62
argument types, especially Arg0, Arg2, AM-ADV, 
AM-LOC, AM-MNR.  
Exp 2a Exp 2b Role 
P R F P R F 
+/-(%)
Overall 74.78 56.25 64.20 88.65 85.61 87.10 22.90
ArgX 78.40 67.32 72.44 91.96 89.73 90.83 18.39
Arg0 85.55 64.40 73.48 92.24 90.59 91.41 17.93
Arg1 81.41 75.11 78.13 92.54 90.49 91.50 13.37
Arg2 34.42 31.56 32.93 86.89 81.35 84.03 51.10
AM 61.96 32.38 42.53 81.27 76.72 78.93 36.40
-ADV 36.00 23.26 28.26 64.02 52.12 57.46 29.20
-DIS 69.55 51.29 59.04 82.71 75.60 79.00 19.96
-LOC 75.51 3.23 6.20 80.05 85.00 82.45 76.25
-MNR 44.67 21.66 29.17 83.44 82.23 82.83 53.66
-MOD 99.38 88.89 93.84 98.00 95.28 96.62 2.78
-NEG 99.80 79.55 88.53 97.82 94.81 96.29 7.76
-TMP 67.95 60.40 63.95 80.96 61.82 70.11 6.16
Table 7. Performance of Exp 2a and Exp 2b 
The performance of Arg0 and Arg2 in our sys-
tem increases considerably because biomedical 
verbs can be successfully identified by BIOSMILE 
but not by the newswire SRL system. For AM-
LOC, the newswire SRL system scored as low as 
76.25% lower than BIOSMILE. This is likely due 
to the reason that in the biomedical domain, many 
biomedical nouns, e.g., organisms and cells, func-
tion as locations, while in the newswire domain, 
they do not. In newswire, the word ?cell? seldom 
appears. However, in biomedical texts, cells repre-
sent the location of many biological reactions, and, 
therefore, if a constituent node on a parsing tree 
contains ?cell?, this node is very likely an AM-
LOC. If we use only newswire texts, the SRL sys-
tem will not learn to recognize this pattern. In the 
biomedical domain, arguments of manner (AM-
MNR) usually describe how to conduct an experi-
ment or how an interaction arises or occurs, while 
in newswire they are extremely broad in scope. 
Without adequate biomedical domain training cor-
pora, systems will easily confuse adverbs of man-
ner (AM-MNR), which are differentiated from 
general adverbials in semantic role labeling, with 
general adverbials (AM-ADV). In addition, the 
performance of the referential arguments of Arg0, 
Arg1, and Arg2 increases significantly. 
Experiment 3 
Table 8 shows the results of Experiment 3. The 
performance does not significantly improve after 
adding NE features. We originally expected that 
NE features would improve recognition of AM 
arguments such as AM-LOC. However, they failed 
to ameliorate the results since in the biomedical 
domain most NEs are just matched parts of a con-
stituent. This results in fewer exact matches. Fur-
thermore, in matched cases, NE information alone 
is insufficient to distinguish argument types. For 
example, even if a constituent exactly matches a 
protein name, we still cannot be sure whether it 
belongs to the subject (Arg0) or object (Arg1). 
Therefore, NE features were not as effective as we 
had expected. 
NE (Exp 3a) Template (Exp 3b) Role 
P R F P R F 
+/-(%)
Overall 88.67 85.59 87.11 89.13 86.07 87.57 0.46
ArgX 91.99 89.70 90.83 91.89 89.73 90.80 -0.03
Arg0 92.41 90.57 91.48 92.19 90.59 91.38 -0.1
Arg1 92.47 90.45 91.45 92.42 90.44 91.42 -0.03
Arg2 86.93 81.3 84.02 87.08 81.66 84.28 0.26
AM 81.30 76.75 78.96 82.96 78.18 80.50 1.54
-ADV 64.11 52.23 57.56 65.66 55.60 60.21 2.65
-DIS 82.51 75.42 78.81 83.00 75.79 79.23 0.42
-LOC 80.07 85.09 82.50 84.24 85.48 84.86 2.36
-MNR 83.50 82.19 82.84 84.56 84.14 84.35 1.51
-MOD 98.14 95.28 96.69 98.00 95.28 96.62 -0.07
-NEG 97.66 94.81 96.21 97.82 94.81 96.29 0.08
-TMP 81.14 62.06 70.33 83.10 63.95 72.28 1.95
Table 8. Performance of Exp 3a and Exp 3b 
6 Conclusions and Future Work 
In Experiment 3b, we used the argument templates 
as features. Since ArgX?s F-score is close to 90%, 
adding the template features does not improve its 
score. However, AM?s F-score increases by 1.54%. 
For AM-ADV, AM-LOC, and AM-TMP, the in-
crease is greater because the automatically gener-
ated templates effectively extract these AMs.  
In Figure 3, we compare the performance of ar-
gument classification models with and without ar-
gument template features. The overall F-score 
improves only slightly. However, the F-scores of 
main adjunct arguments increase significantly. 
The contribution of this paper is threefold. First, 
we construct a biomedical proposition bank, Bio-
Prop, on top of the popular biomedical GENIA 
treebank following the PropBank annotation 
scheme. We employ semi-automatic annotation 
using an SRL system trained on PropBank, thereby 
significantly reducing annotation effort. Second, 
we create BIOSMILE, a biomedical SRL system, 
which uses BioProp as its training corpus. Thirdly, 
we develop a method to automatically generate 
templates that can boost overall performance, es-
63
pecially on location, manner, adverb, and temporal 
arguments. In the future, we will expand BioProp 
to include more verbs and will also integrate an 
automatic parser into BIOSMILE. 
 
Figure 3. Improvement of Template Features 
Overall and on Several Adjunct Types 
Acknowledgement 
We would like to thank Dr. Nianwen Xue for his 
instruction of using the WordFreak annotation tool. 
This research was supported in part by the National 
Science Council under grant NSC94-2752-E-001-
001 and the thematic program of Academia Sinica 
under grant AS94B003. Editing services were pro-
vided by Dorion Berg. 
References  
Babko-Malaya, O. (2005). Propbank Annotation 
Guidelines. 
Bies, A., Ferguson, M., Katz, K., MacIntyre, R., 
Tredinnick, V., Kim, G., et al (1995). Bracketing 
Guidelines for Treebank II Style Penn Treebank 
Project  
Collins, M. J. (1999). Head-driven Statistical Models 
for Natural Language Parsing. Unpublished Ph.D. 
thesis, University of Pennsylvania. 
Darroch, J. N., & Ratcliff, D. (1972). Generalized 
Iterative Scaling for Log-Linear Models. The Annals 
of Mathematical Statistics. 
Hacioglu, K., Pradhan, S., Ward, W., Martin, J. H., & 
Jurafsky, D. (2004). Semantic Role Labeling by 
Tagging Syntactic Chunks. Paper presented at the 
CONLL-04. 
Huang, M., Zhu, X., Hao, Y., Payan, D. G., Qu, K., & 
Li, M. (2004). Discovering patterns to extract 
protein-protein interactions from full texts. 
Bioinformatics, 20(18), 3604-3612. 
Kholodenko, B. N. (2006). Cell-signalling dynamics in 
time and space. Nat Rev Mol Cell Biol, 7(3), 165-176. 
Kim, J. D., Ohta, T., Tateisi, Y., & Tsujii, J. (2003). 
GENIA corpus--semantically annotated corpus for 
bio-textmining. Bioinformatics, 19 Suppl 1, i180-182. 
Leroy, G., Chen, H., & Genescene. (2005). An 
ontology-enhanced integration of linguistic and co-
occurrence based relations in biomedical texts. 
Journal of the American Society for Information 
Science and Technology, 56(5), 457-468. 
Marcus, M. P., Santorini, B., & Marcinkiewicz, M. A. 
(1997). Building a large annotated corpus of English: 
the Penn Treebank. Computational Linguistics, 19. 
Morarescu, P., Bejan, C., & Harabagiu, S. (2005). 
Shallow Semantics for Relation Extraction. Paper 
presented at the IJCAI-05. 
Nocedal, J., & Wright, S. J. (1999). Numerical 
Optimization: Springer. 
Palmer, M., Gildea, D., & Kingsbury, P. (2005). The 
proposition bank: an annotated corpus of semantic 
roles. Computational Linguistics, 31(1). 
Pradhan, S., Hacioglu, K., Kruglery, V., Ward, W., 
Martin, J. H., & Jurafsky, D. (2004). Support vector 
learning for semantic argument classification. 
Journal of Machine Learning  
Punyakanok, V., Roth, D., Yih, W., & Zimak, D. (2004). 
Semantic Role Labeling via Integer Linear 
Programming Inference. Paper presented at the 
COLING-04. 
Surdeanu, M., Harabagiu, S. M., Williams, J., & 
Aarseth, P. (2003). Using Predicate-Argument 
Structures for Information Extraction. Paper 
presented at the ACL-03. 
Tateisi, Y., & Tsujii, J. (2004). Part-of-Speech 
Annotation of Biology Research Abstracts. Paper 
presented at the LREC-04. 
Tateisi, Y., Yakushiji, A., Ohta, T., & Tsujii, J. (2005). 
Syntax Annotation for the GENIA corpus. 
Tsai, T.-H., Wu, C.-W., Lin, Y.-C., & Hsu, W.-L. 
(2005). Exploiting Full Parsing Information to Label 
Semantic Roles Using an Ensemble of ME and SVM 
via Integer Linear Programming. . Paper presented at 
the CoNLL-05. 
Wattarujeekrit, T., Shah, P. K., & Collier, N. (2004). 
PASBio: predicate-argument structures for event 
extraction in molecular biology. BMC Bioinformatics, 
5, 155. 
Xue, N., & Palmer, M. (2004). Calibrating Features for 
Semantic Role Labeling. Paper presented at the 
EMNLP-04. 
Yang, X., Zhou, G., Su, J., & Tan., C. (2004). 
Improving Noun Phrase Coreference Resolution by 
Matching Strings. Paper presented at the IJCNLP-04. 
64
Term Contributed Boundary Tagging by Conditional Random 
Fields for SIGHAN 2010 Chinese Word Segmentation Bakeoff 
Tian-Jian Jiang?? Shih-Hung Liu*? Cheng-Lung Sung*? Wen-Lian Hsu?? 
?Department of 
Computer Science 
National Tsing-Hua University 
*Department of 
Electrical Engineering 
National Taiwan University 
?Institute of 
Information Science 
Academia Sinica 
{tmjiang,journey,clsung,hsu}@iis.sinica.edu.tw 
 
Abstract 
This paper presents a Chinese word 
segmentation system submitted to the 
closed training evaluations of CIPS-
SIGHAN-2010 bakeoff. The system uses 
a conditional random field model with 
one simple feature called term contri-
buted boundaries (TCB) in addition to 
the ?BI? character-based tagging ap-
proach. TCB can be extracted from unla-
beled corpora automatically, and seg-
mentation variations of different do-
mains are expected to be reflected impli-
citly. The experiment result shows that 
TCB does improve ?BI? tagging domain-
independently about 1% of the F1 meas-
ure score. 
1 Introduction 
The CIPS-SIGHAN-2010 bakeoff task of Chi-
nese word segmentation is focused on cross-
domain texts. The design of data set is challeng-
ing particularly. The domain-specific training 
corpora remain unlabeled, and two of the test 
corpora keep domains unknown before releasing, 
therefore it is not easy to apply ordinary machine 
learning approaches, especially for the closed 
training evaluations. 
2 Methodology 
2.1 The ?BI? Character-Based Tagging of 
Conditional Random Field as Baseline 
The character-based ?OBI? tagging of 
Conditional Random Field (Lafferty et al, 2001) 
has been widely used in Chinese word 
segmentation recently (Xue and Shen, 2003; 
Peng and McCallum, 2004; Tseng et al, 2005). 
Under the scheme, each character of a word is 
labeled as ?B? if it is the first character of a 
multiple-character word, or ?I? otherwise. If the 
character is a single-character word itself, ?O? 
will be its label. As Table 1 shows, the lost of 
performance is about 1% by replacing ?O? with 
?B? for character-based CRF tagging on the 
dataset of CIPS-SIGHAN-2010 bakeoff task of 
Chinese word segmentation, thus we choose 
?BI? as our baseline for simplicity, with this 1% 
lost bearing in mind. In tables of this paper, SC 
stands for Simplified Chinese and TC represents 
for Traditional Chinese. Test corpora of SC and 
TC are divided into four domains, where suffix 
A, B, C and D attached, for texts of literature, 
computer, medicine and finance, respectively. 
  R P F OOV 
SC-A OBI 0.906 0.916 0.911 0.539 
 BI 0.896 0.907 0.901 0.508 
SC-B OBI 0.868 0.797 0.831 0.410 
 BI 0.850 0.763 0.805 0.327 
SC-C OBI 0.897 0.897 0.897 0.590 
 BI 0.888 0.886 0.887 0.551 
SC-D OBI 0.900 0.903 0.901 0.472 
 BI 0.888 0.891 0.890 0.419 
TC-A OBI 0.873 0.898 0.886 0.727 
 BI 0.856 0.884 0.870 0.674 
TC-B OBI 0.906 0.932 0.919 0.578 
 BI 0.894 0.920 0.907 0.551 
TC-C OBI 0.902 0.923 0.913 0.722 
 BI 0.891 0.914 0.902 0.674 
TC-D OBI 0.924 0.934 0.929 0.765 
 BI 0.908 0.922 0.915 0.722 
Table 1. OBI vs. BI; where the lost of F > 1%, 
such as SC-B, is caused by incorrect English 
segments that will be discussed in the section 4. 
2.2 Term Contributed Boundary 
The word boundary and the word frequency are 
the standard notions of frequency in corpus-
based natural language processing, but they lack 
the correct information about the actual boun-
dary and frequency of a phrase?s occurrence. 
The distortion of phrase boundaries and frequen-
cies was first observed in the Vodis Corpus 
when the bigram ?RAIL ENQUIRIES? and tri-
gram ?BRITISH RAIL ENQUIRIES? were ex-
amined and reported by O'Boyle (1993). Both of 
them occur 73 times, which is a large number for 
such a small corpus. ?ENQUIRIES? follows 
?RAIL? with a very high probability when it is 
preceded by ?BRITISH.? However, when 
?RAIL? is preceded by words other than ?BRIT-
ISH,? ?ENQUIRIES? does not occur, but words 
like ?TICKET? or ?JOURNEY? may. Thus, the 
bigram ?RAIL ENQUIRIES? gives a misleading 
probability that ?RAIL? is followed by ?EN-
QUIRIES? irrespective of what precedes it. This 
problem happens not only with word-token cor-
pora but also with corpora in which all the com-
pounds are tagged as units since overlapping N-
grams still appear, therefore corresponding solu-
tions such as those of Zhang et al (2006) were 
proposed. 
We uses suffix array algorithm to calculate ex-
act boundaries of phrase and their frequencies 
(Sung et al, 2008), called term contributed 
boundaries (TCB) and term contributed fre-
quencies (TCF), respectively, to analogize simi-
larities and differences with the term frequencies 
(TF). For example, in Vodis Corpus, the original 
TF of the term ?RAIL ENQUIRIES? is 73. 
However, the actual TCF of ?RAIL ENQUI-
RIES? is 0, since all of the frequency values are 
contributed by the term ?BRITISH RAIL EN 
QUIRIES?. In this case, we can see that ?BRIT-
ISH RAIL ENQUIRIES? is really a more fre-
quent term in the corpus, where ?RAIL EN-
QUIRIES? is not. Hence the TCB of ?BRITISH 
RAIL ENQUIRIES? is ready for CRF tagging as 
?BRITISH/TB RAIL/TB ENQUIRIES/TI,? for 
example. 
3 Experiments 
Besides submitted results, there are several 
different experiments that we have done. The 
configuration is about the trade-off between data 
sparseness and domain fitness. For the sake of 
OOV issue, TCBs from all the training and test 
corpora are included in the configuration of 
submitted results. For potentially better consis-
tency to different types of text, TCBs from the 
training corpora and/or test corpora are grouped 
by corresponding domains of test corpora. Table 
2 and Table 3 provide the details, where the 
baseline is the character-based ?BI? tagging, and 
others are ?BI? with additional different TCB 
configurations: TCBall stands for the submitted 
results; TCBa, TCBb, TCBta, TCBtb, TCBtc, 
TCBtd represents TCB extracted from the train-
ing corpus A, B, and the test corpus A, B, C, D, 
respectively. Table 2 indicates that F1 measure 
scores can be improved by TCB about 1%, do-
main-independently. Table 3 gives a hint of the 
major contribution of performance is from TCB 
of each test corpus. 
Table 2. Baseline vs. Submitted Results 
 
 
 
 
 
 
  R P F OOV 
SC-A BI 0.896 0.907 0.901 0.508 
 TCBall 0.917 0.921 0.919 0.699 
SC-B BI 0.850 0.763 0.805 0.327 
 TCBall 0.876 0.799 0.836 0.456 
SC-C BI 0.888 0.886 0.887 0.551 
 TCBall 0.900 0.896 0.898 0.699 
SC-D BI 0.888 0.891 0.890 0.419 
 TCBall 0.910 0.906 0.908 0.562 
TC-A BI 0.856 0.884 0.870 0.674 
 TCBall 0.871 0.891 0.881 0.670 
TC-B BI 0.894 0.920 0.907 0.551 
 TCBall 0.913 0.917 0.915 0.663 
TC-C BI 0.891 0.914 0.902 0.674 
 TCBall 0.900 0.915 0.908 0.668 
TC-D BI 0.908 0.922 0.915 0.722 
 TCBall 0.929 0.922 0.925 0.732 
  F OOV 
SC-A TCBta 0.918 0.690 
 TCBa 0.917 0.679 
 TCBta + TCBa 0.917 0.690 
 TCBall 0.919 0.699 
SC-B TCBtb 0.832 0.465 
 TCBb 0.828 0.453 
 TCBtb + TCBb 0.830 0.459 
 TCBall 0.836 0.456 
SC-C TCBtc 0.897 0.618 
 TCBall 0.898 0.699 
SC-D  TCBtd 0.905 0.557 
 TCBall 0.910 0.562 
Table 3a. Simplified Chinese Domain-specific 
TCB vs. TCBall 
  F OOV 
TC-A TCBta 0.889 0.706 
 TCBa 0.888 0.690 
 TCBta + TCBa 0.889 0.710 
 TCBall 0.881 0.670 
TC-B TCBtb 0.911 0.636 
 TCBb 0.921 0.696 
 TCBtb + TCBb 0.912 0.641 
 TCBall 0.915 0.663 
TC-C TCBtc 0.918 0.705 
 TCBall 0.908 0.668 
TC-D TCBtd 0.927 0.717 
 TCBall 0.925 0.732 
Table 3b. Traditional Chinese Domain-specific 
TCB vs. TCBall 
 
4 Error Analysis 
The most significant type of error in our results 
is unintentionally segmented English words. Ra-
ther than developing another set of tag for Eng-
lish alphabets, we applies post-processing to fix 
this problem under the restriction of closed train-
ing by using only alphanumeric character infor-
mation. Table 4 compares F1 measure score of 
the Simplified Chinese experiment results before 
and after the post-processing. 
 
 
 F1 measure score 
before after 
SC-A OBI 0.911 0.918 
 BI 0.901 0.908 
 TCBta 0.918 0.920 
 TCBta + TCBa 0.917 0.920 
 TCBall 0.919 0.921 
SC-B OBI 0.831 0.920 
 BI 0.805 0.910 
 TCBtb 0.832 0.917 
 TCBtb + TCBb 0.830 0.916 
 TCBall 0.836 0.916 
SC-C OBI 0.897 0.904 
 BI 0.887 0.896 
 TCBtc 0.897 0.901 
 TCBall 0.898 0.902 
SC-D OBI 0.901 0.919 
 BI 0.890 0.908 
 TCBtd 0.905 0.915 
 TCBall 0.908 0.918 
Table 4. F1 measure scores before and after 
English Problem Fixed 
The major difference between gold standards 
of the Simplified Chinese corpora and the Tradi-
tional Chinese corpora is about non-Chinese 
characters. All of the alphanumeric and the 
punctuation sequences are separated from Chi-
nese sequences in the Simplified Chinese corpo-
ra, but can be part of the Chinese word segments 
in the Traditional Chinese corpora. For example, 
a phrase ??? / simvastatin / ? / statins? / ? / ? /
? / ?? (?/? represents the word boundary) from 
the domain C of the test data cannot be either 
recognized by ?BI? and/or TCB tagging ap-
proaches, or post-processed. This is the reason 
why Table 4 does not come along with Tradi-
tional Chinese experiment results. 
Some errors are due to inconsistencies in the 
gold standard of non-Chinese character, For ex-
ample, in the Traditional Chinese corpora, some 
percentage digits are separated from their per-
centage signs, meanwhile those percentage signs 
are connected to parentheses right next to them. 
5 Conclusion 
This paper introduces a simple CRF feature 
called term contributed boundaries (TCB) for 
Chinese word segmentation. The experiment 
result shows that it can improve the basic ?BI? 
tagging scheme about 1% of the F1 measure 
score, domain-independently. 
Further tagging scheme for non-Chinese cha-
racters are desired for recognizing some sophis-
ticated gold standard of Chinese word segmenta-
tion that concatenates alphanumeric characters 
to Chinese characters. 
Acknowledgement 
The CRF model used in this paper is developed based 
on CRF++, http://crfpp.sourceforge.net/ 
Term Contributed Boundaries used in this paper are 
extracted by YASA, http://yasa.newzilla.org/ 
References 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional random fields: proba-
bilistic models for segmenting and labeling se-
quence data. In Proceedings of International Con-
ference of Machine Learning, 591?598. 
Peter O'Boyle. 1993. A Study of an N-Gram Lan-
guage Model for Speech Recognition. PhD thesis. 
Queen's University Belfast. 
Fuchun Peng and Andrew McCallum. 2004. Chinese 
segmentation and new word detection using condi-
tional random fields. In Proceedings of Interna-
tional Conference of Computational linguistics, 
562?568, Geneva, Switzerland. 
Cheng-Lung Sung, Hsu-Chun Yen, and Wen-Lian 
Hsu. 2008. Compute the Term Contributed Fre-
quency. In Proceedings of the 2008 Eighth Inter-
national Conference on Intelligent Systems Design 
and Applications, 325-328, Washington, D.C., 
USA. 
Huihsin Tseng, Pichuan Chang, Galen Andrew, Da-
niel Jurafsky, and Christopher Manning. 2005. A 
conditional random field word segmenter for Sig-
han bakeoff 2005. In Proceedings of the Fourth 
SIGHAN Workshop on Chinese Language 
Processing, Jeju, Korea. 
Nianwen Xue and Libin Shen. 2003. Chinese word-
segmentation as LMR tagging. In Proceedings of 
the Second SIGHAN Workshop on Chinese Lan-
guage Processing. 
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumi-
ta. 2006. Subword-based tagging by conditional 
random fields for Chinese word segmentation. In 
Proceedings of the Human Language Technology 
Conference of the North American Chapter of the 
Association for Computational Linguistics, 193-
196, New York, USA. 
 
