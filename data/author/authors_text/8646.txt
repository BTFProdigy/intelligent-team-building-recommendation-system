61
A Computational Model of Emergent Simple Syntax: Supporting the Natural 
Transition from the One-Word Stage to the Two-Word Stage.  
Kris Jack, Chris Reed, and Annalu Waller 
Division of Applied Computing,  
University of Dundee, 
Dundee, Scotland, DD1 4HN 
[kjack | creed | awaller]@computing.dundee.ac.uk 
 
Abstract 
This paper introduces a system that simulates 
the transition from the one-word stage to the 
two-word stage in child language production.  
Two-word descriptions are syntactically 
generated and compete against one-word 
descriptions from the outset.  Two-word 
descriptions become dominant as word 
combinations are repeatedly recognised, 
forming syntactic categories; resulting in an 
emergent simple syntax.  The system 
demonstrates a similar maturation as children 
as evidenced by phenomena such as 
overextensions and mismatching, and the use 
of one-word descriptions being replaced by 
two-word descriptions over time. 
1 Introduction 
Studies of first language acquisition in children 
have documented general stages in linguistic 
development.  Neither the trigger nor the 
mechanism that takes a child from one stage to the 
next are known.  Stages arise gradually with no 
precise start  or end points, overlapping one 
another (Ingram, 1989). 
The aim of this research is to develop a system 
that autonomously acquires conceptual 
representations of individual words (the ?one-word 
stage?) and also, simultaneously, is capable of 
developing representations of valid multi-word 
structures i.e. simple syntax (the ?two-word 
stage?).  Two-word descriptions are expected to 
emerge as a result of the system state and not be 
artificially triggered. 
The system accepts sentences containing a 
maximum of two words.  It is designed to be 
scalable, allowing larger, more natural sentence 
sizes also.  System input is therefore a mixture of 
both one-word and two-word sentences.  The 
system is required to produce valid descriptions, 
particularly in the two-word stage.  Rules that 
enforce syntactic order, and allow for the 
production of semantically correct descriptions 
from novel concepts, are desirable. 
This paper is sectioned as follows;  pre-one-
word stage linguistic abilities in children are 
briefly discussed to explain why initial system 
functionality assumptions are made; the defining 
characteristics of both the one-word stage and two-
word stage in children are introduced as possible 
benchmarks for the system; a detailed description 
of system design and implementation with 
examples of the learning process and games played 
by the system are presented; a discussion of current 
results along with their possible implications 
follows; a brief review of related works that have 
influenced this research, citing major influences; 
the direction and aims of future research is 
described briefly; and finally, conclusions are 
drawn. 
2 Pre-One-Word Stage Children 
Linguistic abilities can be found in children prior 
to word production.  In terms of comprehension, 
children can distinguish between their mother?s 
voice and a stranger?s voice, male and female 
voices, and sentences spoken in their mother?s 
native language and sentences spoken in a different 
language.  They also show categorical perception 
to voice, can use formant transition information to 
mark articulation, and show intonation sensitivity 
(Pinker, 1994, Jusczyk, 1999). 
In terms of production, children produce noises, 
such as discomfort noises (0-2 months), comfort 
noises (2-4 months), and ?play? vocally with pitch 
and loudness variations (4-7 months) (Pinker, 
1994).  The babbling stage (6-8 months) is 
characterised with the production of recognisable 
syllables.  The syllables are often repeated, such as 
[mamama] and [papapa], with the easiest to 
produce sounds often being associated with 
members of the family (Jakobson, 1971). 
From this evidence it is reasonable to draw 
conclusions about linguistic abilities in the young 
child that can be used to frame assumptions for use 
in the system.  It is assumed that the system can 
receive and produce strings that can be broken 
down into their component words.  These words 
can be compared and equalities can be detected. 
62
3 One-Word Stage and Two-Word Stages 
The system is required to produce one-word 
descriptions in early stages that develop into two-
word descriptions, where appropriate, in latter 
stages..  The recognition of each stage is based on 
the number of words that the system uses at a 
particular point.  In children, the one and two-word 
stages have notable features. 
The one-word, or holophrastic, stage (9-18 
months), is characterised by one-word 
vocalisations that are consistently associated with 
concepts.  These concepts can be either concrete or 
abstract, such as ?mama?, referring to the concrete 
concept of the child?s mother, and ?more?, an 
abstract concept which can be applied in a variety 
of situations (Piaget, 1960). 
Two phenomena that occur during this stage are 
underextensions and overextensions.  An 
underextension is the formation of a word to 
concept association that is too narrow, such as 
?dog? referring to only the family dog.  
Overextension, similarly, is an association that is 
too broad, such as ?dog? referring to all four 
legged animals.  Mismatches, or idiosyncratic 
referencing also occur, resulting in a word being 
associated with an unrelated concept, such as 
?dog? referring to a table (Pinker, 1994).  These 
associations change over time. 
The two-word stage (18-24 months) introduces 
simple syntax into the child?s language faculty.  
Children appear to determine the most important 
words in a sentence and, almost all of the time, use 
them in the same order as an adult would 
(Gleitman and Newport, 1995).  Brown (1973) 
defines a typology to express semantic relations in 
the two-word stage.  It contains ten sets of 
relations, but only one will be considered in this 
paper; attribute  + entity (?red circle?).  During this 
stage, children already demonstrate a three word 
comprehension level (Tomasello and Kruger, 
1992).  The concepts relating to their sentences 
may therefore be more detailed than the phrases 
themselves. 
The system is expected to make the transition 
from the one-word stage to the two-word stage 
without changes to the functionality of the system.  
Once the system begins to run, input is restricted to 
that of sensory (concept based) and vocal (string 
representation) data. 
4 System Design and Implementation 
4.1 Introduction 
The system is designed to learn phrase-to-
concept associations and demonstrate it through 
playing games: a guessing game and a naming 
game.  Games are often used to test, and encourage 
system learning (Steels and Kaplan, 2001).  The 
learning process involves a user selecting an object 
in a scene and naming it.  The guessing game 
involves a user saying a phrase, and the system 
pointing to the object that the phrase refers to.  The 
naming game involves a user pointing to an object 
and the system naming it  The system is not 
physically grounded, so all games are simulated. 
The learning process allows the system to 
acquire associations between phrases and concepts 
while the games test system comprehension and 
system production respectively.  The learning 
process takes a string and concept as input, and 
produces no output.  Comprehension takes a string 
as input, and produces a concept as output, 
whereas production takes a concept as input, and 
produces a string as output. 
4.2 Strings and Concepts 
A string is a list of characters with a fixed order.  
A blank space is used to separate words within the 
string, of which there can be either one or two.  
The system can break strings down into their 
component words. 
A concept is a list of feature values.  The 
system recognises six feature values; red, blue, 
green, white, circle, and square.  There are no in-
built associations between any of the feature 
values.  This form of learning is supported by the 
imageability theory (Paiviom 1971).  No claims 
concerning concept acquisition and formation are 
made in this paper.  All concepts are hard coded 
from the outset. 
The full list of objects used in the games are 
derived from shape and colour combinations; red 
square, red circle, blue square, blue circle, green 
square, green circle, white square, and white 
circle.  Individual feature values can also act as 
concepts, therefore the full list is concepts is the 
list of object plus the list of feature values. 
4.3 Groups 
To associate a string with a concept, the system 
stores a list of groups.  Each group contains an ID, 
one or more description pairs, an observed 
frequency, and zero or more occurrence 
supporter links. 
The ID acts as a unique identifier, allowing the 
group to be found.  A description pair is a string 
and a concept.  Groups must have at least one 
description pair since their primary function is to 
relate a string to a concept.  The observed 
frequency represents the number of times that the 
description pair?s components have been 
associated through system input. 
The occurrence supporter links are a set of group 
IDs.  Each ID in the set refers to a group that 
63
contains a superset of either the description pair, or 
the same value for one component of the 
description pair and a superset of the other e.g. The 
description pair [?red?; red] 1 would be supported 
by the description pair [?red square?; red square].  
A worked example is provided in the next section.  
The links therefore record the number of 
occurrences of the group?s description pair.  The 
occurrence supporter link reinforces the 
description pair?s association and increases the 
total frequency of the group.  The total frequency 
is the group?s observed frequency plus the 
observed frequency of all of its supporters, never 
including a supporter more than once.   
Finally, group equality is defined by groups 
sharing the same description pair. 
4.4 The Learning Process 
At each stage in the learning process, a 
description pair is entered into the system.  The 
system does not attempt to parse the correctness of 
the description.  All data is considered to be 
positive.  The general learning process algorithm is 
detailed in the rest of this section.  Specific 
examples are also provided in Table 1, showing the 
groups? values; ID, description pair, occurrence 
frequency (OF), occurrence supporter links 
(OSLs), and total frequency (TF).  Five steps are 
followed to incorporate the new data: 
1. Identify the description pair. 
2. Find equal and unequal parts. 
3. Update system based on equal parts.. 
4. Update system based on unequal parts. 
5. Re-enter new groups into the system. 
4.4.1 Identify the description pair 
If the description pair exists in a group that is 
already in the system, then that group?s observed 
frequency is incremented.  Otherwise, the system 
creates a new group containing the new 
description.  It is given a unique ID and an 
observed frequency of one.  Assume that the 
system already contains a group based on the 
description pair [?red circle?; red circle].  This has 
an ID of one.  Assume also that the new 
description pair entered is [?red square?; red 
square].  Its group has an ID of two (group #2). 
All description pairs entered into the system are 
called concrete description pairs, this is, the 
system has encountered them directly as input.  
The new group is referred to as a concrete group, 
since it contains a concrete description pair. 
 
 
                                                   
1
 The convention of strings appearing in quotes 
(? red? ), and concepts appearing in italics (red) is 
adopted throughout this paper. 
 
ID Description Pair OF OSLs TF 
#1 [? red circle? ; red circle] 1 [] 1 
#2 [? red square? ; red 
square]
1 [] 1 
#3 [? red? ; red] 0 [#1,# 2] 2 
#4 [? #3 circle? ; #3 circle] 0 [#1] 1 
#5 [? #3 square? ; #3
square]
0 [#2] 1 
#6 [? circle? ; circle],
[? square? ; square]
0 [] 0 
#7 [? #3 #6? ; #3 #6] 0 [#2] 1 
Table 1: Sample data 
4.4.2 Find equal and unequal parts 
The new group is compared to all of the groups 
in the system.  Comparisons are based on the 
groups? description pairs alone.  Strings are 
compared separately from concepts.  A string 
match is found if one of the strings is a subset, or 
exact match, of the other.  Subsets of strings must 
contain complete words.  Words are regarded as 
atomic units.  Concepts are compared in the same 
fashion as strings, where feature values are the 
atomic units.  Successful comparisons create a set 
of equal parts and unequal parts.  Comparison 
results are only used when equal parts exist.  This 
approach is similar to alignment based learning, 
but with the additional component of concepts (van 
Zaanen, 2000). 
In comparing the new group, group #2, to the 
existing group, group #1, the equal part [?red?; 
red] and the unequal part [?circle?; circle], 
[?square?; square] are found.  The comparison 
algorithm is essential to the operation of the 
system.  It is used in the learning process and in the 
games.  Without it, no string or concept relations 
could be drawn2. 
4.4.3 Update system based on equal parts 
When an equal part is found, a new group is 
created.  In the example, an equal part is found 
between group #1 and group #2.  Group #3 is 
created as a result.  The new group is given an 
observed frequency of zero.  The IDs of the groups 
that were compared (group #1 and group #2) are 
added to the new group?s (group #3) occurrence 
supporter links.  If the group already exists, then as 
well as the existing group?s observed frequency 
being incremented, the IDs of the groups that were 
compared are added to the occurrence supporter 
links.  IDs can only appear once in the set of 
occurrence supporters links, so if an ID is already 
in it, then it is not added. 
                                                   
2
 The system assumes full compositionality.  Idioms 
and metaphors are not considered at this stage. 
64
Up until this point, all groups? description pairs 
have contained a string and concept.  Description 
pairs can also contain links to other groups? strings 
and groups? concepts.  These description pairs are 
referred to as abstract description pairs.  If all 
elements of the abstract description pair are links 
to other groups then it is fully abstract, else it is 
partially abstract.  A group that contains an 
abstract description pair is called an abstract 
group.  The group is fully abstract if its abstract 
description pair is fully abstract, else it is a 
partially abstract group.  Once a group has been 
created (as group #3 was), based on a description 
comparison, the system attempts to make two 
abstract groups. 
The new abstract groups (group #4 and group 
#5) are based on substitutions of the new group?s 
ID (group #3) into each of the groups that were  
originally compared.  Group #4 is therefore created 
by substituting group #3 into group #1.  Similarly, 
group #5 is created by substituting group #3 into 
group #2. 
The new abstract groups are given an observed 
frequency of zero (ID?s equal four and five).  Note 
that abstract groups always have an observed 
frequency of zero as they can never been directly 
observed.  The ID of the appropriate group used in 
comparison and later creation is added to the 
occurrence supporters links.  Each abstract group 
therefore has a total frequency equal to that of the 
group of which it is an abstract form. 
4.4.4 Update system based on unequal parts 
Unequal parts are only considered if equal parts 
are found in the comparison.  Otherwise, the 
unequal parts would be the complete set of data 
from both groups, which does not provide useful 
information for comparisson.  For every set of 
unequal parts that is found, a new group is created.  
If there is more than one unequal part then the 
group will contain more than one description pair.  
Such a group is referred to as a multi-group.  Two 
unequal parts were found earlier in comparing 
group #1 and group #2.  They are [?circle?; circle] 
and [?square?; square].  Group #6 is therefore 
created using these two description pairs. 
The creation of a multi-group allows for a fully 
abstract group to be created.  The system uses the 
data from the new multi-group (group #6) and the 
group created through equal parts (group #3).  
Both groups are substituted back into the group 
that was originally being compared (group #1).  
The resulting group (group #7) is fully abstract as 
both equal parts and unequal parts have been used 
to reconstruct the original group (group #1). 
4.4.5 Re-enter new groups into the system 
All groups that have been created through steps 
3 and 4 are compared to all other groups in the 
system.  Results of comparisons are dealt with by 
repeating steps 3-5 with the new results.  By use a 
recursive step like this, all groups are compared to 
one another in the system.  All group equalities are 
therefore created when the round is complete.  The 
amount of information available from every new 
group entered into the system is therefore 
maximised. 
4.5 The Significance of Groups Types 
Four different types of group have been 
identified in the previous section.  Although all 
groups share the same properties, they can be seen 
to represent difference aspects of language.  It is 
the combination and interaction of these groups 
that gives rise to emergent simple syntax.  This 
syntax is bi-gram collocations, but since the system 
is scalable, it is referred to as simple syntax. 
4.5.1 Concrete Groups 
Concrete groups acquire the meaning of 
individual lexemes (associate concepts with 
strings).  They are verifiable in the real world 
through the use of scene based games. 
4.5.2 Multi-Groups 
Multi-groups form syntactic categories based on 
similarities between description pair usage.  Under 
the current system, groups can only have a 
maximum of two description pairs.  If this were to 
be expanded, it is clear that large syntactic 
categories such as noun and verb equivalents 
would arise. 
4.5.3 Partially and Fully Abstract Groups 
Partially and fully abstract groups act as phrasal 
rules in the system.  Abstract values contained 
within the group?s description pairs can relate to 
both concrete groups and multi-groups.  Abstract 
groups that relate to multi-groups offer a choice of 
substitutions. 
For example, group #7 (Table 1) relates a single 
group to a multi-group.  By substitution of groups 
#3 and #6 into group #7, the concrete pairings of 
[?red circle?; red circle] and [?red square?; red 
square] are produced.  The string data are directly 
equivalent to: 
S -> Adj. N, 
where Adj. = {?red?} 
and N = {?circle?, ?square?} 
When a description pair is entered into the 
system, the process of semantic bootstrapping 
takes place.  Lexical items (strings) are associated 
with their meanings (concepts). When group 
65
comparisons are made, syntactic bootstrapping 
begins.  Associations are made between all 
combinations of lexical items throughout the 
system, and all combinations of meanings 
throughout the system. 
The system stores lexical item-meaning 
associations, lexical item-lexical item associations 
and meaning-meaning associations.  This basic 
framework allows for the production of complex 
phrasal rules. 
4.6 Comprehension and Production Through 
Games 
The guessing game tests comprehension while 
the naming game test production.  Comprehension 
takes a string as input, and produces a concept as 
output, whereas production takes a concept as 
input, and produces a string as output.  The 
comprehension and the production algorithms are 
the same, except the first is string based, and the 
second is concept based.  
The algorithm performs two tasks: finding 
concrete groups with exact matches to the input, 
and finding abstract groups with possible matches 
to the input.  Holophrastic matching uses only 
concrete groups.  Syntactic matching performs 
holophrastic matching, followed by further 
matches using abstract groups.  Note that the 
system only performs syntactic matching, which 
includes holophrastic matching.  Holophrastic 
matching is never performed alone, unless in 
testing stages. 
For holophrastic matches, the system searches 
through its list of groups.  Their description pairs 
are compared to the input being searched for.  
There is therefore re-use of the comparison 
algorithm introduced in the learning process.  
When a match is found, the group is added to a list 
of possible results. 
If holophrastic matching is being performed 
alone, then this list of possible results is sorted by 
total frequency.  The group with the highest total 
frequency is output by the system. 
Syntactic matching begins by performing 
holophrastic matching, but does not output a result 
until all abstract groups have been matched too.  It 
is therefore an extension of holophrastic matching.  
Once a first fun of holophrastic matching is 
performed, the input is converted into abstract 
form.  This is performed at the word/feature value 
level.  The most likely element is found by 
searching through the groups, comparing it to the 
description pair, and selecting the group with the 
highest total frequency from those found. 
The group IDs replace the appropriate element in 
the input (just as substitutions were made during 
the learning process).  All multi-groups that 
contain any of the abstract forms are found.  Each 
multi-group?s description pair becomes a 
replacement for the appropriate input?s abstract 
value. 
The new input, which is still in abstract form, is 
searched for, using holophrastic matching again.  
Since the groups found are not exact matches of 
the original input, their total frequency is 
multiplied by an abstract factor.  The abstract 
factor is a value between zero and one inclusive.  
The higher the factor, the greater the effect that 
abstract groups have on the results.  Syntactic 
matches can therefore  produce different results 
based on the value of abstract factor.  The abstract 
factor is not changed from the initiation to 
termination of the system. 
Groups found during the search are added to a 
new list of possible results.  The appropriate 
elements are substituted into the groups abstract 
values to make them concrete.  If an abstract value 
is acting as a substitute (by being found originally 
in a multi-group) then the original input value is 
used, not the replacement element.  This allows the 
abstract group to act as a syntactic rule, but it is 
penalised by the abstract factor so it does not have 
as much influence as concrete groups, that have 
been found to occur through direct input 
associations. 
The groups found throughout the entire syntactic 
search are now contained in a second list of 
possible results.  This list is reduced by removing 
duplicate groups.  For each group that is removed, 
its observed frequency and occurrence supporter 
links are added to the duplicate that is kept in the 
list. 
The two lists from each matching routine are 
merged and sorted by total frequency.  The 
string\concept of the group with the highest total 
frequency is outputted by the system. 
5 Testing and Results 
The system is tested within the following areas: 
1. Comprehension and production of all 
fourteen concepts.  The rate at which full 
comprehension and full production are 
achieved is compared. 
2. Correctness of production matches for 
compound concepts.  The correctness of 
production matches are studied over a 
number of rounds. 
3. Type of production matches for compound 
concepts.  The type of production matches 
favoured, holophrastic or syntactic, are 
compared over a number of rounds 
A match of concept to word or word to concept 
is considered correct if the string describes the 
concept fully.  For example, [?red?; red] and [?red 
66
square?;  red square] are correct, but [?red?; red 
square] and [?red square?; red] are incorrect.  One 
point is given for each correct match, zero for each 
incorrect match. 
Note that all test results are based on the average 
of ten different system trials.  Each result shows a 
broad tendency that will likely be smoothed if 
more trials are run.  All input is randomly 
generated.  The abstract factor is set to 0.4 for all 
tests. 
5.1 Comprehension Vs. Production 
Full comprehension occurs much sooner (see 
Figure 1), on average, than full production.  This 
result is found in children also.  Although 
production and comprehension compete quite 
steadily in early stages of the system, 
comprehension reaches its maximum, on average, 
in 20% of the time that production takes to reach 
its maximum. 
0
2
4
6
8
10
12
14
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49
Production
Comprehension
 
Figure 1: Shows number of correct 
comprehension and production matches 
Full comprehension (fourteen points) is 
achieved, on average, by round 50, while full 
production comes at round 250.  Both holophrastic 
data and syntactic data contribute to the successes.  
Underextensions are found during comprehension.  
For example,  in early rounds, ?green? is used to 
describe only green squares.  This phenomena is 
quickly eliminated in the trials but with a larger set 
of concepts and vocabulary, it is likely to persist 
for more than a few rounds. 
5.2 Correctness of Holophrastic Vs Syntactic 
Matches 
At the end of each round, production is tested 
using the eight compound concepts alone.  These 
are based on the eight observable objects in the 
simulated scene.  Only compound concepts can 
demonstrate simple syntax in this system, as 
singular concepts have associations to single word 
strings. 
 The system uses syntactic matching alone, but 
syntactic matching includes holophrastic matching, 
as discussed earlier.  To determine whether 
holophrastic data is being used, or syntactic data 
when a syntactic match is run, the matching 
algorithm has been split.  The number of correct 
strings produced using holophrastic data and the 
number of correct strings produced using syntactic 
data alone are compared (see Figure 2). 
The data demonstrate that the system uses 
mostly holophrastic matches in early rounds 
(comparable to the one-word stage).  This is 
eliminated in further rounds, in favour or syntactic 
matches alone (the two-word stage).  Note that 
although the holophrastic stage may appear to be 
producing two-words, these words are considered 
to be one-word.  For example, ?allgone? is 
considered to be one-word in early stages of 
linguistic development, as opposed to ?all gone? 
(Ingram, 1989). 
0
1
2
3
4
5
6
1 5 9 13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73
Holophrastic
Syntactic
 
Figure 2: Shows number of correct holophrastic 
and syntactic matches. 
The syntactic data continues to rise, until it 
achieves full production.  The holophrastic stage 
never achieves full production, but peaks, then 
reduces to zero.  This trend occurs as holophrastic 
underextensions such as ?red? representing red 
square become more likely than ?red square? 
representing red square. 
Early syntactic matches are based on novel 
string productions for novel string concepts.  
Holophrastic matching is incapable of producing 
novel strings from novel concepts, as it deals with 
concrete concepts.  Abstract concepts however, 
allow new string combinations to be produced, 
such as ?blue square?, from blue square even 
though neither then string nor concept have been 
encountered before.  Such an abstraction may 
come from a multi-group that associates ?blue? 
with ?red?, while containing a group that contains 
?red square? also.  The novel string ?blue square? 
is therefore abstracted. 
5.3 Use of Holophrastic Vs Syntactic Matches 
The system does not always produce the correct 
strings when a concept is entered.  The strings that 
are produced are a result of either holophrastic or 
syntactic matching.  Regardless of correctness, the 
amount of times that holophrastic matches are 
made over syntactic matches can be compared (see 
Figure 3). 
67
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
1 5 9 13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73
Syntactic
Holophrastic
 
Figure 3: Shows distribution of holophrastic and 
syntactic matches. 
The system relies completely on one-word 
descriptions at the outset, but soon syntactically 
derived two-word descriptions become prevalent.  
It is likely that the one-word stage will last longer 
if larger concept and vocabulary sets are in use. 
The system shows the same form of transition as 
can be seen in children from the one-word stage to 
the two-word stage, without the use of an artificial 
trigger.  The shift is gradual although the use of 
larger concept and vocabulary sets, plus different 
abstract factor values will affect the transition.  
The greater the number of words in multi-groups 
(the greater the size of syntactic categories), the 
lower the abstract factor is required to encourage 
the emergence of simple syntax. 
6 Related Works 
Supporters of computational modelling in 
language acquisition, often promote the practical 
importance of running simulations, where 
evolutionary effects can be recreated in short time 
periods (Zuidema, 2001). 
Although this paper is focussed on an individual 
system, or agent, acquiring language, it is been 
influenced by research into social learning 
(Oliphant and Batali, 1997; Kirby, 1999; Steels 
and Kaplan, 2002).  Social learning demonstrates 
the convergence upon a common language, or set 
of languages, from an uncoordinated proto-
language, within a population of agents.  Social 
learning allows for the playing of games between 
agents, similar to those in this paper, with the 
results being used as further system input, to 
support, or deny associations.  This research can be 
viewed as a form of social learning with one agent 
(string and concept generator) performing the 
teacher role, and the other agent (the system) 
performing the learner role. 
Simulations of both the babbling stage and the 
one-word stage have been developed (Scheler, 
1997; Abidi, 1999).  ACCLAIM, a one-word stage 
simulator, demonstrates that systems can react 
appropriately to changes in situations.  For 
example, when a cessation event is triggered, it 
produces ?Stop?, and when an object is requested, 
it produces ?More?.  Both examples are typical of 
children during the one-word stage (Bloom, 1973). 
Several systems exist that use perceptions to 
encourage language acquisition (Howell, Becker, 
and Jankowicz,, 2001; Roy, 2001).  ELBA learns 
both nouns and verbs from video scenes, starting 
with a blank lexicon.  Such systems have helped in 
the selection of both appropriate input sources and 
feature values to use in this research.  This system 
will also be physically grounded in future. 
The research presented in this paper describes a 
system that drives linguistic development.  Other 
systems have used similar techniques, based on 
syntactic and semantic bootstrapping (Howell and 
Becker, 2001), but have not explained how 
multiple word acquisition is achieved from a single 
word basis. 
Steels (1998) introduces frames that group 
lexical elements together by the roles that they 
play, very similar to groups in this paper.  Frames 
are more dynamic than groups however, 
structurally adapting when words reoccur.  Groups 
do not adapt in this way.  New groups are created 
to describe similarities rather than adapting 
existing ones.  Steels also introduces multiple word 
sentences, but it is unclear as to why agents invent 
a multiple word description over creating a new 
single word description.  The invention is triggered 
and does not emerge.  This research is based on 
real multiple word inputs, so the reason for 
invention is not necessary, unlike the reason for 
adoption i.e. why the system adopts two-word 
descriptions. 
The comparison algorithm, as previously noted, 
is similar to alignment based learning (van Zaanen, 
2000).  The system in this research performs 
perfect alignment requiring exact word matches 
when finding equal parts and unequal parts.  This 
system also uses concepts, reducing the number of 
incorrect groupings, or constituents, when there is 
ambiguity in text.  Unsupervised grammar 
induction can also be found in EMILE (van Zaanen 
and Adriaans, 2001).  EMILE identifies 
substitution classes by means of clustering.  These 
classes are comparable to this system?s groups 
although no concepts are used. 
7 Future Research 
As the system stands, it uses a small input set.  
Further developments are focussed on expanding 
the system.  All ten of Brown?s relations should be 
implemented.  Larger concept and vocabulary sets 
are therefore required.  Extensions to these sets are 
likely to affect underextensions, mismatches, the 
length of pre-syntactic usage time, and the overall 
growth pattern of simple syntax. 
68
8 Conclusion 
This paper offers a potential explanation of the 
mechanism by which the two-word stage emerges 
from the one-word stage.  It suggests that syntactic 
data is sought out from the beginning of language 
acquisition.  This syntactic data is always 
competing with the associations of holophrastic 
data.  Syntax is strengthened when patterns are 
consistently found between strings and concepts, 
and is used in favour of holophrastic data when it 
is sufficiently frequent.  The simple syntax 
continues to grow in strength, ultimately being 
used in favour of holophrastic data in all 
production and comprehension tasks. 
This system provides the foundation for more 
complex, hierarchical, syntax to emerge.  The type 
and volume of input is the only constraint upon the 
system.  The entry into post two-word stages is 
predicted from the system?s robust architecture. 
9 Acknowledgements 
The first author is sponsored by a studentship 
from the EPSRC. 
Thanks to the workshop reviewers for their 
helpful and much appreciated advice. 
References  
S. Abidi, 1996.  A Neural Network Simulation of Child 
Language Development at the One-word Stage.  In
proceedings of IASTED Int. Conf. on Modelling,
Simulation and Optimization, Gold Coast, Australia. 
L. Bloom, 1973.  One Word at a Time.  The use of 
single-word utterances before syntax  The Hague, 
Mouton. 
R.W. Brown, 1986.  Language and categories. In ? A
Study of Thinking? , ed. J.S. Bruner, J.J. Goodnow, 
and G.A. Austin, pages 247-312. New York: John 
Wiley, 1956. Reprint, New Brunswick: Transaction. 
L.R.. Gleitman and Elissa L. Newport, 1995.  The 
Invention of Language by Children: Environmental 
and Biological Influences on the Acquisition 
Language.  In ? An Invitation to Cognitive Science? , 
L.R. Gleitman and M. Liberman, 2nd ed., Vol.1, 
Cambridge, Mass., London, MIT Press. 
S.R. Howell and S. Becker, 2001.  Modelling language 
acquisition: Grammar from the Lexicon?  In 
Proceedings of the Cognitive Science Society..
S.R. Howell, S. Becker, and D. Jankowicz, 2001.  
Modelling Language Acquisition: Lexical Grounding 
Through Perceptual Features.  In Proceedings of the 
2001 Workshop on Developmental Embodied 
Cognition 
J.R. Hurford, M. Studdert-Kennedey, and C. Knight, 
1998. The Emergence of Syntax.  In ? Approaches to 
the evolution of language: social and cognitive 
bases? , Cambridge, Cambridge University Press. 
D. Ingram, 1989. First Language Acquisition.  Method, 
Description and Explanation.  Cambridge: Cambridge 
University Press. 
R. Jakobson, 1971.  Why ?mama? and ?papa?? In 
? Child Language: A Book of Readings? , by A. Bar-
Adon and W. F. Leopold, ed., pages 213-217. 
Englewood Cliffs, NJ:Prentice-Hall.
P.W. Jusczyk, 1999  How infants begin to extract words 
from speech.  Trends in Cognitive Science, 3 (9, 
September):323-328. 
S. Kirby, 1999.  Syntax out of learning: The cultural 
evolution of structured communication in a 
population of induction algorithms. In Proceedings of 
ECAL99 European Conference on Artificial Life, D. 
Floreano et al ed. pages 694-703, Berlin: Springer-
Verlag, 
M. Oliphant and J. Batali 1997. Learning and the 
emergence of coordinated communication. Centre for 
Research in Language Newsletter, 11(1). 
A. Paivio, 1971, Imagery and Verbal Processes. New 
York: Holt, Rinehart & Winston. 
J. Piaget, 1960.  The Language and Thought of the 
Child.  Routledge and K. Paul, 3rd ed.,.  Routledge 
Paperbacks.
S. Pinker, 1994. The Language Instinct.  The New 
Science of Language and Mind. Allen Lane, Penguin 
Press. 
D. Roy, 2001.  Grounded spoken language acquisition: 
Experiments in word learning.  IEEE Transactions on 
Multimedia. 
G. Scheler, 1997d. The transition from babbling to the 
one-word stage: A computational model. In 
Proceedings of GALA '97. 
L. Steels and F. Kaplan, 2001.  AIBO's first words: The 
social learning of language and meaning. Evolution of 
Communication, vol. 4(1):3-32.  John Benjamin?s 
Publishing Company, Amsterdam, Holland. 
L. Steels, 1998.  The Origins of Syntax in visually 
grounded robotic agents. AI 103, 1-24. 
M. Tomasello, and A.C. Kruger, 1992. Joint attention in 
action: Acquiring verbs in ostensive and non-
ostensive contexts. Journal of Child Language 
19:311-333. 
M. van Zaanen, 2000.  Learning structure using 
alignment based learning.  In Proceedings of the 
Third Annual Doctoral Research Colloquium 
(CLUK), pages 75-82. 
M. van Zaanen and P. Adriaans, 2001.  Alignment-
based learning versus EMILE: A comparison.  In 
Proceedings of the Belgian-Dutch Conference on AI 
(BNAIC).
W.H. Zuidema, 2001.  Emergent syntax: the unremitting 
value of computational modelling for understanding 
the origins of complex language. ECAL01, 641-644. 
Springer, Prague, Sept. 10-14, 2001. 
Proceedings of the 12th European Workshop on Natural Language Generation, pages 1?8,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Using NLG to Help Language-Impaired Users Tell Stories and  
Participate in Social Dialogues 
 
 
Ehud Reiter, Ross Turner 
University of Aberdeen 
Aberdeen, UK 
e.reiter@abdn.ac.uk 
csc272@abdn.ac.uk 
Norman Alm, Rolf Black, 
Martin Dempster, Annalu Waller 
University of Dundee 
Dundee, UK 
{nalm,rolfblack,martindempster, 
awaller}@computing.dundee.ac.uk 
 
 
Abstract 
Augmentative and Alternative Communication 
(AAC) systems are communication aids for 
people who cannot speak because of motor or 
cognitive impairments.  We are developing 
AAC systems where users select information 
they wish to communicate, and this is ex-
pressed using an NLG system.  We believe 
this model will work well in contexts where 
AAC users wish to go beyond simply making 
requests or answering questions, and have 
more complex communicative goals such as 
story-telling and social interaction. 
1 Introduction 
Many people have difficulty in communicating 
linguistically because of cognitive or motor im-
pairments.  Such people typically use communi-
cation aids to help them interact with other peo-
ple.  Such communication aids range from sim-
ple tools that do not involve computers, such as 
picture cards, to complex software systems that 
attempt to ?speak? for the impaired user. 
From a technological perspective, even the 
most complex communication aids have typi-
cally been based on fixed (canned) texts or sim-
ple fill-in-the-blank templates; essentially the 
user selects a text or template from a set of pos-
sible utterances, and the system utters it.  We 
believe that while this may be adequate if the 
user is simply making a request (e.g., please give 
me a drink) or answering a question (e.g., I live 
at home), it is not adequate if the user has a more 
complex communicative goal, such as engaging 
in social interaction, or telling a story. 
We are exploring the idea of supporting such 
interactions by building a system which uses ex-
ternal data and/or knowledge sources, plus do-
main and conversational models, to dynamically 
suggest possible messages (event, facts, or opin-
ions, represented as ontology instances) which 
are appropriate to the conversation. The user se-
lects the specific message which he wishes the 
system to speak, and possibly adds simple anno-
tations (e.g., I like this) or otherwise edits the 
message.  The system then creates an appropriate 
linguistic utterance from the selected message, 
taking into consideration contextual factors. 
In this paper we describe two projects on 
which we are working within this framework.  
The goal of the first project is to help non-
speaking children tell stories about their day at 
school to their parents; the goal of the second 
project is to help non-speaking adults engage in 
social conversation. 
2 Background 
2.1 Augmentative and alternative commu-
nication 
Augmentative and alternative communication 
(AAC) is a term that describes a variety of meth-
ods of communication for non-speaking people 
which can supplement or replace speech.  The 
term covers techniques which require no equip-
ment, such as sign language and cards with im-
ages; and also more technologically complex 
systems which use speech synthesis and a variety 
of strategies to create utterances.  
The most flexible AAC systems allow users to 
specify arbitrary words, but communication rates 
are extremely low, averaging 2-10 words per 
minute. This is because many AAC users interact 
slowly with computers because of their impair-
ments.  For example, some of the children we 
work with cannot use their hands, so they use 
scanning interfaces with head switches.  In other 
words, the computer displays a number of op-
1
tions to them, and then scans through these, 
briefly highlighting each option.  When the de-
sired option is highlighted, the child selects it by 
pressing a switch with her head.   This is ade-
quate for communicating basic needs (such as 
hunger or thirst); the computer can display a 
menu of possible needs, and the child can select 
one of the items.  But creating arbitrary messages 
with such an interface is extremely slow, even if 
word prediction is used; and in general such in-
terfaces do not well support complex social in-
teractions such as story telling (Waller, 2006).  
A number of research projects in AAC have 
developed prototype systems which attempt to 
facilitate this type of human-human interaction.  
At their most basic, these systems provide users 
with a library of fixed ?conversational moves? 
which can be selected and uttered.  These moves 
are based on models of the usual shape and con-
tent of conversational encounters (Todman & 
Alm, 2003), and for example include standard 
conversational openings and closings, such as 
Hello and How are you. They also include back-
channel communication such as Uh-huh, Great!, 
and Sorry, can you repeat that. 
It would be very useful to go beyond standard 
openings, closings, and backchannel messages, 
and allow the user to select utterances which 
were relevant to the particular communicative 
context and goals.  Dye et al(1998) developed a 
system based on scripts of common interactions 
(Schank & Abelson, 1977).  For example, a user 
could activate the MakeAnAppointment script, 
and then could select utterances relevant to this 
script, such as I would like to make an appoint-
ment to see the doctor.  As the interaction pro-
gressed, the system would update the selections 
offered to the user based on the current stage of 
the script; for example during time negotiation a 
possible utterance would be I would like to see 
him next week. This system proved effective in 
trials, but needed a large number of scripts to be 
generally effective.  Users could author their own 
texts, which were added to the scripts, but this 
was time-consuming and had to be done in ad-
vance of the conversation. 
Another goal of AAC is to help users narrate 
stories. Narrative and storytelling play a very 
important part in the communicative repertoire of 
all speakers (Schank, 1990). In particular, the 
ability to draw on episodes from one?s life his-
tory in current conversation is vital to maintain-
ing a full impression of one?s personality in deal-
ing with others (Polkinghorne, 1991). Story tell-
ing tools for AAC users have been developed, 
which include ways to introduce a story, tell it at 
the pace required (with diversions) and give 
feedback to comments from listeners (Waller, 
2006); but again these tools are based on a li-
brary of fixed texts and templates. 
2.2 NLG and AAC 
Natural language generation (NLG) systems 
generate texts in English and other human lan-
guages from non-linguistic input (Reiter and 
Dale, 2000).  In their review of NLP and AAC, 
Newell, Langer, and Hickey (1998) suggest that 
NLG could be used to generate complete utter-
ances from the limited input that AAC users are 
able to provide.  For example, the Compansion 
project (McCoy, Pennington, Badman 1998) 
used NLP and NLG techniques to expand tele-
graphic user input, such as Mary go store?, into 
complete utterances, such as Did Mary go to the 
store?  Netzer and Elhadad (2006) allowed users 
to author utterances in the symbolic language 
BLISS, and used NLG to translate this to English 
and Hebrew texts. 
In recent years there has been growing interest 
in data-to-text NLG systems (Reiter, 2007); 
these systems generate texts based on sensor and 
other numerical data, supplemented with ontolo-
gies that specify domain knowledge.  In princi-
ple, it seems that data-to-text techniques should 
allow NLG systems to provide more assistance 
than the syntactic help provided by Compansion.  
For example, if the user wanted to talk about a 
recent football (soccer) match, a data-to-text sys-
tem could get actual data about the match from 
the web, and generate potential utterances from 
this data, such as Arsenal beat Chelsea 2-1 and 
Van Persie scored two goals; the user could then 
select one of these to utter. 
In addition to helping users interact with other 
people, NLG techniques can also be used to edu-
cate and encourage children with disabilities.  
The STANDUP system (Manurung, Ritchie et 
al., 2008), for example, used NLG and computa-
tional humour techniques to allow children who 
use AAC devices to generate novel punning 
jokes.  This provided the children with successful 
experiences of controlling language, gave them 
an opportunity to play with language and explore 
new vocabulary (Waller et al, in press). In a 
small study with nine children with cerebral 
palsy, the children used their regular AAC tools 
more and also performed better on a test measur-
ing linguistic abilities after they used STANDUP 
for ten weeks. 
2
3 Our Architecture 
Our goal is help AAC users engage in com-
plex social interaction by using NLG and data-
to-text technology to create potential utterances 
and conversational contributions for the users. 
The general architecture is shown in Figure 1, 
and Sections 4 and 5 describe two systems based 
on this architecture. 
 
 
The system has the following components: 
Data analysis: read in data, from sensors, 
web information sources, databases, and so forth.  
This module analyses this data and identifies 
messages (in the sense of Reiter and Dale 
(2000)) that the user is likely to want to commu-
nicate; this analysis is partially based on domain, 
conversation, and user models, which may be 
represented as ontologies. 
Editing: allow the user to edit the messages.  
Editing ranges from adding simple annotations to 
specify opinions (e.g., add BAD to Arsenal beat 
Chelsea 2-1 if the user is a Chelsea fan), to using 
an on-screen keyboard to type free-text com-
ments.  Users can also delete messages, specify 
which messages they are most likely to want to 
utter, and create new messages.  Editing is done 
before the actual conversation, so the user does 
not have to do this under time pressure.  The 
amount of editing which can be done partially 
depends on the extent of the user?s disabilities. 
Narration: allows the user to select mes-
sages, and perhaps conversational moves (e.g., 
Hello), in an actual conversational context.  Edit-
ing is possible, but is limited by the need to keep 
the conversation flowing. 
NLG and Speech Synthesis: Generates actual 
utterances from the selected messages, taking 
into account linguistic context, especially a dia-
logue model. 
4 Narrative for Children: How was 
School Today 
The goal of the How was School Today project is 
to enable non-speaking children with major mo-
tor disabilities but reasonable cognitive skills to 
tell a story about what they did at school during 
the day.  The particular children we are working 
with have cerebral palsy, and use wheelchairs.  A 
few of them can use touch screens, but most of 
them use a head switch and scanning interface, 
as described above.  By ?story?, we mean some-
thing similar to Labov?s (1972) conversational 
narrative, i.e., a series of linked real-world events 
which are unusual or otherwise interesting, pos-
sibly annotated with information about the 
child?s feelings, which can be narrated orally. 
We are not expecting stories in the literary sense, 
with character development and complex plots. 
The motivation of the project is to provide the 
children with successful narrative experience. 
Typically developing children develop narrative 
skills from an early age with adults scaffolding 
conversations to elicit narrative, e.g. ?What did 
you do at school today?? (Bruner, 1975). As the 
child?s vocabulary and language competence 
develops, scaffolding is reduced. This progres-
sion is seldom seen in children with complex 
communication needs ? they respond to closed 
questions but seldom take control of conversa-
Sensor 
data 
Web info 
sources 
Other 
external data 
Data analysis: 
select possible 
messages to 
communicate 
Conversation 
model 
Domain model 
User model 
Editing: User adds 
annotations 
User 
 
NLG: 
Generate 
utterance 
Dialogue 
model 
Speech 
synthesis 
Conversation 
partner 
 
Narration: User 
selects what to say 
Prepare content 
Narrate content 
Figure 1:  General architecture 
3
tion (von Tetzchner and Grove, 2003).  Many 
children who use AAC have very limited narra-
tive skills (Soto et al 2006). Research has shown 
that providing children who use AAC with suc-
cessful narrative experiences by providing full 
narrative text can help the development of writ-
ten and spoken narrative skills  (Waller, 2008).  
The system follows the architecture described 
above.  Input data comes from RFID sensors that 
track where the child went during the day; an 
RFID reader is mounted on the child?s wheel-
chair, and RFID tags are placed around the 
school, especially in doorways so we can moni-
tor children entering and leaving rooms.  Teach-
ers have also been given RFID swipe cards 
which they can swipe against a reader, to record 
that they are interacting with the child; this is 
more robust than attempting to infer interaction 
automatically by tracking teachers? position. 
Teachers can also record interactions with ob-
jects (toys, musical instruments, etc), by using 
special swipe cards associated with these objects. 
Last but not least, teachers can record spoken 
messages about what happened during the day. 
An example of how the child?s wheelchair is set 
up is shown in Figure 2. 
   
 
 
Figure 2: System configuration 
 
The data analysis module combines sensor-
derived location and interaction data with a time-
table which records what the child was expected 
to do during the day, and a domain knowledge 
base which includes information about typical 
activities (e.g., if the child?s location is Swim-
mingPool, the child?s activity is probably 
Swimming).  From this it creates a series of 
events (each of which contain a number of mes-
sages) which describe the child?s lessons and 
activities, including divergences from what is 
expected in the timetable.  Several messages may 
be associated with an event.  The data analysis 
module also infers which events and messages it 
believes are most interesting to the child; this is 
partially based on heuristics about what children 
are interested in (e.g., swimming is more inter-
esting than lunch), and partially based on the 
general principle that unexpected things (diver-
gences from the timetable) are more interesting 
than expected things.  No more than five events 
are flagged as interesting, and only these events 
are shown in the editing interface. 
The editing interface allows children to re-
move events they do not want to talk about (per-
haps for privacy reasons) from the list of interest-
ing events.  It also allows children to add mes-
sages that express simple opinions about events; 
i.e., I liked it or I didn?t like it.  The interface is 
designed to be used with a scanning interface, 
and is based on symbols that represent events, 
annotations, etc. 
The narration interface, shown in Figure 3, is 
similar to the editing interface. It allows children 
to choose a specific event to communicate, 
which must be one of the ones they selected dur-
ing the editing phase.  Children are encouraged 
to tell events in temporal order (this is one of the 
narration skills we are trying to teach), but this is 
not mandated, and they can deviate from tempo-
ral order if they wish.   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
    Figure 3: Narration Interface 
 
The NLG system generates actual texts from 
the events selected by the children.  Most of this 
Tablet PC with NLG system and 
swipe-card RFID sensor  
long range 
RFID  
sensor for 
location 
tracking 
Events 
Opinion Annotations 
Messages  
for event 
4
is fairly simple, since the system deliberately 
uses simple ?child-like? language (Section 6).  
However, the system does need to make some 
decisions based on discourse context, including 
choosing appropriate referring expressions (es-
pecially pronouns), and temporal expressions 
(especially when children deviate from pure 
temporal order). 
4.1 Example 
For example, assume that the timetable speci-
fies the following information 
 
 
Assume that the sensors then recorded the fol-
lowing information 
 
Event 1 
      Location: CL_SEC2 
      Time: 13:23:00.0 - 14:07:00.0 
      Interactions: Mrs. Smith, Rolf, Ross 
 
Event 2 
      Location: HALL 
      Time: 14:10:00.0 ? 14:39:00.0 
      Interactions: none 
 
The data analysis module associates Event 1 with 
the Arts and Crafts timetable entry, since the lo-
cation is right, the timetabled teacher is present, 
and the times approximately match.  From this 
two messages are produced: one corresponding 
to I had Arts and Crafts this afternoon with Mrs. 
Smith (the core activity description), and the oth-
er corresponding to Rolf and Ross were there 
(additional information about people not time-
tabled to be there).  The child can add opinions 
using the editing interface; for example, if he 
added a positive annotation to the event, this 
would become an additional message corre-
sponding to It was great. 
For Event 2, the data analysis module notes 
that it does not match a timetabled event. The 
timetable indicates the child should be at Physio-
therapy after Art and Crafts; however, the sensor 
information indicates they were in the hall. The 
system generates a single message corresponding 
to Then I went to the Hall instead of Physiother-
apy to describe this event.  If the child added a 
negative annotation to this message, this would 
become an additional message expressed as I 
didn?t like it. 
4.2 Evaluation 
We conducted an initial evaluation of the How 
was School Today system in January, 2009.  
Two children used the system for four days: Ju-
lie, age 11, who had good cognitive skills but 
was non-verbal because of severe motor impair-
ments; and Jessica, age 13, who had less severe 
motor impairments but who had some cognitive 
and memory impairments (these are not the chil-
drens? real names).  Julie used the system as a 
communication and interaction aid, as described 
above; Jessica used the system partially as a 
memory aid.  The evaluation was primarily 
qualitative: we observed how Julie and Jessica 
used the system, and interviewed their teachers, 
speech therapists, care assistants, and Julie?s 
mother (Jessica?s parents were not available). 
The system worked very well for Julie; she 
learned it quickly, and was able to use it to have 
real conversations about her day with adults, al-
most for the first time in her life.  This validated 
our vision that our technology could help AAC 
users engage in real interaction, and go beyond 
simple question answering and communication 
of basic needs.  The system also worked rea-
sonably well as a memory aid for Jessica, but she 
had a harder time using it, perhaps because of her 
cognitive impairments. 
Staff and Julie?s mother were very supportive 
and pleased with the system.  They had sugges-
tions for improving the system, including a wider 
range of annotations; more phrases about the 
conversation itself, such as Guess what happened 
at school today; and allowing children to request 
teenager language (e.g., really cool). 
From a technical perspective, the system 
worked well overall.   School staff were happy to 
use the swipe cards, which worked well.  There 
were some problems with the location sensors, 
we need better techniques for distinguishing real 
readings from noise.  A surprising amount of 
effort was needed to enter up-to-date knowledge 
(e.g., daily lunch menus), this would need to be 
addressed if the system was used for a period of 
months as opposed to days. 
5 Social Conversation for Adults 
In our second project, we want to build a tool to 
help adults with cerebral palsy engage in social 
conversation about a football match, movie, 
weather, and so forth.  Many people with severe 
disabilities have great difficulty developing new 
interpersonal relationships, and indeed report that 
forming new relationships and taking part in new 
Time Activity Location Teacher 
?? ?? ?? ?? 
13.20 -14 Arts and 
Crafts 
CL_SEC2 Mrs Smith 
14 -14.40 Physiotherapy PHYSIO1 Mrs Jones 
?? ?? ?? ?? 
5
activities are major priorities in their lives (Datil-
lo et al, 2007).  Supporting these goals through 
the development of appropriate technologies is 
important as it could lead to improved social out-
comes. 
This project builds on the TALK system 
(Todman and Alm, 2003), which helped AAC 
users engage in active social conversation. 
TALK partially overcame the problem of low 
communication rate by requiring users to pre-
author their conversational material ahead of 
time, so that when it was needed it could simply 
be selected and output. TALK also used insights 
from Conversation Analysis (Sacks, 1995) to 
provide appropriate functionality in the system 
for social conversation. For example, it sup-
ported opening and closing statements, stepwise 
topic change, and the use of quick-fire utterances 
to provide fast, idiomatic responses to commonly 
encountered situations. This approach led to 
more dynamic AAC-facilitated interactions with 
higher communication rates, and had a positive 
impact on the perceived communicative compe-
tence of the user (Todman, Alm et al, 2007).   
TALK requires the user to spend a substantial 
amount of time pre-authoring material; this is 
perhaps its greatest weakness.  Our idea is to re-
duce the amount of pre-authoring needed, by us-
ing the architecture shown in Fig 1, where much 
of the material is automatically created from data 
sources, ontologies, etc, and the user?s role is 
largely to edit and annotate this material, not to 
create it from scratch. 
We developed an initial prototype system to 
demonstrate this concept in the domain of foot-
ball results (Dempster, 2008).  We are now 
working on another prototype, whose goal is to 
support social conversations about movies, mu-
sic, television shows, etc (which is a much 
broader domain than football).  We have created 
an ontology which can describe events such as 
watching a film, listening to a music track, or 
reading a book.  Each ?event? has both temporal 
and spatial properties which allow descriptions to 
be produced about where and when an event took 
place, and other particulars relating to that par-
ticular class of event.  For example, if the user 
listened to a radio show, we record the name of 
the show, the presenter and the station it was 
broadcast on.  Ultimately we plan to obtain in-
formation about movies, music tracks, etc from 
web-based databases such as IMDB (movies) 
and last.fm (music). 
Of course, databases such as IMDB do not 
contain information such as what the user 
thought of the movie, or who he saw it with.  
Hence we will allow users to add annotations 
with such information.  Some of these annota-
tions will be entered via a structured tool, such as 
a calendar interface that allows users to specify 
when they watched or listened to something. We 
would like to use NaturalOWL (Galanis and An-
droutsopoulos, 2007) as the NLG component of 
the system; it is well suited to describing objects, 
and is intended to be integrated with an ontology.  
As with the How Was School Today project, 
some of the main low-level NLG challenges are 
choosing appropriate referring expressions and 
temporal references, based on the current dis-
course context.  Speech output is done using Ce-
reproc (Aylett and Pidcock, 2007). 
An example of our current narration interface 
is shown in Figure 4.  In the editing interface, the 
user has specified that he went to a concert at 
8pm on Thursday, and that he rated it 8 out of 
10.  The narration interface gives the user a 
choice of a number of messages based on this 
information, together with some standard mes-
sages such as Thanks and Agree. 
 
 
 
Note that unlike the How Was School Today 
project, in this project we do not attempt to infer 
event information from sensors, but we allow 
(and expect) the user to enter much more infor-
mation at the editing stage.  We could in princi-
ple use sensors to pick up some information, 
such as the fact that the user was in the cinema 
from 12 to 2PM on Tuesday, but this is not the 
research focus of this project. 
We plan to evaluate the system using groups 
of both disabled and non-disabled users.  This 
has been shown in the past to be an effective ap-
proach for the evaluation of prototype AAC sys-
tems (Higginbotham, 1995). Initially pairs of 
non-disabled participants will be asked to pro-
duce short conversations with one person using 
the prototype and the other conversing normally.   
Quantitative measures of the communication rate 
6
will be taken as well as more qualitative observa-
tions relating to the usability of the system.  Af-
ter this evaluation we will improve the system 
based on our findings, and then conduct a final 
evaluation with a small group of AAC users. 
6 Discussion: Challenges for NLG 
From an NLG perspective, generating AAC texts 
of the sort we describe here presents different 
challenges from many other NLG applications. 
First of all, realization and even microplanning 
are probably not difficult, because in this context 
the AAC system should generate short simple 
sentences if possible.  This is because the system 
is speaking ?for? someone with limited or devel-
oping linguistic abilities, and it should try to pro-
duce something similar to what the user would 
say himself if he or she had the time to explicitly 
write a text using an on-screen keyboard. 
To take a concrete example, we had originally 
considered using past-perfect tense (a fairly 
complex linguistic construct) in the How was 
School project, when the narrative jumped to an 
earlier point in time.  For example I ate lunch at 
12.  I had gone swimming at 11.  But it was clear 
from corpora of child-written texts that these 
children never used perfect tenses, so instead we 
opted for I ate lunch at 12.  I went swimming at 
11.  This is less linguistically polished, but much 
more in line with what the children might actu-
ally produce. 
Given this desire for linguistic simplicity, re-
alisation is very simple, as is lexical choice (use 
simple words) and aggregation (keep sentences 
short).  The main microplanning challenges re-
late to discourse coherence, in particular refer-
ring expressions and temporal descriptions.   
On the other hand, there are major challenges 
in document planning.  In particular, in the How 
Was School project, we want the output to be a 
proper narrative, in the sense of Labov (1972).  
That is, not just a list of facts and events, but a 
structure with a beginning and end, and with ex-
planatory and other links between components 
(e.g., I had math in the afternoon because we 
went swimming in the morning, if the child nor-
mally has math in the morning).  We also wanted 
the narrative to be interesting and hold the inter-
est of the person the child is communicating 
with.  As pointed out by Reiter et al(2008), cur-
rent NLG systems do not do a good job of gener-
ating narratives.  
Similarly, in the Social Conversations project 
we want the system to generate a social dialogue, 
not just a list of facts about movies and songs.  
Little previous research has been done on gener-
ating social (as opposed to task-oriented) dia-
logues.  One exception is the NECA Socialite 
system (van Deemter et al 2008), but this fo-
cused on techniques for expressing affect, not on 
high-level conversational structure. 
For both stories and social conversations, it 
would be extremely useful to be able to monitor 
what the conversational partner is saying.  This is 
something we hope to investigate in the future.  
As most AAC users interact with a small number 
of conversational partners, it may be feasible to 
use a speech dictation system to detect at least 
some of what the conversational partner says. 
Last but not least, a major challenge implicit 
in our systems and indeed in the general architec-
ture is letting users control the NLG system.   
Our systems are intended to be speaking aids, 
ideally they should produce the same utterances 
as the user would if he was able to talk.  This 
means that users must be able to control the sys-
tems, so that it does what they want it to do, in 
terms of both content and expression.  To the 
best of our knowledge, little is known about how 
users can best control an NLG system. 
7 Conclusion 
Many people are in the unfortunate position of 
not being able to speak or type, due to cognitive 
and/or motor impairments.  Current AAC tools 
allow such people to engage in simple needs-
based communication, but they do not provide 
good support for richer use of language, such as 
story-telling and social conversation.  We are 
trying to develop more sophisticated AAC tools 
which support such interactions, by using exter-
nal data and knowledge sources to produce can-
didate messages, which can be expressed using 
NLG and speech synthesis technology.  Our 
work is still at an early stage, but we believe that 
it has the potential to help AAC users engage in 
richer interactions with other people.  
Acknowledgements 
We are very grateful to Julie, Jessica, and their 
teachers, therapists, carers, and parents for their 
help in building and evaluating the system de-
scribed in Section 4.  Many thanks to the anony-
mous referees and our colleagues at Aberdeen 
and Dundee for their very helpful comments.  
This research is supported by EPSRC grants 
EP/F067151/1 and EP/F066880/1, and by a 
Northern Research Partnership studentship. 
7
References 
Aylett, M. and C. Pidcock (2007). The CereVoice 
Characterful Speech Synthesiser SDK. Proceed-
ings of Proceedings of the 7th International Con-
ference on Intelligent Virtual Agents, pages 413-
414. 
Bruner, J. (1975). From communication to language: 
A psychological perspective. Cognition 3: 255-
289. 
Datillo, J., G. Estrella, L. Estrella, J. Light, D. 
McNaughton and M. Seabury (2007). "I have cho-
sen to live life abundantly": Perceptions of leisure 
by adults who use Augmentative and Alternative 
Communication. Augmentative & Alternative 
Communication 24(1): 16-28. 
van Deemter, K., B Krenn, P Piwek, M Klesen, M 
Schr?der and S Baumann. Fully generated scripted 
dialogue for embodied agents. Artificial Intelli-
gence 172: 1219?1244. 
Dempster, M. (2008). Using natural language genera-
tion to encourage effective communication in non-
speaking people. Proceedings of Young Research-
ers Consortium, ICCHP'08. 
Dye, R., N. Alm, J. Arnott, G. Harper, and A. Morri-
son (1998). A script-based AAC system for trans-
actional interaction.  Natural Language Engineer-
ing, 4(1), 57-71. 
Galanis, D. and I. Androutsopoulos (2007). Generat-
ing Multilingual Descriptions from Linguistically 
Annotated OWL Ontologies: the NaturalOWL Sys-
tem. Proceedings of ENLG 2007. 
Higginbotham, D. J. (1995). Use of nondisabled sub-
jects in AAC Research : Confessions of a research 
infidel. Augmentative and Alternative Communica-
tion 11(1): 2-5. 
Labov, W (1972).  Language in the Inner City. Uni-
versity of Pennsylvania Press. 
Manurung, R., G. Ritchie, H. Pain, A. Waller, D. 
O'Mara and R. Black (2008). The Construction of a 
Pun Generator for Language Skills Development. 
Applied Artificial Intelligence 22(9): 841 ? 869. 
McCoy, K., C. Pennington and A. Badman (1998). 
Compansion: From research prototype to practical 
integration. Natural Language Engineering 4:73-
95. 
Netzer, Y and Elhadad, M (2006). Using Semantic 
Authoring for Blissymbols Communication 
Boards. In Proc of HLT-2006. 
Newell, A., S. Langer and M. Hickey (1998). The role 
of natural language processing in alternative and 
augmentative communication. Natural Language 
Engineering 4:1-16. 
Polkinghorne, D. (1991). Narrative and self-concept. 
Journal of Narrative and Life History, 1(2/3), 135-
153 
Reiter, E (2007). An Architecture for Data-to-Text 
Systems. In Proceedings of ENLG-2007, pages 
147-155. 
Reiter, E. and R. Dale (2000).  Building Natural Lan-
guage Generation Systems.  Cambridge University 
Press. 
Reiter, E,  A. Gatt, F Portet, and M van der Meulen 
(2008). The Importance of Narrative and Other 
Lessons from an Evaluation of an NLG System 
that Summarises Clinical Data (2007). In Proceed-
ings of INLG-2008, pages 97-104. 
Sacks, H. (1995). Lectures on Conversation. G. Jef-
ferson. Cambridge, MA, Blackwell. 
Schank, R. C. (1990). Tell me a story: A new look at 
real and artificial intelligence. New York, Macmil-
lan Publishing Co. 
Schank, R., and R. Abelson (1977).  Scripts, plans, 
goals, and understanding. New Jersey: Lawrence 
Erlbaum. 
Soto, G., E. Hartmann, and D. Wilkins (2006). Ex-
ploring the Elements of Narrative that Emerge in 
the Interactions between an 8-Year-Old Child who 
uses an AAC Device and her Teacher. Augmenta-
tive and Alternative Communication 4:231 ? 241. 
Todman, J. and N. A. Alm (2003). Modelling conver-
sational pragmatics in communication aids. Jour-
nal of Pragmatics 35: 523-538. 
Todman, J., N. A. Alm, D. J. Higginbotham and P. 
File (2007). Whole Utterance Approaches in AAC. 
Augmentative and Alternative Communication 
24(3): 235-254. 
von Tetzchner, S. and N. Grove (2003). The devel-
opment of alternative language forms. In S. von 
Tetzchner and N. Grove (eds), Augmentative and 
Alternative Communication: Developmental Issues, 
pages 1-27. Wiley. 
Waller, A. (2006). Communication Access to Conver-
sational Narrative. Topics in Language Disorders 
26(3): 221-239. 
Waller, A. (2008). Narrative-based Augmentative and 
Alternative Communication: From transactional to 
interactional conversation. Proceedings of ISAAC 
2008, pages 149-160.  
Waller, A., R. Black, D. A. O'Mara, H. Pain, G. Rit-
chie and R. Manurung (In Press). Evaluating the 
STANDUP Pun Generating Software with Chil-
dren with Cerebral Palsy. ACM Transactions on 
Accessible Computing. 
8
Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 1?9,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using NLG and Sensors to Support Personal Narrative for 
Children with Complex Communication Needs 
 
 
Rolf Black Joe Reddington, Ehud Reiter, Nava Tintarev Annalu Waller 
School of Computing Department of Computing Science School of Computing 
University of Dundee            University of Aberdeen University of Dundee            
rolfblack@ 
computing.dundee.ac.uk 
{j.reddington, e.reiter  n.tintarev}@abdn.ac.uk awaller@ 
computing.dundee.ac.uk 
 
 
Abstract 
We are building a tool that helps children with 
Complex Communication Needs1 (CCN) to 
create stories about their day at school. The 
tool uses Natural Language Generation (NLG) 
technology to create a draft story based on 
sensor data of the child?s activities, which the 
child can edit. This work is still in its early 
stages, but we believe it has great potential to 
support interactive personal narrative which is 
not well supported by current Augmentative 
and Alternative Communication (AAC) tools. 
1 Introduction 
Many tools have been developed to help children and 
adults who cannot speak (or who have limited speech) 
communicate better.  However, most of these tools have 
focused on supporting communication for practical 
goals, such as ?I am thirsty.? But human communica-
tion is also used for social goals; we develop friendships 
and other inter-personal relationships via social interac-
tion and communication. The bulk of conversation is 
characterized by free narrative (Cheepen 1988). One of 
the most important types of conversational narrative is 
personal narrative: someone telling a story about what 
happened to him or her. 
 People with limited or no functional speech do tell 
stories, but these tend to be in monologue form, or in a 
sequence of pre-stored utterances on voice output com-
munication aids (Waller 2006). Individuals who use 
                                                          
1
 The term Complex Communication Needs (CCN) describes 
individuals who, due to motor, language, cognitive, and/or 
sensory perceptual impairments (e.g., as a result of cerebral 
palsy), do not develop speech and language skills as expected. 
This heterogeneous group typically experiences restricted 
access to the environment, limited interactions with their 
communication partners, and few opportunities for communi-
cation (Light and Drager 2007). 
Augmentative and Alternative Communication (AAC) 
tools tend to be passive, responding to questions with 
single words or short sentences (e.g. Soto, Hartmann et 
al. 2006) and if able to initiate and maintain extended 
conversations tend to relate experience word for word 
each time they tell a story, even though much of conver-
sation is reused (Clarke and Clarke 1977). This is time 
consuming and physically exhausting ? typical rates 
range from 8 to 10 words per minute up to 12 to 15 per 
minute when techniques such as word prediction are 
used (Higginbotham, Shane et al 2007), with the result 
that people seldom engage in storytelling. Despite the 
importance of narrative, little work has been done on 
specific tools to help language-impaired individuals 
engage in personal storytelling. In this paper, we de-
scribe our work in progress on building a tool that uses 
Natural Language Generation (NLG) technology to help 
children tell stories about their day at school, describing 
both the work we have done to date, and the challenges 
that we face in further developing this concept. 
2 Background 
2.1 AAC 
Technology underpins much of Augmentative and Al-
ternative Communication (AAC), a field that attempts to 
augment natural speech and provides alternative ways to 
communicate for people with limited or no speech. At 
the simplest level, people with Complex Communica-
tion Needs (CCN) can cause a pre-stored message to be 
spoken by activating a single switch. At the most so-
phisticated level, literate users can generate novel text 
using input methods ranging from a single switch to a 
full keyboard.  
Despite advances in AAC, there are still many indi-
viduals for whom communication remains problematic. 
Although some individuals with CCN become effective 
communicators, most do not ? they tend to be passive 
communicators, responding mainly to questions or 
prompts at a one or two word level. Conversational 
1
skills such as initiation, elaboration and storytelling are 
seldom observed (Waller 2006). 
One reason for the reduced levels of communicative 
ability is the cognitive demands of AAC interfaces. Cur-
rent AAC technology provides the user with a purely 
physical link to speech output. The user is required to 
have sufficient cognitive abilities and physical stamina 
to translate what they want to say into the sequence of 
operations needed to produce the desired output. Mne-
monic codes and dynamic displays (Beukelman and 
Mirenda 2005) provide some help in the retrieval 
process, but users still have to master complex retrieval 
and production strategies.  
A second reason for the impoverished quality of 
conversation is the focus of AAC devices on transac-
tional communication; conversation which expresses 
needs wants and information transfer, for example, ?I 
am thirsty?, ?I use a straw for drinking?. Instead, inter-
active conversation is characterized by free narrative 
and phatic conversation, for example, ?Guess what 
happened this morning??, ?Hello?, and ?How are 
you?? Without easy access to extended interactive 
communication, it is difficult to develop the skills 
needed to initiate new topics and engage in storytelling.  
2.2 Importance of Narrative 
 Conversational narratives (oral stories told during 
interactive conversations) are crucial to social engage-
ment. Narratives provide a means for people to relate 
and share experiences, develop organizational skills, 
work through problems, develop self image, express 
personality, give form and meaning to life, and allow 
people to be interesting entertainers (Waller 2006).  
 Narrative skills develop experientially with children 
being able to engage in storytelling even before they are 
verbal (Bruner 1975). Early personal experience stories 
consist of a high point, for example, ?Mummy fall!? 
with adults scaffolding the full story, eliciting the ?who?, 
?what?, ?when? and ?where?. However, not all expe-
riences make good stories. An experience becomes a 
story if the storyteller has an emotional connection to 
the event (Labov, 1972), or if the event is unusual (Qua-
sthoff & Nikolaus, 1982). 
 Parents of typically developing children encourage 
development of narrative skills by eliciting stories from 
their children (Peterson and McCabe 1983), but the de-
velopment of narrative skills is problematic for people 
with CCN. We recall a study where disabled children 
were told different stories more often than typically 
developing peers who were read the same story night 
after night (Light, Binger et al 1994). In doing so, the 
disabled children did not have the chance to learn the 
sequence of stories, or the structure commonly used in 
narrative such as beginning, middle and end. As such, 
initially children should use the same story template 
consistently until they are ready to progress to another 
one. 
 It is difficult to provide access to event information 
which may become a story, and few AAC systems pro-
vide support for interactive story narration. However 
NLG gives us a possibility to change the underlying 
paradigm of AAC. Instead of placing the entire cogni-
tive load on the user, AAC devices can be designed to 
support the retrieval of story events and the scaffolding 
of story narration for individuals with CCN. 
2.3 NLG, Data-to-text 
NLG systems generate texts in English (or other human 
languages) from non-linguistic data (Reiter and Dale 
2000).  Our vision is to use an NLG system to generate 
a draft story, which the child can edit.  The non-
linguistic input to our story-generator is sensor data 
about the child?s activities, including location data 
(where the child was) and interaction data (what people 
and objects the child interacted with).  We also want to 
allow teachers and school staff to enter information 
about the child?s activities (such as voice messages). 
 A number of data-to-text systems (Reiter 2007) have 
been developed in recent years, which generate English 
summaries of sensor and other numerical data.  The 
most popular application area has been weather fore-
casting (generating textual weather forecasts from the 
results of a numerical atmosphere simulation model), 
and indeed several weather forecast generators have 
been fielded and used operationally (Goldberg, Driedger 
et al 1994; Reiter, Sripada et al 2005). A number of 
data-to-text systems have also been developed in the 
medical community, such as BabyTalk (Gatt, Portet et 
al. 2009), which generates summaries of clinical data 
from a neonatal intensive care unit, and the commercial 
Narrative Engine (Harris 2008) which summarizes data 
acquired during a doctor/patient encounter. 
 Most previous research in data-to-text has focused 
on summarizing technical data for expert users, with the 
goal of effectively communicating key information.  In 
our work, in contrast, the focus is on summarizing data 
about everyday events, with the goal of having some-
thing interesting to talk about.  There has been consider-
able work in the computational creativity community on 
generating interesting stories (P?r?z and Sharples 2004), 
but it has focused on fictional written stories, where the 
computer system can say whatever it wishes, without 
the constraint of describing real events. 
 Most previous work in NLG has focused on com-
puter systems which generate texts without human in-
put.  However, in our case we want children to be able 
to annotate (evaluate) and edit stories, as far as their 
abilities permit.  There has been some research on hu-
man post-editing of NLG texts (Sripada, Reiter et al 
2005), but this has focused on editing at the text level.  
2
Since editing at the text level is very laborious for AAC 
users, we need a higher-level interface that lets children 
edit content and structure without needing to type 
words.  We also want children to be able to control how 
a story is narrated, perhaps in response to a listener?s 
questions or body language.  For example children may 
wish to add comments such as ?It was awesome!?, or 
tell events out of sequence. 
    In short, we need to develop interfaces and interac-
tion techniques that allow our users to control the NLG 
system.  Unfortunately there has been very little pre-
vious work on this topic, indeed almost nothing is 
known about Human-Computer Interaction aspects of 
NLG systems.  Developing a better understanding of 
these aspects is one of the main research challenges we 
face from an NLG perspective. 
3 Current and ongoing work 
3.1 ?How was School today??? 
We developed an initial version of ?How was School 
today??? in 2009; see Reiter et al(2009) for more de-
tails about this system. 
 
 
 
Fig. 1: Participating pupil with support worker:  
The prototype system is mounted on the wheelchair, and 
the pupil has access to the system via head switch con-
trolled row/column scanning. 
 
 This system used Radio Frequency Identification 
(RFID), an emerging application in AAC to identify or 
give access to relevant vocabulary (Bart, Riny et al 
2008; DeRuyter and Fried-Oken 2010). Sensors were 
used to track both location (by putting tags on doors, 
which were automatically sensed by a long-range RFID 
reader) and interaction (by asking staff to manually 
swipe RFID cards in a short-range reader when the child 
interacted with a person or object). Staff could also 
record spoken messages about interesting events during 
the day (see Fig. 1). 
The software analyzed this data to remove sensor noise, 
and then compared it to a timetable which specified 
where children were supposed to be, what they were 
supposed to be doing and which teacher was supposed 
to be taking the class throughout the day.  This allowed 
the software to both fill in missing information, and to 
identify divergences from the schedule. The result of 
this process was a series of events (which corresponded 
to classes, for example, maths class), each of which had 
a set of associated messages (interactions during the 
event, divergence from schedule, etc.). 
 After the data analysis was completed, an NLG sys-
tem identified the events most interesting (to the child), 
using a heuristic that took into consideration both how 
inherent interesting an event was (for example, lunch 
was regarded as an inherently interesting event that 
children were likely to want to talk about) and also 
whether an event was unusual or not.  The latter is based 
on the observation that most personal narratives focus 
on unexpected or unusual events.  Unusual events were 
identified by the presence of recorded voice messages 
and by divergence from the timetable, e.g. a different 
teacher present or a different location. The system se-
lected the five most ?interesting? events and displayed 
them to the child in a simple visual editing interface.  In 
this interface the child could delete events he/she did 
not wish to talk about, and also annotate events with 
simple opinions (evaluations), such as I liked it, using 
the evaluation buttons on the interface, generating ap-
propriate utterances according to the last narrated event 
or message.(see Fig. 2).  
     When editing was finished, the NLG system generat-
ed texts describing the events and messages, which the 
child communicated using a simple narration interface 
(which was similar to the editing interface). Emphasis 
was placed on providing quick access to messages to 
minimize the length of pauses between utterances due to 
the physical accessing difficulties of the users. The 
narrative model is based on the Labov social narrative 
model (Labov 1972) which emphasizes the highpoint 
and evaluation. The dialogue model from beginning 
through to highpoint to the end with the user being able 
to add evaluations at any point of the narration. Stories 
are initially chronological order but interactively under 
the control of the user. This control of narration differs 
significantly from current AAC interventions where 
narrative tends to be output in a monologue format.  
 From an NLG perspective, the system was fairly 
straightforward. The most challenging microplanning 
tasks were choosing connectives, time phrases, lexical 
variety in embellishments, and pronouns based on dis-
course context. Connectives and time phrases were ne-
cessary since children could narrate events in different 
orders (for example, ?I went to maths.  Then I went to 
lunch? versus ?I went to lunch.  Before lunch, I went to 
maths?).  Document structuring  was simple because we 
Short range RFID reader 
and microphone for voice 
message recording 
Visual interface 
Access switch in head 
rest 
Long range RFID reader 
3
assumed that the children would choose their own order 
in which to narrate events. In fact some children are not 
able to do this; such children would need to be sup-
ported by a more sophisticated document planner that 
had a model of appropriate text structures in this do-
main. 
 We asked two children to use the system for one 
week for a qualitative formative evaluation.  Research-
ers supplied ongoing support during this, primarily trial 
observing how the children used the system, and dis-
cussing it with teachers, therapists and parents.  Gener-
ally it worked well for one child, Julie2, who had severe 
motor impairment (no independent means of mobility 
and interacted with a computer using a head switch with 
row/column scanning, see Fig. 1). Her expressive abili-
ties were limited but her comprehension skills were 
comparable to her non-disabled peers with some deve-
lopmental delay. The other child, Jessica, had more 
cognitive impairment, and found the interface too diffi-
cult. 
 
 
 
 
Fig. 2: Example screenshot from interface 
1: Navigation: Day and date of story, maximum of five 
story events, exit; 2: Event messages, numbers vary for 
each event. Here: 2 computer-generated messages, 3 
recorded messages, 1 user added evaluation; 
3: Sequential message navigation: previous, repeat, 
next; 4: Evaluation: delete event, negative evaluation, 
positive evaluation; 
 
   
 In a second evaluation, a third child, Eric, joined and 
all three children used the system over two weeks each. 
In this evaluation, we asked teachers and other staff to 
use the system without on-site support from the re-
searchers. This highlighted many practical usability 
issues, such as delays caused by starting the system in 
the morning, and problems caused by limited battery 
life. We eliminated the long-range RFID sensor because 
of its difficult setup; instead we asked staff to swipe 
                                                          
2
 The names of the children mentioned in this paper are 
changed to ensure anonymity. 
door cards when children entered rooms.  However, this 
strategy was not successful, as it was difficult for staff 
to remember to swipe both interactions and location 
changes. 
 The participants took the system home for use with 
their parents who gave positive feedback but also re-
ported issues with system usability (e.g. lack of access 
to stories from previous days) or suitability (too compli-
cated interface for Jessica).   
 Eric?s timetable was different from Julie and Jessi-
ca?s, because he visited college one morning a day, and 
we could not collect data during this period. Since some 
of the most exciting events in a school day happen out-
side the school building (sports and school trips as well 
as college), in the long term we do need to see if we can 
collect data outside as well as inside the school.  
3.2 HWST example 
     Julie used the system on her DynaVox? Vmax? 
Voice Output Communication Aid (VOCA) via head 
switch using row/column scanning. The above transcript 
shows an extract of a conversation Julie had with her 
Speech and Language Therapist (SLT) on day three 
about her experiences during day two. The researcher 
(RA) had been present all day for technical support. The 
conversation extract starts with Julie reporting about her 
morning break.  
     In this example Julie is able to quickly reply to con-
text related questions from her communication partner 
using the evaluations (?So what happened?? ? ?It was 
fun!?). Compared to conversations usually observed 
between aided and unaided partners Julie is able to con-
trol the conversation when starting a new topic after 
talking about the morning break, inviting her communi-
cation partner to prompt for more detailed information. 
Julie provides this with her next generated phrase. 
When she is asked about the event she replies with an 
evaluation the system has generated in relation to its 
previously generated message ?A visitor was there.?. 
We note that the system is able to refer to the correct 
gender of the visitor. 
 
1 Julie {next} [I had break.] 
2 Julie {next} [Lesley was there.] 
3 SLT Lesley was there?  
4 Julie ((Opens mouth in agreement, then turns back 
to screen)) 
5 SLT Ok mhmh. So what happened? 
6 Julie {positive evaluation} [It was fun.] 
7 SLT Oh good! ((laughs)) I?m glad to hear it! 
8 RA We like Lesley. 
9 SLT ((nods in direction of RA)) 
10 Julie ((smiles)) 
11 Julie:  {next} [Then I went to Junior Primary in-
stead of Reading Class.] 
1 
2 
3 
4 
4
12 SLT: Right, you went to Junior Primary? I wonder 
why that was?  
13 Julie: {next} [A visitor was there.] 
14 SLT:  Oh, a visitor, right. Wonder what the visitor 
was doing?  
15 Julie: {next} [?The dental hygienist came to give a 
talk.?] 
16 SLT: Oh, dental hygienist. 
17 Julie:  {previous} [A visitor was there.] 
18 SLT: That was the visitor, okay. That?s why you 
went to junior primary, uhm, what did you 
think of the talk? 
19 Julie:  {positive evaluation} [She was nice.] 
20 SLT: She was nice, that was good! ((laughs)) 
 
Notation: 
- Switch selected button by Julie: {curly brackets} 
- Natural speech: standard text.  
- Computer generated language accessed using one 
button: [standard text in square brackets].  
- Recorded messages accessed using one button: 
[?quoted standard text in square brackets?]. 
- Paralinguistic behaviors:  
((standard text in double brackets)).  
 
3.3  ?How was School Today? ? in the Wild 
We have now started a new project to further develop 
our work, called ?How was School today??? ? in the 
Wild (?in the wild? indicates that the focus is on how the 
technology works in a real school environment). The 
basic goal is to improve the system sufficiently so that it 
can be tried out over a period of several months, with 
children with varying levels and types of impairments; 
we will also work with several schools in the initial 
phase, although for practical reasons the evaluations 
may be at just one school. 
     During this project we will do some work on the 
issues described in Section 4; in particular we will try to 
make the system usable by children with different im-
pairments and ability levels (Section 4.1). This means 
having a very simple interface for children with consi-
derable cognitive impairments (such as Jessica); but 
also giving children with more cognitive abilities the 
opportunity to exert more control over the story (during 
both editing and narration), for example by supporting a 
richer range of annotations, and by making it easier to 
describe events and messages in any order. 
     Another intermediate goal is to improve the integra-
tion of voice messages entered by staff with the com-
puter-generated messages. This could be done by some 
combination of training staff to enter messages in a spe-
cific way (referring to the child in the first person); ask-
ing staff to annotate the messages so the computer 
knows something about their content; and/or using 
speech recognition to analyze the voice messages. In 
general there is a lot of interesting information that can 
only come from staff, and we need to think about the 
best way to help staff enter information in a way that is 
easy for them and useful for our system. 
 Now that a complete system is built, we are also able 
to thoroughly and formally evaluate the system. Mul-
tiple baseline single case study methodology will be 
used (Schlosser 2003) to evaluate the use and impact of 
our system. We intend to have up to four children (with 
varying ability levels) use the system for a period of 3 
months. This will give us a chance to observe the im-
pact of the system on the users and their environment 
such as the children?s interaction with the system and 
how staff at the school envisage using this new tool. 
The observations will be supported by semi-structured 
interviews with the children, their classroom teacher, 
their speech and language therapist and a parent. 
 We will look at the children?s conversations (with 
and without using our system) about interesting, staged 
events with different partners, analyzing them conversa-
tional characteristics such as narrative initiation, struc-
ture, length and evaluation. Analysis methods will 
include the Revised Edinburgh Functional Communica-
tion Profile (REFCP) (Wirz, Skinner et al 1990). 
     However, much of our focus will be on addressing 
the practical issues that make it difficult to use our cur-
rent research prototype over a period of months.  We 
have identified many such issues, both from our pre-
vious evaluations (Section 3.1) and also from a ques-
tionnaire that was distributed to school staff during an 
in-service day. 
      Location tracking ? There are problems with both 
of the techniques we have tried to date (automatically 
reading RFID tags on doors, and asking staff to swipe 
location information).  In this project we intend to try 
tracking the location of a child using Wi-Fi location 
tracking, which seems to be rapidly gaining popularity 
in the commercial world (Liu, Sen et al 2008). 
 Data entry, 2D bar codes ? We need to allow staff 
to easily enter and update information about the children 
(for example, their timetables) and sensor tags (e.g., if a 
new tag is given to a visitor).  For the latter, we want to 
investigate 2D bar codes, which could allow encoding 
of alphanumeric input data without reference to a cen-
tral database. 
 Portability, battery life ? The current system runs on 
a tablet PC (8?-12? touch screen, generic or VOCA 
hardware). During the evaluation, late powering up, run-
down batteries or simply forgetting a component caused 
significant data loss and usability issues. A future proto-
type should favor an ?always-on? system, such as a mo-
bile phone, allowing for easy portability and extended 
battery life. 
 Story generation ? The prototype system was only 
able to create a story towards the end of a day and gave 
5
only access to stories generated on that day. However, 
often the user desired to tell stories that had occurred on 
previous days, or to, say, tell a story at lunch that oc-
curred in the morning.  When data was insufficient for 
the system to create a story, the only output was an error 
message ?Can?t generate story right now.? This fru-
strated users, so future systems should be able to deliver 
a story with incomplete data.  
 Voice messages ? as mentioned above, we want to 
handle these in a more sophisticated way.  From a more 
practical perspective, we also want to make it easier for 
staff to listen to and change previously recorded mes-
sages.  We also want to allow parents to record messag-
es about events at home. 
4 Long-term vision and issues 
4.1 Supporting children with different levels 
and types of impairment 
A key issue in AAC is of course the diversity of AAC 
users. Children with CCN differ enormously in terms of 
cognitive ability, motor ability, and social ability. This 
was clear even in our initial evaluation where we 
worked just with two children, and discovered that our 
interface worked well for Julie but not Jessica. 
 Julie has little functional speech and severe physical 
impairments, and accesses her VOCA using a head 
switch through the slow process of scanning the inter-
face. Her VOCA interface consists of a grid of 15 to 30 
buttons per page, with more than 20 pages of vocabu-
lary. However, her cognitive skills were sufficient for 
her to master the interface on the second day. She used 
the system quite successfully, as shown in the example 
in Section 3.2.  
 Jessica also has severe physical impairments but 
does not use technology to support her communication 
(she has functional speech).  She has cognitive impair-
ments, which (amongst other things) affect her ability to 
remember and place events correctly in time. She had 
more difficulty mastering the interface than Julie. We 
simplified the interface for her (no editing, minimal 
control of narration), and then she displayed pragmatics 
known from typical language development in children, 
by telling her story with no room for interaction of her 
communication partner.  
 We also need to keep in mind that abilities are not 
static, but are likely to progress with age (see also Sec-
tion 4.2) and (hopefully) with the assistance of commu-
nication aids. For example, the WriteTalk project 
showed how pupils were both able to initiate and con-
trol communication more effectively with Talk:About? 
and how their formal writing skills improved over time 
(Waller et al, 1999). 
 In summary, some children may need a very simple 
interface because of cognitive impairments, but this 
should grow with them.  For example, the best narration 
tool for Jessica at her current stage of development is 
probably a single button that advances sequentially 
through the computer-generated story. The challenge is 
to provide an interface that Jessica can initially use via 
repeatedly pressing an ?Advance? button, but which 
gives her the possibility of exerting more control as her 
skills and abilities develop. 
 Other children (such as Julie) may have motor diffi-
culties that restrict the way in which they can interact 
with computer systems, and thus may require simple 
controls although they have reasonable cognitive skills. 
Restricted motor skills make certain tasks, such as en-
tering an arbitrary word, quite difficult and time con-
suming; hence the interface must avoid such tasks, and 
instead endeavor to give the child as much control as 
possible with a minimum amount of data entry.  Once 
these users master a basic story telling structure, it may 
help them develop their conversation skills if they use a 
wide variety of conversation patterns.  For this purpose, 
it may be worthwhile for the system to randomly vary 
the structure and language used in the narratives. 
 Still other children, for example on the autistic spec-
trum, may have good cognitive and motor abilities, but 
not have the experience of expressive communication 
necessary to develop interactive skills. These children 
are more likely to benefit from a system that supports 
the pragmatics of language in general and personal narr-
ative in particular.  For example, children on the high 
functioning end of autism may be comfortable with ra-
ther advanced software, which can help them adapt their 
storytelling according to the intended listener.  Indeed, 
giving these children more complex controls, if done 
correctly, can make the software fun and challenging in 
a positive way. 
     In the long term, as we broaden the range of children 
we work with, there may be overlaps between our work 
and research on tools to help typically developing child-
ren create stories, such as Robertson and Good (2005), 
and also between our work and research on tools to help 
adults with CCN tell personal narratives, such as Demp-
ster (2008).  Ideally it would be very nice to combine 
these efforts and create a story telling tool that could be 
used across the age and impairment spectrum. 
4.2 Narrative across the lifespan 
We would like our tool to be able to support children 
over time, as their abilities grow and as their expe-
riences accumulate. From the perspective of changing 
abilities, the challenge is to offer children an interface 
which is not only appropriate for their current stage of 
development (Sect 4.1), but also allows and indeed en-
6
courages them to exert more control over story content, 
language, and narration as their abilities grow. 
 We would also like our tool to become a repository 
of a child?s personal stories. The ability to relate rele-
vant stories can influence the quality of life, as well as 
social development and successful transitions. The life 
stories of people who use AAC are often held by parents 
and siblings (e.g., stories relating to health care 
(Hemsley, Balandin et al 2007)), and there is the inevit-
able concern that these stories and others are lost as 
parents age and siblings move away.  
 Technology has the potential both to support the 
acquisition of conversational skills for people who use 
AAC and to provide a repository for life stories. In the 
context of our work, it is essential that we provide ways 
of enabling children to develop their narrative skills so 
that they are more able to manage their own story repo-
sitory. In terms of development, young children will 
narrate recent stories regardless of conversational con-
text. By enabling the child to develop story structure by 
scaffolding interaction and enabling children to easily 
annotate stories, the child will begin to anticipate and 
control conversation. 
 Conversational narratives have traditionally not been 
supported by AAC tools partly due to the fact that they 
are so nebulous; they emerge during interactive conver-
sation (to date, events have to be manually input into a 
system and it is difficult to predict what events will be-
come a story); ?new? stories are repeated often (to date it 
is difficult to save conversation online); as stories age 
they are repeated in context (retrieval is often contextual 
e.g. topic based) and they grow longer having more em-
bellishments added to them. The technology we are de-
veloping provides an opportunity for children to access 
information about personal events over time, which they 
can communicate and narrate during a conversation. 
They can also evaluate (annotate) their stories, thereby 
embellishing and lengthening the stories.  However this 
will only be possible if the children can easily access 
previously experienced, generated and saved stories.  
 We can provide fast access to recent stories while 
anticipating the use of older stories such as for example 
those which closely match the current conversation top-
ic. In a research prototype called PROSE (Waller and 
Newell 1997), stories had to be physically tagged; there 
is now the potential to automate topic matching by re-
cognizing topic words spoken by a listener and parsing 
stored information for appropriate stories. Over a life-
time, some stories may fall into disuse, while others will 
be weighted more strongly depending on frequency of 
use and relevance. 
4.3 True dialogue in narration 
The ultimate goal of our research is to enable children to 
tell stories in the context of a social dialogue; for exam-
ple, we want children to be able to chat to their parents 
and other interested parties about what they did during 
the day.  
 Our current system incorporates a simple model of a 
conversation, where children are restricted at any point 
to choosing from a small number of options. The child 
chooses an event to talk about, and then goes through 
the sequence of messages associated with that event.  
The child has the freedom to switch to a different event, 
hence controlling the conversation, and to add annota-
tions/evaluations (for example ?it was fun!?). 
     This is adequate in many cases, but in the long term 
we would like to support more complex conversations; 
for example interrupting a discussion about today?s 
events to talk about what happened yesterday, or to dis-
cuss a particular teacher instead of an event.  We would 
also like children to easily be able to add conversational 
phrases, such as ?Guess what happened today at 
school?. 
 Because our children have motor and cognitive im-
pairments, we cannot present them with a large number 
of options for conversational moves. Ideally, the system 
would detect what the conversational partner wishes to 
talk about, and from this present the child with a small 
number of appropriate choices. For example, if the con-
versational partner asks the child what happened over 
the past week, our system would detect this and then 
give the child the option of talking about any individual 
weekday or the week in general. 
 One way of detecting what the conversational part-
ner intends is to use speech recognition and Natural 
Language Processing (NLP) technology to analyze what 
he or she says. Speech and NLP technology tend to 
work best when it is possible to train the system to the 
user?s voice, and also (in essence) train the user to un-
derstand what the speech/NLP system can and cannot 
do. This should be possible in our context, at least for 
people (such as parents) with whom the child regularly 
interacts. 
 Another possibility is to create a graphical user in-
terface for the conversational partner, perhaps on the 
same device that the child uses, which the partner could 
use to indicate what he/she wants to talk about.  This is 
probably technically easier, but does move away from 
the goal of having as natural a dialogue as possible. 
4.4 Pragmatics of interacting with others 
Currently, ?How was School today??? supports story-
telling between language-impaired children and adults 
who are the children?s parents, carers, teachers, and 
therapists.  But of course for normally developing child-
ren, many of their most important social interactions are 
with other children. 
 An interesting example here is the STANDUP sys-
tem, which was developed to help children who use 
7
AAC create and tell novel punning riddles. The study 
results suggested that children saved the jokes so that 
they could retell them to friends and family (Waller, 
Black et al 2009). Whilst the evidence is anecdotal, 
there did also appear to be a marked increase in joke 
telling by participants, both amongst their peers and 
with adults in the home environment. Hence STANDUP 
succeeded in supporting interaction with other children 
as well as with adults. 
 One of the key challenges in interacting with other 
children, and indeed with adults who are not formally 
involved in the care or teaching of the child, is to adapt 
the story to the interests of the recipients. In other 
words, a child?s parents and teachers will not insist on 
stories that are interesting to them, but other conversa-
tional partners will.  These conversational partners may 
also need additional information.  For example ?Jane 
came to take me to the OT room? makes more sense if 
the recipient knows that Jane is the occupational therap-
ist; parents and teachers already know this, but other 
people may need to be told this. Also if the conversa-
tional partner was present at an event, this should be 
acknowledged and indeed used in the story. For exam-
ple, "Did you really enjoy maths? I thought it was bor-
ing!? 
 In short, telling stories to peers and adults who do 
not know the child well requires adapting the story to 
the interests, knowledge, and involvement of the part-
ner; this is part of learning pragmatics. This is not some-
thing we are looking at currently, but it is something 
that we hope to look at in the future. 
4.5 Security and privacy issues 
We need to ensure that data about the children is private 
and secure.  Taken to its logical conclusion, our project 
would result in an intimate record of the child's life at 
school, home and beyond.  It is important that both the 
raw data and the generated content are under the control 
of the child and his/her guardians, with the child exer-
cising as much control as possible. This is especially 
important since children with learning difficulties are 
very vulnerable; there is potential for great harm if data 
about a child?s activities got into the hands of a mali-
cious outsider. 
 In a study on the software tool TalksBac, which 
supports personal narrative (Waller, Dennis et al 1997), 
privacy issues were coded along with stories. This al-
lowed the NLG process to decide the appropriateness of 
telling a story to a specific communication partner. 
Children in general do not care who they tell their sto-
ries to. Only when older children learn to distinguish 
which story is appropriate for a conversation partner. 
This process could be embedded into the prediction 
algorithm that presents stories for narration. Currently 
prediction on AAC devices only support character, word 
or phrase selection.   
 Another concern is information that is embarrassing 
or otherwise puts the child in a negative light; for exam-
ple, imagine a staff member entered the voice message 
"I refused to eat my lunch today".  We believe that the 
child should be free to delete such messages; she should 
never be forced to include material in a story that she 
does not want to include. 
 A related issue is whether we should allow stories 
generated for one child to use information acquired 
about another child.  In principle this is very valuable, 
for example it allows messages such as ?Jane didn?t eat 
her lunch today?. But is this acceptable from the pers-
pective of ensuring the privacy of data about Jane?s ac-
tivities? On the other hand, this is exactly the sort of 
thing that a normally developed child would say about a 
classmate. 
5 Conclusion 
In addition to having difficulty in communicating de-
sires and needs, language-impaired children also find it 
hard to participate in social linguistic interaction that 
would help create and build up friendships and other 
interpersonal relationships.  We believe that we can help 
these children participate in such interactions by giving 
them a tool that helps them tell a story about their day at 
school, by using an NLG system that has access to sen-
sor and other data about the child?s activities. We are 
still at an early stage in this work, but our initial proto-
type system has shown great potential to improve the 
quality of life of children with limited speech. Our cur-
rent work plans to explore this potential further while 
evaluating the efficacy of the system for four children 
with varying ability levels.  
Acknowledgements 
We would like to express our thanks to the children, 
their parents and staff and the special school where this 
project had its base. Without their valuable contribu-
tions and feedback this research would not have been 
possible. We would also like to thank DynaVox Sys-
tems Ltd for supplying the communication devices to 
run our system on. 
 This research was supported by the UK Engineering 
and Physical Sciences Research Council under grants 
EP/F067151/1, EP/F066880/1, EP/E011764/1, 
EP/H022376/1, and EP/H022570/1. 
8
References 
Agrawal, R. and Ramakrishnan, S. (2000) Privacy-
preserving data mining. ACM International 
Conference on Management of Data, pp. 439--450, 
Bart, H., V. Riny, et al (2008). LinguaBytes. 
Proceedings of the 7th international conference on 
Interaction design and children. Chicago, Illinois, 
ACM: 17-20. 
Beukelman, D. R. and P. Mirenda (2005). Augmentative 
and Alternative Communication: Management of 
Severe Communication Disorders in Children and 
Adults. Baltimore, Paul H. Brookes Publishing Co. 
Bruner, J. (1975). "From communication to language: A 
psychological perspective." Cognition 3: 255-289. 
Cheepen, C. (1988). The predictability of  informal 
conversation. Oxford, Printer Publishers Ltd. 
Clarke, H. H. and E. V. Clarke (1977). Psychology and 
Language. New York, Harcourt Brace Jovanovich. 
Dempster, M. (2008). Using natural language 
generation to encourage effective communication in 
nonspeaking people. Proceedings of Young 
Researchers Consortium, ICCHP'08. 
DeRuyter and Fried-Oken. (2010). "Context-sensitive 
messaging with RFID technology."   Retrieved 2010, 
April 10, from http://aac-rerc.psu.edu/index.php/projecttypes/list 
Gatt, A., F. Portet, et al (2009). "From Data to Text in 
the Neonatal Intensive Care Unit: Using NLG 
Technology for Decision Support and Information 
Management." AI Communications 22: 153-186. 
Goldberg, E., N. Driedger, et al (1994). "Using natural-
language processing to produce weather forecasts." 
IEEE Expert 9(2): 45-53. 
Harris, M. (2008). Building a Large-Scale Commer-cial 
NLG System for an EMR. Proc of INLG-2008. 
Hemsley, B., S. Balandin, et al (2007). "Family 
caregivers discuss roles and  needs in supporting 
adults with cerebral palsy and complex 
communication needs in the hospital setting." 
Journal of Developmental and Physical Disabilities 
19(2): 115-124. 
Higginbotham, D. J., H. Shane, et al (2007). "Access to 
AAC: Present, past, and future." Augmentative & 
Alternative Communication 23(3): 243-257. 
Labov, W. (1972). Language in the inner city: Studies in 
the Black English Vernacular. Philadelphia, 
University of Pennsylvania Press. 
Light, J., C. Binger, et al (1994). "Story Reading 
interactions between preschoolers who use AAC and 
their mothers." Augmentative and Alternative 
Communication 10: 255-268. 
Light, J. and K. Drager (2007). "AAC Technologies for 
Young Children with Complex Communication 
Needs: State of the Science and Future Research 
Directions." Augmentative and Alternative 
Communication 23(3): pp. 204 ? 216. 
Liu, X., A. Sen, et al (2008). A Software Client for Wi-
Fi Based Real-Time Location Tracking of Patients. 
Medical Imaging and Informatics. 
Berlin/Heidelberg, Springer. 4987/2008: 141-150. 
P?r?z, R. P. y. and M. Sharples (2004). "Three 
Computer-Based Models of StoryTelling: BRUTUS, 
MINSTREL, and MEXICA." Knowledge-Based 
Systems 17: 15-29. 
Peterson, C. and A. McCabe (1983). Developmental 
psycholinguistics: Three ways of looking at a child?s 
narrative. New York, Plenum. 
Reiter, E. (2007). An Architecture for Data-to-Text 
Systems. ENLG-2007. 
Reiter, E. and R. Dale (2000). Building Natural-
Language Generation Systems., Cambridge 
University Press. 
Reiter, E., S. Sripada, et al (2005). "Choosing Words in 
Computer-Generated Weather Forecasts." Artificial 
Intelligence 167: 137-169. 
Reiter, E., R. Turner, et al (2009). Using NLG to Help 
Language-Impaired Users Tell Stories and 
Participate in Social Dialogues. ENLG2009. Athens, 
Greece, Association for Computational Linguistics. 
Robertson, J. and J. Good (2005). "Story creation in 
virtual game worlds." Communications of the ACM 
48: 61-65. 
Schlosser, R. W. (2003). The Efficacy of Augmentative 
and Alternative Communication. San Diego, 
Elsevier Science. 
Soto, G., E. Hartmann, et al (2006). "Exploring the 
Elements of Narrative that Emerge in the 
Interactions between an 8-Year-Old Child who uses 
an AAC Device and her Teacher." Augmentative 
and Alternative Communication 22(4): pp. 231 - 
241. 
Sripada, S., E. Reiter, et al (2005). Evaluating an NLG 
System using Post-Edit Data: Lessons Learned. 
Proceedings of ENLG-2005, 10th European 
Workshop on Natural Language Generation, 
Aberdeen, Scotland. 
Waller, A. (2006). "Communication Access to 
Conversational Narrative." Topics in Language 
Disorders 26(3): 221-239. 
Waller, A., R. Black, et al (2009). "Evaluating the 
STANDUP Pun Generating Software with Children 
with Cerebral Palsy." ACM Trans. Access. Comput. 
1(3): 27. 
Waller, A., F. Dennis, et al (1997). "Evaluating the use 
of TalksBac, a predictive communication device for 
non-fluent aphasic adults." International Journal of 
Language and Communication Disorders 33: 45-70. 
Waller, A. and A. F. Newell (1997). "Towards a 
narrative based communication system." European 
Journal of Disorders of Communication 32: 289-
306. 
 
9
Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 148?149,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Workshop on Speech & Language Processing for Assistive Technologies  
Demo Session 
 
 
 
1  ?How was School today...?? A Prototype 
System that uses a Mobile Phone to Sup-
port Personal Narrative for Children 
with Complex Communication  
Rolf Black1, Annalu Waller1, Ehud Reiter2, Nava 
Tintarev2, Joseph Reddington2  
(1University of Dundee, 2University of Aberdeen) 
We will show a sensor based mobile phone proto-
type that supports personal narrative for children 
with complex communication needs. We will 
demonstrate how the phone is used to capture 
voice recordings and information about location, 
people and objects using RFID tags and QR stick-
ers. The challenging environment of a special 
school for prototype testing will be discussed using 
real life experiences. 
 
2 The PhonicStick: Interactive access to 
sounds for people with Complex Com-
munication Needs 
Ha Trinh, Annalu Waller, Rolf Black,  
James Bennet 
(University of Dundee) 
The PhonicStick is a new sound-based speech ge-
nerating device which enables nonspeaking indi-
viduals to access 42 English sounds and blend 
sounds into spoken words. The device can poten-
tially be used both as a literacy learning tool and an 
interactive communication aid for people with 
literacy difficulties. We will discuss how NLP 
technologies, such as speech synthesis and predic-
tive techniques, are utilised to improve the usabili-
ty of the device. 
 
 
 
 
 
 
 
3 Toby Churchill Ltd 
David Mason1, James Bennet2 
(1TLC, 2University of Dundee) 
Lightwriter? SL40.  We will show one of the new 
SL40 Connect mobile phone enabled Lightwri-
ters?. Lightwriters? are portable text-to-speech 
devices with dual displays, one facing the user and 
a second out-facing display allowing natural face-
to-face communication. They are operated by key-
board and use a fast prediction system based on the 
user?s own vocabulary. A certain degree of literacy 
is required to operate a Lightwriter?. 
Lightwriter? SL40 with PhonicStick. We will 
show an SL40 development unit that is running a 
version of Dundee University?s PhonicStick soft-
ware.  This project is an ongoing collaboration 
with the Assistive and Healthcare Technologies 
group in the School of Computing at Dundee.   
VIVOCA. We will be describing a project with 
partners including the University of Sheffield and 
the Assistive Technology team at Barnsley District 
General Hospital. The project is developing and 
evaluating a novel device for supporting the com-
munication needs of people with severe speech 
impairment that will address the limitations of the 
current technology.  It builds on previous work by 
this team (funded by Neat) which has established 
user requirements and identified the technical de-
velopments required.  The device, a Voice Input 
Voice Output Communication Aid (or VIVOCA) 
will take as input the unintelligible disordered 
speech of the users. 
 
 
 
 
 
 
 
 
148
4 SceneTalker: An Utterance-Based AAC 
System Prototype 
Timothy Walsh1, Jan Bedrosian2, Linda Hoag3, 
Kathleen F. McCoy1 
(1University of Delaware; 2Western Michigan Uni-
versity, 3Kansas State University) 
SceneTalker is a prototype utterance-based aug-
mentative and alternative communication system 
that uses scripts (and scenes within scripts) to or-
ganize prestored messages for highly routine goal-
oriented public situations (such as going to a res-
taurant). Many aspects of the system design are 
inspired by a series of experiments on using pres-
tored utterances in public goal-oriented situations 
when the prestored message did not exactly match 
what the user wanted to say. In addition to the 
script structures, we show system messages that 
are not anticipated to be perfect/complete for what 
is needed and strategies of use with stored messag-
es that anticipate the communication partner?s 
follow-up to utterances (adjacency pairs). 
5 SIGHT System 
Charles Greenbacker1, Seniz Demir3, Peng Wu1, 
Sandra Carberry1, Stephanie Elzer2, Kathleen F. 
McCoy1  
(1University of Delaware; 2Millersville University, 
4T?bitak Bilgem, Turkey) 
The SIGHT system is intended to give people with 
visual impairments access to information graphics 
(e.g., bar charts) found in popular media. The sys-
tem generates a textual summary that provides the 
chart?s overall intended message and additional 
salient propositions conveyed by the graphic. 
 
149
NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 19?27,
Montre?al, Canada, June 7?8, 2012. c?2012 Association for Computational Linguistics
Applying Prediction Techniques to Phoneme-based AAC Systems   Keith Vertanen Department of Computer  Science Montana Tech of the University of Montana keithv@keithv.com   
Ha Trinh, Annalu Waller,  Vicki L. Hanson School of Computing University of Dundee {hatrinh, awaller, vlh} @computing.dundee.ac.uk 
Per Ola Kristensson       School of Computer  Science University of St Andrews pok@st-andrews.ac.uk     Abstract 
It is well documented that people with severe speech and physical impairments (SSPI) often experience literacy difficulties, which hinder them from effectively using orthographic-based AAC systems for communication. To address this problem, phoneme-based AAC systems have been proposed, which enable users to access a set of spoken phonemes and combine phonemes into speech output. In this paper we investigate how prediction tech-niques can be applied to improve user perfor-mance of such systems. We have developed a phoneme-based prediction system, which sup-ports single phoneme prediction and pho-neme-based word prediction using statistical language models generated using a crowdsourced AAC-like corpus. We incorpo-rated our prediction system into a hypothetical 12-key reduced phoneme keyboard. A compu-tational experiment showed that our prediction system led to 56.3% average keystroke sav-ings. 1 Introduction Over the last forty years there has been an increas-ing number of high-tech AAC systems developed to provide communication support for individuals with severe speech and physical impairments (SSPI). Most of existing AAC systems can be clas-sified into two categories, namely graphic-based and orthographic-based systems. Graphic-based systems utilize symbols to encode a limited set of frequently used words and utterances, thereby sup-porting fast access to pre-stored items. However, there is a high cognitive overhead associated with 
learning the encoding methods of these systems, which can be problematic for many AAC users, especially those with intellectual disabilities. In addition, users of these systems are limited to pre-programmed items rather than being able to create novel words and messages spontaneously. In con-trast, orthographic-based AAC systems allow users to spell out their own messages. Prediction tech-niques, such as character or word prediction, are often applied to improve the usability and accessi-bility of these systems. However, these systems require users to master literacy skills, a well-documented problem for many children and adults with SSPI (Koppenhaver and Yoder, 1992). The question arises as to how AAC systems can be designed to enable pre-literate users with SSPI to generate novel words and messages in sponta-neous conversations. A potential solution for this question is to adopt a phoneme-to-speech genera-tion approach. This approach allows users to ac-cess a limited set of spoken phonemes and blend phonemes into speech output, thereby enabling them to create spontaneous messages without knowledge of orthographic spelling. This approach has been applied in several phoneme-based AAC systems to support communication (Glennen and DeCoste, 1997) and literacy learning (Black et al, 2008). It has also been utilized as an alternative typing method for people with spelling difficulties (Schroeder, 2005).  Despite such potential, phoneme-based AAC systems have been an under-researched topic. In particular, little work has been done on the applica-tion of Natural Language Processing (NLP) tech-niques to these systems. Thus, in this paper we investigate how prediction methods can be incor-porated into phoneme-based AAC systems to facil-
19
itate phoneme entry. We develop a basic phoneme-based prediction system, which provides predic-tions at both phoneme and word levels based on statistical language modeling techniques. We use a 6-gram phoneme mixture model and a 3-gram word mixture model trained on a large set of AAC-like data assembled from multiple sources, such as Twitter, Blog, and Usenet data. We take into con-sideration issues such as pronunciation variants and user accents in the design of our system. We performed a theoretical evaluation of our system on three different test sets using a simulated inter-face and report results of hit rate and potential key-stroke savings. Finally, we propose a number of further studies to extend the current work. 2 Background  2.1 Phoneme-based AAC Systems The idea of using phonemes in AAC systems was first commercially introduced by Phonic Ear in 1978 in the HandiVoice 110 (Creech, 2004; Glennen and DeCoste, 1997; Williams, 1995). The device provided users with direct access to a mixed vocabulary consisting of pre-programmed words, short phrases, letters, morphemes, and 45 pho-nemes. Users could generate synthetic speech from phoneme sequences using the Votrax speech syn-thesizer. Similar to the HandiVoice is the Finger Foniks, a handheld communicator developed by Words+ (Glennen and DeCoste, 1997). The device enables users to access prerecorded messages and a set of 36 phonemes from which they could gener-ate unlimited speech output. Neither of these de-vices offered any prediction features.  The PhonicStick?, a talking joystick (Black et al, 2008), is a phoneme-based AAC device devel-oped by researchers at the University of Dundee.  Unlike the HandiVoice and the Finger Foniks, the primary use of the PhonicStick? is to facilitate language play and phonics teaching for children with SSPI. The device allows users to access the 42 phonemes used in the Jolly Phonics literacy program (Lloyd, 1998) by moving the joystick along pre-defined paths. A prototype of the Phon-icStick?, using a subset of 6 Jolly Phonics? pho-nemes, has been evaluated with seven children without and with SSPI. Results of the evaluations demonstrated that the participants could create short words using the phonemes. However, some 
participants with poor hand function experienced significant difficulties in using the joystick to se-lect target phonemes (Black et al, 2008). This suggests that the PhonicStick? could benefit from prediction mechanisms to reduce the number of difficult joystick movements required for each phoneme entry. The phoneme-to-speech approach is not only ap-plied in dedicated AAC systems but also in alterna-tive typing interfaces for individuals with spelling difficulties. An example of such applications is the REACH Sound-It-Out Phonetic Keyboard? (Schroeder, 2005). This on-screen keyboard com-prises 40 phonemes and 4 phoneme combinations. It offers two types of prediction features, including phoneme prediction and word prediction. The pho-neme prediction feature uses a pronunciation dic-tionary to determine which phonemes cannot follow the currently selected phonemes. These phonemes are then removed from the keyboard, thereby facilitating users in visually scanning and identifying the next phoneme in the intended word. The word prediction feature also uses a dictionary to search for the most frequently used words that phonetically match the currently selected phoneme sequence. To our knowledge, this is the only cur-rently available system that provides phoneme-based predictions. However, these predictions use a simple dictionary-based prediction algorithm, which does not take into account contextual infor-mation (e.g. prior text). There has been little or no published research into how more advanced NLP techniques can be employed to improve the per-formance of phoneme-based predictions.  2.2 Prediction in AAC Systems Prediction techniques have been extensively uti-lized in many AAC systems to achieve keystroke savings and potential communication rate en-hancement (Garay-Victoria and Abascal, 2005). There are various prediction strategies that have been developed in these systems, of which the most commonly used are character prediction and word prediction. Character prediction anticipates next probable characters given the preceding char-acters. It is typically applied in reduced keyboards and scanning-based AAC systems to augment the scanning process (Lesher et al, 1998). Word pre-diction anticipates the word being entered on the basis of the previously selected characters and 
20
words, thereby saving the user the effort of enter-ing every character of a word.  Most existing prediction systems employ statisti-cal language modelling techniques to perform pre-diction tasks. Prediction accuracy generally increases with higher-order n-gram language mod-els. However, most systems are limited to 6-gram models for character prediction and 3-gram models for word prediction, as the gain from higher-order models is often small at the cost of considerably increased computational and storage resources. To further improve the prediction performance, a number of advanced language modelling tech-niques have been investigated, which take into ac-count additional information such as word recency (Swiffin et al, 1987), syntactic information (Hunnicutt and Caarlberger, 2001; Swiffin et al, 1987), semantic information (Li and Hirst, 2005), and topic modelling (Trnka et al, 2006). These techniques have the potential of improved key-stroke savings at the cost of increased computa-tional complexity. A fundamental issue of the statistical-based pre-diction approach is that its performance is heavily dependent on the size of the training corpus and the degree to which the corpus represents the do-main of use. Therefore, in the development of sta-tistical-based prediction for conversational AAC systems, it may be ideal to construct language models from a large corpus of transcribed conver-sations of real AAC users. However, such a corpus has been unavailable to date. To address this prob-lem, previous research has utilized corpora of tele-phone transcripts, such as the Switchboard corpus, and performed cleanup processing to make them a more appropriate approximate of AAC communi-cation (Lesher and Rinkus, 2002; Trnka et al, 2006). Vertanen and Kristensson (2011) have re-cently proposed a novel solution to this problem by creating a large corpus of fictional AAC messages. Using Amazon Mechanical Market, the researchers crowdsourced a small dataset of AAC-like mes-sages, which was then used to select a much larger set of AAC-like data from Twitter, Blog, and Use-net datasets. The language models trained on this AAC-like corpus were proved to outperform other models trained on telephone transcripts (Vertanen and Kristensson, 2011). 3 Phoneme-based Prediction System  
Although statistical-based predictions have been a well-studied topic, little or no research has been published on how well these predictions can be adapted to phoneme-based AAC systems. In this section, we describe our phoneme-based prediction system, which employs statistical language model-ing techniques to perform phoneme prediction and phoneme-based word prediction.  Phoneme predic-tion predicts probable next phonemes based on the previously entered phonemes. Word prediction predicts the word currently being entered based on the current phoneme prefix and prior words. 3.1 Phoneme Set  Unlike traditional orthographic-based AAC sys-tems that operate on a standard character set, dif-ferent phoneme-based systems tend to use slightly different phoneme sets. For our prediction system, we use the phoneme set from the Jolly Phonics, a systematic synthetic phonics program widely used in the UK for literacy teaching (Lloyd, 1998). The phoneme set, to be called the PHONICS set, con-sists of 42 phonemes, with 17 vowels and 25 con-sonants. By using a literacy-linked phoneme set, our prediction system can readily be integrated into both literacy learning tools (such as the Phon-icStick? joystick (Black et al, 2008)) and com-munication aids. Other systems that use different phoneme sets can also be easily adapted to utilize our prediction system by providing a phoneme mapping scheme between their phoneme sets and the PHONICS set. 3.2 Pronunciation Dictionary 3.2.1 The PHONICS Dictionary The development of phoneme-based predictions requires a pronunciation dictionary, which should be accent-specific as pronunciations may vary across different accents. There has been no dic-tionary to date that contains word pronunciations using the PHONICS set. To address this problem, we built our PHONICS pronunciation dictionary based on the Unisyn1 lexicon, as it provides facili-ties for generating dictionaries in different accents. The Unisyn uses the concept of key-symbols (i.e. meta-phonemes) to encode the characteristics of                                                             1 http://www.cstr.ed.ac.uk/projects/unisyn/ 2 http://aac.unl.edu/vocabulary.html, accessed 4 September 
21
multiple accents into a single base lexicon. Accent-specific rules can then be applied to the base lexi-con to produce pronunciations in a given accent.  To create the PHONICS dictionary, we first de-rived a lexicon in the Edinburgh accent from the base lexicon using a set of Perl scripts supplied with Unisyn. We also performed additional clean-up processing to remove unwanted information, such as stress and boundary markers. We then cre-ated a mapping function from the set of 61 pho-nemes and allophones used in the Edinburgh lexicon to the PHONICS set. As the PHONICS set only contains 42 phonemes, several allophones in the Edinburgh set were mapped to the same pho-nemes in the PHONICS set. This mapping function was then used to convert the Edinburgh lexicon to the PHONICS pronunciation dictionary. The re-sulting dictionary consists of 121,004 pronuncia-tion entries for 117,625 unique words. 3.2.2 The Schwa Phoneme An issue of the phoneme mapping is that the Edin-burgh set contains the schwa phoneme (denoted by the symbol ?@?), which cannot be mapped to any phonemes in the PHONICS set. The schwa, a re-duced form of full vowels in unstressed syllables, occurs in 41,539 entries in the PHONICS diction-ary. An example of a word containing the schwa phoneme is ?today? (/t @ d ai/). While the schwa is the most commonly used vowel sound in spoken English (Gimson and Cruttenden, 2001), it is not included in the Jolly Phonics teaching as it is a dif-ficult concept to understand for literacy learners at early stages.  The simplest solution for this issue would be to explicitly add the schwa phoneme into the PHONICS set in our prediction system. However, learning to use the schwa correctly can be chal-lenging for users with SSPI and literacy difficul-ties. Thus, we decided to support two modes in our system, namely the SCHWA_ON and the SCHWA_OFF modes. In the SCHWA_ON mode, the schwa phoneme is explicitly added to the PHONICS set, increasing the set to 43 phonemes. In the SCHWA_OFF mode, the schwa is not added into the PHONICS set and therefore is not offered to the users for selection. To deal with the absence of the schwa, we employed a basic auto-correction method. To search for a word given a phoneme sequence, we apply a limited set of schwa insertion 
and replacement rules (e.g. replacing vowels with schwas) to generate a set of alternative sequences. These sequences and the original sequence are then used to look up a list of matching words in the PHONICS dictionary. Once the user has selected a word from this list, the correct pronunciation of the selected word (which might include the schwas) would be used to replace the original phoneme se-quence in the currently selected phoneme string. This corrected phoneme string would then be input to the phoneme language model (described in Sec-tion 3.3.1) to predict probable next phonemes.  3.3 Phoneme Prediction We trained a 6-gram phoneme language model starting with training data from: ? Twitter messages collected via the free streaming API between December 2010 and July 2011. 36M sentences, 251M words. ? Blog posts from the ICWSM corpus (Burton et al, 2009). 25M sentences, 387M words. ? Usenet messages (Shaoul and Westbury, 2009). 123M sentences, 1847M words. We used the crowdsourced data from Vertanen and Kristensson (2011) to select AAC-like sentences using cross-entropy difference selection (Moore and Lewis, 2010). The selection process retained 6.9M, 1.6M, and 2.3M words of data from the Twitter, Blog and Usenet data sets respectively. We converted the words in the selected sentences to pronunciation strings using the PHONICS dic-tionary. Whenever we encountered a word with multiple pronunciations, we chose a pronunciation at random. If a sentence had a word not in the PHONICS dictionary, we dropped the entire train-ing sentence. We trained a 6-gram phoneme language model for each of the Twitter, Blog, and Usenet data sets. Estimation of unigrams used Witten-Bell discount-ing while all higher order n-grams used modified Kneser-Ney discounting with interpolation. We then created a mixture model via linear interpola-tion with mixture weights optimized on the crowdsourced development set from Vertanen and Kristensson (2011). The optimized mixture weights were: Twitter 0.54, Blog 0.25, and Usenet 0.21. Our final mixture model has 2.0M parameters and a compressed disk size of 14 MB. 
22
 Figure 1. Hit rates of the phoneme prediction for prediction list lengths 1-15 in the SWCHA_ON and SCHWA_OFF modes. Results on the SPECIALISTS, COMM, and SWITCHTEST test sets.  3.3.1 Hit Rate We evaluated the accuracy of our phoneme predic-tion using hit rate. Hit rate (HR) is defined as the percentage of times that the intended phonemes appear in the prediction list: 
HR = 
? 
Number of times the
phoneme is predicted
Number of phonemes
?100% We computed the hit rates for prediction lists of lengths 1-15 in both SCHWA_ON and SCHWA_OFF modes. The results of this evalua-tion would help inform the decision of the number of predicted items to be presented to the users, which is a key usability factor of prediction sys-tems.  We evaluated the hit rates on the following test sets: ? SPECIALISTS: A collection of context specific conversational phrases recom-mended by AAC professionals2. 966 sen-tences, 3814 words. Out-of-vocabulary (OOV) rate: 0.05%. ? COMM: A collection of sentences written by college students in response to 10 hypo-thetical communication situations (Venkatagiri, 1999). 251 sentences, 1789 words. OOV rate: 0.3%. ? SWITCHTEST: Three telephone tran-scripts taken from the Switchboard corpus, used in Trnka et al (2009). 59 sentences, 508 words. OOV rate: 0.4%. These three test sets are used throughout this pa-per. For each sentence in the test sets, we generat-                                                            2 http://aac.unl.edu/vocabulary.html, accessed 4 September 2011 
ed its pronunciation string using the PHONICS dictionary. During this generation, any time we encountered a word with multiple pronunciations, we chose a pronunciation at random. We manually added pronunciations for OOV words. The gener-ated pronunciations were used to calculate the hit rates in the SCHWA_ON mode. We then created a ?non-schwa? version of each pronunciation string, in which we removed all schwa occurrences by either deleting them or replacing them with appro-priate vowels in the PHONICS set. The ?non-schwa? pronunciations were used to calculate the hit rates in the SCHWA_OFF mode. As shown in Figure 1, the hit rate improved as the prediction list length (L) increased in both the SCHWA_OFF and SCHWA_ON modes for all the three test sets. For most L values, the system per-formed the best on the SPECIALISTS test set and the worst on the SWITCHTEST set. At L=1, the average hit rates for the three test sets were 47.1% in the SCHWA_OFF mode and 50.1% in the SWITCH_ON mode. At L=5 (which is the length usually offered in prediction systems), the average hit rate increased to 76.2% in the SCHWA_OFF mode and 78.4% in the SCHWA_ON mode. At L=15, the system reached high average hit rates of 93.6% in the SCHWA_OFF mode and 94.3% in the SCHWA_ON mode. The SCHWA_ON mode achieved higher hit rates than the SCHWA_OFF mode for all L values. However, the hit rate differences between these two modes tended to diminish as L increased. At L=1, the average difference for the three test sets was 3.0%. At L=5, the average difference reduced to 2.2%. At L=15, the average difference was very small, at 0.7%. 
23
 Figure 2. Hit rates of the word prediction for prediction list lengths 1-15 in the SWCHA_ON and SCHWA_OFF modes for 1-phoneme and 2-phoneme prefixes. Results on the SPECIALISTS, COMM, and SWITCHTEST test sets. 3.4 Phoneme-based Word Prediction We used a publicly available 3-gram word mixture model3, which was created from three 3-gram models trained on AAC-like data from Twitter, Blog, and Usenet (Vertanen and Kristensson, 2011). Although a 4-gram model trained on the same datasets is also available, it was not used in our system as it has been shown to only slightly outperform the 3-gram model at the cost of a much bigger model size (Vertanen and Kristensson, 2011). Our aim is to keep our prediction system?s size reasonably small, thereby allowing it to be easily integrated into devices with limited re-sources, such as mobile devices. To perform word prediction given a phoneme prefix, we first search for a set of matching words in the PHONICS dictionary. In the SCHWA_OFF mode, the phoneme prefix is input to the auto-correction function to generate alternative prefixes, which are then used to look up matching words in the dictionary. If there is no matching word, an unknown word (denoted as <unk>) is returned. The matching words are then input to the word model to calculate their probabilities based on up to two prior words. 3.4.1 Hit Rate We computed the hit rate (HR) of word prediction for prediction list lengths 1-15 in two conditions: (1) after the first phoneme is entered, (2) after the first two phonemes are entered: 
                                                            3 http://www.aactext.org/imagine/lm_mix_top3_3gram_abs0.0.arpa.gz 
HR =
  
? 
Number of times the word is predicted
Number of words
?100% Figure 2 shows the hit rates of word prediction in the SCHWA_OFF and SCHWA_ON modes on the three test sets. As expected, the hit rates improved as the prediction list length (L) increased. Table 1 summarizes the average hit rates for several list lengths for 1-phoneme and 2-phoneme prefixes. At L=5, the average hit rates were 92.5% in the SCHWA_OFF mode and 93.2% in the SCHWA_ON mode after the first two phonemes are entered. This means that in most cases, the in-tended word is predicted after two keystrokes. The SCHWA_ON mode achieved higher hit rates than the SCHWA_OFF mode in all cases. However, the hit rate differences between these two modes were very small (<1%), which implies that our auto-correction mechanism was effective.  
L SCHWA_OFF SCHWA_ON 1-phoneme 2-phoneme 1-phoneme 2-phoneme 1 55.6% 80.4% 55.9% 80.8% 5 79.0% 92.5% 79.7% 93.2% 10 86.0% 94.5% 86.2% 95.0% 15 88.0% 95.1% 88.3% 95.8%  Table 1. Average hit rates of word prediction. 4 Theoretical Evaluation AAC users with physical impairments often expe-rience difficulties in accessing a large number of keys on conventional full-sized keyboards.  To address this problem, previous research has pro-posed the use of reduced keyboards (i.e. keyboards on which each key is assigned a group of charac-
24
ters, such as the 12-key mobile phone keyboard) (Arnott and Javed, 1992; Kushler, 1998). Character prediction and word prediction can be applied to these keyboards to disambiguate characters on each key. We adopted this idea by creating a hypo-thetical 12-key phoneme keyboard and evaluated the benefits of incorporating phoneme prediction and word prediction into the keyboard. 4.1 Phoneme-based Predictive Interface Our 12-key phoneme keyboard contains 8 pho-neme keys, which represent 3 vowel groups and 5 consonant groups. These groups, introduced in the PhonicStick? talking joystick (Black et al, 2008; Lindstr?m and Peronius, 2010), are formed accord-ing to the manner of articulation of the phonemes (see Figure 3a). Each key represents three to seven phonemes; the schwa phoneme is excluded. The phonemes on each key are initially arranged ac-cording to the unigram probabilities estimated by our phoneme language model.   
 Figure 3. Phoneme-based reduced keyboard.  The keyboard provides two phoneme entry modes, namely the MULTITAP and the PREDICTIVE modes. In the MULTITAP mode, the user enters a phoneme by pressing a corre-sponding key repeatedly until the intended pho-neme appears (e.g. pressing the ?Unvoiced Plosives? key 3 times to enter /p/). In the PREDICTIVE mode, the keyboard utilizes our prediction system in its SCHWA_OFF mode to predict probable next phonemes and words. Each time the user presses a key the phoneme prediction is applied to guess which of the possible phonemes on the pressed key is actually the user?s intended phoneme. If the prediction is incorrect, the user can repeatedly press the NEXT key until the correct 
phoneme is selected. After each phoneme selec-tion, we present a list of up to 5 predicted words. We only offer word predictions after the first pho-neme of a new word is entered. If the intended word appears in the prediction list, we assume it takes one keystroke for the user to add the word and a following space to the current sentence (this can be implemented using automatic scanning (Glennen and DeCoste, 1997)). 4.2 Results We evaluated our prediction system using two commonly used metrics: keystroke savings and keystrokes per character. 4.2.1 Keystroke Savings Keystroke Savings (KS) is defined as the percent-age of keystrokes that the user saves by using pre-diction methods compared to using the MULTITAP method: 
  
? 
KS = 1?
PREDICTION
Keystrokes
MULTITAP
Keystrokes
# 
$ 
% 
% 
& 
' 
( 
( 
?100% We computed KS on the three test sets for three methods: (1) only phoneme prediction (PP), (2) only word prediction (WP), (3) combined phoneme prediction and word prediction (PP+WP) (i.e. the PREDICTIVE mode).  As shown in Figure 4, a combined phoneme and word prediction method performed the best with an average keystroke savings of 56.3%. Using only word prediction led to a 46.4% average KS while using only phoneme prediction resulted in 29.9% average KS.  
 Figure 4. Keystroke Savings (KS) for prediction meth-ods on three test sets. 
25
4.2.2 Keystrokes Per Character Keystrokes per character (KSPC) is defined as the average number of keystrokes required to produce a character in the test set: 
  
? 
KSPC=
Keystrokes
Number of characters (including spaces) The evaluation of KSPC allows us to compare our keyboard with existing character-based reduced keyboards. We computed the KSPC for four meth-ods: (1) MULTITAP, (2) PP, (3) WP, (4) PP+WP. For comparison, we also calculated the KSPC for a standard 12-key mobile phone alphabetic keyboard (Figure 3b), which uses the character-based multi-tap method for text entry.  As shown in Figure 5, our frequency-based pho-neme keyboard outperformed the standard mobile phone keyboard even when no prediction methods are applied (i.e. in the MULTITAP mode) (see Figure 5). At an average KSPC of 1.568, our key-board required 19.1% fewer keystrokes per charac-ter than the mobile phone multitap keyboard (KSPC=1.937). There are two reasons that might explain this result. First, on average one phoneme represents more than one character (in our diction-ary, the character/phoneme ratio is 1.208). Second, our keyboard?s phonemes were initially ordered by the unigram frequencies.  When applying only phoneme prediction, the av-erage KSPC decreased to 1.100, which closely ap-proaches the KSPC of a QWERTY keyboard (KSPC=1). The KSPC further reduced to 0.841 with solely word prediction and 0.685 with com-bined phoneme and word prediction.  
 Figure 5. Keystrokes Per Character (KSPC) for different text entry methods on three test sets. 5 Conclusions and Future Work In this paper we have described how statistical lan-guage modeling techniques can be used to provide 
phoneme prediction and word prediction for pho-neme-based AAC systems. Using hit rate meas-urement we demonstrated how the prediction accuracy improved as the prediction list length in-creased. However, a large prediction list might re-sult in an increased time and cognitive workload required from the user to scan the list and select the desired item. Therefore, hit rate data need to be combined with empirical experiments with real users in order to determine an appropriate predic-tion list length.  We evaluated our prediction system on a 12-key phoneme keyboard, in which phonemes are grouped based on the manner of articulation and ordered using our phoneme unigram frequencies. We showed that we could achieve a potential key-stroke savings of 56.3% by applying a combined phoneme and word prediction to our keyboard. Using word prediction alone proved to be more effective than using phoneme prediction alone, in terms of keystroke savings.  We plan to take this work forward by exploring two complementary research directions.  First, we plan to conduct empirical experiments with a group of AAC users to evaluate the usability of our phoneme predictive keyboard. We are inter-ested in finding out if the potential keystroke sav-ings can be translated into an actual keystroke savings and communication rate enhancement. In addition, we will analyze user?s errors in phoneme selection, which can be used to produce a more advanced auto-correction method.  Second, we will explore how our prediction sys-tem can be integrated into existing phoneme-based AAC systems rather than our reduced keyboard. In particular, we will focus on the REACH Sound-It-Out Phonetic Keyboard? (Schroeder, 2005), which uses a different phoneme set than our PHONICS set, and the PhonicStick? (Black et al, 2008), which has the same phoneme groupings as our keyboard. Finally, we will investigate how NLP techniques, such as the joint-multigram model (Bisani and Ney, 2008), can be applied to automatically gener-ate orthographic spellings for OOV words. Our current system simply uses a <unk> placeholder for OOV words. While these words can still be spoken out by synthesizing their phoneme strings, it is potentially more beneficial to suggest actual spellings than to use such a placeholder. 
26
References  John L. Arnott and Muhammad Y. Javed (1992). Probabilistic character disambiguation for reduced keyboard using small text samples. Augmentative and Alternative Communication, 8(3), 215-223. Maximilian Bisani and Hermann Ney (2008). Joint-sequence models for grapheme-to-phoneme conversion. Speech Communication, 50(5), 434-451. Rolf Black, Annalu Waller, Graham Pullin, and Eric Abel (2008). Introducing the PhonicStick: Preliminary evaluation with seven children. Paper presented at the 13th Biennial Conference of the International Society for Augmentative and Alternative Communication Montreal, Canada. Kevin Burton, Akashay Java, and Ian Soboroff (2009). The ICWSM 2009 Spinn3r dataset. Paper presented at the 3rd Annual Conference on Weblog and Social Media. Rick Creech (2004). Rick Creech, 2004 Edwin and Esther Prentke AAC Distinguished Lecturer, from http://www.aacinstitute.org/Resources/PrentkeLecture/2004/RickCreech.html Nestor Garay-Victoria and Julio Abascal (2005). Text prediction systems: a survey. Universal Access in the Information Society, 4, 188-203. Alfred. C. Gimson and Alan Cruttenden (2001). Gimson's Pronunciation of English: Hodder Arnold. Sharon L. Glennen and Denise C. DeCoste (1997). The Handbook of Augmentative and Alternative Communication: Thomson Delmar Learning. Sheri Hunnicutt and Johan Caarlberger (2001). Improving Word prediction using markov models and heuristic methods. Augmentative and Alternative Communication, 17(4), 255-264. David A. Koppenhaver and David E. Yoder, D (1992). Literacy issues in persons with severe speech and physical impairments. In R. Gaylord-Ross, Ed. (Ed.), Issues and research in special education (Vol. 2, pp. 156-201). NY: Teachers College Press, Columbia University, New York. Cliff Kushler (1998). AAC: Using a reduced keyboard. Paper presented at the Technology and Persons with Disabilities Conference, Los Angeles, USA. Gregory W. Lesher and Gerald J. Rinkus (2002). Domain-specific word prediction for augmentatve communication. Paper presented at the The RESNA '02 Annual Conference. Gregory W. Lesher, Bryan J. Moulton, and Jeffrey D. Higginbotham (1998). Techniques for augmenting scanning communication. Augmentative and Alternative Communication, 14, 81-101. Jianhua Li and Graeme Hirst (2005). Semantic knowledge in word completion. Paper presented at the 7th International ACM SIGACCESS Conference on Computers and Accessibility.  
Nina Lindstr?m and Irmeli Peronius (2010). The PhonicStick nursery study: Can phonological awareness be initiated by using a speaking joystick. Uppsala University. Susan M. Lloyd (1998). The Phonics Handbook. Chigwell: Jolly Learning Ltd. Robert C. Moore and William Lewis (2010). Intelligent selection of language model training data. Paper presented at the 48th Annual Meeting of the Association of Computational Linguistics. James E. Schroeder (2005). Improved spelling for persons with learning disabilities. Paper presented at the The 20th Annual International Conference on Technology and Persons with Disabilities, California, USA. Cyrus Shaoul and Chris Westbury (2009). A USENET corpus (2005-2009). University of Alberta, Canada. Andrew L. Swiffin, John L. Arnott,  and Alan Newell (1987). The use of syntax in a predictive communication aid for the physically handicapped. Paper presented at the RESNA 10th Annual Conference, San Jose, California. Andrew L. Swiffin, John L. Arnott, Andrian J. Pickering, and Alan Newell (1987). Adaptive and predictive techniques in a communication prosthesis. Augmentative and Alternative Communication, 3(4), 181-191. Keith Trnka, Debra Yarrington, Kathleen F. McCoy, and Christopher Pennington (2006). Topic modeling in fringe word prediction for AAC. Paper presented at the 11th International Conference on Intelligent User Interfaces.  Keith Trnka, John McCaw, Debra Yarrington, Kathleen F. McCoy, Christopher Pennington (2009). User interaction with word prediction: The effects of prediction quality. ACM Transactions on Accessible Computing, 1(3), 1-34. Horabail S. Venkatagiri (1999). Efficient keyboard layouts for sequential access in augmentative and alternative communication. Augmentative and Alternative Communication, 15(2), 126-134. Keith Vertanen and Per Ola Kristensson (2011). The imagination of Crowds: Conversational AAC language modelling using crowdsourcing and large data sources. Paper presented at the International Conference on Empirical Methods in Natural Language Processing (EMNLP), Edinburgh, United Kingdom. Michael B. Williams (1995). Transitions and transformations. Paper presented at the 9th Annual Minspeak Conference, Wooster, OH.  
27
