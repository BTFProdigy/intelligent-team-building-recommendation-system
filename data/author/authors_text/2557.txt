NAACL HLT Demonstration Program, pages 1?2,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
Demonstration of PLOW: A Dialogue System for One-Shot Task 
Learning
James Allen, Nathanael Chambers, George Ferguson
* 
, Lucian Galescu, Hyuckchul Jung, 
Mary Swift
* 
and William Taysom
Florida Institute for Human and Machine Cognition, Pensacola, FL 32502
*Computer Science Department, University of Rochester, Rochester, NY 14627
Introduction
We describe a system that can learn new 
procedure models effectively from one 
demonstration by the user. Previous work to learn 
tasks through observing a demonstration (e.g., 
Lent & Laird, 2001) has required observing many 
examples of the same task. One-shot learning of 
tasks presents a significant challenge because the 
observed sequence is inherently incomplete ? the 
user only performs the steps required for the 
current situation.  Furthermore, their decision-
making processes, which reflect the control 
structures in the procedure, are not revealed. 
We will demonstrate a system called PLOW 
(Procedural Learning on the Web) that learns task 
knowledge through observation accompanied by a 
natural language ?play-by-play?. Natural 
language (NL) alleviates many task learning 
problems by identifying (i) a useful level of 
abstraction of observed actions; (ii) parameter 
dependencies; (iii) hierarchical structure; (iv) 
semantic relationships between the task and the 
items involved in the actions; and (v) control 
constructs not otherwise observable. Various 
specialized reasoning modules in the system 
communicate and collaborate with each other to 
interpret the user?s intentions, build a task model 
based on the interpretation, and check consistency 
between the learned task and prior knowledge.
The play-by-play approach in NL enables our 
task learning system to build a task with high-
level constructs that are not inferable from 
observed actions alone. In addition to the 
knowledge about task structure, NL also provides 
critical information to transform the observed 
actions into more robust and reliable executable 
forms. Our system learns how to find objects used 
in the task, unifying the linguistic information of 
the objects with the semantic representations of 
the user?s NL descriptions about them.  The 
objects can then be reliably found in dynamic and 
complex environments. See Jung et al(2006) and 
Chambers et al(2006) for more details on the 
PLOW system.
The PLOW System
PLOW learns tasks executable on the web 
involving actions such as navigation, information 
extraction and form filling, and can learn iterative 
steps that operate over lists of objects on pages. 
Figure 1 shows the system during learning a task 
to find publications for a specified author. Upper 
left is the Mozilla browser, in which the user can 
demonstrate action and the system can execute 
actions in a mixed-initiative fashion. The user 
may speak or type to the system (SR output is 
lower right), and PLOW combines knowledge 
from the language and the demonstrated actions to 
produce a parameterized procedure (described in 
generated natural language in the upper right 
corner). Figure 2 shows a complete training 
dialogue in which PLOW learns how to find 
article titles. To save space, simple 
acknowledgments by the system are not shown.
Figure 1: PLOW learning a task
1
Evaluation
The PLOW system was evaluated by independent 
evaluators who considered four task learning 
systems developed in the CALO project. There 
were 16 human subjects who received training on 
each of the systems and who worked through a 
number of successful scripted training sessions 
with each. They were then given ten new 
problems, ranging from slight variations to 
problems they had seen to problems that were 
substantially new. They were free to choose which 
problems to work on and which system to use and 
the resulting tasks learned were tested with 
different settings of the parameters and scored out 
of a total of 4 points based on a complex 
predefined evaluation criteria (not known to the 
developers). The PLOW system did well in the 
test, not only receiving the highest average score 
on tasks learned by a system (figure 3) but also 
was strongly preferred by the users and selected 
more than half the time (figure 4). 
The Demonstration
If we are allowed a presentation we will 
demonstrate PLOW live on a task selected by the 
audience. In addition, we would like to have the 
system available for an extended period of time 
during the conference so that attendees can spend 
time using the system to teach it simple tasks. The 
system runs on a laptop and all that is needed for 
a demo is internet access.
Acknowledgements & References
This work was supported by DARPA grant NBCH-
D-03-0010 under a subcontract from SRI International, 
ONR grant N000140510314, and NSF grant5-28096.
Chambers, N. et al (2006). Using Semantics to Identify 
Web Objects. Proceedings AAAI.
Jung, H., J. Allen, et al (2006). One-Shot Proce-dure 
Learning from Instruction and Observation. FLAIRS, 
Melbourne, FL. 
Lent, M. and Laird, J. (2001)  Learning Procedural 
Knowledge through Observation,  Proc.  of the Intl 
Conf. on Knowledge Capture.
   Figure 3: Average score (out of 4)   Figure 4: User preference for systems (55 trials)
U: Let me show you how to find someone's publications 
U: Go to this webpage 
Navigates to http://scholar.google.com
U: Click on the advanced search link 
Clicks Advanced Scholar Search 
U: Put the author's name here 
Types the name in the Author text field 
U: Click the search button 
Clicks on the Search Scholar button 
U: OK here is the list of results 
Selects the table on page
PLOW: Can you check that I understand the data. 
PLOW: puts up a new window with the results. 
U: OK this is good.
U: this is the article title 
Highlights a title 
PLOW: attempts to find all titles and displays them
U: This is wrong.
PLOW OK
U: This is another article title 
Highlights another correct title 
PLOW: recomputes all the titles and displays them
Switches back to Scholar window
U: OK click next to get more results
Clicks the Next link 
PLOW: When should I stop searching?
U: Get thirty results 
PLOW: OK. I will do that
PLOW: iterates through pages until it gets 30 results
PLOW: I?m done.
U: We're done with the demonstration 
U: Let's practice
PLOW: what is the name of the author?
Enters a new name
PLOW executes the procedure successfully!!
Figure 2: Learning to find titles of an author?s publications (U = user)
2
Proceedings of NAACL HLT 2009: Short Papers, pages 129?132,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Extending Pronunciation Lexicons via Non-phonemic Respellings     Lucian Galescu  Florida Institute for Human and Machine Cognition  40 S Alcaniz St., Pensacola FL 32502, USA lgalescu@ihmc.us    Abstract This paper describes work in progress towards using non-phonemic respellings as an addi-tional source of information besides spelling in the process of extending pronunciation lexicons for speech recognition and text-to-speech systems. Preliminary experimental data indicates that the approach is likely to be successful. The major benefit of the approach is that it makes extending pronunciation lexi-cons accessible to average users. 1 Introduction Speech recognition (SR) systems use pronuncia-tion lexicons to map words into the phoneme-like units used for acoustic modeling. Text-to-speech (TTS) systems also make use of pronunciation lexicons, both internally and as ?exception diction-aries? meant to override the systems? internal grapheme-to-phoneme (G2P) convertors. There are many situations where users might want to aug-ment the pronunciation lexicons of SR and TTS systems, ranging from minor fixes, such as adding a few new words or alternate pronunciations for existing words, to significant development efforts, such as adapting a speech system to a specialized domain, or developing speech systems for new languages by bootstrapping from small amounts of data (Kominek et al, 2008). Unfortunately, extending the pronunciation lexi-con (PL) is not an easy task. Getting expert help is usually impractical, yet users have little or no sup-port if they want to tackle the job themselves. Where available, the user has to either know how to transcribe a word?s pronunciation into the appli-cation?s underlying phone set, or, in rare cases, use pronunciation-by-orthography, whereby word pro-nunciations are respelled using other words (e.g., ?Thailand? is pronounced like ?tie land?). The former method requires a certain skill that is clearly beyond the capabilities of the average user; the latter is extremely limited in scope.  What is needed is a method that would make it easy for the users to specify pronunciations them-selves, without requiring them to be or become expert phoneticians. In this paper we will argue ? 
with backing from some preliminary experiments ? that non-phonemic respelllings might be an acces-sible intermediate representation that will allow speech systems to learn pronunciations directly from user input faster and more accurately. 2 Extending pronunciation lexicons Automatic G2P conversion seems the ideal tool to help users with PL expansion. The user would be shown a ranked list of automatically derived pro-nunciations and would have to pick the correct one. To make such a system more user-friendly, a synthesized waveform could also be presented (Davel and Barnard, 2004; also Kominek et al, 2008). This approach has a major drawback: if the system?s choices are all wrong ? which is, in fact, to be expected, if the number of choices is small ? the user would have to provide their own pronun-ciation by using the system?s phonetic alphabet. In our opinion this precludes the approach from being used by non-specialists. Other systems try to learn pronunciations only from user-provided audio samples, via speech rec-ognition/alignment (Beaufays et al, 2003; see also Bansal et al, 2009 and Chung et al, 2004). In such systems G2P conversion may be used to constrain choices, thereby overcoming the notoriously poor phone-level recognition performance. For example, Beaufays et al (2003) focused on a directory assis-tance SR task, with many out-of-vocabulary proper names. Their procedure works by initializing a hy-pothesis by G2P conversion, and thereafter refin-ing it with hypotheses from the joint alignment of phone lattices obtained from audio samples and the current best hypothesis. Several transformation rules were employed to expand the search space of alternative pronunciations.  While audio-based pronunciation learning may appear to be more user-friendly, it actually suffers from being a slow approach, with many audio samples being needed to achieve reasonable per-formance (the studies cited used up to 15 samples). It is also unclear whether the pronunciations learned are in fact correct, since the approach was mostly used to help increase the performance of a SR system. The SR performance improvements (ranging from 40% to 74%) must be due to better 
129
pronunciations, but we are not aware of the exis-tence of any correctness evaluations.  3 Non-phonemic respellings The method proposed here is aimed at allowing users to directly indicate the pronunciation of a word via non-phonemic respellings (NPRs). With NPRs, a word?s pronunciation is represented ac-cording to the ordinary spelling rules of English, without attempting to represent each sound with a unique symbol. For example, the pronunciation of the word phoneme could be indicated as \FO-neem\, where capitalization indicates stress (boldface, un-derlining, and the apostrophe are also used as stress markers). It is often possible to come up with different respellings, and, indeed, systematicity is not a goal here; rather, the goal is to convey infor-mation about pronunciation using familiar spell-ing-to-sound rules, with no special training or tables of unfamiliar symbols. NPRs are used to indicate the pronunciation of unfamiliar or difficult words by news organizations (mostly for foreign names), the United States Phar-macopoeia (for drug names), as well as countless interest groups (astronomy, horticulture, philoso-phy, etc.). Lately, Merriam-Webster Online1 has started using NPRs in their popular Word of the Day2 feature. Here is a recent example:  girandole ? \JEER-un-dohl\ While NPRs seem to be used by a fairly wide range of audiences, we mustn?t assume that most people are familiar with them. What we do know, however, is that people can learn new pronuncia-tions faster and with fewer errors from NPRs than from phonemic transcriptions and this holds true whether they are linguistically-trained or not (Fraser, 1997). We contend, based on preliminary observations, that not only are NPRs easily de-coded, but people seem to be able to produce rela-tively accurate NPRs, too.  4 Our Approach Our vision is that speech applications would em-ploy user-provided NPRs as an additional source of information besides orthography, and use dedi-cated NPR-to-pronunciation (N2P) models to de-rive hypotheses about the correct pronunciation.  However, before embarking on this project, we ought to answer three questions: 1. Is generic knowledge about grapheme-to-phoneme mappings in English sufficient to de-code pronunciation respellings? Or, in techni-                                                 1 http://www.merriam-webster.com 2 http://www.merriam-webster.com/cgi-bin/mwwod.pl 
cal terms, are generic G2P models going to work as N2P models? 2. Are pronunciation respellings useful in obtain-ing the correct pronunciation of a word beyond the capabilities of a G2P converter?  3. Since we don?t require that average users learn a respelling system, are novice users able to generate useful respellings? In the following we try to answer experimentally the technical counterparts of the first two ques-tions, and report results of a small study designed to answer the third one. 4.1 Data and models We collected a corpus of 2730 words with a to-tal of 2847 NPR transcriptions (some words have multiple NPRs) from National Cancer Institute?s Dictionary of Cancer Terms.3 The dictionary con-tains over 4000 medical terms. Here are a couple of entries (without the definitions): lactoferrin  (LAK-toh-fayr-in) valproic acid  (val-PROH-ik A-sid) Of the 2730 words, 1183 appear in the CMU dictionary (Weide, 1998) ? we?ll call this the ID set. Of note, about 180 words were not truly in-dictionary; for example, Versed (a drug brand name), pronounced \V ER0 S EH1 D\, is different from the in-dictionary word versed, pronounced \V ER1 S T\. We manually aligned all NPRs in the ID set with the phonetic transcriptions. We transcribed phonetically another 928 of the words ? we?ll call this the OOD set ? not found in the CMU dictionary; we verified the phonetic tran-scriptions against the Merriam-Webster Online Medical Dictionary and the New Oxford American Dictionary (McKean, 2005).  For G2P conversion we used a joint 4-gram model (Galescu, 2001) trained on automatic alignments for all entries in the CMU dictionary. We note that joint n-gram models seem to be among the best G2P models available (Polyakova and Bonafonte, 2006; Bisani and Ney, 2008). 4.2 Adequacy of generic G2P models To answer the first question above, we looked at whether the generic joint 4-gram G2P model is adequate for converting NPRs into phonemes.  At first, it appeared that the answer would be negative. We found out that NPRs use GP corre-spondences that do not exist or are extremely rare in the CMU dictionary. For example, the <[ih], \IH\> correspondence is very infrequent in the                                                  3 http://www.cancer.gov 
130
CMU dictionary (and appears only in proper names, e.g., Stihl), but is very frequently used in NPRs. Therefore, for the [ih] grapheme the G2P converter prefers  \IH HH\ to the intended \IH\. Similar problems happen because of the way some diphones are transcribed. Two other peculiarities of the transcription accounted for other errors: a) always preferring /S/ in plurals where /Z/ would be required, and b) using [ayr] to transcribe \EH R\, which uses the very rare <[ay], \EH\> mapping. These deviations from ordinary GP correspon-dences occur with regularity and therefore we were able to fix them with four post-processing rules. We are confident that these rules capture specific choices made during the compilation of the Dic-tionary of Cancer Terms, to reduce ambiguity, and increase consistency, with the expectation that readers would learn to make the correct phonological choices when reading the respellings. Another issue was that the set of GP mappings used in NPRs was extremely small (111) compared to the GP correspondence set obtained automati-cally from the CMU dictionary (1130, many of them occurring only in proper names). However, it turns out that 47524 entries in the CMU dictionary (about 45%) use exclusively GP mappings found in NPRs! This suggests that, while the generic G2P model may not be adequate for the N2P task, the GP mappings used in NPRs are sufficiently com-mon that a more adequate N2P model could be built from generic dictionary entries by selecting only relevant entries for training. Unfortunately we don?t have a full account of all ?exotic? entries in the CMU dictionary, but we expect that by simply removing from the training data the approximately 54K known proper names will yield a reasonable starting point for building N2P models. 4.3 NPR-to-pronunciation conversion To assess the contribution of NPR information to pronunciation prediction, we compare the perform-ance of spelling-to-pronunciation conversion (the baseline) to that of NPR-to-pronunciation conver-sion, as well as to that of a combined spelling and NPR-based conversion, which is our end goal. For the N2P task, we trained two joint 4-gram models: one based on the aligned NPRs, and a sec-ond based on the 47K CMU dictionary entries that use only GP mappings found in NPRs. Then, we interpolated the two models to obtain an NPR-specific model (the weights were not optimized for these experiments), which we?ll call the N2P model. The combined, spelling and NPR-based model was an oracle combination of the G2P and the N2P model. Phone error rates (PER) and word error rates (WER) for both the ID set and the OOD set are shown in Figures 1 and 2, respectively. We 
obtained n-best pronunciations with n from 1 to 10 for the three models considered.  As expected, G2P performance is very good on the ID set, since the test data was used in training the G2P model. Significantly, even though the N2P model is not as good itself, the combined model shows marked error rate reductions: for the top hypothesis it cuts the PER by over 57%, and the WER by over 47% when compared to the G2P per-formance on spelling alone. Since the OOD set represents data unseen by ei-ther the spelling-based model or the NPR-based model, all models? performance is severely de-graded compared to that on the ID set. But here we see that NPR-based pronunciations are already bet-ter than spelling-based ones. For the top hypothe-sis, compared to the performance of the G2P model alone, the N2P model shows almost 19% better PER, and almost 5% better WER, whereas the combined model achieves 49% better PER and close to 31% better WER.  4.4 User-generated NPRs  To answer the third question, we collected user-generated NPRs from five subjects. The subjects were all computer-savvy, with at least a BSc de-gree. Only one subject expressed some familiarity with NPRs (but didn?t generate better NPRs than 
 Figure 1. Phone and word error rates on the ID set. 
 Figure 2. Phone and word error rates for the OOD set. 
0% 
2% 
4% 
6% 
8% 
10% 
1 2 3 4 5 6 7 8 9 10 
PER 
spelling NPR combined 
0% 
10% 
20% 
30% 
40% 
50% 
1 2 3 4 5 6 7 8 9 10 
WER 
0% 
2% 
4% 
6% 
8% 
10% 
12% 
14% 
16% 
1 2 3 4 5 6 7 8 9 10 
PER 
spelling NPR combined 
0% 
10% 
20% 
30% 
40% 
50% 
60% 
70% 
80% 
1 2 3 4 5 6 7 8 9 10 
WER 
131
other subjects). The subjects were shown four examples of NPRs; two of them were recent Word of the Day entries, and had audio attached to them. The other two were selected from the OOD set. With only four words and two different sources we wanted to ensure that users would not be able to train them-selves to a specific system. Subjects understood the problem easily and rarely if ever looked back at the examples during the actual test.   The test involved generating NPRs for 20 of the most difficult words for our generic GP model from the OOD set (e.g., bronchoscope, paren-chyma, etc.). These words turned out to be mostly unfamiliar to users as well (the average familiarity score was just under 1.9 on a 4-point scale. No audio and no feedback were given. Users varied greatly in the choices they made. For the word acupressure, the first two syllables were transcribed as AK-YOO in the Dictionary of Cancer Terms, and users came up with ACK-YOU, AK-U, and AK-YOU. This underscores that a good N2P model would have to account for far more GP mappings than the 111 found in our data. Sometimes users had trouble assigning conso-nants to syllables (syllabification wasn?t required, but subjects tried anyway), on occasion splitting them across syllable boundaries (e.g., \BIL-LIH-RUE-BEN\ for bilirubin), which guarantees an in-sertion error. It is quite likely that some error model might be required to deal with such issues. Nonetheless, even though imperfect, the resulting NPRs showed excellent promise. Looking just at the top hypothesis, whereas the average PER on those 20 words was about 45% for the G2P model, pronunciations obtained from NPRs using the same G2P model (new GP mappings pre-cluded the use of the N2P model described in the previous section) had only around 36% (+/-5%) phone error rate. The combined model showed an even better performance of about 33% (+/-5%) PER. Full results for n-best lists up to n=10 are shown in Figure 3.  
5 Conclusions and Further Work  The experiments we conducted are preliminary, and most of the work remains to be done. More data need to be collected and analyzed before good NPR-to-pronunciation models can be trained. Fur-ther investigations need to be conducted to assess the average users? ability to generate NPRs and how they tend to deviate from the general graph-eme-to-phoneme rules of English.  Nonetheless, we believe these experiments give strong indications that NPRs would be an excellent source of information to improve the quality of pronunciation hypotheses generated from spelling. Moreover, it appears that novice users don?t have much difficulty generating useful NPRs on their own; we expect that their skill would increase with use. Particularly useful would be for the system to be able to provide feedback, including generating NPRs; we have started investigating this reverse problem, of obtaining NPRs from pronunciations, and are encouraged by the initial results. References  D. Bansal, N. Nair, R. Singh, and B. Raj. 2009. A Joint Decoding Algorithm for Multiple-Example-Based Addition of Words to a Pronunciation Lexicon. Proc. ICASSP?2009, pp. 2104-2107. F. Beaufays, et al 2003. Learning Linguistically Valid Pronunciation From Acoustic Data. Proc. Eu-rospeech?03, Geneva, pp. 2593-2596. M. Bisani and H. Ney. 2008. Joint-Sequence Models for Grapheme-to-Phoneme Conversion. Speech Commu-nication, 50(5):434-451. G. Chung, C. Wang, S. Seneff, E. Filisko, and M. Tang. 2004. Combining Linguistic Knowledge and Acous-tic Information in Automatic Pronunciation Lexicon Generation. Proc. Interspeech'04, Jeju Island, Korea. M. Davel and E. Barnard. 2004. The Efficient Genera-tion of Pronunciation Dictionaries: Human Factors during Bootstrapping. Proc. INTERSPEECH 2004, Korea. H. Fraser. 1997. Dictionary pronunciation guides for English. International Journal of Lexicography, 10(3), 181-208. L. Galescu and J. Allen. 2001. Bi-directional Conver-sion Between Graphemes and Phonemes Using a Joint N-gram Model. Proc. 4th ISCA Tutorial and Research Workshop on Speech Synthesis, Scotland. J. Kominek, S. Badaskar, T. Schultz, and A. Black. 2008. Improving Speech Systems Built from Very Little Data. Proc. INTERSPEECH 2008, Australia. E. McKean (ed.). 2005. The New Oxford American Dic-tionary (2nd ed.). Oxford University Press. T. Polyakova and A. Bonafonte, 2006. Learning from Errors in Grapheme-to-Phoneme Conversion. Proc. ISCLP?2006. Pittsburgh, USA. R.L. Weide. 1998. The CMU pronunciation dictionary, release 0.6. http://www.speech.cs.cmu.edu/cgi-bin/cmudict. 
 Figure 3. Phone error rates for user-generated NPRs.  
0% 
10% 
20% 
30% 
40% 
50% 
1 2 3 4 5 6 7 8 9 10 
PER 
Spelling NPR Combined 
132
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 85?88, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Two diverse systems built using
generic components for spoken dialogue
(Recent Progress on TRIPS)
James Allen, George Ferguson, Mary Swift, Amanda Stent, Scott Stoness, 
Lucian Galescu, Nathan Chambers, Ellen Campana, and Gregory Aist
University of Rochester
Computer Science Department
UR Comp Sci RC 270226
Rochester NY 14627 USA
{james, ferguson, swift, stoness,
campana, gaist}
@cs.rochester.edu
Institute for
Human and Machine Cognition
40 South Alcaniz St.
Pensacola FL 32502
{lgalescu,nchambers}@ihmc.us
State University of New York at
Stony Brook
1418 Computer Science
Stony Brook University
Stony Brook NY 11794 USA
stent@cs.sunysb.edu
Abstract
This  paper  describes  recent  progress  on  the
TRIPS architecture for developing spoken-lan-
guage dialogue systems.  The interactive poster
session will include demonstrations of two sys-
tems built using TRIPS: a computer purchas-
ing assistant, and an object placement (and ma-
nipulation) task.
1 Introduction
Building a robust spoken dialogue system for a new
task currently requires considerable effort,  includ-
ing  extensive  data  collection,  grammar  develop-
ment, and building a dialogue manager that drives
the  system using its  "back-end" application (e.g.
database query, planning and scheduling). We de-
scribe progress in an effort to build a generic dia-
logue system that  can be rapidly customized to a
wide range of different types of applications, pri-
marily  by  defining a  domain-specific  task  model
and the interfaces to the back-end systems. This is
achieved by  using generic  components  (i.e.,  ones
that apply in any practical domain) for all stages of
understanding  and developing techniques for rapid-
ly customizing the generic components to new do-
mains  (e.g.  Aist,  Allen,  and  Galescu  2004).  To
achieve this goal we have made several innovations,
including (1) developing domain independent mod-
els of  semantic and  contextual  interpretation,  (2)
developing generic  dialogue  management  compo-
nents based on an abstract  model of collaborative
problem solving, and (3) extensively using an ontol-
ogy-mapping system that connects the domain inde-
pendent representations to the representations/query
languages used by the back-end applications,  and
which is used to automatically optimize the perfor-
mance of the system in the specific domain.
2 Theoretical  Underpinnings:  The Prob-
lem-Solving Model of Dialogue
While many have observed that communication
is a specialized form of joint action that happens to
involve language and that dialogue can be viewed
as collaborative problem solving, very few imple-
mented systems have been explicitly based on these
ideas. Theories of speech act interpretation as inten-
tion recognition have been developed  (including ex-
tensive  prior  work  in  TRIPS'  predecessor,  the
TRAINS project), but have been generally consid-
ered impractical for actual systems.  Planning mod-
els  have been more successful  on the  generation
side, and some systems have used the notion of exe-
cuting explicit task models to track and drive the in-
teractions  (e.g.,  Sidner  and  Rich's  COLLAGEN
framework). But collaborative problem solving, and
dialogue in general, is much more general than exe-
cuting tasks. In our applications, in addition to exe-
cuting tasks, we see dialogue that is used to define
the task (i.e., collaborative planning), evaluate the
task (e.g., estimating how long it will take,  com-
paring options,  or  likely effects),    debug a  task
(e.g., identifying and discussing problems and how
to remedy them), learn new tasks (e.g., by demon-
stration and instruction).
85
In the remainder of the paper, we'll first discuss
the methods we've developed for building dialogue
systems using generic components.  We'll then de-
scribe two systems implemented using the TRIPS
architecture that we will demonstrate at the interac-
tive poster session.
3 Generic Methods:  Ontology Mappings
and Collaborative Problem Solving
The goal of our work is to develop generic spoken
dialogue technology that can be rapidly customized
to new applications, tasks and domains. To do this,
we have developed generic domain independent rep-
resentations not only of sentence meaning but also
of the collaborative actions that are performed by
the speech acts as one engages in dialogue. Further-
more, we need to be able to easily connect these
generic representations to a wide range of different
domain specific task models and applications, rang-
ing from data base query systems to state-of-the-art
planning and scheduling systems.  This  paper  de-
scribes  the  approach  we  have  developed  in  the
TRIPS system. TRIPS is now being used in a wide
range of diverse applications, from interactive plan-
ning (e.g., developing evacuation plans), advice giv-
ing  (e.g.,  a  medication  advisor  (Ferguson  et  al.
2002)),  controlling teams of robots,   collaborative
assistance (e.g., an assistant that can help you pur-
chase a computer, as described in this paper), sup-
porting human learning, and most recently having
the computer  learn (or  be  taught)  tasks,  such as
learning to perform tasks on the web.  Even though
the tasks and domains differ dramatically, these ap-
plications use the same set of core understanding
components. 
The key to supporting such a range of tasks and ap-
plications is the use of a general ontology-mapping
system. This allows the developer to express a set
of mapping rules that translate the generic knowl-
edge representation into the specific representations
used by the back-end applications (called the KR
representation).   In  order  to  support  generic dis-
course processing, we represent these mappings as
a chain of simpler transformations. These represen-
tations are thus transformed in several stages. The
first,  using the ontology mapping rules,  maps the
LF representation into an intermediary representa-
tion (AKRL - the abstract KR language) that has a
generic syntax  but  whose content is  expressed in
terms of the KR ontology. The second stage is a
syntactic transformation that occurs at the time that
calls to the back-end applications actually occur so
that  interactions  occur  in  the  representations  the
back-end expects.   In  addition to  using ontology
mapping to  deal  with the representational  issues,
TRIPS is unique in that it uses a generic model of
collaborative problem solving to drive the dialogue
itself  (e.g.  Allen,  Blaylock,  and  Ferguson 2002).
This model forms the basis of a generic component
(the collaboration manager) that supports both in-
tention recognition to identify the intended speech
acts and their content, planning the system's actions
to respond to the user (or that take initiative), and
providing utterance realization goals to the genera-
tion system. To develop this, we have been develop-
ing  a  generic  ontology  of  collaborative  problem
solving acts, which provide the framework for man-
aging  the  dialogue.  The  collaboration  manager
queries a domain-specific task component in order
to  make  decisions  about  interpretations  and  re-
sponses.
4 TRIPS  Spoken  Dialogue  Interface  to
the CALO Purchasing Assistant 
The CALO project is a large multisite effort which
aims  at  building  a  computerized  assistant  that
learns how to help you with day-to-day tasks. The
overarching goal of the CALO project is to 
... create cognitive software systems, that is,
systems that can reason, learn from experi-
ence, be told what to do, explain what they
are doing, reflect on their experience, and re-
spond robustly to surprise (Mark and Per-
rault 2004). 
Within this broad mandate, one of our current areas
of focus is user-system dialogue regarding the task
of purchasing - including eliciting user needs, de-
scribing possibilities, and reviewing & finalizing a
purchase  decision.  (Not  necessarily  as  discrete
stages; these elements may be interleaved as appro-
priate for the specific item(s) and setting.)  Within
the purchasing domain,  we began with computer
purchasing and have branched out to other equip-
ment such as projectors.
How to help with purchasing? The family of tasks
involving purchasing items online, regardless of the
type of item, have a  number of elements in com-
mon. The process of purchasing has some common
86
dialogue elements - reporting on the range of fea-
tures  available,  allowing the user  to specify con-
straints, and so forth.  Also, regarding the goal that
must be reached at the end of the task, the eventual
item must:
Meet requirements.  The item needs to meet some
sort of user expectations. This could be as arbitrary
as a specific part number, or as compositional - and
amenable to machine understanding -  as  a  set  of
physical  dimensions (length,  width,  height,  mass,
etc.) 
Be approved. Either the system will have the au-
thority to approve it (cf. Amazon's one-click order-
ing system), or more commonly the user will review
and confirm the purchase. In an office environment
the approval process may extend to include review
by a supervisor, such as might happen with an item
costing over (say) $1000. 
Be available. (At  one time a  certain  electronics
store in California had the habit of leaving out floor
models of laptops beyond the point where any were
actually available for sale.  (Perhaps to entice the
unwitting customer into an ?upsale?, that is, buying
a  similar  but  more  expensive  computer.))  On  a
more serious note, computer specifications change
rapidly, and so access to online information about
available  computers  (provided  by  other  research
within CALO) would be important in order to en-
sure that the user can actually order the machine he
or she has indicated a preference for.  
At  the interactive poster  session,  we will demon-
strate some of the current spoken dialogue capabili-
ty related to the CALO task of purchasing equip-
ment.  We will demonstrate a number of the aspects
of the system such as initiating a conversation, dis-
cussing specific requirements,  presenting possible
equipment to purchase,  system-initiated reminders
to ask for supervisor approval for large purchases,
and finalizing a decision to purchase. 
Figure 1. Fruit carts display.
87
5 TRIPS  Spoken  Dialogue  Interface  to
choosing,  placing,  painting,  rotating,
and filling (virtual) fruit carts
TRIPS is versatile in its applications, as we've said
previously.  We hope to also demonstrate an inter-
face to  a  system for  using spoken commands to
modifying, manipulating, and placing objects on a
computer-displayed map.  This  system (aka  ?fruit
carts?)  extends  the  TRIPS  architecture  into  the
realm of continuous understanding.  That is, when
state-of-the-art  dialogue systems listen,  they typi-
cally wait for the end of the utterance before decid-
ing what to do.  People on the other hand do not
wait in this way ? they can act on partial informa-
tion as  it  becomes available.   A classic example
comes  from  M.  Tanenhaus  and  colleagues  at
Rochester: when presented with several objects of
various colors and told to ?click on the yel-?, people
will already tend to be looking relatively more at the
yellow object(s) even before the word ?yellow? has
been completed.  To achieve this type of interactivi-
ty with a dialogue system ? at least at the level of
two or three words at a time, if not parts of words ?
imposes some interesting challenges. For example:
1. Information must flow asynchronously between
dialogue components, so that actions can be trig-
gered based on partial utterances even while the
understanding continues
2. There must be reasonable representations of in-
complete information ? not just ?incomplete sen-
tence?,  but  specifying what  is  present  already
and perhaps what may potentially follow
3. Speech  recognition,  utterance  segmentation,
parsing, interpretation, discourse reasoning, and
actions must all be able to happen in real time
The fruit carts system consists of two main compo-
nents:  first,  a  graphical  interface implemented on
Windows  2000  using  the  .NET  framework,  and
connected to  a  high-quality  eyetracker;  second,  a
TRIPS-driven spoken dialogue interface implement-
ed primarily in LISP.   The actions in this domain
are as follows:
1. Select an object (?take the large plain square?)
2. Move it (?move it to central park?)
3. Rotate  it  (?and then turn  it  left  a  bit  ?  that's
good?)
4. Paint it (?and that one needs to be purple?)
5. Fill it (?and there's a grapefruit inside it?)
Figure 1 shows an example screenshot from the
fruit carts visual display. The natural language in-
teraction  is  designed to  handle  various  ways  of
speaking,  including conventional  definite  descrip-
tions (?move the large square to central park?) and
more interactive language such as (?up towards the
flag pole ? right a bit ? more ? um- stop there.?)
6 Conclusion
In this brief paper,  we have described some of
the recent progress on the TRIPS platform.  In par-
ticular we have focused on two systems developed
in TRIPS: a spoken dialogue interface to a mixed-
initiative purchasing assistant, and a spoken inter-
face for exploring continuous understanding in an
object-placement task.  In  both  cases  the  systems
make use of reusable components ? for input and
output  such as  parsing and speech synthesis,  and
also for dialogue functionality such as mapping be-
tween language,  abstract  semantics,  and  specific
representations for each domain.
References 
Aist,  G.  2004.  Speech,  gaze,  and  mouse  data  from
choosing,  placing,  painting,  rotating,  and  filling
(virtual) vending carts. International Committee for
Co-ordination  and  Standardisation  of  Speech
Databases  (COCOSDA)  2004  Workshop,  Jeju  Is-
land, Korea, October 4, 2004. 
Aist, G.S., Allen, J., and Galescu, L. 2004. Expanding
the linguistic coverage of a spoken dialogue system
by mining human-human dialogue for new sentences
with familiar meanings. Member Abstract, 26th An-
nual  Meeting  of  the  Cognitive  Science  Society,
Chicago, August 5-7, 2004. 
James Allen, Nate Blaylock, and George Ferguson. A
problem-solving model for collaborative agents.  In
First International Joint Conference on Autonomous
Agents and Multiagent Systems, Bologna, Italy, July
15-19 2002. 
George  Ferguson,  James  F.  Allen,  Nate  J.  Blaylock,
Donna K. Byron, Nate W. Chambers, Myrsolava O.
Dzikovska, Lucian Galescu, Xipeng Shen, Robert S.
Swier, and Mary D. Swift.  The Medication Advisor
Project: Preliminary Report, Technical Report 776,
Computer  Science  Dept.,  University  of  Rochester,
May 2002. 
Mark,  B.,  and  Perrault,  R.  (principal  investigators).
2004.  Website for Cognitive Assistant  that  Learns
and Organizes. http://www.ai.sri.com/project/CALO
88
gency dispatcher, cooperating with the system to 
dynamically allocate resources to and make 
plans for solving problems as they arise in the 
world. The setting, Monroe County, NY, is con- 
siderably more complex than our previous do- 
mains (e.g. Pacifica, TRAINS), and raises new 
issues in knowledge representation a d refer- 
ence. Emergencies include requests for medical 
assistance, car accidents, civil disorder, and 
larger problems such as flooding and snow 
storms. Resources at the user's disposal may 
include road crews, electric crews, ambulances, 
police units and helicopters. Some of the in- 
crease in mixed-initiative interaction comes 
from givi-n~ the_ system more knowledge of the 
tasks being solved. Some comes from the fact 
that the solution to one problem may conflict 
with the solution to another, either because of 
scheduling conflicts, scarce resources, or aspects 
of the physical world (e.g. an ambulance can't go 
down a road that has not been plowed). The 
range of tasks and complexity of the world allow 
for problem solving at different levels of granu- 
larity, making it possible for the system to take 
as much control over the task as the user per- 
mits. 
4. Important  Contr ibut ions  
While a number of robust dialogue systems have 
been built in recent years, they mostly have op- 
erated in domains that require little if any rea- 
soning. Rather, the task is hard-coded into the 
system operation. One of the major goals of the 
TRIPS project has been to develop dialogue 
models and system architectures that support 
conversational interaction in domains where 
complex reasoning systems are required. One 
goal has been to build a fairly generic model in 
which different domains can then be specified 
fairly easily. On this front, we are seeing some 
success as we have now constructed versions of 
TRIPS in three different domains, and TRIPS? 
911 will be the fourth. In developing the system 
for new domains, the bulk of the work by far has 
been in system enhancements rather than in 
developing the domain models. 
The TRIPS-911 domain has forced a rethinking 
of the relationship between dialogue- 
management, problem-solving, the system's 
Figure 1: Monroe County map used in TRIPS-911 
own goal-pursuit and generation. The new ar- 
chitecture is designed to support research into 
mixed-initiative interactions, incremental gen- 
eration of content (in which the user might in- 
tervene before the system completes all it has to 
say), rich reference resolution models, and the 
introduction of plan monitoring and plan repair 
into the suite of plan management operations 
supported. The domain also can support longer 
and richer dialogues than in previous domains. 
More complex domains mean even more com- 
plex dialogues. The complexity arises from 
many factors. First, more complex dialogues 
will involve topic progression, development and 
resumption, and more complex referential phe- 
nomena. On the problem solving front, there will 
be more complex corrections, elaborations and 
modifications--forcing us to develop richer 
discourse models. In addition, the complexity of 
the domain demonstrates a need for better 
grounding behavior and a need for incremental 
dialogue-based generation. 
We have by no means solved these problems. 
Rather we have built a rich testbed, designed and 
implemented a plausible architecture, and have 
constructed an initial system to demonstrate 
basic capabilities in each of the problem areas. 
34 
5. Limitations 
TRIPS-911 is a first attempt at handling a do- 
main of this complexity. As such there are many 
capabilities that people have in such situations 
that are beyond the system's current capabilities. 
Some of the most important are: 
? Scale - we can only handle small domains 
and the existing techniques would not ex- 
tend directly to a realistic size 911 operation. 
To scale up we must face some difficult 
problems including reasoning about quanti- 
ties and aggregates, planning in large-scale 
domains (i.e., the real domains are beyond 
the capabilities of current plan technology), 
and performing intention recognition as the 
number of options increases. In addition, for 
an effective dialogue system, all this must be 
done in real-time. 
? Meta-talk - when faced with complex prob- 
lems, people often first generally discuss the 
problem and possible strategies for solving 
it, and later may explicitly direct attention to 
specific subproblems. The current TRIPS 
system does not support such discussion. 
? Time - in the 911 domain there are at least 
two temporal contexts that can be "used" by 
the conversants: there is the actual time (i.e., 
when they are talking), but there also is the 
time relative to a point of focus in a plan, or 
even simply talking about the past or the 
future. TRIPS-911 can currently interpret 
expressions with respect to the actual time. 
? Interleaved generation - when people are 
discussing complex issues, they often have 
to plan to communicate heir content across 
several different utterances. There is no 
guarantee that the other conversant will not 
"interrupt" (e.g., to clarify, correct, suggest 
alternatives, etc) before the entire content is 
conveyed. This requires a rethinking of cur- 
rent practice in generation to make it incre- 
mental and interactive. 
? True interruptions - people may interrupt the 
system while it is talking. It is unclear at this 
stage what the system should assume was 
conveyed. The strategies of assuming noth- 
ing was conveyed, or that all was conveyed 
have obvious faults. We are pursuing alter- 
natives based on knowing when speech was 
interrupted, but using this ififormation suc- 
cessfully remains adifficult problem. 
References 
Allen, James et al An Architecture for a Generic 
Dialogue Shell, to appear, J. Natural Language 
Engineering, 2000. 
Ferguson, George and J. Allen,-TRIPS: An Integrated 
Intelligent Problem-Solving Assistant, Proc. Na- 
tional Conference on AI (AAAI-98), Madison, WI, 
1998. 
35 
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 146?154,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Building Timelines from Narrative Clinical Records: Initial Results 
Based-on Deep Natural Language Understanding
Hyuckchul Jung, James Allen, Nate Blaylock, Will de Beaumont, 
Lucian Galescu, Mary Swift
Florida Institute for Human and Machine Cognition
40 South Alcaniz Street, Pensacola, Florida, USA
{hjung,blaylock,jallen,wbeaumont,lgalescu,mswift}@ihmc.us
Abstract
We present an end-to-end system that proc-
esses narrative clinical records, constructs 
timelines for the medical histories of pa-
tients, and visualizes the results. This work 
is motivated by real clinical records and 
our general approach is based on deep se-
mantic natural language understanding.
1 Introduction
It is critical for physicians and other healthcare 
providers to have complete and accurate knowl-
edge of the medical history of patients that  in-
cludes disease/symptom progression over time and 
related tests/treatments in chronological order. 
While various types of clinical records (e.g., dis-
charge summaries, consultation notes, etc.) contain 
comprehensive medical history information, it  can 
be often challenging and time-consuming to com-
prehend the medical history of patients when the 
information is stored in multiple documents in dif-
ferent  formats and the relations among various 
pieces of information is not explicit.
For decades, researchers have investigated tem-
poral information extraction and reasoning in the 
medical domain (Zhou and Hripcsak, 2007). How-
ever, information extraction in the medical domain 
typically relies on shallow NLP techniques (e.g., 
pattern matching, chunking, templates, etc.),  and 
most temporal reasoning techniques are based on 
structured data with temporal tags (Augusto, 2005; 
Stacey and McGregor, 2007).
In this paper, we present our work on develop-
ing an end-to-end system that  (i) extracts interest-
ing medical concepts (e.g., medical conditions/
tests/treatments), related events and temporal ex-
pressions from raw clinical text records, (ii) con-
structs timelines of the extracted information; and 
(iii) visualizes the timelines, all using deep seman-
tic natural language understanding (NLU). 
Our deep NLU system extracts rich semantic 
information from narrative text records and builds 
logical forms that  contain ontology types as well as 
linguistic features. Ontology- and pattern-based 
extraction rules are used on the logical forms to 
retrieve time points/intervals, medical concepts/
events and their temporal/causal relations that are 
pieced together by our system?s temporal reasoning 
component to create comprehensive timelines.
Our system is an extension to a well-proven 
general-purpose NLP system (Allen et  al., 2000) 
rather than a system specialized to the clinical do-
main, and the temporal reasoning in our system is 
tightly integrated into the NLP system?s deep se-
mantic analysis. We believe this approach will al-
low us to process a broader variety of documents 
and complex forms of temporal expressions.
In the coming sections, we first present a moti-
vating example, a real clinical record of a cancer 
patient. Next, we give an overview of our NLU 
system including how medical ontology is inte-
grated into our system. The overview section is 
followed by detailed description of our information 
extraction and temporal reasoning approach. Then, 
we discuss our results and conclude.
2 Motivating Example
Our work is carried out as a collaboration with the 
Moffitt  Cancer Center (part of the NCI Compre-
hensive Cancer Centers), who have provided us 
with access to clinical records for over 1500 pa-
tients. Figure 1 shows a (de-identified) ?History of 
Present Illness? (HPI) section of a Thoracic Con-
sultation Note from this data set. 
146
The text of this section provides a very detailed 
description of what  problems/tests/treatments an 
anonymous cancer patient went  through over a pe-
riod. Such narrative text is common in clinical 
notes and, because such notes are carefully created 
by physicians, they tend to have only relevant in-
formation about patient medical history. 
Nonetheless, there are lots of challenges in con-
structing complete and accurate medical history 
because of complex temporal expressions/
relations, medical language specific grammar/
jargons, implicit  information and domain-specific 
medical knowledge (Zhou and Hripcsak, 2007).
In this paper, as an initial step towards con-
structing complete timelines from narrative text, 
we focus on sentences with explicit  temporal ex-
pressions listed below (tagged as Line 1 ~ 11) plus 
a sentence in the present tense (Line 12):
1
?
Line 1: She had a left radical nephrectomy in  09/
2007; pathological stage at that time  was a T3 
NX MX. 
?
Line 2: Prior to her surgery CT scan in 08/2007 
showed lung nodules. 
?
Line 3: She was placed on Nexavar in 11/2007. 
?
Line 4: She was started on Afinitor on 03/05/08. 
?
Line 5: She states that prior to starting the Afini-
tor she had no shortness of breath or dyspnea on 
exertion and she was quite active. 
?
Line 6: Unfortunately 4 weeks after starting the 
Afinitor she developed a dry cough and progres-
sive shortness of breath with dyspnea on exer-
tion. 
?
Line 7: She received a 5 day dose pack of 
prednisone and was treated with Augmentin in 
05/2008. 
?
Line 8: She subsequently had a CT scan of the 
chest  done on 05/14/08 that  showed interval de-
velopment of bilateral lower lobe infiltrates that 
were not present on the 02/19/08 scan. 
?
Line 9: Because of her respiratory symptoms, the 
Afinitor was stopped on 05/18/2008. 
?
Line 10: Prior to  the Afinitor she was able to 
walk, do gardening, and swim without any 
shortness of breath.  
?
Line 11: She has had a 140 pound weight  since 
10/2007.
?
Line 12: She denies fevers, chills, hemoptysis or 
chest pain. 
In these 12 sentences, there are instances of 10 
treatments (e.g., procedures such as ?nephrectomy? 
and drugs such as ?Nexavar?), 3 tests (e.g., CT-
scan), 13 problems/symptoms (e.g., lung nodules) 
and 2 other types of clinical findings (e.g., the can-
cer stage level ?T3 NX MX?). There are also 23 
events of various types represented with verbs such 
as  ?had?, ?was?, ?showed?, and ?was started?.
While there are simple expressions such as ?on 
03/05/08? in Line 3, there are also temporal ex-
pressions in more complex forms with time rela-
tions (e.g., ?prior to?), time references (e.g., ?at 
that time?) or event references (e.g., ?4 weeks after 
starting Afinitor?). Throughout  this paper, we will 
use Line 1 ~ 12 as a concrete example based on 
which we develop general techniques to construct 
timelines.
1
 For privacy, identities of patients/physicians were concealed and the dates/time-spans in the original sources were 
altered while maintaining their chronological order. Some measurements and geographic names were also modified.
Figure 1: A sample medical record -- Thoracic 
Consultation Note
1
PAST MEDICAL HISTORY: 
1. History of melanoma of the left arm.  She had excision of 3 sentinel lymph nodes in the left axilla 
that were negative.  This was in 07/2007.
2. Status post right hip replacement.
3. Status post cholecystectomy.
4. Status post renal stone removal.
5. Fracture of the right hip and left wrist in a motor vehicle accident.
6. Diabetes.
7. Elevated cholesterol.
8. Hypertension.
9. Spinal stenosis. 
ALLERGIES:
She has no known drug allergies.  She is allergic to IVP dye which causes shortness of breath.  She 
tolerates IV dye when she is pre treated.  
SOCIAL HISTORY: 
She is born and raised in California and she lived in Florida for 30 years.  She has worked as a 
medical billing analyst.  She has never smoked. She does not use alcoholic beverages.
FAMILY HISTORY:
Her father died at age 69 of prostate cancer.  Her mother died at age 72 of emphysema.  She had 1 
sister who died from melanoma.  
REVIEW OF SYSTEMS: 
A complete review of systems was performed.  See the questionnaire.  She has hypothyroidism.  
She has some back pain related to her spinal stenosis.  She suffers from mild depression.  
CURRENT MEDICATIONS:
1. Carvedilol 6.25 mg p.o. daily.
2. Darvocet N100, 1 tablet as needed.
3. Fish oil, 1000 mg three times a day.
4. Glimepiride 4 mg daily in the morning and 2 mg at bedtime.
5. Lipitor 20 mg daily.
6. Metformin 1000 mg twice daily.
7. Paroxetine 20 mg daily.
8. Synthroid 0.112 mg daily.
9. Tylenol as needed.
10. Vitamin B12, 2500 mcg p.o. twice daily.
XXX X XX-XX-XX 
CONSULTATION DATE: 07/06/2008
RE: XXX BIRTH DATE: XX/XX/XXXX
UR#: XX-XX-XX AGE: 75
THORACIC CONSULTATION NOTE
REQUESTING PHYSICIAN:
XXXXXXXXXX, MD.
REASON FOR CONSULTATION:
Shortness of breath and abnormal chest x ray.
HISTORY OF PRESENT ILLNESS:
Ms. XXX is a 75 year old woman who has a history of metastatic renal cancer.  She had a left radical 
nephrectomy in 09/2007; pathological stage at that time was a T3 NX MX.  Prior to her surgery CT 
scan in 08/2007 showed lung nodules.  These nodules have progressed with time.  She was placed 
on Nexavar in 11/2007.  She subsequently was found to have a new mass in her left nephrectomy 
bed.  She was continued on the Nexavar, however, she showed radiographic progression and the 
Nexavar was discontinued.  She was started on Afinitor on 03/05/08.  She states that prior to starting 
the Afinitor she had no shortness of breath or dyspnea on exertion and she was quite active.  
Unfortunately 4 weeks after starting the Afinitor she developed a dry cough and progressive 
shortness of breath with dyspnea on exertion.  She received a 5 day dose pack of prednisone and 
was treated with Augmentin in 05/2008.  This had no impact on her cough or shortness of breath.  
She subsequently had a CT scan of the chest done on 05/14/08 that showed interval development 
of bilateral lower lobe infiltrates that were not present on the 02/19/08 scan.  She had mediastinal 
and right hilar adenopathy that had increased.  She had multiple lung nodules and there was 
recurrent tumor noted in the left renal bed which was thought to be larger.  Because of her 
respiratory symptoms, the Afinitor was stopped on 05/18/08.  She still has a dry cough.  She is short 
of breath after walking 15 to 20 feet.  She has no shortness of breath at rest.  She denies PND or 
orthopnea.  Prior to the Afinitor she was able to walk, do gardening, and swim without any shortness 
of breath.  She has had a 140 pound weight since 10/2007.  She notices anorexia.  She has no 
travel history.
She denies fevers, chills, hemoptysis or chest pain.  She has never smoked.  She denies 
pneumonia, asthma, wheezing, or myocardial infarction, congestion heart failure or heart murmur.  
She has dogs and cats at home and has had them for a long time and this never caused her 
respiratory problems. 
PHYSICAL EXAMINATION: 
VITAL SIGNS:   Blood pressure 131/74, pulse 106, respiratory rate 20, temperature 97.3, weight 
64.0 kg.
HEENT:   Pupils equal, round, reactive to light.  Extraocular muscles were intact.  Nose and mouth 
were clear. 
NECK:  Trachea midline.  Carotids were 2 plus.  No masses, thyromegaly or adenopathy.  
LUNGS:   Respirations were unlabored.  There is no dullness to percussion or tenderness to 
palpation.  She has some bibasilar dry rales.
HEART:   Regular rate and rhythm without murmur.
ABDOMEN:  Soft, positive bowel sounds, nontender. 
EXTREMITIES:   No clubbing or cyanosis.  She had some mild pedal edema. 
DATABASE:
Chest x ray from 06/01/08 was reviewed.  She had bilateral lower lobe patchy densities.  She had 
some nodular densities bilaterally as well.  There is widening of the mediastinum on the right.  CT 
scan of the chest from 05/14/08 also was reviewed.  She had bilateral lower lobe infiltrates that were 
new.  She had mediastinal and right hilar adenopathy.  She had multiple lung nodules.  There is 
recurrent tumor in the left renal bed that was thought to be larger. 
IMPRESSION:
1. Metastatic renal cancer with multiple lung nodules with mediastinal and hilar adenopathy.  
2. Bilateral lower lobe infiltrates.  These infiltrates had developed after starting the Afinitor, as did 
her shortness of breath and dyspnea on exertion.  She recently started on oxygen by her primary 
care physician when she was found to have exercise O2 saturations of 86%.  She is currently taking 
2 liters of oxygen.  I would be concerned that the infiltrates may be related to pneumonitis from the 
Afinitor.  I also think her shortness of breath, cough and hypoxemia are related to the infiltrates as 
well.  
RECOMMENDATIONS:
1. I reviewed my impressions with the patient.
2. I am going to schedule her for a bronchoscopy and bronchoalveolar lavage.  I am going to get 
baseline pulmonary function tests on her. 
3. She will be seen by Dr. XXX on 08/12/08.  I will call and discuss the case with him pending the 
above results.  The options are likely going to be observation off Afinitor or may consider placing her 
on prednisone, if the bronchoalveolar lavage is unremarkable.  
4. Further recommendations will be made after the above.
Do not type or edit below this line. This will cause format damage.
  
Dictated by XXXX, MD
Electronically Signed
FXXXXXXX, MD 07/10/2008 10:15
________________________
XXXXX, MD
DD: 07/10/2008  9:24 A
DT: 07/13/2008 11:46 A
ID: XXXXXXX.LML
CS: XXXXXX
cc: 
??
HISTORY OF PRESENT ILLNESS:
Ms. XXX is a 75 year ld woman who has a history of metastatic renal 
cancer. She had a left adi al nephrectomy in 09/2007; pathological stage 
at that time was a T3 NX MX. Prior to her surgery CT scan in 08/2007 
showed lung nodules. These nodules have progressed with time. She was 
placed on Nexavar in 11/2007. She subsequently was found to have a 
new mass in her left nephrectomy bed. She was continued on the 
N xavar, owever, she showed radiographic progression and the Nexavar 
was disc ntinued. She was started on Afinitor on 03/05/08. She states 
that p i r to starting the Afinitor she had no shortness of breath or 
dys n a on exe tion and she was quite active. Unfortunately 4 weeks 
after starting the Afinitor she developed a dry cough and progressive 
tness f br th with dyspnea on exertion. She received a 5 day dose 
pack of prednisone and was treated with Augmentin in 05/2008. This had 
no impact on her cough or shortness of breath. She subsequently had a 
CT scan of the chest done on 05/14/08 that showed interval development 
of bilateral lower lobe infiltrates that were not present on the 02/19/08 
scan. She had mediastinal and right hilar adenopathy that had increased. 
She had multiple lung nodules and there was recurrent tumor noted in the 
left renal bed which was thought to be larger. Because of her respiratory 
symptoms, the Afinitor was stopped on 05/18/2008. She still has a dry 
cough. She is short of breath after walking 15 to 20 feet. She has no 
shortness of breath at rest. She denies PND or orthopnea. Prior to the 
Afinitor she was able to walk, do gardening, and swim without any 
shortness of breath. She has had a 140 pound weight since 10/2007. She 
notices anorexia. She has no travel history. She denies fevers, chills, 
hemoptysis or chest pain. She has never smoked. She denies pneumonia, 
asthma, wheezing, or myocardial infarction, congestion heart failure or 
heart murmur. She has dogs and cats at home and has had them for a long 
time and this never caused her respiratory problems.
147
3 Natural Language Understanding 
(NLU) System
Our system is an extension to an existing NLU sys-
tem that is the result of a decade-long research ef-
fort  in developing generic natural language tech-
nology. The system uses a ?deep? understanding 
approach, attempting to find a linked, overall 
meaning for all the words in a paragraph. An archi-
tectural view of the system is shown in Figure 2.
3.1 Core NLU Components
At the core of the system is a packed-forest  chart 
parser which builds constituents bottom-up using a 
best-first search strategy. The core grammar is a 
hand-built, lexicalized context-free grammar, aug-
mented with feature structures and feature unifica-
tion. The parser draws on a general purpose seman-
tic lexicon and ontology which define a range of 
word senses and lexical semantic relations. The 
core semantic lexicon was constructed by hand and 
contains more than 7000 lemmas. It  can be also 
dynamically augmented for unknown words by 
consulting WordNet (Miller, 1995). 
To support  more robust processing as well as 
domain configurability, the core system is in-
formed by a variety of statistical and symbolic pre-
processors. These include several off-the-shelf sta-
tisical NLP tools such as the Stanford POS tagger 
(Toutanova and Manning, 2000), the Stanford 
named-entity recognizer (NER) (Finkel et al, 
2005) and the Stanford Parser (Klein and Manning, 
2003). The output of these and other specialized 
preprocessors (such as a street address recognizer) 
are sent  to the parser as advice. The parser then can 
include or not include this advice (e.g., that a cer-
tain phrase is a named entity) as it searches for the 
optimal parse of the sentence.
The result  of parsing is a frame-like semantic 
representation that we call the Logical Form (LF). 
The LF representation includes semantic types, 
semantic roles for predicate arguments, and de-
pendency relations. Figure 3 shows an LF example 
for the sentence ?She had a left radical nephrec-
tomy in 09/2007?. In the representation, elements 
that start  with colons (e.g., :THEME) are semantic 
roles of ontological concepts, and role values can 
be a variable to refer to another LF term.
3.2 UMLS Integration
By far the most  critical aspect  of porting our ge-
neric NLU components to the task of understand-
ing clinical text  is the need for domain-specific 
lexical and ontologic information. One widely used 
comprehensive resource that  can provide both is 
the National Library of Medicine?s Unified Medi-
cal Language System (UMLS) (Bodenreider, 
2004). UMLS was integrated into or system via 
MetaMap (Aronson and Lang, 2010), a tool also 
developed by NLM, that  can identify and rank 
UMLS concepts in text.
Specifically, we added MetaMap as a special 
kind of named entity recognizer feeding advice 
into the Parser?s input chart (see Figure 2). We run 
MetaMap twice on the input  text  to obtain UMLS 
information both for the maximal constituents, and 
for individual words in those constituents (e.g., 
?lung cancer?, as well as ?lung? and ?cancer?).
The lexicon constructs representations for the 
new words and phrases on the fly. Our general ap-
proach for dealing with how the corresponding 
concepts fit  in our system ontology uses an ontol-
Core Lexicon
& Semantic Ontology
Grammar
Parser
Wordnet
Unknown Word 
Processing
New Lexical Entries
Output
Chart
Input
Chart
Statistical Parser
Bracketing Preferences
Input
Named Entity 
Recognizer
Name 
Hypotheses
POS 
Tagging
POS
Hypotheses
Word Hypotheses
MetaMap UMLS
UMLS
POS/sense 
Hypotheses
LF Semantic 
Representation 
for reasoners
Figure 2: Front-end language processing components with MetaMap and UMLS
148
ogy specialization mechanism which we call on-
tology grafting, whereby new branches are created 
from third party ontological sources, and attached 
to appropriate leaf nodes in our ontology.
The UMLS Semantic Network and certain vo-
cabularies included in the UMLS Metathesaurus 
define concept hierarchies along multiple axes. 
First, we established links between the 15 UMLS 
semantic groups and corresponding concepts in our 
ontology. Second, we selected a list of nodes from 
the SNOMED-CT and NCI hierarchies (27 and 11 
nodes, respectively) and formed ontological 
branches rooted in these nodes that  we grafted onto 
our ontology. 
Based on these processes, UMLS information 
gets integrated into our LF representation. In Fig-
ure 3, the 3rd term has a role called :domain-info 
and, in fact, its value is (UMLS :CUI C2222800 
:CONCEPT "left nephrectomy" :PREFERRED 
"nephrectomy of left kidney (treatment)" 
:SEMANTIC-TYPES (TOPP) :SEMANTIC-
GROUPS (PROC) :SOURCES (MEDCIN MTH)) 
that provides detailed UMLS concept information. 
Here, the semantic type ?TOPP? is a UMLS abbre-
viation for ?Therapeutic or Preventive Procedure?. 
More details about complex issues surrounding 
UMLS integration into our system can be found in 
(Swift et al, 2010).
4 Information Extraction (IE) from Clinical 
Text Records
In this section, we describe how to extract basic 
elements that will be used as a foundation to con-
struct timelines. We first describe our general ap-
proach to extracting information from LF graphs. 
Then we give details specific to the various types 
of information we extract in our system: various 
clinical concepts, temporal concepts (points as well 
as intervals), events and temporal relations.
4.1 LF Pattern-based Extraction
Given LF outputs from the NLU system described 
in Section 3, we use LF pattern-based rules for in-
formation extraction. The basic structure of an ex-
traction rule is a list  of LF patterns followed by a 
unique rule ID and the output specification.
Each LF-pattern specifies a pattern against  an 
LF. Variables can appear anywhere except as role 
names in different formats:
?
?x - (unconstrained) match anything 
?
?!x - match any non-null value
?
(? x V1 V2 ...) - (constrained) match one of the 
specified values V1,  V2, ...
As an example, the extraction rule in Figure 4 
will match LFs that mean a person had a treatment 
or a medical-diagnostic with explicit  UMLS in-
formation (i.e., part of LFs in Figure 3 matches). 
The output specification records critical informa-
tion from the extraction to be used by other rea-
soners. 
The extraction rules have all been developed by 
hand. Nevertheless, they are quite general, since a) 
LF patterns abstract away from lexical and syntac-
tic variability in the broad class of expressions of 
interest (however, lexical and syntactic features 
may be used if needed); and b) LF patterns make 
heavy use of ontological categories, which pro-
vides abstraction at the semantic level.
4.2 Clinical Concept Extraction
Among various types of concepts included in clini-
cal records, we focus on concepts related to 
problems/tests/treatments to build a medical his-
(F V1 (:* ONT::HAVE W::HAVE) :AFFECTED V2 :THEME V3 :MOD V4 :TENSE W::PAST) 
(PRO V2 (:* ONT::PERSON W::SHE) :PROFORM ONT::SHE :CO-REFERENCE V0) 
(A V3 (:* ONT::TREATMENT W::LEFT-RADICAL-NEPHRECTOMY) :DOMAIN-INFO (UMLS .....)
(F V4 (:* ONT::TIME-SPAN-REL W::IN) :OF V1 :VAL V5)
(THE V5 ONT::TIME-LOC :YEAR 2007 :MONTH 9)
Figure 3: LF semantic representation for ?She had a left radical nephrectomy in 09/2007?
(?x1 ?y2 (? type1 ONT::HAVE) :AFFECTED ?y2 :THEME ?y3 :MOD ?y4)
(?x2 ?y2 (? type2 ONT::PERSON)))
(?x3 ?y3 (? type3 ONT::TREATMENT ONT::MEDICAL-DIAGNOSTIC) :DOMAIN-INFO ?!info)
List of LF patterns
-extract-person-has-treatment-or-medical-diagnostic>
(EVENT :type ?type1 :class occurrence :subject ?y2 :object ?y3)
Unique rule ID
Output Specification
Figure 4: An example extraction rule
149
tory and extract  them using extraction rules as de-
scribed above. Figure 5 shows a rule to extract 
substances by matching any LF with a substance 
concept (as mentioned already, subclasses such as 
pharmacologic substances, would also match).
The rule in Figure 5 checks the :quantifier role 
and its value (e.g., none) is used to infer the pres-
ence or the absence of concepts. Using similar 
rules, we extract  additional concepts such as 
medical-disorders-and-conditions, physical-
symptom, treatment, medical-diagnostic, medical-
action and clinical-finding. Here, medical-action 
and clinical-finding are to extract concepts in a 
broader sense.
2
 To cover additional concepts, we 
can straightforwardly update extraction rules.
4.3 Temporal Expression Extraction
Temporal expressions are also extracted in the 
same way but using different  LF patterns. We have 
14 rules to extract  dates and time-spans of varying 
levels of complexity; for the example in Figure 1 
six of these rules were applied. Figure 6 shows LF 
patterns for a rule to extract temporal expressions 
of the form ?until X days/months/years ago?; for 
example, here is what the rule extracts for ?until 3 
days ago?:
(extraction :type time-span :context-rel (:* 
ont::event-time-rel w::until) :reference (time-position 
:context-rel (:* ont::event-time-rel w::ago) :amount 3 
:unit (:* ont::time-unit ont::day))) 
From this type of output, other reasoners can 
easily access necessary information about given 
temporal expressions without investigating the 
whole LF representation on their own.
4.4 Event Extraction
To construct timelines, the concepts of interest 
(Section 4.2) and the temporal expressions (Sec-
tion 4.3) should be pieced together. For that pur-
pose, it  is critical to extract events because they not 
only describe situations that  happen or occur but 
also represent states or circumstances where some-
thing holds. Furthermore, event features provide 
useful cues to reason about  situations surrounding 
extracted clinical concepts.
Here, we do not formally define events, but  refer 
to (Sauri et  al., 2006) for detailed discussion about 
events. While events can be expressed by multiple 
means (e.g., verbs, nominalizations, and adjec-
tives), our extraction rules for events focus on 
verbs and their features such as class, tense, aspect, 
and polarity. Figure 7 shows a rule to extract  an 
event  with the verb ?start? like the one in Line 4, 
?She was started on Afinitor on 03/05/08?. The 
output specification from this rule for Line 4 will 
have the :class, :tense, and :passive roles as (aspec-
tual initiation), past, and true respectively.
These event  features play a critical role in con-
structing timelines (Section 5). For instance, the 
event  class (aspectual initiation) from applying the 
rule in Figure 7 to Line 4 implies that  the concept 
?Afinitor? (a pharmacologic-substance) is not  just 
something tried on the given date, 03/05/08,  but 
something that continued from that date.
4.5 Relation Information Extraction
The relations among extracted concepts (namely, 
conjoined relations between events and set rela-
tions between clinical concepts) also play a key 
role in our approach. When events or clinical con-
cepts are closely linked with such relations, heuris-
tically, they tend to share similar properties that are 
exploited in constructing timelines as described in 
Section 5.
5 Building Timelines from Extracted Results
Extracted clinical concepts, temporal expressions, 
events, and relations (Section 4) are used as a 
2
 While concept classification into certain categories is a very important task in the medical domain, sophisticated 
concept categorization like the one specified in the 2010 i2b2/VA Challenge (https://www.i2b2.org/NLP/Relations/) 
is not the primary goal of this paper. We rather focus on how to associate extracted concepts with other events and 
temporal expressions to build timelines.
(?x1 ?y1 (:* ont::event-time-rel w::until) :val ?val) 
(?x2 ?val (? type2 ont::time-loc) :mod ?mod) 
(?x3 ?mod (? type3 ont::event-time-rel) :displacement 
?displacement) 
(?x4 ?displacement (? type4 ont::quantity) :unit ?unit 
:amount ?amount) 
(?x5 ?amount ont::number :value ?num)
Figure 6: LF patterns to extract a time-span
((?x1 ?y1 (? type1 ONT::SUBSTANCE) :domain-info 
?info :quantifier ?quan)
-extract-substance>
(extraction :type substance :concept ?type1 :umlsinfo 
?info :ont-term ?y1 :quantifier ?quan))
Figure 5: A rule to extract substances
150
foundation to construct timelines that  represent 
patients? medical history. In this section, we pre-
sent  timeline construction processes (as shown in 
Figure 8), using example sentences from Section 2.
Step 1: We first  make connections between events 
and clinical concepts. In the current system, events 
and clinical concepts are extracted in separate rules 
and their relations are not always explicit  in the 
output specification of the rules applied. For in-
stance, Figure 9 shows LFs for the sentence in Line 
7 in a graph format, using simplified LF terms for 
illustration. The clinical concept ?prednisone? and 
the event  ?received? get  extracted by different 
rules and the relation between them is not explicit 
in their output specifications.
To address such a case, for a pair of an event 
and a clinical concept, we traverse LF graphs and 
decide that a relation between them exists if there 
is a path that  goes through certain pre-defined con-
cepts that  do not separate them semantically and 
syntactically (e.g., concepts of measure-units, 
evidence/history, development, and some proposi-
tions).
Step 2: Second, we find temporal expressions as-
sociated with events. This step is relatively 
straightforward. While temporal expressions and 
events get  extracted separately, by investigating 
their LFs, we can decide if a given temporal ex-
pression is a modifier of an event. In Figure 9, the 
time-span-relation (i.e., ?in?) in the dotted-line box 
is a direct modifier of the event ?was treated?.
Step 3: Next, we propagate the association be-
tween events and temporal expressions. That is, 
when the relation between an event and a temporal 
expression is found, we check if the temporal ex-
pression can be associated with additional events 
related to the event  (esp. when the related events 
do not have any associated temporal expression). 
In Figure 9, the event  ?received? does not have a 
temporal expression as a modifier. However, it  is 
conjoined with the event  ?was treated? in the same 
past  tense under the same speech act. Thus, we let 
the event  ?received? share the same temporal ex-
pression with its conjoined event. Here, the con-
joined relation was extracted with relation rules 
described in Section 4.5, which allows us to focus 
on only related events.
Step 4: When temporal expressions do not have 
concrete time values within the expressions, we 
need to designate times for them by looking into 
information in their LFs:
?
Event references: The system needs to find the 
referred event  and gets its time value. For in-
stance, in ?4 weeks after starting Afinitor? (Line 
6),  ?starting Afinitor? refers to a previous event 
in Line 4. The system investigates all events with 
a verb with the same- or sub-type of ont::start 
and Afinitor as its object  (active verbs) or its 
subject (passive verbs). After resolving event 
references, additional time reference or relation 
computation may be required (e.g., computation 
for ?4 weeks after?).
?
Time references: Concrete times for expressions 
like the above example ?N weeks after 
<reference-time>? can be easily computed by 
checking the time displacement  information in 
LFs with the reference time. However, expres-
sions such as ?N days ago?  are based on the 
context of clinical records (e.g., record creation 
(?x1 ?ev (? type1 ont::start) :affected ?affected :tense ?tense :passive ?passive :progressive ?progresive 
  :perfective ?perfective :negation ?negation)
-extract-start-event>
(EVENT :type ?type1 :class (aspectual initiation) :subject ?affected :object null :tense ?tense :passive
   ?passive :progressive ?progresive :perfective ?perfective :negation ?negation :ont-term ?ev)
Figure 7: An event extraction rule example
Inputs: Clinical concepts, Temporal 
Expressions, Events, Relations, LFs
Outputs: Clinical concepts with associated dates 
or timespans.
Steps: 
1. Build links between events and clinical 
concepts
2. Find associated temporal expressions for 
events
3. Propagate temporal expressions through 
relations between events when applicable
4. Compute concrete time values for temporal 
expressions, taking into account the context of 
clinical records
5. Compute time values for clinical concepts 
based on their associated events
Figure 8: Pseudocode for Timeline Construction
151
time). Document creation time is usually repre-
sented as metadata attached to the document  it-
self, or it could be retrieved from a database 
where clinical records are stored. In addition, 
previously mentioned dates or time-spans can be 
referred to using pronouns (e.g., ?at  that/this 
time?). For such expressions, we heuristically 
decide that it refers to the most  recent  temporal 
expression.
?
Time relation: Some temporal expressions have 
directional time relations (e.g., ?until?, ?prior 
to?, and ?after?) specifying intervals with open 
ends. When the ending time of a time span is not 
specified (e.g., ?since 10/2007? in Line 10). We 
heuristically set it from the context of the clinical 
record such as the document creation time.
Step 5: Finally, we designate or compute times on 
or during which the presence or the absence of 
each clinical concept is asserted. Since temporal 
expressions are associated with events, to find time 
values for clinical concepts, we first check the rela-
tions between events and clinical concepts. When 
an event with a concrete time is found for a clinical 
concept, the event?s class is examined. For classes 
such as state and occurrence, the concrete time 
value of the event  is used. In contrast, for an aspec-
tual event, we check its feature (e.g., initiation or 
termination) and look for other aspectual events 
related to the clinical concept and compute a time 
span. For instance, regarding ?Afinitor?, Line 4 
and Line 9 have events with classes (aspectual ini-
tiation) and (aspectual termination) respectively, 
which leads to a time span between the two dates 
in Line 4 and Line 9. Currently, we do not resolve 
conflicting hypotheses. 
Assertion of Presence  or Absence  of Clinical 
Concepts: To check if a certain concept is present 
or not, we take into account quantifier information 
(e.g., none), the negation role values of events, and 
the verb types of events (e.g., ?deny? indicates the 
absence assertion). In addition to such information 
readily available in the output  specifications of the 
clinical concept- and event-extraction rules, we 
also check the path (as in Step 1) that relates the 
clinical concepts and the events, and the quantifiers 
of the concepts in the path are used to compute 
negation values. For instance, given ?The scan 
shows no evidence of lung nodules?, the quantifier 
of the concept  ?evidence? indicates the absence of 
the clinical finding ?lung nodules?.
6 Timeline Results and Discussion 
For the example in Section 2 (Line 1 ~ 12), we ex-
tract all the instances of the clinical concepts and 
the temporal expressions. Out of 23 events, 17 
were extracted. While we missed events such as 
state/was (Line 5), done (Line 8), and walk/do/
swim  (Line 10), our event  extraction rules can be 
extended to cover them if need be.
Figure 10 visualizes the extraction results of the 
example. We use a web widget tool called Simile 
Timeline (www.simile-widgets.org/timeline/). 
Some property values (that  were also extracted by 
rules) are shown alongside some concepts (e.g., 
weight  measurement). Note that not  all extracted 
clinical findings are displayed in Figure 10 because 
we visualize clinical concepts only when they are 
associated with temporal expressions in our LFs. 
For instance, the CT-scan on 05/14/08 in Line 8 is 
not shown because the date was not  associated 
with it due to fragmented LFs from the Parser. 
Figure 9: Graph format LFs of the sentence in Line 7 -- ?She received a 5 day dose pack of 
                  prednisone and was treated with Augmentin in 05/2008.?
152
However, we were still able to extract ?no infil-
trates? and ?scan? from a meaningful fragment.
In addition to the fragmented LF issue, we plan 
to work on temporal reasoning for concepts in the 
sentences without explicit temporal expressions, 
and the current limited event reference resolution 
will be improved. We are also working on evalua-
tion with 48 clinical records from 10 patients. An-
notated results will be created as a gold-standard 
and precision/recall will be measured.
7 Related Work
Temporal information is of crucial importance in 
clinical applications, which is why it  has attracted 
a lot  interest over the last  two decades or more 
(Augusto, 2005). Since so much clinical informa-
tion is still residing in unstructured form, in par-
ticular as text  in the patient?s health record, the last 
decade has seen a number of serious efforts in 
medical NLP  in general (Meystre et  al., 2008) and 
in extracting temporal information from clinical 
text in particular. 
Some of this surge in interest  has been spurred 
by dedicated competitions on extraction of con-
cepts and events from clinical text (such as the 
i2b2 NLP challenges). At the same time, the evolu-
tion of temporal markup languages such as Ti-
meML (Sauri et al, 2006), and temporal 
extraction/inference competitions (such as the two 
TempEval challenges,  Verhagen et  al., 2009) in the 
general area of NLP have led to the development 
of tools such as TARSQI (Verhagen et  al., 2005) 
that could be adapted to the clinical domain.
Although the prevailing paradigm in this area is 
to use superficial methods for extracting and clas-
sifying temporal expressions, it has long been rec-
ognized that higher level semantic processing, in-
cluding discourse-level analysis, would have to be 
performed to get  past the limits of the current  ap-
proaches (cf. Zhou and Hripcsak, 2007). 
Recent attempts to use deeper linguistic features 
include the work of  Bethard et  al. (2007), who 
used syntactic structure in addition to lexical and 
some minor semantic features to classify temporal 
relations of the type we discussed in Section 4.3. 
Savova and her team have also expressed interest 
in testing off-the-shelf deep parsers and semantic 
role labelers for aiding in temporal relation identi-
fication and classification (Savova et al, 2009); 
although we are not  aware of any temporal extrac-
tion results yet, we appreciate their effort in ex-
panding the TimeML annotation schema for the 
clinical domain, as well as their efforts in develop-
ing corpora of clinical text  annotated with temporal 
information.
The work of Mulkar-Mehta et  al. (2009) also 
deserves a mention, even though they apply their 
techniques to biomedical text rather than clinical 
text. They obtain a shallow logical form that repre-
sents predicate-argument relations implicit  in the 
syntax by post-processing the results of a statistical 
parser. Temporal relations are obtained from the 
shallow LF based on a set  of hand-built rules by an 
abductive inference engine.
To our knowledge, however, our system is the 
first  general-purpose NLU system that produces a 
full, deep syntactic and semantic analysis of the 
text as a prerequisite to the extraction and analysis 
of relevant clinical and temporal information.
8 Conclusion
In this paper, we presented a prototype deep natu-
ral language understanding system to construct 
timelines for the medical histories of patients. Our 
approach is generic and extensible to cover a vari-
ety of narrative clinical text  records. The results 
from our system are promising and they can be 
used to support medical decision making.
9 Acknowledgement
This work was supported by the National Cancer 
Institute and the H. Lee Moffitt  Cancer Center and 
Research Institute (Award # RC2CA1488332).
Figure 10: Visualization of timeline results
153
References 
James Allen, Donna Byron, Myroslava Dzikovska, 
George Ferguson, Lucian  Galescu, and Amanda 
Stent.  2000. An architecture for a generic dialogue 
shell. Journal of Natural Language Engineering 
6(3):1?16.
Mary Swift, Nate Blaylock, James Allen, Will de 
Beaumont, Lucian Galescu, and Hyuckchul Jung. 
2010. Augmenting a Deep Natural Language Proc-
essing System with UMLS. Proceedings of the 
Fourth International Symposium on Semantic Mining 
in Biomedicine (poster abstract)
Alan R. Aronson and Fran?ois-Michel Lang.  2010. An 
overview of MetaMap: historical perspective and 
recent advances.  Journal of the American Medical 
Informatics Association. 17:229-236.
Juan C. Augusto. 2005.  Temporal reasoning for decision 
support in medicine. Artificial Intelligence in Medi-
cine, 33(1): 1-24.
Steven Bethard, James H.  Martin, and Sara Klingen-
stein. 2007. Timelines from Text: Identification of 
Syntactic Temporal Relations. In Proceedings of the 
International Conference on Semantic Computing 
(ICSC '07), 11-18.
Olivier Bodenreider. 2004. The Unified Medical Lan-
guage System (UMLS): integrating biomedical ter-
minology. Nucleic Acids Research, Vol. 32.
Jenny Rose Finkel, Trond Grenager, and Christopher 
Manning. 2005. Incorporating Non-local Information 
into Information Extraction Systems by Gibbs Sam-
pling.  Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics.
Dan Klein and Christopher D. Manning.  2003. Fast Ex-
act Inference with a Factored Model for Natural Lan-
guage Parsing. In Advances in Neural Information 
Processing Systems 15 (NIPS 2002), Cambridge, 
MA: MIT Press.
S.  M. Meystre, G. K. Savova, K. C. Kipper-Schuler, J. 
F. Hurdle. 2008. Extracting information from textual 
documents in the electronic health record: a review 
of recent research. IMIA Yearbook of Medical Infor-
matics.
George A. Miller. 1995. WordNet: A lexical database for 
English. Communications of the ACM, 38(5).
R. Mulkar-Mehta, J.R. Hobbs, C.-C. Liu, and X.J. Zhou. 
2009. Discovering causal and temporal relations in 
biomedical texts. In AAAI Spring Symposium, 74-
80.
Roser Sauri, Jessica Littman, Bob Knippen, Robert Gai-
zauskas, Andrea Setzer, and James Pustejovsky. 
2006. TimeML annotation guidelines. (available at 
http://www.timeml.org/site/publications/time 
MLdocs/annguide_1.2.1.pdf)
G. Savova, S. Bethard, W. Styler, J. Martin, M. Palmer, 
J. Masanz, and W. Ward. 2009. Towards temporal 
relation discovery from the clinical narrative. Pro-
ceedings of the Annual AMIA Symposium, 568-572.
Michael Stacey and Carolyn McGregor. 2007.  Temporal 
abstraction in intelligent clinical data analysis: A sur-
vey. Artificial Intelligence in Medicine, 39.
Kristina Toutanova and Christopher D. Manning. 2000. 
Enriching the Knowledge Sources Used in a Maxi-
mum Entropy Part-of-Speech Tagger. In Proceedings 
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large 
Corpora (EMNLP/VLC-2000).
M. Verhagen, I. Mani, R. Sauri, R. Knippen, S.B. Jang, 
J. Littman, A. Rumshisky, J. Phillips, and 
J. Pustejovsky. 2005. Automating temporal annota-
tion with TARSQI. In Proceedings of the ACL 2005 
on Interactive poster and demonstration sessions 
(ACLdemo '05), 81-84. 
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple, 
J. Moszkowicz, and J. Pustejovsky. 2009. The Tem-
pEval challenge: identifying temporal relations in 
text . Language Resources and Evaluation 
 43(2):161-179.
Li Zhou, Carol Friedman, Simon Parsons and George 
Hripcsak. 2005. System Architecture for Temporal 
Information Extraction,  Representation and Reason-
ing in Clinical Narrative Reports. Proceedings of the 
Annual AMIA Symposium.
Li Zhou and George Hripcsak. 2007. Temporal reason-
ing with medical data - A review with emphasis on 
medical natural language processing. Journal of 
Biomedical Informatics, 40.
154
Proceedings of the First Workshop on Metaphor in NLP, pages 36?44,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
 
 
Automatic Metaphor Detection using Large-Scale Lexical Resources and Conventional Metaphor Extraction   Yorick Wilks, Lucian Galescu, James Allen, Adam Dalton Florida Institute for Human and Machine Cognition 15, SE Osceola Ave Ocala, FL, 34471, USA {ywilks,lgalescu,jallen,adalton}@ihmc.us     Abstract The paper presents an experimental algorithm to detect conventionalized metaphors implicit in the lexical data in a resource like WordNet, where metaphors are coded into the senses and so would never be detected by any algorithm based on the violation of preferences, since there would always be a constraint satisfied by such senses. We report an implementation of this algorithm, which was implemented first the preference constraints in VerbNet. We then derived in a systematic way a far more extensive set of constraints based on WordNet glosses, and with this data we reimplemented the detec-tion algorithm and got a substantial improvement in recall. We suggest that this technique could contribute to improve the performance of existing metaphor detec-tion strategies that do not attempt to detect convention-alized metaphors. The new WordNet-derived data is of wider significance because it also contains adjective constraints, unlike any existing lexical resource, and can be applied to any language with a semantic parser (and WN) for it. 1 Introduction Metaphor is ubiquitous in standard language; it is not a fringe or add-on phenomenon. The work de-scribed concerns detecting and interpreting meta-phor on a large scale in corpora. If metaphor is ubiquitous, then locating and interpreting it must be central to any NLP project that aims to under-stand general language. This paper focuses on the initial phase of detection: the identification in text of conceptual combinations that might be deemed metaphoric by a pre-theoretic observer, e.g., ?Bra-zil has economic muscle?, ?Tom is a brick?, or ?The unions have built a fortress round their pen-sions?.  There is a long cultural tradition of de-
scribing and interpreting such phenomena but our goal here is computational: to provide criteria for automatically detecting such cases as candidates for further analysis and interpretation. The key fact is that metaphors are sometimes new and fresh but can be immediately understood: producing them is often the role of poets, creative journalists and writers of all kinds. But many are simply part of the history of the language, and are novel only to those who do not happen to know them already: for example ?Tom is a brick? ? taken to mean that he is a reliable man, but which cannot be literally true ? is actually encoded as a sense of brick in WordNet (WN) (Miller, 1995) even though it is more familiar to UK than US English speakers. This means that lexical resources already con-tain conventionalized metaphors. We propose a simple method for locating and extracting these into the metaphor candidate pool, even when they are not indicated as such in resources like WN (which marks figurative senses very infrequently, unlike some traditional dictionaries). However, we believe these implicit metaphors in WN ? a re-source we intend to use as a semantic/lexical data-base, though transformed as we shall show below ? can be extracted by a simple algorithm, and with-out any need for a priori distinction of literal ver-sus metaphorical.  That distinction, as we noted, depends to a large degree on the temporal snapshot of a language; e.g., no one now would think ?tak-ing a decision? was metaphor, even though deci-sions are not literally taken anywhere. In this paper, we shall present an algorithm for conventionalized metaphor detection, and show results over a standard corpus of examples that 
36
  
demonstrate a possible useful gain in recall of metaphors, our original aim. The algorithm is de-scribed in two implementations (or pipelines) cor-responding, respectively, to the use of WN and VerbNet (Kipper et al, 2000; Kipper et al, 2008) as semantic knowledge-bases, and to their re-placement by our automatically recomputed form of WN, which enables predictions about the pref-erence behavior (see below) of English verbs and adjectives to be better founded than in VerbNet (VN) and on a much larger scale. 2 Background on Metaphor Detection us-ing Preference Violation as Cue In early work on metaphor detection, long preced-ing access to large-scale or annotated corpora, it was suggested as sufficient a criterion for being a metaphor that a ?semantic preference? of a verb or adjective was violated (Wilks, 1978). So, for ex-ample, one might say that the verb drink had a preference for animate agents and liquid objects, in which case ?My car drinks gasoline? violates its subject preference, which might then be a cue to look for metaphor at that point. Similarly, in the ?economic muscle? case mentioned earlier one might say that economic has a preference for ab-stract entities as objects, as in ?economic value?, and muscle is not an abstract entity.  There was discussion in those early days of syntac-tic-semantic interface cases like ?John ran a mile? where a mile might be said to violate the prefer-ence of the (intransitive) verb for a zero object and so again trigger a metaphor. The preference notion was not initially intended to detect metaphor but to semantically disambiguate candidates at those sites by preferring those conceptual entities that did not violate such restrictions. In early work, preferences were largely derived by intuition and sometimes ordered by salience. Later (e.g. Resnik, 1997) there was a range of work on deriving such preferences from corpora; however, in VN the semantic prefer-ences of verbs were again largely intuitive in ori-gin.      Early work linking preference violation to metaphor detection (summarised in Fass and Wilks, 1983, also Martin 1990) worked with hand-crafted resources, but by 1995 Dolan had noted (Dolan, 1995) that large-scale lexical resources would have implications for metaphor detection, and WN was used in conjunction with corpora, by 
(Peters and Wilks, 2003) using symbolic methods and by Mason (2004) and Krishnakumaran and Zhu (2007) using a combination of WN and statis-tical methods. Mason also acquires preferences automatically from corpora, and the latter two pa-pers treat metaphor as a form of anomaly based on rare combinations of surface words and of WN-derived hypernyms, a notion that appears in (Guthrie et al, 2007) but based only on corpus sparsity and not WN codings. Other work on the automatic acquisition of preferences (McCarthy and Carrol, 2003) for WSD has also its considered extension to the detection of classes of metaphor. More recently, work by Shutova (Shutova et al, 2010) has shown that the original preference viola-tion insight can be combined with large-scale in-vestigations, using notions of machine learning and large-scale resources like WN. Our approach is smaller scale and does not involve machine learn-ing: it simply seeks access to implicit metaphors built into the structure of WN by its creators, and which a preference-violation detection criterion cannot, by definition, access. Thus, we view our contribution as complementary to larger efforts on metaphor and interpretation detection, rather than a competing approach. We have not made compari-sons here with the work of (Li and Sporleder, 2010), which is explicitly concerned with idioms, nor with (Markert and Nissim, 2009) which is fo-cused on metonymy. 3 The Conventional Metaphor Detection Hypotheses Where WN codes conventionalized metaphors as senses, as in the initial cases described, then the senses expressing these will NOT violate prefer-ences and so will not be detected by any metaphor-as-violation hypothesis. For example, in ?Jane married a brick? this will not be a preference vio-lation against WN senses because WN explicitly codes brick as a reliable person, though we would almost certainly want to say this sentence contains a metaphor to be detected. The hypothesis we propose is simply this: if we have a word whose main (usually first) sense in WN fails the main preference for the sentence slot it fills, but has a lower, less frequent, sense that satisfies that preference, then we declare that lower sense a metaphorical one. In the case of brick, whose main sense is a PHYSICAL OBJECT, one 
37
  
which clearly fails the equivalence to Tom in the example ?Tom is a brick?. Yet the less frequent listed sense for a reliable person does satisfy the same preference. The work at this stage is not con-cerned with the metaphor-metonymy distinction and this criterion may well capture both, their dis-tinction being, as is well known (e.g. in Fass and Wilks, 1983) hard to establish in the limit. Ours is a purely empirical hypothesis and will work or not, and we argue that it does to a reasonable degree. It does not rest on any assumption of strict ordering of WN senses, only on a tendency (from literal to metaphorical) which is plainly there for any ob-server. 4 Metaphor Detection Experiments We have implemented two versions of conven-tional metaphor detection, using two different lexi-cal resources. We were thus able to divide the hypothesis into two parts, essentially one making use of VN and one within WN only.  In this first pipeline, we use WN together with the verb prefer-ences provided by VN even though those give only patchy coverage of common verbs. At the outset this was the only lexical resource for verb prefer-ences available. VN includes classes of verbs that map members to specific WN senses. VN also provides a hierarchy of verb object/subject inclu-sions, which we use for assessing whether one sen-tence object/subject type appears below another in this simple inclusion hierarchy, and so can be said to be semantically included in it. The selectional restrictions, however, are not linked to any lexi-cons so a mapping was constructed in order to al-low for automated detection of preference violations.  Our first experiment utilizes WN, VN, and the Stanford Parser (de Marneffe et al, 2006) and Named Entity Recognizer (Finkel et al, 2005).  The Stanford Parser identifies the verbs, as well as their corresponding subjects and direct objects. The Stanford Named Entity Recognizer was used to replace sequences of text representing names with WN senses whose hypernyms exist in the se-lectional restriction hierarchy. The first step in determining whether a sentence contains a metaphor is to extract all verbs along with the subject and direct object arguments for each verb.  The Stanford Parser dependencies used to describe the relationships between verbs and 
their arguments include agent, nsubj, and xsubj for subjects and dobj and nsubjpass for direct objects.  The parser also handles copular and prepositional verbs but additional steps are required to link these verbs to their arguments. Once verbs have been extracted and parameter-ized from the sentence, each is checked for prefer-ence violations. A preference is violated if a selectional restriction on one of the thematic roles of a VN class is not satisfied for all VN classes the verb is a member of.  In order for a VN class's preferences to be satisfied, there must be a WN sense for the argument of a verb such that either itself or its hypernym matches the WN senses al-lowed by the selectional restriction in VN class, where the terms in the VN hierarchy have been hand-matched to WN senses. If a sentence contains a verb that does not exist in VN then we must as-sume that it is not violated. 5 Conventionalized Metaphor Detection Closer inspection of false negatives revealed that many of the verbs and the arguments that satisfied their selectional restrictions were unannotated con-ventionalized metaphors.   5.1 Conventionalized Verbs In our approach, a conventionalized verb occurs when two VN Classes have the same member, but one maps to a lower WN sense (in the WN order-ing, which can be taken roughly to mean less fre-quent) than the other.  If the VN Class mapped to the lower sense is satisfied in a sentence, but the other VN Class is not, we say that the verb is used in a conventionalized sense. The verb pour  is a member of four VN classes.  Three of those classes, Pour-9.5, Preparing-26.3-2, and Sub-stance_Emission-43.4 all map to first sense of the word which means to cause to run.  The fourth VN class of pour, Weather-57, maps to the sixth WN sense of the verb, which means to rain heavily.  If we take the example sentence ?Bisciotti has poured money into the team?, we determine that all VN classes that map to the primary WN sense of pour are violated in some way. According to our semantic role labeling heuristic, Pour-9.5 expects money to be a substance, Preparing-26.3-2 ex-pects the team to be an animate, and Sub-stance_Emission-43.4 is violated because Bisciotti is animate.  The only Verb Class that is satisfied is 
38
  
Weather-57, and that class maps to the sixth sense of pour.  Interestingly, there is no VN class mem-ber that maps to the fifth WN sense (supply in large amounts or quantities). The pseudocode for detecting conventional metaphors used as verbs is as follows: ? for each VN Class ? for each member of that class ? for each WN sense of that member with Verb POS ? get the sense number of the WN sense ? associate the sense number to the verb member and selectional re-strictions for the Verb Class ? given a verb in a sentence, decide that the verb is conventionalized if: ? it satisfies the selectional re-strictions of one Verb Class V1 but? ? it violates the selectional restric-tions of another Verb Class V2 and?  ? the sense number of the verb member in V2 is above the sense number of the verb member in V1  5.2 Conventionalized Nouns Let us look again at the example of brick, where the primary sense of the noun is the building mate-rial most are familiar with and the secondary sense refers to a reliable person. For this reason, the noun brick will satisfy any VN class that requires a hu-
man or animate. Without the ability to detect con-ventional metaphors in noun arguments, She married a brick would pass through without detec-tion by preference violation. Here are the WN en-tries for the two senses: ? brick#1 (brick%1:06:00::) (rectangular block of clay baked by the sun or in a kiln; used as a build-ing or paving material)  ? brick#2 (brick%1:18:00::) (a good fellow; helpful and trustworthy)  Less obvious are more abstract words such as zone: ? zone#1 (zone%1:15:00::) (a locally circumscribed place characterized by some distinctive features) ? zone#2 (zone%1:15:02::), geographical zone#1 (geographical_zone%1:15:00::) (any of the re-gions of the surface of the Earth loosely divided ac-cording to latitude or longitude)  ? zone#3 (zone%1:15:01::) (an area or region dis-tinguished from adjacent parts by a distinctive fea-ture or characteristic)  ? zone#4 (zone%1:08:00::), zona#1 (zona%1:08: 00::) ((anatomy) any encircling or beltlike struc-ture)  Zone's primary sense, again, is the anticipated con-cept of circumscribed space. However, the fourth sense deals with anatomy, and therefore is a hypo-nym of body part.  Body part is capable of satisfy-ing any thematic role restricted to animate arguments.  
 Figure 1. Conventionalized verb metaphor detection using WordNet senses  and VerbNet selectional restrictions 
VerbNet WordNet
Parser
Named Entity 
Recognizer
Interface
Metaphor 
Detector
Extract verbs 
and arguments
Is sentence 
a metaphor?
Replace named enitties
Get WordNet hypernym sets for arguments
Find all VerbNet Classes for each verb
Which WordNet senses 
satisfy Selectional 
Restrictions
[None]
Sentence contains 
a metaphor
[One or more]
Set of senses that satisfy selectional restrictions
Does the member of 
the Verb Classes 
satisfied map to the 
primary sense?
[Yes]
No metaphor
[No]
Conventionalized Metaphor
39
  
The pseudocode for detecting conventional metaphors used as nouns is as follows: ? determine if verbs? subjects and di-rect objects satisfy the restriction ? if not, it is a Preference Violation metaphor ? if they do: ? determine if the sense of the sat-isfying word is the primary sense in WN ? if not, it is a conventional metaphor ? otherwise, it is not a metaphor Thus, our overall hypothesis is intended to locate in the very broad WN sense sets those that are ac-tually conventionalized metaphors: we determine that only the first sense, hopefully literal, should be able to satisfy any restriction.  If a lower sense sat-isfies a verb, but the primary sense does not, we classify the satisfaction as being conventionalized, but a metaphor nonetheless.  6 Deriving Preferences and an Ontology from WordNet To date, VerbNet is the most extensive resource for verb roles and restrictions. It provides a rich semantic role taxonomy with some selectional re-strictions. Still, VN has entries for less than 4000 verbs. PropBank (Palmer et al, 2005) has addi-
tional coverage, but uses a more surface oriented role set with no selectional restrictions. On the other hand, WordNet has many more verb entries but they lack semantic role information. However, we believe it is possible to extract automatically a comprehensive lexicon of verbs with semantic roles and selectional restrictions from WN by processing definitions in WN using deep under-standing techniques. Specifically, each verb in WN comes with a gloss that defines the verb sense, and there we can find clues about the semantic roles and their selectional restrictions. Thus, we are test-ing the hypothesis that the semantic roles of the verb being defined are inherited from the roles in its definition, though roles in the latter may be elided or fully specified. For example, consider this entry from WN for one of the senses of the verb kill: S: (v) kill (cause to die; put to death, usually inten-tionally or knowingly) ?This man killed several people when he tried to rob the bank?; ?the farmer killed a pig for the holidays? Let us assume we already know that the verb cause takes three roles, say, a CAUSER, an AFFECTED and an EFFECT role; this leads us to hypothesize that kill would take the same roles. However, the EFFECT role from cause is not inherited by kill as it is fully specified in the definition. The proof of 
  Figure 2. Conventionalized noun metaphor detection using WordNet senses  and VerbNet selectional restrictions 
40
  
this hypothesis is ultimately in how well it predicts the role set. But intuitively, any role in the defini-tion verb (i.e., cause) that is fully filled in the defi-nition has no ?space? for a new argument for that role. Therefore, we conclude that kill takes two roles, filling the CAUSER and AFFECTED roles in the definition. We can now derive selectional restrictions for kill by looking at inherited restrictions from the definition, as well as those that can be derived from the examples. From the definition, the verb cause puts little to no restriction on what the CAUSER role might be. For instance, an animal may cause something, but natural forces cause things as well. Likewise, cause puts little con-straint on what the PATIENT role might be, as one can cause the temperature to rise, or an idea to fade. The restriction from the verb die in the com-plement, however, suggests a restriction of some living object (if we can derive this constraint from die).  We also look at the examples to find more informative restrictions. In the definition of kill, we have two examples of a CAUSER, namely a man and a farmer. Given the hypernym hierarchy of nouns in WordNet, we could look for the most specific subsuming concept in the hierarchy for the concepts MAN and FARMER, finding it to be person%1:03:00.  The fillers for the AFFECTED role in the examples are PEOPLE and PIG, with the most specific WN node being organ-ism%1:03:00). Putting all this together, we pro-duce an entry for kill as follows: kill:  ACTOR/person%1:03:00  PATIENT/organism%1:03:00 To implement this idea we need a number of capa-bilities. First, semantic roles do not appear out of the ether, so we need an initial seed of semantic 
role information. In addition, to process the glosses we need a parser that can build a semantic repre-sentation, including the handling of elided argu-ments. As a start, we use the TRIPS parser (Allen et al, 2008). The TRIPS lexicon provides informa-tion on semantic roles, and the parser can construct the required semantic structures. TRIPS has been shown to be successful at parsing WN glosses in order to build commonsense knowledge bases (Al-len et al, 2011). With around 3000 types, TRIPS offers a reasonable upper-level ontology to serve as the seed for semantic roles. We also use the TRIPS selectional restrictions to bootstrap the process of determining the restrictions for new words. To attain broad lexical coverage, the TRIPS parser uses input from a variety of external re-sources. This includes a subsystem, Wordfinder, for unknown word lookup that accesses WN when an unknown word is encountered. The WN senses have mappings to semantic types in the TRIPS on-tology, although sometimes at a fairly abstract level. When faced with an unknown word, the parser looks up the possible senses in WordNet, maps these to the TRIPS ontology and then uses the verb entries in the TRIPS lexicon associated with these types to suggest possible subcatgoriza-tion frames with mappings to roles. Thus, Word-finder uses the combined information from WN and the TRIPS lexicon and ontology to dynami-cally build lexical entries with approximate seman-tic and syntactic structures for words not in the core lexicon. This process may produce a range of different possibilities based on the different senses and possible subcategorization frames for the verbs that share the same TRIPS type. We feed all of these to the parser and let it determine the entries that best match the definition and examples. While WordNet may have multiple fine-grained senses for a given word, we set a parameter that has the system use only the most frequent sense(s) of the word (cf. McCarthy et al 2004). We use TRIPS to parse the definitions and glosses into a logical form. Figure 3 shows the logical form produced for the definition cause to die. We then search the logical form for structures that signal a potential argument that would fill a role. Besides looking for gaps, we found some other devices that serve the same purpose and oc-cur frequently in WordNet: 
  Figure 3: Abstracted Logical Form for ?cause to die? 
(F CAUSE-MAKE)
(IMPRO LSUBJ)
(IMPRO DOBJ)
(F DIE)
CAUSE
AFFECTED
EFFECT
EXPERIENCER
41
  
? elided arguments (an IMPRO in the logical form); ? indefinite pronouns (e.g., something, some-one); ? prepositional/adverbial forms containing an IMPRO or an indefinite pronoun (e.g., give a benediction to); ? a noun phrase in parentheses (e.g., to re-move (people) from a building). The final condition is probably a WN specific de-vice, and was discovered when working on a 10-verb development set, and occurred twice in that set. Once these arguments are identified, we have a candidate set of roles for the verb. We identify candidate selectional restrictions as described above. Here are a few examples of verbs and their automatically derived roles and restrictions, as computed by our system (here we indicate Word-Net entries by their sense index rather than their sense key, since the index is used in the conven-tional metaphor detection strategy ? see below): bend.v.06: AGENT/being.n.02     PATIENT/physical_entity.n.01 collect.v.03: AGENT /person.n.01     PATIENT/object.n.01 drive.v.01:  AGENT/person.n.01     PATIENT/motor_vehicle.n.01 play.v.13: CAUSE/instrumentality.n.03     EFFECT/music.n.01 walk.v.08: AGENT/being.n.02     GOAL/location.n.01 The techniques described in this section have been used to provide a set of roles with selectional re-strictions for the second IHMC pipeline, described below. The current system takes a list of verbs from a corpus and returns the role names and se-lectional restrictions for every sense of those words in WordNet. The transformations described here all equally able to produce preferences for adjectives, as would be needed to detect ?economic muscle? as a metaphor, which is a form of lexical information not present in any existing database, and the whole process can be applied to any language that pos-sesses a WordNet type lexical resource, and for which we have a capable semantic parser. Hence, these techniques are amenable to being used for detecting metaphorical usage in constructions other 
than just verb-subject and verb-object, as we do here. 7 Conventional Metaphor Detection based on WordNet-Derived Preferences The preferences and ontology derived from WN definitions greatly improve the mapping between selectional restrictions and WN sense keys.  This allows us to replace VN with a new lexical re-source that both improves performance, and re-duces the complexity of discovering preference violations.  In the new pipeline, we can reuse the capabilities developed to extract verbs and their parameters from a sentence.  We also reuse the tie-ins to WN that allow us to determine if one WN sense exists within another's hypernym set. It is the selectional restriction lookup that is greatly simpli-fied in the new lexicon, where verbs are mapped directly to WN senses. The conventional metaphor detection is also simplified because the WN senses are included in the responses to the looked up verbs, allowing us to quickly determine if a satis-fied verb is conventionalized or is satisfied with conventionalized arguments. 8 Results and Conclusion Figure 4 shows the results obtained in a metaphor detection task over a small corpus of 122 sen-tences. Half of these sentences have metaphors and half do not. Of the half that do, approximately half are metaphors about Governance and half are other metaphors. This is not any sort of principled cor-pus but a seed set chosen to give an initial leverage and in a domain chosen by the sponsor (Govern-ance); the selection and implicit annotation were 
	 ? Pipeline	 ?1	 ?(VerbNet	 ?SRs)	 ? Pipeline	 ?2	 ?(WordNet	 ?SRs)	 ?TP 24 50 FP 23 37 TN 48 24 FN 37 11 Precision 0.649 0.575 Recall 0.393 0.82 F1 0.49 0.676  Figure 4. Performance comparison between the first pipeline using VerbNet selectional restrictions (SRs) and the second pipeline using WordNet-derived se-lectional restrictions 
42
  
done by consensus by a large group of twenty or so collaborators. The notion of baseline is irrelevant here, since the choice for every sentence is simply whether it contains a metaphor or not, and could thus be said to be 50% on random assignment of those categories.     From the figures above, it can be seen that the second pipeline does give significant improvement of recall over the first implementation above, even though there is some loss of precision, probably because of the loss of the information in VN. One possibility for integrating a conventional metaphor extraction pipeline like ours with a general meta-phor detection pipeline (including, for example, pattern-based methods and top-down recognition from stored Conceptual Metaphors) would be to OR these two pipelines together and to hope to gain the benefits of both, taking anything as a metaphor that was deemed one by either. However, that is not our aim here: our purpose is only to test the hypothesis that using knowledge derived from existing lexical resources, in combi-nation with some form of the conventionalized metaphor hypothesis, we can achieve good recall performance. On this point we think we have shown the value of the technique. Acknowledgements This work was supported in part by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Defense US Army Research Labo-ratory contract number W911NF-12-C-0020, and NSF grant IIS 1012205. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copy-right annotation thereon.  Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessar-ily representing the official policies or endorse-ments, either expressed or implied, of IARPA, DoD/ARL, or the U.S. Government.  References James Allen, William de Beaumont, Nate Blaylock, George Ferguson, Jansen Orfan, and Mary Swift. 2011. Acquiring commonsense knowledge for a cog-nitive agent. In Proceedings of the AAAI Fall Sympo-sium on Advances in Cognitive Systems (ACS 2011), Arlington, Virginia. 
James Allen, Mary Swift, and Will de Beaumont. 2008. Deep semantic analysis of text. In Proceedings of the 2008 Conference on Semantics in Text Processing (STEP '08), Venice, Italy. pp. 343-354. Marie-Catherine de Marneffe, Bill MacCartney and Christopher D. Manning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006), pp. 449-454. William B. Dolan. 1995. Metaphor as an emergent property of machine-readable dictionaries. In Pro-ceedings of the AAAI 1995 Spring Symposium Series: Representation and Acquisition of Lexical Knowl-edge: Polysemy, Ambiguity and Generativity, pp. 27?32. Dan Fass and Yorick Wilks. 1983. Preference seman-tics, ill-formedness, and metaphor. American Journal of Computational Linguistics, 9(3):178?187. Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sam-pling. In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pp. 363-370. David Guthrie, Louise Guthrie, Ben Allison and Yorick Wilks. 2007. Unsupervised Anomaly Detection. In Proceedings of the 20th international joint confer-ence on Artifical intelligence (IJCAI'07), San Fran-cisco, CA, pp. 1624-1628. Karin Kipper, Hoa Trang Dang, and Martha Palmer. 2000. Class-based construction of a verb lexicon. In Proceedings of the 17th National Conference on Arti-ficial Intelligence, Austin, Texas. pp. 691-696. Karin Kipper, Anna Korhonen, Neville Ryant, and Mar-tha Palmer. 2008. A large-scale classification of Eng-lish verbs. Language Resources and Evaluation 42(1):21-40. Saisuresh Krishnakumaran and Xiaojin Zhu, 2007. Hunting Elusive Metaphors Using Lexical Re-sources, Proceedings of the Workshop on Computa-tional Approaches to Figurative Language, pp. 13-20.  Linlin Li and Caroline Sporleder. 2010. Linguistic Cues for Distinguishing Literal and Non-Literal Usage. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010), Beijing, China, pp.  683-691. Katia Markert and  Nissim Malvina. 2009. Data and Models for Metonymy Resolution. In Language Re-sources and Evaluation, 43(2):123-138. James Martin. 1990. A Computational Model of Meta-phor Interpretation. Academic Press. Zachary J. Mason. 2004. Cormet: A computational, cor-pus-based conventional metaphor extraction sys- tem. Computational Linguistics, 30(1):23?44. 
43
  
Diana McCarthy and John Carrol. 2003. Disambiguat-ing nouns, verbs and adjectives using automatically acquired selectional preferences. Computational Lin-guistics. 29(4): 639-654. Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant word senses in untagged text. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguis-tics (ACL '04), Barcelona, Spain. pp. 280-287. George Miller. 1995. Wordnet: A lexical database for English. Communications of the ACM, 38(11):39-41. Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Se-mantic Roles. Computational Linguistics, 31(1):71-106. Wim Peters and Yorick Wilks. 2003. Data-Driven De-tection of Figurative Language Use in Electronic Language Resources, Metaphor and Symbol, 18(3): 161-174. Philip Resnik, 1997. Selectional preference and sense disambiguation. In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What and How?, Washington, DC, pp. 52-57. Ekaterina Shutova, Li-ping Sun and Anna Korhonen. 2010. Metaphor Identification Using Verb and Noun Clustering. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010), Beijing, China, pp. 1002-1010. Yorick Wilks, 1978. Making Preferences More Active. Artificial Intelligence, 11(3):197-223.  
44
