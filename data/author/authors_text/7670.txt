St ructura l  Feature  Se lect ion  For Eng l i sh -Korean  Stat is t ica l  
Mach ine  Trans la t ion  
Seonho Kim, Juntae Yoon, Mansuk Song 
{ pobi, j tyoon, mssong} @decemb er.yonsei, ac.kr 
\ ] )ept .  of  Computer  Science, 
Yonsci  Univers i ty ,  Seoul,  Korea  
Abstract 
When aligning texts in very different languages such 
as Korean and English, structural features beyond 
word or phrase give useful intbrmation. In this pa- 
per, we present a method for selecting struetm'al 
features of two languages, from which we construct 
a model that assigns the conditional probabilities 
to corresponding tag sequences in bilingual English- 
Korean corpora. For tag sequence mapl)ing 1)etween 
two langauges, we first, define a structural feature 
fllnction which represents tatistical prol)erties of 
elnpirical distribution of a set of training samples. 
The system, based on maximmn entrol)y coneet)t, se- 
le(:ts only ti;atures that pro(luee high increases in log- 
likelihood of training salnl)les. These structurally 
mat)ped features are more informative knowledge for 
statistical machine translation t)etween English and 
Korean. Also, the inforum.tion can help to reduce the 
1)arameter sl)ace of statisti('al alignment 1)y eliminat- 
ing synta(:tically uiflikely alignmenls. 
1 Introduction 
Aligned texts have been used for derivation of 1)ilin- 
gual dictioimries and terminoh)gy databases which 
are useflfl for nlachine translation and cross lan- 
guages infornmtion retriewfl. Thus, a lot of align- 
ment techniques have been suggested at; the sen- 
tence (Gale et al, 1993), phrase (Shin et al, 1996), 
nomt t)hrase (Kupiec, 1993), word (Brown et al, 
1993; Berger et al, 1996; Melamed, 1997), collo- 
cation (Smadja et al, 1996) and terminology level. 
Seine work has used lexical association measures 
for word alignments. However, the association mea- 
sures could be misled since a word in a source lan- 
guage frequently co-occurs with more titan one word 
in a target language. In other work, iterative re- 
estimation techniques have beets emt)loyed. They 
were usually incorporated with the EM algorithm 
mid dynmnic progranmfing. In that case, the prob- 
al)ilities of aligmnents usually served as 1)arameters 
in a model of statistical machine translation. 
In statistical machine translation, IBM 1~5 mod- 
els (Brown et al, 1993) based on the source-chmmel 
model have been widely used and revised for many 
language donmins and applications. It has also 
shortconfing that it needs much iteration time for 
parameter estimation and high decoding complex- 
ity, however. 
Much work has been done to overcome the prob- 
lem. Wu (1996) adopted chammls that eliminate 
syntactically unlikely alignments and Wang et al 
(1998) presented a model based on structures of two 
la.nguages. Tilhnann et al (1997) suggested the 
dynanfie programming lmsed search to select the 
best alignment and preprocessed bilingual texts to 
remove word order differences. Sate et al (1998) 
and Och et al (1998) proposed a model for learn- 
ing translation rules with morphological information 
mid word category in order to improve statistical 
translation. 
Furthemlore, llla,lly researches assullle(t Olle-to- 
one correspondence due to the coml)lexity and com- 
Imtati(m time of statistical aliglunents. Although 
this assumption Ire'ned out t;o 1)e useful for align- 
ment of close lallguages uch as English and French, 
it is not, applicabh~ to very different languages, in 
particular, Korean and English where there is rarely 
(:lose corresl)ondence in order at the word level. For 
such languages, even phrase level alignment, not to 
mei~tion word aligmnent, does not gives good trans- 
lation due to structural diflbrence. Itence, structural 
features beyond word or t)hrase should t)e consid- 
ered to get t)etter translation 1)etween English and 
Koreml. In addition, the construction of structural 
bilingual texts would be more informative for ex- 
tracting linguistic knowledge. 
In this paper, we suggest a method for structural 
mat)t)ing of bilingual language on the basis of the 
maximum entorl)y and feature induction fl'alnework. 
Our model based on POS tag sequence mapl)ing has 
two advantages: First;, it can reduce a lot of 1)armne- 
ters in statistical machilm translation by eliminating 
syntactically unlikely aligmnents. Second, it: can be 
used as a t)reprocessor for lexical alignments of bilin- 
gual corpora although it; (:an be also exl)loited 1)y it- 
self tbr alignment. In this case, it would serve as the 
first stet) of alignment for reducing the 1)arameter 
sI)ace. 
439 
2 Mot ivat ion  
In order to devise parameters for statistical model- 
ing of translation, we started our research from the 
IBM model which has bee:: widely used by :nany 
researches. The IBM model is represented with the 
formula shown in (1) 
l 17t 
v(f, al ) = I I  I-I t(fJ l%)d(jlaj,m, l) 
i=1 j= l  
(1) 
Here, n is the fertility probability that an English 
word generates n h'end:  words, t is tim aligmnent 
probability that the English word c generates the 
French word f ,  and d is the distortion probability 
that an English word in a certain t)osition will gener- 
ate a lh'ench word in a certain 1)osition. This formula 
is Olm of many ways in which p(f, ale ) can tie writtm. 
as the product of a series of conditional prot)at)ilities. 
In above model, the distortion probability is re-- 
lated with positional preference(word order). Since 
Korean is a free order language, the probability is 
not t~asible in English-Korean translation. 
Furthermore, the difference between two lan- 
guages leads to the discordance between words that 
the one-to-one correst)ondence b tween words gen- 
erally does not keel). The n:odel (1), however, as-- 
sumed that an English word cat: be connected with 
multiple French words, but that each French word 
is connected to exactly one English word inch:ding 
the empty word. hl conclusion, many-to-:nany :nap-- 
pings are not allowed in this model. 
According to our ext)eri:nent, inany-to-nmny 
mappings exceed 40% in English and Korean lexical 
aligninents. Only 25.1% of then: can be explained 
by word for word correspondences. It means that we 
need a statistical model which can lmndle phrasal 
mat) pings. 
In the case of the phrasal mappings, a lot of pa- 
rameters hould be searched eve:: if we restrict the 
length of word strings. Moreover, in order to prop-- 
erly estimate t)arameters we need much larger voI-- 
ume of bilingual aligned text than it in word-for- 
word modeling. Even though such a large corpora 
exist sometimes, they do not come up with the lex-- 
ical alignments. 
For this problem, we here consider syntactic fea- 
tures which are importmlt in determining structures. 
A structural feature means here a mapt)ing between 
tag sequences in bilingual parallel sentences. 
If we are concerned with tag sequence alignments, 
it is possible to estimate statistical t)armneters in 
a relatively small size of corpora. As a result, we 
can remarkably reduce the problem space for possi- 
ble lexical alignments, a sort of t probability in (1), 
which improve the complexity of a statistical ma- 
chine translation model. 
If there are similarities between corresponding tag 
sequences in two language, tile structural features 
would be easily computed or recognized. However, 
a tag sequence in English can be often translated 
into a completely different tag sequence in Korean 
as follows. 
can/MD -+ ~-, cul/ENTR1 su/NNDE1 'iss/ AJMA 
da/ENTE 
It nmans that similarities of tag features between two 
languages are not; kept all the time and it is neces- 
saw to get the most likely tag sequence mappings 
that reflect structural correspondences between two 
languages. 
In this paper, the tag sequence mappings are ob- 
taind by automatic feature selection based on the 
maximum entropy model. 
3 Prob lem Set t ing  
In tiffs ctlat)ter, we describe how the features are 
related to the training data. Let tc be an English 
tag sequence and tk be a Korean tag sequence. Let 
Ts be the set of all possible tag sequence niapI)ings in 
a aligned sentence, S. We define a feature function 
(or a feature) as follows: 
1 pair(t~,tk) C "\]-s 
f(t~,tk) = 0 othcrwi.s'c 
It indicates co-occurrence information l)etween 
tags appeared in Ts. f(t?,tk) expresses the infor- 
mation for predicting that te maps into ta.. A fea- 
ture means a sort of inforination for predicting some- 
thing. In our model, co-occurrence information on 
the same aligned sentence is used for a feature, while 
context is used as a feature in Inost of systems using 
maximum entropy. It can be less informative than 
context. Hence, we considered an initial supervision 
and feature selection. 
Our model starts with initial seed(active) features 
for mapI)ing extracted by SUl)ervision. In the next 
step, thature pool is constructed from training sam- 
ples fro:n filtering and oifly features with a large gain 
to the model are added into active feature set. The 
final outputs of our model are the set of active t'ea- 
tures, their gain values, and conditional probabilities 
of features which maximize the model. Tim results 
can be embedded in parameters of statistical ma- 
chine translation and hell) to construct structural 
bilingual text. 
Most alignment algorithm consists of two steps: 
(1) estimate translation probabilities. 
(2) use these probabilities to search for most t)roba- 
ble alignment path. 
Our study is focused on (1), especially the part of 
tag string alignments. 
Next, we will explain the concept of the model. 
We are concerned with an ot)timal statistical inodel 
which can generate the traiifing samples. Nmnely, 
our task is to construct a stochastic model that pro- 
440 
(1) duces outl)ut tag sequenc0, "~k, given a tag sequence ~+~,-~. 
To The l)roblem of interest is to use Salnt/les of ? --J~\,What .... 
tagged sentences to observe the/)charier of the ran- u~, ,~ 
(loin t)roeess. 'rile model p estinmtes tile conditional tt'2,Y 
probability that tile process will outlmt t,~, given t~.. ~ ,~/~ ,o~!! 
It is chosen out of a set of all allowed probability o~,~ ~ 
e}~..?0,, me (tistributions . . . .  
The fbllowing steps are emt)loyed for ()tit" model, v~ / 
Input: a set L of POS-labeled bilingual aligned 
sentences. 
I. Make a set ~: of corresl)ondence pairs of tag 
sequences, (t~, tk) from a small portion of L by 
supervision. 
2. Set 2F into a set of active features, A. 
3. Maximization of 1)arameters, A of at:tire fea- 
tures 1)y I IS(hnproved Iterative Sealing) algo- 
rithm. 
4. Create a feature pool set ?9 of all possible align- 
nmnts a(t(,, tk) from tag seqllellces of samples. 
5. Filter 7 ) using frequency and sintilarity with M. 
6. Coml)ute the atit)roximate gains of tkmtm:es in 
"p. 
7. Select new features(A/') with a large gain vahle, 
and add A. 
Outt)ut: p(tklt~,)whcrc(t(,, t~.) C M and their Ai. 
We I)egan with training samples comi)osed of 
English-Korean aligned sentence t)airs, (e,k). Since 
they included long sentences, w(', 1)roke them into 
shorter ones. The length of training senl;en(:es was 
limited to ml(h',r 14 on the basis of English. It is 
reasona,bh; \])(',(:&llSe we are interested in not lexical 
alignments lint tag sequence aliglmients. The sam- 
ples were tagged using brill's tagger and qVIorany' 
that we iml)lenmnted as a Korean tagger. Figure \] 
shows the POS tags we considered. For simplicity, 
we adjusted some part of Brill's tag set. 
In the, sut)ervision step, 700 aligned sentences were 
used to construct he tag sequences mal)I)ings wlfich 
are referred to as an active feature set A. As Fig- 
ure 2 shows, there are several ways in constructing 
the corresl)ondem;es. We chose the third mapping 
although (1) can be more useflll to explain Korean 
with I)redieate-argunmnt structure. Since a subject 
of a English sentence is always used for a subject 
tbrln in Korean, we exlcuded a subject case fi'onl ar- 
gulnents of a l/redicate. For examl)le, 'they' is only 
used for a subject form, whereas 'me' is used for a 
object form and a dative form. 
II1 tile next step, training events, (t,:, It.) are con- 
structed to make a feature 1)eel froln training sam- 
pies. The event consists of a tag string t,, of a English 
(2) (31 
? ~ lm-  
~" - -  + ~ '~Whatever  
Figure 2: Tag sequence corresl)ondences at the 
phrase level 
1)OS-tagged sentence and a tag string tL~ of the cor- 
responding Korean POS-tagged sentence and it Call 
be represented with indicator functions fi(t~, tk). 
For a given sequence, the features were drawn 
fl'om all adjacent i)ossible I)airs and sonic interrupted 
pairs. Only features (tci, tfii ) out of the feature pool 
that meet the following conditions are extracted. 
? #(l, ei,t~:i) _> 3, # is count 
? there exist H:.~,, where (t(,i,tt.~.) in A and the 
similarity(sanle tag; colin|;) of lki an(1 tkx _> 0.6 
Table \] shows possible tL'atures, for a given aligned 
sentence , 'take her out - g'mdCOrcul baggcuro 
dcrfleoflara'. 
Since the set of the structural ti;atm'es for align- 
ment modeling is vast, we constructed a maximum 
entrol)y model for p(tkltc) by the iterative model 
growing method. 
4 Maximum Ent ropy  
To explain our method, we l)riefly des(:ribe the con- 
(:ept of maximum entrol)y. Recently, many al)- 
lnoaches l)ased on the maximum entroi)y lnodel have 
t)een applied to natural anguage processing (Berger 
eL al., \]994; Berger et al, 1996; Pietra et al, 1997). 
Suppose a model p which assigns a probability to 
a random variable. If we don't have rely knowledge, 
a reasonal)le solution for p is the most unifbrnl dis- 
tribution. As some knowledge to estilnate the model 
p are added, tile solution st)ace of p are more con- 
strained and the model would lie close to the ol\]timal 
probability model. 
For the t)url/ose of getting tile optimal 1)robability 
model, we need to maxi\]nize the unifl)rnlity under 
some constraints we have. ltere, the constraints are 
related with features. A feature, fi is usually rel/re - 
sented with a binary indicator funct, ion. The inlpor- 
tahoe of a feature, fi can be identified by requiring 
that the model accords with it. 
As a (:onstraint, the expected vahle of fi with re- 
spect to tile model P(fi) is supposed to be the same 
as tile exl)ected value of fi with respect o empiri(:al 
distril)ution in training saml)les, P(fi). 
441 
TAG 
cb 
DT 
PW 
JJ 
JJS 
MD 
NNP 
PDT 
PRP 
RB 
RBS 
SYM 
UH 
VBD 
VBN 
WP$ 
NOT 
BED 
BEG 
HVD 
DOD 
DESCRIPTION 
comma 
conjunction,coordinating 
determiner 
foreign word 
adjective, ordinal 
adjective, superlative 
modal auxiliary 
noun, proper, singular 
pre-determiner 
pronoun, personal 
adverb 
adverb, superlative 
symbol 
interjection 
verb, past tense 
verb, past participle 
WH-pronoun, possessive 
not 
be verb, past tense 
be verb, present participle 
have verb, past participle 
do verb, past tense 
TAG 
CD 
EX 
IN 
J JR 
LS 
NN 
NNPS 
POS 
PRP$ 
RBR 
RP 
TO 
VBP 
VBG 
WDT 
WR8 
BEP 
BEN 
HVP 
DOP 
DON 
DESCRIPTION 
sentence terminator TAG 
numeral, cardinal 
existential there NNIN1 
preposition, subordinating NNIN2 
adjective, comparative NNDE1 NNDE2 list item marker PN 
noun, common NU 
noun, proper, plural VBMA 
genitive marker AJMA 
pronoun, possessive CO 
adverb, comparative AX 
particle ADCO 
to or infinitive marker APSE 
verb, present tense CJ 
verb, present participle ANCO 
WH-determiner ANDE 
WH-adverb ANNU 
be verb. present tense EX 
be verb, past participle LQ 
have verb, present tense RQ 
do verb, present tense SY 
do verb, past participle 
POS 
proper noun 
common noun 
common-dependent noun 
unit-dependent noun 
pronoun 
number 
verb 
adjective 
copula 
auxiliary verb 
constituent adverb 
sentential adverb 
conjunctive adverb 
configurative adnominal 
demonstrative adnominal 
numeral adnominal 
exclamination 
left quotation mark 
right quotation mark 
symbols 
TAG 
PPCA1 
PPCA2 
PPCA3 
PPCA4 
PPAD 
PPCJ 
PPAU 
ENTE 
ENCO1 
ENCO2 
ENCO3 
ENTRt 
ENTR2 
ENTR3 
ENCM 
PE 
SF 
PF 
CM 
SO 
Figure 1: English Tags (left) and Korean Tags (right) 
POS 
nominative postposition 
accusative postposition 
possessive postposition 
vocative postposition 
adverbial postposition 
conjunctive postposition 
auxiliary postposition 
final ending 
coordinate ending 
subordinate ending 
auxiliary ending 
adnominal ending 
nominal ending 
adverbial ending 
ending+postposition 
pre-ending 
suffix 
prefix 
comma 
termination 
~P~II'~ l lq  l l l l  l l~l '~l~t I l l i i LU i I f ( i |  I I  IL'II|II~ ;'~ ;~ l i l | ' t l l l  I I I l l  I LI~M 
\[VBI'+IN\] [take+out\] \[1+3\] 
\[wp\] \[tak(q \[t\] 
\[VBP+PI~P\] \[take+her\] \[1+2\] 
\[W3P+PRP+IN\] \[take+her+out\] \[1+2+3\] 
\[PRP\] [ho,-\] [2\] 
\[IN\] \[out\] \[3\] 
\[I'PCA2+P1)AD-FVBMA\] \[rcul+euro+deryeoga\] \[2+4+5\] 
\[PN\] b.... :/~o\] i l l  
\[PI)AI)+VBIvlA-FENTE\] \[reu/+cure-I- dcrycoga+ra\] \[4+5+6\] 
\[NNIN2\] [bagg \] \[3\] 
\[NNIN2+PPAD\] \[bagg+euro\] \[3+41 
\[ENTE\] \[ra\] [6\] 
\[P1)AD+VBMA\] \[curo+deryeoga\] \[4+5\] 
\[PPAD+VBMA+ENTE\] [euro+deryeoga+ra\] \[4+5+6\] 
\[PPCA2+NNIN2+PPAD+VBMA\] \[reul+bagg+curo+ deryeoga \] \[2+3+4+5\] 
\[PPCA2+NNIN2+PPAI)+VBMA+ENTE\] \[reul+bagg+euro+dcryeoga+ra \] \[2+3+4+5+6\] 
\[P1)CA2+NNIN2+PPAD+VBMA\] \[renl+deryeoga \] \[2+3+4+5\] 
\[PPCA2+NNIN2+Pt)AD+VBMA+ENTE\] \[reul+deryeoga+ra\] \[2+3+4+5+6\] 
Table 1: possible tag sequences 
In sun1, the maxilnunl entropy fralnework finds 
the model which has highest entropy(most uniform)~ 
given constraints. It is related to the constrained 
optimization. To select a model from a constrained 
set, C of allowed l)rol)ability distributions, the model 
p, C C with maximum entropy H(p) is chosen. 
In general, for the constrained optimization prob- 
lem, Lagrange inultipliers of the number of features 
can be used. However, it was proved that the model 
with maximum entropy is equivalent o the model 
that maximizes the log likelihood of the training 
samples like (2) if we can assume it as an exponential 
model. 
hi (2), the left side is Lagrangian of the condi-. 
tional entropy and the right side is inaxilnlHn log-. 
likelihood. We use the right side equation of (2) to 
select I. for the best model p,. 
~,g,,~..~,(- ~.,,. ~(~)v(yl~)logv(vlx)+~,,(v(f,)-~(/,))) (2) 
:a,'9,,,ax~, .,,. ~(x,v)lo.~n,(ylx) 
Since t ,  cannot be tbund analytically, we use 
the tbllowing improved iterative scaling algorithm to 
colnpute I ,  of n active features in .4 in total sam-- 
ples. 
1. Start with l i  = 0 for all i 6 {1 ,2 , . . . ,n}  
2. Do for ca.oh i ~ { \ ] ,2 , . . . ,n}  : 
(a) Let AAi be the solution to the log likeli- 
hood 
(b) Update the value of Ai into l i  + A,h, 
~. ..... ~(.,,v)A(:~,v) 
where AAi = log ~, : ,  ~i.~)v~(?11.~)/~(.~,v) 
px(yl:r) = ~-A--e(~ x'f'("':')) zx (:~.) ' , 
z (x) = E:, c(E ,  
3. Stop if not all the Ai have converged, otherwise 
go to step 2 
Tile exponential model is represented as(3). Here, 
l i  is the weight of feature f i .  In ore" model, since 
only one feature is applied to each pair of x and y, 
it can be represented as (4) and fi is the feature 
related with x and y. 
~(ylx) = ~ i  C'f'(x'Y) (3) 
cAifi(x,Y) 
= (4) 
442 
5 Feature  select ion 
Only a small subset of features will 1)e emph)yed in 
a model by sele(:ting useflfl feal;m'es from (;tie flmture 
1)ool 7 ). Let 1).,4 lie (;tie optimal mo(lel constrained 
by a set of active features M and A U J'i 1)e ,/lfi. Le(; 
PAf~ be the ot)timal model in the space of l)rol)abil- 
ity distribution C(Af i ) .  The optimal model can be 
tel)resented as (5). Here, the optimal model means 
a maxilmnn entropy nlodd. 
1 
v~ :, = z,,.(:,;) p'~ (:11,)::' ("'") 
zo,(: .)  = ~ v.~(::l:,,)c"S'(*'"> (5) 
Y 
The imi)rovement of l;he model regarding the ad- 
dition of a single feature f i  can be estiumted by mea- 
suring the difference of maximmn log-likelihood be- 
tween L(pAf~) and L(pA). We denote the gain of 
t~ature f i t i y  A(~lfi) an(l it can be r(!t/resented in
(G). 
A(A . I 'd  - .,..,:,;,,cAI~(.) 
('A:,(,,,) = J~(>t:,)-- L(v,O 
= _ ~(:~)~, .~( : , / I , . ) :  :'('''') 
x y 
?'~P(.fi) IS) 
Note that a model PA has a, set of t)arameters A 
which means weights of teatures. The m(idel P.Afl 
contains the l )a ra . lnetc . rs  an( I  the  new \[)a.l'a, lllCi;('~r (11 
with l'eSl)ect () the t'eal;ure fi. W'hen adding a new 
feature to A, the optimal values (if all parame(ers of 
probability (listril)u(,ion change. To make th(; (:om- 
i)utation of feature selection tractal)le, we al)l)roxi- 
mate that the addition of a feature f i  affec(;s only 
the single 1)aranxeter a, as shown in (5). 
qShe following a.lgoritlnn is used for (;omputing the 
gain of the model with rest)ect o fi. We referred 
to the studies of (Berger et al, 1996; Pietra e.t al., 
1997). We skip tile detailed contents and 1)root~. 
1. Let 
1 i f  P(fi) <_ PA(J;) 
r = -1  oth, erwise 
2. Set a0 = 0 
3. Repeat the following until GAf f (%, )  has con- 
verged : 
Co i l l l ) l l te  0@1,+ i frOll l  og n l lS i l lg  
a log (1 ! -~6t:' ( ' "~)  ~ ctn+l = (xn + 7" ,. (;~:~(~,,): 
Compute GaV~ (a,~+l) using 
GAA (a)  = - Ea,/3(a,') log Z,,(:,:) + ctf)(fi) , 
c'A:, (,~) = ~(k)  - Ex  ~(~0M( : , . ) ,  
G"  :ct~ A:,, , = -- E.~ P(")V2i:, ((fi -- M(;,;))" la;) 
set description # of disjoint total 
features cvtults 
A active feat m'es 1483 4113 
P feature Calldidat, es 3172 63773 
N new f'eaLures 97 5503 
Table 2: Summery of Features Selected 
where  (:~ ~ (l~n+ 1
A f~ = A u f~,  
M(z)  - p~f~ (fila-) , 
PP4S, (fi l ':) --- E .  ~'~s, (:,?l:r)k(:., ~J) 
d. Set ~ AL(Af i )  <-- GAS,(ct.) 
This algorittun is iteratively comtmted using Net- 
Wel l 'S  method. \?e cmt recognize the iml)ortance of a 
fl;ature with the gain value. As mentioned above, it 
means how much the feature accords with the model. 
We viewed the feature as tile information that Q. and 
t, occur together. 
6 Exper imenta l  resu l t s  
The total saml)les consists of 3,000 aligned Sellteiice 
pairs of English-Korean, which were extracted from 
news on the web site of 'Korea Times' and a. maga- 
zine fl)r English learning. 
In the initial step, we manually constucted (;tie 
correspondences of tag sequences with 700 POS- 
tagged sentence I)airs. hi the SUl)ervision step, 
we extracte(t l.,d83 correct tag sequence corresl)on- 
it(miles its shown in Table 2, and it work as active 
features. As a feature I)OOI, 3,172 (lisjoint %a(;ures 
of tag sequence ma.I)pings were retrieved. 1% is very 
important o make atomic thatures. 
We maxinfized A of active features with resl)ect 
to total smnples using improved the iterative scal- 
ing algoritlun. Figure 3 shows Ai of each feature 
.f(Q31,:P+.m,ttO C A. There a.re nlany corresl)on- 
dence 1)atterns with resl)ect o the Englsh tag string, 
'BEP+J J ' .  
Note that p(tt~lQ) is comtmted by the exponential 
model of (4) mid the conditional probability is the 
saine with empirical probal)ility in (7). Since the 
wflue of p(ylx)  shows the maxinmm likelihood, it is 
proved that each A was converged correctly.  
# of  (.% y) occurs in sam, pie 
P(ylx) - n, um, ber o f  t imes  o f  a: (7) 
hi feature selection step, we chose useflll fea- 
tures with the gain threshold of 0.008. Figure 
4 shows some feaures with a large gain. Anion\ 
then1, tag sequences mapping including 'RB'  are er- 
roneous.  It means that position of adverb in Ko- 
rean is very compl icated  to handle. Also, proper 
noun in English aligned coInmon nouns in Korean 
443 
English 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
Feature(x,y) 
Korean 
VBMA+ENCO3+AX+ENTE 
VBMA 
AJ MA 
AJMA+ENTE 
VBMA+ENTE 
NNIN2+CO 
NNIN2+CO+VBMA 
NNIN2+PPCA1 +VBMA+ENTE 
NNIN2+CO+ENTE 
NNIN2+PPCA2+AX+ENTE 
NNIN2+PPCA1 +VBMA 
1 
10.1369 
8.8520 
8.6787 
8.2628 
7.2379 
7.1372 
6.9909 
8.8402 
6.8308 
6.4256 
6.4250 
Figure 3: A 
\[PRP\] you ~/'\[PN\] cJ~ (dangsin) 
"-\[PPAU\] ~-(eun) 
\[RB\] usually \[ADCO\] EttX41_~(dachero) 
HVP\] have, /\[NNIN2\] ~uf~(ilbanseok) 
TO\] to /~ \[PPAD\] 0tl(e) 
\[VBP\] take ; / \ [VBMA\ ]  N(anj) 
,\[ENCO3\] O~OIE~(ayaman) 
\[J J\] regular . . . . . .  ':-:- \[AX\] "$F(ha) 
\[NN\] seating/' \[ENTE\] L E}(nda) 
Figure 5: Best Lexical alignment 
because of tagging errors. Note that in the case of 
'PN+PPCA2+PPAD+VBMA' ,  it is not an adjacent 
string but an interrupted string. It means ttlat a 
verb in English generally map to a verb taking as 
argument the accusative and adverbial postposition 
in Korean. 
One way of testing usefulness of our method is 
to construct structured aligned bil ingual sentences. 
Table 3 shows lexical al ignments using tag sequence 
al ignments drawn from our algorithm for a given 
sentence, 'you usually have to take regular seating 
- dangsineun dachcw ilbanscokc anjayaman handa' 
and Figure 5 shows the best lexical alignment of the 
sentence. 
We conducted the exi)eriment on 100 sentences 
composed of words in length 14 or less and siln- 
lilY chose the most likely paths. As tim result, the 
accuray was about 71.1~. It shows that we can 
partly use the tag sequence alignments for lexical 
alignments. We will extend tlle structural mapping 
model with consideration to the lexical information. 
The parmneters, the conditional probabilities about 
stuctural mappings will be embedded in a statisti- 
cal model. Table 4 shows conditional probabilities 
of seine features according to 'DT+NN'.  In general, 
determiner is translated into NULL or adnominal 
word in Korean. 
7 Conclus ion 
When aligning Englist>Korean sentences, the differ- 
ences of word order and word unit require structural 
information. For tiffs reason, we tried structural tag 
c(x,y) 
162.0C 
45.00 
39.00 
25.00 
9.00 
8.00 
7.00 
6.00 
6.00 
4.00 
4.00 
P(Ylx) 
0.4247 
0.1180 
0.0996 
0.0655 
0.0236 
0.0210 
0.0183 
0.0157 
0.0157 
0.0105 
0.0105 
Example 
English 
are+prepared 
are+careful 
am+healthy 
is+new 
am+sure 
am+rich 
is+selfish 
is+patriotic 
is+reasonable 
is+reprehensible 
is+helpful 
Korean 
~kllKl+0~+?~+~ LI El- 
0-94 8i 
~ J  8t+ ~ LI C\[ 
~X\[+01 
0191~+01+~LIEt 
011~;Xt+9\[+~+EF 
'NPJ ~ +01 +g~ 
.~N+01 +.El 
of active features in A 
t~ 
I)T-t-NN 
I)Tq-NN 
DT-FNN 
DT+NN 
DT-t-NN 
DT-FNN 
I)T-FNN 
etc 
t~ 
NNIN2 
ANI)E+NNIN2 
ANNUWNNDE2 
NNIN2+PPCA1 
NNIN2+NNIN2 
NNIN2-FPPAU 
ADCO 
eI;c 
p(t~l*~) 
0.524131 
0.15161 
0.091036 
0.063515 
0.058322 
0.05768 
0.049622 
Table 4: Conditional Probability 
string mapping using maximum entropy modeling 
and feature selection concept. We devised a nlodel 
that generates a English tag string given a Korean 
tag string. From initial active structural features, 
useful features are extended by feature selection. 
Tile retrieved features and parameters can be em- 
bedded in statistical maclfine translation and reduce 
the complexity of searching. We showed that they 
can helpful to construct structured aligned bilingual 
sentences. 
References  
Adam L. Berger, Peter F. Brown, Stephen A. 
Della Pietra, Vincent J. Della Pietra, John R. 
Gillett, John D. Lafferty, Robert L. Mercer, Harry 
Printz, and Lubos Ures. 1994. The Calldie sys- 
tem for machine translation, hi Proceedings of the 
ARPA Conference on Human Language Technol- 
ogy, Plainsborough, New Jersey. 
Adam L. Berger, Stephen A. Della Pietra, and Vin- 
cent J. Della Pietra. 1996. A maximum entropy 
approacll to natural anguage processing. Compu- 
tational Linguistics, 22(1):39-73. 
Peter F. Brown, John Cocke, Stephen A. Della 
Pietra, Vincent J. Della Pietra, Fredrick Jelinek, 
John D. Lafferty, Robert L. Mercer, and Paul S. 
Roossin. 1990. A statistical approach to nlachine 
translation. Computational Linguistics, 16(2):79- 
85 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, Robert L. Mercer. 1993. The math- 
enlatics of statistical machine translation: pa- 
444 
Feature(x,y) 
X 
VBP+PRP+TO 
BEP+RBR+IN 
DT4-CD 
JJ+IN 
VBG+TO 
BEP 
BEP 
NNP 
NNP 
TO+PRP 
TO+PRP 
MD+FIB 
MD+RB 
MD+BEP 
NNP+NNS 
VBP+TO+VBP 
BED+VSN+IN 
BED+VSN+IN 
Y 
PNYPPCA2+PPAD+VBMA 
PPAD+AJMA 
NU+NNDE2 
PPAD+AJMA+ENTR1 
PPAD+PPCA2+VBMA 
PPCA1 +AJMA 
PPCA1 +AJMA+ENTE 
NNIN2+PPAU 
NNIN2+NNIN2 
PN+PPAD 
PN+PPCA1 
ENTRI+NNDEI+CO 
ENTRI+NNDEI+CO+ENTE 
CO+ENCO2+VBMA 
NNIN2+SF 
PPCA2+VBMA+ENCO2+VBM~ 
PPAD+VBMA+PE+ENTE 
PPAD+VBMA+PE 
r,~; P(ylx) /~ L(Afi) 
9.8687 0.1722 0.0194 
9.6780 0.3265 0.0192 
9.2799 0.2449 0.0190 
9.6343 0.2450 0.0190 
9.9542 0,3269 0.0189 
9.6720 0.2941 0.0188 
9.2724 0.1961 0.0188 
8.7481 0.1225 0.0182 
9.1397 0.1337 0.0180 
9.5634 0.2307 0.0180 
9.2604 0.1730 0.0180 
9.2445 0.1548 0.0177 
9.2564 0.1548 0.0177 
8.5435 0.0934 0.0177 
9.1597 0.1470 0,0176 
8.9928 0.1278 0.0174 
9.1511 0.1704 0.0173 
9.1636 0.1706 0,0173 
English 
send+Nm+to - - 
is+more+than 
the+two 
smarter+than 
serving+to 
is 
are 
IBM 
Harvard 
to+him 
to+her 
? 
? 
should+be 
English+books 
request+to+send 
was+thrown+to 
were+sent+to 
Example 
Korean 
21+ N+-0tF+-~ L\]I 
-h20+~ 
?+N 
~H+~PA+N+L 
~OII3tI+-~+U~ 
-0I+OA 
-01+~+H 
IBM+~- 
3+OIl)II 
~LI +)F 
? 
? 
Ol+OiOI+N 
-N+~Ll l+Kf~+5/N bF 
~011?t1+c3 ~1XI+~+FA 
Figure d: Some fbatm'es with a large gain 
Tag a l ignment  (km( l i t io lml  Lex ica l  aliglulw, nt 
l ) l{P : PN+I )PAU 0.150109 you : dangs in+cun 
lt\]3 : A I )CO 0.142193 usmt l ly  : dachero  
II, B : NNIN2+PI 'A I )  0.038105 usua l ly  : i lbanseol?-l-e 
I IVP+TO : I~N( JO3-bAX+I"NTE 0,982839 have+to  : ayaman+handa 
VBP  : P1)A I )+VBMA 0.05022,l take  : e+an j  
VBP  : VBMA+F,  NCO3+AX+I , ;NTE  0.011110 take  : an jay+aman-Fha+nda 
V I3P  : P1)A I )+VI~MA+ENCO3-}-AX- I -ENT\ ] ,~  0,001851 take  : e -Fan jaya imm+handa 
V I3P -F J J  : NNIN2+PI )A I ) - \ ] -VBMA 0.057657 take- t - regu lar  : i l bansenk+e+,a l l j  
. I . J+NN : NNIN2 0.581791 regu lar+seat ing  : i l l )anseok 
an(, 3: l,exi(:al aligmnents using tag alignments 
rameter estimation. Computational Linguistics, 
19(2):263-311. 
Stanley F. Chert. 1993. Aligning sentences in bilin- 
gual corpora using lexical information. In l'rocccd- 
ings of ACL ,71, 9-16. 
A. P. Dempster, N. M. Laird and 1). 13. l{ubin. 
1976. Maximum likelihood fi'om incomplet,e data 
via the EM algorithm. The Royal ,S'tatistics Soci- 
ety, 39(B) 205-237. 
Williain A. Gale, Kenneth W. Church. 1993. A pro- 
gram fbr aligning senten(:es in bilingual (-orl)ora. 
Coml)utational Linguistics, \]9:75-102. 
Frederick Jelinek. \]997. Statistical Methods for 
Speech Recognition MIT Press. 
Marin Kay, Martin Roscheisen. 1993. Text- 
translation alignment. Computational Linguis- 
tics, 19:121-142. 
Julian Kupiec. 1993. An algorithm tbr finding noun 
phrase corresl)ondenccs in bilingual corl)ola. In 
Proceedings of ACL 31, 17-22. 
Yuji Matsmno~o, Hiroyuki Ishimoto, Takehito Ut- 
sure. 1993. Structural inatching of para.llel texts. 
In Proceedings of ACL 3I, 23-30. 
I. Dan Melame(l. 1997. A word-to-word model of 
translation equivalence. In PTvcccdings of ACL 
35/EACL 8, 1.6-23. 
Frmlz Josef Och mid Ilans Wel)cr. 1.998. hnt)rov- 
ing Statistical Natural Language Translation with 
Categories and Rules. In Procccdings of ACL 
36/COLING, 985-989. 
Stephen A. Della Pietra, Vincent J. Della Pietra, 
John D. La.tl'erty. 1997. llnducing features of ran- 
dora fields. IEEE ~IYansactions on Pattern Anal- 
ysis and Machine Intelligence, 19(4):380-393. 
Frank Smadja, Kathleen R. McKeown, and Vasileios 
Hatziw~ssiloglou. 1996. Translating collocations 
fi)r bilingual lexicons: A statistical approa(:h. 
Computational Linguistics, 22 (1) :1-38. 
Kengo Sate 1998. Maximum Entrol)y Model Learn- 
ing of the Translation Rules. In Procccdinfls of 
ACL 35/COLING, 1171-1175. 
Jung H. Shin, Y(mng S. Han, and Key-Sun 
Choi. 1996. Bilingual knowledge acquisition from 
Korean-English paralM cort)us using aligmnent 
method. In Proceedings of COLING 96. 
C. Tilhnann, S. Vogel, H. Ney, and A. Zubiaga. 
1997. A I)P t)ased sea.rch using monotone a.lign- 
ments in statistical translation. In Procccdings of 
ACL 35/EACL 8, 289-296. 
Ye-Yi Wa.ng and Alex Waibel. 1997. Decoding algo- 
rithm in statistical machine translation. In Pro- 
cccdinfls of ACL 35/EACL 8, 366-372. 
Ye-Yi Wang and Alex Waibel. 1998. Modeling with 
structures in machine translation. In Procccdings 
of ACL 36/COLING 
Dekai Wu 1996. A t)olynonlial-time algorithm for 
statistical machine translation. In Proceeding of 
A CL 34. 
445 
Identifying Temporal Expression and its Syntactic Role Using 
FST and Lexical Data from Corpus 
Juntae Yoon 
jtyoon@daumcorp.eom 
Daum Communications Corp. 
Kangnam-gu~ Smnsung-dong~ 154-8 
Seoul 135-090~ Korea 
Yoonkwan Kim Mansuk Song 
{general,mssong} @december.yonsei.ac.kr 
Dept. of Computer Selene% Engineering College 
Yonsel Univ. 
Seoul 120-749, Korea 
Abst rac t  
Accurate analysis of the temporal expression is cru- 
cial for Korean text processing applications uch 
as information extraction and clmnking for efficient 
syntactic analysis. It is a complicated problem since 
temporal expressions often have the ambiguity of 
syntactic roles. This t)al)er discusses two problenm: 
(1) representing and identiflying the temporal expres- 
sion (2) distinguishing the syntactic tim(lion of the 
temporal exI)ression in case it has a dual syntac- 
tic role. In this paper, temporal expressions and 
the context for disambiguation which is called local 
context are represented using lexical data extracted 
fiom corlms and the finite state transducer. By ex- 
periments, it; turns out that the method is eflimtive 
for temporal expression analysis. In particular, our 
al)t)roach shows the corI)us-based work could make 
a promising result for the t)roblem in a restricted 
domain in t, hat we can eflbctievely deal with a, large 
size of lexical data. 
1 In t roduct ion  
Accurate analysis of the temporal expression is cru- 
cial tbr text processing aplflications uch as informa- 
tion extraction and for chunking for efficient syntac- 
tic analysis. In information extraction, a user might 
want to get a piece of information about an event. 
Typically, the event is related with (late or time,, 
which is represented by temporal expression. 
Chunking is helpflfl for efficient syntactic analy- 
sis by removing irrelevant intermediate constituents 
generated through parsing. It involves the task to 
divide sentences into non-overlatli)ing segments. As 
a result of chunking, parsing would be a problem of 
analysis inside chunks and between chunks (Yoon, et 
al., 1999). Chunking prevents the parser fl'om pro- 
ducing intermediate structures irrelevant o a final 
output, which makes the parser etticient without los- 
ing accuracy. Thus, it turns out that chunking is an 
essential stage tbr the application system like MT 
that should pursue both efficiency and precision. 
Korean, an agglutinative language, has well- 
developed flmctional words such as postposition 
and ending by which the grammatical fimction of 
a phrase is decisively determined. Besides, because 
it is a head final language and so the head always 
follows its complement, the chunking is relatively 
easy. However, we are also faced with an mnbiguity 
problem in chunking, which is often due to the tem- 
poral expression. This is because inany temporal 
nouns are used as the modifier of noun and vert) in 
a sentence. Let us consider the tbllowing examI)les: 
\[Example\] 
la  jinan(last) :l\]('oFd'll,'llt(SlllillIler) 
-+  
lb 
uv i - t teun  
(we/NOM) hamgge(together) san-c(to moun- 
tain) .qassda( went )
\;Ve went to the mountain together last sum- 
Incr. 
j inan(last ) yeorr;um(summer) banghag-c(in va 
-9  
2a 
cation) 'uri-neun(we/NOM) hamgge(together) 
san-c( to mountain) gassda( went )
We, went to the mountain together in the last; 
SUllllller Va(;atioIl. 
i0 weol(October) 0 il(9th)jeo'nyeo.q(evening) 
-+ 
2b 
"7 sit7 o'clock) daetong'ryeong-yi (presi- 
dent/OEN) damh,'wa-ga(talk/NOM) issda(be) 
The president will give a talk at 7:00pro in 
Oct. 7th. 
10 wool(October) 9 il(9th)jconyeog(evening) 
7 sit7 o'clock) bih, aenggipyo-reul(flight ticket;/ 
ACC) yeyaghal su isssev, bnigga(can reserve) 
-+ Can I reserve the flight ticket tor 7:00pro in 
Oct;. 77 
Ill the examples, each temporal expression plays a 
syntactically different role used as noun phrase or 
adverbial phrase (The undeJ'lined is a phrase) al- 
though they comprise the same phrasal fornls. Tile 
temporal expressions in la  and 2a of the example 
serve as tile temporal adverb to modify predicates. 
On the other hand, the temporal expressions iu lb 
and 2b are used as the modifier of other nouns. That 
is, as a temporal noun either contributes to construc- 
tion of a noun compound or modifies a predicate, it; 
causes a structural aml)iguity. 
One sohltion might be that the POS tagger as- 
signs a different ag to each temporal noun e.g. NN 
954 
mid ADV'. However, since (let/en(len(:ies of teml)oral 
llOllllS .~l, re lexically (lecided, it does not seem that 
their synta('t;ic ta.gs could tie ac(:urately t)redi(:ted 
with a relatively small size of PeG tagged (:orl)uS. 
Also, the siml)le rule based a.1)l/roach Callliol; litake 
sa|;isfa.ctory resuli;s without lexi('al information. As 
such, identification of temi)oral expression is a coin- 
plicate(t l)roI)lem in Korean text mmlysis. 
This 1)aper disc, usses identiticatiol~ of temi)oral ex- 
t)ressions ;m(1 their synta(:tic roles, in this t)aper, 
we wotfld deal with two 1)robleins: (1) re, I)resent- 
ing mM idenl;ifying |;he teinl)oral exl)ression (2) dis- 
t inguishing its syntactic t ime|ion in case it; has a 
drml syntacti(: role. Acl;ually, tile two t)rol)lems are 
(:h)sely related since the identifi(:ation and (lisam- 
biguation proc(~,ss WOll\]d lie done un(ler l;\]w, r(',l)re- 
seul;a|;ion s(:henm of |;emporal exI)ression. Tim pro- 
('ess bases (in lexi(:al data exl;ra('l;e(t \['rOlll (:orl)llS ;Ill(t 
the finite state trans(lu(',(u' (FST). A(:(:(n'(ting to our 
ol,servation of texts, we (:ould see that a fe, w wor(ls 
following a. teml)oral llOllll ha.ve great ef\['ect on the 
syntactic funet, ion of the temt)oral noun. Theretbre, 
we note that the stru(:tura.1 amlliguity (:ould lie re- 
solved in lo(:al (:ont(~xts, mM so obtain lexical in- 
tbcmation for the lo(:a\] (:ontexts from (:orlms. The 
lexi(:al da.ta which (:onl;ain (;onl;exl;s for disambigua- 
| ion are ref)resenl;e(t with |:emt)()ral wor(t transition 
()ver the 1EST. 
l kMly  (l(~scrilling our methodology, we tirsl ex- 
l;r:,mI; (',Oll(;or(lail(:(~ dal,a of each I;emt)oral w(/r(t using 
~/ COllCOrd,:t,llC(~ l)rogr;nn. '\['he CO-OCCllrr(}IlC(~S r(~l)l'e- 
SQIlI; relations ll(d;ween l;e\]nl)ora\] wor(ls and also ex- 
plain how I;(~,llll)Ol'al ll()llllS ;~tll(\] COllllllOll ll()llllS }/17o 
combined to gCllCritl;C .:1 (:Oml)OmM noun. It wouhl 
be the like, lihood of woM (:oral)|nail(m, whMl helps 
disambiguate tim syntacti(: role if a teml)oral word 
have a synta(:|;i(: duality. \]n particular, we (:lassi\[y 
t(mq)oral llOllllS into 26 classes in ac(:ordan(:e with 
their meaning a.nd fmwtion. 'l'hus, the w(n'd (:o- 
OC(;llrr(~,llC0,S l)ecoll le t;\]lose ~llll()ll~ |;ell l i)oral (:\]asses 
or | ;elnl)oral  (;lasses and  el;her ll(/lillS~ whi(:\]l resuli;s 
in re(lu(:ing the 1)m-mn(d;er spa(y_ Se, con(1, l;emi)o- 
ral expl 'essiol ls  ('Ollt}tillillg |;he c()-occ/lrrel lces ()\[" fera- 
l)oral (:lasses an(1 other 1\]OllllS are rel)r(;sented with 
the FST to identit~ temporal  ext)ressions and assign 
their syntactic tags in a Sell|;(}llC0,. it has t/een shown 
|;lint; the FST t)resents a very etlMent way for repre- 
senting 1)hrases with locality. The inlmt of the FST 
is the result from morphological analysis a.n(1 POS 
tagging (here, the teml)oral noun is tagged only as 
nora 0. Its ou|;I)Ul; is the syntaci;i(: tag for each word 
in the senl;ence and temi)oral words are al;l;ached tags 
such as noun and adverl), l:igm'e 1 shows tim over- 
all system fl'om the morI)h(/logical analyzer l;() l;he 
chunker. 
Therefore, the, t)ro(;(~,ss atl;at;ll(~s sylll;a(:l,i(; labels 
to the 1)revious exmnl)les so thai; (:hunking w(ml(1 lie 
safely executed from the results as follows: 
\ [Exmnple \ ]  
la.' \[ j /na'n.(last) y(dol'(;ltfl,.( Sllllllller ) \]T A 'ltf'i-'ll.('lt~l. 
(we/SeMI mou,,- 
rain) gassda(went) 
--> \Ve went to the mountain together last smn- 
l l ler. 
b' ) sum,, ,,r ) \].,, x 
e(in vacation) u'ri-neun(we/NOM) hamg.qe 
to we,ltO 
-+ We. went to tile momltain together in the last 
SllIlllllOr vo~cal;iOll. 
2 1' \[.to o il( o ;h ) eve.i.  ) 
7 si(7 o'clock)\]TA dactongr'ycong-yi (presi- 
dent /GEE)  damh,'wa-ga(tall?/NOM)'i,s',s'da(be) 
-~ The president will Give a talk at 7:0()pro ill 
O(:t. 7th. 
21) ' \[10 ',,,,.ol( ( ),'|~ol ier ) :) ',:l(9a) .#o',.~,,,,9( ev~;,ti,lg) 
7 s/(7 o'clock)\]:rN bih, a(:nq.qipyo-re'u,l(flight 
t i cket /ACe)  ycyagh, al su issscubni99a(can re- 
Se, I'V(~,) 
--~ Can I reserw~ the tlight ticket for 7:0()lml in 
()(:t. 7? 
2 Rela ted  Works  
Almey (\] 991) has proposed texL chunldng as a t)l'e -
l iminary st;e l) to tmrsing on the basis of psycho- 
logical (wi(lence. In his work, tim chunk was (le- 
fine(1 as a. t)artitione(l segnmnt wlfi(:h corl:(~,Sl)Oll(ts in 
some way to ln'osodic lmtt(!rns, l\[n addition, con> 
1)lex }/I;|;tlchlllellt (\](?cisions ;Is occurring in NP or 
VP analysis are 1)OStl)one,(l wit;hour \]icing (leci(led 
in (:hunldng. Rmnshaw and Marcus (1995) intro- 
du(:e(l a 1)aseNl' whi(:h is a non-re(:ursive NIL They 
used trmlsfornmtion-1)ase(l learning to i(lentif~y n(/n- 
recto'sire l)aseNPs in a s(mtence. Also, V-typ(~ (:hullk 
was iifl;roduce(l in their system, and so I;lw, y (aied 
to t)artition sentences into non-overlal)l)ing N-type 
;til(t V-tyl)e ('hunks. Y(/on, et al (\].099) have, de- 
fined ('lmnking in various ways for efficient analysis 
of Korean texts mM shown that the, mt:tho(t is very 
eff(;(:tive for practical al)l)li(:ation. 
l leside, s, th(',r(~ have })een many w(/rks based on the 
finite state ma(:hine. The finite state machine i,~ o f  
ten used for systems u(:h as speech t)r(l('essing, liar- 
tern mat('hing, P()S tagging and so forth becmlse 
of its ei\[i('ien(;y of sl)ee(l mid si)aee and its ('onve- 
nience of rei)resenl;ai;ion. As for parsing, it is not 
suitable \['or flfll parsing based on the grammar  that 
has recurrent property, but for partial parsing re- 
quiring simple, sl;&l;e l;rallsitioll, l~.o('he all(1 S(:hal)es 
(1995) have i;ransformed l;he Brilt's rule based tagger 
to the (ll)timized deterministic FST and imI)roved 
the sl)eed mM sl)a.ce of the tagger. A. nora.tile one re- 
lated t( /this work is about local grammar 1)resented 
in Gross (1993), which is suital)le for rel)resenting 
955 
SOlltelleO 
jinan yeoreum banghag-e uri-neun hamgge san-e gassda 
Morp Anal and POS Tagger 
.LJ.I 
jinan/A yeoreum/N banghag/N-c/P uri/PN-ncun/P hamgge/AI) san/N-c/P ga/V-ss/TE.-da/E 
FST Ibr idefifying temporal expression 
jinan/A_m ycorcum/N_m banghag/N-e/P uri/PN-neun/P hamgge/AD san/N-e/P ga/V-ss/TE-da/E 
Text Clmnkcr 
\[jinan/A_ln ycoreum/N_tn banghag/N-c/P\] \[uri/PN-neun/P\] \[hamgge/Al)\] \[san/N-e/P\] \[ga/V-ss/TE-da/E\] 
Figure 1: System overview fl'om the inort)hological nalyzer from the chunker 
rigid phrases, collocations and idioms unlike global 
grammar for describing sentences of a language in 
a formal level. The temporal expression was repre- 
sented with loc~l grammar in his work, where it, was 
claimed that the formalism of finite automata could 
be easily used to represent them. 
3 Acquiring Co-occurrence of 
Temporal Expression 
3.1 Categorizing Temporal Nouns 
Since many words have in common a similar mean- 
ing and flmction, they can be categorized by their 
features. So do temporal nouns. That is, we say that 
'Sunday' and 'Monday' have the same features and 
so would take the similar behavior patterns uch as 
co-occurring with the similar words in a sentence or 
phrase. Hence, in the frst  place we categorize tem- 
poral nouns according to their meaning and func- 
tion. We first select 259 temporal nouns and divide 
them into 26 classes as shown in Table 1. Among 
them, some temporal words have syntactic duality 
and others play one syntactic role. Thus, the dis- 
ambiguation process would be applied only to the 
words with dual syntactic functions. 
3.2 Acquisition of Temporal Expressions 
f rom Corpus 
Temporal words would be combined with each other 
in order to be made reference to time, which is called 
temporal expression. Since a temt)oral expression is 
typically composed of one or a few temt)oral words, 
it seems to be possible to describe a grammar of 
modifying noun 
jeonlsilll-eUll nlasisseossda 
(hmch/NOM) (was delicious) 
oneul(today) ~ @  . . . . .  @ 
~"X hagayo-c(lo school) gassda(wenl) 
modifying predicate 
Figure 2: Syntactic flmctional ainbiguity of tcnlpo- 
ral expression 
the temporal expression with a simple model like fi- 
nite automata. Ill tile practical system, however, we 
are confronted with a complicated problenl in treat- 
ing teml)oral expressions since many temporal words 
have a functional ambiguity used as both a nominal 
and predicate modifier. For instance, a temporal 
noun oneul(today) could play a different role in the 
similar situation as shown in Figure 2. In the first 
and the second path, the words to follow oneul are all 
noun, but the roles (dependency rela;ions) of oncul 
are different. 
Accurate classitication of their syntactic flmctions 
is crucial for the application system since great dif- 
ference would be made according to accuracy of the 
dependency result. Practically, we therefore should 
take into consideration the structural ambiguity res- 
olution as well as their representation itself in identi- 
956 
word cat(;gory class # l;eml)oral words 
modifier 1 ol(l;his), jinan(lasl;), . . .  gemt)oral 1)refixes 
l l l l l l l \ ] )e r  2 
3 -10 
l lH l l lbe l "  . ? . 
era~ age 
l;emporal unit .Sg(li(Cell~l'y), 7~ygoT~(ye~l.l.'),... 
Lelll l)oral llOllllS 
years 
lno~|l, hs 
19 
20 21 
weeks 
- - - -~  day of week 
(lays I (lay\] 
day2 
time1 
1;line ~ 1;|me2 
~111( \ [  \[ seaso l l  
(lura- I Sl)eeiti e (hlration 
1;lOIl _I edge 
11 ftosaenfldae(1)aleoz(71c), . . .  
12 9eumnyeon(th is  year)-, saehae(new year ) , . . .  
13 ~ month),  jeon.qwol(January), . . .  
14 Oeum:#;,(this week), naeju(nexl; week) , . . .  
15 | lye'/ /(Sunday),  wolyo' il, . . .  
16 17 h, aru(one day), ch,'ascog(Thanksgiving day), . . .  
18 o,~Tia, lgo , lay ) ,  , , , : '  i l ( * ;omorrow) ,  . . . 
.saebyeoo( (lawn ) , achim( morning) , . . .  
yeonmal(year-end~, . . .  
22 ~st ) r ing) ,  yeoreum(smmner) , . . .  
23 hwar~:~eolgi(time of season changing), . . .  
24-25 ch, ogi(early t, ime), j'm~,gban(mid), . . .  
l;emporal suffixes ~eml)oral suffixes 26 dongan(duri l lg),  .~,aenae(l;hrough), . . .  
Table 1: Categorizal.ion of l;eml)oral words 
t~ying l;eml)oral exl)ressions. The poinl; I;hal; we note 
here is thai; we eould pre(liel; the synt;a(:l;ic fllncl;ion of 
I;emt)oral words 1)y looking ahead one or l;wo words. 
Namely, looldng at; a Dw words thai; follows a 1;em- 
poral word we can figure oul; which word the tempo- 
ral expression modifies, and call l;he following words 
local conl, e:rt. 
Unfortunal;ely, it is not easy t() define t, he, local 
conl;exI; for del;ermilfing 1;he synt, ael;i(: flm('l;ion of 
eae.h temporal  word 1)eeause l;hey are lexieally re- 
lai;ed. Thai; is, il; is wholly ditl('.renl; fl:om each wor(t 
wlw,|;her a I;(nnl)oral noun would modit)- ()l;h(w i1()1111 
(:o form ~ (:Oml)Om~d noun or mo(lii~y a 1)re(li(:al;e as 
m). adverbial l)hrase. ()ur al)t)roa/:h is l;o use co l  
pus to acquire informal;ion atmut, l;he local ('oni;exl;. 
Since we could obtain fi'om eorl)uS as many exam- 
ples as needed, rules for comt)ound word generation 
can be (:onstructe(1 from l;\]le examI)les. In l;his 1)al)el', 
we l lSe CO-OCClllTe, II(;O rela|;ions of l;emlmral lO/ l l lS ex -  
t,r,acted froll\] large corpus I;o represent and consi;ru(:t; 
rules for idenl;ifieal;ion of l:emporal expressions. 
As lnenl;ioned before, we would 1)a.y a|;ten(,ion (;o 
two t)oinl;s here,: (l.) In whal; order a tenq)oral ex- 
1)ression would 1)e represenl;ed with temt)oral words, 
i.e. descrit)|;ion of the temporal  exi)ression nel~work. 
(2) how the local context would 1)e described to re- 
solve tile ambiguity of the syntactic t im(l ion of tem- 
1)oral ext)ressions. For this tmrpose, we tirs|; extract 
examl)le sentences containing each of 259 l;eml)oral 
words from eorlms using l;he KA IST  concordance 
progrmn :l (KAIST,  1998). The numl)er of t, elnporal 
words is small and so we could mmmal ly  manii)u- 
late lexieal dal;a ext;racted frOlll corl)us. Figure 3 
1KAIST corlms consists of about 50 million cojeols, l';ojeol 
is a sl)acing unit; comi)oscd of a content word and functional 
words.  
shows exanl l) le sen|;enees at)out, ye.o'reum(sununer) 
ext;ra(:t;ed l)y the. coneor(lanee l)rogram. 
Second, we s(;leet only l, he t)hrases related wit;h 
temporal  words fl'om the examples (Table 4). As 
shown in Table 4, yeoreum is associated wii;h va.ry- 
ing words. Temporal  words like temporal  pre.tixes 
can come before it and coIlllIlOil llOl_lllS C}lll follow ig. 
In (,his stage we describe con(;exts of each temporal  
word and (;he olll;1)U(; (syn(;ae(;ic tag of (;he tOtal)oral 
word) under the given eonl;ex(;. In l)artieular, each 
l;eml)oral word is assigned a (.emporat class. Be.sides, 
or;her nouns serve as local cent;exits for disaml)igua- 
(;ion of syntac(;ic flmc(;ion of t, emporal  words. 
lS:om (;he examl)les , we can see t, lmC if ha're(night), 
byeo(jau, g(villa), ba, ngh, ag(vacal, ion) and so on follows 
it, yeoreum serves as a (:olnponent of a ( 'ompomld 
noun with the following word. On t, he other hand, 
the word naenac wtfich means all the t ime is a tem- 
poral noun and forms a teml)oral adverl)ial l )h l '}/se 
wil;h ()Idler 1)receding temporal  nomL Moreover, yeo- 
7'eum(sllnuner) might represent ime-relal, ed expres- 
sion with t)receding l;eml)oral prefixes. 
4 I dent i fy ing  Tempora l  Express ions  
and Chunk ing  
4.1 Represent ing  Tempora l  Express ion  
Us ing  FST  
The co-occurrence data extracted by tile way de- 
scribed ill the previous section can be represented 
with a finite state machine (Figure 5). For synt;ae- 
t ie :\[inlet;ion disambigual;ion an(t chunking, the au- 
tomata  should produce an out, lm|;, which leads to a 
fiifite st;ate t;ranslhleer. In fact, individual deserip- 
l;ion for each data could be integral,ed into one large 
FST and represented as the right-hand side in Fig- 
ure 5. A finite sta.te transducer is defined with a nix- 
957 
left context word right context 
~ ;._l_~otl = ~1~,~11 ~ .~o,~ ~_ ~,~.. \[ 4~- 
-~1~-  A~I ~ ~-lo~l,g ~-  ~1ol ~t - .  \[o:1-~- 
4~-~ ~-~-~-~- ~1 &~l~ll ~1~,~-. \ [d~ 
0 
~k.  ~1~11 ?' ? 
~,,t~xl ~ ~-~@~1~11 ~ ?4'q ~. \ [x l~  ?4~\] 
~d-~dol~l-q ~t~- ~o1-7-~I-!" ~ \[~ll o:t~-\] 
,'~ ~q-~l 4 o~_ ,? .  \[o I.d oq ~\] 
~\] 'd~ ? lq  ~<-ol- d ~ ;q "d 
'gb~\] ~-<>11 ~.  o_~ -, ~ ~-1~1-~- 
~o~*. ~\] '~ <gl~- V-~?II -~}~ 
.j-q-\] _~ol~x- I , lu&~ o,o~ ,~ 2,2, 2~...J~. 
,~Xor\]Ol~tq, :z~} "J.-*~ls1-~'li ol ~,\] 
-~-~\]~ ~x~lq- ~1~,4 ~. ~'t-l-b 
~'l~xl'4 ~~-~t ~1;'I ~ ~1~~ ~1~ 
Figure 3: Example concordance data of yeorcum(summer) 
befbre temporal noml after outlmt freq 
yeoreum( suulmer ) 
x\] ~.!-/ t~ (ji.,.,,~, ~ant) 
~/tlO (hae,yeal') 
o\] ~/ ld (ibcon,thin) 
o:t ~-/t22 
o:t-~-/t2.,_ 
o-1 ~- / t.2.2 
oq ~-/t,2., 
o~ ~-, /~  
o~ ~-/t._,., 
r~\[(bam,night) TN 2 
vo~,(banghagyacation) TN 7 
'~ ~-( bycoljang,villa ) TN 1 
~( jumal ,  weekends) TN 1 
J,~ 71 (.qam,.qi,flu ) TN 1 
q\] q\]/t2~ (nacnae,all the time) TA 1 
~-F~-(na,,eunJ/TOP) TA \] 
6.25, \]- \] , ('m,a.?rmag, the last) TA 2 
~ (.leo'ntuncun,1)attle/.\[ 0 I  ) TA 1 
Figure 4: Temporal expression phrases elected fronl examl)les 
tuple (Ej, E2, Q, i, F, E) where: E1 is a finite input 
alphabet; E2 in a finite output alphabet; Q in a ti- 
nite net of states or vertices; i E Q in the initial state; 
F C_ (2 is the set of final staten; E C Q ? E~: ~ E.; x (2 
is the set of transitions or edges. 
Although the syntactic function of a temporal ex- 
pression would be nondeterministically selected fl'om 
the context, temporal expressions and the lexical 
data of local context can be represented in a de- 
terministic way due to their finite length. For the 
deterministic FST, we define the partial functions ? 
and ? where q?a = q' iffd(q, a) = {q'} and q,a = w' 
iff ?q' E Q such that q?a = q' and 5(q, a, q') = {w'} 
(R.oche and Schabes, 1995). Then, a nubsequential 
FST is a eight-tui)le (El, E2, Q, i, F, ?, *, p) where: 
E1,E2,Q, i  and F are the smnc as the FST; ? is 
the deterministic state transition fimetion that maps 
() x E1 on Q; ? is the deterministic emission fimction 
Figure 6: 
T = (~,, r,.~, O,i, F, o , . ,p})  
)2~ = {tl, t~2, t~6, wi, wj} 
E2 = {TN, TA,  NT}  
0 = {o, 1,2,3} 
i = 0, F = {3} 
0c4tl =1,  O,t1 =TN,  
1?t22 =2,  l * t~=G 
2?t~< =3, O*twi =TN-NT,  
2 @ t.~6 =3, O * t.2s = TA, 
2 ? twj =3,  O * t, u = TA_NT,  
p(3) = 
Deternfinistic FST resulted from Figure 5 
that  maps Q x E1 on E~; p : F --) 22~ is the final 
outtmt fluiction. 
Our teniporal co-occurrence data can lie relive- 
sented with a deterministic finite state transducer 
958 
} ?~g~'tl\[1111 N , bdl i#~\[  
}~?~CUUltl N I~) colJanglNI 
i , * .  ! N I~mylHL'IN I
/ )c, nc ~I'A ;., ) :"" .( &, 
jin;ulY\[ N /  
~?,w,,,,,n X , j.,,,,,l,x I 
~ c(~etll,l/I N ila ?11 a,'\[ \['A 
j ln;m/I N ?,l?lbrlltl A ilajlclmiN I 
k,e/IN ),,,reumflA ?,25/NI 
kw/ IN  ~?,liclnllJ\[ a ilk ij~tll Ik~\]N I 
\ d ................................................. \, ............. ( ' /  
Figure 5: Fiifite sl;al;e nta('hin(~ (:onsl;rucl;ed wil;h l;he (bd;a in Figure d 
O t'/:"H-( ) 
0 I 
wl /!l 'N, NT' 
t2'-'(' ~ ( ) !~';IT A 
u,j /'I'A, NT 
ll" :: {barn, jumol, t, a l tgha 9 . . . .  } 
mi (i lI" 
wj ~ 1V 
Figure 7: A d(;1;erministic tinil, e sl, aW, (,ransduc('a' 1;o 
i)roce, s,s temi)oral ex\])re,,qsion 
in a similar way. The, sut)s('qu(',ntial FST f()r our 
sysl,em is (l(,,fined as in Figure (i and Figur(~ 7 ilhLs- 
l.ral;es I;he tral~sdu(:(!r in l?igur(~ 6. In L\]m tiI~me, ti 
is a c\]ass 1:o whi(:h lhe (:eml)oral w()rd 1)elongs in l;he 
lx',mporat (:las,qiti('at;ion. wi is a word ol;her l;han l,em- 
1)oral ones 1;hal; has l;he pr(',(:(~(ting t eml)()ral wor(l 1)(', 
il:s modiiier, and wj is not; such a word 1;() make a 
compound noun. TN,  TA and NT  are synt, aciic 
tags. A word t;agged with 5/'N would modify a su('- 
ceeding l lO l l l l  like, barn(night), bangh.ag(vacati(m). A 
word al:t, ached with TA  would lnodify a predica.lx~' 
aim one with NT  nmans ii; is not; a 1;emporal word. 
A(:mally, individual FSTs are coml)in(;d into one aim 
rules for tagging of temporal words are pul; over l;h(; 
.,J. The rule is applied according to the prioriW 
by  f req l le l l cy  il l  case  lllOrl2 t ;hal l  ()lie ()ll{;l)ill; a re  \ ] )os-  
sible for a (:Oilt;ex|;. Nmnely, it; is a rule-l)ased system 
W\]I()I'(I ~\]le r l l l es  al'e, (~xl;ra(;|;(?(l f ro l l l  (;ort)l lS. 
4.2  Chun ldng  
Afl;er the FST of l;enlt)oral (',xt)ressions adds I;o woMs 
syntactic tags such as TN and TA, chunking is con- 
ducted with l'eSlllI;s frolll OllI;l)llI;S 1)y t;h(' FST. As we 
said earlier, (:hunldng in Korean is relal;iv(;ly easy 
only if t;h0, t;eml)oral exi)l'essioll wou\](t be success- 
fully recognized. Act;ua.lly, our ('hunker is also based 
on the, finil;e s(;a,l;e machine. The following is an ex- 
mnl)le for (:hunldng rules. 
iN1,) ~ (NF (NP) I (2V)* (2VIO* (UN) 
('rNI,) ~ {TN)* (N)* (XP) 
\]\](!re: j\r is a noun wil;h(ml, rely \])ost.t)osit.ion , NP  is 
a noun wit.h a. 1)oSl;l)OSil,ion, TN is a t;enll)oral noml 
recogniz(~,d as modifying a suc(:ox'ding n(mn, NU is 
a number and UN is a uni(; n(mn. Afl;e,r t('mporal 
l.a.gging, 1;he ('hunker l;ransforms 'NT '  into N, NP, 
(d,(',. according I,o morl)hologi(:al consi;itueid;s and 
their I)OS. I h'io, tty, t;he, rule says thai; an NP ctmnk is 
mad(', from eil3mr NI '  or l;emporal NIL An NP would 
\])(! (:()llsLrll('l,(',(1 wi i ,h  on(;  o r  lll()l'(} llOllll.q ;ill(1 \[;boil" 
modilie(~ or with a noun (lUanl;ified. A TN\] ), whi(:h 
is r(',lal:ed with lime, is made from n(mns moditied 
by t('ml)oral words wlfi(:h would 1)(', i(t(;nl;itied by the 
FST. By i(l(mtifi(:ation of lx!mporal (',xpressi()n and 
chunking, tlm following (',xmnl)k', senl~elu:e, is chunked 
as  | ) ( ; low.  
? j inan(lasQ ycor~'um(summer) bau, ghag-e(in 
? s * " v,,,:,,.(;io,,) ,,,.,.,,-,,.,;,,,,4,,,~./s un.~) k~o,,,,VV,,- 
t(.~o(,.o,m,,,~;(,F) .,,~,(th~.,,o~)d(.~'-,~',,l(.,,iqOl~.J) 
sassda(bought;) 
-+ \,Vo, bought l;hrce c()mlmt(~'rs in the lasl; 
81111111101" V~IC~I(;i()l l .  
? jin(t?Vl,N ',~l(:OTC'tt'lllq'N balzgha(j-CNl, '~tri-nc'ltlZNl, 
kco'vnl)yUl, eON SCNU dae-reUINl, sassdav 
? \[jinawl'N yCOVCUmTN ban!lh, ag-CNl,\]Nc 
\['uri-ne, wnNP\]NC: \[kcompyuteoN SeN?: dac- 
?'C?tlNI,\] N(; sassdav  
5 Exper imenta l  Resu l t s  
For l;hc ext)erinmnl; a.bouL l;eint)oral expre, ssion, we 
e, xla'aci;ed 300 senl;enc('~s (:onl;a.ining temporal expres- 
sions from E\]?I{I )()S cortms. Table 2 shows the r(',- 
959 
~c'*?unnrL ~ ( 
y~,~cl,,,,ItX { 
ya~cu,,,fl N ( 
) ?,~c,,,n/I N~ ( 
) b'un*N'l ~: : 
) t') ?"II:~'U'/N.I i 
g;,,,,~,lXl~ 
,.?aadD~ 
. xy?l}l?onlfI'A / -  ) 
!+,,+,,,aN )++,.+,,,,,,,A< 2 '':''L"''~'L ,2 
,~,.,ax -( y?,,~,,,,~,L~ /,,~j,,.,,,.xr(; ,~
Figure 5: Finite state machine constructed with the data ill Figure, 4 
wi /TN,  NT  
. .  t i /TN  C~ 1.22/{ F , / "  t2o/'l 'A " " ' '  
0 2 \ - - / "  
wj /TA ,  NT  
IV = {barn, jumal ,  ba~dhao, . . .}  
Wl G IV 
wj  ~ II / 
Figul'e 7: A {lel;('~rministi{: finil;{; stat(; trans{lucer t{} 
process temporal expression 
in a similar way. The subs(xlU{'.ntial FST for our 
sy:stem is detined as in Figure {i and l?igu\]{~ 7 illus- 
trates th{. ~ trans(hl{:er in Figure. (i. In the tigurc, ti 
is a {'lass to which the tc.mporal w{}rd 1}elongs in the. 
temporal classification. "wi is a word {}the.r than tem- 
poral ones that has |;11{; prex:e.ding temporal word be 
its moditier, and 'wj is not such a word to make a 
COml)Oui:d noun. TN,  TA and NT are synta{'ti{: 
tags. A word tagged with TN would mo(lit~y a suc- 
(:ceding noun like barn(night), ban.qh, ag(vacation). A
word attached with TA wouhl mo{lii2y a predicate 
and oi1{; with NT  means it is not a temporal word. 
Actually, individual FSTs arc {:oml}ined int{) one. an(l 
rules for tagging of teml}oral wor{ts are put over the. 
FST. The rule is al}plied according to the priority 
by fro(tllOll{;y ill case m(}re than o11o (}uttmt are pos- 
sible for a context. Namely, it is a rule-based system 
where the rules are extracted fi'om corl}us. 
4 .2  Clmnk ing  
After the FST of temporal exi}ressiolls adds to words 
syntactic tags such as TN and TA, chunking is {:on- 
ducted with results t iom outl)uts 1)y the Fsr\] '. As we 
said earlier, {:lmnking in Kore.an is relatively easy 
only if the teml}oral ext}ression would l)e suc{:e.ss- 
fltlly recognized. Actually, our clmnker is also 1)ased 
on the finite, state lnachine. The tbllowing is an ex- 
ample tbr chunking rule.s. 
(Nl~h,.,~) -? (NP) I (TNP) 
(NP) -~ (N)* (NP) I (N)* (Nu)* (uN) 
('rNu) -~ ('rN)* (N)* (NP) 
Here, N is a noun without any 1)ostl)osition, N/? is 
a noun with a postposition, TN is a temporal noun 
recoglfized as modii~ying a succeeding 1101111, NU is 
a numbe.r and UN is a unit noun. Aft, er tcmI)oral 
tagging, the chunker transforms 'NT'  into N, NP, 
(~tc. according to morphological constituents and 
their POS. Brietly: the rule. says that an NP clmnk is 
made fl'om either NP or temporal NIL An NP would 
1)e (:onst, rll(;te.(1 with one. or lnOl'O llOlllIS and their 
m{}{lifie{~ {)r with a noun quantified. A TNP, whi{:h 
is re, lated with time, is made fr{)m nouns mo(liticd 
by teml}oral words which w{mld be ide, ntitied by the, 
FST. By identification {)f t('mt){}ral ex\]}ression and 
chunldng, the following exami)le sentence is ctmnked 
as below. 
? jinan(last) ycorcum(summer) ban.qhag-c(in 
vacation) 'ari-ncun(we/SUB3) kco'm, pyu- 
.<thr,,{,) 
sa.ssda(bought) 
-+  \VC bOl lght  1;t117o.o comi)uters in the last 
Sll l l l l l ler vacal0io\] l .  
? jinanTN ycoreumTN banghag-cN1, uvi-ncunNp 
kcom, pyuteoN SeNU dac-vculNp sassdav 
? \[jinanTN yeorelt~lZ,l,N ban.qha.q-cNP\]N(~' 
\ [ Iw i -T tC ' l t * tNP \ ]N  C \[kcompyutcoN SCNU dac- 
rculN P\]N(; sassdav 
5 Exper imenta l  Resu l t s  
bbr the {;xi}erinmnt al}out temporal exI}ressi{m, we 
extracted 300 senten{:es containing teml}oral expres- 
si(ms from ETRI  POS corlms. Tal}le 2 shows the r{'.- 
959 
t precision J_?gcall 
rate (%) 97.5 90.56 
Table 2: Results of identifying temporal expression 
no chunking-\[ using chuifldng 
4.8 -\[ 3.3 avg. # of cand \[ 
Table 3: Reduction of candidates resulted from 
chunking 
sults from identit:ying temporal expressions and dis- 
aml/iguatil~g their syntactic functions. From the re- 
sult in the table we see that the method is very effec- 
t, ive in that it very accurately identifies all tile tem- 
poral expressions aim assigns them syntactic tags. 
And, Table 2 shows the reduction resulted from 
chunking after temporal expression identification. 
We take into consideration the average numl)cr of 
head candidates for each word since our parser is 
dependency based one. The test was conducted on 
the tirst file (about 800 sentences) of KAIST tree- 
bank (Choict  al. , 1994). The number was reduced 
by 51% in candidates compared to the system with 
no chunking, whMl makes pa.rsing efficient. 
Most of errors were caused by tile case where tein- 
poral words have different syntactic roles under the 
same context. In this case, the global context such as 
the whole sentence or intcrscnt;cntial infornlation or 
sometimes very soi)histicated processing ixneeded to 
resolve the prol)lem, l~br instance, '82 ~tycoTt(year) 
h, yco'njac-yi(now/Gl;N)' could be used two-.way. If 
the speech time is the year 1982, then h, yeou(fl, e-yi are 
conlbined with 82 nycon to represent time. Other- 
wise, 82 does not ino(til~y hycordac-yi, wlfich cannot 
be recognized only with the local context. Neverthe- 
less, the system is promising in that generally it can 
ilnprove e\[flciency without losing accuracy which is 
crucial for the pracl;ical system. 
6 Conclusions 
hi this paper, we presented a method for identifi- 
cation of temporal exi)ressions and their syntactic 
functions based on FST aim lexical data extracted 
fl:om corpus. Since tenlporal words have the syntac- 
tic ambiguity when used in a sentence, it; is impo> 
tant to identify the syntactic functioll as well as the 
temporal expression itself. 
For the purpose, we manually extracted lexical c(> 
occurrences t?om large corpus aim it was possible as 
the number of temporal nouns is tractable nough 
to manipulate lexical data t)y hand. As shown in 
tile result, lexical co-occurrences are crucial for dis- 
mnbiguating the syntactic flmction of the tenlporal 
expression. Besides, the finite state approach pro- 
vide(l an eftieient model for temporal expression pro- 
cessing. Combined with the clmnker, it helped re- 
nmrkably lessen, by 1)runing irrelewmt candidates, 
intermediate structures generated while parsing. 
References 
Almey, S. 199t. Parsing By Chunks. ill Berwick, 
Abney, and Tenny, editors, Principlc-\]3ascd Par.s- 
ing. Boston: Klnwer Acadenlic Publishers. 
Choi, K. S., tIan, Y. S., Han, Y. G., and Kwon, O. 
W. 1994. KAIST Tree Bank Project for Korean: 
Present and Future Develot)ment. In Proceedings 
of the l~ttcrnational Workshop on ,%ara, blc Natu- 
ral Language Resources. 
Ciravegna, F. and Lavelli, A. 1997. Controlling 
Bottom-Ut) Chart Parsers through Text Clmnk- 
ing. In Proceedings of the 5th International Work- 
shop on Parsing ~);chnology. 
Collins, M. J. 1996. A New Statistical Parser Based 
on Bigram Lexical l)et)endencies. Ill Proceedings 
of the 3~th Annual Meeting of the ACL. 
Elgot, C. C. and Mezei, J. E. 1965. On relations de- 
fined by generalized finite alltonlata. \[B~d r Journal 
of Rcscarc.h and Development, 9, 47-65. 
Gross, M. 1993. Local Grmnmars and their I{ep- 
resen~ation by Finite Automata. Data, Descrip- 
tion, Discourse: l'apcrs on l;hc English language 
i77, \[tor'lto,ur" of John Mc\[\[ Sinclair, Michael Hoey 
(ed). London: HarperCollins Publishers. 
KAIST. KAIST Concordance Program. URL 
htt;p://(:sfive.kaist.ac.kr/kcp/. 
Mohri, M. 1997. Finite-state ~\]5:ans(luccrs in lan- 
guage and Spe(~ch Processing. 6*omp'ul;ational 
Li'n.gui.stics , Vol 23, No (2). 
Ramstmw, L. A. and Marcus, M. \]'. 1995. Text 
Chunking Using Transtbrmation-Based l,earning. 
In Proceedings of the ACL Workshop o'n l/cry 
Large Corpora. 
l{oche, E. and Schabes, Y. 1995. Deterministic 
Part-of-Speech Tagging with Finite-State Trans- 
ducers. Computatiou, al Li'n, guistics, Vol 21, No 
(2). 
Roche, E. and Schabes, Y. 1997. Finite-State Lan- 
guage Processing. The MIT Press. 
Skut, W. and Brants, T. 1999. Chunk Tagger. 
llIl Proceedings of ESSLLI-98 Workshop on Au- 
tomated Acquisition of Syntax and Parsing. 
Sproat, R. W., Shih, W., Gale, W. and Chang, 
N. 1994. A Stochastic Finite-State Word- 
segmentation Algorithm for Chinese. In Pwceed- 
ings of th, c 32rid Annual Meeting of ACL 
Yoon, .J., Choi, K. S. and Song, M. 1999. Three 
Types of Ctmnking in Korean and l)ependency 
Analysis Based on Lexical Association. In l'm- 
cccdings of ICCPOL '99. 
960 
 
	ff