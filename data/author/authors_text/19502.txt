Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 79?88, Dublin, Ireland, August 23-29 2014.
Joint Inference and Disambiguation of Implicit Sentiments via
Implicature Constraints
Lingjia Deng
1
, Janyce Wiebe
1,2
, Yoonjung Choi
2
1
Intelligent Systems Program, University of Pittsburgh
2
Department of Computer Science, University of Pittsburgh
lid29@pitt.edu, wiebe@cs.pitt.edu, yjchoi@cs.pitt.edu
Abstract
This paper addresses implicit opinions expressed via inference over explicit sentiments and
events that positively/negatively affect entities (goodFor/badFor, gfbf events). We incorporate
the inferences developed by implicature rules into an optimization framework, to jointly improve
sentiment detection toward entities and disambiguate components of gfbf events. The framework
simultaneously beats the baselines by more than 10 points in F-measure on sentiment detection
and more than 7 points in accuracy on gfbf polarity disambiguation.
1 Introduction
Previous work in NLP on sentiment analysis has mainly focused on explicit sentiments. However, as
noted in (Deng and Wiebe, 2014), many opinions are expressed implicitly, as shown by this example:
Ex(1) The reform would lower health care costs, which would be a tremendous positive change across the entire
health-care system.
There is an explicit positive sentiment toward the event of ?reform lower costs?. However, in expressing
this sentiment, the writer also implies he is negative toward the ?costs?, since he?s happy to see the costs
being decreased. Moreover, the writer may be positive toward ?reform? since it contributes to the ?lower?
event. Such inferences may be seen as opinion-oriented implicatures (i.e., defeasible inferences)
1
.
We develop a set of rules for inferring and detecting implicit sentiments from explicit sentiments and
events such as ?lower? (Wiebe and Deng, 2014). In (Deng et al., 2013), we investigate such events,
defining a badFor (bf) event to be an event that negatively affects the theme and a goodFor (gf) event to
be an event that positively affects the theme of the event.
2
Here, ?lower? is a bf event. According to their
annotation scheme, goodFor/badFor (gfbf) events have NP agents and themes (though the agent may be
implicit), and the polarity of a gf event may be changed to bf by a reverser (and vice versa).
The ultimate goal of this work is to utilize gfbf information to improve detection of the writer?s senti-
ments toward entities mentioned in the text. However, this requires resolving several ambiguities: (Q1)
Given a document, which spans are gfbf events? (Q2) Given a gfbf text span, what is its polarity, gf
or bf? (Q3) Is the polarity of a gfbf event being reversed? (Q4) Which NP in the sentence is the agent
and which is the theme? (Q5) What are the writer?s sentiments toward the agent and theme, positive
or negative? Fortunately, the implicature rules in (Deng and Wiebe, 2014) define dependencies among
these ambiguities. As in Ex(1), the sentiments toward the agent and theme, the sentiment toward the gfbf
event (positive or negative), and the polarity of the gfbf event (gf or bf) are all interdependent. Thus,
rather than having to take a pipeline approach, we are able to develop an optimization framework which
exploits these interdependencies to jointly resolve the ambiguities.
Specifically, we develop local detectors to analyze the four individual components of gfbf events,
(Q2)-(Q5) above. Then, we propose an Integer Linear Programming (ILP) framework to conduct global
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Specifically, we focus on generalized conversational implicature (Grice, 1967; Grice, 1989).
2
Compared to (Deng et al., 2013), we change the term ?object? to ?theme? as the later is more appropriate for this task.
79
inference, where the gfbf events and their components are variables and the interdependencies defined by
the implicature rules are encoded as constraints over relevant variables in the framework. The reason we
do not address (Q1) is that the gold standard we use for evaluation contains sentiment annotations only
toward the agents and themes of gfbf events. We are only able to evaluate true hits of gfbf events. Thus,
the input to the system is the set of the text spans marked as gfbf events in the corpus. The results show
that, compared to the local detectors, the ILP framework improves sentiment detection by more than 10
points in F-measure and disambiguating gfbf polarity by more than 7 points in the accuracy, without any
loss in accuracy for other two components.
2 Related Work
Most work in sentiment analysis focuses on classifying explicit sentiments and extracting explicit opinion
expressions, holders and targets (Wiebe et al., 2005; Johansson and Moschitti, 2013; Yang and Cardie,
2013). There is some work investigating features that directly indicate implicit sentiments (Zhang and
Liu, 2011; Feng et al., 2013). In contrast, we focus on how we can bridge between explicit and implicit
sentiments via inference. To infer the implicit sentiments related to gfbf events, some work mines various
syntactic patterns (Choi and Cardie, 2008), proposes linguistic templates (Zhang and Liu, 2011; Anand
and Reschke, 2010; Reschke and Anand, 2011), or generates a lexicon of patient polarity verbs (Goyal
et al., 2013). Different from their work, which do not cover all cases relevant to gfbf events, (Deng and
Wiebe, 2014) defines a generalized set of implicature rules and proposes a graph-based model to achieve
sentiment propagation between the agents and themes of gfbf events. However, that system requires
all of the gfbf information (Q1)-(Q4) to be input from the manual annotations; the only ambiguity it
resolves is sentiments toward entities. In contrast, the method in this paper tackles four ambiguities
simultaneously. Further, as we will see below in Section 6, the improvement over the local detectors by
the current method is greater than that by the previous method, even though it operates over the noisy
output of local components automatically.
Different from pipeline architectures, where each step is computed independently, joint inference has
often achieved better results. Roth and Yih (2004) formulate the task of information extraction using
Integer Linear Programming (ILP). Since then, ILP has been widely used in various tasks in NLP, in-
cluding semantic role labeling (Punyakanok et al., 2004; Punyakanok et al., 2008; Das et al., 2012),
joint extraction of opinion entities and relations (Choi et al., 2006; Yang and Cardie, 2013), co-reference
resolution (Denis and Baldridge, 2007), and summarization (Martins and Smith, 2009). The most similar
ILP model to ours is (Somasundaran and Wiebe, 2009), which improves opinion polarity classification
using discourse constraints in an ILP model. However, their work addresses discourse relations among
explicit opinions in different sentences.
3 GoodFor/BadFor Event and Implicature
This work addresses sentiments toward, in general, states and events which positively or negatively
affect entities. Deng et al. (2013) (hereafter DCW) identify a clear case that occurs frequently in opinion
sentences, namely the gfbf events mentioned above. As defined in DCW, a gf event is an event that
positively affects the theme of the event and a bf event is an event that negatively affects the theme.
According to the annotation schema, gfbf events have NP agents and themes (though the agent may be
implicit). In the sentence ?President Obama passed the bill?, the agent of the gf ?passed? is ?President
Obama? and the theme is ?the bill?. In the sentence ?The bill was denied?, the agent of the bf ?was
denied? is implicit. The polarity of a gf event may be changed to bf by a reverser (and vice versa). For
example, in ?The reform will not worsen the economy,? ?not? is a reverser and it reverses the polarity
from bf to gf.
3
The constraints we encode in the ILP framework described below are based on implicature rules in
(Deng and Wiebe, 2014). Table 1 gives two rule schemas, each of which defines four specific rules. In
3
DCW also introduce retainers. We don?t analyze retainers in this work since they do not affect the polarity of gfbfs, and
only 2.5% of gfbfs have retainers in the corpus.
80
s(gfbf) gfbf ? s(agent) s(theme) s(gfbf) gfbf ? s(agent) s(theme)
1 positive gf ? positive positive 3 positive bf ? positive negative
2 negative gf ? negative negative 4 negative bf ? negative positive
Table 1: Rule Schema 1 & Rule Schema 3 (Deng and Wiebe, 2014)
the table, s(?) = ? means that the writer?s sentiment toward ? is ?, where ? is a gfbf event, or the agent
or theme of a gfbf event, and ? is either positive or negative. P? Q means to infer Q from P.
Applying the rules to Ex(1): the writer expresses a positive sentiment (?positive?) toward a bf event
(?lower?), thus matching Case 3 in Table 1. We infer that the writer is positive toward the agent (?re-
form?) and negative toward the theme (?costs?). Two other rule schemas (not shown) make the same
inferences as Rule Schemas 1 and 3 but in the opposite direction. As we can see, if two entities partic-
ipate in a gf event, the writer has the same sentiment toward the agent and theme, while if two entities
participate in a bf event, the writer has opposite sentiments toward them. Later we use this observation
in our experiments.
4 Global Optimization Framework
Optimization is performed over two sets of variables. The first set is GFBF, containing a variable for
each gfbf event in the document. The other set is Entity, containing a variable for each agent or theme
candidate. Each variable k in GFBF has its corresponding agent and theme variables, i and j, in Entity.
The three form a triple unit, ?i, k, j?. The set Triple consists of each ?i, k, j?, recording the correspon-
dence between variables in GFBF and Entity. The goal of the framework is to assign optimal labels to
variables in Entity and GFBF. We first introduce how we recognize candidates for agents and themes,
then introduce the optimization framework, and then define local scores that are input to the framework.
4.1 Local Agents and Theme Candidates Detector
We extract two agent candidates and two theme candidates for each gfbf event (one each will ultimately
be chosen by the ILP model).
4
We use syntax, and the output of the SENNA (Collobert et al., 2011)
semantic role labeling tool. SENNA labels the A0 (subject), A1 (object), and A2 (indirect object) spans
for each predicate, if possible. To extract the semantic agent candidate: If SENNA labels a span as A0
of the gfbf event, we consider it as the semantic agent; if there is no A0 but A1 is labeled, we consider
A1; if there is no A0 or A1 but A2 is labeled, we consider A2. To extract the syntactic agent candidate,
we find the nearest noun in front of the gfbf span, and then extract any other word that depends on the
noun according to the dependency parse. Similarly, to extract the semantic theme candidate, we consider
A1, A2, A0 in order. To extract the syntactic theme candidate, the same procedure is conducted as for
the syntactic agent, but the nearest noun should be after the gfbf. If there is no A0, A1 or A2, then there
is only one agent candidate, implicit and only one theme candidate, null. We treat a null theme as an
incorrect span in the later evaluations. If the two agent (theme) candidate spans are the same, there is
only one candidate.
4.2 Integer Linear Programming Framework
We use Integer Linear Programming (ILP) to assign labels to variables. Variables in Entity will be
assigned positive or negative, representing the writer?s sentiments toward them. We may have two candi-
date agents for a gfbf and that we will choose between them. Thus, only one agent is assigned a positive
or negative label; the other is considered to be an incorrect agent of the gfbf (similarly for the theme can-
didates). Each variable in GFBF will be assigned the label gf or bf. Optionally, it may also be assigned
the label reversed. Label gf or bf is the polarity of the gfbf event; reversed is assigned if the polarity is
reversed (e.g., for ?not harmed?, the labels are bf and reversed).
The objective function of the ILP is:
4
This framework is able to handle any number of candidates. The methods we tried using more candidates did not perform
as well - the gain in recall was offset by larger losses in precision.
81
min
u
1gf
,u
1bf
...
(
? 1 ?
?
i?GFBF?Entity
?
c?L
i
p
ic
u
ic
)
+
?
?i,k,j??Triple
?
ikj
+
?
?i,k,j??Triple
?
ikj
(1)
subject to
u
ic
? {0, 1}, ?i, c ?
ikj
, ?
ikj
? {0, 1},??i, k, j? ? Triple (2)
where L
i
is the set of labels given to ?i ? GFBF ? Entity. If i ? GFBF, L
i
is {gf, bf, reversed} ({gf,
bf, r}, for short). If i ? Entity, L
i
is {positive, negative} ({pos, neg}, for short). u
ic
is a binary in-
dicator representing whether the label c is assigned to the variable i. When an indicator variable is 1,
the corresponding label is selected. p
ic
is the score given by local detectors, introduced in the following
sections. Variables ?
ikj
and ?
ikj
are binary slack variables that correspond to the gfbf implicature con-
straints of ?i, k, j?. When a given slack variable is 1, the corresponding triple violates the implicature
constraints. Minimizing the objective function could achieve two goals at the same time. The first part
(?1 ?
?
i
?
c
p
ic
u
ic
) tries to select a set of labels that maximize the scores given by the local detectors.
The second part (
?
ikj
?
ikj
+
?
ikj
?
ikj
) aims at minimizing the cases where gfbf implicature constraints
are violated. Here we do not force each triple to obey the implicature constraints, but to minimize the
violating cases. For each variable, we have defined constraints:
?
c?L
GFBF
?
u
kc
= 1, ?k ? GFBF (3)
?
i?Entity
?i,k,j??Triple
?
c?L
Entity
u
ic
= 1,?k ? GFBF (4)
?
j?Entity,
?i,k,j??Triple
?
c?L
Entity
u
jc
= 1,?k ? GFBF (5)
where L
GFBF
?
in Equation (3) is a subset of L
GFBF
, consisting of {gf, bf}. Equation (3) means a
gfbf must be either gf or bf. But it is free to choose whether it is being reversed. Recall that we have two
agent candidates (a1,a2) for a gfbf. Thus we have four agent indicators in Equation (4): u
a1,pos
, u
a1,neg
,
u
a2,pos
and u
a2,neg
. Equation (4) ensures that three of them are 0 and one of them is 1. For instance,
u
a1,pos
assigned 1 means that candidate a1 is selected to be the agent span and pos is selected to be its
polarity. In this way, the framework disambiguates the agent span and sentiment polarity simultaneously.
(Similar comments apply for the theme candidates in Equation (5).)
According to the implicature rules in Table 1 in Section 3, the writer has the same sentiment toward
entities in a gf relation. Thus, for each triple unit ?i, k, j?, the gf constraints are applied via the following:
|
?
i,?i,k,j?
u
i,pos
?
?
j,?i,k,j?
u
j,pos
|+ |u
k,gf
? u
k,r
| <= 1 + ?
ikj
, ?k ? GFBF (6)
|
?
i,?i,k,j?
u
i,neg
?
?
j,?i,k,j?
u
j,neg
|+ |u
k,gf
? u
k,r
| <= 1 + ?
ikj
, ?k ? GFBF (7)
We use |u
k,gf
? u
k,r
| to represent whether this triple is gf. In Equation (6), if this value is 1, then the
triple should follow the gf constraints. In that case, ?
ikj
= 0 means that the triple doesn?t violate the
gf constraints, and |
?
i
u
i,pos
?
?
j
u
j,pos
| must be 0. Further, in this case,
?
i
u
i,pos
and
?
j
u
j,pos
are
constrained to be of the same value (both 1 or 0) ? that is, entities i and j must be both positive or both
not positive. However, if ?
ikj
= 1, Equation (6) does not constrain the values of the variables at all. If
|u
k,gf
? u
k,r
| is 0, representing that the triple is not gf, then Equation (6) does not constrain the values
of the variables. Similar comments apply to Equation (7).
In contrast, the writer has opposite sentiments toward entities in a bf relation.
|
?
i,?i,k,j?
u
i,pos
+
?
j,?i,k,j?
u
j,pos
? 1|+ |u
k,bf
? u
k,r
| <= 1 + ?
ikj
, ?k ? GFBF (8)
|
?
i,?i,k,j?
u
i,neg
+
?
j,?i,k,j?
u
j,neg
? 1|+ |u
k,bf
? u
k,r
| <= 1 + ?
ikj
,?k ? GFBF (9)
We use |u
k,bf
? u
k,r
| to represent whether this triple is bf. In Equation (8), if a triple is bf and the
constraints are not violated, then |
?
i
u
i,pos
+
?
j
u
j,pos
? 1| must be 0. Further, in this case,
?
i
u
i,pos
82
ugf
u
bf
u
r
|u
gf
? u
r
| |u
bf
? u
r
| u
gf
u
bf
u
r
|u
gf
? u
r
| |u
bf
? u
r
|
A 1 0 0 1 0 C 0 1 0 0 1
B 0 1 1 1 0 D 1 0 1 0 1
Table 2: Truth table of being reversed or not (k is omitted)
and
?
j
u
j,pos
are constrained to be of the opposite value ? that is, if entity i is positive then entity j must
not be positive. Similar comments apply to Equation (9).
Note that above we use |u
k,gf
?u
k,r
| and |u
k,bf
?u
k,r
| to represent whether a triple is gf or bf. In Table
2, we show that they always take opposite values and that they are consistent with the actual polarities.
In Table 2, Case A means the triple is gf and Case B means the triple is bf but it is reversed. In both
cases, |u
gf
? u
r
| = 1, indicating that the triple should follow the gf constraints. Similarly for Case C
and Case D to follow the bf constraints.
4.3 Local GoodFor/BadFor Score: p
k,gf
, p
k,bf
We utilize a sense-level gfbf lexicon by (Choi et al., 2014). In total there are 6,622 gf senses and 3,290
bf senses. The gf lexicon covers 64% of the gf words in the corpus and the bf lexicon covers 42% of the
bf words. We then look up the gfbf span k in the gfbf lexicon. If k only appears in the gf lexicon, then
p
k,gf
= 1 ?  and p
k,bf
= . Here  = 0.0001, to prevent there being any 0 scores in our computation.
If k only appears in the bf lexicon, then p
k,bf
= 1 ?  and p
i,gf
= . If k appears in both the gf and bf
lexicon, and there are a senses in the gf lexicon and b senses in the bf lexicon, then p
k,gf
= a/(a + b)
and p
k,bf
= b/(a + b). If k is not in either lexicon, then p
k,gf
= p
k,bf
= . If there is more than one
word in the gfbf span, we take the maximum score.
4.4 Local Reversed Score: p
k,r
As introduced in Section 3, a reverser changes the polarity of a gfbf. First, we build reverser lexicons
from Wilson?s shifter lexicon (2008), namely the entries labeled as genshifter, negation, and shiftneg.
We create two lexicons: one with the verbs and the other with the non-verb entries, excluding nouns,
adjectives, and adverbs, since most non-verb reversers are prepositions or subordinating conjunctions.
There are 219 reversers in the entire corpus; 134 (61.19%) are instances of words in one of the two
lexicons. Based on the lexicon, we categorize reversers into three classes. Examples are shown below.
Ex(2) They will not be able to water down your coverage.
Ex(3) ... how a massive new bureaucracy will cut costs without hurting the old and the helpless.
Ex(4) The new law includes new rules to prevent insurance companies from overcharging patients.
Negation: An instance in this category is ?not? in Ex(2). If any word in the gfbf span has a neg
dependency relation according to the Stanford dependency parser, then we consider the gfbf to be negated
(i.e., reversed). In this case the path between the negator and the gfbf is labeled neg and the length of the
path is one.
Other Non-Verb: This category consists of words such as ?without? in Ex(3) (others are ?never? and
?few?, etc). These words lower the extent of the gfbf event. We look in the sentence for instances of
words in the non-verb reverser lexicon, which are not tagged as noun, verb, adj, or adv. For any found,
we examine the path in the dependency parse between the potential reverser and the gfbf span. If the
path has at least one of advmod, pcomp, cc, xcomp, nsubj, neg and the length of the path is less than four
(learnt from development set), the event is considered to be reversed.
Verb: In Ex(4), the verb ?prevent? stops the gfbf event ?overcharging? from happening. We call such
words Verb reverser (others are ?prohibit? and ?ban?, etc). We look in the sentence for instances of words
in the verb reverser lexicon. For any that appear before the gfbf span in the sentence, if the path has at
least one of xcomp, pcomp, obj and the length of the path is less than four, then the event is reversed.
For the triple ?companies, overcharging, patients? in Ex(4), though it is reversed by ?prevent?, the agent
of the reverser, which is ?law?, is different from the agent of the gfbf, which is ?companies?, so the bf
83
within the ?overcharging? event is not reversed.
5
Though we extract the Verb reversers to evaluate the
performance of recognizing a reverser, in the optimization framework, gfbf events with Verb reversers
are not considered to be reversed, since almost all Verb reversers introduce new agents.
Different from other scores, p
k,r
could be negative. According to the heuristics above, the probability
of a gfbf event being reversed decreases as the length of the path increases. We define p
k,r
so it is
inversely proportional to the length of the path. Further, to make sense of a gfbf triple ?agent, gfbf,
theme?, where, e.g., the local detectors label it ?pos, bf, pos?, the framework is choosing the smaller
one from (a) ?1 ? p
k,r
? u
k,r
(it has a reverser) versus (b) 1 ? ?
ikj
(it is an exception to the rules). The
framework assigns u
k,r
= 0 and ?
ikj
= 1 if ?1 ? p
k,r
> 1. It assigns u
k,r
= 1 and ?
ikj
= 0 if
?1 ? p
k,r
<= 1. For gfbf events which have Negation or Other Non-verb reversers, since we use the
length four as a threshold in the heuristics above, we define p
k,r
=
1
d
?
5
4
, so that ?1 ? p
k,r
=
5
4
?
1
d
> 1
if d > 4. For gfbf events for which no reverser word appears in the sentence, or those which only have
Verb reversers, p
k,r
= ?1 ?
5
4
(so ?1 ? p
k,r
> 1), so that the framework chooses case (b) (choosing the
gfbf event to be not reversed).
4.5 Local Sentiment Score: p
i,pos
, p
i,neg
In the corpus of DCW, only the writer?s sentiments toward the agents and the themes of gfbf events are
annotated. Thus, since there are many false negatives of sentiments toward entities, the corpus does
not support training a classifier. Therefore, we adopt the same local sentiment detector from (Deng
and Wiebe, 2014), using available resources to detect writer?s sentiments toward all agent and theme
candidates.
6
The sentiment scores range from 0.5 to 1.
5 Co-reference In the Framework
So far the constraints in the framework are within a gfbf triple. Consider the following example:
Ex(5) The reform will decrease the healthcare costs and improve the medical qualify as expected.
The two gfbfs, ?decrease? and ?improve? have the same agent, ?reform?. Thus, if there is more than
one gfbf in a sentence, and the path between the two gfbfs in dependency parse contains only conj or
xcomp, and there is no other noun between the latter gfbf and the conjunction, we assume the two agents
are the same and the sentiments toward them should be the same. Thus, for any i, j ? Entity, if i, j
co-refer
7
, or they are the same agent as described above, Coref(i, j) = 1 (otherwise 0). We add two
more constraints, similar to the gf constraints in Equations (6) and (7), as shown in Equation (10) and
(11). where ?
ij
is a slack variable, e(i) is the set of agent/theme candidates linked to the same gfbf as i
is. If Coref(i, j) = 0, Equations (10) and (11) do not constrain the variables. The objective function in
Equation (12) is updated to incorporate these new constraints.
|
?
e(i)
u
i,pos
?
?
e(j)
u
j,pos
|+ Coref(i, j) <= 1 + ?
ij
,?i, j ? Entity (10)
|
?
e(i)
u
i,neg
?
?
e(j)
u
j,neg
|+ Coref(i, j) <= 1 + ?
ij
, ?i, j ? Entity (11)
min
u
1gf
,u
1bf
...
(
? 1 ?
?
i?GFBF?Entity
?
c?L
i
p
ic
u
ic
)
+
?
?i,k,j??Triple
?
ikj
+
?
?i,k,j??Triple
?
ikj
+
?
i,j?Entity
?
ij
(12)
6 Experiment and Performance
In this section we introduce the data we use, the baseline methods, the evaluations and the results. In
addition, we give examples illustrating how opinion inference may improve performances.
5
DCW defines here is a triple chain: ?law, prevent ?companies, overcharging, patients??. The reverser is changing the
polarity between ?law? and ?patients?, but it does not change the polarity between ?companies? and ?patients?.
6
We use Opinion Extractor (Johansson and Moschitti, 2013) , opinionFinder (Wilson et al., 2005), MPQA subjectivity
lexicon (Wilson et al., 2005), General Inquirer (Stone et al., 1966) and a connotation lexicon (Feng et al., 2013), to detect
writer?s sentiments toward all agent and theme candidates, and all gfbf events. We adopt Rule 1 and Rule 3 to infer from the
sentiment toward event to the sentiment toward theme. Then we conduct a majority voting based on the results.
7
We use the co-reference resolution system from (Stoyanov et al., 2010).
84
6.1 Experiment Data
We use the ?Affordable Care Act? corpus of DCW, consisting of 134 online editorials and blogs. In total,
there are 1,762 annotated triples, out of which 692 are gf or retainers and 1,070 are bf or reversers. From
the writer?s perspective, 1,495 noun phrases are annotated positive, 1,114 noun phrases are negative
and the remaining 8 are neutral. This indicates that there are many opinions in the corpus. Out of 134
documents in the corpus, 3 do not have any annotation. 6 are used as a development set to develop the
heuristics in Sections 4 and 5. We use the remaining 125 for the experiments.
6.2 Baseline Methods and Evaluation Metrics
We compare the output of the global optimization framework with the outputs of baseline systems built
from the local detectors in Section 4. For the gfbf polarity and reverser ambiguities, the local detectors
directly provide a disambiguation result. For the agent/theme span and sentiment ambiguities, the local
sentiment detector assigns positive and negative scores to each candidate. The framework chooses among
the combined options. Thus, for comparison, we build a baseline system that combines the outputs of
the local agent/theme candidate detector and the local sentiment detector.
Recall from Section 4, a variable k ? GFBF has two agent candidates, a1 and a2 ? Entity. Together
there are four binary indicator variables: u
a1,pos
, u
a1,neg
, u
a2,pos
and u
a2,neg
. Among these indicator
variables whose corresponding local scores (e.g., p
a1,pos
is the score of u
a1,pos
) are larger than 0.5,
the baseline system (denoted Local) chooses the one with the largest local sentiment score. If there is
a tie, it prefers the variable representing the semantic candidate. If there is still a tie, it chooses the
variable representing the majority polarity (positive). If all the local scores of the four variables are
0.5 (neutral), Local fails to recognize any sentiment for that entity, so it assigns 0 to all the indicator
variables. Local+coref takes the maximum local score of the entities if they co-ref, and assigns each
entity the maximum score before disambiguation.
Another baseline, Majority, always chooses the semantic candidate and the majority polarity.
To evaluate the performance in detecting sentiment, we use precision, recall, and F-measure. We do
not take into account any agent or theme manually annotated as neutral (there are only 8).
P =
#(auto=gold & gold!=neutral)
#auto!=neutral
Accuracy = R =
#(auto=gold & gold!=neutral)
#gold!=neutral
F =
2*P*R
P+R
(13)
In the equations, auto is the system?s output and gold is the gold-standard label from annotations. Since
we don?t take into account any neutral agent or theme, #gold!=neutral equals to all nodes in the exper-
iment set. Thus accuracy is equal to recall. We only report recall here. Here we have two definitions
of auto=gold: (1) Strict evaluation means that, by saying auto=gold, the agent/theme must have the
same polarity and must be the same NP as the gold standard, and (2) Relaxed evaluation means the
agent/theme has the same polarity as the gold standard, regardless whether the span is correct or not.
Note that according to DCW, an implicit agent isn?t annotated with any sentiment. Thus, for an
implicit agent in gold, if auto outputs the span ?implicit?, we treat it as a correct span with correct
polarity, regardless what sentiment auto gives to it. If auto outputs any span other than ?implicit?, we
treat it as a wrong span with wrong polarity, regardless of its sentiment as well. For the theme span, if
auto outputs a ?null? theme candidate, we treat it as a wrong span but we evaluate its sentiment according
to gold.
To evaluate extracting candidate span, we use accuracy. The baseline for this task always chooses the
semantic candidate. To evaluate gfbf polarity and reverser, we also use accuracy.
Note that although we evaluate the performance in different tasks separately, the framework resolves
all the ambiguities at the same time.
6.3 Results
We report the performance results for (A) sentiment detection in Table 3, on two sets. One is the subset
containing the agents and themes where auto has the correct spans with gold. The other is the set of
all agents and themes. As shown in Table 3, ILP significantly improves performance, approximately
10-20 points on F-measure over different baselines. Though Local has a competitive precision with
85
correct span subset whole set, strict eval whole set, relaxed eval
P R F P R F P R F
1 ILP 0.6421 0.6421 0.6421 0.4401 0.4401 0.4401 0.5939 0.5939 0.5939
2 Local 0.6409 0.3332 0.4384 0.4956 0.2891 0.3652 0.5983 0.3490 0.4408
3 ILP+coref 0.6945 0.6945 0.6945 0.4660 0.4660 0.4660 0.6471 0.6471 0.6471
4 Local+coref 0.6575 0.3631 0.4678 0.5025 0.3103 0.3836 0.6210 0.3834 0.4741
5 Majority 0.5792 0.5792 0.5792 0.3862 0.3862 0.3862 0.5462 0.5462 0.5462
Table 3: Performances of sentiment detection
ILP, it has a much lower recall. That means the local sentiment detector cannot recognize implicit
sentiments toward most entities. But ILP is able to recognize more entities correctly. By adding coref,
performance improves for both ILP and Local. In comparison to (Deng and Wiebe, 2014), our current
method improves more in F-measure (2.43 points more) over local sentiment detector than the earlier
work, even though the earlier work takes the manual annotations of all the gfbf information as input.
In terms of the other tasks: For (B) agent/theme span, the baseline achieves 66.67% in accuracy, com-
pared to 68.54% and 67.10% for ILP and ILP+coref, respectively. For (C) gfbf polarity, the baseline
has an accuracy of 70.68%, whereas ILP achieves 77.25% and ILP+coref achieves 77.47%, respectively,
both 7 points higher. This improvement is interesting because it represents cases in which the optimiza-
tion framework is able to infer the correct polarity even though the gfbf span is not recognized by the
local detector (i.e., the span isn?t in the gfbf lexicon). For (D) reverser, the baseline is 88.07% in accu-
racy. ILP and ILP+coref are competitive with the baseline: 89% and 88.07% respectively. Note that both
our local detector and ILP surpass the majority class (not reversed) which has an accuracy of 86.60%.
Following (Akkaya et al., 2009), since ILP is unsupervised without multiple runs, we adopt McNe-
mar?s test to measure statistical significance of our improvements (Dietterich, 1998). In Table 3, the
improvements in recalls of Line 1 over 2, Line 3 over 4, and Lines 1&3 over 5 are statistically significant
at the p < .001 level. The improvements of Line 3 over 1 are statistically significant at the p < .005
level. For accuracy of gfbf polarity, the improvement is significant at the p < .001 level.
6.4 Examples
This sections gives simplified examples to illustrate how the framework can improve over the local
detectors. The explicit sentiment clues referred to in this section are from MPQA lexicon.
Ex(6) The reform would curb skyrocketing costs in the long run.
The local sentiment detector assigns ?costs? negative due to the single sentiment clue, ?skyrocketing?.
Since the agent and theme are in a bf triple, and the writer is negative toward that theme, we can infer
the writer is positive toward the agent. This illustrates how we improve recall on sentiments.
Ex(7) The supposedly costly reform will curb skyrocketing costs in the long run.
In Ex(7), agent ?reform? is labeled negative because ?costly? is a negative clue in the lexicon. (?sup-
posedly? is not in it.) However, in Ex(7), it is actually positive. The agent?s negative score is 0.6, and
its positive score is 0.5 due to the absence of a positive clue. Since the theme is negative too, by the bf
constraints, we expect to see a positive agent. If we were to assign negative to the agent, the objective
function would have -0.6 subjectivity score and +1 in violation penalty, together giving +0.4. If we as-
sign positive, the subjectivity score is -0.5, and there is no violation, resulting in a total score of -0.5.
Thus, the framework correctly chooses the positive label. This shows how we can improve precision on
sentiments.
Ex(8) The great reform will curb skyrocketing costs in the long run.
In this case, the agent is positive and the theme is negative. If the gfbf word ?curb? is not in the lexicon,
we could still infer its polarity. Given that the entities in the triple have different sentiments, to not violate
86
the implicature rules, the framework will assign it bf, or assign it gf along with reversed. However, there
is no reverser word in the sentence, so the reversed score p
r
= ?
5
4
. The framework will assign the
reverser indicator u
r
= 0, in order to avoid a gain in the objective function by ?1 ? p
r
? u
r
. Thus
the framework assigns the label bf to ?curb?. This is how the framework can improve the accuracy of
recognizing gfbf polarity.
7 Conclusion
The ultimate goal of this work is to utilize gfbf information to improve detection of the writer?s
sentiments toward entities mentioned in the text. Using an unsupervised optimization framework that
incorporates gfbf implicature rules as constraints, our method improves over local sentiment recognition
by almost 20 points in F-measure and over all sentiment baselines by over 10 points in F-measure. The
global optimization framework jointly infers the polarity of gfbf events, whether or not they are reversed,
which candidate NPs are the agent and theme, and the writer?s sentiments toward them. In addition
to beating the baselines for sentiment detection, the framework significantly improves the accuracy of
gfbf polarity disambiguation. This work not only automatically utilizes gfbf information to improve
sentiment detection, it also proposes a framework for jointly solving various ambiguities related to gfbf
events.
Acknowledgement This work was supported in part by DARPA-BAA-12-47 DEFT grant. We would
like to thank the anonymous reviewers for their helpful feedback.
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009. Subjectivity word sense disambiguation. In Proceedings
of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP
?09, pages 190?199, Stroudsburg, PA, USA. Association for Computational Linguistics.
Pranav Anand and Kevin Reschke. 2010. Verb classes as evaluativity functor classes. In Interdisciplinary Work-
shop on Verbs. The Identification and Representation of Verb Features.
Yejin Choi and Claire Cardie. 2008. Learning with compositional semantics as structural inference for subsen-
tential sentiment analysis. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language
Processing, pages 793?801, Honolulu, Hawaii, October. Association for Computational Linguistics.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint extraction of entities and relations for opinion recognition.
In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ?06,
pages 431?439, Stroudsburg, PA, USA. Association for Computational Linguistics.
Yoonjung Choi, Janyce Wiebe, and Lingjia Deng. 2014. Lexical acquisition for opinion inference: A sense-level
lexicon of benefactive and malefactive events. In 5th Workshop on Computational Approaches to Subjectivity,
Sentiment & Social Media Analysis.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493?2537, November.
Dipanjan Das, Andr?e FT Martins, and Noah A Smith. 2012. An exact dual decomposition algorithm for shallow
semantic parsing with constraints. In Proceedings of the First Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Evaluation, pages 209?217. Association for Computational
Linguistics.
Lingjia Deng and Janyce Wiebe. 2014. Sentiment propagation via implicature constraints. In Meeting of the
European Chapter of the Association for Computational Linguistics (EACL-2014).
Lingjia Deng, Yoonjung Choi, and Janyce Wiebe. 2013. Benefactive/malefactive event and writer attitude anno-
tation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2:
Short Papers), pages 120?125, Sofia, Bulgaria, August. Association for Computational Linguistics.
87
Pascal Denis and Jason Baldridge. 2007. Joint determination of anaphoricity and coreference resolution using
integer programming. In Human Language Technologies 2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 236?243,
Rochester, New York, April. Association for Computational Linguistics.
Thomas G. Dietterich. 1998. Approximate statistical tests for comparing supervised classification learning algo-
rithms. Neural Computation, 10:1895?1923.
Song Feng, Jun Sak Kang, Polina Kuznetsova, and Yejin Choi. 2013. Connotation lexicon: A dash of sentiment
beneath the surface meaning. In Proceedings of the 51th Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), Sofia, Bulgaria, Angust. Association for Computational Linguistics.
Amit Goyal, Ellen Riloff, and Hal Daum III. 2013. A computational model for plot units. Computational
Intelligence, 29(3):466?488.
Herbert Paul Grice. 1967. Logic and conversation. The William James lectures.
Herbert Paul Grice. 1989. Studies in the Way of Words. Harvard University Press.
Richard Johansson and Alessandro Moschitti. 2013. Relational features in fine-grained opinion analysis. Compu-
tational Linguistics, 39(3).
Andr?e F. T. Martins and Noah a. Smith. 2009. Summarization with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing
- ILP ?09, pages 1?9, Morristown, NJ, USA. Association for Computational Linguistics.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zimak. 2004. Semantic role labeling via integer linear
programming inference. In Proceedings of the 20th international conference on Computational Linguistics,
page 1346. Association for Computational Linguistics.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008. The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics, 34(2):257?287.
Kevin Reschke and Pranav Anand. 2011. Extracting contextual evaluativity. In Proceedings of the Ninth Interna-
tional Conference on Computational Semantics, IWCS ?11, pages 370?374, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Dan Roth and Wen-tau Yih. 2004. A linear programming formulation for global inference in natural language
tasks. In CONLL.
Swapna Somasundaran and Janyce Wiebe. 2009. Recognizing stances in online debates. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 226?234, Suntec, Singapore, August. Association for Computational
Linguistics.
P.J. Stone, D.C. Dunphy, M.S. Smith, and D.M. Ogilvie. 1966. The General Inquirer: A Computer Approach to
Content Analysis. MIT Press, Cambridge.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen Riloff, David Buttler, and David Hysom. 2010. Corefer-
ence resolution with reconcile. In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ?10, pages
156?161, Stroudsburg, PA, USA. Association for Computational Linguistics.
Janyce Wiebe and Lingjia Deng. 2014. An account of opinion implicatures. arXiv:1404.6491v1 [cs.CL].
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in
language ann. Language Resources and Evaluation, 39(2/3):164?210.
Theresa Wilson, Janyce Wiebe, , and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In HLP/EMNLP, pages 347?354.
Theresa Wilson. 2008. Fine-grained subjectivity analysis. Ph.D. thesis, Doctoral Dissertation, University of
Pittsburgh.
Bishan Yang and Claire Cardie. 2013. Joint Inference for Fine-grained Opinion Extraction. In Proceedings of
ACL, pages 1640?1649.
Lei Zhang and Bing Liu. 2011. Identifying noun product features that imply opinions. In Proceedings of the
49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages
575?580, Portland, Oregon, USA, June. Association for Computational Linguistics.
88
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1181?1191,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
+/-EffectWordNet: Sense-level Lexicon Acquisition for Opinion Inference
Yoonjung Choi and Janyce Wiebe
Department of Computer Science
University of Pittsburgh
yjchoi, wiebe@cs.pitt.edu
Abstract
Recently, work in NLP was initiated on a
type of opinion inference that arises when
opinions are expressed toward events
which have positive or negative effects
on entities (+/-effect events). This paper
addresses methods for creating a lexicon
of such events, to support such work on
opinion inference. Due to significant
sense ambiguity, our goal is to develop a
sense-level rather than word-level lexicon.
To maximize the effectiveness of different
types of information, we combine a
graph-based method using WordNet
1
relations and a standard classifier using
gloss information. A hybrid between the
two gives the best results. Further, we
provide evidence that the model is an
effective way to guide manual annotation
to find +/-effect senses that are not in the
seed set.
1 Introduction
Opinion mining (or sentiment analysis) identifies
positive or negative opinions in many kinds of
texts such as reviews, blogs, and news articles. It
has been exploited in many application areas such
as review mining, election analysis, and infor-
mation extraction. While most previous research
focusses on explicit opinion expressions, recent
work addresses a type of opinion inference that
arises when opinions are expressed toward events
which have positive or negative effects on enti-
ties (Deng et al., 2013; Deng and Wiebe, 2014).
We call such events +/-effect events.
2
Deng and
Wiebe (2014) show how sentiments toward one
1
WordNet 3.0, http://wordnet.princeton.edu/
2
While the term goodFor/badFor is used in previous pa-
pers (Deng et al., 2013; Deng and Wiebe, 2014; Deng et al.,
2014), we have since decided that +/-effect is a better term.
entity may be propagated to other entities via
opinion inference rules. They give the following
example:
(1) The bill would curb skyrocketing
health care costs.
The writer expresses an explicit negative senti-
ment (by skyrocketing) toward the object (health
care costs). The event, curb, has a negative effect
on costs, since they are reduced. We can reason
that the writer is positive toward the event because
it has a negative effect on costs, toward which the
writer is negative. From there, we can reason that
the writer is positive toward the bill, since it is
the agent of the positive event. Deng and Wiebe
(2014) show that such inferences may be exploited
to significantly improve explicit sentiment analy-
sis systems.
However, to achieve its results, the system de-
veloped by Deng and Wiebe (2014) requires that
all instances of +/-effect events in the corpus be
manually provided as input. For the system to
be fully automatic, it needs to be able to recog-
nize +/-effect events automatically. This paper
addresses methods for creating lexicons of such
events, to support such work on opinion inference.
We have discovered that there is significant sense
ambiguity, meaning that words often have mix-
tures of senses among the classes +effect, -effect,
and Null. Thus, we develop a sense-level rather
than word-level lexicon.
One of our goals is to investigate whether
the +/-effect property tends to be shared among
semantically-related senses, and another is to
use a method that applies to all word senses, not
just to the senses of words in a given word-level
lexicon. Thus, we build a graph-based model in
which each node is a WordNet sense, and edges
represent semantic WordNet relations between
senses. In addition, we hypothesized that glosses
also contain useful information. Thus, we develop
1181
a supervised gloss classifier and define a hybrid
model which gives the best overall performance.
Finally, because all WordNet verb senses are
incorporated into the model, we investigate the
ability of the method to identify unlabeled senses
that are likely to be +/-effect senses. We find that
by iteratively labeling the top-weighted unlabeled
senses and rerunning the model, it may be used as
an effective method for guiding annotation efforts.
2 Background
There are many varieties of +/-effect events, in-
cluding creation/destruction (changes in states in-
volving existence), gain/loss (changes in states
involving possession), and benefit/injury (Anand
and Reschke, 2010; Deng et al., 2013). The cre-
ation, gain, and benefit classes are +effect events.
For example, baking a cake has a positive effect on
the cake because it is created;
3
increasing the tax
rate has a positive effect on the tax rate; and com-
forting the child has a positive effect on the child.
The antonymous classes of each are -effect events:
destroying the building has a negative effect on the
building; demand decreasing has a negative effect
on demand; and killing Bill has a negative effect
on Bill.
4
While sentiment (Esuli and Sebastiani, 2006;
Wilson et al., 2005; Su and Markert, 2009) and
connotation lexicons (Feng et al., 2011; Kang et
al., 2014) are related, sentiment, connotation, and
+/-effects are not the same; a single event may
have different sentiment and +/-effect polarities,
for example. Consider the following example:
perpetrate:
S: (v) perpetrate, commit, pull (perform
an act, usually with a negative connota-
tion) ?perpetrate a crime?; ?pull a bank
robbery?
This sense of perpetuate has a negative
connotation, and is an objective term in
SentiWordNet. However, it has a positive
effect on the object, a crime, since performing a
crime brings it into existence.
3
Deng et al. (2013) point out that +/-effect objects are not
equivalent to benefactive/malefactive semantic roles. An ex-
ample they give is She baked a cake for me: a cake is the ob-
ject of the +effect event baked as just noted, while me is the
filler of its benefactive semantic role (Ziga and Kittil, 2010).
4
Their annotation manual, which gives additional cases, is
available with the annotated data at http://mpqa.cs.pitt.edu/.
As we mentioned, the +/-effect ambiguity can-
not be avoided in a word-level lexicon. In the
+/-effect corpus of Deng et al. (2013),
5
+/-effect
events and their agents and objects are annotated
at the word level. In that corpus, 1,411 +/-effect in-
stances are annotated; 196 different +effect words
and 286 different -effect words appear in these
instances. Among them, 10 words appear in
both +effect and -effect instances, accounting for
9.07% of all annotated instances. They show that
+/-effect events (and the inferences that motivate
this work) appear frequently in sentences with ex-
plicit sentiment. Further, all instances of +/-effect
words that are not identified as +/-effect events are
false hits from the perspective of a recognition sys-
tem.
The following is an example of a word with
senses of different classes:
purge:
S: (v) purge (oust politically) ?Deng
Xiao Ping was purged several times
throughout his lifetime? -effect
S: (v) purge (clear of a charge) +effect
S: (v) purify, purge, sanctify (make pure
or free from sin or guilt) ?he left the
monastery purified? +effect
S: (v) purge (rid of impurities) ?purge
the water?; ?purge your mind? +effect
This is part of the WordNet output for the word
purge. In the first sense, the polarity is -effect
since it has a negative effect on the object, Deng
Xizo Ping. However, the other cases have positive
effect on the object. Moreover, although a word
may not have both +effect and -effect senses, it
may have mixtures of ((+effect or -effect) and
Null). A purely word-based approach is blind to
these cases.
3 Related Work
Lexicons are widely used in sentiment analysis
and opinion mining. Several works such as Hatzi-
vassiloglou and McKeown (1997), Turney and
Littman (2003), Kim and Hovy (2004), Strappar-
ava and Valitutti (2004), and Peng and Park (2011)
have tackled automatic lexicon expansion or ac-
quistion. However, in most such work, the lexi-
cons are word-level rather than sense-level.
5
Called the goodFor/badFor corpus in that paper.
1182
For the related (but different) tasks of de-
veloping subjectivity, sentiment and connota-
tion lexicons, some do take a sense-level ap-
proach. Esuli and Sebastiani (2006) construct
SentiWordNet. They assume that terms with
the same polarity tend to have similar glosses. So,
they first expand a manually selected seed set of
senses using WordNet lexical relations such as
also-see and direct antonymy and train two clas-
sifiers, one for positive and another for negative.
As features, a vectorial representation of glosses
is adopted. These classifiers were applied to all
WordNet senses to measure positive, negative, and
objective scores. In extending their work (Esuli
and Sebastiani, 2007), the PageRank algorithm is
applied to rank senses in terms of how strongly
they are positive or negative. In the graph, each
sense is one node, and two nodes are connected
when they contain the same words in their Word-
Net glosses. Moreover, a random-walk step is
adopted to refine the scores in their recent work
(Baccianella et al., 2010). In contrast, our ap-
proach uses WordNet relations and graph propa-
gation in addition to gloss classification.
Gyamfi et al. (2009) construct a classifier to la-
bel the subjectivity of word senses. The hierarchi-
cal structure and domain information in WordNet
are exploited to define features in terms of sim-
ilarity (using the LCS metric in Resnik (1995))
of target senses and a seed set of senses. Also,
the similarity of glosses in WordNet is consid-
ered. Even though they investigated the hierarchi-
cal structure by LCS values, WordNet relations are
not exploited directly.
Su and Markert (2009) adopt a semi-supervised
mincut method to recognize the subjectivity of
word senses. To construct a graph, each node cor-
responds to one WordNet sense and is connected
to two classification nodes (one for subjectivity
and another for objectivity) via a weighted edge
that is assigned by a classifier. For this classifier,
WordNet glosses, relations, and monosemous
features are considered. Also, several WordNet
relations (e.g., antonymy, similiar-to, direct
hypernym, etc.) are used to connect two nodes.
Although they make use of both WordNet glosses
and relations, and gloss information is utilized
for a classifier, this classifier is generated only
for weighting edges between sense nodes and
classification nodes, not for classifying all senses.
Kang et al. (2014) present a unified model that
assigns connotation polarities to both words and
senses. They formulate the induction process as
collective inference over pairwise-Markov Ran-
dom Fields, and apply loopy belief propagation
for inference. Their approach relies on selectional
preferences of connotative predicates; the polarity
of a connotation predicate suggests the polarity of
its arguments. We have not discovered an analo-
gous type of predicate for the problem we address.
Goyal et al. (2010) generate a lexicon of patient
polarity verbs (PPVs) that impart positive or neg-
ative states on their patients. They harvest PPVs
from a Web corpus by co-occurance with Kind and
Evil agents and by bootstrapping over conjunc-
tions of verbs. Riloff et al. (2013) learn positive
sentiment phrases and negative situation phrases
from a corpus of tweets with hashtag ?sarcasm?.
However, both of these methods are word-level
rather than sense-level.
Ours is the first NLP research into developing
a sense-level lexicon for events that have negative
or positive effects on entities.
4 +/-Effect Word-Level Seed Lexicon
and Sense Annotations
To create the corpus used in this work, we devel-
oped a word-level seed lexicon, and then manually
annotated all the senses of the words in that lexi-
con.
FrameNet
6
is based on a theory of meaning
called Frame Semantics. In FrameNet, a Lexical
Unit (LU) is a pairing of a word with a meaning,
i.e., it corresponds to a sense in WordNet. Each
LU of a polysemous word belongs to a different
semantic frame, which is a description of a type
of event, relation, or entity and, where appropri-
ate, its participants. For instance, in the Creating
frame, the definition is that a Cause leads to the
formation of a Created entity. It has a positive
effect on the object, Created entity. This frame
contains about 10 LUs such as assemble, create,
yield, and so on. FrameNet consists of about 1,000
semantic frames and about 10,000 LUs.
FrameNet is a useful resource to select +/-effect
words since each semantic frame covers multi-
ple LUs. We believe that using FrameNet to
find +/-effect words is easier than finding +/-effect
words without any information since words may
6
FrameNet, https://framenet.icsi.berkeley.edu/fndrupal/
1183
be filtered by semantic frames. To select +/-effect
words, an annotator (who is not a co-author) first
identified promising frames as +/-effect and ex-
tracted all LUs from them. Then, he went through
them and picked out the LUs which he judged to
be +effect or -effect. In total, 736 +effect LUs and
601 -effect LUs were selected from 463 semantic
frames.
While Deng et al. (2013) and Deng and Wiebe
(2014) specifically focus on events affecting ob-
jects (i.e., themes), we do not want to limit the
lexicon to only that case. Sometimes, events have
positive or negative effects on agents or other en-
tities as well. Thus, in this paper, we consider
a sense to be +effect (-effect) if it has +effect
(-effect) on an entity, which may be the agent, the
theme, or some other entity.
In a previous paper (Choi et al., 2014), we con-
ducted a study of the sense-level +/-effect prop-
erty. For the evaluation, two annotators (who
are co-authors of that paper) independently anno-
tated senses of selected words, where some are
from pure +effect (-effect) words (i.e., all senses
of the words are classified into the same class)
and some are from mixed words (i.e., the words
have both +effect and -effect senses). In the agree-
ment study, we calculated percent agreement and
? (Artstein and Poesio, 2008), and achieved 0.84
percent agreement and 0.75 ? value.
For a seed set and an evaluation set in this pa-
per, we need annotated sense-level +/-effect data.
Mappings between FrameNet and WordNet are
not perfect. Thus, we opted to manually anno-
tate the senses of the words in the word-level lexi-
con. We first extracted all words from 736 +effect
LUs and 601 -effect LUs; this extracts 606 +effect
words and 537 -effect words (the number of words
is smaller than the number of LUs because one
word can have more than one LU). Among them,
14 words (e.g., crush, order, etc.) are in both the
+effect word set and the -effect word set. That is,
these words have both +effect and -effect mean-
ings. Recall that this annotator was focusing on
frames, not on words - he did not look at all the
senses of all the words. As we will see just below,
when all the senses of all the words are annotated,
a much higher percentage of the words have both
+effect and -effect senses. We will also see that
many of the senses are revealed to be Null, show-
ing that +effect vs. Null and -effect vs. Null ambi-
guities are quite prevalent.
A different annotator (a co-author) then went
through all senses of all the words from the pre-
vious step and manually annotated each sense as
to whether it is +effect, -effect, or Null. Note that
this annotator participated in an agreement study
with positive results in Choi et al. (2014).
For the experiments in this paper, we divided
this annotated data into two equal-sized sets. One
is a fixed test set that is used to evaluate both the
graph model and the gloss classifier. The other set
is used as a seed set by the graph model, and as a
training set by the gloss classifer. Table 1 shows
the distribution of the data. In total, there are 258
+effect senses, 487 -effect senses, and 880 Null
senses. To avoid too large a bias toward the Null
class,
7
we randomly chose half (i.e., the Null set
contains 440 senses). Half of each set is used as
seed and training data, and the other half is used
for evaluation.
+effect -effect Null
# annotated data 258 487 880
# Seed/TrainSet 129 243 220
# TestSet 129 244 220
Table 1: Distribution of annotated data.
5 Graph-based Semi-Supervised
Learning for WordNet Relations
WordNet (Miller et al., 1990) is organized by se-
mantic relations such as hypernymy, troponymy,
grouping, and so on. These semantic relations can
be used to build a network. Since the most fre-
quently encoded relation is the super-subordinate
relation, most verb senses are arranged into hi-
erarchies; verb senses towards the bottom of the
graph express increasingly specific manner. Thus,
by following this hierarchical information, we hy-
pothesized that +/-effect polarity tends to propa-
gate. We use a graph-based semi-supervised learn-
ing (GSSL) method to carry out the label propaga-
tion.
5.1 Graph Formulation
We formulate a graph for semi-supervised learning
as follows. Let G = {X,E,W} be the undirected
graph in which X is the set of nodes, E is the set
7
As mentioned in the introduction, we want our method
to be able to identify unlabeled senses that are likely to be
+/-effect senses (see Section 8); we resize the Null class to
support this goal.
1184
of edges, and W represents the edge weights (i.e.,
the weight of edge E
ij
is W
ij
). The weight matrix
is a non-negative matrix.
Each data point in X = {x
1
, ... ,x
n
} is one
sense. The labeled data of X is represented as
X
L
= {x
1
, ... ,x
l
} and the unlabeled data is rep-
resented as X
U
= {x
l+1
, ... ,x
n
}). The labeled
data X
L
is associated with labels Y
L
= {y
1
, ...
,y
l
}, where y
i
? {1, ..., c} (c is the number of
classes). As is typical in such settings, l  n:
n is 13,767, i.e., the number of verb senses in
WordNet. Seed/TrainSet in Table 1 is the labeled
data.
To connect two nodes, WordNet relations are
utilized. We first connect nodes by the hierar-
chical relations. Since hypernym relations repre-
sent more general senses and troponym relations
represent more specific verb senses, we hypothe-
sized that hypernyms or troponyms of a verb sense
tends to have its same polarity. Verb groups rela-
tions that represent verb senses having a similar
meaning are also promising. Even though verb-
group coverage is not large, its relations are reli-
able since they are manually grouped. The entail-
ment relation is defined as the verb Y is entailed
by X if you must be doing Y by doing X . Since
pairs connected by this relation are co-extensive,
we can assume that both are the same type of
event. The synonym relation is not used because
it is already defined in senses (i.e., each node in
the graph is a synset), and the antonym relation is
also not applied since the weight matrix should be
non-negative. The weight value of all edges is 1.0.
5.2 Label Propagation
Given a constructed graph, the label inference (or
prediction) task is to propagate the seed labels to
the unlabeled nodes. One of the classic GSSL la-
bel propagation methods is the local and global
consistency (LGC) method suggested by Zhou et
al. (2004). The LGC method is a graph transduc-
tion algorithm which is sufficiently smooth with
respect to the intrinsic structure revealed by known
labeled and unlabeled data. The cost function typ-
ically involves a tradeoff between the smoothness
of the predicted labels over the entire graph and
the accuracy of the predicted labels in fitting the
given labeled nodes X
L
. LGC fits in a univariate
regularization framework, where the output ma-
trix is treated as the only variable in optimization,
and the optimal solutions can be easily obtained by
solving a linear system. Thus, we adopt the LGC
method in this paper. Although there are some ro-
bust GSSL methods for handling noisy labels, we
do not need to handle noisy labels because our in-
put is the annotated data.
Let F be a n ? c matrix to save the output
values of label propagation. So, we can label
each instance x
i
as a label y
i
= argmax
j?c
F
ij
after the label propagation. The initial discrete la-
bel matrix Y , which is also n ? c, is defined as
Y
ij
= 1 if x
i
is labeled as y
i
= j in Y
L
, and
Y
ij
= 0 otherwise. The vertex degree matrix
D = diag([D
11
, ..., Dnn]) is defined by D
ii
=
?
n
j=1
W
ij
.
LGC defines the cost function Q which inte-
grates two penalty components, global smooth-
ness and local fitting (? is the regularization pa-
rameter):
Q =
1
2
n
?
i=1
n
?
j=1
W
ij
?
F
i
?
D
ii
?
F
j
?
D
jj
?
2
+?
n
?
i=1
?F
i
? Y
i
?
2
The first part of the cost function is the
smoothness constraint: a good classifying func-
tion should not change too much between nearby
points. That is, if x
i
and x
j
are connected with
an edge, the difference between them should be
small. The second is the fitting constraint: a good
classifying function should not change too much
from the initial label assignment. The final label
prediction matrix F can be obtained by minimiz-
ing the cost function Q.
5.3 Experimental Results
Note that, in the rest of this paper, all tables except
the last one give results on the same fixed test set
(TestSet in Table 1).
We can apply the graph model in two ways.
? UniGraph: All three classes (+effect, -effect,
and Null) are represented in one graph.
? BiGraph: Two separate graphs are first con-
structed and then combined. One graph is for
classifying +effect and Other (i.e., -effect or
Null). This graph is called +eGraph. The
other graph, called -eGraph, is for classify-
ing -effect and Other (i.e., +effect or Null).
1185
UniGraph BiGraph BiGraph*
baseline-
0.411
accuracy
accuracy 0.630 0.623 0.658
+effect P 0.621 0.610 0.642
R 0.655 0.647 0.680
F 0.637 0.628 0.660
-effect P 0.644 0.662 0.779
R 0.720 0.677 0.612
F 0.680 0.670 0.686
Null P 0.615 0.583 0.583
R 0.516 0.550 0.695
F 0.561 0.561 0.634
Table 2: Results of UniGraph, BiGraph, and Bi-
Graph*.
These are combined into one model as fol-
lows. Nodes that are labeled as +effect by
+eGraph and Other by -eGraph are regarded
as +effect, and nodes that are labeled as
-effect by -eGraph and Other by +eGraph are
regarded as -effect. If nodes are labeled as
+effect by +eGraph and -effect by -eGraph,
they are deemed to be Null. Nodes that are
labeled Other by both graphs are also consid-
ered as Null.
We had two motivations for experimenting
with the BiGraph model: (1) SVM, the super-
vised learning method used for gloss classifica-
tion, tends to have better performance on binary
classification tasks, and (2) the two graphs of the
combined model can ?negotiate? with each other
via constraints.
In Table 2, we calculate precision (P), recall (R),
and f-measure (F) for all three classes. The base-
line shown in the top row is the accuracy of a ma-
jority class classifier. The first two columns of Ta-
ble 2 show the results of UniGraph and BiGraph
when they are built using the hypernym, troponym,
and verb group relations. UniGraph outperforms
BiGraph in this experiment.
To improve the results by performing some-
thing possible with BiGraph (but not UniGraph),
constraints are added when determining the class.
As we explained, the label of instance x
i
is
determined by F
i
in the graph. When the label
of x
i
is decided to be j, we can say that its con-
fidence value is F
ij
. There are two constraints as
follows.
H+T +V +E
+effect P 0.653 0.642 0.651
R 0.660 0.680 0.683
F 0.656 0.660 0.667
-effect P 0.784 0.779 0.786
R 0.547 0.612 0.604
F 0.644 0.686 0.683
Null P 0.557 0.583 0.564
R 0.735 0.695 0.691
F 0.634 0.634 0.621
Table 3: Effect of each relation
? If a sense is labeled as +effect (-effect), but
the confidence value is less than a threshold,
we count it as Null.
? If a sense is labeled as both +effect and -effect
by BiGraph, we choose the label with the
higher confidence value only if the higher one
is larger than a threshold and the lower one is
less than a threshold.
The thresholds are determined on Seed/TrainSet
by running BiGraph several times with different
thresholds, and choosing the one that gives the
best performance on Seed/TrainSet. (The chosen
value is 0.025 for +effect and 0.03 for -effect).
As can be seen in Table 2, BiGraph with con-
straints (called BiGraph*) outperforms not only
BiGraph without any constraints but also Uni-
Graph. Especially, for BiGraph*, the recall of the
Null class is considerably increased, showing that
constraints not only help overall, but are particu-
larly important for detecting Null cases.
Table 3 gives ablation results, showing the con-
tribution of each WordNet relation in BiGraph*.
With only hierarchical information (i.e., hyper-
nym (H) and troponym (T) relations), it already
shows good performance for all classes. How-
ever, they cannot cover some senses. Among the
13,767 verb senses in WordNet, 1,707 (12.4%)
cannot be labeled because there are not sufficient
hierarchical links to propagate polarity informa-
tion. When adding the verb group (+V) rela-
tion, it shows improvement in both +effect and
-effect. Especially, the recall for +effect and
-effect is significantly increased. In addition, the
coverage of the 13,767 verb senses increases to
95.1%. For entailment (+E), whereas adding it
shows a slight improvement in +effect (and in-
creases coverage by 1.1 percentage points), the
1186
performance is decreased a little bit in the -effect
and Null classes. Since the average f-measure for
all classes is the highest with hypernym (H), tro-
ponym (T), and verb group (V) relations (not en-
tailment), we only consider these three relations
when constructing the graph.
6 Supervised Learning applied to
WordNet Glosses
In WordNet, each sense contains a gloss consist-
ing of a definition and optional example sentences.
Since a gloss consists of several words and there
are no direct links between glosses, we believe that
a word vector representation is appropriate to uti-
lize gloss information as in Esuli and Sebastiani
(2006). For that, we adopt an SVM classifier.
6.1 Features
Two different feature types are used.
Word Features (WF): The bag-of-words
model is applied. We do not ignore stop words
for several reasons. Since most definitions and ex-
amples are not long, each gloss contains a small
number of words. Also, among them, the total vo-
cabulary of WordNet glosses is not large. More-
over, some prepositions such as against are some-
times useful to determine the polarity (+effect or
-effect).
Sentiment Features (SF): Some glosses of
+effect (-effect) senses contain positive (negative)
words. For instance, the definition of {hurt#4,
injure#4} is ?cause damage or affect negatively.?
It contains a negative word, negatively. Since a
given event may positively (negatively) affect enti-
ties, some definitions or examples already contain
positive (negative) words to express this. Thus, as
features, we check how many positive (negative)
words a given gloss contains. To detect sentiment
words, the subjectivity lexicon provided by Wil-
son et al. (2005)
8
is utilized.
6.2 Gloss Classifier
We have three classes, +effect, -effect, and Null.
Since SVM shows better performance on binary
classification tasks, we generate two binary clas-
sifiers, one (+eClassifier) to determine whether
a given sense is +effect or Other, and another
(-eClassifier) to classify whether a given sense is
-effect or Other. Then, they are combined as in
BiGraph.
8
Available at http://mpqa.cs.pitt.edu/
6.3 Experimental Results
Seed/TrainSet in Table 1 is used to train the two
classifiers, and TestSet is utilized for the evalua-
tion. So, the training set for +eClassifier consists
of 129 +effect instances and 463 Other instances,
and the training set for -eClassifier contains 243
-effect instances and 349 Other instances. As a
baseline, we adopt a majority class classifier.
Table 4 shows the results on TestSet. Perfor-
mance is better for the -effect than for the +effect
class, perhaps because the -effect class has more
instances.
When sentiment features (SF) are added,
all metric values increase, providing evidence
that sentiment features are helpful to determine
+/-effect classes.
WF WF+SF
baseline accuracy 0.411
accuracy 0.509 0.539
+effect P 0.541 0.588
R 0.354 0.393
F 0.428 0.472
-effect P 0.616 0.672
R 0.500 0.511
F 0.552 0.580
Null P 0.432 0.451
R 0.612 0.657
F 0.507 0.535
Table 4: Results of the gloss classifier.
7 Hybrid Method
To use more combined knowledge, the gloss clas-
sifier and BiGraph* can be combined. That is, for
WordNet gloss information, the gloss classifier is
utilized, and for WordNet relations, BiGraph* is
used. With the Hybrid method, we can see not
only the effect of propagation by WordNet rela-
tions but also the usefulness of gloss information
and sentiment features. Also, while BiGraph*
cannot cover all senses in WordNet, the Hybrid
method can.
The outputs of the gloss classifier and Bi-
Graph* are combined as follows. The label of
the gloss classifier is one of +effect, -effect, Null,
or Both (when a given sense is classified as both
+effect by +eClassifier and -effect by -eClassifier).
Possible labels of BiGraph* are +effect, -effect,
Null, Both, or None (when a given sense is not
1187
labeled by BiGraph*). There are five rules:
? If both labels are +effect (-effect), it is +effect
(-effect).
? If one of them is Both and the other is +effect
(-effect), it is +effect (-effect).
? If the label of BiGraph* is None, believe the
label of the gloss classifier
? If both labels are Both, it is Null
? Otherwise, it is Null
The results for Hybrid are given in the first
row of the lower half of Table 5; the results for
BiGraph* are in the first row of the upper half,
for comparison. Generally, the Hybrid method
shows better performance than the gloss classifier
and BiGraph*. In the Hybrid method, since more
+/-effect senses are detected than by BiGraph*,
while precision is decreased, recall is increased
by more. However, by the same token, the over-
all performance for the Null class is decreased.
Actually, that is expected since the Null class is
determined by the Other class in the gloss clas-
sifier and BiGraph*. Through this experiment, we
see that the Hybrid method is better for classifying
+/-effect senses.
7.1 Model Comparison
To provide evidence for our assumption that dif-
ferent models are needed for different information
to maximize effectiveness, we compare the hy-
brid method with the supervised learning and the
graph-based learning (GSSL) methods, each uti-
lizing both WordNet relations and gloss informa-
tion.
Supervised Learning (onlySL): The gloss clas-
sifier is trained with word features and sentiment
features for WordNet Gloss. To exploit Word-
Net relations in supervised learning, especially
the hierarchical information, we use least com-
mon subsumer (LCS) values as in Gyamfi et al.
(2009), which, recall, performs supervised learn-
ing of subjective/objective senses. The values are
calculated as follows. For a target sense t and a
seed set S, the maximum LCS value between a
target sense and a member of the seed set is found
as:
Score(t, S) = max
s?S
LCS(t, s)
With this LCS feature and the features described
in Section 6, we run SVM on the same training and
test data. For LCS values, the similarity using the
information content proposed by Resnik (1995) is
measured. WordNet Similarity
9
package provides
pre-computed pairwise similarity values for that.
Table 6 shows results of onlySL. Compared to
Table 4, while +effect and Null classes show a
slight improvement, the performance is degraded
for -effect. This means that the added feature is
rather harmful to -effect. Even though the hierar-
chical feature is very helpful to expand +/-effect,
it is not helpful for onlySL since SVM cannot cap-
ture propagation according to the hierarchy.
Graph-based Learning (onlyGraph): In Sec-
tion 5, the graph is constructed by using Word-
Net relations. To apply WordNet gloss informa-
tion in onlyGraph, we calculate a cosine similarity
between glosses. If the similarity value is higher
than a threshold, two nodes are connected with this
similarity value. The threshold is determined by
training and testing on Seed/TrainSet (the chosen
value is 0.3).
Comparing Tables 2 and 6, BiGraph* generally
outperforms onlyGraph (the exception is precision
of +effect). By gloss similarity, many nodes are
connected to each other. However, since uncertain
connections can cause incorrect propagation in the
graph, this negatively affects the performance.
Through this experiment, we see that since each
type of information has a different character, we
need different models to maximize the effective-
ness of each type. Thus, the hybrid method with
different models can have better performance.
Hybrid onlySL onlyGraph
+effect P 0.610 0.584 0.701
R 0.735 0.400 0.364
F 0.667 0.475 0.480
-effect P 0.717 0.778 0.651
R 0.669 0.316 0.562
F 0.692 0.449 0.603
Null P 0.556 0.440 0.473
R 0.520 0.813 0.679
F 0.538 0.571 0.557
Table 6: Comparison to onlySL and onlyGraph.
9
WordNet Similarity,
http://wn-similarity.sourceforge.net/
1188
+effect -effect Null
P R F P R F P R F
BiGraph* Initial 0.642 0.680 0.660 0.779 0.612 0.686 0.583 0.695 0.634
1st 0.636 0.684 0.663 0.770 0.632 0.694 0.591 0.672 0.629
2nd 0.642 0.701 0.670 0.748 0.656 0.699 0.605 0.655 0.629
3rd 0.636 0.708 0.670 0.779 0.652 0.710 0.599 0.669 0.632
4th 0.681 0.674 0.678 0.756 0.674 0.712 0.589 0.669 0.626
Hybrid Initial 0.610 0.735 0.667 0.717 0.669 0.692 0.556 0.520 0.538
1st 0.614 0.713 0.672 0.728 0.681 0.704 0.562 0.523 0.542
2nd 0.613 0.743 0.672 0.716 0.697 0.706 0.559 0.497 0.526
3rd 0.616 0.739 0.672 0.717 0.706 0.712 0.559 0.494 0.525
4th 0.688 0.681 0.684 0.712 0.764 0.732 0.565 0.527 0.545
Table 5: Results of an iterative approach.
8 Guided Annotation
Recall that Seed/TrainSet and TestSet, the data
used so far, are all the senses of the words in a
word-level +/-effect lexicon. This section presents
evidence that our method can guide annotation ef-
forts to find other words that have +/-effect senses.
A bonus is that the method pinpoints particular
+/-effect senses of those words.
All unlabeled data are senses of words that are
not included in the original lexicon. Since pre-
sumably the majority of verbs do not have any
+/-effect senses, a sense randomly selected from
WordNet is very likely to be Null. We explore an
iterative approach to guided annotation, using Bi-
Graph* and Hybrid as the method for assigning
labels.
The system is initially created as described
above using Seed/TrainSet as the initial seed set.
Each iteration has four steps: 1) rank all unlabeled
data (i.e., the data other than TestSet and the cur-
rent seed set) based on the F
ij
confidence values
(see Section 5.3); 2) choose the top 5% and manu-
ally annotate them (the same annotator as above
did this); 3) add them to the seed set; 4) rerun
the system using the expanded seed set. We per-
formed four iterations in this paper.
The upper and lower parts of Table 5 show the
intial results and the results after each iteration for
BiGraph* and Hybrid. Recall that these are results
on the fixed set, TestSet. Overall for both mod-
els, f-measure increases for both the +effect and
-effect classes as more seeds are added, mainly
due to improvements in recall. The evaluation on
the fixed set is also useful in the annotation process
because it trades off +/-effect vs. Null annotations.
If the new manual annotations were biased, in that
they incorrectly label Null senses as +/-effect, then
the f-measure results would instead degrade on the
fixed TestSet, since the system is created each time
using the increased seed set.
We now consider the accuracy of the system
on the newly labeled annotated data in Step 2.
Note that our method is similar to Active Learn-
ing (Tong and Koller, 2001), in that both auto-
matically identify which unlabeled instances the
human should annotate next. However, in active
learning, the goal is to find instances that are diffi-
cult for a supervised learning system. In our case,
the goal is to find needles in the haystack of Word-
Net senses. In Step 3, we add the newly labeled
senses to the seed set, enabling the model to find
unlabeled senses close to the new seeds when the
system is rerun for the next iteration.
We assess the system?s accuracy on the newly
labeled data by comparing the system?s labels with
the human?s new labels. Accuracy for +effect and
-effect is calculated such as:
Accuracy
+effect
=
# annotated +effect
# top 5% +effect data
Accuracy
?effect
=
# annotated -effect
# top 5% -effect data
That is, the accuracy means that out of the top 5%
of the +effect (-effect) data as scored by the sys-
tem, what percentage are correct as judged by a
human annotator. Table 7 shows the accuracy for
each iteration in the top part and the number of
senses labeled in the bottom part. As can be seen,
the accuracies range between 60% and 78%; these
1189
values are much higher than what would be ex-
pected if labeling senses of words randomly cho-
sen from WordNet.
10
The annotator spent, on av-
erage, approximately an hour to label 100 senses.
For finding new words with +/-effect usages, it
would be much more cost-effective if a significant
percentage of the data chosen for annotation are
senses of words that in fact have +/-effect senses.
1st 2nd 3rd 4th
+effect 65.63% 62.50% 63.79% 59.83%
-effect 73.55% 73.97% 77.78% 70.30%
+effect 128 122 116 117
-effect 155 146 153 145
total 283 268 269 262
Table 7: Accuracy and frequency of the top 5% for
each iteration
9 Conclusion and Future Work
In this paper, we investigated methods for creat-
ing a sense-level +/-effect lexicon. To maximize
the effectiveness of each type of information, we
combined a graph-based method using WordNet
relations and a standard classifier using gloss in-
formation. A hybrid between the two gives the
best results. Further, we provide evidence that the
model is an effective way to guide manual anno-
tation to find +/-effect words that are not in the
seed word-level lexicon. This is important, as the
likelihood that a random WordNet sense (and thus
word) is +effect or -effect is not large.
So as not to limit the inferences that may be
drawn, our annotations include events that are
+effect or -effect either the agent or object. In fu-
ture work, we plan to exploit corpus-based meth-
ods using patterns as in Goyal et al. (2010) com-
bined with semantic role labeling to refine the lex-
icon to distinguish which is the affected entity.
Further, to actually exploit the acquired lexicon to
process corpus data, an appropriate coarse-grained
sense disambiguation process must be added, as
Akkaya et al. (2009) and Akkaya et al. (2011) did
for subjective/objective classification.
We hope the general methodology will be ef-
fective for other semantic properties. In opin-
ion mining and sentiment analysis this is partic-
10
For reference, in 5th iteration, the +effect accuracy is
60.18% and the -effect accuracy is 69.93%, and in 6th itera-
tion, the +effect accuracy is 59.81% and the -effect accuracy
is 69.12%.
ularly needed, because different meanings of pos-
itive and negative are appropriate for different ap-
plications. This is a way to create lexicons that are
customized with respect to one?s own definitions.
It would be promising to combine our method
with other methods to enable it to find +effect
and -effect senses that are outside the coverage
of WordNet. However, a WordNet-based lexicon
gives a substantial base to build from.
Acknowledgments
This work was supported in part by DARPA-BAA-
12-47 DEFT grant #12475008 and National Sci-
ence Foundation grant #IIS-0916046. We would
like to thank the reviewers for their helpful sug-
gestions and comments.
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2009. Subjectivity word sense disambiguation. In
Proceedings of EMNLP 2009, pages 190?199.
Cem Akkaya, Janyce Wiebe, Alexander Conrad, and
Rada Mihalcea. 2011. Improving the impact of sub-
jectivity word sense disambiguation on contextual
opinion analysis. In Proceedings of CoNLL 2011,
pages 87?96.
Pranna Anand and Kevin Reschke. 2010. Verb classes
as evaluativity functor classes. In Interdisciplinary
Workshop on Verbs. The Identification and Repre-
sentation of Verb Features.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput.
Linguist., 34(4):555?596.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of LREC, pages 2200?2204.
Yoonjung Choi, Lingjia Deng, and Janyce Wiebe.
2014. Lexical acquisition for opinion inference:
A sense-level lexicon of benefactive and malefac-
tive events. In Proceedings of the 5th Workshop
on Computational Approaches to Subjectivity, Sen-
timent and Social Media Analysis (WASSA), pages
107?112. Association for Computational Linguis-
tics.
Lingjia Deng and Janyce Wiebe. 2014. Sentiment
propagation via implicature constraints. In Proceed-
ings of EACL.
Lingjia Deng, Yoonjung Choi, and Janyce Wiebe.
2013. Benefactive/malefactive event and writer atti-
tude annotation. In Proceedings of 51st ACL, pages
120?125.
1190
Lingjia Deng, Janyce Wiebe, and Yoonjung Choi.
2014. Joint inference and disambiguation of implicit
sentiments via implicature constraints. In Proceed-
ings of COLING, page 7988.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In Proceedings of 5th LREC, pages
417?422.
Andrea Esuli and Fabrizio Sebastiani. 2007. Pager-
anking wordnet synsets: An application to opinion
mining. In Proceedings of ACL, pages 424?431.
Song Feng, Ritwik Bose, and Yejin Choi. 2011. Learn-
ing general connotation of words using graph-based
algorithms. In Proceedings of EMNLP, pages 1092?
1103.
Amit Goyal, Ellen Riloff, and Hal DaumeIII. 2010.
Automatically producing plot unit representations
for narrative text. In Proceedings of EMNLP, pages
77?86.
Yaw Gyamfi, Janyce Wiebe, Rada Mihalcea, and Cem
Akkaya. 2009. Integrating knowledge for subjectiv-
ity sense labeling. In Proceedings of NAACL HLT
2009, pages 10?18.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of ACL, pages 174?181.
Jun Seok Kang, Song Feng, Leman Akoglu, and Yejin
Choi. 2014. Connotationwordnet: Learning conno-
tation over the word+sense network. In Proceedings
of the 52nd ACL, page 15441554.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of 20th
COLING, pages 1367?1373.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990.
Wordnet: An on-line lexical database. International
Journal of Lexicography, 13(4):235?312.
Wei Peng and Dae Hoon Park. 2011. Generate adjec-
tive sentiment dictionary for social media sentiment
analysis using constrained nonnegative matrix fac-
torization. In Proceedings of ICWSM.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity. In Proceedings of 14th
IJCAI, pages 448?453.
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-
dra De Silva, Nathan Gilbert, and Ruihong Huang.
2013. Sarcasm as contrast between a positive sen-
timent and negative situation. In Proceedings of
EMNLP, pages 704?714.
Carlo Strapparava and Alessandro Valitutti. 2004.
Wordnet-affect: An affective extension of wordnet.
In Proceedings of 4th LREC, pages 1083?1086.
Fangzhong Su and Katja Markert. 2009. Subjectiv-
ity recognition on word senses via semi-supervised
mincuts. In Proceedings of NAACL HLT 2009,
pages 1?9.
Simon Tong and Daphne Koller. 2001. Support vector
machin active learning with applications to text clas-
sification. Journal of Machine Learning Research,
2:45?66.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on Infor-
mation Systems, 21(4):315?346.
Theresa Wilson, Janyce Wiebe, , and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of HLT-
EMNLP, pages 347?354.
Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal,
Jason Weston, and Bernhard Scholkopf. 2004.
Learning with local and global consistency. Ad-
vances in Neural Information Processing Systems,
16:321?329.
Fernando Ziga and Seppo Kittil. 2010. Benefactives
and malefactives, Typological perspectives and case
studies. John Benjamins Publishing.
1191
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 120?125,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Benefactive/Malefactive Event and Writer Attitude Annotation
Lingjia Deng ?, Yoonjung Choi ?, Janyce Wiebe ??
? Intelligent System Program, University of Pittsburgh
? Department of Computer Science, University of Pittsburgh
?lid29@pitt.edu, ?{yjchoi,wiebe}@cs.pitt.edu
Abstract
This paper presents an annotation scheme
for events that negatively or positively
affect entities (benefactive/malefactive
events) and for the attitude of the writer
toward their agents and objects. Work on
opinion and sentiment tends to focus on
explicit expressions of opinions. However,
many attitudes are conveyed implicitly,
and benefactive/malefactive events are
important for inferring implicit attitudes.
We describe an annotation scheme and
give the results of an inter-annotator
agreement study. The annotated corpus is
available online.
1 Introduction
Work in NLP on opinion mining and sentiment
analysis tends to focus on explicit expressions of
opinions. Consider, however, the following sen-
tence from the MPQA corpus (Wiebe et al, 2005)
discussed by (Wilson and Wiebe, 2005):
(1) I think people are happy because
Chavez has fallen.
The explicit sentiment expression, happy, is pos-
itive. Yet (according to the writer), the people
are negative toward Chavez. As noted by (Wil-
son and Wiebe, 2005), the attitude toward Chavez
is inferred from the explicit sentiment toward the
event. An opinion-mining system that recognizes
only explicit sentiments would not be able to per-
ceive the negative attitude toward Chavez con-
veyed in (1). Such inferences must be addressed
for NLP systems to be able to recognize the full
range of opinions conveyed in language.
The inferences arise from interactions be-
tween sentiment expressions and events such as
fallen, which negatively affect entities (malefac-
tive events), and events such as help, which pos-
itively affect entities (benefactive events). While
some corpora have been annotated for explicit
opinion expressions (for example, (Kessler et
al., 2010; Wiebe et al, 2005)), there isn?t a
previously published corpus annotated for bene-
factive/malefactive events. While (Anand and
Reschke, 2010) conducted a related annotation
study, their data are artificially constructed sen-
tences incorporating event predicates from a fixed
list, and their annotations are of the writer?s
attitude toward the events. The scheme pre-
sented here is the first scheme for annotating, in
naturally-occurring text, benefactive/malefactive
events themselves as well as the writer?s attitude
toward the agents and objects of those events.
2 Overview
For ease of communication, we use the terms
goodFor and badFor for benefactive and malefac-
tive events, respectively, and use the abbreviation
gfbf for an event that is one or the other. There are
many varieties of gfbf events, including destruc-
tion (as in kill Bill, which is bad for Bill), cre-
ation (as in bake a cake, which is good for the
cake), gain or loss (as in increasing costs, which
is good for the costs), and benefit or injury (as in
comforted the child, which is good for the child)
(Anand and Reschke, 2010).
The scheme targets clear cases of gfbf events.
The event must be representable as a triple of con-
tiguous text spans, ?agent, gfbf, object?. The
agent must be a noun phrase, or it may be implicit
(as in the constituent will be destroyed). The ob-
ject must be a noun phrase.
120
Another component of the scheme is the influ-
encer, a word whose effect is to either retain or
reverse the polarity of a gfbf event. For example:
(2) Luckily Bill didn?t kill him.
(3) The reform prevented companies
from hurting patients.
(4) John helped Mary to save Bill.
In (2) and (3), didn?t and prevented, respectively,
reverse the polarity from badFor to goodFor (not
killing Bill is good for Bill; preventing companies
from hurting patients is good for the patients). In
(4), helped is an influencer which retains the polar-
ity (i.e., helping Mary to save Bill is good for Bill).
Examples (3) and (4) illustrate the case where an
influencer introduces an additional agent (reform
in (3) and John in (4)).
The agent of an influencer must be a noun
phrase or implicit. The object must be another in-
fluencer or a gfbf event.
Note that, semantically, an influencer can be
seen as good for or bad for its object. A reverser
influencer makes its object irrealis (i.e., not hap-
pen). Thus, it is bad for it. In (3), for example,
prevent is bad for the hurting event. A retainer in-
fluencer maintains its object, and thus is good for
it. In (4), for example, helped maintains the sav-
ing event. For this reason, influencers and gfbf
events are sometimes combined in the evaluations
presented below (see Section 4.2).
Finally, the annotators are asked to mark the
writer?s attitude towards the agents of the influ-
encers and gfbf events and the objects of the gfbf
events. For example:
(5) GOP Attack on Reform Is a Fight
Against Justice.
(6) Jettison any reference to end-of-life
counselling.
In (5), there are two badFor events: ?GOP, Attack
on, Reform? and ?GOP Attack on Reform,Fight
Against, Justice?. The writer?s attitude toward
both agents is negative, and his or her attitude
toward both objects is positive. In (6), the
writer conveys a negative attitude toward end-of-
life counselling. The coding manual instructs the
annotators to consider whether an attitude of the
writer is communicated or revealed in the particu-
lar sentence which contains the gfbf event.
3 Annotation Scheme
There are four types of annotations: gfbf event,
influencer, agent, and object. For gfbf events, the
agent, object, and polarity (goodFor or badFor) are
identified. For influencers, the agent, object and
effect (reverse or retain) are identified. For agents
and objects, the writer?s attitude is marked (posi-
tive, negative, or none). The annotator links agents
and objects to their gfbf and influencer annotations
via explicit IDs. When an agent is not mentioned
explicitly, the annotator should indicate that it is
implicit. For any span the annotator is not certain
about, he or she can set the uncertain option to be
true.
The annotation manual includes guidelines to
help clarify which events should be annotated.
Though it often is, the gfbf span need not be a
verb or verb phrase. We saw an example above,
namely (5). Even though attack on and fight
against are not verbs, we still mark them because
they represent events that are bad for the object.
Note that, Goyal et al (2012) present a method for
automatically generating a lexicon of what they
call patient polarity verbs. Such verbs correspond
to gfbf events, except that gfbf events are, concep-
tually, events, not verbs, and gfbf spans are not
limited to verbs (as just noted).
Recall from Section 2 that annotators should
only mark gfbf events that may be represented as a
triple, ?agent,gfbf,object?. The relationship should
be perceptible by looking only at the spans in the
triple. If, for example, another argument of the
verb is needed to perceive the relationship, the an-
notators should not mark that event.
(7) His uncle left him a massive amount
of debt.
(8) His uncle left him a treasure.
There is no way to break these sentences into
triples that follow our rules. ?His uncle, left, him?
doesn?t work because we cannot perceive the po-
larity looking only at the triple; the polarity de-
pends on what his uncle left him. ?His uncle, left
him, a massive amount of debt? isn?t correct: the
event is not bad for the debt, it is bad for him. Fi-
nally, ?His uncle, left him a massive amount of
debt, Null? isn?t correct, since no object is iden-
tified.
Note that him in (7) and (8) are both consid-
ered benefactive semantic roles (Zu?n?iga and Kit-
tila?, 2010). In general, gfbf objects are not equiva-
121
lent to benefactive/malefactive semantic roles. For
example, in our scheme, (7) is a badFor event and
(8) is a goodFor event, while him fills the benefac-
tive semantic role in both. Further, according to
(Zu?n?iga and Kittila?, 2010), me is the filler of the
benefactive role in She baked a cake for me. Yet,
in our scheme, a cake is the object of the good-
For event; me is not included in the annotations.
The objects of gfbf events are what (Zu?n?iga and
Kittila?, 2010) refer to as the primary targets of the
events, whereas, they state, beneficiary semantic
roles are typically optional arguments. The reason
we annotate only the primary objects (and agents)
is that the clear cases of attitude implicatures mo-
tivating this work (see Section 1) are inferences
toward agents and primary objects of gfbf events.
Turning to influencers, there may be chains of
them, where the ultimate polarity and agent must
be determined compositionally. For example, the
structure of Jack stopped Mary from trying to kill
Bill is a reverser influencer (stopped) whose object
is a retainer influencer (trying) whose object is, in
turn, a badFor event (kill). The ultimate polarity of
this event is goodFor and the ?highest level? agent
is Jack. In our scheme, all such chains of lengthN
are treated as N ? 1 influencers followed by a sin-
gle gfbf event. It will be up to an automatic system
to calculate the ultimate polarity and agent using
rules such as those presented in, e.g., (Moilanen
and Pulman, 2007; Neviarouskaya et al, 2010).
To save some effort, the annotators are not
asked to mark retainer influencers which do not in-
troduce new agents. For example, for Jack stopped
trying to kill Bill, there is no need to mark ?trying.?
Of course, all reverser influencers must be marked.
4 Agreement Study
To validate the reliability of the annotation
scheme, we conducted an agreement study. In this
section we introduce how we designed the agree-
ment study, present the evaluation method and
give the agreement results. Besides, we conduct
a second-step consensus study to further analyze
the disagreement.
4.1 Data and Agreement Study Design
For this study, we want to use data that is rich in
opinions and implicatures. Thus we used the cor-
pus from (Conrad et al, 2012), which consists of
134 documents from blogs and editorials about a
controversial topic, ?the Affordable Care Act?.
To measure agreement on various aspects of
the annotation scheme, two annotators, who are
co-authors, participated in the agreement study;
one of the two wasn?t involved in developing the
scheme. The new annotator first read the anno-
tation manual and discussed it with the first an-
notator. Then, the annotators labelled 6 docu-
ments and discussed their disagreements to recon-
cile their differences. For the formal agreement
study, we randomly selected 15 documents, which
have a total of 725 sentences. These documents do
not contain any examples in the manual, and they
are different from the documents discussed during
training. The annotators then independently anno-
tated the 15 selected documents.
4.2 Agreement Study Evaluation
We annotate four types of items (gfbf event, influ-
encer, agent, and object) and their corresponding
attributes. As noted above in Section 2, influencers
can also be viewed as gfbf events. Also, the two
may be combined together in chains. Thus, we
measure agreement for gfbf and influencer spans
together, treating them as one type. Then we
choose the subset of gfbf and influencer annota-
tions that both annotators identified, and measure
agreement on the corresponding agents and ob-
jects.
Sometimes the annotations differ even though
the annotators recognize the same gfbf event.
Consider the following sentence:
(9) Obama helped reform curb costs.
Suppose the annotations given by the annotators
were:
Ann 1. ?Obama, helped, curb?
?reform, curb, costs?
Ann 2. ?Obama, helped, reform?
The two annotators do agree on the ?Obama,
helped, reform? triple, the first one marking helped
as a retainer and the other marking it as a goodFor
event. To take such cases into consideration in our
evaluation of agreement, if two spans overlap and
one is marked as gfbf and the other as influencer,
we use the following rules to match up their agents
and objects:
? for a gfbf event, consider its agent and object
as annotated;
122
? for an influencer, assign the agent of the in-
fluencer?s object to be the influencer?s object,
and consider its agent as annotated and the
newly-assigned object. In (9), Ann 2?s anno-
tations remain the same and Ann 1?s become
?Obama, helped, reform? and ?reform, curb,
costs?.
We use the same measurement for agreement
for all types of spans. Suppose A is a set of an-
notations of a particular type and B is the set of
annotations of the same type from the other anno-
tator. For any text span a ? A and b ? B, the span
coverage c measures the overlap between a and b.
Two measures of c are adopted here.
Binary: As in (Wilson and Wiebe, 2003), if two
spans a and b overlap, the pair is counted as 1,
otherwise 0.
c1(a, b) = 1 if |a ? b| > 0
Numerical: (Johansson and Moschitti, 2013)
propose, for the pairs that are counted as 1 by c1, a
measure of the percentage of overlapping tokens,
c2(a, b) =
|a ? b|
|b|
where |a| is the number of tokens in span a, and ?
gives the tokens that two spans have in common.
As (Breck et al, 2007) point out, c2 avoids the
problem of c1, namely that c1 does not penalize a
span covering the whole sentence, so it potentially
inflates the results.
Following (Wilson and Wiebe, 2003), treat-
ing each set A and B in turn as the gold-
standard, we calculate the average F-measure, de-
noted agr(A,B). agr(A,B) is calculated twice,
once with c = c1 and once with c = c2.
match(A,B) =
?
a?A,b?B,
|a?b|>0
c(a, b)
agr(A||B) = match(A,B)|B|
agr(A,B) = agr(A||B) + agr(B||A)2
Now that we have the sets of annotations on
which the annotators agree, we use ? (Artstein
and Poesio, 2008) to measure agreement for the
attributes. We report two ? values: one for the
polarities of the gfbf events, together with the ef-
fects of the influencers, and one for the writer?s
gfbf & agent object
influencer
all anno- c1 0.70 0.92 1.00
tations c2 0.69 0.87 0.97
only c1 0.75 0.92 1.00
certain c2 0.72 0.87 0.98
consensus c1 0.85 0.93 0.99
study c2 0.81 0.88 0.98
Table 1: Span overlapping agreement agr(A,B)
in agreement study and consensus study.
polarity & effect attitude
all 0.97 0.89
certain 0.97 0.89
Table 2: ? for attribute agreement.
attitude toward the agents and objects. Note that,
as in Example (9), sometimes one annotator marks
a span as gfbf and the other marks it as an influ-
encer; in such cases we regard retain and goodfor
as the same attribute value and reverse and badfor
as the same value. Table 1 gives the agr values
and Table 2 gives the ? values.
4.3 Agreement Study Results
Recall that the annotator could choose whether
(s)he is certain about the annotation. Thus, we
evaluate two sets: all annotations and only those
annotations that both annotators are certain about.
The results are shown in the top four rows in Table
1.
The results for agents and objects in Table 1 are
all quite good, indicating that, given a gfbf or in-
fluencer, the annotators are able to correctly iden-
tify the agent and object.
Table 1 also shows that results are not signifi-
cantly worse when measured using c2 rather than
c1. This suggests that, in general, the annotators
have good agreement concerning the boundaries
of spans.
Table 2 shows that the ? values are high for both
sets of attributes.
4.4 Consensus Analysis
Following (Medlock and Briscoe, 2007), we ex-
amined what percentage of disagreement is due to
negligence on behalf of one or the other annota-
tor (i.e., cases of clear gfbfs or influencers that
were missed), though we conducted our consensus
123
study in a more independent manner than face-to-
face discussion between the annotators. For anno-
tator Ann1, we highlighted sentences for which
only Ann2 marked a gfbf event, and gave Ann1?s
annotations back to him or her with the highlights
added on top. For Ann2 we did the same thing.
The annotators reconsidered their highlighted sen-
tences, making any changes they felt they should,
without communicating with each other. There
could be more than one annotation in a highlighted
sentence; the annotators were not told the specific
number.
After re-annotating the highlighted sentences,
we calculate the agreement score for all the an-
notations. As shown in the last two rows in Table
1, the agreement for gfbf and influencer annota-
tions increases quite a bit. Similar to the claim
in (Medlock and Briscoe, 2007), it is reasonable
to conclude that the actual agreement is approx-
imately lower bounded by the initial values and
upper bounded by the consensus values, though,
compared to face-to-face consensus, we provide a
tighter upper bound.
5 Corpus and Examples
Recall from in Section 4.1 that we use the corpus
from (Conrad et al, 2012), which consists of 134
documents with a total of 8,069 sentences from
blogs and editorials about ?the Affordable Care
Act?. There are 1,762 gfbf and influencer annota-
tions. On average, more than 20 percent of the sen-
tences contain a gfbf event or an influencer. Out of
all gfbf and influencer annotations, 40 percent are
annotated as goodFor or retain and 60 percent are
annotated as badFor or reverse. For agents and ob-
jects, 52 percent are annotated as positive and 47
percent as negative. Only 1 percent are annotated
as none, showing that almost all the sentences (in
this corpus of editorials and blogs) which con-
tain gfbf annotations are subjective. The annotated
corpus is available online1.
To illustrate various aspects of the annotation
scheme, in this section we give several examples
from the corpus. In the examples below, words
in square brackets are agents or objects, words in
italics are influencers, and words in boldface are
gfbf events.
1. And [it] will enable [Obama and the
Democrats] - who run Washington - to get
1http://mpqa.cs.pitt.edu/
back to creating [jobs].
(a) Creating is goodFor jobs; the agent is
Obama and the Democrats.
(b) The phrase to get back to is a retainer in-
fluencer. But, the agent span is also Obama
and the Democrats, as the same with the
goodFor, so we don?t have to give an anno-
tation for it.
(c) The phrase enable is a retainer influencer.
Since its agent span is different (namely, it),
we do create an annotation for it.
2. [Repealing [the Affordable Care Act]] would
hurt [families, businesses, and our econ-
omy].
(a) Repealing is a badFor event since it de-
prives the object, the Affordable Care Act, of
its existence. In this case the agent is implicit.
(b) The agent of the badFor event hurt is the
whole phrase Repealing the Affordable Care
Act. Note that the agent span is in fact a noun
phrase (even though it refers to an event).
Thus, it doesn?t break the rule that all agent
gfbf spans should be noun phrases.
3. It is a moral obligation to end this indefensi-
ble neglect of [hard-working Americans].
(a) This example illustrates a gfbf that cen-
ters on a noun (neglect) rather than on a verb.
(b) It also illustrates the case when two words
can be seen as gfbf events: both end and ne-
glect of can be seen as badFor events. Fol-
lowing our specification, they are annotated
as a chain ending in a single gfbf event: end
is an influencer that reverses the polarity of
the badFor event neglect of.
6 Conclusion
Attitude inferences arise from interactions
between sentiment expressions and benefac-
tive/malefactive events. Corpora have been
annotated in the past for explicit sentiment ex-
pressions; this paper fills in a gap by presenting
an annotation scheme for benefactive/malefactive
events and the writer?s attitude toward the agents
and objects of those events. We conducted an
agreement study, the results of which are positive.
Acknowledgement This work was supported
in part by DARPA-BAA-12-47 DEFT grant
#12475008 and National Science Foundation
grant #IIS-0916046. We would like to thank the
anonymous reviewers for their helpful feedback.
124
References
Pranav Anand and Kevin Reschke. 2010. Verb classes
as evaluativity functor classes. In Interdisciplinary
Workshop on Verbs. The Identification and Repre-
sentation of Verb Features.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput.
Linguist., 34(4):555?596, December.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In Pro-
ceedings of the 20th international joint conference
on Artifical intelligence, IJCAI?07, pages 2683?
2688, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Alexander Conrad, Janyce Wiebe, Hwa, and Rebecca.
2012. Recognizing arguing subjectivity and argu-
ment tags. In Proceedings of the Workshop on
Extra-Propositional Aspects of Meaning in Com-
putational Linguistics, ExProM ?12, pages 80?88,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Amit Goyal, Ellen Riloff, and Hal Daum III. 2012. A
computational model for plot units. Computational
Intelligence, pages no?no.
Richard Johansson and Alessandro Moschitti. 2013.
Relational features in fine-grained opinion analysis.
Computational Linguistics, 39(3).
Jason S. Kessler, Miriam Eckert, Lyndsay Clark, and
Nicolas Nicolov. 2010. The 2010 icwsm jdpa sen-
timent corpus for the automotive domain. In 4th
Int?l AAAI Conference on Weblogs and Social Media
Data Workshop Challenge (ICWSM-DWC 2010).
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics.
Karo Moilanen and Stephen Pulman. 2007. Senti-
ment composition. In Proceedings of RANLP 2007,
Borovets, Bulgaria.
Alena Neviarouskaya, Helmut Prendinger, and Mit-
suru Ishizuka. 2010. Recognition of affect, judg-
ment, and appreciation in text. In Proceedings of the
23rd International Conference on Computational
Linguistics, COLING ?10, pages 806?814, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2/3):164?210.
Theresa Wilson and Janyce Wiebe. 2003. Annotating
opinions in the world press. In Proceedings of the
4th ACL SIGdial Workshop on Discourse and Dia-
logue (SIGdial-03), pages 13?22.
Theresa Wilson and Janyce Wiebe. 2005. Annotat-
ing attributions and private states. In Proceedings of
ACL Workshop on Frontiers in Corpus Annotation
II: Pie in the Sky.
F. Zu?n?iga and S. Kittila?. 2010. Introduction. In
F. Zu?n?iga and S. Kittila?, editors, Benefactives and
malefactives, Typological studies in language. J.
Benjamins Publishing Company.
125
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 221?228, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
CPN-CORE: A Text Semantic Similarity System Infused
with Opinion Knowledge
Carmen Banea[?, Yoonjung Choi?, Lingjia Deng?, Samer Hassan?, Michael Mohler?
Bishan Yang
?
, Claire Cardie
?
, Rada Mihalcea[?, Janyce Wiebe?
[University of North Texas
Denton, TX
?University of Pittsburgh
Pittsburgh, PA
?Google Inc.
Mountain View, CA
?Language Computer Corp.
Richardson, TX
?
Cornell University
Ithaca, NY
Abstract
This article provides a detailed overview of the
CPN text-to-text similarity system that we par-
ticipated with in the Semantic Textual Similar-
ity task evaluations hosted at *SEM 2013. In
addition to more traditional components, such
as knowledge-based and corpus-based met-
rics leveraged in a machine learning frame-
work, we also use opinion analysis features to
achieve a stronger semantic representation of
textual units. While the evaluation datasets are
not designed to test the similarity of opinions,
as a component of textual similarity, nonethe-
less, our system variations ranked number 38,
39 and 45 among the 88 participating systems.
1 Introduction
Measures of text similarity have been used for a long
time in applications in natural language processing
and related areas. One of the earliest applications
of text similarity is perhaps the vector-space model
used in information retrieval, where the document
most relevant to an input query is determined by
ranking documents in a collection in reversed or-
der of their angular distance with the given query
(Salton and Lesk, 1971). Text similarity has also
been used for relevance feedback and text classifi-
cation (Rocchio, 1971), word sense disambiguation
(Lesk, 1986; Schutze, 1998), and extractive summa-
rization (Salton et al, 1997), in the automatic evalu-
ation of machine translation (Papineni et al, 2002),
?carmen.banea@gmail.com
? rada@cs.unt.edu
text summarization (Lin and Hovy, 2003), text co-
herence (Lapata and Barzilay, 2005) and in plagia-
rism detection (Nawab et al, 2011).
Earlier work on this task has primarily focused on
simple lexical matching methods, which produce a
similarity score based on the number of lexical units
that occur in both input segments. Improvements
to this simple method have considered stemming,
stopword removal, part-of-speech tagging, longest
subsequence matching, as well as various weight-
ing and normalization factors (Salton and Buckley,
1997). While successful to a certain degree, these
lexical similarity methods cannot always identify the
semantic similarity of texts. For instance, there is an
obvious similarity between the text segments ?she
owns a dog? and ?she has an animal,? yet these
methods will mostly fail to identify it.
More recently, researchers have started to con-
sider the possibility of combining the large number
of word-to-word semantic similarity measures (e.g.,
(Jiang and Conrath, 1997; Leacock and Chodorow,
1998; Lin, 1998; Resnik, 1995)) within a semantic
similarity method that works for entire texts. The
methods proposed to date in this direction mainly
consist of either bipartite-graph matching strate-
gies that aggregate word-to-word similarity into a
text similarity score (Mihalcea et al, 2006; Islam
and Inkpen, 2009; Hassan and Mihalcea, 2011;
Mohler et al, 2011), or data-driven methods that
perform component-wise additions of semantic vec-
tor representations as obtained with corpus mea-
sures such as latent semantic analysis (Landauer et
al., 1997), explicit semantic analysis (Gabrilovich
and Markovitch, 2007), or salient semantic analysis
221
(Hassan and Mihalcea, 2011).
In this paper, we describe the system variations
with which we participated in the *SEM 2013 task
on semantic textual similarity (Agirre et al, 2013).
The system builds upon our earlier work on corpus-
based and knowledge-based methods of text seman-
tic similarity (Mihalcea et al, 2006; Hassan and
Mihalcea, 2011; Mohler et al, 2011; Banea et al,
2012), while also incorporating opinion aware fea-
tures. Our observation is that text is not only similar
on a semantic level, but also with respect to opin-
ions. Let us consider the following text segments:
?she owns a dog? and ?I believe she owns a dog.?
The question then becomes how similar these text
fragments truly are. Current systems will consider
the two sentences semantically equivalent, yet to a
human, they are not. A belief is not equivalent to a
fact (and for the case in point, the person may very
well have a cat or some other pet), and this should
consequently lower the relatedness score. For this
reason, we advocate that STS systems should also
consider the opinions expressed and their equiva-
lence. While the *SEM STS task is not formulated
to evaluate this type of similarity, we complement
more traditional corpus and knowledge-based meth-
ods with opinion aware features, and use them in
a meta-learning framework in an arguably first at-
tempt at incorporating this type of information to in-
fer text-to-text similarity.
2 Related Work
Over the past years, the research community has
focused on computing semantic relatedness using
methods that are either knowledge-based or corpus-
based. Knowledge-based methods derive a measure
of relatedness by utilizing lexical resources and on-
tologies such as WordNet (Miller, 1995) to measure
definitional overlap, term distance within a graph-
ical taxonomy, or term depth in the taxonomy as
a measure of specificity. We explore several of
these measures in depth in Section 3.3.1. On the
other side, corpus-based measures such as Latent
Semantic Analysis (LSA) (Landauer et al, 1997),
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007), Salient Semantic Analysis
(SSA) (Hassan and Mihalcea, 2011), Pointwise Mu-
tual Information (PMI) (Church and Hanks, 1990),
PMI-IR (Turney, 2001), Second Order PMI (Islam
and Inkpen, 2006), Hyperspace Analogues to Lan-
guage (Burgess et al, 1998) and distributional simi-
larity (Lin, 1998) employ probabilistic approaches
to decode the semantics of words. They consist
of unsupervised methods that utilize the contextual
information and patterns observed in raw text to
build semantic profiles of words. Unlike knowledge-
based methods, which suffer from limited coverage,
corpus-based measures are able to induce a similar-
ity between any given two words, as long as they
appear in the very large corpus used as training.
3 Semantic Textual Similarity System
3.1 Task Setup
The STS task consists of labeling one sentence pair
at a time, based on the semantic similarity existent
between its two component sentences. Human as-
signed similarity scores range from 0 (no relation)
to 5 (semantivally equivalent). The *SEM 2013 STS
task did not provide additional labeled data to the
training and testing sets released as part of the STS
task hosted at SEMEVAL 2012 (Agirre et al, 2012);
our system variations were trained on SEMEVAL
2012 data.
The test sets (Agirre et al, 2013) consist of
text pairs extracted from headlines (headlines,
750 pairs), sense definitions from WordNet and
OntoNotes (OnWN, 561 pairs), sense definitions
from WordNet and FrameNet (FNWN, 189 pairs),
and data used in the evaluation of machine transla-
tion systems (SMT, 750 pairs).
3.2 Resources
Various subparts of our framework use several re-
sources that are described in more detail below.
Wikipedia1 is the most comprehensive encyclo-
pedia to date, and it is an open collaborative effort
hosted on-line. Its basic entry is an article which in
addition to describing an entity or an event also con-
tains hyperlinks to other pages within or outside of
Wikipedia. This structure (articles and hyperlinks)
is directly exploited by semantic similarity methods
such as ESA (Gabrilovich and Markovitch, 2007),
or SSA (Hassan and Mihalcea, 2011)2.
1www.wikipedia.org
2In the experiments reported in this paper, all the corpus-
based methods are trained on the English Wikipedia download
from October 2008.
222
WordNet (Miller, 1995) is a manually crafted lex-
ical resource that maintains semantic relationships
such as synonymy, antonymy, hypernymy, etc., be-
tween basic units of meaning, or synsets. These rela-
tionships are employed by various knowledge-based
methods to derive semantic similarity.
The MPQA corpus (Wiebe and Riloff, 2005) is
a newswire data set that was manually annotated
at the expression level for opinion-related content.
Some of the features derived by our opinion extrac-
tion models were based on training on this corpus.
3.3 Features
Our system variations derive the similarity score of a
given sentence-pair by integrating information from
knowledge, corpus, and opinion-based sources3.
3.3.1 Knowledge-Based Features
Following prior work from our group (Mihalcea
et al, 2006; Mohler and Mihalcea, 2009), we em-
ploy several WordNet-based similarity metrics for
the task of sentence-level similarity. Briefly, for each
open-class word in one of the input texts, we com-
pute the maximum semantic similarity4 that can be
obtained by pairing it with any open-class word in
the other input text. All the word-to-word similarity
scores obtained in this way are summed and normal-
ized to the length of the two input texts. We provide
below a short description for each of the similarity
metrics employed by this system.
The shortest path (Path) similarity is equal to:
Simpath =
1
length
(1)
where length is the length of the shortest path be-
tween two concepts using node-counting.
The Leacock & Chodorow (Leacock and
Chodorow, 1998) (LCH) metric is equal to:
Simlch = ? log
length
2 ?D
(2)
where length is the length of the shortest path be-
tween two concepts using node-counting, and D is
the maximum depth of the taxonomy.
The Lesk (Lesk) similarity of two concepts is de-
fined as a function of the overlap between the cor-
responding definitions, as provided by a dictionary.
3The abbreviation in italics accompanying each method al-
lows for cross-referencing with the results listed in Table 2.
4We use the WordNet::Similarity package (Pedersen et al,
2004).
It is based on an algorithm proposed by Lesk (1986)
as a solution for word sense disambiguation.
The Wu & Palmer (Wu and Palmer, 1994) (WUP )
similarity metric measures the depth of two given
concepts in the WordNet taxonomy, and the depth
of the least common subsumer (LCS), and combines
these figures into a similarity score:
Simwup =
2 ? depth(LCS)
depth(concept1) + depth(concept2)
(3)
The measure introduced by Resnik (Resnik, 1995)
(RES) returns the information content (IC) of the
LCS of two concepts:
Simres = IC(LCS) (4)
where IC is defined as:
IC(c) = ? logP (c) (5)
and P (c) is the probability of encountering an in-
stance of concept c in a large corpus.
The measure introduced by Lin (Lin, 1998) (Lin)
builds on Resnik?s measure of similarity, and adds
a normalization factor consisting of the information
content of the two input concepts:
Simlin =
2 ? IC(LCS)
IC(concept1) + IC(concept2)
(6)
We also consider the Jiang & Conrath (Jiang and
Conrath, 1997) (JCN ) measure of similarity:
Simjnc =
1
IC(concept1) + IC(concept2)? 2 ? IC(LCS)
(7)
3.3.2 Corpus Based Features
While most of the corpus-based methods induce
semantic profiles in a word-space, where the seman-
tic profile of a word is expressed in terms of its co-
occurrence with other words, LSA, ESA and SSA
rely on a concept-space representation, thus express-
ing a word?s semantic profile in terms of the im-
plicit (LSA), explicit (ESA), or salient (SSA) con-
cepts. This departure from the sparse word-space to
a denser, richer, and unambiguous concept-space re-
solves one of the fundamental problems in semantic
relatedness, namely the vocabulary mismatch.
Latent Semantic Analysis (LSA) (Landauer et al,
1997). In LSA, term-context associations are cap-
tured by means of a dimensionality reduction op-
erated by a singular value decomposition (SVD)
223
on the term-by-context matrix T, where the ma-
trix is induced from a large corpus. This reduc-
tion entails the abstraction of meaning by collaps-
ing similar contexts and discounting noisy and ir-
relevant ones, hence transforming the real world
term-context space into a word-latent-concept space
which achieves a much deeper and concrete seman-
tic representation of words5.
Random Projection (RP ) (Dasgupta, 1999). In RP,
a high dimensional space is projected onto a lower
dimensional one, using a randomly generated ma-
trix. (Bingham and Mannila, 2001) show that unlike
LSA or principal component analysis (PCA), RP
is computationally efficient for large corpora, while
also retaining accurate vector similarity and yielding
comparable results.
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007). ESA uses encyclopedic
knowledge in an information retrieval framework to
generate a semantic interpretation of words. It relies
on the distribution of words inside Wikipedia arti-
cles, thus building a semantic representation for a
given word using a word-document association.
Salient Semantic Analysis (SSA) (Hassan and Mi-
halcea, 2011). SSA incorporates a similar seman-
tic abstraction as ESA, yet it uses salient con-
cepts gathered from encyclopedic knowledge, where
a ?concept? represents an unambiguous expression
which affords an encyclopedic definition. Saliency
in this case is determined based on the word being
hyperlinked in context, implying that it is highly rel-
evant to the given text.
In order to determine the similarity of two text
fragments, we employ two variations: the typical
cosine similarity (cos) and a best alignment strat-
egy (align), which we explain in more detail in
the paragraph below. Both variations were paired
with the ESA, and SSA systems resulting in four
similarity scores that were used as features by our
meta-system, namely ESAcos, ESAalign, SSAcos,
and SSAalign; in addition, we also used BOWcos,
LSAcos, and RPcos.
Best Alignment Strategy (align). Let Ta and Tb be
two text fragments of size a and b respectively. After
removing all stopwords, we first determine the num-
5We use the LSA implementation available at code.
google.com/p/semanticvectors/.
ber of shared terms (?) between Ta and Tb. Second,
we calculate the semantic relatedness of all possible
pairings between non-shared terms in Ta and Tb. We
further filter these possible combinations by creating
a list ? which holds the strongest semantic pairings
between the fragments? terms, such that each term
can only belong to one and only one pair.
Sim(Ta, Tb) =
(? +
?|?|
i=1 ?i)? (2ab)
a+ b
(8)
where ?i is the similarity score for the ith pairing.
3.3.3 Opinion Aware Features
We design opinion-aware features to capture sen-
tence similarity on the subjectivity level based on the
output of three subjectivity analysis systems. Intu-
itively, two sentences are similar in terms of sub-
jectivity if there exists similar opinion expressions
which also share similar opinion holders.
OpinionFinder (Wilson et al, 2005) is a publicly
available opinion extraction model that annotates the
subjectivity of new text based on the presence (or
absence) of words or phrases in a large lexicon. The
system consists of a two step process, by feeding
the sentences identified as subjective or objective
by a rule-based high-precision classifier to a high-
recall classifier that iteratively learns from the re-
maining corpus. For each sentence in a STS pair,
the two classifiers provide two predictions; a subjec-
tivity similarity score (SUBJSL) is computed as fol-
lows. If both sentences are classified as subjective
or objective, the score is 1; if one is subjective and
the other one is objective, the score is -1; otherwise
it is 0. We also make use of the output of the sub-
jective expression identifier in OpinionFinder. We
first record how many expressions the two sentences
have: feature NUMEX1 and NUMEX2. Then we
compare how many tokens these expressions share
and we normalize by the total number of expressions
(feature EXPR).
We compute the difference between the probabil-
ities of the two sentences being subjective (SUBJD-
IFF), by employing a logistic regression classifier
using LIBLINEAR (Fan et al, 2008) trained on the
MPQA corpus. The smaller the difference, the more
similar the sentences are in terms of subjectivity.
We also employ features produced by the opinion-
extraction model of Yang and Cardie (Yang and
Cardie, 2012), which is better suited to process ex-
224
pressions of arbitrary length. Specifically, for each
sentence, we extract subjective expressions and gen-
erate the following features. SUBJCNT is a binary
feature which is equal to 1 if both sentences con-
tain a subjective expression. DSEALGN marks the
number of shared words between subjective expres-
sions in two sentences, while DSESIM represents
their similarity beyond the word level. We repre-
sent the subjective expressions in each sentence as
a feature vector, containing unigrams extracted from
the expressions, their part-of-speech, their WordNet
hypernyms and their subjectivity label6, and com-
pute the cosine similarity between the feature vec-
tors. The holder of the opinion expressions is ex-
tracted with the aid of a dependency parser7. In most
cases, the opinion holder and the opinion expression
are related by the dependency relation subj. This re-
lation is used to expand the verb dependents in the
opinion expression and identify the opinion holder
or AGENT.
3.4 Meta-learning
Each metric described above provides one individ-
ual score for every sentence-pair in both the train-
ing and test set. These scores then serve as in-
put to a meta-learner, which adjusts their impor-
tance, and thus their bearing on the overall similar-
ity score predicted by the system. We experimented
with regression and decision tree based algorithms
by performing 10-fold cross validation on the 2012
training data; these types of learners are particularly
well suited to maintain the ordinality of the seman-
tic similarity scores (i.e. a score of 4.5 is closer
to either 4 or 5, implying that the two sentences
are mostly or fully equivalent, while also being far
further away from 0, implying no semantic relat-
edness between the two sentences). We obtained
consistent results when using support vector regres-
sion with polynomial kernel (Drucker et al, 1997;
Smola and Schoelkopf, 1998) (SV R) and random
subspace meta-classification with tree learners (Ho,
1998) (RandSubspace)8.
We submitted three system variations based
on the training corpus (first word in the sys-
6Label is based on the OpinionFinder subjectivity lexicon
(Wiebe et al, 2005).
7nlp.stanford.edu/software/
8Included with the Weka framework (Hall et al, 2009); we
used the default values for both algorithms.
System FNWN headlines OnWN SMT Mean
comb.RandSubSpace 0.331 0.677 0.514 0.337 0.494
comb.SVR 0.362 0.669 0.510 0.341 0.494
indv.RandSubspace 0.331 0.677 0.548 0.277 0.483
baseline-tokencos 0.215 0.540 0.283 0.286 0.364
Table 1: Evaluation results (Agirre et al, 2013).
tem name) or the learning methodology (second
word) used: comb.RandSubspace, comb.SV R and
indv.RandSubspace. For comb, training was per-
formed on the merged version of the entire 2012 SE-
MEVAL dataset. For indv, predictions for OnWN
and SMT test data were based on training on
matching OnWN and SMT 9 data from 2012, pre-
dictions for the other test sets were computed using
the combined version (comb).
4 Results and Discussion
Table 2 lists the correlations obtained between
the scores assigned by each one of the features
we used and the scores assigned by the human
judges. It is interesting to note that overall, corpus-
based measures are stronger performers compared to
knowledge-based measures. The top contenders in
the former group are ESAalign, SSAalign, LSAcos,
and RPcos, indicating that these methods are able to
leverage a significant amount of semantic informa-
tion from text. While LSAcos achieves high corre-
lations on many of the datasets, replacing the singu-
lar value decomposition operation by random pro-
jection to a lower-dimension space (RP ) achieves
competitive results while also being computation-
ally efficient. This observation is in line with prior
literature (Bingham and Mannila, 2001). Among
the knowledge-based methods, JCN and Path
achieve high performance on more than five of the
datasets. In some cases, particularly on the 2013
test data, the shortest path method (Path) peforms
better or on par with the performance attained by
other knowledge-based measures, despite its com-
putational simplicity. While opinion-based mea-
sures do not exhibit the same high correlation, we
should remember that none of the datasets displays
consistent opinion content, nor were they anno-
tated with this aspect in mind, in order for this in-
formation to be properly leveraged and evaluated.
9The SMT training set is a combination of SMTeuroparl
(in this paper abbreviated as SMTep) and SMTnews data.
225
Train 2012 Test 2012 Test 2013
Feature SMTep MSRpar MSRvid SMTep MSRpar MSRvid OnWN SMTnews FNWN headlines OnWN SMT
Knowledge-based measures
JCN 0.51 0.49 0.63 0.48 0.48 0.64 0.62 0.28 0.38 0.72 0.71 0.34
LCH 0.45 0.48 0.49 0.47 0.49 0.54 0.54 0.3 0.39 0.69 0.69 0.32
Lesk 0.5 0.48 0.59 0.5 0.47 0.63 0.64 0.4 0.4 0.71 0.7 0.33
Lin 0.48 0.49 0.54 0.48 0.48 0.56 0.57 0.27 0.28 0.65 0.66 0.3
Path 0.5 0.49 0.62 0.48 0.49 0.65 0.62 0.35 0.43 0.72 0.73 0.34
RES 0.48 0.47 0.55 0.49 0.47 0.6 0.62 0.33 0.28 0.64 0.7 0.31
WUP 0.42 0.46 0.38 0.44 0.48 0.42 0.48 0.26 0.19 0.55 0.6 0.25
Corpus-based measures
BOW cos 0.51 0.47 0.69 0.32 0.44 0.71 0.66 0.37 0.34 0.68 0.52 0.32
ESA cos 0.53 0.34 0.71 0.44 0.3 0.77 0.63 0.44 0.34 0.55 0.35 0.27
ESA align 0.55 0.56 0.75 0.49 0.52 0.78 0.69 0.38 0.46 0.71 0.47 0.34
SSA cos 0.4 0.34 0.63 0.4 0.22 0.71 0.6 0.42 0.35 0.48 0.47 0.26
SSA align 0.54 0.56 0.74 0.49 0.51 0.77 0.68 0.38 0.44 0.69 0.46 0.34
LSA cos 0.65 0.48 0.76 0.36 0.45 0.79 0.67 0.45 0.25 0.63 0.61 0.32
RP cos 0.6 0.49 0.78 0.46 0.43 0.79 0.7 0.45 0.38 0.68 0.57 0.34
Opinion-aware measures
AGENT 0.16 0.15 0.05 0.11 0.12 0.03 n/a -0.01 n/a 0.08 -0.04 0.11
DSEALGN 0.18 0.2 0.11 0.05 0.11 0.11 0.07 0.06 -0.1 0.08 0.13 0.1
DSESIM 0.12 0.15 0.05 0.1 0.08 0.07 0.04 0.08 0.05 0.08 0.04 0.08
EXPR 0.17 0.19 0.06 0.18 0.18 0.02 0.07 0 0.13 0.08 0.18 0.17
NUMEX1 0.12 0.22 -0.03 0.07 0.16 -0.05 -0.01 -0.01 -0.01 -0.03 0.08 0.1
NUMEX2 -0.25 0.19 0.01 0.06 0.14 -0.03 0.01 0.06 0.09 -0.05 0.03 0.11
SUBJCNT 0.14 0.19 0.01 0.09 0.07 0.03 0.02 0.08 0.05 0.05 0.05 0.09
SUBJDIFF -0.07 -0.07 -0.17 -0.27 -0.13 -0.22 -0.17 -0.12 -0.04 -0.12 -0.2 -0.12
SUBJSL 0.15 -0.11 0.07 0.23 0.01 0.07 0.11 -0.08 0.15 0.07 -0.03 0
Table 2: Correlation of individual features for the training and test sets with the gold standard.
Nonetheless, we notice several promising features,
such as DSEALIGN and EXPR. Lower cor-
relations seem to be associated with shorter spans
of text, since when averaging all opinion-based cor-
relations per dataset, MSRvid (x2), OnWN (x2),
and headlines display the lowest average correla-
tion, ranging from 0 to 0.03. This matches the
expectation that opinionated content can be easier
identified in longer contexts, as additional subjective
elements amount to a stronger prediction. The other
seven datasets consist of longer spans of text; they
display an average opinion-based correlation be-
tween 0.07 and 0.12, with the exception of FNWN
and SMTnews at 0.04 and 0.01, respectively.
Our systems performed well, ranking 38, 39 and
45 among the 88 competing systems in *SEM 2013
(see Table 1), with the best being comb.SVR and
comb.RandSubspace, both with a mean correlation
of 0.494. We noticed from our participation in
SEMEVAL 2012 (Banea et al, 2012), that training
and testing on the same type of data achieves the
best results; this receives further support when con-
sidering the performance of the indv.RandSubspace
variation on the OnWN data10, which exhibits a
10The SMT test data is not part of the same corpus as either
0.034 correlation increase over our next best sys-
tem (comb.RandSubspace). While we do surpass the
bag-of-words cosine baseline (baseline-tokencos)
computed by the task organizers by a 0.13 differ-
ence in correlation, we fall short by 0.124 from the
performance of the best system in the STS task.
5 Conclusions
To participate in the STS *SEM 2013 task, we con-
structed a meta-learner framework that combines
traditional knowledge and corpus-based methods,
while also introducing novel opinion analysis based
metrics. While the *SEM data is not particularly
suited for evaluating the performance of opinion fea-
tures, this is nonetheless a first step toward conduct-
ing text similarity research while also considering
the subjective dimension of text. Our system varia-
tions ranked 38, 39 and 45 among the 88 participat-
ing systems.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340 and IIS awards #1018613,
SMTep or SMTnews.
226
#0208798 and #0916046. This work was sup-
ported in part by DARPA-BAA-12-47 DEFT grant
#12475008. Any opinions, findings, and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily reflect
the views of the National Science Foundation or the
Defense Advanced Research Projects Agency.
References
E. Agirre, D. Cer, M. Diab, and A. Gonzalez. 2012.
Semeval-2012 task 6: A pilot on semantic textual sim-
ilarity. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), in con-
junction with the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012).
E. Agirre, D. Cer, M. Diab, A. Gonzalez-Agirre, and W.
Guo. 2013. *SEM 2013 Shared Task: Semantic Tex-
tual Similarity, including a Pilot on Typed-Similarity.
In Proceedings of the Second Joint Conference on Lex-
ical and Computational Semantics (*SEM 2013), At-
lanta, GA, USA.
C. Banea, S. Hassan, M. Mohler, and R. Mihalcea. 2012.
UNT: A supervised synergistic approach to seman-
tic text similarity. In Proceedings of the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012), pages 635?642, Montreal, Canada.
E. Bingham and H. Mannila. 2001. Random projection
in dimensionality reduction: applications to image and
text data. In Proceedings of the seventh ACM SIGKDD
international conference on Knowledge discovery and
data mining (KDD 2001), pages 245?250, San Fran-
cisco, CA, USA.
C. Burgess, K. Livesay, and K. Lund. 1998. Explorations
in context space: words, sentences, discourse. Dis-
course Processes, 25(2):211?257.
K. Church and P. Hanks. 1990. Word association norms,
mutual information, and lexicography. Computational
Linguistics, 16(1):22?29.
S. Dasgupta. 1999. Learning mixtures of Gaussians. In
40th Annual Symposium on Foundations of Computer
Science (FOCS 1999), pages 634?644, New York, NY,
USA.
H. Drucker, C. J. Burges, L. Kaufman, A. Smola, and
Vladimir Vapnik. 1997. Support vector regression
machines. Advances in Neural Information Process-
ing Systems, 9:155?161.
R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. 2008.
Liblinear: A library for large linear classification. The
Journal of Machine Learning Research, 9:1871?1874.
E. Gabrilovich and S. Markovitch. 2007. Comput-
ing semantic relatedness using Wikipedia-based ex-
plicit semantic analysis. In Proceedings of the 20th
AAAI International Conference on Artificial Intelli-
gence (AAAI?07), pages 1606?1611, Hyderabad, In-
dia.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and Ian H. Witten. 2009. The WEKA data
mining software: An update. SIGKDD Explorations,
11(1).
S. Hassan and R. Mihalcea. 2011. Measuring semantic
relatedness using salient encyclopedic concepts. Arti-
ficial Intelligence, Special Issue.
T. K. Ho. 1998. The Random Subspace Method for
Constructing Decision Forests. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 20(8):832?
844.
A. Islam and D. Inkpen. 2006. Second order co-
occurrence PMI for determining the semantic similar-
ity of words. In Proceedings of the 5th Conference on
Language Resources and Evaluation (LREC 06), vol-
ume 2, pages 1033?1038, Genoa, Italy, July.
A. Islam and D. Inkpen. 2009. Semantic Similarity of
Short Texts. In Nicolas Nicolov, Galia Angelova, and
Ruslan Mitkov, editors, Recent Advances in Natural
Language Processing V, volume 309 of Current Issues
in Linguistic Theory, pages 227?236. John Benjamins,
Amsterdam & Philadelphia.
J. J. Jiang and D. W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
International Conference Research on Computational
Linguistics (ROCLING X), pages 9008+, September.
T. K. Landauer, T. K. L, D. Laham, B. Rehder, and M.
E. Schreiner. 1997. How well can passage meaning
be derived without using word order? a comparison of
latent semantic analysis and humans.
M. Lapata and R. Barzilay. 2005. Automatic evaluation
of text coherence: Models and representations. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence, Edinburgh.
C. Leacock and M. Chodorow. 1998. Combining local
context and WordNet similarity for word sense identi-
fication. In WordNet: An Electronic Lexical Database,
pages 305?332.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In SIGDOC ?86: Pro-
ceedings of the 5th annual international conference on
Systems documentation, pages 24?26, New York, NY,
USA. ACM.
C. Lin and E. Hovy. 2003. Automatic evaluation of sum-
maries using n-gram co-occurrence statistics. In Pro-
ceedings of Human Language Technology Conference
(HLT-NAACL 2003), Edmonton, Canada, May.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Interna-
227
tional Conference on Machine Learning, pages 296?
304, Madison, Wisconsin.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of text
semantic similarity. In Proceedings of the American
Association for Artificial Intelligence (AAAI 2006),
pages 775?780, Boston, MA, US.
G. A. Miller. 1995. WordNet: a Lexical database for
English. Communications of the Association for Com-
puting Machinery, 38(11):39?41.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the European Association for Compu-
tational Linguistics (EACL 2009), Athens, Greece.
M. Mohler, R. Bunescu, and R. Mihalcea. 2011. Learn-
ing to grade short answer questions using semantic
similarity measures and dependency graph alignments.
In Proceedings of the Association for Computational
Linguistics ? Human Language Technologies (ACL-
HLT 2011), Portland, Oregon, USA.
R. M. A. Nawab, M. Stevenson, and P. Clough. 2011.
External plagiarism detection using information re-
trieval and sequence alignment: Notebook for PAN at
CLEF 2011. In Proceedings of the 5th International
Workshop on Uncovering Plagiarism, Authorship, and
Social Software Misuse (PAN 2011).
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318, Philadelphia, PA.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet:: Similarity-Measuring the Relatedness of
Concepts. Proceedings of the National Conference on
Artificial Intelligence, pages 1024?1025.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence, pages 448?453.
J. Rocchio, 1971. Relevance feedback in information re-
trieval. Prentice Hall, Ing. Englewood Cliffs, New Jer-
sey.
G. Salton and C. Buckley. 1997. Term weighting ap-
proaches in automatic text retrieval. In Readings in
Information Retrieval. Morgan Kaufmann Publishers,
San Francisco, CA.
G. Salton and M. Lesk, 1971. The SMART Retrieval Sys-
tem: Experiments in Automatic Document Processing,
chapter Computer evaluation of indexing and text pro-
cessing. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997.
Automatic text structuring and summarization. Infor-
mation Processing and Management, 2(32).
H. Schutze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?124.
A. Smola and B. Schoelkopf. 1998. A tutorial on sup-
port vector regression. NeuroCOLT2 Technical Re-
port NC2-TR-1998-030.
P. D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning
(ECML?01), pages 491?502, Freiburg, Germany.
J. Wiebe and E. Riloff. 2005. Creating subjective and
objective sentence classifiers from unannotated texts.
In Proceedings of the 6th international conference on
Computational Linguistics and Intelligent Text Pro-
cessing (CICLing 2005), pages 486?497, Mexico City,
Mexico.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39(2-3):165?210.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff,
and Siddharth Patwardhan. 2005. OpinionFinder:
A system for subjectivity analysis. In Proceedings
of HLT/EMNLP on Interactive Demonstrations, pages
34?35, Vancouver, BC, Canada.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexical
selection. In Proceedings of the 32nd annual meeting
on Association for Computational Linguistics, pages
133?-138, Las Cruces, New Mexico.
B. Yang and C. Cardie. 2012. Extracting opinion expres-
sions with semi-markov conditional random fields. In
Proceedings of the conference on Empirical Meth-
ods in Natural Language Processing. Association for
Computational Linguistics.
228
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 107?112,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
Lexical Acquisition for Opinion Inference:
A Sense-Level Lexicon of Benefactive and Malefactive Events
Yoonjung Choi
1
, Lingjia Deng
2
, and Janyce Wiebe
1,2
1
Department of Computer Science
2
Intelligent Systems Program
University of Pittsburgh
yjchoi@cs.pitt.edu, lid29@pitt.edu, wiebe@cs.pitt.edu
Abstract
Opinion inference arises when opinions
are expressed toward states and events
which positive or negatively affect entities,
i.e., benefactive and malefactive events.
This paper addresses creating a lexicon of
such events, which would be helpful to in-
fer opinions. Verbs may be ambiguous,
in that some meanings may be benefac-
tive and others may be malefactive or nei-
ther. Thus, we use WordNet to create a
sense-level lexicon. We begin with seed
senses culled from FrameNet and expand
the lexicon using WordNet relationships.
The evaluations show that the accuracy of
the approach is well above baseline accu-
racy.
1 Introduction
Opinions are commonly expressed in many kinds
of written and spoken text such as blogs, reviews,
new articles, and conversation. Recently, there
have been a surge in reserach in opinion analy-
sis (sentiment analysis) research (Liu, 2012; Pang
and Lee, 2008).
While most past researches have mainly ad-
dressed explicit opinion expressions, there are a
few researches for implicit opinions expressed via
implicatures. Deng and Wiebe (2014) showed
how sentiments toward one entity may be prop-
agated to other entities via opinion implicature
rules. Consider The bill would curb skyrocketing
health care costs. Note that curb costs is bad for
the object costs since the costs are reduced. We
can reason that the writer is positive toward the
event curb since the event is bad for the object
health care costs which the writer expresses an ex-
plicit negative sentiment (skyrocketing). We can
reason from there that the writer is positive toward
the bill, since it is the agent of the positive event.
These implicature rules involve events that pos-
itively or negatively affect the object. Such events
are called malefactive and benefactive, or, for ease
of writing, goodFor (gf ) and badFor (bf ) (here-
after gfbf). The list of gfbf events and their polari-
ties (gf or bf) are necessary to develop a fully auto-
matic opinion inference system. On first thought,
one might think that we only need lists of gfbf
words. However, it turns out that gfbf terms may
be ambiguous ? a single word may have both gf
and bf meanings.
Thus, in this work, we take a sense-level ap-
proach to acquire gfbf lexicon knowledge, lead-
ing us to employ lexical resources with fine-
grained sense rather than word representations.
For that, we adopt an automatic bootstrapping
method which disambiguates gfbf polarity at the
sense-level utilizing WordNet, a widely-used lex-
ical resource. Starting from the seed set manually
generated from FrameNet, a rich lexicon in which
words are organized by semantic frames, we ex-
plore how gfbf terms are organized in WordNet via
semantic relations and expand the seed set based
on those semantic relations.
The expanded lexicon is evaluated in two ways.
First, the lexicon is evaluated against a corpus that
has been annotated with gfbf information at the
word level. Second, samples from the expanded
lexicon are manually annotated at the sense level,
which gives some idea of the prevalence of gfbf
lexical ambiguity and provides a basis for sense-
level evaluation. Also, we conduct the agreement
study. The results show that the expanded lexi-
con covers more than half of the gfbf instances
in the gfbf corpus, and the system?s accuracy, as
measured against the sense-level gold standard, is
substantially higher than baseline. In addition, in
the agreement study, the annotators achieve good
agreement, providing evidence that the annotation
task is feasible and that the concept of gfbf gives
us a natural coarse-grained grouping of senses.
107
2 The GFBF Corpus
A corpus of blogs and editorials about the Afford-
able Care Act, a controversial topic, was manu-
ally annotated with gfbf information by Deng et
al. (2013)
1
. This corpus provides annotated gfbf
events and the agents and objects of the events. It
consists of 134 blog posts and editorials. Because
the Affordable Health Care Act is a controversial
topic, the data is full of opinions. In this corpus,
1,411 gfbf instances are annotated, each including
a gfbf event, its agent, and its object (615 gf in-
stances and 796 bf instances). 196 different words
appear in gf instances and 286 different words ap-
pear in bf instances; 10 words appear in both.
3 Sense-Level GFBF Ambiguity
A word may have one or more meanings. For
that, we use WordNet
2
, which is a large lexical
database of English (Miller et al., 1990). In Word-
Net, nouns, verbs, adjectives, and adverbs are or-
ganized by semantic relations between meanings
(senses). We assume that a sense is exactly one
of gf, bf, or neither. Since words often have more
than one sense, the polarity of a word may or may
not be consistent, as the following WordNet exam-
ples show.
? A word with only gf senses: encourage
S1: (v) promote, advance, boost, further, en-
courage (contribute to the progress or growth
of)
S2: (v) encourage (inspire with confidence;
give hope or courage to)
S3: (v) encourage (spur on)
? A word with only bf senses: assault
S1: (v) assail, assault, set on, attack (attack
someone physically or emotionally)
S2: (v) rape, ravish, violate, assault, dis-
honor, dishonour, outrage (force (someone)
to have sex against their will)
S3: (v) attack, round, assail, lash out, snipe,
assault (attack in speech or writing)
All senses of encourage are good for the object,
and all senses of assault are bad for the object.
The polarity is always same regardless of sense.
In such cases, for our purposes, which particular
sense is being used does not need to be determined
because any instance of the word will be good for
1
Available at http://mpqa.cs.pitt.edu/corpora/gfbf/
2
WordNet, http://wordnet.princeton.edu/
(bad for); that is, word-level approaches can work
well. However, word-level approaches are not ap-
plicable for all the words. Consider the following:
? A word with gf and neutral senses: inspire
S3: (v) prompt, inspire, instigate (serve as the
inciting cause of)
S4: (v) cheer, root on, inspire, urge, barrack,
urge on, exhort, pep up (spur on or encourage
especially by cheers and shouts)
S6: (v) inhale, inspire, breathe in (draw in
(air))
? A word with bf and neutral senses: neutral-
ize
S2: (v) neutralize, neutralise, nullify, negate
(make ineffective by counterbalancing the ef-
fect of)
S6: (v) neutralize, neutralise (make chemi-
cally neutral)
The words inspire and neutralize both have 6
senses (we list a subset due to space limitations).
For inspire, while S3 and S4 are good for the ob-
ject, S6 doesn?t have any polarity, i.e., it is a neu-
tral (we don?t think of inhaling air as good for the
air). Also, while S2 of neutralize is bad for the
object, S6 is neutral (neutralizing a solution just
changes its pH). Thus, if word-level approaches
are applied using these words, some neutral in-
stances may be incorrectly classified as gf or bf
events.
? A word with gf and bf senses: fight
S2: (v) fight, oppose, fight back, fight down,
defend (fight against or resist strongly)
S4: (v) crusade, fight, press, campaign, push,
agitate (exert oneself continuously, vigor-
ously, or obtrusively to gain an end or engage
in a crusade for a certain cause or person; be
an advocate for)
As mentioned in Section 2, 10 words are ap-
peared in both gf and bf instances. Since only
words and not senses are annotated in the corpus,
such conflicts arise. These 10 words account for
9.07% (128 instances) of all annotated instances.
One example is fight. In the corpus instance fight
for a piece of legislation, fight is good for the ob-
ject, a piece of legislation. This is S4. However,
in the corpus instance we need to fight this repeal,
the meaning of fight here is S2, so fight is bad for
the object, this repeal.
108
Thesefore, approaches for determining the gfbf
polarity of an instance that are sense-level instead
of word-level promise to have higher precision.
4 Lexicon Acquisition
In this section, we develop a sense-level gfbf lex-
icon by exploiting WordNet. The method boot-
straps from a seed lexicon and iteratively follows
WordNet relations. We consider only verbs.
4.1 Seed Lexicon
To preserve the corpus for evaluation, we created
a seed set that is independent from the corpus. An
annotator who didn?t have access to the corpus
manually selected gfbf words from FrameNet
3
in
the light of semantic frames. The annotator found
592 gf words and 523 bf words. Decomposing
each word into its senses in WordNet, there are
1,525 gf senses and 1,154 bf senses. 83 words ex-
tracted from FrameNet overlap with gfbf instances
in the corpus. For independence, those words were
discarded. Among the senses of the remaining
words, we randomly choose 200 gf senses and 200
bf senses.
4.2 Expansion Method
In WordNet, verb senses are arranged into hier-
archies, that is, verb senses towards the bottom
of the trees express increasingly specific manners.
Thus, we can follow hypernym relations to more
general senses and troponym relations to more spe-
cific verb senses. Since the troponym relation
refers to a specific elaboration of a verb sense, we
hypothesized that troponyms of a synset tends to
have its same polarity (i.e., gf or bf). We only con-
sider the direct troponyms in a single iteration. Al-
though the hypernym is a more general term, we
hypothesized that direct hypernyms tend to have
the the same or neutral polarity, but not the oppo-
site polarity. Also, the verb groups are promising;
even though the coverage is incomplete, we expect
the verb groups to be the most helpful.
WordNet Similarity
4
, is a facility that provides a
variety of semantic similarity and relatedness mea-
sures based on information found in the Word-
Net lexical database. We choose Jiang&Conrath
(1997) (jcn) method which has been found to be
effective for such tasks by NLP researchers. When
two concetps aren?t related at all, it returns 0. The
3
FrameNet, https://framenet.icsi.berkeley.edu/fndrupal/
4
WN Similarity, http://wn-similarity.sourceforge.net/
more they are related, the higher the value is re-
tuned. We regarded words with similarity values
greater than 1.0 to be similar words.
Beginning with its seed set, each lexicon (gf and
bf) is expanded iteratively. On each iteration, for
each sense in the current lexicon, all of its direct
troponyms, direct hypernyms, and members of the
same verb group are extracted and added to the
lexicon for the next iteration. Similarity, for each
sense, all words with above-threshold jcn values
are added. For new senses that are extracted for
both the gf and bf lexicons, we ignore such senses,
since there is conflicting evidence (recall that we
assume a sense has only one polarity, even if a
word may have senses of different polarities).
4.3 Corpus Evaluation
In this section, we use the gfbf annotations in the
corpus as a gold standard. The annotations in the
corpus are at the word level. To use the annota-
tions as a sense-level gold standard, all the senses
of a word marked gf (bf) in the corpus are con-
sidered to be gf (bf). While this is not ideal, this
allows us to evaluate the lexicon against the only
corpus evidence available.
The 196 words that appear in gf instances in
the corpus have a total of 897 senses, and the 286
words that appear in bf instances have a total of
1,154 senses. Among them, 125 senses are con-
flicted: a sense of a word marked gf in the corpus
could be a member of the same synset as a sense
of a word marked bf in the corpus. For a more reli-
able gold-standard set, we ignored these conflicted
senses. Thus, the gold-standard set contains 772 gf
senses and 1,029 bf senses.
Table 1 shows the results after five iterations of
lexicon expansion. In total, the gf lexicon contains
4,157 senses and the bf lexicon contains 5,071
senses. The top half gives the results for the gf
lexicon and the bottom half gives the results for
the bf lexicon. In the table, gfOverlap means the
overlap between the senses in the lexicon in that
row and the gold-standard gf set, while bfOverlap
is the overlap between the senses in the lexicon in
that row and the gold-standard bf set. That is, of
the 772 senses in the gf gold standard, 449 (58%)
are in the gf expanded lexicon while 105 (14%)
are in the bf expanded lexicon.
Accuracy (Acc) for gf is calculated as #gfOver-
lap / (#gfOverlap + #bfOverlap) and bf is calcu-
lated as #bfOverlap / (#gfOverlap + #bfOverlap).
109
goodFor
#senses #gfOverlap #bfOverlap Acc
Total 4,157 449 176 0.72
WN Sim 1,073 134 75 0.64
Groups 242 69 24 0.74
Troponym 4,084 226 184 0.55
Hypernym 223 75 33 0.69
badFor
#senses #gfOverlap #bfOverlap Acc
Total 5,071 105 562 0.84
WN Sim 1,008 34 190 0.85
Groups 255 11 86 0.89
Troponym 4,258 66 375 0.85
Hypernym 286 16 77 0.83
Table 1: Results after lexicon expansion
Overall, accuracy is higher for the bf than the
gf lexicon. The results in the table are broken
down by semantic relation. Note that the individ-
ual counts do not sum to the totals because senses
of different words may actually be the same sense
in WordNet. The results for the bf lexicon are con-
sistently high over all semantic relations. The re-
sults for the gf lexicon are more mixed, but all re-
lations are valuable.
The WordNet Similarity is advantageous be-
cause it detects similar senses automatically, so
may provide coverage beyond the semantic rela-
tions coded in WordNet.
Overall, the verb group is the most informative
relation, as we suspected.
Although the gf-lexicon accuracy for the tro-
ponym relation is not high, it has the advantage
is that it yields the most number of senses. Its
lower accuracy doesn?t support our original hy-
pothesis. We first thought that verbs lower down in
the hierarchy would tend to have the same polar-
ity since they express specific manners character-
izing an event. However, this hypothesis is wrong.
Even though most troponyms have the same polar-
ity, there are many exceptions. For example, pro-
tect#v#1, which means the first sense of the verb
protect, has 18 direct troponyms such as cover
for#v#1, overprotect#v#2, and so on. protect#v#1
is a gf event because the meaning is ?shielding
from danger? and most troponyms are also gf
events. However, overprotect#v#2, which is one
of troponyms of protect#v#1, is a bf event.
For the hypernym relation, the number of de-
tected senses is not large because many were al-
ready detected in previous iterations (in general,
there are fewer nodes on each level as hypernym
links are traversed).
4.4 Sense Annotation Evaluation
For a more direct evaluation, two annotators, who
are co-authors, independently annotated a sample
of senses. We randomly selected 60 words among
the following classes: 10 pure gf words (i.e., all
senses of the words are classified by the expan-
sion method, and all senses are put into the gf lex-
icon), 10 pure bf words, 20 mixed words (i.e., all
senses of the words are classified by the expan-
sion method, and some senses are put into the gf
lexicon while others are put into the bf lexicon),
and 20 incomplete words (i.e., some senses of the
words are not classified by the expansion method).
The total number of senses is 151; 64 senses
are classified as gf, 56 senses are classified as bf,
and 31 senses are not classified. We included more
mixed than pure words to make the results of the
study more informative. Further, we wanted to in-
cluded non-classified senses as decoys for the an-
notators. The annotators only saw the sense en-
tries from WordNet. They didn?t know whether
the system classified a sense as gf or bf or whether
it didn?t classify it at all.
Table 2 evaluates the lexicons against the man-
ual annotations, and in comparison to the ma-
jority class baseline. The top half of the table
shows results when treating Anno1?s annotations
as the gold standard, and the bottom half shows
the results when treating Anno2?s as the gold stan-
dard. Among 151 senses, Anno1 annotated 56
senses (37%) as gf, 51 senses (34%) as bf, and
44 senses (29%) as neutral. Anno2 annotated 66
senses (44%) as gf, 55 senses (36%) as bf, and
30 (20%) senses as neutral. The incorrect cases
are divided into two sets: incorrect opposite con-
sists of senses that are classified as the opposite
polarity by the expansion method (e.g., the sense
is classified into gf, but annotator annotates it as
bf), and incorrect neutral consists of senses that
the expansion method classifies as gf or bf, but the
annotator marked it as neutral. We report the accu-
racy and the percentage of cases for each incorrect
case. The accuracies substantially improve over
baseline for both annotators and for both classes.
In Table 3, we break down the results into gfbf
classes. The gf accuracy measures the percentage
of correct gf senses out of all senses annotated as
gf according to the annotations (same as bf accu-
racy). As we can see, accuracy is higher for the
bf than the gf. The conclusion is consistent with
what we have discovered in Section 4.3.
110
By Anno1, 8 words are detected as mixed
words, that is, they contain both gf and bf senses.
By Anno2, 9 words are mixed words (this set in-
cludes the 8 mixed words of Anno1). Among
the randomly selected 60 words, the proportion of
mixed words range from 13.3% to 15%, according
to the two annotators. This shows that gfbf lexical
ambiguity does exist.
To measure agreement between the annotators,
we calculate two measures: percent agreement and
? (Artstein and Poesio, 2008). ? measures the
amount of agreement over what is expected by
chance, so it is a stricter measure. Percent agree-
ment is 0.84 and ? is 0.75.
accuracy % incorrect % incorrect base-
opposite neutral line
Anno1 0.53 0.16 0.32 0.37
Anno2 0.57 0.24 0.19 0.44
Table 2: Results against sense-annotated data
gf accuracy bf accuracy baseline
Anno1 0.74 0.83 0.37
Anno2 0.68 0.74 0.44
Table 3: Accuracy broken down for gfbf
5 Related Work
Lexicons are widely used in sentiment analysis
and opinion extraction. There are several previ-
ous works to acquire or expand sentiment lexi-
cons such as (Kim and Hovy, 2004), (Strapparava
and Valitutti, 2004), (Esuli and Sebastiani, 2006),
(Gyamfi et al., 2009), (Mohammad and Turney,
2010) and (Peng and Park, 2011). Such senti-
ment lexicons are helpful for detecting explicitly
stated opinions, but are not sufficient for recog-
nizing implicit opinions. Inferred opinions often
have opposite polarities from the explicit senti-
ment expressions in the sentence; explicit senti-
ments must be combined with benefactive, male-
factive state and event information to detect im-
plicit sentiments. There are few previous works
closest to ours. (Feng et al., 2011) build con-
notation lexicons that list words with connotative
polarity and connotative predicates. Goyal et al.
(2010) generate a lexicon of patient polarity verbs
that imparts positive or negative states on their pa-
tients. Riloff et al. (2013) learn a lexicon of nega-
tive situation phrases from a corpus of tweets with
hashtag ?sarcasm?.
Our work is complementary to theirs in that
their acquisition methods are corpus-based, while
we acquire knowledge from lexical resources.
Further, all of their lexicons are word level while
ours are sense level. Finally, the types of entries
among the lexicons are related but not the same.
Ours are specifically designed to support the au-
tomatic recognition of implicit sentiments in text
that are expressed via implicature.
6 Conclusion and Future Work
In this paper, we developed a sense-level gfbf
lexicon which was seeded by entries culled from
FrameNet and then expanded by exploiting se-
mantic relations in WordNet. Our evaluations
show that such lexical resources are promising for
expanding such sense-level lexicons. Even though
the seed set is completely independent from the
corpus, the expanded lexicon?s coverage of the
corpus is not small. The accuracy of the expanded
lexicon is substantially higher than baseline accu-
racy. Also, the results of the agreement study are
positive, providing evidence that the annotation
task is feasible and that the concept of gfbf gives
us a natural coarse-grained grouping of senses.
However, there is still room for improvement.
We believe that gf/bf judgements of word senses
could be effectively crowd-sourced; (Akkaya et
al., 2010), for example, effectively used Ama-
zon Mechanical Turk (AMT) for similar coarse-
grained judgements. The idea would be to use au-
tomatic expansion methods to create a sense-level
lexicon, and then have AMT workers judge the
entries in which we have least confidence. This
would be much more time- and cost-effective.
The seed sets we used are small - only 400 total
senses. We believe it will be worth the effort to
create larger seed sets, with the hope to mine many
additional gfbf senses from WordNet.
To exploit the lexicon to recognize sentiments in
a corpus, the word-sense ambiguity we discovered
needs to be addressed. There is evidence that the
performance of word-sense disambiguation sys-
tems using a similar coarse-grained sense inven-
tory is much better than when the full sense inven-
tory is used (Akkaya et al., 2009; Akkaya et al.,
2011). That, coupled with the fact that our study
suggests that many words are unambiguous with
respect to the gfbf distinction, makes us hopeful
that gfbf information may be practically exploited
to improve sentiment analysis in the future.
111
7 Acknowledgments
This work was supported in part by DARPA-BAA-
12-47 DEFT grant #12475008.
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2009. Subjectivity word sense disambiguation. In
Proceedings of EMNLP 2009, pages 190?199.
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon mechanical turk
for subjectivity word sense disambiguation. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Cre-
ating Speech and Language Data with Amazon?s
Mechanical Turk, pages 195?203.
Cem Akkaya, Janyce Wiebe, Alexander Conrad, and
Rada Mihalcea. 2011. Improving the impact of
subjectivity word sense disambiguation on contex-
tual opinion analysis. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning, pages 87?96.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput.
Linguist., 34(4):555?596, December.
Lingjia Deng and Janyce Wiebe. 2014. Sentiment
propagation via implicature constraints. In Proceed-
ings of EACL.
Lingjia Deng, Yoonjung Choi, and Janyce Wiebe.
2013. Benefactive/malefactive event and writer atti-
tude annotation. In Proceedings of 51st ACL, pages
120?125.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In Proceedings of 5th LREC, pages
417?422.
Song Feng, Ritwik Bose, and Yejin Choi. 2011. Learn-
ing general connotation of words using graph-based
algorithms. In Proceedings of EMNLP, pages 1092?
1103.
Amit Goyal, Ellen Riloff, and Hal DaumeIII. 2010.
Automatically producing plot unit representations
for narrative text. In Proceedings of EMNLP, pages
77?86.
Yaw Gyamfi, Janyce Wiebe, Rada Mihalcea, and Cem
Akkaya. 2009. Integrating knowledge for subjectiv-
ity sense labeling. In Proceedings of NAACL HLT
2009, pages 10?18.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. In Proceedings of COLING.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of 20th
COLING, pages 1367?1373.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan & Claypool.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990.
Wordnet: An on-line lexical database. International
Journal of Lexicography, 13(4):235?312.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: Us-
ing mechanical turk to create an emotion lexicon. In
Proceedings of the NAACL-HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1?135.
Wei Peng and Dae Hoon Park. 2011. Generate adjec-
tive sentiment dictionary for social media sentiment
analysis using constrained nonnegative matrix fac-
torization. In Proceedings of ICWSM.
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-
dra De Silva, Nathan Gilbert, and Ruihong Huang.
2013. Sarcasm as contrast between a positive sen-
timent and negative situation. In Proceedings of
EMNLP, pages 704?714.
Carlo Strapparava and Alessandro Valitutti. 2004.
Wordnet-affect: An affective extension of wordnet.
In Proceedings of 4th LREC, pages 1083?1086.
112
