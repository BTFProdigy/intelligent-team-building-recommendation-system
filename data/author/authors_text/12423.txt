Dimensionality Reduction with Multilingual Resource 
YingJu Xia                      Hao Yu                        Gang Zou 
Fujitsu Research & Development Center Co.,LTD. 
13F Tower A, Ocean International Center, No.56 Dong Si Huan Zhong Rd, Chaoyang District, 
Beijing, China, 100025 
{yjxia,yu,zougang}@cn.fujitsu.com 
 
 
Abstract 
Query and document representation is a 
key problem for information retrieval and 
filtering. The vector space model (VSM) 
has been widely used in this domain. But 
the VSM suffers from high dimensionality. 
The vectors built from documents always 
have high dimensionality and contain too 
much noise. In this paper, we present a 
novel method that reduces the dimensional-
ity using multilingual resource. We intro-
duce a new metric called TC to measure the 
term consistency constraints. We deduce a 
TC matrix from the multilingual corpus and 
then use this matrix together with the term-
by-document matrix to do the Latent Se-
mantic Indexing (LSI). By adopting differ-
ent TC threshold, we can truncate the TC 
matrix into small size and thus lower the 
computational cost of LSI. The experimen-
tal results show that this dimensionality re-
duction method improves the retrieval per-
formance significantly. 
1 Introduction 
1.1 Basic concepts 
The vast amount of electronic information that is 
available today requires effective techniques for 
accessing relevant information from it. The meth-
odologies developed in information retrieval aim at 
devising effective means to extract relevant docu-
ments in a collection when a user query is given. In 
information retrieval and filtering, Query and 
document representation is a key problem and 
many techniques have been developed. Among 
these techniques, the vector space model (VSM) 
proposed by Salton (1971; 1983) has been widely 
used. In the VSM, a document is represented by a 
vector of terms. The cosine of the angle between 
two document vectors indicates the similarity be-
tween the corresponding documents. A smaller 
angle corresponds to a larger cosine value and in-
dicates higher document similarity. A query, which 
describes the information need, is encoded as a 
vector as well. Retrieval of documents that satisfy 
the information need is achieved by finding the 
documents most similar to the query, or equiva-
lently, the document vectors closest to the query 
vector. There are several advantages to this ap-
proach beyond its mathematical simplicity. Above 
all, it is efficient to compute and store the word 
counts. This is one reason that why VSM is widely 
used for query and document representation. But 
this method has problem that the vectors built from 
documents always have high dimensionality and 
contain too much noise. The high dimensionality 
causes high computational and memory require-
ments while noise in the vectors degrades the sys-
tem performance. 
1.2 Related works 
To address these problems, many dimensionality 
reduction techniques have been applied to query 
and document representation. Among these tech-
niques, Latent Semantic Indexing (LSI) (Deer-
wester et al, 1990; Hofmann, 1999; Ding, 2000; 
Jiang and Littman, 2000; Ando, 2001; Kokiopou-
lou and Saad, 2004; Lee et al, 2006) is a well-
known approach. LSI constructs a smaller docu-
ment matrix that retains only the most important 
information from the original by using the Singular 
Value Decomposition (SVD). Many modifications 
have been made to this approach (Hofmann, 1999; 
Ding, 2000; Jiang and Littman, 2000; Kokiopoulou 
613
and Saad, 2004; Sun et al, 2004; Husbands et al, 
2005). Among them, IRR (Ando and Lee, 2001) is 
a subspace-projection method that counteracts ten-
dency to ignore minority-class documents. This is 
done by repeatedly rescaling vectors to amplify the 
presence of documents poorly represented in pre-
vious iterations. 
 In concept indexing (CI) (Karypis and Han, 
2000) method, the original set of documents is first 
clustered into k similar groups, and then for each 
group, the centroid vector (i.e., the vector obtained 
by averaging the documents in the group) is used 
as one of the k axes of the lower dimensional space. 
The key motivation behind this dimensionality re-
duction approach is the view that each centroid 
vector represents a concept present in the collec-
tion, and the lower dimensional representation ex-
presses each document as a function of these con-
cepts. George and Han (2000) extend concept in-
dexing in the context of supervised dimensionality 
reduction. To capture the concept, phrase also has 
been used as indexing entries (Mao and Chu, 2002).  
The LPI method (Isbell and Viola, 1999) tries to 
discover the local structure and obtains a compact 
document representation subspace that best detects 
the essential semantic structure. The LPI uses Lo-
cality Preserving Projections (LPP) (Xiaofei He 
and Partha, 2003) to learn a semantic space for 
document representation. Xiaofei He et al, (2004) 
try to get sets of highly-related words, queries and 
documents are represented by their distance to 
these sets. These algorithms have successfully re-
duced the dimensionality and improve the retrieval 
performance but at the mean time they led to a 
high computational complexity.  
1.3 Our method 
In this study, we propose a novel method that re-
duces the dimensionality using multilingual re-
source. We first introduce a new metric called TC 
to measure the term consistency constraints. We 
use this metric to deduce a TC matrix from the 
multilingual corpus. Then we combine this matrix 
to the term-by-document matrix and do the Latent 
Semantic Indexing. By adopting different TC 
threshold, we can truncate the TC matrix into small 
size and thus lower the computational cost of LSI.  
The remainder of this paper is organized as fol-
lows. Section 2 describes the dimensionality reduc-
tion method using multilingual resource. Section 3 
shows the experimental results to evaluate the di-
mensionality reduction method. Finally, we pro-
vide conclusions and remarks of future work in 
Section 4. 
2 Dimensionality reduction using multi-
lingual resource 
2.1 Motivation 
As mentioned above, the queries and documents 
are represented by vectors of terms. The weight of 
each term indicates its contribution to the vectors. 
Many weighting schemes have been proposed. The 
simplest form is to use the term-frequency (TF) as 
the term weight. In this condition, a document can 
be represented as a vector ),...,,( 21 ntftftfd =
r
, where 
is the frequency of the ith term in the document. 
A widely used refinement to this model is to 
weight each term based on its inverse document 
frequency (IDF) in the documents collection. This 
is commonly done by multiplying the frequency of 
each term i by , where N is the total 
number of documents in the collection, and is 
the number of documents that contain the ith term. 
This leads to the TF-IDF representation of the 
documents. Although the TF-IDF weighting 
scheme has many variants (Buckley, 1985; Berry 
et al, 1999; Robertson et al, 1999), the idea is the 
same one that uses the statistical information such 
as TF and IDF to calculate the term weight of 
vectors.  
itf
)/log( idfN
idf
This kind of statistical information is independ-
ence with languages. For example, in one language, 
say La, we have a vocabulary Va = {w1a, w2a, ?, 
wna} and a documents collection Da = {d1a, d2a,?, 
dma }. If this documents collection has a parallel 
corpus in language Lb, say, Db = {d1b, d2b,?, dmb } 
and a vocabulary Vb = {w1b, w2b, ?, wnb}. When 
we put a query Qka = {qk1a, qk2a ,?, qkla } (qkia ?Va) 
into an information retrieval system. The informa-
tion retrieval system will converts the query Qka 
and the documents in the collection Da into vectors. 
By calculating the similarity between query Qka 
and each document dia, the system selects the 
documents whose similarity is higher than a 
threshold as the results Rka. If we translate the 
query Qka into language Lb and get query Qkb, when 
putting the Qkb into the same information retrieval 
system, we get the retrieval results Rkb. Since the 
Qka and Qkb contain the same content and only ex-
pressed in different languages. We expect that Rka 
614
and Rkb will contain the same content. If this as-
sumption holds, the vocabulary which is used to 
build queries and documents vectors should have 
high representative ability. Since the weight of 
each term in the vector is calculated by the statisti-
cal information such as TF and IDF. If the vocabu-
lary Va and Vb have high representative ability, 
their statistical information will be consistent as 
well. This is the main motivation of our dimen-
sionality reduction method. 
2.2 Dimensionality reduction method 
The most straightforward way to measure the 
word?s representability in multilingual resource is 
to calculate the TF and IDF of each word in differ-
ent languages. But this method has one problem 
that the TF-IDF scheme is dedicated for each sin-
gle document, the same word will have different 
weight in different documents. It is impractical to 
impose the consistency constraint to every docu-
ment. Even we can do that, this method still has the 
drawback that it is very difficult to port to another 
documents collection. To address this problem, we 
consider the whole documents collection as one 
single document. In this condition, the IDF will be 
a fixed number. 
We introduce a new metric to measure the term 
consistency called TC. Figure 1 and Figure 2 illus-
trate the basic idea. In these figures, the curve La 
shows the word logarithmic frequency in the 
documents collection of language La, the curve Lb 
shows the corresponding translation?s logarithmic 
frequency in the documents collection of language 
Lb. TCi and TCj are the term consistency of wi and 
wj respectively.  
Figure 1 shows the TC in normal condition that 
the average word frequency in language a is proxi-
mate to that of language b. In this case, the TC is 
defined as below: 
))log(/)log(),log(/)min(log()( ai
b
i
b
i
a
i
b
i ffffwTC =       (1) 
Here fia  is the frequency of wia in language a. fib  
is the frequency of the wia?s translation in language 
b. In multilingual case, the TC(wi) will be defined 
as below: 
))(...),(min()( ni
b
ii wTCwTCwTC =   (
In the case that 
2) 
the average word frequency in 
lan
to calculate the TC of wi as below: 
 (3) 
Here H is distance between the moving average 
guage a is different with that of language b, we 
will first calculate the moving average as shown in 
the Figure 2. After that, we use the moving average 
)))/(log()log(),log(/))min((log()( HfffHfwTC ai
b
i
b
i
a
i
b
i ++=
     
and the original one. 
words
Fr
eq
ue
nc
y
Language a
Language b
w i w j
TC i
TC j
 
Figure 1. TC in normal condition 
words
Fr
eq
ue
nc
y
Language a
Language b
Moving average
w i w j
TC i
TC j
 
Figure 2. TC in shift condition 
Once we get the guage a, 
we present i diag(TC , 
?  
do = 
B
): 
TC of every word in lan
t in a diagonal matrix T =tt? 1
TC2, ? , TCt), TC1 ? TC2 ? ?  ? TCt. 
When applying the TC matrix tT in informa-
tion retrieval, we combine T  into the t
t
tt? erm-by-
cument matrix dtA ? . Where dtA ? [aij] and the 
aij is the weight of term i in ument j. We get a 
new matrix dtttdt ATB ??? = . Then following the 
classical LSI dt?  by a low-rank ap-
proximation derived from its truncated Singular 
Value Decomposition (SVD
T
ndnnntdt VUB ???? ?=  
Here IUUT = , IVV T = ,
doc
, we replace 
),...,,( 21 ndiag ???=?  
== 0...... 121? =??? + nr? r?? ? ? .  
ain pro m of LSI is that it usually led to
a high computational complexity sinc he matrix 
matri
The m ble  
e t
dt?  usually in 10B 3-105 dimensional space. To 
lower the computational cost, we truncate the TC 
x ttT ?  according to different TC threshold 
and get a new matrix ),...,,(? 21 ttt TCTCTCdiagT =? , 
0......21 1 ===???? r TCTCTCTCTC . Then 
??
+ tr
we get AT ??= . Since r is small than t, the drrrdrB ?
615
computational cost on the matrix wil
. Note that 
b). To  this one-to-many phe-
no
stem to evaluate 
eduction method presented in 
Section 2. The term weight in the term-by-
do
comes from Chinese Linguistic 
eseldc.org/
drB ?? l lower 
than tB ? the matrix drB ??  is deduced 
from the TC matrix ttT ?  which is sorted by word 
representative ability. It will contain less noise and 
outperform the original matrix dtA ? . The experi-
mental results have shown the effective of this 
method.  
For one word w
d
i
a in languag , there are al-
ways several translations in language L
e La
b, say (wi1b, 
wi2b,?, wik  handle
menon, we calculate the co-occurrence of wia 
and each translation and select the highest one as 
the translation of wia.  
3 Experiments 
We adopt a VSM based IR sy
the dimensionality r
cument matrix is calculated by the TF-IDF 
weighting scheme. 
3.1 Training and test corpora 
The training corpus 
Data Consortium (http://www.chin , ab-
?2004-863-
 (?2003-863-006?). It is 
a C
breviate as CLDC). Its code number is 
009?. This parallel corpus contains parallel texts in 
Chinese, English and Japanese. It is aligned to sen-
tence level. The sentence alignment is manually 
verified and the sampling examination shows the 
accuracy reaches 99.9%.  
The experiments are conducted on two test cor-
pora. The first one is the information retrieval test 
corpus gotten from CLDC
hinese IR corpus and contains 20 topics for test. 
Each topic has key words and description and nar-
rative. The second one is the Reuters 2001 data 
(http://about.reuters.com/researchandstandards/cor
pus/ ). This corpus is a collection of about 810,000 
Reuters English news stories from August 20, 1996 
to August 19, 1997. It was used by the TREC-10
Filtering Tracks (Robertson and Soboroff, 2002). 
In TREC-10, 84 Reuters categories were used to 
simulate user profiles.  
The evaluate measure is a version of van 
Rijsbergen(1979)?s F measure with ?=1(we de-
note it as F1). 
3.2 Experimental results 
The table1 and table2 show the experimental re-
sults conducted on Chinese and English test Cor-
pus respectively. In these tables, we compare our 
method with basic LSI and LPI (Xiaofei et.al, 
2004). In the table1, the ?C-E? means the TC ma-
trix gotten from Chinese-English training collec-
tion (deduced from the trilingual training corpus). 
The ?C-J? means that the TC matrix gotten from 
Chinese-Japanese training collection, and so force 
the ?C-E-J?. All the TC matrices have been normal-
ized to range from 0 to 1. The threshold ? is used 
to truncate the TC matrix into small size. Bigger ? 
corresponds to smaller truncated TC matrix. Note 
that here ? is discrete since for some ?, the size of 
truncated matrix is very similar. For example, 
when ? = 0.85 and ? = 0.9, the size of truncated TC 
matrices are the same one.  
LSI: 0.3785,  LPI: 0.405 
? C-E C-J C-E-J 
0.3 0.404 0.4014 0.4124 
0.4 0.4098 0.406 0.4185 
0.45 0.4159 0.4185 0.4226 
0.5 0.4204 0.4124 0.4105 
0.55 0.4061 0.4027 0.3997 
0.6 0.3913 0.3992 0.396 
0.8 0.3856 0.3867 0.3842 
0.85 0.3744 0.3754 0.3768 
Table1.F1 measure of Chinese test corpus 
LSI: 0.3416,  LPI: 0.3556 
? E-C E-J E-C-J 
0.3 0.356 0.3478 0.3578 
0.4 0.3578 0.3596 0.3702 
0.45 0.3698 0.3651 0.3734 
0.5 0.3636 0.3575 0.363  
0.55 0.3523 0.3564 0.3477 
0.6 0.3422 0.3448 0.3458 
0.8 0.3406 0.3397 0.3378 
0.85 0.3304 0.3261 0.3278 
Table2. F1 measure of English test corpus 
 
From the experimental results, we can see that 
our method make great enhancement to the basic 
LSI method. And our method also outperforms the 
LPI method in both test corpora. Comparing the 
performance on different training collection, we 
can find that the difference is subtle. In Chinese 
test corpus, the TC matrix gotten from C-E-J train-
ing collection get the best performance (F1=0.4226) 
at ?=0.45 while the C-E test collection get 0.4204 
616
at ?=0.5 and the C-J test collection get 0.4185 at 
?=0.45. For the English test corpus, the trilingual 
training collection also gets the best performance. 
But the difference between bilingual and trilingual 
training collection is also subtle (E-C-J: F1=0.3734, 
E-C: F1=0.3698, E-J: F1=0.3651). In the English 
test corpus, all the training collection get the best 
performance at ?=0.45.  
As mentioned before, the bigger ? means the 
smaller size of the truncated TC matrix. While 
small size of the truncated TC matrix means low 
computational cost and high system speed. This is 
one of the advantages of our method over the tradi-
tional LSI method. We conducted some experi-
ments to test the system speed on different thresh-
old ?. We use the number of documents per sec-
ond (docs/s) to denote this kind of system speed. 
The experiment is conducted on the personal com-
puter with a Pentium (R) 4 processor @2.8GHz, 
256 KB cache and 512 MB memory. Table 3 
shows the experimental results that the ? vs. sys-
tem speed and Figure 3 illustrates the F1 measure 
vs. the system speed.  
Baseline(LSI): 566.5 docs/s 
? C-E C-J C-E-J 
0.3 1039.3 1034.4 1355.0 
0.4 1148.4 1188.9 1372.5 
0.45 1290.5 1246.9 1391.3 
0.5 1323.9 1323.3 1469.6 
0.55 1393.3 1392.6 1563.8 
0.6 1413.3 1508.8 1590.1 
0.8 1513.1 1555.6 1660.5 
0.85 1641.1 1778.2 1773.5 
Table 3. ? vs. system speed 
0.37
0.38
0.39
0.4
0.41
0.42
0.43
1000 1200 1400 1600 1800
docs/s
F 
M
ea
su
re
C-E
C-J
C-E-J
 
Figure 3. F1 measure vs. system speed 
4 Conclusions 
In this paper, we present a novel method that re-
duces the dimensionality using multilingual re-
source. We deduce a TC matrix from the multilin-
gual corpus and then truncate it to small size ac-
cording to different TC threshold. Then we use the 
truncated matrix together with the term-by-
document matrix to do the LSI analysis. Since the 
truncated TC matrix is sorted by word representa-
tive ability. It will contain less noise than the origi-
nal term-by-document matrix. The experimental 
results have shown the effectiveness of this method. 
In the future, we will try to find the optimal 
truncate threshold ? automatically. And since it 
is more difficult to get the parallel corpora than 
comparable corpora, we will explore using com-
parable corpora to do the dimensionality reduc-
tion. 
Acknowledgement 
This research was carried out through financial 
support provided under the NEDO International 
Joint Research Grant Program (NEDO Grant). 
References 
Ando R. K., ?Latent Semantic Space: Iterative Scaling 
improves precision of inter-document similarity 
measurement?, in Proc. of the 23th International 
ACM SIGIR, Athens, Greece, 2000. 
Ando R. K., and Lee L., ?Iterative Residual Rescaling: 
An Analysis and Generalization of LSI?, in Proc. of 
the 24th International ACM SIGIR, New Orleans, 
LA, 2001. 
Arampatzis A., Beney J., Koster C.H.A., and T.P. van 
der Weide. KUN on the TREC9 Filtering Track: In-
crementality, decay, and theshold optimization for 
adaptive filtering systems. The ninth Text Retrieval 
Conference, November 9-12, 2000 Gaithersburg, MD, 
Avi Arampatzis and Andre van Hameren The Score-
Distributional Threshold Optimization for Adaptive 
Binary Classification Tasks , SIGIR?01, September 
9-12,2001, New Orleans, Louisiana,USA. 285-293 
Berry M., Drmac Z., and Jessup E.. Matrices, vector 
spaces, and information retrieval. SIAM Review, 
41(2):pp335-362, 1999. 
Bingham E. and Mannila H., ?Random Projection in 
dimensionality reduction: applications to image and 
text data?, Proc. Of the seventh ACM SIGKDD In-
ternational Conference on Knowledge Discovery and 
Data Mining, p. 245-250,2001. 
Buckley C.. Implementation of the SMART information 
retrieval system. Technical Report TR85-686, De-
partment of Computer Science, Cornell University, 
617
Ithaca, NY 14853, May 1985. Source code available 
at ftp://ftp.cs.cornell.edu/pub/smart. 
C.H. Lee, H.C. Yang, and S.M. Ma, ?A Novel Multi-
Language Text Categorization System Using Latent 
Semantic Indexing?, The First International Confer-
ence on Innovative Computing, Information and 
Control (ICICIC-06), Beijing, China, 2006. 
C.J. van Rijsbergen. Information Retrieval, chapter 7. 
Butterworths, 2 edition, 1979. 
Deerwester S. C., Dumais S. T., Landauer T. K., Furnas 
G. W., and harshman R. A., ?Indexing by Latent Se-
mantic Analysis?, Journal of the American Society of 
Information Science, 41(6):391-407, 1990. 
Ding C. H.. A probabilistic model for dimensionality 
reduction in information retrieval and filtering. In 
Proc. of 1st SIAM Computational Information Re-
trieval Workshop, October 2000. 
George Karypis, Eui-Hong (Sam) Han, Fast supervise d 
dimensionality reduction algorithm with applications 
to document categorization & retrieval ,Proceedings 
of the ninth international conference on Information 
and knowledge management, November 2000 
Hofmann T., ?Probabilistic Latent Semantic Indexing?, 
in Proc. of the 22th International ACM SIGIR, 
Berkeley, California, 1999. 
Husbands, P., Simon, H., and Ding, C. Term norm dis-
tribution and its effects on latent semantic indexing, 
Information Processing and Management: an Interna-
tional Journal, v.41 n.4, p.777-787, July 2005 
Isbell C. L. and Viola P., ?Restructuring Sparse High 
Dimensional Data for Effective Retrieval?, Advances 
in Neural Information Systems, 1999. 
Jiang F. and Littman M.L., Approximate dimension 
equalization in vector-based information retrieval. 
Proc. 17th Int'l Conf. Machine Learning, 2000. 
Karypis G. and Han E.H.. Concept indexing: A fast di-
mensionality reduction algorithm with applications to 
document retrieval & categorization. Technical Re-
port TR-00-016, Department of Computer Science, 
University of USA 
Kokiopoulou E., Saad Y., Polynomial filtering in latent 
semantic indexing for information re-
trieval ,Proceedings of the 27th annual international 
ACM SIGIR conference on Research and develop-
ment in information retrieval SIGIR '04,July 2004 
Mao W. and Chu W.W.. Free-text medical document 
retrieval via phrase-based vector space model. In 
Proceedings of AMIA Annual Symp 2002. 
Minnesota, Minneapolis, 2000. Available on the WWW 
at URL http://www.cs.umn.edu/~karypis. 
Robertson SE, Walker S, Beaulieu M,Okapi at TREC-7: 
automatic ad hoc, filtering, VLC and interactive 
track- Proceedings of the seventh Text Retrieval 
Conference, TREC-7, pp. 253-264 ,1999 
Robertson, S., & Soboroff, I., The TREC-10 Filtering 
track final report. Proceeding of the Tenth Text RE-
trieval Conference (TREC-10) pp. 26-37. National 
Institute of Standards and Technology, special publi-
cation 500-250., 2002 
Salton, G, the SMART Retrieval System ? Experiments 
in Automatic Document Processing. Prentice-Hall, 
Englewood. Cliffs, New Jersey,1971. 
Salton, G., Dynamic Information and Library process-
ing. Prentice-Hall, Englewood Cliffs, New Jer-
sey,1983. 
Salton, G and McGill. M.J., Introduction to Modern 
Information retrieval. McGraw Hill, New York,1983. 
Sun, J.T. , Chen , Z. , Zeng , H.J. , Lu, Y.C. , Shi, C.Y. 
and Ma, W.Y. ,?Supervised Latent Semantic Index-
ing for Document Categorization? , In Proceedings of 
the Fourth IEEE International Conference on Data 
Mining 2004 
Xiaofei He and Partha Niyogi, ?Locality Preserving 
Projections?,in Advances in Neural Information 
Processing Systems 16, Vancouver, Canada, 2003. 
Xiaofei He, Deng Cai, Haifeng Liu, Wei-Ying Ma, Lo-
cality preserving indexing for document representa-
tion, Proceedings of the 27th annual international 
ACM SIGIR conference on Research and develop-
ment in information retrieval SIGIR '04, July 2004 
Zhai C., Jansen P., Roma N., Stoica E., and Evans D.A.. 
Optimization in CLARIT adaptive filtering. In pro-
ceeding of the Eight Text Retrieval Conference 1999, 
253-258. 
Zhang Y., and Callan J.. Maximum likelihood Estima-
tion for Filtering Thresholds. SIGIR?01, September 
9-12,2001, New Orleans, Louisiana,USA. 294-302 
618
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 827?834,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Infrastructure for standardization of Asian language resources
Tokunaga Takenobu
Tokyo Inst. of Tech.
Virach Sornlertlamvanich
TCL, NICT
Thatsanee Charoenporn
TCL, NICT
Nicoletta Calzolari
ILC/CNR
Monica Monachini
ILC/CNR
Claudia Soria
ILC/CNR
Chu-Ren Huang
Academia Sinica
Xia YingJu
Fujitsu R&D Center
Yu Hao
Fujitsu R&D Center
Laurent Prevot
Academia Sinica
Shirai Kiyoaki
JAIST
Abstract
As an area of great linguistic and cul-
tural diversity, Asian language resources
have received much less attention than
their western counterparts. Creating a
common standard for Asian language re-
sources that is compatible with an interna-
tional standard has at least three strong ad-
vantages: to increase the competitive edge
of Asian countries, to bring Asian coun-
tries to closer to their western counter-
parts, and to bring more cohesion among
Asian countries. To achieve this goal, we
have launched a two year project to create
a common standard for Asian language re-
sources. The project is comprised of four
research items, (1) building a description
framework of lexical entries, (2) building
sample lexicons, (3) building an upper-
layer ontology and (4) evaluating the pro-
posed framework through an application.
This paper outlines the project in terms of
its aim and approach.
1 Introduction
There is a long history of creating a standard
for western language resources. The human
language technology (HLT) society in Europe
has been particularly zealous for the standardiza-
tion, making a series of attempts such as EA-
GLES1, PAROLE/SIMPLE (Lenci et al, 2000),
ISLE/MILE (Calzolari et al, 2003) and LIRICS2.
These continuous efforts has been crystallized as
activities in ISO-TC37/SC4 which aims to make
an international standard for language resources.
1http://www.ilc.cnr.it/Eagles96/home.html
2lirics.loria.fr/documents.html
(1) Description 
framework of lexical 
entries
(2) Sample lexicons
(4) Evaluation 
through application
(3) Upper layer 
ontologyrefinement
description classification
refinement
evaluationevaluation
Figure 1: Relations among research items
On the other hand, since Asia has great lin-
guistic and cultural diversity, Asian language re-
sources have received much less attention than
their western counterparts. Creating a common
standard for Asian language resources that is com-
patible with an international standard has at least
three strong advantages: to increase the competi-
tive edge of Asian countries, to bring Asian coun-
tries to closer to their western counterparts, and to
bring more cohesion among Asian countries.
To achieve this goal, we have launched a two
year project to create a common standard for
Asian language resources. The project is com-
prised of the following four research items.
(1) building a description framework of lexical
entries
(2) building sample lexicons
(3) building an upper-layer ontology
(4) evaluating the proposed framework through
an application
Figure 1 illustrates the relations among these re-
search items.
Our main aim is the research item (1), building
a description framework of lexical entries which
827
fits with as many Asian languages as possible, and
contributing to the ISO-TC37/SC4 activities. As
a starting point, we employ an existing descrip-
tion framework, the MILE framework (Bertagna
et al, 2004a), to describe several lexical entries of
several Asian languages. Through building sam-
ple lexicons (research item (2)), we will find prob-
lems of the existing framework, and extend it so
as to fit with Asian languages. In this extension,
we need to be careful in keeping consistency with
the existing framework. We start with Chinese,
Japanese and Thai as target Asian languages and
plan to expand the coverage of languages. The re-
search items (2) and (3) also comprise the similar
feedback loop. Through building sample lexicons,
we refine an upper-layer ontology. An application
built in the research item (4) is dedicated to evalu-
ating the proposed framework. We plan to build an
information retrieval system using a lexicon built
by extending the sample lexicon.
In what follows, section 2 briefly reviews the
MILE framework which is a basis of our de-
scription framework. Since the MILE framework
is originally designed for European languages, it
does not always fit with Asian languages. We ex-
emplify some of the problems in section 3 and sug-
gest some directions to solve them. We expect
that further problems will come into clear view
through building sample lexicons. Section 4 de-
scribes a criteria to choose lexical entries in sam-
ple lexicons. Section 5 describes an approach
to build an upper-layer ontology which can be
sharable among languages. Section 6 describes
an application through which we evaluate the pro-
posed framework.
2 The MILE framework for
interoperability of lexicons
The ISLE (International Standards for Language
Engineering) Computational Lexicon Working
Group has consensually defined the MILE (Mul-
tilingual ISLE Lexical Entry) as a standardized
infrastructure to develop multilingual lexical re-
sources for HLT applications, with particular at-
tention toMachine Translation (MT) and Crosslin-
gual Information Retrieval (CLIR) application
systems.
The MILE is a general architecture devised
for the encoding of multilingual lexical informa-
tion, a meta-entry acting as a common representa-
tional layer for multilingual lexicons, by allowing
integration and interoperability between different
monolingual lexicons3.
This formal and standardized framework to en-
code MILE-conformant lexical entries is provided
to lexicon and application developers by the over-
all MILE Lexical Model (MLM). As concerns
the horizontal organization, the MLM consists of
two independent, but interlinked primary compo-
nents, the monolingual and the multilingual mod-
ules. The monolingual component, on the vertical
dimension, is organized over three different repre-
sentational layers which allow to describe differ-
ent dimensions of lexical entries, namely the mor-
phological, syntactic and semantic layers. More-
over, an intermediate module allows to define
mechanisms of linkage and mapping between the
syntactic and semantic layers. Within each layer, a
basic linguistic information unit is identified; basic
units are separated but still interlinked each other
across the different layers.
Within each of the MLM layers, different types
of lexical object are distinguished :
? the MILE Lexical Classes (MLC) represent
the main building blocks which formalize
the basic lexical notions. They can be seen
as a set of structural elements organized in
a layered fashion: they constitute an on-
tology of lexical objects as an abstraction
over different lexical models and architec-
tures. These elements are the backbone of
the structural model. In the MLM a defini-
tion of the classes is provided together with
their attributes and the way they relate to each
other. Classes represent notions like Inflec-
tionalParadigm, SyntacticFunction, Syntac-
ticPhrase, Predicate, Argument,
? the MILE Data Categories (MDC) which
constitute the attributes and values to adorn
the structural classes and allow concrete en-
tries to be instantiated. MDC can belong to
a shared repository or be user-defined. ?NP?
and ?VP? are data category instances of the
class SyntacticPhrase, whereas and ?subj?
and ?obj? are data category instances of the
class SyntacticFunction.
? lexical operations, which are special lexical
entities allowing the user to define multilin-
3MILE is based on the experience derived from exist-
ing computational lexicons (e.g. LE-PAROLE, SIMPLE, Eu-
roWordNet, etc.).
828
gual conditions and perform operations on
lexical entries.
Originally, in order to meet expectations placed
upon lexicons as critical resources for content pro-
cessing in the Semantic Web, the MILE syntactic
and semantic lexical objects have been formalized
in RDF(S), thus providing a web-based means to
implement the MILE architecture and allowing for
encoding individual lexical entries as instances of
the model (Ide et al, 2003; Bertagna et al, 2004b).
In the framework of our project, by situating our
work in the context of W3C standards and relying
on standardized technologies underlying this com-
munity, the original RDF schema for ISLE lexi-
cal entries has been made compliant to OWL. The
whole data model has been formalized in OWL by
using Prote?ge? 3.2 beta and has been extended to
cover the morphological component as well (see
Figure 2). Prote?ge? 3.2 beta has been also used as
a tool to instantiate the lexical entries of our sam-
ple monolingual lexicons, thus ensuring adherence
to the model, encoding coherence and inter- and
intra-lexicon consistency.
3 Existing problems with the MILE
framework for Asian languages
In this section, we will explain some problematic
phenomena of Asian languages and discuss pos-
sible extensions of the MILE framework to solve
them.
Inflection The MILE provides the powerful
framework to describe the information about in-
flection. InflectedForm class is devoted to de-
scribe inflected forms of a word, while Inflec-
tionalParadigm to define general inflection rules.
However, there is no inflection in several Asian
languages, such as Chinese and Thai. For these
languages, we do not use the Inflected Form and
Inflectional Paradigm.
Classifier Many Asian languages, such as
Japanese, Chinese, Thai and Korean, do not dis-
tinguish singularity and plurality of nouns, but use
classifiers to denote the number of objects. The
followings are examples of classifiers of Japanese.
? inu
(dog)
ni
(two)
hiki
(CL)
? ? ? two dogs
? hon
(book)
go
(five)
satsu
(CL)
? ? ? five books
?CL? stands for a classifier. They always follow
cardinal numbers in Japanese. Note that differ-
ent classifiers are used for different nouns. In the
above examples, classifier ?hiki? is used to count
noun ?inu (dog)?, while ?satsu? for ?hon (book)?.
The classifier is determined based on the semantic
type of the noun.
In the Thai language, classifiers are used in var-
ious situations (Sornlertlamvanich et al, 1994).
The classifier plays an important role in construc-
tion with noun to express ordinal, pronoun, for in-
stance. The classifier phrase is syntactically gener-
ated according to a specific pattern. Here are some
usages of classifiers and their syntactic patterns.
? Enumeration
(Noun/Verb)-(cardinal number)-(CL)
e.g. nakrian
(student)
3 khon
(CL)
? ? ? three students
? Ordinal
(Noun)-(CL)-/thi:/-(cardinal number)
e.g. kaew
(glass)
bai
(CL)
thi: 4
(4th)
? ? ? the 4th glass
? Determination
(Noun)-(CL)-(Determiner)
e.g. kruangkhidlek
(calculator)
kruang
(CL)
nii
(this)
? ? ? this calculator
Classifiers could be dealt as a class of the part-
of-speech. However, since classifiers depend on
the semantic type of nouns, we need to refer to
semantic features in the morphological layer, and
vice versa. Some mechanism to link between fea-
tures beyond layers needs to be introduced into the
current MILE framework.
Orthographic variants Many Chinese words
have orthographic variants. For instance, the con-
cept of rising can be represented by either char-
acter variants of sheng1: ? or ?. However,
the free variants become non-free in certain com-
pound forms. For instance, only? allowed for?
? ?liter?, and only? is allowed for?? ?to sub-
lime?. The interaction of lemmas and orthographic
variations is not yet represented in MILE.
Reduplication as a derivational process In
some Asian languages, reduplication of words de-
rives another word, and the derived word often has
a different part-of-speech. Here are some exam-
ples of reduplication in Chinese. Man4 ? ?to be
slow? is a state verb, while a reduplicated form
829
Inflectional
Paradigm
Lexical Entry SyntacticUnit
Form Lemmatized Form Stem
Inflected Form
Combiner
Calculator Mrophfeat
Operation Argument
Morph
DataCats
0..*
0..* 0..*
0..*
0..*
0..1
0..*
0..*
1..*
<LemmatizedForm rdf:ID="LFstar">
  <hasInflectedForm>
    <InflectedForm rdf:ID="stars">
      <hasMorphoFeat>
<MorphoFeat rdf:ID="pl">
  <number rdf:datatype="http://www.w3c.org/
2001/ XMLSchema#string">
    plural
  </number>
</MorphoFeat>
      </hasMorphoFeat>
    </InflectedForm>
  </hasInflectedForm>
  <hasInflectedForm>
    <InflectedForm rdf:ID="star">
      <hasMorphoFeat>
<MorphoFeat rdf:ID="sg">
  <number rdf:datatype="http://www.w3c.org/
2001/ XMLSchema#string">
    singular
  </number>
</MorphoFeat>
      </hasMorphoFeat>
    </InflectedForm>
  </hasInflectedForm>
</LemmatiedForm>
Figure 2: Formalization of the morphological layer and excerpt of a sample RDF instantiation
man4-man4 ?? is an adverb. Another example
of reduplication involves verbal aspect. Kan4 ?
?to look? is an activity verb, while the reduplica-
tive form kan4-kan4 ??, refers to the tentative
aspect, introducing either stage-like sub-division
or the event or tentativeness of the action of the
agent. This morphological process is not provided
for in the current MILE standard.
There are also various usages of reduplication in
Thai. Some words reduplicate themselves to add a
specific aspect to the original meaning. The redu-
plication can be grouped into 3 types according to
the tonal sound change of the original word.
? Word reduplication without sound change
e.g. /dek-dek/ ? ? ? (N) children, (ADV) child-
ishly, (ADJ) childish
/sa:w-sa:w/ ? ? ? (N) women
? Word reduplication with high tone on the first
word
e.g. /dam4-dam/ ? ? ? (ADJ) extremely black
/bo:i4-bo:i/ ? ? ? (ADV) really often
? Triple word reduplication with high tone on
the second word
e.g. /dern-dern4-dern/ ?? (V) intensively walk
/norn-norn4-norn/??(V) intensively sleep
In fact, only the reduplication of the same sound
is accepted in the written text, and a special sym-
bol, namely /mai-yamok/ is attached to the origi-
nal word to represent the reduplication. The redu-
plication occurs in many parts-of-speech, such as
noun, verb, adverb, classifier, adjective, preposi-
tion. Furthermore, various aspects can be added
to the original meaning of the word by reduplica-
tion, such as pluralization, emphasis, generaliza-
tion, and so on. These aspects should be instanti-
ated as features.
Change of parts-of-speech by affixes Af-
fixes change parts-of-speech of words in
Thai (Charoenporn et al, 1997). There are
three prefixes changing the part-of-speech of the
original word, namely /ka:n/, /khwa:m/, /ya:ng/.
They are used in the following cases.
? Nominalization
/ka:n/ is used to prefix an action verb and
/khwa:m/ is used to prefix a state verb
in nominalization such as /ka:n-tham-nga:n/
(working), /khwa:m-suk/ (happiness).
? Adverbialization
An adverb can be derived by using /ya:ng/ to
prefix a state verb such as /ya:ng-di:/ (well).
Note that these prefixes are also words, and form
multi-word expressions with the original word.
This phenomenon is similar to derivation which
is not handled in the current MILE framework.
Derivation is traditionally considered as a different
phenomenon from inflection, and current MILE
focuses on inflection. The MILE framework is al-
ready being extended to treat such linguistic phe-
nomenon, since it is important to European lan-
guages as well. It would be handled in either the
morphological layer or syntactic layer.
830
Function Type Function types of predicates
(verbs, adjectives etc.) might be handled in a
partially different way for Japanese. In the syn-
tactic layer of the MILE framework, Function-
Type class is prepared to denote subcategorization
frames of predicates, and they have function types
such as ?subj? and ?obj?. For example, the verb
?eat? has two FunctionType data categories of
?subj? and ?obj?. Function types basically stand
for positions of case filler nouns. In Japanese,
cases are usually marked by postpositions and case
filler positions themselves do not provide much in-
formation on case marking. For example, both of
the following sentences mean the same, ?She eats
a pizza.?
? kanojo
(she)
ga
(NOM)
piza
(pizza)
wo
(ACC)
taberu
(eat)
? piza
(pizza)
wo
(ACC)
kanojo
(she)
ga
(NOM)
taberu
(eat)
?Ga? and ?wo? are postpositions which mark
nominative and accusative cases respectively.
Note that two case filler nouns ?she? and ?pizza?
can be exchanged. That is, the number of slots is
important, but their order is not.
For Japanese, we might use the set of post-
positions as values of FunctionType instead of
conventional function types such as ?subj? and
?obj?. It might be an user defined data category or
language dependent data category. Furthermore,
it is preferable to prepare the mapping between
Japanese postpositions and conventional function
types. This is interesting because it seems more
a terminological difference, but the model can be
applied also to Japanese.
4 Building sample lexicons
4.1 Swadesh list and basic lexicon
The issue involved in defining a basic lexicon for a
given language is more complicated than one may
think (Zhang et al, 2004). The naive approach of
simply taking the most frequent words in a lan-
guage is flawed in many ways. First, all frequency
counts are corpus-based and hence inherit the bias
of corpus sampling. For instance, since it is eas-
ier to sample written formal texts, words used pre-
dominantly in informal contexts are usually under-
represented. Second, frequency of content words
is topic-dependent and may vary from corpus to
corpus. Last, and most crucially, frequency of a
word does not correlate to its conceptual necessity,
which should be an important, if not only, criteria
for core lexicon. The definition of a cross-lingual
basic lexicon is even more complicated. The first
issue involves determination of cross-lingual lexi-
cal equivalencies. That is, how to determine that
word a (and not a?) in language A really is word b
in language B. The second issue involves the deter-
mination of what is a basic word in a multilingual
context. In this case, not even the frequency of-
fers an easy answer since lexical frequency may
vary greatly among different languages. The third
issue involves lexical gaps. That is, if there is a
word that meets all criteria of being a basic word
in language A, yet it does not exist in language D
(though it may exist in languages B, and C). Is this
word still qualified to be included in the multilin-
gual basic lexicon?
It is clear not all the above issues can be un-
equivocally solved with the time frame of our
project. Fortunately, there is an empirical core lex-
icon that we can adopt as a starting point. The
Swadesh list was proposed by the historical lin-
guist Morris Swadesh (Swadesh, 1952), and has
been widely used by field and historical linguists
for languages over the world. The Swadesh list
was first proposed as lexico-statistical metrics.
That is, these are words that can be reliably ex-
pected to occur in all historical languages and can
be used as the metrics for quantifying language
variations and language distance. The Swadesh
list is also widely used by field linguists when
they encounter a new language, since almost all
of these terms can be expected to occur in any
language. Note that the Swadesh list consists of
terms that embody human direct experience, with
culture-specific terms avoided. Swadesh started
with a 215 items list, before cutting back to 200
items and then to 100 items. A standard list of
207 items is arrived at by unifying the 200 items
list and the 100 items list. We take the 207 terms
from the Swadesh list as the core of our basic lex-
icon. Inclusion of the Swadesh list also gives us
the possibility of covering many Asian languages
in which we do not have the resources to make a
full and fully annotated lexicon. For some of these
languages, a Swadesh lexicon for reference is pro-
vided by a collaborator.
4.2 Aligning multilingual lexical entries
Since our goal is to build a multilingual sample
lexicon, it is required to align words in several
831
Asian languages. In this subsection, we propose
a simple method to align words in different lan-
guages. The basic idea for multilingual alignment
is an intermediary by English. That is, first we
prepare word pairs between English and other lan-
guages, then combine them together to make cor-
respondence among words in several languages.
The multilingual alignment method currently we
consider is as follows:
1. Preparing the set of frequent words of each
language
Suppose that {Jw
i
}, {Cw
i
}, {Tw
i
} is the
set of frequent words of Japanese, Chinese
and Thai, respectively. Now we try to con-
struct a multilingual lexicon for these three
languages, however, our multilingual align-
ment method can be easily extended to han-
dle more languages.
2. Obtaining English translations
A word Xw
i
is translated into a set of En-
glish words EXw
ij
by referring to the bilin-
gual dictionary, where X denotes one of our
languages, J , C or T . We can obtain map-
pings as in (1).
Jw
1
: EJw
11
, EJw
12
, ? ? ?
Jw
2
: EJw
21
, EJw
22
, ? ? ?
...
Cw
1
: ECw
11
, ECw
12
, ? ? ?
Cw
2
: ECw
21
, ECw
22
, ? ? ?
...
Tw
1
: ETw
11
, ETw
12
, ? ? ?
Tw
2
: ETw
21
, ETw
22
, ? ? ?
...
(1)
Notice that this procedure is automatically
done and ambiguities would be left at this
stage.
3. Generating new mapping
From mappings in (1), a new mapping is gen-
erated by inverting the key. That is, in the
new mapping, a key is an English word Ew
i
and a correspondence for each key is sets
of translations XEw
ij
for 3 languages, as
shown in (2):
Ew
1
: (JEw
11
, JEw
12
, ? ? ?)
(CEw
11
, CEw
12
, ? ? ?)
(TEw
11
, TEw
12
, ? ? ?)
Ew
2
: (JEw
21
, JEw
22
, ? ? ?)
(CEw
21
, CEw
22
, ? ? ?)
(TEw
21
, TEw
22
, ? ? ?)
...
(2)
Notice that at this stage, correspondence be-
tween different languages is very loose, since
they are aligned on the basis of sharing only
a single English word.
4. Refinement of alignment
Groups of English words are constructed by
referring to the WordNet synset information.
For example, suppose that Ew
i
and Ew
j
be-
long to the same synset S
k
. We will make a
new alignment by making an intersection of
{XEw
i
} and {XEw
j
} as shown in (3).
Ew
i
: (JEw
i1
, ??) (CEw
i1
, ??) (TEw
i1
, ??)
Ew
j
: (JEw
j1
, ??)(CEw
j1
, ??)(TEw
j1
, ??)
? intersection
S
k
: (JEw?
k1
, ??)(CEw?
k1
, ??)(TEw?
k1
, ??)
(3)
In (3), the key is a synset S
k
, which is sup-
posed to be a conjunction of Ew
i
and Ew
j
,
and the counterpart is the intersection of set
of translations for each language. This oper-
ation would reduce the number of words of
each language. That means, we can expect
that the correspondence among words of dif-
ferent languages becomes more precise. This
new word alignment based on a synset is a
final result.
To evaluate the performance of this method,
we conducted a preliminary experiment using the
Swadesh list. Given the Swadesh list of Chi-
nese, Italian, Japanese and Thai as a gold stan-
dard, we tried to replicate these lists from the En-
glish Swadesh list and bilingual dictionaries be-
tween English and these languages. In this experi-
ment, we did not perform the refinement step with
WordNet. From 207 words in the Swadesh list,
we dropped 4 words (?at?, ?in?, ?with? and ?and?)
due to their too many ambiguities in translation.
As a result, we obtained 181 word groups
aligned across 5 languages (Chinese, English, Ital-
ian, Japanese and Thai) for 203 words. An
aligned word group was judged ?correct? when the
words of each language include only words in the
Swadesh list of that language. It was judged ?par-
tially correct? when the words of a language also
include the words which are not in the Swadesh
list. Based on the correct instances, we obtain
0.497 for precision and 0.443 for recall. These fig-
ures go up to 0.912 for precision and 0.813 for re-
call when based on the partially correct instances.
This is quite a promising result.
832
5 Upper-layer ontology
The empirical success of the Swadesh list poses
an interesting question that has not been explored
before. That is, does the Swadesh list instantiates a
shared, fundamental human conceptual structure?
And if there is such as a structure, can we discover
it?
In the project these fundamental issues are as-
sociated with our quest for cross-lingual interop-
erability. We must make sure that the items of
the basic lexicon are given the same interpreta-
tion. One measure taken to ensure this consists in
constructing an upper-ontology based on the ba-
sic lexicon. Our preliminary work of mapping the
Swadesh list items to SUMO (Suggested Upper
Merged Ontology) (Niles and Pease, 2001) has al-
ready been completed. We are in the process of
mapping the list to DOLCE (Descriptive Ontology
for Linguistic and Cognitive Engineering) (Ma-
solo et al, 2003). After the initial mapping, we
carry on the work to restructure the mapped nodes
to form a genuine conceptual ontology based on
the language universal basic lexical items. How-
ever one important observation that we have made
so far is that the success of the Swadesh list is
partly due to its underspecification and to the lib-
erty it gives to compilers of the list in a new lan-
guage. If this idea of underspecification is essen-
tial for basic lexicon for human languages, then we
must resolve this apparent dilemma of specifying
them in a formal ontology that requires fully spec-
ified categories. For the time being, genuine ambi-
guities resulted in the introduction of each disam-
biguated sense in the ontology. We are currently
investigating another solution that allows the in-
clusion of underspecified elements in the ontology
without threatening its coherence. More specifi-
cally we introduce a underspecified relation in the
structure for linking the underspecified meaning
to the different specified meaning. The specified
meanings are included in the taxonomic hierarchy
in a traditional manner, while a hierarchy of un-
derspecified meanings can be derived thanks to the
new relation. An underspecified node only inherits
from the most specific common mother of its fully
specified terms. Such distinction avoids the clas-
sical misuse of the subsumption relation for rep-
resenting multiple meanings. This method does
not reflect a dubious collapse of the linguistic and
conceptual levels but the treatment of such under-
specifications as truly conceptual. Moreover we
Internet
Query
Local 
DB
User interest
 model
Topic
Feedback
Search
engine
Crawler
Retrieval
results
Figure 3: The system architecture
hope this proposal will provide a knowledge rep-
resentation framework for the multilingual align-
ment method presented in the previous section.
Finally, our ontology will not only play the role
of a structured interlingual index. It will also serve
as a common conceptual base for lexical expan-
sion, as well as for comparative studies of the lex-
ical differences of different languages.
6 Evaluation through an application
To evaluate the proposed framework, we are build-
ing an information retrieval system. Figure 3
shows the system architecture.
A user can input a topic to retrieve the docu-
ments related to that topic. A topic can consist
of keywords, website URL?s and documents which
describe the topic. From the topic information, the
system builds a user interest model. The system
then uses a search engine and a crawler to search
for information related to this topic in WWW and
stores the results in the local database. Generally,
the search results include many noises. To filter
out these noises, we build a query from the user
interest model and then use this query to retrieve
documents in the local database. Those documents
similar to the query are considered as more related
to the topic and the user?s interest, and are returned
to the user. When the user obtains these retrieval
results, he can evaluate these documents and give
the feedback to the system, which is used for the
further refinement of the user interest model.
Language resources can contribute to improv-
ing the system performance in various ways.
Query expansion is a well-known technique which
expands user?s query terms into a set of similar and
related terms by referring to ontologies. Our sys-
tem is based on the vector space model (VSM) and
traditional query expansion can be applicable us-
ing the ontology.
There has been less research on using lexical in-
833
formation for information retrieval systems. One
possibility we are considering is query expansion
by using predicate-argument structures of terms.
Suppose a user inputs two keywords, ?hockey?
and ?ticket? as a query. The conventional query
expansion technique expands these keywords to
a set of similar words based on an ontology. By
referring to predicate-argument structures in the
lexicon, we can derive actions and events as well
which take these words as arguments. In the above
example, by referring to the predicate-argument
structure of ?buy? or ?sell?, and knowing that
these verbs can take ?ticket? in their object role,
we can add ?buy? and ?sell? to the user?s query.
This new type of expansion requires rich lexical
information such as predicate argument structures,
and the information retrieval system would be a
good touchstone of the lexical information.
7 Concluding remarks
This paper outlined a new project for creating a
common standard for Asian language resources
in cooperation with other initiatives. We start
with three Asian languages, Chinese, Japanese
and Thai, on top of the existing framework which
was designed mainly for European languages.
We plan to distribute our draft to HLT soci-
eties of other Asian languages, requesting for
their feedback through various networks, such
as the Asian language resource committee net-
work under Asian Federation of Natural Language
Processing (AFNLP)4, and Asian Language Re-
source Network project5. We believe our ef-
forts contribute to international activities like ISO-
TC37/SC46 (Francopoulo et al, 2006) and to the
revision of the ISO Data Category Registry (ISO
12620), making it possible to come close to the
ideal international standard of language resources.
Acknowledgment
This research was carried out through financial
support provided under the NEDO International
Joint Research Grant Program (NEDO Grant).
References
F. Bertagna, A. Lenci, M. Monachini, and N. Calzo-
lari. 2004a. Content interoperability of lexical re-
sources, open issues and ?MILE? perspectives. In
4http://www.afnlp.org/
5http://www.language-resource.net/
6http://www.tc37sc4.org/
Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC2004),
pages 131?134.
F. Bertagna, A. Lenci, M. Monachini, and N. Calzo-
lari. 2004b. The MILE lexical classes: Data cat-
egories for content interoperability among lexicons.
In A Registry of Linguistic Data Categories within
an Integrated Language Resources Repository Area
? LREC2004 Satellite Workshop, page 8.
N. Calzolari, F. Bertagna, A. Lenci, and M. Mona-
chini. 2003. Standards and best practice for mul-
tilingual computational lexicons. MILE (the mul-
tilingual ISLE lexical entry). ISLE Deliverable
D2.2&3.2.
T. Charoenporn, V. Sornlertlamvanich, and H. Isahara.
1997. Building a large Thai text corpus ? part-
of-speech tagged corpus: ORCHID?. In Proceed-
ings of the Natural LanguageProcessing Pacific Rim
Symposium.
G. Francopoulo, G. Monte, N. Calzolari, M. Mona-
chini, N. Bel, M. Pet, and C. Soria. 2006. Lex-
ical markup framework (LMF). In Proceedings of
LREC2006 (forthcoming).
N. Ide, A. Lenci, and N. Calzolari. 2003. RDF in-
stantiation of ISLE/MILE lexical entries. In Pro-
ceedings of the ACL 2003 Workshop on Linguistic
Annotation: Getting the Model Right, pages 25?34.
A. Lenci, N. Bel, F. Busa, N. Calzolari, E. Gola,
M. Monachini, A. Ogonowsky, I. Peters, W. Peters,
N. Ruimy, M. Villegas, and A. Zampolli. 2000.
SIMPLE: A general framework for the development
of multilingual lexicons. International Journal of
Lexicography, Special Issue, Dictionaries, Thesauri
and Lexical-Semantic Relations, XIII(4):249?263.
C. Masolo, A. Borgo, S.; Gangemi, N. Guarino, and
A. Oltramari. 2003. Wonderweb deliverable d18
?ontology library (final)?. Technical report, Labo-
ratory for Applied Ontology, ISTC-CNR.
I. Niles and A Pease. 2001. Towards a standard upper
ontology. In Proceedings of the 2nd International
Conference on Formal Ontology in Information Sys-
tems (FOIS-2001).
V. Sornlertlamvanich, W. Pantachat, and S. Mek-
navin. 1994. Classifier assignment by corpus-
based approach. In Proceedings of the 15th Inter-
national Conference on Computational Linguistics
(COLING-94), pages 556?561.
M. Swadesh. 1952. Lexico-statistical dating of pre-
historic ethnic contacts: With special reference to
north American Indians and Eskimos. In Proceed-
ings of the American Philo-sophical Society, vol-
ume 96, pages 452?463.
H. Zhang, C. Huang, and S. Yu. 2004. Distributional
consistency: A general method for defining a core
lexicon. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC2004), pages 1119?1222.
834
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 145?152,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Query Expansion using LMF-Compliant Lexical Resources
Tokunaga Takenobu
Tokyo Inst. of Tech.
Dain Kaplan
Tokyo Inst. of Tech.
Nicoletta Calzolari
ILC/CNR
Monica Monachini
ILC/CNR
Claudia Soria
ILC/CNR
Virach Sornlertlamvanich
TCL, NICT
Thatsanee Charoenporn
TCL, NICT
Xia Yingju
Fujitsu R&D Center
Chu-Ren Huang
The Hong Kong Polytec. Univ.
Shu-Kai Hsieh
National Taiwan Normal Univ.
Shirai Kiyoaki
JAIST
Abstract
This paper reports prototype multilin-
gual query expansion system relying on
LMF compliant lexical resources. The
system is one of the deliverables of a
three-year project aiming at establish-
ing an international standard for language
resources which is applicable to Asian
languages. Our important contributions
to ISO 24613, standard Lexical Markup
Framework (LMF) include its robustness
to deal with Asian languages, and its ap-
plicability to cross-lingual query tasks, as
illustrated by the prototype introduced in
this paper.
1 Introduction
During the last two decades corpus-based ap-
proaches have come to the forefront of NLP re-
search. Since without corpora there can be no
corpus-based research, the creation of such lan-
guage resources has also necessarily advanced
as well, in a mutually beneficial synergetic re-
lationship. One of the advantages of corpus-
based approaches is that the techniques used
are less language specific than classical rule-
based approaches where a human analyses the
behaviour of target languages and constructs
rules manually. This naturally led the way
for international resource standardisation, and in-
deed there is a long standing precedent in the
West for it. The Human Language Technol-
ogy (HLT) society in Europe has been particu-
larly zealous in this regard, propelling the cre-
ation of resource interoperability through a se-
ries of initiatives, namely EAGLES (Sanfilippo et
al., 1999), PAROLE/SIMPLE (Lenci et al, 2000),
ISLE/MILE (Ide et al, 2003), and LIRICS1. These
1http://lirics.loria.fr/
continuous efforts have matured into activities in
ISO-TC37/SC42, which aims at making an inter-
national standard for language resources.
However, due to the great diversity of languages
themselves and the differing degree of technolog-
ical development for each, Asian languages, have
received less attention for creating resources than
their Western counterparts. Thus, it has yet to be
determined if corpus-based techniques developed
for well-computerised languages are applicable on
a broader scale to all languages. In order to effi-
ciently develop Asian language resources, utilis-
ing an international standard in this creation has
substantial merits.
We launched a three-year project to create an
international standard for language resources that
includes Asian languages. We took the following
approach in seeking this goal.
? Based on existing description frameworks,
each research member tries to describe sev-
eral lexical entries and find problems with
them.
? Through periodical meetings, we exchange
information about problems found and gen-
eralise them to propose solutions.
? Through an implementation of an application
system, we verify the effectiveness of the pro-
posed framework.
Below we summarise our significant contribution
to an International Standard (ISO24613; Lexical
Markup Framework: LMF).
1st year After considering many characteristics
of Asian languages, we elucidated the shortcom-
ings of the LMF draft (ISO24613 Rev.9). The
draft lacks the following devices for Asian lan-
guages.
2http://www.tc37sc4.org/
145
(1) A mapping mechanism between syntactic
and semantic arguments
(2) Derivation (including reduplication)
(3) Classifiers
(4) Orthography
(5) Honorifics
Among these, we proposed solutions for (1) and
(2) to the ISO-TC37 SC4 working group.
2nd year We proposed solutions for above the
(2), (3) and (4) in the comments of the Committee
Draft (ISO24613 Rev. 13) to the ISO-TC37 SC4
working group. Our proposal was included in DIS
(Draft International Standard).
(2?) a package for derivational morphology
(3?) the syntax-semantic interface resolving the
problem of classifiers
(4?) representational issues with the richness of
writing systems in Asian languages
3rd year Since ISO 24613 was in the FDIS stage
and fairly stable, we built sample lexicons in Chi-
nese, English, Italian, Japanese, and Thai based
on ISO24613. At the same time, we implemented
a query expansion system utilising rich linguis-
tic resources including lexicons described in the
ISO 24613 framework. We confirmed that a sys-
tem was feasible which worked on the tested lan-
guages (including both Western and Asian lan-
guages) when given lexicons compliant with the
framework. ISO 24613 (LMF) was approved by
the October 2008 ballot and published as ISO-
24613:2008 on 17th November 2008.
Since we have already reported our first 2 year
activities elsewhere (Tokunaga and others, 2006;
Tokunaga and others, 2008), we focus on the
above query expansion system in this paper.
2 Query expansion using
LMF-compliant lexical resources
We evaluated the effectiveness of LMF on a mul-
tilingual information retrieval system, particularly
the effectiveness for linguistically motivated query
expansion.
The linguistically motivated query expansion
system aims to refine a user?s query by exploiting
the richer information contained within a lexicon
described using the adapted LMF framework. Our
lexicons are completely complaint with this inter-
national standard. For example, a user inputs a
keyword ?ticket? as a query. Conventional query
expansion techniques expand this keyword to a
set of related words by using thesauri or ontolo-
gies (Baeza-Yates and Ribeiro-Neto, 1999). Using
the framework proposed by this project, expand-
ing the user?s query becomes a matter of following
links within the lexicon, from the source lexical
entry or entries through predicate-argument struc-
tures to all relevant entries (Figure 1). We focus
on expanding the user inputted list of nouns to rel-
evant verbs, but the reverse would also be possible
using the same technique and the same lexicon.
This link between entries is established through
the semantic type of a given sense within a lexical
entry. These semantic types are defined by higher-
level ontologies, such as MILO or SIMPLE (Lenci
et al, 2000) and are used in semantic predicates
that take such semantic types as a restriction ar-
gument. Since senses for verbs contain a link to
a semantic predicate, using this semantic type, the
system can then find any/all entries within the lexi-
con that have this semantic type as the value of the
restriction feature of a semantic predicate for any
of their senses. As a concrete example, let us con-
tinue using the ?ticket? scenario from above. The
lexical entry for ?ticket? might contain a semantic
type definition something like in Figure 2.
<LexicalEntry ...>
<feat att="POS" val="N"/>
<Lemma>
<feat att="writtenForm"
val="ticket"/>
</Lemma>
<Sense ...>
<feat att="semanticType"
val="ARTIFACT"/>
...
</Sense>
...
</LexicalEntry>
Figure 2: Lexical entry for ?ticket?
By referring to the lexicon, we can then derive
any actions and events that take the semantic type
?ARTIFACT? as an argument.
First all semantic predicates are searched for ar-
guments that have an appropriate restriction, in
this case ?ARTIFACT? as shown in Figure 3, and
then any lexical entries that refer to these predi-
cates are returned. An equally similar definition
would exist for ?buy?, ?find? and so on. Thus,
by referring to the predicate-argument structure of
related verbs, we know that these verbs can take
146
<LexicalEntry ...>
  <feat att="POS" val="Noun"/>
  <Lemma>
    <feat att="writtenForm" val="ticket"/>
  </Lemma>
  <Sense ...>
    <feat att="semanticType" val="ARTIFACT"/>
    ...
  </Sense>
  ...
</LexicalEntry>
User Inputs
ticket
<Sense>
<SemanticFeature>
Semantic Features of type 
"restriction" that take 
Sense's semanticType
All senses for 
matched nouns
<SemanticPredicate 
  id="pred-sell-1">
  <SemanticArgument>
    <feat att="label" val="X"/>
    <feat att="semanticRole" val="Agent"/>
    <feat att="restriction" val="Human"/>
  </SemanticArgument>
  ...
  <SemanticArgument>
    <feat att="label" val="Z"/>
    <feat att="semanticRole" val="Patient"/>
    <feat att="restriction" 
          val="ARTIFACT,LOCATION"/>
  </SemanticArgument>
</SemanticPredicate>
All Semantic Predicates 
that contain matched 
Semantic Features
<Sense>
Senses that use matched 
Semantic Predicates
<LexicalEntry ...>
  <feat att="POS" val="Verb"/>
  <Lemma>
    <feat att="writtenForm" val="sell"/>
  </Lemma>
  <Sense id="sell-1" ...>
    ...
    <PredicativeRepresentation
      predicate="pred-sell-1" ...>
  </Sense>
</LexicalEntry>
<LexicalEntry>
<SemanticPredicate>
<LexicalEntry>
System outputs
"sell", ...
For each <Sense> find all 
<SemanticArgument> that 
take this semanticType as 
a feature of type 
"restriction"
Find all verbs <LexicalEntry> 
that use these 
<SemanticPredicate>
All verbs that have 
matched Senses
Figure 1: QE Process Flow
147
<LexicalEntry ...>
<feat att="POS" val="V"/>
<Lemma>
<feat att="writtenForm"
val="sell"/>
</Lemma>
<Sense id="sell-1" ...>
<feat att="semanticType"
val="Transaction"/>
<PredicativeRepresentation
predicate="pred-sell-1"
correspondences="map-sell1">
</Sense>
</LexicalEntry>
<SemanticPredicate id="pred-sell-1">
<SemanticArgument ...>
...
<feat att="restriction"
val="ARTIFACT"/>
</SemanticArgument>
</SemanticPredicate>
Figure 3: Lexical entry for ?sell? with its semantic
predicate
?ticket? in the role of object. The system then re-
turns all relevant entries, here ?buy?, ?sell? and
?find?, in response to the user?s query. Figure 1
schematically shows this flow.
3 A prototype system in detail
3.1 Overview
To test the efficacy of the LMF-compliant lexi-
cal resources, we created a system implementing
the query expansion mechanism explained above.
The system was developed in Java for its ?com-
pile once, run anywhere? portability and its high-
availability of reusable off-the-shelf components.
On top of Java 5, the system was developed us-
ing JBoss Application Server 4.2.3, the latest stan-
dard, stable version of the product at the time of
development. To provide fast access times, and
easy traversal of relational data, a RDB was used.
The most popular free open-source database was
selected, MySQL, to store all lexicons imported
into the system, and the system was accessed, as a
web-application, via any web browser.
3.2 Database
The finalised database schema is shown in Fig-
ure 4. It describes the relationships between en-
tities, and more or less mirrors the classes found
within the adapted LMF framework, with mostly
only minor exceptions where it was efficacious for
querying the data. Due to space constraints, meta-
data fields, such as creation time-stamps have been
left out of this diagram. Since the system also al-
lows for multiple lexicons to co-exist, a lexicon id
resides in every table. This foreign key has been
highlighted in a different color, but not connected
via arrows to make the diagram easier to read. In
addition, though in actuality this foreign key is not
required for all tables, it has been inserted as a con-
venience for querying data more efficiently, even
within join tables (indicated in blue). Having mul-
tiple lexical resources co-existing within the same
database allows for several advantageous features,
and will be described later. Some tables also con-
tain a text id, which stores the original id attribute
for that element found within the XML. This is
not used in the system itself, and is stored only for
reference.
3.3 System design
As mentioned above, the application is deployed
to JBoss AS as an ear-file. The system it-
self is composed of java classes encapsulating
the data contained within the database, a Pars-
ing/Importing class for handling the LMF XML
files after they have been validated, and JSPs,
which contain HTML, for displaying the inter-
face to the user. There are three main sections
to the application: Search, Browse, and Config-
ure. Explaining last to first, the Configure section,
shown in Figure 5, allows users to create a new
lexicon within the system or append to an exist-
ing lexicon by uploading a LMF XML file from
their web browser, or delete existing lexicons that
are no longer needed/used. After import, the data
may be immediately queried upon with no other
changes to system configuration, from within both
the Browse and Search sections. Regardless of
language, the rich syntactic/semantic information
contained within the lexicon is sufficient for car-
rying out query expansion on its own.
The Browse section (Figure 6) allows the user to
select any available lexicon to see the relationships
contained within it, which contains tabs for view-
ing all noun to verb connections, a list of nouns, a
list of verbs, and a list of semantic types. Each has
appropriate links allowing the user to easily jump
to a different tab of the system. Clicking on a noun
takes them to the Search section (Figure 7). In this
section, the user may select many lexicons to per-
form query extraction on, as is visible in Figure 7.
148
semantic_link 
VARCHAR (64)
sense
sense_id
PRIMARY KEY
synset_id
FOREIGN KEY
syn_sem_correspondence_id
FOREIGN KEY
semantic_predicate_id
FOREIGN KEY
semantic_type
VARCHAR (64)
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
lexicon_id
FOREIGN KEY
text_id
VARCHAR (100)
semantic_predicate_id
PRIMARY KEY
semantic_predicate
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
semantic_argument_id
PRIMARY KEY
semantic_argument
value
VARCHAR (100)
attribute
VARCHAR (100)
lexicon_id
FOREIGN KEY
semantic_feature_id
PRIMARY KEY
semantic_feature
lexicon_id
FOREIGN KEY
semantic_argument_id
FOREIGN KEY
semantic_predicate_id
FOREIGN KEY
semantic_predicate_to_argument
lexicon_id
FOREIGN KEY
semantic_feature_id
FOREIGN KEY
semantic_argument_id 
FOREIGN KEY
semantic_argument_to_feature
description
TEXT
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
synset_id
PRIMARY KEY
synset
written_form
VARCHAR (64) NOT NULL
part_of_speech
ENUM( 'Verb', 'Noun' , 'Unknown')
lexical_entry
text_id
VARCHAR (64)
entry_id 
PRIMARY KEY
lexicon_id 
FOREIGN KEY
semantic_feature
FOREIGN KEY
syntactic_feature
FOREIGN KEY
lexicon_id
FOREIGN KEY
argument_map_id
PRIMARY KEY
syn_sem_argument_map
lexicon_id
FOREIGN KEY
argument_map_id
FOREIGN KEY
syn_sem_correspondence_id 
FOREIGN KEY
syn_sem_correspondence_to_map
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
syn_sem_correspondence_id
PRIMARY KEY
syn_sem_correspondence
lexicon_id
FOREIGN KEY
sense_id
FOREIGN KEY
entry_id
FOREIGN KEY
lexical_entry_to_sense
lexicon_id
FOREIGN KEY
text_id
VARCHAR (100)
frame_id
PRIMARY KEY
subcat_frame
lexicon_id
FOREIGN KEY
frame_id
FOREIGN KEY
sense_id
FOREIGN KEY
entry_id
FOREIGN KEY
lexical_entry_to_subcat_frame
lexicon_id
FOREIGN KEY
text_id
VARCHAR (64)
syntactic_argument_id
PRIMARY KEY
syntactic_argument
value
VARCHAR (100)
attribute
VARCHAR (100)
lexicon_id
FOREIGN KEY
syntactic_feature_id
PRIMARY KEY
syntactic_feature
lexicon_id
FOREIGN KEY
syntactic_argument_id
FOREIGN KEY
frame_id
FOREIGN KEY
subcat_frame_to_argument
lexicon_id
FOREIGN KEY
syntactic_feature_id
FOREIGN KEY
syntactic_argument_id 
FOREIGN KEY
syntactic_argument_to_feature
description
VARCHAR(128)
language
VARCHAR(64)
lexicon_id
PRIMARY KEY
lexicon
relation_type 
VARCHAR (64)
lexicon_id
FOREIGN KEY
related_sense_id
FOREIGN KEY
sense_id
FOREIGN KEY
sense_relation
Figure 4: Database schema
Figure 5: QE System - Configure Figure 6: QE System - Browse
149
Figure 7: QE System - Search
3.4 Semantic information
This new type of query expansion requires rich
lexical information. We augmented our data using
the SIMPLE ontology for semantic types, using
the same data for different languages. This had
the added benefit of allowing cross-language ex-
pansion as a result. In steps two and three of Fig-
ure 1 when senses are retrieved that take specific
semantic types as arguments, this process can be
done across all (or as many as are selected) lex-
icons in the database. Thus, results such as are
shown in Figure 7 are possible. In this figure the
Japanese word for ?nail? is entered, and results for
both selected languages, Japanese and Italian, are
returned. This feature requires the unification of
the semantic type ontology strata.
3.5 Possible extension
Next steps for the QE platform are to explore the
use of other information already defined within the
adapted framework, specifically sense relations.
Given to the small size of our sample lexicon, data
sparsity is naturally an issue, but hopefully by ex-
ploring and exploiting these sense relations prop-
erly, the system may be able to further expand a
user?s query to include a broader range of selec-
tions using any additional semantic types belong-
ing to these related senses. The framework also
contains information about the order in which syn-
tactic arguments should be placed. This informa-
tion should be used to format the results from the
user?s query appropriately.
4 An Additional Evaluation
We conducted some additional query expansion
experiments using a corpus that was acquired from
Chinese LDC (No. ?2004-863-009?) as a base (see
below). This corpus marked an initial achievement
in building a multi-lingual parallel corpus for sup-
porting development of cross-lingual NLP appli-
cations catering to the Beijing 2008 Olympics.
The corpus contains parallel texts in Chinese,
English and Japanese and covers 5 domains that
are closely related to the Olympics: traveling, din-
ing, sports, traffic and business. The corpus con-
sists of example sentences, typical dialogues and
articles from the Internet, as well as other language
teaching materials. To deal with the different lan-
guages in a uniform manner, we converted the cor-
pus into our proposed LMF-compliant lexical re-
sources framework, which allowed the system to
expand the query between all the languages within
the converted resources without additional modifi-
cations.
As an example of how this IR system func-
tioned, suppose that Mr. Smith will be visiting
Beijing to see the Olympic games and wants to
know how to buy a newspaper. Using this system,
he would first enter the query ?newspaper?. For
this query, with the given corpus, the system re-
turns 31 documents, fragments of the first 5 shown
below.
(1) I?ll bring an English newspaper immediately.
(2) Would you please hand me the newspaper.
(3) There?s no use to go over the newspaper ads.
(4) Let?s consult the newspaper for such a film.
(5) I have little confidence in what the newspa-
pers say.
Yet it can be seen that the displayed results are not
yet useful enough to know how to buy a newspa-
per, though useful information may in fact be in-
cluded within some of the 31 documents. Using
the lexical resources, the query expansion module
suggests ?buy?, ?send?, ?get?, ?read?, and ?sell?
as candidates to add for a revised query.
Mr. Smith wants to buy a newspaper, so he se-
lects ?buy? as the expansion term. With this query
the system returns 11 documents, fragments of the
first 5 listed below.
(6) I?d like some newspapers, please.
150
(7) Oh, we have a barber shop, a laundry, a store,
telegram services, a newspaper stand, table
tennis, video games and so on.
(8) We can put an ad in the newspaper.
(9) Have you read about the Olympic Games of
Table Tennis in today?s newspaper, Miss?
(10) newspaper says we must be cautious about
tidal waves.
This list shows improvement, as information about
newspapers and shopping is present, but still ap-
pears to lack any documents directly related to
how to buy a newspaper.
Using co-occurrence indexes, the IR system
returns document (11) below, because the noun
?newspaper? and the verb ?buy? appear in the
same sentence.
(11) You can make change at some stores, just buy
a newspaper or something.
From this example it is apparent that this sort
of query expansion is still too naive to apply to
real IR systems. It should be noted, however, that
our current aim of evaluation was in confirming
the advantage of LMF in dealing with multiple
languages, for which we conducted a similar run
with Chinese and Japanese. Results of these tests
showed that in following the LMF framework in
describing lexical resources, it was possibile to
deal with all three languages without changing the
mechanics of the system at all.
5 Discussion
LMF is, admittedly, a ?high-level? specification,
that is, an abstract model that needs to be fur-
ther developed, adapted and specified by the lex-
icon encoder. LMF does not provide any off-the-
shelf representation for a lexical resource; instead,
it gives the basic structural components of a lexi-
con, leaving full freedom for modeling the partic-
ular features of a lexical resource. One drawback
is that LMF provides only a specification manual
with a few examples. Specifications are by no
means instructions, exactly as XML specifications
are by no means instructions on how to represent
a particular type of data.
Going from LMF specifications to a true instan-
tiation of an LMF-compliant lexicon is a long way,
and comprehensive, illustrative and detailed ex-
amples for doing this are needed. Our prototype
system provides a good starting example for this
direction. LMF is often taken as a prescriptive
description, and its examples taken as pre-defined
normative examples to be used as coding guide-
lines. Controlled and careful examples of conver-
sion to LMF-compliant formats are also needed to
avoid too subjective an interpretation of the stan-
dard.
We believe that LMF will be a major base
for various SemanticWeb applications because it
provides interoperability across languages and di-
rectly contributes to the applications themselves,
such as multilingual translation, machine aided
translation and terminology access in different lan-
guages.
From the viewpoint of LMF, our prototype
demonstrates the adaptability of LMF to a rep-
resentation of real-scale lexicons, thus promoting
its adoption to a wider community. This project
is one of the first test-beds for LMF (as one of
its drawbacks being that it has not been tested on
a wide variety of lexicons), particularly relevant
since it is related to both Western and Asian lan-
guage lexicons. This project is a concrete attempt
to specify an LMF-compliant XML format, tested
for representative and parsing efficiency, and to
provide guidelines for the implementation of an
LMF-compliant format, thus contributing to the
reduction of subjectivity in interpretation of stan-
dards.
From our viewpoint, LMF has provided a for-
mat for exchange of information across differently
conceived lexicons. Thus LMF provides a stan-
dardised format for relating them to other lexical
models, in a linguistically controlled way. This
seems an important and promising achievement in
order to move the sector forward.
6 Conclusion
This paper described the results of a three-year
project for creating an international standard for
language resources in cooperation with other ini-
tiatives. In particular, we focused on query expan-
sion using the standard.
Our main contribution can be summarised as
follows.
? We have contributed to ISO TC37/SC4 ac-
tivities, by testing and ensuring the portabil-
ity and applicability of LMF to the devel-
opment of a description framework for NLP
lexicons for Asian languages. Our contribu-
tion includes (1) a package for derivational
151
morphology, (2) the syntax-semantic inter-
face with the problem of classifiers, and (3)
representational issues with the richness of
writing systems in Asian languages. As of
October 2008, LMF including our contribu-
tions has been approved as the international
standard ISO 26413.
? We discussed Data Categories necessary
for Asian languages, and exemplified sev-
eral Data Categories including reduplication,
classifier, honorifics and orthography. We
will continue to harmonise our activity with
that of ISO TC37/SC4 TDG2 with respect to
Data Categories.
? We designed and implemented an evaluation
platform of our description framework. We
focused on linguistically motivated query ex-
pansion module. The system works with lexi-
cons compliant with LMF and ontologies. Its
most significant feature is that the system can
deal with any language as far as the those lex-
icons are described according to LMF. To our
knowledge, this is the first working system
adopting LMF.
In this project, we mainly worked on three
Asian languages, Chinese, Japanese and Thai, on
top of the existing framework which was designed
mainly for European languages. We plan to dis-
tribute our results to HLT societies of other Asian
languages, requesting for their feedback through
various networks, such as the Asian language re-
source committee network under Asian Federation
of Natural Language Processing (AFNLP)3, and
the Asian Language Resource Network project4.
We believe our efforts contribute to international
activities like ISO-TC37/SC45 (Francopoulo et al,
2006).
Acknowledgments
This research was carried out through financial
support provided under the NEDO International
Joint Research Grant Program (NEDO Grant).
References
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Modern
Information Retrieval. Addison-Wesley.
3http://www.afnlp.org/
4http://www.language-resource.net/
5http://www.tc37sc4.org/
G. Francopoulo, G. Monte, N. Calzolari, M. Mona-
chini, N. Bel, M. Pet, and C. Soria. 2006. Lex-
ical markup framework (LMF). In Proceedings of
LREC2006.
N. Ide, A. Lenci, and N. Calzolari. 2003. RDF in-
stantiation of ISLE/MILE lexical entries. In Pro-
ceedings of the ACL 2003 Workshop on Linguistic
Annotation: Getting the Model Right, pages 25?34.
A. Lenci, N. Bel, F. Busa, N. Calzolari, E. Gola,
M. Monachini, A. Ogonowsky, I. Peters, W. Peters,
N. Ruimy, M. Villegas, and A. Zampolli. 2000.
SIMPLE: A general framework for the development
of multilingual lexicons. International Journal of
Lexicography, Special Issue, Dictionaries, Thesauri
and Lexical-Semantic Relations, XIII(4):249?263.
A. Sanfilippo, N. Calzolari, S. Ananiadou,
R. Gaizauskas, P. Saint-Dizier, and P. Vossen.
1999. EAGLES recommendations on semantic
encoding. EAGLES LE3-4244 Final Report.
T. Tokunaga et al 2006. Infrastructure for standard-
ization of Asian language resources. In Proceedings
of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 827?834.
T. Tokunaga et al 2008. Adapting international stan-
dard for asian language technologies. In Proceed-
ings of the Sixth International Language Resources
and Evaluation (LREC?08).
152
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 653?661,
Beijing, August 2010
Structure-Aware Review Mining and Summarization
Fangtao Li1, Chao Han1, Minlie Huang1, Xiaoyan Zhu1,
Ying-Ju Xia2, Shu Zhang2 and Hao Yu2
1State Key Laboratory of Intelligent Technology and Systems?
1Tsinghua National Laboratory for Information Science and Technology?
1Department of Computer Science and Technology, Tsinghua University
2Fujitsu Research and Development Center
fangtao06@gmail.com; zxy_dcs@tsinghua.edu.cn
Abstract
In this paper, we focus on object feature 1
1 Introduction
based review summarization. Different from 
most of previous work with linguistic rules or 
statistical methods, we formulate the review
mining task as a joint structure tagging prob-
lem. We propose a new machine learning 
framework based on Conditional Random 
Fields (CRFs). It can employ rich features to 
jointly extract positive opinions, negative opi-
nions and object features for review sentences.
The linguistic structure can be naturally inte-
grated into model representation. Besides li-
near-chain structure, we also investigate con-
junction structure and syntactic tree structure
in this framework. Through extensive experi-
ments on movie review and product review 
data sets, we show that structure-aware mod-
els outperform many state-of-the-art ap-
proaches to review mining.
With the rapid expansion of e-commerce, people 
are more likely to express their opinions and 
hands-on experiences on products or services
they have purchased. These reviews are impor-
tant for both business organizations and personal 
costumers. Companies can decide on their strat-
egies for marketing and products improvement. 
Customers can make a better decision when pur-
1 Note that there are two meanings for word ?feature?. 
We use ?object feature? to represent the target entity,
which the opinion expressed on, and use ?feature? as
the input for machine learning methods.
chasing products or services. Unfortunately, 
reading through all customer reviews is difficult, 
especially for popular items, the number of re-
views can be up to hundreds or even thousands. 
Therefore, it is necessary to provide coherent 
and concise summaries for these reviews.
Figure 1. Feature based Review Summarization
Inspired by previous work (Hu and Liu, 2004; 
Jin and Ho, 2009), we aim to provide object fea-
ture based review summarization. Figure 1 
shows a summary example for movie ?Gone 
with the wind?. The object (movie) features, 
such as ?movie?, ?actor?, with their correspond-
ing positive opinions and negative opinions, are 
listed in a structured way. The opinions are 
ranked by their frequencies. This provides a con-
cise view for reviews. To accomplish this goal, 
we need to do three tasks:  1), extract all the ob-
ject features and opinions; 2), determine the sen-
timent polarities for opinions; 3), for each object 
feature, determine the relevant opinions, i.e. ob-
ject feature-opinion pairs.
For the first two tasks, most previous studies
employ linguistic rules or statistical methods (Hu 
and Liu, 2004; Popescu and Etzioni 2005). They 
mainly use unsupervised learning methods,
which lack an effective way to address infre-
quent object features and opinions. They are also
hard to incorporate rich overlapping features.
Gone With The Wind:
Movie:
     Positive: great, good, amazing, ? , breathtaking
     Negative: bad, boring, waste time, ? , mistake
Actor: 
     Positive: charming , brilliant , great, ? , smart 
     Negative: poor, fail, dirty, ? , lame
Music:
     Positive: great, beautiful, very good, ? , top
     Negative: annoying, noise, too long, ? , unnecessary 
    ? ?
653
Actually, there are many useful features, which 
have not been fully exploited for review mining.
Meanwhile, most of previous methods extract 
object features, opinions, and determine the po-
larities for opinions separately. In fact, the object 
features, positive opinions and negative opinions
correlate with each other. 
In this paper, we formulate the first two tasks,
i.e. object feature, opinion extraction and opi-
nion polarity detection, as a joint structure tag-
ging problem, and propose a new machine learn-
ing framework based on Conditional Random 
Fields (CRFs). For each sentence in reviews, we 
employ CRFs to jointly extract object features,
positive opinions and negative opinions, which 
appear in the review sentence. This framework
can naturally encode the linguistic structure. Be-
sides the neighbor context with linear-chain 
CRFs, we propose to use Skip-chain CRFs and 
Tree CRFs to utilize the conjunction structure
and syntactic tree structure. We also propose a
new unified model, Skip-Tree CRFs to integrate 
these structures. Here, ?structure-aware? refers 
to the output structure, which model the relation-
ship among output labels. This is significantly 
different from the previous input structure me-
thods, which consider the linguistic structure as 
heuristic rules (Ding and Liu, 2007) or input fea-
tures for classification (Wilson et al 2009). Our 
proposed framework has the following advan-
tages: First, it can employ rich features for re-
view mining. We will analyze the effect of fea-
tures for review mining in this framework.
Second, the framework can utilize the relation-
ship among object features, positive opinions 
and negative opinions. It jointly extracts these 
three types of expressions in a unified way.
Third, the linguistic structure information can be 
naturally integrated into model representation,
which provides more semantic dependency for 
output labels. Through extensive experiments on 
movie review and product review, we show our 
proposed framework is effective for review min-
ing.
The rest of this paper is organized as follows: 
In Section 2, we review related work. We de-
scribe our structure aware review mining me-
thods in Section 3. Section 4 demonstrates the 
process of summary generation. In Section 5, we 
present and discuss the experiment results. Sec-
tion 6 is the conclusion and future work.
2 Related Work
Object feature based review summary has been 
studied in several papers. Zhuang et al (2006) 
summarized movie reviews by extracting object 
feature keywords and opinion keywords. Object 
feature-opinion pairs were identified by using a 
dependency grammar graph. However, it used a
manually annotated list of keywords to recognize 
movie features and opinions, and thus the system 
capability is limited. Hu and Liu (2004) pro-
posed a statistical approach to capture object 
features using association rules. They only con-
sidered adjective as opinions, and the polarities 
of opinions are recognized with WordNet expan-
sion to manually selected opinion seeds. Popescu 
and Etzioni (2005) proposed a relaxation labe-
ling approach to utilize linguistic rules for opi-
nion polarity detection. However, most of these 
studies focus on unsupervised methods, which
are hard to integrate various features. Some stu-
dies (Breck et al 2007; Wilson et al 2009; Ko-
bayashi et al 2007) have used classification 
based methods to integrate various features. But 
these methods separately extract object features
and opinions, which ignore the correlation 
among output labels, i.e. object features and opi-
nions. Qiu et al (2009) exploit the relations of 
opinions and object features by adding some lin-
guistic rules. However, they didn?t care the opi-
nion polarity. Our framework can not only em-
ploy various features, but also exploit the corre-
lations among the three types of expressions, i.e.
object features, positive opinions, and negative 
opinions, in a unified framework. Recently, Jin 
and Ho (2009) propose to use Lexicalized HMM
for review mining. Lexicalized HMM is a va-
riant of HMM. It is a generative model, which is 
hard to integrate rich, overlapping features. It 
may encounter sparse data problem, especially 
when simultaneously integrating multiple fea-
tures. Our framework is based on Conditional 
Random Fields (CRFs). CRFs is a discriminative 
model, which can easily integrate various fea-
tures.
These are some studies on opinion mining with 
Conditional Random Fields. For example, with 
CRFs, Zhao et al(2008) and McDonald et al 
(2007) performed sentiment classification in sen-
tence and document level; Breck et al(2007) 
identified opinion expressions from newswire 
documents; Choi et al (2005) determined opi-
654
nion holders to opinions also from newswire da-
ta. None of previous work focuses on jointly ex-
tracting object features, positive opinions and 
negative opinions simultaneously from review 
data. More importantly, we also show how to 
encode the linguistic structure, such as conjunc-
tion structure and syntactic tree structure, into 
model representation in our framework. This is 
significantly different from most of previous 
studies, which consider the structure information 
as heuristic rules (Hu and Liu, 2004) or input 
features (Wilson et al 2009).
Recently, there are some studies on joint sen-
timent/topic extraction (Mei et al 2007; Titov 
and McDonald, 2008; Snyder and Barzilay, 
2007). These methods represent reviews as sev-
eral coarse-grained topics, which can be consi-
dered as clusters of object features. They are
hard to indentify the low-frequency object fea-
tures and opinions. While in this paper, we will 
extract all the present object features and corres-
ponding opinions with their polarities. Besides, 
the joint sentiment/topic methods are mainly
based on review document for topic extraction.
In our framework, we focus on sentence-level
review extraction.
3 Structure Aware Review Mining
3.1 Problem Definition
To produce review summaries, we need to first 
finish two tasks: identifying object features, opi-
nions, and determining the polarities for opi-
nions. In this paper, we formulate these two 
tasks as a joint structure tagging problem. We
first describe some related definitions:
Definition (Object Feature): is defined as whole 
target expression that the subjective expressions 
have been commented on. Object features can be 
products, services or their elements and proper-
ties, such as ?character?, ?movie?, ?director? for 
movie review, and ?battery?, ?battery life?,
?memory card? for product review.
Definition (Review Opinion): is defined as the 
whole subjective expression on object features.
For example, in sentence ?The camera is easy to 
use?, ?easy to use? is a review opinion. ?opinion? 
is used for short.
Definition (Opinion Polarity): is defined as the 
sentiment category for review opinion. In this 
paper, we consider two types of polarities: posi-
tive opinion and negative opinion. For example,
?easy to use? belongs to positive opinion.
For our review mining task, we need to 
represent three types of expressions: object fea-
tures, positive opinions, and negative opinions. 
These expressions may be words, or whole
phrases. We use BIO encoding for tag represen-
tation, where the non-opinion and neutral opi-
nion words are represented as ?O?. With Nega-
tion (N), which is only one word, such as ?not?,
?don?t?, as an independent tag, there are totally 8 
tags, as shown in Table 1. The following is an 
example to denote the tags:
The/O camera/FB comes/O with/O a/O piti-
ful/CB 32mb/FB compact/FI flash/FI card/FI ./O
FB Feature Beginning CB Negative Beginning
FI Feature Inside CI Negative Inside
PB Positive Beginning N Negation Word 
PI Positive Inside O Other 
Table 1. Basic Tag Set for Review Mining
3.2 Structure Aware Model
In this section, we describe how to encode dif-
ferent linguistic structure into model representa-
tion based on our CRFs framework.
3.2.1 Using Linear CRFs.
For each sentence in a review, our task is to ex-
tract all the object features, positive opinions and 
negative opinions. This task can be modeled as a 
classification problem. Traditional classification 
tools, e.g. Maximum Entropy model (Berger et 
al, 1996), can be employed, where each word or 
phrase will be treated as an instance. However, 
they independently consider each word or 
phrase, and ignore the dependency relationship 
among them.
Actually, the context information plays an im-
portant role for review mining. For example, 
given two continuous words with same part of 
speech, if the previous word is a positive opi-
nion, the next word is more likely a positive opi-
nion. Another example is that if the previous 
word is an adjective, and it is an opinion, the 
next noun word is more likely an object feature.
To this end, we formulate the review mining 
task as a joint structure tagging problem, and 
propose a general framework based on Condi-
tional Random Fields (CRFs) (Lafferty et al, 
2001) which are able to model the dependencies 
655
y1 yn-1y3y2 yn
x1 xn-1x3x2 xn
(a) Linear-chain  CRFs
y4
x1 xn-1x3x2 xnxn-2
?
x4
y1 yn-2y3 yn
y2 yn-1
(c) Tree-CRFs
y4
x1 xn-1x3x2 xnxn-2
?
x4
y1 yn-2y3 yn
y2 yn-1
(d) Skip-Tree CRFs
(b) Skip-chain  CRFs
Figure 2 CRFs models
between nodes. (See Section 3.2.5 for more 
about CRFs)
In this section, we propose to use linear-chain
CRFs to model the sequential dependencies be-
tween continuous words, as discussed above. It 
views each word in the sentence as a node, and 
adjacent nodes are connected by an edge. The 
graphical representation is shown in Figure 2(a).
Linear CRFs can make use of dependency rela-
tionship among adjacent words.
3.2.2 Leveraging Conjunction Structure
We observe that the conjunctions play important 
roles on review mining: If the words or phrases 
are connected by conjunction ?and?, they mostly 
belong to the same opinion polarity. If the words 
or phrases are connected by conjunction ?but?, 
they mostly belong to different opinion polarity,
as reported in (Hatzivassiloglou and McKeown,
1997; Ding and Liu, 2007). For example, ?This
phone has a very cool and useful feature ? the
speakerphone?, if we only detect ?cool?, it is 
hard to determine its opinion polarity. But if we 
see ?cool? is connected with ?useful? by con-
junction ?and?, we can easily acquire the polari-
ty of ?cool? as positive. This conjunction struc-
ture not only helps to determine the opinions, but 
also helps to recognize object features. For ex-
ample, ?I like the special effects and music in 
this movie?, with word ?music? and conjunction
?and?, we can easily detect that ?special effects? 
as an object feature.
To model the long distance dependency with 
conjunctions, we use Skip-chain CRFs model to 
detect object features and opinions. The graphi-
cal representation of a Skip-chain CRFs, given in 
Figure 2(b), consists of two types of edges: li-
near-edge (

to 

) and skip-edge (

to 

). 
The linear-edge is described as linear CRFs. The 
skip-edge is imported as follows:
We first identify the conjunctions in the re-
view sentence, with a collected conjunction set,
including ?and?, ?but?, ?or?, ?however?, ?al-
though? etc. For each conjunction, we extract its 
connected two text sequences. The nearest two 
words with same part of speech from the two 
text sequences are connected with the skip-edge. 
Here, we just consider the noun, adjective, and 
adverb. For example, in ?good pictures and 
beautiful music?, there are two skip-edges: one 
connects two adjective words ?good? and ?beau-
tiful?; the other connects two nouns ?pictures? 
and ?music?. We also employ the general senti-
ment lexicons, SentiWordNet (Esuli and Sebas-
tiani, 2006), to connect opinions. Two nearest 
opinion words, detected by sentiment lexicon,
from two sequences, will also be connected by 
skip-edge. If the nearest distance exceeds the 
threshold, this skip edge will be discarded. Here,
we consider the threshold as nine.
Skip-chain CRFs improve the performance of 
review mining, because it naturally encodes the 
conjunction structure into model representation 
with skip-edges.
3.2.3 Leveraging Syntactic Tree Structure
Besides the conjunction structure, the syntactic 
tree structure also helps for review mining. The
tree denotes the syntactic relationship among 
words. In a syntactic dependency representation, 
each node is a surface word. For example, the 
corresponding dependency tree (Klein and Man-
ning, 2003) for the sentence, ?I really like this 
long movie?, is shown in Figure 3.
y1 yn-1y3y2 yn
x1 xn-1x3x2 xn
656
like
longthis
really movieI
nsubj dobjadvmod
det amod
Figure 3. Syntactic Dependency Tree Representation
In linear-chain structure and skip-chain structure, 
?like? and ?movie? have no direct edge, but in 
syntactic tree, ?movie? is directly connected 
with ?like?, and their relationship ?dobj? is also 
included, which shows ?movie? is an objective 
of ?like?. It can provide deeper syntactic depen-
dencies for object features, positive opinions and 
negative opinions. Therefore, it is important to 
consider the syntactic structure in the review 
mining task. 
In this section, we propose to use Tree CRFs to
model the syntactic tree structure for review 
mining. The representation of a Tree CRFs is 
shown in Figure 2(c). The syntactic tree structure 
is encoded into our model representation. Each 
node is corresponding to a word in the depen-
dency tree. The edge is corresponding to depen-
dency tree edge. Tree CRFs can make use of de-
pendency relationship in syntactic tree structure
to boost the performance.
3.2.4 Integrating Conjunction Structure and 
Syntactic Tree Structure
Conjunction structure provides the semantic re-
lations correlated with conjunctions. Syntactic 
tree structure provides dependency relation in 
the syntactic tree. They represent different se-
mantic dependencies. It is interesting to consider 
these two dependencies in a unified model. We 
propose Skip-Tree CRFs, to combine these two 
structure information. The graphical representa-
tion of a Skip-Tree CRFs, given in Figure 2(d),
consists of two types of edges: tree edges and 
conjunction skip-edges. We hope to simulta-
neously model the dependency in conjunction 
structure and syntactic tree structure.
We also notice that there is a relationship 
?conj? in syntactic dependency tree. However, 
we find that it only connects two head words for 
a few coordinating conjunction, such as ?and", 
?or", ?but?. Our designed conjunction skip-edge
provides more information for joint structure 
tagging. We analyze more conjunctions to con-
nect not only two head words, but also the words 
with same part of speech. We also connect the 
words with sentiment lexicon. We will show that 
the skip-tree CRFs, which combine the two 
structures, is effective in the experiment section.
3.2.5 Conditional Random Fields
A CRFs is an undirected graphical model G of 
the conditional distribution (	|
). Y are the 
random variables over the labels of the nodes 
that are globally conditioned on X, which are the 
random variables of the observations. The condi-
tional probability is defined as: 
P
(
	 
|


)
=  
1
(
)
    



(, 	|, 
)
,
+   



(, 	|, 
)
