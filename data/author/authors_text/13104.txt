First Joint Conference on Lexical and Computational Semantics (*SEM), pages 667?672,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
 SAGAN: An approach to Semantic Textual Similarity  
based on Textual Entailment 
 
Julio Castillo??      Paula Estrella? 
                  
?
FaMAF, UNC, Argentina  
                      
?
UTN-FRC, Argentina 
                        jotacastillo@gmail.com 
                     pestrella@famaf.unc.edu.ar 
 
 
 
 
 
 
 
 
Abstract 
In this paper we report the results obtained 
in the Semantic Textual Similarity (STS) 
task, with a system primarily developed for 
textual entailment. Our results are quite 
promising, getting a run ranked 39 in the 
official results with overall Pearson, and 
ranking 29 with the Mean metric. 
 
1 Introduction 
For the last couple of years the research com-
munity has focused on a deeper analysis of natural 
languages, seeking to capture the meaning of the 
text in different contexts: in machine translation 
preserving the meaning of the translations is cru-
cial to determine whether a translation is useful or 
not, in question-answering understanding the ques-
tion leads to the desired answers (while the oppo-
site case makes a system rather frustrating to the 
user) and the examples could continue. In this 
newly defined task, Semantic Textual Similarity, 
there is hope that efforts in different areas will be 
shared and united towards the goal of identifying 
meaning and recognizing equivalent, similar or 
unrelated texts. Our contribution to the task, is 
from a textual entailment point of view, as will be 
described below. 
The paper is organized as follows: Section 2 de-
scribes the relevant tasks, Section 3 describes the 
architecture of the system, then Section 4 shows 
the experiments carried out and the results ob-
tained, and Section 5 presents some conclusions 
and future work. 
2 Related work  
In this section we briefly describe two different 
tasks that are closely related and in which our sys-
tem has participated with very promising results. 
 
2.1 Textual Entailment 
 
Textual Entailment (TE) is defined as a generic 
framework for applied semantic inference, where 
the core task is to determine whether the meaning 
of a target textual assertion (hypothesis, H) can be 
inferred from a given text (T). For example, given 
the pair (T,H): 
T: Fire bombs were thrown at the Tunisian embas-
sy in Bern 
H: The Tunisian embassy in Switzerland was at-
tacked 
we can conclude that T entails H. 
 
The recently created challenge ?Recognising 
Textual Entailment? (RTE) started in 2005 with 
the goal of providing a binary answer for each pair 
(H,T), namely whether there is entailment or not 
(Dagan et al, 2006). The RTE challenge has mu-
tated over the years, aiming at accomplishing more 
667
accurate and specific solutions; for example, in 
2008 a three-way decision was proposed (instead 
of the original binary decision) consisting of ?en-
tailment?, ?contradiction? and ?unknown?; in 2009 
the organizers proposed a pilot task, the Textual 
Entailment Search (Bentivogli et al 2009), consist-
ing in finding all the sentences in a set of docu-
ments that entail a given Hypothesis and since 
2010 there is a Novelty Detection Task, which 
means that RTE systems are required to judge 
whether the information contained in each H is 
novel with respect to (i.e., not entailed by) the in-
formation contained in the corpus. 
2.2 Semantic Textual Similarity 
The pilot task STS was recently defined in 
Semeval 2012 (Aguirre et al, 2012) and has as 
main objective measuring the degree of semantic 
equivalence between two text fragments. STS is 
related to both Recognizing Textual Entailment 
(RTE) and Paraphrase Recognition, but has the 
advantage of being a more suitable model for mul-
tiple NLP applications.  
As mentioned before, the goal of the RTE task 
(Bentivogli et al 2009) is determining whether the 
meaning of a hypothesis H can be inferred from a 
text T. Thus, TE is a directional task and we say 
that T entails H, if a person reading T would infer 
that H is most likely true.  The difference with STS 
is that STS consists in determining how similar 
two text fragments are, in a range from 5 (total 
semantic equivalence) to 0 (no relation). Thus, 
STS mainly differs from TE in that the classifica-
tion is graded instead of binary. In this manner, 
STS is filling the gap between several tasks. 
3 System architecture  
Sagan is a RTE system (Castillo and Cardenas, 
2010) which has taken part of several challenges, 
including the Textual Analysis Conference 2009 
and TAC 2010, and the Semantic Textual Similari-
ty and Cross Lingual Textual Entailment for con-
tent synchronization as part of the Semeval 2012. 
 The system is based on a machine learning ap-
proach and it utilizes eight WordNet-based 
(Fellbaum, 1998) similarity measures, as explained 
in (Castillo, 2011), with the purpose of obtaining 
the maximum similarity between two WordNet 
concepts. A concept is a cluster of synonymous 
terms that is called a synset in WordNet. These 
text-to-text similarity measures are based on the 
following word-to-word similarity metrics: 
(Resnik, 1995), (Lin, 1997), (Jiang and Conrath, 
1997), (Pirr? and Seco, 2008), (Wu & Palmer, 
1994), Path Metric, (Leacock & Chodorow, 1998), 
and a semantic similarity to sentence level named 
SemSim (Castillo and Cardenas,2010).  
 
Pre-Processing
Similarity Score
MSR
Word Level Semantic Metrics
Extraction Features 
SVM with 
Regression
Test Set:  MSR, 
MSRvid,Europarl, 
SMT-news, WN
RUN 1 
Normalizer Stemmer Parser
Resnik SemSimW&PLin ...
Sentence Level Semantic Metric
MSR+MSRvid
RUN 2
RUN 3 
MSR+MSRvid
+Europarl
Training sets:
...
Fig.1. System architecture  
 
The system construct a model of the semantic 
similarity of two texts (T,H) as a function of the 
semantic similarity of the constituent words of 
both phrases. In order to reach this objective, we 
used a text to text similarity measure which is 
based on word to word similarity. Thus, we expect 
that combining word to word similarity metrics to 
text level would be a good indicator of text to text 
similarity.  
Additional information about how to produce 
feature vectors as well as each word- and sentence-
level metric can be found in (Castillo, 2011). The 
architecture of the system is shown in Figure 1. 
The training set used for the submitted runs are 
those provided by the organizers of the STS. How-
ever we also experimented with RTE datasets as 
described in the next Section.  
668
4 Experiments and Results 
For preliminary experiments before the STS Chal-
lenge, we used the training set provided by the 
organizers, denoted with "_train", and consisting of 
750 pairs of sentences from the MSR Paraphrase 
Corpus (MSRpar), 750 pairs of sentences from the 
MSRvid Corpus (MSRvid), 459 pairs of sentences 
of the Europarl WMT2008 development set (SMT-
eur). We also used the RTE datasets from Pascal 
RTE Challenge (Dagan et al, 2006) as part of our 
training sets. Additionally, at the testing stage, we 
used the 399 pairs of  news conversation (SMT-
news) and 750 pairs of sentences where the first 
one comes from Ontonotes and the second one 
from a WordNet definition (On-WN).   
In STS Challenge it was required that participat-
ing systems do not use the test set of MSR-
Paraphrase, the text of the videos in MSR-Video, 
and the data from the evaluation tasks at any WMT 
to develop or train their systems. Additionally, we 
also assumed that the dataset to be processed was 
unknown in the testing phase, in order to avoid any 
kind of tuning of the system. 
4.1 Preliminary Experiments 
In a preliminary study performed before the final 
submission, we experimented with three machine 
learning algorithms Support Vector Machine 
(SVM) with regression and polynomial kernel, 
Multilayer perceptron (MLP), and Linear Regres-
sion (LR). Table 1 shows the results obtained with 
10-fold cross validation technique and Table 2 
shows the results of testing them with two datasets 
and 3 classifiers over MSR_train. 
 
Classifier Pearson c.c 
SVM with regression 0.54 
MLP 0.51 
LinearRegression 0.54 
Table 1. Results obtained using MSR training set 
(MSRpar + MSRvid) with 10 fold-cross validation. 
 
Training set & ML algorithm Pearson c.c 
Europarl + SVM w/ regression 0.61 
Europarl + MLP 0.44 
Europarl + linear regression 0.61 
MSRvid + SVM w/ regression 0.70 
MSRvid + MLP 0.52 
MSRvid + linear regression 0.69 
Table 2. Results obtained using MSR training set  
Results reported in Table 1 show that we 
achieved the best performance with SVM with 
regression and Linear Regression classifiers and 
using MLP we obtained the worst results to predict 
each dataset. To our surprise, a linear regression 
classifier reports better accuracy that MLP, it may 
be mainly due to the correlation coefficient used, 
namely Pearson, which is a measure of a linear 
dependence between two variables and linear re-
gression builds a model assuming linear influence 
of independent features. We believe that using 
Spearman correlation should be better than using 
the Pearson coefficient given that Spearman as-
sumes non-linear correlation among variables. 
However, it is not clear how it behaves when sev-
eral dataset are combined to obtain a global score. 
Indeed, further discussion is needed in order to 
find the best metric to the STS pilot task. Given 
these results, in our submission for the STS pilot 
task we used a combination of STS datasets as 
training set and the SVM with regression classifier. 
Because our approach is mainly based on ma-
chine learning the quality and quantity of dataset is 
a key factor to determine the performance of the 
system, thus we decided to experiment with RTE 
datasets too (Bentivogli et el., 2009) with the aim 
of increasing the size of the training set.  
To achieve this goal, first we chose the RTE3 
dataset because it is simpler than subsequent da-
tasets and it was proved to provide a high accuracy 
predicting other datasets (Castillo, 2011). Second, 
taking into account that RTE datasets are binary 
classified as YES or NO entailment, we assumed 
that a non entailment can be treated as a value of 
2.0 in the STS pilot task and an entailment can be 
thought of as a value of 3.0 in STS. Of course, 
many pairs classified as 3.0 could be mostly equiv-
alent (4.0) or completely equivalent (5.0) but we 
ignored this fact in the following experiment.  
 
Training set Test set Pearson 
c.c. 
RTE3 MSR_train 0.4817 
RTE3 MSRvid_train 0.5738 
RTE3 Europarl_train 0.4746 
MSR_train+RTE3 MSRvid_train 0.5652 
MSR_train+RTE3 Europarl_train 0.5498 
MSRvid_train+RTE3 MSR_train 0.4559 
MSRvid_train+RTE3 Europarl_train 0.4964 
Table 3. Results obtained using RTE in the training sets 
and SVM w/regression as classifier 
 
669
From these experiments we conclude that RTE3 
alone is not enough to adequately predict neither of 
the STS datasets, and it is understandable if we 
note that only one pair with 2.0 and 3.0 scores are 
present in this dataset.  
On the other hand, by combining RTE3 with a 
STS corpus we always obtain a slight decrease in 
performance in comparison to using STS alone. It 
is likely due to an unbalanced set and possible 
contradictory pairs (e.g: a par in RTE3 classified as 
3.0 when it should be classified 4.3). Thus, we 
conclude that in order to use the RTE datasets our 
system needs a manual annotation of the degree of 
semantic similarity of every pair <T,H> of RTE 
dataset. 
Having into account that in our training phase 
we obtained a decrease in performance using RTE 
datasets we decided not to submit any run using 
the RTE datasets. 
4.2 Submission to the STS shared task  
Our participation in the shared task consisted of 
three different runs using a SVM classifier with 
regression; the runs were set up as follows: 
- Run 1: system trained on a subset of the Mi-
crosoft Research Paraphrase Corpus (Dolan and 
Brockett, 2005), named MSR and consisting of 
750 pairs of sentences marked with a degree of 
similarity from 5 to 0.  
- Run 2: in addition to the MSR corpus we incor-
porated another 750 sentences extracted from the 
Microsoft Research Video Description Corpus 
(MSRvid), annotated in the same way as MSR. 
- Run 3: to the 1500 sentences from the MSR and 
MSRvid corpus we incorporated 734 pairs of sen-
tences from the Europarl corpus used as develop-
ment set in the WMT 2008; all sentences are 
annotated with the degree of similarity from 5 to 0. 
It is very interesting to note that we used the 
same system configurations for every dataset of 
each RUN. In this manner, we did not perform any 
kind of tuning to a particular dataset before our 
submission. We decided to ignore the "name" of 
each dataset and apply our system regardless of the 
particular dataset. Surely, if we take into account 
where each dataset came from we can develop a 
particular strategy for every one of them, but we 
assumed that this kind of information is unknown 
to our system.  
The official scores of the STS pilot task is the 
Pearson correlation coefficient, and other varia-
tions of Pearson which were proposed by the or-
ganizers with the aim of better understanding the 
behavior of the competing systems among the dif-
ferent scenarios. 
These metric are named ALL (overall Pearson), 
ALLnrm (normalized Pearson) and Mean 
(weighted mean), briefly described below: 
- ALL: To compute this metric, first a new dataset 
with the union of the five gold datasets is created 
and then the Pearson correlation is calculated over 
this new dataset. 
- ALLnrm: In this metric, the Pearson correlation 
is computed after the system outputs for each da-
taset are fitted to the gold standard using least 
squares. 
- Mean: This metric is a weighted mean across the 
five datasets, where the weight is given by the 
quantity of pairs in each dataset. 
Table 5 report the results achieved with these 
metrics followed by an individual Pearson correla-
tion for each dataset. 
Interestingly, if we analyze the size of data sets, 
we see that the larger the training set used, the 
greater the efficiency gains with ALL metric. In 
effect, RUN3 used 2234 pairs, RUN2 used 1500 
pairs and RUN1 was composed by 750 pairs. This 
highlights the need for larger datasets for the pur-
pose of building more accurate models.  
With ALLnrm our system achieved better re-
sults but since this metric is based on normalized 
Pearson correlation which assumes a linear correla-
tion, we believe that this metric is not representa-
tive of the underlying phenomenon. For example, 
conducting manual observation we can see that 
pairs from SMT-news are much harder to classify 
than MSRvid pairs. This results can also be evi-
denced from others participating teams who almost 
always achieved better results with MSRvid than 
SMT-news dataset.  
The last metric proposed is the Mean and we are 
ranked 29 among participating teams. It is proba-
bly due to the weight of SMT-news (399 pairs) is 
smaller than MSR or MSRvid. 
Mean metrics seems to be more suitable for this 
task but lack an important issue, do not have into 
account the different "complexity" of the datasets. 
It is also a issue for all metrics proposed. We be-
lieve that incorporating to Mean metric a complex-
ity factor weighting for each dataset based on a 
670
human judge assignment could be more suitable 
for the STS evaluation. We think in complexity as 
an underlying concept referring to the difficulty of 
determine how semantically related two sentences 
are to one another. Thus, two sentences with high 
lexical overlap should have a low complexity and 
instead two sentences that requires deep inference 
to determine similarity should have a high com-
plexity. This should be heighted by human annota-
tors and could be a method for a more precise 
evaluation of STS systems. 
 
Finally, we suggested measuring this new chal-
lenging task using a weighted Mean of the 
Spearman's rho correlation coefficient by incorpo-
rating a factor to weigh the difficulty of each da-
taset. 
 
 
 
 
 
 
Run ALL Rank ALLnrm 
Rank
Nrm 
Mean 
Rank
Mean 
MSR
par 
MSR
vid 
SMT-
eur 
On-
WN 
SMT-
news 
Best Run ,8239 1 ,8579 2 ,6773 1 ,6830 ,8739 ,5280 ,6641 ,4937 
Worst Run -,0260 89 ,5933 89 ,1016 89 ,1109 ,0057 ,0348 ,1788 ,1964 
Sagan-RUN1 ,5522 57 ,7904 47 ,5906 29 ,5659 ,7113 ,4739 ,6542 ,4253 
Sagan-RUN2 ,6272 42 ,8032 37 ,5838 34 ,5538 ,7706 ,4480 ,6135 ,3894 
Sagan-RUN3 ,6311 39 ,7943 45 ,5649 46 ,5394 ,7560 ,4181 ,5904 ,3746 
Table 5. Official results of the STS challenge 
 
5 Conclusions and future work 
In this paper we present Sagan, an RTE system 
applied to the task of Semantic Textual Similarity. 
After a preliminary study of the classifiers perfor-
mance for the task, we decided to use a combina-
tion of STS datasets for training and the classifier 
SVM with regression. With this setup the system 
was ranked 39 in the best run with overall Pearson, 
and ranked 29 with Mean metric. However, both 
rankings are based on the Pearson correlation coef-
ficient and we believe that this coefficient is not 
the best suited for this task, thus we proposed a 
Mean Spearman's rho correlation coefficient 
weighted by complexity, instead. Therefore, fur-
ther application of other metrics should be one in 
order to find the most representative and fair eval-
uation metric for this task. Finally, while promis-
ing results were obtained with our system, it still 
needs to be tested on a diversity of settings. This is 
work in progress, as the system is being tested as a 
metric for the evaluation of machine translation, as 
reported in (Castillo and Estrella, 2012). 
References  
Christoph Tillmann, Stephan Vogel, Hermann Ney, 
Arkaitz Zubiaga, and Hassan Sawaf. 1997. Acceler-
ated DP Based Search For Statistical Translation. In 
Proceedings of the 5th European Conference on 
Speech Communication and Technology  
(EUROSPEECH-97). 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic 
Evaluation of Machine Translation. In Proceedings 
of the 40th AnnualMeeting of the Association for 
Computational Linguistics(ACL-02), pages 311?318. 
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and 
Hermann Ney. 2000. A Evaluation Tool for Machine 
Translation:Fast Evaluation for MT Research. In 
Proceedings of the 2nd International Conference on 
Language Resources and Evaluation (LREC-2000). 
G. Doddington. 2002. Automatic Evaluation of Machine 
Translation Quality using N-gram Co-occurrence 
Statistics.  In Proceedings of the 2nd International 
Conference on Human Language Technology Re-
search (HLT-02), pages 138?145, San Francisco, 
CA, USA. 
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the 43th Annual Meeting of the 
Association of Computational Linguistics (ACL-05), 
pages 65?72. 
Michael Denkowski and Alon Lavie. 2011. METEOR-
NEXT and the METEOR Paraphrase Tables: Im-
proved Evaluation Support For Five Target Lan-
guages. Proceedings of the ACL 2010 Joint 
Workshop on Statistical Machine Translation and 
Metrics MATR. 
671
He Yifan, Du Jinhua, Way Andy, and Van Josef . 2010. 
The DCU dependency-based metric in WMT-
MetricsMATR 2010. In: WMT 2010 - Joint Fifth 
Workshop on Statistical Machine Translation and 
Metrics MATR, ACL, Uppsala, Sweden. 
Chi-kiu Lo and Dekai Wu. 2011. MEANT: inexpensive, 
high-accuracy, semi-automatic metric for evaluating 
translation utility based on semantic roles. 49th An-
nual Meeting of the Association for Computational 
Linguistic (ACL-2011). Portland, Oregon, US. 
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, 
Linnea Micciulla, and John Makhoul. 2006. A Study 
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of the 7th Conference of the 
Association for Machine Translation in the Americas 
(AMTA-06), pages 223?231. 
Ido Dagan, Oren Glickman and Bernardo Magnini. 
2006. The PASCAL Recognising Textual Entailment 
Challenge. In Qui?onero-Candela, J.; Dagan, I.; 
Magnini, B.; d'Alch?-Buc, F. (Eds.) Machine Learn-
ing Challenges. Lecture Notes in Computer Science , 
Vol. 3944, pp. 177-190, Springer. 
Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido 
Dagan, Marc Dymetman, and Idan Szpektor. 2009. 
Source-language entailment modeling for translating 
unknown terms. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL. 
Stroudsburg, PA, USA, 791-799. 
Wilker Aziz and Marc Dymetmany and Shachar Mirkin 
and Lucia Specia and Nicola Cancedda and Ido Da-
gan. 2010. Learning an Expert from Human Annota-
tions in Statistical Machine Translation: the Case of 
Out-of-VocabularyWords. In: Proceedings of the 
14th annual meeting of the European Association for 
Machine Translation (EAMT), Saint-Rapha, France. 
Dahlmeier, Daniel  and  Liu, Chang  and  Ng, Hwee 
Tou. 2011.TESLA at WMT 2011: Translation Evalu-
ation and Tunable Metric.In: Proceedings of the 
Sixth Workshop on Statistical Machine Translation. 
ACL,  pages 78-84, Edinburgh, Scotland. 
S. Pado, D. Cer, M. Galley, D. Jurafsky and C. Man-
ning. 2009. Measuring Machine Translation Quality 
as Semantic Equivalence: A Metric Based on Entail-
ment Features. Journal of MT 23(2-3), 181-193.  
S. Pado, M. Galley, D. Jurafsky and C. Manning. 2009a. 
Robust Machine Translation Evaluation with Entail-
ment Features. Proceedings of ACL 2009. 
Eneko Agirre, Daniel Cer, Mona Diab and Aitor Gonza-
lez-Agirre.    2012. SemEval-2012 Task 6: A Pilot on 
Semantic Textual Similarity.    In Proceedings of the 
6th International Workshop on Semantic    Evalua-
tion (SemEval 2012), in conjunction with the First 
Joint    Conference on Lexical and Computational 
Semantics (*SEM 2012). 
Bentivogli, Luisa, Dagan Ido, Dang Hoa, Giampiccolo, 
Danilo, Magnini Bernardo.2009.The Fifth PASCAL 
RTE Challenge. In: Proceedings of the Text Analysis 
Conference.  
Fellbaum C. 1998. WordNet: An Electronic Lexical 
Database, volume 1. MIT Press. 
Castillo Julio. 2011. A WordNet-based semantic ap-
proach to textual entailment and cross-lingual textu-
al entailment. International Journal of Machine 
Learning and Cybernetics - Springer, Volume 2, 
Number 3. 
Castillo Julio and Cardenas Marina. 2010. Using sen-
tence semantic similarity based onWordNet in recog-
nizing textual entailment. Iberamia 2010. In LNCS, 
vol 6433. Springer, Heidelberg, pp 366?375. 
Castillo Julio. 2010. A semantic oriented approach to 
textual entailment using WordNet-based measures. 
MICAI 2010. LNCS, vol 6437. Springer, Heidelberg, 
pp 44?55. 
Castillo Julio. 2010. Using machine translation systems 
to expand a corpus in textual entailment. In: Proceed-
ings of the Icetal 2010. LNCS, vol 6233, pp 97?102. 
Resnik P. 1995. Information content to evaluate seman-
tic similarity in a taxonomy. In: Proceedings of IJCAI 
1995, pp 448?453 907. 
Castillo Julio, Cardenas Marina. 2011. An Approach to 
Cross-Lingual Textual Entailment using Online Ma-
chine Translation Systems. Polibits Journal. Vol 44. 
Castillo Julio and Estrella Paula. 2012. Semantic Textu-
al Similarity for MT evaluation. NAACL 2012  
Seventh Workshop on Statistical Machine Transla-
tion. WMT 2012, Montreal, Canada. 
Lin D. 1997. An information-theoretic definition of 
similarity. In: Proceedings of Conference on Machine 
Learning, pp 296?304 909. 
Jiang J, Conrath D.1997. Semantic similarity based on 
corpus statistics and lexical taxonomy. In: Proceed-
ings of theROCLINGX 911 
Pirro G., Seco N. 2008. Design, implementation and 
evaluation of a new similarity metric combining fea-
ture and intrinsic information content. In: ODBASE 
2008, Springer LNCS. 
Wu Z, Palmer M. 1994. Verb semantics and lexical 
selection. In: Proceedings of the 32nd ACL 916. 
Leacock C, Chodorow M. 1998. Combining local con-
text and WordNet similarity for word sense identifi-
cation. MIT Press, pp 265?283 919 
Hirst G, St-Onge D . 1998. Lexical chains as represen-
tations of context for the detection and correction of 
malapropisms. MIT Press, pp 305?332 922 
Banerjee S, Pedersen T. 2002. An adapted lesk algo-
rithm for word sense disambiguation using WordNet. 
In: Proceeding of CICLING-02 
William B. Dolan and Chris Brockett.2005. Automati-
cally Constructing a Corpus of Sentential Para-
phrases. Third International Workshop on 
Paraphrasing (IWP2005). Asia Federation of Natural 
Language Processing. 
672
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 721?726,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
 SAGAN: A Machine Translation Approach for  
Cross-Lingual Textual Entailment 
 
Julio Castillo1,2 and Marina Cardenas2
 
1UNC-FaMAF, Argentina
 
2UTN-FRC, Argentina 
{jotacastillo, ing.marinacardenas}@gmail.com
 
                      
 
 
  
 
Abstract 
This paper describes our participation in the 
task denominated Cross-Lingual Textual En-
tailment (CLTE) for content synchronization. 
We represent an approach to CLTE  using 
machine translation to tackle the problem of 
multilinguality. Our system resides on ma-
chine learning and in the use of WordNet as 
semantic source knowledge. Results are very 
promising always achieving results above 
mean score.  
1 Introduction 
 
This paper describes the participation of Sagan, a 
TE and CLTE system, in the new task of Cross 
Lingual Textual Entailment for Content Synchro-
nization. 
 The objective of the Recognizing Textual En-
tailment (RTE) task (Dagan et al, 2006) is deter-
mining whether the meaning of a text fragment that 
we call hypothesis H can be inferred from another 
text fragment T. In this manner, we say that T en-
tails H, if a person reading T would infer that H is 
most likely true. Thus, this definition assumes 
common human understanding of language and 
common background knowledge. 
In that context, Cross-Lingual Textual Entail-
ment addresses textual entailment recognition in 
the challenging application scenario of content 
synchronization. Thus, CLTE constitutes a gener-
alization of Textual Entailment task (also Mono-
lingual Textual Entailment) , but envisioning a 
larger number of application areas in NLP, includ-
ing question answering, information retrieval, in-
formation extraction, and document summariza-
tion, across different languages. 
Content synchronization could be used to keep 
consistence among documents written in different 
languages. For example, a CLTE system can be 
used in Wikipedia articles to inform lectors which 
information is absent or inconsistent in comparison 
to other page in a different language. 
This new task has to face more additional issues 
than monolingual TE. Among them, we emphasize 
the ambiguity, polysemy, and coverage of the re-
sources. Another additional problem is the necessi-
ty for semantic inference across languages, and the 
limited availability of multilingual knowledge 
resources.  
The CLTE for content synchronization specifi-
cally consist on determining the entailment rela-
tionship between two text fragment T1 and T2 
which are assumed belong a related topic. 
Four alternatives are possible in this relation-
ship: 
- Bidirectional : It is a semantic equivalence be-
tween T1 and T2. 
- Forward : It is an unidirectional entailment 
from T1 to T2. 
- Backward: It is an unidirectional entailment 
from T2 to T1. 
- No Entailment: It means that there is no en-
tailment between T1 and T2. 
The paper is organized as follows: Section 2 de-
scribes the relevant work done on cross-lingual 
textual entailment and related tasks, Section 3 de-
scribes the architecture of the system, then Section 
4 shows experiments and results; and finally Sec-
721
tion 5 summarize some conclusions and future 
work. 
2 Related work  
In this section we briefly describe two tasks that 
are closely related to CLTE. 
 
2.1 Textual Entailment 
 
The objective of the recognizing textual entail-
ment (RTE) task (Dagan et al, 2006) is determin-
ing whether or not the meaning of a ??hypothesis?? 
(H) can be inferred from a ??text?? (T).  
The two-way RTE task consists of deciding 
whether: T entails H, in which case the pair will be 
marked as ??Entailment??, otherwise the pair will 
be marked as ??No Entailment??. This definition of 
entailment is based on (and assumes) average hu-
man understanding of language as well as average 
background knowledge. 
Recently the RTE4 Challenge has changed to a 
three-way task (Bentivogli et al 2009) that consists 
in distinguishing among ??Entailment??, ??Contra-
diction?? and ??Unknown?? when there is no infor-
mation to accept or reject the hypothesis. 
 The RTE challenge has mutated over the years, 
aiming at accomplishing more accurate and specif-
ic solutions; in 2009 the organizers proposed a 
pilot task, the Textual Entailment Search 
(Bentivogli et al 2009), consisting in finding all 
the sentences in a set of documents that entail a 
given Hypothesis and since 2010 there is a Novelty 
Detection Task, which means that RTE systems are 
required to judge whether the information con-
tained in each H is novel with respect to (i.e., not 
entailed by) the information contained in the cor-
pus. 
Thus, the new CLTE task can be thought as a 
generalized problem of RTE, which has to face 
new challenges as scarcity of resources to multi-
lingual scenario, among others issues. 
2.2 Semantic Textual Similarity 
The pilot task STS was recently defined in 
Semeval 2012 (Aguirre et al, 2012) and has as 
main objective measuring the degree of semantic 
equivalence between two text fragments. STS is 
related to both Recognizing Textual Entailment 
(RTE) and Paraphrase Recognition, but has the 
advantage of being a more suitable model for mul-
tiple NLP applications.  
As mentioned before, the goal of the RTE task 
(Bentivogli et al 2009) is determining whether the 
meaning of a hypothesis H can be inferred from a 
text T. The main difference with STS is that STS 
consists in determining how similar two text frag-
ments are, in a range from 5 (total semantic 
equivalence) to 0 (no relation). Thus, STS mainly 
differs from TE and Paraphrasing in that the classi-
fication is graded instead of binary and also STS 
assumes bidirectional equivalence but in TE the 
equivalence is only directional. In this manner, 
STS is filling the gap between TE and Paraphrase. 
2.3 Cross-Lingual Textual Entailment 
There are a few previous works on CLTE, the 
first one was the definition of this new task 
(Mehdad et al, 2010). Afterwards, the creation of 
CLTE corpus by using Mechanical Turk is de-
scribed on (Negri et al, 2011) and a corpus freely 
available for CLTE is published (Castillo, 2011). 
To our knowledge, two approach are proposed 
to address this new challenging task, one consist of 
using machine translation to move on towards 
monolingual textual entailment scenario and then 
apply classic techniques for RTE (Mehdad et al, 
2010;  Castillo and Cardenas, 2011), and the other 
is based on exploit databases of paraphrases 
(Mehdad et al, 2011). Both techniques obtained 
similar results and the accuracy achieved by them 
is not a statically significant difference. 
In previous work (Castillo, 2010; Castillo and 
Cardenas, 2011) we addressed the CLTE focusing 
on English-Spanish language pair and released a 
bilingual textual entailment corpus. This paper is 
based on that work in order to tackling the problem 
across different language pairs Spanish-English 
(SPA-ENG), Italian-English (ITA-ENG), French-
English (FRA-ENG) and German-English (GER-
ENG) and we also used an approach based on ma-
chine translation.  
 
3 System architecture  
Sagan is a CLTE system (Castillo and Cardenas, 
2010) which has taken part of several challenges, 
including the Textual Analysis Conference 2009 
and TAC 2010, and the Semantic Textual Similari-
722
ty Semeval 2012 (Aguirre et al, 2012; Castillo and 
Estrella, 2012) and Cross Lingual Textual Entail-
ment for content synchronization as part of the 
Semeval 2012 (Negri et al, 2012). 
 The system is based on a machine learning ap-
proach and it utilizes eight WordNet-based 
(Fellbaum, 1998) similarity measures with the 
purpose of obtaining the maximum similarity be-
tween two concepts. We used SVM as classifier 
with polynomial kernel. The system determines the 
entailment based on the semantic similarity of two 
texts (T,H) viewed as a function of the semantic 
similarity of the constituent words of both phrases. 
Thereby, we expect that combining word to word 
similarity metrics to text level would be a good 
indicator of text to text similarity.  
These text-to-text similarity measures are based 
on the following word-to-word similarity metrics: 
(Resnik, 1995), (Lin, 1997), (Jiang and Conrath, 
1997), (Pirr? and Seco, 2008), (Wu and Palmer, 
1994), Path Metric, (Leacock and Chodorow, 
1998), and a semantic similarity to sentence level 
named SemSim (Castillo and Cardenas, 2010).  
Additional information about how to produce 
feature vectors as well as each word- and sentence-
level metric can be found in (Castillo, 2011). The 
architecture of the system is shown in Figure 1. 
WordNet
* CLTE_DEU-ENG, 
* CLTE_FRA-ENG, 
* CLTE_SPA-ENG, 
*CLTE_ITA-ENG, 
* CLTE_DEU+FRA+SPA+ITA-
ENG, 
*CLTE_DEU+FRA+SPA+ITA-
ENG+RTE3-TS-CL
* CLTE_DEU-ENG, 
* CLTE_FRA-ENG, 
* CLTE_SPA-ENG, 
*CLTE_ITA-ENG
CLTE
Adaptation Layer
TE 
engine
Entailment 
Result
Bidirectional
Backward
Google 
Traslate
Forward
Knowledge Resources
Web Resources
Training Sets
Test Sets
RTE3-4C+RTE4-4C
RTE3-4C
Training Sets
No 
Entailment
Pre-Processing
Fig.1. System architecture  
In the preprocessing module we performed 
string normalization across different languages by 
using a lookup table for lexical entries, and then 
date and time normalization is carried out. 
CLTE adaption layer is composed by four ma-
chine translation sub-modules that bring back each 
<Ti ,H> pair into the monolingual case ENG-ENG. 
Where Ti can be given in Spanish, German, Italian 
or French. 
The training set used to the submitted runs are 
whose provided by the organizers of the CLTE for 
Content Synchronization Task and a combination 
of RTE datasets, such as it is described in the Sec-
tion Experiments and Results.  
4 Experiments and Results 
The dataset provided by the organizers consists of 
500 CLTE pairs translated to four languages fol-
lowing the crowdsourcing-based methodology 
proposed in (Negri et al, 2011). Also, for test pur-
pose additional 500 pairs are provided. Both da-
tasets are balanced with respect to the four 
entailment judgments (bidirectional, forward, 
backward, and no entailment). 
We also performed experiments using traditional 
RTE datasets. Because of the RTE datasets are 
binary classified as NO (no-entailment) and YES 
(entailment), then we assumed that NO class is 
"no-entailment" and YES class is "forward" in the 
CLTE task. Certainly, the corpus tagged in this 
way will have contradictory information, since 
several pairs classified as forward should be classi-
fied as bidirectional, and also several pairs classi-
fied as no-entailment could be backwards, but the 
objective is experimenting  whether we can gain 
accuracy in our RTE system despite of these (few) 
contradictory cases.  
Additionally, in our experiments we used an al-
gorithm (Castillo,2010) to generate additional 
training data, in other words to expand a data set. It 
is based on a Double Translation Process (dtp) or 
round-trip translation. Double translation process 
can be defined as the process of starting with an S 
(String in English), translating it to a foreign lan-
guage F(S), for example Spanish, and finally back 
into the English source language F-1(S).  
We applied the algorithm starting with RTE3 
and RTE4 datasets. Thus, the augmented corpus is 
denoted RTE3-4C which is tagged according to the 
three-way task composed of: 340 pairs Contradic-
723
tion, 1520 pairs Yes, and 1114 pairs Unknown. In 
the case of the two-way task, it is composed by 
1454 pairs No, and 1520 pairs Yes. 
The other dataset augmented is denoted RTE4-
4C, and has the following composition: 546 pairs 
Contradiction, 1812 pairs Entailment, and 1272 
pairs Unknown. Therefore, in the two-way task, 
there are 1818 pairs No (No Entailment), and 1812 
pairs Yes (Entailment) in this data set. 
The idea behind using RTE3-4C and RTE3-4C 
is providing to our system an increased dataset 
aiming to acquire more semantic variability. 
In our system submission we report the experi-
ments performed with the test sets provided by 
CLTE organizers which is composed by four da-
tasets of 500 pairs each one.  
4.1 Submission to the CLTE shared task  
With the aims of applying the monolingual textual 
entailment techniques, in the CLTE domain, we 
utilized the Google translate as MT system to bring 
back the <T,H> pairs into the monolingual case. 
Then we generated a feature vector for every 
<T,H> pair with both training and test sets, and 
used monolingual textual entailment engine to 
classify the pairs. First we described the dataset 
used and then explain each submitted run. 
The datasets used are listed below: 
 
 - CLTE_Esp+Fra+Ita+Ger: dataset composed 
by all language pairs. 
 - RTE3-TS-CL: a ENG-SPA cross lingual tex-
tual entailment corpus (Castillo,2011) composed 
by 200 pairs (108 Entailment, 32 Contradiction, 60 
Unknown). 
- RTE3-4C: an augmented dataset based on 
RTE3. 
- RTE4-4C: an augmented dataset based on 
RTE4. 
 
Our participation in the shared task consisted of 
four different runs produced with the same feature 
set, and the main differences rely on the amount 
and type of training data. Each run is described 
below: 
 
 - RUN1: system trained on CLTE_Esp+ 
Fra+Ger+Ita corpus in addition to the RTE3-TS-
CL dataset. 
- RUN2: system trained on CLTE_Esp, 
CLTE_Fra, CLTE_Ger and CLTE_Ita corpus. At 
testing phase, the system chooses the right dataset 
according to the language that it is processing. 
-  RUN3: system trained using all training data 
that came from different language pairs.  
We remark that we can combine the training da-
ta because of we used a machine translation sub-
module that bring back each <T,H> pair into the 
monolingual case ENG-ENG. 
-  RUN4: In RUN4 the training set is com-
posed by all pairs of CLTE_Esp+Fra+Ita+Ger and 
RTE3-4C+ RTE4-4C datasets. 
Ten teams participated in this CLTE task, eight 
submitting runs to all language pairs. For Spanish 
28 runs were submitted and 20 runs were submit-
ted for the other languages. The results achieved 
by our system is showed in Table 1. 
 
Team id 
Team 
system 
id 
Score (Accuracy) Run Rank  
SPA-
ENG 
ITA-
ENG 
FRA-
ENG 
DEU-
ENG 
SPA ITA FRA DEU 
Sagan run1 0.342 0.352 0.346 0.342 16 6 9 9 
Sagan run2 0.328 0.352 0.336 0.310 19 7 11 13 
Sagan run3 0.346 0.356 0.330 0.332 14 5 12 12 
Sagan run4 0.340 0.330 0.310 0.310 17 12 13 14 
System 
Rank 
 7 4 5 6 
    
 
The results reported show that our best run is 
ranking above the average for all languages. The 
same situation occurs when ranking the systems, 
except for Spanish where the system is placed on 
7th among 10 teams. 
We achieved the highest result of 0.356 with 
RUN3 in the pair ITA-ENG which is placed fourth 
among participating systems. 
We also note that, in general, training the system 
with the pairs of all datasets achieved better results 
than training separately for each dataset. Further-
more, if we analyze RUN4 vs. RUN3 we can con-
clude that incorporating additional RTE dataset 
produces a very unbalanced dataset resulting in a 
decrease in performance. In (Castillo, 2011) we 
experimented with these expanded datasets over 
monolingual RTE and CLTE tasks and we showed 
gain in performance, thus we suspect that the de-
crease is more due to unbalanced dataset than to 
noise introduced by the double translation process. 
Interesting, the Corpus RTE3-TS-CL dataset uti-
lized in the RUN1 helps to improve the results in 
FRA-ENG and DEU-ENG language pairs. 
724
The Table 2 shows that our system predict with 
high F-measure to bidirectional and no-entailment 
entailment judgments in all language pairs, but has 
problems to distinguish the forward and backward 
entailment judgments.  
 
 It is probably due to our systems is based on 
semantic overlap between T and H, resulting the 
backwards particularly difficult to predict to our 
system.  
 
Run 
id 
Language 
pair 
Precision 
Recall 
 
F-measure Score 
(Accuracy) 
Mean Score- 
all runs  
F B NE BI F B NE BI F B NE BI 
Run3 SPA-ENG 0.23 0.27 0.42 0.42 0.20 0.22 0.45 0.51 0.21 0.25 0.43 0.46 0.346 0.346 
Run3 ITA-ENG 0.31 0.25 0.40 0.46 0.30 0.22 0.51 0.40 0.30 0.23 0.45 0.43 0.356 0.336 
Run1 FRA-ENG 0.24 0.30 0.39   0.43 0.17 0.34 0.57 0.30 0.20 0.32 0.47 0.36 0.346 0.336 
Run1 DEU-ENG 0.25 0.23 0.41 0.44 0.17 0.26 0.60 0.34 0.20 0.25 0.49 0.39 0.342 0.336 
Table 2. Official results for Precision, Recall and F-measure 
 
5 Conclusions and future work 
In this paper we explained our participation in the 
new challenging task of Cross-Lingual Textual 
Entailment (CLTE) for Content Synchronization. 
This task also could presents benefit as a metric for 
machine translation evaluation, as reported in 
(Castillo and Estrella, 2012). 
  This work focuses on CLTE based on Machine 
translation to bring back the problem into the mon-
olingual Textual Entailment (TE) scenario. This 
decoupled approach between Textual Entailment 
and Machine Translation has several advantages, 
such as taking benefits of the most recent advances 
in machine translation, the ability to test the effi-
ciency of different MT systems, as well as the abil-
ity to scale the system easily to any language pair.  
Results achieved are promising and additional 
work is needed in order to address the problem of 
distinguish among forward, backward and bidirec-
tional entailment judgments.  
Future work will be oriented to tackle the prob-
lem with backwards. Finally, we remark the neces-
sity of bigger corpus tagged in four-way 
classification, for all language pairs. 
References  
Ido Dagan, Oren Glickman and Bernardo Magnini. 
2006. The PASCAL Recognising Textual Entailment 
Challenge. In Qui?onero-Candela, J.; Dagan, I.; 
Magnini, B.; d'Alch?-Buc, F. (Eds.) Machine Learn-
ing Challenges. Lecture Notes in Computer Science , 
Vol. 3944, pp. 177-190, Springer. 
M. Negri, A. Marchetti, Y. Mehdad, L. Bentivogli, and 
D. Giampiccolo. 2012. Semeval-2012 Task 8: Cross-
lingual Textual Entailment for Content Synchroniza-
tion. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012). 
L. Bentivogli, P. Clark, I. Dagan, H. T. Dang, and D. 
Giampiccolo. 2010. The Sixth PASCAL Recognizing 
Textual Entailment Challenge. In TAC 2010 Work-
shop Proceedings, NIST, Gaithersburg, MD, USA. 
Y. Mehdad, M. Negri, and M. Federico. 2010. Towards 
Cross-Lingual Textual Entailment. In Proceedings of 
NAACL-HLT 2010. 
Eneko Agirre, Daniel Cer, Mona Diab and Aitor Gonza-
lez-Agirre. 2012. SemEval-2012 Task 6: A Pilot on 
Semantic Textual Similarity. In Proceedings of the 
6th International Workshop on Semantic Evalua-tion 
(SemEval 2012), in conjunction with the First Joint 
Conference on Lexical and Computational Semantics 
(*SEM 2012).  
Bentivogli, Luisa, Dagan Ido, Dang Hoa, Giampiccolo, 
Danilo, Magnini Bernardo.2009.The Fifth PASCAL 
RTE Challenge. In: Proceedings of the Text Analysis 
Conference.  
Fellbaum C. 1998. WordNet: An Electronic Lexical 
Database, volume 1. MIT Press.  
Castillo Julio. 2011. A WordNet-based semantic ap-
proach to textual entailment and cross-lingual textu-
al entailment. International Journal of Machine 
Learning and Cybernetics - Springer, Volume 2, 
Number 3. 
Castillo Julio and Cardenas Marina. 2010. Using sen-
tence semantic similarity based onWordNet in recog-
nizing textual entailment. Iberamia 2010. In LNCS, 
vol 6433. Springer, Heidelberg, pp 366?375. 
Castillo Julio. 2010. A semantic oriented approach to 
textual entailment using WordNet-based measures. 
MICAI 2010. LNCS, vol 6437. Springer, Heidelberg, 
pp 44?55. 
Castillo Julio. 2010. Using machine translation systems 
to expand a corpus in textual entailment. In: Proceed-
ings of the Icetal 2010. LNCS, vol 6233, pp 97?102. 
M. Negri, L. Bentivogli, Y. Mehdad, D. Giampiccolo, 
and A. Marchetti. 2011. Divide and Conquer: 
Crowdsourcing the Creation of Cross-Lingual Textu-
725
al Entailment Corpora. In Proceedings of the Con-
ference on Empirical Methods in Natural. EMNLP 
2011. 
Resnik P. 1995. Information content to evaluate seman-
tic similarity in a taxonomy. In: Proceedings of IJCAI 
1995, pp 448?453. 
Castillo Julio, Cardenas Marina. 2011. An Approach to 
Cross-Lingual Textual Entailment using Online Ma-
chine Translation Systems. Polibits Journal. Vol 44. 
Castillo Julio and Estrella Paula. 2012. Semantic Textu-
al Similarity for MT evaluation. NAACL 2012 Sev-
enth Workshop on Statistical Machine Translation. 
WMT 2012, Montreal, Canada.  
Lin D. 1997.An information-theoretic definition of simi-
larity. In: Proceedings of Conference on Machine 
Learning, pp 296?304. 
Jiang J, Conrath D.1997. Semantic similarity based on 
corpus statistics and lexical taxonomy. In: Proceed-
ings of the ROCLINGX. 
Pirro G., Seco N. 2008. Design, implementation and 
evaluation of a new similarity metric combining fea-
ture and intrinsic information content. In: ODBASE 
2008, Springer LNCS. 
Wu Z, Palmer M. 1994. Verb semantics and lexical 
selection. In: Proceedings of the 32nd ACL 916. 
Leacock C, Chodorow M. 1998. Combining local con-
text and WordNet similarity for word sense identifi-
cation. MIT Press, pp 265?283. 
Hirst G, St-Onge D . 1998. Lexical chains as represen-
tations of context for the detection and correction of 
malapropisms. MIT Press, pp 305?332. 
Banerjee S, Pedersen T. 2002. An adapted lesk algo-
rithm for word sense disambiguation using WordNet. 
In: Proceeding of CICLING-02. 
William B. Dolan and Chris Brockett.2005. Automati-
cally Constructing a Corpus of Sentential Para-
phrases. Third International Workshop on 
Paraphrasing (IWP2005). Asia Federation of Natural 
Language Processing. 
Castillo Julio and Estrella Paula. 2012. SAGAN: An 
approach to Semantic Textual Similarity based on 
Textual Entailment. In Proceedings of the 6th Inter-
national Workshop on Semantic Evaluation 
(SemEval 2012), in conjunction with the First Joint 
Conference on Lexical and Computational Semantics 
(*SEM 2012).  
Mehdad Y., M. Negri, and M. Federico. 2011. Using 
Parallel Corpora for Cross-lingual Textual Entail-
ment. In Proceedings of ACL-HLT 2011. 
 
 
726
Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas,
pages 62?67, Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
A Machine Learning Approach for  
Recognizing Textual Entailment in Spanish 
 
 
Julio Javier Castillo 
National University of C?rdoba 
Ciudad Universitaria, 5000 
C?rdoba, Argentina 
jotacastillo@gmail.com 
  
 
 
 
Abstract 
This paper presents a system that uses ma-
chine learning algorithms for the task of re-
cognizing textual entailment in Spanish 
language. The datasets used include SPARTE 
Corpus and a translated version to Spanish of 
RTE3, RTE4 and RTE5 datasets. The features 
chosen quantify lexical, syntactic and seman-
tic level matching between text and hypothe-
sis sentences. We analyze how the different 
sizes of datasets and classifiers could impact 
on the final overall performance of the RTE 
classification of two-way task in Spanish. The 
RTE system yields 60.83% of accuracy and a 
competitive result of 66.50% of accuracy is 
reported by train and test set taken from 
SPARTE Corpus with 70% split. 
1 Introduction 
The objective of the Recognizing Textual Entail-
ment Challenge is determining whether the mean-
ing of the Hypothesis (H) can be inferred from a 
text (T) (Ido Dagan et al, 2006). This challenge 
has been organized by NIST in recent years.  
Another related antecedent was Answer Valida-
tion Exercise (AVE), part of Cross Language 
Evaluation Forum (CLEF), whose objective is to 
develop systems which are able to decide whether 
the answer to a question is correct or not (Pe?as et 
al, 2006). It was a three year-old track, from 2006 
to 2008. 
AVE challenge was an evaluation framework 
for Question Answering (QA) systems to promote 
the development and evaluation of subsystems 
aimed at validating the correctness of the answers 
given by a QA system. The Answer Validation 
task must select the best answer for the final out-
put. There is a subtask for each language involved 
in QA, the Spanish is one of these. Thus, AVE task 
is very similar to RTE (Recognition of Textual 
Entailments). 
In this paper, we address the RTE task problem 
of determining the entailment value between Text 
and Hypothesis pairs in Spanish, applying machine 
learning techniques.  
In the past, RTEs Challenges machine learning 
algorithms were widely used for the task of recog-
nizing textual entailment (Marneffe et al, 2006; 
Zanzotto et al, 2007; Castillo, 2009) and they have 
reported goods results for English language. Also, 
our system applies machine learning algorithms to 
the Spanish. 
We built a set of datasets based on public avail-
able datasets for English, together to SPARTE 
(Pe?as et al 2006), an available Corpus in Spanish. 
This corpus contains 2962 hypothesis with a doc-
ument label and a True/False value indicating 
whether the document entails the hypothesis or not. 
Up to our knowledge, SPARTE corpus in the only 
corpus aimed at evaluating RTE systems in Span-
ish. 
Finally, we generated a feature vector with the 
following components for both Text and Hypothe-
sis: Levenshtein distance, a lexical distance based 
on Levenshtein, a semantic similarity measure 
Wordnet based, and the LCS (longest common 
62
substring) metric; in order to characterize the rela-
tionships between the Text and the Hypothesis. 
The remainder of the paper is organized as fol-
lows. Section 2 shows the system description, whe-
reas Section 3 describes the results of experimental 
evaluation and discussion of them. Section 4 dis-
cusses opportunities of collaboration. Finally, Sec-
tion 5 summarizes the conclusions and lines for 
future work. 
2 System  Description 
This section provides an overview of our system 
which is based on a machine learning approach for 
recognizing textual entailment to the Spanish. The 
system produces feature vectors for the available 
development data RTE3, RTE4, RTE5, and 
SPARTE(Pe?as et al 2006). Weka (Witten and 
Frank, 2000) is used to train classifiers on these 
feature vectors. 
The SPARTE Corpus, was built from the Span-
ish corpora used at Cross-Language Evaluation 
Forum (CLEF) for evaluating QA systems during 
the years 2003, 2004 and 2005. This corpus con-
tains 2962 hypothesis with a True/False value indi-
cating whether the document entails the hypothesis 
or not. 
Due to, all available dataset of PASCAL Text 
Analysis Conference were in English, we trans-
lated every dataset to Spanish by using an online 
translator engine1. So, we had a Spanish dataset but 
with some translation errors provided by the trans-
lator. It is important to note, that the ?quality? of 
the translation is given by the Translator engine, 
and we suppose that the sense of the sentence 
should not be modified by the Translator. Indeed, it 
is the situation for the majority of the cases that we 
analyzed. The new datasets were named RTE3-Sp 
(Spanish), RTE4-Sp, and RTE5-Sp. 
The following example is the pair number 799 
from RTE3-Sp with False as entailment value. 
 
Text: 
Otros dos marines, Tyler Jackson y Juan Jodka 
III, ya han se declar? culpables de asalto agra-
vantes y conspiraci?n para obstruir la justicia y 
fueron condenados a 21 meses y 18 meses, respec-
tivamente. 
 
                                                        
1 http://www.microsofttranslator.com/ 
Hypothesis:  
 Tyler Jackson ha sido condenado a 18 meses. 
 
This example shows a little noisy (and a minimal 
syntactic error) in the translation of the Text to 
Spanish (instead of ?ya han se declar?? should be 
?ya se han declarado?); but the whole meaning was 
not changed. 
Also, we show a pair example (pair id=3) taken 
from Sparte Corpus with False as entailment value: 
 
Text: ?Cu?l es la capital de Croacia? 
 
Hypothesis :   
  La capital de Croacia es ONU. 
 
In a similar way, all pairs from SPARTE belong to 
QA task and these are syntactically simpler than 
RTE?s Corpus pairs. 
Additionally, we generate the following devel-
opment sets: RTE3-Sp+RTE4-Sp, and SPARTE-
Bal+RTE3-Sp+RTE4-Sp in order to train with dif-
ferent corpus and different sizes.  In all cases, 
RTE5-Sp TAC 2009 gold standard dataset was 
used as test-set.  
Also, we did additional experiments with 
SPARTE, using cross-validation technique and 
percentage split method, in order to test the accu-
racy of our system taking only this corpus as de-
velopment and training set. 
 
2.1 Features 
We experimented with the following four ma-
chine learning algorithms: Support Vector Ma-
chine (SVM), Multilayer Perceptron(MLP), 
Decision Trees(DT) and AdaBoost(AB).  
The Decision Trees are interesting because we 
can see what features were selected from the top 
levels of the trees. SVM and AdaBoost were se-
lected because they are known for achieving high 
performances, and MLP was used because it has 
achieved high performance in others NLP tasks.  
We experimented with various settings for the 
machine learning algorithms, including only the 
results for the best parameters. 
We generated a feature vector with the follow-
ing components for every possible <T,H>: Le-
venshtein distance, a lexical distance based on 
Levenshtein, a semantic similarity measure Word-
63
net based, and the LCS (longest common sub-
string) metric. 
We chose only four features in order to learn 
the development sets, having into account that 
larger feature sets do not necessarily lead to im-
proving classification performance because it 
could increase the risk of overfitting the training 
data.  
Below the motivation for the input features: 
Levenshtein distance is motivated by the good re-
sults obtained as a measure of similarity between 
two strings. Using stems, this measure improves 
the Levenshtein over words. The lexical distance 
feature based on Levenshtein distance is interesting 
because works to a sentence level. Semantic simi-
larity using WordNet is interesting because of the 
capture of the semantic similarity between T and H 
to sentence level. Longest common substring is 
selected because it is easy to implement and pro-
vides a good measure for word overlap.  
2.2 Lexical Distance 
The standard Levenshtein distance is a string me-
tric for measuring the amount of difference be-
tween two strings. This distance quantifies the 
number of changes (character based) to generate 
one text string (T) from the other (H). The algo-
rithm works independently from the language that 
we are analyzing. 
We used a Spanish Stemmer that stems words 
in Spanish based on a modified version of the 
Snowball algorithm2. 
Additionally, by using Levenshtein distance we 
defined a lexical distance and the procedure is the 
following: 
? Each string T and H are divided in a list of 
tokens. 
? The similarity between each pair of tokens 
in T and H is performed using the Le-
venshtein distance over stems. 
? The string similarity between two lists of 
tokens is reduced to the problem of ?bipar-
tite graph matching?, performed using the 
Hungarian algorithm (Kuhn, 1955) over 
this bipartite graph. Then, we found the as-
signment that maximizes the sum of rat-
ings of each token. Note that each graph 
node is a token of the list. 
                                                        
2 http://snowball.tartarus.org/ 
The final score is calculated by:  
))(),(( HLengthTLengthMax
TotalSim
finalscore ?  
 
Where: 
TotalSim is the sum of the similarities with 
the optimal assignment in the graph. 
Length (T) is the number of tokens in T. 
Length (H) is the number of tokens in H. 
2.3 Wordnet Distance 
Since, all datasets are in Spanish, we need to con-
vert <T, H> pair to English. In the case of RTEs-
Sp datasets, this action will backward to the Eng-
lish language (source).  
Our ideal case would be to use EuroWordNet3 
to obtain the semantic information that we need, 
but we won?t be able to access to this resource.  
Thus, WordNet is used to calculate the seman-
tic similarity between T and H. The following pro-
cedure is applied: 
1. Word sense disambiguation using the Lesk 
algorithm (Lesk, 1986), based on Wordnet defini-
tions. 
2. A semantic similarity matrix between words 
in T and H is defined. Words are used only in syn-
onym and hyperonym relationship. The Breadth 
First Search algorithm is used over these tokens; 
similarity is calculated by using two factors: length 
of the path and orientation of the path. 
3. To obtain the final score, we use matching 
average. 
The semantic similarity between two words is 
computed as: 
)()(
)),((
2),(
tDepthsDepth
tsLCSDepth
tsSim
?
??  
Where: s,t are source and target words that we 
are comparing (s is in H and t is in T). Depth(s) is 
the shortest distance from the root node to the cur-
rent node. LCS(s,t):is the least common subsume 
of s and t. 
The matching average (step 3) between two 
sentences X and Y is calculated as follows: 
)()(
),(
2
YLengthXLength
YXMatch
erageMatchingAv
?
??  
 
                                                        
3 http://www.illc.uva.nl/EuroWordNet/ 
64
2.4 Longest Common Substring 
Given two strings, T of length n and H of length m, 
the Longest Common Sub-string (LCS) problem 
(Dan, 1999) will find the longest string that is a 
substring of both T and H. It is found by dynamic 
programming.  
 
 
 
 
 
3 Experimental Evaluation and Discus-
sion of the Results 
 
With the aim of exploring the differences among 
training sets and machine learning algorithms, we 
did many experiments looking for the best result to 
our system. 
First, we converted the RTE4 and RTE5 data-
sets with Contradiction/Unknown/Entailment pair 
information to a binary True/False problem, named 
two-way problem. 
Then, we used the following combination of da-
tasets: RTE3-Sp, RTE4-Sp, RTE3-Sp+RTE4-Sp, 
SPARTE-Bal (balanced SPARTE Corpus with the 
same number of true and false cases), and 
SPARTE-Bal+ RTE3-Sp+RTE4-Sp. The training 
set SPARTE-Balanced was created by taking all 
true cases and randomly taking false cases, and 
then we build a balanced training set containing 
1352 pairs, with 676 true and 676 false pairs. 
We used four classifiers to learn every devel-
opment set: (1) Support Vector Machine, (2) Ada 
Boost, (3) Multilayer Perceptron (MLP) and (4) 
Decision Tree using the open source WEKA Data 
Mining Software (Witten & Frank, 2005). In all the 
tables results we show only the accuracy of the 
best classifier. 
The results obtained to predict RTE5-Sp in a 
two-way classification task are summarized in Ta-
ble 1 below. In addition, table 2 shows our results 
reported in RTE two-way classification task by 
using with Cross Validation technique with 10 
folds. 
 
 
 
 
 
Dataset Classifier Accuracy% 
RTE3-Sp+RTE4-Sp SVM 60.83% 
RTE3-Sp SVM 60.50% 
RTE4-Sp MLP 60.50% 
SPARTE-Bal+ 
RTE3-Sp+RTE4-Sp 
MLP 60.17% 
SPARTE-Bal DT 50% 
Baseline - 50% 
Table 1.Results obtained in two-way classification task. 
 
Dataset Classifier Accuracy% 
SPARTE-Bal DT 68.19% 
RTE3-Sp SVM 66.50% 
RTE3-Sp+RTE4-Sp MLP 61.44% 
RTE4-Sp MLP 59.60% 
SPARTE-Bal+ 
RTE3-Sp+RTE4-Sp 
AdaBoost 56.83% 
Baseline - 50% 
Table 2.Results obtained with Cross Validation 10 folds 
in two-way task. 
 
The performance in all cases was clearly above 
those baselines. Only when using SPARTE-Bal we 
obtained a result equal to the baseline (50% true 
pairs and 50% false pairs). 
The SPARTE-Balanced dataset yields the worst 
results, maybe because this dataset contains only 
pairs with QA task, and an additional reason, could 
be that SPARTE is syntactically simpler than 
PASCAL RTE. In that sense, some authors have 
reported low performance when using syntactically 
simpler datasets; for instance, by using BPI4 data-
set to predict RTEs datasets in English. Therefore, 
SPARTE seems to be not enough good training set 
to predict RTEs test sets. 
The best performance of our system was 
achieved with SVM classifier with RTE3-
Sp+RTE4-Sp dataset; it was 60.83% of accuracy. 
In the majority of the cases, SVM or MLP classifi-
ers appear as ?favorite? in all classification tasks. 
Surprisingly, in the two-way task, a slight and 
not statistical significant difference of 0.66% be-
tween the best and worst combination (except for 
SPARTE-Bal) of datasets and classifiers is found. 
So, it suggests that the combination of dataset and 
classifiers do not produce a strong impact predict-
ing RTE5-Sp, at least, for these feature sets. 
 
                                                        
4 http://www.cs.utexas.edu/users/pclark/bpi-test-suite/ 
))(),(min(
)),((
),(
HLengthTLength
HTMaxComSubLength
HTlcs ?
65
Also, we observed that by including SPARTE-Bal 
to RTE3-Sp+RTE4-Sp dataset, the performance 
slightly decreases, although this difference was not 
statistical significant. 
The results obtained in table 2(and table 4) with 
SPARTE-Bal and decision tree algorithm, are the 
best for cross-validation experiments. In fact, an 
accuracy of 68.19% was obtained, which is 
18.19% bigger than the result obtained in table 1, 
and was statistical significant. 
Finally, we assessed our system only over the 
SPARTE Corpus.  First, we used cross validation 
technique with ten folds over SPARTE-Bal, testing 
over our four classifiers. Then, we tested 
SPARTE-Bal by splitting the corpus in training set 
(70%), and test set (30%). 
 The results are shown in the tables 4 and 5 below. 
 
Classifier Accuracy% 
DT 68.19% 
MLP 62.64% 
AdaBoost 61.31% 
SVM 60.35% 
Baseline 50% 
Table 4.Results obtained with Cross Validation 10 folds 
in two-way task to predict SPARTE. 
 
 
Classifier Accuracy% 
DT 66.50% 
AdaBoost 62.31% 
SVM 59.60% 
MLP 52.70% 
Baseline 50% 
Table 5.Results obtained with SPARTE with split 70%. 
 
The results on cross-validation are better than 
those obtained on test set, which is most probably 
due to overfitting of classifiers. 
Table 5 shows a good performance of 66.50%, 
predicting test set and using Decision trees. These 
results are opposed to the bad performance re-
ported by SPARTE to predict RTEs datasets. Here, 
in fact, the syntactic complexity and original task 
do not change between train and test set; and it 
seems to be the main problem with the low per-
formance of SPARTE in Table 1.  
 
 
3.1 Related Work 
Up to our knowledge, there are not available re-
sults of other teams that used SPARTE to predict 
RTE, or used RTEs applied to Spanish.  However, 
some comparison with other results for Spanish 
could be done in AVE Challenge (Alberto T?llez- 
Valero et al, 2008; Ferr?ndez et al, 2008; Castillo, 
2008), but we will need to modify our system to 
test AVE 2008 test set and computing different 
metric for the ranking of the result.  
On the other hand, comparing the results ob-
tained with English in RTE5 TAC Challenge, we 
obtained a result not statistical significant with re-
spect to the median score for English systems that 
is 61.17% of accuracy. Also, our system could be 
compared to independent-language RTE systems. 
To finish, we think that several improvements 
could be done in order to improve the accuracy of 
the system, using syntactic features, more semantic 
information, and new external resources such as 
Acronyms database.  
4 Opportunities  for Collaboration 
Our work is oriented to create a Textual Entailment 
System. Such system could be used by another sys-
tem or teams of others Universities, as an internal 
module.  
The entailment relations between texts or 
strings are very useful for a variety of Natural 
Language Processing applications, such as Ques-
tion Answering, Information Extraction, Informa-
tion Retrieval and Document Summarization. 
For example, a RTE module could be used in a 
Question Answering system, where the answer of a 
question must be entailed by the text that supports 
the correctness of the answer; or an Automatic 
Summarization system could eliminate the passag-
es whose meaning is already entailed by other pas-
sages and, by this way, reduce the size of the 
passages. 
In addition, a question answering system could 
be enhanced by a RTE module, and also, these re-
sults are useful as Answer Validation System.  
Our system was designed having in mind the 
interoperation among systems. Thus, the system 
inputs accept files in .xml format, and the output is 
text plain files and .xml files. 
On the other hand, one of the resources that 
would allow this work advance is the EuroWord-
66
net, because it could provide additional semantic 
information improving our semantic features, and 
so the performance of our system. Due to being an 
expensive and not freely available resource, we are 
avoiding using it, but we expect to be able to use it 
in the future. In section 3, we used Wordnet in or-
der to obtain the relationship between two different 
concepts. Since Wordnet includes only synsets for 
English and not for Spanish, we have translated the 
<t,h> pairs to English using the online Microsoft 
Bing translator5, in order to use Wordnet. As a re-
sult, a loss of performance was obtained. We be-
lieve that the use of EuroWordNet could benefit 
our semantic features. 
Currently, we are keeping improving our sys-
tem, and we are looking forward to get opportuni-
ties for collaboration with other teams of all the 
Americas. 
5 Conclusion and Future work  
In this paper we present an initial RTE System 
based for the Spanish language, based on machine 
learning techniques that uses some of the available 
textual entailment corpus and yields 60.83% of 
accuracy.  
One issue found is that SPARTE Corpus seems 
to be not useful to predict RTEs-Sp datasets, be-
cause of the syntactic simplicity and the absence of 
task information different to QA task.  
On the other hand, we found that a competitive 
result of 66.50%acc is reported by train and test set 
taken from SPARTE Corpus. 
Future work is oriented to experiment with ad-
ditional lexical and semantic similarities features 
and to test the improvements they may yield. Also, 
we must explore how to decrease the computation-
al cost of the system. Our plan is keeping applying 
machine learning algorithms, testing with new fea-
tures, and adding new source of knowledge.  
References  
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danillo 
Giampiccolo, and Bernardo Magnini. 2009. The Fifth 
PASCAL Recognizing Textual Entailment Challenge. 
In proceedings of Textual Analysis Conference 
(TAC). NIST, Maryland USA.  
Adrian Iftene, Mihai-Alex Moruz.2009. UAIC Partici-
pation at RTE5, TAC 2009, Gaithersburg, Maryland, 
USA. 
                                                        
5 http://www.microsofttranslator.com/ 
S. Mirkin, R. Bar-Haim, J. Berant, I. Dagan, E. Shnarch, 
A. Stern, and I. Szpektor.2009. Bar-Ilan University?s 
submission to RTE5, TAC 2009, Gaithersburg, Mary-
land, USA. 
Castillo, Julio. Sagan in TAC2009: Using Support Vec-
tor Machines in Recognizing Textual Entailment and 
TE Search Pilot task. TAC 2009, Gaithersburg, Mar-
yland, USA. 
Marie-Catherine de Marneffe, Bill MacCartney, Trond 
Grenager, Daniel Cer, Anna Rafferty and Christopher 
D. Manning. 2006. Learning to distinguish valid tex-
tual entailments. RTE2 Challenge, Italy. 
F. Zanzotto, Marco Pennacchiotti and Alessandro Mo-
schitti.2007. Shallow Semantics in Fast Textual En-
tailment Rule Learners, RTE3, Prague. 
Ian H. Witten and Eibe Frank. 2005. ?Data Mining: 
Practical machine learning tools and techniques", 
2nd Edition, Morgan Kaufmann, San Francisco, 
USA. 
Anselmo Pe?as, Alvaro Rodrigo, Felisa Verdejo. 
SPARTE, a Test Suite for Recognising Textual En-
tailment in Spanish. Cicling 2006, Mexico. 
Pe?as A., Rodrigo A., Sama V., and Verdejo F. Over-
view of the Answer Validation Exercise 2006, In-
Working notes for the Cross Language Evaluation 
Forum Workshop (CLEF 2006), September 2006, 
Spain. 
Ido Dagan, Oren Glickman and Bernardo Magnini. The 
PASCAL Recognising Textual Entailment Challenge. 
In Qui?onero-Candela, J.; Dagan, I.; Magnini, B.; 
d'Alch?-Buc, F. (Eds.) Machine Learning Chal-
lenges. Lecture Notes in Computer Science , Vol. 
3944, pp. 177-190, Springer, 2006. 
M. Lesk. Automatic sense disambiguation using ma-
chine readable dictionaries: How to tell a pine cone 
from a ice cream cone. In SIGDOC ?86, 1986. 
Harold W. Kuhn, The Hungarian Method for the 
assignment problem, Naval Research Logistics Quar-
terly. 1955 
Alberto T?llez-Valero, Antonio Juarez-Gonzalez, Ma-
nuel Montes-y-Gomez, Luis Villasenior-Pineda. 
INAOE at QA@CLEF 2008:Evaluating Answer Va-
lidation in Spanish Question Answering. CLEF 2008. 
Julio J. Castillo. The Contribution of FaMAF at 
QA@CLEF 2008.Answer ValidationExercise.CLEF 
2008. 
Oscar Ferr?ndez, Rafael Mu?oz, and Manuel Palomar. 
A Lexical Semantic Approach to AVE. CLEF 2008. 
67
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 52?58,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
 Semantic Textual Similarity for MT evaluation  Julio Castillo??               Paula Estrella? ?FaMAF, UNC, Argentina ??UTN-FRC, Argentina jotacastillo@gmail.com  pestrella@famaf.unc.edu.ar      Abstract 
This paper describes the system used for our participation in the WMT12 Machine Transla-tion evaluation shared task.  We also present a new approach to Machine Translation evaluation based on the recently defined task Semantic Textual Similarity. This problem is addressed using a textual entail-ment engine entirely based on WordNet se-mantic features. We described results for the Spanish-English, Czech-English and German-English language pairs according to our submission on the Eight Workshop on Statistical Machine Translation. Our first experiments reports a competitive score to system level. 1 Introduction The evaluation of Machine Translation (MT) has become as important as MT itself over the last few years. This is evidenced by the fact that there are now specific forums to present and test new met-rics, such as the Workshop for Statistical MT (WMT) or the NIST MetricsMatr. Every year a vast number of MT metrics are created, the majori-ty being automatic, and seeking to find an efficient, low labor-intensive and reliable evaluation method as an alternative to human-based evaluation.  Automatic metrics employ different evaluation strategies: classical MT automatic metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington. 2002), WER (Tillmann et al, 1997), PER (Nie?en et al, 2000) are language-independent based on n-gram matching (considering or not the ordering of words in a sentence); other use some kind of lan-guage-specific knowledge, for example METEOR (Banerjee et al, 2005), which uses WordNet to 
match synonyms if exact matchings do not occur, and METEOR-NEXT (Denkowski et al, 2010) that, in addition to METEOR?s features, incorpo-rates paraphrases; and more sophisticated metrics use deeper linguistic information, as for example the DCU-LFG metric (Yifan et al, 2010).  However, relatively few attempts have been made to use semantic information for MT evalua-tion. Moreover, only one work has been published about using semantic equivalence (known as Tex-tual Entailment) of texts for MT evaluation. In this work we propose an improved metric, based on TE features, that indicates to what extent a candidate sentence is equivalent to a reference. The paper is organized as follows: Section 2 de-scribes the relevant work done on semantic orient-ed MT evaluation, Section 3 describes the architecture of the system to compute our metric, then Section 4 relates TE and semantic textual similarity to MT, and Section 5 presents some results obtained with our TE-based metric; and finally Section 6 summarize some conclusions and future work. 2 Related work  Given the vast literature in the field of MT evalua-tion, in this section we briefly mention a few at-tempts to evaluate MT based on semantic features, which we deem most recent and important. 2.1 Semantics for MT evaluation Gim?nez and M?rquez (2007) present a set of met-rics operating over shallow semantic structures, which they call linguistic elements, with the idea that a sentence can be seen as a ?bag? of LEs. Pos-sible LEs are word forms, part-of-speech tags, dependency relationships, syntactic phrases, named 
52
entities, semantic roles, etc. The metrics calculate the similarity of a candidate to one or more refer-ences by calculating the overlap and matches of LEs, and the resulting score is the highest obtained from the individual comparisons to each reference. The shallow-semantic evaluation is performed by computing the matching and overlap of named entities and semantic roles, after automatically annotating the sentences.  Following this work, Gim?nez and M?rquez (2009) propose the family of metrics discourse representation structure (DRS) based on the Dis-course Representation Theory of Kamp (1981), where a discourse is represented in structure that is essentially a variation of first-order predicate cal-culus. These sets of metrics are then used to evalu-ate poor quality MT, concluding that semantic oriented metrics are more stable at the system lev-el, while at the sentence level their performance decreases (probably due to external factors, for example if a parse tree of the sentence is not avail-able, the metric cannot be computed). More recently, Lo and Wu (2011) present a new semi-automated metric, MEANT, that assesses translation utility by matching semantic role fillers. Their hypothesis is that a good translation is one that lets a reader get the central information of the sentence. Conceptually, MEANT is defined in terms of f-score, calculated by averaging the trans-lation accuracy for all frames in the MT output across the number of frames in the MT out-put/reference translations. To determine the trans-lation accuracy for each semantic role filler in the reference and machine translations, they ask hu-mans to indicate if a role filler translation is cor-rect, incorrect or partially correct, hence being a semi-automatic metric. According to Lo and Wu (2011) MEANT can be run using inexpensive un-trained monolingual human judges and yet it corre-lates with human judgments on adequacy as well as other labor-intensive metrics, such as HTER (Snover et al, 2006), which needs to train humans to find the closest right translation.  2.2 Textual Entailment in MT  Textual Entailment (TE) is defined as a generic framework for applied semantic inference, where the core task is to determine whether the meaning of a target textual assertion (hypothesis, H) can be inferred from a given text (T). For example, given the pair (H,T): 
H: The Tunisian embassy in Switzerland was at-tacked T: Fire bombs were thrown at the Tunisian embas-sy in Bern we can conclude that T entails H.  The recently created challenge ?Recognising Textual Entailment? (RTE) started in 2005 with goal of providing a binary answer for each pair (H,T), namely whether there is entailment holds or not (Dagan et al, 2006). The RTE challenge has mutated over the years, aiming at accomplishing more accurate and specific solutions; for example, 2008 a three-way decision was proposed (instead of the original binary decision) consisting of ?en-tailment?, ?contradiction? and ?unknown?; in 2009 the organizers proposed a pilot task, the Textual Entailment Search (Bentivogli et al, 2009), con-sisting in finding all the sentences in a set of doc-uments that entail a given Hypothesis and since 2010 there is a Novelty Detection Task, which means that RTE systems are required to judge whether the information contained in each H is novel with respect to (i.e., not entailed by) the in-formation contained in the corpus. This task is quite close to the goal of MT and MT evaluation given that a correct translation should be semantically equivalent to its reference, and thus both translations should entail each other. Despite this close relation, at present there are only two works using TE in MT, namely Mirkin et al (2009) proposes to handle OOV(Out-of-vocabulary words) terms by generating alternative source sentences for translation but instead of simply using paraphrases they use entailed texts; the other contribution is by Aziz et al (2010), in which TE features are integrated into standard SMT workflow (i.e. they dynamically generate alternative entailed words to replace OOVs). More directly related to our work, is that of Pad? et al, (2009) that uses TE to evaluate MT. The main idea is to find out if the translation para-phrases (entails) the reference using entailment features. This is implementing by checking for entailment both from the candidate to the reference and from the reference to the candidate; best can-didates are thus assumed to be those that both en-tail and are entailed by the references and worst candidates are assumed to be those that neither entail the references nor are entailed by these ref-erences. Pad? et al (2009a) found that entailment-
53
based features extracted from partially ill-formed translations are sufficiently robust to be predictive for translation quality. Our approach differs from that of Pad? et al (2009) in that we do not have a binary entailment relation; instead we try to state in a scale of 0 ? 5 the degree of similarity between a candidate and a reference. This approach has very recently been proposed as a new task of the Semantic Evaluation Exercises 2012, called Semantic Textual Similarity (STS) by Aguirre et al (2012) and is explained in more detail in Section 4.  3 System architecture  Sagan is a RTE textual entailment system which has taken part of several challenges, including the Textual Analysis Conference 2009 and TAC 2010, and the Semantic Textual Similarity (Castillo and Estrella, 2012) and Cross Lingual Textual Entail-ment for content synchronization (Castillo and Cardenas, 2012) as part of the *SEM 2012 Task8 (Negri et al, 2012). The system is based on a machine learning ap-proach for STS. We adapted this system to produce feature vectors for all MT outputs for all language pairs ES-EN, DE-EN, FR-EN and CS-EN. It is worth noting that we work on all pairs into English because the system was run in a  monolingual set-ting to take advantage of all the resources available for EN. This Semantic Textual Similarity engine utilizes eight WordNet-based similarity measures, as ex-plained in (Castillo, 2011), with the purpose of obtaining the maximum similarity between two concepts. These text-to-text similarity measures are based on the following word-to-word similarity metrics: (Resnik, 1995), (Lin, 1997), (Jiang and Conrath, 1997), (Pirr? and Seco, 2008), (Wu & Palmer, 1994), Path Metric, (Leacock & Chodor-ow, 1998), and a semantic similarity to sentence level named SemSim (Castillo and Cardenas, 2010).  Additional information about how to produce feature vector and metric to word and sentence level can be found in (Castillo, 2011). The output of the system as modified for this workshop, is a similarity score between 5 and 0, where 5 means a perfect semantic similarity (ap-plied to MT it means that a candidate is indeed a good translation) and 0 means that there is no se-
mantic similarity between the pair, i.e. in MT terms, the candidate is not a translation. The architecture of the system is shown in Fig-ure 1.  
Pre-Processing
Result
Similarity Score
Testset: 
Lenguage 
Pairs XX->EN
Word Level Semantic Metrics
Feature Extraction
SVM with 
Regression
Training Set:  
MSRPC_STS
RUN 1 
Normalizer Stemming Parser
Resnik SemSimW&PLin ...
Gold 
Reference-
EN
MLP
Sentence Level Semantic Metric
 Fig.1. STS system architecture for MT evaluation  The system computes the semantic similarity of two texts (T,H) as a function of the semantic simi-larity of the constituent words of both phrases. A graph matching algorithm is used to determine the overall similarity between two text fragments. As a result, a text to text similarity measure is built based on word to word similarity. It is as-sumed that combining word to word similarity metrics to text level would be a good indicator of text to text similarity. 4 Sagan for MT evaluation Sagan for MT evaluation is based on a core devel-opment to approach the Semantic Textual Similari-ty task(STS). The pilot task STS was recently defined in Semeval 2012 (Aguirre et al, 2012) and has as main objective measuring the degree of semantic equivalence between two text fragments. STS is related to both Recognizing Textual En-tailment (RTE) and Paraphrase Recognition, but 
54
has the advantage of being a more suitable model for multiple NLP applications.  As mentioned before, the goal of the RTE task (Bentivogli et al, 2009) is determining whether the meaning of a hypothesis H can be inferred from a text T. Thus, TE is a directional task and we say that T entails H, if a person reading T would infer that H is most likely true.  The difference with STS is that STS consists in determining how similar two text fragments are, in a range from 5 (total semantic equivalence) to 0 (no relation). Thus, STS mainly differs from TE and Paraphrasing in that the classification is graded instead of binary. In this manner, STS is filling the gap between TE and Paraphrase. In view of this, our claim is that the output of MT systems will be more strongly correlated with humans if we have a higher STS score between MT system output and the reference translation.  To apply Sagan to MT evaluation, we first, pre-process the pairs from Microsoft Research Para-phrase Corpus (Dolan and Brockett, 2005) with dates and time normalization, and then optional modules are applied depending on the metric we want to calculate. Second, we compute 8 sentence level semantic features, and, finally, for every segment generated by systems participating at WMT 2012, we determine the semantic similarity score between that output and the given reference translation. The scores are then normalized to a value in the range 0 ? 1. 5 Experiments and results For the WMT 2012 we participated in the Czech-English and Spanish-English evaluation task but we did not have enough time to extensively test our metric on a diverse range of settings (i.e. dif-ferent corpora and language pairs), given that it was developed for the STS task, which released the data and results only a couple of months ago. However, we are now running experiments to get a better picture of the metric's ability to rate translation quality. In this section we report results obtained by training the system on the WMT 2011 data and testing on the news test portion, only for the Spanish-English pair. Although the system handles both SVM with regression and MLP clas-sifiers, well known to have good performance on natural language applications, we only submit the results obtained using SVM with regression due to 
previous experiments that consistently showed higher accuracy using SVM instead of MLP. At the system level, we calculated the Spearman Rank Correlation Coefficient (?) to compare our metric's behavior with respect to the human based metric applied in WMT 2011. The result is ? = 0.96 indicating a strong positive correlation. More-over, we successfully reproduce the systems rank-ing given by humans regarding the best and worst systems.   System Id Human score Sagan score online-B 0.72 0.71 online-A 0.72 0.71 systran 0.66 0.7 koc 0.67 0.69 alacant 0.66 0.69 rbmt-1 0.63 0.69 rbmt-4 0.6 0.69 rbmt-3 0.61 0.69 uedin 0.51 0.68 rbmt-2 0.6 0.68 upm 0.5 0.68 rbmt-5 0.51 0.68 ufal-um 0.47 0.67 cu-zeman 0.16 0.59 hyderabad 0.17 0.58 Table 1.  Sagan's score for ES-EN WMT 2011 news test set.  When correlating our metric to other automatic metrics, we find that it better correlates with Mete-or-Rank and Adq (Denkowski and Lavie, 2011), Tesla-b (Dahlmaier et al, 2011) and MPF (Popo-vic, 2011), with a correlation coefficient of 0.96. On the other hand, the worst correlations are found against Tesla-f, F15 (Bicici and Yuret, 2011) and the TER baseline (Snover et al, 2006).  We also performed experiments to segment lev-el with the language pair ES-EN. We used the MSR_STS as training set and the newstest2011 from WMT 2011 as test set. MSR_STS1 is com-posed by 750 sentence pairs with a graded seman-tic relationship ranging from 5 (equivalence) to 0 (no-equivalence). As result, we obtained a Kendall-tau correlation coefficient of 0.29 to segment-level for translations                                                             1 http://www.cs.york.ac.uk/semeval-2012/task6/ 
55
into English. These preliminary results, although low, shows that STS and Textual Entailment could be used to address the problem of MT evaluation. Clearly, further improvements are needed and we suspect that higher score can be reached using bigger training data. We also remark the necessity of larger corpus of STS providing a graded score among sentences. At the segment level, we show in Table 2 some examples found by manually inspecting the results.  Example Number MT out-put Texts Sagan score 2397 Reference Adelaida, 4 years old, wants a doll or a bicycle, while her sister Isabel, 3 years old, would like a Barbie doll. 
0.95 
Online-A Adelaide, of 4 years, want a doll or a bicy-cle, while his sister Isabel, 3 years, would like a Barbie doll. 2417 Reference "I strongly rely on the Charter."  
0.18 
Online-A "Me I based mainly on the letter." 45 Reference But there is a snag in that.  
0.105 
Alacant However, there is a fly in the ointment. 1510 Reference Unfortunately, even Scarlett Johansson might struggle to raise China's subterranean regard for these city squads. 
0.5206 
cu-zeman Lamentablemente, until scarlett johans-son should fight to increase the ?nfimo respect of china for with these es-cuadrones the city. Table 2. Sagan's score for some illustrative ES-EN WMT 2011 example pairs showing the score between MT outputs and the reference translation. The example number 2397 shows a sentence that achieves a high score (0.95) but that has an 
agreement error (marked in bold), that prevented Sagan from assigning the highest score. Otherwise, the instance number 2417 has a score of 0.18 showing that Sagan correctly penalizes ill-formed or meaningless sentences. Similarly, the example number 45 has a very low score which quantifies the dissimilarity with the reference translation. Finally, the last example provided shows that the translation remains words in the original Span-ish language (marked in bold). This manual inspection will be complemented with a deeper study of the correlations at the sen-tence level. 6 Conclusions and future work In this paper we introduced a new metric for MT evaluation based on Semantic Textual Similarity computed over textual entailment features. The metric's goal is to provide an indicative score of the extent to which two texts (a candidate transla-tion and a reference) are equivalent. This goal is more complex than classical binary decisions in the field of TE and is a new approach to bring to-gether the knowledge from different areas that a similar ambitions. While promising results were found at the sys-tem level, the metric still needs to be tested on a diversity of settings and at the segment level; this is work in progress and results will be reported in due time. References  Jes?s Gim?nez and Llu?s M?rquez. 2007. Linguistic Features for Automatic Evaluation of Heterogeneous MT Systems. In Proceedings of the ACL Workshop on Statistical Machine Translation, pages 256?264. Christoph Tillmann, Stephan Vogel, Hermann Ney, Arkaitz Zubiaga, and Hassan Sawaf. 1997. Acceler-ated DP Based Search For Statistical Translation. In Proceedings of the 5th European Conference on Speech Communication and Technology  (EUROSPEECH-97). Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th AnnualMeeting of the Association for Computational Linguistics(ACL-02), pages 311?318. Sonja Nie?en, Franz Josef Och, Gregor Leusch, and Hermann Ney. 2000. A Evaluation Tool for Machine Translation:Fast Evaluation for MT Research. In 
56
Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC-2000). G. Doddington. 2002. Automatic Evaluation of Machine Translation Quality using N-gram Co-occurrence Statistics.  In Proceedings of the 2nd International Conference on HLT, pp. 138?145, San Francisco, CA, USA. Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments. In Pro-ceedings of the 43th ACL, pages 65?72. Michael Denkowski and Alon Lavie. 2010. METEOR-NEXT and the METEOR Paraphrase Tables: Im-proved Evaluation Support For Five Target Lan-guages. Proceedings of the ACL 2010 Joint Workshop on Statistical Machine Translation and Metrics MATR. He Yifan, Du Jinhua, Way Andy, and Van Josef . 2010. The DCU dependency-based metric in WMT-MetricsMATR 2010. In: WMT 2010 - Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR, ACL, Uppsala, Sweden. Kamp H. 1981. A theory of truth and semantic repre-sentation. In Groenendijk, J., Janssen, T., & Stokhof, M. (Eds.), Formal methods in the study of language, No. 135, pp. 277?322. Mathematical Centre, Am-sterdam. Chi-kiu Lo and Dekai Wu. 2011. MEANT: inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles. 49th An-nual Meeting of the Association for Computational Linguistic (ACL-2011). Portland, Oregon, US. Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Anno-tation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA-06), pages 223?231. Ido Dagan, Oren Glickman and Bernardo Magnini. 2006. The PASCAL Recognising Textual Entailment Challenge. In Qui?onero-Candela, J.; Dagan, I.; Magnini, B.; d'Alch?-Buc, F. (Eds.) Machine Learn-ing Challenges. LNCS, Vol. 3944, pp. 177-190. Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido Dagan, Marc Dymetman, and Idan Szpektor. 2009. Source-language entailment modeling for translating unknown terms. ACL 2009. Vol. 2. Stroudsburg, PA, USA, 791-799. Wilker Aziz and Marc Dymetmany and Shachar Mirkin and Lucia Specia and Nicola Cancedda and Ido Da-gan. 2010. Learning an Expert from Human Annota-tions in Statistical Machine Translation: the Case of Out-of-VocabularyWords. In: Proceedings of the 14th annual meeting of the European Association for Machine Translation (EAMT), Saint-Rapha, France. 
Dahlmeier, Daniel  and  Liu, Chang  and  Ng, Hwee Tou. 2011.TESLA at WMT 2011: Translation Evalu-ation and Tunable Metric.In: Proceedings of the Sixth Workshop on Statistical Machine Translation. Association for Computational Linguistics, pages 78-84, Edinburgh, Scotland. S. Pado, D. Cer, M. Galley, D. Jurafsky and C. Man-ning. 2009. Measuring Machine Translation Quality as Semantic Equivalence: A Metric Based on Entail-ment Features. Journal of MT 23(2-3), 181-193.  S. Pado, M. Galley, D. Jurafsky and C. Manning. 2009a. Robust Machine Translation Evaluation with Entail-ment Features. Proceedings of ACL 2009. Eneko Agirre, Daniel Cer, Mona Diab and Aitor Gonzalez-Agirre.    2012. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity.    In Proceed-ings of the 6th International Workshop on Semantic    Evaluation (SemEval 2012), in conjunction with the First Joint    Conference on Lexical and Computa-tional Semantics (*SEM 2012). Bentivogli, Luisa, Dagan Ido, Dang Hoa, Giampiccolo, Danilo, Magnini Bernardo.2009.The Fifth PASCAL RTE Challenge. In: Proceedings of the TAC.  Castillo Julio. 2011. A WordNet-based semantic ap-proach to textual entailment and cross-lingual textu-al entailment. International Journal of Machine Learning and Cybernetics - Springer, Volume 2, Number 3. Estrella Paula, Popescu-Belis A. and King M. 2007. A New Method for the Study of Correlations between MT Evaluation Metrics and Some Surprising Results. In: Proceedings of TMI-07- 11th Conference on The-oretical and Methodological Issues in Machine Translation -, Skvvde, Sweden. Castillo Julio and Cardenas Marina. 2010. Using sen-tence semantic similarity based onWordNet in recog-nizing textual entailment. Iberamia 2010. In LNCS, vol 6433. Springer, Heidelberg, pp 366?375. Castillo Julio. 2010. A semantic oriented approach to textual entailment using WordNet-based measures. MICAI 2010. LNCS, vol 6437. Springer, Heidelberg, pp 44?55. Castillo Julio. 2010. Using machine translation systems to expand a corpus in textual entailment. In: Proceed-ings of the Icetal 2010. LNCS, vol 6233, pp 97?102. Resnik P. 1995. Information content to evaluate seman-tic similarity in a taxonomy. In: Proceedings of IJCAI 1995, pp 448?453 907 Lin D. 1997.An information-theoretic definition of simi-larity. In: Proceedings of Conference on Machine Learning, pp 296?304 909 Jiang J, Conrath D.1997. Semantic similarity based on corpus statistics and lexical taxonomy. In: Proceed-ings of theROCLINGX 911 Pirro G., Seco N. 2008. Design, implementation and evaluation of a new similarity metric combining fea-
57
ture and intrinsic information content. In: ODBASE 2008, Springer LNCS. Wu Z, Palmer M. 1994. Verb semantics and lexical selection. In: Proceedings of the 32nd ACL 916. Leacock C, Chodorow M. 1998. Combining local con-text and WordNet similarity for word sense identifi-cation. MIT Press, pp 265?283 919 Hirst G, St-Onge D . 1998. Lexical chains as represen-tations of context for the detection and correction of malapropisms. MIT Press, pp 305?332 922 Banerjee S, Pedersen T. 2002. An adapted lesk algo-rithm for word sense disambiguation using WordNet. In: Proceeding of CICLING-02 Castillo Julio and Estrella Paula. 2012.  SAGAN: An approach to Semantic Textual Similarity based on Textual Entailment. In Proceedings of the 6th Inter-national Workshop on Semantic    Evaluation (SemEval 2012), in conjunction with the First Joint    Conference on Lexical and Computational Semantics (*SEM 2012). Castillo Julio and Cardenas Marina. 2012. SAGAN: A Machine Translation Approach for Cross-Lingual Textual Entailment. In Proceedings of the 6th Inter-national Workshop on Semantic    Evaluation (SemEval 2012), in conjunction with the First Joint    Conference on Lexical and Computational Semantics (*SEM 2012).  Chris Callison-Burch, Philipp Koehn, Christof Monz, Omar Zaidan. 2011.Findings of the 2011Workshop on Statistical Machine Translation. WMT 2011. M. Negri, A. Marchetti, Y. Mehdad, L. Bentivogli, and D. Giampiccolo. 2012. Semeval-2012. Task 8: Cross-lingual Textual Entailment for Content Syn-chronization. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012). William B. Dolan and Chris Brockett.2005. Automati-cally Constructing a Corpus of Sentential Para-phrases. Third International Workshop on Paraphras-ing (IWP2005). Asia Federation of Natural Language Processing.    
   
58
