An Algorithm for Situation Classification 
of Chinese Verbs 
Xiaodan Zhu, Chunfa Yuan 
State Key Laboratory for Intelligent 
Technology and System, DepL of Computer 
Science & Technology, Tsinghua 
University, Beijing 100084, P.R.C. 
K.F.Wong, Wenjie.Li 
Dept. of System Engineenng and Engineering 
Management, Chinese University of 
HongKong 
Abst rac t  
Temporal information analysis is very 
important for Chinese Information Process. 
Comparing with English, Chinese is quite 
different in temporal information 
expression. Based on the feature of Chinese 
a phase-based method is proposed to deal 
with Chinese temporal information. To this 
end, an algorithm is put forward to classify 
verbs into different situation types 
automatically. About 2981 verbs were 
tested. The result has shown that the 
algorithm is effective. 
1.*Introduction 
We are now launching a research 
project on Events Extraction from Chinese 
Financial News, which requires us to extract 
the related temporal information from news. 
Temporal expressions in Chinese form a 
complex system. We cannot fully 
understand the temporal information only by 
extracting the verbs, adverbs, auxiliary 
words and temporal phrases. Instead, more 
profound analysis is needed. In this paper, 
we first introduce the temporal system of 
Chinese, then we put forward a method in 
dealing with Chinese temporal information, 
in which situation types is very important. 
Therefore, an algorithm is rendered to 
classify verbs into several situation types. 
1.1 Temporal System of Chinese 
Commonly, Chinese linguists \[3\]\[4\] 
think that the temporal system of Chinese 
includes three parts: phase, tense and aspect. 
Each of these represents ome profile of 
temporal expression (these definitions are a 
little different from linguistic theory of 
English). 
(1) Phase. A sentence may describe a static 
state or an action; an action may be durative 
or instantaneous; a durative action may 
indicate a terminal or not. All of these are 
the research fields of phase. So, static vs. 
dynamic, durative vs. instantaneous, telic vs. 
non-telic are three pairs of phase features. 
Phase depends fully on meaning. According 
to phase features, we can classify the verbs 
into different situation types. 
(2) Tense. Tense describes the relations 
between an event (E), reference time (R) and 
speaking time (S). First, taking S as the 
origin, we can get three reIations between R 
and S: if R is before S, the sentence 
describes past; if R is the same time as S, it 
describes present; if R is after S, it desci'ibes 
future. This is called primary tense. 
Secondly, we can get three relations 
between E and S: If E is before R, we call it 
anterior; if E is the same time as R, we call it 
simple; otherwise we call it posterior. This is 
called secondary tense. Therefore, there are 
nine tenses including anterior past, anterior 
future, simple future, posterior present, etc. 
* Supported by National Natural Science Foundation 
of China (69975008) and 973 project (G 1998030507) 
140 
(3) Aspect . Aspect reflects the way we 
observe an event. For the same event, there 
are many perspectives. We can take the 
event as atomic and not consider its inner 
structure, and call it perfective. We can 
consider it being in process, and call it 
imperfective. For imperfective, we can 
observe it at a"position before it, at the 
beginning of it, in the middle of it, etc. 
Different perspectives lead to different 
expressions in the language. 
Phase, tense and aspect are not 
independent even though they are different 
conceptions; each of them can influence and 
restrict he others, ultimately building up the 
complex temporal system of Chinese. 
1.2 Phase-based Chinese temporal 
information analysis 
Most languages express temporal 
information through phase, tense and aspect, 
however, for different languages, the 
relative importance of the three parts is 
different. A very important feature of 
English is that tense and aspect are 
expressed by variation of predicates. But for 
Chinese, predicates keep the same form no 
matter how the tense and aspect are 
different. 
Therefore, in English, temporal 
information analysis mainly considers tense 
and aspect, as well as temporal adjective and 
time words and phrases. But in Chinese, 
tense and aspect of a sentence are not very 
clear, verbs do not vary in form with the 
change of tense and aspect. So we suggest 
basing temporal information analysis on 
phase. We mainly perceive the situation type 
of a sentence, then roughly acquire tense 
from adverbs and auxiliary words. After 
considering the temporal phrases, we can 
understand the temporal information of 
single event fully. Finally, according to the 
absolute temporal information of single 
event, we can get the temporal relation 
between two events. Phase-based temporal 
information analysis has been used in our 
research on Event Extraction from Financial 
News, in which the most important and 
fundamental problem is to acquire the 
situation types of a sentence. 
1.3 Situation Classification of Chinese 
Verbs 
In the West, research on situation has a 
long history. The earliest can be traced to 
the times of Aristotle. In resent years, 
Western researchers have published a large 
volume of papers, which present many 
points of view. The most important are 
Vendler(1967), Bache(1982), and Smith 
(1985) They approximately classify the 
situation as four types: 
state, activity, accomplishment, and 
achievement. 
Chinese researchers have also done 
considerable work, among which the most 
typical research were done by Chen\[3\] and 
Ma\[5\]. 
Ma\[5\] stated that the situation of a 
sentence is fully determined by the situation 
of the main verb of the sentence. He use 
three phases: static, durative, telic to classify 
verbs into four situational types 
V1,V2,V3,V4. 
Static Durative Telic 
VI + + + 
V2 + + 
V3 + 
V4 + 
Table 1.1 
Chen\[3\] stated that the situation of a 
sentence not only depends on the main verb 
of the sentence but also on other parts of the 
sentence. That is, although the main verb is 
the most important in determining a 
sentence situation, other parts such as 
adverbs also have effect. Cheri s 
classification is more detailed. 
141 
NO. 
(1) 
(2) 
(3) 
(4) 
(5) 
(6) 
(7) 
(8) 
(9) 
00) 
Verb types 
Attribute 
Mental state 
Position 
Action and 
Mental Activity 
Verb-object Structure 
Change 
Directional Action 
Instantaneous Change 
Instantaneous Action 
Verb-verb or 
Verb-adjective 
Instances 
:E(be), ~(equal) 
;~:l~'~(believe), ~l~J~(re~'et) 
~.~i(stand), ~(sit), J\]~j 
(lie) 
gf~jump), ,~. (think), 
~i=~q~uess) 
i~t~(read (books)), 
I1~(sing (songs)) 
(become) 
/EgE(run up), ~,_J2(climb on) 
~.(die), ~l ie) ,  IS(snap) ..  
~t~(sit), ~td/(stand) 
~J(push down), 
~,TJ~..(smash (into pieces)) 
Table 1.2 
Static 
Situation types 
State + 
Activity 
Accomplishment 
Simple change 
Complex change 
Dura- Telic verb types 
tive (table above) 
(1) (2) 
(3) 
+ (3) (4) 
(5) 
+ + (3) (4) 
+ (6) (7) 
(S) (9) 
(10) 
Table 1.3 
From the tables above, we can find that 
some words(such as (3) and (4) in table 1.3) 
can belong to more than one category, so 
Chen use modifiers, auxiliary words and 
prepositions to eliminate the ambiguity. 
State Acti- 
vity 
Ell l~ l  I \ [  
~1~ +V 
v~ 
~v 
v+(y) 
+TQP+ 
~act) 
V+(T) + + 
+TQP+ 
~m,e)  
TQP: Time Quantity Phrase, (-) : 
? 
(-1 ? ? 
(-) (-) + 
? 
Accom- Complex Simple 
plishment change change 
+ 
? ? 
? ? 
? ? ? 
in most case, it is 
Table 1.4 
2. Our Classification Algorithm for 
Verbg Situation 
2.1 Guiding Thoughts 
(1) Our algorithm is for information 
processing 
eMa\[5\] uses three pairs of phase features in 
classifying, but from which we can not get 
an automatic classification algorithm for 
computers; the classification can only be 
done manually. 
eln linguistics, telicity is a phase feature 
used in classifying. In table 1.1 the 
difference between category V2 and V3, in 
table 1.3, the difference between "activivJ' 
and "accomplishmenf', are attributed to 
telicity. But in information process, we need 
not distinguish whether an event is telic or 
not. For example, 
Exp. 1 
~)~t\]~'j~, (He is playing the flute) 
'~ .  (He is playing a song '%iangzhu " )
Chen\[3\] thinks that in Exp. 1, the first 
sentence has the features: dynamicity, and 
durativity, and non-telicity; it belongs 
to "activi~' . The second sentence has the 
features dynamicity, durativity, and telicity, 
because in the second sentence, there is a 
default terminal . . . .  when the song 
~angzhd' is over, the action '~la~ is 
over, so the sentence belongs to 
'hccomplishmenf instead of  "activity. 
However we think such discrimination is 
useless for information extraction, because 
telicity is an ambiguous concept itself. What 
we need is to acquire the exact duration of 
the event. So if we knew the event is 
durative or not, and got the temporal 
phrases, we can know terminal time of the 
event. Besides, whether an event is telic or 
not can not be attributed to collocation and 
only can be done manually(as the exp 1 
shows). For these reasons, we consider the 
two verbs in Exp. 1 belonging to the same 
situational type, that is, we do not use 
talicity as a phase feature to classifying 
verbs. 
142 
(2) Separate classification of the verb 
situation from classification of the sentence 
situation. 
Chen\[3\] points that some verbs belong 
to more than one category, and gives a 
method to distinguish these cases. To make 
the ideal more clear, we use two steps to 
complete the seritence situation recognition. 
In this paper, we render an algorithm to 
classify verbs into different categories, 
which is the basis of another research ... .  
recognition of sentence situation, which will 
be discussed in future work. 
'Men(MentalityJ' can follow '~1~ (very). 
Verbs in the "AmlS' category can followed 
by "~-~(preposition-objec0' structures, etc. 
The following is the set of collocational 
features. 
Verb+T 
~ll~+Verb 
Verb+~ 
:i-+Verb 
Verb+~ 
Static verbs Amb Act Ins 
Att Men 
+ ? + 
+ 
(-) + (+) 
(-) (-) + 
? 
Table 2.1 
2.2 Classification Method 
We classify the verbs into five categories , 
Att(Attribute), Men(Mentality), Act 
(Activity) , Ins(Instantaneous) , Amb 
(Ambiguous). 
Att: ~(be), ~'(equal), '~'(include), m~(accord with) 
Men: ~.~J~(like), ~.,(belittle), ~(love), ~ff~ 
(be satisfied with) 
Act: ~(draw), ~l~l(gab), ~(drink), ~( run)  
In;  ~?~(explore), ~l;~(extinguish), I~(snap), 
(discovery) 
Amb: ~.~(sit), ~i(stand), Jig(lie), ~(kneel), ~:(bring), 
~(hang), ~(wear), ~-~(install) 
Amb(Ambiguous) include those words 
which describe different situations in 
different context. For example: 
Exp. 2: 
(they hung the picture on the wall. ) 
(Picture is hanging on the wall.) 
In Exp. 2, the two sentences have the 
same predicate '~  (hang). In the first 
sentence, '~  descnbes an instantaneous 
action, but the second sentence describes a
state. In English, forms of these two 
predicates are different; while in Chinese, 
they are the same. For this reason, we 
consider it ambiguous and indistinguishable 
without context. 
We have pointed out previously that 
phase depends only on meaning. However 
different situational types collocate with 
different words. So the essence of our 
algorithm is replace semantic judgement 
with collocational judgement. For example, 
2.3 Implementation of the algorithm 
According to table 2.1, a classification 
algorithm was designed, and we use two 
resources to implement our algorithm: The 
Contemporary Chinese Cihai \[11\] (which 
we will refer to as the Cihai below) 
dictionary and the Machine Tractable 
Dictionary of Contemporary Chinese 
Predicate Verbs \[12\](which we will refer to 
as the predicate dictionary below). The 
Cihai dictionary includes 12,000 entries and 
700,000 collocation instances, predicate 
dictionary includes about 3000 verbs with 
their semantic information, case relations 
and detailed collocation information. These 
two dictionaries both include some of the 
collocation information that the algorithm 
needs. 
Considenng the features of these two 
dictionaries, we adjust part of our algorithm: 
(1) In predicate dictionary, there is a slot 
named "verb typ? , which includes 
'transitive verlY , 'fntransifive verB' , 
~ttribufive verlS", 'linking vertt' etc. So, at 
the beginning of the algorithm, we judge if 
the verb is a "linking verlS' (
~(be~', ~(equa l~ '  ,etc) or a "possessive 
verlS' ('~-q~J' (have)). If it is, we directly 
classify the verb as "att(attribute~' without 
further processing. 
(2) The predicate dictionary provides the 
case relation of verbs, and their semantic 
ategodes. We restrict the agent of verbs in 
the "Men(mentality~' tobelong to one of: 
143 
"{ .)kI"(people), "{ )l,.,~} "(human), "1 
)l,~} "(multitude) , {~s:} 
(collectivity)'; "{:~:-~?~}(creatures)'; ,,~- ~,-ii,~,~,jtr 
bel ieff ,  "{gJJl~}(animal)" (3) Because 
Cihai includes collocation instances instead 
of collocation relations, we should consider 
synonyms. To be exact, when we determine 
whether a verb belongs to "Men(Mentality~' 
or not, we judge if it can follow 
(very) and synonyms such as 
'trY'P; 
However, some seldom seen instances were 
included. All these cause some errors. 
The final algorithm is as follows: 
if (a verb is labeled as" linking verb" or  '~:~ossessive vertf 
in predicate dictionary ) 
then the verb belongs to "Att(Attribute)" 
else if (the verb can follow ":~\[~"(very) and synonyms "~.~,I~", 
"1-?~"; '~l"~ '~71~2', "~" )  and (its agenf s 
semantic belongs to setl*) 
then the verb belongs to "Men(Mentality)" 
else if (it can follow ";t~E") or (be followed by"~") 
then if (it can be followed by "preposition-object" 
structure) 
then the verb belongs to "Amb(Ambiguous)" 
else the verb belongs to "Act(Activity)" 
else if (it can be followed by"T") 
then the verb belongs to"Ins(Instantaneous)" 
else the verb belongs tff unknown" 
*setl={human, multitude, collectivity, creatures, belief, 
animal } 
3. Resu l ts  and  Ana lys i s  
3.1 Results 
We use the algorithm above to classify 
the 2981 words in predicate dictionary, at 
the same time, we do the classification 
manually, Table 3.1 is the result: 
Att Men Amb Ins Act Un- Totel 
known 
Human 20 112 500 662 1683 4* 2981 
Algo- 20 111 537 691 1519 i 103 2981 
Rithm i 
*this 4 words are not verb. 
Table 3.1 
Table 2.2 shows the details: 
by a lgo -  Classifying b human 
rithm Att Men Amb Ins Act Non- Totel 
verb 
Art 20 0 0 0 0 0 20 
Men 0 99 1 1 10 0 111 
Amb 0 0 473 9 55 0 537 
Ins 0 0 3 637 51 0 691 
Act 0 1 12 2 1504 1519 
Un- 0 12 11 13 63 103 
known 
Totel 20 112 500 662 1683 4 2981 
Table 3.2 
Table 3.3 shows precision and recall: 
Att Men Amb Ins Act Average 
Precision 100.0 89.9 88.1 92.2 99.0 93.8 
Recall 100.0 88.4 94.6 96.2 89.4 93.7 
Table 3.3 
3.2 Analysis 
We mainly analyze the errors: 
(1) Failure of algorithm 
Chinese .is a very complex language. 
Replacing semantic judgment by using 
collocations has limitation itself. For 
example, in most cases, whether a verb is 
durative or not can be decided by whether it 
can be used in such structure "verb-g ~"  (In 
most case, "~ represents an action in 
progress). But some instantaneous verbs 
such as ~.(knocky, can also be used in 
such structure to express a repeated action. 
(2) Errors caused by the resources 
(2.1) Collocation incompleteness in 
Cihai: for example, '~(d isagreeJ '  can 
collocate with '~  (very), but this 
collocation is not included in Cihai. 
(2.2)Errors caused by predicate dictionary: 
It is obvious that a certain proportion of 
dictionary errors is inevitable. For example, 
though '~  (beg) can follow '~i~E to 
represent the action is in progress, it is not 
included in the corresponding slot of 
predicate dictionary. 
(2.3) The inconsistency between the two 
dictionaries: For example, 
NN (admire), (mind), and ( 
belittle) are included in predicate dictionary 
but not in Cihai. Although ~ (regret) is 
included in Cihai, it is taken as an adjective 
instead of a verb. 
144 
4. Conclusion 
In this paper, we advance a phase-based 
method to analyze temporal information in 
Chinese. For this purpose, an algorithm is 
rendered to classify verbs into different 
situation types. Because a verl5 s situation 
depends on the meaning of the verb, the 
essence of our algorithm takes advantage of 
collocations to avoid semantics. The result 
shows the algorithm is successful. We also 
believe that if the errors caused by resources 
were eliminated, the result would be 
improved significantly. 
Although the five categories are defined 
by us, they can describe basic situations of 
Chinese. The classification algorithm itself 
is independent of resources, so it can be 
applied to other resources (dictionaries) if
these resources include sufficient collocation 
information. Furthermore, Discarding 
dictionaries and doing classification directly 
on large-scale real corpus, especially in 
certain domain, deserve the future research. 
Our algorithm is very useful for the 
future analysis of sentence situation for 
Information Extraction system and for 
dictionary construction. 
\[5\]Ma Qingzhu. Time Quantity Phrase and 
Categories of Verbs: China Chinese. 
Vol.2,1981. 
\[6\]Hu Yushu & Fan Xiao. Research on 
Verbs, Henan Univ. Press, 1995 
\[7\]Hwang C.H. & Schubert L. K. 
Interpreting Tense, Aspect and Time 
Adverbials: A Compositional, Unified 
Approach : Proceeding of 1st International 
Conference in Temporal Logic, Bonn, 
Germany, July 1994. 
\[8\]Allen J.F. Towards a General Theory of 
Action and Time: Artificial Intelligence, 
23,123-154. 
\[9\]Allen J.F. & George, F.. Action and 
Events in Interval Temporal Logic: Journal 
of Logic and Computation, Special Issue on 
Actions and Processes, 1994. 
\[10lion Androutsopoulos, Graeme Ritche & 
Peter Thanisch. Time ,Tense and Aspect 
in Natural Language Database Interface, 
CMP-LG Mar 1998. 
\[l l\]Ni Wenjie . Contemporary Chinese 
Cihai, People China Press, 1994. 
\[ 12\]Chen Qunxiu Designing and 
implement of Machine Tractable Dictionary 
of Contemporary Chinese Predicate 
Verbs, Proceedings of ICCC96, Singapore, 
Jun, 1996. 
References 
\[ 1 \]Message Understanding Conference 
Website, http://www.muc.saic.com. 
\[2\]Message Understanding Evaluation and 
Conference: Proceedings of 3rd-6th APRA 
Workshops, Morgan Kaufmann Publishers 
Inc., 1996. 
\[3\]Chen ping. Discussion On Temporal 
System of Contemporary Chinese: China 
Chinese Vol.6,1998. 
\[4\]Gong Qianyan. Phase, Tense and Aspect 
of Chinese, Commercial Press. 
145 
Single Character Chinese Named Entity Recognition 
Xiaodan Zhu, Mu Li, Jianfeng Gao and Chang-Ning Huang 
Microsoft Research, Asia  
Beijing 100080, China 
xdzhu@msrchina.research.microsoft.com 
{t-muli,jfgao,cnhuang}@microsoft.com 
 
 
Abstract 
Single character named entity (SCNE) is a 
name entity (NE) composed of one Chinese 
character, such as  ?
 
? (zhong1, China) 
and ?? e2,Russia. SCNE is very 
common in written Chinese text. However, 
due to the lack of in-depth research, SCNE 
is a major source of errors in named entity 
recognition (NER). This paper formulates 
the SCNE recognition within the source-
channel model framework. Our experi-
ments show very encouraging results: an F-
score of 81.01% for single character loca-
tion name recognition, and an F-score of 
68.02% for single character person name 
recognition. An alternative view of the 
SCNE recognition problem is to formulate 
it as a classification task. We construct two 
classifiers based on maximum entropy 
model (ME) and vector space model 
(VSM), respectively. We compare all pro-
posed approaches, showing that the source-
channel model performs the best in most 
cases. 
1 Introduction 
The research of named entity recognition (NER) 
becomes very popular in recent years due to its 
wide applications and the Message Understanding 
Conference (MUC) which provides a standard test-
bed for NER evaluation. Recent research on Eng-
lish NER includes (Collins, 2002; Isozaki, 2002; 
Zhou, 2002; etc.). Chinese NER research includes 
(Liu, 2001; Zheng, 2000; Yu, 1998; Chen, 1998; 
Shen, 1995; Sun, 1994; Zhang, 1992 etc.) 
In Chinese NEs, there is a special kind of NE, 
called single character named entity (SCNE), on 
which there is little in-depth research. SCNE is a 
NE composed of only one Chinese character, such 
as the location name ?
 
? (zhong1,China) and 
? ? (e2,Russia) in the phrase ?  ? 
(zhong1-e2-mao4-yi4, trade between China and 
Russia). SCNE is very common in written Chinese 
text. For instance, SCNE accounts for 8.17% of all 
NE tokens according to our statistics on a 10MB 
corpus. However, due to the lack of research, 
SCNE is a major source of errors in NER. Among 
three state-of-the-art systems we have, the best F-
scores of single character location (SCL) and sin-
gle character person (SCP) are 43.63% and 43.48% 
respectively. This paper formulates the SCNE rec-
ognition within the source-channel model frame-
work. Our results show very encouraging 
performance. We achieve an F-score of 81.01% for 
SCL recognition and an F-score of 68.02% for 
SCP recognition. An alternative view of the SCNE 
recognition problem is to formulate it as a 
classification task. For example, ?
 
? is a SCNE 
in ?
 
?, but not in ?
 
?(bei3-jing1-
si4-zhong1, Beijing No.4 High School). We then 
construct two classifiers respectively based on two 
statistical models: maximum entropy model (ME) 
and vector space model (VSM). We compare these 
two classifiers with the source-channel model, 
showing that the source-channel model is slightly 
better. We then compare the source-channel model 
with other three state-of-the-art NER systems. 
The remainder of this paper is structured as 
follows: Section 2 introduces the task of SCNE 
recognition and related work. Section 3 and 4 pro-
pose the source-channel model and two classifiers 
for SCNE recognition, respectively. Section 5 pre-
sents experimental results and error analysis. Sec-
tion 6 gives conclusion. 
2 SCNE Recognition and Related Work 
We consider three types of SCNE in this paper: 
single character location name (SCL), person 
name (SCP), and organization name (SCO). Be-
low are examples: 
1.    SCL: ?
 
?and ?? in ?
 
? 
2.  SCP: ? ? (zhou1, Zhou) in ? ? 
(zhou1-zong3-li3,Premier Zhou), 
3.  SCO: ?? (guo2, Kuomingtang Party) 
and ?? (gong4, Communist Party ) in 
?  ? (Guo2-gong4-he2-zuo4, 
Cooperation between Kuomingtang Part 
and Communist Party) 
SCNE is very common in written Chinese text. 
As shown in Table 1, SCNE accounts for 8.17% 
of all NE tokens on the 10MB corpus. Especially, 
14.65% of location names are SCLs. However, 
due to the lack of research, SCNE is a major 
source of errors in NER. In our experiments 
described below, we focus on SCL and SCP, 
while SCO is not considered because of its small 
number in the data.  
 
 # SCNE # NE #SCNE / #NE 
PN 5,892 129,317 4.56% 
LN 32,483 221,713 14.65% 
ON 356 122,779 0.29% 
Total 38,731 473,809 8.17% 
 
Table 1.  Proportion of SCNE in NE 
To our knowledge, most NER systems do not 
report SCNE recognition results separately. 
Some systems (e.g. Liu, 2001) even do not in-
clude SCNE in recognition task. SCNE recogni-
tion is achieved using the same technologies as 
for NER, which can be roughly classified into 
rule-based methods and statistical-based methods, 
while most of state-of-the-art systems use hybrid 
approaches. 
Wang (1999) and Chen (1998) used linguistic 
rules to detect NE with the help of the statistics 
from dictionary.  Ji(2001), Zheng (2000), 
Shen(1995) and Sun(1994) used statistics from 
dictionaries and large corpus to generate PN or 
LN candidates, and used linguistic rules to filter 
the result, and Yu (1998) used language model to 
filter. Liu (2001) applied statistical and linguistic 
knowledge alternatively in a seven-step proce-
dure. Unfortunately, most of these results are 
incomparable due to the different test sets used, 
except the results of Chen (1998) and Yu (1998). 
They took part in Multilingual Entity Task 
(MET-2) on Chinese, held together with MUC-7. 
Between them, Yu (1998)?s results are slightly 
better.    However, these two comparable sys-
tems did not report their results on SCNE sepa-
rately. To evaluate our results, we compare with 
three state-of-the-art system we have. These sys-
tems include: MSWS, PBWS and LCWS. The 
former two are developed by Microsoft? and the 
last one comes from by Beijing Language Uni-
versity.  
3 SCNE Recognition Using an Improved 
Source-Channel Model 
3.1 Improved Source-Channel Model1 
We first conduct SCNE recognition within a 
framework of improved source-channel models, 
which is applied to Chinese word segmentation.  
We define Chinese words as one of the following 
four types: (1) entries in a lexicon, (2) morpho-
logically derived words, (3) named entity (NE), 
and (4) factoid.  Examples are 
1. lexicon word:  (peng2-you3, friend). 
2. morph-derived word: 		



 (gao1-gao1-
xing4-xing4 , happily) 
3. named entity:  
(wei1-ruan3-gong1-
si1, Microsoft Corporation) 
4. factoid2:    (yi1-yue4-jiu3-ri4, Jan 9th)  
Chinese NER is achieved within the framework. 
To make our later discussion on SCNE clear, we 
introduce the model briefly. 
We are given Chinese sentence S, which is 
a character string. For all possible word segmen-
tations W, we will choose the one which 
achieves the highest conditional probability W* 
= argmax
 w P(W|S). According to Bayes? law and 
dropping the constant denominator, we acquire 
the following equation: 
                                                     
1
 This follows the description of (Gao, 2003). 
2
 We define ten types of factoid: date, time (TIME), percent-
age, money, number (NUM), measure, e-mail, phone number, 
and WWW. 
)|()(maxarg* WSPWPW
W
=
 
(1) 
Following our Chinese word definition, we de-
fine word class C as follows: (1) each lexicon 
word is defined as a class; (2) each morphologi-
cally derived word is defined as a class; (3) each 
type of named entities is defined as a class, e.g. 
all person names belong to a class PN, and (4) 
each type of factoids is defined as a class, e.g. all 
time expressions belong to a class TIME. We 
therefore convert the word segmentation W into 
a word class sequence C. Eq. 1 can then be 
rewritten as: 
)|()(maxarg* CSPCPC
C
= . 
(2) 
Eq. 2 is the basic form of the source-channel 
models for Chinese word segmentation. The 
models assume that a Chinese sentence S is gen-
erated as follows: First, a person chooses a se-
quence of concepts (i.e., word classes C) to 
output, according to the probability distribution 
P(C); then the person attempts to express each 
concept by choosing a sequence of characters, 
according to the probability distribution P(S|C).  
We use different types of channel models 
for different types of Chinese words. This brings 
several advantages. First, different linguistic 
constraints can be easily added to corresponding 
channel models (see Figure 1). These constraints 
can be dynamic linguistic knowledge acquired 
through statistics or intuitive rules compiled by 
linguists. Second, this framework is data-driven, 
which makes it easy to adapt to other languages. 
We have three channel models for PN, LN and 
ON respectively. (see Figure 1) 
However, although Eq. 2 suggests that channel 
model probability and source model probability 
can be combined through simple multiplication, in 
practice some weighting is desirable. There are two 
reasons. First, some channel models are poorly 
estimated, owing to the sub-optimal assumptions 
we make for simplicity and the insufficiency of the 
training corpus. Combining the channel model 
probability with poorly estimated source model 
probabilities according to Eq. 2 would give the 
context model too little weight. Second, as seen in 
Figure 1, the channel models of different word 
classes are constructed in different ways (e.g. name 
entity models are n-gram models trained on cor-
pora, and factoid models are compiled using lin-
guistic knowledge). Therefore, the quantities of 
channel model probabilities are likely to have 
vastly different dynamic ranges among different 
word classes. One way to balance these probability 
quantities is to add several channel model weight 
CW, each for one word class, to adjust the channel 
model probability P(S|C) to P(S|C)CW. In our ex-
periments, these weights are determined empiri-
cally on a development set. 
Given the source-channel models, the procedure 
of word segmentation involves two steps: first, 
given an input string S, all word candidates are 
generated (and stored in a lattice). Each candidate 
is tagged with its class and the probability P(S?|C), 
where S? is any substring of S. Second, Viterbi 
search is used to select (from the lattice) the most 
probable word segmentation (i.e. word class se-
quence C*) according to Eq. 2.  
Word class Channel model Linguistic Constraints 
Lexicon word (LW) P(S|LW)=1 if S forms a lexicon entry, 
0 otherwise.  
Word lexicon 
Morphologically derived word 
(MW) 
P(S|MW)=1 if S forms a morph lexicon 
entry, 0 otherwise.  
Morph-lexicon 
Person name (PN) Character bigram  family name list, Chinese PN patterns 
Location name (LN) Character bigram  LN keyword list, LN lexicon, LN abbr. list 
Organization name (ON) Word class bigram ON keyword list, ON abbr. List 
Factoid (FT) P(S|G)=1 if S can be parsed using a 
factoid grammar G, 0 otherwise 
Factoid rules (presented by FSTs). 
Figure 1. Channel models (Gao, 2003) 
3.2 Improved Model for SCNE Recognition 
Although our results show that the source-
channel models achieve the state-of-the-art word 
segmentation performance, they cannot handle 
SCNE very well. Error analysis shows that 
11.6% person name errors come from SCP, and 
47.7% location names come from SCL. There 
are two reasons accounting for it: First, SCNE is 
generated in a different way from that of multi-
character NE. Second, the context of SCNE is 
different from other NE. For example, SCNE 
usually appears one after another such as ? 


?. But this is not the case for multi-
character NE.  
To solve the first problem, we add two new 
channel models to Figure 1, that is, define each 
type of SCNE (i.e. SCL and SCP) as a individual 
class (i.e. NE_SCL and NE_SCP) with its chan-
nel probability P(Sj |NE_SCL), and P(Sj 
|NE_SCP). P(Sj |NE_SCL) is calculated by Eq. 3. 

=
=
n
i
i
j
j
SSCL
SSCL
SCLNE
1
|)(|
|)(|
  )_|P(S
 (3) 
Here, Sj is a character in SCL list which is ex-
tracted from training corpus. |SCL(Sj)| is the 
number of tokens Sj , which are labeled as SCL 
in training corpus. n is the size of SCL list, 
which includes 177 SCL. Similarly, P(Sj |NE_SCP) is calculated by Eq. 4, and the SCP 
list includes 151 SCP. 

=
=
n
i
i
j
j
SSCP
SSCP
SCPNE
1
|)(|
|)(|
  )_|P(S
 (4) 
We also use two CW to balance their channel 
probabilities with other NE?s. 
To solve the second problem, we trained a new 
source model P(C) on the re-annotated training 
corpus, where all SCNE are tagged by SCL or SCP.  
For example, ?? in ??is tagged as SCP 
instead of PN, and ? ? in ? 

? is tagged as 
SCL in stead of LN. 
4   Character-based Classifiers 
In this section, SCNE recognition is formulated 
as a binary classification problem. Our motiva-
tions are two folds. First, most NER systems do 
not use source-channel model, so our method 
described in the previous section cannot be ap-
plied. However, if we define SCNE as a binary 
classification problem, it would be possible to 
build a separate recognizer which can be used 
together with any NER systems. Second, we are 
interested in comparing the performance of 
source-channel models with that of other meth-
ods. 
For each Chinese character, a classifier is 
built to estimate how likely an occurrence of this 
Chinese character in a text is a SCNE. Some ex-
amples of these Chinese character as well as 
their probabilities of being a SCNE is shown in 
Table 2.  
 
	
 

	




 



Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 197?200,
New York, June 2006. c?2006 Association for Computational Linguistics
Comparing the roles of textual, acoustic and spoken-language features 
on spontaneous-conversation summarization  
Xiaodan Zhu Gerald Penn 
Department of Computer Science, University of Toronto 
10 Kings College Rd., Toronto, Canada 
{xzhu, gpenn} @cs.toronto.edu
 
Abstract 
This paper is concerned with the 
summarization of spontaneous 
conversations. Compared with broadcast 
news, which has received intensive study, 
spontaneous conversations have been less 
addressed in the literature.  Previous 
work has focused on textual features 
extracted from transcripts. This paper 
explores and compares the effectiveness 
of both textual features and speech-
related features. The experiments show 
that these features incrementally improve 
summarization performance. We also find 
that speech disfluencies, which  have been 
removed as noise in previous work, help 
identify important utterances, while the 
structural feature is less effective than it 
is in broadcast news. 
1 Introduction 
Spontaneous conversations are a very important 
type of speech data. Distilling important 
information from them has commercial and other 
importance. Compared with broadcast news, which 
has received the most intensive studies (Hori and 
Furui, 2003; Christensen et al 2004; Maskey and 
Hirschberg, 2005), spontaneous conversations have 
been less addressed in the literature.  
Spontaneous conversations are different from 
broadcast news in several aspects: (1) spontaneous 
conversations are often less well formed 
linguistically, e.g., containing more speech 
disfluencies and false starts; (2) the distribution of 
important utterances in spontaneous conversations 
could be different from that in broadcast news, e.g., 
the beginning part of news often contains 
important information, but in conversations, 
information may be more evenly distributed; (3) 
conversations often contain discourse clues, e.g., 
question-answer pairs and speakers? information, 
which can be utilized to keep the summary 
coherent; (4) word error rates (WERs) from speech 
recognition are usually much higher in 
spontaneous conversations.  
Previous work on spontaneous-conversation 
summarization has mainly focused on textual 
features (Zechner, 2001; Gurevych and Strube, 
2004), while speech-related features have not been 
explored for this type of speech source. This paper 
explores and compares the effectiveness of both 
textual features and speech-related features.  The 
experiments show that these features incrementally 
improve summarization performance. We also 
discuss problems (1) and (2) mentioned above. For 
(1), Zechner (2001) proposes to detect and remove 
false starts and speech disfluencies from transcripts, 
in order to make the text-format summary concise 
and more readable. Nevertheless, it is not always 
necessary to remove them. One reason is that 
original utterances are often more desired to ensure 
comprehensibility and naturalness if the summaries 
are to be delivered as excerpts of audio (see section 
2), in order to avoid the impact of WER. Second, 
disfluencies are not necessarily noise; instead, they 
show regularities in a number of dimensions 
(Shriberg, 1994), and correlate with many factors 
including topic difficulty (Bortfeld et al 2001). 
Rather than removing them, we explore the effects 
of disfluencies on summarization, which, to our 
knowledge, has not yet been addressed in the 
literature. Our experiments show that they improve 
summarization performance.   
To discuss problem (2), we explore and compare 
both textual features and speech-related features, 
as they are explored in broadcast news (Maskey 
and Hirschberg, 2005).  The experiments show that 
the structural feature (e.g. utterance position) is 
less effective for summarizing spontaneous 
conversations than it is in broadcast news. MMR 
197
and lexical features are the best. Speech-related 
features follow. The structural feature is least 
effective. We do not discuss problem (3) and (4) in 
this paper. For problem (3), a similar idea has been 
proposed to summarize online blogs and 
discussions. Problem (4) has been partially 
addressed by (Zechner & Waibel, 2000); but it has 
not been studied together with acoustic features. 
2 Utterance-extraction-based 
summarization 
Still at its early stage, current research on speech 
summarization targets a less ambitious goal: 
conducting extractive, single-document, generic, 
and surface-level-feature-based summarization.  
The pieces to be extracted could correspond to 
words (Koumpis, 2002; Hori and Furui, 2003). The 
extracts could be utterances, too. Utterance 
selection is useful. First, it could be a preliminary 
stage applied before word extraction, as proposed 
by Kikuchi et al (2003) in their two-stage 
summarizer. Second, with utterance-level extracts, 
one can play the corresponding audio to users, as 
with the speech-to-speech summarizer discussed in 
Furui et al (2003). The advantage of outputting 
audio segments rather than transcripts is that it 
avoids the impact of WERs caused by automatic 
speech recognition (ASR). We will focus on 
utterance-level extraction, which at present appears 
to be the only way to ensure comprehensibility and 
naturalness if the summaries are to be delivered as 
excerpts of audio themselves.  
Previous work on spontaneous conversations 
mainly focuses on using textual features. Gurevych 
& Strube (2004) develop a shallow knowledge-
based approach. The noun portion of WordNet is 
used as a knowledge source. The noun senses were 
manually disambiguated rather than automatically. 
Zechner (2001) applies maximum marginal 
relevance (MMR) to select utterances for 
spontaneous conversation transcripts. 
3 Classification based utterance 
extraction  
Spontaneous conversations contain more 
information than textual features. To utilize these 
features, we reformulate the utterance selection 
task as a binary classification problem, an 
utterance is either labeled as ?1? (in-summary) or 
?0? (not-in-summary). Two state-of-the-art 
classifiers, support vector machine (SVM) and 
logistic regression (LR), are used. SVM seeks an 
optimal separating hyperplane, where the margin is 
maximal. In our experiments, we use the OSU-
SVM package. Logistic regression (LR) is indeed a 
softmax linear regression, which models the 
posterior probabilities of the class label with the 
softmax of linear functions of feature vectors. For 
the binary classification that we require in our 
experiments, the model format is simple.  
 
3.1 Features 
The features explored in this paper include: 
(1) MMR score: the score calculated with MMR 
(Zechner, 2001) for each utterance. 
(2) Lexicon features: number of named entities, 
and utterance length (number of words). The 
number of named entities includes: person-
name number, location-name number, 
organization-name number, and the total 
number. Named entities are annotated 
automatically with a dictionary. 
(3) Structural features: a value is assigned to 
indicate whether a given utterance is in the first, 
middle, or last one-third of the conversation. 
Another Boolean value is assigned to indicate 
whether this utterance is adjacent to a speaker 
turn or not.  
(4) Prosodic features: we use basic prosody: the 
maximum, minimum, average and range of 
energy, as well as those of fundamental 
frequency, normalized by speakers.  All these 
features are automatically extracted. 
(5) Spoken-language features: the spoken-language 
features include number of repetitions, filled 
pauses, and the total number of them. 
Disfluencies adjacent to a speaker turn are not 
counted, because they are normally used to 
coordinate interaction among speakers. 
Repetitions and pauses are detected in the same 
way as described in Zechner (2001). 
4 Experimental results 
4.1 Experiment settings 
The data used for our experiments come from 
SWITCHBOARD. We randomly select 27 
conversations, containing around 3660 utterances. 
The important utterances of each conversation are 
198
manually annotated. We use f-score and the 
ROUGE score as evaluation metrics. Ten-fold 
cross validation is applied to obtain the results 
presented in this section. 
4.2 Summarization performance 
4.2.1 F-score 
Table-1 shows the f-score of logistic regression 
(LR) based summarizers, under different 
compression ratios, and with incremental features 
used. 
 10% 15% 20% 25% 30%
(1)  MMR .246 .309 .346 .355 .368
(2) (1) +lexicon .293 .338 .373 .380 .394
(3) (2)+structure .334 .366 .400 .409 .404
(4) (3)+acoustic .336 .364 .388 .410 .415
(5) (4)+spoken language .333 .376 .410 .431 .422 
Table 1. f-score of LR summarizers using incremental features 
Below is the f-score of SVM-based summarizer: 
 10% 15% 20% 25% 30%
(1) MMR .246 .309 .346 .355 .368
(2) (1) +lexicon .281 .338 .354 .358 .377
(3) (2)+structural .326 .371 .401 .409 .408
(4) (3)+acoustic .337 .380 .400 .422 .418
(5) (4)+spoken language .353 .380 .416 .424 .423 
Table 2. f-score of SVM summarizers using incremental features 
Both tables show that the performance of 
summarizers improved, in general, with more 
features used. The use of lexicon and structural 
features outperforms MMR, and the speech-related 
features, acoustic features and spoken language 
features produce additional improvements.  
4.2.2 ROUGE 
The following tables provide the ROUGE-1 scores: 
 10% 15% 20% 25% 30%
(1) MMR .585 .563 .523 .492 .467
(2) (1) +lexicon .602 .579 .543 .506 .476
(3) (2)+structure .621 .591 .553 .516 .482
(4) (3)+acoustic .619 .594 .554 .519 .485
(5) (4)+spoken language .619 .600 .566 .530 .492 
Table 3. ROUGE-1 of LR summarizers using incremental features 
 
 10% 15% 20% 25% 30%
(1) MMR .585 .563 .523 .492 .467
(2) (1) +lexicon .604 .581 .542 .504 .577
(3) (2)+structure .617 .600 .563 .523 .490
(4) (3)+acoustic .629 .610 .573 .533 .496
(5)(4)+spoken language .628 .611 .576 .535 .502 
Table 4. ROUGE-1 of SVM summarizers using incremental features 
The ROUGE-1 scores show similar tendencies to 
the f-scores: the rich features improve 
summarization performance over the baseline 
MMR summarizers. Other ROUGE scores like 
ROUGE-L show the same tendency, but are not 
presented here due to the space limit.  
Both the f-score and ROUGE indicate that, in 
general, rich features incrementally improve 
summarization performance.  
4.3 Comparison of features 
To study the effectiveness of individual features, 
the receiver operating characteristic (ROC) curves 
of these features are presented in Figure-1 below. 
The larger the area under a curve is, the better the 
performance of this feature is. To be more exact, 
the definition for the y-coordinate (sensitivity) and 
the x-coordinate (1-specificity) is: 
ratenegtivetrue
FPTN
TN
yspecificit
ratepositivetrue
FNTP
TP
ysensitivit
=+=
=+=  
where TP, FN, TN and FP are true positive, false 
negative, true negative, and false positive, 
respectively. 
 
Figure-1. ROC curves for individual features 
Lexicon and MMR features are the best two 
individual features, followed by spoken-language 
and acoustic features. The structural feature is least 
effective.   
Let us first revisit the problem (2) discussed 
above in the introduction. The effectiveness of the 
structural feature is less significant than it is in 
broadcast news. According to the ROC curves 
presented in Christensen et al (2004), the 
structural feature (utterance position) is one of the 
best features for summarizing read news stories, 
and is less effective when news stories contain 
spontaneous speech. Both their ROC curves cover 
larger area than the structural feature here in figure 
1, that is, the structure feature is less effective for 
summarizing spontaneous conversation than it is in 
broadcast news. This reflects, to some extent, that 
199
information is more evenly distributed in 
spontaneous conversations.  
Now let us turn to the role of speech disfluencies, 
which are very common in spontaneous 
conversations. Previous work detects and removes 
disfluencies as noise. Indeed, disfluencies show 
regularities in a number of dimensions (Shriberg, 
1994). They correlate with many factors including 
the topic difficulty (Bortfeld et al 2001). Tables 1-
4 above show that they improve summarization 
performance when added upon other features. 
Figure-1 shows that when used individually, they 
are better than the structural feature, and also better 
than acoustic features at the left 1/3 part of the 
figure, where the summary contains relatively 
fewer utterances. Disfluencies, e.g., pauses, are 
often inserted when speakers have word-searching 
problem, e.g., a problem finding topic-specific 
keywords:  
Speaker A: with all the uh sulfur and all that other 
stuff they're dumping out into the atmosphere. 
The above example is taken from a conversation 
that discusses pollution. The speaker inserts a filled 
pause uh in front of the word sulfur. Pauses are not 
randomly inserted. To show this, we remove them 
from transcripts. Section-2 of SWITCHBOARD 
(about 870 dialogues and 189,000 utterances) is 
used for this experiment. Then we insert these 
pauses back randomly, or insert them back at their 
original places, and compare the difference. For 
both cases, we consider a window with 4 words 
after each filled pause. We average the tf.idf scores 
of the words in each of these windows. Then, for 
all speaker-inserted pauses, we obtain a set of 
averaged tf.idf scores. And for all randomly-
inserted pauses, we have another set. The mean of 
the former set (5.79 in table 5) is statistically 
higher than that of the latter set (5.70 in table 5). 
We can adjust the window size to 3, 2 and 1, and 
then get the following table. 
Window size 1 2 3 4 
Insert Randomly 5.69 5.69 5.70 5.70Mean of 
tf.idf score Insert by speaker  5.72 5.82 5.81 5.79
Difference is significant? (t-test, p<0.05) Yes Yes Yes Yes 
Table 5.  Average tf.idf scores of words following filled pauses. 
The above table shows that instead of randomly 
inserting pauses, real speakers insert them in front 
of words with higher tf.idf scores. This helps 
explain why disfluencies work. 
5 Conclusions 
Previous work on summarizing spontaneous 
conversations has mainly focused on textual 
features. This paper explores and compares both 
textual and speech-related features. The 
experiments show that these features incrementally 
improve summarization performance. We also find 
that speech disfluencies, which are removed as 
noise in previous work, help identify important 
utterances, while the structural feature is less 
effective than it is in broadcast news.  
6 References 
Bortfeld, H., Leon, S.D., Bloom, J.E., Schober, M.F., & 
Brennan, S.E. 2001. Disfluency Rates in Conversation: 
Effects of Age, Relationship, Topic Role, and Gender. 
Language and Speech, 44(2): 123-147  
Christensen, H., Kolluru, B., Gotoh, Y., Renals, S., 2004. 
From text summarisation to style-specific 
summarisation for broadcast news. Proc. ECIR-2004. 
Furui, S., Kikuichi T. Shinnaka Y., and Hori C. 2003. 
Speech-to-speech and speech to text summarization,. 
First International workshop on Language 
Understanding and Agents for Real World Interaction, 
2003. 
Gurevych I. and Strube M. 2004. Semantic Similarity 
Applied to Spoken Dialogue Summarization. COLING-
2004. 
Hori C. and Furui S., 2003. A New Approach to Automatic 
Speech Summarization IEEE Transactions on 
Multimedia, Vol. 5, NO. 3, September 2003,  
Kikuchi T., Furui S. and Hori C., 2003. Automatic Speech 
Summarization Based on Sentence Extraction and 
Compaction, Proc. ICASSP-2003. 
Koumpis K., 2002. Automatic Voicemail Summarisation 
for Mobile Messaging Ph.D. Thesis, University of 
Sheffield, UK, 2002. 
Maskey, S.R., Hirschberg, J. "Comparing Lexial, 
Acoustic/Prosodic, Discourse and Structural Features 
for Speech Summarization", Eurospeech 2005. 
Shriberg, E.E. (1994). Preliminaries to a Theory of Speech 
Disfluencies. Ph.D. thesis, University of California at 
Berkeley. 
Zechner K. and Waibel A., 2000. Minimizing word error 
rate in textual summaries of spoken language. NAACL-
2000. 
Zechner K., 2001. Automatic Summarization of Spoken 
Dialogues in Unrestricted Domains. Ph.D. thesis, 
Carnegie Mellon University, November 2001. 
200
Proceedings of ACL-08: HLT, pages 470?478,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Critical Reassessment of Evaluation Baselines for Speech Summarization
Gerald Penn and Xiaodan Zhu
University of Toronto
10 King?s College Rd.
Toronto M5S 3G4 CANADA
 
gpenn,xzhu  @cs.toronto.edu
Abstract
We assess the current state of the art in speech
summarization, by comparing a typical sum-
marizer on two different domains: lecture data
and the SWITCHBOARD corpus. Our re-
sults cast significant doubt on the merits of this
area?s accepted evaluation standards in terms
of: baselines chosen, the correspondence of
results to our intuition of what ?summaries?
should be, and the value of adding speech-
related features to summarizers that already
use transcripts from automatic speech recog-
nition (ASR) systems.
1 Problem definition and related literature
Speech is arguably the most basic, most natural form
of human communication. The consistent demand
for and increasing availability of spoken audio con-
tent on web pages and other digital media should
therefore come as no surprise. Along with this avail-
ability comes a demand for ways to better navigate
through speech, which is inherently more linear or
sequential than text in its traditional delivery.
Navigation connotes a number of specific tasks,
including search, but also browsing (Hirschberg et
al., 1999) and skimming, which can involve far
more analysis and manipulation of content than the
spoken document retrieval tasks of recent NIST
fame (1997 2000). These would include time com-
pression of the speech signal and/or ?dichotic? pre-
sentations of speech, in which a different audio track
is presented to either ear (Cherry and Taylor, 1954;
Ranjan et al, 2006). Time compression of speech,
on the other hand, excises small slices of digitized
speech data out of the signal so that the voices speak
all of the content but more quickly. The excision
can either be fixed rate, for which there have been
a number of experiments to detect comprehension
limits, or variable rate, where the rate is determined
by pause detection and shortening (Arons, 1992),
pitch (Arons, 1994) or longer-term measures of lin-
guistic salience (Tucker and Whittaker, 2006). A
very short-term measure based on spectral entropy
can also be used (Ajmal et al, 2007), which has
the advantage that listeners cannot detect the vari-
ation in rate, but they nevertheless comprehend bet-
ter than fixed-rate baselines that preserve pitch pe-
riods. With or without variable rates, listeners can
easily withstand a factor of two speed-up, but Likert
response tests definitively show that they absolutely
hate doing it (Tucker and Whittaker, 2006) relative
to word-level or utterance-level excisive methods,
which would include the summarization-based strat-
egy that we pursue in this paper.
The strategy we focus on here is summariza-
tion, in its more familiar construal from compu-
tational linguistics and information retrieval. We
view it as an extension of the text summarization
problem in which we use automatically prepared,
imperfect textual transcripts to summarize speech.
Other details are provided in Section 2.2. Early
work on speech summarization was either domain-
restricted (Kameyama and Arima, 1994), or prided
itself on not using ASR at all, because of its unreli-
ability in open domains (Chen and Withgott, 1992).
Summaries of speech, however, can still be delivered
audially (Kikuchi et al, 2003), even when (noisy)
transcripts are used.
470
The purpose of this paper is not so much to in-
troduce a new way of summarizing speech, as to
critically reappraise how well the current state of
the art really works. The earliest work to con-
sider open-domain speech summarization seriously
from the standpoint of text summarization technol-
ogy (Valenza et al, 1999; Zechner and Waibel,
2000) approached the task as one of speech tran-
scription followed by text summarization of the re-
sulting transcript (weighted by confidence scores
from the ASR system), with the very interesting re-
sult that transcription and summarization errors in
such systems tend to offset one another in overall
performance. In the years following this work, how-
ever, some research by others on speech summa-
rization (Maskey and Hirschberg, 2005; Murray et
al., 2005; Murray et al, 2006, inter alia) has fo-
cussed de rigueur on striving for and measuring the
improvements attainable over the transcribe-then-
summarize baseline with features available from
non-transcriptional sources (e.g., pitch and energy
of the acoustic signal) or those, while evident in tex-
tual transcripts, not germane to texts other than spo-
ken language transcripts (e.g., speaker changes or
question-answer pair boundaries).
These ?novel? features do indeed seem to help,
but not by nearly as much as some of this recent
literature would suggest. The experiments and the
choice of baselines have largely been framed to il-
luminate the value of various knowledge sources
(?prosodic features,? ?named entity features? etc.),
rather than to optimize performance per se ? al-
though the large-dimensional pattern recognition al-
gorithms and classifiers that they use are inappropri-
ate for descriptive hypothesis testing.
First, most of the benefit attained by these novel
sources can be captured simply by measuring the
lengths of candidate utterances. Only one paper we
are aware of (Christensen et al, 2004) has presented
the performance of length on its own, although the
objective there was to use length, position and other
simple textual feature baselines (no acoustics) to
distinguish the properties of various genres of spo-
ken audio content, a topic that we will return to in
Section 2.1.1 Second, maximal marginal relevance
1Length features are often mentioned in the text of other
work as the most beneficial single features in more hetero-
(MMR) has also fallen by the wayside, although it
too performs very well. Again, only one paper that
we are aware of (Murray et al, 2005) provides an
MMR baseline, and there MMR significantly out-
performs an approach trained on a richer collection
of features, including acoustic features. MMR was
the method of choice for utterance selection in Zech-
ner and Waibel (2000) and their later work, but it
is often eschewed perhaps because textbook MMR
does not directly provide a means to incorporate
other features. There is a simple means of doing so
(Section 2.3), and it is furthermore very resilient to
low word-error rates (WERs, Section 3.3).
Third, as inappropriate uses of optimization meth-
ods go, the one comparison that has not made it
into print yet is that of the more traditional ?what-is-
said? features (MMR, length in words and named-
entity features) vs. the avant-garde ?how-it-is-said?
features (structural, acoustic/prosodic and spoken-
language features). Maskey & Hirschberg (2005)
divide their features into these categories, but only
to compute a correlation coefficient between them
(0.74). The former in aggregate still performs sig-
nificantly better than the latter in aggregate, even if
certain members of the latter do outperform certain
members of the former. This is perhaps the most re-
assuring comparison we can offer to text summariza-
tion and ASR enthusiasts, because it corroborates
the important role that ASR still plays in speech
summarization in spite of its imperfections.
Finally, and perhaps most disconcertingly, we
can show that current speech summarization per-
forms just as well, and in some respects even bet-
ter, with SWITCHBOARD dialogues as it does with
more coherent spoken-language content, such as lec-
tures. This is not a failing of automated systems
themselves ? even humans exhibit the same ten-
dency under the experimental conditions that most
researchers have used to prepare evaluation gold
standards. What this means is that, while speech
summarization systems may arguably be useful and
are indeed consistent with whatever it is that humans
are doing when they are enlisted to rank utterances,
this evaluation regime simply does not reflect how
well the ?summaries? capture the goal-orientation or
geneous systems, but without indicating their performance on
their own.
471
higher-level purpose of the data that they are trained
on. As a community, we have been optimizing an
utterance excerpting task, we have been moderately
successful at it, but this task in at least one impor-
tant respect bears no resemblance to what we could
convincingly call speech summarization.
These four results provide us with valuable insight
into the current state of the art in speech summariza-
tion: it is not summarization, the aspiration to mea-
sure the relative merits of knowledge sources has
masked the prominence of some very simple base-
lines, and the Zechner & Waibel pipe-ASR-output-
into-text-summarizer model is still very competitive
? what seems to matter more than having access
to the raw spoken data is simply knowing that it is
spoken data, so that the most relevant, still textu-
ally available features can be used. Section 2 de-
scribes the background and further details of the ex-
periments that we conducted to arrive at these con-
clusions. Section 3 presents the results that we ob-
tained. Section 4 concludes by outlining an ecologi-
cally valid alternative for evaluating real summariza-
tion in light of these results.
2 Setting of the experiment
2.1 Provenance of the data
Speech summarizers are generally trained to sum-
marize either broadcast news or meetings. With
the exception of one paper that aspires to compare
the ?styles? of spoken and written language ceteris
paribus (Christensen et al, 2004), the choice of
broadcast news as a source of data in more recent
work is rather curious. Broadcast news, while open
in principle in its range of topics, typically has a
range of closely parallel, written sources on those
same topics, which can either be substituted for spo-
ken source material outright, or at the very least
be used corroboratively alongside them. Broadcast
news is also read by professional news readers, using
high quality microphones and studio equipment, and
as a result has very lower WER ? some even call
ASR a solved problem on this data source. Broad-
cast news is also very text-like at a deeper level. Rel-
ative position within a news story or dialogue, the
dreaded baseline of text summarization, works ex-
tremely well in spoken broadcast news summariza-
tion, too. Within the operating region of the receiver
operating characteristics (ROC) curve most relevant
to summarizers (0.1?0.3), Christensen et al (2004)
showed that position was by far the best feature in
a read broadcast news system with high WER, and
that position and length of the extracted utterance
were the two best with low WER. Christensen et
al. (2004) also distinguished read news from ?spon-
taneous news,? broadcasts that contain interviews
and/or man-in-the-field reports, and showed that in
the latter variety position is not at all prominent
at any level of WER, but length is. Maskey &
Hirschberg?s (2005) broadcast news is a combina-
tion of read news and spontaneous news.
Spontaneous speech, in our view, particularly in
the lecture domain, is our best representative of what
needs to be summarized. Here, the positional base-
line performs quite poorly (although length does ex-
tremely well, as discussed below), and ASR per-
formance is far from perfect. In the case of lec-
tures, there are rarely exact transcripts available, but
there are bulleted lines from presentation slides, re-
lated research papers on the speaker?s web page and
monographs on the same topic that can be used to
improve the language models for speech recogni-
tion systems. Lectures have just the right amount of
props for realistic ASR, but still very open domain
vocabularies and enough spontaneity to make this a
problem worth solving. As discussed further in Sec-
tion 4, the classroom lecture genre also provides us
with a task that we hope to use to conduct a better
grounded evaluation of real summarization quality.
To this end, we use a corpus of lectures recorded
at the University of Toronto to train and test our sum-
marizer. Only the lecturer is recorded, using a head-
worn microphone, and each lecture lasts 50 minutes.
The lectures in our experiments are all undergradu-
ate computer science lectures. The results reported
in this paper used four different lectures, each from
a different course and spoken by a different lecturer.
We used a leave-one-out cross-validation approach
by iteratively training on three lectures worth of ma-
terial and testing on the one remaining. We combine
these iterations by averaging. The lectures were di-
vided at random into 8?15 minute intervals, how-
ever, in order to provide a better comparison with
the SWITCHBOARD dialogues. Each interval was
treated as a separate document and was summarized
separately. So the four lectures together actually
472
provide 16 SWITCHBOARD-sized samples of ma-
terial, and our cross-validation leaves on average
four of them out in a turn.
We also use part of the SWITCHBOARD cor-
pus in one of our comparisons. SWITCHBOARD
is a collection of telephone conversations, in which
two participants have been told to speak on a cer-
tain topic, but with no objective or constructive
goal to proceed towards. While the conversations
are locally coherent, this lack of goal-orientation is
acutely apparent in all of them ? they may be as
close as any speech recording can come to being
about nothing.2 We randomly selected 27 conver-
sations, containing a total of 3665 utterances (iden-
tified by pause length), and had three human anno-
tators manually label each utterance as in- or out-
of-summary. Interestingly, the interannotator agree-
ment on SWITCHBOARD (   		 ) is higher
than on the lecture corpus (0.372) and higher than
the   -score reported by Galley (2006) for the ICSI
meeting data used by Murray et al (2005; 2006),
in spite of the fact that Murray et al (2005) primed
their annotators with a set of questions to consider
when annotating the data.3 This does not mean that
the SWITCHBOARD summaries are qualitatively
better, but rather that annotators are apt to agree
more on which utterances to include in them.
2.2 Summarization task
As with most work in speech summarization, our
strategy involves considering the problem as one
of utterance extraction, which means that we are
not synthesizing new text or speech to include in
summaries, nor are we attempting to extract small
phrases to sew together with new prosodic contours.
Candidate utterances are identified through pause-
length detection, and the length of these pauses has
been experimentally calibrated to 200 msec, which
results in roughly sentence-sized utterances. Sum-
marization then consists of choosing the best N% of
these utterances for the summary, where N is typ-
2It should be noted that the meandering style of SWITCH-
BOARD conversations does have correlates in text processing,
particularly in the genres of web blogs and newsgroup- or wiki-
based technical discussions.
3Although we did define what a summary was to each anno-
tator beforehand, we did not provide questions or suggestions
on content for either corpus.
ically between 10 and 30. We will provide ROC
curves to indicate performance as a function over all
N. An ROC is plotted along an x-axis of specificity
(true-negative-rate) and a y-axis of sensitivity (true-
positive-rate). A larger area under the ROC corre-
sponds to better performance.
2.3 Utterance isolation
The framework for our extractive summarization ex-
periments is depicted in Figure 1. With the excep-
tion of disfluency removal, it is very similar in its
overall structure to that of Zechner?s (2001). The
summarizer takes as input either manual or auto-
matic transcripts together with an audio file, and
has three modules to process disfluencies and extract
features important to identifying sentences.
Figure 1: Experimental framework for summarizing
spontaneous conversations.
During sentence boundary detection, words that
are likely to be adjacent to an utterance boundary
are determined. We call these words trigger words.
False starts are very common in spontaneous
speech. According to Zechner?s (2001) statistics on
the SWITCHBOARD corpus, they occur in 10-15%
of all utterances. A decision tree (C4.5, Release
8) is used to detect false starts, trained on the POS
tags and trigger-word status of the first and last four
words of sentences from a training set. Once false
starts are detected, these are removed.
We also identify repetitions as a sequence of be-
tween 1 and 4 words which is consecutively re-
473
peated in spontaneous speech. Generally, repetitions
are discarded. Repetitions of greater length are ex-
tremely rare statistically and are therefore ignored.
Question-answer pairs are also detected and
linked. Question-answer detection is a two-stage
process. The system first identifies the questions and
then finds the corresponding answer. For (both WH-
and Yes/No) question identification, another C4.5
classifier was trained on 2,000 manually annotated
sentences using utterance length, POS bigram oc-
currences, and the POS tags and trigger-word status
of the first and last five words of an utterance. After
a question is identified, the immediately following
sentence is labelled as the answer.
2.4 Utterance selection
To obtain a trainable utterance selection module that
can utilize and compare rich features, we formu-
lated utterance selection as a standard binary clas-
sification problem, and experimented with several
state-of-the-art classifiers, including linear discrim-
inant analysis LDA, support vector machines with
a radial basis kernel (SVM), and logistic regression
(LR), as shown in Figure 2 (computed on SWITCH-
BOARD data). MMR, Zechner?s (2001) choice, is
provided as a baseline. MMR linearly interpolates
a relevance component and a redundancy compo-
nent that balances the need for new vs. salient in-
formation. These two components can just as well
be mixed through LR, which admits the possibility
of adding more features and the benefit of using LR
over held-out estimation.
0 0.2 0.4 0.6 0.8 10
0.2
0.4
0.6
0.8
1
R
ec
al
l
Precision
 
 
LR?full?fea
LDA?full?fea
SVM?full?fea
LR?MMR?fea
MMR
Figure 2: Precision-recall curve for several classifiers on
the utterance selection task.
As Figure 2 indicates, there is essentially no dif-
ference in performance among the three classifiers
we tried, nor between MMR and LR restricted to
the two MMR components. This is important, since
we will be comparing MMR to LR-trained classi-
fiers based on other combinations of features below.
The ROC curves in the remainder of this paper have
been prepared using the LR classifier.
2.5 Features extracted
While there is very little difference realized across
pattern recognition methods, there is much more at
stake with respect to which features the methods use
to characterize their input. We can extract and use
the features in Figure 3, arranged there according to
their knowledge source.
We detect disfluencies in the same manner as
Zechner (2001)). Taking ASR transcripts as input,
we use the Brill tagger (Brill, 1995) to assign POS
tags to each word. There are 42 tags: Brill?s 38 plus
four which identify filled-pause disfluencies:
  empty coordinating conjunctions (CO),
  lexicalized filled pauses (DM),
  editing terms (ET), and
  non-lexicalized filled pauses (UH).
Our disfluency features include the number of each
of these, their total, and also the number of repeti-
tions. Disfluencies adjacent to a speaker turn are ig-
nored, however, because they occur as a normal part
of turn coordination between speakers.
Our preliminary experiments suggest that speaker
meta-data do not improve on the quality of summa-
rization, and so this feature is not included.
We indicate with bold type the features that indi-
cate some quantity of length, and we will consider
these as members of another class called ?length,?
in addition to their given class above. In all of the
data on which we have measured, the correlation be-
tween time duration and number of words is nearly
1.00 (although pause length is not).
2.6 Evaluation of summary quality
We plot receiver operating characteristic (ROC)
curves along a range of possible compression pa-
rameters, and in one case, ROUGE scores. ROUGE
474
1. Lexical features
  MMR score4,
  utterance length (in words),
2. Named entity features ? number of:
  person names,
  location names
  organization names
  the sum of these
3. Structural features
  utterance position, labelled as first, middle, or
last one-third of the conversation
  a Boolean feature indicating whether an utter-
ance is adjacent to a speaker turn
1. Acoustic features ? min, max and avg. of:5
  pitch
  energy
  speaking rate
  (unfilled) pause length
  time duration (in msec)
2. ?Spoken language? features
  disfluencies
  given/new information
  question/answer pair identification
Figure 3: Features available for utterance selection by knowledge source. Features in bold type quantify length. In our
experiments, we exclude these from their knowledge sources, and study them as a separate length category.
and F-measure are both widely used in speech sum-
marization, and they have been shown by others
to be broadly consistent on speech summarization
tasks (Zhu and Penn, 2005).
3 Results and analysis
3.1 Lecture corpus
The results of our evaluation on the lecture data ap-
pear in Figure 4. As is evident, there is very little
difference among the combinations of features with
this data source, apart from the positional baseline,
?lead,? which simply chooses the first N% of the
utterances. This performs quite poorly. The best
performance is achieved by using all of the features
together, but the length baseline, which uses only
those features in bold type from Figure 3, is very
close (no statistically significant difference), as is
MMR.6
4When evaluated on its own, the MMR interpolating param-
eter is set through experimentation on a held-out dataset, as in
Zechner (2001). When combined with other features, its rele-
vance and redundancy components are provided to the classifier
separately.
5All of these features are calculated on the word level and
normalized by speaker.
6We conducted the same evaluation without splitting the lec-
tures into 8?15 minute segments (so that the summaries sum-
marize an entire lecture), and although space here precludes
the presentation of the ROC curves, they are nearly identical
Figure 4: ROC curve for utterance selection with the lec-
ture corpus with several feature combinations.
3.2 SWITCHBOARD corpus
The corresponding results on SWITCHBOARD are
shown in Figure 5. Again, length and MMR are
very close to the best alternative, which is again all
of features combined. The difference with respect
to either of these baselines is statistically significant
within the popular 10?30% compression range, as
is the classifier trained on all features but acoustic
to those on the segments shown here.
475
Figure 5: ROC curve for SWITCHBOARD utterance se-
lection with several feature combinations.
(not shown). The classifier trained on all features
but spoken language features (not shown) is not sig-
nificantly better, so it is the spoken language fea-
tures that make the difference, not the acoustic fea-
tures. The best score is also significantly better than
on the lecture data, however, particularly in the 10?
30% range. Our analysis of the difference suggests
that the much greater variance in utterance length in
SWITCHBOARD is what accounts for the overall
better performance of the automated system as well
as the higher human interannotator agreement. This
also goes a long way to explaining why the length
baseline is so good.
Still another perspective is to classify features as
either ?what-is-said? (MMR, length and NE fea-
tures) or ?how-it-is-said? (structural, acoustic and
spoken-language features), as shown in Figure 6.
What-is-said features are better, but only barely so
within the usual operating region of summarizers.
3.3 Impact of WER
Word error rates (WERs) arising from speech recog-
nition are usually much higher in spontaneous con-
versations than in read news. Having trained ASR
models on SWITCHBOARD section 2 data with
our sample of 27 conversations removed, the WER
on that sample is 46%. We then train a language
model on SWITCHBOARD section 2 without re-
moving the 27-conversation sample so as to delib-
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Se
ns
itiv
ity
1?Specificity
 
 
all
what?is?said
how?it?is?said
Figure 6: ROC curves for textual and non-textual fea-
tures.
erately overfit the model. This pseudo-WER is then
39%. We might be able to get less WER by tuning
the ASR models or by using more training data, but
that is not the focus here. Summarizing the auto-
matic transcripts generated from both of these sys-
tems using our LR-based classifier with all features,
as well as manual (perfect) transcripts, we obtain the
ROUGE?1 scores in Table 1.
WER 10% 15% 20% 25% 30%
0.46 .615 .591 .556 .519 .489
0.39 .615 .591 .557 .526 .491
0 .619 .600 .566 .530 .492
Table 1: ROUGE?1 of LR system with all features under
different WERs.
Table 1 shows that WERs do not impact summa-
rization performance significantly. One reason is
that the acoustic and structural features are not af-
fected by word errors, although WERs can affect
the MMR, spoken language, length and NE features.
Figures 7 and 8 present the ROC curves of the MMR
and spoken language features, respectively, under
different WERs. MMR is particularly resilient,
even on SWITCHBOARD. Keywords are still often
correctly recognized, even in the presence of high
WER, although possibly because the same topic is
discussed in many SWITCHBOARD conversations.
476
Figure 7: ROC curves for the effectiveness of MMR
scores on transcripts under different WERs.
Figure 8: ROC curves for the effectiveness of spoken lan-
guage features on transcripts under different WERs.
When some keywords are misrecognized (e.g. hat),
furthermore, related words (e.g. dress, wear) still
may identify important utterances. As a result, a
high WER does not necessarily mean a worse tran-
script for bag-of-keywords applications like sum-
marization and classification, regardless of the data
source. Utterance length does not change very much
when WERs vary, and in addition, it is often a la-
tent variable that underlies some other features? role,
e.g., a long utterance often has a higher MMR score
than a short utterance, even when the WER changes.
Note that the effectiveness of spoken language
features varies most between manually and automat-
ically generated transcripts just at around the typi-
cal operating region of most summarization systems.
The features of this category that respond most to
WER are disfluencies. Disfluency detection is also
at its most effective in this same range with respect
to any transcription method.
4 Future Work
In terms of future work in light of these results,
clearly the most important challenge is to formu-
late an experimental alternative to measuring against
a subjectively classified gold standard in which an-
notators are forced to commit to relative salience
judgements with no attention to goal orientation and
no requirement to synthesize the meanings of larger
units of structure into a coherent message. It is here
that using the lecture domain offers us some addi-
tional assistance. Once these data have been tran-
scribed and outlined, we will be able to formulate
examinations for students that test their knowledge
of the topics being lectured upon: both their higher-
level understanding of goals and conceptual themes,
as well as factoid questions on particular details. A
group of students can be provided with access to a
collection of entire lectures to establish a theoreti-
cal limit. Experimental and control groups can then
be provided with access only to summaries of those
lectures, prepared using different sets of features, or
different modes of delivery (text vs. speech), for ex-
ample. This task-based protocol involves quite a bit
more work, and at our university, at least, there are
regulations that preclude us placing a group of stu-
dents in a class at a disadvantage with respect to an
examination for credit that need to be dealt with. It
is, however, a far better means of assessing the qual-
ity of summaries in an ecologically valid context.
It is entirely possible that, within this protocol, the
baselines that have performed so well in our experi-
ments, such as length or, in read news, position, will
utterly fail, and that less traditional acoustic or spo-
ken language features will genuinely, and with sta-
tistical significance, add value to a purely transcript-
based text summarization system. To date, how-
ever, that case has not been made. He et al (1999)
conducted a study very similar to the one suggested
above and found no significant difference between
using pitch and using slide transition boundaries. No
ASR transcripts or length features were used.
477
References
M. Ajmal, A. Kushki, and K. N. Plataniotis. 2007. Time-
compression of speech in informational talks using
spectral entropy. In Proceedings of the 8th Interna-
tional Workshop on Image Analysis for Multimedia In-
teractive Services (WIAMIS-07).
B Arons. 1992. Techniques, perception, and applications
of time-compressed speech. In American Voice I/O
Society Conference, pages 169?177.
B. Arons. 1994. Speech Skimmer: Interactively Skim-
ming Recorded Speech. Ph.D. thesis, MIT Media Lab.
E. Brill. 1995. Transformation-based error-driven learn-
ing and natural language processing: A case study
in part-of-speech tagging. Computational Linguistics,
21(4):543?565.
F. Chen and M. Withgott. 1992. The use of emphasis
to automatically summarize a spoken discourse. In
Proceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP),
volume 1, pages 229?232.
E. Cherry and W. Taylor. 1954. Some further exper-
iments on the recognition of speech, with one and
two ears. Journal of the Acoustic Society of America,
26:554?559.
H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.
2004. From text summarisation to style-specific sum-
marisation for broadcast news. In Proceedings of the
26th European Conference on Information Retrieval
(ECIR-2004), pages 223?237.
M. Galley. 2006. A skip-chain conditional random field
for ranking meeting utterances by importance. In Pro-
ceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP 2006).
L. He, E. Sanocki, A. Gupta, and J. Grudin. 1999. Auto-
summarization of audio-video presentations. In MUL-
TIMEDIA ?99: Proceedings of the seventh ACM in-
ternational conference on Multimedia (Part 1), pages
489?498.
J. Hirschberg, S. Whittaker, D. Hindle, F. Pereira, and
A. Singhal. 1999. Finding information in audio: A
new paradigm for audio browsing and retrieval. In
Proceedings of the ESCA/ETRW Workshop on Access-
ing Information in Spoken Audio, pages 117?122.
M. Kameyama and I. Arima. 1994. Coping with about-
ness complexity in information extraction from spo-
ken dialogues. In Proceedings of the 3rd International
Conference on Spoken Language Processing (ICSLP),
pages 87?90.
T. Kikuchi, S. Furui, and C. Hori. 2003. Two-stage au-
tomatic speech summarization by sentence extraction
and compaction. In Proceedings of the ISCA/IEEE
Workshop on Spontaneous Speech Processing and
Recognition (SSPR), pages 207?210.
S. Maskey and J. Hirschberg. 2005. Comparing lex-
ial, acoustic/prosodic, discourse and structural features
for speech summarization. In Proceedings of the 9th
European Conference on Speech Communication and
Technology (Eurospeech), pages 621?624.
G. Murray, S. Renals, and J. Carletta. 2005. Extractive
summarization of meeting recordings. In Proceedings
of the 9th European Conference on Speech Communi-
cation and Technology (Eurospeech), pages 593?596.
G. Murray, S. Renals, J. Moore, and J. Carletta. 2006. In-
corporating speaker and discourse features into speech
summarization. In Proceedings of the Human Lan-
guage Technology Conference - Annual Meeting of the
North American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL), pages 367?374.
National Institute of Standards. 1997?2000. Pro-
ceedings of the Text REtrieval Conferences.
http://trec.nist.gov/pubs.html.
Abhishek Ranjan, Ravin Balakrishnan, and Mark
Chignell. 2006. Searching in audio: the utility of tran-
scripts, dichotic presentation, and time-compression.
In CHI ?06: Proceedings of the SIGCHI conference on
Human Factors in computing systems, pages 721?730,
New York, NY, USA. ACM Press.
S. Tucker and S. Whittaker. 2006. Time is of the essence:
an evaluation of temporal compression algorithms. In
CHI ?06: Proceedings of the SIGCHI conference on
Human Factors in computing systems, pages 329?338,
New York, NY, USA. ACM Press.
R. Valenza, T. Robinson, M. Hickey, and R. Tucker.
1999. Summarization of spoken audio through infor-
mation extraction. In Proceedings of the ESCA/ETRW
Workshop on Accessing Information in Spoken Audio,
pages 111?116.
K. Zechner and A. Waibel. 2000. Minimizing word er-
ror rate in textual summaries of spoken language. In
Proceedings of the 6th Applied Natural Language Pro-
cessing Conference and the 1st Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics (ANLP/NAACL), pages 186?193.
K. Zechner. 2001. Automatic Summarization of Spo-
ken Dialogues in Unrestricted Domains. Ph.D. thesis,
Carnegie Mellon University.
X. Zhu and G. Penn. 2005. Evaluation of sentence selec-
tion for speech summarization. In Proceedings of the
RANLP workshop on Crossing Barriers in Text Sum-
marization Research, pages 39?45.
478
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 549?557,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Summarizing multiple spoken documents: finding evidence from
untranscribed audio
Xiaodan Zhu, Gerald Penn and Frank Rudzicz
University of Toronto
10 King?s College Rd.,
Toronto, M5S 3G4, ON, Canada
{xzhu,gpenn,frank}@cs.toronto.edu
Abstract
This paper presents a model for summa-
rizing multiple untranscribed spoken doc-
uments. Without assuming the availabil-
ity of transcripts, the model modifies a
recently proposed unsupervised algorithm
to detect re-occurring acoustic patterns in
speech and uses them to estimate similari-
ties between utterances, which are in turn
used to identify salient utterances and re-
move redundancies. This model is of in-
terest due to its independence from spo-
ken language transcription, an error-prone
and resource-intensive process, its abil-
ity to integrate multiple sources of infor-
mation on the same topic, and its novel
use of acoustic patterns that extends pre-
vious work on low-level prosodic feature
detection. We compare the performance of
this model with that achieved using man-
ual and automatic transcripts, and find that
this new approach is roughly equivalent
to having access to ASR transcripts with
word error rates in the 33?37% range with-
out actually having to do the ASR, plus
it better handles utterances with out-of-
vocabulary words.
1 Introduction
Summarizing spoken documents has been exten-
sively studied over the past several years (Penn
and Zhu, 2008; Maskey and Hirschberg, 2005;
Murray et al, 2005; Christensen et al, 2004;
Zechner, 2001). Conventionally called speech
summarization, although speech connotes more
than spoken documents themselves, it is motivated
by the demand for better ways to navigate spoken
content and the natural difficulty in doing so ?
speech is inherently more linear or sequential than
text in its traditional delivery.
Previous research on speech summarization has
addressed several important problems in this field
(see Section 2.1). All of this work, however,
has focused on single-document summarization
and the integration of fairly simplistic acoustic
features, inspired by work in descriptive linguis-
tics. The issues of navigating speech content are
magnified when dealing with larger collections ?
multiple spoken documents on the same topic. For
example, when one is browsing news broadcasts
covering the same events or call-centre record-
ings related to the same type of customer ques-
tions, content redundancy is a prominent issue.
Multi-document summarization on written docu-
ments has been studied for more than a decade
(see Section 2.2). Unfortunately, no such effort
has been made on audio documents yet.
An obvious way to summarize multiple spo-
ken documents is to adopt the transcribe-and-
summarize approach, in which automatic speech
recognition (ASR) is first employed to acquire
written transcripts. Speech summarization is ac-
cordingly reduced to a text summarization task
conducted on error-prone transcripts.
Such an approach, however, encounters several
problems. First, assuming the availability of ASR
is not always valid for many languages other than
English that one may want to summarize. Even
when it is, transcription quality is often an issue?
training ASR models requires collecting and an-
notating corpora on specific languages, dialects,
or even different domains. Although recognition
errors do not significantly impair extractive sum-
marizers (Christensen et al, 2004; Zhu and Penn,
2006), error-laden transcripts are not necessarily
browseable if recognition errors are higher than
certain thresholds (Munteanu et al, 2006). In
such situations, audio summaries are an alterna-
tive when salient content can be identified directly
from untranscribed audio. Third, the underlying
paradigm of most ASR models aims to solve a
549
classification problem, in which speech is seg-
mented and classified into pre-existing categories
(words). Words not in the predefined dictionary
are certain to be misrecognized without excep-
tion. This out-of-vocabulary (OOV) problem is
unavoidable in the regular ASR framework, al-
though it is more likely to happen on salient words
such as named entities or domain-specific terms.
Our approach uses acoustic evidence from the
untranscribed audio stream. Consider text sum-
marization first: many well-known models such
as MMR (Carbonell and Goldstein, 1998) and
MEAD (Radev et al, 2004) rely on the reoccur-
rence statistics of words. That is, if we switch
any word w1 with another word w2 across an
entire corpus, the ranking of extracts (often sen-
tences) will be unaffected, because no word-
specific knowledge is involved. These mod-
els have achieved state-of-the-art performance in
transcript-based speech summarization (Zechner,
2001; Penn and Zhu, 2008). For spoken docu-
ments, such reoccurrence statistics are available
directly from the speech signal. In recent years, a
variant of dynamic time warping (DTW) has been
proposed to find reoccurring patterns in the speech
signal (Park and Glass, 2008). This method has
been successfully applied to tasks such as word
detection (Park and Glass, 2006) and topic bound-
ary detection (Malioutov et al, 2007).
Motivated by the work above, this paper ex-
plores the approach to summarizing multiple spo-
ken documents directly over an untranscribed au-
dio stream. Such a model is of interest because of
its independence from ASR. It is directly applica-
ble to audio recordings in languages or domains
when ASR is not possible or transcription quality
is low. In principle, this approach is free from the
OOV problem inherent to ASR. The premise of
this approach, however, is to reliably find reoccur-
ing acoustic patterns in audio, which is challeng-
ing because of noise and pronunciation variance
existing in the speech signal, as well as the dif-
ficulty of finding alignments with proper lengths
corresponding to words well. Therefore, our pri-
mary goal in this paper is to empirically determine
the extent to which acoustic information alone can
effectively replace conventional speech recogni-
tion with or without simple prosodic feature de-
tection within the multi-document speech summa-
rization task. As shown below, a modification of
the Park-Glass approach amounts to the efficacy
of a 33-37% WER ASR engine in the domain
of multiple spoken document summarization, and
also has better treatment of OOV items. Park-
Glass similarity scores by themselves can attribute
a high score to distorted paths that, in our context,
ultimately leads to too many false-alarm align-
ments, even after applying the distortion thresh-
old. We introduce additional distortion penalty
and subpath length constraints on their scoring to
discourage this possibility.
2 Related work
2.1 Speech summarization
Although abstractive summarization is more de-
sirable, the state-of-the-art research on speech
summarization has been less ambitious, focus-
ing primarily on extractive summarization, which
presents the most important N% of words,
phrases, utterances, or speaker turns of a spo-
ken document. The presentation can be in tran-
scripts (Zechner, 2001), edited speech data (Fu-
rui et al, 2003), or a combination of these (He
et al, 2000). Audio data amenable to summa-
rization include meeting recordings (Murray et al,
2005), telephone conversations (Zhu and Penn,
2006; Zechner, 2001), news broadcasts (Maskey
and Hirschberg, 2005; Christensen et al, 2004),
presentations (He et al, 2000; Zhang et al, 2007;
Penn and Zhu, 2008), etc.
Although extractive summarization is not as
ideal as abstractive summarization, it outperforms
several comparable alternatives. Tucker and Whit-
taker (2008) have shown that extractive summa-
rization is generally preferable to time compres-
sion, which speeds up the playback of audio doc-
uments with either fixed or variable rates. He et
al. (2000) have shown that either playing back im-
portant audio-video segments or just highlighting
the corresponding transcripts is significantly bet-
ter than providing users with full transcripts, elec-
tronic slides, or both for browsing presentation
recordings.
Given the limitations associated with ASR, it is
no surprise that previous work (He et al, 1999;
Maskey and Hirschberg, 2005; Murray et al,
2005; Zhu and Penn, 2006) has studied features
available in audio. The focus, however, is pri-
marily limited to prosody. The assumption is that
prosodic effects such as stress can indicate salient
information. Since a direct modeling of compli-
cated compound prosodic effects like stress is dif-
550
ficult, they have used basic features of prosody in-
stead, such as pitch, energy, duration, and pauses.
The usefulness of prosody was found to be very
limited by itself, if the effect of utterance length is
not considered (Penn and Zhu, 2008). In multiple-
spoken-document summarization, it is unlikely
that prosody will be more useful in predicating
salience than in single document summarization.
Furthermore, prosody is also unlikely to be appli-
cable to detecting or handling redundancy, which
is prominent in the multiple-document setting.
All of the work above has been conducted on
single-document summarization. In this paper
we are interested in summarizing multiple spo-
ken documents by using reoccurrence statistics of
acoustic patterns.
2.2 Multiple-document summarization
Multi-document summarization on written text
has been studied for over a decade. Compared
with the single-document task, it needs to remove
more content, cope with prominent redundancy,
and organize content from different sources prop-
erly. This field has been pioneered by early work
such as the SUMMONS architecture (Mckeown
and Radev, 1995; Radev and McKeown, 1998).
Several well-known models have been proposed,
i.e., MMR (Carbonell and Goldstein, 1998), multi-
Gen (Barzilay et al, 1999), and MEAD (Radev
et al, 2004). Multi-document summarization has
received intensive study at DUC. 1 Unfortunately,
no such efforts have been extended to summarize
multiple spoken documents yet.
Abstractive approaches have been studied since
the beginning. A famous effort in this direction
is the information fusion approach proposed in
Barzilay et al (1999). However, for error-prone
transcripts of spoken documents, an abstractive
method still seems to be too ambitious for the time
being. As in single-spoken-document summariza-
tion, this paper focuses on the extractive approach.
Among the extractive models, MMR (Carbonell
and Goldstein, 1998) and MEAD (Radev et al,
2004), are possibly the most widely known. Both
of them are linear models that balance salience and
redundancy. Although in principle, these mod-
els allow for any estimates of salience and re-
dundancy, they themselves calculate these scores
with word reoccurrence statistics, e.g., tf.idf,
and yield state-of-the-art performance. MMR it-
1http://duc.nist.gov/
eratively selects sentences that are similar to the
entire documents, but dissimilar to the previously
selected sentences to avoid redundancy. Its de-
tails will be revisited below. MEAD uses a redun-
dancy removal mechanism similar to MMR, but
to decide the salience of a sentence to the whole
topic, MEAD uses not only its similarity score
but also sentence position, e.g., the first sentence
of each new story is considered important. Our
work adopts the general framework of MMR and
MEAD to study the effectiveness of the acoustic
pattern evidence found in untranscribed audio.
3 An acoustics-based approach
The acoustics-based summarization technique
proposed in this paper consists of three consecu-
tive components. First, we detect acoustic patterns
that recur between pairs of utterances in a set of
documents that discuss a common topic. The as-
sumption here is that lemmata, words, or phrases
that are shared between utterances are more likely
to be acoustically similar. The next step is to com-
pute a relatedness score between each pair of ut-
terances, given the matching patterns found in the
first step. This yields a symmetric relatedness ma-
trix for the entire document set. Finally, the relat-
edness matrix is incorporated into a general sum-
marization model, where it is used for utterance
selection.
3.1 Finding common acoustic patterns
Our goal is to identify subsequences within acous-
tic sequences that appear highly similar to regions
within other sequences, where each sequence con-
sists of a progression of overlapping 20ms vec-
tors (frames). In order to find those shared pat-
terns, we apply a modification of the segmen-
tal dynamic time warping (SDTW) algorithm to
pairs of audio sequences. This method is similar
to standard DTW, except that it computes multi-
ple constrained alignments, each within predeter-
mined bands of the similarity matrix (Park and
Glass, 2008).2 SDTW has been successfully ap-
plied to problems such as topic boundary detec-
tion (Malioutov et al, 2007) and word detection
(Park and Glass, 2006). An example application
of SDTW is shown in Figure 1, which shows the
results of two utterances from the TDT-4 English
dataset:
2Park and Glass (2008) used Euclidean distance. We used
cosine distance instead, which was found to be better on our
held-out dataset.
551
I: the explosion in aden harbor killed seven-
teen u.s. sailors and injured other thirty
nine last month.
II: seventeen sailors were killed.
These two utterances share three words: killed,
seventeen, and sailors, though in different orders.
The upper panel of Figure 1 shows a matrix of
frame-level similarity scores between these two
utterances where lighter grey represents higher
similarity. The lower panel shows the four most
similar shared subpaths, three of which corre-
spond to the common words, as determined by the
approach detailed below.
Figure 1: Using segmental dynamic time warping
to find matching acoustic patterns between two ut-
terances.
Calculating MFCC
The first step of SDTW is to represent each utter-
ance as sequences of Mel-frequency cepstral coef-
ficient (MFCC) vectors, a commonly used repre-
sentation of the spectral characteristics of speech
acoustics. First, conventional short-time Fourier
transforms are applied to overlapping 20ms Ham-
ming windows of the speech amplitude signal.
The resulting spectral energy is then weighted
by filters on the Mel-scale and converted to 39-
dimensional feature vectors, each consisting of 12
MFCCs, one normalized log-energy term, as well
as the first and second derivatives of these 13 com-
ponents over time. The MFCC features used in
the acoustics-based approach are the same as those
used below in the ASR systems.
As in (Park and Glass, 2008), an additional
whitening step is taken to normalize the variances
on each of these 39 dimensions. The similarities
between frames are then estimated using cosine
distance. All similarity scores are then normalized
to the range of [0, 1], which yields similarity ma-
trices exemplified in the upper panel of Figure 1.
Finding optimal paths
For each similarity matrix obtained above, local
alignments of matching patterns need to be found,
as shown in the lower panel of Figure 1. A sin-
gle global DTW alignment is not adequate, since
words or phrases held in common between utter-
ances may occur in any order. For example, in Fig-
ure 1 killed occurs before all other shared words in
one document and after all of these in the other, so
a single alignment path that monotonically seeks
the lower right-hand corner of the similarity ma-
trix could not possibly match all common words.
Instead, multiple DTWs are applied, each starting
from different points on the left or top edges of the
similarity matrix, and ending at different points on
the bottom or right edges, respectively. The width
of this diagonal band is proportional to the esti-
mated number of words per sequence.
Given an M -by-N matrix of frame-level simi-
larity scores, the top-left corner is considered the
origin, and the bottom-right corner represents an
alignment of the last frames in each sequence. For
each of the multiple starting points p0 = (x0, y0)
where either x0 = 0 or y0 = 0, but not neces-
sarily both, we apply DTW to find paths P =
p0, p1, ..., pK that maximize
?
0? i? K sim(pi),
where sim(pi) is the cosine similarity score of
point pi = (xi, yi) in the matrix. Each point on the
path, pi, is subject to the constraint |xi ? yi| < T ,
where T limits the distortion of the path, as we
determine experimentally. The ending points are
pK = (xK , yK) with either xK = N or yK =
M . For considerations of efficiency, the multi-
ple DTW processes do not start from every point
on the left or top edges. Instead, they skip every
T such starting points, which still guarantees that
there will be no blind-spot in the matrices that are
inaccessible to all DTW search paths.
Finding optimal subpaths
After the multiple DTW paths are calculated, the
optimal subpath on each is then detected in or-
der to find the local alignments where the simi-
larity is maximal, which is where we expect ac-
tual matched phrases to occur. For a given path
P = p0, p2, ..., pK , the optimal subpath is defined
to be a continuous subpath, P ? = pm, pm+1..., pn
552
that maximizes
?
m?i?n sim(pi)
n?m+1 , 0 ? n ? m ? k,
and m ? n + 1 ? L. That is, the subpath is at
least as long as L and has the maximal average
similarity. L is used to avoid short alignments that
correspond to subword segments or short function
words. The value of L is determined on a devel-
opment set.
The version of SDTW employed by (Malioutov
et al, 2007) and Park and Glass (2008) employed
an algorithm of complexity O(Klog(L)) from
(Lin et al, 2002) to find subpaths. Lin et al (2002)
have also proven that the length of the optimal sub-
path is between L and 2L? 1, inclusively. There-
fore, our version uses a very simple algorithm?
just search and find the maximum of average simi-
larities among all possible subpaths with lengths
between L and 2L ? 1. Although the theoreti-
cal upper bound for this algorithm is O(KL), in
practice we have found no significant increase in
computation time compared with the O(Klog(L))
algorithm?L is actually a constant for both Park
and Glass (2008) and us, it is much smaller than
K, and the O(Klog(L)) algorithm has (constant)
overhead of calculating right-skew partitions.
In our implementation, since most of the time is
spent on calculating the average similarity scores
on candidate subpaths, all average scores are
therefore pre-calculated incrementally and saved.
We have also parallelized the computation of sim-
ilarities by topics over several computer clusters.
A detailed comparison of different parallelization
techniques has been conducted by Gajjar et al
(2008). In addition, comparing time efficiency
between the acoustics-based approach and ASR-
based summarizers is interesting but not straight-
forward since a great deal of comparable program-
ming optimization needs to be additionally consid-
ered in the present approach.
3.2 Estimating utterance-level similarity
In the previous stage, we calculated frame-level
similarities between utterance pairs and used these
to find potential matching patterns between the
utterances. With this information, we estimate
utterance-level similarities by estimating the num-
bers of true subpath alignments between two utter-
ances, which are in turn determined by combining
the following features associated with subpaths:
Similarity of subpath
We compute similarity features on each subpath.
We have obtained the average similarity score of
each subpath as discussed in Section 3.1. Based
on this, we calculate relative similarity scores,
which are computed by dividing the original sim-
ilarity of a given subpath by the average similar-
ity of its surrounding background. The motivation
for capturing the relative similarity is to punish
subpaths that cannot distinguish themselves from
their background, e.g., those found in a block of
high-similarity regions caused by certain acoustic
noise.
Distortion score
Warped subpaths are less likely to correspond to
valid matching patterns than straighter ones. In
addition to removing very distorted subpaths by
applying a distortion threshold as in (Park and
Glass, 2008), we also quantitatively measured the
remaining ones. We fit each of them with least-
square linear regression and estimate the residue
scores. As discussed above, each point on a sub-
path satisfies |xi ? yi| < T , so the residue cannot
be bigger than T . We used this to normalize the
distortion scores to the range of [0,1].
Subpath length
Given two subpaths with nearly identical average
similarity scores, we suggest that the longer of the
two is more likely to refer to content of interest
that is shared between two speech utterances, e.g.,
named entities. Longer subpaths may in this sense
therefore be more useful in identifying similarities
and redundancies within a speech summarization
system. As discussed above, since the length of a
subpath len(P ?) has been proven to fall between
L and 2L ? 1, i.e., L ? len(P ?) ? 2L ? 1,
given a parameter L, we normalize the path length
to (len(P ?) ? L)/L, corresponding to the range
[0,1).
The similarity scores of subpaths can vary widely
over different spoken documents. We do not use
the raw similarity score of a subpath, but rather
its rank. For example, given an utterance pair, the
top-1 subpath is more likely to be a true alignment
than the rest, even if its distortion score may be
higher. The similarity ranks are combined with
distortion scores and subpath lengths simply as
follows. We divide subpaths into the top 1, 3, 5,
and 10 by their raw similarity scores. For sub-
paths in each group, we check whether their dis-
tortion scores are below and lengths are above
553
some thresholds. If they are, in any group, then
the corresponding subpaths are selected as ?true?
alignments for the purposes of building utterance-
level similarity matrix. The numbers of true align-
ments are used to measure the similarity between
two utterances. We therefore have 8 threshold pa-
rameters to estimate, and subpaths with similarity
scores outside the top 10 are ignored. The rank
groups are checked one after another in a decision
list. Powell?s algorithm (Press et al, 2007) is used
to find the optimal parameters that directly mini-
mize summarization errors made by the acoustics-
based model relative to utterances selected from
manual transcripts.
3.3 Extractive summarization
Once the similarity matrix between sentences in a
topic is acquired, we can conduct extractive sum-
marization by using the matrix to estimate both
similarity and redundancy. As discussed above,
we take the general framework of MMR and
MEAD, i.e., a linear model combining salience
and redundancy. In practice, we used MMR in our
experiments, since the original MEAD considers
also sentence positions 3 , which can always been
added later as in (Penn and Zhu, 2008).
To facilitate our discussion below, we briefly re-
visit MMR here. MMR (Carbonell and Goldstein,
1998) iteratively augments the summary with ut-
terances that are most similar to the document
set under consideration, but most dissimilar to the
previously selected utterances in that summary, as
shown in the equation below. Here, the sim1 term
represents the similarity between a sentence and
the document set it belongs to. The assumption is
that a sentence having a higher sim1 would better
represent the content of the documents. The sim2
term represents the similarity between a candidate
sentence and sentences already in the summary. It
is used to control redundancy. For the transcript-
based systems, the sim1 and sim2 scores in this
paper are measured by the number of words shared
between a sentence and a sentence/document set
mentioned above, weighted by the idf scores of
these words, which is similar to the calculation of
sentence centroid values by Radev et al (2004).
3The usefulness of position varies significantly in differ-
ent genres (Penn and Zhu, 2008). Even in the news domain,
the style of broadcast news differs from written news, for
example, the first sentence often serves to attract audiences
(Christensen et al, 2004) and is hence less important as in
written news. Without consideration of position, MEAD is
more similar to MMR.
Note that the acoustics-based approach estimates
this by using the method discussed above in Sec-
tion 3.2.
Nextsent = argmax
tnr,j
(? sim1(doc, tnr,j)
?(1 ? ?)maxtr,ksim2(tnr,j, tr,k))
4 Experimental setup
We use the TDT-4 dataset for our evaluation,
which consists of annotated news broadcasts
grouped into common topics. Since our aim in this
paper is to study the achievable performance of the
audio-based model, we grouped together news sto-
ries by their news anchors for each topic. Then we
selected the largest 20 groups for our experiments.
Each of these contained between 5 and 20 articles.
We compare our acoustics-only approach
against transcripts produced automatically from
two ASR systems. The first set of transcripts
was obtained directly from the TDT-4 database.
These transcripts contain a word error rate of
12.6%, which is comparable to the best accura-
cies obtained in the literature on this data set.
We also run a custom ASR system designed to
produce transcripts at various degrees of accu-
racy in order to simulate the type of performance
one might expect given languages with sparser
training corpora. These custom acoustic mod-
els consist of context-dependent tri-phone units
trained on HUB-4 broadcast news data by se-
quential Viterbi forced alignment. During each
round of forced alignment, the maximum likeli-
hood linear regression (MLLR) transform is used
on gender-dependent models to improve the align-
ment quality. Language models are also trained on
HUB-4 data.
Our aim in this paper is to study the achievable
performance of the audio-based model. Instead
of evaluating the result against human generated
summaries, we directly compare the performance
against the summaries obtained by using manual
transcripts, which we take as an upper bound to
the audio-based system?s performance. This ob-
viously does not preclude using the audio-based
system together with other features such as utter-
ance position, length, speaker?s roles, and most
others used in the literature (Penn and Zhu, 2008).
Here, we do not want our results to be affected by
them with the hope of observing the difference ac-
curately. As such, we quantify success based on
ROUGE (Lin, 2004) scores. Our goal is to evalu-
554
ate whether the relatedness of spoken documents
can reasonably be gleaned solely from the surface
acoustic information.
5 Experimental results
We aim to empirically determine the extent to
which acoustic information alone can effectively
replace conventional speech recognition within the
multi-document speech summarization task. Since
ASR performance can vary greatly as we dis-
cussed above, we compare our system against
automatic transcripts having word error rates of
12.6%, 20.9%, 29.2%, and 35.5% on the same
speech source. We changed our language mod-
els by restricting the training data so as to obtain
the worst WER and then interpolated the corre-
sponding transcripts with the TDT-4 original au-
tomatic transcripts to obtain the rest. Figure 2
shows ROUGE scores for our acoustics-only sys-
tem, as depicted by horizontal lines, as well as
those for the extractive summaries given automatic
transcripts having different WERs, as depicted
by points. Dotted lines represent the 95% con-
fidence intervals of the transcript-based models.
Figure 2 reveals that, typically, as the WERs of au-
tomatic transcripts increase to around 33%-37%,
the difference between the transcript-based and the
acoustics-based models is no longer significant.
These observations are consistent across sum-
maries with different fixed lengths, namely 10%,
20%, and 30% of the lengths of the source docu-
ments for the top, middle, and bottom rows of Fig-
ure 2, respectively. The consistency of this trend is
shown across both ROUGE-2 and ROUGE-SU4,
which are the official measures used in the DUC
evaluation. We also varied the MMR parameter ?
within a typical range of 0.4?1, which yielded the
same observation.
Since the acoustics-based approach can be ap-
plied to any data domain and to any language
in principle, this would be of special interest
when those situations yield relatively high WER
with conventional ASR. Figure 2 also shows the
ROUGE scores achievable by selecting utterances
uniformly at random for extractive summarization,
which are significantly lower than all other pre-
sented methods and corroborate the usefulness of
acoustic information.
Although our acoustics-based method performs
similarly to automatic transcripts with 33-37%
WER, the errors observed are not the same, which
0 0.1 0.2 0.3 0.4 0.5
0.7
0.75
0.8
0.85
0.9
0.95
1
Len=10% Rand=0.197
R
O
UG
E?
SU
4
Word error rate
0 0.1 0.2 0.3 0.4 0.5
0.7
0.75
0.8
0.85
0.9
0.95
1
Len=20%, Rand=0.340
R
O
UG
E?
SU
4
Word error rate
0 0.1 0.2 0.3 0.4 0.5
0.7
0.75
0.8
0.85
0.9
0.95
1
Len=30%, Rand=0.402
R
O
UG
E?
SU
4
Word error rate
0 0.1 0.2 0.3 0.4 0.5
0.7
0.75
0.8
0.85
0.9
0.95
1
Len=10%, Rand=0.176
R
O
UG
E?
2
Word error rate
0 0.1 0.2 0.3 0.4 0.5
0.7
0.75
0.8
0.85
0.9
0.95
1
Len=20%, Rand=0.324
R
O
UG
E?
2
Word error rate
0 0.1 0.2 0.3 0.4 0.5
0.7
0.75
0.8
0.85
0.9
0.95
1
Len=30%, Rand=0.389
R
O
UG
E?
2
Word error rate
Figure 2: ROUGE scores and 95% confidence in-
tervals for the MMR-based extractive summaries
produced from our acoustics-only approach (hori-
zontal lines), and from ASR-generated transcripts
having varying WER (points). The top, middle,
and bottom rows of subfigures correspond to sum-
maries whose lengths are fixed at 10%, 20%, and
30% the sizes of the source text, respectively. ? in
MMR takes 1, 0.7, and 0.4 in these rows, respec-
tively.
we attribute to fundamental differences between
these two methods. Table 1 presents the number
of different utterances correctly selected by the
acoustics-based and ASR-based methods across
three categories, namely those sentences that are
correctly selected by both methods, those ap-
pearing only in the acoustics-based summaries,
and those appearing only in the ASR-based sum-
maries. These are shown for summaries having
different proportional lengths relative to the source
documents and at different WERs. Again, correct-
ness here means that the utterance is also selected
when using a manual transcript, since that is our
defined topline.
A manual analysis of the corpus shows that
utterances correctly included in summaries by
555
Summ. Both ASR Aco.-
length only only
WER=12.6%
10% 85 37 8
20% 185 62 12
30% 297 87 20
WER=20.9%
10% 83 36 10
20% 178 65 19
30% 293 79 24
WER=29.2%
10% 77 34 16
20% 172 58 25
30% 286 64 31
WER=35.5%
10% 75 33 18
20% 164 54 33
30% 272 67 45
Table 1: Utterances correctly selected by both
the ASR-based models and acoustics-based ap-
proach, or by either of them, under different
WERs (12.6%, 20.9%, 29.2%, and 35.5%) and
summary lengths (10%, 20%, and 30% utterances
of the original documents)
the acoustics-based method often contain out-of-
vocabulary errors in the corresponding ASR tran-
scripts. For example, given the news topic of the
bombing of the U.S. destroyer ship Cole in Yemen,
the ASR-based method always mistook the word
Cole, which was not in the vocabulary, for cold,
khol, and called. Although named entities and
domain-specific terms are often highly relevant
to the documents in which they are referenced,
these types of words are often not included in
ASR vocabularies, due to their relative global rar-
ity. Importantly, an unsupervised acoustics-based
approach such as ours does not suffer from this
fundamental discord. At the very least, these find-
ings suggest that ASR-based summarization sys-
tems augmented with our type of approach might
be more robust against out-of-vocabulary errors.
It is, however, very encouraging that an acoustics-
based approach can perform to within a typical
WER range within non-broadcast-news domains,
although those domains can likewise be more
challenging for the acoustics-based approach. Fur-
ther experimentation is necessary. It is also of sci-
entific interest to be able to quantify this WER as
an acoustics-only baseline for further research on
ASR-based spoken document summarizers.
6 Conclusions and future work
In text summarization, statistics based on word
counts have traditionally served as the foundation
of state-of-the-art models. In this paper, the simi-
larity of utterances is estimated directly from re-
curring acoustic patterns in untranscribed audio
sequences. These relatedness scores are then in-
tegrated into a maximum marginal relevance lin-
ear model to estimate the salience and redundancy
of those utterance for extractive summarization.
Our empirical results show that the summarization
performance given acoustic information alone is
statistically indistinguishable from that of modern
ASR on broadcast news in cases where the WER
of the latter approaches 33%-37%. This is an en-
couraging result in cases where summarization is
required, but ASR is not available or speech recog-
nition performance is degraded. Additional anal-
ysis suggests that the acoustics-based approach
is useful in overcoming situations where out-of-
vocabulary error may be more prevalent, and we
suggest that a hybrid approach of traditional ASR
with acoustics-based pattern matching may be the
most desirable future direction of research.
One limitation of the current analysis is that
summaries are extracted only for collections of
spoken documents from among similar speakers.
Namely, none of the topics under analysis consists
of a mix of male and female speakers. We are cur-
rently investigating supervised methods to learn
joint probabilistic models relating the acoustics of
groups of speakers in order to normalize acoustic
similarity matrices (Toda et al, 2001). We sug-
gest that if a stochastic transfer function between
male and female voices can be estimated, then the
somewhat disparate acoustics of these groups of
speakers may be more easily compared.
References
R. Barzilay, K. McKeown, and M. Elhadad. 1999. In-
formation fusion in the context of multi-document
summarization. In Proc. of the 37th Association for
Computational Linguistics, pages 550?557.
J. G. Carbonell and J. Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings
of the 21st annual international ACM SIGIR con-
ference on research and development in information
retrieval, pages 335?336.
H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.
2004. From text summarisation to style-specific
556
summarisation for broadcast news. In Proceedings
of the 26th European Conference on Information Re-
trieval (ECIR-2004), pages 223?237.
S. Furui, T. Kikuichi, Y. Shinnaka, and C. Hori. 2003.
Speech-to-speech and speech to text summarization.
In First International workshop on Language Un-
derstanding and Agents for Real World Interaction.
M. Gajjar, R. Govindarajan, and T. V. Sreenivas. 2008.
Online unsupervised pattern discovery in speech us-
ing parallelization. In Proc. Interspeech, pages
2458?2461.
L. He, E. Sanocki, A. Gupta, and J. Grudin. 1999.
Auto-summarization of audio-video presentations.
In Proceedings of the seventh ACM international
conference on Multimedia, pages 489?498.
L. He, E. Sanocki, A. Gupta, and J. Grudin. 2000.
Comparing presentation summaries: Slides vs. read-
ing vs. listening. In Proceedings of ACM CHI, pages
177?184.
Y. Lin, T. Jiang, and Chao. K. 2002. Efficient al-
gorithms for locating the length-constrained heavi-
est segments with applications to biomolecular se-
quence analysis. J. Computer and System Science,
63(3):570?586.
C. Lin. 2004. Rouge: a package for automatic
evaluation of summaries. In Proceedings of the
42st Annual Meeting of the Association for Com-
putational Linguistics (ACL), Text Summarization
Branches Out Workshop, pages 74?81.
I Malioutov, A. Park, B. Barzilay, and J. Glass. 2007.
Making sense of sound: Unsupervised topic seg-
mentation over acoustic input. In Proc. ACL, pages
504?511.
S. Maskey and J. Hirschberg. 2005. Comparing lexial,
acoustic/prosodic, discourse and structural features
for speech summarization. In Proceedings of the
9th European Conference on Speech Communica-
tion and Technology (Eurospeech), pages 621?624.
K. Mckeown and D.R. Radev. 1995. Generating sum-
maries of multiple news articles. In Proc. of SIGIR,
pages 72?82.
C. Munteanu, R. Baecker, G Penn, E. Toms, and
E. James. 2006. Effect of speech recognition ac-
curacy rates on the usefulness and usability of we-
bcast archives. In Proceedings of SIGCHI, pages
493?502.
G. Murray, S. Renals, and J. Carletta. 2005.
Extractive summarization of meeting recordings.
In Proceedings of the 9th European Conference
on Speech Communication and Technology (Eu-
rospeech), pages 593?596.
A. Park and J. Glass. 2006. Unsupervised word ac-
quisition from speech using pattern discovery. Proc.
ICASSP, pages 409?412.
A. Park and J. Glass. 2008. Unsupervised pattern dis-
covery in speech. IEEE Trans. ASLP, 16(1):186?
197.
G. Penn and X. Zhu. 2008. A critical reassessment of
evaluation baselines for speech summarization. In
Proc. of the 46th Association for Computational Lin-
guistics, pages 407?478.
W.H. Press, S.A. Teukolsky, W.T. Vetterling, and B.P.
Flannery. 2007. Numerical recipes: The art of sci-
ence computing.
D. Radev and K. McKeown. 1998. Generating natural
language summaries from multiple on-line sources.
In Computational Linguistics, pages 469?500.
D. Radev, H. Jing, M. Stys, and D. Tam. 2004.
Centroid-based summarization of multiple docu-
ments. Information Processing and Management,
40:919?938.
T. Toda, H. Saruwatari, and K. Shikano. 2001. Voice
conversion algorithm based on gaussian mixture
model with dynamic frequency warping of straight
spectrum. In Proc. ICASPP, pages 841?844.
S. Tucker and S. Whittaker. 2008. Temporal compres-
sion of speech: an evaluation. IEEE Transactions
on Audio, Speech and Language Processing, pages
790?796.
K. Zechner. 2001. Automatic Summarization of Spo-
ken Dialogues in Unrestricted Domains. Ph.D. the-
sis, Carnegie Mellon University.
J. Zhang, H. Chan, P. Fung, and L Cao. 2007. Compar-
ative study on speech summarization of broadcast
news and lecture speech. In Proc. of Interspeech,
pages 2781?2784.
X. Zhu and G. Penn. 2006. Summarization of spon-
taneous conversations. In Proceedings of the 9th
International Conference on Spoken Language Pro-
cessing, pages 1531?1534.
557
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 764?772,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Improving Automatic Speech Recognition for Lectures through
Transformation-based Rules Learned from Minimal Data
Cosmin Munteanu??
?National Research Council Canada
46 Dineen Drive
Fredericton E3B 9W4, CANADA
Cosmin.Munteanu@nrc.gc.ca
Gerald Penn?
?University of Toronto
Dept. of Computer Science
Toronto M5S 3G4, CANADA
{gpenn,xzhu}@cs.toronto.edu
Xiaodan Zhu?
Abstract
We demonstrate that transformation-based
learning can be used to correct noisy
speech recognition transcripts in the lec-
ture domain with an average word error
rate reduction of 12.9%. Our method is
distinguished from earlier related work by
its robustness to small amounts of training
data, and its resulting efficiency, in spite of
its use of true word error rate computations
as a rule scoring function.
1 Introduction
Improving access to archives of recorded lectures
is a task that, by its very nature, requires research
efforts common to both Automatic Speech Recog-
nition (ASR) and Human-Computer Interaction
(HCI). One of the main challenges to integrating
text transcripts into archives of webcast lectures is
the poor performance of ASR systems on lecture
transcription. This is in part caused by the mis-
match between the language used in a lecture and
the predictive language models employed by most
ASR systems. Most ASR systems achieve Word
Error Rates (WERs) of about 40-45% in realis-
tic and uncontrolled lecture conditions (Leeuwis
et al, 2003; Hsu and Glass, 2006).
Progress in ASR for this genre requires both
better acoustic modelling (Park et al, 2005;
Fu?gen et al, 2006) and better language modelling
(Leeuwis et al, 2003; Kato et al, 2000; Munteanu
et al, 2007). In contrast to some unsupervised ap-
proaches to language modelling that require large
amounts of manual transcription, either from the
same instructor or on the same topic (Nanjo and
Kawahara, 2003; Niesler and Willett, 2002), the
solution proposed by Glass et al (2007) uses half
of the lectures in a semester course to train an
ASR system for the other half or for when the
course is next offered, and still results in signifi-
cant WER reductions. And yet even in this sce-
nario, the business case for manually transcrib-
ing half of the lecture material in every recorded
course is difficult to make, to say the least. Manu-
ally transcribing a one-hour recorded lecture re-
quires at least 5 hours in the hands of qualified
transcribers (Hazen, 2006) and roughly 10 hours
by students enrolled in the course (Munteanu et
al., 2008). As argued by Hazen (2006), any ASR
improvements that rely on manual transcripts need
to offer a balance between the cost of producing
those transcripts and the amount of improvement
(i.e. WER reductions).
There is some work that specializes in adap-
tive language modelling with extremely limited
amounts of manual transcripts. Klakow (2000)
filters the corpus on which language models are
trained in order to retain the parts that are more
similar to the correct transcripts on a particular
topic. This technique resulted in relative WER
reductions of between 7% and 10%. Munteanu
et al (2007) use an information retrieval tech-
nique that exploits lecture presentation slides, au-
tomatically mining the World Wide Web for doc-
uments related to the topic as attested by text
on the slides, and using these to build a better-
matching language model. This yields about an
11% relative WER reduction for lecture-specific
language models. Following upon other applica-
tions of computer-supported collaborative work to
address shortcomings of other systems in artificial
intelligence (von Ahn and Dabbish, 2004), a wiki-
based technique for collaboratively editing lecture
transcripts has been shown to produce entirely cor-
764
rected transcripts, given the proper motivation for
students to participate (Munteanu et al, 2008).
Another approach is active learning, where the
goal is to select or generate a subset of the avail-
able data that would be the best candidate for ASR
adaptation or training (Riccardi and Hakkani-Tur,
2005; Huo and Li, 2007).1 Even with all of these,
however, there remains a significant gap between
this WER and the threshold of 25%, at which lec-
ture transcripts have been shown with statistical
significance to improve student performance on
a typical lecture browsing task (Munteanu et al,
2006).
People have also tried to correct ASR output in
a second pass. Ringger and Allen (1996) treated
ASR errors as noise produced by an auxiliary
noisy channel, and tried to decode back to the per-
fect transcript. This reduced WER from 41% to
35% on a corpus of train dispatch dialogues. Oth-
ers combine the transcripts or word lattices (from
which transcripts are extracted) of two comple-
mentary ASR systems, a technique first proposed
in the context of NIST?s ROVER system (Fiscus,
1997) with a 12% relative error reduction (RER),
and subsequently widely employed in many ASR
systems.
This paper tries to correct ASR output using
transformation-based learning (TBL). This, too,
has been attempted, although on a professional
dictation corpus with a 35% initial WER (Peters
and Drexel, 2004). They had access to a very large
amount of manually transcribed data ? so large,
in fact, that the computation of true WER in the
TBL rule selection loop was computationally in-
feasible, and so they used a set of faster heuristics
instead. Mangu and Padmanabhan (2001) used
TBL to improve the word lattices from which the
transcripts are decoded, but this method also has
efficiency problems (it begins with a reduction of
the lattice to a confusion network), is poorly suited
to word lattices that have already been heavily
domain-adapted because of the language model?s
low perplexity, and even with higher perplexity
models (the SWITCHBOARD corpus using a lan-
1This work generally measures progress by reduction in
the size of training data rather than relative WER reduction.
Riccardi and Hakkani-Tur (2005) achieved a 30% WER with
68% less training data than their baseline. Huo and Li (2007)
worked on a small-vocabulary name-selection task that com-
bined active learning with acoustic model adaptation. They
reduced the WER from 15% to 3% with 70 syllables of acous-
tic adaptation, relative to a baseline that reduced the WER to
3% with 300 syllables of acoustic adaptation.
guage model trained over a diverse range of broad-
cast news and telephone conversation transcripts),
was reported to produce only a 5% WER reduc-
tion.
What we show in this paper is that a true WER
calculation is so valuable that a manual transcrip-
tion of only about 10 minutes of a one-hour lecture
is necessary to learn the TBL rules, and that this
smaller amount of transcribed data in turn makes
the true WER calculation computationally feasi-
ble. With this combination, we achieve a greater
average relative error reduction (12.9%) than that
reported by Peters and Drexel (2004) on their dic-
tation corpus (9.6%), and an RER over three times
greater than that of our reimplementation of their
heuristics on our lecture data (3.6%). This is on
top of the average 11% RER from language model
adaptation on the same data. We also achieve
the RER from TBL without the obligatory round
of development-set parameter tuning required by
their heuristics, and in a manner that is robust to
perplexity. Less is more.
Section 2 briefly introduces Transformation-
Based Learning (TBL), a method used in various
Natural Language Processing tasks to correct the
output of a stochastic model, and then introduces
a TBL-based solution for improving ASR tran-
scripts for lectures. Section 3 describes our exper-
imental setup, and Section 4 analyses its results.
2 Transformation-Based Learning
Brill?s tagger introduced the concept of
Transformation-Based Learning (TBL) (Brill,
1992). The fundamental principle of TBL is
to employ a set of rules to correct the output
of a stochastic model. In contrast to traditional
rule-based approaches where rules are manually
developed, TBL rules are automatically learned
from training data. The training data consist of
sample output from the stochastic model, aligned
with the correct instances. For example, in Brill?s
tagger, the system assigns POSs to words in a text,
which are later corrected by TBL rules. These
rules are learned from manually-tagged sentences
that are aligned with the same sentences tagged
by the system. Typically, rules take the form of
context-dependent transformations, for example
?change the tag from verb to noun if one of the
two preceding words is tagged as a determiner.?
An important aspect of TBL is rule scor-
ing/ranking. While the training data may suggest
765
a certain transformation rule, there is no guarantee
that the rule will indeed improve the system?s ac-
curacy. So a scoring function is used to rank rules.
From all the rules learned during training, only
those scoring higher than a certain threshold are
retained. For a particular task, the scoring func-
tion ideally reflects an objective quality function.
Since Brill?s tagger was first introduced, TBL
has been used for other NLP applications, includ-
ing ASR transcript correction (Peters and Drexel,
2004). A graphical illustration of this task is pre-
sented in Figure 1. Here, the rules consist of
Figure 1: General TBL algorithm. Transformation
rules are learned from the alignment of manually-
transcribed text (T ) with automatically-generated
transcripts (TASR) of training data, ranked accord-
ing to a scoring function (S) and applied to the
ASR output (T ?ASR) of test data.
word-level transformations that correct n-gram se-
quences. A typical challenge for TBL is the heavy
computational requirements of the rule scoring
function (Roche and Schabes, 1995; Ngai and
Florian, 2001). This is no less true in large-
vocabulary ASR correction, where large training
corpora are often needed to learn good rules over
a much larger space (larger than POS tagging, for
example). The training and development sets are
typically up to five times larger than the evaluation
test set, and all three sets must be sampled from the
same cohesive corpus.
While the objective function for improving the
ASR transcript is WER reduction, the use of this
for scoring TBL rules can be computationally pro-
hibitive over large data-sets. Peters and Drexel
(2004) address this problem by using an heuris-
tic approximation to WER instead, and it appears
that their approximation is indeed adequate when
large amounts of training data are available. Our
approach stands at the opposite side of this trade-
off ? restrict the amount of training data to a bare
minimum so that true WER can be used in the
rule scoring function. As it happens, the mini-
mum amount of data is so small that we can au-
tomatically develop highly domain-specific lan-
guage models for single 1-hour lectures. We show
below that the rules selected by this function lead
to a significant WER reduction for individual lec-
tures even if a little less than the first ten minutes of
the lecture are manually transcribed. This combi-
nation of domain-specificity with true WER leads
to the superior performance of the present method,
at least in the lecture domain (we have not experi-
mented with a dictation corpus).
Another alternative would be to change the
scope over which TBL rules are ranked and eval-
uated, but it is well known that globally-scoped
ranking over the entire training set at once is so
useful to TBL-based approaches that this is not
a feasible option ? one must either choose an
heuristic approach, such as that of Peters and
Drexel (2004) or reduce the amount of training
data to learn sufficiently robust rules.
2.1 Algorithm and Rule Discovery
As our proposed TBL adaptation operates di-
rectly on ASR transcripts, we employ an adapta-
tion of the specific algorithm proposed by Peters
and Drexel (2004), which is schematically repre-
sented in Figure 1. This in turn was adapted from
the general-purpose algorithm introduced by Brill
(1992).
The transformation rules are contextual word-
replacement rules to be applied to ASR tran-
scripts, and are learned by performing a word-
level alignment between corresponding utterances
in the manual and ASR transcripts of training
data, and then extracting the mismatched word
sequences, anchored by matching words. The
matching words serve as contexts for the rules?
application. The rule discovery algorithm is out-
lined in Figure 2; it is applied to every mismatch-
ing word sequence between the utterance-aligned
manual and ASR transcripts.
For every mismatching sequence of words, a set
766
? for every sequence of words c0w1 . . . wnc1 in the
ASR output that is deemed to be aligned with a
corresponding sequence c0w?1 . . . w?mc1 in the
manual transcript:
? add the following contextual replacements to the
set of discovered rules:
/ c0w1 . . . wnc1 / c0w?1 . . . w?mc1 /
/ c0w1 . . . wn / c0w?1 . . . w?m /
/ w1 . . . wnc1 / w?1 . . . w?mc1 /
/ w1 . . . wn / w?1 . . . w?m /
? for each i such that 1 ? i < min(n, m), add
the following contextual replacements to the set of
discovered rules:
/ c0w1 . . . wi / c0w?1 . . . w?a(i) /
/ wi+1 . . . wnc1 / w?a(i+1) . . . w?mc1 /
/ w1 . . . wi / w?1 . . . w?a(i) /
/ wi+1 . . . wn / w?a(i+1) . . . w?m /
Figure 2: The discovery of transformation rules.
of contextual replacement rules is generated. The
set contains the mismatched pair, by themselves
and together with three contexts formed from the
left, right, and both anchor context words. In
addition, all possible splices of the mismatched
pair and the surrounding context words are also
considered.2 Rules are shown here as replace-
ment expressions in a sed-like syntax. Given the
rule r = /w1 . . . wn/w?1 . . . w?m/, every instance
of the n-gram w1 . . . wn appearing in the current
transcript is replaced with the n-gram w?1 . . . w?m.
Rules cannot apply to their own output. Rules that
would result in arbitrary insertions of single words
(e.g. / /w1/) are discarded. An example of a rule
learned from transcripts is presented in Figure 3.
2.2 Scoring Function and Rule Application
The scoring function that ranks rules is the main
component of any TBL algorithm. Assuming a
relatively small size for the available training data,
a TBL scoring function that directly correlates
with WER can be conducted globally over the en-
tire training set. In keeping with TBL tradition,
however, rule selection itself is still greedily ap-
proximated. Our scoring function is defined as:
SWER(r, TASR, T ) = WER(TASR, T )
?WER(?(r, TASR), T ),
2The splicing preserves the original order of the word-
level utterance alignment, i.e., the output of a typical dynamic
programming implementation of the edit distance algorithm
(Gusfield, 1997). For this, word insertion and deletion oper-
ations are treated as insertions of blanks in either the manual
or ASR transcript.
Utterance-align ASR output and correct transcripts:
ASR: the okay one and you come and get your seats
Correct: ok why don?t you come and get your seats
?
Insert sentence delimiters (to serve as possible
anchors for the rules):
ASR: <s> the okay one and you come and get your seats </s>
Correct: <s> ok why don?t you come and get your seats </s>
?
Extract the mismatching sequence, enclosed by
matching anchors:
ASR: <s> the okay one and you
Correct: <s> ok why don?t you
?
Output all rules for replacing the incorrect ASR
sequence with the correct text, using the entire
sequence (a) or splices (b), with or without
surrounding anchors:
(a) the okay one and / ok why don?t
(a) the okay one and you / ok why don?t you
(a) <s> the okay one and / <s> ok why don?t
(a) <s> the okay one and you / <s> ok why don?t you
(b) the okay / ok
(b) <s> the okay / <s> ok
(b) one and / why don?t
(b) one and you / why don?t you
(b) the okay one / ok why
(b) <s> the okay one / <s> ok why
(b) and / don?t
(b) and you / don?t you
Figure 3: An example of rule discovery.
where ?(r, TASR) is the result of applying rule r
on text TASR.
As outlined in Figure 1, rules that occur in the
training sample more often than an established
threshold are ranked according to the scoring func-
tion. The ranking process is iterative: in each iter-
ation, the highest-scoring rule rbest is selected. In
subsequent iterations, the training data TASR are
replaced with the result of applying the selected
rule on them (TASR ? ?(rbest, TASR)) and the re-
maining rules are scored on the transformed train-
ing text. This ensures that the scoring and ranking
of remaining rules takes into account the changes
brought by the application of the previously se-
lected rules. The iterations stop when the scoring
function reaches zero: none of the remaining rules
improves the WER on the training data.
On testing data, rules are applied to ASR tran-
767
scripts in the same order in which they were se-
lected.
3 Experimental Design
Several combinations of TBL parameters were
tested with no tuning or modifications between
tests. As the proposed method was not refined dur-
ing the experiments, and since one of the goals of
our proposed approach is to eliminate the need for
developmental data sets, the available data were
partitioned only into training and test sets, with
one additional hour set aside for code development
and debugging.
It can be assumed that a one-hour lecture given
by the same instructor will exhibit a strong cohe-
sion, both in topic and in speaking style, between
its parts. Therefore, in contrast to typical TBL
solutions, we have evaluated our TBL-based ap-
proach by partitioning each 50 minute lecture into
a training and a test set, where the training set is
smaller than the test set. As mentioned in the intro-
duction, it is feasible to obtain manual transcripts
for the first 10 to 15 minutes of a lecture. As such,
the evaluation was carried out with two values for
the training size: the first fifth (TS = 20%) and
the first third (TS = 33%) of the lecture being
manually transcribed.
Besides the training size parameter, during all
experimental tests a second parameter was also
considered: the rule pruning threshold (RT ). As
described in Section 2.2, of all the rules learned
during the rule discovery step, only those that oc-
cur more often than the threshold are scored and
ranked. This parameter can be set as low as 1 (con-
sider all rules) or 2 (consider all rules that occur
at least twice over the training set). For larger-
scale tasks, the threshold serves as a pruning al-
ternative to the computational burden of scoring
several thousand rules. A large threshold could
potentially lead to discrediting low-frequency but
high-scoring rules. Due to the intentionally small
size of our training data for lecture TBL, the low-
est threshold was set to RT = 2. When a de-
velopment set is available, several values for the
RT parameter could be tested and the optimal one
chosen for the evaluation task. Since we used no
development set, we tested two more values for the
rule pruning threshold: RT = 5 and RT = 10.
Since our TBL solution is an extension of the
solution proposed in Peters and Drexel (2004),
their heuristic is our baseline. Their scoring func-
tion is the expected error reduction:
XER = ErrLen ? (GoodCnt?BadCnt),
a WER approximation computed over all instances
of rules applicable to the training set which reflects
the difference between true positives (the number
of times a rule is correctly applied to errorful tran-
scripts ? GoodCnt) and false positives (the in-
stances of correct text being unnecessarily ?cor-
rected? by a rule ? BadCnt). These are weighted
by the length in words (ErrLen) of the text area
that matches the left-hand side of the replacement.
3.1 Acoustic Model
The experiments were conducted using the
SONIC toolkit (Pellom, 2001). We used the
acoustic model distributed with the toolkit, which
was trained on 30 hours of data from 283 speak-
ers from the WSJ0 and WSJ1 subsets of the
1992 development set of the Wall Street Jour-
nal (WSJ) Dictation Corpus. Our own lectures
consist of eleven lectures of approximately 50
minutes each, recorded in three separate courses,
each taught by a different instructor. For each
course, the recordings were performed in different
weeks of the same term. They were collected in
a large, amphitheatre-style, 200-seat lecture hall
using the AKG C420 head-mounted directional
microphone. The recordings were not intrusive,
and no alterations to the lecture environment or
proceedings were made. The 1-channel record-
ings were digitized using a TASCAM US-122 au-
dio interface as uncompressed audio files with a
16KHz sampling rate and 16-bit samples. The au-
dio recordings were segmented at pauses longer
than 200ms, manually for one instructor and au-
tomatically for the other two, using the silence
detection algorithm described in Placeway et al
(1997). Our implementation was manually fine-
tuned for every instructor in order to detect all
pauses longer than 200ms while allowing a maxi-
mum of 20 seconds in between pauses.
The evaluation data are described in Table 1.
Four evaluations tasks were carried out; for in-
structor R, two separate evaluation sessions, R-1
and R-2, were conducted, using two different lan-
guage models.
The pronunciation dictionary was custom-built
to include all words appearing in the corpus on
which the language model was trained. Pronunci-
ations were extracted from the 5K-word WSJ dic-
tionary included with the SONIC toolkit and from
768
Evaluation
task name R-1 R-2 G-1 K-1
Instructor R. G. K.
Gender Male Male Female
Age Early 60s Mid 40s Early 40s
Segmentation manual automatic automatic
# lectures 4 3 4
Lecture topic Interactive Software Unix pro-
media design design gramming
Language model WSJ-5K WEB ICSISWB WSJ-5K
Table 1: The evaluation data.
the 100K-word CMU pronunciation dictionary.
For all models, we allowed one non-dictionary
word per utterance, but only for lines longer than
four words. For allowable non-dictionary words,
SONIC?s sspell lexicon access tool was used to
generate pronunciations using letter-to-sound pre-
dictions. The language models were trained us-
ing the CMU-CAM Language Modelling Toolkit
(Clarkson and R., 1997) with a training vocabu-
lary size of 40K words.
3.2 Language Models
The four evaluations were carried out using the
language models given in Table 1, either custom-
built for a particular topic or the baseline models
included in the SONIC toolkit, as follows:
WSJ-5K is the baseline model of the SONIC
toolkit. It is a 5K-word model built using the same
corpus as the base acoustic model included in the
toolkit.
ICSISWB is a 40K-word model created
through the interpolation of language models built
on the entire transcripts of the ICSI Meeting cor-
pus and the Switchboard corpus. The ICSI Meet-
ing corpus consists of recordings of university-
based multi-speaker research meetings, totaling
about 72 hours from 75 meetings (Janin et al,
2003). The Switchboard (SWB) corpus (Godfrey
et al, 1992) is a large collection of about 2500
scripted telephone conversations between approx-
imately 500 English-native speakers, suitable for
the conversational style of lectures, as also sug-
gested in (Park et al, 2005).
WEB is a language model built for each par-
ticular lecture, using information retrieval tech-
niques that exploit the lecture slides to automat-
ically mine the World Wide Web for documents
related to the presented topic. WEB adapts IC-
SISWB using these documents to build a language
model that better matches the lecture topic. It is
also a 40K-word model built on training corpora
with an average file size of approximately 200 MB
per lecture, and an average of 35 million word to-
kens per lecture.
It is appropriate to take the difference between
ICSISWB and WSJ-5K to be one of greater genre
specificity, whereas the difference between WEB
and ICSISWB is one of greater topic-specificity.
Our experiments on these three models (Munteanu
et al, 2007) shows that the topic adaptation pro-
vides nearly all of the benefit.
4 Results
Tables 2, 3 and 43 present the evaluation results
ICSISWB Lecture 1 Lecture 2 Lecture 3
TS = % 20 33 20 33 20 33
Initial WER 50.93 50.75 54.10 53.93 48.79 49.35
XER RT = 10 46.63 49.38 49.93 48.61 49.52 50.43
RT = 5 48.34 49.75 49.32 48.81 49.58 49.26
RT = 2 54.05 56.84 52.01 49.11 50.37 51.66
XER-NoS RT = 10 49.54 49.38 54.10 53.93 48.79 48.24
RT = 5 49.54 49.31 56.70 55.50 48.51 48.42
RT = 2 59.00 59.28 57.61 55.03 50.41 52.67
SWER RT = 10 46.63 46.53 49.80 48.44 45.83 45.42
RT = 5 46.63 45.60 47.75 47.23 44.76 44.44
RT = 2 44.48 44.30 47.46 47.02 43.60 44.13
Table 4: Experimental evaluation: WER values for
instructor G using the ICSISWB language model.
for instructors R and G. The transcripts were ob-
tained through ASR runs using three different lan-
guage models. The TBL implementation with our
scoring function SWER brings relative WER re-
ductions ranging from 10.5% to 14.9%, with an
average of 12.9%.
These WER reductions are greater than those
produced by the XER baseline approach. It is not
possible to provide confidence intervals since the
proposed method does not tune parameters from
sampled data (which we regard as a very positive
quality for such a method to have). Our specu-
lative experimentation with several values for TS
and RT , however, leads us to conclude that this
method is significantly less sensitive to variations
in both the training size TS and the rule pruning
threshold RT than earlier work, making it suitable
for application to tasks with limited training data
? a result somewhat expected since rules are vali-
dated through direct WER reductions over the en-
tire training set.
3Although WSJ-5K and ICSISWB exhibited nearly the
same WER in our earlier experiments on all lecturers, we
did find upon inspection of the transcripts in question that
ICSISWB was better interpretable on speakers that had more
casual speaking styles, whereas WSJ-5K was better on speak-
ers with more rehearsed styles. We have used whichever of
these baselines was the best interpretable in our experiments
here (WSJ-5K for R and K, ICSISWB for G).
769
WSJ-5K Lecture 1 Lecture 2 Lecture 3 Lecture 4
TS = % 20 33 20 33 20 33 20 33
Initial WER 50.48 50.93 51.31 51.90 50.28 49.23 54.39 54.04
XER RT = 10 49.97 49.82 49.27 49.77 46.85 48.08 52.17 50.58
RT = 5 50.01 50.07 49.99 51.13 48.39 47.37 50.91 49.62
RT = 2 49.87 51.75 49.52 51.13 47.13 47.31 52.70 50.56
XER-NoS RT = 10 47.25 46.82 49.98 48.72 48.44 45.21 51.37 49.73
RT = 5 49.03 48.78 47.37 51.25 47.84 44.07 49.54 48.97
RT = 2 52.21 53.47 49.31 52.29 50.85 49.41 50.63 51.81
SWER RT = 10 45.18 44.58 49.06 45.97 46.49 45.30 49.60 47.95
RT = 5 44.82 43.82 46.73 45.52 45.64 43.18 47.79 46.74
RT = 2 44.04 43.99 45.81 45.16 44.35 41.49 46.89 44.28
Table 2: Experimental evaluation: WER values for instructor R using the WSJ-5K language model.
WEB Lecture 1 Lecture 2 Lecture 3 Lecture 4
TS = % 20 33 20 33 20 33 20 33
Initial WER 45.54 45.85 43.36 43.87 46.69 47.14 49.78 49.38
XER RT = 10 42.91 43.90 42.44 43.81 46.78 45.35 46.92 49.65
RT = 5 43.45 43.81 42.65 44.37 46.90 42.12 47.34 46.04
RT = 2 43.26 45.46 44.19 44.66 43.77 45.12 61.54 60.40
XER-NoS RT = 10 43.51 42.97 42.11 41.98 44.66 46.59 47.24 46.30
RT = 5 44.96 42.98 40.01 40.52 44.66 41.74 47.23 44.35
RT = 2 46.72 48.16 44.79 45.87 40.44 44.32 61.84 64.40
SWER RT = 10 41.98 41.44 42.11 40.75 44.66 45.27 47.24 45.85
RT = 5 40.97 40.56 38.85 39.08 44.66 40.84 45.27 42.39
RT = 2 40.67 40.47 38.00 38.07 40.00 40.08 43.31 41.52
Table 3: Experimental evaluation: WER values for instructor R using the WEB language models.
As for how the transcripts improve, words with
lower information content (e.g., a lower tf.idf
score) are corrected more often and with more
improvement than words with higher information
content. The topic-specific language model adap-
tation that the TBL follows upon benefits words
with higher information content more. It is possi-
ble that the favour observed in TBL with SWER
towards lower information content is a bias pro-
duced by the preceding round of language model
adaptation, but regardless, it provides a much-
needed complementary effect. This can be ob-
served in Tables 2 and 3, in which TBL produces
nearly the same RER in either table for any lecture.
We have also extensively experimented with the
usability of lecture transcripts on human subjects
(Munteanu et al, 2006), and have found that task-
based usability varies in linear relation to WER.
An analysis of the rules selected by both TBL
implementations revealed that using the XER ap-
proximation leads to several single-word rules be-
ing selected, such as rules removing all instances
of frequent stop-words such as ?the? and ?for? or
pronouns such as ?he.? Therefore, an empirical
improvement (XER ?NoS) of the baseline was
implemented that, beside pruning rules below the
RT threshold, omits such single-word rules from
being selected. As shown in Tables 2, 3 and 4,
this restriction slightly improves the performance
of the approximation-based TBL for some values
of the RT and TS parameters, although it still
does not consistently match the WER reductions
of our scoring function.
Although the experimental evaluation shows
positive improvements in transcript quality
through TBL, in particular when using the SWER
scoring function, an exception is illustrated in
Table 5. The recordings for this evaluation were
collected from a course on Unix programming,
and lectures were highly interactive. Instructor
K used numerous examples of C or Shell code,
many of them being developed and tested in
class. While the keywords from a programming
language can be easily added to the ASR lexicon,
the pronunciation of such abbreviated forms (es-
pecially for Shell programming) and of mostly all
variable and custom function names proved to be
a significant difficulty for the ASR system. This,
combined with a high speaking rate and often
inconsistently truncated words, led to few TBL
rules occurring even above the lowest RT = 2
threshold (despite many TBL rules being initially
discovered).
As previously mentioned, one of the drawbacks
of global TBL rule scoring is the heavy compu-
tational burden. The experiments conducted here,
however, showed an average learning time of one
hour per one-hour lecture, reaching at most three
770
WSJ-5K Lecture 1 Lecture 2 Lecture 3 Lecture 4
TS = % 20 33 20 33 20 33 20 33
Initial WER 44.31 44.06 46.12 45.80 51.10 51.19 53.92 54.89
XER RT = 10 44.31 44.06 46.12 46.55 51.10 51.19 53.92 54.89
RT = 5 44.31 44.87 46.82 47.47 51.10 51.19 53.96 55.56
RT = 2 47.46 55.21 50.54 51.01 52.60 54.93 57.48 60.46
XER-NoS RT = 10 44.31 44.06 46.12 46.55 51.10 51.19 53.92 54.89
RT = 5 44.31 44.87 46.82 47.47 51.10 51.19 53.96 55.56
RT = 2 46.43 54.41 50.54 51.01 53.01 55.02 57.47 60.02
SWER RT = 10 44.31 44.06 46.12 45.80 51.10 51.19 53.92 54.89
RT = 5 44.31 44.05 46.11 45.88 51.10 51.19 53.92 54.89
RT = 2 44.34 44.07 46.03 45.89 50.96 50.93 54.01 55.16
Table 5: Experimental evaluation: WER values for instructor K using the WSJ-5K language model.
hours4 for a threshold of 2 when training over tran-
scripts for one third of a lecture. Therefore, it can
be concluded that, despite being computationally
more intensive than a heuristic approximation (for
which the learning time is on the order of just a
few minutes), a TBL system using a global, WER-
correlated scoring function not only produces bet-
ter transcripts, but also produces them in a feasible
amount of time with only a small amount of man-
ual transcription for each lecture.
5 Summary and Discussion
One of the challenges to reducing the WER of
ASR transcriptions of lecture recordings is the
lack of manual transcripts on which to train var-
ious ASR improvements. In particular, for one-
hour lectures given by different lecturers (such as,
for example, invited presentations), it is often im-
practical to manually transcribe parts of the lecture
that would be useful as training or development
data. However, transcripts for the first 10-15 min-
utes of a particular lecture can be easily obtained.
In this paper, we presented a solution that im-
proves the quality of ASR transcripts for lectures.
WER is reduced by 10% to 14%, with an average
reduction of 12.9%, relative to initial values. This
is achieved by making use of manual transcripts
from as little as the first 10 minutes of a one-hour
lecture. The proposed solution learns word-level
transformation-based rules that attempt to replace
parts of the ASR transcript with possible correc-
tions. The experimental evaluation carried out
over eleven lectures from three different courses
and instructors shows that this amount of manual
transcription can be sufficient to further improve a
lecture-specific ASR system.
4It should be noted that, in order to preserve compatibil-
ity with other software tools, the code developed for these
experiments was not optimized for speed. It is expected that
a dedicated implementation would result in even lower run-
times.
In particular, we demonstrated that a true WER-
based scoring function for the TBL algorithm is
both feasible and effective with a limited amount
of training data and no development data. The pro-
posed function assigns scores to TBL rules that di-
rectly correlate with reductions in the WER of the
entire training set, leading to a better performance
than that of a heuristic approximation. Further-
more, a scoring function that directly optimizes
for WER reductions is more robust to variations
in training size as well as to the value of the rule
pruning threshold. As little as a value of 2 can be
used for the threshold (scoring all rules that occur
at least twice), with limited impact on the com-
putational burden of learning the transformation
rules.
References
E. Brill. 1992. A simple rule-based part of speech
tagger. In Proc. 3rd Conf. on Applied NLP (ANLP),
pages 152 ? 155.
P.R. Clarkson and Rosenfeld R. 1997. Statistical lan-
guage modeling using the CMU-Cambridge Toolkit.
In Proc. Eurospeech, volume 1, pages 2707?2710.
J.G. Fiscus. 1997. A post-processing system to yield
reduced word error rates: Recognizer output voting
error reduction (ROVER). In Proc. IEEE Workshop
on Automatic Speech Recognition and Understand-
ing (ASRU), pages 347?354.
C. Fu?gen, M. Kolss, D. Bernreuther, M. Paulik,
S. Stu?ker, S. Vogel, and A. Waibel. 2006. Open
domain speech recognition & translation: Lectures
and speeches. In Proc. IEEE Conf. on Acoustics,
Speech, and Signal Processing (ICASSP), volume 1,
pages 569?572.
J. Glass, T.J. Hazen, S. Cyphers, I. Malioutov,
D. Huynh, and R. Barzilay. 2007. Recent progress
in the MIT spoken lecture processing project. In
Proc. 10th EuroSpeech / 8th InterSpeech, pages
2553?2556.
771
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In Proc. IEEE Conf.
Acoustics, Speech, and Signal Processing (ICASSP),
pages 517?520.
D. Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences. Cambridge University Press.
T.J. Hazen. 2006. Automatic alignment and error
correction of human generated transcripts for long
speech recordings. In Proc. 9th Intl. Conf. on Spo-
ken Language Processing (ICSLP) / InterSpeech,
pages 1606?1609.
B-J. Hsu and J. Glass. 2006. Style & topic lan-
guage model adaptation using HMM-LDA. In Proc.
ACL Conf. on Empirical Methods in NLP (EMNLP),
pages 373?381.
Q. Huo and W. Li. 2007. An active approach
to speaker and task adaptation based on automatic
analysis of vocabulary confusability. In Proc. 10th
EuroSpeech / 8th InterSpeech, pages 1569?1572.
A. Janin, Baron D., J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The ICSI meeting cor-
pus. In Proc. IEEE Conf. on Acoustics, Speech, and
Signal Processing (ICASSP), pages 364?367.
K. Kato, H. Nanjo, and T. Kawahara. 2000. Au-
tomatic transcription of lecture speech using topic-
independent language modeling. In Proc. Intl. Conf.
on Spoken Language Processing (ICSLP), volume 1,
pages 162?165.
D. Klakow. 2000. Selecting articles from the language
model training corpus. In Proc. IEEE Conf. on
Acoustics, Speech, and Signal Processing (ICASSP),
pages 1695?1698.
E. Leeuwis, M. Federico, and M. Cettolo. 2003. Lan-
guage modeling and transcription of the TED corpus
lectures. In Proc. Intl. Conf. on Acoustics, Speech,
and Signal Processing (ICASSP), volume 1, pages
232?235.
L. Mangu and M. Padmanabhan. 2001. Error correc-
tive mechanisms for speech recognition. In Proc.
IEEE Conf. on Acoustics, Speech, and Signal Pro-
cessing (ICASSP), pages 29?32.
C. Munteanu, R. Baecker, and G. Penn. 2008. Collab-
orative editing for improved usefulness and usabil-
ity of transcript-enhanced webcasts. In Proc. ACM
SIGCHI Conf. (CHI), pages 373?382.
C. Munteanu, R. Baecker, G. Penn, E. Toms, and
D. James. 2006. The effect of speech recognition
accuracy rates on the usefulness and usability of we-
bcast archives. In Proc. ACM SIGCHI Conf. (CHI),
pages 493?502.
C. Munteanu, G. Penn, and R. Baecker. 2007. Web-
based language modelling for automatic lecture tran-
scription. In Proc. 10th EuroSpeech / 8th Inter-
Speech, pages 2353?2356.
H. Nanjo and T. Kawahara. 2003. Unsupervised lan-
guage model adaptation for lecture speech recogni-
tion. In Proc. ISCA / IEEE Workshop on Sponta-
neous Speech Processing and Recognition (SSPR).
G. Ngai and R. Florian. 2001. Transformation-based
learning in the fast lane. In Proc. 2nd NAACL, pages
1?8.
T. Niesler and D. Willett. 2002. Unsupervised lan-
guage model adaptation for lecture speech transcrip-
tion. In Proc. Intl. Conf. on Spoken Language Pro-
cessing (ICSLP/Interspeech), pages 1413?1416.
A. Park, T. J. Hazen, and J. R. Glass. 2005. Auto-
matic processing of audio lectures for information
retrieval: Vocabulary selection and language model-
ing. In Proc. IEEE Conf. on Acoustics, Speech, and
Signal Processing (ICASSP).
B. L. Pellom. 2001. SONIC: The university of col-
orado continuous speech recognizer. Technical Re-
port #TR-CSLR-2001-01, University of Colorado.
J. Peters and C. Drexel. 2004. Transformation-based
error correction for speech-to-text systems. In Proc.
Intl. Conf. on Spoken Language Processing (IC-
SLP/Interspeech), pages 1449?1452.
P. Placeway, S. Chen, M. Eskenazi, U. Jain, V. Parikh,
B. Raj, M. Ravishankar, R. Rosenfeld, K. Seymore,
and M. Siegler. 1997. The 1996 HUB-4 Sphinx-3
system. In Proc. DARPA Speech Recognition Work-
shop.
G. Riccardi and D. Hakkani-Tur. 2005. Active learn-
ing: Theory and applications to automatic speech
recognition. IEEE Trans. Speech and Audio Pro-
cessing, 13(4):504?511.
E. K. Ringger and J. F. Allen. 1996. Error correction
via a post-processor for continuous speech recogni-
tion. In Proc. IEEE Conf. on Acoustics, Speech, and
Signal Processing (ICASSP), pages 427?430.
E. Roche and Y. Schabes. 1995. Deterministic part-of-
speech tagging with finite-state transducers. Com-
putational Linguistics, 21(2):227?253.
L. von Ahn and L. Dabbish. 2004. Labeling images
with a computer game. In Proc. ACM SIGCHI Conf.
(CHI), pages 319?326.
772
Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 19?27,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Prior Derivation Models For Formally Syntax-based Translation Using
Linguistically Syntactic Parsing and Tree Kernels
Bowen Zhou
IBM T. J. Watson Research Center
Yorktown Heights, NY
zhou@us.ibm.com
Bing Xiang
IBM T. J. Watson Research Center
Yorktown Heights, NY
bxiang@us.ibm.com
Xiaodan Zhu
Dept. of Computer Science
University of Toronto
xzhu@cs.toronto.edu
Yuqing Gao
IBM T. J. Watson Research Center
Yorktown Heights, NY
yuqing@us.ibm.com
Abstract
This paper presents an improved formally
syntax-based SMT model, which is enriched
by linguistically syntactic knowledge obtained
from statistical constituent parsers. We pro-
pose a linguistically-motivated prior deriva-
tion model to score hypothesis derivations on
top of the baseline model during the trans-
lation decoding. Moreover, we devise a
fast training algorithm to achieve such im-
proved models based on tree kernel meth-
ods. Experiments on an English-to-Chinese
task demonstrate that our proposed models
outperformed the baseline formally syntax-
based models, while both of them achieved
significant improvements over a state-of-the-
art phrase-based SMT system.
1 Introduction
In recent years, syntax-based translation models
(Chiang, 2007; Galley et al, 2004; Liu et al, 2006)
have shown promising progress in improving trans-
lation quality. There are two major elements ac-
counting for such an improvement: namely the in-
corporation of phrasal translation structures adopted
from widely applied phrase-based models (Och
and Ney, 2004) to handle local fluency, and the
engagement of synchronous context-free grammars
(SCFG), which enhances the generative capacity of
the underlying model that is limited by finite-state
machinery.
Approaches to syntax-based translation models
using SCFG can be further categorized into two
classes, based on their dependency on annotated cor-
pus. Following Chiang (Chiang, 2007), we note the
following distinction between these two classes:
? Linguistically syntax-based: models that utilize
structures defined over linguistic theory and
annotations (e.g., Penn Treebank), and SCFG
rules are derived from parallel corpus that is
guided by explicitly parsing on at least one side
of the parallel corpus. Examples among others
are (Yamada and Knight, 2001) and (Galley et
al., 2004).
? Formally syntax-based: models are based on
hierarchical structures of natural language but
synchronous grammars are automatically ex-
tracted from parallel corpus without any usage
of linguistic knowledge or annotations. Exam-
ples include Wu?s (Wu, 1997) ITG and Chi-
ang?s hierarchical models (Chiang, 2007).
While these two often resemble in appearance,
from practical viewpoints, there are some distinc-
tions in training and decoding procedures differen-
tiating formally syntax-based models from linguis-
tically syntax-based models. First, the former has
no dependency on available linguistic theory and
annotations for targeting language pairs, and thus
the training and rule extraction are more efficient.
Secondly, the decoding complexity of the former is
lower 1, especially when integrating a n-gram based
1The complexity is dominated by synchronous parsing and
boundary words keeping. Thus binary SCFG employed in for-
mally syntax-based systems help to maintain efficient CKY de-
coding. Recent work by (Zhang et al, 2006) shows a practi-
cally efficient approach that binarizes linguistically SCFG rules
when possible.
19
language model, which is a key element to ensure
translation output quality.
On the other hand, available linguistic theory
and annotations could provide invaluable benefits in
grammar induction and scoring, as shown by recent
progress on such models (Galley et al, 2006). In
contrast, formally syntax-based grammars often lack
explicit linguistic constraints.
In this paper, we propose a scheme to enrich for-
mally syntax-based models with linguistically syn-
tactic knowledge. In other words, we maintain our
grammar to be based on formal syntax on surface,
but incorporate linguistic knowledge into our mod-
els to leverage syntax theory and annotations.
Our goal is two-fold. First, how to score SCFG
rules whose general abstraction forms are unseen in
the training data is an important question to answer.
In hierarchical models, Chiang (Chiang, 2007) uti-
lizes heuristics where certain assumptions are made
on rule distributions to obtain relative frequency
counts. We intend to explore if additional linguisti-
cally parsing information would be beneficial to im-
prove the scoring of formally syntactic SCFG gram-
mars. Secondly, we note that SCFG-based models
often come with an excessive memory consumption
as its rule size is an order of magnitude larger com-
pared to phrase-based models, which challenges its
practical deployment for online real-time translation
tasks. Furthermore, formal syntax rules are often re-
dundant as they are automatically extracted without
linguistic supervision. Therefore, we are motivated
to study approaches to further score and rank formal
syntax rules based on syntax-inspired methods, and
eventually to prune unnecessary rules without loss
of performance in general.
In our study, we propose a linguistically-
motivated method to train prior derivation models
for formally syntax-based translation. In this frame-
work, prior derivation models can be viewed as a
smoothing of rule translation models, addressing
the weakness of the baseline model estimation that
relies on relative counts obtained from heuristics.
First, we apply automatic parsers to obtain syntax
annotations on the English side of the parallel cor-
pus. Next, we extract tree fragments associated with
phrase pairs, and measure similarity between such
tree fragments using kernel methods (Collins and
Duffy, 2002; Moschitti, 2006). Finally, we score
and rank rules based on their minimal cluster sim-
ilarity of their nonterminals, which is used to com-
pute the prior distribution of hypothesis derivations
during decoding for improved translation.
The remainder of the paper is organized as fol-
lows. We start with a brief review of some related
work in Sec. 2. In Sec. 3, we describe our formally
syntax-based models and decoder implementation,
that is established as our baseline system. Sec. 4
presents the approach to score formal SCFG rules
using kernel methods. Experimental results are pro-
vided in Sec. 5. Finally, Sec. 6 summarized our con-
tributions with discussions and future work.
2 Related Work
Syntax-based translation models engaged with
SCFG have been actively investigated in the liter-
ature (Wu, 1997; Yamada and Knight, 2001; Gildea,
2003; Galley et al, 2004; Satta and Peserico, 2005).
Recent work by (Chiang, 2007; Galley et al,
2006) shows promising improvements compared to
phrase-based models for large-scale tasks. How-
ever, few previous work directly applied linguisti-
cally syntactic information into a formally syntax-
based models, which is explored in this paper.
Kernel methods leverage the fact that the only op-
eration in a procedure is the evaluation of inner dot
products between pairs of observations, where the
inner product is thus replaced with a Mercer kernel
that provides an efficient way to carry out computa-
tion when original feature dimension is large or even
infinite. Collins and Duffy (Collins and Duffy, 2002)
suggested to employ convolution kernels to measure
similarity between two trees in terms of their sub-
structures, and more recently, Moschitti (Moschitti,
2006) described in details a fast implementation of
tree kernels. To our knowledge, this paper is one of
the few efforts of applying kernel methods for im-
proved translation.
3 Formally Syntax-based Models
An SCFG is a synchronous rewriting system gen-
erating source and target side string pairs simulta-
neously based on context-free grammar. Each syn-
chronous production (i.e., rule) rewrites a nonter-
minal into a pair of strings, ? and ?, with both
terminals and nonterminals in both languages, sub-
20
ject to the constraint that there is a one-to-one cor-
respondence between nonterminal occurrences on
the source and target side. In particular, formally
syntax-based models explore hierarchical structures
of natural language and utilize only a unified nonter-
minal symbol X in the grammar,
X ? ??, ?,??, (1)
where ? is the one-to-one correspondence between
X?s in ? and ?, which is indicated by under-
scripted co-indices on both sides. For example,
some English-to-Chinese production rules can be
represented as follows:
X ? ?X1enjoy readingX2, (2)
X1xihuan(enjoy) yuedu(reading)X2?
X ? ?X1enjoy readingX2,
X1xihuan(enjoy)X2yuedu(reading)?
The set of rules, denoted as R, are automatically ex-
tracted from sentence-aligned parallel corpus (Chi-
ang, 2007). First, bidirectional word-level align-
ment is carried out on the parallel corpus running
GIZA++ (Och and Ney, 2000). Based on the result-
ing Viterbi alignments Ae2f and Af2e, the union,
AU = Ae2f ? Af2e, is taken as the symmetrizedword-level alignment. Next, bilingual phrase pairs
consistent with word alignments are extracted from
AU (Och and Ney, 2004). Specifically, any pairof consecutive sequences of words below a maxi-
mum length M is considered to be a phrase pair
if its component words are aligned only within the
phrase pair and not to any words outside. The re-
sulting bilingual phrase pair inventory is denoted as
BP . Each phrase pair PP ? BP is represented as
a production rule X ? ?f ji , elk?, which we refer toas phrasal rules. The SCFG rule set encloses all
phrase pairs, i.e., BP ? R. Next, we loop through
each phrase pair PP and generalize the sub-phrase
pair contained in PP, denoted as SPe and SPf sub-ject to SP = (SPf , SPe) ? BP , with co-indexednonterminal symbols. We thereby obtain a new rule.
We limit the number of nonterminals in each rule
no more than two, thus ensuring the rank of SCFG is
two. To reduce rule size and spurious ambiguity, we
apply constraints described in (Chiang, 2007). In
addition, we require that the sub-phrases being ab-
stracted by correspondent nonterminals have to be
aligned together in the original phrase pair, which
significantly reduces the number of rules. We will
hereafter refer to rules with nonterminal symbols as
abstract rules to distinguish them from phrasal rules.
Finally, an implicit glue rule is embedded with de-
coder to allow for translations that can be achieved
by sequentially linking sub-translations generated
chunk-by-chunk:
X ? ?X1X2, X1X2?. (3)
That is, X is also our sentence start symbol.
During such a rule extraction procedure, we note
that there is a many-to-many mapping between
phrase pairs (contiguous word sequences without
nonterminals) and derived rules (a mixed combina-
tion of word and nonterminal sequences). In other
words, one original phrase pair can induce a num-
ber of different rules, and the same rule can also be
derived from a number of different phrase pairs.
3.1 Models
All rules in R are paired with statistical parame-
ters (i.e., weighted SCFG), which combines with
other features to form our models using a log-linear
framework. Translation using SCFG for an input
sentence f is casted as to find the optimal derivation
on source and target side (as the grammar is syn-
chronous, the derivations on source and target sides
are identical). By ?optimal?, it indicates that the
derivation D maximizes following log-linear mod-
els over all possible derivations:
P (D) ? PLM (e)?LM?
?
i
?
X?<?,?>?D ?i(X ?< ?, ? >)?i , (4)
where the set of ?i(X ?< ?, ? >) are featuresdefined over given production rule, and PLM (e) isthe language model score on hypothesized output,
the ?i is the feature weight.Our baseline model follows Chiang?s hierarchical
model (Chiang, 2007) in conjunction with additional
features:
? conditional probabilities in both directions:
P (?|?) and P (?|?);
? lexical weights (Koehn et al, 2003) in both di-
rections: Pw(?|?) and Pw(?|?);
21
? word counts |e|;
? rule counts |D|;
? target n-gram language model PLM (e);
? glue rule penalty to learn preference of non-
terminal rewriting over serial combination
through Eq. 3;
Moreover, we propose an additional feature, namely
the abstraction penalty, to account for the accumu-
lated number of nonterminals applied in D:
? abstraction penalty exp(?Na), where Na =
?
X?<?,?>?D n(?)
where n(?) is the number of nonterminals in ?. This
feature aims to learn the preference among phrasal
rules, and abstract rules with one or two nontermi-
nals. This makes our syntax-based model includes a
total of nine features.
The training procedure described in (Chiang,
2007) employs heuristics to hypothesize a distri-
bution of possible rules. A count one is assigned
to every phrase pair occurrence, which is equally
distributed among rules that are derived from this
phrase pair. Hypothesizing this distribution as our
observations on rule occurrence, relative-frequency
estimation is used to obtain P (?|?) and P (?|?).
We note that, however, these parameters are of-
ten poorly estimated due to the usage of inaccurate
heuristics, which is the major problem that we alle-
viate in Sec. 4.
3.2 Decoder
The objective of our syntax-based decoder is to
search for the optimal derivation tree D from a for-
est of trees that can represent the input sentence. The
target side is mapped accordingly at each nontermi-
nal node in the tree, and a traverse of these nodes
obtains the target translation. Fig. 1 shows an ex-
ample for chart parsing that produces the translation
from the best parse.
Our decoder implements a modified CKY parser
in C++ with integrated n-gram language model scor-
ing. During search, chart cells are filled in a bottom-
up fashion until a tree rooted from nonterminal is
generated that covers the entire input sentence. The
dynamic programming item we bookkeep is denoted
Figure 1: A chart parsing-based decoding on SCFG pro-
duces translation from the best parse: f1f2f3f4f5 ?
e5e6e3e4e1e2.
as [X, i, j; eb], indicating a sub-tree rooted with Xthat has covered input from position i to j generat-
ing target translation with boundary words eb. Tospeed up the decoding, a pruning scheme similar to
the cube pruning (Chiang, 2007) is performed dur-
ing search.
4 Prior Derivation Models
As mentioned above, decoding searches for the op-
timal tree on source side to cover the input sentence
with respect to given models, as shown in Eq. 4.
Among these feature functions, ?i measures howlikely the source and hypothesized target sub-trees
rooted from same X are paired together through
symmetric conditional probabilities (e.g., P (?|?)
and P (?|?)), and the target language model mea-
sures the fluency on target string. It should be noted
that, however, the baseline models do not discrim-
inate between different parses on source side when
the target side is unknown.
Therefore, if we could obtain some prior distribu-
tions for the source side derivations, we can rewrite
Eq. 4 as:
P (D) ? PLM (e)?LM ?
?
X?<?,?>?D
(?i ?i(X ?< ?, ? >)?i)L(X ?< ?, ? >)?L ,(5)
where L(?) is a feature function defined over a pro-
duction but only depending on one side of the rules
22
and asterisk denotes arbitrary symbol sequences on
the other side consistent with our grammars 2. The
production of L(?) over all rules observed in a
derivation D measures the prior distribution of D.
In the baseline model, as a special case, we can see
that L(?) is a constant function.
The motivation is straightforward since some
derivations should be preferred over others. One
may make an analogy between our prior derivation
distributions to non-uniform source side segmenta-
tion models in phrase-based systems. However, it
should be noted that prior derivation models influ-
ence not only on phrase choices as what segmenta-
tion models do, but also on ordering options due to
the nonterminal usage in syntax-based models.
In principle, some quantitative schemes are
needed to evaluate the monolingual derivation prior
probability. Our scheme links the source side deriva-
tion prior probability with the expected ambiguity
on target side generation when mapping the source
side to target side given the derivation. That is, a
given source side derivation is favored if it intro-
duces less ambiguity on target generation compared
to others.
Let us revisit the rules in Eq. 2. We notice that
the same source side maps into different target or-
ders depending on the syntactic role (e.g., NP or PP)
of X2 in the rule. Furthermore, the following areexample rules trained from real data (see Sec. 5):
X ? ?X1passX2, X1gei (give)X2? (6)
X ? ?X1passX2, X1jingguo (traverse)X2? (7)
X ? ?X1passX2, X1piao (ticket)X2? (8)
Above three rules cover pretty well for different us-
ages of pass in English and its correspondence in
Chinese. Typically, applying the rule to inputs such
as ?my pass expired? obtains reasonable translations
with baseline models. However, it will fail on inputs
like ?my pass to the zoo? as none of th rules provides
a correct translation of X1X2piao (ticket) when X2is a prepositional phrase.
Such linguistic phenomena, among others, indi-
cates that the higher variation of syntax structures
2In general, we can plug in either L(X ?< ?, ? >) or
L(X ?< ?, ? >) here. For illustration purposes, we assume
that the model is on the source side.
the nonterminal embodies, the more translation op-
tions on target side needed to account for various
syntactic roles on source side. This suggests that
our prior derivation models should prefer nonter-
minals that cover more syntactically homogeneous
constituents. Such a model is thus proposed in
Sec. 4.1.
The prior derivation model can also be viewed
as a smoothing on rule translation probabilities esti-
mated using heuristics, as we mentioned in Sec. 3.1.
When there are more translation options, we deem
that there are more ambiguity for this rule. In cases
where some dominating translation option is overes-
timated from hypothesized distributions, all transla-
tion options of this rule are discounted as they are
less favored by prior derivation models.
4.1 Model Syntactic Variations
Each abstract rule is generalized from a set of origi-
nal relevant phrase pairs by grouping an appropriate
set of sub-phrases into a nonterminal symbol, with
each sub-phrase linked to a tree list. Therefore, the
joined tree lists form a forest for this nonterminal
symbol in the rule. For every abstract rule, we de-
fine the rule forest to be the set of tree fragments of
all sub-phrases abstracted within this rule.
We parse the English side of parallel corpus to ob-
tain a syntactic tree for each English sentence. For
each phrase extracted from this sentence, we de-
fine the tree fragment for this phrase as the mini-
mal set of internal tree whose leaves span exactly
over this phrase. As a common practice, we pre-
serve all phrase pairs in BP including those who
are not consistent with parser sub-trees. Therefore,
there will be many phrases that cross over syntactic
sub-trees, which subsequently produced tree frag-
ments lacking a root. We label those as ?incom-
plete? tree fragments, and introduce a parent node of
?INC? on top of them to form a single-rooted sub-
tree. For example, Fig. 2 shows the tree fragments
for phrases of ?reading books? and ?enjoy reading?,
where the latter is an ?incomplete? tree fragment.
Moreover, for sentences failed on parsing, we la-
bel all phrases extracted from those sentences with a
root of ?EMPTY?.
Subset trees of tree fragments are defined as any
sub-graph that contains more than one nodes, with
the restriction that entire rule productions must be
23
(a) S


HH
H
NP
PRP
I
VP


HH
H
VBP
enjoy
NP
 HHNN
reading
NNS
books
(b) NP
 HHNN
reading
NNS
books
(c) INC
 HHVBP
enjoy
NN
reading
Figure 2: Syntax parsing tree (a) and tree fragments for
phrases ?reading books? (b) and ?enjoy reading? (c).
NP
 HHNN
reading
NNS
books
NP
 HHNN
reading
NNS
NP
 HHNN NNS
books
NP
 HHNN NNS
NN
reading
NNS
books
Figure 3: Subset trees of the NP covering ?reading
books?.
included (Collins and Duffy, 2002). Fig. 3 enumer-
ates a list of subset trees for fragment (b) in Fig. 2.
To measure syntactic homogeneity, we define the
fragment similarity K(T1, T2) as the number ofcommon subset trees between two tree fragments T1and T2. Conceptually, if we enumerate all possiblesubset trees 1, . . . ,M , we can represent each tree
fragment T as a vector h(T ) = (c1, . . . , cM ) witheach element as the count of occurrences of each
subset tree in T . Thus, the similarity can be ex-
pressed by the inner products of these two vectors.
Note that M will be a huge number for our problem,
and thus we need kernel methods presented below to
make computation tractable.
4.2 Kernel Methods
Collins and Duffy (Collins and Duffy, 2002) intro-
duced a method employing convolution kernels to
measure similarity between two trees in terms of
their sub-structures. If we define an indicator func-
tion Ii(n) to be 1 if subset tree i is rooted at node nand 0 otherwise, we have:
K(T1, T2) =
?
n1?N1
?
n2?N2
C(n1, n2) (9)
where C(n1, n2) = ?i Ii(n1)Ii(n2) and N1, N2are the set of nodes in the tree fragment T1 and T2respectively. It is noted that C(n1, n2) can be com-puted recursively (Collins and Duffy, 2002):
1. C(n1, n2) = 0 if the productions at n1 and n2are different;
2. C(n1, n2) = 1 if the productions at n1 and n2are the same and both are pre-terminals;
3. Otherwise,
C(n1, n2) = ?
nc(n1)
?
j=1
(1 + C(chjn1 , ch
j
n2)) (10)
where chjn1 is the jth child of node n1, nc(n1) isthe number of children at n1 and 0 < ? ? 1 is adecay factor to discount the effects of deeper tree
structures.
In principle, the computational complexity of
Eq. 10 is O(|N1| ? |N2|). However, as noted by(Collins and Duffy, 2002), the worst case is quite un-
common to natural language syntactic trees. More
recently, Moschitti (Moschitti, 2006) introduced in
details a fast implementation of tree kernels, where a
node pair set is first constructed for those associated
with same production rules. Our work follows Mos-
chitti?s implementation, which runs in linear time on
average. We compute the normalized similarity as
K ?(T1, T2) = K(T1,T2)?K(T1,T1)?
?
K(T2,T2)
to ensure simi-
larity is normalized between 0 and 1.
4.3 Prior Derivation Cost
First we define the purity of a nonterminal forest
(with respect to a given rule) Pur(X) as the aver-
age similarity of all tree fragments in the cluster:
Pur(X) = 2N(N ? 1)
?
j
?
i<j
K ?(Ti, Tj), (11)
where N is number of tree fragments in the forest of
X . We now can define the derivation cost L(X ?<
24
?, ? >) for a rule production as:
L(X ?< ?, ? >) =
? log(( min
X1,X2??
(Pur(X1), Pur(X2)))k), (12)
where k ? 1 is the degree of smoothness. Note that
the prior derivation cost is set as L(?) = 0 by defini-
tion for phrasal rules.
Eq. 11 is quadratic complexity with N , however,
we note that rules with a large N will typically score
poorly on prior derivation models, and thus we can
avoid the computation for those by assigning them
a large cost. With the fast kernel computation, the
training procedure involved with the prior derivation
models for the task presented in Sec. 5 is about 5
times slower on a single machine, compared with
the training of the baseline system. However, we
note that our training procedure can be computed in
parallel, and therefore the training speed is not a bot-
tleneck when multiple CPUs are available.
5 Experiments
We perform our experiments on an English-to-
Chinese translation task in travel domain. Our train-
ing set contains 482017 parallel sentences (with
4.4M words on the English side), which are col-
lected from transcription and human translation of
conversations. The vocabulary size is 37K for En-
glish and 44K for Chinese after segmentation.
Our evaluation data is a held out data set of 2755
sentences pairs. We extracted every one out of two
sentence pairs into the dev-set, and left the remain-
der as the test-set. We thereby obtained a dev-set of
1378 sentence pairs, and a test-set with 1377 sen-
tence pairs. In both cases, there are about 15K run-
ning words on English side. All Chinese sentences
in training, dev and test sets are all automatically
segmented into words. Minimum-error-rate training
(Och, 2003) are conducted on dev-set to optimize
feature weights maximizing the BLEU score up to 4-
grams, and the obtained feature weights are blindly
applied on the test-set. To compare performances
excluding tokenization effects, all BLEU scores are
optimized (on dev-set) and reported (on test-set) at
Chinese character-level.
From training data, we extracted an initial phrase
pair set with 3.7M entries for phrases up to 8 words
on Chinese side. We trained a 4-gram language
model for Chinese at word level, which is shared by
all translation systems reported in this paper, using
the Chinese side of the parallel corpus that contains
around 2M segmented words.
We compare the proposed models with two base-
lines: a state-of-the-art phrase-based system and a
formal syntax-based system as described in Sec. 3.
The phrase-based system employs the 3.7M phrase
pairs to build the translation model, and it con-
tains a total set of 8 features, most of which are
identical to our baseline formal syntax-based model.
The difference only lies on that the glue and ab-
straction penalty are not applicable for phrase-based
system. Instead, a lexicalized reordering model is
trained from the word-aligned parallel corpus for
the phrase-based system. More details about our
multiple-graph based phrasal SMT can be found in
(Zhou et al, 2006; Zhou et al, 2008). For the base-
line syntax-based system, we generated a total of
15M rules and used 9 features.
We chose the Stanford parser (Klein and Man-
ning, 2002) as the English parser in our experiments
due to its high accuracy and relatively faster speed.
It was trained on the Wall Street Journal section of
the Penn Treebank. During the parsing, the input
English sentences were tokenized first, in a style
consistent with the data in the Penn Treebank.
We sent 482017 English sentences to the parser.
There were 1221 long sentences failed, less than
0.3% of the whole set. After the word alignment
and phrase extraction on the parallel corpus, we ob-
tained 2.2M unique English phrases. Among them
there are about 34K phrases having an empty tree
in their corresponding tree lists, due to the failure
in parsing. The number of unique tree fragments
for English phrases is 2.5M. Out of them there are
750K marked as incomplete. As mentioned previ-
ously, each rule covers a set of phrases, with each
phrase linked to a tree list. The total number of rules
with unique English side is around 8M.
The distribution of the number of rules over the
number of corresponding trees is shown in Table 1.
We observe that the majority of rules in our model
has less than 150 tree fragments. Therefore, consid-
ering the quadratic complexity in Eq. 11, we pun-
ish the rules with more than 150 unique tree frag-
ments with some floor cluster purity to speed up
25
Table 1: Distribution of rules over trees
Number of trees Number of rules
(0, 10] 3636766
(10, 20] 1556806
(20, 30] 989848
(30, 40] 916606
(40, 50] 488469
(50, 60] 270484
(60, 70] 198438
(70, 80] 86921
(80, 90] 58280
(90, 100] 29147
(100, 150] 437231
> 150 81060
the training. Not surprisingly, the rules with a large
number of tree fragments are typically those with
few stop words as terminals. For instance, the rule
X ?< X1aX2, ? > comes with more than 100Ktrees for the X1.
Table 2: English-to-Chinese BLEU score result on test-
set (character-based)
Models BLEU(4-gram)
Phrase-based 42.11
Formally Syntax-based 43.75
Formally Syntax-based
with prior derivation 44.51
Translation results are presented in Table 2
with character-based BLEU scores using 2 refer-
ences. Our baseline formally syntax-based mod-
els achieved the BLEU score of 43.75, an abso-
lute improvement of 1.6 point improvement over
phrase-based models. The improvement is statisti-
cally significant with p < 0.01 using the sign-test
described by (Collins et al, 2005). Applying the
prior derivation model into the syntax-based system,
BLEU score is further improved to 44.51, obtained
an another absolute improvement of 0.8 point, which
is also significantly better than our baseline syntax-
based models (p < 0.05).
6 Discussion and Summary
We introduced a prior derivation model to enhance
formally syntax-based SCFG for translation. Our
approach links a prior rule distribution with the syn-
tactic variations of abstracted sub-phrases, which
is modeled by distance measuring of linguistically
syntax parsing tree fragments using kernel meth-
ods. The proposed model has improved translation
performance over both phrase-based and formally
syntax-based models. Moreover, such a prior dis-
tribution can also be used to rank and prune SCFG
rules to reduce memory usage for online translation
systems based on syntax-based models.
Although the experiments in this paper are con-
ducted for prior derivation models on source side
in an English-to-Chinese task, we are interested in
applying this to foreign-to-English models as well.
As what we pointed out in Sec. 4, target side prior
derivation model fits with our framework as well.
7 Acknowledgement
The authors would like to thank Stanley F. Chen and
the anonymous reviewers for their helpful comments
on this paper.
References
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Comput. Linguist., 33(2):201?228.
Michael Collins and Nigel Duffy. 2002. Convolution
kernels for natural language. In Advances in Neural
Information Processing Systems 14.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL, pages 531?540.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
of HLT/NAACL-04, Boston, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc. of
ACL, pages 961?968.
Daniel Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proc. of ACL, pages 80?87.
Dan Klein and Christopher D. Manning. 2002. Fast exact
inference with a factored model for natural language
parsing. In NIPS, pages 3?10.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc.
NAACL/HLT.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proc. of ACL, pages 609?616.
26
Alessandro Moschitti. 2006. Making tree kernels practi-
cal for natural language learning. In Proc. of EACL.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proc. of ACL, pages 440?447, Hong
Kong, China, October.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Comput. Linguist., 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL, pages
160?167.
Giorgio Satta and Enoch Peserico. 2005. Some computa-
tional complexity results for synchronous context-free
grammars. In Proc. of HLT/EMNLP, pages 803?810.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Comput. Linguist., 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proc. of ACL, pages
523?530.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proc. of HLT/NAACL, pages 256?263.
Bowen Zhou, Stan F. Chen, and Yuqing Gao. 2006. Fol-
som: A fast and memory-efficient phrase-based ap-
proach to statistical machine translation. In IEEE/ACL
Workshop on Spoken Language Technology.
Bowen Zhou, Rong Zhang, and Yuqing Gao. 2008. Lex-
icalized reordering in multiple-graph based statistical
machine translation. In Proc. ICASSP.
27
Coling 2010: Poster Volume, pages 1550?1557,
Beijing, August 2010
Imposing Hierarchical Browsing Structures onto Spoken Documents
Xiaodan Zhu & Colin Cherry
Institute for Information Technology
National Research Council Canada
{Xiaodan.Zhu,Colin.Cherry}@nrc-cnrc.gc.ca
Gerald Penn
Department of Computer Science
University of Toronto
gpenn@cs.toronto.edu
Abstract
This paper studies the problem of im-
posing a known hierarchical structure
onto an unstructured spoken document,
aiming to help browse such archives.
We formulate our solutions within a
dynamic-programming-based alignment
framework and use minimum error-
rate training to combine a number of
global and hierarchical constraints. This
pragmatic approach is computationally
efficient. Results show that it outperforms
a baseline that ignores the hierarchical
and global features and the improvement
is consistent on transcripts with different
WERs. Directly imposing such hierar-
chical structures onto raw speech without
using transcripts yields competitive
results.
1 Introduction
Though speech has long served as a basic method
of human communication, revisiting and brows-
ing speech content had never been a possibility
before human can record their own voice. Re-
cent technological advances in recording, com-
pressing, and distributing such archives have led
to the consistently increasing availability of spo-
ken content.
Along with this availability comes a demand for
better ways to browse such archives, which is in-
herently more difficult than browsing text. In re-
lying on human beings? ability to browse text, a
solution is therefore to reduce the speech brows-
ing problem to a text browsing task through tech-
nologies that can automatically convert speech to
text, i.e., the automatic speech recognition (ASR).
Research along this line has implicitly changed
the traditional speaking-for-hearing and writing-
for-reading construals: now speech can be read
through its transcripts, though it was not originally
intended for this purpose, which in turn raises a
new set of problems.
The efficiency and convenience of reading spo-
ken documents are affected by at least two facts.
First, the quality of transcripts can impair brows-
ing efficiency, e.g., as shown in (Stark et al, 2000;
Munteanu et al, 2006), though if the goal is only
to browse salient excerpts, recognition errors on
the extracts can be reduced by considering the
confidence scores assigned by ASR (Zechner and
Waibel, 2000; Hori and Furui, 2003).
Even if transcription quality is not a problem,
browsing transcripts is not straightforward. When
intended to be read, written documents are al-
most always presented as more than uninterrupted
strings of text. Consider that for many writ-
ten documents, e.g., books, indicative structures
such as section/subsection headings and tables-of-
contents are standard constituents created manu-
ally to help readers. Structures of this kind, how-
ever, are rarely aligned with spoken documents.
In this paper, we are interested in addressing
the second issue: adding hierarchical browsable
structures to speech transcripts. We define a hi-
erarchical browsable structure as a set of nested
labelled bracketing which, when placed in text,
partition the document into labeled segments. Ex-
amples include the sequence of numbered sec-
tion headings in this paper, or the hierarchical
slide/bullet structure in the slides of a presenta-
tion.
1550
An ideal solution to this task would directly in-
fer both the hierarchical structure and the labels
from unstructured spoken documents. However,
this is a very complex task, involving the analysis
of not only local but also high-level discourse over
large spans of transcribed speech. Specifically for
spoken documents, spoken-language characteris-
tics as well as the lack of formality and thematic
boundaries in transcripts violate many conditions
that a reliable algorithm (Marcu, 2000) relies on
and therefore make the task even harder.
In this paper, we aim at a less ambitious but
naturally occurring problem: imposing a known
hierarchical structure, e.g., presentation slides,
onto the corresponding document, e.g., presenta-
tion transcripts. Given an ordered, nested set of
topic labels, we must place the labels so as to
correctly segment the document into appropriate
units. Such an alignment would provide a useful
tool for presentation browsing, where a user could
easily navigate through a presentation by clicking
on bullets in the presentation slides. The solution
to this task should also provide insights and tech-
niques that will be useful in the harder structure-
inference task, where hierarchies and labels are
not given.
We present a dynamic-programming-based
alignment framework that considers global docu-
ment features and local hierarchical features. This
pragmatic approach is computationally efficient
and outperforms a baseline alignment that ignores
the hierarchical structure of bullets within slides.
We also explore the impact of speech recognition
errors on this task. Furthermore, we study the
feasibility of directly aligning a structure to raw
speech, as opposed to a transcript.
2 Related work
Topic/slide boundary detection The previous
work most directly related to ours is research that
attempts to find flat structures of spoken docu-
ments, such as topic and slide boundaries. For
example, the work of (Chen and Heng, 2003;
Ruddarraju, 2006; Zhu et al, 2008) aims to find
slide boundaries in the corresponding lecture tran-
scripts. Malioutov et al (2007) developed an ap-
proach to detecting topic boundaries of lecture
recordings by finding repeated acoustic patterns.
None of this work, however, has involved hierar-
chical structures that exist at different levels of a
document.
In addition, researchers have also analyzed
other multimedia channels, e.g., video (Liu et al,
2002; Wang et al, 2003; Fan et al, 2006), to de-
tect slide transitions. Such approaches, however,
are unlikely to find semantic structures that are
more detailed than slide transitions, e.g., the bullet
hierarchical structures that we are interested in.
Building tables-of-contents on written text A
notable effort going further than topic segmenta-
tion is the work by Branavan et al (2007), which
aims at the ultimate goal of building tables-of-
contents for written texts. However, the authors
assumed the availability of the hierarchical struc-
tures and the corresponding text spans. Therefore,
their problem was restricted to generating titles for
each span. Our work here can be thought of as the
inverse problem, in which the title of each section
is known, but the corresponding segments in the
spoken documents are unknown. Once the corre-
spondence is found, an existing hierarchical struc-
ture along with its indicative titles is automatically
imposed on the speech recordings. Moreover, this
paper studies spoken documents instead of writ-
ten text. We believe it is more attractive not only
because of the necessity of browsing spoken con-
tent in a more efficient way but also the general
absence of helpful browsing structures that are of-
ten available in written text, as we have already
discussed above.
Rhetoric analysis In general, analyzing dis-
course structures can provide thematic skeletons
(often represented as trees) of a document as well
as relationship between the nodes in the trees. Ex-
amples include the widely known discourse pars-
ing work by Marcu (2000). However, when the
task involves the understanding of high-level dis-
course, it becomes more challenging than just
finding local discourse conveyed on small spans of
text; e.g., the latter is more likely to benefit from
the presence of discourse markers. Specifically
for spoken documents, spoken-language charac-
teristics as well as the absence of formality and
thematic boundaries in transcripts pose additional
1551
difficulty. For example, the boundaries of sen-
tences, paragraphs, and larger text blocks like sec-
tions are often missing. Together with speech
recognition errors as well as other speech charac-
teristics such as speech disfluences, they will im-
pair the conditions on which an effective and reli-
able algorithm of discourse analysis is often built.
3 Problem formulation
We are given a speech sequence U =
u1, u2, ..., um, where ui is an utterance. De-
pending on the application, ui can either stand
for the audio or transcript of the utterance. We
are also given a corresponding hierarchical struc-
ture. In our work, this is a sequence of lecture
slides containing a set of slide titles and bullets,
B = {b1, b2, ..., bn}, organized in a tree structure
T (?,?,?), where ? is the root of the tree that
concatenates all slides of a lecture; i.e., each slide
is a child of the root ? and each slide?s bullets
form a subtree. In the rest of this paper, the word
bullet means both the title of a slide (if any) and
any bullet in it. ? is the set of nodes of the tree
(both terminal and non-terminals, excluding the
root ?), each corresponding to a bullet bi in the
slides. ? is the edge set. With the definitions, our
task is herein to find the triple (bi, uk, ul), denot-
ing that a bullet bi starts from the kth utterance
uk and ends at the lth. Constrained by the tree
structure, the text span corresponding to an an-
cestor bullet contains those corresponding to its
descendants; i.e., if a bullet bi is the ancestor of
another bullet bj in the tree, the acquired bound-
ary triples (bi, uk1, ul1) and (bj , uk2, ul2) should
satisfy uk1 ? uk2 and ul1 ? ul2. In implemen-
tation, we only need to find the starting point of a
bullet, i.e., a pair (bi, uk), since we know the tree
structure in advance and therefore we know that
the starting position of the next sibling bullet is
the ending boundary for the current bullet.
4 Our approaches
Our task is to find the correspondence between
slide bullets and a speech sequence or its tran-
scripts. Research on finding correspondences be-
tween parallel texts pervades natural language
processing. For example, aligning bilingual sen-
tence pairs is an essential step in training ma-
chine translation models. In text summarization,
the correspondence between human-written sum-
maries and their original texts has been identified
(Jing, 2002), too. In speech recognition, forced
alignment is applied to align speech and tran-
scripts. In this paper, we keep the general frame-
work of alignment in solving our problem.
Our solution, however, should be flexible to
consider multiple constraints such as those con-
veyed in hierarchical bullet structures and global
word distribution. Accordingly, the model pro-
posed in this paper depends on two orthogonal
strategies to ensure efficiency and richness of the
model. First of all, we formulate all our solutions
within a classic dynamic programming framework
to enforce computational efficiency (section 4.1).
On the other hand, we explore the approach to in-
corporating hierarchical and global features into
the alignment framework (Section 4.2). The as-
sociated parameters are then optimized with Pow-
ell?s algorithm (Section 4.3).
4.1 A pre-order walk of bullet trees
We formulate our solutions within the classic
dynamic-programming-based alignment frame-
work, dynamic time warping (DTW). To this end,
we need to sequentialize the given hierarchies,
i.e., bullet trees. We propose to do so through a
pre-order walk of a bullet tree; i.e., at any step
of a recursive traversal of the tree, the alignment
model always visits the root first, followed by its
children in a left-to-right order. This sequential-
ization actually corresponds to a reasonable as-
sumption: words appearing earlier on a given slide
are spoken earlier by the speaker. The pre-order
walk is also used by (Branavan et al, 2007) to
reduce the search space of their discriminative
table-of-contents generation. Our sequentializa-
tion strategy can be intuitively thought of as re-
moving indentations that lead each bullet. As
shown in Figure 1, the right panel is a bullet array
resulting from a pre-walk of the slide in the left
panel. In our baseline model, the resulted bullet
array is directly aligned with lecture utterances.
Other orders of bullet traversal could also be
considered, e.g., when speech does not strictly fol-
low bullet orders. In general, one can regard our
1552
task here as a tagging problem to allow further
flexibility on bullet-utterance correspondence, in
which bullets are thought of as tags. However,
considering the fact that bullets are created to or-
ganize speech and in most cases they correspond
to the development of speech content monotoni-
cally, this paper focuses on addressing the prob-
lem in the alignment framework.
Figure 1: A pre-order walk of a bullet tree.
4.2 Incorporating hierarchical and global
features
Our models should be flexible enough to consider
constraints that could be helpful, e.g., the hierar-
chical bullet structures and global word distribu-
tion. We propose to consider all these constraints
in the phase of estimating similarity matrices. To
this end, we use two levels of similarity matrices
to capture local tree constraints and global word
distributions, respectively.
First of all, information conveyed in the hierar-
chies of bullet trees should be considered, such as
the potentially discriminative nature between two
sibling bullets (Branavan et al, 2007) and the re-
lationships between ancestor and descendant bul-
lets. We incorporate them in the bullet-utterance
similarity matrices. Specifically, when estimating
the similarity between a bullet bi and an utterance
uj , we consider local tree constraints based on
where the node bi is located on the slide. We do
so by accounting for first and second-order tree
features. Given a bullet, bi, we first represent it
as multiple vectors, one for each of the following:
its own words, the words appearing in its parent
bullet, grandparent, children, grandchildren, and
the bullets immediately adjacent to bi. That is, bi
is now represented as 6 vectors of words (we do
not discriminate between its left and right siblings
and put these words in the same vector). Simi-
larity between the bullet bi and an utterance uj is
calculated by taking a weighted average over the
similarities between each of the 6 vectors and the
utterance uj . A linear combination is used and the
weights are optimized on a development set.
Global property of word distributions could be
helpful, too. A general term often has less dis-
criminative power in the alignment framework
than a word that is localized to a subsection of
the document and is related to specific subtopics.
For example, in a lecture that teaches introductory
computer science topics, aligning a general term
?computer? should receive a smaller weight than
aligning some topic-specific terms such as ?au-
tomaton.? The latter word is more likely to appear
in a more narrow text span. It is not straightfor-
ward to directly calculate idf scores unless a lec-
ture is split into smaller segments in some way.
Instead, in our models, the distribution property
of a word is considered in word-level similarity
matrices with the following formula.
sim(wi, wj) =
{
0 : i 6= j
1? ? var(wi)maxk(var(wk)) : i = j
Aligning different words receives no bonus,
while matching the same word between bullets
and utterances receives a score of 1 minus a dis-
tribution penalty, as shown in the formula above.
The function var(wi) calculates the standard vari-
ance of the positions where the word wi appears.
Divided by the maximal standard variance of word
positions in the same lecture, the score is normal-
ized to [0,1]. This distribution penalty is weighted
by ?, which is tuned in a development set. Again,
a general term is expected to have a larger posi-
tional variance.
Once a word-level matrix is acquired, it is com-
bined with the bullet-utterance level matrix dis-
cussed above. Specifically, when measuring the
similarity between a word vector (one of the 6
vectors) and the transcripts of an utterance, we
sum up the word-level similarity scores of all
matching words between them, normalize the re-
sulted score by the length of the vector and ut-
terance, and then renormalize it to the range
1553
[0, 1] within the same spoken document. The
final bullet-utterance similarity matrix is incor-
porated into the pre-order-walk suquentialization
discussed above, when alignment is conducted.
4.3 Parameter optimization
Powell?s algorithm (Press et al, 2007) is used to
find the optimal weights for the constraints we in-
corporated above, to directly minimize the objec-
tive function, i.e., the Pk and WindowDiff scores
that we will discuss later. As a summary, we have
7 weights to tune: a weight for each of the fol-
lowing: parent bullet, grandparent, adjacent sib-
lings, children, grandchildren, and the current bul-
let, plus the word distribution penalty ?. The val-
ues of these weights are determined on a develop-
ment set.
Note that the model we propose here does not
exclude the use of further features; instead, many
other features, such as smoothed word similarity
scores, can be easily added to this model. We
are conservative on our model complexity here,
in terms of number of weights need to be tuned,
for the consideration of the size of data that we
can used to estimate these weights. Finally, with
all the 7 weights being determined, we apply the
standard dynamic time warping (DTW).
5 Experimental set-up
5.1 Data
We use a corpus of lectures recorded at a large
research university. The correspondence between
bullets and speech utterances are manually an-
notated in a subset of this lecture corpus, which
contains approximately 30,000 word tokens in
its manual transcripts. Intuitively, this roughly
equals a 120-page double-spaced essay in length.
The lecturer?s voice was recorded with a head-
mounted microphone with a 16kHz sampling rate
and 16-bit samples. Students? comments and
questions were not recorded. The speech is split
into utterances by pauses longer than 200ms, re-
sulting in around 4000 utterances. There are 119
slides that are composed of 921 bullets. A sub-
set containing around 25% consecutive slides and
their corresponding speech/transcripts are used as
our development set to tune the parameters dis-
cussed earlier; the rest data are used as our test
set.
5.2 Evaluation metric
We evaluate our systems according to how well
the segmentation implied by the inferred bullet
alignment matches that of the manually anno-
tated gold-standard bullet algnment. Though one
may consider that different bullets may be of dif-
ferent importance, in this paper we do not use
any heuristics to judge this and we treat all bul-
lets equally in our evaluation. We evaluate our
systems with the Pk and WindowDiff metrics
(Malioutov et al, 2007; Beeferman et al, 1999;
Pevsner and Hearst, 2002). Note that for both
metrics, the lower a score is, the better the per-
formance of a system is. The Pk score computes
the probability of a randomly chosen pair of words
being inconsistently separated. The WindowDiff
is a variant of Pk; it penalizes false positives and
near misses equally.
6 Experimental results
6.1 Alignment performance
Table 1 presents the results on automatic tran-
scripts with a 39% WER, a typical WER in realis-
tic and uncontrolled lecture conditions (Leeuwis
et al, 2003; Hsu and Glass, 2006). The tran-
scripts were generated with the SONIC toolkit
(Pellom, 2001). The acoustic model was trained
on the Wall Street Journal dictation corpus. The
language model was trained on corpora obtained
from the Web through searching the words ap-
pearing on slides as suggested by (Munteanu et
al., 2007).
Pk WindowDiff
UNI 0.481 0.545
TT 0.469 0.534
B-ALN 0.283 0.376
HG-ALN 0.266 0.359
Table 1: The Pk and WindowDiff scores of uni-
form segmentation (UNI), TextTiling (TT), base-
line alignment (B-ALN), and alignment with hier-
archical and global information (HG-ALN).
From Table 1, we can see that the model that
1554
utilizes the hierarchical structures of slides and
global distribution of words, i.e., the HG-ALN
model, reduces both Pk and WindowDiff scores
over the baseline model, B-ALN. As discussed
earlier, the baseline is a re-implementation of
standard dynamic time warping based only on a
pre-order walk of the slides, while the HG-ALN
model incorporates also hierarchical bullet con-
straints and global word distribution.
Table 1 also presents the performance of a
typical topic segmentation algorithm, TextTiling
(Hearst, 1997). Note that similar to (Malioutov et
al., 2007), we force the number of predicted topic
segments to be the target number, i.e., in our task,
the number of bullets. The results show that both
the Pk and WindowDiff scores of TextTiling are
significantly higher than those of the alignment al-
gorithms. Our manual analysis suggests that many
segments are as short as several utterances and the
difference between two consecutive segments is
too subtle to be captured by a lexical cohesion-
based method such as TextTiling. For compari-
son, We also present the results of uniform seg-
mentation (UNI), which simply splits the tran-
script of each lecture evenly into segments with
same numbers of words.
6.2 Performance under different WERs
Speech recognition errors within reasonable
ranges often have very small impact on many spo-
ken language processing tasks such as spoken lan-
guage retrieval (Garofolo et al, 2000) and speech
summarization (Christensen et al, 2004; Maskey,
2008; Murray, 2008; Zhu, 2010). To study the
impact of speech recognition errors on our task
here, we experimented with the alignment mod-
els on manual transcripts as well as on automatic
transcripts with different WERs, including a 39%
and a 46% WER produced by two real recogni-
tion systems. To increase the spectrum of our ob-
servation, we also overfit our ASR models to ob-
tain smaller WERs at the levels of 11%, 19%, and
30%.
From Figure 2, we can see that at all levels of
these different WERs, the HG-ALN model con-
sistently outperforms the B-ALN system (the AU-
DIO model will be discussed below). The Pk
and WindowDiff curves also show that the align-
0 0.1 0.2 0.3 0.4
0.24
0.26
0.28
0.3
0.32
Pk under different WERs
Pk
Word error rate
 
 
B?ALN
HG?ALN
AUDIO
0 0.1 0.2 0.3 0.4
0.34
0.36
0.38
0.4
WindowDiff under different WERs
W
in
do
wD
iff
Word error rate
 
 
B?ALN
HG?ALN
AUDIO
Figure 2: The impact of different WERs on the
alignment models. The performance of an audio-
based model (AUDIO) is also presented.
ment performance is sensitive to recognition er-
rors, particularly when the WER is in the range of
30%?45%, suggesting that the problem we study
here can benefit from the improvement of current
ASR systems in this range, e.g., the recent ad-
vance achieved in (Glass et al, 2007).
6.3 Imposing hierarchical structures onto
raw speech
We can actually impose hierarchical structures di-
rectly onto raw speech, through estimating the
similarity between bullets and speech. This en-
ables navigation through the raw speech by using
slides; e.g., one can hear different parts of speech
by clicking a bullet. We apply keyword spotting to
solve this problem, which detects the occurrences
of each bullet word in the corresponding lecture
audio.
1555
In this paper, we use a token-passing based al-
gorithm provided in the ASR toolkit SONIC (Pel-
lom, 2001). Since the slides are given in advance,
we manually add into the pronunciation dictio-
nary the words that appear in slides but not in
the pronunciation dictionary. To estimate sim-
ilarity between a word vector (discussed earlier
in Section 4.2) and an utterance, we sum up all
keyword-spotting confidence scores assigned be-
tween them, normalize the resulted score by the
length of the vector and the duration of the utter-
ance, and then renormalize it to the range [0, 1]
within the same spoken lecture.
We present the performance of our bullet-audio
alignment model (AUDIO) in Figure 2 so that one
can compare its effectiveness with the transcrip-
tion based methods. The figure shows that the
performance of the AUDIO model is comparable
to the baseline transcription-based model, i.e., B-
ALN, when the WERs of the transcripts are in the
range of 37%?39%. The performance is compara-
ble to the HG-ALN model when WERs are in the
range of 42%?44%. Also, this suggests that incor-
porating hierarchical and global features compen-
sates for the performance degradation of speech
recognition in this range when the WER is 4%-
6% higher.
Note that we did not observe that the perfor-
mance is different when incorporating hierarchi-
cal information and global word distributions into
the AUDIO model, so the AUDIO results in Fig-
ure 2 are the performance of both types of meth-
ods. The current keyword spotting component
yields a high false-positive rate; e.g., it incorrectly
reports many words that are acoustically similar to
parts of other words that really appear in an utter-
ance. This happened even when a high threshold
is set. The noise impairs the benefit of hierarchical
and distribution features.
7 Conclusions and discussions
This paper investigates the problem of imposing
a known hierarchical structure onto an unstruc-
tured spoken document. Results show that incor-
porating local hierarchical constraints and global
word distributions in the efficient dynamic pro-
gramming framework yields a better performance
over the baseline. Further experiments on a wide
range of WERs confirm that the improvement is
consistent, and show that both types of models
are sensitive to speech recognition errors, partic-
ularly when WER increases to 30% and above.
Moreover, directly imposing hierarchical struc-
tures onto raw speech through keyword spotting
achieves competitive performance.
References
Beeferman, D., A. Berger, and J. Lafferty. 1999.
Statistical models for text segmentation. Machine
Learning, 34(1-3):177?210.
Branavan, S., Deshpande P., and Barzilay R. 2007.
Generating a table-of-contents: A hierarchical dis-
criminative approach. In Proc. of Annual Meeting
of the Association for Computational Linguistics.
Chen, Y. and W. J. Heng. 2003. Automatic synchro-
nization of speech transcript and slides in presenta-
tion. In Proc. International Symposium on Circuits
and Systems.
Christensen, H., B. Kolluru, Y. Gotoh, and S. Re-
nals. 2004. From text summarisation to style-
specific summarisation for broadcast news. In Proc.
of the 26th European Conference on Information
Retrieval, pages 223?237.
Fan, Q., K. Barnard, A. Amir, A. Efrat, and M. Lin.
2006. Matching slides to presentation videos using
sift and scene background. In Proc. of ACM Inter-
national Workshop on Multimedia Information Re-
trieval, pages 239?248.
Garofolo, J., G. Auzanne, and E. Voorhees. 2000.
The trec spoken document retrieval track: A success
story. In Proc. of Text Retrieval Conference, pages
16?19.
Glass, J., T. Hazen, S. Cyphers, I. Malioutov,
D. Huynh, and R. Barzilay. 2007. Recent progress
in the mit spoken lecture processing project. Proc.
of Annual Conference of the International Speech
Communication Association, pages 2553?2556.
Hearst, M. 1997. Texttiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Hori, C. and S. Furui. 2003. A new approach to au-
tomatic speech summarization. IEEE Transactions
on Multimedia, 5(3):368?378.
Hsu, B. and J. Glass. 2006. Style and topic language
model adaptation using hmm-lda. In Proc. of Con-
ference on Empirical Methods in Natural Language
Processing.
1556
Jing, H. 2002. Using hidden markov modeling to
decompose human-written summaries. Computa-
tional Linguistics, 28(4):527?543.
Leeuwis, E., M. Federico, and M. Cettolo. 2003. Lan-
guage modeling and transcription of the ted corpus
lectures. In Proc. of IEEE International Conference
on Acoustics, Speech and Signal Processing.
Liu, T., R. Hjelsvold, and J. R. Kender. 2002. Analysis
and enhancement of videos of electronic slide pre-
sentations. In Proc. IEEE International Conference
on Multimedia and Expo.
Malioutov, I., A. Park, B. Barzilay, and J. Glass. 2007.
Making sense of sound: Unsupervised topic seg-
mentation over acoustic input. In Proc. of Annual
Meeting of the Association for Computational Lin-
guistics, pages 504?511.
Marcu, D. 2000. The theory and practice of discourse
parsing and summarization. The MIT Press.
Maskey, S. 2008. Automatic Broadcast News Speech
Summarization. Ph.D. thesis, Columbia University.
Munteanu, C., R. Baecker, G. Penn, E. Toms, and
E. James. 2006. Effect of speech recognition accu-
racy rates on the usefulness and usability of webcast
archives. In Proc. of ACM Conference on Human
Factors in Computing Systems, pages 493?502.
Munteanu, C., G. Penn, and R. Baecker. 2007.
Web-based language modelling for automatic lec-
ture transcription. In Proc. of Annual Conference
of the International Speech Communication Associ-
ation.
Murray, G. 2008. Using Speech-Specific Character-
istics for Automatic Speech Summarization. Ph.D.
thesis, University of Edinburgh.
Pellom, B. L. 2001. Sonic: The university of col-
orado continuous speech recognizer. Tech. Rep. TR-
CSLR-2001-01, University of Colorado.
Pevsner, L. and M. Hearst. 2002. A critique and im-
provement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28:19?36.
Press, W.H., S.A. Teukolsky, W.T. Vetterling, and B.P.
Flannery. 2007. Numerical recipes: The art of sci-
ence computing. Cambridge University Press.
Ruddarraju, R. 2006. Indexing Presentations Using
Multiple Media Streams. Ph.D. thesis, Georgia In-
stitute of Technology. M.S. Thesis.
Stark, L., S. Whittaker, and J. Hirschberg. 2000. Find-
ing information in audio: A new paradigm for au-
dio browsing and retrieval. In Proc. of International
Conference on Spoken Language Processing.
Wang, F., C. W. Ngo, and T. C. Pong. 2003. Synchro-
nization of lecture videos and electronic slides by
video text analysis. In Proc. of ACM International
Conference on Multimedia.
Zechner, K. and A. Waibel. 2000. Minimizing word
error rate in textual summaries of spoken language.
In Proc. of Applied Natural Language Processing
Conference and Meeting of the North American
Chapter of the Association for Computational Lin-
guistics, pages 186?193.
Zhu, X., X. He, C. Munteanu, and G. Penn. 2008. Us-
ing latent dirichlet alocation to incorporate domain
knowledge for topic transition detection. In Proc.
of Annual Conference of the International Speech
Communication Association.
Zhu, X. 2010. Summarizing Spoken Documents
Through Utterance Selection. Ph.D. thesis, Univer-
sity of Toronto.
1557
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 607?615,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Bilingual Sentiment Consistency for Statistical Machine Translation 
 
Boxing Chen and Xiaodan Zhu 
National Research Council Canada 
1200 Montreal Road, Ottawa, Canada, K1A 0R6 
{Boxing.Chen, Xiaodan.Zhu}@nrc-cnrc.gc.ca 
  
 
Abstract 
In this paper, we explore bilingual sentiment 
knowledge for statistical machine translation 
(SMT). We propose to explicitly model the 
consistency of sentiment between the source 
and target side with a lexicon-based approach. 
The experiments show that the proposed mod-
el significantly improves Chinese-to-English 
NIST translation over a competitive baseline. 
1 Introduction 
The expression of sentiment is an interesting and 
integral part of human languages. In written text 
sentiment is conveyed by senses and in speech also 
via prosody. Sentiment is associated with both 
evaluative (positive or negative) and potency (de-
gree of sentiment) ? involving two of the three 
major semantic differential categories identified by 
Osgood et al. (1957). 
Automatically analyzing the sentiment of mono-
lingual text has attracted a large bulk of research, 
which includes, but is not limited to, the early ex-
ploration of (Turney, 2002; Pang et al., 2002; Hat-
zivassiloglou & McKeown, 1997). Since then, 
research has involved a variety of approaches and 
been conducted on various type of data, e.g., prod-
uct reviews, news, blogs, and the more recent so-
cial media text.  
As sentiment has been an important concern in 
monolingual settings, better translation of such 
information between languages could be of interest 
to help better cross language barriers, particularly 
for sentiment-abundant data. Even when we ran-
domly sampled a subset of sentence pairs from the 
NIST Open MT1 training data, we found that about 
48.2% pairs contain at least one sentiment word on 
both sides, and 22.4% pairs contain at least one 
                                                          
1 http://www.nist.gov/speech/tests/mt 
intensifier word on both sides, which suggests a 
non-trivial percent of sentences may potentially 
involve sentiment in some degree2.  
 
# snt. 
pairs 
% snt. with 
sentiment words 
% snt. with 
intensifiers 
103,369 48.2% 22.4% 
 
 
Table 1.  Percentages of sentence pairs that contain sen-
timent words on both sides or intensifiers3 on both sides. 
 
One expects that sentiment has been implicitly 
captured in SMT through the statistics learned 
from parallel corpus, e.g., the phrase tables in a 
phrase-based system. In this paper, we are interest-
ed in explicitly modeling sentiment knowledge for 
translation. We propose a lexicon-based approach 
that examines the consistency of bilingual subjec-
tivity, sentiment polarity, intensity, and negation. 
The experiments show that the proposed approach 
improves the NIST Chinese-to-English translation 
over a strong baseline. 
In general, we hope this line of work will help 
achieve better MT quality, especially for data with 
more abundant sentiment, such as social media text. 
2 Related Work  
Sentiment analysis and lexicon-based approach-
es Research on monolingual sentiment analysis can 
be found under different names such as opinion, 
stance, appraisal, and semantic orientation, among 
others. The overall goal is to label a span of text 
either as positive, negative, or neutral ? some-
times the strength of sentiment is a concern too. 
                                                          
2 The numbers give a rough idea of sentiment coverage; it 
would be more ideal if the estimation could be conducted on 
senses instead of words, which, however, requires reliable 
sense labeling and is not available at this stage. Also, accord-
ing to our human evaluation on a smaller dataset, two thirds of 
such potentially sentimental sentences do convey sentiment.  
3 The sentiment and intensifier lexicons used to acquire these 
numbers are discussed in Section 3.2. 
  
607
The granularities of text have spanned from words 
and phrases to passages and documents.  
Sentiment analysis has been approached mainly 
as an unsupervised or supervised problem, alt-
hough the middle ground, semi-supervised ap-
proaches, exists. In this paper, we take a lexicon-
based, unsupervised approach to considering sen-
timent consistency for translation, although the 
translation system itself is supervised. The ad-
vantages of such an approach have been discussed 
in (Taboada et al., 2011). Briefly, it is good at cap-
turing the basic sentiment expressions common to 
different domains, and certainly it requires no bi-
lingual sentiment-annotated data for our study. It 
suits our purpose here of exploring the basic role 
of sentiment for translation. Also, such a method 
has been reported to achieve a good cross-domain 
performance (Taboada et al., 2011) comparable 
with that of other state-of-the-art models.  
Translation for sentiment analysis A very inter-
esting line of research has leveraged labeled data in 
a resource-rich language (e.g., English) to help 
sentiment analysis in a resource-poorer language. 
This includes the idea of constructing sentiment 
lexicons automatically by using a translation dic-
tionary (Mihalcea et al., 2007), as well as the idea 
of utilizing parallel corpora or automatically trans-
lated documents to incorporate sentiment-labeled 
data from different languages (Wan, 2009; Mihal-
cea et al., 2007).  
Our concern here is different ? instead of uti-
lizing translation for sentiment analysis; we are 
interested in the SMT quality itself, by modeling 
bilingual sentiment in translation. As mentioned 
above, while we expect that statistics learned from 
parallel corpora have implicitly captured sentiment 
in some degree, we are curious if better modeling 
is possible. 
Considering semantic similarity in translation 
The literature has included interesting ideas of in-
corporating different types of semantic knowledge 
for SMT. A main stream of recent efforts have 
been leveraging semantic roles (Wu and Fung, 
2009; Liu and Gildea, 2010; Li et al., 2013) to im-
prove translation, e.g., through improving reorder-
ing. Also, Chen et al. (2010) have leveraged sense 
similarity between source and target side as addi-
tional features.  In this work, we view a different 
dimension, i.e., semantic orientation, and show that 
incorporating such knowledge improves the trans-
lation performance. We hope this work would add 
more evidences to the existing literature of lever-
aging semantics for SMT, and shed some light on 
further exploration of semantic consistency, e.g., 
examining other semantic differential factors. 
3 Problem & Approach 
3.1 Consistency of sentiment 
Ideally, sentiment should be properly preserved in 
high-quality translation. An interesting study con-
ducted by Mihalcea et al. (2007) suggests that in 
most cases the sentence-level subjectivity is pre-
served by human translators. In their experiments, 
one English and two Romanian native speakers 
were asked to independently annotate the senti-
ment of English-Romanian sentence pairs from the 
SemCor corpus (Miller et al., 1993), a balanced 
corpus covering a number of topics in sports, poli-
tics, fashion, education, and others. These human 
subjects were restricted to only access and annotate 
the sentiment of their native-language side of sen-
tence pairs. The sentiment consistency was ob-
served by examining the annotation on both sides. 
Automatic translation should conform to such a 
consistency too, which could be of interest for 
many applications, particularly for sentiment-
abundant data. On the other hand, if consistency is 
not preserved for some reason, e.g., alignment 
noise, enforcing consistency may help improve the 
translation performance. In this paper, we explore 
bilingual sentiment consistency for translation. 
3.2 Lexicon-based bilingual sentiment analysis 
To capture bilingual sentiment consistency, we use 
a lexicon-based approach to sentiment analysis. 
Based on this, we design four groups of features to 
represent the consistency. 
The basic idea of the lexicon-based approach is 
first identifying the sentiment words, intensifiers, 
and negation words with lexicons, and then calcu-
lating the sentiment value using manually designed 
formulas. To this end, we adapted the approaches 
of (Taboada et al., 2011) and (Zhang et al., 2012) 
so as to use the same formulas to analyze the sen-
timent on both the source and the target side.  
The English and Chinese sentiment lexicons we 
used are from (Wilson et al. 2005) and (Xu and Lin, 
2007), respectively. We further use 75 English in-
608
tensifiers listed in (Benzinger, 1971; page 171) and 
81 Chinese intensifiers from (Zhang et al., 2012). 
We use 17 English and 13 Chinese negation words.  
Similar to (Taboada et al., 2011) and (Zhang et 
al., 2012), we assigned a numerical score to each 
sentiment word, intensifier, and negation word. 
More specifically, one of the five values: -0.8, -0.4, 
0, 0.4, and 0.8, was assigned to each sentiment 
word in both the source and target sentiment lexi-
cons, according to the strength information anno-
tated in these lexicons. The scores indicate the 
strength of sentiment. Table 2 lists some examples. 
Similarly, one of the 4 values, i.e., -0.5, 0.5, 0.7 
and 0.9, was manually assigned to each intensifier 
word, and a -0.8 or -0.6 to the negation words. All 
these scores will be used below to modify and shift 
the sentiment value of a sentiment unit.  
Sentiment words Intensifiers Negation words 
impressive (0.8) 
good (0.4) 
actually (0.0) 
worn (-0.4) 
depressing (-0.8) 
extremely (0.9) 
very (0.7) 
pretty (0.5) 
slightly (-0.5) 
not (-0.8) 
rarely (-0.6) 
Table 2: Examples of sentiment words and their senti-
ment strength; intensifiers and their modify rate; nega-
tion words and their negation degree. 
 
Each sentiment word and its modifiers (negation 
words and intensifiers) form a sentiment unit. We 
first found all sentiment units by identifying senti-
ment words with the sentiment lexicons and their 
modifiers with the corresponding lexicon in a 7-
word window. Then, for different patterns of sen-
timent unit, we calculated the sentiment values 
using the formulas listed in Table 3, where these 
formulas are adapted from (Taboada et al., 2011) 
and (Zhang et al., 2012) so as to be applied to both 
languages.  
 
Sen. 
unit 
Sen. value  
formula 
 
Example 
Sen. 
value 
ws S(ws) good 0.40 
wnws D(wn)S(ws) not good -0.32 
wiws (1+R(wi))S(ws) very good 0.68 
wnwiws (1+ D(wn)R(wi))S(ws) not very good 0.176 
wiwnws D(wn)(1+R(wi))S(ws) very not good
4 -0.544 
Table 3: Heuristics used to compute the lexicon-based 
sentiment values for different types of sentiment units. 
                                                          
4 The expression ?very not good? is ungrammatical in English. 
However, in Chinese, it is possible to have this kind of expres-
sion, such as ??????, whose transliteration is ?very not 
beautiful?, meaning ?very ugly?. 
For notation, S(ws) stands for the strength of 
sentiment word ws, R(wi) is degree of the intensifi-
er word wi, and D(wn) is the negation degree of the 
negation word wn. 
Above, we have calculated the lexicon based 
sentiment value (LSV) for any given unit ui, and 
we call it lsv(ui) below. If a sentence or phrase s 
contains multiple sentiment units, its lsv-score is a 
merge of the individual lsv-scores of all its senti-
ment units: 
 
)))((()( 1 iN ulsvbasismergslsv ?             (1) 
 
where the function basis(.) is a normalization func-
tion that performs on each lsv(ui). For example, the 
basis(.) function could be a standard sign function 
that just examines if a sentiment unit is positive or 
negative, or simply an identity function (using the 
lsv-scores directly). The merg(.) is a function that 
merge the lsv-scores of individual sentiment units, 
which may take several different forms below in 
our feature design. For example, it can be a mean 
function to take the average of the sentiment units? 
lsv-scores, or a logic OR function to examine if a 
sentence or phrase contains positive or negative 
units (depending on the basis function). It can also 
be a linear function that gives different weights to 
different units according to further knowledge, e.g., 
syntactic information. In this paper, we only lever-
age the basic, surface-level analysis5. 
In brief, our model here can be thought of as a 
unification and simplification of both (Taboada et 
al., 2011) and (Zhang et al., 2012), for our bilin-
gual task. We suspect that better sentiment model-
ing may further improve the general translation 
performance or the quality of sentiment in transla-
tion. We will discuss some directions we think in-
teresting in the future work section. 
3.3 Incorporating sentiment consistency into 
phrase-based SMT 
In this paper, we focus on exploring sentiment 
consistency for phrase-based SMT. However, the 
approach might be used in other translation 
framework. For example, consistency may be con-
sidered in the variables used in hierarchical transla-
tion rules (Chiang, 2005).   
                                                          
5 Note that when sentiment-annotated training data are availa-
ble, merg(.) can be trained, e.g., if assuming it to be the wide-
ly-used (log-) linear form. 
609
We will examine the role of sentiment con-
sistency in two ways: designing features for the 
translation model and using them for re-ranking. 
Before discussing the details of our features, we 
briefly recap phrase-based SMT for completeness. 
Given a source sentence f, the goal of statistical 
machine translation is to select a target language 
string e which maximizes the posterior probability 
P(e|f). In a phrase-based SMT system, the transla-
tion unit is the phrases, where a "phrase" is a se-
quence of words. Phrase-based statistical machine 
translation systems are usually modeled through a 
log-linear framework (Och and Ney, 2002) by in-
troducing the hidden word alignment variable a 
(Brown et al., 1993). 
)),~,~((maxarg~ 1,* ? ?? Mm mmae afeHe ?
       (2) 
where e~ is a string of phrases in the target lan-
guage, f~ is the source language string, 
),~,~( afeHm  are feature functions, and weights 
m? are typically optimized to maximize the scoring 
function (Och, 2003). 
3.4 Feature design  
In Section 3.2 above, we have discussed our lexi-
con-based approach, which leverages lexicon-
based sentiment consistency. Below, we describe 
the specific features we designed for our experi-
ments. For a phrase pair ( ef ~,~ ) or a sentence pair 
(f, e)6, we propose the following four groups of 
consistency features. 
Subjectivity The first group of features is designed 
to check the subjectivity of a phrase or a sentence 
pair (f, e). This set of features examines if the 
source or target side contains sentiment units. As 
the name suggests, these features only capture if 
subjectivity exists, but not if a sentiment is positive, 
negative, or neutral. We include four binary fea-
tures that are triggered in the following condi-
tions?satisfaction of each condition gives the 
corresponding feature a value of 1 and otherwise 0. 
? F1: if neither side of the pair (f, e) contains at 
least one sentiment unit; 
                                                          
6 For simplicity, we hereafter use the same notation (f, e) to 
represent both a phrase pair and a sentence pair, when no con-
fusion arises. 
? F2: if only one side contains sentiment units;  
? F3: if the source side contains sentiment 
units; 
? F4: if the target side contains sentiment units. 
Sentiment polarity The second group of features 
check the sentiment polarity. These features are 
still binary; they check if the polarities of the 
source and target side are the same.  
? F5: if the two sides of the pair (f, e) have the 
same polarity; 
? F6: if at least one side has a neutral senti-
ment; 
? F7: if the polarity is opposite on the two 
sides, i.e., one is positive and one is negative.  
Note that examining the polarity on each side 
can be regarded as a special case of applying Equa-
tion 1 above. For example, examining the positive 
sentiment corresponds to using an indicator func-
tion as the basis function: it takes a value of 1 if 
the lsv-score of a sentiment unit is positive or 0 
otherwise, while the merge function is the logic 
OR function. The subjectivity features above can 
also be thought of similarly. 
Sentiment intensity The third group of features is 
designed to capture the degree of sentiment and 
these features are numerical. We designed two 
types of features in this group.  
Feature F8 measures the difference of the LSV 
scores on the two sides. As shown in Equation (3), 
we use a mean function7  as our merge function 
when computing the lsv-scores with Equation (1), 
where the basis function is simply the identity 
function. 
? ?? ni iulsvnslsv 01 )(1)(
                    (3) 
Feature F9, F10, and F11 are the second type in 
this group of features, which compute the ratio of 
sentiment units on each side and examine their dif-
ference. 
? F8: |)()(|),( 118 elsvflsvefH ??                                 
? F9: |)()(|),(9 elsvflsvefH ?? ??                          
                                                          
7 We studied several different options but found the average 
function is better than others for our translation task here, e.g., 
better than giving more weight to the last unit. 
610
? F10: |)()(|),(10 elsvflsvefH ?? ??                         
? F11: |)()(|),(11 elsvflsvefH ???? ??                             
lsv+(.) calculates the ratio of a positive sentiment 
units in a phrase or a sentence, i.e., the number of 
positive sentiment units divided by the total num-
ber of words of the phrase or the sentence. It corre-
sponds to a special form of Equation 1, in which 
the basis function is an indicator function as dis-
cussed above, and the merge function adds up all 
the counts and normalizes the sum by the length of 
the phrase or the sentence concerned. Similarly, 
lsv-(.) calculates the ratio of negative units and  
lsv+-(.) calculates that for both types of units.  The 
length of sentence here means the number of word 
tokens. We experimented with and without remov-
ing stop words when counting them, and found that 
decision has little impact on the performance. We 
also used the part-of-speech (POS) information in 
the sentiment lexicons to help decide if a word is a 
sentiment word or not, when we extract features; 
i.e., a word is considered to have sentiment only if 
its POS tag also matches what is specified in the 
lexicons8. Using POS tags, however, did not im-
prove our translation performance.  
Negation The fourth group of features checks the 
consistency of negation words on the source and 
target side. Note that negation words have already 
been considered in computing the lsv-scores of 
sentiment units. One motivation is that a negation 
word may appear far from the sentiment word it 
modifies, as mentioned in (Taboada et al., 2011) 
and may be outside the window we used to calcu-
late the lsv-score above. The features here addi-
tionally check the counts of negation words. This 
group of features is binary and triggered by the 
following conditions. 
? F12: if neither side of the pair (f, e) contain 
negation words; 
? F13: if both sides have an odd number of 
negation words or both sides have an even 
number of them; 
? F14:  if both sides have an odd number of 
negation words not appearing outside any 
sentiment units, or if both sides have an even 
number of such negation words; 
                                                          
8 The Stanford POS tagger (Toutanova et al., 2003) was 
used to tag phrase and sentence pairs for this purpose. 
? F15: if both sides have an odd number of 
negation words appearing in all sentiment 
units, or if both sides have an even number 
of such negation words. 
4 Experiments 
4.1 Translation experimental settings 
Experiments were carried out with an in-house 
phrase-based system similar to Moses (Koehn et 
al., 2007).  Each corpus was word-aligned using 
IBM model 2, HMM, and IBM model 4, and the 
phrase table was the union of phrase pairs extract-
ed from these separate alignments, with a length 
limit of 7. The translation model was smoothed in 
both directions with Kneser-Ney smoothing (Chen 
et al., 2011).  We use the hierarchical lexicalized 
reordering model (Galley and Manning, 2008), 
with a distortion limit of 7. Other features include 
lexical weighting in both directions, word count, a 
distance-based RM, a 4-gram LM trained on the 
target side of the parallel data, and a 6-gram Eng-
lish Gigaword LM. The system was tuned with 
batch lattice MIRA (Cherry and Foster, 2012). 
We conducted experiments on NIST Chinese-to-
English translation task. The training data are from 
NIST Open MT 2012. All allowed bilingual corpo-
ra were used to train the translation model and re-
ordering models. There are about 283M target 
word tokens. The development (dev) set comprised 
mainly data from the NIST 2005 test set, and also 
some balanced-genre web-text from NIST training 
data. Evaluation was performed on NIST 2006 and 
2008, which have 1,664 and 1,357 sentences, 
39.7K and 33.7K source words respectively. Four 
references were provided for all dev and test sets. 
4.2 Results  
Our evaluation metric is case-insensitive IBM 
BLEU (Papineni et al., 2002), which performs 
matching of n-grams up to n = 4; we report BLEU 
scores on two test sets NIST06 and NIST08. Fol-
lowing (Koehn, 2004), we use the bootstrap 
resampling test to do significance testing. In Table 
4-6, the sign * and ** denote statistically signifi-
cant gains over the baseline at the p < 0.05 and p < 
0.01 level, respectively. 
 
 
611
 NIST06 NIST08 Avg. 
Baseline 35.1 28.4 31.7 
+feat. group1 35.6** 29.0** 32.3 
+feat. group2 35.3* 28.7* 32.0 
+feat. group3 35.3 28.7* 32.0 
+feat. group4 35.5* 28.8* 32.1 
+feat. group1+2 35.8** 29.1** 32.5 
+feat. group1+2+3 36.1** 29.3** 32.7 
+feat. group1+2+3+4 36.2** 29.4** 32.8 
 
Table 4: BLEU(%) scores on two original test sets for 
different feature combinations. The sign * and ** indi-
cate statistically significant gains over the baseline at 
the p < 0.05 and p < 0.01 level, respectively. 
 
Table 4 summarizes the results of the baseline 
and the results of adding each group of features 
and their combinations. We can see that each indi-
vidual feature group improves the BLEU scores of 
the baseline, and most of these gains are signifi-
cant. Among the feature groups, the largest im-
provement is associated with the first feature 
group, i.e., the subjectivity features, which sug-
gests the significant role of modeling the basic sub-
jectivity. Adding more features results in further 
improvement; the best performance was achieved 
when using all these sentiment consistency fea-
tures, where we observed a 1.1 point improvement 
on the NIST06 set and a 1.0 point improvement on 
the NIST08 set, which yields an overall improve-
ment of about 1.1 BLEU score. 
To further observe the results, we split each of 
the two (i.e., the NIST06 and NIST08) test sets 
into three subsets according to the ratio of senti-
ment words in the reference. We call them low-
sen, mid-sen and high-sen subsets, denoting lower, 
middle, and higher sentiment-word ratios, respec-
tively. The three subsets contain roughly equal 
number of sentences.  Then we merged the two 
low-sen subsets together, and similarly the two 
mid-sen and high-sen subsets together, respective-
ly. Each subset has roughly 1007 sentences. 
 
 low-sen mid-sen high-sen 
baseline 33.4 32.3 29.3 
+all feat. 34.4** 33.5** 30.4** 
improvement 1.0 1.2 1.1 
 
Table 5: BLEU(%) scores on three sub test sets with 
different sentiment ratios. 
 
Table 5 shows the performance of baseline and 
the system with sentiment features (the last system 
of Table 4) on these subsets. First, we can see that 
both systems perform worse as the ratio of senti-
ment words increases. This probably indicates that 
text with more sentiment is harder to translate than 
text with less sentiment. Second, it is interesting 
that the largest improvement is seen on the mid-sen 
sub-set. The larger improvement on the mid-
sen/high-sen subsets than on the low-sen may indi-
cate the usefulness of the proposed features in cap-
turing sentiment information. The lower 
improvement on high-sen than on mid-sen proba-
bly indicates that the high-sen subset is hard any-
way and using simple lexicon-level features is not 
sufficient. 
Sentence-level reranking Above, we have incor-
porated sentiment features into the phrase tables. 
To further confirm the usefulness of the sentiment 
consistency features, we explore their role for sen-
tence-level reranking. To this end, we re-rank 
1000-best hypotheses for each sentence that were 
generated with the baseline system. All the senti-
ment features were recalculated for each hypothe-
sis. We then re-learned the weights for the 
decoding and sentiment features to select the best 
hypothesis. The results are shown in Table 6. We 
can see that sentiment features improve the per-
formance via re-ranking. The improvement is sta-
tistically significant, although the absolute 
improvement is less than that obtained by incorpo-
rating the sentiment features in decoding. Not that 
as widely known, the limited variety of candidates 
in reranking may confine the improvement that 
could be achieved. Better models on the sentence 
level are possible. In addition, we feel that ensur-
ing sentiment and its target to be correctly paired is 
of interest. Note that we have also combined the 
last system in Table 4 with the reranking system 
here; i.e., sentiment consistency was incorporated 
in both ways, but we did not see further improve-
ment, which suggests that the benefit of the senti-
ment features has mainly been captured in the 
phrase tables already. 
 
feature NIST06 NIST08 Avg. 
baseline 35.1 28.4 31.7 
+ all feat.  35.4* 28.9** 32.1 
 
Table 6: BLEU(%) scores on two original test sets on 
sentence-level sentiment features. 
612
Human evaluation We conducted a human evalu-
ation on the output of the baseline and the system 
that incorporates all the proposed sentiment fea-
tures (the last system in Table 4). For this purpose, 
we randomly sampled 250 sentences from the two 
NIST test sets according to the following condi-
tions. First, the selected sentences should contain 
at least one sentiment word?in this evaluation, we 
target the sentences that may convey some senti-
ment. Second, we do not consider sentences short-
er than 5 words or longer than 50 words; or where 
outputs of the baseline system and the system with 
sentiment feature were identical. The 250 selected 
sentences were split into 9 subsets, as we have 9 
human evaluators (none of the authors of this paper 
took part in this experiment). Each subset contains 
26 randomly selected sentences, which are 234 
sentences in total. The other 16 sentences are ran-
domly selected to serve as a common data set: they 
are added to each of the 9 subsets in order to ob-
serve agreements between the 9 annotators. In 
short, each human evaluator was presented with 42 
evaluation samples. Each sample is a tuple contain-
ing the output of the baseline system, that of the 
system considering sentiment, and the reference 
translation. The two automatic translations were 
presented in a random order to the evaluators. 
As in (Callison-Burch et al., 2012), we per-
formed a pairwise comparison of the translations 
produced by the systems. We asked the annotators 
the following two questions Q1 and Q2: 
? Q1(general preference): For any reason, 
which of the two translations do you prefer 
according to the provided references, other-
wise mark ?no preference?? 
? Q2 (sentiment preference):  Does the refer-
ence contains sentiment? If so, in terms of 
the translations of the sentiment, which of 
the two translations do you prefer, otherwise 
mark ?no preference?? 
 
We computed Fleiss?s Kappa (Fleiss, 1971) on 
the common set to measure inter-annotator agree-
ment, 
all? . Then, we excluded one and only one 
annotator at a time to compute i? (Kappa score 
without i-th annotator, i.e., from the other eight). 
Finally, we removed the annotation of the two an-
notators whose answers were most different from 
the others?: i.e., annotators with the biggest 
iall ?? ?  values. As a result, we got a Kappa score 
0.432 on question Q1 and 0.415 on question Q2, 
which both mean moderate agreement. 
 
 base win bsc win equal total 
Translation 58 
(31.86%) 
82 
(45.05%) 
42 
(23.09%) 
182 
Sentiment 30 
(22.39%) 
49 
(36.57%) 
55 
(41.04%) 
134 
 
Table 7: Human evaluation preference for outputs from 
baseline vs. system with sentiment features. 
 
This left 7 files from 7 evaluators. We threw 
away the common set in each file, leaving 182 
pairwise comparisons. Table 6 shows that the eval-
uators preferred the output from the system with 
sentiment features 82 times, the output from the 
baseline system 58 times, and had no preference 
the other 42 times. This indicates that there is a 
human preference for the output from the system 
that incorporated the sentiment features over those 
from the baseline system at the p<0.05 significance 
level (in cases where people prefer one of them). 
For question Q2, the human annotators regarded 48 
sentences as conveying no sentiment according to 
the provided reference, although each of them con-
tains at least one sentiment word (a criterion we 
described above in constructing the evaluation set). 
Among the remaining 134 sentences, the human 
annotators preferred the proposed system 49 times 
and the baseline system 30 times, while they mark 
no-preference 55 times. The result shows a human 
preference for the proposed model that considers 
sentiment features at the p<0.05 significance level 
(in the cases where the evaluators did mark a pref-
erence). 
 
4.3 Examples 
We have also manually examined the translations 
generated by our best model (the last model of Ta-
ble 4, named BSC below) and the baseline model 
(BSL), and we attribute the improvement to two 
main reasons: (1) checking sentiment consistency 
on a phrase pair helps punish low-quality phrase 
pairs caused by word alignment error, (2) such 
consistency checking also improves the sentiment 
of the translation to better match the sentiment of 
the source. 
613
(1) 
 
 
 
Phr. pairs   
REF 
BSL 
BSC 
     ?? ||| talks   vs.    ?? ||| peace talks  
? help the palestinians and the israelis to resume peace talks ? 
? help the israelis and palestinians to resumption of the talks ? 
? help the israelis and palestinians to resume peace talks ? 
(2) 
 
 
 
Phr. pairs 
REF 
BSL 
BSC 
     ?? ||| war    vs.   ?? ||| preparing for  
? the national team is preparing for matches with palestine and Iraq ? 
? the national team 's match with the palestinians and the iraq war ? 
? the national team preparing for the match with the palestinian and iraq ? 
(3) 
 
 
REF 
BSL 
BSC 
? in china we have top-quality people , ever-improving facilities ? 
? we have talents in china , an increasing number of facilities ? 
? we have outstanding talent in china , more and better facilities ? 
(4) 
 
 
REF 
BSL 
BSC 
? continue to strive for that ? 
? continue to struggle ? 
? continue to work hard to achieve ? 
 
Table 8: Examples that show how sentiment helps improve our baseline model. REF is a reference translation, BSL 
stands for baseline model, and BSC (bilingual sentiment consistency) is the last model of Table 4. 
 
In the first two examples of Table 8, the first 
line shows two phrase pairs that are finally chosen 
by the baseline and BSC system, respectively. The 
next three lines correspond to a reference (REF), 
translation from BSL, and that from the BSC sys-
tem. The correct translations of ???? should be 
?peace negotiations? or ?peace talks?, which have 
a positive sentiment, while the word ?talks? 
doesn?t convey sentiment at all. By punishing the 
phrase pair ??? ||| talks?, the BSC model was 
able to generate a better translation. In the second 
example, the correct translation of ???? should 
be ?prepare for?, where neither side conveys sen-
timent. The incorrect phrase pair ??? ||| war? is 
generated from incorrect word alignment. Since 
?war? is a negative word in our sentiment lexicon, 
checking sentiment consistency helps down-weight 
such incorrect translations. Note also that the in-
correct phrase pair ??? ||| war? is not totally irra-
tional, as the literal translation of ??? ? is 
?prepare for war?. 
Similarly, in the third example, ?outstanding tal-
ent? is closer with respect to sentiment to the refer-
ence ?top-quality people? than ?talent? is; ?more 
and better? is closer with respect to sentiment to 
the reference ?ever-improving? than ?an increasing 
number? is. These three examples also help us un-
derstand the benefit of the subjectivity features 
discussed in Section 3.4. In the fourth example, 
?work hard to achieve? has a positive sentiment, 
same as ?strive?, while ?struggle? is negative. We 
can see that the BSC model is able to preserve the 
original sentiment better (the 9 human evaluators 
who were involved in our human evaluation (Sec-
tion 4.3) all agreed with this). 
5 Conclusions and future work 
We explore lexicon-based sentiment consistency 
for statistical machine translation. By incorporating 
lexicon-based subjectivity, polarity, intensity, and 
negation features into the phrase-pair translation 
model, we observed a 1.1-point improvement of 
BLEU score on NIST Chinese-to-English transla-
tion. Among the four individual groups of features, 
subjectivity consistency yields the largest im-
provement. The usefulness of the sentiment fea-
tures has also been confirmed when they are used 
for re-ranking, for which we observed a 0.4-point 
improvement on the BLEU score. In addition, hu-
man evaluation shows the preference of the human 
subjects towards the translations generated by the 
proposed model, in terms of both the general trans-
lation quality and the sentiment conveyed. 
In the paper, we propose a lexicon-based ap-
proach to the problem. It is possible to employ 
more complicated models. For example, with the 
involvement of proper sentiment-annotated data, if 
available, one may train a better sentiment-analysis 
model even for the often-ungrammatical phrase 
pairs or sentence candidates. Another direction we 
feel interesting is ensuring that sentiment and its 
target are not only better translated but also better 
paired, i.e., their semantic relation is preserved. 
This is likely to need further syntactic or semantic 
analysis at the sentence level, and the semantic role 
labeling work reviewed in Section 2 is relevant. 
614
References 
C. Banea, R. Mihalcea, J. Wiebe and S. Hassan. 2008.  
Multilingual subjectivity analysis using machine  
translation. In Proc. of EMNLP. 
E. M. Benzinger. 1971. Intensifiers in current English. 
PhD. Thesis. University of Florida. 
P. F. Brown, S. Della Pietra, V. Della J. Pietra, and R. 
Mercer. 1993. The mathematics of Machine Transla-
tion: Parameter estimation. Computational Linguis-
tics, 19(2): 263-312. 
C. Callison-Burch, P. Koehn, C. Monz, R. Soricut, and 
L. Specia. 2012. Findings of the 2012 Workshop on 
Statistical Machine Translation. In Proc. of WMT. 
D. Chiang. 2005. A hierarchical phrase-based model for 
statistical machine translation. In Proc. of ACL, 263?
270. 
B. Chen, G. Foster, and R. Kuhn. 2010. Bilingual Sense 
Similarity for Statistical Machine Translation. In 
Proc. of ACL, 834-843.  
B. Chen, R. Kuhn, G. Foster, and H. Johnson. 2011. 
Unpacking and transforming feature functions: New 
ways to smooth phrase tables. In Proc. of MT Sum-
mit. 
C. Cherry and G. Foster. 2012. Batch tuning strategies 
for statistical machine translation. In Proc. of 
NAACL. 
J. L. Fleiss. 1971. Measuring nominal scale agreement 
among many raters. Psychological Bulletin, 76(5): 
378?382. 
M. Galley and C. D. Manning. 2008. A simple and ef-
fective hierarchical phrase reordering model. In Proc. 
of EMNLP: 848?856. 
V. Hatzivassiloglou and K. McKeown. 1997. Predicting 
the semantic orientation of adjectives. In Proc. of 
EACL: 174-181.  
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Proc. of EMNLP: 
388?395. 
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. 
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, 
R. Zens, C. Dyer, O. Bojar, A. Constantin and E. 
Herbst. 2007. Moses: Open Source Toolkit for Statis-
tical Machine Translation. In Proc. of ACL, 177-180. 
J. Li, P. Resnik and H. Daume III. 2013. Modeling Syn-
tactic and Semantic Structures in Hierarchical 
Phrase-based Translation. In Proc. of NAACL, 540-
549.  
D. Liu and D. Gildea. 2010. Semantic role features for 
machine translation. In Proc. of COLING,  716?724. 
R. Mihalcea, C. Banea and J. Wiebe. 2007. Learning  
multilingual subjective language via cross-lingual  
projections. In Proc. of ACL. 
F. J. Och and H. Ney. 2002. Discriminative Training 
and Maximum Entropy Models for Statistical Ma-
chine Translation. In Proc. of ACL. 
F. J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. of ACL. 
C. E. Osgood, G. J. Suci, and  P. H. Tannenbaum. 1957. 
The measurement of meaning. University of Illinois 
Press. 
B. Pang, L. Lee, S. Vaithyanathan. 2002. Thumbs up?: 
sentiment classification using machine learning tech-
niques. In Proc. of EMNLP, 79-86.  
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. 
Bleu: a method for automatic evaluation of ma-chine 
translation. In Proc. of ACL, 311?318. 
M. Taboada, M. Tofiloski, J. Brooke, K. Voll, and M. 
Stede. 2011. Lexicon-Based Methods for Sentiment 
Analysis. Computational Linguistics. 37(2): 267-307. 
K. Toutanova, D. Klein, C. Manning, and Y. Singer. 
2003. Feature-Rich Part-of-Speech Tagging with a 
Cyclic Dependency Network. In Proc. of HLT-
NAACL, 252-259. 
P. Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proc. of ACL, 417-424. 
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM based 
word alignment in statistical translation. In Proc. of 
COLING. 
X. Wan. 2009. Co-Training for Cross-Lingual Senti-
ment Classification. In proc. of ACL, 235-243.  
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recogniz-
ing Contextual Polarity in Phrase-Level Sentiment 
Analysis. In Proc. of EMNLP. 
D. Wu and P. Fung. 2009. Semantic Roles for SMT: A 
Hybrid Two-Pass Model. In Proc. of NAACL, 13-16.  
L. Xu and H. Lin. 2007. Ontology-Driven Affective 
Chinese Text Analysis and Evaluation Method. In 
Lecture Notes in Computer Science Vol. 4738, 723-
724, Springer. 
C. Zhang, P. Liu, Z. Zhu, and M. Fang. 2012. A Senti-
ment Analysis Method Based on a Polarity Lexicon. 
Journal of Shangdong University (Natural Science). 
47(3): 47-50. 
615
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 304?313,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
An Empirical Study on the Effect of Negation Words on Sentiment
Xiaodan Zhu, Hongyu Guo, Saif Mohammad and Svetlana Kiritchenko
National Research Council Canada
1200 Montreal Road
Ottawa, K1A 0R6, ON, Canada
{Xiaodan.Zhu,Hongyu.Guo,Saif.Mohammad,Svetlana.Kiritchenko}
@nrc-cnrc.gc.ca
Abstract
Negation words, such as no and not, play
a fundamental role in modifying sentiment
of textual expressions. We will refer to a
negation word as the negator and the text
span within the scope of the negator as the
argument. Commonly used heuristics to
estimate the sentiment of negated expres-
sions rely simply on the sentiment of ar-
gument (and not on the negator or the ar-
gument itself). We use a sentiment tree-
bank to show that these existing heuristics
are poor estimators of sentiment. We then
modify these heuristics to be dependent on
the negators and show that this improves
prediction. Next, we evaluate a recently
proposed composition model (Socher et
al., 2013) that relies on both the negator
and the argument. This model learns the
syntax and semantics of the negator?s ar-
gument with a recursive neural network.
We show that this approach performs bet-
ter than those mentioned above. In ad-
dition, we explicitly incorporate the prior
sentiment of the argument and observe that
this information can help reduce fitting er-
rors.
1 Introduction
Morante and Sporleder (2012) define negation to
be ?a grammatical category that allows the chang-
ing of the truth value of a proposition?. Nega-
tion is often expressed through the use of nega-
tive signals or negators?words like isn?t and never,
and it can significantly affect the sentiment of
its scope. Understanding the impact of negation
on sentiment is essential in automatic analysis of
sentiment. The literature contains interesting re-
search attempting to model and understand the
behavior (reviewed in Section 2). For example,
Figure 1: Effect of a list of common negators
in modifying sentiment values in Stanford Senti-
ment Treebank. The x-axis is s(~w), and y-axis
is s(w
n
, ~w). Each dot in the figure corresponds
to a text span being modified by (composed with)
a negator in the treebank. The red diagonal line
corresponds to the sentiment-reversing hypothesis
that simply reverses the sign of sentiment values.
a simple yet influential hypothesis posits that a
negator reverses the sign of the sentiment value
of the modified text (Polanyi and Zaenen, 2004;
Kennedy and Inkpen, 2006). The shifting hypoth-
esis (Taboada et al, 2011), however, assumes that
negators change sentiment values by a constant
amount. In this paper, we refer to a negation word
as the negator (e.g., isn?t), a text span being mod-
ified by and composed with a negator as the ar-
gument (e.g., very good), and entire phrase (e.g.,
isn?t very good) as the negated phrase.
The recently available Stanford Sentiment Tree-
bank (Socher et al, 2013) renders manually anno-
tated, real-valued sentiment scores for all phrases
in parse trees. This corpus provides us with the
data to further understand the quantitative behav-
ior of negators, as the effect of negators can now
be studied with arguments of rich syntactic and se-
mantic variety. Figure 1 illustrates the effect of a
common list of negators on sentiment as observed
304
on the Stanford Sentiment Treebank.1 Each dot in
the figure corresponds to a negated phrase in the
treebank. The x-axis is the sentiment score of its
argument s(~w) and y-axis the sentiment score of
the entire negated phrase s(w
n
, ~w).
We can see that the reversing assumption (the
red diagonal line) does capture some regularity of
human perception, but rather roughly. Moreover,
the figure shows that same or similar s(~w) scores
(x-axis) can correspond to very different s(w
n
, ~w)
scores (y-axis), which, to some degree, suggests
the potentially complicated behavior of negators.2
This paper describes a quantitative study of
the effect of a list of frequent negators on sen-
timent. We regard the negators? behavior as an
underlying function embedded in annotated data;
we aim to model this function from different as-
pects. By examining sentiment compositions of
negators and arguments, we model the quantita-
tive behavior of negators in changing sentiment.
That is, given a negated phrase (e.g., isn?t very
good) and the sentiment score of its argument
(e.g., s(?very good??) = 0.5), we focus on un-
derstanding the negator?s quantitative behavior in
yielding the sentiment score of the negated phrase
s(?isn
?
t very good
??
).
We first evaluate the modeling capabilities of
two influential heuristics and show that they cap-
ture only very limited regularity of negators? ef-
fect. We then extend the models to be dependent
on the negators and demonstrate that such a sim-
ple extension can significantly improve the per-
formance of fitting to the human annotated data.
Next, we evaluate a recently proposed composi-
tion model (Socher, 2013) that relies on both the
negator and the argument. This model learns the
syntax and semantics of the negator?s argument
with a recursive neural network. This approach
performs significantly better than those mentioned
above. In addition, we explicitly incorporate the
prior sentiment of the argument and observe that
this information helps reduce fitting errors.
1The sentiment values have been linearly rescaled from
the original range [0, 1] to [-0.5, 0.5]; in the figure a negative
or positive value corresponds to a negative or a positive sen-
timent respectively; zero means neutral. The negator list will
be discussed later in the paper.
2Similar distribution is observed in other data such as
Tweets (Kiritchenko et al, 2014).
2 Related work
Automatic sentiment analysis The expression of
sentiment is an integral component of human lan-
guage. In written text, sentiment is conveyed with
word senses and their composition, and in speech
also via prosody such as pitch (Mairesse et al,
2012). Early work on automatic sentiment anal-
ysis includes the widely cited work of (Hatzivas-
siloglou and McKeown, 1997; Pang et al, 2002;
Turney, 2002), among others. Since then, there has
been an explosion of research addressing various
aspects of the problem, including detecting sub-
jectivity, rating and classifying sentiment, label-
ing sentiment-related semantic roles (e.g., target
of sentiment), and visualizing sentiment (see sur-
veys by Pang and Lee (2008) and Liu and Zhang
(2012)).
Negation modeling Negation is a general gram-
matical category pertaining to the changing of the
truth values of propositions; negation modeling is
not limited to sentiment. For example, paraphrase
and contradiction detection systems rely on detect-
ing negated expressions and opposites (Harabagiu
et al, 2006). In general, a negated expression and
the opposite of the expression may or may not con-
vey the same meaning. For example, not alive has
the same meaning as dead, however, not tall does
not always mean short. Some automatic methods
to detect opposites were proposed by Hatzivas-
siloglou and McKeown (1997) and Mohammad et
al. (2013).
Negation modeling for sentiment An early yet
influential reversing assumption conjectures that a
negator reverses the sign of the sentiment value
of the modified text (Polanyi and Zaenen, 2004;
Kennedy and Inkpen, 2006), e.g., from +0.5 to -
0.5, or vice versa. A different hypothesis, called
the shifting hypothesis in this paper, assumes that
negators change the sentiment values by a con-
stant amount (Taboada et al, 2011; Liu and Sen-
eff, 2009). Other approaches to negation modeling
have been discussed in (Jia et al, 2009; Wiegand
et al, 2010; Lapponi et al, 2012; Benamara et al,
2012).
In the process of semantic composition, the ef-
fect of negators could depend on the syntax and
semantics of the text spans they modify. The ap-
proaches of modeling this include bag-of-word-
based models. For example, in the work of
(Kennedy and Inkpen, 2006), a feature not good
will be created if the word good is encountered
305
within a predefined range after a negator.
There exist different ways of incorporating
more complicated syntactic and semantic infor-
mation. Much recent work considers sentiment
analysis from a semantic-composition perspec-
tive (Moilanen and Pulman, 2007; Choi and
Cardie, 2008; Socher et al, 2012; Socher et al,
2013), which achieved the state-of-the-art perfor-
mance. Moilanen and Pulman (2007) used a col-
lection of hand-written compositional rules to as-
sign sentiment values to different granularities of
text spans. Choi and Cardie (2008) proposed a
learning-based framework. The more recent work
of (Socher et al, 2012; Socher et al, 2013) pro-
posed models based on recursive neural networks
that do not rely on any heuristic rules. Such mod-
els work in a bottom-up fashion over the parse
tree of a sentence to infer the sentiment label of
the sentence as a composition of the sentiment ex-
pressed by its constituting parts. The approach
leverages a principled method, the forward and
backward propagation, to learn a vector represen-
tation to optimize the system performance. In
principle neural network is able to fit very compli-
cated functions (Mitchell, 1997), and in this paper,
we adapt the state-of-the-art approach described in
(Socher et al, 2013) to help understand the behav-
ior of negators specifically.
3 Negation models based on heuristics
We begin with previously proposed methods that
leverage heuristics to model the behavior of nega-
tors. We then propose to extend them to consider
lexical information of the negators themselves.
3.1 Non-lexicalized assumptions and
modeling
In previous research, some influential, widely
adopted assumptions posit the effect of negators
to be independent of both the specific negators and
the semantics and syntax of the arguments. In this
paper, we call a model based on such assumptions
a non-lexicalized model. In general, we can sim-
ply define this category of models in Equation 1.
That is, the model parameters are only based on
the sentiment value of the arguments.
s(w
n
, ~w)
def
= f(s(~w)) (1)
3.1.1 Reversing hypothesis
A typical model falling into this category is the
reversing hypothesis discussed in Section 2, where
a negator simply reverses the sentiment score s(~w)
to be ?s(~w); i.e., f(s(~w)) = ?s(~w).
3.1.2 Shifting hypothesis
Basic shifting Similarly, a shifting based model
depends on s(~w) only, which can be written as:
f(s(~w)) = s(~w) ? sign(s(~w)) ? C (2)
where sign(.) is the standard sign function
which determines if the constant C should be
added to or deducted from s(w
n
): the constant is
added to a negative s(~w) but deducted from a pos-
itive one.
Polarity-based shifting As will be shown in our
experiments, negators can have different shifting
power when modifying a positive or a negative
phrase. Thus, we explore the use of two different
constants for these two situations, i.e., f(s(~w)) =
s(~w)?sign(s(~w))?C(sign(s(~w))). The constant
C now can take one of two possible values. We
will show that this simple modification improves
the fitting performance statistically significantly.
Note also that instead of determining these con-
stants by human intuition, we use the training data
to find the constants in all shifting-based models
as well as for the parameters in other models.
3.2 Simple lexicalized assumptions
The above negation hypotheses rely on s(~w). As
intuitively shown in Figure 1, the capability of the
non-lexicalized heuristics might be limited. Fur-
ther semantic or syntactic information from either
the negators or the phrases they modify could be
helpful. The most straightforward way of expand-
ing the non-lexicalized heuristics is probably to
make the models to be dependent on the negators.
s(w
n
, ~w)
def
= f(w
n
, s(~w)) (3)
Negator-based shifting We can simply extend the
basic shifting model above to consider the lexi-
cal information of negators: f(s(~w)) = s(~w) ?
sign(s(~w)) ?C(w
n
). That is, each negator has its
own C . We call this model negator-based shift-
ing. We will show that this model also statistically
significantly outperforms the basic shifting with-
out overfitting, although the number of parameters
have increased.
306
Combined shifting We further combine the
negator-based shifting and polarity-based shift-
ing above: f(s(~w)) = s(~w) ? sign(s(~w)) ?
C(w
n
, sign(s(~w))). This shifting model is
based on negators and the polarity of the text
they modify: constants can be different for each
negator-polarity pair. The number of parameters
in this model is the multiplication of number
of negators by two (the number of sentiment
polarities). This model further improves the fitting
performance on the test data.
4 Semantics-enriched modeling
Negators can interact with arguments in complex
ways. Figure 1 shows the distribution of the ef-
fect of negators on sentiment without considering
further semantics of the arguments. The question
then is that whether and how much incorporating
further syntax and semantic information can help
better fit or predict the negation effect. Above, we
have considered the semantics of the negators. Be-
low, we further make the models to be dependent
on the arguments. This can be written as:
s(w
n
, ~w)
def
= f(w
n
, s(~w), r(~w)) (4)
In the formula, r(~w) is a certain type of repre-
sentation for the argument ~w and it models the se-
mantics or/and syntax of the argument. There ex-
ist different ways of implementing r(~w). We con-
sider two models in this study: one drops s(~w) in
Equation 4 and directly models f(w
n
, r(~w)). That
is, the non-uniform information shown in Figure 1
is not directly modeled. The other takes into ac-
count s(~w) too.
For the former, we adopt the recursive neu-
ral tensor network (RNTN) proposed recently by
Socher et al (2013), which has showed to achieve
the state-of-the-art performance in sentiment anal-
ysis. For the latter, we propose a prior sentiment-
enriched tensor network (PSTN) to take into ac-
count the prior sentiment of the argument s(~w).
4.1 RNTN: Recursive neural tensor network
A recursive neural tensor network (RNTN) is
a specific form of feed-forward neural network
based on syntactic (phrasal-structure) parse tree
to conduct compositional sentiment analysis. For
completeness, we briefly review it here. More de-
tails can be found in (Socher et al, 2013).
As shown in the black portion of Figure 2, each
instance of RNTN corresponds to a binary parse
Figure 2: Prior sentiment-enriched tensor network
(PSTN) model for sentiment analysis.
tree of a given sentence. Each node of the parse
tree is a fixed-length vector that encodes composi-
tional semantics and syntax, which can be used to
predict the sentiment of this node. The vector of a
node, say p
2
in Figure 2, is computed from the d-
dimensional vectors of its two children, namely a
and p
1
(a, p
1
? R
d?1
), with a non-linear function:
p
2
= tanh(
[
a
p
1
]
T
V
[1:d]
[
a
p
1
]
+ W
[
a
p
1
]
) (5)
where, W ? Rd?(d+d) and V ? R(d+d)?(d+d)?d
are the matrix and tensor for the composition func-
tion. A major difference of RNTN from the con-
ventional recursive neural network (RRN) (Socher
et al, 2012) is the use of the tensor V in order
to directly capture the multiplicative interaction of
two input vectors, although the matrix W implic-
itly captures the nonlinear interaction between the
input vectors. The training of RNTN uses conven-
tional forward-backward propagation.
4.2 PSTN: Prior sentiment-enriched tensor
network
The non-uniform distribution in Figure 1 has
showed certain correlations between the sentiment
values of s(w
n
, ~w) and s(~w), and such informa-
tion has been leveraged in the models discussed in
Section 3. We intend to devise a model that imple-
ments Equation 4. It bridges between the models
we have discussed above that use either s(~w) or
r(~w).
We extend RNTN to directly consider the senti-
ment information of arguments. Consider the node
p
2
in Figure 2. When calculating its vector, we
aim to directly engage the sentiment information
of its right child, i.e., the argument. To this end,
we make use of the sentiment class information of
307
p1
, noted as psen
1
. As a result, the vector of p
2
is
calculated as follows:
p
2
= tanh(
[
a
p
1
]
T
V
[1:d]
[
a
p
1
]
+ W
[
a
p
1
]
(6)
+
[
a
p
sen
1
]
T
V
sen
[1:d]
[
a
p
sen
1
]
+ W
sen
[
a
p
sen
1
]
)
As shown in Equation 6, for the node vector
p
1
? R
d?1
, we employ a matrix, namely W sen ?
R
d?(d+m) and a tensor, V sen ? R(d+m)?(d+m)?d,
aiming at explicitly capturing the interplays be-
tween the sentiment class of p
1
, denoted as psen
1
(?
R
m?1), and the negator a. Here, we assume the
sentiment task has m classes. Following the idea
of Wilson et al (2005), we regard the sentiment of
p
1
as a prior sentiment as it has not been affected
by the specific context (negators), so we denote
our method as prior sentiment-enriched tensor net-
work (PSTN). In Figure 2, the red portion shows
the added components of PSTN.
Note that depending on different purposes, psen
1
can take the value of the automatically predicted
sentiment distribution obtained in forward propa-
gation, the gold sentiment annotation of node p
1
,
or even other normalized prior sentiment value or
confidence score from external sources (e.g., sen-
timent lexicons or external training data). This
is actually an interesting place to extend the cur-
rent recursive neural network to consider extrinsic
knowledge. However, in our current study, we fo-
cus on exploring the behavior of negators. As we
have discussed above, we will use the human an-
notated sentiment for the arguments, same as in
the models discussed in Section 3.
With the new matrix and tensor, we then have
? = (V, V
sen
,W,W
sen
,W
label
, L) as the PSTN
model?s parameters. Here, L denotes the vector
representations of the word dictionary.
4.2.1 Inference and Learning
Inference and learning in PSTN follow a forward-
backward propagation process similar to that in
(Socher et al, 2013), and for completeness, we
depict the details as follows. To train the model,
one first needs to calculate the predicted sentiment
distribution for each node:
p
sen
i
= W
label
p
i
, p
sen
i
? R
m?1
and then compute the posterior probability over
the m labels:
y
i
= softmax(psen
i
)
During learning, following the method used by
the RNTN model in (Socher et al, 2013), PSTN
also aims to minimize the cross-entropy error be-
tween the predicted distribution yi ? Rm?1 at
node i and the target distribution ti ? Rm?1 at that
node. That is, the error for a sentence is calculated
as:
E(?) =
?
i
?
j
t
i
j
logyi
j
+ ? ???
2 (7)
where, ? represents the regularization hyperpa-
rameters, and j ? m denotes the j-th element of
the multinomial target distribution.
To minimize E(?), the gradient of the objec-
tive function with respect to each of the param-
eters in ? is calculated efficiently via backprop-
agation through structure, as proposed by Goller
and Kchler (1996). Specifically, we first compute
the prediction errors in all tree nodes bottom-up.
After this forward process, we then calculate the
derivatives of the softmax classifiers at each node
in the tree in a top-down fashion. We will discuss
the gradient computation for the V sen and W sen
in detail next. Note that the gradient calculations
for the V,W,W label, L are the same as that of pre-
sented in (Socher et al, 2013).
In the backpropogation process of the training,
each node (except the root node) in the tree car-
ries two kinds of errors: the local softmax error
and the error passing down from its parent node.
During the derivative computation, the two errors
will be summed up as the complement incoming
error for the node. We denote the complete incom-
ing error and the softmax error vector for node i
as ?i,com ? Rd?1 and ?i,s ? Rd?1, respectively.
With this notation, the error for the root node p
2
can be formulated as follows.
?
p
2
,com
= ?
p
2
,s
= (W
T
(y
p
2
? t
p
2
)) ? f
?
([a; p
1
]) (8)
where ? is the Hadamard product between the two
vectors and f ? is the element-wise derivative of
f = tanh. With the results from Equation 8, we
then can calculate the derivatives for the W sen at
node p
2
using the following equation:
?E
p
2
W
sen
= ?
p
2
,com
([a; p
sen
1
])
T
Similarly, for the derivative of each slice k(k =
308
1, . . . , d) of the V sen tensor, we have the follow-
ing:
?E
p
2
V
sen
[k]
= ?
p
2
,com
k
[
a
p
sen
1
] [
a
p
sen
1
]
T
Now, let?s form the equations for computing the
error for the two children of the p
2
node. The dif-
ference for the error at p
2
and its two children is
that the error for the latter will need to compute the
error message passing down from p
2
. We denote
the error passing down as ?p2,down, where the left
child and the right child of p
2
take the 1st and 2nd
half of the error ?p2,down, namely ?p2,down[1 : d]
and ?p2,down[d + 1 : 2d], respectively. Follow-
ing this notation, we have the error message for
the two children of p
2
, provided that we have the
?
p
2
,down:
?
p
1
,com
= ?
p
1
,s
+ ?
p
2
,down
[d + 1 : 2d]
= (W
T
(y
p
1
? t
p
1
)) ? f
?
([b; c])
+ ?
p
2
,down
[d + 1 : 2d]
The incoming error message of node a can be
calculated similarly. Finally, we can finish the
above equations with the following formula for
computing ?p2,down:
?
p
2
,down
= (W
T
?
p
2
,com
) ? f
?
([a; p
1
]) + ?
tensor
where
?
tensor
= [?
V
[1 : d] + ?
V
sen
[1 : d], ?
V
[d + 1 : 2d]]
=
d
?
k=1
?
p
2
,com
k
(V
[k]
+ (V
[k]
)
T
)? f
?
([a; p
1
])[1 : d]
+
d
?
k=1
?
p
2
,com
k
(V
sen
[k]
+ (V
sen
[k]
)
T
)? f
?
([a; p
sen
1
])[1 : d]
+
d
?
k=1
?
p
2
,com
k
(V
[k]
+ (V
[k]
)
T
)? f
?
([a; p
1
])[d + 1 : 2d]
After the models are trained, they are applied to
predict the sentiment of the test data. The orig-
inal RNTN and the PSTN predict 5-class senti-
ment for each negated phrase; we map the out-
put to real-valued scores based on the scale that
Socher et al (2013) used to map real-valued senti-
ment scores to sentiment categories. Specifically,
we conduct the mapping with the formula: preal
i
=
y
i
? [0.1 0.3 0.5 0.7 0.9]; i.e., we calculate the dot
product of the posterior probability yi and the scal-
ing vector. For example, if yi = [0.5 0.5 0 0 0],
meaning this phrase has a 0.5 probability to be
in the first category (strong negative) and 0.5 for
the second category (weak negative), the resulting
p
real
i
will be 0.2 (0.5*0.1+0.5*0.3).
5 Experiment set-up
Data As described earlier, the Stanford Sentiment
Treebank (Socher et al, 2013) has manually anno-
tated, real-valued sentiment values for all phrases
in parse trees. This provides us with the training
and evaluation data to study the effect of negators
with syntax and semantics of different complex-
ity in a natural setting. The data contain around
11,800 sentences from movie reviews that were
originally collected by Pang and Lee (2005). The
sentences were parsed with the Stanford parser
(Klein and Manning, 2003). The phrases at all
tree nodes were manually annotated with one of 25
sentiment values that uniformly span between the
positive and negative poles. The values are nor-
malized to the range of [0, 1].
In this paper, we use a list of most frequent
negators that include the words not, no, never, and
their combinations with auxiliaries (e.g., didn?t).
We search these negators in the Stanford Senti-
ment Treebank and normalize the same negators to
a single form; e.g., ?is n?t?, ?isn?t?, and ?is not?
are all normalized to ?is not?. Each occurrence of
a negator and the phrase it is directly composed
with in the treebank, i.e., ?w
n
, ~w?, is considered
a data point in our study. In total, we collected
2,261 pairs, including 1,845 training and 416 test
cases. The split of training and test data is same as
specified in (Socher et al, 2013).
Evaluation metrics We use the mean absolute er-
ror (MAE) to evaluate the models, which mea-
sures the averaged absolute offsets between the
predicted sentiment values and the gold stan-
dard. More specifically, MAE is calculated as:
MAE =
1
N
?
?w
n
, ~w?
|(s?(w
n
, ~w) ? s(w
n
, ~w))|,
where s?(w
n
, ~w) denotes the gold sentiment value
and s(w
n
, ~w) the predicted one for the pair
?w
n
, ~w?, and N is the total number of test in-
stances. Note that mean square error (MSE) is an-
other widely used measure for regression, but it is
less intuitive for out task here.
6 Experimental results
Overall regression performance Table 1 shows
the overall fitting performance of all models. The
first row of the table is a random baseline, which
309
simply guesses the sentiment value for each test
case randomly in the range [0,1]. The table shows
that the basic reversing and shifting heuristics do
capture negators? behavior to some degree, as their
MAE scores are higher than that of the baseline.
Making the basic shifting model to be dependent
on the negators (model 4) reduces the prediction
error significantly as compared with the error of
the basic shifting (model 3). The same is true
for the polarity-based shifting (model 5), reflect-
ing that the roles of negators are different when
modifying positive and negative phrases. Merging
these two models yields additional improvement
(model 6).
Assumptions MAE
Baseline
(1) Random 0.2796
Non-lexicalized
(2) Reversing 0.1480*
(3) Basic shifting 0.1452*
Simple-lexicalized
(4) Negator-based shifting 0.1415?
(5) Polarity-based shifting 0.1417?
(6) Combined shifting 0.1387?
Semantics-enriched
(7) RNTN 0.1097**
(8) PSTN 0.1062??
Table 1: Mean absolute errors (MAE) of fitting
different models to Stanford Sentiment Treebank.
Models marked with an asterisk (*) are statisti-
cally significantly better than the random baseline.
Models with a dagger sign (?) significantly outper-
form model (3). Double asterisks ** indicates a
statistically significantly different from model (6),
and the model with the double dagger ??is signif-
icantly better than model (7). One-tailed paired
t-test with a 95% significance level is used here.
Furthermore, modeling the syntax and seman-
tics with the state-of-the-art recursive neural net-
work (model 7 and 8) can dramatically improve
the performance over model 6. The PSTN model,
which takes into account the human-annotated
prior sentiment of arguments, performs the best.
This could suggest that additional external knowl-
edge, e.g., that from human-built resources or au-
tomatically learned from other data (e.g., as in
(Kiritchenko et al, 2014)), including sentiment
that cannot be inferred from its constituent expres-
sions, might be incorporated to benefit the current
is
_n
ev
e
r
w
ill
_n
ot
is
_n
ot
do
es
_n
ot
ba
re
ly
w
a
s
_
n
o
t
c
o
u
ld
_n
ot
n
o
t
di
d_
no
t
u
n
lik
e
ly
do
_n
ot
c
a
n
_
n
o
t
n
o
ha
s_
no
t
s
u
pe
rfi
ci
al
w
o
u
ld
_n
ot
s
ho
ul
d_
no
t
0.05
0.10
0.15
0.20
0.25
0.30
Figure 3: Effect of different negators in shifting
sentiment values.
neural-network-based models as prior knowledge.
Note that the two neural network based models
incorporate the syntax and semantics by represent-
ing each node with a vector. One may consider
that a straightforward way of considering the se-
mantics of the modified phrases is simply memo-
rizing them. For example, if a phrase very good
modified by a negator not appears in the train-
ing and test data, the system can simply memorize
the sentiment score of not very good in training
and use this score at testing. When incorporating
this memorizing strategy into model (6), we ob-
served a MAE score of 0.1222. It?s not surprising
that memorizing the phrases has some benefit, but
such matching relies on the exact reoccurrences of
phrases. Note that this is a special case of what the
neural network based models can model.
Discriminating negators The results in Table 1
has demonstrated the benefit of discriminating
negators. To understand this further, we plot in
Figure 3 the behavior of different negators: the
x-axis is a subset of our negators and the y-axis
denotes absolute shifting in sentiment values. For
example, we can see that the negator ?is never?
on average shifts the sentiment of the arguments
by 0.26, which is a significant change considering
the range of sentiment value is [0, 1]. For each
negator, a 95% confidence interval is shown by
the boxes in the figure, which is calculated with
the bootstrapping resampling method. We can ob-
serve statistically significant differences of shift-
ing abilities between many negator pairs such as
that between ?is never? and ?do not? as well as
between ?does not? and ?can not?.
Figure 3 also includes three diminishers (the
310
is
_n
ot
(n
n)
is
_n
ot
(n
p)
do
es
_n
ot
(n
n)
do
es
_n
ot
(n
p)
n
o
t(n
n)
n
o
t(n
p)
do
_n
ot
(n
n)
do
_n
ot
(n
p)
n
o
(n
n)
n
o
(n
p)
0.15
0.20
0.25
0.30
Figure 4: The behavior of individual negators in
negated negative (nn) and negated positive (np)
context.
white bars), i.e., barely, unlikely, and superficial.
By following (Kennedy and Inkpen, 2006), we ex-
tracted 319 diminishers (also called understate-
ment or downtoners) from General Inquirer3. We
calculated their shifting power in the same man-
ner as for the negators and found three diminish-
ers having shifting capability in the shifting range
of these negators. This shows that the boundary
between negators and diminishers can by fuzzy.
In general, we argue that one should always con-
sider modeling negators individually in a senti-
ment analysis system. Alternatively, if the model-
ing has to be done in groups, one should consider
clustering valence shifters by their shifting abili-
ties in training or external data.
Figure 4 shows the shifting capacity of negators
when they modify positive (blue boxes) or nega-
tive phrases (red boxes). The figure includes five
most frequently used negators found in the sen-
timent treebank. Four of them have significantly
different shifting power when composed with pos-
itive or negative phrases, which can explain why
the polarity-based shifting model achieves im-
provement over the basic shifting model.
Modeling syntax and semantics We have seen
above that modeling syntax and semantics through
the-state-of-the-art neural networks help improve
the fitting performance. Below, we take a closer
look at the fitting errors made at different depths
of the sentiment treebank. The depth here is de-
fined as the longest distance between the root of a
negator-phrase pair ?w
n
, ~w? and their descendant
3http://www.wjh.harvard.edu/ inquirer/
Figure 5: Errors made at different depths in the
sentiment tree bank.
leafs. Negators appearing at deeper levels of the
tree tend to have more complicated syntax and se-
mantics. In Figure 5, the x-axis corresponds to
different depths and y-axis is the mean absolute
errors (MAE).
The figure shows that both RNTN and PSTN
perform much better at all depths than the model
6 in Table 1. When the depths are within 4,
the RNTN performs very well and the (human
annotated) prior sentiment of arguments used
in PSTN does not bring additional improvement
over RNTN. PSTN outperforms RNTN at greater
depths, where the syntax and semantics are more
complicated and harder to model. The errors made
by model 6 is bumpy, as the model considers
no semantics and hence its errors are not depen-
dent on the depths. On the other hand, the er-
rors of RNTN and PSTN monotonically increase
with depths, indicating the increase in the task dif-
ficulty.
7 Conclusions
Negation plays a fundamental role in modifying
sentiment. In the process of semantic compo-
sition, the impact of negators is complicated by
the syntax and semantics of the text spans they
modify. This paper provides a comprehensive
and quantitative study of the behavior of negators
through a unified view of fitting human annota-
tion. We first measure the modeling capabilities of
two influential heuristics on a sentiment treebank
and find that they capture some effect of negation;
however, extending these non-lexicalized models
to be dependent on the negators improves the per-
311
formance statistically significantly. The detailed
analysis reveals the differences in the behavior
among negators, and we argue that they should al-
ways be modeled separately. We further make the
models to be dependent on the text being modi-
fied by negators, through adaptation of a state-of-
the-art recursive neural network to incorporate the
syntax and semantics of the arguments; we dis-
cover this further reduces fitting errors.
References
Farah Benamara, Baptiste Chardon, Yannick Mathieu,
Vladimir Popescu, and Nicholas Asher. 2012. How
do negation and modality impact on opinions? In
Proceedings of the ACL-2012 Workshop on Extra-
Propositional Aspects of Meaning in Computational
Linguistics, pages 10?18, Jeju, Republic of Korea.
Yejin Choi and Claire Cardie. 2008. Learning with
compositional semantics as structural inference for
subsentential sentiment analysis. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?08, pages 793?801,
Honolulu, Hawaii.
Christoph Goller and Andreas Kchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In In Proc. of
the ICNN-96, pages 347?352, Bochum, Germany.
IEEE.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast and contradiction in text
processing. In AAAI, volume 6, pages 755?762.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 8th Conference of Euro-
pean Chapter of the Association for Computational
Linguistics, EACL ?97, pages 174?181, Madrid,
Spain.
Lifeng Jia, Clement Yu, and Weiyi Meng. 2009. The
effect of negation on sentiment analysis and retrieval
effectiveness. In Proceedings of the 18th ACM Con-
ference on Information and Knowledge Manage-
ment, CIKM ?09, pages 1827?1830, Hong Kong,
China. ACM.
Alistair Kennedy and Diana Inkpen. 2006. Senti-
ment classification of movie reviews using contex-
tual valence shifters. Computational Intelligence,
22(2):110?125.
Svetlana Kiritchenko, Xiaodan Zhu, and Saif Moham-
mad. 2014. Sentiment analysis of short informal
texts. (to appear) Journal of Artificial Intelligence
Research.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430, Sapporo, Japan. Association for Computational
Linguistics.
Emanuele Lapponi, Jonathon Read, and Lilja Ovre-
lid. 2012. Representing and resolving negation
for sentiment analysis. In Jilles Vreeken, Charles
Ling, Mohammed Javeed Zaki, Arno Siebes, Jef-
frey Xu Yu, Bart Goethals, Geoffrey I. Webb, and
Xindong Wu, editors, ICDM Workshops, pages 687?
692. IEEE Computer Society.
Jingjing Liu and Stephanie Seneff. 2009. Review sen-
timent scoring via a parse-and-paraphrase paradigm.
In EMNLP, pages 161?169, Singapore.
Bing Liu and Lei Zhang. 2012. A survey of opin-
ion mining and sentiment analysis. In Charu C. Ag-
garwal and ChengXiang Zhai, editors, Mining Text
Data, pages 415?463. Springer US.
Franc?ois Mairesse, Joseph Polifroni, and Giuseppe
Di Fabbrizio. 2012. Can prosody inform sentiment
analysis? experiments on short spoken reviews. In
ICASSP, pages 5093?5096, Kyoto, Japan.
Tom M Mitchell. 1997. Machine learning. 1997. Burr
Ridge, IL: McGraw Hill, 45.
Saif M. Mohammad, Bonnie J. Dorr, Graeme Hirst, and
Peter D. Turney. 2013. Computing lexical contrast.
Computational Linguistics, 39(3):555?590.
Karo Moilanen and Stephen Pulman. 2007. Senti-
ment composition. In Proceedings of RANLP 2007,
Borovets, Bulgaria.
Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special is-
sue. Computational linguistics, 38(2):223?260.
Bo Pang and Lillian Lee. 2005. Seeing stars: Ex-
ploiting class relationships for sentiment categoriza-
tion with respect to rating scales. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics, ACL ?05, pages 115?124.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1?2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79?86, Philadelphia, USA.
Livia Polanyi and Annie Zaenen. 2004. Contextual
valence shifters. In Exploring Attitude and Affect in
Text: Theories and Applications (AAAI Spring Sym-
posium Series).
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
312
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?12,
Jeju, Korea. Association for Computational Linguis-
tics.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?13, Seattle, USA. Association for Compu-
tational Linguistics.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional Linguistics, 37(2):267?307.
Peter Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In ACL, pages 417?424, Philadel-
phia, USA.
Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andre?s Montoyo. 2010. A
survey on the role of negation in sentiment analysis.
In Proceedings of the Workshop on Negation and
Speculation in Natural Language Processing, NeSp-
NLP ?10, pages 60?68, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ?05, pages 347?354, Stroudsburg, PA, USA.
Association for Computational Linguistics.
313
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 321?327, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
NRC-Canada: Building the State-of-the-Art in
Sentiment Analysis of Tweets
Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu
National Research Council Canada
Ottawa, Ontario, Canada K1A 0R6
{saif.mohammad,svetlana.kiritchenko,xiaodan.zhu}@nrc-cnrc.gc.ca
Abstract
In this paper, we describe how we created two
state-of-the-art SVM classifiers, one to de-
tect the sentiment of messages such as tweets
and SMS (message-level task) and one to de-
tect the sentiment of a term within a message
(term-level task). Among submissions from
44 teams in a competition, our submissions
stood first in both tasks on tweets, obtaining
an F-score of 69.02 in the message-level task
and 88.93 in the term-level task. We imple-
mented a variety of surface-form, semantic,
and sentiment features. We also generated
two large word?sentiment association lexi-
cons, one from tweets with sentiment-word
hashtags, and one from tweets with emoticons.
In the message-level task, the lexicon-based
features provided a gain of 5 F-score points
over all others. Both of our systems can be
replicated using freely available resources.1
1 Introduction
Hundreds of millions of people around the world ac-
tively use microblogging websites such as Twitter.
Thus there is tremendous interest in sentiment anal-
ysis of tweets across a variety of domains such as
commerce (Jansen et al, 2009), health (Chew and
Eysenbach, 2010; Salathe? and Khandelwal, 2011),
and disaster management (Verma et al, 2011; Man-
del et al, 2012).
1The three authors contributed equally to this paper. Svet-
lana Kiritchenko developed the system for the message-level
task, Xiaodan Zhu developed the system for the term-level task,
and Saif Mohammad led the overall effort, co-ordinated both
tasks, and contributed to feature development.
In this paper, we describe how we created two
state-of-the-art SVM classifiers, one to detect the
sentiment of messages such as tweets and SMS
(message-level task) and one to detect the sentiment
of a term within a message (term-level task). The
sentiment can be one out of three possibilities: posi-
tive, negative, or neutral. We developed these classi-
fiers to participate in an international competition or-
ganized by the Conference on Semantic Evaluation
Exercises (SemEval-2013) (Wilson et al, 2013).2
The organizers created and shared sentiment-labeled
tweets for training, development, and testing. The
distributions of the labels in the different datasets is
shown in Table 1. The competition, officially re-
ferred to as Task 2: Sentiment Analysis in Twitter,
had 44 teams (34 for the message-level task and 23
for the term-level task). Our submissions stood first
in both tasks, obtaining a macro-averaged F-score
of 69.02 in the message-level task and 88.93 in the
term-level task.
The task organizers also provided a second test
dataset, composed of Short Message Service (SMS)
messages (no training data of SMS messages was
provided). We applied our classifiers on the SMS
test set without any further tuning. Nonetheless, the
classifiers still obtained the first position in identify-
ing sentiment of SMS messages (F-score of 68.46)
and second position in detecting the sentiment of
terms within SMS messages (F-score of 88.00, only
0.39 points behind the first ranked system).
We implemented a number of surface-form, se-
mantic, and sentiment features. We also gener-
ated two large word?sentiment association lexicons,
2http://www.cs.york.ac.uk/semeval-2013/task2
321
Table 1: Class distributions in the training set (Train), de-
velopment set (Dev) and testing set (Test). The Train set
was accessed through tweet ids and a download script.
However, not all tweets were accessible. Below is the
number of Train examples we were able to download.
The Dev and Test sets were provided by FTP.
Dataset Positive Negative Neutral Total
Tweets
Message-level task:
Train 3,045 (37%) 1,209 (15%) 4,004 (48%) 8,258
Dev 575 (35%) 340 (20%) 739 (45%) 1,654
Test 1,572 (41%) 601 (16%) 1,640 (43%) 3,813
Term-level task:
Train 4,831 (62%) 2,540 (33%) 385 (5%) 7,756
Dev 648 (57%) 430 (38%) 57 (5%) 1,135
Test 2,734 (62%) 1,541 (35%) 160 (3%) 4,435
SMS
Message-level task:
Test 492 (23%) 394 (19%) 1,208 (58%) 2,094
Term-level task:
Test 1,071 (46%) 1,104 (47%) 159 (7%) 2,334
one from tweets with sentiment-word hashtags, and
one from tweets with emoticons. The automatically
generated lexicons were particularly useful. In the
message-level task for tweets, they alone provided a
gain of more than 5 F-score points over and above
that obtained using all other features. The lexicons
are made freely available.3
2 Sentiment Lexicons
Sentiment lexicons are lists of words with associa-
tions to positive and negative sentiments.
2.1 Existing, Automatically Created Sentiment
Lexicons
The manually created lexicons we used include the
NRC Emotion Lexicon (Mohammad and Turney,
2010; Mohammad and Yang, 2011) (about 14,000
words), the MPQA Lexicon (Wilson et al, 2005)
(about 8,000 words), and the Bing Liu Lexicon (Hu
and Liu, 2004) (about 6,800 words).
2.2 New, Tweet-Specific, Automatically
Generated Sentiment Lexicons
2.2.1 NRC Hashtag Sentiment Lexicon
Certain words in tweets are specially marked with
a hashtag (#) to indicate the topic or sentiment. Mo-
3www.purl.com/net/sentimentoftweets
hammad (2012) showed that hashtagged emotion
words such as joy, sadness, angry, and surprised are
good indicators that the tweet as a whole (even with-
out the hashtagged emotion word) is expressing the
same emotion. We adapted that idea to create a large
corpus of positive and negative tweets.
We polled the Twitter API every four hours from
April to December 2012 in search of tweets with ei-
ther a positive word hashtag or a negative word hash-
tag. A collection of 78 seed words closely related
to positive and negative such as #good, #excellent,
#bad, and #terrible were used (32 positive and 36
negative). These terms were chosen from entries for
positive and negative in the Roget?s Thesaurus.
A set of 775,000 tweets were used to generate a
large word?sentiment association lexicon. A tweet
was considered positive if it had one of the 32 pos-
itive hashtagged seed words, and negative if it had
one of the 36 negative hashtagged seed words. The
association score for a term w was calculated from
these pseudo-labeled tweets as shown below:
score(w) = PMI(w, positive)? PMI(w, negative)
(1)
where PMI stands for pointwise mutual informa-
tion. A positive score indicates association with pos-
itive sentiment, whereas a negative score indicates
association with negative sentiment. The magni-
tude is indicative of the degree of association. The
final lexicon, which we will refer to as the NRC
Hashtag Sentiment Lexicon has entries for 54,129
unigrams and 316,531 bigrams. Entries were also
generated for unigram?unigram, unigram?bigram,
and bigram?bigram pairs that were not necessarily
contiguous in the tweets corpus. Pairs with cer-
tain punctuations, ?@? symbols, and some function
words were removed. The lexicon has entries for
308,808 non-contiguous pairs.
2.2.2 Sentiment140 Lexicon
The sentiment140 corpus (Go et al, 2009) is a
collection of 1.6 million tweets that contain pos-
itive and negative emoticons. The tweets are la-
beled positive or negative according to the emoti-
con. We generated a sentiment lexicon from this
corpus in the same manner as described above (Sec-
tion 2.2.1). This lexicon has entries for 62,468
unigrams, 677,698 bigrams, and 480,010 non-
contiguous pairs.
322
3 Task: Automatically Detecting the
Sentiment of a Message
The objective of this task is to determine whether a
given message is positive, negative, or neutral.
3.1 Classifier and features
We trained a Support Vector Machine (SVM) (Fan
et al, 2008) on the training data provided. SVM
is a state-of-the-art learning algorithm proved to be
effective on text categorization tasks and robust on
large feature spaces. The linear kernel and the value
for the parameter C=0.005 were chosen by cross-
validation on the training data.
We normalized all URLs to http://someurl and all
userids to @someuser. We tokenized and part-of-
speech tagged the tweets with the Carnegie Mellon
University (CMU) Twitter NLP tool (Gimpel et al,
2011). Each tweet was represented as a feature vec-
tor made up of the following groups of features:
? word ngrams: presence or absence of contigu-
ous sequences of 1, 2, 3, and 4 tokens; non-
contiguous ngrams (ngrams with one token re-
placed by *);
? character ngrams: presence or absence of con-
tiguous sequences of 3, 4, and 5 characters;
? all-caps: the number of words with all charac-
ters in upper case;
? POS: the number of occurrences of each part-
of-speech tag;
? hashtags: the number of hashtags;
? lexicons: the following sets of features were
generated for each of the three manually con-
structed sentiment lexicons (NRC Emotion
Lexicon, MPQA, Bing Liu Lexicon) and for
each of the two automatically constructed lex-
icons (Hashtag Sentiment Lexicon and Senti-
ment140 Lexicon). Separate feature sets were
produced for unigrams, bigrams, and non-
contiguous pairs. The lexicon features were
created for all tokens in the tweet, for each part-
of-speech tag, for hashtags, and for all-caps to-
kens. For each token w and emotion or po-
larity p, we used the sentiment/emotion score
score(w, p) to determine:
? total count of tokens in the tweet with
score(w, p) > 0;
? total score =
?
w?tweet score(w, p);
? the maximal score =
maxw?tweetscore(w, p);
? the score of the last token in the tweet with
score(w, p) > 0;
? punctuation:
? the number of contiguous sequences of
exclamation marks, question marks, and
both exclamation and question marks;
? whether the last token contains an excla-
mation or question mark;
? emoticons: The polarity of an emoticon was
determined with a regular expression adopted
from Christopher Potts? tokenizing script:4
? presence or absence of positive and nega-
tive emoticons at any position in the tweet;
? whether the last token is a positive or neg-
ative emoticon;
? elongated words: the number of words with one
character repeated more than two times, for ex-
ample, ?soooo?;
? clusters: The CMU pos-tagging tool provides
the token clusters produced with the Brown
clustering algorithm on 56 million English-
language tweets. These 1,000 clusters serve as
alternative representation of tweet content, re-
ducing the sparcity of the token space.
? the presence or absence of tokens from
each of the 1000 clusters;
? negation: the number of negated contexts. Fol-
lowing (Pang et al, 2002), we defined a negated
context as a segment of a tweet that starts
with a negation word (e.g., no, shouldn?t) and
ends with one of the punctuation marks: ?,?,
?.?, ?:?, ?;?, ?!?, ???. A negated context af-
fects the ngram and lexicon features: we add
? NEG? suffix to each word following the nega-
tion word (?perfect? becomes ?perfect NEG?).
The ? NEG? suffix is also added to polarity and
emotion features (?POLARITY positive? be-
comes ?POLARITY positive NEG?). The list
of negation words was adopted from Christo-
pher Potts? sentiment tutorial.5
4http://sentiment.christopherpotts.net/tokenizing.html
5http://sentiment.christopherpotts.net/lingstruc.html
323
3.2 Experiments
We trained the SVM classifier on the set of 9,912
annotated tweets (8,258 in the training set and 1,654
in the development set). We applied the model to the
test set of 3,813 unseen tweets. The same model was
applied unchanged to the other test set of 2,094 SMS
messages as well. The bottom-line score used by the
task organizers was the macro-averaged F-score of
the positive and negative classes. The results ob-
tained by our system on the training set (ten-fold
cross-validation), development set (when trained on
the training set), and test sets (when trained on the
combined set of tweets in the training and devel-
opment sets) are shown in Table 2. The table also
shows baseline results obtained by a majority clas-
sifier that always predicts the most frequent class as
output. Since the bottom-line F-score is based only
on the F-scores of positive and negative classes (and
not on neutral), the majority baseline chose the most
frequent class among positive and negative, which
in this case was the positive class. We also show
baseline results obtained using an SVM and unigram
features alone. Our system (SVM and all features)
obtained a macro-averaged F-score of 69.02 on the
tweet set and 68.46 on the SMS set. In the SemEval-
2013 competition, our submission ranked first on
both datasets. There were 48 submissions from 34
teams for this task.
Table 3 shows the results of the ablation experi-
ments where we repeat the same classification pro-
cess but remove one feature group at a time. The
most influential features for both datasets turned out
to be the sentiment lexicon features: they provided
gains of more than 8.5%. It is interesting to note
that tweets benefited mostly from the automatic sen-
timent lexicons (NRC Hashtag Lexicon and the Sen-
timent140 Lexicon) whereas the SMS set benefited
more from the manual lexicons (MPQA, NRC Emo-
tion Lexicon, Bing Liu Lexicon). Among the au-
tomatic lexicons, both the Hashtag Sentiment Lex-
icon and the Sentiment140 Lexicon contributed to
roughly the same amount of improvement in perfor-
mance on the tweet set.
The second most important feature group for
the message-level task was that of ngrams (word
and character ngrams). Expectedly, the impact of
ngrams on the SMS dataset was less extensive since
Table 2: Message-level Task: The macro-averaged F-
scores on different datasets.
Classifier Tweets SMS
Training set: Majority 26.94 -
SVM-all 67.20 -
Development set: Majority 26.85 -
SVM-all 68.72 -
Test set: Majority 29.19 19.03
SVM-unigrams 39.61 39.29
SVM-all 69.02 68.46
Table 3: Message-level Task: The macro-averaged F-
scores obtained on the test sets with one of the feature
groups removed. The number in the brackets is the dif-
ference with the all features score. The biggest drops are
shown in bold.
Experiment Tweets SMS
all features 69.02 68.46
all - lexicons 60.42 (-8.60) 59.73 (-8.73)
all - manual lex. 67.45 (-1.57) 65.64 (-2.82)
all - auto. lex. 63.78 (-5.24) 67.12 (-1.34)
all - Senti140 lex. 65.25 (-3.77) 67.33 (-1.13)
all - Hashtag lex. 65.22 (-3.80) 70.28 (1.82)
all - ngrams 61.77 (-7.25) 67.27 (-1.19)
all - word ngrams 64.64 (-4.38) 66.56 (-1.9)
all - char. ngrams 67.10 (-1.92) 68.94 (0.48)
all - negation 67.20 (-1.82) 66.22 (-2.24)
all - POS 68.38 (-0.64) 67.07 (-1.39)
all - clusters 69.01 (-0.01) 68.10 (-0.36)
all - encodings (elongated, emoticons, punctuations,
all-caps, hashtags) 69.16 (0.14) 68.28 (-0.18)
the classifier model was trained only on tweets.
Attention to negations improved performance on
both datasets. Removing the sentiment encoding
features like hashtags, emoticons, and elongated
words, had almost no impact on performance, but
this is probably because the discriminating informa-
tion in them was also captured by some other fea-
tures such as character and word ngrams.
4 Task: Automatically Detecting the
Sentiment of a Term in a Message
The objective of this task is to detect whether a term
(a word or phrase) within a message conveys a pos-
itive, negative, or neutral sentiment. Note that the
same term may express different sentiments in dif-
ferent contexts.
324
4.1 Classifier and features
We trained an SVM using the LibSVM package
(Chang and Lin, 2011) and a linear kernel. In ten-
fold cross-validation over the training data, the lin-
ear kernel outperformed other kernels implemented
in LibSVM as well as a maximum-entropy classi-
fier. Our model leverages a variety of features, as
described below:
? word ngrams:
? presence or absence of unigrams, bigrams,
and the full word string of a target term;
? leading and ending unigrams and bigrams;
? character ngrams: presence or absence of two-
and three-character prefixes and suffixes of all
the words in a target term (note that the target
term may be a multi-word sequence);
? elongated words: presence or absence of elon-
gated words (e.g., ?sooo?);
? emoticons: the numbers and categories of
emoticons that a term contains6;
? punctuation: presence or absence of punctua-
tion sequences such as ??!? and ?!!!?;
? upper case:
? whether all the words in the target start
with an upper case letter followed by
lower case letters;
? whether the target words are all in upper-
case (to capture a potential named entity);
? stopwords: whether a term contains only stop-
words. If so, separate features indicate whether
there are 1, 2, 3, or more stop-words;
? lengths:
? the length of a target term (number of
words);
? the average length of words (number of
characters) in a term;
? a binary feature indicating whether a term
contains long words;
6http://en.wikipedia.org/wiki/List of emoticons
? negation: similar to those described for the
message-level task. Whenever a negation word
was found immediately before the target or
within the target, the polarities of all tokens af-
ter the negation term were flipped;
? position: whether a term is at the beginning,
end, or another position;
? sentiment lexicons: we used automatically cre-
ated lexicons (NRC Hashtag Sentiment Lexi-
con, Sentiment140 Lexicon) as well as manu-
ally created lexicons (NRC Emotion Lexicon,
MPQA, Bing Liu Lexicon).
? total count of tokens in the target term
with sentiment score greater than 0;
? the sum of the sentiment scores for all to-
kens in the target;
? the maximal sentiment score;
? the non-zero sentiment score of the last to-
ken in the target;
? term splitting: when a term contains a hash-
tag made of multiple words (e.g., #biggest-
daythisyear), we split the hashtag into compo-
nent words;
? others:
? whether a term contains a Twitter user
name;
? whether a term contains a URL.
The above features were extracted from target
terms as well as from the rest of the message (the
context). For unigrams and bigrams, we used four
words on either side of the target as the context. The
window size was chosen through experiments on the
development set.
4.2 Experiments
We trained an SVM classifier on the 8,891 annotated
terms in tweets (7,756 terms in the training set and
1,135 terms in the development set). We applied the
model to 4,435 terms in the tweets test set. The same
model was applied unchanged to the other test set of
2,334 terms in unseen SMS messages as well. The
bottom-line score used by the task organizers was
the macro-averaged F-score of the positive and neg-
ative classes.
325
The results on the training set (ten-fold cross-
validation), the development set (trained on the
training set), and the test sets (trained on the com-
bined set of tweets in the training and development
sets) are shown in Table 4. The table also shows
baseline results obtained by a majority classifier that
always predicts the most frequent class as output,
and an additional baseline result obtained using an
SVM and unigram features alone. Our submission
obtained a macro-averaged F-score of 88.93 on the
tweet set and was ranked first among 29 submissions
from 23 participating teams. Even with no tuning
specific to SMS data, our SMS submission still ob-
tained second rank with an F-score of 88.00. The
score of the first ranking system on the SMS set was
88.39. A post-competition bug-fix in the bigram fea-
tures resulted in a small improvement: F-score of
89.10 on the tweets set and 88.34 on the SMS set.
Note that the performance is significantly higher
in the term-level task than in the message-level task.
This is largely because of the ngram features (see
unigram baselines in Tables 2 and 4). We analyzed
the labeled data provided to determine why ngrams
performed so strongly in this task. We found that the
percentage of test tokens already seen within train-
ing data targets was 85.1%. Further, the average ra-
tio of instances pertaining to the most dominant po-
larity of a target term to the total number of instances
of that target term was 0.808.
Table 5 presents the ablation F-scores. Observe
that the ngram features were the most useful. Note
also that removing just the word ngram features or
just the character ngram features results in only a
small drop in performance. This indicates that the
two feature groups capture similar information.
The sentiment lexicon features are the next most
useful group?removing them leads to a drop in F-
score of 3.95 points for the tweets set and 4.64 for
the SMS set. Modeling negation improves the F-
score by 0.72 points on the tweets set and 1.57 points
on the SMS set.
The last two rows in Table 5 show the results ob-
tained when the features are extracted only from the
target (and not from its context) and when they are
extracted only from the context of the target (and
not from the target itself). Observe that even though
the context may influence the polarity of the tar-
get, using target features alone is substantially more
Table 4: Term-level Task: The macro-averaged F-scores
on the datasets. The official scores of our submission are
shown in bold. SVM-all* shows results after a bug fix.
Classifier Tweets SMS
Training set: Majority 38.38 -
SVM-all 86.80 -
Development set: Majority 36.34 -
SVM-all 86.49 -
Test set: Majority 38.13 32.11
SVM-unigrams 80.28 78.71
official SVM-all 88.93 88.00
SVM-all* 89.10 88.34
Table 5: Term-level Task: The F-scores obtained on the
test sets with one of the feature groups removed. The
number in brackets is the difference with the all features
score. The biggest drops are shown in bold.
Experiment Tweets SMS
all features 89.10 88.34
all - ngrams 83.86 (-5.24) 80.49 (-7.85)
all - word ngrams 88.38 (-0.72) 87.37 (-0.97)
all - char. ngrams 89.01 (-0.09) 87.31 (-1.03)
all - lexicons 85.15 (-3.95) 83.70 (-4.64)
all - manual lex. 87.69 (-1.41) 86.84 (-1.5)
all - auto lex. 88.24 (-0.86) 86.65 (-1.69)
all - negation 88.38 (-0.72) 86.77 (-1.57)
all - stopwords 89.17 (0.07) 88.30 (-0.04)
all - encodings (elongated words, emoticons, punctns.,
uppercase) 89.16 (0.06) 88.39 (0.05)
all - target 72.97 (-16.13) 68.96 (-19.38)
all - context 85.02 (-4.08) 85.93 (-2.41)
useful than using context features alone. Nonethe-
less, adding context features improves the F-scores
by roughly 2 to 4 points.
5 Conclusions
We created two state-of-the-art SVM classifiers, one
to detect the sentiment of messages and one to de-
tect the sentiment of a term within a message. Our
submissions on tweet data stood first in both these
subtasks of the SemEval-2013 competition ?Detect-
ing Sentiment in Twitter?. We implemented a variety
of features based on surface form and lexical cate-
gories. The sentiment lexicon features (both manu-
ally created and automatically generated) along with
ngram features (both word and character ngrams)
led to the most gain in performance.
326
Acknowledgments
We thank Colin Cherry for providing his SVM code
and for helpful discussions.
References
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27:1?27:27.
Cynthia Chew and Gunther Eysenbach. 2010. Pan-
demics in the Age of Twitter: Content Analysis of
Tweets during the 2009 H1N1 Outbreak. PLoS ONE,
5(11):e14118+, November.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
Lin C.-J. 2008. LIBLINEAR: A Library for Large
Linear Classification. Journal of Machine Learning
Research, 9:1871?1874.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-Speech Tagging for
Twitter: Annotation, Features, and Experiments. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter
Sentiment Classification using Distant Supervision. In
Final Projects from CS224N for Spring 2008/2009 at
The Stanford Natural Language Processing Group.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages 168?
177, New York, NY, USA. ACM.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Twitter power: Tweets as electronic
word of mouth. Journal of the American Society for
Information Science and Technology, 60(11):2169?
2188.
Benjamin Mandel, Aron Culotta, John Boulahanis,
Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue.
2012. A demographic analysis of online sentiment
during hurricane irene. In Proceedings of the Second
Workshop on Language in Social Media, LSM ?12,
pages 27?36, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions Evoked by Common Words and Phrases: Using
Mechanical Turk to Create an Emotion Lexicon. In
Proceedings of the NAACL-HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, LA, California.
Saif Mohammad and Tony Yang. 2011. Tracking Sen-
timent in Mail: How Genders Differ on Emotional
Axes. In Proceedings of the 2nd Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis (WASSA 2.011), pages 70?79, Portland, Ore-
gon. Association for Computational Linguistics.
Saif Mohammad. 2012. #Emotional Tweets. In Pro-
ceedings of the First Joint Conference on Lexical
and Computational Semantics (*SEM), pages 246?
255, Montre?al, Canada. Association for Computa-
tional Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment Classification Using
Machine Learning Techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79?86, Philadelphia, PA.
Marcel Salathe? and Shashank Khandelwal. 2011. As-
sessing vaccination sentiments with online social me-
dia: Implications for infectious disease dynamics and
control. PLoS Computational Biology, 7(10).
Sudha Verma, Sarah Vieweg, William Corvey, Leysia
Palen, James Martin, Martha Palmer, Aaron Schram,
and Kenneth Anderson. 2011. Natural language pro-
cessing to the rescue? extracting ?situational aware-
ness? tweets during mass emergency. In International
AAAI Conference on Weblogs and Social Media.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the confer-
ence on Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ?05,
pages 347?354, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter. In
Proceedings of the International Workshop on Seman-
tic Evaluation, SemEval ?13, Atlanta, Georgia, USA,
June.
327
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 437?442,
Dublin, Ireland, August 23-24, 2014.
NRC-Canada-2014: Detecting Aspects and Sentiment
in Customer Reviews
Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and Saif M. Mohammad
National Research Council Canada
1200 Montreal Rd., Ottawa, ON, Canada
{Svetlana.Kiritchenko, Xiaodan.Zhu, Colin.Cherry, Saif.Mohammad}
@nrc-cnrc.gc.ca
Abstract
Reviews depict sentiments of customers
towards various aspects of a product or
service. Some of these aspects can be
grouped into coarser aspect categories.
SemEval-2014 had a shared task (Task 4)
on aspect-level sentiment analysis, with
over 30 teams participated. In this pa-
per, we describe our submissions, which
stood first in detecting aspect categories,
first in detecting sentiment towards aspect
categories, third in detecting aspect terms,
and first and second in detecting senti-
ment towards aspect terms in the laptop
and restaurant domains, respectively.
1 Introduction
Automatically identifying sentiment expressed in
text has a number of applications, including track-
ing sentiment towards products, movies, politi-
cians, etc.; improving customer relation models;
and detecting happiness and well-being. In many
applications, it is important to associate sentiment
with a particular entity or an aspect of an entity.
For example, in reviews, customers might express
different sentiment towards various aspects of a
product or service they have availed. Consider:
The lasagna was great, but the service
was a bit slow.
The review is for a restaurant, and we can gather
from it that the customer has a positive sentiment
towards the lasagna they serve, but a negative sen-
timent towards the service.
The SemEval-2014 Task 4 (Aspect Based Sen-
timent Analysis) is a shared task where given a
customer review, automatic systems are to deter-
mine aspect terms, aspect categories, and senti-
ment towards these aspect terms and categories.
An aspect term is defined to be an explicit men-
tion of a feature or component of the target prod-
uct or service. The example sentence above has
Restaurants Laptops
Term T-Sent. Cat. C-Sent. Term T-Sent.
3 2 1 1 3 1
Table 1: Rank obtained by NRC-Canada in vari-
ous subtasks of SemEval-2014 Task 4.
the aspect term lasagna. Similar aspect terms can
be grouped into aspect categories. For example,
lasagna and other food items can be grouped into
the aspect category of ?food?. In Task 4, customer
reviews are provided for two domains: restaurants
and laptops. A fixed set of five aspect categories
is defined for the restaurant domain: food, ser-
vice, price, ambiance, and anecdotes. Automatic
systems are to determine if any of those aspect
categories are described in a review. The exam-
ple sentence above describes the aspect categories
of food (positive sentiment) and service (negative
sentiment). For the laptop reviews, there is no as-
pect category detection subtask. Further details of
the task and data can be found in the task descrip-
tion paper (Pontiki et al., 2014).
We present an in-house sequence tagger to de-
tect aspect terms and supervised classifiers to de-
tect aspect categories, sentiment towards aspect
terms, and sentiment towards aspect categories. A
summary of the ranks obtained by our submissions
to the shared task is provided in Table 1.
2 Lexical Resources
2.1 Unlabeled Reviews Corpora
Apart from the training data provided for Task 4,
we compiled large corpora of reviews for restau-
rants and laptops that were not labeled for aspect
terms, aspect categories, or sentiment. We gen-
erated lexicons from these corpora and used them
as a source of additional features in our machine
learning systems.
437
Yelp restaurant reviews corpus: The Yelp
Phoenix Academic Dataset
1
contains customer re-
views posted on the Yelp website. The businesses
for which the reviews are posted are classified into
over 500 categories. Further, many of the busi-
nesses are assigned multiple business categories.
We identified all food-related business categories
(58 categories) that were grouped along with the
category ?restaurant? and extracted all customer
reviews for these categories. We will refer to this
corpus of 183,935 reviews as the Yelp restaurant
reviews corpus.
Amazon laptop reviews corpus: McAuley and
Leskovec (2013) collected reviews posted on
Amazon.com from June 1995 to March 2013. A
subset of this corpus is marked as reviews for elec-
tronic products. We extracted from this subset all
reviews that mention either laptop or notebook.
We will refer to this collection of 124,712 reviews
as the Amazon laptop reviews corpus.
Both the Yelp and the Amazon reviews have one
to five star ratings associated with each review. We
treated the one- and two-star reviews as negative
reviews, and the four- and five-star reviews as pos-
itive reviews.
2.2 Lexicons
Sentiment Lexicons: From the Yelp restaurant
reviews corpus, we automatically created an in-
domain sentiment lexicon for restaurants. Follow-
ing Turney and Littman (2003) and Mohammad
et al. (2013), we calculated a sentiment score for
each term w in the corpus:
score (w) = PMI (w , pos)?PMI (w ,neg) (1)
where pos denotes positive reviews and neg de-
notes negative reviews. PMI stands for pointwise
mutual information:
PMI (w , pos) = log
2
freq (w , pos) ?N
freq (w) ? freq (pos)
(2)
where freq (w, pos) is the number of times a term
w occurs in positive reviews, freq (w) is the to-
tal frequency of term w in the corpus, freq (pos)
is the total number of tokens in positive reviews,
and N is the total number of tokens in the cor-
pus. PMI (w ,neg) was calculated in a similar
way. Since PMI is known to be a poor estima-
tor of association for low-frequency events, we ig-
nored terms that occurred less than five times in
each (positive and negative) groups of reviews.
1
http://www.yelp.com/dataset_challenge
A positive sentiment score indicates a greater
overall association with positive sentiment,
whereas a negative score indicates a greater asso-
ciation with negative sentiment. The magnitude is
indicative of the degree of association.
Negation words (e.g., not, never) can signifi-
cantly affect the sentiment of an expression (Zhu
et al., 2014). Therefore, when generating the sen-
timent lexicons we distinguished terms appearing
in negated contexts (defined as text spans between
a negation word and a punctuation mark) and af-
firmative (non-negated) contexts. The sentiment
scores were then calculated separately for the two
types of contexts. For example, the term good in
affirmative contexts has a sentiment score of 1.2
whereas the same term in negated contexts has a
score of -1.4. We built two lexicons, Yelp Restau-
rant Sentiment AffLex and Yelp Restaurant Senti-
ment NegLex, as described in (Kiritchenko et al.,
2014).
Similarly, we generated in-domain sentiment
lexicons from the Amazon laptop reviews corpus.
In addition, we employed existing out-of-
domain sentiment lexicons: (1) large-coverage au-
tomatic tweet sentiment lexicons, Hashtag Sen-
timent lexicons and Sentiment140 lexicons (Kir-
itchenko et al., 2014), and (2) three manually cre-
ated sentiment lexicons, NRC Emotion Lexicon
(Mohammad and Turney, 2010), Bing Liu?s Lex-
icon (Hu and Liu, 2004), and the MPQA Subjec-
tivity Lexicon (Wilson et al., 2005).
Yelp Restaurant Word?Aspect Association
Lexicon: The Yelp restaurant reviews corpus was
also used to generate a lexicon of terms associated
with the aspect categories of food, price, service,
ambiance, and anecdotes. Each sentence of the
corpus was labeled with zero, one, or more of the
five aspect categories by our aspect category clas-
sification system (described in Section 5). Then,
for each term w and each category c an associa-
tion score was calculated as follows:
score (w , c) = PMI (w , c)? PMI (w ,?c) (3)
2.3 Word Clusters
Word clusters can provide an alternative represen-
tation of text, significantly reducing the sparsity
of the token space. Using Brown clustering algo-
rithm (Brown et al., 1992), we generated 1,000
word clusters from the Yelp restaurant reviews
corpus. Additionally, we used publicly available
438
word clusters generated from 56 million English-
language tweets (Owoputi et al., 2013).
3 Subtask 1: Aspect Term Extraction
The objective of this subtask is to detect aspect
terms in sentences. We approached this problem
using in-house entity-recognition software, very
similar to the system used by de Bruijn et al.
(2011) to detect medical concepts. First, sen-
tences were tokenized to split away punctuation,
and then the token sequence was tagged using a
semi-Markov tagger (Sarawagi and Cohen, 2004).
The tagger had two possible tags: O for outside,
and T for aspect term, where an aspect term could
tag a phrase of up to 5 consecutive tokens. The
tagger was trained using the structured Passive-
Aggressive (PA) algorithm with a maximum step-
size of C = 1 (Crammer et al., 2006).
Our features can be divided into two categories:
emission and transition features. Emission fea-
tures couple the tag sequence y to the input w.
Most of these work on the token level, and con-
join features of each token with the tag covering
that token. If a token is the first or last token cov-
ered by a tag, then we produce a second copy of
each of its features to indicate its special position.
Let w
i
be the token being tagged; its token fea-
ture templates are: token-identity within a win-
dow (w
i?2
. . . w
i+2
), lower-cased token-identity
within a window (lc(w
i?2
) . . . lc(w
i+2
)), and pre-
fixes and suffixes of w
i
(up to 3 characters in
length). There are only two phrase-level emission
feature templates: the cased and uncased identity
of the entire phrase covered by a tag, which al-
low the system to memorize complete terms such
as, ?getting a table? or ?fish and chips.? Transi-
tion features couple tags with tags. Let the cur-
rent tag be y
j
. Its transition feature templates are
short n-grams of tag identities: y
j
; y
j
, y
j?1
; and
y
j
, y
j?1
, y
j?2
.
During development, we experimented with the
training algorithm, trying both PA and the simpler
structured perceptron (Collins, 2002). We also
added the lowercased back-off features. In Ta-
ble 2, we re-test these design decisions on the test
set, revealing that lower-cased back-off features
made a strong contribution, while PA training was
perhaps not as important. Our complete system
achieved an F1-score of 80.19 on the restaurant
domain and 68.57 on the laptop domain, ranking
third among 24 teams in both.
Restaurants
System P R F1
NRC-Canada (All) 84.41 76.37 80.19
All ? lower-casing 83.68 75.49 79.37
All ? PA + percep 83.37 76.45 79.76
Laptops
System P R F1
NRC-Canada (All) 78.77 60.70 68.57
All ? lower-casing 78.11 60.55 68.22
All ? PA + percep 77.76 61.47 68.66
Table 2: Test set ablation experiments for Sub-
task 1: Aspect Term Detection.
4 Subtask 2: Aspect Term Polarity
In this subtask, the goal is to detect sentiment ex-
pressed towards a given aspect term. For example,
in sentence ?The asian salad is barely eatable.? the
aspect term asian salad is referred to with negative
sentiment. There were defined four categories of
sentiment: positive, negative, neutral, or conflict.
The conflict category is assigned to cases where
an aspect term is mentioned with both positive and
negative sentiment.
To address this multi-class classification prob-
lem, we trained a linear SVM classifier using
the LibSVM software (Chang and Lin, 2011).
Sentences were first tokenized and parsed with
the Stanford CoreNLP toolkits
2
to obtain part-of-
speech (POS) tags and (collapsed) typed depen-
dency parse trees (de Marneffe et al., 2006). Then,
features were extracted from (1) the target term it-
self; (2) its surface context, i.e., a window of n
words surrounding the term; (3) the parse context,
i.e., the nodes in the parse tree that are connected
to the target term by at most three edges.
Surface features: (1) unigrams (single words)
and bigrams (2-word sequences) extracted from a
term and its surface context; (2) context-target bi-
grams (i.e., bigrams formed by a word from the
surface context and a word from the term itself).
Lexicon features: (1) the number of posi-
tive/negative tokens; (2) the sum of the tokens?
sentiment scores; (3) the maximal sentiment score.
The lexicon features were calculated for each
manually and automatically created sentiment lex-
icons described in Section 2.2.
Parse features: (1) word- and POS-ngrams in
2
http://nlp.stanford.edu/software/corenlp.shtml
439
Laptops Rest.
System Acc. Acc.
NRC-Canada (All) 70.49 80.16
All ? sentiment lexicons 63.61 77.13
All ? Yelp lexicons 68.65 77.85
All ? Amazon lex. 68.13 80.11
All ? manual lexicons 67.43 78.66
All ? tweet lexicons 69.11 78.57
All ? parse features 69.42 78.40
Table 3: Test set ablation experiments for Sub-
task 2: Aspect Term Polarity.
the parse context; (2) context-target bigrams, i.e.,
bigrams composed of a word from the parse con-
text and a word from the term; (3) all paths that
start or end with the root of the target terms. The
idea behind the use of the parse features is that
sometimes an aspect term is separated from its
modifying sentiment phrase and the surface con-
text is insufficient or even misleading for detect-
ing sentiment expressed towards the aspect. For
example, in sentence ?The food, though different
from what we had last time, is actually great? the
word great is much closer to the word food in the
parse tree than in the surface form. Furthermore,
the features derived from the parse context can
help resolve local syntactic ambiguity (e.g., the
word bad in the phrase ?a bad sushi lover? modi-
fies lover and not sushi).
Table 3 presents the results of our official sub-
mission on the test sets for the laptop and restau-
rant domains. On the laptop dataset, our system
achieved the accuracy of 70.49 and was ranked
first among 32 submissions from 29 teams. From
the ablation experiments we see that the most sig-
nificant gains come from the use of the sentiment
lexicons; without the lexicon features the perfor-
mance of the system drops by 6.88 percentage
points. Observe that the features derived from
the out-of-domain Yelp Restaurant Sentiment lex-
icon are very helpful on the laptop domain. The
parse features proved to be useful as well; they
contribute 1.07 percentage points to the final per-
formance. On the restaurant data, our system ob-
tained the accuracy of 80.16 and was ranked sec-
ond among 36 submissions from 29 teams.
5 Subtask 3: Aspect Category Detection
The objective of this subtask is to detect aspect
categories discussed in a given sentence. There
Restaurants
System P R F1
NRC-Canada (All) 91.04 86.24 88.58
All ? lex. resources 86.53 78.34 82.23
All ?W?A lexicon 88.47 80.10 84.08
All ? word clusters 90.84 86.15 88.43
All ? post-processing 91.47 84.78 88.00
Table 4: Test set ablation experiments for Sub-
task 3: Aspect Category Detection. ?W?A lexicon?
stands for Yelp Restaurant Word?Aspect Associa-
tion Lexicon.
are 5 pre-defined categories for the restaurant do-
main: food, price, service, ambience, and anec-
dotes/miscellaneous. Each sentence can be la-
beled with one or more categories from the pre-
defined set. No aspect categories were defined for
the laptop domain.
We addressed the subtask as a multi-class multi-
label text classification problem. Five binary one-
vs-all Support Vector Machine (SVM) classifiers
were built, one for each category. The parameter C
was optimized through cross-validation separately
for each classifier. Sentences were tokenized
and stemmed with Porter stemmer (Porter, 1980).
Then, the following sets of features were gener-
ated for each sentence: ngrams, stemmed ngrams,
character ngrams, non-contiguous ngrams, word
cluster ngrams, and lexicon features. For the lex-
icon features, we used the Yelp Restaurant Word?
Aspect Association Lexicon and calculated the cu-
mulative scores of all terms appeared in the sen-
tence for each aspect category. Separate scores
were calculated for unigram and bigram entries.
Sentences with no category assigned by any of the
five classifiers went through the post-processing
step. For each such sentence, a category c with the
maximal posterior probability P (c|d) was identi-
fied and the sentence was labeled with the category
c if P (c|d) ? 0.4.
Table 4 presents the results on the restaurant test
set. Our system obtained the F1-score of 88.58
and was ranked first among 21 submissions from
18 teams. Among the lexical resources (lexicons
and word clusters) employed in the system, the
Word?Aspect Association Lexicon provided the
most gains: an increase in F1-score of 4.5 points.
The post-processing step also proved to be benefi-
cial: the recall improved by 1.46 points increasing
the overall F1-score by 0.58 points.
440
6 Subtask 4: Aspect Category Polarity
In the Aspect Category Polarity subtask, the goal
is to detect the sentiment expressed towards a
given aspect category in a given sentence. For
each input pair (sentence, aspect category), the
output is a single sentiment label: positive, neg-
ative, neutral, or conflict.
We trained one multi-class SVM classifier
(Crammer and Singer, 2002) for all aspect cate-
gories. The feature set was extended to incorpo-
rate the information about a given aspect category
c using a domain adaptation technique (Daum?e III,
2007) as follows: each feature f had two copies,
f general (for all the aspect categories) and f c
(for the specific category of the instance). For ex-
ample, for the input pair (?The bread is top notch
as well.?, ?food?) two copies of the unigram top
would be used: top general and top food . With
this setup the classifier can take advantage of the
whole training dataset to learn common sentiment
features (e.g., the word good is associated with
positive sentiment for all aspect categories). At the
same time, aspect-specific sentiment features can
be learned from the training instances pertaining
to a specific aspect category (e.g., the word deli-
cious is associated with positive sentiment for the
category ?food?).
Sentences were tokenized and part-of-speech
tagged with CMU Twitter NLP tool (Gimpel et al.,
2011). Then, each sentence was represented as a
feature vector with the following groups of fea-
tures: ngrams, character ngrams, non-contiguous
ngrams, POS tags, cluster ngrams, and lexicon
features. The lexicon features were calculated as
described in Section 4.
A sentence can refer to more than one aspect
category with different sentiment. For example,
in the sentence ?The pizza was delicious, but the
waiter was rude.?, food is described with posi-
tive sentiment while service with negative. If the
words delicious and rude occur in the training set,
the classifier can learn that delicious usually refers
to food (with positive sentiment) and rude to ser-
vice (with negative sentiment). If these terms do
not appear in the training set, their polarities can
still be inferred from sentiment lexicons. How-
ever, sentiment lexicons do not distinguish among
aspect categories and would treat both words, de-
licious and rude, as equally applicable to both cat-
egories, ?food? and ?service?. To (partially) over-
come this problem, we applied the Yelp Restau-
Restaurants
System Accuracy
NRC-Canada (All) 82.93
All ? lexical resources 74.15
All ? lexicons 75.32
All ? Yelp lexicons 79.22
All ? manual lexicons 82.44
All ? tweet lexicons 84.10
All ? word clusters 82.93
All ? aspect term features 82.54
Table 5: Test set ablation experiments for Sub-
task 4: Aspect Category Polarity.
rant Word?Aspect Association Lexicon to collect
all the terms having a high or moderate associ-
ation with the given aspect category (e.g., pizza,
delicious for the category ?food? and waiter, rude
for the category ?service?). Then, the feature set
described above was augmented with the same
groups of features generated just for the terms as-
sociated with the given category. We call these
features aspect term features.
Table 5 presents the results on the test set for
the restaurant domain. Our system achieved the
accuracy of 82.93 and was ranked first among 23
submissions from 20 teams. The ablation exper-
iments demonstrate the significant impact of the
lexical resources employed in the system: 8.78
percentage point gain in accuracy. The major ad-
vantage comes from the sentiment lexicons, and
specifically from the in-domain Yelp Restaurant
Sentiment lexicons. The out-of-domain tweet sen-
timent lexicons did not prove useful on this sub-
task. Also, word clusters did not offer additional
benefits on top of those provided by the lexicons.
The use of aspect term features resulted in gains
of 0.39.
7 Conclusion
The paper describes supervised machine-learning
approaches to detect aspect terms and aspect cat-
egories and to detect sentiment expressed towards
aspect terms and aspect categories in customer re-
views. Apart from common surface-form features
such as ngrams, our approaches benefit from the
use of existing and newly created lexical resources
such as word?aspect association lexicons and sen-
timent lexicons. Our submissions stood first on 3
out of 4 subtasks, and within the top 3 best results
on all 6 task-domain evaluations.
441
References
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27:1?27:27.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Koby Crammer and Yoram Singer. 2002. On the algo-
rithmic implementation of multiclass kernel-based
vector machines. The Journal of Machine Learning
Research, 2:265?292.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
Hal Daum?e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics, ACL
?07, pages 256 ? 263.
Berry de Bruijn, Colin Cherry, Svetlana Kiritchenko,
Joel Martin, and Xiaodan Zhu. 2011. Machine-
learned solutions for three stages of clinical infor-
mation extraction: the state of the art at i2b2 2010.
Journal of the American Medical Informatics Asso-
ciation, 18(5):557?562.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In Proceedings of the International Conference on
Language Resources and Evaluation, LREC ?06.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: Annotation, features, and experiments.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, ACL ?11.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Mo-
hammad. 2014. Sentiment analysis of short infor-
mal texts. Journal of Artificial Intelligence Research
(to appear).
Julian McAuley and Jure Leskovec. 2013. Hidden fac-
tors and hidden topics: understanding rating dimen-
sions with review text. In Proceedings of the 7th
ACM conference on Recommender systems, pages
165?172. ACM.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: Using
Mechanical Turk to create an emotion lexicon. In
Proceedings of the NAACL-HLT Workshop on Com-
putational Approaches to Analysis and Generation
of Emotion in Text, LA, California.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. NRC-Canada: Building the state-
of-the-art in sentiment analysis of tweets. In Pro-
ceedings of the International Workshop on Semantic
Evaluation, SemEval ?13, Atlanta, Georgia, USA,
June.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT, pages 380?390.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. SemEval-2014 Task 4:
Aspect based sentiment analysis. In Proceedings of
the International Workshop on Semantic Evaluation,
SemEval ?14, Dublin, Ireland, August.
M.F. Porter. 1980. An algorithm for suffix stripping.
Program, 3:130?137.
Sunita Sarawagi and William W Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Advances in Neural Information Pro-
cessing Systems, volume 17, pages 1185?1192.
Peter Turney and Michael L Littman. 2003. Measuring
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on Infor-
mation Systems, 21(4).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ?05, pages 347?354, Stroudsburg, PA, USA.
Xiaodan Zhu, Hongyu Guo, Saif M. Mohammad, and
Svetlana Kiritchenko. 2014. An empirical study on
the effect of negation words on sentiment. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics, ACL ?14.
442
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 443?447,
Dublin, Ireland, August 23-24, 2014.
NRC-Canada-2014: Recent Improvements in
the Sentiment Analysis of Tweets
Xiaodan Zhu, Svetlana Kiritchenko, and Saif M. Mohammad
National Research Council Canada
Ottawa, Ontario, Canada K1A 0R6
{xiaodan.zhu,svetlana.kiritchenko,saif.mohammad}@nrc-cnrc.gc.ca
Abstract
This paper describes state-of-the-art statis-
tical systems for automatic sentiment anal-
ysis of tweets. In a Semeval-2014 shared
task (Task 9), our submissions obtained
highest scores in the term-level sentiment
classification subtask on both the 2013 and
2014 tweets test sets. In the message-level
sentiment classification task, our submis-
sions obtained highest scores on the Live-
Journal blog posts test set, sarcastic tweets
test set, and the 2013 SMS test set. These
systems build on our SemEval-2013 senti-
ment analysis systems (Mohammad et al.,
2013) which ranked first in both the term-
and message-level subtasks in 2013. Key
improvements over the 2013 systems are
in the handling of negation. We create
separate tweet-specific sentiment lexicons
for terms in affirmative contexts and in
negated contexts.
1 Introduction
Automatically detecting sentiment of tweets (and
other microblog posts) has attracted extensive
interest from both the academia and industry.
The Conference on Semantic Evaluation Exercises
(SemEval) organizes a shared task on the senti-
ment analysis of tweets with two subtasks. In the
message-level task, the participating systems are
to identify whether a tweet as a whole expresses
positive, negative, or neutral sentiment. In the
term-level task, the objective is to determine the
sentiment of a marked target term (a single word
or a multi-word expression) within the tweet. Our
submissions stood first in both subtasks in 2013.
This paper describes improvements over that sys-
Evaluation Set Term-level Task Message-level Task
Twt14 1 4
Twt13 1 2
Sarc14 3 1
LvJn14 2 1
SMS13 2 1
Table 1: Overall rank of NRC-Canada sentiment
analysis models in Semeval-2014 Task 9 under the
constrained condition. The rows are five evalua-
tion datasets and the columns are the two subtasks.
tem and the subsequent submissions to the 2014
shared task (Rosenthal et al., 2014).
The training data for the SemEval-2014 shared
task is same as that of SemEval-2013 (about
10,000 tweets). The 2014 test set has five sub-
categories: a tweet set provided newly in 2014
(Twt14), the tweet set used for testing in the 2013
shared task (Twt13), a set of tweets that are sarcas-
tic (Sarc14), a set of sentences from the blogging
website LiveJournal (LvJn14), and the set of SMS
messages used for testing in the 2013 shared task
(SMS13). Instances from these categories were in-
terspersed in the provided test set. The partici-
pants were not told about the source of the indi-
vidual messages. The objective was to determine
how well a system trained on tweets generalizes to
texts from other domains.
Our submissions to SemEval-2014 Task 9,
ranked first in five out of the ten subtask?dataset
combinations. In the other evaluation sets as well,
our submissions performed competitively. The
results are summarized in Table 1. As we will
show, automatically generated tweet-specific lexi-
cons were especially helpful in all subtask?dataset
combinations. The results also show that even
though our models are trained only on tweets, they
generalize well to data from other domains.
443
Our systems are based on supervised SVMs and
a number of surface-form, semantic, and senti-
ment features. The major improvement in our
2014 system over the 2013 system is in the way it
handles negation. Morante and Sporleder (2012)
define negation to be ?a grammatical category that
allows the changing of the truth value of a propo-
sition?. Negation is often expressed through the
use of negative signals or negators, words such as
isnt and never, and it can significantly affect the
sentiment of its scope. We create separate tweet-
specific sentiment lexicons for terms in affirmative
contexts and in negated contexts. That is, we au-
tomatically determine the average sentiment of a
term when occurring in an affirmative context, and
separately the average sentiment of a term when
occurring in a negated context.
2 Our Systems
Our SemEval-2014 systems are based on our
SemEval-2013 systems (Mohammad et al., 2013).
For completeness, we briefly revisit our previ-
ous approach, which uses support vector machine
(SVM) as the classification algorithm and lever-
ages the following features.
Lexicon features These features are generated by
using three manually constructed sentiment lexi-
cons and two automatically constructed lexicons.
The manually constructed lexicons include the
NRC Emotion Lexicon (Mohammad and Turney,
2010; Mohammad and Yang, 2011), the MPQA
Lexicon (Wilson et al., 2005), and the Bing Liu
Lexicon (Hu and Liu, 2004). The two automati-
cally constructed lexicons, the Hashtag Sentiment
Lexicon and the Sentiment140 Lexicon, were cre-
ated specifically for tweets (Mohammad et al.,
2013).
The sentiment score of each term (e.g., a word
or bigram) in the automatically constructed lexi-
cons is computed by measuring the PMI (point-
wise mutual information) between the term and
the positive or negative category of tweets using
the formula:
SenScore (w) = PMI(w, pos)? PMI(w, neg)
(1)
where w is a term in the lexicons. PMI(w, pos)
is the PMI score between w and the positive class,
and PMI(w, neg) is the PMI score between w
and the negative class. Therefore, a positive Sen-
Score (w) suggests a stronger association of word
w with positive sentiment and vice versa. The
magnitude indicates the strength of association.
Note that the sentiment class of the tweets used
to construct the lexicons was automatically iden-
tified either from hashtags or from emoticons as
described in (Mohammad et al., 2013).
With these lexicons available, the following fea-
tures were extracted for a text span. Here a text
span can be a target term, its context, or an en-
tire tweet, depending on the task. The lexicon
features include: (1) the number of sentiment to-
kens in a text span; sentiment tokens are word
tokens whose sentiment scores are not zero in a
lexicon; (2) the total sentiment score of the text
span:
?
w?textSpan
SenScore (w); (3) the maxi-
mal score: max
w?textSpan
SenScore (w); (4) the
total positive and negative sentiment scores of the
text span; (5) the sentiment score of the last token
in the text span. Note that all these features are
generated, when applicable, by using each of the
sentiment lexicons mentioned above.
Ngrams We employed two types of ngram fea-
tures: word ngrams and character ngrams. The
former reflect the presence or absence of contigu-
ous or non-contiguous sequences of words, and
the latter are sequences of prefix/suffix characters
in each word. These features are same as in our
last year?s submission.
Negation The number of negated contexts. Our
definition of a negated context follows Pang et al.
(2002), which will be described in more details be-
low in Section 2.1.
POS The number of occurrences of each part-
of-speech tag. We tokenized and part-of-speech
tagged the tweets with the Carnegie Mellon Uni-
versity (CMU) Twitter NLP tool (Gimpel et al.,
2011).
Cluster features The CMU POS-tagging tool pro-
vides the token clusters produced with the Brown
clustering algorithm from 56 million English-
language tweets. These 1,000 clusters serve as an
alternative representation of tweet content, reduc-
ing the sparsity of the token space.
Encodings The encoding features are derived
from hashtags, punctuation marks, emoticons,
elongated words, and uppercased words.
For the term-level task, all the above features
are extracted for target terms and their context,
where a context is a window of words surround-
ing a target term. For the message-level task, the
features are extracted from the whole tweet.
444
In the term-level task, we used the LIB-
SVM (Chang and Lin, 2011) tool with the follow-
ing parameters: -t 0 -b 1 -m 1000. The total num-
ber of features is about 115,000. In the message-
level task, we used an in-house implementation of
SVM with a linear kernel. The parameter C was
set to 0.005. The total number of features was
about 1.5 million.
2.1 Improving Lexicons and Negation
Models
An important advantage of our SemEval-2013
systems comes from the use of the two high-
coverage tweet-specific sentiment lexicons. In
the SemEval-2014 submissions, we improve these
lexicons by incorporating negation modeling into
the lexicon generation process.
2.1.1 Improving Sentiment Lexicons
A word in a negated context has a different eval-
uative nature than the same word in an affirma-
tive (non-negated) context. We have proposed a
lexicon-based approach (Kiritchenko et al., 2014)
to determining the sentiment of words in these two
situations by automatically creating separate senti-
ment lexicons for the affirmative and negated con-
texts. In this way, we do not need to employ any
explicit assumptions to model negation.
To achieve this, a tweet corpus is split into two
parts: Affirmative Context Corpus and Negated
Context Corpus. Following the work of Pang et al.
(2002), we define a negated context as a segment
of a tweet that starts with a negation word (e.g., no,
shouldn?t) and ends with one of the punctuation
marks: ?,?, ?.?, ?:?, ?;?, ?!?, ???. The list of negation
words was adopted from Christopher Potts? senti-
ment tutorial.
1
Thus, part of a tweet that is marked
as negated is included into the negated context cor-
pus while the rest of the tweet becomes part of the
affirmative context corpus. The sentiment label
for the tweet is kept unchanged in both corpora.
Then, we generate an affirmative context lexicon
from the affirmative context corpus and a negated
context lexicon from the negated context corpus
using the technique described in (Kiritchenko et
al., 2014).
Furthermore, we refined the method of con-
structing the negated context lexicons by split-
ting a negated context into two parts: the imme-
diate context consisting of a single token that di-
rectly follows a negation word, and the distant
1
http://sentiment.christopherpotts.net/lingstruc.html
context consisting of the rest of the tokens in the
negated context. This has two benefits. Intu-
itively, negation affects words directly following
the negation words more strongly than more dis-
tant words. Second, immediate-context scores are
less noisy. Our simple negation scope identifica-
tion algorithm can at times fail and include parts
of a tweet that are not actually negated (e.g., if a
punctuation mark is missing). Overall, a sentiment
word can have up to three scores, one for affirma-
tive context, one for immediate negated context,
and one for distant negated context.
We reconstructed the Hashtag Sentiment Lexi-
con and the Sentiment140 Lexicon with this ap-
proach and used them in our SemEval-2014 sys-
tems.
2.1.2 Discriminating Negation Words
Different negation words, e.g., never and didn?t,
can have different effects on sentiment (Zhu et al.,
2014; Taboada et al., 2011). In our SemEval-2014
submission, we discriminate negation words in the
term-level models. For example, the word accept-
able appearing in a sentence this is never accept-
able is marked as acceptable beNever, while in
the sentence this is not acceptable, it is marked
as acceptable beNot. In this way, different nega-
tors (e.g., be not and be never) are treated differ-
ently. Note that we do not differentiate the tense
and person of auxiliaries in order to reduce sparse-
ness (e.g., was not and am not are treated in the
same way). This new representation is used to ex-
tract ngrams and lexicon-based features.
3 Results
Overall performance The evaluation metric used
in the competition is the macro-averaged F-
measure calculated over the positive and negative
categories. Table 2 presents the overall perfor-
mance of our models. NRC13 and NRC14 are
the systems we submitted to SemEval-2013 and
SemEval-2014, respectively. The integers in the
brackets are our official ranks in SemEval-2014
under the constrained condition.
In the term-level task, our submission ranked
first on the two Tweet datasets among 14 teams.
The results show that we achieved significant im-
provements over our last year?s submission: the F-
score improves from 85.19 to 86.63 on the Twt14
data and from 89.10 to 90.14 on the Twt13 data.
More specifically, on the Twt14 data, the approach
described in Section 2.1.1 improved our F-score
445
Term-level Message-level
NRC13 NRC14 NRC13 NRC14
Twt14 85.19 86.63(1) 68.88 69.85(4)
Twt13 89.10 90.14(1) 69.02 70.75(2)
Sarc14 78.16 77.13(3) 47.64 58.16(1)
LvJn14 84.96 85.49(2) 74.01 74.84(1)
SMS13 88.34 88.03(2) 68.34 70.28(1)
Table 2: Overall performance of the NRC-Canada
sentiment analysis systems.
from 85.19 to 86.37, and discriminating nega-
tion words (discussed in Section 2.1.2) further im-
proved the F-score from 86.37 to 86.63.
Our system ranked second on the LvJn14 and
SMS13 dataset. Note that the term-level system
that ranked first on LvJn14 performed worse than
our system on SMS13 and the system that ranked
first on SMS13 showed worse results than ours on
LvJn14, indicating that our term-level models in
general have good generalizability on these two
out-of-domain datasets.
On the message-level task, again the NRC14
system showed significant improvements over the
last year?s system on all five datasets. It achieved
the second best result on the Twt13 data and the
fourth result on the Twt14 data among 42 teams.
It was also the best system to predict sentiment in
sarcastic tweets (Sarc14). Furthermore, the system
proved to generalize well to other types of short
informal texts; it placed first on the two out-of-
domain datasets: SMS13 and LvJn14. We observe
a major improvement of our message-level model
on Sarc14 over our last year?s model, but as the
size of Sarc14 is small (86 tweets), more data and
analysis would be desirable to help better under-
stand this phenomenon.
Contribution of features Table 3 presents the re-
sults of ablation experiments on all five test sets for
the term-level task. The features derived from the
manual and automatic lexicons proved to be useful
on four datasets. The only exception is the Sarc14
data where removing lexicon features results in no
performance improvement. Considering that this
test set is very small (only about 100 test terms),
further investigation would be desirable if a larger
dataset becomes available. Also, in sarcasm the
real sentiment of a text span may be different from
its literal sentiment. In such a situation, a system
that correctly recognizes the literal sentiment may
actually make mistakes in capturing the real sen-
timent. The last two rows in Table 3 show the re-
sults obtained when the features are extracted only
from the target (and not from its context) and when
they are extracted only from the context of the tar-
get (and not from the target itself). Observe that
even though the context may influence the polar-
ity of the target, using target features alone is sub-
stantially more useful than using context features
alone. Nonetheless, adding context features im-
proves the F-scores in general.
On the message-level task (Table 4), the fea-
tures derived from the sentiment lexicons and, in
particular, from our large-coverage tweet-specific
lexicons turned out to be the most influential. The
use of the lexicons provided consistent gains of 9?
11 percentage points not only on tweet datasets,
but also on out-of-domain SMS and LiveJournal
data. Note that removing the features derived from
the manual lexicons as well as removing the ngram
features improves the performance on the Twt14
dataset. However, this effect is not observed on
the Twt13 and the out-of-domain test sets. The
possible explanation of this phenomenon is minor
overfitting on the tweet data.
4 Conclusions
We presented supervised statistical systems for
message-level and term-level sentiment analysis
of tweets. They incorporate many surface-form,
semantic, and sentiment features. Among sub-
missions from over 40 teams in the Semeval-
2014 shared task ?Sentiment Analysis in Twit-
ter?, our submissions ranked first in five out of
the ten subtask-dataset combinations. The sin-
gle most useful set of features are those obtained
from automatically generated tweet-specific lexi-
cons. We obtained significant improvements over
our previous system (which ranked first in the
2013 shared task) notably by estimating the senti-
ment of words in affirmative and negated contexts
separately. Also, since different negation words
impact sentiment differently, we modeled different
negation words separately in our term-level sys-
tem. This too led to an improvement in F-score.
The results on different kinds of evaluation sets
show that even though our systems are trained only
on tweets, they generalize well to text from other
domains such as blog posts and SMS messages.
Many of the resources we created and used are
made freely available.
2
2
www.purl.com/net/sentimentoftweets
446
Experiment Twt14 Twt13 Sarc14 LvJn14 SMS13
all features 86.63 90.14 77.13 85.49 88.03
all - lexicons 81.98 86.25 80.74 80.00 83.91
all - manu. lex. 86.08 89.25 75.32 84.13 87.69
all - auto. lex. 86.05 88.32 80.38 83.96 86.18
all - ngrams 83.31 86.67 72.95 81.58 82.41
all - target 72.93 74.19 63.09 72.21 69.34
all - context 84.40 88.83 77.22 82.99 87.97
Table 3: Term-level Task: The macro-averaged F-scores obtained on the 5 test sets with one of the feature
groups removed.
Experiment Twt14 Twt13 Sarc14 LvJn14 SMS13
all features 69.85 70.75 58.16 74.84 70.28
all - lexicons 60.59 60.04 47.17 65.80 60.56
all - manu. lex. 71.84 69.84 53.34 73.41 66.60
all - auto. lex. 63.40 65.08 47.57 71.76 66.94
all - ngrams 70.02 67.90 44.58 74.43 68.45
Table 4: Message-level Task: The macro-averaged F-scores obtained on the 5 test sets with one of the
feature groups removed.
Acknowledgments
We thank Colin Cherry for providing his SVM
code and for helpful discussions.
References
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27:1?27:27.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: Annotation, features, and experiments.
In Proceedings of ACL.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of KDD,
pages 168?177, New York, NY, USA. ACM.
Svetlana Kiritchenko, Xiaodan Zhu, and Saif Moham-
mad. 2014. Sentiment analysis of short informal
texts. (To appear) Journal of Artificial Intelligence
Research.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: Using
Mechanical Turk to create an emotion lexicon. In
Proceedings of the NAACL-HLT Workshop on Com-
putational Approaches to Analysis and Generation
of Emotion in Text, LA, California.
Saif M. Mohammad and Tony (Wenda) Yang. 2011.
Tracking sentiment in mail: How genders differ on
emotional axes. In Proceedings of the ACL Work-
shop on Computational Approaches to Subjectivity
and Sentiment Analysis, Portland, OR, USA.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the International Workshop on Semantic Evalua-
tion, SemEval ?13, Atlanta, Georgia, USA, June.
Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special is-
sue. Computational linguistics, 38(2):223?260.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79?86, Philadelphia, PA.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Proceedings of
SemEval-2014, Dublin, Ireland.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional Linguistics, 37(2):267?307.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of HLT-
EMNLP, HLT ?05, pages 347?354, Stroudsburg, PA,
USA.
Xiaodan Zhu, Hongyu Guo, Saif Mohammad, and
Svetlana Kiritchenko. 2014. An empirical study on
the effect of negation words on sentiment. In Pro-
ceedings of ACL, Baltimore, Maryland, USA, June.
447
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 210?218,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
A Normalized-Cut Alignment Model for Mapping Hierarchical Semantic
Structures onto Spoken Documents
Xiaodan Zhu
Institute for Information Technology
National Research Council Canada
Xiaodan.Zhu@nrc-cnrc.gc.ca
Abstract
We propose a normalized-cut model for the
problem of aligning a known hierarchical
browsing structure, e.g., electronic slides of
lecture recordings, with the sequential tran-
scripts of the corresponding spoken docu-
ments, with the aim to help index and access
the latter. This model optimizes a normalized-
cut graph-partitioning criterion and considers
local tree constraints at the same time. The ex-
perimental results show the advantage of this
model over Viterbi-like, sequential alignment,
under typical speech recognition errors.
1 Introduction
Learning semantic structures of written text has been
studied in a number of specific tasks, which include,
but not limited to, those finding semantic represen-
tations for individual sentences (Ge and Mooney,
2005; Zettlemoyer and Collins, 2005; Lu et al,
2008), and those constructing hierarchical structures
among sentences or larger text blocks (Marcu, 2000;
Branavan et al, 2007). The inverse problem of the
latter kind, e.g., aligning certain form of already-
existing semantic hierarchies with the corresponding
text sequence, is not so much a prominent problem
for written text as it is for spoken documents. In this
paper, we study a specific type of such a problem, in
which a hierarchical browsing structure, i.e., elec-
tronic slides of oral presentations, have already ex-
isted, the goal being to impose such a structure onto
the transcripts of the corresponding speech, with the
aim to help index and access spoken documents as
such.
Navigating audio documents is often inherently
much more difficult than browsing text; an obvi-
ous solution, in relying on human beings? ability to
read text, is to conduct a speech-to-text conversion
through automatic speech recognition (ASR). Im-
plicitly, solutions as such change the conventional
speaking-for-hearing construals: now speech can be
read through its transcripts, though, in most cases,
it was not intended for this purpose, which in turn
raises a new set of problems.
The convenience and efficiency of reading tran-
scripts (Stark et al, 2000; Munteanu et al, 2006)
are first affected by errors produced in transcrip-
tion channels for various reasons, though if the goal
is only to browse salient excerpts, recognition er-
rors on the extracts can be reduced by consider-
ing ASR confidence scores (Xie and Liu, 2010;
Hori and Furui, 2003; Zechner and Waibel, 2000):
trading off the expected salience of excerpts with
their recognition-error rate could actually result in
the improvement of excerpt quality in terms of the
amount of important content being correctly pre-
sented (Zechner and Waibel, 2000).
Even if transcription quality were not a problem,
browsing transcripts is not straightforward. When
intended to be read, written documents are almost
always presented as more than uninterrupted strings
of text. Consider that for many written docu-
ments, e.g., books, indicative structures such as sec-
tion/subsection headings and tables-of-contents are
standard constituents created manually to help read-
ers. Structures of this kind, even when existing, are
rarely aligned with spoken documents completely.
This paper studies the problem of imposing a
210
known hierarchical browsing structure, e.g., the
electronic slides of lecture recordings, onto the se-
quential transcripts of the corresponding spoken
document, with the aim to help index and hence ac-
cess the latter more effectively. Specifically, we pro-
pose a graph-partitioning approach that optimizes a
normalized-cut criterion globally, in traversing the
given hierarchical semantic structures. The exper-
imental results show the advantage of this model
over Viterbi-like, sequential alignment, under typi-
cal speech recognition errors.
2 Related work
Flat structures of spoken documents Much pre-
vious work, similar to its written-text counterpart,
has attempted to find certain flat structures of spoken
documents, such as topic and slide boundaries. For
example, the work of (Chen and Heng, 2003; Rud-
darraju, 2006; Zhu et al, 2008) aims to find slide
boundaries in the corresponding lecture transcripts.
Malioutov et al (2007) developed an approach to
detecting topic boundaries of lecture recordings by
finding repeated acoustic patterns. None of this
work, however, has involved hierarchical structures
of a spoken document. Research has also resorted
to other multimedia channels, e.g., video (Liu et al,
2002; Wang et al, 2003; Fan et al, 2006), to detect
slide transitions. This type of research, however, is
unlikely to recover semantic structures in more de-
tails than slide boundaries.
Hierarchical structures of spoken documents
Recently, research has started to align hierarchical
browsing structures with spoken documents, given
that inferring such structures directly from spoken
documents is still too challenging. Zhu et al (2010)
investigates bullet-slide alignment by first sequen-
tializing bullet trees with a pre-order walk before
conducting alignment, through which the problem
is reduced to a string-to-string alignment problem
and an efficient Viterbi-like method can be naturally
applied. In this paper, we use such a sequential
alignment as our baseline, which takes a standard
dynamic-programming process to find the optimal
path on an M-by-N similarity matrix, where M and
N denote the number of bullets and utterances in a
lecture, respectively. Specifically, we chose the path
that maps each bullet to an utterance to achieve the
highest total bullet-utterance similarity score; this
path can be found within a standard O(MN2) time
complexity.
A pre-order walk of the hierarchical tree is a natu-
ral choice, since speakers of presentations often fol-
low such a order in developing their talk; i.e., they
often talk about a bullet first and then each of its chil-
dren in sequence. A pre-order walk is also assumed
by Branavan et al (2007) in their table-of-content
generation task, a problem in which a hierarchical
structure has already been assumed (aligned) with a
span of written text, but the title of each node needs
to be generated.
In principle, such a sequential-alignment ap-
proach allows a bullet to be only aligned to one ut-
terance in the end, which does not model the basic
properties of the problem well, where the content in
a bullet is often repeated not only when the speaker
talks about it but also, very likely, when he discusses
the descendant bullets. Second, we suspect that
speech recognition errors, when happening on the
critical anchoring words that bridging the alignment,
would make a sequential-alignment algorithm much
less robust, compared with methods based on many-
to-many alignment. This is very likely to happen,
considering that domain-specific words are likely to
be the critical words in deciding the alignment, but
they are also very likely to be mis-recognized by an
ASR system at the same time, e.g., due to out-of-
vocabulary issue or language-model sparseness. We
will further discuss this in more details later in our
result section. Third, the hierarchical structures are
lost in the sequentialization of bullets, though some
remedy could be applied, e.g., by propagating a par-
ent bullet?s information onto its children (Zhu et al,
2010).
On the other hand, we should also note that the
benefit of formulating the problem as a sequential
alignment problem is its computational efficiency:
the solution can be calculated with conventional
Viterbi-like algorithms. This property is also impor-
tant for the task, since the length of a spoken docu-
ment, such as a lecture, is often long enough to make
inefficient algorithms practically intractable.
An important question is therefore how to, in prin-
ciple, model the problem better. The second is how
time efficient the model is. Malioutov and Barzi-
lay (2006) describe a dynamic-programming version
211
of a normalized-cut-based model in solving a topic
segmentation problem for spoken documents. In-
spired by their work, we will propose a model based
on graph partitioning in finding the correspondence
between bullets and the regions of transcripts that
discuss them; the proposed model runs in polyno-
mial time. We will empirically show its benefit on
both improving the alignment performance over a
sequential alignment and its robustness to speech
recognition errors.
3 Problem
We are given a speech sequence U = u1, u2, ..., uN ,
where ui is an utterance, and the corresponding hi-
erarchical structure, which, in our work here, is a
sequence of lecture slides containing a set of slide ti-
tles and bullets, B = {b1, b2, ..., bM}, organized in a
tree structure T (<,?,?), where < is the root of the
tree that concatenates all slides of a lecture; i.e., each
slide is a child of the root < and each slide?s bullets
form a subtree. In the rest of this paper, the word
bullet means both the title of a slide (if any) and any
bullet in it, if not otherwise noted. ? is the set of
nodes of the tree (both terminal and non-terminals,
excluding the root <), each corresponding to a bullet
bm in the slides. ? is the edge set. With the defini-
tions, our task is herein to find the triple (bi, uj , uk),
denoting that a bullet bi is mapped to a region of lec-
ture transcripts that starts from the jth utterance uj
and ends at the kth, inclusively. Constrained by the
tree structure, the transcript region corresponding to
an ancestor bullet contains those corresponding to
its descendants; i.e., if a bullet bi is the ancestor of
another bullet bn in the tree, the acquired boundary
triples (bi, uj1 , uk1) and (bi, uj2 , uk2) should satisfy
j1 ? j2 and k1 ? k2. Figure 1 shows a slide, its
structure, and the correspondence between one of
its bullets and a region of transcribed utterances (the
root that concatenates all such slides of a lecture to-
gether is not shown here).
4 A graph-partitioning approach
The generative process of lecture speech, with re-
gard to a hierarchical structure (here, bullet trees),
is characterized in general by a speaker?s producing
detailed content for each bullet when discussing it,
during which sub-bullets, if any, are talked about re-
Figure 1: A slide, its tree structure, and the correspon-
dence between one of its bullets and a region of tran-
scribed utterances (uj, uj+1..., uk).
cursively. By its nature of the problem, words in a
bullet could be repeated multiple times, even when
the speaker traverses to talk about the descendant
bullets in the depth of the sub-trees. In principle,
a model would be desirable to consider such proper-
ties between a slide bullet, including all its descen-
dants, and utterance transcripts, as well as the con-
straints of bullet trees. We formulate the problem
of finding the correspondence between bullets and
transcripts as a graph-partitioning problem, as de-
tailed below.
The correspondence between bullets and tran-
scribed utterances is evidenced by the similarities
between them. In a graph that contains a set of bul-
lets and utterances as its vertices and similarities be-
tween them as its edges, our aim is to place bound-
aries to partition the graph into smaller ones in order
to obtain triples, e.g., (bi, uj , uk), that optimize cer-
tain criterion. Inspired by the work of (Malioutov
and Barzilay, 2006; Shi and Malik, 2000), we op-
timize a normalized-cut score, in which the total
weight of edges being cut by the boundaries is mini-
mized, normalized by the similarity between the bul-
let bi and the entire vertices, as well as between the
transcript region uj , ..., uk and the entire vertices,
respectively.
Consider a simple two-set case first, in which a
boundary is placed on a graph G = (V,E) to sepa-
rate its vertices V into two sets, A and B, with all the
edges between these two sets being removed. The
objective, as we have mentioned above, is to mini-
mize the following normalized-cut score:
212
Ncut(A,B) = cut(A,B)assoc(A,V ) +
cut(A,B)
assoc(B,V ) (1)
where,
cut(A,B) =
?
a?A,b?B
w(a, b)
assoc(A,V ) =
?
a?A,v?V
w(a, v)
assoc(B,V ) =
?
b?B,v?V
w(b, v)
In equation (1), cut(A,B) is the total weight of
the edges being cut, i.e., those connecting A with
B, while assoc(A,V ) and assoc(B,V ) are the total
weights of the edges that connect A with all vertices
V , and B with V , respectively; w(a, b) is an edge
weight between a vertex a and b.
In general, minimizing such a normalized-cut
score has been shown to be NP-complete. In our
problem, however, the solution is constrained by
the linearity of segmentation on transcripts, simi-
lar to that in (Malioutov and Barzilay, 2006). In
such a situation, a polynomial-time algorithm exists.
Malioutov and Barzilay (2006) describe a dynamic-
programming algorithm to conduct topic segmenta-
tion for spoken documents. We modify the method
to solve our alignment problem here, which, how-
ever, needs to cope with the bipartite graphs between
bullets and transcribed sentences rather than sym-
metric similarity matrices among utterances them-
selves. We also need to integrate this in considering
the hierarchical structures of bullet trees.
We first consider a set of sibling bullets, b1, ..., bm,
that appear on the same level of a bullet tree and
share the same parent bp. For the time being, we
assume the corresponding region of transcripts has
already been identified for bp, say u1, ..., un. We
connect each bullet in b1, ..., bm with utterances in
u1, ..., un by their similarity, which results in a bi-
partite graph. Our task here is to place m ? 1
boundaries onto the bipartite graph to partition the
graph into m bipartite graphs and obtain triples, e.g.,
(bi, uj , uk), to align bi to uj, ..., uk , where bi ?
{b1, ..., bm} and uj, uk ? {u1, ..., bn} and j <= k.
Since we have all descendant bullets to help the par-
titioning, when constructing the bipartite graph, we
actually include also all descendant bullets of each
bullet bi, but ignoring their orders within each bi.
We will revisit this in more details later. We find
optimal normalized cuts in a dynamic-programming
process with the following recurrence relation:
C[i, k] = min
j?k
{C[i? 1, j] +D[i, j + 1, k]} (2)
B[i, k] = argmin
j?k
{C[i?1, j]+D[i, j +1, k]} (3)
In equation (2) and (3), C[i, k] is the opti-
mal/minimal normalized-cut value of aligning the
first i sibling bullets, b1, ..., bi, with the first k ut-
terances, u1, ..., bk , while B[i, k] records the back-
tracking indices corresponding to the optimal path
yielding the current C[i, k]. As shown in equation
(2), C[i, k] is computed by updating C[i? 1, j] with
D[i, j + 1, k], for all possible j s.t. j ? k, where
D[i, j +1, k] is a normalized-cut score for the triple
(bi, uj+1, uk) and is defined as follows:
D[i, j + 1, k] = cut(Ai,j+1,k, V \Ai,j+1,k)assoc(Ai,j+1,k, V )
(4)
where Ai,j+1,k is the vertex set that contains the
bullet bi (including its descendant bullets, if any,
as discussed above) and the utterances uj+1, ..., uk ;
V \ Ai,j+1,k is its complement set.
Different from the topic segmentation problem
(Malioutov et al, 2007), we need to remember the
normalized-cut values between any region uj , ..., uk
and any bullet bi in our task, so we need to use
the additional subscript i in Ai,j+1,k, while in topic
segmentation, the computation of both cut(.) and
assoc(.) is only dependant on the left boundary j
and right boundary k. Note that the similarity matrix
here is not symmetric as it is in topic segmentation,
but m by n, where m is the number of bullets, while
n is the number of utterances.
For any triple (bi, uj+1, uk), there are two differ-
ent types of edges being cut: those between Bin
def=
{bi} (again, including bi and all its descendant bul-
lets) and Uout def= {u1, ..., uj , uk+1, ..., um}, as well
as those between Bout def= {b1, ..., bi?1, bi+1, ..., bm}
and Uin def= {uj+1, ..., uk}. We discriminate
these two types of edges. Accordingly, cut(.) and
213
assoc(.) in equation (4) are calculated with equation
(5) and (6) below by linearly combining the weights
of these two types of edges with ?, whose value is
decided with a small held-out data.
cut(Ai,j+1,k, V \ Ai,j+1,k) =
?
?
b?Bin,u?Uout
w(b, u)
+(1? ?)
?
b??Bout,u??Uin
w(b?, u?) (5)
assoc(Ai,j+1,k, V ) = ?
?
b?Bin,u?V
w(b, u)
+(1? ?)
?
b??Uin,u??V
w(b?, u?) (6)
In addition, different form that in topic segmen-
tation, where a segment must not be empty, we
shall allow a bullet bi to be aligned to an empty
region, to model the situation that a bullet is not
discussed by the speaker. To do so, we made j in
equation (2) and (3) above to be able to equal to
k in the subscript, i.e., j ? k. Specifically, when
j = k, the set Ai,j+1,k has no internal edges, and
D[i, j + 1, k] is either equal to 1, or often not de-
fined if assoc(Ai,j+1,k, V ) = 0. For the latter, we
reset D[i, j + 1, k] to be 1.
A visual example of partitioning sibling bullets
b1, b2, and b3 is shown in Figure 2, in which the
descendant bullets of them (here, b4, b5, and b6) are
also considered. Note that we only show direct chil-
dren of b1 here, while, as discussed above, all de-
scendant bullets, if any, will be considered.
Figure 2: A visual example of partitioning sibling bullets
b1, b2, and b3.
Up to now, we have only considered partition-
ing sibling bullets by assuming the boundaries of
their parent on lecture transcripts have already been
given, where the sibling bullets and the correspond-
ing transcripts form a bipartite graph. When parti-
tioning the entire bullet trees and all utterances for a
lecture, the graph contains not only a bipartite graph
but also the hierarchical trees themselves. We de-
couple this two parts of graph by a top-down traver-
sal of the bullet trees: starting from the root, for each
node on the bullet tree, we apply the normalized-cut
algorithm discussed above to find the corresponding
regions of transcripts for all its direct children, and
repeat this process recursively. In each visit to parti-
tion a group of sibling bullets, to allow the first child
to have a different starting point from its parent bul-
let (the speaker may spend some time on the parent
bullet itself before talking about each child bullet),
we inserted an extra child in front of the first child
and copy the text of the parent bullet to it. Note that
in each visit to partition a group of sibling bullets,
the solution found is optimal on that level, which,
again, results in a powerful model since all descen-
dant bullets, if any, are all considered. For exam-
ple, processing high-level bullets first is expected
to benefit from the richer information of using all
their descendants in helping find the boundaries on
transcripts accurately. Recall that we have discussed
above how to incorporate the descendant bullets into
this process. It would also dramatically reduce the
searching space of partitioning lower-level bullets.
As far as computational complexity is concerned,
the graph-partitioning method discussed above is
polynomial, O(MN2), with M and N denoting
the number of bullets and utterances in a lecture,
respectively. Note that M is often much smaller
than N , M  N . In more details, the loop ker-
nel of the algorithm is computing D[i, j, k]. This
in total needs to compute 12 (MN2) values, which
can be pre-calculated and stored before dynamic-
programming decoding runs; the later, as normal, is
O(MN2), too.
5 Experiment set-up
5.1 Corpus
Our experiment uses a corpus of four 50-minute
third-year university lectures taught by the same in-
structor on the topics of human-computer interac-
tion (HCI), which contain 119 slides composed of
214
921 bullets prepared by the lecturer himself. The
automatic transcripts of the speech contain approxi-
mately 30,000 word tokens, roughly equal to a 120-
page double-spaced essay in length. The lecturer?s
voice was recorded with a head-mounted micro-
phone with a 16kHz sampling rate and 16-bit sam-
ples, while students? comments and questions were
not recorded. The speech is split into utterances by
pauses longer than 200ms, resulting in around 4000
utterances. The slides and automatic transcripts of
one lecture were held out to decide the value of ? in
differentiating the two different types of edges be-
ing cut, as discussed in Section 4. The boundaries
between adjacent slides were marked manually dur-
ing the lectures were recorded, by the person who
oversaw the recording process, while the boundaries
between bullets within a slide were annotated after-
wards by another human annotator.
5.2 Building the graphs
The lecture speech was first transcribed into text au-
tomatically with ASR models. The first ASR model
is a baseline with its acoustic model trained on the
WSJ0 and WSJ1 subsets of the 1992 development
set of the Wall Street Journal (WSJ) dictation cor-
pus, which contains 30 hours of data spoken by
283 speakers. The language model was trained on
the Switchboard corpus, which contains 2500 tele-
phone conversations involving about 500 English-
native speakers, which was suggested to be suit-
able for the conversational style of lectures, e.g.,
by (Munteanu et al, 2007; Park et al, 2005). The
whole model yielded a word error rate (WER) at
0.48. In the remainder of this paper, we call the
model as ASR Model 1.
The second model is an advanced one using the
same acoustic model. However, its language model
was trained on domain-related documents obtained
from the Web through searching the words appear-
ing on slides, as suggested by Munteanu et al
(2007). This yielded a WER of 0.43, which is a
typical WER for lectures and conference presenta-
tions (Leeuwis et al, 2003; Hsu and Glass, 2006;
Munteanu et al, 2007), though a lower WER is
possible in a more ideal condition (Glass et al,
2007), e.g., when the same course from the previous
semester by the same instructor is available. The 3-
gram language models were trained using the CMU-
CAM Language Modelling Toolkit (Clarkson and
Rosenfeld, 1997), and the transcripts were generated
with the SONIC toolkit (Pellom, 2001). The out-
of-vocabulary rates are 0.3% in the output of ASR
Model 1 and 0.1% in that of Model 2, respectively.
Both bullets and automatic transcripts were
stemmed and stop words in them were removed. We
then calculated the similarity between a bullet and
an utterance with the number of overlapping words
shared, normalized by their lengths. Note that using
several other typical metrics, e.g., cosine, resulted
in a similar trend of performance change?our con-
clusions below are consistent under these situations,
though the specific performance scores (i.e., word
offsets) are different. Finally, the similarities be-
tween bullets and utterances yielded a single M-by-
N similarity matrix for each lecture to be aligned,
with M and N denoting the number of bullets in
slides and utterances in transcripts, respectively.
5.3 Evaluation metric
The metric used in our evaluation is
straightforward?automatically acquired bound-
aries on transcripts for each slide bullet are
compared against the corresponding gold-standard
boundaries to calculate offsets measured in number
of words. The offset scores are averaged over all
boundaries to evaluate model performance. Though
one may consider that different bullets may be of
different importance, in this paper we do not use
any heuristics to judge this and we treat all bullets
equally in our evaluation.
Note that topic segmentation research often uses
metrics such as Pk and WindowDiff (Malioutov
et al, 2007; Beeferman et al, 1999; Pevsner and
Hearst, 2002). Our problem here, as an alignment
problem, has an exact 1-to-1 correspondence be-
tween a gold and automatic boundary, in which we
can directly measure the exact offset of each bound-
ary.
6 Experimental results
Table 1 presents the experimental results obtained
on the automatic transcripts generated by the ASR
models discussed above, with WERs at 0.43 and
0.48, respectively, which are typical WERs for lec-
tures and conference presentations in realistic and
215
less controlled situations. SEQ-ALN in the table
stands for the Viterbi-like, sequential alignment dis-
cussed above in section 2, while G-CUT is the
graph-partitioning approach proposed in this paper.
The values in the table are the average word-offset
scores counted after stop-words having been re-
moved.
WER=0.43 WER=0.48
SEQ-ALN 15.22 20.38
G-CUT 13.41 16.77
Offs. Reduction 12% 18%
Table 1: The average word offsets of automatic bound-
aries from the gold-standard.
Table 1 shows that comparing these two
polynomial-time models, G-CUT reduces the aver-
age offsets of SEG-ALN under both WERs. On the
transcripts with 0.48 WER, the average word-offset
score is reduced by approximately 18% from 20.38
to 16.77, while for the transcripts with WER at 0.43,
the offset reduction is 12%, from 15.22 to 13.41.
Since both models use exactly the same input simi-
larity matrices, the differences between their results
confirm the advantage of the modeling principle be-
hind the proposed approach. Although the graph-
partitioning model could be extended further, e.g.,
with the approach in (Zhu et al, 2010), our primary
interest here is the principle modeling advantage of
this normalized-cut framework.
The results in Table 1 also suggest that the graph-
partitioning model is more robust to speech recog-
nition errors: when WERs increase from 0.43 to
0.48, the error of G-CUT increases by 25%, from
13.41 to 16.77, while that of SEQ-ALN increases by
44%, from 15.22 to 20.38. We due this to the fact
that the graph-partitioning model considers multiple
alignments between bullets, including their descen-
dants, and the transcribed utterances, where mis-
matching between bullet and transcript words, e.g.,
that caused by recognition errors, is less likely to
impact the graph-partitioning method, which bases
its optimization criterion on multiple alignments,
e.g., when calculating cut(.) and assoc(.) in equa-
tion (5) and (6). Recall that the ASR Model 2 in-
cludes domain-specific Web data to train the lan-
guage models, which were acquired by using bul-
let words to search the Web. It is expected to in-
crease the recognition accuracy on domain words,
particularly those appearing on the slides. There-
fore, Model 2 is likely to particularly increase the
correct matching between bullets and transcript.
The results in Table 1 also show the usefulness
of better ASR modeling on the structure-imposing
task here. As discussed in the introduction sec-
tion earlier, browsing automatic transcripts of long
spoken documents, such as lectures, is affected by
both speech recognition errors and lack of browsing
structures. Table 1 shows that the improvement in
solving the first problem also helps the second.
Last, from a pragmatic viewpoint of system de-
velopment, the graph-partitioning algorithm is sim-
ple to implement: the essence of equation (2)-(6) is
to find the optimal normalized-cut score character-
ized by computing D[i, j + 1, k] and updating the
formulae with it, which is not much more compli-
cate to build than the baseline. Also, the practical
speed difference between these two types of models
is not obvious on our dataset.
7 Conclusion
This paper proposes a graph-partitioning approach
for aligning a known hierarchical structure with the
transcripts of the corresponding spoken document
through optimizing a normalized-cut criterion. This
approach models the basic properties of the prob-
lem and is quadratic-time. Experimental results
show both its advantage on improving the alignment
performance over a standard sequential-alignment
baseline and its robustness to speech recognition er-
rors, while both take as input exactly the same simi-
larity matrices. From a pragmatic viewpoint of sys-
tem development, this graph-partitioning-based al-
gorithm is simple to implement. We believe immedi-
ate further work such as combining the normalized-
cut model with CYK-like dynamic programing to
traverse the semantic trees in alignment could help
us further understand the problem, though such
models need much more memory in practice if not
properly optimized and have a higher time complex-
ity. Also, topic-segmentation (cohesion) models can
be naturally combined with the alignment model dis-
cussed here. We will study such problems as our
immediate future work.
216
References
D. Beeferman, A. Berger, and J. Lafferty. 1999. Statisti-
cal models for text segmentation. Machine Learning,
34(1-3):177?210.
S. Branavan, Deshpande P., and Barzilay R. 2007. Gen-
erating a table-of-contents: A hierarchical discrimina-
tive approach. In Proc. of Annual Meeting of the As-
sociation for Computational Linguistics.
Y. Chen and W. J. Heng. 2003. Automatic synchroniza-
tion of speech transcript and slides in presentation. In
Proc. International Symposium on Circuits and Sys-
tems.
P. Clarkson and R. Rosenfeld. 1997. Statistical language
modeling using the cmu-cambridge toolkit. In Proc. of
ISCA European Conf. on Speech Communication and
Technology, pages 2707?2710.
Q. Fan, K. Barnard, A. Amir, A. Efrat, and M. Lin. 2006.
Matching slides to presentation videos using sift and
scene background. In Proc. of ACM International
Workshop on Multimedia Information Retrieval, pages
239?248.
R. Ge and R. J. Mooney. 2005. A statistical semantic
parser that integrates syntax and semantics. In Proc.
of Computational Natural Language Learnine, pages
9?16.
J. Glass, T. Hazen, S. Cyphers, I. Malioutov, D. Huynh,
and R. Barzilay. 2007. Recent progress in the mit
spoken lecture processing project. Proc. of Annual
Conference of the International Speech Communica-
tion Association, pages 2553?2556.
C. Hori and S. Furui. 2003. A new approach to auto-
matic speech summarization. IEEE Transactions on
Multimedia, 5(3):368?378.
B. Hsu and J. Glass. 2006. Style and topic language
model adaptation using hmm-lda. In Proc. of Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
E. Leeuwis, M. Federico, and M. Cettolo. 2003. Lan-
guage modeling and transcription of the ted corpus lec-
tures. In Proc. of IEEE International Conference on
Acoustics, Speech and Signal Processing.
T. Liu, R. Hjelsvold, and J. R. Kender. 2002. Analysis
and enhancement of videos of electronic slide presen-
tations. In Proc. IEEE International Conference on
Multimedia and Expo.
W. Lu, H. T. Ng, W. S. Lee, and L. S. Zettlemoyer. 2008.
A generative model for parsing natural language to
meaning representations. In Proc. of Empirical Meth-
ods in Natural Language Processing, pages 783?792.
I. Malioutov and R. Barzilay. 2006. Minimum cut model
for spoken lecture segmentation. In Proc. of Interna-
tional Conference on Computational Linguistics and
Annual Meeting of the Association for Computational
Linguistics.
I. Malioutov, A. Park, R. Barzilay, and J. Glass. 2007.
Making sense of sound: Unsupervised topic segmen-
tation over acoustic input. In Proc. of Annual Meet-
ing of the Association for Computational Linguistics,
pages 504?511.
D. Marcu. 2000. The theory and practice of discourse
parsing and summarization. The MIT Press.
C. Munteanu, R. Baecker, G. Penn, E. Toms, and
E. James. 2006. Effect of speech recognition accu-
racy rates on the usefulness and usability of webcast
archives. In Proc. of ACM Conference on Human Fac-
tors in Computing Systems, pages 493?502.
C. Munteanu, G. Penn, and R. Baecker. 2007. Web-
based language modelling for automatic lecture tran-
scription. In Proc. of Annual Conference of the Inter-
national Speech Communication Association.
A. Park, T. Hazen, and J. Glass. 2005. Automatic pro-
cessing of audio lectures for information retrieval. In
Proc. of IEEE Conf. on Acoustics, Speech, and Signal
Processing, pages 497?500.
B. L. Pellom. 2001. Sonic: The university of colorado
continuous speech recognizer. Tech. Rep. TR-CSLR-
2001-01, University of Colorado.
L. Pevsner and M. Hearst. 2002. A critique and im-
provement of an evaluation metric for text segmenta-
tion. Computational Linguistics, 28:19?36.
R. Ruddarraju. 2006. Indexing Presentations Using Mul-
tiple Media Streams. Ph.D. thesis, Georgia Institute of
Technology. M.S. Thesis.
J. Shi and J. Malik. 2000. Normalized cuts and image
segmentation. IEEE Trans. Pattern Anal. Mach. In-
tell., 22.
L. Stark, S. Whittaker, and J. Hirschberg. 2000. Find-
ing information in audio: A new paradigm for audio
browsing and retrieval. In Proc. of International Con-
ference on Spoken Language Processing.
F. Wang, C. W. Ngo, and T. C. Pong. 2003. Synchroniza-
tion of lecture videos and electronic slides by video
text analysis. In Proc. of ACM International Confer-
ence on Multimedia.
S. Xie and Y. Liu. 2010. Using confusion networks for
speech summarization. In Proc. of International Con-
ference on Human Language Technology and Annual
Meeting of North American Chapter of the Association
for Computational Linguistics.
K. Zechner and A. Waibel. 2000. Minimizing word er-
ror rate in textual summaries of spoken language. In
Proc. of Applied Natural Language Processing Con-
ference and Meeting of the North American Chapter of
the Association for Computational Linguistics, pages
186?193.
217
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Proc.
of Uncertainty in Artificial Intelligence, pages 658?
666.
X. Zhu, X. He, C. Munteanu, and G. Penn. 2008. Us-
ing latent dirichlet alocation to incorporate domain
knowledge for topic transition detection. In Proc. of
Annual Conference of the International Speech Com-
munication Association.
X. Zhu, C. Cherry, and G. Penn. 2010. Imposing hierar-
chical browsing structures onto spoken documents. In
Proc. of International Conference on Computational
Linguistics.
218
Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 28?35,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Ecological Validity and the Evaluation of Speech Summarization Quality
Anthony McCallum Cosmin Munteanu
University of Toronto National Research Council Canada
40 St. George Street 46 Dineen Drive
Toronto, ON, Canada Fredericton, NB, Canada
mccallum@cs.toronto.edu cosmin.munteanu@nrc-cnrc.gc.ca
Gerald Penn Xiaodan Zhu
University of Toronto National Research Council Canada
40 St. George Street 1200 Montreal Road
Toronto, ON, Canada Ottawa, ON, Canada
gpenn@cs.toronto.edu xiaodan.zhu@nrc-cnrc.gc.ca
Abstract
There is little evidence of widespread adoption of 
speech summarization systems. This may be due in 
part to the fact that the natural language heuristics 
used  to  generate  summaries  are  often  optimized 
with respect to a class of evaluation measures that, 
while  computationally  and  experimentally  inex-
pensive,  rely on subjectively selected gold stand-
ards  against  which  automatically  generated  sum-
maries  are  scored.  This  evaluation  protocol  does 
not take into account the usefulness of a summary 
in assisting the listener in achieving his or her goal.
     In this paper we study how current measures 
and methods for evaluating summarization systems 
compare to human-centric evaluation criteria. For 
this, we have designed and conducted an ecologic-
ally valid evaluation that determines the value of a 
summary  when  embedded  in  a  task,  rather  than 
how closely a summary resembles a gold standard. 
The results of our evaluation demonstrate  that  in 
the  domain  of  lecture  summarization,  the  well-
known  baseline  of  maximal  marginal  relevance 
(Carbonell and Goldstein, 1998) is statistically sig-
nificantly  worse  than  human-generated  extractive 
summaries,  and even worse than having no sum-
mary at  all in  a simple quiz-taking task. Priming 
seems to have no statistically significant effect on 
the usefulness  of the human summaries.  In addi-
tion, ROUGE scores and, in particular, the context-
free annotations that are often supplied to ROUGE 
as references, may not always be reliable as inex-
pensive proxies for ecologically valid evaluations. 
In fact, under some conditions, relying exclusively 
on ROUGE may even lead to scoring human-gen-
erated summaries that are inconsistent in their use-
fulness relative to using no summaries very favour-
ably.
1 Background and Motivation
Summarization  maintains  a  representation  of  an 
entire spoken document, focusing on those utter-
ances (sentence-like units) that are most important 
and therefore does not require the user to process 
everything that has been said. Our work focuses on 
extractive summarization where a selection of ut-
terances is chosen from the original spoken docu-
ment in order to make up a summary.
Current  speech  summarization  research  has 
made extensive use  of intrinsic  evaluation meas-
ures  such  as  F-measure,  Relative  Utility,  and 
ROUGE  (Lin,  2004),  which  score  summaries 
against  subjectively  selected  gold  standard  sum-
maries obtained using human annotators. These an-
notators are asked to arbitrarily select (in or out) or 
rank utterances, and in doing so commit to relative 
salience judgements with no attention to goal ori-
entation  and  no  requirement  to  synthesize  the 
meanings of larger units of structure into a coher-
ent message.
28
Given this  subjectivity,  current  intrinsic evalu-
ation measures are unable to properly judge which 
summaries  are  useful  for real-world applications. 
For  example,  intrinsic  evaluations  have  failed  to 
show that summaries created by algorithms based 
on complex linguistic and acoustic features are bet-
ter  than  baseline  summaries  created  by  simply 
choosing the positionally first utterances or longest 
utterances in a spoken document (Penn and Zhu, 
2008).  What  is  needed  is  an  ecologically  valid 
evaluation  that  determines  how valuable  a  sum-
mary is when embedded in a task, rather than how 
closely  a  summary  matches  the  subjective  utter-
ance level scores assigned by annotators.
   Ecological validity is "the ability of experiments 
to tell us how real people operate in the real world" 
(Cohen, 1995). This is often obtained by using hu-
man judges, but it is important to realize that the 
mere use of human subjects provides no guarantee 
as  to the ecological  validity  of their  judgements. 
When utterances are merely ranked with numerical 
scores  out  of  context,  for  example,  the  human 
judges who perform this task are not performing a 
task that they generally perform in their daily lives, 
nor does the task correspond to how they would 
create or use a good summary if they did have a 
need for one. In fact, there may not even be a guar-
antee that they  understand the task --- the notions 
of ?importance,? ?salience? and the like, when de-
fining the criterion by which utterances are selec-
ted, are not easy to circumscribe. Judgements ob-
tained in this fashion are no better than those of the 
generative linguists who leaned back in their arm-
chairs in the 1980s to introspect on the grammatic-
ality  of  natural  language  sentences.  The  field  of 
computational linguistics could only advance when 
corpora became electronically available to invest-
igate language that was written in an ecologically 
valid context.
   Ours is not the first ecologically valid experiment 
to be run in the context of speech summarization, 
however.  He et al (1999; 2000) conducted a very 
thorough  and  illuminating  study  of  speech  sum-
marization in the lecture domain that  showed (1) 
speech summaries are indeed very useful to have 
around, if they are done properly, and (2) abstract-
ive summaries do not seem to add any statistically 
significant advantage to the quality of a summary 
over  what  topline  extractive  summaries  can 
provide. This is very good news; extractive sum-
maries are worth creating. Our study extends this 
work by attempting to evaluate the relative quality 
of  extractive  summaries.  We  conjecture  that  it 
would be very difficult for this field to progress un-
less we have a means of accurately measuring ex-
tractive summarization quality. Even if the meas-
ure comes at great expense, it is important to do.
   Another noteworthy paper is that of Liu and Liu 
(2010), who, in addition to collecting human sum-
maries of six meetings, conducted a subjective as-
sessment of the quality of those summaries  with 
numerically  scored  questionnaires.  These  are 
known as Likert scales, and they form an important 
component of any human-subject study in the field 
of human-computer interaction. Liu and Liu (2010) 
cast  considerable doubt on the value of ROUGE 
relative to these questionnaires. We will focus here 
on an objective, task-based measure that typically 
complements those subjective assessments.
2 Spontaneous Speech
Spontaneous speech is often not linguistically well-
formed,  and  contains  disfluencies,  such  as  false 
starts,  filled pauses,  and repetitions.  Additionally, 
spontaneous speech is more vulnerable to automat-
ic speech recognition (ASR) errors, resulting in a 
higher  word  error  rate  (WER).  As  such,  speech 
summarization has the most potential for domains 
consisting  of  spontaneous  speech  (e.g.  lectures, 
meeting recordings). Unfortunately, these domains 
are not easy to evaluate compared to highly struc-
tured  domains  such  as  broadcast  news.  Further-
more,  in  broadcast  news,  nearly  perfect  studio 
acoustic  conditions  and  professionally  trained 
readers  results  in  low  ASR WER,  making  it  an 
easy domain to summarize. The result is that most 
research has been conducted in this domain. How-
ever,  a positional  baseline performs very well  in 
summarizing broadcast news (Christensen, 2004), 
meaning that simply taking the first  N utterances 
provides a very challenging baseline, questioning 
the value of summarizing this domain. In addition, 
the widespread availability  of written  sources  on 
the same topics means that there is not a strong use 
case for speech summarization over simply sum-
marizing the equivalent  textual  articles on which 
the news  broadcasts  were  based.   This  makes  it 
even more difficult to preserve ecological validity.
University lectures present a much more relev-
ant domain, with less than ideal acoustic conditions 
but  structured  presentations  in  which  deviation 
29
from written sources (e.g., textbooks) is common-
place.  Here,  a  positional  baseline  performs  very 
poorly. The lecture domain also lends itself well to 
a  task-based  evaluation  measure;  namely  univer-
sity level quizzes or exams. This constitutes a real-
world problem in a domain that is also representat-
ive of other spontaneous speech domains that can 
benefit from speech summarization.
3 Ecologically Valid Evaluation
As pointed out by Penn and Zhu (2008), current 
speech summarizers have been optimized to per-
form an utterance selection task that may not ne-
cessarily reflect how a summarizer is able to cap-
ture the goal orientation or purpose of the speech 
data. In our study, we follow methodologies estab-
lished in the field of Human-Computer Interaction 
(HCI) for evaluating an algorithm or system ? that 
is, determining the benefits a system brings to its 
users, namely usefulness, usability, or utility, in al-
lowing a user to reach a specific goal. Increasingly, 
such user-centric evaluations are carried out within 
various  natural  language  processing  applications 
(Munteanu et  al.,  2006).  The  prevailing  trend in 
HCI  is  for  conducting  extrinsic  summary  evalu-
ations (He et al, 2000; Murray et al, 2008; Tucker 
et al, 2010), where the value of a summary is de-
termined by how well the summary can be used to 
perform a specific task rather than comparing the 
content of a summary to an artificially created gold 
standard. We have conducted an ecologically valid 
evaluation of speech summarization that has evalu-
ated summaries  under real-world conditions, in a 
task-based manner.
The university lecture domain is an example of a 
domain where summaries are an especially suitable 
tool  for  navigation.  Simply  performing  a  search 
will not result in the type of understanding required 
of students in their lectures. Lectures have topics, 
and there is a clear communicative goal. For these 
reasons, we have chosen this domain for our evalu-
ation. By using actual university lectures as well as 
university students representative of the users who 
would make use of a speech summarization system 
in this domain, all results obtained are ecologically 
valid.
3.1Experimental Overview
We conducted a within-subject experiment where 
participants  were  provided  with  first  year  soci-
ology university lectures on a lecture browser sys-
tem installed on a desktop computer. For each lec-
ture, the browser made accessible the audio, manu-
al transcripts, and an optional summary. Evaluation 
of a summary was based on how well the user of 
the summary was able to complete a quiz based on 
the content of the original lecture material.
It is important to note that not all extrinsic eval-
uation is ecologically valid.  To ensure ecological 
validity in this study, great care was taken to en-
sure that human subjects were placed under condi-
tions that result in behavior that would be expected 
in actual real-world tasks.
3.2Evaluation
Each quiz consisted of 12 questions, and were de-
signed to be representative of what students were 
expected to learn in the class, incorporating factual 
questions  only,  to  ensure  that  variation  in  parti-
cipant  intelligence had a minimal impact  on res-
ults.  In  addition,  questions  involved  information 
that was distributed equally throughout the lecture, 
but at the same time not linearly in the transcript or 
audio  slider,  which  would  have  allowed  parti-
cipants to predict where the next answer might be 
located. Finally, questions were designed to avoid 
content that was thought to be common knowledge 
in  order  to  minimize  the  chance  of  participants 
having previous knowledge of the answers.
All  questions  were  short  answer  or  fill-in-the-
blank. Each quiz consisted of an equal number of 
four distinct  types  of questions,  designed so that 
performing a simple search would not be effective, 
though  no  search  functionality  was  provided. 
Question types do not appear in any particular or-
der on the quiz and were not grouped together.
Type  1: These  questions  can  be  answered 
simply  by  looking  at  the  slides.  As  such,  these 
questions  could  be  answered  correctly  with  or 
without a summary as slides were available in all 
conditions.
Type 2:  Slides provide an indication of where 
the content required to answer these questions are 
located. Access to the corresponding utterances is 
still required to find the answer to the questions.
30
Type 3: Answers to these questions can only be 
found  in  the  transcript  and  audio.  The  slides 
provide no hint as to where the relevant content is 
located.
Type 4: These questions are more complicated 
and require a certain level of topic comprehension. 
These questions often require connecting concepts 
from various portions of the lecture. These ques-
tions are more difficult and were included to min-
imize  the chance  that  participants  would already 
know the answer to questions without watching the 
lecture.
A teaching assistant for the sociology class from 
which  our  lectures  were  obtained  generated  the 
quizzes  used in the evaluation.  This teaching as-
sistant had significant experience in the course, but 
was not involved in the design of this study and did 
not have any knowledge relating to our hypotheses 
or  the  topic  of  extractive  summarization.  These 
quizzes provided an ecologically valid quantitative 
measure of whether a given summary was useful. 
Having this evaluation metric in place, automated 
summaries  were  compared  to  manual  summaries 
created by each participant in a previous session.
3.3Participants
Subjects  were  recruited  from  a  large  university 
campus,  and  were  limited  to  undergraduate  stu-
dents  who  had  at  least  two  terms  of  university 
studies,  to  ensure  familiarity  with  the  format  of 
university-level lectures and quizzes. Students who 
had taken the first year sociology course we drew 
lectures  from  were  not  permitted  to  participate. 
The study was conducted with 48 participants over 
the  course  of  approximately  one  academic 
semester.
3.4Method
Each evaluation session began by having a parti-
cipant perform a short warm-up with a portion of 
lecture content, allowing the participant to become 
familiar with the lecture browser interface. Follow-
ing  this,  the  participant  completed  four  quizzes, 
one  for  each  of  four  lecture-condition  combina-
tions. There were a total of four lectures and four 
conditions.  Twelve  minutes  were  given  for  each 
quiz. During this time, the participant was able to 
browse the audio, slides, and summary. Each lec-
ture was about forty minutes in length, establishing 
a time constraint. Lectures and conditions were ro-
tated using a Latin square for counter balancing. 
All participants completed each of the four condi-
tions.
One week prior to his or her evaluation session, 
each participant was brought in and asked to listen 
to  and  summarize  the  lectures  beforehand.  This 
resulted  in  the  evaluation  simulating  a  scenario 
where  someone  has  heard  a  lecture  at  least  one 
week in the past and may or may not remember the 
content during an exam or quiz. This is similar to 
conditions most university students experience.
3.5Conditions
The lecture audio recordings were manually tran-
scribed and segmented into utterances, determined 
by 200 millisecond pauses,  resulting in segments 
that  correspond  to  natural  sentences  or  phrases. 
The task of summarization consisted of choosing a 
set of utterances for inclusion in a summary (ex-
tractive summarization), where the total summary 
length was bounded by 17-23% of the words in the 
lecture;  a percentage typical  to most  summariza-
tion scoring tasks. All participants were asked to 
make use of the browser interface for four lectures, 
one for each of the following conditions:  no sum-
mary,  generic  manual summary,  primed manual  
summary, and automatic summary.
No  summary: This condition  served  as  a 
baseline where no summary was provided, but par-
ticipants  had  access  to  the  audio  and  transcript. 
While  all  lecture  material  was  provided,  the 
twelve-minute time constraint made it impossible 
to listen to the lecture in its entirety.
Generic  manual  summary: I  this  condition, 
each  participant  was  provided  with  a  manually 
generated summary. Each summary was created by 
the participant him or herself in a previous session. 
Only  audio  and text  from the  in-summary  utter-
ances  were  available  for  use.  This  condition 
demonstrates how a manually created summary is 
able to aid in the task of taking a quiz on the sub-
ject matter.
Primed manual summary: Similar to above, in 
this condition, a summary was created manually by 
selecting a set of utterances from the lecture tran-
script.  For  primed  summaries,  full  access  to  a 
priming quiz, containing all of the questions in the 
evaluation quiz as well as several additional ques-
tions, was available  at the time of summary cre-
31
ation. This determines the value of creating sum-
maries with a particular task in mind, as opposed to 
simply choosing utterances that are felt to be most 
important or salient.
Automatic  summary: The  procedure  for  this 
condition was identical to the generic manual sum-
mary condition from the point of view of the parti-
cipant.  However, during the evaluation phase, an 
automatically generated summary was provided in-
stead of the summary that the participant created 
him or herself. The algorithm used to generate this 
summary was an implementation of  MMR  (Car-
bonell and Goldstein, 1998). Cosine similarity with 
tf-idf term weighting was used to calculate similar-
ity. Although the redundancy component of MMR 
makes  it  especially  suitable  for  multi-document 
summarization,  there  is  no  negative  effect  if  re-
dundancy is not an issue. It is worth noting that our 
lectures are longer than material typically summar-
ized, and lectures in general are more likely to con-
tain  redundant  material  than  a  domain  such  as 
broadcast news. There was only one MMR sum-
mary generated for each lecture, meaning that mul-
tiple participants made use of identical summaries. 
The automatic summary was created by adding the 
highest scoring utterances one at a time until the 
sum of the length of all of the selected utterances 
reached 20% of the number of words in the origin-
al  lecture.  MMR was  chosen  as  it  is  commonly 
used  in  summarization.  MMR  is  a  competitive 
baseline,  even  among  state-of-art  summarization 
algorithms, which tend to correlate well with it.
What  this  protocol  does  not  do  is  pit  several 
strategies  for  automatic  summary  generation 
against  each  other.   That  study,  where  more  ad-
vanced summarization algorithms will also be ex-
amined, is forthcoming.  The present experiments 
have the collateral benefit  of  serving as a means 
for collecting ecologically valid human references 
for that study.
3.6Results
Quizzes were scored by a teaching assistant for the 
sociology  course  from  which  the  lectures  were 
taken. Quizzes were marked as they would be in 
the  actual  course  and  each  question  was  graded 
with equal  weight  out  of  two marks.  The scores 
were then converted to a percentage. The resulting 
scores (Table 1) are 49.3+-17.3% for the  no sum-
mary condition,  48.0+-16.2%  for  the  generic  
manual  summary  condition,  49.1+-15.2% for  the 
primed summary  condition,  and 41.0+-16.9% for 
MMR. These scores are lower than averages expec-
ted in a typical university course. This can be par-
tially  attributed  to  the  existence  of  a  time  con-
straint.
Condition Average Quiz Score
no summary 49.3+-17.3%
generic manual summary 48.0+-16.2%
primed manual summary 49.1+-15.2%
automatic summary (MMR) 41.0+-16.9%
Table 1. Average Quiz Scores
Execution  of  the  Shapiro-Wilk Test  confirmed 
the scores are normally distributed and Mauchly's 
Test of Sphericity indicates that the sphericity as-
sumption holds. Skewness and Kurtosis tests were 
also  employed  to  confirm  normality.  A repeated 
measures  ANOVA determined  that  scores  varied 
significantly between conditions (F(3,141)=5.947, 
P=0.001). Post-hoc tests using the Bonferroni cor-
rection  indicate  that  the  no  summary,  generic  
manual  summary,  and  primed  manual  summary 
conditions all  resulted  in  higher  scores  than  the 
automatic (MMR) summary condition. The differ-
ence  is  significant  at  P=0.007,  P=0.014 and 
P=0.012 respectively. Although normality was as-
sured, the Friedman Test further confirms a signi-
ficant  difference  between  conditions 
(?2(3)=11.684, P=0.009).
4 F-measure
F-measure is an evaluation metric that balances 
precision and recall which has been used to evalu-
ate summarization. Utterance level F-measure 
scores were calculated using the same summaries 
used in our human evaluation. In addition, three 
annotators were asked to create conventional gold 
standard summaries using binary selection. Annot-
ators were not primed in any sense, did not watch 
the lecture videos, and had no sense of the higher 
level purpose of their annotations. We refer to the 
resulting summaries as context-free as they were 
not created under ecologically valid conditions. F-
measure was also calculated with reference to 
these.
The F-measure results (Table 2) point out a few 
interesting phenomena. Firstly, when evaluating a 
32
given  peer  summary  type  with  the  same  model 
type,  the  generic-generic  scores  are  higher  than 
both  the  primed-primed and  context-free-con-
text-free summaries. This means that generic sum-
maries tend to share more utterances with each oth-
er, than primed summaries do, which are more var-
ied. This seems unintuitive at first, but could po-
tentially be explained by the possibility that differ-
ent participants focused on different aspects of the 
priming quiz, due to either perceived importance, 
or lack of time (or summary space) to address all 
of the priming questions.
Peer Type Model Type Average F-measure
generic generic 0.388 
primed generic 0.365 
MMR generic 0.214 
generic primed 0.365 
primed primed 0.374 
MMR primed 0.209 
generic context-free 0.371 
primed context-free 0.351 
MMR context-free 0.243 
context-free context-free 0.374 
Table 2. Average F-measure
We  also  observe  that  generic  summaries  are 
more similar to conventionally annotated (context-
free) summaries than either primed or MMR are. 
This  makes  sense  and  also  confirms  that  even 
though primed summaries do not significantly out-
perform generic summaries in the quiz taking task, 
they are inherently distinguishable from each other.
Furthermore,  when  evaluating  MMR using  F-
measure,  we  see that  MMR summaries  are  most 
similar to the context-free summaries, whose utter-
ance selections can be considered somewhat arbit-
rary.  Our  quiz  results  confirm MMR is  signific-
antly worse  than  generic  and primed summaries. 
This casts doubt on the practice of using similarly 
annotated  summaries  as  gold  standards  for  sum-
marization evaluation using ROUGE.
5 ROUGE Evaluation
More  common  than  F-measure,  ROUGE  (Lin, 
2004) is often used to evaluate summarization. Al-
though Lin (2004) claimed to have demonstrated 
that ROUGE correlates well with human summar-
ies,  both  Murray  et  al.  (2005),  and Liu  and Liu 
(2010) have cast doubt upon this.  It is important to 
acknowledge, however, that ROUGE is actually a 
family of measures, distinguished not only by the 
manner  in  which  overlap  is  measured  (1-grams, 
longest  common  subsequences,  etc.),  but  by  the 
provenience of the summaries that are provided to 
it as references.  If these are not ecologically valid, 
there is no sense in holding ROUGE accountable 
for an erratic result.
   To examine how ROUGE fairs under ecologic-
ally  valid  conditions,  we  calculated  ROUGE-1, 
ROUGE-2,  ROUGE-L, and ROUGE-SU4 on our 
data using the standard options outlined in previ-
ous DUC evaluations. ROUGE scores were calcu-
lated  for  each  of  the  generic  manual  summary, 
primed manual summary, and automatic summary 
conditions.  Each  summary  in  a  given  condition 
was  evaluated  once  against  the  generic  manual  
summaries  and  once  using  the  primed  manual 
summaries.  Similar  to  Liu  and  Liu  (2010), 
ROUGE  evaluation  was  conducted  using  leave-
one-out on the model summary type and averaging 
the results.
In addition to calculating ROUGE on the sum-
maries from our ecologically valid evaluation, we 
also followed  more  conventional  ROUGE evalu-
ation  and  used  the  same  context-free  annotator 
summaries as were used in our F-measure calcula-
tions above. Using these context-free summaries, 
the original  generic  manual,  primed manual,  and 
automatic  summaries  were  evaluated  using 
ROUGE.  The  result  of  these  evaluations  are 
presented in Table 3.
Looking at the ROUGE scores, we can see that 
when evaluated by each type of model summary, 
MMR  performs  worse  than  either  generic  or 
primed manual summaries. This is consistent with 
our quiz results, and perhaps shows that ROUGE 
may be able to distinguish human summaries from 
MMR.  Looking  at  the  generic-generic,  primed-
primed,  and  context-free-context-free scores,  we 
can get a sense of how much agreement there was 
between summaries. It is not surprising that con-
text-free  annotator  summaries  showed  the  least 
agreement,  as  these  summaries  were  generated 
with no higher purpose in mind. This suggests that 
using annotators to generate gold standards in such 
a manner is not ideal.  In addition, real world ap-
plications  for  summarization  would  conceivably 
33
rarely consist of a situation where a summary was 
created for no apparent reason. More interesting is 
the observation that, when measured by ROUGE, 
primed summaries have less in common with each 
other than generic summaries do. The difference, 
however,  is  less  pronounced  when  measured  by 
ROUGE than by F-measure. This is likely due to 
the fact that ROUGE can account for semantically 
similar utterances.
Peer 
type
Model type R-1 R-2 R-L R-SU4
generic generic 0.75461 0.48439 0.75151 0.51547 
primed generic 0.74408 0.46390 0.74097 0.49806 
MMR generic 0.71659 0.40176 0.71226 0.44838 
generic primed 0.74457 0.46432 0.74091 0.49844 
primed primed 0.74693 0.46977 0.74344 0.50254 
MMR primed 0.70773 0.38874 0.70298 0.43802 
generic context-free 0.72735 0.46421 0.72432 0.49573 
primed context-free 0.71793 0.44325 0.71472 0.47805 
MMR context-free 0.69233 0.37600 0.68813 0.42413 
context-
free
context-free 0.70707 0.44897 0.70365 0.48019 
Table 3. Average ROUGE Scores
5.1Correlation with Quiz Scores
In order to assess the ability of ROUGE to predict 
quiz scores, we measured the correlation between 
ROUGE scores and quiz scores on a per participant 
basis. Similar to Murray et al (2005), and Liu and 
Liu (2010), we used Spearman?s rank coefficient 
(rho) to measure the correlation between ROUGE 
and our human evaluation. Correlation was meas-
ured both by calculating Spearman's rho on all data 
points (?all? in Table 4) and by performing the cal-
culation separately for each lecture and averaging 
the results (?avg?). Significant rho values (p-value 
less than 0.05) are shown in bold.
Note that there are not many bolded values, in-
dicating  that  there  are  few  (anti-)correlations 
between quiz scores and ROUGE. The rho values 
reported by Liu and Liu (2010) correspond to the 
?all?  row of  our  generic-context-free  scores  (Liu 
and Liu (2010) did not report ROUGE-L), and we 
obtained  roughly  the  same  scores  as  they
did.  In  contrast  to  this,  our  "all"  generic-generic 
correlations are very low. It is possible that the lec-
tures condition the parameters of the correlation to 
such an extent that fitting all of the quiz-ROUGE
pairs to the same correlation across lectures is un-
reasonable. It may therefore be more useful to look 
at rho  values computed by lecture. For these val-
ues, our R-SU4 scores are not as high relative to R-
1 and R-2 as those reported by Liu and Liu (2010). 
It is also worth noting that the use of context-free 
binary selections as a reference results in increased 
correlation for generic summaries, but substantially 
decreases correlation for primed summaries.
With the exception that generic references prefer 
generic  summaries  and  primed  references  prefer 
primed  summaries,  all  other  values  indicate  that 
both generic and primed summaries are better than 
MMR.  However,  instead  of  ranking  summary 
types,  what  is  important  here  is  the  ecologically 
valid quiz scores.  Our data provides no evidence 
that ROUGE scores accurately predict quiz scores. 
6 Conclusions
We have presented an investigation into how cur-
rent  measures  and  methodologies  for  evaluating 
summarization systems compare to human-centric 
evaluation  criteria.  An  ecologically-valid  evalu-
ation was conducted that determines the value of a 
summary  when  embedded  in  a  task,  rather  than 
how closely a summary resembles a gold standard. 
The  resulting  quiz  scores  indicate  that  manual 
summaries  are  significantly  better  than  MMR. 
ROUGE scores were calculated using the summar-
ies created in the study. In addition, more conven-
tional context-free annotator summaries were also 
used in ROUGE evaluation. Spearman's rho indic-
ated  no  correlation  between  ROUGE scores  and 
our ecologically valid quiz scores. The results offer 
evidence that ROUGE scores and particularly con-
text-free  annotator-generated  summaries  as  gold 
standards may not always be reliably used in place 
of an ecologically valid evaluation.
34
Peer type Model type R-1 R-2 R-L R-SU4
generic generic all 0.017 0.066 0.005 0.058 
lec1 0.236 0.208 0.229 0.208 
lec2 0.276 0.28 0.251 0.092 
lec3 0.307 0.636 0.269 0.428 
lec4 0.193 -0.011 0.175 0.018 
avg 0.253 0.278 0.231 0.187 
primed generic all -0.097 -0.209 -0.090 -0.192 
lec1 -0.239 -0.458 -0.194 -0.458 
lec2 -0.306 -0.281 -0.306 -0.316 
lec3 0.191 0.142 0.116 0.255 
lec4 -0.734 -0.78 -0.769 -0.78 
avg -0.272 -0.344 -0.288 -0.325 
generic primed all 0.009 0.158 -0.004 0.133 
lec1 0.367 0.247 0.367 0.162 
lec2 0.648 0.425 0.634 0.304 
lec3 0.078 0.417 0.028 0.382 
lec4 0.129 0.079 0.115 0.025 
avg 0.306 0.292 0.286 0.218 
primed primed all 0.161 0.042 0.161 0.045 
lec1 0.042 -0.081 0.042 -0.194 
lec2 0.238 0.284 0.259 0.284 
lec3 0.205 0.12 0.205 0.12 
lec4 0.226 0.423 0.314 0.423 
avg 0.178 0.187 0.205 0.158 
generic con-
text-free
all 0.282 0.306 0.265 0.347 
lec1 -0.067 0.296 -0.004 0.325 
lec2 0.414 0.414 0.438 0.319 
lec3 0.41 0.555 0.41 0.555 
lec4 0.136 0.007 0.136 0.054 
avg 0.223 0.318 0.245 0.313 
primed con-
text-free
all -0.146 -0.282 -0.151 -0.305 
lec1 0.151 -0.275 0.151 -0.299 
lec2 -0.366 -0.611 -0.366 -0.636 
lec3 0.273 0.212 0.273 0.202 
lec4 -0.815 -0.677 -0.825 -0.755 
avg -0.189 -0.338 -0.192 -0.372 
Table 4. Correlation (Spearman's rho) between Quiz 
Scores and ROUGE
7 References 
J. Carbonell and J. Goldstein. 1998. The use of mmr, di-
versity-based reranking for reordering documents and 
producing summaries. In Proceedings of the 21st an-
nual  international  ACM SIGIR  conference  on  Re-
search  and  development  in  information  retrieval, 
335-336, ACM.
P. R. Cohen. 1995.  Empirical methods for artificial in-
telligence.  Volume 55. MIT press Cambridge, Mas-
sachusetts.
H.  Christensen,  B.  Kolluru,  Y.  Gotoh,  and S.  Renals. 
2004. From text summarisation to style-specific sum-
marisation for broadcast news. Advances in Informa-
tion Retrieval, 223-237.
L. He, E. Sanocki, A. Gupta, and J. Grudin. 1999. Auto-
summarization of audio-video presentations. In  Pro-
ceedings  of  the  seventh  ACM international  confer-
ence on Multimedia (Part 1), 489-498. ACM.
L. He, E. Sanocki, A. Gupta, and J. Grudin. 2000. Com-
paring presentation summaries: slides vs. reading vs. 
listening. In Proc. of the SIGCHI, 177-184, ACM.
C. Lin.  2004.  Rouge:  a  package for  automatic  evalu-
ation of summaries. In Proc. of ACL, Text Summariz-
ation Branches Out Workshop, 74?81.
F. Liu and Y. Liu. 2010. Exploring correlation between 
rouge and human evaluation on meeting summaries. 
Audio,  Speech,  and  Language  Processing,  IEEE  
Transactions on, 18(1):187-196.
C. Munteanu,  R.  Baecker,  G. Penn,  E.  Toms,  and  D. 
James. 2006. The effect of speech recognition accur-
acy rates on the usefulness and usability of webcast 
archives. In  Proceedings of the SIGCHI conference 
on Human Factors in  computing systems,  493-502, 
ACM.
G. Murray, S. Renals, J. Carletta, and J. Moore. 2005. 
Evaluating automatic summaries of meeting record-
ings. In Proc. of the ACL 2005 MTSE Workshop, Ann 
Arbor, MI, USA, 33-40.
G.  Murray,  T.  Kleinbauer,  P.  Poller,  S.  Renals,  J. 
Kilgour, and T. Becker. 2008. Extrinsic summariza-
tion  evaluation:  A  decision  audit  task.  Machine 
Learning for Multimodal Interaction, 349-361.
G. Penn and X. Zhu. 2008. A critical reassessment of 
evaluation baselines for speech summarization. Proc.  
of ACL-HLT.
S. Tucker, O. Bergman, A. Ramamoorthy, and S. Whit-
taker.  2010.  Catchup:  a  useful  application  of  time-
travel in meetings. In Proc. of CSCW, 99-102, ACM.
S. Tucker and S. Whittaker.  2006.  Time is  of  the  es-
sence:  an  evaluation  of  temporal  compression  al-
gorithms. In Proc. of the SIGCHI, 329-338, ACM.
35
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 32?41,
Baltimore, Maryland, USA. June 27, 2014.
c
?2014 Association for Computational Linguistics
Semantic Role Labeling of Emotions in Tweets
Saif M. Mohammad, Xiaodan Zhu, and Joel Martin
National Research Council Canada
Ottawa, Ontario, Canada K1A 0R6
{saif.mohammad,xiaodan.zhu,joel.martin}@nrc-cnrc.gc.ca
Abstract
Past work on emotion processing has fo-
cused solely on detecting emotions, and
ignored questions such as ?who is feeling
the emotion (the experiencer)?? and ?to-
wards whom is the emotion directed (the
stimulus)??. We automatically compile a
large dataset of tweets pertaining to the
2012 US presidential elections, and anno-
tate it not only for emotion but also for
the experiencer and the stimulus. We then
develop a classifier for detecting emotion
that obtains an accuracy of 56.84 on an
eight-way classification task. Finally, we
show how the stimulus identification task
can also be framed as a classification task,
obtaining an F-score of 58.30.
1 Introduction
Detecting emotions in text has a number of ap-
plications including tracking sentiment towards
politicians, movies, and products (Pang and Lee,
2008), identifying what emotion a newspaper
headline is trying to evoke (Bellegarda, 2010),
developing more natural text-to-speech systems
(Francisco and Gerv?as, 2006), detecting how peo-
ple use emotion-bearing-words and metaphors to
persuade and coerce others (for example, in pro-
paganda) (K?ovecses, 2003), tracking response to
natural disasters (Mandel et al., 2012), and so
on. With the rapid proliferation of microblogging,
there is growing amount of emotion analysis re-
search on newly available datasets of Twitter posts
(Mandel et al., 2012; Purver and Battersby, 2012;
Mohammad, 2012b). However, past work has fo-
cused solely on detecting emotional state. It has
ignored questions such as ?who is feeling the emo-
tion (the experiencer)?? and ?towards whom is the
emotion directed (the stimulus)??.
In this paper, we present a system that analyzes
tweets to determine who is feeling what emotion,
and towards whom. We use tweets from the 2012
US presidential elections as our dataset, since we
expect political tweets to be particularly rich in
emotions. Further, the dataset will be useful for
applications such as determining political align-
ment of tweeters (Golbeck and Hansen, 2011;
Conover et al., 2011b), identifying contentious
issues (Maynard and Funk, 2011), detecting the
amount of polarization in the electorate (Conover
et al., 2011a), and so on.
Detecting the who, what, and towards whom
of emotions is essentially a semantic role-labeling
problem (Gildea and Jurafsky, 2002). The seman-
tic frame for ?emotions? in FrameNet (Baker et al.,
1998) is shown in Table 1. In this work, we fo-
cus on the roles of Experiencer, State, and Stim-
ulus. Note, however, that the state or emotion is
often not explicitly present in text. Other roles
such as Reason, Degree, and Event are also of sig-
nificance, and remain suitable avenues for future
work.
We automatically compile a large dataset of
2012 US presidential elections using a small num-
ber of hand-chosen hashtags. Next we annotate
the tweets for Experiencer, State, and Stimulus
by crowdsourcing to Amazon?s Mechanical Turk.
1
We analyze the annotations to determine the dis-
tributions of different types of roles, and show that
the dataset is rich in emotions. We develop a clas-
sifier for emotion detection that obtains an accu-
racy of 56.84. We find that most of the tweets
express emotions of the tweeter, and only a few
are indicative of the emotions of someone else.
Finally, we show how the stimulus identification
task can be framed as a classification task that cir-
cumvents more complicated problems of detecting
entity mentions and coreferences. Our supervised
classifier obtains an F-score of 58.30 on this task.
1
https://www.mturk.com/mturk/welcome
32
Table 1: The FrameNet frame for emotions. The three roles investigated in this paper are shown in bold.
Role Description
Core:
Event The Event is the occasion or happening that Experiencers in a certain emotional state participate in.
Experiencer The Experiencer is the person or sentient entity that experiences or feels the emotions.
Expressor The body part, gesture, or other expression of the Experiencer that reflects his or her emotional state.
State The State is the abstract noun that describes a more lasting experience by the Experiencer.
Stimulus The Stimulus is the person, event, or state of affairs that evokes the emotional response in the Experiencer.
Topic The Topic is the general area in which the emotion occurs. It indicates a range of possible Stimulus.
Non-Core:
Circumstances The Circumstances is the condition(s) under which the Stimulus evokes its response.
Degree The extent to which the Experiencer?s emotion deviates from the norm for the emotion.
Empathy target The Empathy target is the individual or individuals with which the Experiencer identifies emotionally.
Manner Any way the Experiencer experiences the Stimulus which is not covered by more specific frame elements.
Parameter The Parameter is a domain in which the Experiencer experiences the Stimulus.
Reason The Reason is the explanation for why the Stimulus evokes a certain emotional response.
2 Related Work
Our work here is related to emotion analysis, se-
mantic role labeling (SRL), and information ex-
traction (IE).
Much of the past work on emotion detection
focuses on emotions argued to be the most ba-
sic. For example, Ekman (1992) proposed six ba-
sic emotions?joy, sadness, anger, fear, disgust,
and surprise. Plutchik (1980) argued in favor
of eight?Ekman?s six, surprise, and anticipation.
Many of the automatic systems use affect lexi-
cons pertaining to these basic emotions such as
the NRC Emotion Lexicon (Mohammad and Tur-
ney, 2010), WordNet Affect (Strapparava and Val-
itutti, 2004), and the Affective Norms for English
Words.
2
Affect lexicons are lists of words and as-
sociated emotions.
Emotion analysis techniques have been applied
to many different kinds of text (Mihalcea and Liu,
2006; Genereux and Evans, 2006; Neviarouskaya
et al., 2009; Mohammad, 2012a). More recently
there has been work on tweets as well (Bollen
et al., 2011; Tumasjan et al., 2010; Mohammad,
2012b). Bollen et al. (2011) measured tension,
depression, anger, vigor, fatigue, and confusion
in tweets. Tumasjan et al. (2010) study Twitter
as a forum for political deliberation. Mohammad
(2012b) developed a classifier to identify emotions
using tweets with emotion word hashtags as la-
beled data. However, none of this work explores
the many semantic roles of emotion.
Semantic role labeling (SRL) identifies seman-
tic arguments and roles with regard to a predicate
2
http://www.purl.org/net/NRCEmotionLexicon
http://csea.phhp.ufl.edu/media/anewmessage.html
in a sentence (Gildea and Jurafsky, 2002; M`arquez
et al., 2008; Palmer et al., 2010). More recently,
there has also been some work on semantic role
labeling of tweets for verb and nominal predi-
cates (Liu et al., 2012; Liu et al., 2011). There
exists work on extracting opinions and the top-
ics of opinions, however most of it if focused on
opinions about product features (Popescu and Et-
zioni, 2005; Zhang et al., 2010; Kessler and Ni-
colov, 2009). For example, (Kessler and Nicolov,
2009) identifies semantic relations between sen-
timent expressions and their targets for car and
digital-camera reviews. However, there is no work
on semantic role labeling of emotions in tweets.
We use many of the ideas developed in the senti-
ment analysis work and apply them to detect the
stimulus of emotions in the electoral tweets data.
Our work here is also related to template filling
in information extraction (IE), for example as de-
fined in MUC (Grishman, 1997), which extracts
information (entities) from a document to fill out
a pre-defined template, such as the date, location,
target, and other information about an event.
3 Challenges of Semantic Role Labeling
of Emotions in Tweets
Semantic role labeling of emotions in tweets poses
certain unique challenges. Firstly, there are many
differences between tweets and linguistically well-
formed texts, such as written news (Liu et al.,
2012; Ritter et al., 2011). Tweets are often less
well-formed?they tend to be colloquial, have
misspellings, and have non-standard tokens. Thus,
methods depending heavily on deep language un-
derstanding such as syntactic parsing (Kim and
Hovy, 2006) are less reliable.
33
Secondly, in a traditional SRL system, an ar-
gument frame is a cohesive structure with strong
dependencies between the arguments. Thus it is
often beneficial to develop joint models to identify
the various elements of a frame (Toutanova et al.,
2005). However, these assumptions are less viable
when dealing with emotions in tweets. For exam-
ple, there is no reason to believe that people with a
certain name will have the same emotions towards
the same entities. On the other hand, if we make
use of information beyond the target tweet to inde-
pendently identify the political leanings of a per-
son, then that information can help determine the
person?s emotions towards certain entities. How-
ever, that is beyond the scope of this paper. Thus
we develop independent classifiers for identifying
experiencer, state, and stimulus.
Often, the goal in SRL and IE template filling
is the labeling of text spans in the original text.
However, emotions are often not explicitly stated
in text. Thus we develop a system that assigns an
emotion to a tweet even though that emotion is not
explicitly mentioned. The stimulus of the emo-
tion may also not be mentioned. Consider Happy
to see #4moreyears come into reality. The stimu-
lus of the emotion joy is to see #4moreyears come
into reality. However, the tweet essentially con-
veys the tweeter?s joy towards Barack Obama be-
ing re-elected as president. One may argue that
the true stimulus here is Barack Obama. Thus it is
useful to normalize mentions and resolve the co-
reference, for example, all mentions of Barack H.
Obama, Barack, Obama, and #4moreyears should
be directed to the same entity. Thus, we ground
(in the same sense as in language grounding) the
emotional arguments to the predefined entities.
Through our experiments we show the target of an
emotion in political tweets is often one among a
handful of entities. Thus we develop a classifier to
identify which of these pre-chosen entities is the
stimulus in a given tweet.
4 Data Collection and Annotation
4.1 Identifying Electoral Tweets
We created a corpus of tweets by polling the Twit-
ter Search API, during August and September
2012, for tweets that contained commonly known
hashtags pertaining to the 2012 US presidential
elections. Table 2 shows the query terms we
used. Apart from 21 hashtags, we also collected
tweets with the words Obama, Barack, or Rom-
Table 2: Query terms used to collect tweets per-
taining to the 2012 US presidential elections.
#4moreyears #Barack #campaign2012
#dems2012 #democrats #election
#election2012 #gop2012 #gop
#joebiden2012 #mitt2012 #Obama
#ObamaBiden2012 #PaulRyan2012 #president
#president2012 #Romney #republicans
#RomneyRyan2012 #veep2012 #VP2012
Barack Obama Romney
ney. We used these additional terms because they
are names of the two presidential candidates, and
the probability that these words were used to refer
to somebody else in tweets posted in August and
September of 2012 was low.
The Twitter Search API was polled every four
hours to obtain new tweets that matched the query.
Close to one million tweets were collected, which
we will make freely available to the research com-
munity. The query terms which produced the high-
est number of tweets were those involving the
names of the presidential candidates, as well as
#election2012, #campaign, #gop, and #president.
We used the metadata tag ?iso language code?
to identify English tweets. Since this tag is not al-
ways accurate, we also discarded tweets that did
not have at least two valid English words. We
used the Roget Thesaurus as the English word in-
ventory.
3
This step also helps discard very short
tweets and tweets with a large proportion of mis-
spelled words. Since we were interested in deter-
mining the source and target of emotions in tweets,
we decided to focus on original tweets as opposed
to retweets. We discarded retweets, which can eas-
ily be identified through the presence of RT, rt, or
Rt in the tweet (usually in the beginning of the
post). Finally, there remained close to 170,000
original English tweets.
4.2 Annotating Emotions by Crowdsourcing
We used Amazon?s Mechanical Turk service to
crowdsource the annotation of the electoral tweets.
We randomly selected about 2,000 tweets, each by
a different Twitter user. We set up two question-
naires on Mechanical Turk for the tweets. The first
questionnaire was used to determine the number
of emotions in a tweet and also whether the tweet
was truly relevant to the US politics.
3
www.gutenberg.org/ebooks/10681
34
Questionnaire 1: Emotions in the US election tweets
Tweet: Mitt Romney is arrogant as hell.
Q1. Which of the following best describes the emotions in
this tweet?
? This tweet expresses or suggests an emotional attitude
or response to something.
? This tweet expresses or suggests two or more contrast-
ing emotional attitudes or responses.
? This tweet has no emotional content.
? There is some emotion here, but the tweet does not give
enough context to determine which emotion it is.
? It is not possible to decide which of the above options
is appropriate.
Q2. Is this tweet about US politics and elections?
? Yes, this tweet is about US politics and elections.
? No, this tweet has nothing to do with US politics or
anybody involved in it.
These questionnaires are called HITs (Human In-
telligence Tasks) in Mechanical Turk parlance. We
posted 2042 HITs corresponding to 2042 tweets.
We requested responses from at least three anno-
tators for each HIT. The response to a HIT by an
annotator is called an assignment. In Mechanical
Turk, an annotator may provide assignments for as
many HITs as they wish. Thus, even though only
three annotations are requested per HIT, dozens
of annotators contribute assignments for the 2,042
tweets.
The tweets that were marked as having one
emotion were chosen for annotation by the Ques-
tionnaire 2. We requested responses from at least
five annotators for each of these HITs. Below is
an example:
Questionnaire 2:
Who is feeling what, and towards whom?
Tweet: Mitt Romney is arrogant as hell.
Q1. Who is feeling or who felt an emotion?
Q2. What emotion? Choose one of the options from below
that best represents the emotion.
? anger or annoyance or hostility or fury
? anticipation or expectancy or interest
? disgust or dislike
? fear or apprehension or panic or terror
? joy or happiness or elation
? sadness or gloominess or grief or sorrow
? surprise
? trust or like
Table 3: Questionnaire 1: Percentage of tweets
in each category of Q1. Only those tweets that
were annotated by at least two annotators were in-
cluded. A tweet belongs to category X if it is an-
notated with X more often than all other categories
combined. There were 1889 such tweets in total.
Percentage
of tweets
suggests an emotional attitude 87.98
suggests two contrasting attitudes 2.22
no emotional content 8.21
some emotion; not enough context 1.32
unknown; not enough context 0.26
all 100.0
Q3. Towards whom or what?
After performing a small pilot annotation
effort, we realized that the stimulus in most of
the electoral tweets was one among a handful
of entities. Thus we reformulated question 3 as
shown below:
Q3. What best describes the target of the emotion?
? Barack Obama and/or Joe Biden
? Mitt Romney and/or Paul Ryan
? Some other individual
? Democratic party, democrats, or DNC
? Republican party, republicans, or RNC
? Some other institution
? Election campaign, election process, or elections
? The target is not specified in the tweet
? None of the above
4.3 Annotation Analyses
For each annotator and for each question, we cal-
culated the probability with which the annotator
agreed with the response chosen by the majority
of the annotators. We identified poor annotators as
those that had an agreement probability more than
two standard deviations away from the mean. All
annotations by these annotators were discarded.
We determine whether a tweet is to be assigned
a particular category based on strong majority
vote. That is, a tweet belongs to category X if
it was annotated by at least three annotators and
only if at least half of the annotators agreed with
each other. Percentage of tweets in each of the five
categories of Q1 are shown in Table 3. Observe
that the majority category for Q1 is ?suggests an
emotion??87.98% of the tweets were identified
as having an emotional attitude.
35
Table 4: Questionnaire 2: Percentage of tweets
in the categories of Q2. Only those tweets that
were annotated by at least three annotators were
included. A tweet belongs to category X if it is
annotated with X more often than all other cate-
gories combined. There were 965 such tweets.
Percentage
Emotion of tweets
anger 7.41
anticipation 5.01
disgust 47.75
fear 1.98
joy 6.58
sadness 0.83
surprise 6.37
trust 24.03
all 100.00
Responses to Q2 showed that a large majority
(95.56%) of the tweets were relevant to US pol-
itics and elections. This shows that the hashtags
shown earlier in Table 2 were effective in identify-
ing political tweets.
As mentioned earlier, only those tweets that
were marked as having an emotion (with high
agreement) were annotated further through Ques-
tionnaire 2.
Responses to Q1 of Questionnaire 2 revealed
that in the vast majority of the cases (99.825%),
the tweets contains emotions of the tweeter. The
data did include some tweets that referred to emo-
tions of others such as Romney, GOP, and pres-
ident, but these instances are rare. Tables 4 and
5 give the distributions of the various options for
Questions 2, and 3 of Questionnaire 2. Table 4
shows that disgust (49.32%) is by far the most
dominant emotion in the tweets of 2012 US pres-
idential elections. The next most prominent emo-
tion is that of trust (23.73%). About 61% of the
tweets convey negative emotions towards some-
one or something. Table 5 shows that the stimulus
of emotions was often one of the two presidential
candidates (close to 55% of the time)?Obama:
29.90%, Romney: 24.87%.
4.3.1 Inter-Annotator Agreement
We calculated agreement statistics on the full set
of annotations, and not just on the annotations with
a strong majority as described in the previous sec-
tion. Table 6 shows inter-annotator agreement
(IAA) for the questions?the average percentage of
times two annotators agree with each other. An-
other way to gauge agreement is by calculating
the average probability with which an annotator
Table 5: Questionnaire 2: Percentage of tweets in
the categories of Q3. A tweet belongs to category
X if it is annotated with X more often than all other
categories combined. There were 973 such tweets.
Percentage
Whom of tweets
Barack Obama and/or Joe Biden 29.90
Mitt Romney and/or Paul Ryan 24.87
Some other individual 5.03
Democratic party, democrats, or DNC 2.46
Republican party, republicans, or RNC 8.42
Some other institution 1.23
Election campaign or process 4.93
The target is not specified in the tweet 1.95
None of the above 21.17
all 100.00
Table 6: Agreement statistics: inter-annotator
agreement (IAA) and average probability of
choosing the majority class (APMS).
IAA APMS
Questionnaire 1:
Q1 78.02 0.845
Q2 96.76 0.974
Questionnaire 2:
Q1 52.95 0.731
Q2 59.59 0.736
Q3 44.47 0.641
picks the majority class. The last column in Ta-
ble 6 shows the average probability of picking the
majority class (APMS) by the annotators (higher
numbers indicate higher agreement). Observe that
there is high agreement on determining whether a
tweet has an emotion or not, and on determining
whether the tweet is related to the 2012 US pres-
idential elections or not. The questions in Ques-
tionnaire 2 pertaining to the experiencer, state, and
stimulus were less straightforward and tend to re-
quire more context than just the target tweet for
a clear determination, but yet the annotations had
moderate agreement.
4.4 Access to the data
All of the data is made freely available through the
first author?s website:
http://www.purl.org/net/PoliticalTweets2012
It includes: (1) the complete set of tweets collected
from the Twitter API with hashtags shown in Ta-
ble 2, (2) the subset of English tweets, (3) Ques-
tionnaires 1 and 2, (4) and tweets annotated as per
Questionnaires 1 and 2.
36
5 Automatically Detecting Semantic
Roles of Emotions in Tweets
Since in most instances (99.83%) the experiencer
of emotions in a tweet is the tweeter, we focus
on automatically detecting the other two semantic
roles: the emotional state and the stimulus.
Due to the unique challenges of semantic role
labeling of emotions in tweets described earlier
in the paper, we treat the detection of emotional
state and stimulus as two subtasks for which
we train state-of-the-art support vector machine
(SVM) classifiers. SVM is a learning algorithm
proved to be effective on many classification tasks
and robust on large feature spaces. In our ex-
periments, we exploited several different classi-
fiers and found SVM outperforms others such as
maximum-entropy models (i.e., logistic regres-
sion). We also tested the most popular kernels
such as the polynomial and RBF kernels with dif-
ferent parameters in stratified ten-fold cross val-
idation. We found that a simple linear kernel
yielded the best performance. We used the Lib-
SVM package (Chang and Lin, 2011).
As mentioned earlier, there is fair amount of
work on emotion detection in non-tweet texts
(Boucouvalas, 2002; Holzman and Pottenger,
2003; Ma et al., 2005; John et al., 2006; Mihalcea
and Liu, 2006; Genereux and Evans, 2006; Aman
and Szpakowicz, 2007; Tokuhisa et al., 2008;
Neviarouskaya et al., 2009) as well as on tweets
(Kim et al., 2009; Tumasjan et al., 2010; Bollen et
al., 2011; Mohammad, 2012b; Choudhury et al.,
2012; Wang et al., 2012). In the experiments be-
low we draw from various successfully used fea-
tures described in these papers. More specifically,
the system we use builds on the classifier and fea-
tures used in two previous systems: (1) the sys-
tem described in (Mohammad, 2012b) which was
shown to perform significantly better than some
other previous systems on the news paper head-
lines corpus and the system described in (Moham-
mad et al., 2013) which ranked first (among 44
participating teams) in a 2013 SemEval competi-
tion on detecting sentiment in tweets).
The goal of the experiments in this section is
to apply a state-of-the art emotion detection sys-
tem on the electoral tweets data. We want to
set up baseline performance for emotion detec-
tion on this new dataset and also validate the data
by showing that automatic classifiers can obtain
results that are greater than random and major-
ity baselines. In Section 5.2, we apply the SVM
classifier and various features for the first time on
the task of detecting the stimulus of an emotion in
tweets. In each experiment, we report results of
ten-fold stratified cross-validation.
5.1 Detecting emotional state
5.1.1 Features
We included the following features for detecting
emotional state in tweets.
Word n-grams: We included unigrams (single
words) and bigrams (two-word sequences) into
our feature set. All words were stemmed with
Porter?s stemmer (Porter, 1980).
Punctuations: number of contiguous sequences of
exclamation marks, question marks, or a combina-
tion of them.
Elongated words: the number of words with the
final character repeated 3 or more times (soooo,
mannnnnn, etc). (Elongated words have been used
similarly in (Brody and Diakopoulos, 2011).)
Emoticons: presence/absence of positive and neg-
ative emoticons. The emoticon and its polar-
ity were determined through a regular expres-
sion adopted from Christopher Potts? tokenizing
script.
4
Emotion Lexicons: We used the NRC word?
emotion association lexicon (Mohammad and Tur-
ney, 2010) to check if a tweet contains emo-
tional words. The lexicon contains human anno-
tations of emotion associations for about 14,200
word types. The annotation includes whether
a word is positive or negative (sentiments), and
whether it is associated with the eight basic emo-
tions (joy, sadness, anger, fear, surprise, antici-
pation, trust, and disgust). If a tweet has three
words that have associations with emotion joy,
then the LexEmo emo joy feature takes a value
of 3. We also counted the number of words
with regard to the Osgood?s (Osgood et al., 1957)
semantic differential categories (LexOsg) built
for Wordnet (LexOsg wn) and General Inquirer
(LexOsg gi). To reduce noise, we only consid-
ered the words that have an adjective or adverb
sense in Wordnet.
Negation features: We examined tweets to deter-
mine whether they contained negators such as no,
not, and shouldn?t. An additional feature deter-
mined whether the negator was located close to an
4
http://sentiment.christopherpotts.net/tokenizing.html
37
Table 7: Results for emotion detection.
Accuracy
random baseline 30.26
majority baseline 47.75
automatic SVM system 56.84
upper bound 69.80
Table 8: The accuracies obtained with one of the
feature groups removed. The number in brackets
is the difference with the all features score. The
biggest drop is shown in bold.
Difference from
Experiment Accuracy all features
all features 56.84 0
all - ngrams 53.35 -3.49
all - word ngrams 54.44 -2.40
all - char. ngrams 56.32 -0.52
all - lexicons 54.34 -2.50
all - manual lex. 55.17 -1.67
all - auto lex. 55.38 -1.46
all - negation 55.80 -1.04
all - encodings (elongated words, emoticons, punctns.,
uppercase) 56.82 -0.02
emotion word (as determined by the emotion lex-
icon) in the tweet and in the dependency parse of
the tweet. The list of negation words was adopted
from Christopher Potts? sentiment tutorial.
5
Position features: We included a set of position
features to capture whether the feature terms de-
scribed above appeared at the beginning or the end
of the tweet. For example, if one of the first five
terms in a tweet is a joy word, then the feature
LexEmo joy begin was triggered.
Combined features Though non-linear models
like SVM (with non-linear kernels) can cap-
ture interactions between features, we explic-
itly combined some of our features. For ex-
ample, we concatenated all emotion categories
found in a given tweet. If the tweet contained
both surprise and disgust words, a binary feature
?LexEmo surprise disgust? was triggered. Also,
if a tweet contained more than one joy word
and no other emotion words, then the feature
LexEmo joy only was triggered.
5.1.2 Results
Table 7 shows the results. We included two base-
lines here: the random baseline corresponds to a
system that randomly guesses the emotion of a
tweet, whereas the majority baseline assigns all
5
http://sentiment.christopherpotts.net/lingstruc.html
tweets to the majority category (disgust). Since
the data is significantly skewed towards disgust,
the majority baseline is relative high.
The automatic system obtained by the classi-
fier in identifying the emotions (56.84), which is
significantly higher than the majority baseline. It
should be noted that the highest scores in the Se-
mEval 2013 task of detecting sentiment analysis of
tweets was around 69% (Mohammad et al., 2013).
That task even though related involved only three
classes (positive, negative, and neutral). Thus it is
not surprising that for an 8-way classification task,
the performance is somewhat lower.
The upper bound of the task here is not 100%?
human annotators do not always agree with each
other. To estimate the upper bound we can expect
an automatic system to achieve, for each tweet we
randomly sampled an human annotation from its
multiple annotations and treated it as a system out-
put. We compare it with the majority category
chosen from the remaining human annotations for
that tweet. Such sampling is conducted over all
tweets and then evaluated. The results table shows
this upper bound.
Table 8 shows results of ablation experiments?
the accuracies obtained with one of the feature
groups removed. The higher the drop in per-
formance, the more useful is that feature. Ob-
serve that the ngrams are the most useful fea-
tures, followed by the emotion lexicons. Most of
the gain from ngrams come through word ngrams,
but character ngrams provide small gains as well.
Both the manual and automatic sentiment lexi-
cons were found to be useful to a similar degree.
Paying attention to negation was also beneficial,
whereas emotional encodings such as elongated
words, emoticons, and punctuations did not help
much. It is possible that much of the discrimi-
nating information they might have is already pro-
vided by unigram and character ngram features.
5.2 Detecting emotion stimulus
As discussed earlier, instead of detecting and la-
beling the original text spans, we ground the emo-
tion stimulus directly to the predefined entities.
This allows us to circumvent mention detection
and co-reference resolution on linguistically less
well-formed text. We treat the problem as a classi-
fication task, in which we classify a tweet into one
of the categories defined in Table 5. We believe
that a similar approach is also possible in other
38
Table 9: Results for detecting stimulus.
P R F
random baseline 16.45 20.87 18.39
majority baseline 34.45 38.00 36.14
automatic rule-based system 43.47 55.15 48.62
automatic SVM system 57.30 59.32 58.30
upper bound 82.87 81.36 82.11
domains such as natural disaster tweets and epi-
demic surveillance tweets. We perform a ten-fold
stratified cross-validation.
5.2.1 Features
We used the features below for detecting emotion
stimulus:
Word ngrams: Same as described earlier for
emotional state.
Lexical features: We collected lexicons that
contain a variety of words and phrases describing
the categories in Table 5. For example, the Re-
publican party may be called as ?gop? or ?Grand
Old Party?; all such words or phrases are all put
into the lexicon called ?republican?. We counted
how many words in a given tweet are from each of
these lexicons.
Hashtag features: Hashtags related to the U.S.
election were collected. We organized them into
different categories and use them to further smooth
the sparseness. For example, ?#4moreyear? and
?#obama? are put into the same hashtag lexicon
and any occurrence of such hashtags in a tweet
triggers the feature ?hashtag obama generalized?,
indicating that this is a general version of hashtag
related to president Barack Obama.
Position features: Same as described earlier for
emotional state.
Combined features As discussed earlier, we ex-
plicitly combined some of the above features. For
example, we first concatenate all lexicon and hash-
tag categories found in a given tweet?if the tweet
contains both the general hashtag of ?obama?
and ?romney?, a binary feature ?Hashtag general
obama romney? takes the value of 1.
5.2.2 Results
Table 9 shows the results obtained by the system.
Overall, the system obtains an F-measure of 58.30.
The table also shows upper-bound and baselines
calculated just as described earlier for the emo-
tional state category. We added results for an
additional baseline, rule-based system, here that
chose the stimulus to be: Obama if the tweet had
the terms obama or #obama; Romney if the tweet
had the terms romney or #romney; Republicans if
the tweet had the terms republican, republicans,
or #republicans; Democrats if the tweet had the
terms democrats, democrat, or #democrats; and
Campaign if the tweet had the terms #election or
#campaign. If two or more of the above rules are
triggered in the same tweet, then a label is chosen
at random. This rule-based system based on hand-
chosen features obtains an F-score of 48.62, show-
ing that there are sufficiently many tweets where
key words alone are not sufficient to disambiguate
the true stimulus. Observe that the SVM-based au-
tomatic system performs markedly better than the
majority baseline and also the rule-based system
baseline.
6 Conclusions and Future Work
In this paper, we framed emotion detection as a se-
mantic role labeling problem, focusing not just on
emotional state but also on experiencer and stimu-
lus. We chose tweets about the 2012 US presiden-
tial elections as our target domain. We automati-
cally compiled a large dataset of these tweets using
hashtags, and annotated them first for presence of
emotions, and then for the different semantic roles
of emotions. All of the data is made freely avail-
able.
We found that a large majority of these tweets
(88.1%) carry some emotional attitude towards
someone or something. Further, tweets that con-
vey disgust are twice as prevalent than those that
convey trust. We found that most tweets express
emotions of the tweeter themselves, and the stim-
ulus is often one among a few handful of entities.
We developed a classifier for emotion detection
that obtained an accuracy of 56.84 on an eight-
way classification task. Finally, we showed how
the stimulus identification task can be framed as
a classification task in which our system outper-
forms competitive baselines.
Our future work involves exploring the use of
more tweets from the same user to determine their
political leanings, and use that as an additional fea-
ture in emotion detection. We are also interested in
automatically identifying other semantic roles of
emotions such as degree, reason, and empathy tar-
get (described in Table 1). We believe that a more
sophisticated sentiment analysis applications and
a better understanding of affect require the deter-
mination of semantic roles of emotion.
39
References
Saima Aman and Stan Szpakowicz. 2007. Identifying
expressions of emotion in text. In Vclav Matou?sek
and Pavel Mautner, editors, Text, Speech and Dia-
logue, volume 4629 of Lecture Notes in Computer
Science, pages 196?205. Springer Berlin / Heidel-
berg.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics -
Volume 1, ACL ?98, pages 86?90, Stroudsburg, PA.
Association for Computational Linguistics.
Jerome Bellegarda. 2010. Emotion analysis using la-
tent affective folding and embedding. In Proceed-
ings of the NAACL-HLT 2010 Workshop on Compu-
tational Approaches to Analysis and Generation of
Emotion in Text, Los Angeles, California.
Johan Bollen, Alberto Pepe, and Huina Mao. 2011.
Modeling public mood and emotion: Twitter senti-
ment and socio-economic phenomena. In The Inter-
national AAAI Conference on Weblogs and Social
Media (ICWSM), Barcelona, Spain.
Anthony C. Boucouvalas. 2002. Real time text-
to-emotion engine for expressive internet commu-
nication. Emerging Communication: Studies on
New Technologies and Practices in Communication,
5:305?318.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using
word lengthening to detect sentiment in microblogs.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 562?570, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1?27:27.
Munmun De Choudhury, Scott Counts, and Michael
Gamon. 2012. Not all moods are created equal! ex-
ploring human emotional states in social media. In
The International AAAI Conference on Weblogs and
Social Media (ICWSM).
M D Conover, J Ratkiewicz, M Francisco, B Gonc,
A Flammini, and F Menczer. 2011a. Political po-
larization on Twitter. Networks, 133(26):89?96.
Michael D Conover, Bruno Goncalves, Jacob
Ratkiewicz, Alessandro Flammini, and Filippo
Menczer. 2011b. Predicting the political alignment
of Twitter users. In IEEE Third International
Conference on Privacy Security Risk and Trust and
IEEE Third International Conference on Social
Computing, pages 192?199. IEEE.
Paul Ekman. 1992. An argument for basic emotions.
Cognition and Emotion, 6(3):169?200.
Virginia Francisco and Pablo Gerv?as. 2006. Auto-
mated mark up of affective information in english
texts. In Petr Sojka, Ivan Kopecek, and Karel Pala,
editors, Text, Speech and Dialogue, volume 4188 of
Lecture Notes in Computer Science, pages 375?382.
Springer Berlin / Heidelberg.
Michel Genereux and Roger Evans. 2006. Distin-
guishing affective states in weblogs. In AAAI-2006
Spring Symposium on Computational Approaches to
Analysing Weblogs, pages 27?29, Stanford, USA.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Jennifer Golbeck and Derek Hansen. 2011. Com-
puting political preference among twitter followers.
In Proceedings of the SIGCHI Conference on Hu-
man Factors in Computing Systems, CHI ?11, pages
1105?1108, New York, NY. ACM.
Ralph Grishman. 1997. Information extraction: Tech-
niques and challenges. In SCIE, pages 10?27.
Lars E. Holzman and William M. Pottenger. 2003.
Classification of emotions in internet chat: An appli-
cation of machine learning using speech phonemes.
Technical report, Leigh University.
David John, Anthony C. Boucouvalas, and Zhe Xu.
2006. Representing emotional momentum within
expressive internet communication. In Proceed-
ings of the 24th IASTED international conference on
Internet and multimedia systems and applications,
pages 183?188, Anaheim, CA. ACTA Press.
Jason S. Kessler and Nicolas Nicolov. 2009. Targeting
sentiment expressions through supervised ranking of
linguistic configurations. In 3rd Int?l AAAI Confer-
ence on Weblogs and Social Media (ICWSM 2009).
S. Kim and E. Hovy. 2006. Extracting opinions, opin-
ion holders, and topics expressed in online news me-
dia text. In Proceedings of the Workshop on Senti-
ment and Subjectivity in Text, pages 1?8.
Elsa Kim, Sam Gilbert, Michael J. Edwards, and Er-
hardt Graeff. 2009. Detecting sadness in 140
characters: Sentiment analysis of mourning Michael
Jackson on twitter.
Zolt?an K?ovecses. 2003. Metaphor and Emotion: Lan-
guage, Culture, and Body in Human Feeling (Stud-
ies in Emotion and Social Interaction). Cambridge
University Press.
X. Liu, K. Li, M. Zhou, and Z. Xiong. 2011. En-
hancing semantic role labeling for tweets using self-
training. In AAAI.
X. Liu, Z. Fu, F. Wei, and M. Zhou. 2012. Collective
nominal semantic role labeling for tweets. In AAAI.
Chunling Ma, Helmut Prendinger, and Mitsuru
Ishizuka. 2005. Emotion estimation and reasoning
based on affective textual interaction. In J. Tao and
R. W. Picard, editors, First International Conference
on Affective Computing and Intelligent Interaction
(ACII-2005), pages 622?628, Beijing, China.
40
Benjamin Mandel, Aron Culotta, John Boulahanis,
Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue.
2012. A demographic analysis of online sentiment
during Hurricane Irene. In Proceedings of the Sec-
ond Workshop on Language in Social Media, LSM
?12, pages 27?36, Stroudsburg, PA. Association for
Computational Linguistics.
Llu??s M`arquez, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008. Se-
mantic role labeling: an introduction to the special
issue. Computational Linguistics, 34(2):145?159.
Diana Maynard and Adam Funk. 2011. Automatic
detection of political opinions in tweets. gateacuk,
7117:81?92.
Rada Mihalcea and Hugo Liu. 2006. A corpus-
based approach to finding happiness. In AAAI-2006
Spring Symposium on Computational Approaches to
Analysing Weblogs, pages 139?144. AAAI Press.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: Us-
ing mechanical turk to create an emotion lexicon. In
Proceedings of the NAACL-HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, LA, California.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the seventh international workshop on Seman-
tic Evaluation Exercises (SemEval-2013), Atlanta,
Georgia, USA.
Saif Mohammad. 2012a. Portable features for clas-
sifying emotional text. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 587?591, Montr?eal,
Canada.
Saif M. Mohammad. 2012b. #Emotional tweets. In
Proceedings of the First Joint Conference on Lexi-
cal and Computational Semantics - Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation, SemEval
?12, pages 246?255, Stroudsburg, PA.
Alena Neviarouskaya, Helmut Prendinger, and Mit-
suru Ishizuka. 2009. Compositionality principle in
recognition of fine-grained emotions from text. In
Proceedings of the Third International Conference
on Weblogs and Social Media, pages 278?281, San
Jose, California.
C.E. Osgood, Suci G., and P. Tannenbaum. 1957.
The measurement of meaning. University of Illinois
Press.
Martha Palmer, Daniel Gildea, and Nianwen Xue.
2010. Semantic role labeling. Synthesis Lectures
on Human Language Technologies, 3(1):1?103.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1?2):1?135.
Robert Plutchik. 1980. A general psychoevolutionary
theory of emotion. Emotion: Theory, research, and
experience, 1(3):3?33.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 339?346, Stroudsburg, PA,
USA.
M. Porter. 1980. An algorithm for suffix stripping.
Program, 14:130?137.
Matthew Purver and Stuart Battersby. 2012. Ex-
perimenting with distant supervision for emotion
classification. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association
for Computational Linguistics, EACL ?12, pages
482?491, Stroudsburg, PA. Association for Compu-
tational Linguistics.
A. Ritter, S. Clark, Mausam, and O. Etzioni. 2011.
Named entity recognition in tweets: An experimen-
tal study. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1524?1534.
Carlo Strapparava and Alessandro Valitutti. 2004.
Wordnet-Affect: An affective extension of WordNet.
In Proceedings of the 4th International Conference
on Language Resources and Evaluation (LREC-
2004), pages 1083?1086, Lisbon, Portugal.
Ryoko Tokuhisa, Kentaro Inui, and Yuji Matsumoto.
2008. Emotion classification using massive exam-
ples extracted from the web. In Proceedings of the
22nd International Conference on Computational
Linguistics - Volume 1, COLING ?08, pages 881?
888, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 589?596, Stroudsburg, PA. Association
for Computational Linguistics.
Andranik Tumasjan, Timm O Sprenger, Philipp G
Sandner, and Isabell M Welpe. 2010. Predicting
elections with Twitter : What 140 characters reveal
about political sentiment. Word Journal Of The In-
ternational Linguistic Association, pages 178?185.
Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan,
and Amit P. Sheth. 2012. Harnessing twitter
?big data? for automatic emotion identification. In
Proceedings of the 2012 ASE/IEEE International
Conference on Social Computing, SOCIALCOM-
PASSAT ?12, pages 587?592, Washington, DC,
USA. IEEE Computer Society.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O?Brien-Strain. 2010. Extracting and ranking prod-
uct features in opinion documents. In Proceedings
of the 23rd International Conference on Compu-
tational Linguistics: Posters, COLING ?10, pages
1462?1470, Stroudsburg, PA, USA. Association for
Computational Linguistics.
41
