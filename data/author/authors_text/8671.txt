Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1129?1136
Manchester, August 2008
Multi-Criteria-based Strategy to Stop Active Learning for Data An-
notation 
Jingbo Zhu   Huizhen Wang 
Natural Language Processing Laboratory 
Northeastern University 
Shenyang, Liaoning, P.R.China 110004 
zhujingbo@mail.neu.edu.cn 
wanghuizhen@mail.neu.edu.cn 
Eduard Hovy 
University of Southern California 
Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
hovy@isi.edu 
 
Abstract 
In this paper, we address the issue of de-
ciding when to stop active learning for 
building a labeled training corpus. Firstly, 
this paper presents a new stopping crite-
rion, classification-change, which con-
siders the potential ability of each unla-
beled example on changing decision 
boundaries. Secondly, a multi-criteria-
based combination strategy is proposed 
to solve the problem of predefining an 
appropriate threshold for each confi-
dence-based stopping criterion, such as 
max-confidence, min-error, and overall-
uncertainty. Finally, we examine the ef-
fectiveness of these stopping criteria on 
uncertainty sampling and heterogeneous 
uncertainty sampling for active learning. 
Experimental results show that these 
stopping criteria work well on evaluation 
data sets, and the combination strategies 
outperform individual criteria. 
1 Introduction 
Creating a large labeled training corpus is very 
expensive and time-consuming in some real-
world applications. For example, it is a crucial 
issue for automated word sense disambiguation 
task, because validations of sense definitions and 
sense-tagged data annotation have to be done by 
human experts, e.g. OntoNotes project (Hovy et 
al., 2006).  
Active learning aims to minimize the amount 
of human labeling effort by automatically select-
ing the most informative unlabeled example for 
human annotation. In recent years active learning 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
has been widely studied in natural language 
processing (NLP) applications, such as word 
sense disambiguation (WSD) (Chen et al, 2006; 
Zhu and Hovy, 2007), text classification (TC) 
(Lewis and Gale, 1994; McCallum and Nigam, 
1998a), named entity recognition (Shen et al, 
2004), chunking (Ngai and Yarowsky, 2000), 
and statistical parsing (Tang et al, 2002). 
However, deciding when to stop active learn-
ing is still an unsolved problem and seldom men-
tioned issue in previous studies. Actually it is a 
very important practical issue in real-world ap-
plications, because it obviously makes no sense 
to continue the active learning procedure until 
the whole unlabeled corpus has been labeled. 
The active learning process can be ended when 
the current classifier reaches the maximum effec-
tiveness. In principle, how to learn a stopping 
criterion is a problem of estimation of classifier 
(i.e. learner) effectiveness during active learning 
(Lewis and Gale, 1994).  
In this paper, we address the issue of a stop-
ping criterion for pool-based active learning with 
uncertainty sampling (Lewis and Gale, 1994), 
and propose a multi-criteria-based approach to 
determining when to stop active learning process. 
Firstly, this paper makes a comprehensive analy-
sis on some confidence-based stopping criteria 
(Zhu and Hovy, 2007), including max-
confidence, min-error and overall-uncertainty, 
then proposes a new stopping criterion, classifi-
cation-change, which considers the potential 
ability of each unlabeled example on changing 
decision boundaries. Secondly, a combination 
strategy is proposed to solve the problem of pre-
defining an appropriate threshold for each confi-
dence-based stopping criterion in a specific task.  
In uncertainty sampling scheme, the most un-
certain unlabeled example is considered as the 
most informative case selected by active learner 
at each learning cycle. However, an uncertain 
example for one classifier may be not an uncer-
1129
tain example for other classifiers. When using 
active learning for real-world applications such 
as WSD, it is possible that a classifier of one type 
selects samples for training a classifier of another 
type, called the heterogeneous approach (Lewis 
and Catlett, 1994). For example, the final trained 
classifier for WSD is often different from the 
classifier used in active learning for constructing 
the training corpus.  
To date, no one has studied the stopping crite-
rion issue for the heterogeneous approach. In this 
paper, we examine the effectiveness of each 
stopping criterion on both traditional uncertainty 
sampling and heterogeneous uncertainty sam-
pling for active learning. Experimental results of 
active learning for WSD and TC tasks show that 
these proposed stopping criteria work well on 
evaluation data sets, and the combination strate-
gies outperform individual criteria. 
2 Active Learning Process 
In this paper, we are interested in uncertainty 
sampling for pool-based active learning (Lewis 
and Gale, 1994), in which an unlabeled example 
x with maximum uncertainty is selected to aug-
ment the training data at each learning cycle. The 
maximum uncertainty implies that the current 
classifier has the least confidence on its classifi-
cation of this unlabeled example.  
Actually active learning is a two-stage process 
in which a small number of labeled samples and 
a large number of unlabeled examples are first 
collected in the initialization stage, and a closed-
loop stage of query and retraining is adopted.  
Procedure: Active Learning Process 
Input: initial small training set L, and pool of unla-
beled data set U 
Use L to train the initial classifier C  
Repeat 
1. Use the current classifier C to label all unla-
beled examples in U 
2. Use uncertainty sampling technique to select m 
most informative unlabeled examples, and ask 
oracle H for labeling 
3. Augment L with these m new examples, and 
remove them from U 
4. Use L to retrain the current classifier C 
Until the predefined stopping criterion SC is met. 
Figure 1. Active learning with uncertainty sam-
pling technique 
3 Stopping Criteria for Active Learning  
In this section, we mainly address the problem of 
general stopping criteria for active leanring, and 
study how to define a reasonable and appropriate 
stopping criterion SC shown in Fig. 1. 
3.1 Effectiveness Estimation and Confi-
dence Estimation 
To examine whether the classifier has reached 
the maximum effectiveness during active learn-
ing procedure, it seems an appealing solution 
when repeated learning cycles show no signifi-
cant performance improvement. However, this is 
often not feasible. To investigate the impact of 
performance change on defining a stopping crite-
rion for active learning, we first give an example 
of active learning for WSD shown in Fig. 2. 
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0  20  40  60  80  100  120  140  160  180  200  220  240  260  280  300
A
cc
ur
ac
y
Number of Learned Examples
Active Learning for WSD task
interest
 
Figure 2. An example of active learning for WSD 
on word ?interest?. 
 
Fig. 2 shows that the accuracy performance 
generally increases, but apparently degrades at 
iterations 30, 90 and 190, and does not change 
anymore during iterations 220-260 in the active 
learning process. Actually the first time of the 
highest performance of 91.5% is achieved at 900 
which is not shown in Fig. 2. Although the accu-
racy performance curve shows an increasing 
trend, it is not monotonically increasing. It is not 
easy to automatically determine the point of no 
significant performance improvement on the 
validation set, because points such as 30 or 90 
would mislead a final judgment.  
Besides, there is a problem of performance es-
timation of the current classifier during active 
learning process, because a separate validation 
set should be prepared in advance, a procedure 
that causes additional (high) cost since it is often 
done manually. Besides, how many samples are 
required for the pregiven separate validation set 
is an open question. Too few samples may not be 
adequate for a reasonable estimate and may re-
sult in an incorrect result. Too many samples 
would increase the building cost.  
To define a stopping criterion for active learn-
ing, Zhu and hovy (2007) considered the estima-
tion of the classifier?s effectiveness as the second 
1130
task of confidence estimation of the classifier on 
its classification of all remaining unlabeled data. 
In the following section, we first introduce two 
confidence-based criteria, max-confidence and 
min-error, proposed by Zhu and Hovy (2007). 
3.2 Max-Confidence 
In uncertainty sampling scheme, if the uncer-
tainty value of the most informative unlabeled 
example is sufficiently small, we can assume that 
the current classifier has sufficient confidence on 
its classification of the remaining unlabeled data. 
So the active learning process can be ended. 
Based on such assumption, Zhu and Hovy (2007) 
proposed max-confidence criterion based on the 
uncertainty estimation of the most informative 
unlabeled example. Its strategy is to consider 
whether the uncertainty value of the most infor-
mative unlabeled example is less than a very 
small predefined threshold. 
3.3 Min-Error 
As shown in Fig. 1, in uncertainty sampling 
scheme, the current classifier has the least confi-
dence on its classification of these top-m selected 
unlabeled examples. If the current classifier can 
correctly classify these most informative exam-
ples, we can assume that the current classifier 
have sufficient confidence on its classification of 
the remaining unlabeled data. Based on such as-
sumption, Zhu and Hovy (2007) proposed min-
error criterion based on feedback from the oracle. 
Its strategy is to consider whether the current 
classifier can correctly predict the labels on these 
selected unlabeled examples, or the accuracy 
performance of the current classifier on these 
most informative examples is larger than a prede-
fined threshold.  
3.4 Overall-Uncertainty 
The motivation behind the overall-uncertainty 
method is similar to that of the max-confidence 
method. However, the max-confidence method 
only considers the most informative example at 
each learning cycle. The overall-uncertainty 
method considers the overall uncertainty on all 
unlabeled examples. If the overall uncertainty of 
all unlabeled examples becomes very small, we 
can assume that the current classifier has suffi-
cient confidence on its classification of the re-
maining unlabeled data. Based on such assump-
tion, we propose overall-uncertainty method 
which is to consider whether the average uncer-
tainty value of all remaining unlabeled examples 
is less than a very small predefined threshold. 
3.5 Classification-Change 
There is another problem of estimating classifier 
performance during active learning process. 
Cross-validation on the training set is almost im-
practical during the active learning procedure, 
because the alternative of requiring a held-out 
validation set for active learning is counterpro-
ductive. Hence we should look for a self-
contained method. 
Actually the motivation behind uncertainty 
sampling is to find some unlabeled examples 
near decision boundaries, and use them to clarify 
the position of decision boundaries. The current 
classifier considers such unlabeled examples near 
decision boundaries as the most informative ex-
amples in uncertainty sampling scheme for active 
learning. In other words, we assume that an 
unlabeled example with maximum uncertainty 
has the highest chance to change the decision 
boundaries. 
Based on the above analysis, we think the ac-
tive learning process can stop if there is no unla-
beled example that can potentially change the 
decision boundaries. However, in practice, it is 
almost impossible to exactly recognize which 
unlabeled example can truly change the decision 
boundaries in the next learning cycle, because 
the true label of each unlabeled example is un-
known. 
To solve this problem, we make an assump-
tion that labeling an unlabeled example may shift 
the decision boundaries if this example was pre-
viously ?outside? and is now ?inside?. In other 
words, if an unlabeled example is automatically 
assigned to two different labels during two recent 
learning cycles 2 , we think that the labeling of 
this unlabeled example has a good chance to 
change the decision boundaries.  
Based on such assumption, we propose a new 
approach based on classification change of each 
unlabeled example during two recent consecutive 
learning cycles (?previous? and ?current?), called 
the classification-change method. Its strategy is 
to stop the active learning process by considering 
whether no classification change happens to the 
remaining unlabeled examples during two recent 
consecutive learning cycles. If true, we assume 
that the current classifier has sufficient confi-
dence on its classification of the remaining unla-
                                                 
2 For example, an unlabeled example x was classified into 
class A at ith iteration, and class B at i+1th iteration. 
1131
beled data, because all unlabeled examples near 
decision boundaries have been exhausted, and no 
further labeling will affect active learner. 
4 Combination Strategy  
As for the above three confidence-based stopping 
criteria such as max-confidence, min-error and 
overall-uncertainty, how to automatically deter-
mine an appropriate threshold in a specific task is 
a crucial problem. We think that different appro-
priate thresholds are needed for various active 
learning applications.  
To solve this problem, in this section we pro-
pose a general combination strategy by consider-
ing the best of both classification-change and a 
confidence-based criterion, in which the prede-
fined threshold of the confidence-based stopping 
criterion can be automatically updated during 
active learning.  
The motivation behind the general combina-
tion strategy is to check whether the active learn-
ing becomes stable (i.e. check whether the classi-
fication-change method is met) when the current 
confidence-based stopping criterion is satisfied. 
If not, we think there are some remaining unla-
beled examples that can potentially shift the de-
cision boundaries, even if they are considered as 
certain cases from the current classifier?s view-
points. In this case, the threshold of the current 
confidence-based stopping criterion should be 
automatically revised to keep continuing the ac-
tive learning process. The general combination 
strategy can be summarized as follows. 
Procedure: General combination strategy 
Given: 
z stopping criterion 1: max-confidence or min-
error or overall-uncertainty 
z Stopping criterion 2: classification-change 
z The predefined threshold for stopping criterion 1 
is initially set to ? 
Steps(during active learning process): 
1. First check whether stopping criterion 1 is satis-
fied. If yes, go to 2; 
2. Then check whether stopping criterion 2 is satis-
fied. If yes, goto 4), otherwise goto 3; 
3. Automatically update the current threshold to be 
a new smaller value for max-confidence and 
overall-uncertainty, or to be a new larger value 
for min-error, and then goto 1. 
4. Stop active learning process.  
Figure 3. General combination strategy 
 
? Strategy 1: This strategy combines the max-
confidence and classification-change meth-
ods simultaneously.  
? Strategy 2: This strategy combines the min-
error and classification-change methods si-
multaneously. 
? Strategy 3: This strategy combines the over-
all-uncertainty and classification-change 
methods simultaneously. 
5 Evaluation 
5.1 Experimental Settings 
In the following sections, we evaluate the 
effectiveness of seven stopping criteria for active 
learning for WSD and TC tasks, including max-
confidence (MC), min-error (ME), overall-
uncertainty (OU), classification-change (CC), 
strategy 1 (CC-MC), strategy 2 (CC-ME), and 
strategy 3 (CC-OU). Following previous studies 
(Zhu and Hovy, 2007), the predefined thresh-
olds3 used for MC, ME and OU are set to 0.01, 
0.9 and 0.01, respectively. 
To evaluate the effectiveness of each stopping 
criterion, we first construct two types of baseline 
methods called ?All? and ?First? methods. ?All? 
method is defined as when all unlabeled exam-
ples in the pool are learned. ?First? method is 
defined as when the current classifier reaches the 
same performance of the ?All? method at the 
first time during the active learning process.  
A better stopping criterion can not only 
achieve almost the same performance given by 
the ?All? baseline method (i.e. accuracy 
performance), but also learn almost the same 
number of unlabeled examples by the ?First? 
baseline method (i.e. percentage performance).  
In uncertainty sampling scheme, the well-
known entropy-based uncertainty measurement 
(Chen et al, 2006; Schein and Ungar, 2007) is 
used in our active learning study as follows: 
( ) ( | ) log ( | )
y Y
UM x P y x P y x
?
= ??         (1) 
where P(y|x) is the a posteriori probability. We 
denote the output class y?Y={y1, y2, ?, yk}. UM 
is the uncertainty measurement function based on 
the entropy estimation of the classifier?s 
posterior distribution. 
We utilize maximum entropy (MaxEnt) model 
(Berger et al, 1996) to design the basic classifier 
used in active learning for WSD and TC tasks. 
The advantage of the MaxEnt model is the ability 
to freely incorporate features from diverse 
sources into a single, well-grounded statistical 
                                                 
3 In the following experiments, these thresholds are also 
used as initial values of ? for individual criteria in the gen-
eral combination strategy shown in Fig. 3. 
1132
model. A publicly available MaxEnt toolkit4 was 
used in our experiments. To build the MaxEnt-
based classifier for WSD, three knowledge 
sources are used to capture contextual informa-
tion: unordered single words in topical context, 
POS of neighboring words with position infor-
mation, and local collocations, which are the 
same as the knowledge sources used in (Lee and 
Ng, 2002). In the design of text classifier, the 
maximum entropy model is also utilized, and no 
feature selection technique is used. 
In the following active learning comparison 
experiments, the algorithm starts with a 
randomly chosen initial training set of 10 labeled 
examples, and makes 10 queries after each 
learning iteration. A 10 by 10-fold cross-
validation was performed. All results reported 
are the average of 10 trials in each active 
learning process. In the following comparison 
experiments, the performance reported on 
Ontonotes data set is the macro-average on ten 
nouns, and the performance on TWA data set is 
the macro-average on six words. 
5.2 Data Sets 
Six publicly available natural data sets have been 
used in the following active learning comparison 
experiments. Three data sets are used for TC 
tasks: WebKB, Comp2a and Comp2b. The other 
three data sets are used for WSD tasks: 
OntoNotes, Interest and TWA.  
The WebKB dataset was widely used in TC 
research. Following previous studies (McCallum 
and Nigam, 1998b), we use the four most popu-
lous categories: student, faculty, course and pro-
ject, altogether containing 4199 web pages. In 
the preprocessing step, we only remove those 
words that occur merely once without using 
stemming. The resulting vocabulary has 23803 
words. 
The Comp2a data set consists of comp.os.ms-
windows.misc and comp.sys.ibm.pc.hardware 
subset of NewsGroups. The Comp2b data set 
consists of comp.graphics and comp.windows.x 
categories from NewsGroups. Both two data sets 
have been previously used in active learning for 
TC (Roy and McCallum, 2001; Schein and Un-
gar, 2007). 
The OntoNotes project (Hovy et al, 2006) 
uses the WSJ part of the Penn Treebank. The 
senses of noun words occurring in OntoNotes are 
linked to the Omega ontology. Ontonotes has 
                                                 
                                                4See  http://homepages.inf.ed.ac.uk/s0450736/maxent_ 
toolkit.html 
been used previously in active learning for WSD 
tasks (Zhu and Hovy, 2007). In the following 
comparison experiments, we focus on 10 most 
frequent nouns 5  previously used in (Zhu and 
Hovy, 2007): rate, president, people, part, point, 
director, revenue, bill, future, and order.  
The Interest data set developed by Bruce and 
Wiebe (1994) has been previously used for WSD 
(Ng and Lee, 1996). This data set consists of 
2369 sentences of the noun ?interest? with its 
correct sense manually labeled. The noun 
?interest? has six different senses in this data set.  
TWA developed by Mihalcea and Yang on 2003, 
is sense tagged data for six words with two-way 
ambiguities, previously used in WSD research. 
These six words are bass, crane, motion, palm, 
plant and tank. All instances were drawn from 
the British National Corpus. 
5.3 Stopping Criteria for Uncertainty Sam-
pling 
In order to evaluate the effectiveness of our stop-
ping criteria, we first apply them to uncertainty 
sampling for active learning for WSD and TC 
tasks. Table 1 shows that ?First? method gener-
ally achieves higher performance than that of the 
?All? method.  We can see from the ?Average? 
row that stopping criteria MC, ME, CC-MC, CC-
ME and CC-OU achieve close average accuracy 
performance to the ?All? method whereas OU 
and CC achieve lower average accuracy 
performance. OU method achieves the lowest 
average accuracy performance. CC-ME achieves 
the highest average accuracy of 89.6%, followed 
by CC-MC. 
Compared to the ?First? method, CC-OU 
achieves the best average percentage 
performance of 37.03% (i.e. the closest one to 
the ?First? method), followed by ME method. On 
six evaluation data sets, Table 1 shows that CC-
ME method achieves 4 out of 6 highest accuracy 
performances, followed by CC-MC and MC 
methods. And CC-ME method also achieves 3 
out of 6 best percentage performance, followed 
by CC, CC-OU and ME methods.  
Among these four individual stopping criteria, 
ME outperforms MC, OU and CC. However, ME 
method can only be applied to batch-based 
selection because ME criterion is based on the 
feedback from Oracle. Too few informative 
candidates may not be adequate for obtaining a 
reasonable feedback for ME criterion.  
 
5 See http://www.nlplab.com/ontonotes-10-nouns.rar 
1133
Data set All First MC ME OU CC CC-MC CC-ME CC-OU
0.910 0.911 0.910 0.910 0.837 0.912 0.912 0.913 0.912 WebKB 
100% 31.50% 27.11% 29.11% 8.42% 31.53% 32.37% 33.02% 31.53%
0.880 0.884 0.877 0.879 0.868 0.876 0.879 0.880 0.876 Comp2a 
100% 35.12% 31.35% 31.28% 23.29% 27.35% 32.36% 36.80% 27.35% 
0.900 0.901 0.887 0.888 0.880 0.879 0.891 0.893 0.882 Comp2b 
100% 41.66% 37.52% 36.76% 28.36% 30.80% 37.95% 40.03% 31.81% 
0.939 0.942 0.929 0.934 0.928 0.936 0.940 0.939 0.939 Ontonotes 
100% 22.81% 30.19% 22.14% 21.81% 18.96% 34.77% 25.60% 24.75% 
0.908 0.910 0.910 0.906 0.906 0.901 0.910 0.906 0.906 Interest 
100% 29.83% 37.54% 28.25% 28.51% 25.55% 37.54% 28.67% 28.62% 
0.846 0.858 0.843 0.844 0.837 0.820 0.841 0.845 0.838 TWA 
100% 59.67% 80.34% 72.71% 70.47% 61.54% 86.99% 80.15% 78.12% 
0.897 0.901 0.892 0.893 0.876 0.887 0.895 0.896 0.892 Average 
100% 37.43% 40.67% 36.71% 30.14% 32.62% 43.66% 40.71% 37.03%
Table 1. Effectiveness of seven stopping criteria for uncertainty sampling for active learning. For each 
data set, Table 1 shows the accuracy of the classifier and percentage of learned instances over all 
unlabeled data when each stopping criterion is met. The boldface numbers indicate the best corre-
sponding performances. 
Data set All MC ME OU CC CC-MC CC-ME CC-OU 
0.858 0.808 0.818 0.601 0.820 0.820 0.824 0.820 WebKB 
100% 27.11% 29.11% 8.42% 31.53% 32.37% 33.02% 31.53% 
0.894 0.838 0.839 0.825 0.837 0.838 0.846 0.837 Comp2a 
100% 31.35% 31.28% 23.29% 27.35% 32.36% 36.80% 27.35% 
0.922 0.884 0.882 0.878 0.874 0.885 0.883 0.879 Comp2b 
100% 37.52% 36.76% 28.36% 30.80% 37.95% 40.03% 31.81% 
0.925 0.923 0.924 0.921 0.921 0.932 0.927 0.929 Ontonotes 
100% 30.19% 22.14% 21.81% 18.96% 34.77% 25.60% 24.75% 
0.899 0.906 0.890 0.890 0.885 0.906 0.891 0.890 Interest 
100% 37.54% 28.25% 28.51% 25.55% 37.54% 28.67% 28.62% 
0.812 0.784 0.793 0.765 0.775 0.799 0.810 0.794 TWA 
100% 80.34% 72.71% 70.47% 61.54% 86.99% 80.15% 78.12% 
0.885 0.857 0.857 0.813 0.852 0.863 0.863 0.858 Average 
100% 40.67% 36.71% 30.14% 32.62% 43.66% 40.71% 37.03% 
Table 2. Effectiveness of seven stopping criteria for heterogeneous uncertainty sampling for active 
learning. Table 2 shows the accuracy of the classifier and percentage of learned instances over all 
unlabeled data when each stopping criterion is met. The boldface numbers indicate the best corre-
sponding performances. 
 
Interestingly, our proposed CC method 
acheves the best macro-average percentage 
performance on the TWA data set, however, 
other criteria work poorly, compared to the 
?First? method. Actually the sense distribution of 
each noun in TWA set is very skewed. From 
WSD experimental results on TWA, we found 
that only few learned instances can train the 
MaxEnt-based classifier with the highest 
accuracy performance.  
In Table 1, the boldface numbers indicate the 
best performances. Three combination strategies 
achieve 12 out of 16 best performances 6 . We 
                                                                                                                          
6 CC and CC-OU methods achieve the same best percentage 
performance of 31.53% on WebKB data set. MC and CC-
think the general combination strategy 
outperform individual stopping criteria for 
uncertainty sampling for active learning, because 
four individual stopping criteria only totally 
achieve 4 out of 16 best performances. 
5.4 Stopping Criteria for Heterogeneous 
Uncertainty Sampling 
In the following comparison experiments on het-
erogeneous uncertainty sampling, a MaxEnt-
based classifier is used to select the most infor-
mative examples for training an another type of 
classifier based on multinomial na?ve Bayes (NB) 
model (McCallum and Nigam, 1998b).  
 
MC methods achieve the same highest accuracy perform-
ance of 91% on Interest data set.  
1134
Table 2 shows that the NB-based classifier 
trained on all data (i.e. ?All method?) achieves 
only 1.2% lower average accuracy performance 
than that of MaxEnt-based classifier. However, 
we can see from Table 2 that accuracy perform-
ances of each stopping criterion for heterogene-
ous uncertainty sampling are apparently lower 
than that for uncertainty sampling shown in Ta-
ble 1. The main reason is that an uncertain ex-
ample for one classifier (i.e. MaxEnt) may not be 
an uncertain example for other classifiers (i.e. 
NB). This comparison experiments aim to ana-
lyze the accuracy effectiveness of stopping crite-
ria for heterogeneous uncertainty sampling, 
compared to that for uncertainty sampling shown 
in Table 1. Therefore we do not provide the re-
sults of the ?First? method for heterogeneous 
uncertainty sampling. The ?Average? row shows 
that CC-MC and CC-ME achieve the highest 
average accuracy performance of 86.3%, fol-
lowed by CC-OU. On six data sets, CC-ME 
achieves 3 out of 6 highest accuracy perform-
ances.  
Interestingly, these stopping criteria work very 
well on the Ontonotes and Interest data sets. 
Three combination strategies achieve higher ac-
curacy performance than the ?All? method on 
Ontonotes. However, the accuracy performances 
of these seven stopping criteria for heterogene-
ous uncertainty sampling on WebKB, Comp2a, 
Comp2b, and TWA degrade, compared to the 
?All? method.  
The general combination strategy achieves 7 
out of 9 boldface accuracy performances7. And 
only MC method achieves other 2 boldface accu-
racy performances. Experimental results show 
that the general combination strategy outper-
forms individual stopping criteria in overall for 
heterogeneous uncertainty sampling.   
6 Related Work 
 Zhu and Hovy (2007) proposed a confidence-
based framework to predict the upper bound and 
the lower bound for a stopping criterion in active 
learning. Actually this framework is a very 
coarse solution that simply uses max-confidence 
method to predict the upper bound, and uses min-
error method to predict the lower bound. Zhu et. 
al. (2008) proposed a minimum expected error 
strategy to learn a stopping criterion through es-
                                                 
7 MC and CC-MC methods achieve the same highest accu-
racy performance of 90.6% on Interest data set. CC-MC and 
CC-CA methods achieve the same highest average accuracy 
performance of 86.3%. 
timation of the classifier?s expected error on fu-
ture unlabeled examples. However, both two 
studies did not give an answer to the problem of 
how to define an appropriate threshold for the 
stopping criterion in a specific task.   
Vlachos (2008) also studied a stopping crite-
rion of active learning based on the estimate of 
the classifier?s confidence, in which a separate 
and large dataset is prepared in advance to esti-
mate the classifier?s confidence. However, there 
is a risk to be misleading because how many 
examples are required for this pregiven separate 
dataset is an open question in real-world 
applications, and it can not guarantee that the 
classifier shows a rise-peak-drop confidence 
pattern during active learning process.  
Schohn and Cohn (2000) proposed a stopping 
criterion for active learning with support vector 
machines based on an assumption that the data 
used is linearly separable. However, in most real-
world cases this assumption seems to be 
unreasonable and difficult to satisfy. And their 
stopping criterion cannot be applied for active 
learning with other type of classifier such as NB, 
MaxEnt models.  
7 Discussion 
We believe that a classifier?s performance 
change is a good signal of stopping the active 
learning process. It is worth studying further how 
to combine the factor of performance change 
with our proposed stopping criteria. 
Among these stopping criteria, ME, CC, CC-
ME can be used directly for committee-based 
sampling (Engelson and Dagan, 1999) for active 
learning. However, to use MC, OU, CC-MC and 
CC-OU for committee-based sampling, we 
should adopt a new uncertainty measurement 
such as vote entropy to measure the uncertainty 
of each unlabled example in the pool. 
In the above active learning comparison 
experiments, the confidence estimation for each 
confidence-based stopping criterion is done 
within the unlabeled pool U. We think that for 
these confidence-based stopping criteria except 
SA method, confidence estimation on a large-
scale outside unlabeled data set is worth studying 
in the future work. 
8 Conclusion and Future Work 
In this paper, we address the stopping criterion 
issue of active learning, and propose a new stop-
ping criterion, classification-change, which con-
siders the potential ability of each unlabeled ex-
1135
ample on changing decision boundaries. To solve 
the problem of predefining an appropriate 
threshold for each confidence-based stopping 
criterion, a multi-criteria-based general combina-
tion strategy is proposed. Experimental results on 
uncertainty sampling and heterogeneous uncer-
tainty sampling show that these stopping criteria 
work well on evaluation data sets, and combina-
tion strategies can achieve better performance 
than individual criteria. Some interesting future 
work is to investigate further how to combine the 
best of these criteria, and how to consider per-
formance change to define an appropriate stop-
ping criterion for active learning.  
Acknowledgments 
This work was supported in part by the National 
863 High-tech Project (2006AA01Z154) and the 
Program for New Century Excellent Talents in 
University (NCET-05-0287). 
References 
Berger Adam L., Vincent J. Della Pietra, Stephen A. 
Della Pietra. 1996. A maximum entropy approach 
to natural language processing. Computational 
Linguistics 22(1):39?71. 
Bruce Rebecca and Janyce Wiebe. 1994. Word sense 
disambiguation using decomposable models. Pro-
ceedings of the 32nd annual meeting on Associa-
tion for Computational Linguistics, pp. 139-146. 
Chen Jinying, Andrew Schein, Lyle Ungar and Mar-
tha Palmer. 2006. An empirical study of the behav-
ior of active learning for word sense disambigua-
tion. Proceedings of the main conference on Hu-
man Language Technology Conference of the 
North American Chapter of the Association of 
Computational Linguistics, pp. 120-127 
Engelson S. Argamon and I. Dagan. 1999. Commit-
tee-based sample selection for probabilistic classi-
fiers. Journal of Artificial Intelligence Research 
(11):335-360. 
Hovy Eduard, Mitchell Marcus, Martha Palmer, 
Lance Ramshaw and Ralph Weischedel. 2006. 
Ontonotes: The 90% Solution. In Proceedings of 
the Human Language Technology Conference of 
the NAACL, pp. 57-60. 
Lee Yoong Keok and Hwee Tou Ng. 2002. An em-
pirical evaluation of knowledge sources and learn-
ing algorithm for word sense disambiguation. In 
Proceedings of the ACL conference on Empirical 
methods in natural language processing, pp. 41-48 
Lewis David D. and Jason Catlett. 1994. Heterogene-
ous uncertainty sampling for supervised learning. 
In Proceedings of 11th International Conference on 
Machine Learning, pp. 148-156 
Lewis David D. and William A. Gale. 1994. A se-
quential algorithm for training text classifiers. In 
Proceedings of the 17th annual international ACM 
SIGIR conference on Research and development in 
information retrieval, pp. 3-12 
McCallum Andrew and Kamal Nigam. 1998a. Em-
ploying EM in pool-based active learning for text 
classification. In Proceedings of the 15th Interna-
tional Conference on Machine Learning, pp.350-
358 
McCallum Andrew and Kamal Nigam. 1998b. A 
comparison of event models for na?ve bayes text 
classification. In AAAI-98 workshop on learning 
for text categorization. 
Ng Hwee Tou and Hian Beng Lee. 1996. Integrating 
multiple knowledge sources to disambiguate word 
sense: an exemplar-based approach. In Proceed-
ings of the 34th Annual Meeting of the Association 
for Computational Linguistics, pp.40-47 
Ngai Grace and David Yarowsky. 2000. Rule writing 
or annotation: cost-efficient resource usage for 
based noun phrase chunking. In Proceedings of the 
38th Annual Meeting of the Association for Com-
putational Linguistics, pp. 117-125 
Roy Nicholas and Andrew McCallum. 2001. Toward 
optimal active learning through sampling estima-
tion of error reduction. In Proceedings of the 
Eighteenth International Conference on Machine 
Learning, pp. 441-448 
Schein Andrew I. and Lyle H. Ungar. 2007. Active 
learning for logistic regression: an evaluation. 
Machine Learning 68(3): 235-265 
Schohn Greg and David Cohn. 2000. Less is more: 
Active learning with support vector machines. In 
Proceedings of the Seventeenth International Con-
ference on Machine Learning, pp. 839-846 
Shen Dan, Jie Zhang, Jian Su, Guodong Zhou and 
Chew-Lim Tan. 2004. Multi-criteria-based active 
learning for named entity recognition. In Proceed-
ings of the 42nd Annual Meeting on Association 
for Computational Linguistics. 
Tang Min, Xiaoqiang Luo and Salim Roukos. 2002. 
Active learning for statistical natural language 
parsing. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics, 
pp. 120-127 
Vlachos Andreas. 2008. A stopping criterion for ac-
tive learning. Computer Speech and Language. 
22(3): 295-312 
Zhu Jingbo and Eduard Hovy. 2007. Active learning 
for word sense disambiguation with methods for 
addressing the class imbalance problem. In Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and 
Computational Natural Language Learning, pp. 
783-790 
Zhu Jingbo, Huizhen Wang and Eduard Hovy. 2008. 
Learning a stopping criterion for active learning 
for word sense disambiguation and text classifica-
tion. In Proceedings of the Third International Joint 
Conference on Natural Language Processing, pp. 
366-372 
1136
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1137?1144
Manchester, August 2008
Active Learning with Sampling by Uncertainty and Density for Word 
Sense Disambiguation and Text Classification  
Jingbo Zhu  Huizhen Wang  Tianshun Yao 
Natural Language Processing Laboratory 
Northeastern University 
Shenyang, Liaoning, P.R.China 110004 
zhujingbo@mail.neu.edu.cn 
wanghuizhen@mail.neu.edu.cn 
Benjamin K Tsou 
Language Information Sciences 
Research Centre 
City University of Hong Kong 
HK, P.R.China 
rlbtsou@cityu.edu.hk 
 
Abstract 
This paper addresses two issues of active 
learning. Firstly, to solve a problem of 
uncertainty sampling that it often fails by 
selecting outliers, this paper presents a 
new selective sampling technique, sam-
pling by uncertainty and density (SUD), 
in which a k-Nearest-Neighbor-based 
density measure is adopted to determine 
whether an unlabeled example is an out-
lier. Secondly, a technique of sampling 
by clustering (SBC) is applied to build a 
representative initial training data set for 
active learning. Finally, we implement a 
new algorithm of active learning with 
SUD and SBC techniques. The experi-
mental results from three real-world data 
sets show that our method outperforms 
competing methods, particularly at the 
early stages of active learning.  
1 Introduction 
Creating a large labeled training corpus is expen-
sive and time-consuming in some real-world ap-
plications (e.g. word sense annotation), and is 
often a bottleneck to build a supervised classifier 
for a new application or domain. Our study aims 
to minimize the amount of human labeling ef-
forts required for a supervised classifier (e.g. for 
automated word sense disambiguation) to 
achieve a satisfactory performance by using ac-
tive learning.  
Among the techniques to solve the knowledge 
bottleneck problem, active learning is a widely 
used framework in which the learner has the abil-
ity to automatically select the most informative 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
unlabeled examples for human annotation. The 
ability of the active learner can be referred to as 
selective sampling. Uncertainty sampling (Lewis 
and Gale, 1994) is a popular selective sampling 
technique, and has been widely studied in natural 
language processing (NLP) applications such as 
word sense disambiguation (WSD) (Chen et al, 
2006; Chan and Ng, 2007), text classification 
(TC) (Lewis and Gale, 1994; Zhu et al, 2008), 
statistical syntactic parsing (Tang et al, 2002), 
and named entity recognition (Shen et al, 2004).  
Actually the motivation behind uncertainty 
sampling is to find some unlabeled examples 
near decision boundaries, and use them to clarify 
the position of decision boundaries. However, 
uncertainly sampling often fails by selecting out-
liers (Roy and McCallum, 2001; Tang et al, 
2002). These selected outliers (i.e. unlabeled ex-
amples) have high uncertainty, but can not pro-
vide much help to the learner. To solve the out-
lier problem, we proposed in this paper a new 
method, sampling by uncertainty and density 
(SUD), in which a K-Nearest-Neighbor-based 
density (KNN-density) measure is used to deter-
mine whether an unlabeled example is an outlier, 
and a combination strategy based on KNN-
density measure and uncertainty measure is de-
signed to select the most informative unlabeled 
examples for human annotation at each learning 
iteration.  
The second effort we made is to study how to 
build a representative initial training data set for 
active learning. We think building a more repre-
sentative initial training data set is very helpful 
for active learning. In previous studies on active 
learning, the initial training data set is generally 
generated at random, based on an assumption 
that random sampling will be likely to build the 
initial training set with same prior data distribu-
tion as that of whole corpus. However, this situa-
tion seldom occurs in real-world applications due 
to the small size of initial training set used. In 
1137
this paper, we utilize an approach, sampling by 
clustering (SBC), to selecting the most represen-
tative examples to form initial training data set 
for active learning. To do it, the whole unlabeled 
corpus should be first clustered into predefined 
number of clusters (i.e. the predefined size of the 
initial training data set). The example closest to 
the centroid of each cluster will be selected to 
augment initial training data set, which is consid-
ered as the most representative case.  
Finally, we describe an implementation of ac-
tive learning with SUD and SBC techniques. Ex-
perimental results of active learning for WSD 
and TC tasks show that our proposed method 
outperforms competing methods, particularly at 
the early stages of active learning process. It is 
noteworthy that these proposed techniques are 
easy to implement, and can be easily applied to 
several learners, such as Maximum Entropy 
(ME), na?ve Bayes (NB) and Support Vector 
Machines (SVMs). 
2 Active Learning Process 
In this work, we are interested in uncertainty 
sampling (Lewis and Gale, 1994) for pool-based 
active learning, in which an unlabeled example x 
with maximum uncertainty is selected for human 
annotation at each learning cycle. The maximum 
uncertainty implies that the current classifier (i.e. 
the learner) has the least confidence on its classi-
fication of this unlabeled example.  
Actually active learning is a two-stage process 
in which a small number of labeled samples and 
a large number of unlabeled examples are first 
collected in the initialization stage, and a closed-
loop stage of query and retraining is adopted.  
Procedure: Active Learning Process 
Input: initial small training set L, and pool of unla-
beled data set U 
Use L to train the initial classifier C  
Repeat 
1. Use the current classifier C to label all unla-
beled examples in U 
2. Use uncertainty sampling technique to select 
m2  most informative unlabeled examples, and 
ask oracle H for labeling 
3. Augment L with these m new examples, and 
remove them from U 
4. Use L to retrain the current classifier C 
Until the predefined stopping criterion SC is met. 
Figure 1. Active learning with uncertainty sam-
pling technique 
                                                 
2 A batch-based sample selection labels the top-m most 
informative unlabeled examples at each learning cycle to 
decrease the number times the learner is retrained. 
3 Uncertainty Measures 
In real-world applications, only limited size of 
training sample set can be provided to train a 
supervised classifier. Due to manual efforts in-
volved, such brings up a considerable issue: what 
is the best subset of examples to annotate. In the 
uncertainty sampling scheme, the unlabeled ex-
ample with maximum uncertainty is viewed as 
the most informative case. The key point of un-
certainty sampling is how to measure the uncer-
tainty of an unlabeled example x. 
3.1 Entropy Measure 
The well-known entropy is a popular uncertainty 
measurement widely used in previous studies on 
active learning (Tang et al, 2002; Chen et al 
2006; Zhu and Hovy, 2007): 
?
?
?=
Yy
xyPxyPxH )|(log)|()(             (1) 
where P(y|x) is the a posteriori probability. We 
denote the output class y?Y={y1, y2, ?, yk}. H is 
the uncertainty measurement function based on 
the entropy estimation of the classifier?s 
posterior distribution. 
In the following comparison experiments, the 
uncertainty sampling based on entropy criterion 
is considered as the baseline method, also called 
traditional uncertainty sampling.  
3.2 Density*Entropy Measure 
To analyze the outlier problem of traditional un-
certainty sampling, we first give an example to 
explain our motivation. 
 
Figure 2. An example of two points A and B with 
maximum uncertainty at the ith learning iteration 
 
As mentioned in Section 1, the motivation be-
hind uncertainty sampling is to find some unla-
beled examples near decision boundaries, and 
assume that these examples have the maximum 
uncertainty. Fig. 2 shows two unlabeled exam-
ples A and B with maximum uncertainty at the ith 
1138
learning cycle. Roughly speaking, there are three 
unlabeled examples near or similar to B, but, 
none for A. We think example B has higher rep-
resentativeness than example A, and A is likely 
to be an outlier. We think adding B to the train-
ing set will help the learner more than A.  
The motivation of our study is that we prefer 
not only the most informative example in terms 
of uncertainty measure, but also the most repre-
sentative example in terms of density measure. 
The density measure can be evaluated based on 
how many examples there are similar or near to it. 
An example with high density degree is less 
likely to be an outlier.  
In most real-world applications, because the 
scale of unlabeled corpus would be very large, 
Tang et al (2002) and Shen et al (2004) evalu-
ated the density of an example within a cluster. 
Unlike their work 3 , we adopt a new approach, 
called K-Nearest-Neighbor-based density (KNN-
density) measure, to evaluating the density of an 
unlabeled example x. Given a set of K (i.e. =20 
used in our experiments) most similar examples 
S(x)={s1, s2, ?, sK} of the example x,  the KNN-
density DS(.) of example x is defined as: 
K
sx
xDS xSs
i
i
?
?= )(
),cos(
)(                     (2) 
As discussed above, we prefer to select exam-
ples with maximum uncertainty and highest den-
sity for human annotation. We think getting their 
labels can help the learner greatly. To do it, we 
proposed a new method, sampling by uncertainty 
and density (SUD), in which entropy-based un-
certainty measure and KNN-density measure are 
considered simultaneously.  
In SUD scheme, a new uncertainty measure, 
called density*entropy measure4 , is defined as: 
)()()( xHxDSxDSH ?=                 (3) 
4 Initial Training Set Generation 
As shown in Fig. 1, only a small number of train-
ing samples are provided at the beginning of ac-
tive learning process. In previous studies on ac-
tive learning, the initial training set is generally 
generated by random sampling from the whole 
unlabeled corpus. However, random sampling 
technique can not guarantee selecting a most rep-
                                                 
3 We also tried their cluster-based density measure, but per-
formance was essentially degraded.  
4 We also tried other ways like ?*DS(x)+(1-?) H(x) 
measure used in previous studies, but it seems to be random. 
Actually it is very difficult to determine an appropriate? 
value for a specific task.  
resentative subset, because the size of initial 
training set is generally too small (e.g. 10). We 
think selecting some representative examples to 
form initial training set can help the active 
learner.  
In this section we utilize an approach, sam-
pling by clustering (SBC), to selecting the most 
representative examples to form initial training 
data set. In the SBC scheme, the whole unlabeled 
corpus has been first clustered into a predefined 
number of clusters (i.e. the predefined size of the 
initial training set). The example closest to the 
centroid of each cluster will be selected to aug-
ment initial training set, which is viewed as the 
most representative case.  
We use the K-means clustering algorithm 
(Duda and Hart, 1973) to cluster examples in the 
whole unlabeled corpus. In the following K-
means clustering algorithm, the traditional cosine 
measure is adopted to estimate the similarity be-
tween two examples, that is 
ji
ji
ji ww
ww
ww ?
?=),cos(                     (4) 
where wi and wj are the feature vectors of the ex-
amples i and j.  
To summarize the SBC-based initial training 
set generation algorithm, let U={U1, U2, ?, UN} 
be the set of unlabeled examples to be clustered, 
and k be the predefined size of initial training 
data set. In other words, SBC technique selects k 
most representative unlabeled examples from U 
to generate the initial training data set. The SBC-
based initial training set generation procedure is 
summarized as follows: 
SBC-based Initial Training Set Generation 
Input: U, k 
Phrase 1: Cluster the corpus U into k clusters 
? j(j=1,?,k) by using K-means clustering algo-
rithm as follows: 
1. Initialization. Randomly choosing k exam-
ples as the centroid ?j(j=1,?,k) for initial 
clusters ? j(j=1,?,k), respectively.  
2. Re-partition {U1, U2, ?, UN} into k clus-
ters ?  j(j=1,?,k), where 
}.),,cos(),cos(:{ jtUUU tijiij ??=? ??  
3. Re-estimate the centroid ?j for each clus-
ters ? j, that is: 
m
U
jiU
i
j
?
??=? , where m is the size of ?  j.
4. Repeat Step 2 and Step 3 until the algo-
rithm converges. 
1139
Phrase 2: Select the example uj closest to the 
centroid?j for each cluster j to augment ini-
tial training data set ?, where 
?
]},1[,),,cos(),cos(:{ kjUuUuu ijjijjj ???=? ??
Return?; 
 
The computation complexity of the K-means 
clustering algorithm is O(NdkT), where d is the 
number of features and T is the number of itera-
tions. In practice, we can define the stopping cri-
terion (i.e. shown in Step 4) of K-means cluster-
ing algorithm that relative change of the total 
distortion is smaller than a threshold.  
5 Active Learning with SUD and SBC  
Procedure: Active Learning with SUD and SBC 
Input: Pool of unlabeled data set U; k is the prede-
fined size of initial training data set 
Initialization.  
z Evaluate the density of each unlabeled example 
in terms of KNN-density measure; 
z Use SBC technique to generate the small initial 
training data set of size k. 
Use L to train the initial classifier C  
Repeat 
1. Use the current classifier C to label all unla-
beled examples in U 
2. Use uncertainty sampling technique in terms 
of density*entropy measure to select m most 
informative unlabeled examples, and ask ora-
cle H for labeling, namely SUD scheme.  
3. Augment L with these m new examples, and 
remove them from U 
4. Use L to retrain the current classifier C 
Until the predefined stopping criterion SC is met. 
Figure 3. Active learning with SUD and SBC  
 
Fig. 3 shows the algorithm of active learning 
with SUD and SBC techniques. Actually there 
are some variations. For example, if the initial 
training data set is generated by SBC, and en-
tropy-based uncertainty measure is used, it is 
active learning with SBC. Similarly, if the initial 
training data set is generated at random, and the 
density*entropy uncertainty measure is used, it is 
active learning with SUD. If both SBC and SUD 
techniques are not used, we call it (traditional) 
uncertainty sampling as baseline method.  
6 Evaluation 
In the following comparison experiments, we 
evaluate the effectiveness of various active learn-
ing methods for WSD and TC tasks on three pub-
licly available real-world data sets. 
6.1 Deficiency Measure 
To compare various active learning methods, 
deficiency is a statistic developed to compare 
performance of active learning methods globally 
across the learning curve, which has been used in 
previous studies (Schein and Unga, 2007). The 
deficiency measure can be defined as: 
?
?
=
=
?
?= n
t tn
n
t tn
n
REFaccREFacc
ALaccREFacc
REFALDef
1
1
))()((
))()((
),( (5) 
where acct is the average accuracy at tth learning 
iteration. REF is the baseline active learning 
method, and AL is the active learning variant of 
the learning algorithm of REF, e.g. active learn-
ing with SUD and SBC. n refers to the evaluation 
stopping points (i.e. the number of learned ex-
amples). Smaller deficiency value (i.e. <1.0) in-
dicates AL method is better than REF method. 
Conversely, a larger value (i.e. >1.0) indicates a 
negative result. 
In the following comparison experiments, we 
evaluate the effectiveness of six active learning 
methods, including random sampling (random), 
uncertainty sampling (uncertainty), SUD, ran-
dom sampling with SBC (random+SBC), uncer-
tainty sampling with SBC (uncertainty+SBC), 
and SUD with SBC (SUD+SBC). ?+SBC? indi-
cates initial training data set generated by SBC 
technique. Otherwise, initial training set is gen-
erated at random. To evaluate deficiency of each 
method, the REF method (i.e. the baseline 
method) defined in Equation (5) refers to (tradi-
tional) uncertainty sampling. 
6.2 Experimental Settings 
We utilize a maximum entropy (ME) model 
(Berger et al, 1996) to design the basic classifier 
for WSD and TC tasks. The advantage of the ME 
model is the ability to freely incorporate features 
from diverse sources into a single, well-grounded 
statistical model. A publicly available ME tool-
kit 5  was used in our experiments. To build the 
ME-based classifier for WSD, three knowledge 
sources are used to capture contextual informa-
tion: unordered single words in topical context, 
POS of neighboring words with position infor-
mation, and local collocations, which are the 
same as the knowledge sources used in (Lee and 
Ng, 2002). In the design of text classifier, the 
maximum entropy model is also utilized, and no 
feature selection technique is used. 
                                                 
5See  http://homepages.inf.ed.ac.uk/s0450736/maxent_ 
toolkit.html 
1140
In the following comparison experiments, the 
algorithm starts with a initial training set of 10 
labeled examples, and make 10 queries after each 
learning iteration. A 10 by 10-fold cross-
validation was performed. All results reported 
are the average of 10 trials in each active 
learning process.  
6.3 Data Sets 
Three publicly available natural data sets have 
been used in the following active learning com-
parison experiments. Interest data set is used for 
WSD tasks. Comp2 and WebKB data sets are 
used for TC tasks.  
The Interest data set developed by Bruce and 
Wiebe (1994) has been previously used for WSD 
(Ng and Lee, 1996). This data set consists of 
2369 sentences of the noun ?interest? with its 
correct sense manually labeled. The noun 
?interest? has six different senses in this data set.  
The Comp2 data set consists of comp.graphics 
and comp.windows.x categories from News-
Groups,  which has been previously used in ac-
tive learning for TC (Roy and McCallum, 2001; 
Schein and Ungar, 2007). 
The WebKB dataset was widely used in TC 
research. Following previous studies (McCallum 
and Nigam, 1998), we use the four most popu-
lous categories: student, faculty, course and pro-
ject, altogether containing 4199 web pages. In 
the preprocessing step, we remove those words 
that occur merely once without using stemming. 
The resulting vocabulary has 23803 words. 
 
Data sets Interest Comp2 WebKB 
Accuracy 0.908 0.90 0.91 
Table 1. Average accuracy of supervised learning 
on each data set when all examples have been 
learned. 
6.4 Active Learning for WSD Task 
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  50  100  150  200  250  300
A
cc
ur
ac
y
Number of Learned Examples
Active Learning for WSD on Interest
random
random + SBC
uncertainty
uncertainty + SBC
SUD
SUD + SBC
 
Figure 4. Active learning curve for WSD on In-
terest data set 
 
 
Random Random+SBC Uncertainty 
1.926 1.886 NA 
Uncertainty+SBC SUD SUD+SBC 
0.947 0.811 0.758 
Table 2. Average deficiency achieved by various 
active learning methods on Interest data set. The 
stopping point is 300.  
 
Fig. 4 depicts performance curves of various ac-
tive learning methods for WSD task on Interest 
data set. Among these six methods, random sam-
pling method shows the worst performance. SUD 
method constantly outperforms uncertainty sam-
pling. As discussed above, SUD method prefers 
not only the most uncertainty examples, but also 
the most representative examples. In the SUD 
scheme, the factor of KNN-density can effec-
tively avoid selecting the outliers that often cause 
uncertainty sampling to fail.  
It is noteworthy that using SBC to generate 
initial training data set can improve random (-
0.04 deficiency), uncertainty (-0.053 deficiency) 
and SUD (-0.053 deficiency) methods, respec-
tively. If the initial training data set is generated 
at random, the initial accuracy is only 55.6%. 
Interestingly, SBC achieves 62.2% initial accu-
racy, and makes 6.6% accuracy performance im-
provement. However, SBC only makes perform-
ance improvement for each method at the early 
stages of active learning. After 50 unlabeled ex-
amples have been learned, it seems that SBC has 
very little contribution to random, uncertainty 
and SUD methods. Table 2 shows that the best 
method is SUD with SBC (0.758 deficiency), 
followed by SUD method. 
6.5 Active Learning for TC Tasks 
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0  50  100  150
A
cc
ur
ac
y
Number of Learned Examples
Active Learning for Text Classification on Comp2
uncertainty
uncertainty + SBC
SUD
SUD + SBC
 
Figure 5. Active learning curve for text classifi-
cation on Comp2 data set 
 
Uncertainty Uncertainty+SBC SUD SUD+SBC
NA 0.409 0.588 0.257 
Table 3. Average deficiency achieved by various 
active learning methods on Comp2 data set. The 
stopping point is 150.  
1141
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0  50  100  150
A
cc
ur
ac
y
Number of Learned Examples
Active Learning for Text Classification on WebKB
uncertainty
uncertainty + SBC
SUD
SUD + SBC
 
Figure 6. Active learning curve for text classifi-
cation on WebKB data set 
 
Uncertainty Uncertainty+SBC SUD SUD+SBC 
NA 0.669 0.748 0.595 
Table 4. Average deficiency achieved by various 
active learning methods on WebKB data set. The 
stopping point is 150.  
 
Fig. 5 and 6 show the effectiveness of various 
active learning methods for text classification 
tasks. Since random sampling performs poorly as 
shown in Fig. 4, it is not further shown in Fig. 5 
and 6. We only compare uncertainty sampling 
and our proposed methods for both text classifi-
cation tasks.  
Similarly, SUD method constantly outper-
forms uncertainty sampling on two data sets. 
SBC greatly improves uncertainty sampling (i.e. 
0.591 and 0.331 deficiencies degraded) and SUD 
method (i.e. 0.331 and 0.153 deficiencies de-
graded), respectively. Interestingly, unlike WSD 
task shown in Fig. 4, Table 3 and 4 show that 
uncertainty sampling with SBC outperforms our 
SUD method for text classification on both data 
sets. The reason is that SBC makes about 15% 
initial accuracy improvement on Comp2 data set, 
and about 23% initial accuracy improvement on 
WebKB data set. Such improvements indicate 
that selecting high representative initial training 
set is very necessary and helpful for active learn-
ing. Table 3 and 4 show that the best active 
learning method for TC task is SUD with SBC, 
following by uncertainty sampling with SBC 
method. It is noteworthy that on WebKB uncer-
tainty sampling with SBC (0.669 deficiency) 
achieves only slight better performance than 
SUD method (0.748 deficiency) as shown in Ta-
ble 4, simply because SBC only introduce good 
performance improvement at the early stages. 
Actually on WebKB SUD method achieves 
slight better performance than uncertainty sam-
pling with SBC after about 50 unlabeled exam-
ples have been learned. 
7 Related Work 
In recent years active learning has been widely 
studied in various natural language processing 
(NLP) tasks, such as word sense disambiguation 
(Chen et al, 2006; Zhu and Hovy, 2007), text 
classification (TC) (Lewis and Gale, 1994; 
McCallum and Nigam, 1998), named entity 
recognition (NER) (Shen et al, 2004), chunking 
(Ngai and Yarowsky, 2000), information 
extraction (IE) (Thompson et al, 1999), and 
statistical parsing (Tang et al, 2002). 
In addition to uncertainty sampling, there is 
another popular selective sampling scheme, 
Query-by-committee (Engelson and Dagan, 
1999), which generates a committee of classifiers 
(always more than two classifiers) and selects the 
next unlabeled example by the principle of 
maximal disagreement among these classifiers. A 
method similar to committee-based sampling is 
co-testing proposed by Muslea et al (2000), 
which trains two learners individually on two 
compatible and uncorrelated views that should be 
able to reach the same classification accuracy. In 
practice, however, these conditions of view se-
lection are difficult to meet in real-world applica-
tions. Cohn et al (1996) and Roy and McCallum 
(2001) proposed a method that directly optimizes 
expected future error on future test examples. 
However, the computational complexity of their 
methods is very high.  
There are some similar previous studies (Tang 
et al, 2002; Shen et al, 2004) in which the rep-
resentativeness criterion in active learning is 
considered. Unlike our sampling by uncertainty 
and density technique, Tang et al (2002) adopted 
a sampling scheme of most uncertain per cluster 
for NLP parsing, in which the learner selects the 
sentence with the highest uncertain score from 
each cluster, and use the density to weight the 
selected examples while we use density informa-
tion to select the most informative examples. Ac-
tually the scheme of most uncertain per cluster 
still can not solve the outlier problem faced by 
uncertainty sampling technique. Shen et al 
(2004) proposed an approach to selecting exam-
ples based on informativeness, representativeness 
and diversity criteria. In their work, the density 
of an example is evaluated within a cluster, and 
multiple criteria have been linearly combined 
with some coefficients. However, it is difficult to 
automatically determine sufficient coefficients in 
real-world applications. Perhaps there are differ-
ent appropriate coefficients for various applica-
tions.  
1142
8 Discussion 
For batch mode active learning, we found some-
times there is a redundancy problem that some 
selected examples are identical or similar. Such 
situation would reduce the representativeness of 
selected examples. To solve this problem, we 
tried the sampling scheme of ?most uncertain per 
cluster? (Tang et al, 2002) to select the most 
informative examples. We think selecting exam-
ples from each cluster can alleviate the redun-
dancy problem. However, this sampling scheme 
works poorly for WSD and TC on the three data 
sets, compared to traditional uncertainty sam-
pling. From the clustering results, we found these 
resulting clusters are very imbalanced. It makes 
sense that more informative examples are con-
tained in a bigger cluster. In this work, we only 
use SUD technique to select the most informative 
examples for active learning. We plan to study 
how combining SBC and SUD techniques can 
enhance the selection of the most informative 
examples in the future work. 
Furthermore, we think that a misclassified 
unlabeled example may convey more 
information than a correctly classified unlabeled 
example which is closer to the decision boundary. 
But there is a difficulty that the true label of each 
unlabeled example is unknown. To use misclassi-
fication information to select the most informa-
tive examples, we should study how to automati-
cally determine whether an unlabeled example 
has been misclassified. For example, we can 
make an assumption that an unlabeled example 
may be misclassified if this example was previ-
ously ?outside? and is now ?inside?. We will 
study this issue in the future work.  
Actually these proposed techniques can be 
easily applied for committee-based sampling for 
active learning. However, to do so, we should 
adopt a new uncertainty measurement such as 
vote entropy to measure the uncertaity of each 
unlabled example in committee-based sampling 
scheme. 
9 Conclusion and Future Work 
In this paper, we have addressed two issues of 
active learning, involving the outlier problem of 
traditional uncertainty sampling, and initial train-
ing data set generation. To solve the outlier prob-
lem of traditional uncertainly sampling, we pro-
posed a new method of sampling by uncertainty 
and density (SUD) in which KNN-density meas-
ure and uncertainty measure are combined to-
gether to select the most informative unlabeled 
example for human annotation at each learning 
cycle. We employ a method of sampling by clus-
tering (SBC) to generate a representative initial 
training data set. Experimental results on three 
evaluation data sets show that our combined 
SUD with SBC method achieved the best per-
formance compared to other competing methods, 
particularly at the early stages of active learning 
process. In future work, we will focus on the re-
dundancy problem faced by batch mode active 
learning, and how to make use of misclassified 
information to select the most useful examples 
for human annotation. 
Acknowledgments 
This work was supported in part by the National 
863 High-tech Project (2006AA01Z154) and the 
Program for New Century Excellent Talents in 
University (NCET-05-0287). 
References 
Berger Adam L., Vincent J. Della Pietra, Stephen A. 
Della Pietra. 1996. A maximum entropy approach 
to natural language processing. Computational 
Linguistics 22(1):39?71. 
Bruce Rebecca and Janyce Wiebe. 1994. Word sense 
disambiguation using decomposable models. Pro-
ceedings of the 32nd annual meeting on Associa-
tion for Computational Linguistics, pp. 139-146. 
Chan Yee Seng and Hwee Tou Ng. 2007. Domain 
adaptation with active learning for word sense dis-
ambiguation. Proceedings of the 45th annual meet-
ing on Association for Computational Linguistics, 
pp. 49-56 
Chen Jinying, Andrew Schein, Lyle Ungar and 
Martha Palmer. 2006. An empirical study of the 
behavior of active learning for word sense disam-
biguation. Proceedings of the main conference on 
Human Language Technology Conference of the 
North American Chapter of the Association of 
Computational Linguistics, pp. 120-127 
Cohn David A., Zoubin Ghahramani and Michael I. 
Jordan. 1996. Active learning with statistical mod-
els. Journal of Artificial Intelligence Research, 4, 
129?145. 
Duda Richard O. and Peter E. Hart. 1973. Pattern 
classification and scene analysis. New York: 
Wiley. 
Engelson S. Argamon and I. Dagan. 1999. Commit-
tee-based sample selection for probabilistic classi-
fiers. Journal of Artificial Intelligence Research 
(11):335-360. 
1143
Lee Yoong Keok and Hwee Tou Ng. 2002. An em-
pirical evaluation of knowledge sources and learn-
ing algorithm for word sense disambiguation. In 
Proceedings of the ACL-02 conference on Empiri-
cal methods in natural language processing, pp. 41-
48 
Lewis David D. and William A. Gale. 1994. A se-
quential algorithm for training text classifiers. In 
Proceedings of the 17th annual international ACM 
SIGIR conference on Research and development in 
information retrieval, pp. 3-12 
McCallum Andrew and Kamal Nigam. 1998. A com-
parison of event models for na?ve bayes text classi-
fication. In AAAI-98 workshop on learning for text 
categorization. 
Muslea Ion, Steven Minton and Craig A. Knoblock. 
2000. Selective sampling with redundant views. In 
Proceedings of the Seventeenth National Confer-
ence on Artificial Intelligence and Twelfth Confer-
ence on Innovative Applications of Artificial Intel-
ligence, pp. 621-626. 
Ng Hwee Tou and Hian Beng Lee. 1996. Integrating 
multiple knowledge sources to disambiguate word 
sense: an exemplar-based approach. In Proceed-
ings of the Thirty-Fourth Annual Meeting of the 
Association for Computational Linguistics, pp. 40-
47 
 Ngai Grace and David Yarowsky. 2000. Rule writing 
or annotation: cost-efficient resource usage for 
based noun phrase chunking. In Proceedings of the 
38th Annual Meeting of the Association for Com-
putational Linguistics, pp. 117-125 
Roy Nicholas and Andrew McCallum. 2001. Toward 
optimal active learning through sampling estima-
tion of error reduction. In Proceedings of the 
Eighteenth International Conference on Machine 
Learning, pp. 441-448 
Schein Andrew I. and Lyle H. Ungar. 2007. Active 
learning for logistic regression: an evaluation. 
Machine Learning 68(3): 235-265 
Schohn Greg and David Cohn. 2000. Less is more: 
Active learning with support vector machines. In 
Proceedings of the Seventeenth International Con-
ference on Machine Learning, pp. 839-846 
Shen Dan, Jie Zhang, Jian Su, Guodong Zhou and 
Chew-Lim Tan. 2004. Multi-criteria-based active 
learning for named entity recognition. In Proceed-
ings of the 42nd Annual Meeting on Association 
for Computational Linguistics. 
Tang Min, Xiaoqiang Luo and Salim Roukos. 2002. 
Active learning for statistical natural language 
parsing. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics, 
pp. 120-127 
Thompson Cynthia A., Mary Elaine Califf and Ray-
mond J. Mooney. 1999. Active learning for natural 
language parsing and information extraction. In 
Proceedings of the Sixteenth International Confer-
ence on Machine Learning, pp. 406-414 
Zhu Jingbo and Eduard Hovy. 2007. Active learning 
for word sense disambiguation with methods for 
addressing the class imbalance problem. In Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and 
Computational Natural Language Learning, pp. 
783-790 
Zhu Jingbo, Huizhen Wang and Eduard Hovy. 2008. 
Learning a stopping criterion for active learning 
for word sense disambiguation and text classifica-
tion. In Proceedings of the Third International Joint 
Conference on Natural Language Processing, pp. 
366-372 
1144
Learning a Stopping Criterion for Active Learning for Word Sense 
Disambiguation and Text Classification 
Jingbo Zhu   Huizhen Wang 
Natural Language Processing Lab  
Northeastern University 
Shenyang, Liaoning, P.R.China, 110004 
Zhujingbo@mail.neu.edu.cn
wanghuizhen@mail.neu.edu.cn 
Eduard Hovy 
University of Southern California 
Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
hovy@isi.edu
 
Abstract 
In this paper, we address the problem of 
knowing when to stop the process of active 
learning. We propose a new statistical 
learning approach, called minimum 
expected error strategy, to defining a 
stopping criterion through estimation of the 
classifier?s expected error on future 
unlabeled examples in the active learning 
process. In experiments on active learning 
for word sense disambiguation and text 
classification tasks, experimental results 
show that the new proposed stopping 
criterion can reduce approximately 50% 
human labeling costs in word sense 
disambiguation with degradation of 0.5% 
average accuracy, and approximately 90% 
costs in text classification with degradation 
of 2% average accuracy. 
1 Introduction 
Supervised learning models set their parameters 
using given labeled training data, and generally 
outperform unsupervised learning methods when 
trained on equal amount of training data. However, 
creating a large labeled training corpus is very 
expensive and time-consuming in some real-world 
cases such as word sense disambiguation (WSD).  
Active learning is a promising way to minimize 
the amount of human labeling effort by building an 
system that automatically selects the most informa-
tive unlabeled example for human annotation at 
each annotation cycle. In recent years active learn-
ing  has attracted a lot of research interest, and has 
been studied in many natural language processing 
(NLP) tasks, such as text classification (TC) 
(Lewis and Gale, 1994; McCallum and Nigam, 
1998), chunking (Ngai and Yarowsky, 2000), 
named entity recognition (NER) (Shen et al, 2004; 
Tomanek et al, 2007), part-of-speech tagging 
(Engelson and Dagan, 1999), information 
extraction (Thompson et  al., 1999), statistical 
parsing (Steedman et al, 2003), and word sense 
disambiguation (Zhu and Hovy, 2007).  
Previous studies reported that active learning 
can help in reducing human labeling effort. With 
selective sampling techniques such as uncertainty 
sampling (Lewis and Gale, 1994) and committee-
based sampling (McCallum and Nigam, 1998), the 
size of the training data can be significantly re-
duced for text classification (Lewis and Gale, 
1994; McCallum and Nigam, 1998), word sense 
disambiguation (Chen, et al 2006; Zhu and Hovy, 
2007), and named entity recognition (Shen et al, 
2004; Tomanek et al, 2007) tasks.  
Interestingly, deciding when to stop active 
learning is an issue seldom mentioned issue in 
these studies. However, it is an important practical 
topic, since it obviously makes no sense to 
continue the active learning procedure until the 
whole corpus has been labeled. How to define an 
adequate stopping criterion remains an unsolved 
problem in active learning. In principle, this is a 
problem of estimation of classifier effectiveness 
(Lewis and Gale, 1994). However, in real-world 
applications, it is difficult to know when the 
classifier reaches its maximum effectiveness 
before all unlabeled examples have been 
annotated. And when the unlabeled data set 
becomes very large, full annotation is almost 
impossible for human annotator.  
In this paper, we address the issue of a stopping 
criterion for active learning, and propose a new 
statistical learning approach, called minimum ex-
366
pected error strategy, that defines a stopping crite-
rion through estimation of the classifier?s expected 
error on future unlabeled examples. The intuition is 
that the classifier reaches maximum effectiveness 
when it results in the lowest expected error on 
remaining unlabeled examples. This proposed 
method is easy to implement, involves small 
additional computation costs, and can be applied to 
several different learners, such as Naive Bayes 
(NB), Maximum Entropy (ME), and Support 
Vector Machines (SVMs) models. Comparing with 
the confidence-based stopping criteria proposed by 
Zhu and Hovy (2007), experimental results show 
that the new proposed stopping criterion achieves 
better performance in active learning for both the 
WSD and TC tasks. 
2 Active Learning Process and Problem 
of General Stopping Criterion 
2.1 Active Learning Process 
Active learning is a two-step semi-supervised 
learning process in which a small number of la-
beled samples and a large number of unlabeled 
examples are first collected in the initialization 
stage, and a close-loop stage of query and retrain-
ing is adopted. The purpose of active learning is to 
minimize the amount of human labeling effort by 
having the system in each cycle automatically se-
lect for human annotation the most informative 
unannotated case.   
Procedure: Active Learning Process 
Input: initial small training set L, and pool of 
unlabeled data set U 
Use L to train the initial classifier C (i.e. a classi-
fier for uncertainty sampling or a set of classifiers 
for committee-based sampling) 
Repeat 
? Use the current classifier C  to label all 
unlabeled examples in U 
? Based on active learning rules R such as un-
certainty sampling or committee-based sam-
pling, present m top-ranked unlabeled ex-
amples to oracle H for labeling 
? Augment L with the m new examples, and 
remove them from U 
? Use L to retrain the current classifier C 
Until the predefined stopping criterion SC is met. 
Figure 1. Active learning process 
In this work, we are interested in selective sam-
pling for pool-based active learning, and focus on 
uncertainty sampling (Lewis and Gale, 1994). The 
key point is how to measure the uncertainty of an 
unlabeled example, in order to select a new exam-
ple with maximum uncertainty to augment the 
training data. The maximum uncertainty implies 
that the current classifier has the least confidence 
in its classification of this unlabeled example x. 
The well-known entropy is a good uncertainty 
measurement widely used in active learning: 
( ) ( | ) log ( | )
y Y
UM x P y x P y x
?
= ??         (1) 
where P(y|x) is the a posteriori probability. We 
denote the output class y?Y={y1, y2, ?, yk}. UM is 
the uncertainty measurement function based on the 
entropy estimation of the classifier?s posterior 
distribution. 
2.2 General Stopping Criteria 
As shown in Fig. 1, the active learning process 
repeatedly provides the most informative unlabeled 
examples to an oracle for annotation, and update 
the training set, until the predefined stopping 
criterion SC is met. In practice, it is not clear how 
much annotation is sufficient for inducing a 
classifier with maximum effectiveness (Lewis and 
Gale, 1994). This procedure can be implemented 
by defining an appropriate stopping criterion for 
active learning.  
In active learning process, a general stopping 
criterion SC can be defined as: 
1 (
0 ,AL
effectiveness C
SC
otherwise
) ???= ??         (2) 
where ? is a user predefined constant and the func-
tion effectiveness(C) evaluates the effectiveness of 
the current classifier. The learning process ends 
only if the stopping criterion function SCAL is equal 
to 1. The value of constant ? represents a tradeoff 
between the cost of annotation and the effective-
ness of the resulting classifier. A larger ? would 
cause more unlabeled examples to be selected for 
human annotation, and the resulting classifier 
would be more robust. A smaller ? means the re-
sulting classifier would be less robust, and less 
unlabeled examples would be selected to annotate.  
In previous work (Shen et al, 2004; Chen et al, 
2006; Li and Sethi, 2006; Tomanek et al, 2007), 
there are several common ways to define the func-
367
tion effectiveness(C). First, previous work always 
used a simple stopping condition, namely, when 
the training set reaches desirable size. However, it 
is almost impossible to predefine an appropriate 
size of desirable training data guaranteed to induce 
the most effective classifier. Secondly, the learning 
loop can end if no uncertain unlabeled examples 
can be found in the pool. That is, all informative 
examples have been selected for annotation. 
However, this situation seldom occurs in real-
world applications. Thirdly, the active learning 
process can stop if the targeted performance level 
is achieved. However, it is difficult to predefine an 
appropriate and achievable performance, since it 
should depend on the problem at hand and the 
users? requirements.  
2.3 Problem of Performance Estimation 
An appealing solution has the active learning 
process end when repeated cycles show no 
significant performance improvement on the test 
set. However, there are two open problems. The 
first question is how to measure the performance of 
a classifier in active learning. The second one is 
how to know when the resulting classifier reaches 
the highest or adequate performance. It seems 
feasible that a separate validation set can solve 
both problems. That is, the active learning process 
can end if there is no significant performance 
improvement on the validation set. But how many 
samples are required for the pregiven separate 
validation set is an open question. Too few 
samples may not be adequate for a reasonable 
estimation and may result in an incorrect result. 
Too many samples would cause additional high 
cost because the separate validation set is generally 
constructed manually in advance.  
3 Statistical Learning Approach 
3.1 Confidence-based Strategy 
To avoid the problem of performance estimation 
mentioned above, Zhu and Hovy (2007) proposed 
a confidence-based framework to predict the upper 
bound and the lower bound for a stopping criterion 
in active learning. The motivation is to assume that 
the current training data is sufficient to train the 
classifier with maximum effectiveness if the cur-
rent classifier already has acceptably strong confi-
dence on its classification results for all remained 
unlabeled data.  
The first method to estimate the confidence of 
the classifier is based on uncertainty measurement, 
considering whether the entropy of each selected 
unlabeled example is less than a small predefined 
threshold. Here we call it Entropy-MCS. The 
stopping criterion SC Entropy-MCS can be defined as: 
 
1 , ( )
0 ,
E
Entropy MCS
x U UM x
SC
otherwise
?
?
? ? ??= ??
    (3) 
where ?E is a user predefined entropy threshold and 
the function UM(x) evaluates the uncertainty of 
each unlabeled example x.  
The second method to estimate the confidence 
of the classifier is based on feedback from the ora-
cle when the active learner asks for true labels for 
selected unlabeled examples, by considering 
whether the current trained classifier could 
correctly predict the labels or the accuracy 
performance of predictions on selected unlabeled 
examples is already larger than a predefined 
accuracy threshold. Here we call it OracleAcc-
MCS. The stopping criterion SCOracleAcc-MCS can be 
defined as: 
1 (
0 ,
) A
OracleAcc MCS
OracleAcc C
SC
otherwise
?
?
??= ??
    (4) 
where ?A is a user predefined accuracy threshold 
and function OracleAcc(C) evaluates accuracy per-
formance of the classifier on these selected unla-
beled examples through feedback of the Oracle.  
3.2 Minimum Expected Error Strategy 
In fact, these above two confidence-based methods 
do not directly estimate classifier performance that 
closely reflects the classifier effectiveness, because 
they only consider entropy of each unlabeled 
example and accuracy on selected informative 
examples at each iteration step. In this section we 
therefore propose a new statistical learning ap-
proach to defining a stopping criterion through es-
timation of the classifier?s expected error on all 
future unlabeled examples, which we call minimum 
expected error strategy (MES). The motivation 
behind MES is that the classifier C (a classifier for 
uncertainty sampling or set of classifiers for com-
mittee-based sampling) with maximum effective-
ness is the one that results in the lowest expected 
368
error on whole test set in the learning process. The 
stopping criterion SC MES is defined as: 
1 ( )
0 ,
err
MES
Error C
SC
otherwise
???= ??           (5) 
where ?err is a user predefined expected error 
threshold and the function Error(C) evaluates the 
expected error of the classifier C that closely re-
flects the classifier effectiveness. So the key point 
of defining MES-based stopping criterion SC MES is 
how to calculate the function Error(C) that denotes 
the expected error of the classifier C.  
Suppose given a training set L and an input 
sample x, we can write the expected error of the 
classifier C as follows: 
( ) ( ( ) | ) ( )Error C R C x x P x dx= ?           (6) 
where P(x) represents the known marginal distribu-
tion of x. C(x) represents the classifier?s decision 
that is one of k classes: y?Y={y1, y2, ?, yk}. R(yi|x) 
denotes a conditional loss for classifying the input 
sample x into a class yi that can be defined as 
1
( | ) [ , ] ( | )
k
i j
j
R y x i j P y x?
=
=?             (7) 
where P(yj|x) is the a posteriori probability pro-
duced by the classifier C. ?[i,j] represents a zero-
one loss function for every class pair {i,j} that as-
signs no loss to a correct classification, and assigns 
a unit loss to any error. 
In this paper, we focus on pool-based active 
learning in which a large unlabeled data pool U is 
available, as described Fig. 1. In active learning 
process, our interest is to estimate the classifier?s 
expected error on future unlabeled examples in the 
pool U. That is, we can stop the active learning 
process when the active learner results in the low-
est expected error over the unlabeled examples in 
U. The pool U can provide an estimate of P(x). So 
for minimum error rate classification (Duda and 
Hart. 1973) on unlabeled examples, the expected 
error of the classifier C can be rewritten as 
1
( ) (1 max ( | ))
y Y
x U
Error C P y x
U ??
= ??        (8) 
Assuming N unlabeled examples in the pool U, 
the total time is O(N) for automatically determin-
ing whether the proposed stopping criterion SCMES 
is satisfied in the active learning.  
If the pool U is very large (e.g. more than 
100000 examples), it would still cause high com-
putation cost at each iteration of active learning. A 
good approximation is to estimate the expected 
error of the classifier using a subset of the pool, not 
using all unlabeled examples in U. In practice, a 
good estimation of expected error can be formed 
with few thousand examples. 
4 Evaluation 
In this section, we evaluate the effectiveness of 
three stopping criteria for active learning for word 
sense disambiguation and text classification as 
follows: 
? Entropy-MCS ? stopping active learning 
process when the stopping criterion function 
SCEntropy-MCS defined in (3) is equal to 1, where 
?E=0.01, 0.001,  0.0001.  
? OracleAcc-MCS ? stopping active learning 
process when the stopping criterion function 
SCOracleAcc-MCS defined in (4) is equal to 1, 
where ?A=0.9, 1.0.  
? MES ? stopping active learning process when 
the stopping criterion function SCMES defined 
in (5) is equal to 1, where ?err=0.01, 0.001, 
0.0001.  
The purpose of defining stopping criterion of 
active learning is to study how much annotation is 
sufficient for a specific task. To comparatively 
analyze the effectiveness of each stopping criterion, 
a baseline stopping criterion is predefined as when 
all unlabeled examples in the pool U are learned. 
Comparing with the baseline stopping criterion, a 
better stopping criterion not only achieves almost 
the same performance, but also has needed to learn 
fewer unlabeled examples when the active learning 
process is ended. In other words, for a stopping 
criterion of active learning, the fewer unlabeled 
examples that have been leaned when it is met, the 
bigger reduction in human labeling cost is made. 
In the following active learning experiments, a 
10 by 10-fold cross-validation was performed. All 
results reported are the average of 10 trials in each 
active learning process.  
4.1 Word Sense Disambiguation 
The first comparison experiment is active learning 
for word sense disambiguation. We utilize a 
maximum entropy (ME) model (Berger et al, 
1996) to design the basic classifier used in active 
learning for WSD. The advantage of the ME model 
is the ability to freely incorporate features from 
369
diverse sources into a single, well-grounded statis-
tical model. A publicly available ME toolkit 
(Zhang et. al., 2004) was used in our experiments. 
In order to extract the linguistic features necessary 
for the ME model in WSD tasks, all sentences con-
taining the target word are automatically part-of-
speech (POS) tagged using the Brill POS tagger 
(Brill, 1992). Three knowledge sources are used to 
capture contextual information: unordered single 
words in topical context, POS of neighboring 
words with position information, and local colloca-
tions. These are same as the knowledge sources 
used in (Lee and Ng, 2002) for supervised auto-
mated WSD tasks.  
The data used for comparison experiments was 
developed as part of the OntoNotes project (Hovy 
et al, 2006), which uses the WSJ part of the Penn 
Treebank (Marcus et al, 1993). The senses of 
noun words occurring in OntoNotes are linked to 
the Omega ontology (philpot et al, 2005). In 
OntoNotes, at least two human annotators 
manually annotate the coarse-grained senses of 
selected nouns and verbs in their natural sentence 
context. In this experiment, we used several tens of 
thousands of annotated OntoNotes examples, 
covering in total 421 nouns with an inter-annotator 
agreement rate of at least 90%. We find that 302 
out of 421 nouns occurring in OntoNotes are 
ambiguous, and thus are used in the following 
WSD experiments. For these 302 ambiguous 
nouns, there are 3.2 senses per noun, and 172 
instances per noun.  
The active learning algorithms start with a 
randomly chosen initial training set of 10 labeled 
samples for each noun, and make 10 queries after 
each learning iteration. Table 1 shows the 
effectiveness of each stopping criterion tested on 
active learning for WSD on these ambiguous 
nouns? WSD tasks. We analyze average accuracy 
performance of the classifier and average 
percentage of unlabeled examples learned when 
each stopping criterion is satisfied in active 
learning for WSD tasks. All accuracies and 
percentages reported in Table 1 are macro-
averages over these 302 ambiguous nouns. 
 
 
 
 
 
 
Stopping Criterion Average accuracy 
Average 
percentage 
all unlabeled examples learned 87.3% 100% 
Entropy-MCS method (0.0001) 86.8% 81.8% 
Entropy-MCS method (0.001) 86.8% 75.8% 
Entropy-MCS method (0.01) 86.8% 68.6% 
OracleAcc-MCS method (0.9) 86.8% 56.5% 
OracleAcc-MCS method (1.0) 86.8% 62.4% 
MES method (0.0001) 86.8% 67.1% 
MES method (0.001) 86.8% 58.8% 
MES method (0.01) 86.8% 52.7% 
Table 1. Effectiveness of each stopping criterion of 
active learning for WSD on OnteNotes. 
 
Table 1 shows that these stopping criteria 
achieve the same accuracy of 86.8% which is 
within 0.5% of the accuracy of the baseline method 
(all unlabeled examples are labeled). It is obvious 
that these stopping criteria can help reduce the hu-
man labeling costs, comparing with the baseline 
method. The best criterion is MES method 
(?err=0.01), following by OracleAcc-MCS method 
(?A=0.9). MES method (?err=0.01) and OracleAcc-
MCS method (?A=0.9) can make 47.3% and 44.5% 
reductions in labeling costs, respectively. Entropy-
MCS method is apparently worse than MES and 
OracleAcc-MCS methods. The best of the 
Entropy-MCS method is the one with ?E=0.01 
which makes approximately 1/3 reduction in 
labeling costs. We also can see from Table 1 that 
for Entropy-MCS and MES methods, reduction 
rate becomes smaller as the ? becomes smaller. 
4.2 Text Classification 
The second data set is for active learning for text 
classification using the WebKB corpus 1  
(McCallum et al, 1998). The WebKB dataset was 
formed by web pages gathered from various uni-
versity computer science departments. In the fol-
lowing active learning experiment, we use four 
most populous categories: student, faculty, course 
and project, altogether containing 4,199 web pages. 
Following previous studies (McCallum et al, 
1998), we only remove those words that occur 
merely once without using stemming or stop-list. 
The resulting vocabulary has 23,803 words. In the 
design of the text classifier, the maximum entropy 
model is also utilized, and no feature selection 
technique is used. 
                                                 
1 See http://www.cs.cmu.edu/~textlearning 
370
The algorithm is initially given 20 labeled ex-
amples, 5 from each class. Table 2 shows the 
effectiveness of each stopping criterion of active 
learning for text classification on WebKB corpus. 
All results reported are the average of 10 trials. 
Stopping Criterion Average accuracy 
Average 
percentage 
all unlabeled examples learned 93.5% 100% 
Entropy-MCS method (0.0001) 92.5% 23.8% 
Entropy-MCS method (0.001) 92.4% 22.3% 
Entropy-MCS method (0.01) 92.5% 21.8% 
OracleAcc-MCS method (0.9) 91.5% 13.1% 
OracleAcc-MCS method (1.0) 92.5% 24.5% 
MES method (0.0001) 92.1% 17.9% 
MES method (0.001) 92.0% 15.6% 
MES method (0.01) 91.5% 10.9% 
Table 2. Effectiveness of each stopping criterion of 
active learning for TC on WebKB corpus. 
 
From results shown in Table 2, we can see that 
MES method (?err=0.01) already achieves 91.5% 
accuracy in 10.9% unlabeled examples learned. 
The accuracy of all unlabeled examples learned is 
93.5%. This situation means the approximately 
90% remaining unlabeled examples only make 
only 2% performance improvement. Like the 
results of WSD shown in Table 1, for Entropy-
MCS and MES methods used in active learning for 
text classification tasks, the corresponding 
reduction rate becomes smaller as the value of ? 
becomes smaller. MES method (?err=0.01) can 
make approximately 90% reduction in human la-
beling costs and results in 2% accuracy perform-
ance degradation. The Entropy-MCS method 
(?E=0.01) can make approximate 80% reduction in 
costs and results in 1% accuracy performance 
degradation. Unlike the results of WSD shown in 
Table 1, the OracleAcc-MCS method (?A=1.0) 
makes the smallest reduction rate of 75.5%. 
Actually in real-world applications, the selection of 
a stopping criterion is a tradeoff issue between 
labeling cost and effectiveness of the classifier.  
5 Discussion 
It is interesting to investigate the impact of per-
formance change on defining a stopping criterion, 
so we show an example of active learning for 
WSD task in Fig. 2.  
 0.8
 0.82
 0.84
 0.86
 0.88
 0.9
 0.92
 0.94
 0  20  40  60  80  100  120  140  160  180  200  220
A
cc
ur
ac
y
Number of Learned Examples
Active Learning for WSD task
rate-n
 
Figure 2. An example of active learning for WSD 
on noun ?rate? in OntoNotes. 
 
Fig. 2 shows that the accuracy performance gen-
erally increases, but apparently degrades at the it-
erations ?20?, ?80?, ?170?, ?190?, and ?200?, and 
does not change anymore during the iterations 
[?130?-?150?] or [?200?-?220?] in the active learn-
ing process. Actually the first time of the highest 
performance of 95% achieved is at ?450?, which is 
not shown in Fig. 2. In other words, although the 
accuracy performance curve shows an increasing 
trend, it is not monotonously increasing. From Fig. 
2 we can see that it is not easy to automatically 
determine the point of no significant performance 
improvement on the validation set, because points 
such as ?20? or ?80? would mislead final judgment. 
However, we do believe that the change of per-
formance is a good signal to stop active learning 
process. So it is worth studying further how to 
combine the factor of performance change with our 
proposed stopping criteria of active learning.  
The OracleAcc-MCS method would not work if 
only one or too few informative examples are 
queried at the each iteration step in the active 
learning. There is an open issue how many selected 
unlabeled examples at each iteration are adequate 
for the batch-based sample selection.  
For these stopping crieria, there is no general 
method to automatically determine the best 
threshold for any given task. It may therefore be 
necessary to use a dynamic threshold change tech-
nique in which the predefined threshold can be 
automatically modified if the performance is still 
significantly improving when the stopping crite-
rion is met during active learning process.  
371
6 Conclusion and Future Work 
In this paper, we address the stopping criterion is-
sue of active learning, and analyze the problems 
faced by some common ways to stop the active 
learning process. In essence, defining a stopping 
criterion of active learning is a problem of estimat-
ing classifier effectiveness. The purpose of defin-
ing stopping criterion of active learning is to know 
how much annotation is sufficient for a special task. 
To determine this, this paper proposes a new statis-
tical learning approach, called minimum expected 
error strategy, for defining a stopping criterion 
through estimation of the classifier?s expected er-
ror on future unlabeled examples during the active 
learning process. Experimental results on word 
sense disambiguation and text classification tasks 
show that new proposed minimum expected error 
strategy outperforms the confidence-based strategy, 
and achieves promising results. The interesting 
future work is to study how to combine the best of 
both strategies, and how to consider performance 
change to define an appropriate stopping criterion 
for active learning.  
Acknowledgments 
This work was supported in part by the National 
Natural Science Foundation of China under Grant 
(60473140), the National 863 High-tech Project 
(2006AA01Z154); the Program for New Century 
Excellent Talents in University(NCET-05-0287).  
References 
A. L. Berger, S. A. Della, and V. J  Della. 1996. A 
maximum entropy approach to natural language 
processing. Computational Linguistics 22(1):39?71. 
E Brill. 1992. A simple rule-based part of speech tag-
ger. In the Proceedings of the Third Conference on 
Applied Natural Language Processing. 
J. Chen, A. Schein, L. Ungar, M. Palmer. 2006. An 
empirical study of the behavior of active learning for 
word sense disambiguation. In Proc. of HLT-
NAACL06 
R. O. Duda and P. E. Hart. 1973. Pattern classification 
and scene analysis. New York: Wiley.  
S. A. Engelson and I. Dagan. 1999. Committee-based 
sample selection for probabilistic classifiers. Journal 
of Artificial Intelligence Research. 
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw and R. 
Weischedel. 2006. Ontonotes: The 90% Solution. In 
Proc. of HLT-NAACL06. 
Y.K. Lee and. H.T. Ng. 2002. An empirical evaluation 
of knowledge sources and learning algorithm for 
word sense disambiguation. In Proc. of EMNLP02 
D. D. Lewis and W. A. Gale. 1994. A sequential algo-
rithm for training text classifiers. In Proc. of SIGIR-
94 
M. Li, I. K. Sethi. 2006. Confidence-based active learn-
ing. IEEE transaction on pattern analysis and ma-
chine intelligence, 28(8):1251-1261. 
M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 
1993. Building a large annotated corpus of English: 
the Penn Treebank. Computational Linguistics, 
19(2):313-330 
A. McCallum and K. Nigram. 1998. Employing EM in 
pool-based active learning for text classification. In 
Proc. of 15th ICML 
G. Ngai and D. Yarowsky. 2000. Rule writing or anno-
tation: cost-efficient resource usage for based noun 
phrase chunking. In Proc. of ACL-02  
A. Philpot, E. Hovy and P. Pantel. 2005. The Omega 
Ontology. In Proc. of ONTOLEX Workshop at 
IJCNLP. 
D. Shen, J. Zhang, J. Su, G. Zhou and C. Tan. 2004. 
Multi-criteria-based active learning for named entity 
recognition. In Prof. of ACL-04. 
M. Steedman, R. Hwa, S. Clark, M. Osborne, A. Sakar, 
J. Hockenmaier, P. Ruhlen, S. Baker and J. Crim. 
2003. Example selection for bootstrapping statistical 
parsers. In Proc. of HLT-NAACL-03 
C. A. Thompson, M. E. Califf and R. J. Mooney. 1999. 
Active learning for natural language parsing and in-
formation extraction. In Proc. of ICML-99. 
K. Tomanek, J. Wermter and U. Hahn. 2007. An ap-
proach to text corpus construction which cuts anno-
tation costs and maintains reusability of annotated 
data. In Proc. of EMNLP/CoNLL07 
L. Zhang, J. Zhu, and T. Yao. 2004. An evaluation of 
statistical spam filtering techniques. ACM Transac-
tions on Asian Language Information Processing, 
3(4):243?269. 
J. Zhu, E. Hovy. 2007. Active learning for word sense 
disambiguation with methods for addressing the 
class imbalance problem. In Proc. of 
EMNLP/CoNLL07 
372
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 217?220,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Designing Special Post-processing Rules for SVM-based
Chinese Word Segmentation
Muhua Zhu, Yilin Wang, Zhenxing Wang, Huizhen Wang, Jingbo Zhu
Natural Language Processing Lab
Northeastern University
No.3-11, Wenhua Road, Shenyang, Liaoning, China, 110004
{zhumh, wangyl, wangzx, wanghz}@ics.neu.edu.cn
zhujingbo@mail.neu.edu.cn
Abstract
We participated in the Third Interna-
tional Chinese Word Segmentation Bake-
off. Specifically, we evaluated our Chi-
nese word segmenter NEUCipSeg in
the close track, on all four corpora,
namely Academis Sinica (AS), City Uni-
versity of Hong Kong (CITYU), Mi-
crosoft Research (MSRA), and Univer-
sity of Pennsylvania/University of Col-
orado (UPENN). Based on Support Vec-
tor Machines (SVMs), a basic segmenter
is designed regarding Chinese word seg-
mentation as a problem of character-based
tagging. Moreover, we proposed post-
processing rules specially taking into ac-
count the properties of results brought
out by the basic segmenter. Our system
achieved good ranks in all four corpora.
1 SVM-based Chinese Word Segmenter
We built out segmentation system following (Xue
and Shen, 2003), regarding Chinese word segmen-
tation as a problem of character-based tagging.
Instead of Maximum Entropy, we utilized Sup-
port Vector Machines as an alternate. SVMs are
a state-of-the-art learning algorithm, owing their
success mainly to the ability in control of general-
ization error upper-bound, and the smooth integra-
tion with kernel methods. See details in (Vapnik,
1995). We adopted svm-light1 as the specific
implementation of the model.
1.1 Problem Formalization
By formalizing Chinese word segmentation into
the problem of character-based tagging, we as-
1http://svmlight.joachims.org/
signed each character to one and only one of the
four classes: word-prefix, word-suffix,
word-stem and single-character. For
example, given a two-word sequence????
??, the Chinese words for ?Southeast Asia(?
??) people(?) ?, the character ???is as-
signed to the category word-prefix, indicating
the beginning of a word;???is assigned to the
category word-stem, indicating the middle po-
sition of a word; ???belongs to the category
word-suffix, meaning the ending of a Chinese
word; and last,???is assigned to the category
single-character, indicating that the single
character itself is a word.
1.2 Feature Templates
We utilized four of the five basic feature templates
suggested in (Low et al , 2005), described as
follows:
? Cn(n = ?2,?1, 0, 1, 2)
? CnCn+ 1(n = ?2,?1, 0, 1)
? Pu(C0)
? T (C?2)T (C?1)T (C0)T (C1)T (C2)
where C refers to a Chinese character. The first
two templates specify a context window with the
size of five characters, where C0 stands for the
current character: the former describes individual
characters and the latter presents bigrams within
the context window. The third template checks
if current character is a punctuation or not, and
the last one encodes characters? type, including
four types: numbers, dates, English letters and
the type representing other characters. See de-
tail description and the example in (Low et al
, 2005). We dropped template C?1C1, since,
217
in experiments, it seemed not to perform well
when incorporated by SVMs. Slightly different
from (Low et al , 2005), character set repre-
senting dates are expanded to include ????
???????????????????,
the Chinese characters for ?day?, ?month?, ?year?,
?hour?,?minute?,?second?, respectively.
2 Post-processing Rules
Segmentation results of SVM-based segmenter
have their particular properties. In respect to the
properties of segmentation results produced by the
SVM-based segmenter, we extracted solely from
training data comprehensive and effective post-
processing rules, which are grouped into two cate-
gories: The rules, termed IV rules, make ef-
forts to fix segmentation errors of character se-
quences, which appear both in training and test-
ing data; Rules seek to recall some OOV(Out
Of Vocabulary) words, termed OOV rules. In
practice, we sampled out a subset from train-
ing dataset as a development set for the analysis
of segmentation results produced by SVM-based
segmenter. Note that, in the following, we defined
Vocabulary to be the collection of words ap-
pearing in training dataset and Segmentation
Unit to be any isolated character sequence as-
sumed to be a valid word by a segmenter. A
segmentation unit can be a correctly seg-
mented word or an incorrectly segmented charac-
ter sequence.
2.1 IV Rules
The following rules are named IV rules, pur-
suing the consistence between segmentation re-
sults and training data. The intuition underlying
the rules is that since training data give somewhat
specific descriptions for most of the words in it, a
character sequence in testing data should be seg-
mented in accordance with training data as much
as possible.
Ahead of post-processing, all words in the
training data are grouped into two distinct sets:
the uniquity set, which consists of words
with unique segmentation in training data and the
ambiguity set, which includes words having
more than one distinct segmentations in training
data. For example, the character sequence???
??has two kinds of segmentations, as?? ?
??(new century) and?????(as a compo-
nent of some Named-Entity, such as the name of a
restaurant).
? For each word in the uniquity set, check
whether it is wrongly segmented into more
than one segmentation units by the SVM-
based segmenter. If true, the continuous seg-
mentation units corresponding to the word
are grouped into the united one. The in-
tuition underlying this post-processing rule
is that SVM-based segmenter prefers two-
character words or single-character words
when confronting the case that the segmenter
has low self-confidence in some character-
sequence segmentation. For example, ??
???(duplicate) was segmented as ??
???and ????(unify) was split
into ?? ??. This phenomenon is
caused by the imbalanced data distribution.
Specifically, characters belonging to category
word-stem are much less than other three
categories.
? For each segmentation unit in the result
produced by SVM-based segmenter, check
whether the unit can be segmented into more
than one IV words and, meanwhile, the words
exist in a successive form for at least once in
training data . If true, replace the segmen-
tation unit with corresponding continuously
existing words. The intuition underlying this
rule is that SVM-based segmenter tends to
combine a word with some suffix, such as
???????, two Chinese characters
representing ?person?. For example, ??
? ??(Person in registration) tends to be
grouped as a single unit.
? For any sequence in the ambiguity set, such
as ?????, check if the correct seg-
mentation can be determined by the con-
text surrounding the sequence. Without los-
ing the generality, in the following explana-
tion, we assume each sequence in the am-
biguity set has two distinct segmentations.
we collected from training data the word
preceding a sequence where each existence
of the sequence has one of its segmenta-
tions, into a collection, named preceding
word set, and, correspondingly, the fol-
lowing word into another set, which is
termed following word set. Analog-
ically, we can produce preceding word
218
set and following word set for an-
other case of segmentation. When an am-
biguous sequence appears in testing data, the
surrounding context (in fact, just one preced-
ing word and a following word) is extracted.
If the context has overlapping with either of
the pre-extracted contexts of the same se-
quence which are from training data, the seg-
mentation corresponding to one of the con-
texts is retained.
? More over, we took a look into the annotation
errors existing in training data. We assume
there unavoidably exist some annotation mis-
takes. For example, in UPENN, the sequence
????(abbreviation for China and Amer-
ica) exists, for eighty-seven times, as a whole
word and only one time, exists as?? ??.
We regarded the segmentation?? ??as
an annotation error. Generally, when the ra-
tio of two kinds of segmentations is greater
than a pre-determined threshold (the value is
set seven in our system), the sequence is re-
moved from the ambiguity set and added as
a word of unique segmentation into the uniq-
uity set.
2.2 OOV Rules
The following rules are termed OOV rules,
since they are utilized to recall some of the
wrongly segmented OOV words. A OOV word
is frequently segmented into two continuous OOV
segmentation units. For example, the OOV
word?????(Vatican) was frequently seg-
mented as ??? ??, where both ??
??and ???are OOV character sequences.
Continuous OOVs present a strong clue of po-
tential segmentation errors. A rule is designed
to merge some of continuous OOVs into a cor-
rect segmentation unit. The designed rule is ap-
plicable to all four corpora. Moreover, since dis-
tinction between different segmentation standards
frequently leads to very different segmentation of
a same OOV words in different corpora, we de-
signed rules particularly for MSRA and UPENN
respectively, to recall more OOVs.
? For two continuous OOVs, check whether
at least one of them is a single-character
word. If true, group the continuous OOVs
into a segmentation unit. The reason for
the constraint of at least one of continuous
OOVs being single-character word is that not
all continuous OOVs should be combined,
for example, ??? ???, both ??
??(Germany merchant) and????(the
company name) are OOVs, but this sequence
is a valid segmentation unit. On the other
hand, we assume appropriately that most of
the cases for character being single-character
word have been covered by training data.
That is, once a single character is a OOV seg-
mentation unit, there exists a segmentation
error with high possibility.
? MSRA has very different segmentation stan-
dard from other three corpora, mainly be-
cause it requires to group several continuous
words together into a Name Entity. For ex-
ample, the word???????(the Min-
istry of Foreign Affairs of China) appear-
ing in MSRA is generally annotated into two
words in other corpora, as????(China)
and?????(the Ministry of Foreign Af-
fairs). In our system, we first gathered all
the words from the training data whose length
are greater than six Chinese characters, filter-
ing out dates and numbers, which was cov-
ered by Finite State Automation as
a pre-processing stage. For each words col-
lected, regard the first two and three charac-
ters as NE prefix, which indicates the be-
ginning of a Name Entity. The collection of
prefixes is termed Sp(refix). Analogously, the
collection Ss(uffix) of suffixes is brought up
in the same way. Obviously not all the pre-
fixes (suffixes) are good indicators for Name
Entities. Partly inheriting from (Brill, 1995),
we applied error-driven learning to filter pre-
fixes in Sp and suffixes in Ss. Specifically,
if a prefix and a suffix are both matched in
a sequence, all the characters between them,
together with the prefix and the suffix, are
merged into a single segmentation unit. The
resulted unit is compared with corresponding
sequence in training data. If they were not ex-
actly matched, the prefix and suffix were re-
moved from collections respectively. Finally
resulted Sp and Ss are utilized to recognize
Name Entities in the initial segmentation re-
sults.
? UPENN has different segmentation standard
from other three corpora in that, for some
219
Corpus R P F ROOV RIV
AS 0.949 0.940 0.944 0.694 0.960
MSRA 0.955 0.956 0.956 0.650 0.966
UPENN 0.940 0.914 0.927 0.634 0.969
CITYU 0.965 0.971 0.968 0.719 0.981
Table 1: Our official SIGHAN bakeoff results
Locations, such as ?????(Beijing
) and Organizations, such as ???
??(the Ministry of Foreign Affairs), the
last Chinese character presents a clue that
the character with high possibility is a suf-
fix of some words. In fact, SVM-based seg-
menter sometimes mistakenly split an OOV
word into a segmentation unit followed by a
suffix. Thus, when some suffixes exist as a
single-character segmentation unit, it should
be grouped with the preceding segmentation
unit. Undoubtedly not all suffixes are appro-
priate to this rule. To gather a clean collec-
tion of suffixes, we first clustered together the
words with the same suffix, filtering accord-
ing to the number of instances in each clus-
ter. Second, the same as above, error-driven
method is utilized to retain effective suffixes.
3 Evaluation Results
We evaluated the Chinese word segmentation
system in the close track, on all four cor-
pora, namely Academis Sinica (AS), City Uni-
versity of Hong Kong (CITYU), Microsoft Re-
search (MSRA), and University of Pennsylva-
nia/University of Colorado (UPENN). The results
are depicted in Table 1, where columns R,P and
F refer to Recall, Precision, F measure
respectively, and ROOV , RIV for the recall of out-
of-vocabulary words and in-vocabulary words.
In addition to final results reported in Bake-
off, we also conducted a series of experiments to
evaluate the contributions of IV rules and OOV
rules. The experimental results are showed in
Table 2, where V1, V2, V3 represent versions
of our segmenters, which compose differently of
components. In detail, V1 represents the basic
SVM-based segmenter; V2 represents the seg-
menter which applied IV rules following SVM-
based segmentation; V3 represents the segmenter
composing of all the components, that is, includ-
ing SVM-based segmenter, IV rules and OOV
rules. Since the OOV ratio is much lower than IV
correspondence, the improvement made by OOV
rules is not so dramatic as IV rules.
Corpus V1 v2 v3
AS 0.932 0.94 0.944
MSRA 0.939 0.954 0.956
UPENN 0.914 0.923 0.927
CITYU 0.955 0.966 0.968
Table 2: Word segmentation accuracy(F Measure)
resulted from post-processing rules
4 Conclusions and future work
We added post-processing rules to SVM-based
segmenter. By doing so, we our segmentation sys-
tem achieved comparable results in the close track,
on all four corpora. But on the other hand, post-
processing rules have the problems of confliction,
which limits the number of rules. We expect to
transform rules into features of SVM-based seg-
menter, thus incorporating information carried by
rules in a more elaborate manner.
Acknowledgements
This research was supported in part by the Na-
tional Natural Science Foundation of China(No.
60473140) and by Program for New Century Ex-
cellent Talents in University(No. NCET-05-0287).
References
Nianwen Xue and Libin Shen. 2003. Chinese Word
segmentation as LMR tagging. In Proceedings of
the Second SIGHAN Workshop on Chinese Lan-
guage Processing,pages 176-179.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Berlin: Springer-Verlag.
Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo. 2005.
A Maximum Entropy Approach to Chinese Word
Segmentation. In Proceeding of the Fifth SIGHAN
Workshop on Chinese Language Processing, pages
161-164.
Eric.Brill. 1995. Transformation-based error-driven
learning and natural language processing:A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543-565.
220
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 143?151,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Chinese-English Organization Name Translation Based on Correla-
tive Expansion 
Feiliang Ren, Muhua Zhu,  Huizhen Wang,   Jingbo Zhu  
Natural Language Processing Lab, Northeastern University, Shenyang, China 
{renfeiliang,zhumuhua}@gmail.com 
{wanghuizhen,zhujingbo}@mail.neu.edu.cn 
Abstract 
This paper presents an approach to trans-
lating Chinese organization names into 
English based on correlative expansion. 
Firstly, some candidate translations are 
generated by using statistical translation 
method. And several correlative named 
entities for the input are retrieved from a 
correlative named entity list. Secondly, 
three kinds of expansion methods are 
used to generate some expanded queries. 
Finally, these queries are submitted to a 
search engine, and the refined translation 
results are mined and re-ranked by using 
the returned web pages. Experimental re-
sults show that this approach outperforms 
the compared system in overall transla-
tion accuracy.  
1 Introduction 
There are three main types of named entity: loca-
tion name, person name, and organization name. 
Organization name translation is a subtask of 
named entity translation. It is crucial for many 
NLP tasks, such as cross-language information 
retrieval, machine translation, question and an-
swering system. For organization name transla-
tion, there are two problems among it which are 
very difficult to handle.  
Problem I: There is no uniform rule that can 
be abided by to select proper translation methods 
for the inside words of an organization name. For 
example, a Chinese word ????, when it is used 
as a modifier for a university, it is translated to 
Northeastern for ?????/Northeastern Uni-
versity?, and is translated to Northeast for ???
????/Northeast Forestry University?, and is 
mapped to Chinese Pinyin Dongbei for ????
???/Dongbei University of Finance and Eco-
nomics?. It is difficult to decide which transla-
tion method should be chosen when we translate 
the inside words of an organization name.  
Problem II: There is no uniform rule that can 
be abided by to select proper translation order 
and proper treatment of particles Here particles 
refer to prepositions and articles) for an input 
organization name. For example, the organiza-
tion name ???????/China Construction 
Bank? and the organization name ??????
?/Agricultural Bank of China?, they are very 
similar both in surface forms and in syntax struc-
tures, but their translation orders are different, 
and their treatments of particles are also different. 
Generally, there are two strategies usually 
used for named entity translation in previous re-
search. One is alignment based approach, and the 
other is generation based approach. Alignment 
based approach (Chen et al 2003; Huang et al 
2003; Hassan and Sorensen, 2005; and so on) 
extracts named entities translation pairs from 
parallel or comparable corpus by some alignment 
technologies, and this approach is not suitable to 
solve the above two problems. Because new or-
ganization names are constantly being created, 
and alignment based method usually fails to 
cover these new organization names that don?t 
occur in the bilingual corpus.  
Traditional generation based approach (Al-
Onaizan and Knight, 2002; Jiang et al.2007; 
Yang et al 2008; and so on) usually consists of 
two parts. Firstly, it will generate some candidate 
translations for the input; then it will re-rank 
these candidate translations to assign the correct 
translations high ranks. Cheng and Zong [2008] 
proposed another generation based approach for 
organization name translation, which directly 
translates organization names according to their 
inherent structures. But their approach still can?t 
solve the above two problems. This is because 
the amount of organization names is so huge and 
many of them have their own special translation 
rules to handle the above two problems. And the 
inherent structures don?t reveal these translation 
rules. Traditional generation based approach is 
suitable for organization name translation. But in 
previous research, the final translation perform-
ance depends on the candidate translation gen-
143
eration process greatly. If this generation process 
failed, it is impossible to obtain correct result 
from the re-ranking process. In response to this, 
Huang et al [2005] proposed a novel method that 
mined key phrase translation form web by using 
topic-relevant hint words. But in their approach, 
they removed the candidate translation genera-
tion process, which will improve extra difficult 
during mining phrase. Besides, in their approach, 
the features considered to obtain topic-relevant 
words are not so comprehensive, which will af-
fect the quality of returned web pages where the 
correct translations are expected to be included. 
There is still much room for the improvement 
process of the topic-relevant words extraction.  
Inspired by the traditional generation based 
named entity translation strategy and the ap-
proach proposed by Huang et al, we propose an 
organization name translation approach that min-
ing the correct translations of input organization 
name from the web. Our aim is to solve the 
above two problems indirectly by retrieving the 
web pages that contain the correct translation of 
the input and mining the correct translation from 
them. Given an input organization name, firstly, 
some candidate translations are generated by us-
ing statistical translation method. And several 
correlative named entities for the input are re-
trieved from a correlative named entity list. Sec-
ondly, expanded queries are generated by using 
three kinds of query expansion methods. Thirdly, 
these queries are submitted to a search engine, 
and the final translation results are mined and re-
ranked by using the returned web pages.  
The rest of this paper is organized as follows, 
section 2 presents the extraction process of cor-
relative named entities, section 3 presents a detail 
description of our translation method for Chinese 
organization name, and section 4 introduces our 
parameter evaluation method, and section 5 is the 
experiments and discussions part, finally conclu-
sions and future work are given in section 6.  
2 Extraction of Correlative Named En-
tities 
The key of our approach is to find some web 
pages that contain the correct translation of the 
input. With the help of correlative named entities 
(here if two named entities are correlative, it 
means that they are usually used to describe the 
same topic), it is easier to find such web pages. 
This is because that in the web, one web page 
usually has one topic. Thus if two named entities 
are correlative, they are very likely to occur in 
pair in some web pages.  
The correlative named entity list is constructed 
in advance. During translation, the correlative 
named entities for the input organization name 
are retrieved from this list directly. To set up this 
correlative named entity list, an about 180GB-
sized collection of web pages are used. Totally 
there are about 100M web pages in this collec-
tion. Named entities are recognized from every 
web page by using a NER tool. This NER tool is 
trained by CRF model 1  with the corpus from 
SIGHAN-20082.  
2.1 Features Used 
During the extraction of correlative named enti-
ties, the following features are considered.  
Co-occurrence in a Document The more of-
ten two named entities co-occur in a document, 
the more likely they are correlative. This feature 
is denoted as 1 2( , )iCoD n n , which means the co-
occurrence of named entities 1n and 2n  in a docu-
ment iD . This feature is also the main feature 
used in Huang et al [2005].   
Co-occurrence in Documents The more often 
two named entities co-occur in different docu-
ments, the more likely they are correlative. This 
feature is denoted as 1 2( , )CoDs n n , which means 
the number of documents that both 1n  and 2n oc-
cur in. 
Distance The closer two named entities is in a 
document, the more likely they are correlative. 
This feature is denoted as 1 2( , )iDistD n n , which 
means the number of words between 1n and 2n  
in a document iD . 
Mutual Information Mutual information is a 
metric to measure the correlation degree of two 
words. The higher two named entities? mutual 
information, the more likely they are correlative. 
And the mutual information of named entities 
1n and 2n  in a document iD is computed as fol-
lowing formula. 
1 2
1 2 1 2
1 2
( , )
( , ) ( , ) log
( ) ( )i
p n n
MID n n p n n
p n p n
= ?  (1) 
Jaccard Similarity Jaccard similarity is also a 
metric to measure the correlative degree of two 
words. The higher two named entities? Jaccard 
                                                 
1 http://www.chasen.org/~taku/software/CRF++/ 
2 http://www.china-language.gov.cn/bakeoff08/ 
144
similarity, the more likely they are correlative. 
And Jaccard similarity is computed as following 
formula. 
1 2
1 2
1 2 1 2
( , )
( , )
( ) ( ) ( , )
CoDs n n
Jaccard n n
D n D n CoDs n n
= + ? (2) 
where ( )iD n  is the number of documents that 
in occurs in, and  ( , )i jCoDs n n  is the number of 
documents that both in  and jn occur in. 
TF-IDF TF-IDF is a weight computation 
method usually used in information retrieval. 
Here for a named entity in , TF-IDF is used to 
measure the importance of its correlative named 
entities. The TF-IDF value of jn in a document 
iD is computed as following formula. 
( ) log
( )i j ij j
N
TF IDF n tf
D n
? = ?               (3) 
where ijtf is the frequency of jn in docu-
ment iD , N is the number of total documents, 
and ( )jD n is the number of documents that 
jn occurs in.  
2.2 Feature Combination 
During the process of feature combination, every 
feature is normalized, and the final correlative 
degree of two named entities is the linear combi-
nation of these normalized features, and it is 
computed as following formula.   
1 2
( , ) ( , )
( , )
( , ) ( , )
k i j
i jk
i j
k i j i j
j k j
CoD n n CoDs n n
C n n
CoD n n CoDs n n
? ?= +
?
?? ?
3 4
1 ( , )( , )
1 ( , )
( , )
k i j
k i jk k
k i j
k i j j kj k
MID n nDistD n n
MID n n
DistD n n
? ?+ +
? ?
????
5 6
( )( , )
( , ) ( )
k j
i j k
i j k j
j k j
TF IDF nJaccard n n
Jaccard n n TF IDF n
? ?
?
+ + ?
?
? ??
(4) 
Finally, for every organization name in , its 
top-K correlative named entities are selected to 
construct the correlative named entity list.  
During translation, the correlative words for 
the input can be retrieved from this correlative 
list directly. If the input is not included in this list, 
the same method as in Huang et al [2005] is 
used to obtain the needed correlative words.  
3 Organization Name Translation 
Based on Correlative Expansion 
3.1 Statistical Translation Module 
The first step of our approach is to generate some 
candidate translations for every input organiza-
tion name. As shown in table 1, these candidate 
translations are used as query stems during query 
expansion. We use Moses3, a state of the art pub-
lic machine translation tool, to generate such 
candidate translations. Here Moses is trained 
with the bilingual corpus that is from the 4th 
China Workshop on Machine Translation4. Total 
there are 868,947 bilingual Chinese-English sen-
tence pairs on news domain in this bilingual cor-
pus. Moses receives an organization name as in-
put, and outputs the N-best results as the candi-
date translations of the input organization name. 
Total there are six features used in Moses: phrase 
translation probability, inverse phrase translation 
probability, lexical translation probability, in-
verse lexical translation probability, language 
model, and sentence length penalty. All the 
needed parameters are trained with MERT 
method (Och, 2003) by using a held-out devel-
opment set.  
3.2 Query Expansions 
Because the amount of available web pages is so 
huge, the query submitted to search engine must 
be well designed. Otherwise, the search engine 
will return large amount of un-related web pages. 
This will enlarge the difficulty of mining phase. 
Here three kinds of expansion methods are pro-
posed to generate some queries by combining the 
clues given by statistical translation method and 
the clues given by correlative named entities of 
the input. And these correlative named entities 
are retrieved from the correlative named entities 
list before the query expansions process. These 
three kinds of expansions are explained as fol-
lows. 
3.2.1 Monolingual Expansion  
Given an input organization name in , suppose 
is is one of its candidate translations, and jn is 
one of its correlative named entities. If jn can be 
reliably translated5, we expand is with this reli-
                                                 
3 http://www.statmt.org/moses/  
4 http://www.nlpr.ia.ac.cn/cwmt-2008  
5 A word can be reliably translated means either it has 
a unique dictionary translation or it is a Chinese 
145
able translation ( )jt n  to form a query 
? is + ( )jt n ?. This kind of expansion is called as 
monolingual expansion.  
For two named entities, if they are correlative, 
their translations are likely correlative too. So 
their translations are also likely to occur in pair 
in some web pages. Suppose a query generated 
by this expansion is ? is + ( )jt n ?, if the candidate 
translation is is the correct translation of the in-
put, there must be some returned web pages that 
contain is completely. Otherwise, it is still possi-
ble to obtain some returned web pages that con-
tain the correct translation. This is because that 
the search engine will return both the web pages 
that include the query completely and the web 
pages that include the query partly. And for a 
translation candidate is and the correct transla-
tion 'is , they are very likely to have some com-
mon words, so some of their returned web pages 
may overlap each other. Thus it can be expected 
that when we submit ? is + ( )jt n ? to search en-
gine, it will return some web pages that include 
? 'is + ( )jt n ? or include 'is .  This is very helpful 
for the mining phase. 
3.2.2 Bilingual Expansion  
Given an input organization name in , suppose 
is is one of its candidate translations, we ex-
pand is with in  to form a query ? is + in ?. This 
kind of expansion is called as bilingual expan-
sion. 
Bilingual expansion is very useful to verify 
whether a candidate translation is the correct 
translation. To give readers more information or 
they are not sure about the translation of original 
named entity, the Chinese authors usually in-
clude both the original form of a named entity 
and its translation in the mix-language web pages 
[Fei Huang et al 2005]. So the correct translation 
pair is likely to obtain more supports from the 
returned web pages than those incorrect transla-
tion pairs. Thus bilingual expansion is very use-
ful for the re-ranking phase. 
Besides, for an input organization name, if one 
of its incorrect candidate translations is  is very 
                                                                          
person name and can be translated by Pinyin map-
ping.  
similar to the correct translation 'is  in surface 
form, the correct translation is also likely to be 
contained in the returned web pages by using this 
kind of queries. The reason for this is the search 
mechanism of search engine, which has been 
explained above in monolingual expansion. 
3.2.3 Mix-language Expansion  
Given an input organization name in , suppose 
is is one of its candidate translations, and jn is 
one of its correlative named entities. We ex-
pand is with jn  to form a query ? is + jn ?. This 
kind of expansion is called as mix-language ex-
pansion.  
Mix-language expansion is a necessary com-
plement to the other two expansions. Besides, 
this mix-language expansion is more prone to 
obtain some mix-language web pages that may 
contain both the original input organization name 
and its correct translation.  
3.3 Mining 
When the expanded queries are submitted to 
search engine, the correct translation of the input 
organization name may be contained in the re-
turned web pages. Because the translation of an 
organization name must be also an organization 
name, we mine the correct translation of the in-
put among the English organization names. Here 
we use the Stanford named entity recognition 
toolkits6  to recognize all the English organiza-
tion names in the returned web pages. Then align 
these recognized organization names to the input 
by considering the following features. 
Mutual Translation Probability The transla-
tion probability measures the semantic equiva-
lence between a source organization name and its 
target candidate translation. And mutual transla-
tion probability measures this semantic equiva-
lence in two directions. For simplicity, here we 
use IBM model-1(Brown et al 1993), which 
computes two organization names? translation 
probability using the following formula. 
11
1
( | ) ( | )
J L
j lJ
lj
p f e p f e
L ==
= ??                  (6) 
where ( | )j lp f e is the lexical translation prob-
ability. Suppose the input organization name 
is in , is is one of the recognized English organi-
                                                 
6  http://nlp.stanford.edu/software/CRF-NER.shtml 
146
zation names, the mutual translation probability 
of in and is  is computed as: 
( , ) ( | ) (1 ) ( | )i i i i i imp n s p n s p s n? ?= + ?      (7) 
Golden Translation Ratio For two organiza-
tion names, their golden translation ratio is de-
fined as the percentage of words in one organiza-
tion name whose reliable transactions can be 
found in another organization name. This feature 
is used to measure the probability of one named 
entity is the translation of the other. It is com-
puted as following formula. 
( , ) ( , )
( , ) (1 )
| | | |
i j j i
i j
i j
G n s G s n
GR n s
n s
? ?= + ?   (8) 
where ( , )i jG n s is the number of golden trans-
lated words from in to js , and ( , )j iG s n  is the 
number of golden translated words from js to in .  
Co-occurrence In Web Pages For an input 
organization name in and a recognized candidate 
translation js , the more often they co-occur in 
different web pages, the more likely they are 
translations of each other. This feature is denoted 
as ( , )i jCoS n s , which means the number of web 
pages that both 1n  and js occur in. 
Input Matching Ratio This feature is defined 
as the percentage of the words in the input that 
can be found in a returned web page. For those 
mix-language web pages, this feature is used to 
measure the probability of the correct translation 
occurring in a returned web page. It is computed 
as the following formula. 
| |
( , )
| |
i k
i k
i
n s
IMR n s
n
?=                             (9) 
where ks is the k th?  returned web page. 
Correlative Named Entities Matching Ratio 
This feature is defined as the percentage of the 
words in a correlative named entity that can be 
found in a returned web page. This feature is also 
used to measure the probability of the correct 
translation occurring in a returned web page. It is 
computed as the following formula. 
| |
_ ( , )
| |
i k
i k
i
c s
CW MR c s
c
?=                   (10) 
The final confidence score of in and jt to be a 
translation pair is measured by following formula. 
As in formula 4, here every factor will be is nor-
malized during computation.   
1 2( , ) ( , ) ( , )i j i j i jC n t mp n t GR n t? ?= +  
4
3
( , )
( , )
( , )
i j
i k
ki j
j
CoSs n n
IMR n s
CoS n n K
??+ + ??
5 _ ( , )i k
i k
CW MR c s
K I
?+ ? ??           (11) 
where K is the number of returned web pages, 
I is the number of correlative named entities for 
the input organization name. 
For every input organization name, we remain 
a fixed number of mined candidate translations 
with the highest confidence scores. And add 
them to the original candidate translation set to 
form a revised candidate translation set.  
3.4 Re-ranking 
The aim of mining is to improve recall. And in 
the re-ranking phase, we hope to improve preci-
sion by assigning the correct translation a higher 
rank. The features considered here for the re-
ranking phase are listed as follows.  
Confidence Score The confidence score of 
in and jt  is not only useful for the mining phase, 
but also is useful for the re-ranking phase. The 
higher this score, the higher rank this candidate 
translation should be assigned.  
Inclusion Ratio For Bilingual Query This 
feature is defined as the percentage of the re-
turned web pages that the bilingual query is 
completely matched. It is computed as the fol-
lowing formula. 
( )
_ ( )
( )
i
i
i
h q
EHR BQ q
H q
=                           (12) 
where ( )ih q is the number of web pages that 
match the query iq completely, and ( )iH q is the 
total number of returned web pages for query iq . 
Candidate Inclusion Ratio for Monolingual 
Query and Mix-language Query This feature is 
defined as the percentage of the returned web 
pages that the candidate translation is completed 
matched. This feature for monolingual query is 
computed as formula 13, and this feature for 
mix-language query is computed as formula 14. 
( )_ ( ) ( )
i
i
i
h sECHR MlQ s H q=                (13) 
( )_ ( ) ( )
i
i
i
h sECHR MixQ s H q=              (14) 
where ( )ih s  is the number of web pages that 
match the candidate translation is completely, and 
147
( )iH q is the total number of returned web pages 
for query iq .  
Finally, the above features are combined with 
following formula.  
2
1( , ) ( , ) _ ( )i j i j i
i
R n t C n t EHR BQ q
N
??= + ?  
3 _ ( )i
i
ECHR MlQ s
M
?+ ?
4 _ ( )i
i
ECHR MixQ s
L
?+ ?              (15) 
where N is the number of candidate transla-
tions, M and L  are the number of monolingual 
queries and mix-language queries respectively. 
At last the revised candidate translation set is 
re-ranked according to this formula, and the top-
K results are outputted as the input?s translation 
results.  
4 Parameters Evaluations 
In above formula (4), formula (11) and formula 
(15), the parameters i? are interpolation feature 
weights, which reflect the importance of different 
features. We use some held-our organization 
name pairs as development set to train these pa-
rameters. For those parameters in formula (4), we 
used those considered features solely one by one, 
and evaluated their importance according to their 
corresponding inclusion ratio of correct transla-
tions when using mix-language expansion and 
the final weights are assigned according to the 
following formula. 
i
i
i
i
InclusionRate
InclusionRate
? = ?                   (16) 
Where iInclusionRate  is the inclusion rate 
when considered feature if  only. The inclusion 
rate is defined as the percentage of correct trans-
lations that are contained in the returned web 
pages as Huang et al[2005] did. 
To obtain the parameters in formula (11), we 
used those considered features solely one by one, 
and computed their corresponding precision on 
development set respectively, and final weights 
are assigned according to following formula. 
i
i
i
i
P
P
? = ?                              (17) 
Where iP  is the precision when considered 
feature if  only. And for the parameters in for-
mula (15), their assignment method is the same 
with the method used for formula (11). 
5 Experiments and Discussions 
We use a Chinese to English organization name 
translation task to evaluate our approach. The 
experiments consist of four parts. Firstly, we 
evaluate the contribution of the correlative 
named entities for obtaining the web pages that 
contain the correct translation of the input. Sec-
ondly, we evaluate the contribution of different 
query expansion methods. Thirdly, we investi-
gate to which extents our approach can solve the 
two problems mentioned in section 1. Finally, we 
evaluate how much our approach can improve 
the overall recall and precision. Note that for 
simplicity, we use 10-best outputs from Moses as 
the original candidate translations for every input. 
And the search engine used here is Live7. 
5.1 Test Set 
The test set consists of 247 Chinese organization 
names recognized from 2,000 web pages that are 
downloaded from Sina8. These test organization 
names are translated by a bilingual speaker given 
the text they appear in. And these translations are 
verified from their official government web 
pages respectively. During translation, we don?t 
use any contextual information. 
5.2 Contribution of Correlative Named En-
tities 
The contribution of correlative named entities is 
evaluated by inclusion rate, and we compare the 
inclusion rate with different amount of correla-
tive named entities and different amount of re-
turned web pages. The experimental results are 
shown in Table 1 (here we use all these three 
kinds of expanding strategies).  
# of correlative named enti-
ties used 
 
1 5 10 
1 0.17 0.39 0.47 
5 0.29 0.63 0.78 
#of web 
pages used
10    0.32 0.76 0.82 
Table 1. Comparisons of inclusion rate  
From these results we can find that our ap-
proach obtains an inclusion rate of 82% when we 
use 10 correlative named entities and 10 returned 
web pages. We notice that there are some Chi-
nese organization names whose correct English 
translations have multiple standards. For exam-
ple,  the organization name ?????is translated 
                                                 
7  http://www.live.com/ 
8  http://news.sina.com.cn/ 
148
into ?Department of Defense? when it refers to a 
department in US, but  is translated into ?Minis-
try of Defence? when it refers to a department in 
UK or in Singapore. This problem affects the 
actual inclusion rate of our approach. Another 
factor that affects the inclusion rate is the search 
engine used. There is a small difference in the 
inclusion rate when different search engines are 
used. For example, the Chinese organization 
name ?????/China CITIC Bank?, because 
the word ???? is an out-of-vocabulary word,  
the best output from Moses is ?of the bank?. 
With such candidate translation, none of our 
three expansion methods works. But when we 
used Google as search engine instead, we mined 
the correct translation. 
From these results we can conclude that by us-
ing correlative named entities, the returned web 
pages are more likely to contain the correct trans-
lations of the input organization names. 
5.3 Contribution of Three Query Expansion 
Methods 
In this section, we evaluate the contribution of 
these three query expansion methods respectively. 
To do this, we use them one by one during trans-
lation, and compare their inclusion rates respec-
tively. Experimental results are shown in Table 2. 
#of web pages 
used 
 
1 5 10
1 0.002 0.0020.004
5 0.017 0.0190.019
Monolingual 
Expansion 
Only 10 0.021 0.0370.051
1 0.112 0.1590.174
5 0.267 0.3270.472
Bilingual 
 Expansion
Only 10 0.285 0.4140.669
1 0.098 0.1380.161
5 0.231 0.3070.386
# of  
correlative 
named enti-
ties used 
Mix-language 
Expansion
Only 10 0.249 0.3980.652
Table 2. Inclusion rate of different kinds of query 
expansion methods 
From Table 2 we can see that bilingual expan-
sion and mix-language expansion play greater 
roles than monolingual expansion in obtaining 
the web pages that contain the correct transla-
tions of the inputs. This is because the condition 
of generating monolingual queries is too strict, 
which requires a reliable translation for the cor-
relative named entity. In most cases, this condi-
tion cannot be satisfied. So for many input or-
ganization names, we cannot generate any mono-
lingual queries for them at all. This is the reason 
why monolingual expansion obtains so poorer an 
inclusion rate compared with the other two ex-
pansions. To evaluate the true contribution of 
monolingual expansion method, we carry out 
another experiment. We select 10 organization 
names randomly from the test set, and translate 
all of their correlative named entities into English 
by a bilingual speaker. Then we evaluate the in-
clusion rate again on this new test set. The ex-
perimental results are shown in Table 3. 
# of correlative named enti-
ties used 
 
1 5 10 
1 0.2 0.3 0.6 
5 0.4 0.7 0.9 
#of web 
pages used
10    0.4 0.8 0.9 
Table 3. Inclusion rate for monolingual expan-
sion on new test set 
From Table 3 we can conclude that, if most of 
the correlative named entities can be reliably 
translated, the queries generated by this mono-
lingual expansion will play greater role in obtain-
ing the web pages that contain the correct trans-
lations of the inputs. 
From those results in Table 2 we can conclude 
that, these three kinds of expansions complement 
each other. Using them together can obtain 
higher inclusion rate than using anyone of them 
only. 
5.4 Efficiency on Solving Problem I and 
Problem II 
In this section, we investigate to which extents 
our approach can solve the two problems men-
tioned in section 1.We compare the wrong trans-
lation numbers caused by these two problems 
(another main kind of translation error is caused 
by the translation of out-of-vocabulary words) 
between Moses and our approach. The experi-
mental results are shown in Table 4.  
 Moses Results Our method
Problem I 44 3 
Problem II 30 0 
Table 4. Comparison of error numbers 
From Table 4 we can see that our approach is 
very effective on solving these two problems. 
Almost all of the errors caused by these two 
problems are corrected by our approach. Only 
three wrong translations are not corrected. This is 
because that there are some Chinese organization 
names whose correct English translations have 
multiple standards, such as the correct translation 
of organization name ?????depends on its 
nationality, which has been explained in section 
5.2. 
149
5.5 Our Approach vs. Other Approaches  
In this section, we compare our approach with 
other two methods: Moses and the approach pro-
posed by Huang et al [2005]. We compare their 
accuracy of Top-K results. For both our approach 
and Huang et al?s approach, we use 10 correla-
tive words for each input organization name and 
use 10 returned web pages for mining the correct 
translation result. The experimental results are 
shown in Table 5. 
 Moses  
Results 
Huang?s 
Results 
Our  
Results 
Top 1 0.09 0.44 0.53 
Top 5 0.18 0.61 0.73 
Top 10 0.31 0.68 0.79 
Table 5. Moses results vs. our results 
Moses is a state-of-the-art translation method, 
but it can hardly handle the organization name 
translation well. In addition to the errors caused 
by the above two problems mentioned in section 
1, the out-of-vocabulary problem is another ob-
stacle for Moses. For example, when translating 
the organization name ?????????
/International Tsunami Information Centre?, be-
cause the word ???? is an out-of-vocabulary 
word, Moses fails to give correct translation. But 
for those approaches that have a web mining 
process during translation, both the out-of-
vocabulary problem and the two problems men-
tioned in section 1 are less serious. This is the 
reason that Moses obtains the lowest perform-
ance compared with the other two approaches. 
Our approach is also superior to Huang?s method, 
as shown in the above table. We think this is be-
cause of the following three reasons. The first 
reason is that in our approach, we use a transla-
tion candidate generation process. Although 
these candidates are usually not so good, they 
can still provide some very useful clue informa-
tion for the web retrieval process. The second 
reason is that the features considered for correla-
tive words extraction in our approach are more 
comprehensive. Most of the time (except for the 
case that the input is not included in the correla-
tive word list) our approach is more prone to ob-
tain better correlative words for the input. The 
third reason is that our approach use more query 
expansion strategies than Huang?s approach. 
These expansion strategies may complement 
each other and improve the probability of obtain-
ing the web pages that contain the correct trans-
lations For example, both Moses and Huang?s 
approach failed to translate the organization 
name ??????????. But in our approach, 
with the candidate translation ?International In-
formation Centre? that is generated by Moses, 
our approach still can obtain the web page that 
contains the correct translation when using bilin-
gual expansion. Thus the correct translation ?In-
ternational Tsunami Information Centre? is 
mined out during the sequent mining process.  
From table 5 we also notice that the final re-
call of our approach is a little lower than the in-
clusion rate as show in table 1. This means that 
our approach doesn?t mine all the correct transla-
tions that are contained in the returned web pages. 
One of the reasons is that some of the input or-
ganization names are not clearly expressed. For 
example, an input organization name ?????
??, although its correct translation ?University 
of California, Berkeley? is contained in the re-
turned web pages, this correct translation cannot 
be mined out by our approach. But if it is ex-
pressed as ??????????????, its 
correct translation can be mined from the re-
turned web pages easily. Besides, the recognition 
errors of NER toolkits will also reduce the final 
recall of our approach.  
6 Conclusions and Future Work 
In this paper, we present a new organization 
name translation approach. It uses some correla-
tive named entities of the input and some query 
expansion strategies to help the search engine to 
retrieve those web pages that contain the correct 
translation of the input. Experimental results 
show that for most of the inputs, their correct 
translations are contained in the returned web 
pages. By mining these correct translations and 
re-ranking them, the two problems mentioned in 
section 1 are solved effectively. And recall and 
precision are also improved correspondingly.  
In the future, we will try to improve the ex-
traction perform of correlative named entities. 
We will also try to apply this approach to the 
person name translation and location name trans-
lation. 
Acknowledgments  
This work was supported by the open fund of 
National Laboratory of Pattern Recognition, In-
stitute of Automation Chinese Academy of Sci-
ence, P.R.C, and was also supported in part by 
National Science Foundation of China 
(60873091), Natural Science Foundation of 
Liaoning Province (20072032) and Shenyang 
Science and Technology Plan (1081235-1-00). 
150
References 
Chen Hsin-Hsi, Changhua Yang, and Ying Lin. 2003. 
Learning formulation and transformation rules for 
multilingual named entities. Proceedings of the 
ACL 2003 Workshop on Multilingual and Mixed-
language Named Entity Recognition. pp1-8. 
Dekang Lin, Shaojun Zhao, Durme Benjamin Van 
Drume, Marius Pasca. Mining Parenthetical Trans-
lations from the Web by Word Alignment,  ACL08. 
pp994-1002. 
Fan Yang, Jun Zhao, Bo Zou, Kang Liu, Feifan Liu. 
2008. Chinese-English Backward Transliteration 
Assisted with Mining Monolingual Web Pages. 
ACL2008. pp541-549. 
Fei Huang, Stephan Vogel and Alex Waibel. 2003. 
Automatic Extraction of Named Entity Translin-
gual Equivalence Based on Multi-feature Cost 
Minimization. Proceedings of the 2003 Annual 
Conference of the Association for Computational 
Linguistics, Workshop on Multilingual and Mixed-
language Named Entity Recognition.   
Fei Huang, Stephan vogel and Alex Waibel. 2004. 
Improving Named Entity Translation Combining 
Phonetic and Semantic Similarities. Proceedings of 
the HLT/NAACL. pp281-288.  
Fei Huang, Ying Zhang, Stephan Vogel. 2005. Min-
ing Key Phrase Translations from Web Corpora. 
HLT-EMNLP2005, pp483-490. 
Feng, Donghui, Yajuan LV, and Ming Zhou. 2004. A 
new approach for English-Chinese named entity 
alignment. Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing 
(EMNLP 2004), pp372-379. 
Franz Josef Och. 2003. Minimum Error Rate Training 
in Statistical Machine Translation. ACL2003. 
pp160-167. 
Jin-Shea Kuo, Haizhou Li, Ying-Kuei Yang. Learning 
Transliteration Lexicon from the Web. COL-
ING/ACL2006. pp1129-1136. 
Hany Hassan and Jeffrey Sorensen. 2005. An Inte-
grated Approach for Arabic-English Named Entity 
Translation. Proceedings of ACL Workshop on 
Computational Approaches to Semitic Languages. 
pp87-93. 
Lee, Chun-Jen and Jason S.Chang and Jyh-Shing 
Roger Jang. 2004a. Bilingual named-entity pairs 
extraction from parallel corpora. Proceedings of 
IJCNLP-04 Workshop on Named Entity Recogni-
tion for Natural Language Processing Application. 
pp9-16. 
Lee, Chun-Jen, Jason S.Chang and Thomas C. 
Chuang. 2004b. Alignment of bilingual named en-
tities in parallel corpora using statistical model. 
Lecture Notes in Artificial Intelligence. 3265:144-
153. 
Lee, Chun-Jen, Jason S.Chang, and Jyh-Shing Roger 
Jang. 2005. Extraction of transliteration pairs from 
parallel corpora using a sta Acquisition of English-
Chinese transliterated word pairs from parallel-
aligned text using a statistical transliteration model. 
Information Sciences.  
Long Jiang, Ming Zhou, Lee-Feng Chien, Cheng Niu. 
[2007]. Named Entity Translation with Web Min-
ing and Transliteration. IJCAI-2007. 
Moore, Robert C. 2003. Learning translations of 
named-entity phrases form parallel corpora. ACL-
2003. pp259-266. 
Peter F. Brown, Vincent J. Della Pietra, Stephen A. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2):263-311.  
Y. Al-Onaizan and K. Knight. 2002. Translating 
named entities using monolingual and bilingual re-
sources. In Proceedings of the 40th Annual Meeting 
of the Association for Computational Linguistics, 
pp400-408. 
Ying Zhang and Phil Vines Using the Web for Auto-
mated Translation Extraction in Cross-Language 
Information Retrieval. SIGIR2004,pp162-169. 
Yufeng Chen, Chengqing Zong. A Structure-based 
Model for Chinese Organization Name Translation. 
ACM Transactions on Asian Language Information 
Processing, 2008, 7(1), pp1-30. 
151
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 739?748,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Boosting-based System Combination for Machine Translation 
 
Tong Xiao, Jingbo Zhu, Muhua Zhu, Huizhen Wang 
 
Natural Language Processing Lab.  
Northeastern University, China 
{xiaotong,zhujingbo,wanghuizhen}@mail.neu.edu.cn 
zhumuhua@gmail.com 
 
 
Abstract 
In this paper, we present a simple and effective 
method to address the issue of how to generate 
diversified translation systems from a single 
Statistical Machine Translation (SMT) engine 
for system combination. Our method is based 
on the framework of boosting. First, a se-
quence of weak translation systems is gener-
ated from a baseline system in an iterative 
manner. Then, a strong translation system is 
built from the ensemble of these weak transla-
tion systems. To adapt boosting to SMT sys-
tem combination, several key components of 
the original boosting algorithms are redes-
igned in this work. We evaluate our method on 
Chinese-to-English Machine Translation (MT) 
tasks in three baseline systems, including a 
phrase-based system, a hierarchical phrase-
based system and a syntax-based system. The 
experimental results on three NIST evaluation 
test sets show that our method leads to signifi-
cant improvements in translation accuracy 
over the baseline systems. 
1 Introduction 
Recent research on Statistical Machine Transla-
tion (SMT) has achieved substantial progress. 
Many SMT frameworks have been developed, 
including phrase-based SMT (Koehn et al, 2003), 
hierarchical phrase-based SMT (Chiang, 2005), 
syntax-based SMT (Eisner, 2003; Ding and 
Palmer, 2005; Liu et al, 2006; Galley et al, 2006; 
Cowan et al, 2006), etc. With the emergence of 
various structurally different SMT systems, more 
and more studies are focused on combining mul-
tiple SMT systems for achieving higher transla-
tion accuracy rather than using a single transla-
tion system. 
The basic idea of system combination is to ex-
tract or generate a translation by voting from an 
ensemble of translation outputs. Depending on 
how the translation is combined and what voting 
strategy is adopted, several methods can be used 
for system combination, e.g. sentence-level com-
bination (Hildebrand and Vogel, 2008) simply 
selects one from original translations, while 
some more sophisticated methods, such as word-
level and phrase-level combination (Matusov et 
al., 2006; Rosti et al, 2007), can generate new 
translations differing from any of the original 
translations. 
One of the key factors in SMT system combi-
nation is the diversity in the ensemble of transla-
tion outputs (Macherey and Och, 2007). To ob-
tain diversified translation outputs, most of the 
current system combination methods require 
multiple translation engines based on different 
models. However, this requirement cannot be 
met in many cases, since we do not always have 
the access to multiple SMT engines due to the 
high cost of developing and tuning SMT systems. 
To reduce the burden of system development, it 
might be a nice way to combine a set of transla-
tion systems built from a single translation en-
gine. A key issue here is how to generate an en-
semble of diversified translation systems from a 
single translation engine in a principled way. 
Addressing this issue, we propose a boosting-
based system combination method to learn a 
combined translation system from a single SMT 
engine. In this method, a sequence of weak trans-
lation systems is generated from a baseline sys-
tem in an iterative manner. In each iteration, a 
new weak translation system is learned, focusing 
more on the sentences that are relatively poorly 
translated by the previous weak translation sys-
tem. Finally, a strong translation system is built 
from the ensemble of the weak translation sys-
tems. 
Our experiments are conducted on Chinese-to-
English translation in three state-of-the-art SMT 
systems, including a phrase-based system, a hier-
archical phrase-based system and a syntax-based 
739
Input:   a model u, a sequence of (training) samples {(f1, r1), ..., (fm, rm)} where fi is the 
i-th source sentence, and ri is the set of reference translations for fi. 
Output: a new translation system 
Initialize: D1(i) = 1 / m for all i = 1, ..., m 
For t = 1, ..., T 
1. Train a translation system u(?*t) on {(fi, ri)} using distribution Dt 
2. Calculate the error rate t? of u(?*t) on {(fi, ri)} 
3. Set 
1 1ln( )
2
t
t
t
?? ?
+=                                                         (3)
4. Update weights 
1
( )( )
t il
t
t
t
D i eD i
Z
? ?
+ =                                                    (4)
            where li is the loss on the i-th training sample, and Zt is the normalization factor. 
Output the final system:  
v(u(?*1), ..., u (?*T)) 
Figure 1: Boosting-based System Combination 
system. All the systems are evaluated on three 
NIST MT evaluation test sets. Experimental re-
sults show that our method leads to significant 
improvements in translation accuracy over the 
baseline systems. 
2 Background 
Given a source string f, the goal of SMT is to 
find a target string e* by the following equation. 
* arg max(Pr( | ))
e
e e f=                (1) 
where Pr( | )e f is the probability that e is the 
translation of the given source string f. To model 
the posterior probability Pr( | )e f , most of the 
state-of-the-art SMT systems utilize the log-
linear model proposed by Och and Ney (2002), 
as follows, 
1
' 1
exp( ( , ))
Pr( | )
exp( ( , '))
M
m m
m
M
m m
e m
h f e
e f
h f e
?
?
=
=
?= ?
?
? ?      (2) 
where {hm( f, e ) | m = 1, ..., M} is a set of fea-
tures, and ?m is the feature weight corresponding 
to the m-th feature. hm( f, e ) can be regarded as a 
function that maps every pair of source string f 
and target string e into a non-negative value, and 
?m can be viewed as the contribution of hm( f, e ) 
to the overall score Pr( | )e f . 
In this paper, u denotes a log-linear model that 
has M fixed features {h1( f ,e ), ..., hM( f ,e )}, ? = 
{?1, ..., ?M} denotes the M parameters of u, and 
u(?) denotes a SMT system based on u with pa-
rameters ?. Generally, ? is trained on a training 
data set1 to obtain an optimized weight vector ?* 
and consequently an optimized system u(?*). 
3 Boosting-based System Combination 
for Single Translation Engine  
Suppose that there are T available SMT systems 
{u1(?*1), ..., uT(?*T)}, the task of system combina-
tion is to build a new translation system 
v(u1(?*1), ..., uT(?*T)) from {u1(?*1), ..., uT(?*T)}. 
Here v(u1(?*1), ..., uT(?*T)) denotes the combina-
tion system which combines translations from the 
ensemble of the output of each ui(?*i). We call 
ui(?*i) a member system of v(u1(?*1), ..., uT(?*T)). 
As discussed in Section 1, the diversity among 
the outputs of member systems is an important 
factor to the success of system combination. To 
obtain diversified member systems, traditional 
methods concentrate more on using structurally 
different member systems, that is u1? u2 ?...? 
uT. However, this constraint condition cannot be 
satisfied when multiple translation engines are 
not available.  
In this paper, we argue that the diversified 
member systems can also be generated from a 
single engine u(?*) by adjusting the weight vector 
?* in a principled way. In this work, we assume 
that u1 = u2 =...= uT  = u. Our goal is to find a se-
ries of ?*i and build a combined system from 
{u(?*i)}. To achieve this goal, we propose a 
                                                 
1 The data set used for weight training is generally called 
development set or tuning set in the SMT field. In this paper, 
we use the term training set to emphasize the training of 
log-linear model. 
740
boosting-based system combination method (Fig-
ure 1). 
Like other boosting algorithms, such as 
AdaBoost (Freund and Schapire, 1997; Schapire, 
2001), the basic idea of this method is to use 
weak systems (member systems) to form a strong 
system (combined system) by repeatedly calling 
weak system trainer on different distributions 
over the training samples. However, since most 
of the boosting algorithms are designed for the 
classification problem that is very different from 
the translation problem in natural language proc-
essing, several key components have to be redes-
igned when boosting is adapted to SMT system 
combination. 
3.1 Training 
In this work, Minimum Error Rate Training 
(MERT) proposed by Och (2003) is used to es-
timate feature weights ? over a series of training 
samples. As in other state-of-the-art SMT sys-
tems, BLEU is selected as the accuracy measure 
to define the error function used in MERT. Since 
the weights of training samples are not taken into 
account in BLEU2, we modify the original defi-
nition of BLEU to make it sensitive to the distri-
bution Dt(i) over the training samples. The modi-
fied version of BLEU is called weighted BLEU 
(WBLEU) in this paper. 
Let E = e1 ... em be the translations produced 
by the system, R = r1 ... rm be the reference trans-
lations where ri = {ri1, ..., riN}, and Dt(i) be the 
weight of the i-th training sample (fi, ri). The 
weighted BLEU metric has the following form: 
{ }
( )
11 1
11
1/ 4
m
4 1 1
m
1
1
  WBLEU( , )
( ) min | ( ) |
exp 1 max 1,
( ) | ( ) |
( ) ( ) ( )
     (5)
( ) ( )
m
ijti j N
m
iti
N
i ijt n ni j
in t ni
E R
D i g r
D i g e
D i g e g r
D i g e
= ? ?
=
= =
= =
? ?? ?? ?? ?= ? ?? ?? ?? ?? ?? ?? ?
? ?? ?? ?? ?? ?
?
?
?? ?
I U
 
where ( )ng s  is the multi-set of all n-grams in a 
string s. In this definition, n-grams in ei and {rij} 
are weighted by Dt(i). If the i-th training sample 
has a larger weight, the corresponding n-grams 
will have more contributions to the overall score 
WBLEU( , )E R . As a result, the i-th training 
sample gains more importance in MERT. Obvi-
                                                 
2 In this paper, we use the NIST definition of BLEU where 
the effective reference length is the length of the shortest 
reference translation. 
ously the original BLEU is just a special case of 
WBLEU when all the training samples are 
equally weighted. 
As the weighted BLEU is used to measure the 
translation accuracy on the training set, the error 
rate is defined to be: 
1 WBLEU( , )t E R? = ?               (6) 
3.2 Re-weighting 
Another key point is the maintaining of the dis-
tribution Dt(i) over the training set. Initially all 
the weights of training samples are set equally. 
On each round, we increase the weights of the 
samples that are relatively poorly translated by 
the current weak system so that the MERT-based 
trainer can focus on the hard samples in next 
round. The update rule is given in Equation 4 
with two parameters t?  and li in it. 
t?  can be regarded as a measure of the im-
portance that the t-th weak system gains in boost-
ing. The definition of t?  guarantees that t?  al-
ways has a positive value3. A main effect of t?  
is to scale the weight updating (e.g. a larger t?  
means a greater update). 
li is the loss on the i-th sample. For each i, let 
{ei1, ..., ein} be the n-best translation candidates 
produced by the system. The loss function is de-
fined to be: 
*
1
1BLEU( , ) BLEU( , )ki i i ij i
j
l e e
k =
= ? ?r r  (7) 
where BLEU(eij, ri) is the smoothed sentence-level 
BLEU score (Liang et al, 2006) of the transla-
tion e with respect to the reference translations ri, 
and ei* is the oracle translation which is selected 
from {ei1, ..., ein} in terms of BLEU(eij, ri). li can 
be viewed as a measure of the average cost that 
we guess the top-k translation candidates instead 
of the oracle translation. The value of li counts 
for the magnitude of weight update, that is, a lar-
ger li means a larger weight update on Dt(i). The 
definition of the loss function here is similar to 
the one used in (Chiang et al, 2008) where only 
the top-1 translation candidate (i.e. k = 1) is 
taken into account. 
3.3 System Combination Scheme 
In the last step of our method, a strong transla-
tion system v(u(?*1), ..., u(?*T)) is built from the 
                                                 
3 Note that the definition of t?  here is different from that in 
the original AdaBoost algorithm (Freund and Schapire, 
1997; Schapire, 2001) where t?  is a negative number when 
0.5t? > . 
741
ensemble of member systems {u(?*1), ..., u(?*T)}. 
In this work, a sentence-level combination 
method is used to select the best translation from 
the pool of the n-best outputs of all the member 
systems.  
Let H(u(?*t)) (or Ht for short) be the set of the 
n-best translation candidates produced by the t-th 
member system u(?*t), and H(v) be the union set 
of all Ht (i.e. ( ) tH v H=U ). The final translation 
is generated from H(v) based on the following 
scoring function: 
*
1
( )
arg max ( ) ( , ( ))T t tt
e H v
e e e H v? ? ?=?= ? +?    (8) 
where ( )t e?  is the log-scaled model score of e in 
the t-th member system, and t?  is the corre-
sponding feature weight. It should be noted that 
ie H?  may not exist in any 'i iH ? . In this case, 
we can still calculate the model score of e in any 
other member systems, since all the member sys-
tems are based on the same model and share the 
same feature space. ( , ( ))e H v?  is a consensus-
based scoring function which has been success-
fully adopted in SMT system combination (Duan 
et al, 2009; Hildebrand and Vogel, 2008; Li et 
al., 2009). The computation of ( , ( ))e H v?  is 
based on a linear combination of a set of n-gram 
consensuses-based features.  
( , ( )) ( , ( ))n n
n
e H v h e H v? ? + += ? +?  
( , ( ))n n
n
h e H v? ? ???            (9) 
For each order of n-gram, ( , ( ))nh e H v
+ and 
( , ( ))nh e H v
?  are defined to measure the n-gram 
agreement and disagreement between e and other 
translation candidates in H(v), respectively. n? +  
and n? ? are the feature weights corresponding to 
( , ( ))nh e H v
+ and ( , ( ))nh e H v
? . As ( , ( ))nh e H v
+ and 
( , ( ))nh e H v
?  used in our work are exactly the 
same as the features used in (Duan et al, 2009) 
and similar to the features used in (Hildebrand 
and Vogel, 2008; Li et al, 2009), we do not pre-
sent the detailed description of them in this paper. 
If p orders of n-gram are used in computing 
( , ( ))e H v? , the total number of features in the 
system combination will be 2T p+ ? (T model-
score-based features defined in Equation 8 and 
2 p?  consensus-based features defined in Equa-
tion 9). Since all these features are combined 
linearly, we use MERT to optimize them for the 
combination model. 
4 Optimization 
If implemented naively, the translation speed of 
the final translation system will be very slow. 
For a given input sentence, each member system 
has to encode it individually, and the translation 
speed is inversely proportional to the number of 
member systems generated by our method. For-
tunately, with the thought of computation, there 
are a number of optimizations that can make the 
system much more efficient in practice. 
A simple solution is to run member systems in 
parallel when translating a new sentence. Since 
all the member systems share the same data re-
sources, such as language model and translation 
table, we only need to keep one copy of the re-
quired resources in memory. The translation 
speed just depends on the computing power of 
parallel computation environment, such as the 
number of CPUs. 
Furthermore, we can use joint decoding tech-
niques to save the computation of the equivalent 
translation hypotheses among member systems. 
In joint decoding of member systems, the search 
space is structured as a translation hypergraph 
where the member systems can share their trans-
lation hypotheses. If more than one member sys-
tems share the same translation hypothesis, we 
just need to compute the corresponding feature 
values only once, instead of repeating the com-
putation in individual decoders. In our experi-
ments, we find that over 60% translation hy-
potheses can be shared among member systems 
when the number of member systems is over 4. 
This result indicates that promising speed im-
provement can be achieved by using the joint 
decoding and hypothesis sharing techniques. 
Another method to speed up the system is to 
accelerate n-gram language model with n-gram 
caching techniques. In this method, a n-gram 
cache is used to store the most frequently and 
recently accessed n-grams. When a new n-gram 
is accessed during decoding, the cache is 
checked first. If the required n-gram hits the 
cache, the corresponding n-gram probability is 
returned by the cached copy rather than re-
fetching the original data in language model. As 
the translation speed of SMT system depends 
heavily on the computation of n-gram language 
model, the acceleration of n-gram language 
model generally leads to substantial speed-up of 
SMT system. In our implementation, the n-gram 
caching in general brings us over 30% speed im-
provement of the system. 
742
5 Experiments  
Our experiments are conducted on Chinese-to-
English translation in three SMT systems. 
5.1 Baseline Systems 
The first SMT system is a phrase-based system 
with two reordering models including the maxi-
mum entropy-based lexicalized reordering model 
proposed by Xiong et al (2006) and the hierar-
chical phrase reordering model proposed by Gal-
ley and Manning (2008). In this system all 
phrase pairs are limited to have source length of 
at most 3, and the reordering limit is set to 8 by 
default4. 
The second SMT system is an in-house reim-
plementation of the Hiero system which is based 
on the hierarchical phrase-based model proposed 
by Chiang (2005).  
The third SMT system is a syntax-based sys-
tem based on the string-to-tree model (Galley et 
al., 2006; Marcu et al, 2006), where both the 
minimal GHKM and SPMT rules are extracted 
from the bilingual text, and the composed rules 
are generated by combining two or three minimal 
GHKM and SPMT rules. Synchronous binariza-
tion (Zhang et al, 2006; Xiao et al, 2009) is per-
formed on each translation rule for the CKY-
style decoding. 
In this work, baseline system refers to the sys-
tem produced by the boosting-based system 
combination when the number of iterations (i.e. 
T ) is set to 1. To obtain satisfactory baseline per-
formance, we train each SMT system for 5 times 
using MERT with different initial values of fea-
ture weights to generate a group of baseline can-
didates, and then select the best-performing one 
from this group as the final baseline system (i.e. 
the starting point in the boosting process) for the 
following experiments. 
5.2 Experimental Setup 
Our bilingual data consists of 140K sentence 
pairs in the FBIS data set5. GIZA++ is employed 
to perform the bi-directional word alignment be-
tween the source and target sentences, and the 
final word alignment is generated using the inter-
sect-diag-grow method. All the word-aligned 
bilingual sentence pairs are used to extract 
phrases and rules for the baseline systems. A 5-
gram language model is trained on the target-side 
                                                 
4 Our in-house experimental results show that this system 
performs slightly better than Moses on Chinese-to-English 
translation tasks. 
5 LDC catalog number: LDC2003E14 
of the bilingual data and the Xinhua portion of 
English Gigaword corpus. Berkeley Parser is 
used to generate the English parse trees for the 
rule extraction of the syntax-based system. The 
data set used for weight training in boosting-
based system combination comes from NIST 
MT03 evaluation set. To speed up MERT, all the 
sentences with more than 20 Chinese words are 
removed. The test sets are the NIST evaluation 
sets of MT04, MT05 and MT06. The translation 
quality is evaluated in terms of case-insensitive 
NIST version BLEU metric. Statistical signifi-
cant test is conducted using the bootstrap re-
sampling method proposed by Koehn (2004). 
Beam search and cube pruning (Huang and 
Chiang, 2007) are used to prune the search space 
in all the three baseline systems. By default, both 
of the beam size and the size of n-best list are set 
to 20. 
In the settings of boosting-based system com-
bination, the maximum number of iterations is 
set to 30, and k (in Equation 7) is set to 5. The n-
gram consensuses-based features (in Equation 9) 
used in system combination ranges from unigram 
to 4-gram. 
5.3 Evaluation of Translations 
First we investigate the effectiveness of the 
boosting-based system combination on the three 
systems.  
Figures 2-5 show the BLEU curves on the de-
velopment and test sets, where the X-axis is the 
iteration number, and the Y-axis is the BLEU 
score of the system generated by the boosting-
based system combination. The points at itera-
tion 1 stand for the performance of the baseline 
systems. We see, first of all, that all the three 
systems are improved during iterations on the 
development set. This trend also holds on the test 
sets. After 5, 7 and 8 iterations, relatively stable 
improvements are achieved by the phrase-based 
system, the Hiero system and the syntax-based 
system, respectively. The BLEU scores tend to 
converge to the stable values after 20 iterations 
for all the systems. Figures 2-5 also show that the 
boosting-based system combination seems to be 
more helpful to the phrase-based system than to 
the Hiero system and the syntax-based system. 
For the phrase-based system, it yields over 0.6 
BLEU point gains just after the 3rd iteration on 
all the data sets.  
Table 1 summarizes the evaluation results, 
where the BLEU scores at iteration 5, 10, 15, 20 
and 30 are reported for the comparison. We see 
that the boosting-based system method stably ac- 
743
 33
 34
 35
 36
 37
 38
 0  5  10  15  20  25  30
B
LE
U
4[
%
]
iteration number
BLEU on MT03 (dev.)
phrase-based
hiero
syntax-based
Figure 2: BLEU scores on the development set 
 33
 34
 35
 36
 37
 38
 0  5  10  15  20  25  30
B
LE
U
4[
%
]
iteration number
BLEU on MT04 (test)
phrase-based
hiero
syntax-based
Figure 3: BLEU scores on the test  set of MT04 
 32
 33
 34
 35
 36
 37
 0  5  10  15  20  25  30
B
LE
U
4[
%
]
iteration number
BLEU on MT05 (test)
phrase-based
hiero
syntax-based
Figure 4: BLEU scores on the test set of MT05 
 30
 31
 32
 33
 34
 35
 0  5  10  15  20  25  30
B
LE
U
4[
%
]
iteration number
BLEU on MT06 (test)
phrase-based
hiero
syntax-based
Figure 5: BLEU scores on the test set of MT06 
 
Phrase-based Hiero Syntax-based 
Dev. MT04 MT05 MT06 Dev. MT04 MT05 MT06 Dev. MT04 MT05 MT06 
Baseline 33.21 33.68 32.68 30.59 33.42 34.30 33.24 30.62 35.84 35.71 35.11 32.43 
Baseline+600best 33.32 33.93 32.84 30.76 33.48 34.46 33.39 30.75 35.95 35.88 35.23 32.58 
Boosting-5Iterations 33.95* 34.32* 33.33* 31.33* 33.73 34.48 33.44 30.83 36.03 35.92 35.27 33.09 
Boosting-10Iterations 34.14* 34.68* 33.42* 31.35* 33.75 34.65 33.75* 31.02 36.14 36.39* 35.47 33.15*
Boosting-15Iterations 33.99* 34.78* 33.46* 31.45* 34.03* 34.88* 33.98* 31.20* 36.36* 36.46* 35.53* 33.43*
Boosting-20Iterations 34.09* 35.11* 33.56* 31.45* 34.17* 35.00* 34.04* 31.29* 36.44* 36.79* 35.77* 33.36*
Boosting-30Iterations 34.12* 35.16* 33.76* 31.59* 34.05* 34.99* 34.05* 31.30* 36.52* 36.81* 35.71* 33.46*
Table 1: Summary of the results (BLEU4[%]) on the development and test sets. * = significantly better 
than baseline (p < 0.05). 
  
hieves significant BLEU improvements after 15 
iterations, and the highest BLEU scores are gen-
erally yielded after 20 iterations.  
Also as shown in Table 1, over 0.7 BLEU 
point gains are obtained on the phrase-based sys-
tem after 10 iterations. The largest BLEU im-
provement on the phrase-based system is over 1 
BLEU point in most cases. These results reflect 
that our method is relatively more effective for 
the phrase-based system than for the other two 
systems, and thus confirms the fact we observed 
in Figures 2-5. 
We also investigate the impact of n-best list 
size on the performance of baseline systems. For 
the comparison, we show the performance of the 
baseline systems with the n-best list size of 600 
(Baseline+600best in Table 1) which equals to 
the maximum number of translation candidates 
accessed in the final combination system (combi- 
ne 30 member systems, i.e. Boosing-30Iterations). 
744
 15
 20
 25
 30
 35
 40
 0  5  10  15  20  25  30
D
iv
er
si
ty
 (T
E
R
[%
])
iteration number
Diversity on MT03 (dev.)
phrase-based
hiero
syntax-based
Figure 6: Diversity on the development set 
 10
 15
 20
 25
 30
 35
 0  5  10  15  20  25  30
D
iv
er
si
ty
 (T
E
R
[%
])
iteration number
Diversity on MT04 (test)
phrase-based
hiero
syntax-based
Figure 7: Diversity on the test set of MT04 
 15
 20
 25
 30
 35
 0  5  10  15  20  25  30
D
iv
er
si
ty
 (T
E
R
[%
])
iteration number
Diversity on MT05 (test)
phrase-based
hiero
syntax-based
Figure 8: Diversity on the test set of MT05 
 15
 20
 25
 30
 35
 40
 0  5  10  15  20  25  30
D
iv
er
si
ty
 (T
E
R
[%
])
iteration number
Diversity on MT06 (test)
phrase-based
hiero
syntax-based
Figure 9: Diversity on the test set of MT06 
 
As shown in Table 1, Baseline+600best obtains 
stable improvements over Baseline. It indicates 
that the access to larger n-best lists is helpful to 
improve the performance of baseline systems. 
However, the improvements achieved by Base-
line+600best are modest compared to the im-
provements achieved by Boosting-30Iterations. 
These results indicate that the SMT systems can 
benefit more from the diversified outputs of 
member systems rather than from larger n-best 
lists produced by a single system. 
5.4 Diversity among Member Systems 
We also study the change of diversity among the 
outputs of member systems during iterations. 
The diversity is measured in terms of the Trans-
lation Error Rate (TER) metric proposed in 
(Snover et al, 2006). A higher TER score means 
that more edit operations are performed if we 
transform one translation output into another 
translation output, and thus reflects a larger di-
versity between the two outputs. In this work, the 
TER score for a given group of member systems 
is calculated by averaging the TER scores be-
tween the outputs of each pair of member sys-
tems in this group. 
Figures 6-9 show the curves of diversity on 
the development and test sets, where the X-axis 
is the iteration number, and the Y-axis is the di-
versity. The points at iteration 1 stand for the 
diversities of baseline systems. In this work, the 
baseline?s diversity is the TER score of the group 
of baseline candidates that are generated in ad-
vance (Section 5.1). 
We see that the diversities of all the systems 
increase during iterations in most cases, though a 
few drops occur at a few points. It indicates that 
our method is very effective to generate diversi-
fied member systems. In addition, the diversities 
of baseline systems (iteration 1) are much lower 
745
than those of the systems generated by boosting 
(iterations 2-30). Together with the results shown 
in Figures 2-5, it confirms our motivation that 
the diversified translation outputs can lead to 
performance improvements over the baseline 
systems. 
Also as shown in Figures 6-9, the diversity of 
the Hiero system is much lower than that of the 
phrase-based and syntax-based systems at each 
individual setting of iteration number. This inter-
esting finding supports the observation that the 
performance of the Hiero system is relatively 
more stable than the other two systems as shown 
in Figures 2-5. The relative lack of diversity in 
the Hiero system might be due to the spurious 
ambiguity in Hiero derivations which generally 
results in very few different translations in trans-
lation outputs (Chiang, 2007). 
5.5 Evaluation of Oracle Translations 
In this set of experiments, we evaluate the oracle 
performance on the n-best lists of the baseline 
systems and the combined systems generated by 
boosting-based system combination. Our primary 
goal here is to study the impact of our method on 
the upper-bound performance.  
Table 2 shows the results, where Base-
line+600best stands for the top-600 translation 
candidates generated by the baseline systems, 
and Boosting-30iterations stands for the ensem-
ble of 30 member systems? top-20 translation 
candidates. As expected, the oracle performance 
of Boosting-30Iterations is significantly higher 
than that of Baseline+600best. This result indi-
cates that our method can provide much ?better? 
translation candidates for system combination 
than enlarging the size of n-best list naively. It 
also gives us a rational explanation for the sig-
nificant improvements achieved by our method 
as shown in Section 5.3. 
 
Data 
Set 
Method Phrase-
based 
Hiero Syntax-
based 
Baseline+600best 46.36 46.51 46.92 Dev. 
Boosting-30Iterations 47.78* 47.44* 48.70* 
Baseline+600best 43.94 44.52 46.88 MT04 
Boosting-30Iterations 45.97* 45.47* 49.40* 
Baseline+600best 42.32 42.47 45.21 MT05 
Boosting-30Iterations 44.82* 43.44* 47.02* 
Baseline+600best 39.47 39.39 40.52 MT06 
Boosting-30Iterations 41.51* 40.10* 41.88* 
Table 2: Oracle performance of various systems. 
* = significantly better than baseline (p < 0.05). 
6 Related Work 
Boosting is a machine learning (ML) method that 
has been well studied in the ML community 
(Freund, 1995; Freund and Schapire, 1997; 
Collins et al, 2002; Rudin et al, 2007), and has 
been successfully adopted in natural language 
processing (NLP) applications, such as document 
classification (Schapire and Singer, 2000) and 
named entity classification (Collins and Singer, 
1999). However, most of the previous work did 
not study the issue of how to improve a single 
SMT engine using boosting algorithms. To our 
knowledge, the only work addressing this issue is 
(Lagarda and Casacuberta, 2008) in which the 
boosting algorithm was adopted in phrase-based 
SMT. However, Lagarda and Casacuberta 
(2008)?s method calculated errors over the 
phrases that were chosen by phrase-based sys-
tems, and could not be applied to many other 
SMT systems, such as hierarchical phrase-based 
systems and syntax-based systems. Differing 
from Lagarda and Casacuberta?s work, we are 
concerned more with proposing a general 
framework which can work with most of the cur-
rent SMT models and empirically demonstrating 
its effectiveness on various SMT systems. 
There are also some other studies on building 
diverse translation systems from a single transla-
tion engine for system combination. The first 
attempt is (Macherey and Och, 2007). They em-
pirically showed that diverse translation systems 
could be generated by changing parameters at 
early-stages of the training procedure. Following 
Macherey and Och (2007)?s work, Duan et al 
(2009) proposed a feature subspace method to 
build a group of translation systems from various 
different sub-models of an existing SMT system. 
However, Duan et al (2009)?s method relied on 
the heuristics used in feature sub-space selection. 
For example, they used the remove-one-feature 
strategy and varied the order of n-gram language 
model to obtain a satisfactory group of diverse 
systems. Compared to Duan et al (2009)?s 
method, a main advantage of our method is that 
it can be applied to most of the SMT systems 
without designing any heuristics to adapt it to the 
specified systems. 
7 Discussion and Future Work 
Actually the method presented in this paper is 
doing something rather similar to Minimum 
Bayes Risk (MBR) methods. A main difference 
lies in that the consensus-based combination 
method here does not model the posterior prob-
ability of each hypothesis (i.e. all the hypotheses 
are assigned an equal posterior probability when 
we calculate the consensus-based features). 
746
Greater improvements are expected if MBR 
methods are used and consensus-based combina-
tion techniques smooth over noise in the MERT 
pipeline. 
In this work, we use a sentence-level system 
combination method to generate final transla-
tions. It is worth studying other more sophisti-
cated alternatives, such as word-level and 
phrase-level system combination, to further im-
prove the system performance. 
Another issue is how to determine an appro-
priate number of iterations for boosting-based 
system combination. It is especially important 
when our method is applied in the real-world 
applications. Our empirical study shows that the 
stable and satisfactory improvements can be 
achieved after 6-8 iterations, while the largest 
improvements can be achieved after 20 iterations. 
In our future work, we will study in-depth prin-
cipled ways to determine the appropriate number 
of iterations for boosting-based system combina-
tion. 
8 Conclusions 
We have proposed a boosting-based system com-
bination method to address the issue of building 
a strong translation system from a group of weak 
translation systems generated from a single SMT 
engine. We apply our method to three state-of-
the-art SMT systems, and conduct experiments 
on three NIST Chinese-to-English MT evalua-
tions test sets. The experimental results show that 
our method is very effective to improve the 
translation accuracy of the SMT systems. 
Acknowledgements 
This work was supported in part by the National 
Science Foundation of China (60873091) and the 
Fundamental Research Funds for the Central 
Universities (N090604008). The authors would 
like to thank the anonymous reviewers for their 
pertinent comments, Tongran Liu, Chunliang 
Zhang and Shujie Yao for their valuable sugges-
tions for improving this paper, and Tianning Li 
and Rushan Chen for developing parts of the 
baseline systems. 
References  
David Chiang. 2005. A hierarchical phrase-based 
model for statistical machine translation. In Proc. 
of ACL 2005, Ann Arbor, Michigan, pages 263-
270. 
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201-228. 
David Chiang, Yuval Marton and Philip Resnik. 2008. 
Online Large-Margin Training of Syntactic and 
Structural Translation Features. In Proc. of 
EMNLP 2008, Honolulu, pages 224-233. 
Michael Collins and Yoram Singer. 1999. Unsuper-
vised Models for Named Entity Classification. In 
Proc. of EMNLP/VLC 1999, pages 100-110. 
Michael Collins, Robert Schapire and Yoram Singer. 
2002. Logistic Regression, AdaBoost and Bregman 
Distances. Machine Learning, 48(3): 253-285. 
Brooke Cowan, Ivona Ku?erov? and Michael Collins. 
2006. A discriminative model for tree-to-tree trans-
lation. In Proc. of EMNLP 2006, pages 232-241. 
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency 
insertion grammars. In Proc. of ACL 2005, Ann 
Arbor, Michigan, pages 541-548. 
Nan Duan, Mu Li, Tong Xiao and Ming Zhou. 2009. 
The Feature Subspace Method for SMT System 
Combination. In Proc. of EMNLP 2009, pages 
1096-1104. 
Jason Eisner. 2003. Learning non-isomorphic tree 
mappings for machine translation. In Proc. of ACL 
2003, pages 205-208. 
Yoav Freund. 1995. Boosting a weak learning algo-
rithm by majority. Information and Computation, 
121(2): 256-285. 
Yoav Freund and Robert Schapire. 1997. A decision-
theoretic generalization of on-line learning and an 
application to boosting. Journal of Computer and 
System Sciences, 55(1):119-139. 
Michel Galley, Jonathan Graehl, Kevin Knight, 
Daniel Marcu, Steve DeNeefe, Wei Wang and 
Ignacio Thayer. 2006. Scalable inferences and 
training of context-rich syntax translation models. 
In Proc. of ACL 2006, Sydney, Australia, pages 
961-968. 
Michel Galley and Christopher D. Manning. 2008. A 
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proc. of EMNLP 2008, Hawaii, 
pages 848-856. 
Almut Silja Hildebrand and Stephan Vogel. 2008. 
Combination of machine translation systems via 
hypothesis selection from combined n-best lists. In 
Proc. of the 8th AMTA conference, pages 254-261. 
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language 
models. In Proc. of ACL 2007, Prague, Czech Re-
public, pages 144-151. 
747
Philipp Koehn, Franz Och and Daniel Marcu. 2003. 
Statistical Phrase-Based Translation. In Proc. of 
HLT-NAACL 2003, Edmonton, USA, pages 48-54. 
Philipp Koehn. 2004. Statistical Significance Tests for 
Machine Translation Evaluation. In Proc. of 
EMNLP 2004, Barcelona, Spain, pages 388-395. 
Antonio Lagarda and Francisco Casacuberta. 2008. 
Applying Boosting to Statistical Machine Transla-
tion. In Proc. of the 12th EAMT conference, pages 
88-96. 
Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li and 
Ming Zhou. 2009. Collaborative Decoding: Partial 
Hypothesis Re-Ranking Using Translation Consen-
sus between Decoders. In Proc. of ACL-IJCNLP 
2009, Singapore, pages 585-592. 
Percy Liang, Alexandre Bouchard-C?t?, Dan Klein 
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In Proc. of 
COLING/ACL 2006, pages 104-111. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. In Proc. of ACL 2006, pages 609-616. 
Wolfgang Macherey and Franz Och. 2007. An Em-
pirical Study on Computing Consensus Transla-
tions from Multiple Machine Translation Systems. 
In Proc. of EMNLP 2007, pages 986-995. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi and 
Kevin Knight. 2006. SPMT: Statistical machine 
translation with syntactified target language 
phrases. In Proc. of EMNLP 2006, Sydney, Aus-
tralia, pages 44-52. 
Evgeny Matusov, Nicola Ueffing and Hermann Ney. 
2006. Computing consensus translation from mul-
tiple machine translation systems using enhanced 
hypotheses alignment. In Proc. of EACL 2006, 
pages 33-40. 
Franz Och and Hermann Ney. 2002. Discriminative 
Training and Maximum Entropy Models for Statis-
tical Machine Translation. In Proc. of ACL 2002, 
Philadelphia, pages 295-302. 
Franz Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proc. of ACL 
2003, Japan, pages 160-167. 
Antti-Veikko Rosti, Spyros Matsoukas and Richard 
Schwartz. 2007. Improved Word-Level System 
Combination for Machine Translation. In Proc. of 
ACL 2007, pages 312-319. 
Cynthia Rudin, Robert Schapire and Ingrid Daube-
chies. 2007. Analysis of boosting algorithms using 
the smooth margin function.  The Annals of Statis-
tics, 35(6): 2723-2768. 
Robert Schapire and Yoram Singer. 2000. BoosTexter: 
A boosting-based system for text categorization. 
Machine Learning, 39(2/3):135-168. 
Robert Schapire. The boosting approach to machine 
learning: an overview. 2001. In Proc. of MSRI 
Workshop on Nonlinear Estimation and Classifica-
tion, Berkeley, CA, USA, pages 1-23. 
Matthew Snover, Bonnie Dorr, Richard Schwartz, 
Linnea Micciulla and John Makhoul. 2006. A 
Study of Translation Edit Rate with Targeted Hu-
man Annotation. In Proc. of the 7th AMTA confer-
ence, pages 223-231. 
Tong Xiao, Mu Li, Dongdong Zhang, Jingbo Zhu and 
Ming Zhou. 2009. Better Synchronous Binarization 
for Machine Translation. In Proc. of EMNLP 2009, 
Singapore, pages 362-370. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for 
Statistical Machine Translation. In Proc. of ACL 
2006, Sydney, pages 521-528. 
Hao Zhang, Liang Huang, Daniel Gildea and Kevin 
Knight. 2006. Synchronous Binarization for Ma-
chine Translation. In Proc. of HLT-NAACL 2006, 
New York, USA, pages 256- 263. 
748
A Multi-stage Clustering Framework for Chinese Personal 
Name Disambiguation 
    Huizhen Wang, Haibo Ding, Yingchao Shi, Ji Ma,  Xiao Zhou, Jingbo Zhu 
Natural Language Processing Laboratory, 
Northeatern University 
 Shenyang, Liaoning, China 
{wanghuizhen|zhujingbo@mail.neu.edu.cn 
{dinghb|shiyc|maji}@mail.neu.edu.cn 
 
Abstract 
This paper presents our systems for the 
participation of Chinese Personal Name 
Disambiguation task in the CIPS-
SIGHAN 2010. We submitted two dif-
ferent systems for this task, and both of 
them all achieve the best performance. 
This paper introduces the multi-stage 
clustering framework and some key 
techniques used in our systems, and 
demonstrates experimental results on 
evaluation data. Finally, we further dis-
cuss some interesting issues found dur-
ing the development of the system. 
1 Introduction 
Personal name disambiguation (PND) is very 
important for web search and potentially other 
natural language applications such as question 
answering. CIPS-SIGHAN bakeoffs provide a 
platform to evaluate the effectiveness of various 
methods on Chinese PND task.  
Different from English PND, word segmenta-
tion techniques are needed for Chinese PND 
tasks. In practice, person names are highly am-
biguous because different people may have the 
same name, and the same name can be written 
in different ways. It?s an n-to-n mapping of per-
son names to the specific people. There are two 
main challenges on Chinese PND: the first one 
is how to correctly recognize personal names in 
the text, and the other is how to distinguish dif-
ferent persons who have the same name. For 
address these challenges, we designed a rule-
based combination technique to improve NER 
performance and propose a multi-stage cluster-
ing framework for Chinese PND. We partici-
pated in the bakeoff of the Chinese PND task, 
on the test set and the diagnosis test set, our two 
systems are ranked at the 1st and 2nd position. 
The rest of this paper is organized as follows. 
In Section 2, we first give the key features and 
techniques used in our two systems. In Section 
3, experimental results on the evaluation test 
data demonstrated that our methods are effec-
tive to disambiguate the personal name, and 
discussions on some issues we found during the 
development of the system are given. In Section 
4, we conclude our work. 
2 System Description 
In this section, we describe the framework of 
our systems in more detail, involving data pre-
processing, discard-class document identifica-
tion, feature definition, clustering algorithms, 
and sub-system combination. 
2.1 Data Preprocessing 
There are around 100-300 news articles per per-
sonal name in the evaluation corpus. Each arti-
cle is stored in the form of XML and encoded in 
UTF-8. At first, each news article should be 
preprocessed as follows: 
 Use a publicly available Chinese encoding 
Converter tool to convert each news article 
from UTF-8 coding into GB1; 
 Remove all XML tags; 
 Process Chinese word segmentation, part-
of-speech (POS) tagging and name entity 
recognition (NER); 
The performance of word segmentation and 
NER tools generally affect the effectiveness of 
our Chinese PND systems. During system de-
                                                 
1
 http://www.mandarintools.com/ 
veloping process, we found that the publicly 
available NER systems obtain unsatisfactory 
performance on evaluation data. To address this 
challenge, we propose a new rule-based combi-
nation technique to improve NER performance. 
In our combination framework, two different 
NER systems are utilized, including a CRF-
based NER system and our laboratory?s NER 
system (Yao et al,2002). The latter was imple-
mented based on the maximum matching prin-
ciple and some linguistic post-preprocessing 
rules. Since both two NER systems adopt dif-
ferent technical frameworks, it is possible to 
achieve a better performance by means of sys-
tem combination techniques.  
The basic idea of our combination method is 
to first simply combine the results produced by 
both NER systems, and further utilize some 
heuristic post-processing rules to refine NE 
identification results. To achieve this goal, we 
first investigate error types caused by both NER 
systems, and design some post-preprocessing 
rules to correct errors or select the appropriate 
NER results from disagreements. Notice that 
such rules are learned from sample data (i.e., 
training set), not from test set. Experimental 
results demonstrate satisfactory NER perform-
ance by introducing these heuristic refinement 
rules as follows:  
 Conjunction Rules. Two NEs separated 
by a conjunction (such as ???,???,???, 
??? ) belong to the same type, e.g., ???
/adj.?/???/person?. Such a conjunc-
tion rule can help NER systems make a 
consistent prediction on both NEs, e.g., ??
?/person? and ???/person?.  
 Professional Title Rules. Professional title 
words such as ???? are strong indicators 
of person names, e.g., ???/???. Such a 
rule can be written in the form of ?profes-
sional_title+person_name?.  
 Suffix Rules. If an identified person name 
is followed by a suffix of another type of 
named entities such as location, it is not a 
true person name, for example, ?????
???/person ?/?/???. Since ??? is 
a suffix of a location name. ??????
??/person ?/location-suffix? should be 
revised to be a new location name, namely 
?????????/location?. 
 Foreign Person Name Rules. Two identi-
fied person names connected by a dot are 
merged into a single foreign person name, 
e.g., ??/./???? => ??.???? 
 Chinese Surname Rules. Surnames are 
very important for Chinese person name 
identification. However, some common 
surnames can be single words depending 
upon the context, for example, the Chinese 
word ??? can be either a surname or a 
quantifier. To tackle this problem, some 
post-processing rules for ??, ?, ?, ?, 
?? are designed in our system. 
 Query-Dependent Rules. Given a query 
person name A, if the string AB occurring 
in the current document has been identified 
as a single person name many times in 
other documents, our system would tend to 
segment AB as a single person name rather 
than as A/B. For example, if ????? was 
identified as a true person name more than 
one time in other documents, in such a case, 
???/??/?/??/?=> ???/???
/person??/? 
Incorporating these above post-processing 
rules, our NER system based on heuristic post-
processing rules shows 98.89% precision of 
NER on training set.  
2.2 Discard-Class Document Identification 
Seen from evaluation data, there are a lot of 
documents belonging to a specific class, re-
ferred to as discard-class. In the discard-class, 
the query person name occurring in the docu-
ment is not a true person name. For example, a 
query word ???? is a famous ocean name not 
a person name in the sentence ???????
???????????????. In such a 
case, the corresponding document is considered 
as discard-class. Along this line, actually the 
discard-class document identification is very 
simple task. If a document does not contain a 
true person name that is the same as the query 
or contains the query, it is a discard-class 
document.  
2.3 Feature Definition 
To identify different types of person name and 
for the PND purpose, some effective binary fea-
tures are defined to construct the document rep-
resentation as feature vectors as follows: 
 Personal attributes: involving profes-
sional title, affiliation, location, co-
occurrence person name and organization 
related to the given query.   
 NE-type Features: collecting all NEs oc-
curring in the context of the given query. 
There are two kinds of NE-type features 
used in our systems, local features and 
global features. The global features are de-
fined with respect to the whole document 
while the local features are extracted only 
from the two or three adjacent sentences 
for the given query.  
 BOW-type features: constructing the con-
text feature vector based on bag-of-word 
model. Similarly, there are local and global 
BOW-type features with respect to the con-
text considered.  
2.4 A Multi-stage Clustering Framework  
Seen from the training set, 36% of person 
names indicate journalists, 10% are sportsmen, 
and the remaining are common person names. 
Based on such observations, it is necessary to 
utilize different methodology to PND on differ-
ent types of person names, for example, because 
the most effective information to distinguish 
different journalists are the reports? location and 
colleagues, instead of the whole document con-
tent. To achieve a satisfactory PND perform-
ance, in our system we design three different 
modules for analyzing journalist, sportsman and 
common person name, respectively.  
2.4.1 PND on the Journalist Class 
In our system, some regular expressions are 
designed to determine whether a person name is 
a journalist or not. For example: 
 ??? /ni */ns */t */t ?? |? /n (/w .* 
[?/w */ni ?/w ]* query name/nh .*)/w 
 (/w .*query name/nh .*)/w 
 [*/nh]* query name/nh [*/nh] 
 ? ? | ? ? /n [*/nh]* query name/nh 
[*/nh]* 
To disambiguate on the journalist class, our 
system utilizes a rule-based clustering technique 
distinguish different journalists. For each 
document containing the query person name as 
journalists, we first extract the organization and 
the location occurring in the local context of the 
query. Two such documents can be put into the 
same cluster if they contain the same organiza-
tion or location names, otherwise not. In our 
system, a location dictionary containing prov-
ince-city information extracted from Wikipedia 
is used to identify location name. For example: 
??? (?? ?? ?? ?? ?), ??(??
? ?? ??? ?? ???). Based on this 
dictionary, it is very easy to map a city to its 
corresponding province.  
2.4.2 PND on the Sportsman Class 
Like done in PND on the journalist class, we 
also use rule-based clustering techniques for 
disambiguating sportsman class. The major dif-
ference is to utilize topic features for PND on 
the sportsman class. If the topic of the given 
document is sports, this document can be con-
sidered as sportsman class. The key is to how to 
automatically identify the topic of the document 
containing the query. To address this challenge, 
we adopt a domain knowledge based technique 
for document topic identification. The basic 
idea is to utilize a domain knowledge dictionary 
NEUKD developed by our lab, which contains 
more than 600,000 domain associated terms and 
the corresponding domain features. Some do-
main associated terms defined in NEUKD are 
shown in Table 1.  
 
Domain associated term Domain feature concept 
???(football team) Football, Sports 
???? 
(cycling team) Traffic, Sports, cycling 
???? 
(Chinese chess) Sports, Chinese chess 
??(white side) Sports, the game of go 
????? 
(Chicago bulls) Sports, basketball 
 
Table 1: Six examples defined in the NEUKD 
 
In the domain knowledge based topic identi-
fication algorithm, all domain associated terms 
occurring in the given document are first 
mapped into domain features such as football, 
basketball or cycling. The most frequent do-
main feature is considered as the most likely 
topic. See Zhu and Chen (2005) for details. 
Two documents with the same topic can be 
grouped into the same cluster.  
 
 
Table 2: Examples of PND on Sportsman Class 
 
2.4.3 Multi-Stage Clustering Framework 
We proposed a multi-stage clustering frame-
work for PND on common person name class, 
as shown In Figure 1.  
In the multi-stage clustering framework, the 
first-stage is to adopt strict rule-based hard clus-
tering algorithm using the feature set of per-
sonal attributes. The second-stage is to imple-
ment constrained hierarchical agglomerative 
clustering using NE-type local features. The 
third-stage is to design hierarchical agglomera-
tive clustering using BOW-type global features. 
By combining those above techniques, we sub-
mitted the first system named NEU_1. 
2.4.4 The second system 
Besides, we also submitted another PND system 
named NEU_2 by using the single-link hierar-
chical agglomerative clustering algorithm in 
which the distance of two clusters is the cosine 
similarity of their most similar members (Ma-
saki et al, 2009, Duda et al, 2004). The differ-
ence between our two submission systems 
NEU_1 and NEU_2 is the feature weighting 
method. The motivation of feature weighting 
method used in NEU_2 is to assume that words 
surrounding the query person name in the given 
document are more important features than 
those far away from it, and person name and 
location names occurring in the context are 
more discriminative features than common 
words for PND purpose. Along this line, in the 
feature weighting scheme used in NEU_2, for 
each feature extracted from the sentence con-
taining the query person name, the weight of a 
word-type feature with the POS of ?ns?, ?ni? 
or ?nh ? is assigned  as 3, Otherwise 1.5; For 
the features extracted from other sentences, the 
weight of a word with the POS of ?ns?or ?nh ? 
is set to be 2, the ones of ?ni? POS is set to 1.5, 
otherwise 1.0. 
 
Algorithm 1: Multi-stage Clustering Framework 
Input: a person name pn, and its related document 
set D={d1, d2, ?, dm} in which each document di 
contains the person name pn; 
Output: clustering results C={C1,C2, ?,Cn}, where 
CCi =?i
 and ?=? ji CC  
For each di?D do 
 Si = {s|pn?s, s?di}; 
ORGi={t|t?s, s?Si, POS(t)= ni}; 
PERi={t|t?s, s?Si, POS(t)=nh} ; 
Ldi = {t|t?s, s?Si }; //local feature set 
Gdi = {t|t?di}; //global feature set 
Ci = {di} ; 
End for 
Stage 1: Strict rules-based clustering 
 Begin 
 For each Ci ? C do 
If ??? ji ORGORG or 
2?? ji PERPER  
Then Ci = Ci ?Cj;  
ORGi = ORGi?ORGj ; 
PERi = PERi?PERj ; 
Remove Cj from C ; 
End for 
End  
Stage 2: Constrained hierarchical agglomerative 
clustering algorithm using local features 
Begin  
         Set each c ?C as an initial cluster; 
 do  
),(maxarg],[
,
ji
CCC
ji CCsimCC
ji ?
=  
),cos(max
),(max),(
,
,
yxjyix
jyix
ddCdCd
yxCdCdji
LL
ddsimCCsim
??
??
=
=
 
Ci = Ci ?Cj; 
Remove Cj from C ; 
        until  sim(Ci,Cj) < ?. 
End 
Stage 3: Constrained hierarchical agglomerative 
clustering algorithm using global features, i.e., util-
ize the same algorithm used in stage 2 by consider-
ing the global feature set G for cosine-based similar-
ity calculation instead of the local feature set L. 
 
Figure 1: Multi-stage Clustering Framework 
Person name Document no. sports 
?? 081 ?? 
?? 094 ?? 
?? 098 ?? 
?? 100 ?? 
2.5 Final Result Generation 
As discussed above, there are many modules for 
PND on Chinese person name. In our NEU_1, 
the final results are produced by combining 
outputs of discard-class document clustering, 
journalist-class clustering, sportsman-class 
clustering and multi-stage clustering modules. 
In NEU-2 system, the outputs of discard-class 
document clustering, journalist-class clustering, 
sportsman-class clustering and single-link 
clustering modules are combined to generate 
the final results.  
3 Evaluation 
3.1 Experimental Settings 
 Training data: containing about 30 Chinese 
person names, and a set of about 100-300 
news articles are provided for each person 
name.  
 Test data: similar to the training data, and 
containing 26 unseen Chinese personal 
names, provided by the SIGHAN organizer.  
 Performance evaluation metrics (Artiles et 
al., 2009): B_Cubed and P_IP metrics. 
3.2 Results 
Table 3 shows the performance of our two 
submission systems NEU_1 and NEU_2 on the 
test set of Sighan2010 Chinese personal name 
disambiguation task. 
  
B_Cubed P_IP System 
No. P R F P IP F 
NEU_1 95.76 88.37 91.47 96.99 92.58 94.56 
NEU_2 95.08 88.62 91.15 96.73 92.73 94.46 
 
Table 3: Results on the test data 
 
NEU-1 system was implemented by the 
multi-stage clustering framework that uses sin-
gle-link clustering method. In this framework, 
there are two threshold parameters ? and ?. 
Both threshold parameters are tuned from train-
ing data sets.  
After the formal evaluation, the organizer 
provided a diagnosis test designed to explore 
the relationship between Chinese word segmen-
tation and personal name disambiguation. In the 
diagnosis test, the personal name disambigua-
tion task was simplified and limited to the 
documents in which the personal name is 
tagged correctly. The performance of our two 
systems on the diagnosis test set of Sighan2010 
Chinese personal name disambiguation task are 
shown in Table 4. 
 
B_Cubed P_IP System 
no. P R F  P IP F  
NEU_1 95.6 89.74 92.14 96.83 93.62 95.03 
NEU_2 94.53 89.99 91.66 96.41 93.8 94.9 
 
Table 4: Results of the diagnosis test on test 
data 
 
As shown in the Table 3 and Table 4, NEU-1 
system achieves the highest precision and F 
values on the test data and the diagnosis test 
data. 
3.3 Discussion 
We propose a multi-stage clustering framework 
for Chinese personal name disambiguation. The 
evaluation results demonstrate that the features 
and key techniques our systems adopt are effec-
tive. Our systems achieve the best performance 
in this competition. However, our recall values 
are not unsatisfactory. In such a case, there is 
still much room for improvement. Observed 
from experimental results, some interesting is-
sues are worth being discussed and addressed in 
our future work as follows: 
(1) For PND on some personal names, the 
document topic information seems not effective. 
For example, the personal name "?? (Guo 
Hua)" in training set represent one shooter and 
one billiards player. The PND system based on 
traditional clustering method can not effectively 
work in such a case due to the same sports topic. 
To solve this problem, one solution is to suffi-
ciently combine the personal attributes and 
document topic information for PND on this 
person name. 
(2) For the journalist-class personal names, 
global BOW-type features are not effective in 
this case as different persons can report on the 
same or similar events. For example, there are 
four different journalists named ????(Zhu 
Jianjun)? in the training set, involving different 
locations such as Beijing, Zhengzhou, Xining or 
Guangzhou. We can distinguish them in terms 
of the location they are working in.  
(3) We found that some documents in the 
training set only contain lists of news title and 
the news reporter. In this case, we can not dis-
criminate the persons with respect to the loca-
tion of entire news. It?s worth studying some 
effective solution to address this challenge in 
our future work.  
(4) Seen from the experimental results, some 
personal names such as ???(Li gang)? are 
wrong identified because this person is associ-
ated with multiple professional titles and affili-
ates. In this case, the use of exact matching 
methods can not yield satisfactory results. For 
example, the query name ???(Li gang)? in 
the documents 274 and 275 is the president of  
???????????(China International 
Culture Association)? while in the documents 
202, 225 and 228, he is the director of ????
???????(Bureau of External Cultural 
Relations of Chinese Ministry of Culture)?. To 
group both cases into the same cluster, it?s 
worth mining the relations and underlying se-
mantic relations between entities to achieve this 
goal.  
 
4 Conclusion 
This paper presents our two Chinese personal 
name disambiguation systems in which various 
constrained hierarchical agglomerative cluster-
ing algorithms using local or global features are 
adopted. The bakeoff results show that our sys-
tems achieve the best performance. In the future, 
we will pay more attention on the personal at-
tribute extraction and unsupervised learning 
approaches for Chinese personal name disam-
biguation.  
5 Acknowledgements 
This work was supported in part by the National 
Science Foundation of China (60873091) and 
the Fundamental Research Funds for the Cen-
tral Universities. 
 
References 
Artiles, Javier, Julio Gonzalo and Satoshi Sekine. 
2009. ?WePS 2 Evaluation Campaign: overview of 
the Web People Search Clustering Task,? In 2nd 
Web People Search Evaluation Workshop (WePS 
2009), 18th WWW Conference. 
Duda, Richard O., Peter E.Hart, and David G.Stork. 
2004. Pattern Classification. China Machine Press. 
Masaki, Ikeda, Shingo Ono, Issei Sato, Minoru Yo-
shida, and Hiroshi Nakagawa. 2009. Person Name 
Disambiguation on the Web by TwoStage Clustering. 
In 2nd Web People Search Evaluation Workshop 
(WePS 2009), 18th WWW Conference. 
Yao, Tianshun, Zhu Jingbo , Zhang Li, Yang Ying. 
Nov. 2002. Natural Language Processing , Second 
Edition, Tsinghua press. 
Zhu, Jingbo and Wenliang Chen. 2005. Some Stud-
ies on Chinese Domain Knowledge Dictionary and 
Its Application to Text Classification. In Proc. of 
SIGHAN4. 
 
 
 
