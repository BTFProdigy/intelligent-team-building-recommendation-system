Modern Natural Language Interfaces to Databases:
Composing Statistical Parsing with Semantic Tractability
Ana-Maria Popescu Alex Armanasu Oren Etzioni
University of Washington
{amp, alexarm, etzioni, daveko, ayates}@cs.washington.edu
David Ko Alexander Yates
Abstract
Natural Language Interfaces to Databases
(NLIs) can benefit from the advances in statis-
tical parsing over the last fifteen years or so.
However, statistical parsers require training on
a massive, labeled corpus, and manually cre-
ating such a corpus for each database is pro-
hibitively expensive. To address this quandary,
this paper reports on the PRECISE NLI, which
uses a statistical parser as a ?plug in?. The pa-
per shows how a strong semantic model cou-
pled with ?light re-training? enables PRECISE
to overcome parser errors, and correctly map
from parsed questions to the corresponding
SQL queries. We discuss the issues in using
statistical parsers to build database-independent
NLIs, and report on experimental results with
the benchmark ATIS data set where PRECISE
achieves 94% accuracy.
1 Introduction and Motivation
Over the last fifteen years or so, much of the NLP
community has focused on the use of statistical
and machine learning techniques to solve a wide
range of problems in parsing, machine translation,
and more. Yet, classical problems such as building
Natural Language Interfaces to Databases (NLIs)
(Grosz et al, 1987) are far from solved.
There are many reasons for the limited success of
past NLI efforts (Androutsopoulos et al, 1995). We
highlight several problems that are remedied by our
approach. First, manually authoring and tuning a se-
mantic grammar for each new database is brittle and
prohibitively expensive. In response, we have im-
plemented a ?transportable? NLI that aims to mini-
mize manual, database-specific configuration. Sec-
ond, NLI systems built in the 70s and 80s had lim-
ited syntactic parsing capabilities. Thus, we have an
opportunity to incorporate the important advances
made by statistical parsers over the last two decades
in an NLI.
However, attempting to use a statistical parser in
a database-independent NLI leads to a quandary. On
the one hand, to parse questions posed to a particu-
lar database, the parser has to be trained on a corpus
of questions specific to that database. Otherwise,
many of the parser?s decisions will be incorrect. For
example, the Charniak parser (trained on the 40,000
sentences in the WSJ portion of the Penn Treebank)
treats ?list? as a noun, but in the context of the ATIS
database it is a verb.1 On the other hand, manually
creating and labeling a massive corpus of questions
for each database is prohibitively expensive.
We consider two methods of resolving this
quandary and assess their performance individually
and in concert on the ATIS data set. First, we use
a strong semantic model to correct parsing errors.
We introduce a theoretical framework for discrim-
inating between Semantically Tractable (ST) ques-
tions and difficult ones, and we show that ST ques-
tions are prevalent in the well-studied ATIS data
set (Price, 1990). Thus, we show that the seman-
tic component of the NLI task can be surprisingly
easy and can be used to compensate for syntactic
parsing errors. Second, we re-train the parser using
a relatively small set of 150 questions, where each
word is labeled by its part-of-speech tag.
To demonstrate how these methods work in prac-
tice, we sketch the fully-implemented PRECISE
NLI, where a parser is a modular ?plug in?. This
modularity enables PRECISE to leverage continuing
advances in parsing technology over time by plug-
ging in improved parsers as they become available.
The remainder of this paper is organized as fol-
lows. We describe PRECISE in Section 2, sketch our
theory in Section 3, and report on our experiments
in Section 4. We consider related work in Section 5,
and conclude in Section 6.
2 The PRECISE System Overview
Our recent paper (Popescu et al, 2003) introduced
the PRECISE architecture and its core algorithm for
1This is an instance of a well known machine learning prin-
ciple ? typically, a learning algorithm is effective when its test
examples are drawn from roughly the same distribution as its
training examples.
reducing semantic interpretation to a graph match-
ing problem that is solved by MaxFlow. In this sec-
tion we provide a brief overview of PRECISE, focus-
ing on the components necessary to understanding
its performance on the ATIS data set in Section 4.
To discuss PRECISE further, we must first intro-
duce some terminology. We say that a database is
made up of three types of elements: relations, at-
tributes and values. Each element is unique: an at-
tribute element is a particular column in a particular
relation and each value element is the value of a
particular attribute. A value is compatible with its
attribute and also with the relation containing this
attribute. An attribute is compatible with its rela-
tion. Each attribute in the database has associated
with it a special value, which we call a wh-value,
that corresponds to a wh-word (what, where, etc.).
We define a lexicon as a tuple (T, E, M), where
T is a set of strings, called tokens (intuitively, tokens
are strings of one or more words, like ?New York?);
E is a set of database elements, wh-values, and join
paths; 2 and M is a subset of T ? E ? a binary
relation between tokens and database elements.
PRECISE takes as input a lexicon and a parser.
Then, given an English question, PRECISE maps it
to one (or more) corresponding SQL queries. We
concisely review how PRECISE works through a
simple example. Consider the following question
q: ?What are the flights from Boston to Chicago??
First, the parser plug-in automatically derives a
dependency analysis for q from q?s parse tree,
represented by the following compact syntactic log-
ical form: LF (q) = what(0), is(0, 1), f light(1),
from(1, 2), boston(2), to(1, 3), chicago(3).
LF (q) contains a predicate for each question word.
Head nouns correspond to unary predicates whose
arguments are constant identifiers.
Dependencies are encoded by equality con-
straints between arguments to different predicates.
The first type of dependency is represented by noun
and adjective pre-modifiers corresponding to unary
predicates whose arguments are the identifiers for
the respective modified head nouns. A second type
of dependency is represented by noun postmodifiers
and mediated by prepositions (in the above exam-
ple, ?from? and ?to?). The prepositions correspond
to binary predicates whose arguments specify the at-
tached noun phrases. For instance, ?from? attaches
?flight? to ?boston?. Finally, subject/predicate,
predicate/direct object and predicate/indirect object
dependency information is computed for the various
2A join path is a set of equality constraints between the at-
tributes of two or more tables. See Section 3 for more details
and a formal definition.
verbs present in the question. Verbs correspond to
binary or tertiary predicates whose arguments indi-
cate what noun phrases play the subject and object
roles. In our example, the verb ?is? mediates the
dependency between ?what? and ?flight?. 3
PRECISE?s lexicon is generated by automatically
extracting value, attribute, and relation names from
the database. We manually augmented the lexicon
with relevant synonyms, prepositions, etc..
The tokenizer produces a single complete
tokenization of this question and lemmatizes
the tokens: (what, is, flight, from,
boston, to, chicago). By looking up the
tokens in the lexicon, PRECISE efficiently retrieves
the set of potentially matching database elements
for every token. In this case, what, boston and
chicago are value tokens, to and from are at-
tribute tokens and flight is a relation token.
In addition to this information, the lexicon also
contains a set of restrictions for tokens that are
prepositions or verbs. The restrictions specify the
database elements that are allowed to match to the
arguments of the respective preposition or verb. For
example, from can take as arguments a flight and
a city. The restrictions also specify the join paths
connecting these relations/attributes. The syntactic
logical form is used to retrieve the relevant set of
restrictions for a given question.
The matcher takes as input the information de-
scribed above and reduces the problem of satisfy-
ing the semantic constraints imposed by the defi-
nition of a valid interpretation to a graph matching
problem (Popescu et al, 2003). In order for each
attribute token to match a value token, Boston
and Chicago map to the respective values of the
database attribute city.cityName, from maps to
flight.fromAirport or fare.fromAirport and to
maps to flight.toAirport or fare.toAirport. The
restrictions validate the output of the matcher and
are then used in combination with the syntactic in-
formation to narrow down even further the possi-
ble interpretations for each token by enforcing lo-
cal dependencies. For example, the syntactic in-
formation tells us that ?from? refers to ?flight? and
since ?flight? uniquely maps to flight, this means
that from will map to flight.fromAirport rather
than fare.fromAirport (similarly, to maps to
flight.toAirport and whatmaps to flight.flightId).
Finally, the matcher compiles a list of all relations
satisfying all the clauses in the syntactic logical
form using each constant and narrows down the set
3PRECISE uses a larger set of constraints on dependency
relations, but for brevity, we focus on those relevant to our ex-
amples.
of possible interpretations for each token accord-
ingly. Each set of (constant, corresponding database
element) pairs represents a semantic logical form.
The query generator takes each semantic logical
form and uses the join path information available in
the restrictions to form the final SQL queries corre-
sponding to each semantic interpretation.
pronoun verb noun prep noun prep noun prep noun
NP NP
NP
PP
PP
PP
NPNP NP
VP
S
NP
WhatareflightsfromBostontoChicagoonMonday?
Figure 1: Example of an erroneous parse tree corrected
by PRECISE?s semantic over-rides. PRECISE detects that the
parser attached the PP ?on Monday? to ?Chicago? in error.
PRECISE attempts to re-attach ?on Monday? first to the PP
?to Chicago?, and then to the NP ?flights from Boston to
Chicago?, where it belongs.
2.1 Parser Enhancements
We used the Charniak parser (Charniak, 2000) for
the experiments reported in this paper. We found
that the Charniak parser, which was trained on
the WSJ corpus, yielded numerous syntactic errors.
Our first step was to hand tag a set of 150 questions
with Part Of Speech (POS) tags, and re-train the
parser?s POS tagger. As a result, the probabilities
associated with certain tags changed dramatically.
For example, initially, ?list? was consistently tagged
as a noun, but after re-training it was consistently la-
beled as a verb. This change occurs because, in the
ATIS domain, ?list? typically occurs in imperative
sentences, such as ?List all flights.?
Focusing exclusively on the tagger drastically re-
duced the amount of data necessary for re-training.
Whereas the Charniak parser was originally trained
on close to 40,000 sentences, we only required 150
sentences for re-training. Unfortunately, the re-
trained parser still made errors when solving dif-
ficult syntactic problems, most notably preposition
attachment and preposition ellipsis. PRECISE cor-
rects both types of errors using semantic informa-
tion.
We refer to PRECISE?s use of semantic informa-
tion to correct parser errors as semantic over-rides.
Specifically, PRECISE detects that an attachment de-
cision made by the parser is inconsistent with the
semantic information in its lexicon.4 When this oc-
curs, PRECISE attempts to repair the parse tree as
follows. Given a noun phrase or a prepositional
phrase whose corresponding node n in the parse tree
has the wrong parent p, PRECISE traverses the path
in the parse tree from p to the root node, search-
ing for a suitable node to attach n to. PRECISE
chooses the first ancestor of p such that when n is
attached to the new node, the modified parse tree
agrees with PRECISE?s semantic model. Thus, the
semantic over-ride procedure is a generate-and-test
search where potential solutions are generated in the
order of ancestors of node n in the parse tree. The
procedure?s running time is linear in the depth of the
parse tree.
Consider, for example, the question ?What are
flights from Boston to Chicago on Monday?? The
parser attaches the prepositional phrase ?on Mon-
day? to ?Chicago? whereas it should be attached to
?flights? (see Figure 1). The parser merely knows
that ?flights?, ?Boston?, and ?Chicago? are nouns. It
then uses statistics to decide that ?on Monday? is
most likely to attach to ?Chicago?. However, this
syntactic decision is inconsistent with the semantic
information in PRECISE?s lexicon ? the preposition
?on? does not take a city and a day as arguments,
rather it takes a flight and a day.
Thus, PRECISE decides to over-ride the parser
and attach ?on? elsewhere. As shown in Figure
1, PRECISE detects that the parser attached the PP
?on Monday? to ?Chicago? in error. PRECISE at-
tempts to re-attach ?on Monday? first to the PP ?to
Chicago?, and then to the NP ?flights from Boston
to Chicago?, where it belongs. While in our ex-
ample the parser violated a constraint in PRECISE?s
lexicon, the violation of any semantic constraint will
trigger the over-ride procedure.
In the above example, we saw how semantic over-
rides help PRECISE fix prepositional attachment er-
rors; they also enable it to correct parser errors
in topicalized questions (e.g., ?What are Boston to
Chicago flights??) and in preposition ellipsis (e.g.,
when ?on? is omitted in the question ?What are
flights from Boston to Chicago Monday??).
Unfortunately, semantic over-rides do not correct
all of the parser?s errors. Most of the remaining
parser errors fall into the following categories: rel-
ative clause attachment, verb attachment, numeric
4We say that node n is attached to node p if p is the parent
of n in the parse tree.
noun phrases, and topicalized prepositional phrases.
In general, semantic over-rides can correct local at-
tachment errors, but cannot over-come more global
problems in the parse tree. Thus, PRECISE can be
forced to give up and ask the user to paraphrase her
question.
3 PRECISE Theory
The aim of this section is to explain the theoretical
under-pinnings of PRECISE?s semantic model. We
show that PRECISE always answers questions from
the class of Semantically Tractable (ST) questions
correctly, given correct lexical and syntactic infor-
mation.5
We begin by introducing some terminology that
builds on the definitions given Section 2.
3.1 Definitions
A join path is a set of equality constraints between
a sequence of database relations. More formally, a
join path for relations R1, . . . , Rn is a set of con-
straints C ? {Ri.a = Ri+1.b|1 ? i ? n?1}. Here
the notation Ri.a refers to the value of attribute a in
relation Ri.
We say a relation between token set T and a set
of database elements and join paths E respects a
lexicon L if it is a subset of M .
A question is simply a string of characters. A to-
kenization of a question (with respect to a lexicon)
is an ordered set of strings such that each element
of the tokenization is an element of the lexicon?s to-
ken set, and the concatenation of the elements of the
tokenization, in order, is equal to the original ques-
tion. For a given lexicon and question, there may
be zero, one, or several tokenizations. Any question
that has at least one tokenization is tokenizable.
An attachment function is a function FL,q : T ?
T , where L is the lexicon, q is a question, and T
is the set of tokens in the lexicon. The attachment
function is meant to represent dependency informa-
tion available to PRECISE through a parser. For
example, if a question includes the phrase ?restau-
rants in Seattle?, the attachment function would at-
tach ?Seattle? to ?restaurants? for this question. Not
all tokens are attached to something in every ques-
tion, so the attachment function is not a total func-
tion. We say that a relation R between tokens
in a question q respects the attachment function if
?t1, t2, R(t1, t2) ? (FL,q(t1) = t2) ? (FL,q does
not take on a value for t1).
5We do not claim that NLI users will restrict their questions
to the ST subset of English in practice, but rather that identify-
ing classes of questions as semantically tractable (or not), and
experimentally measuring the prevalence of such questions, is
a worthwhile avenue for NLI research.
In an NLI, interpretations of a question are SQL
statements. We define a valid interpretation of a
question as being an SQL statement that satisfies a
number of conditions connecting it to the tokens in
the question. Because of space constraints, we pro-
vide only one such constraint as an example: There
exists a tokenization t of the question and a set of
database elements E such that there is a one-to-one
map from t to E respecting the lexicon, and for each
value element v ? E, there is exactly one equality
constraint in the SQL clause that uses v.
For a complete definition of a valid interpretation,
see (Popescu et al, 2003).
3.2 Semantic Tractability Model
In this section we formally define the class of
ST questions, and show that PRECISE can prov-
ably map such questions to the corresponding SQL
queries. Intuitively, ST questions are ?easy to un-
derstand? questions where the words or phrases
correspond to database elements or constraints on
join paths. Examining multiple questions sets and
databases, we have found that nouns, adjectives, and
adverbs in ?easy? questions refer to database rela-
tions, attributes, or values.
Moreover, the attributes and values in a question
?pair up? naturally to indicate equality constraints in
SQL. However, values may be paired with implicit
attributes that do not appear in the question (e.g., the
attribute ?cuisine? in ?What are the Chinese restau-
rants in Seattle?? is implicit). Interestingly, there is
no notion of ?implicit value? ? the question ?What
are restaurants with cuisine in Seattle?? does not
make sense.
A preposition indicates a join between the rela-
tions corresponding to the arguments of the prepo-
sition. For example, consider the preposition ?from?
in the question ?what airlines fly from Boston to
Chicago?? ?from? connects the value ?Boston? (in
the relation ?cities?) to the relation ?airlines?. Thus,
we know that the corresponding SQL query will join
?airlines? and ?cities?.
We formalize these observations about questions
below. We say that a question q is semantically
tractable using lexicon L and attachment function
FL,q if:
1. It is possible to split q up into words and
phrases found in L. (More formally, q is to-
kenizable according to L.)
2. While words may have multiple meanings in
the lexicon, it must be possible to find a one-
to-one correspondence between tokens in the
question and some set of database elements.
(More formally, there exists a tokenization t
and a set of database elements and join paths
Et such that there is a bijective function f from
t to Et that respects L.)
3. There is at least one such set Et that has exactly
one wh-value.
4. It is possible to add ?implicit? attributes to Et
to get a set E ?t with exactly one compatible
attribute for every value. (More formally, for
some Et with a wh-value there exist attributes
a1, . . . , an such that E ?t = Et ? {a1, . . . , an}
and there is a bijective function g from the set
of value elements (including wh-values) V to
the set of attribute elements A in E ?t.)
5. At least one such E ?t obeys the syntactic
restrictions of FL,q. (More formally, let
A? = A ? Et. Then we require that
{(f?1(g?1(a)), f?1(a)) | a ? A?} respects
FL,q.)
3.3 Results and Discussion
We say that an NLI is sound for a class of questions
Q using lexicon L and attachment function FL if
for every input q ? Q, every output of the NLI is a
valid interpretation. We say the NLI is complete if
it returns all valid interpretations. Our main result is
the following:
Theorem 1 Given a lexicon L and attachment
function FL, PRECISE is sound and complete for the
class of semantically tractable questions.
In practical terms, the theorem states that given
correct and complete syntactic and lexical informa-
tion, PRECISE will return exactly the set of valid
interpretations of a question. If PRECISE is missing
syntactic or semantic constraints, it can generate ex-
traneous interpretations that it ?believes? are valid.
Also, if a person uses a term in a manner incon-
sistent with PRECISE?s lexicon, then PRECISE will
interpret her question incorrectly. Finally, PRECISE
will not answer a question that contains words ab-
sent from its lexicon.
The theorem is clearly an idealization, but the ex-
periments reported in Section 4 provide evidence
that it is a useful idealization. PRECISE, which em-
bodies the model of semantic tractability, achieves
very high accuracy because in practice it either has
correct and complete lexical and syntactic informa-
tion or it has enough semantic information to com-
pensate for its imperfect inputs. In fact, as we ex-
plained in Section 2.1, PRECISE?s semantic model
enables it to correct parser errors in some cases.
Finding all the valid interpretations for a question
is computationally expensive in the worst case (even
just tokenizing a question is NP-complete (Popescu
et al, 2003)). Moreover, if the various syntac-
tic and semantic constraints are fed to a standard
constraint solver, then the problem of finding even
a single valid interpretation is exponential in the
worst case. However, we have been able to formu-
late PRECISE?s constraint satisfaction problem as a
graph matching problem that is solved in polyno-
mial time by the MaxFlow algorithm:
Theorem 2 For lexicon L, PRECISE finds one valid
interpretation for a tokenization T of a semantically
tractable question in time O(Mn2), where n is the
number of tokens in T and M is the maximum num-
ber of interpretations that a token can have in L.
4 Experimental Evaluation
Semantic Tractability (ST) theory and PRECISE?s
architecture raise a four empirical questions that
we now address via experiments on the ATIS data
set (Price, 1990): how prevalent are ST questions?
How effective is PRECISE in mapping ATIS ques-
tions to SQL queries? What is the impact of se-
mantic over-rides? What is the impact of parser re-
training? Our experiments utilized the 448 context-
independent questions in the ATIS ?Scoring Set A?.
We chose the ATIS data set because it is a standard
benchmark (see Table 2) where independently gen-
erated questions are available to test the efficacy of
an NLI.
We found that 95.8% of the ATIS questions were
ST questions. We classified each question as ST
(or not) by running PRECISE on the question and
System Setup PRECISE PRECISE-1
ParserORIG 61.9% 60.3%
ParserORIG+ 89.7% 85.5%
ParserTRAINED 92.4% 88.2%
ParserTRAINED+ 94.0% 89.2%
ParserCORRECT 95.8% 91.9%
Table 1: Impact of Parser Enhancements. The PRECISE
column records the percentage of questions where the small
set of SQL queries returned by PRECISE contains the cor-
rect query; PRECISE-1 refers to the questions correctly in-
terpreted if PRECISE is forced to return exactly one SQL
query. ParserORIG is the original version of the parser,
ParserTRAINED is the version re-trained for the ATIS do-
main, and ParserCORRECT is the version whose output is
corrected manually. System configurations marked by +
indicate the automatic use of semantic over-rides to correct
parser errors.
PRECISE PRECISE-1 AT&T CMU MIT SRI BBN UNISYS MITRE HEY
94.0% 89.1% 96.2% 96.2% 95.5% 93% 90.6% 76.4% 69.4% 92.5%
Table 2: Accuracy Comparison between PRECISE , PRECISE-1 and the major ATIS NLIs. Only PRECISE and the HEY NLI
are database independent. All results are for performance on the context-independent questions in ATIS.
recording its response. Intractable questions were
due to PRECISE?s incomplete semantic informa-
tion. Consider, for example, the ATIS request ?List
flights from Oakland to Salt Lake City leaving after
midnight Thursday.? PRECISE fails to answer this
question because it lacks a model of time, and so
cannot infer that ?after midnight Thursday? means
?early Friday morning.?
In addition, we found that the prevalence of ST
questions in the ATIS data is consistent with our ear-
lier results on the set of 1,800 natural language ques-
tions compiled by Ray Mooney in his experiments
in three domains (Tang and Mooney, 2001). As re-
ported in (Popescu et al, 2003), we found that ap-
proximately 80% of Mooney?s questions were ST.
PRECISE performance on the ATIS data was also
comparable to its performance on the Mooney data
sets.
Table 1 quantifies the impact of the parser en-
hancements discussed in Section 2.1. Since PRE-
CISE can return multiple distinct SQL queries when
it judges a question to be ambiguous, we report its
results in two columns. The left column (PRECISE)
records the percentage of questions where the set
of returned SQL queries contains the correct query.
The right column (PRECISE-1) records the percent-
age of questions where PRECISE is correct if it is
forced to return exactly one query per question. In
our experiments, PRECISE returned a single query
92.4% of the time, and returned two queries the rest
of the time. Thus, the difference between the two
columns is not great.
Initially, plugging the Charniak parser into PRE-
CISE yielded only 61.9% accuracy. Introducing se-
mantic over-rides to correct prepositional attach-
ment and preposition ellipsis errors increased PRE-
CISE?s accuracy to 89.7% ? the parser?s erroneous
POS tags still led PRECISE astray in some cases.
After re-training the parser on 150 POS-tagged
ATIS questions, but without utilizing semantic over-
rides, PRECISE achieved 92.4% accuracy. Combin-
ing both re-training and semantic over-rides, PRE-
CISE achieved 94.0% accuracy. This accuracy is
close to the maximum that PRECISE can achieve,
given its incomplete semantic information? we
found that, when all parsing errors are corrected by
hand, PRECISE?s accuracy is 95.8%.
To assess PRECISE?s performance, we compared
it with previous work. Table 2 shows PRECISE?s
accuracy compared with the most successful ATIS
NLIs (Minker, 1998). We also include, for com-
parison, the more recent database-independent HEY
system (He and Young, 2003). All systems were
compared on the ATIS scoring set ?A?, but we
did ?clean? the questions by introducing sentence
breaks, removing verbal errors, etc.. Since we could
add modules to PRECISE to automatically handle
these various cases, we don?t view this as signifi-
cant.
Given the database-specific nature of most previ-
ous ATIS systems, it is remarkable that PRECISE is
able to achieve comparable accuracy. PRECISE does
return two interpretations a small percentage of the
time. However, even when restricted to returning
a single interpretation, PRECISE-1 still achieved an
impressive 89.1% accuracy (Table 1).
5 Related Work
We discuss related work in three categories:
Database-independent NLIs, ATIS-specific NLIs,
and sublanguages.
Database-independent NLIs There has been ex-
tensive previous work on NLIs (Androutsopoulos et
al., 1995), but three key elements distinguish PRE-
CISE. First, we introduce a model of ST questions
and show that it produces provably correct inter-
pretations of questions (subject to the assumptions
of the model). We measure the prevalence of ST
questions to demonstrate the practical import of our
model. Second, we are the first to use a statistical
parser as a ?plug in?, experimentally measure its
efficacy, and analyze the attendant challenges. Fi-
nally, we show how to leverage our semantic model
to correct parser errors in difficult syntactic cases
(e.g., prepositional attachment). A more detailed
comparison of PRECISE with a wide range of NLI
systems appears in (Popescu et al, 2003). The
advances in this paper over our previous one in-
clude: reformulation of ST THEORY, the parser re-
training, semantic over-rides, and the experiments
testing PRECISE on the ATIS data.
ATIS NLIs The typical ATIS NLIs used either
domain-specific semantic grammars (Seneff, 1992;
Ward and Issar, 1996) or stochastic models that re-
quired fully annotated domain-specific corpora for
reliable parameter estimation (Levin and Pieraccini,
1995). In contrast, since it uses its model of se-
mantically tractable questions, PRECISE does not
require heavy manual processing and only a small
number of annotated questions. In addition, PRE-
CISE leverages existing domain-independent pars-
ing technology and offers theoretical guarantees ab-
sent from other work. Improved versions of ATIS
systems such as Gemini (Moore et al, 1995) in-
creased their coverage by allowing an approximate
question interpretation to be computed from the
meanings of some question fragments. Since PRE-
CISE focuses on high precision rather than recall, we
analyze every word in the question and interpret the
question as a whole. Most recently, (He and Young,
2003) introduced the HEY system, which learns a
semantic parser without requiring fully-annotated
corpora. HEY uses a hierarchical semantic parser
that is trained on a set of questions together with
their corresponding SQL queries. HEY is similar to
(Tang and Mooney, 2001). Both learning systems
require a large set of questions labeled by their SQL
queries?an expensive input that PRECISE does not
require?and, unlike PRECISE, both systems can-
not leverage continuing improvements to statistical
parsers.
Sublanguages The early work with the most sim-
ilarities to PRECISE was done in the field of sublan-
guages. Traditional sublanguage work (Kittredge,
1982) has looked at defining sublanguages for var-
ious domains, while more recent work (Grishman,
2001; Sekine, 1994) suggests using AI techniques
to learn aspects of sublanguages automatically. Our
work can be viewed as a generalization of tradi-
tional sublanguage research. We restrict ourselves
to the semantically tractable subset of English rather
than to a particular knowledge domain. Finally, in
addition to offering formal guarantees, we assess the
prevalence of our ?sublanguage? in the ATIS data.
6 Conclusion
This paper is the first to provide evidence that sta-
tistical parsers can support NLIs such as PRECISE.
We identified the quandary associated with appro-
priately training a statistical parser: without special
training for each database, the parser makes numer-
ous errors, but creating a massive, labeled corpus of
questions for each database is prohibitively expen-
sive. We solved this quandary via light re-training
of the parser?s tagger and via PRECISE?s semantic
over-rides, and showed that in concert these meth-
ods enable PRECISE to rise from 61.9% accuracy to
94% accuracy on the ATIS data set. Even though
PRECISE is database independent, its accuracy is
comparable to the best of the database-specific ATIS
NLIs developed in previous work (Table 2).
References
I. Androutsopoulos, G. D. Ritchie, and P. Thanisch.
1995. Natural Language Interfaces to Databases - An
Introduction. In Natural Language Engineering, vol
1, part 1, pages 29?81.
E. Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proc. of NAACL-2000.
R. Grishman. 2001. Adaptive information extraction
and sublanguage analysis. In Proc. of IJCAI 2001.
B.J. Grosz, D. Appelt, P. Martin, and F. Pereira. 1987.
TEAM: An Experiment in the Design of Trans-
portable Natural Language Interfaces. In Artificial In-
telligence 32, pages 173?243.
Y. He and S. Young. 2003. A data-driven spoken lan-
guage understanding system. In IEEE Workshop on
Automatic Speech Recognition and Understanding.
R. Kittredge. 1982. Variation and homogeneity of sub-
languages. In R. Kittredge and J. Lehrberger, editors,
Sublanguage: Studies of Language in Restricted Se-
mantic Domains, pages 107?137. de Gruyter, Berlin.
E. Levin and R. Pieraccini. 1995. Chronus, the next gen-
eration. In Proc. of the DARPA Speech and Natural
Language Workshop, pages 269?271.
W. Minker. 1998. Evaluation methodologies for inter-
active speech systems. In First International Confer-
ence on Language Resources and Evaluation, pages
801?805.
R. Moore, D. Appelt, J. Dowding, J. M. Gawron, and
D. Moran. 1995. Combining linguistic and statistical
knowledge sources in natural-language processing for
atis. In Proc. of the ARPA Spoken Language Technol-
ogy Workshop.
A. Popescu, O. Etzioni, and H. Kautz. 2003. Towards a
theory of natural language interfaces to databases. In
Proc. of IUI-2003.
P. Price. 1990. Evaluation of spoken language systems:
the atis domain. In Proc. of the DARPA Speech and
Natural Language Workshop, pages 91?95.
S. Sekine. 1994. A New Direction For Sublanguage
NLP. In Proc. of the International Conference on New
Methods in Language Processing, pages 165?177.
S. Seneff. 1992. Robust parsing for spoken language
systems. In Proc. of the IEEE International Confer-
ence on Acoustics, Speech and Signal Processing.
L.R. Tang and R.J. Mooney. 2001. Using Multiple
Clause Constructors in Inductive Logic Programming
for Semantic Parsing. In Proc. of the 12th Eu-
ropean Conference on Machine Learning (ECML-
2001), Freiburg, Germany, pages 466?477.
W. Ward and S. Issar. 1996. Recent improvements in the
cmu spoken language understanding system. In Proc.
of the ARPA Human Language Technology Workshop,
pages 213?216.
Proceedings of NAACL HLT 2007, pages 121?130,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Unsupervised Resolution of Objects and Relations on the Web
Alexander Yates
Turing Center
Computer Science and Engineering
University of Washington
Box 352350
Seattle, WA 98195, USA
ayates@cs.washington.edu
Oren Etzioni
Turing Center
Computer Science and Engineering
University of Washington
Box 352350
Seattle, WA 98195, USA
etzioni@cs.washington.edu
Abstract
The task of identifying synonymous re-
lations and objects, or Synonym Resolu-
tion (SR), is critical for high-quality infor-
mation extraction. The bulk of previous
SR work assumed strong domain knowl-
edge or hand-tagged training examples.
This paper investigates SR in the con-
text of unsupervised information extrac-
tion, where neither is available. The pa-
per presents a scalable, fully-implemented
system for SR that runs in O(KN log N)
time in the number of extractions N and
the maximum number of synonyms per
word, K. The system, called RESOLVER,
introduces a probabilistic relational model
for predicting whether two strings are
co-referential based on the similarity of
the assertions containing them. Given
two million assertions extracted from the
Web, RESOLVER resolves objects with
78% precision and an estimated 68% re-
call and resolves relations with 90% pre-
cision and 35% recall.
1 Introduction
Web Information Extraction (WIE) sys-
tems extract assertions that describe a rela-
tion and its arguments from Web text (e.g.,
(is capital of,D.C.,United States)). WIE systems
can extract hundreds of millions of assertions
containing millions of different strings from the
Web (e.g., the TEXTRUNNER system (Banko et al,
2007)).1 WIE systems often extract assertions that
describe the same real-world object or relation using
different names. For example, a WIE system might
extract (is capital city of,Washington,U.S.),
which describes the same relationship as above but
contains a different name for the relation and each
argument.
Synonyms are prevalent in text, and the Web cor-
pus is no exception. Our data set of two million as-
sertions extracted from a Web crawl contained over
a half-dozen different names each for the United
States and Washington, D.C., and three for the ?is
capital of? relation. The top 80 most commonly
extracted objects had an average of 2.9 extracted
names per entity, and several had as many as 10
names. The top 100 most commonly extracted re-
lations had an average of 4.9 synonyms per relation.
We refer to the problem of identifying synony-
mous object and relation names as Synonym Res-
olution (SR).2 An SR system for WIE takes a set of
assertions as input and returns a set of clusters, with
each cluster containing coreferential object strings
or relation strings. Previous techniques for SR have
focused on one particular aspect of the problem, ei-
ther objects or relations. In addition, the techniques
either depend on a large set of training examples, or
are tailored to a specific domain by assuming knowl-
edge of the domain?s schema. Due to the number
and diversity of the relations extracted, these tech-
1For a demo see www.cs.washington.edu/research/textrunner.
2Ironically, SR has a number of synonyms in the literature,
including Entity Resolution, Record Linkage, and Deduplica-
tion.
121
niques are not feasible for WIE systems. Schemata
are not available for the Web, and hand-labeling
training examples for each relation would require a
prohibitive manual effort.
In response, we present RESOLVER, a novel,
domain-independent, unsupervised synonym resolu-
tion system that applies to both objects and relations.
RESOLVER clusters coreferential names together us-
ing a probabilistic model informed by string similar-
ity and the similarity of the assertions containing the
names. Our contributions are:
1. A scalable clustering algorithm that runs in
time O(KN log N) in the number of extrac-
tions N and maximum number of synonyms
per word, K, without discarding any poten-
tially matching pair, under exceptionally weak
assumptions about the data.
2. An unsupervised probabilistic model for pre-
dicting whether two object or relation names
co-refer.
3. An empirical demonstration that RESOLVER
can resolve objects with 78% precision and
68% recall, and relations with 90% precision
and 35% recall.
The next section discusses previous work. Section
3 introduces our probabilistic model for SR. Section
4 describes our clustering algorithm. Section 5 de-
scribes extensions to our basic SR system. Section
6 presents our experiments, and section 7 discusses
our conclusions and areas for future work.
2 Previous Work
The DIRT algorithm (Lin and Pantel, 2001) ad-
dresses a piece of the unsupervised SR problem.
DIRT is a heuristic method for finding synonymous
relations, or ?inference rules.? DIRT uses a depen-
dency parser and mutual information statistics over
a corpus to identify relations that have similar sets of
arguments. In contrast, our algorithm provides a for-
mal probabilistic model that applies equally well to
relations and objects, and we provide an evaluation
of the algorithm in terms of precision and recall.
There are many unsupervised approaches for ob-
ject resolution in databases, but unlike our algo-
rithm these approaches depend on a known, fixed
schema. Ravikumar and Cohen (Ravikumar and Co-
hen, 2004) present an unsupervised approach to ob-
ject resolution using Expectation-Maximization on
a hierarchical graphical model. Several other re-
cent approaches leverage domain-specific informa-
tion and heuristics for object resolution. For ex-
ample, many (Dong et al, 2005; Bhattacharya and
Getoor, 2005; Bhattacharya and Getoor, 2006) rely
on evidence from observing which strings appear as
arguments to the same relation simultaneously (e.g.,
co-authors of the same publication). While this is
useful information when resolving authors in the ci-
tation domain, it is extremely rare to find relations
with similar properties in extracted assertions. None
of these approaches applies to the problem of resolv-
ing relations. See (Winkler, 1999) for a survey of
this area.
Several supervised learning techniques make en-
tity resolution decisions (Kehler, 1997; McCallum
and Wellner, 2004; Singla and Domingos, 2006), but
of course these systems depend on the availability
of training data, and often on a significant number
of labeled examples per relation of interest. These
approaches also depend on complex probabilistic
models and learning algorithms, and they have order
O(n3) time complexity, or worse. They currently do
not scale to the amounts of data extracted from the
Web. Previous systems were tested on at most a few
thousand examples, compared with millions or hun-
dreds of millions of extractions from WIE systems
such as TEXTRUNNER.
Coreference resolution systems (e.g., (Lappin and
Leass, 1994; Ng and Cardie, 2002)), like SR sys-
tems, try to merge references to the same object (typ-
ically pronouns, but potentially other types of noun
phrases). This problem differs from the SR problem
in several ways: first, it deals with unstructered text
input, possibly with syntactic annotation, rather than
relational input. Second, it deals only with resolv-
ing objects. Finally, it requires local decisions about
strings; that is, the same word may appear twice in a
text and refer to two different things, so each occur-
rence of a word must be treated separately.
The PASCAL Recognising Textual Entailment
Challenge proposes the task of recognizing when
two sentences entail one another, and many authors
have submitted responses to this challenge (Dagan et
al., 2006). Synonym resolution is a subtask of this
problem. Our task differs significantly from the tex-
tual entailment task in that it has no labeled training
122
data, and its input is in the form of relational extrac-
tions rather than raw text.
Two probabilistic models for information extrac-
tion have a connection with ours. Our probabilistic
model is partly inspired by the ball-and-urns abstrac-
tion of information extraction presented by Downey
et al (2005) Our task and probability model are dif-
ferent from theirs, but we make many of the same
modeling assumptions. Second, we follow Snow et
al.?s work (2006) on taxonomy induction in incorpo-
rating transitive closure constraints in our probabil-
ity calculations, as explained below.
3 Probabilistic Model
Our probabilistic model provides a formal, rigorous
method for resolving synonyms in the absence of
training data. It has two sources of evidence: the
similarity of the strings themselves (i.e., edit dis-
tance) and the similarity of the assertions they ap-
pear in. This second source of evidence is some-
times referred to as ?distributional similarity? (Hin-
dle, 1990).
Section 3.2 presents a simple model for predict-
ing whether a pair of strings co-refer based on string
similarity. Section 3.3 then presents a model called
the Extracted Shared Property (ESP) Model for pre-
dicting whether a pair of strings co-refer based on
their distributional similarity. Finally, a method is
presented for combining these models to come up
with an overall prediction for coreference decisions
between two clusters of strings.
3.1 Terminology and Notation
We use the following notation to describe the proba-
bilistic models. The input is a data set D containing
extracted assertions of the form a = (r, o1, . . . , on),
where r is a relation string and each oi is an object
string representing the arguments to the relation. In
our data, all of the extracted assertions are binary, so
n = 2. The subset of all assertions in D containing
a string s is called Ds.
For strings si and sj , let Ri,j be the random vari-
able for the event that si and sj refer to the same
entity. Let Rti,j denote the event that Ri,j is true,
and Rfi,j denote the event that it is false.
A pair of strings (r, s2) is called a property of
a string s1 if there is an assertion (r, s1, s2) ? D
or (r, s2, s1) ? D. A pair of strings (s1, s2) is
an instance of a string r if there is an assertion
(r, s1, s2) ? D. Equivalently, the property p =
(r, s2) applies to s1, and the relation r applies to
the instance i = (s1, s2). Finally, two strings x and
y share a property (or instance) if both x and y are
extracted with the same property (or instance).
3.2 String Similarity Model
Many objects appear with multiple names that are
substrings, acronyms, abbreviations, or other sim-
ple variations of one another. Thus string similarity
can be an important source of evidence for whether
two strings co-refer. Our probabilistic String Sim-
ilarity Model (SSM) assumes a similarity function
sim(s1, s2): STRING? STRING ? [0, 1]. The
model sets the probability of s1 co-referring with s2
to a smoothed version of the similarity:
P (Rti,j |sim(s1, s2)) =
? ? sim(s1, s2) + 1
?+ ?
The particular choice of ? and ? make little differ-
ence to our results, so long as they are chosen such
that the resulting probability can never be one or
zero. In our experiments ? = 20 and ? = 5, and we
use the well-known Monge-Elkan string similarity
function for objects and the Levenshtein string edit-
distance function for relations (Cohen et al, 2003).
3.3 The Extracted Shared Property Model
The Extracted Shared Property (ESP) Model out-
puts the probability that s1 and s2 co-refer
based on how many properties (or instances) they
share. As an example, consider the strings
?Mars? and ?Red Planet?, which appear in our
data 659 and 26 times respectively. Out of
these extracted assertions, they share four proper-
ties. For example, (lacks,Mars, ozone layer) and
(lacks,Red P lanet, ozone layer) both appear as
assertions in our data. The ESP model determines
the probability that ?Mars? and ?Red Planet? refer
to the same entity after observing k, the number of
properties that apply to both, n1, the total number
of extracted properties for ?Mars?, and n2, the total
number of extracted properties for ?Red Planet.?
ESP models the extraction of assertions as a
generative process, much like the URNS model
(Downey et al, 2005). For each string si, a certain
123
number, Pi, of properties of the string are written on
balls and placed in an urn. Extracting ni assertions
that contain si amounts to selecting a subset of size
ni from these labeled balls.3 Properties in the urn are
called potential properties to distinguish them from
extracted properties.
To model coreference decisions, ESP uses a pair
of urns, containing Pi and Pj balls respectively, for
the two strings si and sj . Some subset of the Pi
balls have the exact same labels as an equal-sized
subset of the Pj balls. Let the size of this sub-
set be Si,j . The ESP model assumes that corefer-
ential strings share as many potential properties as
possible, though only a few of the potential proper-
ties will be extracted for both. For non-coreferential
strings, the number of shared potential properties is a
strict subset of the potential properties of each string.
Thus if Ri,j is true then Si,j = min(Pi, Pj), and if
Ri,j is false then Si,j < min(Pi, Pj).
The ESP model makes several simplifying as-
sumptions in order to make probability predictions.
As is suggested by the ball-and-urn abstraction, it
assumes that each ball for a string is equally likely
to be selected from its urn. Because of data sparsity,
almost all properties are very rare, so it would be dif-
ficult to get a better estimate for the prior probability
of selecting a particular potential property. Second,
it assumes that without knowing the value of k, ev-
ery value of Si,j is equally likely, since we have no
better information. Finally, it assumes that all sub-
sets of potential properties are equally likely to be
shared by two non-coreferential objects, regardless
of the particular labels on the balls, given the size of
the shared subset.
Given these assumptions, we can derive an ex-
pression for P (Rti,j). First, note that there are(Pi
ni
)(Pj
nj
) total ways of extracting ni and nj asser-
tions for si and sj . Given a particular value of Si,j ,
the number of ways in which ni and nj assertions
can be extracted such that they share exactly k is
given by
Count(k, ni, nj |Pi, Pj , Si,j) =
(Si,j
k
)?
r,s?0
(Si,j?k
r+s
)(r+s
r
)( Pi?Si,j
ni?(k+r)
)( Pj?Si,j
nj?(k+s)
)
By our assumptions,
3Unlike the URNS model, balls are drawn without replace-
ment because each extracted property is distinct in our data.
P (k|ni, nj , Pi, Pj , Si,j) =
Count(k, ni, nj |Pi, Pj , Si,j)(Pi
ni
)(Pj
nj
) (1)
Let Pmin = min(Pi, Pj). The result below fol-
lows from Bayes? Rule and our assumptions above:
Proposition 1 If two strings si and sj have Pi and
Pj potential properties (or instances), and they ap-
pear in extracted assertions Di and Dj such that
|Di| = ni and |Dj | = nj , and they share k extracted
properties (or instances), the probability that si and
sj co-refer is:
P (Rti,j |Di, Dj , Pi, Pj) =
P (k|ni, nj , Pi, Pj , Si,j = Pmin)?
k?Si,j?Pmin P (k|ni, nj , Pi, Pj , Si,j)
(2)
Substituting equation 1 into equation 2 gives us a
complete expression for the probability we are look-
ing for.
Note that the probability for Ri,j depends on just
two hidden parameters, Pi and Pj . Since we have
no labeled data to estimate these parameters from,
we tie these parameters to the number of times the
respective strings si and sj are extracted. Thus we
set Pi = N ? ni, and we set N = 50 in our experi-
ments.
3.4 Combining the Evidence
For each potential coreference relationship Ri,j ,
there are now two pieces of probabilistic evidence.
Let Eei,j be the evidence for ESP, and let Esi,j be the
evidence for SSM. Our method for combining the
two uses the Na??ve Bayes assumption that each piece
of evidence is conditionally independent, given the
coreference relation:
P (Esi,j , Eei,j |Ri,j) = P (Esi,j |Ri,j)P (Eei,j |Ri,j)
Given this simplifying assumption, we can com-
bine the evidence to find the probability of a cofer-
ence relationship by applying Bayes? Rule to both
sides (we omit the i, j indices for brevity):
P (Rt|Es, Ee) =
P (Rt|Es)P (Rt|Ee)(1? P (Rt))?
i?{t,f} P (Ri|Es)P (Ri|Ee)(1? P (Ri))
124
3.5 Comparing Clusters of Strings
Our algorithm merges clusters of strings with one
another, using one of the above models. However,
these models give probabilities for coreference deci-
sions between two individual strings, not two clus-
ters of strings.
We follow the work of Snow et al (2006) in in-
corporating transitive closure constraints in proba-
bilistic modeling, and make the same independence
assumptions. The benefit of this approach is that the
calculation for merging two clusters depends only
on coreference decisions between individual strings,
which can be calculated independently.
Let a clustering be a set of coreference relation-
ships between pairs of strings such that the corefer-
ence relationships obey the transitive closure prop-
erty. We let the probability of a set of assertions D
given a clustering C be:
P (D|C) =
?
Rti,j?C
P (Di ?Dj |Rti,j)?
?
Rfi,j?C
P (Di ?Dj |Rfi,j)
The metric used to determine if two clusters
should be merged is the likelihood ratio, or the prob-
ability for the set of assertions given the merged
clusters over the probability given the original clus-
tering. Let C ? be a clustering that differs from C
only in that two clusters in C have been merged in
C ?, and let ?C be the set of coreference relation-
ships in C ? that are true, but the corresponding ones
in C are false. This metric is given by:
P (D|C ?)/P (D|C) =
?
Rti,j??C P (R
t
i,j |Di ?Dj)(1? P (Rti,j))?
Rti,j??C(1? P (R
t
i,j |Di ?Dj))P (Rti,j)
The probability P (Rti,j |Di?Dj) may be supplied
by the SSM, ESP, or combination model. In our ex-
periments, we let the prior for the SSM model be
0.5. For the ESP and combined models, we set the
prior to P (Rti,j) = 1min(P1,P2) .
4 RESOLVER?s Clustering Algorithm
Our clustering algorithm iteratively merges clusters
of co-referential names, making each iteration in
S := set of all strings
For each property or instance p,
Sp := {s ? S|s has property p}
1. Scores := {}
2. Build index mapping properties (and instances)
to strings with those properties (instances)
3. For each property or instance p:
If |Sp| < Max:
For each pair {s1, s2} ? Sp:
Add mergeScore(s1, s2) to Scores
4. Repeat until no merges can be performed:
Sort Scores
UsedClusters := {}
While score of top clusters c1, c2
is above Threshold:
Skip if either is in UsedClusters
Merge c1 and c2
Add c1, c2 to UsedClusters
Merge properties containing c1, c2
Recalculate merge scores as in Steps 1-3
Figure 1: RESOLVER?s Clustering Algorithm
time O(N log N) in the number of extracted as-
sertions. The algorithm requires only basic assump-
tions about which strings to compare. Previous work
on speeding up clustering algorithms for SR has ei-
ther required far stronger assumptions, or else it has
focused on heuristic methods that remain, in the
worst case, O(N2) in the number of distinct objects.
Our algorithm, a greedy agglomerative clustering
method, is outlined in Figure 1. The first novel part
of the algorithm, step 3, compares pairs of strings
that share the same property or instance, so long as
no more than Max strings share that same property
or instance. After the scores for all comparisons are
made, each string is assigned its own cluster. Then
the scores are sorted and the best cluster pairs are
merged until no pair of clusters has a score above
threshold. The second novel aspect of this algorithm
is that as it merges clusters in Step 4, it merges prop-
erties containing those clusters in a process we call
mutual recursion, which is discussed below.
This algorithm compares every pair of clusters
that have the potential to be merged, assuming two
properties of the data. First, it assumes that pairs
of clusters with no shared properties are not worth
125
comparing. Since the number of shared properties
is a key source of evidence for our approach, these
clusters almost certainly will not be merged, even if
they are compared, so the assumption is quite rea-
sonable. Second, the approach assumes that clus-
ters sharing only properties that apply to very many
strings (more than Max) need not be compared.
Since properties shared by many strings provide lit-
tle evidence that the strings are coreferential, this as-
sumption is reasonable for SR. We use Max = 50
in our experiments. Less than 0.1% of the properties
are thrown out using this cutoff.
4.1 Algorithm Analysis
Let D be the set of extracted assertions. The follow-
ing analysis shows that one iteration of merges takes
time O(N log N), where N = |D|. Let NC be
the number of comparisons between strings in step
3. To simplify the analysis, we consider only those
properties that contain a relation string and an argu-
ment 1 string. Let A be the set of all such properties.
NC is linear in N :4
NC =
?
p?A
|Sp| ? (|Sp| ? 1)
2
? (Max? 1)2 ?
?
p?A
|Sp|
= (Max? 1)2 ?N
Note that this bound is quite loose because most
properties apply to only a few strings. Step 4 re-
quires time O(N log N) to sort the comparison
scores and perform one iteration of merges. If the
largest cluster has size K, in the worst case the al-
gorithm will take K iterations. In our experiments,
the algorithm never took more than 9 iterations.
4.2 Relation to other speed-up techniques
The merge/purge algorithm (Hernandez and Stolfo,
1995) assumes the existence of a particular attribute
such that when the data set is sorted on this attribute,
matching pairs will all appear within a narrow win-
dow of one another. This algorithm is O(M log M)
where M is the number of distinct strings. However,
there is no attribute or set of attributes that comes
4If the Max parameter is allowed to vary with log|D|,
rather than remaining constant, the same analysis leads to a
slightly looser bound that is still better than O(N2).
close to satisfying this assumption in the context of
domain-independent information extraction.
There are several techniques that often provide
speed-ups in practice, but in the worst case they
make O(M2) comparisons at each merge iteration,
where M is the number of distinct strings. This can
cause problems on very large data sets. Notably,
McCallum et al (2000) use a cheap comparison
metric to place objects into overlapping ?canopies,?
and then use a more expensive metric to cluster ob-
jects appearing in the same canopy. The RESOLVER
clustering algorithm is in fact an adaptation of the
canopy method; it adds the restriction that strings are
not compared when they share only high-frequency
properties. The canopy method works well on high-
dimensional data with many clusters, which is the
case with our problem, but its time complexity is
worse than ours.
For information extraction data, a complexity of
O(M2) in the number of distinct strings turns out
to be considerably worse than our algorithm?s com-
plexity of O(N log N) in the number of extracted
assertions. This is because the data obeys a Zipf law
relationship between the frequency of a string and its
rank, so the number of distinct strings grows linearly
or almost linearly with the number of assertions.5
4.3 Mutual Recursion
Mutual recursion refers to the novel property of
our algorithm that as it clusters relation strings to-
gether into sets of synonyms, it collapses proper-
ties together for object strings and potentially finds
more shared properties between coreferential object
strings. Likewise, as it clusters objects together into
sets of coreferential names, it collapses instances of
relations together and potentially finds more shared
instances between coreferential relations. Thus the
clustering decisions for relations and objects mutu-
ally depend on one another.
For example, the strings ?Kennedy? and ?Pres-
ident Kennedy? appear in 430 and 97 assertions
in our data, respectively, but none of their ex-
tracted properties match exactly. Many properties,
5The exact relationship depends on the shape parameter z
of the Zipf curve. If z < 1, as it is for our data set, the num-
ber of total extractions grows linearly with the number of dis-
tinct strings extracted. If z = 1, then n extractions will contain
?( nln n ) distinct strings.
126
however, almost match. For example, the asser-
tions (challenged,Kennedy,Premier Krushchev)
and (stood up to,President Kennedy,Kruschev)
both appear in our data. Because ?challenged? and
?stood up to? are similar, and ?Krushchev? and ?Pre-
mier Krushchev? are similar, our algorithm is able
to merge these pairs into two clusters, thereby creat-
ing a new shared property between ?Kennedy? and
?President Kennedy.? Eventually it can merge these
two strings as well.
5 Extensions to RESOLVER
While the basic RESOLVER system can cluster syn-
onyms accurately and quickly, there is one type of
error that it frequently makes. In some cases, it has
difficulty distinguishing between similar pairs of ob-
jects and identical pairs. For example, ?Virginia?
and ?West Virginia? share several extractions be-
cause they have the same type, and they have high
string similarity. As a result, RESOLVER clusters
these two together. The next two sections describe
two extensions to RESOLVER that address the prob-
lem of similarity vs. identity.
5.1 Function Filtering
RESOLVER can use functions and one-to-one rela-
tions to help distinguish between similar and identi-
cal pairs. For example, West Virginia and Virginia
have different capitals: Richmond and Charleston,
respectively. If both of these facts are extracted, and
if RESOLVER knows that the ?capital of? relation is
functional, it should prevent Virginia and West Vir-
ginia from merging.
The Function Filter prevents merges between
strings that have different values for the same func-
tion. More precisely, it decides that two strings y1
and y2 match if their string similarity is above a high
threshold. It prevents a merge between strings x1
and x2 if there exist a function f and extractions
f(x1, y1) and f(x2, y2), and there are no such ex-
tractions such that y1 and y2 match (and vice versa
for one-to-one relations). Experiments described in
section 6 show that the Function Filter can improve
the precision of RESOLVER without significantly af-
fecting its recall.
While the Function Filter currently uses func-
tions and one-to-one relations as negative evidence,
it is also possible to use them as positive evidence.
For example, the relation ?married? is not strictly
one-to-one, but for most people the set of spouses
is very small. If a pair of strings are extracted
with the same spouse?e.g., ?FDR? and ?President
Roosevelt? share the property (?married?, ?Eleanor
Roosevelt?)?this is far stronger evidence that the
two strings are identical than if they shared some
random property.
Unfortunately, various techniques that attempted
to model this insight, including a TF-IDF weighting
of properties, yielded essentially no improvement of
RESOLVER. One major reason is that there are rel-
atively few examples of shared functional or one-
to-one properties because of sparsity. This idea de-
serves more investigation, however, and is an area
for future work.
5.2 Using Web Hitcounts
While names for two similar objects may often ap-
pear together in the same sentence, it is relatively
rare for two different names of the same object to
appear in the same sentence. RESOLVER exploits
this fact by querying the Web to determine how often
a pair of strings appears together in a large corpus.
When the hitcount is high, RESOLVER can prevent
the merge.
Specifically, the Coordination-Phrase Filter
searches for hitcounts of the phrase ?s1 and s2?,
where s1 and s2 are a candidate pair for merging.
It then computes a variant of pointwise mutual
information, given by
coordination score(s1, s2) = hits(s1 and s2)
2
hits(s1)? hits(s2)
The filter prevents any merge for which the coor-
dination score is above a threshold, which is de-
termined on a development set. The results of
Coordination-Phrase filtering are discussed in the
next section.
6 Experiments
Our experiments demonstrate that the ESP model
is significantly better at resolving synonyms than a
widely-used distributional similarity metric, the co-
sine similarity metric (CSM) (Salton and McGill,
1983), and that RESOLVER is significantly better at
127
resolving synonyms than either of its components,
SSM or ESP.
We test these models on a data set of 2.1 million
assertions extracted from a Web crawl.6 All models
ran over all assertions, but compared only those ob-
jects or relations that appeared at least 25 times in
the data, to give the ESP and CSM models sufficient
data for estimating similarity. However, the mod-
els do use strings that appear less than 25 times as
features. In all, the data contains 9,797 distinct ob-
ject strings and 10,151 distinct relation strings that
appear at least 25 times.
We judged the precision of each model by manu-
ally labeling all of the clusters that each model out-
puts. Judging recall would require inspecting not
just the clusters that the system outputs, but the en-
tire data set, to find all of the true clusters. Be-
cause of the size of the data set, we instead esti-
mated recall over a smaller subset of the data. We
took the top 200 most frequent object strings and top
200 most frequent relation strings in the data. For
each one of these high-frequency strings, we man-
ually searched through all strings with frequency
over 25 that shared at least one property, as well
as all strings that contained one of the keywords in
the high-frequency strings or obvious variations of
them. We manually clustered the resulting matches.
The top 200 object strings formed 51 clusters of size
greater than one, with an average cluster size of 2.9.
For relations, the top 200 strings and their matches
formed 110 clusters with size greater than one, with
an average cluster size of 4.9. We measured the re-
call of our models by comparing the set of all clus-
ters containing at least one of the high-frequency
words against these gold standard clusters.
For our precision and recall measures, we only
compare clusters of size two or more, in order to
focus on the interesting cases. Using the term hy-
pothesis cluster for clusters created by one of the
models, we define the precision of a model to be the
number of elements in all hypothesis clusters which
are correct divided by the total number of elements
in hypothesis clusters. An element s is marked cor-
rect if a plurality of the elements in s?s cluster refer
to the same entity as s; we break ties arbitrarily, as
6The data is made available at
http://www.cs.washington.edu/homes/ayates/.
they do not affect results. We define recall as the
sum over gold standard clusters of the most num-
ber of elements found in a single hypothesis cluster,
divided by the total number of elements in gold stan-
dard clusters.
For the ESP and SSM models in our experiment,
we prevented mutual recursion by clustering rela-
tions and objects separately. Only the full RE-
SOLVER system uses mutual recursion. For the CSM
model, we create for each distinct string a row vec-
tor, with each column representing a property. If that
property applies to the string, we set the value of
that column to the inverse frequency of the property
and zero otherwise. CSM finds the cosine of the an-
gle between the vectors for each pair of strings, and
merges the best pairs that score above threshold.
Each model requires a threshold parameter to de-
termine which scores are suitable for merging. For
these experiments we arbitrarily chose a threshold
of 3 for the ESP model (that is, the data needs to
be 3 times more likely given the merged cluster than
the unmerged clusters in order to perform the merge)
and chose thresholds for the other models by hand so
that the difference between them and ESP would be
roughly even between precision and recall, although
for relations it was harder to improve the recall. It is
an important item for future work to be able to esti-
mate these thresholds and perhaps other parameters
of our models from unlabeled data, but the chosen
parameters worked well enough for the experiments.
Table 1 shows the precision and recall of our models.
6.1 Discussion
ESP significantly outperforms CSM on both object
and relation clustering. CSM had particular trouble
with lower-frequency strings, judging far too many
of them to be co-referential on too little evidence. If
the threshold for clustering using CSM is increased,
however, the recall begins to approach zero.
ESP and CSM make predictions based on a very
noisy signal. ?Canada,? for example, shares more
properties with ?United States? in our data than
?U.S.? does, even though ?Canada? appears less of-
ten than ?U.S.? The results show that both models
perform below the SSM model on its own for object
merging, and both perform slightly better than SSM
on relations because of SSM?s poor recall.
We found a significant improvement in both pre-
128
Objects Relations
Model Prec. Rec. F1 Prec. Rec. F1
CSM 0.51 0.36 0.42 0.62 0.29 0.40
ESP 0.56 0.41 0.47 0.79 0.33 0.47
SSM 0.62 0.53 0.57 0.85 0.25 0.39
RESOLVER 0.71 0.66 0.68 0.90 0.35 0.50
Table 1: Comparison of the cosine similarity metric (CSM), RESOLVER components (SSM and ESP), and the RESOLVER
system. Bold indicates the score is significantly different from the score in the row above at p < 0.05 using the chi-squared test
with one degree of freedom. Using the same test, RESOLVER is also significantly different from ESP and CSM in recall on objects,
and from CSM and SSM in recall on relations. RESOLVER?s F1 on objects is a 19% increase over SSM?s F1. RESOLVER?s F1 on
relations is a 28% increase over SSM?s F1.
cision and recall when using a combined model over
using SSM alone. RESOLVER?s F1 is 19% higher
than SSM?s on objects, and 28% higher on relations.
In a separate experiment we found that mutual re-
cursion provides mixed results. A combination of
SSM and ESP without mutual recursion had a preci-
sion of 0.76 and recall of 0.59 on objects, and a pre-
cision of 0.91 and recall of 0.35 on relations. Mutual
recursion increased recall and decreased precision
for both objects and relations. None of the differ-
ences were statistically significant, however.
There is clearly room for improvement on the SR
task. Except for the problem of confusing similar
and identical pairs (see section 5), error analysis
shows that most of RESOLVER?s mistakes are be-
cause of two kinds of errors:
1. Extraction errors. For example, ?US News?
gets extracted separately from ?World Report?, and
then RESOLVER clusters them together because they
share almost all of the same properties.
2. Multiple word senses. For example, there are two
President Bushes; also, there are many terms like
?President? and ?Army? that can refer to many dif-
ferent entities.
6.2 Experiments with Extensions
The extensions to RESOLVER attempt to address
the confusion between similar and identical pairs.
Experiments with the extensions, using the same
datasets and metrics as above, demonstrate that the
Function Filter (FF) and the Coordination-Phrase
Filter (CPF) boost RESOLVER?s performance.
FF requires as input the set of functional and one-
to-one relations in the data. Table 2 contains a sam-
is capital of is capital city of
named after was named after
headquartered in is headquartered in
Table 2: A sample of the set of functions used by the Func-
tion Filter.
Model Prec. Rec. F1
RESOLVER 0.71 0.66 0.68
RESOLVER+FF 0.74 0.66 0.70
RESOLVER+CPF 0.78 0.68 0.73
RESOLVER+FF+CPF 0.78 0.68 0.73
Table 3: Comparison of object merging results for the
RESOLVER system, RESOLVER plus Function Filtering (RE-
SOLVER+FF), RESOLVER plus Coordination-Phrase Filter-
ing (RESOLVER+CPF), and RESOLVER plus both types of fil-
tering (RESOLVER+FF+CPF). Bold indicates the score is sig-
nificantly different from RESOLVER?s score at p < 0.05 us-
ing the chi-squared test with one degree of freedom. RE-
SOLVER+CPF?s F1 on objects is a 28% increase over SSM?s
F1, and a 7% increase over RESOLVER?s F1.
pling of the manually-selected functions used in our
experiment. Automatically discovering such func-
tions from extractions has been addressed in Ana-
Maria Popescu?s dissertation (Popescu, 2007), and
we did not attempt to duplicate this effort in RE-
SOLVER.
Table 3 contains the results of our experiments.
With coordination-phrase filtering, RESOLVER?s F1
is 28% higher than SSM?s on objects, and 6% higher
than RESOLVER?s F1 without filtering. While func-
tion filtering is a promising idea, FF provides a
smaller benefit than CPF on this dataset, and the
129
merges that it prevents are, with a few exceptions,
a subset of the merges prevented by CPF. This is in
part due to the limited number of functions available
in the data. In addition to outperforming FF on this
dataset, CPF has the added advantage that it does not
require additional input, like a set of functions.
7 Conclusion and Future Work
We have shown that the unsupervised and scalable
RESOLVER system is able to find clusters of co-
referential object names in extracted relations with
a precision of 78% and a recall of 68% with the aid
of coordination-phrase filtering, and can find clus-
ters of co-referential relation names with precision
of 90% and recall of 35%. We have demonstrated
significant improvements over using simple similar-
ity metrics for this task by employing a novel prob-
abilistic model of coreference.
In future work, we plan to use RESOLVER on a
much larger data set of over a hundred million as-
sertions, further testing its scalability and its abil-
ity to improve in accuracy given additional data.
We also plan to add techniques for handling mul-
tiple word senses. Finally, to make the probabilistic
model more accurate and easier to use, we plan to
investigate methods for automatically estimating its
parameters from unlabeled data.
Acknowledgements
This research was supported in part by NSF grants
IIS-0535284 and IIS-0312988, DARPA contract
NBCHD030010, ONR grant N00014-05-1-0185 as
well as gifts from Google, and carried out at the Uni-
versity of Washington?s Turing Center. We thank
Doug Downey, Michele Banko, Stef Schoenmack-
ers, Dan Weld, Fei Wu, and the anonymous review-
ers for their helpful comments on previous drafts.
References
M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. In IJCAI.
I. Bhattacharya and L. Getoor. 2005. Relational Clus-
tering for Multi-type Entity Resolution. In 11th ACM
SIGKDD Workshop on Multi Relational Data Mining.
I. Bhattacharya and L. Getoor. 2006. Query-time entity
resolution. In KDD.
W.W. Cohen, P. Ravikumar, and S.E. Fienberg. 2003.
A comparison of string distance metrics for name-
matching tasks. In IIWeb.
I. Dagan, O. Glickman, and B. Magnini. 2006. The PAS-
CAL Recognising Textual Entailment Challenge. Lec-
ture Notes in Computer Science, 3944:177?190.
X. Dong, A.Y. Halevy, and J. Madhavan. 2005. Refer-
ence reconciliation in complex information spaces. In
SIGMOD.
D. Downey, O. Etzioni, and S. Soderland. 2005. A Prob-
abilistic Model of Redundancy in Information Extrac-
tion. In IJCAI.
M. A. Hernandez and S. J. Stolfo. 1995. The
merge/purge problem for large databases. In SIG-
MOD.
D. Hindle. 1990. Noun classification from predicage-
argument structures. In ACL.
A. Kehler. 1997. Probabilistic coreference in informa-
tion extraction. In EMNLP.
S. Lappin and H. J. Leass. 1994. An algorithm for
pronominal anaphora resolution. Computational Lin-
guistics, 20(4):535?561.
D. Lin and P. Pantel. 2001. DIRT ? Discovery of Infer-
ence Rules from Text. In KDD.
A. McCallum and B. Wellner. 2004. Conditional models
of identity uncertainty with application to noun coref-
erence. In NIPS.
A. McCallum, K. Nigam, and L. Ungar. 2000. Efficient
clustering of high-dimensional data sets with applica-
tion to reference matching. In KDD.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In ACL.
Ana-Maria Popescu. 2007. Information Extraction from
Unstructured Web Text. University of Washington.
P. Ravikumar and W. W. Cohen. 2004. A hierarchical
graphical model for record linkage. In UAI.
G. Salton and M.J. McGill. 1983. Introduction to Mod-
ern Information Retrieval. McGraw-Hill.
P. Singla and P. Domingos. 2006. Entity Resolution with
Markov Logic. In ICDM.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
COLING/ACL.
W.E. Winkler. 1999. The state of record linkage and cur-
rent research problems. Technical report, U.S. Bureau
of the Census, Washington, D.C.
130
NAACL HLT Demonstration Program, pages 25?26,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
TextRunner: Open Information Extraction on the Web
Alexander Yates
Michael Cafarella
Michele Banko
Oren Etzioni
University of Washington
Computer Science and Engineering
Box 352350
Seattle, WA 98195-2350
{ayates,banko,hastur,mjc,etzioni,soderlan}@cs.washington.edu
Matthew Broadhead
Stephen Soderland
1 Introduction
Traditional information extraction systems
have focused on satisfying precise, narrow,
pre-specified requests from small, homoge-
neous corpora. In contrast, the TextRunner
system demonstrates a new kind of informa-
tion extraction, called Open Information Ex-
traction (OIE), in which the system makes a
single, data-driven pass over the entire cor-
pus and extracts a large set of relational
tuples, without requiring any human input.
(Banko et al, 2007) TextRunner is a fully-
implemented, highly scalable example of OIE.
TextRunner?s extractions are indexed, al-
lowing a fast query mechanism.
Our first public demonstration of the Text-
Runner system shows the results of perform-
ing OIE on a set of 117 million web pages. It
demonstrates the power of TextRunner in
terms of the raw number of facts it has ex-
tracted, as well as its precision using our novel
assessment mechanism. And it shows the abil-
ity to automatically determine synonymous re-
lations and objects using large sets of extrac-
tions. We have built a fast user interface for
querying the results.
2 Previous Work
The bulk of previous information extraction
work uses hand-labeled data or hand-crafted
patterns to enable relation-specific extraction
(e.g., (Culotta et al, 2006)). OIE seeks to
avoid these requirements for human input.
Shinyama and Sekine (Shinyama and
Sekine, 2006) describe an approach to ?un-
restricted relation discovery? that does away
with many of the requirements for human in-
put. However, it requires clustering of the doc-
uments used for extraction, and thus scales in
quadratic time in the number of documents.
It does not scale to the size of the Web.
For a full discussion of previous work, please
see (Banko et al, 2007), or see (Yates and Et-
zioni, 2007) for work relating to synonym res-
olution.
3 Open IE in TextRunner
OIE presents significant new challenges for in-
formation extraction systems, including
Automation of relation extraction, which in
traditional information extraction uses hand-
labeled inputs.
Corpus Heterogeneity on the Web, which
makes tools like parsers and named-entity tag-
gers less accurate because the corpus is differ-
ent from the data used to train the tools.
Scalability and efficiency of the system.
Open IE systems are effectively restricted to
a single, fast pass over the data so that they
can scale to huge document collections.
In response to these challenges, Text-
Runner includes several novel components,
which we now summarize (see (Banko et al,
2007) for details).
1. Single Pass Extractor
The TextRunner extractor makes a sin-
gle pass over all documents, tagging sen-
tences with part-of-speech tags and noun-
phrase chunks as it goes. For each pair of noun
phrases that are not too far apart, and subject
to several other constraints, it applies a clas-
sifier described below to determine whether or
not to extract a relationship. If the classifier
25
deems the relationship trustworthy, a tuple of
the form t = (ei, rj , ek) is extracted, where
ei, ek are entities and rj is the relation between
them. For example, TextRunner might ex-
tract the tuple (Edison, invented, light bulbs).
On our test corpus (a 9 million document sub-
set of our full corpus), it took less than 68
CPU hours to process the 133 million sen-
tences. The process is easily parallelized, and
took only 4 hours to run on our cluster.
2. Self-Supervised Classifier
While full parsing is too expensive to apply to
the Web, we use a parser to generate training
examples for extraction. Using several heuris-
tic constraints, we automatically label a set
of parsed sentences as trustworthy or untrust-
worthy extractions (positive and negative ex-
amples, respectively). The classifier is trained
on these examples, using features such as the
part of speech tags on the words in the re-
lation. The classifier is then able to decide
whether a sequence of POS-tagged words is a
correct extraction with high accuracy.
3. Synonym Resolution
Because TextRunner has no pre-defined re-
lations, it may extract many different strings
representing the same relation. Also, as with
all information extraction systems, it can ex-
tract multiple names for the same object. The
Resolver system performs an unsupervised
clustering of TextRunner?s extractions to
create sets of synonymous entities and rela-
tions. Resolver uses a novel, unsupervised
probabilistic model to determine the probabil-
ity that any pair of strings is co-referential,
given the tuples that each string was extracted
with. (Yates and Etzioni, 2007)
4. Query Interface
TextRunner builds an inverted index of
the extracted tuples, and spreads it across a
cluster of machines. This architecture sup-
ports fast, interactive, and powerful relational
queries. Users may enter words in a relation or
entity, and TextRunner quickly returns the
entire set of extractions matching the query.
For example, a query for ?Newton? will return
tuples like (Newton, invented, calculus). Users
may opt to query for all tuples matching syn-
onyms of the keyword input, and may also opt
to merge all tuples returned by a query into
sets of tuples that are deemed synonymous.
4 Experimental Results
On our test corpus of 9 million Web doc-
uments, TextRunner extracted 7.8 million
well-formed tuples. On a randomly selected
subset of 400 tuples, 80.4% were deemed cor-
rect by human reviewers.
We performed a head-to-head compari-
son with a state-of-the-art traditional in-
formation extraction system, called Know-
ItAll. (Etzioni et al, 2005) On a set of ten
high-frequency relations, TextRunner found
nearly as many correct extractions as Know-
ItAll (11,631 to 11,476), while reducing the
error rate of KnowItAll by 33% (18% to
12%).
Acknowledgements
This research was supported in part by NSF
grants IIS-0535284 and IIS-0312988, DARPA
contract NBCHD030010, ONR grant N00014-
05-1-0185 as well as gifts from Google, and
carried out at the University of Washington?s
Turing Center.
References
M. Banko, M. J. Cafarella, S. Soderland,
M. Broadhead, and O. Etzioni. 2007. Open In-
formation Extraction from the Web. In IJCAI.
A. Culotta, A. McCallum, and J. Betz. 2006. Inte-
grating Probabilistic Extraction Models and Re-
lational Data Mining to Discover Relations and
Patterns in Text. In HLT-NAACL.
O. Etzioni, M. Cafarella, D. Downey, S. Kok,
A. Popescu, T. Shaked, S. Soderland, D. Weld,
and A. Yates. 2005. Unsupervised Named-
Entity Extraction from the Web: An Experi-
mental Study. Artificial Intelligence, 165(1):91?
134.
Y. Shinyama and S. Sekine. 2006. Preemptive
Information Extraction Using Unrestricted Re-
lation Discovery. In HLT-NAACL.
A. Yates and O. Etzioni. 2007. Unsupervised Res-
olution of Objects and Relations on the Web. In
NAACL-HLT.
26
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 27?34,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Detecting Parser Errors Using Web-based Semantic Filters
Alexander Yates Stefan Schoenmackers
University of Washington
Computer Science and Engineering
Box 352350
Seattle, WA 98195-2350
{ayates, stef, etzioni} @cs.washington.edu
Oren Etzioni
Abstract
NLP systems for tasks such as question
answering and information extraction typ-
ically rely on statistical parsers. But the ef-
ficacy of such parsers can be surprisingly
low, particularly for sentences drawn from
heterogeneous corpora such as the Web.
We have observed that incorrect parses of-
ten result in wildly implausible semantic
interpretations of sentences, which can be
detected automatically using semantic in-
formation obtained from the Web.
Based on this observation, we introduce
Web-based semantic filtering?a novel,
domain-independent method for automat-
ically detecting and discarding incorrect
parses. We measure the effectiveness of
our filtering system, called WOODWARD,
on two test collections. On a set of TREC
questions, it reduces error by 67%. On
a set of more complex Penn Treebank
sentences, the reduction in error rate was
20%.
1 Introduction
Semantic processing of text in applications such
as question answering or information extraction
frequently relies on statistical parsers. Unfortu-
nately, the efficacy of state-of-the-art parsers can
be disappointingly low. For example, we found
that the Collins parser correctly parsed just 42%
of the list and factoid questions from TREC 2004
(that is, 42% of the parses had 100% precision and
100% recall on labeled constituents). Similarly,
this parser produced 45% correct parses on a sub-
set of 100 sentences from section 23 of the Penn
Treebank.
Although statistical parsers continue to improve
their efficacy over time, progress is slow, par-
ticularly for Web applications where training the
parsers on a ?representative? corpus of hand-
tagged sentences is not an option. Because of the
heterogeneous nature of text on the Web, such a
corpus would be exceedingly difficult to generate.
In response, this paper investigates the possibil-
ity of detecting parser errors by using semantic in-
formation obtained from the Web. Our fundamen-
tal hypothesis is that incorrect parses often result
in wildly implausible semantic interpretations of
sentences, which can be detected automatically in
certain circumstances. Consider, for example, the
following sentence from the Wall Street Journal:
?That compares with per-share earnings from con-
tinuing operations of 69 cents.? The Collins parser
yields a parse that attaches ?of 69 cents? to ?op-
erations,? rather than ?earnings.? By computing
the mutual information between ?operations? and
?cents? on the Web, we can detect that this attach-
ment is unlikely to be correct.
Our WOODWARD system detects parser errors
as follows. First, it maps the tree produced by a
parser to a relational conjunction (RC), a logic-
based representation language that we describe in
Section 2.1. Second, WOODWARD employs four
distinct methods for analyzing whether a conjunct
in the RC is likely to be ?reasonable? as described
in Section 2.
Our approach makes several assumptions. First,
if the sentence is absurd to begin with, then a cor-
rect parse could be deemed incorrect. Second, we
require a corpus whose content overlaps at least in
part with the content of the sentences to be parsed.
Otherwise, much of our semantic analysis is im-
possible.
In applications such as Web-based question an-
swering, these assumptions are quite natural. The
27
questions are about topics that are covered exten-
sively on the Web, and we can assume that most
questions link verbs to nouns in reasonable com-
binations. Likewise, when using parsing for infor-
mation extraction, we would expect our assump-
tions to hold as well.
Our contributions are as follows:
1. We introduce Web-based semantic filtering?
a novel, domain-independent method for de-
tecting and discarding incorrect parses.
2. We describe four techniques for analyzing
relational conjuncts using semantic informa-
tion obtained from the Web, and assess their
efficacy both separately and in combination.
3. We find that WOODWARD can filter good
parses from bad on TREC 2004 questions for
a reduction of 67% in error rate. On a harder
set of sentences from the Penn Treebank, the
reduction in error rate is 20%.
The remainder of this paper is organized as fol-
lows. We give an overview of related work in Sec-
tion 1.1. Section 2 describes semantic filtering, in-
cluding our RC representation and the four Web-
based filters that constitute the WOODWARD sys-
tem. Section 3 presents our experiments and re-
sults, and section 4 concludes and gives ideas for
future work.
1.1 Related Work
The problem of detecting parse errors is most sim-
ilar to the idea of parse reranking. Collins (2000)
describes statistical techniques for reranking alter-
native parses for a sentence. Implicitly, a rerank-
ing method detects parser errors, in that if the
reranking method picks a new parse over the orig-
inal one, it is classifying the original one as less
likely to be correct. Collins uses syntactic and lex-
ical features and trains on the Penn Treebank; in
contrast, WOODWARD uses semantic features de-
rived from the web. See section 3 for a comparison
of our results with Collins?.
Several systems produce a semantic interpreta-
tion of a sentence on top of a parser. For example,
Bos et al (2004) build semantic representations
from the parse derivations of a CCG parser, and
the English Resource Grammar (ERG) (Toutanova
et al, 2005) provides a semantic representation us-
ing minimal recursion semantics. Toutanova et al
also include semantic features in their parse se-
lection mechanism, although it is mostly syntax-
driven. The ERG is a hand-built grammar and thus
does not have the same coverage as the grammar
we use. We also use the semantic interpretations
in a novel way, checking them against semantic
information on the Web to decide if they are plau-
sible.
NLP literature is replete with examples of sys-
tems that produce semantic interpretations and
use semantics to improve understanding. Sev-
eral systems in the 1970s and 1980s used hand-
built augmented transition networks or semantic
networks to prune bad semantic interpretations.
More recently, people have tried incorporating
large lexical and semantic resources like WordNet,
FrameNet, and PropBank into the disambiguation
process. Allen (1995) provides an overview of
some of this work and contains many references.
Our work focuses on using statistical techniques
over large corpora, reducing the need for hand-
built resources and making the system more robust
to changes in domain.
Numerous systems, including Question-
Answering systems like MULDER (Kwok et
al., 2001), PiQASso (Attardi et al, 2001), and
Moldovan et al?s QA system (2003), use parsing
technology as a key component in their analysis
of sentences. In part to overcome incorrect parses,
Moldovan et al?s QA system requires a complex
set of relaxation techniques. These systems
would greatly benefit from knowing when parses
are correct or incorrect. Our system is the first
to suggest using the output of a QA system to
classify the input parse as good or bad.
Several researchers have used pointwise mu-
tual information (PMI) over the Web to help make
syntactic and semantic judgments in NLP tasks.
Volk (2001) uses PMI to resolve preposition at-
tachments in German. Lapata and Keller (2005)
use web counts to resolve preposition attachments,
compound noun interpretation, and noun count-
ability detection, among other things. And Mark-
ert et al (2003) use PMI to resolve certain types of
anaphora. We use PMI as just one of several tech-
niques for acquiring information from the Web.
2 Semantic Filtering
This section describes semantic filtering as imple-
mented in the WOODWARD system. WOODWARD
consists of two components: a semantic interpreter
that takes a parse tree and converts it to a conjunc-
tion of first-order predicates, and a sequence of
four increasingly sophisticated methods that check
semantic plausibility of conjuncts on the Web. Be-
low, we describe each component in turn.
28
1. What(NP1) ? are(VP1, NP1, NP2) ? states(NP2) ? producing(VP2, NP2, NP3) ? oil(NP3) ? in(PP1, NP2, U.S.)
2. What(NP1) ? states(NP2) ? producing(VP1, NP3, NP2, NP1) ? oil(NP3) ? in(PP1, NP2, U.S.)
Figure 2: Example relational conjunctions. The first RC is the correct one for the sentence ?What are oil producing
states in the U.S.?? The second is the RC derived from the Collins parse in Figure 1. Differences between the two RCs
appear in bold.

	 



		 
	 

Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1465?1474,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Quantifier Scope Disambiguation Using Extracted Pragmatic Knowledge:
Preliminary Results
Prakash Srinivasan
Temple University
1805 N. Broad St.
Wachman Hall 324
Philadelphia, PA 19122
prakash.srinivasan@temple.edu
Alexander Yates
Temple University
1805 N. Broad St.
Wachman Hall 324
Philadelphia, PA 19122
yates@temple.edu
Abstract
It is well known that pragmatic knowl-
edge is useful and necessary in many dif-
ficult language processing tasks, but be-
cause this knowledge is difficult to acquire
and process automatically, it is rarely used.
We present an open information extrac-
tion technique for automatically extracting
a particular kind of pragmatic knowledge
from text, and we show how to integrate
the knowledge into a Markov Logic Net-
work model for quantifier scope disam-
biguation. Our model improves quantifier
scope judgments in experiments.
1 Introduction
It has long been a goal of the natural language
processing (NLP) community to be able to inter-
pret language utterances into logical representa-
tions of their meaning. Quantifier scope ambigu-
ity has been recognized as one particularly chal-
lenging aspect of this problem. For example, the
following sentence has two possible readings, de-
pending on the scope of its quantifiers:
Every boy wants a dog.
One reading of this sentence is that there exists a
single dog in the world which all boys want. The
second, and usually preferred, reading is that the
sentence is describing a separate ?wanting? rela-
tion for each boy, and that the dog in question is
a function of the boy who wants it. In this read-
ing, there may be as many different dogs as boys,
although it leaves open the possibility that several
of the boys want the same dog. In logic, these two
readings can be represented as follows:
1. ?
d?Dogs
?
b?Boys
wants(b, d)
2. ?
b?Boys
?
d?Dogs
wants(b, d)
The readings differ only in the order of the quanti-
fiers. The quantifier that comes first in each ex-
pression is said to have wide scope; the second
quantifier has narrow scope.
Linguists and NLP researchers have come up
with several theories and mechanisms for automat-
ically determining the scope of quantified linguis-
tic expressions. Despite a long history of proposed
solutions, however, researchers have for the most
part abandoned this task as hopeless because of
?overwhelming evidence suggesting that quanti-
fier scope is a phenomenon that must be treated at
the pragmatic level? (Saba and Corriveau, 2001).
For example, in active voice clauses, the quantifier
for the subject noun is usually preferred for wide
scope over the quantifier of the predicate noun
(Kurtzman and MacDonald, 1993). But such pref-
erences can easily be overruled by world knowl-
edge:
A doctor lives in every city.
1. ?
d?Docs
?
c?Cities
lives in(d, c)
(A single doctor lives in all cities.)
2. ?
c?Cities
?
d?Docs
lives in(d, c)
(Each city has a different doctor living there.)
Syntactic preferences would normally indicate
that reading 1 is better, but in this particular case
common-sense knowledge of the world overrules
that preference and makes reading 2 far more
probable.
Open-domain pragmatic knowledge is usually
not available to language processing systems, but
that is beginning to change. Recent research in
open information extraction (Banko and Etzioni,
2008; Davidov and Rappaport, 2008) has shown
that we can extract large amounts of relational data
from open-domain text with high accuracy. Here,
we show how we can connect the two fields, by ex-
tracting a targeted form of pragmatic knowledge
for use in quantifier scope disambiguation. Our
contributions are:
1) We build an extraction mechanism for extract-
ing pragmatic knowledge about relations. In par-
1465
ticular, we extract knowledge about the expected
sizes of the sets of objects that participate in the
relations. The task of identifying functional re-
lationships is a subtask of our extraction problem
that has received recent attention in the literature
(Ritter et al, 2008).
2) We devise a novel probabilistic model in the
Markov Logic Network framework for reasoning
over possible readings of sentences that involve
quantifier scope ambiguities. The model is able
to assign a probability that a particular reading is
plausible, given the pragmatic knowledge we ex-
tract.
3) We provide an empirical demonstration that our
system is able to resolve quantifier scope ambigu-
ities in cases where the syntactic and lexical fea-
tures used by previous systems are of no help.
The remainder of this paper is organized as fol-
lows. The next section describes previous work.
Section 3 shows how the problem can be formu-
lated as a task of assigning probabilities to possi-
ble worlds, and that the crucial difference between
them has to do with the number of objects partic-
ipating in individual relationships. Section 4 dis-
cusses our techniques for extracting the pragmatic
knowledge that allows us to make judgments about
quantifier scope. Section 5 presents our proba-
bilistic model for resolving scope ambiguities. We
present an empirical study in section 6, and section
7 concludes and suggests items for future work.
2 Related Work
Quantifier scope disambiguation has received at-
tention in linguistics and computational linguis-
tics since at least the 1970s. Montague (1973)
gave a seminal treatment of quantifier ambiguities,
and argued that a particular syntax-based mech-
anism known as ?quantifying-in? could resolve
scope ambiguities. Since then, most work on dis-
ambiguation has focused on syntactic clues for de-
termining which readings of an ambiguous state-
ment are possible, and of the set of possible read-
ings, which ones are preferred (Van Lehn, 1978;
Hobbs and Shieber, 1987; Poesio, 1993a; Hurum,
1988; Moran, 1988). For instance, one linguis-
tic study (Kurtzman and MacDonald, 1993) deter-
mined that in active voice sentences where quan-
tifiers in the subject and object give rise to scope
ambiguity, there is a preference for the reading in
which the subject quantifier has wide scope ? the
direct reading is acceptable 70-80% of the time,
whereas the indirect reading is acceptable 30-40%
of the time. Sentences that are similar in all re-
spects except that they are passive voice have no
such preference. Nevertheless, in these studies
both readings are often quite plausible. In addition
to syntactic clues, other studies have noted that
the choice of quantifier has a significant effect on
scope disambiguation (e.g., ?each? has a greater
tendency for wide scope than ?every?) (Van Lehn,
1978; Alshawi, 1990). Most authors have noted
that both syntactic and lexical evidence fall short
of a full solution, and that pragmatic knowledge
(knowledge about the world) is necessary for this
task (Van Lehn, 1978; Saba and Corriveau, 1997;
Moran, 1988). Saba and Corriveau (2001) recently
proposed a test for quantifier scope disambigua-
tion using pragmatic knowledge. However, they
do not show how to extract the necessary infor-
mation, nor do they implement or evaluate their
proposed test.
Due to the difficulty of the problem, several
authors have devised techniques for ?underspec-
ified? logical representations that can efficiently
store multiple ambiguous readings, and they de-
vise techniques for automated reasoning using un-
derspecified representations (Reyle, 1995; Late-
cki, 1992; Poesio, 1993b). Others (Hobbs and
Shieber, 1987; Park, 1988) have devised compu-
tational mechanisms for generating all of the pos-
sible readings of statements exhibiting quantifier
ambiguity, especially in cases involving more than
two quantifiers.
Detecting functions in extracted relational data
has been studied in several contexts. Ritter et
al.(2008) use knowledge of functions to determine
when two extracted relationships contradict one
another. Knowledge of functions has also been
important in finding synonyms (Yates and Etzioni,
2009) and in review mining (Popescu, 2007). We
extend this work by extracting not just a binary
determination of whether a relation is functional,
but a distribution over the expected number of ar-
guments for that relation. Our technique also dif-
fers from previous work based on extracted rela-
tionships between named entities. We leverage
domain-independent extraction patterns involving
numeric phrases, as discussed below; our tech-
nique is complementary to existing approaches
and could in fact be combined with them for even
greater accuracy. Finally, we apply the extracted
knowledge in a novel way to quantifier scope dis-
ambiguation.
Our work is similar in spirit to several recent
1466
projects that use semantic reasoning over extracted
knowledge for a novel approach to well-known
tasks. For example, Schoenmackers et al(2008)
have recently used extracted knowledge for the
task of predicting whether a new extracted fact is
correct. Yates et al(2006) use extracted knowl-
edge to determine whether a parse of a sentence
has a plausible semantic interpretation. We extend
this new line of attack to a hard problem in lan-
guage understanding.
3 Possible Worlds Framework
We now present a framework for reasoning about
quantifier scope ambiguities, and for choosing
among possible readings based on pragmatic
knowledge (or world knowledge ? we use the
terms interchangeably). We first present a formal
description of the quantifier scope disambiguation
(QSD) problem. We then describe the crucial dif-
ferences between the ?possible worlds? evoked by
different readings of an ambiguous statement.
3.1 Representation of Readings
We follow Copestake et al (2005), among oth-
ers, in representing quantifiers as modal oper-
ators with three arguments: a variable name
for the variable being quantified; a logical for-
mula, called the restriction, which defines the set
of objects over which the variable may range;
and a second logical formula, called the body,
which defines the expression in which the quan-
tified variable takes part. For example, we
represent the sentence ?Every dog barks? as:
every(x,dog(x),barks(x)).
For the sake of clarity and convenience, we re-
strict our attention to a common syntactic form
of sentences, where the semantic representation
is relatively well-understood: active-voice English
sentences in which the subject noun phrase is
quantified, and a noun phrase in the predicate (ei-
ther an object of the verb, or an object of a prepo-
sition attached to the verb) is also quantified. For
a sentence with the following structure, in which
p
i
and q
j
represent predicates introduced by mod-
ifiers like adjectives and prepositional phrases,
(
S
(
NP
(
DET
Q
1
)(
N
[p
1
, . . . , p
n
]C
1
))
(
V P
(
V
R)(
NP
(
DET
Q
2
)(
N
[q
1
, . . . , q
m
]C
2
))
we can represent the two possible readings of the
sentence as:
direct reading:
Q
1
(x,C
1
(x) ? p
1
(x) ? . . . ? p
n
(x),
Q
2
(y, C
2
(y) ? q
1
(y) ? . . . ? q
m
(y),
R(x, y)))
(1)
indirect reading:
Q
2
(y, C
2
(y) ? q
1
(y) ? . . . ? q
m
(y),
Q
1
(x,C
1
(x) ? p
1
(x) ? . . . ? p
n
(x),
R(x, y)))
(2)
By making the restriction to this type of sen-
tences, we can isolate the effects of pragmatics on
scope disambiguation decisions from the effects
of syntax, since all test cases have essentially the
same syntax. As we show below, for certain types
of relations, the preference for interpretations may
be drastically different from the general preference
for the direct reading, even though the syntax of
the sentences we investigate matches the syntax
studied by Kurtzman and MacDonald (1993).
3.2 Readings, Possible Worlds, and World
Knowledge
The different logical forms for the direct and indi-
rect readings describe different ?possible worlds.?
For instance, the direct reading of ?A doctor lives
in every city? describes worlds in which there is a
single doctor who manages to reside in each city
of the world simultaneously. This reading is ?pos-
sible? in the sense that it does not contradict itself.
In logical terms, if ? represents the direct reading
of this sentence, ? 0 ?. Using some imagination
one could devise a scenario, perhaps in an online
game world, that satisfies ?.
Nevertheless, the indirect reading is strongly
preferred for this statement in the absence of any
context that indicates an abnormal world. The
indirect reading ?? describes worlds where every
city is inhabited by some doctor, but potentially a
different doctor per city. Using pragmatic knowl-
edge, the reader can easily deduce that this logical
statement is a much more likely reading than ?.
Let B represent the reader?s pragmatic knowledge,
including facts like ?People don?t simultaneously
live in more than one city,? and, ?There are at least
hundreds of cities in the world.? The reader can
easily deduce that B  ??. We now turn to meth-
ods for extracting the necessary pragmatic knowl-
edge B from text.
1467
4 Extraction Techniques to Support QSD
Decisions
Saba and Corriveau (2001) point out that there is
a restricted form of pragmatic knowledge that can
be used in many instances of QSD. Consider the
facts that were used above to determine that ??
is preferable to ?. The facts fall into two basic
categories of knowledge: 1) the size of class C
(e.g., how many cities are there?), and 2) the ex-
pected number of Y participants in a relationship
R, given that there is exactly 1 X participant (e.g.,
how many cities does 1 doctor live in?). In both
cases, we are concerned with extracting sizes of
sets.
Previous extraction systems have attempted to
estimate set sizes based on extracted named en-
tities. Downey et al (2005)estimate the size of
classes based on the number of named-entities ex-
tracted for the class. As far as we are aware, find-
ing the expected size of an argument set for a rela-
tion is a novel task for information extraction, but
several researchers (Ritter et al, 2008; Yates and
Etzioni, 2009) have investigated the special case
of detecting functional relations ? those relations
where the expected size of the Y argument set is
precisely 1. As with class size extraction, they
use extractions involving named-entity arguments
to find functional relations.
Approaches that depend on named-entity ex-
tractions have several disadvantages: they must
find a large set of named-entities for every set,
which can be time-consuming and difficult. Also,
many classes, like ?trees? and ?hot dogs,? have no
or very few named instances, but many un-named
instances, so approaches based on named entities
have little hope. In fact, besides classes like peo-
ple, locations, and organizations (and their sub-
classes), there are few classes that have a large
number of named instances. For classes that do
have named instances, synonymy, polysemy, and
extraction errors are common problems that can
all affect estimates of size (Ritter et al, 2008).
Rather than indirectly determining set sizes
from extracted instances, our system directly ex-
tracts estimates of set sizes. It uses numeric
phrases, like ?two trees,? ?hundreds of students,?
or ?billions of stars,? to associate numeric values
with sets. Table 1 lists the numeric phrases we use.
Currently, we use only numeric phrases with ex-
plicit values or ranges of values, but it may be pos-
sible to increase the recall of our extraction tech-
nique by incorporating more approximate phrases
Numeric Phrase Value
no | none | zero 0
a | one | this | the 1
two 2
.
.
.
.
.
.
one hundred | a hundred 100
.
.
.
.
.
.
hundreds of 100
thousands of 1,000
tens of thousands of 10,000
.
.
.
.
.
.
Table 1: Numeric phrases used in our extraction pat-
terns. For the word ?the?, we require that it be followed di-
rectly by a singular noun, to try to weed out plural usages.
like ?several,? ?many,? or even bare plurals. We
do not match numbers expressed in digits (e.g.,
1234) because we found that they produced too
many noisy extractions, such as dates and times.
For words like ?hundreds,? we set the value of the
word to be the lower limit (i.e., 100). This gives
a conservative estimate of the value, but our tech-
niques described below can help to compensate for
this bias.
Table 2 lists examples of the hand-crafted,
domain-independent extraction patterns we use.
Our extraction patterns generate two types of ex-
tractions, one for classes and one for relations. For
classes, each extraction E consists of a class name
E
c
and a number E
n
indicating the size of some
subset S ? E
c
. For instance, the 4gram ?hun-
dreds of students are? matches our first pattern.
The numeric phrase ?hundreds of? here indicates
that some subset S ? E
c
= students has a size in
the hundreds. After processing a large corpus, our
system can determine a probability distribution for
the size of a class given by:
P
C
(size(C) = N) =
|{E | E
c
= C ? E
n
= N}|
|{E | E
c
= C}|
In practice, we only include the largest 20% of the
numbers N in the set of extractions for a class to
estimate that class?s size.
The second type of extraction we get from our
patterns are relational extractions. Each relational
extraction F consists of a relation name F
r
, and
possibly names for the classes of its two argu-
ments, F
c1
, F
c2
. In addition, the extraction con-
tains values for the size of both arguments, F
n1
1468
Pattern Extraction
<numeric> <word>+ (of | are | have) E
c
= <word>+, E
n
= value(<numeric>)
(I | he | she) <word>+ <numeric> <noun> F
r
= <word>+, F
c1
= people, F
c2
= <noun>,
F
n1
= 1, F
n2
= value(<numeric>)
it is pastParticiple(<verb>) by <numeric> F
r
= <verb>,F
c2
= thing,
F
n1
= value(<numeric>), F
n2
= 1
is the <word> of <numeric> F
r
= is the <word> of,
F
n1
= 1, F
n2
= value(<numeric>)
Table 2: Sample extraction patterns for discovering classes (E
c
) and their sizes (E
n
), or relations (F
r
) and the expected
set size of their arguments (F
n1
and F
n2
).
and F
n2
respectively. For example, the fragment
?she visited four countries? matches the second
pattern in Table 2, with F
r
= visited, F
n1
= 1,
and F
n2
= 4. Note that in our extraction patterns,
one of of the arguments is always constrained to be
a singleton set, like ?he? or ?it.? This restriction
allows us to avoid quantifier scope ambiguity in
the extraction process: if we extracted phrases like
?Two men married two women,? it would be un-
clear which quantifier has wide scope, and there-
fore how many men and women are participating
in each ?married? relationship. By using singular
pronouns, we avoid this confusion; in almost all
cases, these pronouns have wide scope, and indi-
cate a single element.1
Based on these extractions, our system de-
termines two distributions for each relation:
P
Left
R
(n) and PRight
R
(n). The PLeft
R
distribution
represents the probability that the left argument of
R is a set of size n, given that the right argument
is a singleton set, and likewise for PRight
R
. We de-
termine the distributions from the extractions by
maximum likelihood estimation:
P
Left
R
(n) =
|{F | F
r
= R,F
n1
= n, F
n2
= 1}|
|{F | F
r
= R,F
n2
= 1}|
P
Right
R
(n) =
|{F | F
r
= R,F
n2
= n, F
n1
= 1}|
|{F | F
r
= R,F
n1
= 1}|
For example, for the relation is the father
of, we might see the fragment ?he is the father
of two children? far more often than ?he is the
father of twenty children.? PRightis the father of
would therefore have a relatively low probability
for n = 20. As one would expect, the relation
1An example of an exception to this rule from our data set
is the sentence ?It is worn by millions of women.? Here, ?it?
refers to a class of items such as a brand, and thus may refer
to a different item for each of the ?millions of women.?
visited appears more often with ?twenty,? and
the relation married never does. Their PRight
distributions are comparatively higher and lower,
respectively than the one for is the father
of at n = 20.
In practice, we create histograms of the ex-
tracted counts for both our E and F extractions,
and our probability distributions are really dis-
tributions over the buckets in these histograms,
rather than over all possible set sizes. To help
combat sparse counts for large numeric values, we
use buckets of exponentially increasing width for
larger numeric values. Thus between n = 0 and
10, buckets have size 1, between 10 and 100 they
have size 10, and so on.
We also create distributions in the same way for
relations together with their extracted argument
classes. Since counts for these extractions tend to
be much more sparse, we interpolate these distri-
butions with the distribution for just the relation,
and with the distribution for the relation and just
one class. We use equal weights for all interpo-
lated distributions.
5 A Probabilistic Model for Quantifier
Scope Disambiguation
QSD requires reasoning about different possible
states of the world. This involves logical reason-
ing, since the direct and indirect readings differ in
the number of objects that exist in models satis-
fying each reading, and the number of relation-
ships between those objects. QSD also involves
probabilistic reasoning, since none of the extracted
knowledge is certain. We leverage recent work on
Markov Logic Networks (MLNs) (Richardson and
Domingos, 2006) to incorporate both types of rea-
soning into our technique for QSD. We next briefly
review MLNs, before describing our model and
1469
methods for training it.
5.1 Markov Logic Networks
Syntactically, an MLN consists of a set of first-
order logical formulas F and a real-valued weight
w
F
for each F ? F. Semantically, an MLN
defines a probability distribution over possible
groundings of the logical formulas. That is, if U
denotes the set of all objects in the universe, andG
denotes the set of all possible ways to ground ev-
ery F ? F (i.e., substitute an element from U for
every variable in F ), then an MLN defines a distri-
bution over truth assignments to the grounded for-
mulas G ? G. Let I denote the set of all possible
interpretations of G ? that is, each I ? I assigns
true or false to every G ? G. The probabil-
ity of a particular interpretation I according to the
MLN is given by :
P (I) =
1
Z
exp
(
?
F?F
w
F
? n(F, I)
)
Z =
?
I?I
exp
(
?
F?F
w
F
? n(F, I)
)
where n(F, I) gives the number of groundings of
F that are true in interpretation I .
The equation above provides an expression for
P (I) when U , or at least the size of U , is known
and fixed. When we are interpreting expressions
like ?every city? or ?every doctor?, however, we
require extracted knowledge to inform the system
of the correct number of ?city? or ?doctor? objects.
Since our extractions are uncertain, they provide a
distribution P (|U | = n) for the size of a class.
Using P (|U |), we can still calculate P (I), even
without knowing the exact size of U :
P (I) =
?
n
P (|U | = n)P (I | |U | = n)
5.2 MLN Classifier for QSD
Let Q be a QSD problem, consisting of a rela-
tion Q
r
, a class for the first argument of the re-
lation Q
c1
, a class for the second argument Q
c2
,
and quantifiiers Q
q1
, Q
q2
for each argument. We
construct an MLN model for Q using the follow-
ing logical formulas:
1) Clustering: We allow members of each class
to belong to clusters denoted by ?, but each ele-
ment can belong to no more than one cluster. This
is represented by the following formula, which has
infinite weight.
?
x?Q
c1
?Q
c2
,?,?
?
x ? ? ? x ? ?
?
? ? = ?
?
2) Relation between clusters: Every cluster of
class 1 elements must participate in the relation
Q
r
with exactly one cluster of class 2 elements,
and vice versa. We represent this participation in
Q
r
with a series of logical relations R
m,n
, each of
which indicates that a cluster of size m is partic-
ipating in Q
r
with a cluster of size n. We use a
set of formulas for each setting of m and n, each
having infinite weight.
?
??Q
c1
?!
?
?
?Q
c2
,m,n
R
m,n
(?, ?
?
)
?
?
?
?Q
c2
?!
??Q
c1
,m,n
R
m,n
(?, ?
?
)
?
?,?
?
R
m,n
(?, ?
?
) ? |?| = m ? |?
?
| = n
3) Prefer relations between clusters of the ap-
propriate size: We include a set of formulas with
finite weight that express the preference for a par-
ticular relation to have arguments of a certain size.
There is a separate formula for each setting of m
and n, with a separate weight w
m,n
for each.
?
?,?
?
R
m,n
(?, ?
?
)
This formula does most of the work of our clas-
sifier. For a given relation, such as the lives
in(Person, City) relation, we can set the
weights w
m,n
so that the model prefers worlds
where each person lives in just one place. For in-
stance, we can set the weight w
1,1
relatively high,
so that the model is more likely to make clusters of
size 1, which then participate in the R
1,1
relation.
We describe how we choose the w
m,n
weights
below, but first we explain how to incorporate the
quantifiers Q
q1
and Q
q2
into the model. Unfortu-
nately, every natural language quantifier has dif-
ferent semantics (Barwise and Cooper, 1981), and
thus they affect our model in different ways. Here,
we restrict our attention to the two common quan-
tifiers ?a? and ?every?, but note that the MLN
framework is a powerful tool for incorporating
the logical semantics and statistical preferences of
other quantifiers.
For the quantifier ?a?, we require that the rela-
tion have no argument clusters with size more than
1 for that class. Thus if Q
q1
= ?a?, we restrict
R
m,n
to R
1,n
, and vice versa if Q
q2
= ?a.? Fur-
thermore, we require that at least one element of
the class belong to a cluster: ?
x,?
x ? ? has infi-
nite weight. For ?every,? we require that every el-
ement of the class that ?every? modifies to be part
1470
of some cluster. To effect this change, we simply
put an infinite weight on the formula ?
x
?
?
x ? ?.
Our MLN model is general in the sense that
for any QSD problem Q, it can determine prob-
abilities for any possible world corresponding to
a reading of Q. For our purposes, we are primar-
ily interested in the direct and indirect readings of
any Q involving ?a? and ?every.? To predict the
correct reading for a given Q, we simply check to
see which has the higher probability according to
our MLN model.
5.3 Parameter Estimation
Our MLN model for QSD requires settings for
the w
m,n
parameters for each QSD problem Q.
The standard approach to this problem would be
to estimate these parameters from labeled train-
ing data. We reject the standard supervised frame-
work, however, because each distinct relation Q
r
requires different settings of the parameters, and
therefore a standard supervised approach would
require manually labeled training data for every
relation Q
r
.
A second approach that is made possible by
our extraction technique is to set the parameters
using the extracted distributions. We tried this
approach by setting w
1,n
= logP
Right
Q
r
(n) and
w
m,1
= logP
Left
Q
r
(m); since we only consider
sentences containing the quantifier ?a?, one of m
and n will always be 1. Unfortunately, in our ex-
periments we found that this setting for the param-
eters often gave far too little weight for large val-
ues of m and n, and as a consequence, the classi-
fier would systematically judge one reading to be
more likely than another.
To counteract this problem, we take a hybrid ap-
proach to parameter estimation, informed by both
labeled training data and the extracted distribu-
tions. Crucially, our approach, which we call ZIPF
FLATTENING, has only two parameters that need
to be trained using a supervised approach, and
these parameters do not depend on the relation R.
Thus, the approach minimizes the amount of train-
ing data we need to a practical level.
ZIPF FLATTENING works by correcting the P
R
distributions to give higher weight to larger values
of m and n. First, we estimate a Zipf distribution
from the raw extracted counts for each argument
of relation R. To fit a Zipf curve, we use least-
squares linear regression on the log-log plot of the
extracted counts to find parameters z
R
and c
R
such
that
log (count) = z
R
? log (argSize) + c
R
? count = e
c
R
? argSize
z
R
We can perform this part automatically, using only
the extraction data and no manually labeled train-
ing data, for every relation. However, the fitted
Zipf distribution needs to be corrected for the sys-
tematic bias in the extracted counts. To do this, we
introduce two parameters, ?
1
and ?
2
, that we use
to scale back the sharp falloff in the Zipf distribu-
tion. Our flattened distribution has the form:
count = e
?
1
c
R
? argSize
?
2
z
R
When ?
2
is less than 1, the resulting curve has a
less steep slope, and greater weight is placed on
the large values of m and n, as desired. Our last
step is to interpolate the PRight
R
and PLeft
R
distri-
butions with the flattened Zipf distribution to come
up with corrected distributions for the right and
left argument sizes of R. We use equal weights
on the two distributions to interpolate. Note that if
the original counts from the extraction system in-
clude counts for only one argument size, then it is
impossible to estimate a Zipf distribution, and we
simply fall back on the extracted distribution. We
do not include counts for an argument size of zero
in this process.
To estimate the parameters ?
i
, we collect a
training set of QSD problems Q, labeled with the
correct reading for each (direct or indirect), and
run the extractor for the relations Q
r
appearing in
the training set. We then perform a gradient de-
scent search to find optimal settings for the ?
i
on
the training data.
6 Experiments
We report on two sets of experiments. The first
tests our extraction technique on its own, and the
second tests the accuracy of our complete QSD
system, including the extraction mechanisms and
the prediction model, on a quantifier scope disam-
biguation task.
6.1 Function Detection Experiment
Function detection is an important task in its own
right, and has been used in several previous ap-
plications (Ritter et al, 2008; Yates and Etzioni,
2009; Popescu, 2007). To turn our extraction
system into a classifier for functions vs. non-
functions, we simply checked whether there were
1471
Num Precision Recall F1
Functions 54 .79 .76 .77
Non-functions 74 .83 .85 .84
Table 3: Precision and recall for detecting functions us-
ing the numeric extraction technique.
any extractions for R with F
n2
> 1. If so, we pre-
dicted that the R was nonfunctional, and otherwise
we predicted it was functional.
We used the Web1Tgram Corpus of n-grams
provided by Google, Inc to extract classes, rela-
tions, and counts. This corpus contains counts for
2- through 5-grams that appear on the Web pages
indexed by Google. Counts are included in this
data set for all n-grams that appeared at least 40
times in their text. We ran our extraction tech-
niques on the 3-, 4- and 5-grams. To create a test
set, we sampled a set of 200 relations from our ex-
tractions, removed any relations that consisted of
punctuations, stopwords, or other non-relational
items. We then manually labeled the remainder
as functions or non-functions.
Table 3 shows our results. A baseline sys-
tem that simply predicts the majority class (non-
functions) on this data set would achieve an ac-
curacy of 56%, well below the 81% accuracy
of our classifier. Many of the relations in our
test set, like built(Person, House) and is
riding(Person,Animal), do not ordinarily
have named-entity extractions for both arguments,
and would therefore not be amenable to previous
function detection approaches.
Some of our technique?s errors highlight inter-
esting difficulties with function detection. For in-
stance, while we labeled the is capital of
relation as a function, our technique predicted that
it was not. It turns out that the country of Bolivia
has two capitals, and the South Asian region of
Jammu and Kashmir also has two capitals. Both
of these facts are prominent enough on the Web
to cause our system to detect a small probability
for PRight
capital of
(2). Thus any label for this rela-
tion is somewhat unsatisfying: it is almost entirely
functional, but not strictly so. By generalizing the
problem to one of determining a distribution for
the size of the argument, we can handle these bor-
der cases in a useful way for QSD, as discussed
below.
6.2 Preliminary QSD Experiments
We test our complete QSD system on two impor-
tant tasks. In the first, the system is presented
with a series of QSD problems Q in which the
first quantifier Q
q1
is always ?a,? and the second
(Q
q2
) is always ?every.? Each example is manu-
ally labeled to indicate whether a direct or indirect
reading of the sentence is preferred, and the sys-
tem is charged with predicting the preferred read-
ing. In the second task, each Q has ?every? as
the first quantifier, and ?a? as the second quanti-
fier. Since indirect readings are very rarely pre-
ferred for active-voice sentences of this form,we
charge the system with making a different type of
prediction: determine whether the indirect reading
is plausible or not. The system assumes that ev-
ery sentence has a plausible direct reading, but by
determining whether the indirect reading is plau-
sible, it can determine whether the sentence is am-
biguous between the two readings.
We created data sets for these tasks by sampling
our 5grams for examples containing the relations
in our function experiment. From this set, we se-
lected phrases that involved named classes for the
arguments to the relation. When a class was miss-
ing, we either manually supplied one, or discarded
the example. We then constructed two examples
from each combination of relation and argument
classes: one example in which the first argument
is constrained by the quantifier ?a? and the sec-
ond by ?every,? and a second example in which
the quantifiers are reversed. Finally, we manually
labeled every example with a preference for direct
or indirect reading (in the case of ?a/every? exam-
ples) or with a plausibility judgment for the indi-
rect reading (in the case of ?every/a? examples).
Our final test sets included 46 labeled examples
for each task. Further experiments involving mul-
tiple annotators, as in the experiments of Kurtz-
man and MacDonald (1993), are of course desir-
able, but note that even their experiments included
just 32 labeled examples.
Table 4 shows our results for the first QSD task,
and Table 5 shows our results for the second one.
In each case, we compare our supervised Cor-
rected MLN model against an Uncorrected MLN
model that uses no supervised data, and simply
takes its weights straight from our extracted distri-
butions. The supervised model uses a training cor-
pus of 10 manually labeled examples for each task,
five from each class. We also compare against a
majority class baseline. Note that the Corrected
1472
Direct Indirect
System Acc. P R P R
All-Direct BL .53 .53 1.0 0.0 0.0
Uncorrected MLN .58 .78 .30 .53 .90
Corrected MLN .74 .77 .74 .71 .75
Table 4: Our trained MLN outperforms two other sys-
tems at predicting whether sentences of the form ?A/some
<class 1> <relation> every <class 2>? should have di-
rect or indirect readings. We measure accuracy over the
whole dataset, as well as precision and recall for the two sub-
sets labeled with direct and indirect readings, respectively.
Plausible Implaus.
System Acc. P R P R
All-Plausible BL .67 .67 1.0 0.0 0.0
Uncorrected MLN .49 .89 .28 .38 .93
Corrected MLN .72 .76 .86 .60 .43
Table 5: Our trained MLN outperforms two other sys-
tems at predicting whether sentences of the form ?Every
<class 1> <relation> a/some <class 2>? have a plausi-
ble indirect reading or not. We measure accuracy over the
whole dataset, as well as precision and recall for the two sub-
sets labeled with plausible and implausible indirect readings.
MLN model has balanced recall numbers for the
two classes in both of our tasks, compared with the
Uncorrected MLN. This indicates that our ZIPF
FLATTENING technique is accurately learning bet-
ter weights to remove the systematic bias in the
Uncorrected MLN.
Our results demonstrate the utility of our ex-
tracted distributions for these difficult tasks. Al-
though the extracted data prevents us from deter-
mining that is capital of should be classi-
fied as a function, since almost all of the prob-
ability mass in PRight is still on n ? {0, 1}.
Thus, the probability for the direct reading of
a sentence like ?Some city is the capital of ev-
ery country? is still very low. Likewise, even
though our system (correctly) determines that the
relation is a parent of is non-functional,
it does not therefore group it with other non-
functional relations like visited. The dis-
tribution PRightis parent of(n) is skewed to much
smaller numbers for n than is the distribution for
visited, and thus the indirect reading for ?A
person is the parent of every child? is much more
likely than the indirect reading of ?A person vis-
ited every country.?
The biggest hurdle for better performance is
noise in our extraction technique. Polysemous re-
lations sometimes have large counts for large ar-
gument sizes in one sense, but not another. Us-
ing argument classes to disambiguate relations can
help, but extractions for relations in combination
with argument classes are much more sparse. Im-
proved extraction techniques could directly impact
performance on the QSD task.
7 Conclusion and Future Work
We have demonstrated targeted methods for ex-
tracting world knowledge that is necessary for
making quantifier scope disambiguation deci-
sions. We have also demonstrated a novel,
minimally-supervised, statistical relational model
in the Markov Logic Network framework for mak-
ing QSD decisions based on extracted pragmatics.
While our preliminary results for QSD are
promising, there are clearly many areas for im-
provement. We will need to handle more kinds of
quantifiers in our MLN model. Our current system
is biased towards using purely pragmatic knowl-
edge, but a complete system should also integrate
syntactic and lexical constraints and preferences.
Also, discourses can introduce knowledge that di-
rectly affects QSD problems, such as constraints
on the size of a particular set that is discussed in
the discourse. Integrating our technique for QSD
with discourse processing is a major challenge that
we hope to address.
References
Hiyan Alshawi. 1990. Resolving quasi logical forms.
Computational Linguistics, 16(3):133?144.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional information extraction.
In Proceedings of the ACL.
J. Barwise and R. Cooper. 1981. Generalized quanti-
fiers and natural language. Linguistics and Philoso-
phy, 4(2):150?219.
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal recursion semantics:
An introduction. Research on Language and Com-
putation, 3:281?332.
D. Davidov and A. Rappaport. 2008. Unsupervised
discovery of generic relationships using pattern clus-
ters and its evaluation by automatically generated
SAT analogy questions. In Proceedings of the ACL.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2005. A Probabilistic Model of Redundancy in In-
formation Extraction. In IJCAI.
1473
Jerry R. Hobbs and Stuart M. Shieber. 1987. An algo-
rithm for generating quantifier scopings. Computa-
tional Linguistics, 13(1-2):47?63.
Sven Hurum. 1988. Handling scope ambiguities in
English. In Proceedings of the Second Conference
on Applied Natural Language Processing, pages 58?
65.
Howard S. Kurtzman and Maryellen C. MacDonald.
1993. Resolution of quantifier scope ambiguities.
Cognition, 48:243?279.
Longin Latecki. 1992. Connection relations and quan-
tifier scope. In Proceedings of the ACL.
Richard Montague. 1973. The proper treatment of
quantification in ordinary English. In Jaakko Hin-
tikka, Julius Moravcsik, and Patrick Suppes, editors,
Approaches to Natural Languages, pages 221?242.
Reidel, Dordrecht.
Douglas B. Moran. 1988. Quantifier scoping in the
SRI core language engine. In Proceedings of the
26th Annual Meeting of the Assoc. for Comp. Lin-
guistics, pages 33?40.
Jong C. Park. 1988. Quantifier scope and constituency.
In Proceedings of the 26th Annual Meeting of the
Assoc. for Comp. Linguistics, pages 33?40.
Massimo Poesio. 1993a. Assigning a semantic scope
to operators. In Proceedings of the ACL.
Massimo Poesio. 1993b. Assigning a semantic scope
to operators. In Proceedings of the Second Confer-
ence on Situation Theory and Its Applications.
Ana-Maria Popescu. 2007. Information Extraction
from Unstructured Web Text. Ph.D. thesis, Univer-
sity of Washington.
Uwe Reyle. 1995. On reasoning with ambiguities. In
Proceedings of the EACL, pages 1?8.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning,
62:107?136.
Alan Ritter, Doug Downey, Stephen Soderland, and
Oren Etzioni. 2008. It?s a contradiction ? No, it?s
not: A case study using functional relations. In Em-
pirical Methods in Natural Language Processing.
Walid S. Saba and Jean-Pierre Corriveau. 1997. A
pragmatic treatment of quantification in natural lan-
guage. In Proceedings of the National Conference
on Artificial Intelligence.
Walid S. Saba and Jean-Pierre Corriveau. 2001. Plau-
sible reasoning and the resolution of quantifier scope
ambiguities. Studia Logica, 67:271?289.
Stefan Schoenmackers, Oren Etzioni, and Dan Weld.
2008. Scaling textual inference to the web. In Pro-
ceedings of EMNLP.
Kurt Van Lehn. 1978. Determining the scope of En-
glish quantifiers. Technical Report AI-TR-483, AI
Lab, MIT.
Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised methods for determining object and relation
synonyms on the web. Journal of Artificial Intelli-
gence Research (JAIR), 34:255?296, March.
Alexander Yates, Stefan Schoenmackers, and Oren Et-
zioni. 2006. Detecting parser errors using web-
based semantic filters. In Proceedings of EMNLP.
1474
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 495?503,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Distributional Representations for Handling Sparsity in Supervised
Sequence-Labeling
Fei Huang
Temple University
1805 N. Broad St.
Wachman Hall 324
tub58431@temple.edu
Alexander Yates
Temple University
1805 N. Broad St.
Wachman Hall 324
yates@temple.edu
Abstract
Supervised sequence-labeling systems in
natural language processing often suffer
from data sparsity because they use word
types as features in their prediction tasks.
Consequently, they have difficulty estimat-
ing parameters for types which appear in
the test set, but seldom (or never) ap-
pear in the training set. We demonstrate
that distributional representations of word
types, trained on unannotated text, can
be used to improve performance on rare
words. We incorporate aspects of these
representations into the feature space of
our sequence-labeling systems. In an ex-
periment on a standard chunking dataset,
our best technique improves a chunker
from 0.76 F1 to 0.86 F1 on chunks begin-
ning with rare words. On the same dataset,
it improves our part-of-speech tagger from
74% to 80% accuracy on rare words. Fur-
thermore, our system improves signifi-
cantly over a baseline system when ap-
plied to text from a different domain, and
it reduces the sample complexity of se-
quence labeling.
1 Introduction
Data sparsity and high dimensionality are the twin
curses of statistical natural language processing
(NLP). In many traditional supervised NLP sys-
tems, the feature space includes dimensions for
each word type in the data, or perhaps even combi-
nations of word types. Since vocabularies can be
extremely large, this leads to an explosion in the
number of parameters. To make matters worse,
language is Zipf-distributed, so that a large frac-
tion of any training data set will be hapax legom-
ena, very many word types will appear only a few
times, and many word types will be left out of
the training set alogether. As a consequence, for
many word types supervised NLP systems have
very few, or even zero, labeled examples from
which to estimate parameters.
The negative effects of data sparsity have been
well-documented in the NLP literature. The per-
formance of state-of-the-art, supervised NLP sys-
tems like part-of-speech (POS) taggers degrades
significantly on words that do not appear in the
training data, or out-of-vocabulary (OOV) words
(Lafferty et al, 2001). Performance also degrades
when the domain of the test set differs from the do-
main of the training set, in part because the test set
includes more OOV words and words that appear
only a few times in the training set (henceforth,
rare words) (Blitzer et al, 2006; Daume? III and
Marcu, 2006; Chelba and Acero, 2004).
We investigate the use of distributional repre-
sentations, which model the probability distribu-
tion of a word?s context, as techniques for find-
ing smoothed representations of word sequences.
That is, we use the distributional representations
to share information across unannotated examples
of the same word type. We then compute features
of the distributional representations, and provide
them as input to our supervised sequence label-
ers. Our technique is particularly well-suited to
handling data sparsity because it is possible to im-
prove performance on rare words by supplement-
ing the training data with additional unannotated
text containing more examples of the rare words.
We provide empirical evidence that shows how
distributional representations improve sequence-
labeling in the face of data sparsity.
Specifically, we investigate empirically the
effects of our smoothing techniques on two
sequence-labeling tasks, POS tagging and chunk-
ing, to answer the following:
1. What is the effect of smoothing on sequence-
labeling accuracy for rare word types? Our best
smoothing technique improves a POS tagger by
11% on OOV words, and a chunker by an impres-
sive 21% on OOV words.
495
2. Can smoothing improve adaptability to new do-
mains? After training our chunker on newswire
text, we apply it to biomedical texts. Remark-
ably, we find that the smoothed chunker achieves
a higher F1 on the new domain than the baseline
chunker achieves on a test set from the original
newswire domain.
3. How does our smoothing technique affect sam-
ple complexity? We show that smoothing drasti-
cally reduces sample complexity: our smoothed
chunker requires under 100 labeled samples to
reach 85% accuracy, whereas the unsmoothed
chunker requires 3500 samples to reach the same
level of performance.
The remainder of this paper is organized as fol-
lows. Section 2 discusses the smoothing problem
for word sequences, and introduces three smooth-
ing techniques. Section 3 presents our empirical
study of the effects of smoothing on two sequence-
labeling tasks. Section 4 describes related work,
and Section 5 concludes and suggests items for fu-
ture work.
2 Smoothing Natural Language
Sequences
To smooth a dataset is to find an approximation of
it that retains the important patterns of the origi-
nal data while hiding the noise or other compli-
cating factors. Formally, we define the smoothing
task as follows: let D = {(x, z)|x is a word se-
quence, z is a label sequence} be a labeled dataset
of word sequences, and let M be a machine learn-
ing algorithm that will learn a function f to pre-
dict the correct labels. The smoothing task is to
find a function g such that when M is applied to
D? = {(g(x), z)|(x, z) ? D}, it produces a func-
tion f ? that is more accurate than f .
For supervised sequence-labeling problems in
NLP, the most important ?complicating factor?
that we seek to avoid through smoothing is the
data sparsity associated with word-based represen-
tations. Thus, the task is to find g such that for
every word x, g(x) is much less sparse, but still
retains the essential features of x that are useful
for predicting its label.
As an example, consider the string ?Researchers
test reformulated gasolines on newer engines.? In
a common dataset for NP chunking, the word ?re-
formulated? never appears in the training data, but
appears four times in the test set as part of the
NP ?reformulated gasolines.? Thus, a learning al-
gorithm supplied with word-level features would
have a difficult time determining that ?reformu-
lated? is the start of a NP. Character-level features
are of little help as well, since the ?-ed? suffix is
more commonly associated with verb phrases. Fi-
nally, context may be of some help, but ?test? is
ambiguous between a noun and verb, and ?gaso-
lines? is only seen once in the training data, so
there is no guarantee that context is sufficient to
make a correct judgment.
On the other hand, some of the other contexts
in which ?reformulated? appears in the test set,
such as ?testing of reformulated gasolines,? pro-
vide strong evidence that it can start a NP, since
?of? is a highly reliable indicator that a NP is to
follow. This example provides the intuition for our
approach to smoothing: we seek to share informa-
tion about the contexts of a word across multiple
instances of the word, in order to provide more in-
formation about words that are rarely or never seen
in training. In particular, we seek to represent each
word by a distribution over its contexts, and then
provide the learning algorithm with features com-
puted from this distribution. Importantly, we seek
distributional representations that will provide fea-
tures that are common in both training and test
data, to avoid data sparsity. In the next three sec-
tions, we develop three techniques for smoothing
text using distributional representations.
2.1 Multinomial Representation
In its simplest form, the context of a word may be
represented as a multinomial distribution over the
terms that appear on either side of the word. If V is
the vocabulary, or the set of word types, and X is a
sequence of random variables over V , the left and
right context of Xi = v may each be represented
as a probability distribution over V: P (Xi?1|Xi =
v) and P (Xi+1|X = v) respectively.
We learn these distributions from unlabeled
texts in two different ways. The first method com-
putes word count vectors for the left and right con-
texts of each word type in the vocabulary of the
training and test texts. We also use a large col-
lection of additional text to determine the vectors.
We then normalize each vector to form a proba-
bility distribution. The second technique first ap-
plies TF-IDF weighting to each vector, where the
context words of each word type constitute a doc-
ument, before applying normalization. This gives
greater weight to words with more idiosyncratic
distributions and may improve the informativeness
of a distributional representation. We refer to these
techniques as TF and TF-IDF.
496
To supply a sequence-labeling algorithm with
information from these distributional representa-
tions, we compute real-valued features of the con-
text distributions. In particular, for every word
xi in a sequence, we provide the sequence labeler
with a set of features of the left and right contexts
indexed by v ? V: F leftv (xi) = P (Xi?1 = v|xi)
and F rightv (xi) = P (Xi+1 = v|xi). For exam-
ple, the left context for ?reformulated? in our ex-
ample above would contain a nonzero probability
for the word ?of.? Using the features F(xi), a se-
quence labeler can learn patterns such as, if xi has
a high probability of following ?of,? it is a good
candidate for the start of a noun phrase. These
features provide smoothing by aggregating infor-
mation across multiple unannotated examples of
the same word.
2.2 LSA Model
One drawback of the multinomial representation
is that it does not handle sparsity well enough,
because the multinomial distributions themselves
are so high-dimensional. For example, the two
phrases ?red lamp? and ?magenta tablecloth?
share no words in common. If ?magenta? is never
observed in training, the fact that ?tablecloth? ap-
pears in its right context is of no help in connecting
it with the phrase ?red lamp.? But if we can group
similar context words together, putting ?lamp? and
?tablecloth? into a category for household items,
say, then these two adjectives will share that cat-
egory in their context distributions. Any pat-
terns learned for the more common ?red lamp?
will then also apply to the less common ?magenta
tablecloth.? Our second distributional represen-
tation aggregates information from multiple con-
text words by grouping together the distributions
P (xi?1 = v|xi = w) and P (xi?1 = v?|xi = w)
if v and v? appear together with many of the same
words w. Aggregating counts in this way smooths
our representations even further, by supplying bet-
ter estimates when the data is too sparse to esti-
mate P (xi?1|xi) accurately.
Latent Semantic Analysis (LSA) (Deerwester et
al., 1990) is a widely-used technique for comput-
ing dimensionality-reduced representations from a
bag-of-words model. We apply LSA to the set of
right context vectors and the set of left context vec-
tors separately, to find compact versions of each
vector, where each dimension represents a com-
bination of several context word types. We nor-
malize each vector, and then calculate features as
above. After experimenting with different choices
for the number of dimensions to reduce our vec-
tors to, we choose a value of 10 dimensions as the
one that maximizes the performance of our super-
vised sequence labelers on held-out data.
2.3 Latent Variable Language Model
Representation
To take smoothing one step further, we present
a technique that aggregates context distributions
both for similar context words xi?1 = v and v?,
and for similar words xi = w and w?. Latent
variable language models (LVLMs) can be used to
produce just such a distributional representation.
We use Hidden Markov Models (HMMs) as the
main example in the discussion and as the LVLMs
in our experiments, but the smoothing technique
can be generalized to other forms of LVLMs, such
as factorial HMMs and latent variable maximum
entropy models (Ghahramani and Jordan, 1997;
Smith and Eisner, 2005).
An HMM is a generative probabilistic model
that generates each word xi in the corpus con-
ditioned on a latent variable Yi. Each Yi in the
model takes on integral values from 1 to S, and
each one is generated by the latent variable for the
preceding word, Yi?1. The distribution for a cor-
pus x = (x1, . . . , xN ) given a set of state vectors
y = (y1, . . . , yN ) is given by:
P (x|y) =
?
i
P (xi|yi)P (yi|yi?1)
Using Expectation-Maximization (Dempster et
al., 1977), it is possible to estimate the distribu-
tions for P (xi|yi) and P (yi|yi?1) from unlabeled
data. We use a trained HMM to determine the op-
timal sequence of latent states y?i using the well-
known Viterbi algorithm (Rabiner, 1989). The
output of this process is an integer (ranging from 1
to S) for every word xi in the corpus; we include a
new boolean feature for each possible value of yi
in our sequence labelers.
To compare our models, note that in the multi-
nomial representation we directly model the prob-
ability that a word v appears before a word w:
P (xi?1 = v|xi = w)). In our LSA model, we find
latent categories of context words z, and model the
probability that a category appears before the cur-
rent word w: P (xi?1 = z|xi = w). The HMM
finds (probabilistic) categories Y for both the cur-
rent word xi and the context word xi?1, and mod-
els the probability that one category follows the
497
other: P (Yi|Yi?1). Thus the HMM is our most
extreme smoothing model, as it aggregates infor-
mation over the greatest number of examples: for
a given consecutive pair of words xi?1, xi in the
test set, it aggregates over all pairs of consecutive
words x?i?1, x?i where x?i?1 is similar to xi?1 and
x?i is similar to xi.
3 Experiments
We tested the following hypotheses in our experi-
ments:
1. Smoothing can improve the performance of
a supervised sequence labeling system on words
that are rare or nonexistent in the training data.
2. A supervised sequence labeler achieves greater
accuracy on new domains with smoothing.
3. A supervised sequence labeler has a better sam-
ple complexity with smoothing.
3.1 Experimental Setup
We investigate the use of smoothing in two test
systems, conditional random field (CRF) models
for POS tagging and chunking. To incorporate
smoothing into our models, we follow the follow-
ing general procedure: first, we collect a set of
unannotated text from the same domain as the test
data set. Second, we train a smoothing model on
the text of the training data, the test data, and the
additional collection. We then automatically an-
notate both the training and test data with features
calculated from the distributional representation.
Finally, we train the CRF model on the annotated
training set and apply it to the test set.
We use an open source CRF software package
designed by Sunita Sajarwal and William W. Co-
hen to implement our CRF models.1 We use a set
of boolean features listed in Table 1.
Our baseline CRF system for POS tagging fol-
lows the model described by Lafferty et al(2001).
We include transition features between pairs of
consecutive tag variables, features between tag
variables and words, and a set of orthographic fea-
tures that Lafferty et al found helpful for perfor-
mance on OOV words. Our smoothed models add
features computed from the distributional repre-
sentations, as discussed above.
Our chunker follows the system described by
Sha and Pereira (2003). In addition to the tran-
sition, word-level, and orthographic features, we
include features relating automatically-generated
POS tags and the chunk labels. Unlike Sha and
1Available from http://sourceforge.net/projects/crf/
CRF Feature Set
Transition zi=z
zi=z and zi?1=z?
Word xi=w and zi=z
POS ti=t and zi=z
Orthography for every s ? {-ing, -ogy, -
ed, -s, -ly, -ion, -tion, -ity},
suffix(xi)= s and zi=z
xi is capitalized and zi = z
xi has a digit and zi = z
TF, TF-IDF, and
LSA features
for every context type v,
F leftv (xi) and F rightv (xi)
HMM features yi=y and zi = z
Table 1: Features used in our CRF systems. zi vari-
ables represent labels to be predicted, ti represent tags (for
the chunker), and xi represent word tokens. All features are
boolean except for the TF, TF-IDF, and LSA features.
Pereira, we exclude features relating consecutive
pairs of words and a chunk label, or features re-
lating consecutive tag labels and a chunk label,
in order to expedite our experiments. We found
that including such features does improve chunk-
ing F1 by approximately 2%, but it also signifi-
cantly slows down CRF training.
3.2 Rare Word Accuracy
For these experiments, we use the Wall Street
Journal portion of the Penn Treebank (Marcus et
al., 1993). Following the CoNLL shared task from
2000, we use sections 15-18 of the Penn Treebank
for our labeled training data for the supervised
sequence labeler in all experiments (Tjong et al,
2000). For the tagging experiments, we train and
test using the gold standard POS tags contained in
the Penn Treebank. For the chunking experiments,
we train and test with POS tags that are automati-
cally generated by a standard tagger (Brill, 1994).
We tested the accuracy of our models for chunking
and POS tagging on section 20 of the Penn Tree-
bank, which corresponds to the test set from the
CoNLL 2000 task.
Our distributional representations are trained on
sections 2-22 of the Penn Treebank. Because we
include the text from the train and test sets in our
training data for the distributional representations,
we do not need to worry about smoothing them
? when they are decoded on the test set, they
498
Freq: 0 1 2 0-2 all
#Samples 438 508 588 1534 46661
Baseline .62 .77 .81 .74 .93
TF .76 .72 .77 .75 .92
TF-IDF .82 .75 .76 .78 .94
LSA .78 .80 .77 .78 .94
HMM .73 .81 .86 .80 .94
Table 2: POS tagging accuracy: our HMM-smoothed
tagger outperforms the baseline tagger by 6% on rare
words. Differences between the baseline and the HMM are
statistically significant at p < 0.01 for the OOV, 0-2, and all
cases using the two-tailed Chi-squared test with 1 degree of
freedom.
will not encounter any previously unseen words.
However, to speed up training during our exper-
iments and, in some cases, to avoid running out
of memory, we replaced words appearing twice or
fewer times in the data with the special symbol
*UNKNOWN*. In addition, all numbers were re-
placed with another special symbol. For the LSA
model, we had to use a more drastic cutoff to fit
the singular value decomposition computation into
memory: we replaced words appearing 10 times or
fewer with the *UNKNOWN* symbol. We initial-
ize our HMMs randomly. We run EM ten times
and take the model with the best cross-entropy on
a held-out set. After experimenting with differ-
ent variations of HMM models, we settled on a
model with 80 latent states as a good compromise
between accuracy and efficiency.
For our POS tagging experiments, we measured
the accuracy of the tagger on ?rare? words, or
words that appear at most twice in the training
data. For our chunking experiments, we focus on
chunks that begin with rare words, as we found
that those were the most difficult for the chunker
to identify correctly. So we define ?rare? chunks
as those that begin with words appearing at most
twice in training data. To ensure that our smooth-
ing models have enough training data for our test
set, we further narrow our focus to those words
that appear rarely in the labeled training data, but
appear at least ten times in sections 2-22. Tables 2
and 3 show the accuracy of our smoothed models
and the baseline model on tagging and chunking,
respectively. The line for ?all? in both tables indi-
cates results on the complete test set.
Both our baseline tagger and chunker achieve
respectable results on their respective tasks for
all words, and the results were good enough for
Freq: 0 1 2 0-2 all
#Samples 133 199 231 563 21900
Baseline .69 .75 .81 .76 .90
TF .70 .82 .79 .77 .89
TF-IDF .77 .77 .80 .78 .90
LSA .84 .82 .83 .84 .90
HMM .90 .85 .85 .86 .93
Table 3: Chunking F1: our HMM-smoothed chunker
outperforms the baseline CRF chunker by 0.21 on chunks
that begin with OOV words, and 0.10 on chunks that be-
gin with rare words.
us to be satisfied that performance on rare words
closely follows how a state-of-the-art supervised
sequence-labeler behaves. The chunker?s accuracy
is roughly in the middle of the range of results for
the original CoNLL 2000 shared task (Tjong et
al., 2000) . While several systems have achieved
slightly higher accuracy on supervised POS tag-
ging, they are usually trained on larger training
sets.
As expected, the drop-off in the baseline sys-
tem?s performance from all words to rare words
is impressive for both tasks. Comparing perfor-
mance on all terms and OOV terms, the baseline
tagger?s accuracy drops by 0.31, and the baseline
chunker?s F1 drops by 0.21. Comparing perfor-
mance on all terms and rare terms, the drop is less
severe but still dramatic: 0.19 for tagging and 0.15
for chunking.
Our hypothesis that smoothing would improve
performance on rare terms is validated by these ex-
periments. In fact, the more aggregation a smooth-
ing model performs, the better it appears to be at
smoothing. The HMM-smoothed system outper-
forms all other systems in all categories except
tagging on OOV words, where TF-IDF performs
best. And in most cases, the clear trend is for
HMM smoothing to outperform LSA, which in
turn outperforms TF and TF-IDF. HMM tagging
performance on OOV terms improves by 11%, and
chunking performance by 21%. Tagging perfor-
mance on all of the rare terms improves by 6%,
and chunking by 10%. In chunking, there is a
clear trend toward larger increases in performance
as words become rarer in the labeled data set, from
a 0.02 improvement on words of frequency 2, to an
improvement of 0.21 on OOV words.
Because the test data for this experiment is
drawn from the same domain (newswire) as the
499
training data, the rare terms make up a relatively
small portion of the overall dataset (approximately
4% of both the tagged words and the chunks).
Still, the increased performance by the HMM-
smoothed model on the rare-word subset con-
tributes in part to an increase in performance on
the overall dataset of 1% for tagging and 3% for
chunking. In our next experiment, we consider
a common scenario where rare terms make up a
much larger fraction of the test data.
3.3 Domain Adaptation
For our experiment on domain adaptation, we fo-
cus on NP chunking and POS tagging, and we
use the labeled training data from the CoNLL
2000 shared task as before. For NP chunking, we
use 198 sentences from the biochemistry domain
in the Open American National Corpus (OANC)
(Reppen et al, 2005) as or our test set. We man-
ually tagged the test set with POS tags and NP
chunk boundaries. The test set contains 5330
words and a total of 1258 NP chunks. We used
sections 15-18 of the Penn Treebank as our labeled
training set, including the gold standard POS tags.
We use our best-performing smoothing model, the
HMM, and train it on sections 13 through 19 of
the Penn Treebank, plus the written portion of
the OANC that contains journal articles from bio-
chemistry (40,727 sentences). We focus on chunks
that begin with words appearing 0-2 times in the
labeled training data, and appearing at least ten
times in the HMM?s training data. Table 4 con-
tains our results. For our POS tagging experi-
ments, we use 561 MEDLINE sentences (9576
words) from the Penn BioIE project (PennBioIE,
2005), a test set previously used by Blitzer et
al.(2006). We use the same experimental setup as
Blitzer et al: 40,000 manually tagged sentences
from the Penn Treebank for our labeled training
data, and all of the unlabeled text from the Penn
Treebank plus their MEDLINE corpus of 71,306
sentences to train our HMM. We report on tagging
accuracy for all words and OOV words in Table
5. This table also includes results for two previous
systems as reported by Blitzer et al (2006): the
semi-supervised Alternating Structural Optimiza-
tion (ASO) technique and the Structural Corre-
spondence Learning (SCL) technique for domain
adaptation.
Note that this test set for NP chunking con-
tains a much higher proportion of rare and OOV
words: 23% of chunks begin with an OOV word,
and 29% begin with a rare word, as compared with
Baseline HMM
Freq. # R P F1 R P F1
0 284 .74 .70 .72 .80 .89 .84
1 39 .85 .87 .86 .92 .88 .90
2 39 .79 .86 .83 .92 .90 .91
0-2 362 .75 .73 .74 .82 .89 .85
all 1258 .86 .87 .86 .91 .90 .91
Table 4: On biochemistry journal data from the OANC,
our HMM-smoothed NP chunker outperforms the base-
line CRF chunker by 0.12 (F1) on chunks that begin with
OOV words, and by 0.05 (F1) on all chunks. Results in
bold are statistically significantly different from the baseline
results at p < 0.05 using the two-tailed Fisher?s exact test.
We did not perform significance tests for F1.
All Unknown
Model words words
Baseline 88.3 67.3
ASO 88.4 70.9
SCL 88.9 72.0
HMM 90.5 75.2
Table 5: On biomedical data from the Penn BioIE
project, our HMM-smoothed tagger outperforms the
SCL tagger by 3% (accuracy) on OOV words, and by
1.6% (accuracy) on all words. Differences between the
smoothed tagger and the SCL tagger are significant at p <
.001 for all words and for OOV words, using the Chi-squared
test with 1 degree of freedom.
1% and 4%, respectively, for NP chunks in the test
set from the original domain. The test set for tag-
ging also contains a much higher proportion: 23%
OOV words, as compared with 1% in the original
domain. Because of the increase in the number of
rare words, the baseline chunker?s overall perfor-
mance drops by 4% compared with performance
on WSJ data, and the baseline tagger?s overall per-
formance drops by 5% in the new domain.
The performance improvements for both the
smoothed NP chunker and tagger are again im-
pressive: there is a 12% improvement on OOV
words, and a 10% overall improvement on rare
words for chunking; the tagger shows an 8% im-
provement on OOV words compared to out base-
line and a 3% improvement on OOV words com-
pared to the SCL model. The resulting perfor-
mance of the smoothed NP chunker is almost iden-
tical to its performance on the WSJ data. Through
smoothing, the chunker not only improves by 5%
500
in F1 over the baseline system on all words, it in
fact outperforms our baseline NP chunker on the
WSJ data. 60% of this improvement comes from
improved accuracy on rare words.
The performance of our HMM-smoothed chun-
ker caused us to wonder how well the chunker
could work without some of its other features. We
removed all tag features and all features for word
types that appear fewer than 20 times in training.
This chunker achieves 0.91 F1 on OANC data, and
0.93 F1 on WSJ data, outperforming the baseline
system in both cases. It has only 20% as many fea-
tures as the baseline chunker, greatly improving
its training time. Thus our smoothing features are
more valuable to the chunker than features from
POS tags and features for all but the most common
words. Our results point to the exciting possibil-
ity that with smoothing, we may be able to train a
sequence-labeling system on a small labeled sam-
ple, and have it apply generally to other domains.
Exactly what size training set we need is a ques-
tion that we address next.
3.4 Sample Complexity
Our complete system consists of two learned com-
ponents, a supervised CRF system and an unsu-
pervised smoothing model. We measure the sam-
ple complexity of each component separately. To
measure the sample complexity of the supervised
CRF, we use the same experimental setup as in
the chunking experiment on WSJ text, but we vary
the amount of labeled data available to the CRF.
We take ten random samples of a fixed size from
the labeled training set, train a chunking model on
each subset, and graph the F1 on the labeled test
set, averaged over the ten runs, in Figure 1. To
measure the sample complexity of our HMM with
respect to unlabeled text, we use the full labeled
training set and vary the amount of unlabeled text
available to the HMM. At minimum, we use the
text available in the labeled training and test sets,
and then add random subsets of the Penn Tree-
bank, sections 2-22. For each subset size, we take
ten random samples of the unlabeled text, train an
HMM and then a chunking model, and graph the
F1 on the labeled test set averaged over the ten
runs in Figure 2.
The results from our labeled sample complex-
ity experiment indicate that sample complexity is
drastically reduced by HMM smoothing. On rare
chunks, the smoothed system reaches 0.78 F1 us-
ing only 87 labeled training sentences, a level that
the baseline system never reaches, even with 6933
baseline (all)
HMM (all)
HMM (rare)
0.6
0.7
0.8
0.9
1
F1
 (C
hu
nk
in
g)
Labeled Sample Complexity
baseline (rare)
0.2
0.3
0.4
0.5
1 10 100 1000 10000
F1
 (C
hu
nk
in
g)
Number of Labeled Sentences (log scale)
Figure 1: The smoothed NP chunker requires less than
10% of the samples needed by the baseline chunker to
achieve .83 F1, and the same for .88 F1.
Baseline (all)
HMM (all) HMM (rare)
0.80
0.85
0.90
0.95
F1
 (C
hu
nk
in
g)
Unlabeled Sample Complexity
Baseline (rare)
0.70
0.75
0 10000 20000 30000 40000
F1
 (C
hu
nk
in
g)
Number of Unannotated Sentences
Figure 2: By leveraging plentiful unannotated text, the
smoothed chunker soon outperforms the baseline.
labeled sentences. On the overall data set, the
smoothed system reaches 0.83 F1 with 50 labeled
sentences, which the baseline does not reach un-
til it has 867 labeled sentences. With 434 labeled
sentences, the smoothed system reaches 0.88 F1,
which the baseline system does not reach until it
has 5200 labeled samples.
Our unlabeled sample complexity results show
that even with access to a small amount of unla-
beled text, 6000 sentences more than what appears
in the training and test sets, smoothing using the
HMM yields 0.78 F1 on rare chunks. However, the
smoothed system requires 25,000 more sentences
before it outperforms the baseline system on all
chunks. No peak in performance is reached, so
further improvements are possible with more unla-
beled data. Thus smoothing is optimizing perfor-
mance for the case where unlabeled data is plenti-
ful and labeled data is scarce, as we would hope.
4 Related Work
To our knowledge, only one previous system ?
the REALM system for sparse information extrac-
501
tion ? has used HMMs as a feature represen-
tation for other applications. REALM uses an
HMM trained on a large corpus to help determine
whether the arguments of a candidate relation are
of the appropriate type (Downey et al, 2007). We
extend and generalize this smoothing technique
and apply it to common NLP applications involv-
ing supervised sequence-labeling, and we provide
an in-depth empirical analysis of its performance.
Several researchers have previously studied
methods for using unlabeled data for tagging and
chunking, either alone or as a supplement to la-
beled data. Ando and Zhang develop a semi-
supervised chunker that outperforms purely su-
pervised approaches on the CoNLL 2000 dataset
(Ando and Zhang, 2005). Recent projects in semi-
supervised (Toutanova and Johnson, 2007) and un-
supervised (Biemann et al, 2007; Smith and Eis-
ner, 2005) tagging also show significant progress.
Unlike these systems, our efforts are aimed at us-
ing unlabeled data to find distributional represen-
tations that work well on rare terms, making the
supervised systems more applicable to other do-
mains and decreasing their sample complexity.
HMMs have been used many times for POS
tagging and chunking, in supervised, semi-
supervised, and in unsupervised settings (Banko
and Moore, 2004; Goldwater and Griffiths, 2007;
Johnson, 2007; Zhou, 2004). We take a novel per-
spective on the use of HMMs by using them to
compute features of each token in the data that
represent the distribution over that token?s con-
texts. Our technique lets the HMM find param-
eters that maximize cross-entropy, and then uses
labeled data to learn the best mapping from the
HMM categories to the POS categories.
Smoothing in NLP usually refers to the prob-
lem of smoothing n-gram models. Sophisticated
smoothing techniques like modified Kneser-Ney
and Katz smoothing (Chen and Goodman, 1996)
smooth together the predictions of unigram, bi-
gram, trigram, and potentially higher n-gram se-
quences to obtain accurate probability estimates in
the face of data sparsity. Our task differs in that we
are primarily concerned with the case where even
the unigram model (single word) is rarely or never
observed in the labeled training data.
Sparsity for low-order contexts has recently
spurred interest in using latent variables to repre-
sent distributions over contexts in language mod-
els. While n-gram models have traditionally dom-
inated in language modeling, two recent efforts de-
velop latent-variable probabilistic models that ri-
val and even surpass n-gram models in accuracy
(Blitzer et al, 2005; Mnih and Hinton, 2007).
Several authors investigate neural network mod-
els that learn not just one latent state, but rather a
vector of latent variables, to represent each word
in a language model (Bengio et al, 2003; Emami
et al, 2003; Morin and Bengio, 2005).
One of the benefits of our smoothing technique
is that it allows for domain adaptation, a topic
that has received a great deal of attention from
the NLP community recently. Unlike our tech-
nique, in most cases researchers have focused on
the scenario where labeled training data is avail-
able in both the source and the target domain
(e.g., (Daume? III, 2007; Chelba and Acero, 2004;
Daume? III and Marcu, 2006)). Our technique uses
unlabeled training data from the target domain,
and is thus applicable more generally, including
in web processing, where the domain and vocab-
ulary is highly variable, and it is extremely diffi-
cult to obtain labeled data that is representative of
the test distribution. When labeled target-domain
data is available, instance weighting and similar
techniques can be used in combination with our
smoothing technique to improve our results fur-
ther, although this has not yet been demonstrated
empirically. HMM-smoothing improves on the
most closely related work, the Structural Corre-
spondence Learning technique for domain adap-
tation (Blitzer et al, 2006), in experiments.
5 Conclusion and Future Work
Our study of smoothing techniques demonstrates
that by aggregating information across many
unannotated examples, it is possible to find ac-
curate distributional representations that can pro-
vide highly informative features to supervised se-
quence labelers. These features help improve se-
quence labeling performance on rare word types,
on domains that differ from the training set, and
on smaller training sets.
Further experiments are of course necessary
to investigate distributional representations as
smoothing techniques. One particularly promis-
ing area for further study is the combination of
smoothing and instance weighting techniques for
domain adaptation. Whether the current tech-
niques are applicable to structured prediction
tasks, like parsing and relation extraction, also de-
serves future attention.
502
References
Rie Kubota Ando and Tong Zhang. 2005. A high-
performance semi-supervised learning method for
text chunking. In ACL.
Michele Banko and Robert C. Moore. 2004. Part of
speech tagging in context. In COLING.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
C. Biemann, C. Giuliano, and A. Gliozzo. 2007. Un-
supervised pos tagging supporting supervised meth-
ods. Proceeding of RANLP-07.
J. Blitzer, A. Globerson, and F. Pereira. 2005. Dis-
tributed latent variable models of lexical cooccur-
rences. In Proceedings of the Tenth International
Workshop on Artificial Intelligence and Statistics.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
E. Brill. 1994. Some Advances in Rule-Based Part of
Speech Tagging. In AAAI, pages 722?727, Seattle,
Washington.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy classifier: Little data can help a
lot. In EMNLP.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meet-
ing on Association for Computational Linguistics,
pages 310?318, Morristown, NJ, USA. Association
for Computational Linguistics.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In ACL.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by
latent semantic analysis. Journal of the American
Society of Information Science, 41(6):391?407.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Likelihood from incomplete data via the EM algo-
rithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1?38.
Doug Downey, Stefan Schoenmackers, and Oren Et-
zioni. 2007. Sparse information extraction: Unsu-
pervised language models to the rescue. In ACL.
A. Emami, P. Xu, and F. Jelinek. 2003. Using a
connectionist model in a syntactical based language
model. In Proceedings of the International Confer-
ence on Spoken Language Processing, pages 372?
375.
Zoubin Ghahramani and Michael I. Jordan. 1997. Fac-
torial hidden markov models. Machine Learning,
29(2-3):245?273.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully bayesian approach to unsupervised part-of-
speech tagging. In ACL.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers. In EMNLP.
J. Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data.
In Proceedings of the International Conference on
Machine Learning.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn Treebank. Com-
putational Linguistics, 19(2):313?330.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th International Conference
on Machine Learning, pages 641?648, New York,
NY, USA. ACM.
F. Morin and Y. Bengio. 2005. Hierarchical probabilis-
tic neural network language model. In Proceedings
of the International Workshop on Artificial Intelli-
gence and Statistics, pages 246?252.
PennBioIE. 2005. Mining the bibliome project.
http://bioie.ldc.upenn.edu/.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?
285.
Randi Reppen, Nancy Ide, and Keith Suderman. 2005.
American national corpus (ANC) second release.
Linguistic Data Consortium.
F. Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
Human Language Technology - NAACL.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 354?362, Ann Arbor, Michigan, June.
Erik F. Tjong, Kim Sang, and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared task:
Chunking. In Proceedings of the 4th Conference on
Computational Natural Language Learning, pages
127?132.
Kristina Toutanova and Mark Johnson. 2007. A
bayesian LDA-based model for semi-supervised
part-of-speech tagging. In NIPS.
GuoDong Zhou. 2004. Discriminative hidden Markov
modeling with long state dependence using a kNN
ensemble. In COLING.
503
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 116?127, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Linking Named Entities to Any Database
Avirup Sil?
Temple University
Philadelphia, PA
avi@temple.edu
Ernest Cronin?
St. Joseph?s University
Philadelphia, PA
ernest.cronin@gmail.com
Penghai Nie
St. Joseph?s University
Philadelphia, PA
nph87903@gmail.com
Yinfei Yang
St. Joseph?s University
Philadelphia, PA
yangyin7@gmail.com
Ana-Maria Popescu
Yahoo! Labs
Sunnyvale, CA
amp@yahoo-inc.com
Alexander Yates
Temple University
Philadelphia, PA
yates@temple.edu
Abstract
Existing techniques for disambiguating named
entities in text mostly focus on Wikipedia as
a target catalog of entities. Yet for many
types of entities, such as restaurants and
cult movies, relational databases exist that
contain far more extensive information than
Wikipedia. This paper introduces a new task,
called Open-Database Named-Entity Disam-
biguation (Open-DB NED), in which a system
must be able to resolve named entities to sym-
bols in an arbitrary database, without requir-
ing labeled data for each new database. We
introduce two techniques for Open-DB NED,
one based on distant supervision and the other
based on domain adaptation. In experiments
on two domains, one with poor coverage by
Wikipedia and the other with near-perfect cov-
erage, our Open-DB NED strategies outper-
form a state-of-the-art Wikipedia NED system
by over 25% in accuracy.
1 Introduction
Named-entity disambiguation (NED) is the task of
linking names mentioned in text with an established
catalog of entities (Bunescu and Pasca, 2006; Rati-
nov et al2011). It is a vital first step for se-
mantic understanding of text, such as in grounded
semantic parsing (Kwiatkowski et al2011), as
well as for information retrieval tasks like person
name search (Chen and Martin, 2007; Mann and
Yarowsky, 2003).
NED requires a catalog of symbols, called refer-
ents, to which named-entities will be resolved. Most
NED systems today use Wikipedia as the catalog of
referents, but exclusive focus on Wikipedia as a tar-
get for NED systems has significant drawbacks: de-
spite its breadth, Wikipedia still does not contain all
or even most real-world entities mentioned in text.
As one example, it has poor coverage of entities that
are mostly important in a small geographical region,
such as hotels and restaurants, which are widely dis-
cussed on the Web. 57% of the named-entities in
the Text Analysis Conference?s (TAC) 2009 entity
linking task refer to an entity that does not appear
in Wikipedia (McNamee et al2009). Wikipedia is
clearly a highly valuable resource, but it should not
be thought of as the only one.
Instead of relying solely on Wikipedia, we pro-
pose a novel approach to NED, which we refer to
as Open-DB NED: the task is to resolve an en-
tity to Wikipedia or to any relational database that
meets mild conditions about the format of the data,
described below. Leveraging structured, relational
data should allow systems to achieve strong accu-
racy, as with domain-specific or database-specific
NED techniques like Hoffart et al NED system
for YAGO (Hoffart et al2011). And because of
the availability of huge numbers of databases on
the Web, many for specialized domains, a success-
ful system for this task will cover entities that a
Wikipedia NED or database-specific system cannot.
We investigate two complementary learning
strategies for Open-DB NED, both of which signifi-
cantly relax the assumptions of traditional NED sys-
tems. The first strategy, a distant supervision ap-
proach, uses the relational information in a given
database and a large corpus of unlabeled text to
learn a database-specific model. The second strat-
116
egy, a domain adaptation approach, assumes a sin-
gle source database that has accompanying labeled
data. Classifiers in this setting must learn a model
that transfers from the source database to any new
database, without requiring new training data for the
new database. Experiments show that both strategies
outperform a state-of-the-art Wikipedia NED sys-
tem by wide margins without requiring any labeled
data from the test domain, highlighting the signifi-
cant advantage of having domain-specific relational
data.
The next section contrasts Open-DB NED with
previous work. Section 3 formalizes the task. Sec-
tions 4 and 5 present our distant supervision strategy
and domain-adaptation strategy, respectively. Sec-
tion 6 introduces a technique that is a hybrid of the
two learning strategies. Section 7 describes our ex-
periments, and Section 8 concludes.
2 Previous Work
As mentioned above, restricting the catalog of ref-
erents to Wikipedia, as most recent NED systems
do (Bunescu and Pasca, 2006; Mihalcea and Cso-
mai, 2007; Fader et al2009; Han and Zhao, 2009;
Kulkarni et al2009; Ratinov et al2011), can re-
strict the coverage of the system. Zhou et al2010)
estimate that 23% of names in Yahoo! news arti-
cles have no referent in Wikipedia, and Cucerzan
(2007) estimates the rate at 16% in MSNBC news
articles. There is reason to suspect that these esti-
mates are on the low side, however, as news tends to
cover popular entities, which are most likely to ap-
pear in Wikipedia; the mentions in TAC?s 2009 en-
tity linking task are drawn from both newswire and
blogs, and have a far higher rate (57%) of missing
Wikipedia entries. Lin et al2012) find that 33% of
mentions in a corpus of 500 million Web documents
cannot be linked to Wikipedia.
NED systems that are focused on specific do-
mains (or verticals) greatly benefit from reposito-
ries of domain-specific knowledge, only a subset
of which may be found in Wikipedia. For exam-
ple, Pantel and Fuxman (2011) use a query-click
graph to resolve names in search engine queries to a
large product catalog from a commercial search en-
gine, while Dalvi et al2009; 2012) focus on movie
and restaurant databases. Bellare and McCallum
(2009) use the sequence information available in ci-
tation text to link author, title, and venue names to a
publication database. Open-DB NED systems work
on any database, so they can serve as baselines for
domain-specific NED tasks, as well as provide dis-
ambiguation for domains where no domain-specific
NED system exists.
Numerous previous studies have considered dis-
tant or weak supervision from a single relational
database as an alternative to manual supervision for
information extraction (Hoffmann et al2011; Weld
et al2009; Bellare and McCallum, 2007; Bunescu
and Mooney, 2007; Mintz et al2009; Riedel et al
2010; Yao et al2010). In contrast to these sys-
tems, our distant supervision NED system provides
a meta-algorithm for generating an NED system for
any database and any entity type.
Existing domain adaptation or transfer learning
approaches are inappropriate for the Open-DB NED
task, either because they require labeled data in both
the source and target domains (Daume? III et al
2010; Ben-David et al2010), or because they lever-
age some notion of distributional similarity between
words in the source and target domains (Blitzer et
al., 2006; Huang and Yates, 2009), which does not
apply to the database symbols across the two do-
mains. Instead, our domain adaptation technique
uses domain-independent features of relational data,
which apply regardless of the actual contents of the
database, as explained further below.
3 The Open-DB NED Problem and
Assumptions
3.1 Problem Formulation
A mention is an occurrence of a named-entity
in a document. Formally, a mention m =
(d, start, end) is a triple consisting of a document
d, as well as a start and end position for the men-
tion within the document. We say that d is the
context of m. A relational database is a 2-tuple
(S,R). Here, S is a set of symbols for constants,
attributes, and relations in the database, and R =
{r1, . . . , rn} is a set of relation instances of the form
ri = {(c1,1, . . . , c1,ki), . . . , (cni,1, . . . , cni,ki)},
where each cj is taken from S, ki is the arity of re-
lation ri and ni is the number of known instances
of ri. We will write example database symbols in
117
movie 
id title year 
1 Next Door 1975 
2 Next Door 2005 
3 Next Door 2008 
4 Next Door 2008 
5 Next Door 2010 
? ? ? 
actor 
id name 
1 Nicole Kreux 
2 Richard Ryan 
3 Kristoffer Joner 
4 Lee Perkins 
5 Carla Valentine 
? ? 
acted_in 
movie_id actor_id role 
5 1 Evelyn 
5 2 Bruce 
2 3 John 
1 4 Kid 
3 5 Elana 
? ? ? 
player 
id name height position 
1 Carlos Lee 6?2? LF 
2 Rob Bironas 6?0? K 
3 Chris Johnson 6?3? 3B 
4 Chris Johnson 5?11? RB 
5 Chris Johnson 6?1? DB 
? ? ? 
team 
id name 
1 San Diego Padres 
2 Houston Texans 
3 Tennessee Titans 
4 Oakland Raiders 
5 Houston Astros 
? ? 
plays_for 
player_id team_id 
4 3 
5 2 
3 5 
1 5 
2 3 
? ? 
Figure 1: Example movie database (above) and sports
database (below) in BCNF.
teletype, and mentions in ?quotations.? For a
particular database DB, we refer to its components
as DB.S and DB.R. For a set of databases D, de-
fine the set of referents as SD = (
?
DB?DDB.S)?
{OOD}, where OOD is a special symbol indicat-
ing something that is ?out of database?, or not found
in any of the databases in D.
Given a corpus C, a set of mentions M that oc-
cur in C, and a set of databases D, the Open-DB
NED task is to produce a function f : M ? SD,
which identifies an appropriate target symbol from
one of the databases in D, or determines that the
mention is OOD. Note that this problem formula-
tion assumes no labeled data. This is significantly
more challenging than traditional NED settings, but
allows the system to generalize easily to any new
database. In the domain adaptation section below,
we relax this condition somewhat, to allow labeled
data for a small number of initial databases; the sys-
tem must then transfer what it learns from the la-
beled domains to any new database. Also note that
the focus for this paper is disambiguation; we as-
sume that the set of mentions are correctly demar-
cated in the input text. Previous systems, such as
Lex (Downey et al2007), have investigated the task
of finding correct named-entity boundaries in text.
3.2 Assumptions
To allow our systems to handle arbitrary databases,
we need to make some assumptions about a standard
format for the data. We will assume that databases
are provided in a particular form, called Boyce-Codd
Normal Form (BCNF) (Silberschatz et al2010).
A relational schema is said to be in BCNF when
all redundancy based on functional dependency has
been removed, although other types of redundancy
may still exist. Formally, a schema R is said to
be in BCNF with respect to a set of functional de-
pendencies F if for every one of the dependencies
(X ? Y ) ? F , either
1. Y ? X , meaning this is a trivial functional de-
pendency, or
2. X is a superkey, meaning that X is a set of at-
tributes that together define a unique ID for the
relation.
In practice, this is a relatively safe assumption as
database designers often aim for even stricter normal
forms. For databases not in BCNF, such as tables
extracted from Web pages, standard algorithms ex-
ist for converting them into BCNF, given appropri-
ate functional dependencies, although there are sets
of functional dependencies for which BCNF is not
achievable. Figure 1 shows two example databases
in BCNF. We use these tables as examples through-
out the paper.
We will additionally assume that all attributes, in-
cluding names and nicknames, of entities that are
covered by the database are treated as functional de-
pendencies of the entity. Again, in practice, this
is a fairly safe assumption as this is part of good
database design, but if a database does not con-
form to this, then there will be some entities in the
database that our algorithms cannot resolve to. This
assumption implies that it is enough to use the set of
superkeys for relations as the set of possible refer-
ents; our algorithms make use of this fact.
Finally, we will assume the existence of a func-
tion ?(s, t) which indicates whether the text t is a
valid surface form of database symbol s. Our exper-
iments in Section 7.3 explore several possible simple
definitions for this function.
4 A Distant Supervision Strategy for
Open-DB NED
Our first approach to the Open-DB NED problem re-
lies on the fact that, while many mentions are indeed
ambiguous and difficult to resolve correctly, most
118
mentions have only a very small number of possi-
ble referents in a given database. ?Chris Johnson?
is the name of doubtless thousands of people, but
for articles that are reasonably well-aligned with our
sports database, most of the time the name will refer
to just three different people. Most sports names are
in fact less ambiguous still. Thus, taking a corpus of
unlabeled sports articles, we use the information in
the database to provide (uncertain) labels, and then
train a log-linear model from this probabilistically-
labeled data.
This strategy requires a set of features for the
model. Traditionally, such features would be hand-
crafted for a particular domain and database. As a
first step towards our Open-DB system, we present
a log-linear model for disambiguation, as well as a
simple feature-generation algorithm that produces a
large set of useful features from a BCNF database.
We then present a distant-supervision learning pro-
cedure for this model.
4.1 Disambiguation Model
Let SD be the set of possible referents. We construct
a vector of feature functions f(m, s) describing the
degree to which m and s ? SD appear to match
one another. The feature functions are described be-
low. The model includes a vector of weights w, one
weight per feature function, and sets the probability
of entity s given m and w as:
P (s|m,w) =
exp (w ? f(m, s))
?
s??SD
exp (w ? f(m, s?))
(1)
4.2 Database-driven Feature Generation
Figure 2 shows our algorithm for automatically gen-
erating feature functions fi(m, s) from a BCNF
database. As mentioned above, we only need to con-
sider resolving to database symbols s that are keys,
or unique IDs, for some tuple in a database. For
an entity in the database with key id, the feature
generation algorithm generates two types of feature
functions: attribute counts and similar entity counts.
Each of these features measures the similarity be-
tween the information stored in the database about
the entity id, and the information in the text in d sur-
rounding mention m.
An attribute count feature function fatti,j (m, id)
for the jth attribute of relation ri counts how many
Algorithm: Feature Generation
Input: DB, a database in BCNF
Output: F, a set of feature functions
Initialization: F? ?
Attribute Count Feature Functions:
For each relation ri ? DB.R
For each j in {1, . . . , ki}
Define function fatti,j (m, id):
count? 0
Identify the tuple t ? ri containing id
val? tj
count? count +
ContextMatches(val,m)
return count
F? F ? {fatti,j }
Similar-Entity Count Feature Functions:
For each relation ri ? DB.R
For each j in {1, . . . , ki}
Define function fsimi,j (m, id):
count? 0
Identify the tuple t ? ri containing id
val? tj
Identify the set of similar tuples T ?:
T ? = {t?|t? ? ri, t?j = val}
For each tuple t? ? T ?
For each j? ? {1, . . . , ki}
val? ? t?j
count? count +
ContextMatches(val?,m)
return count
F? F ? {fsimi,j }
Figure 2: Feature generation algorithm. The
ContextMatches(s,m) function counts how many
times a string that matches database symbol s appears
in the context of m. In our implementation, we use all
of d(m) as the context. Matching between strings and
database symbols is discussed in Sec. 7.3.
attributes of the entity id appear near m. For exam-
ple, if id is 5 in the movie relation in Figure 1, the
feature function for attribute year would count how
often 2010 matches the text surrounding mention
m. Defining precisely whether a database symbol
?matches? a word or phrase is a subtle issue; we ex-
plore several possibilities in Section 7.3. In addition
119
to attribute counts for attributes within a single rela-
tion, we also use attributes from relations that have
been inner-joined on primary key and foreign key
pairs. For example, for movies, we include attributes
such as director name, genre, and actor name. High
values for these attribute count features indicate that
the text around m closely matches the information
in the database about entity id, and therefore id is a
strong candidate for the referent of m. We use the
whole document as the context for finding matches,
although other variants are worth future investiga-
tion.
A similar entity count feature function
fsimi,j (m, id) for the jth attribute in relation ri
counts how many entities similar to id are men-
tioned in the neighborhood of m. As an example,
consider a mention of ?Chris Johnson?, id = 3,
and the similar entity feature for the position
attribute of the players relation in the sports
database. The feature function would first identify
that 3B is the position of the player with id = 3. It
would then identify all players that had the same
position. Finally, it would count how often any
attributes of this set of players appear near ?Chris
Johnson?. Likewise, the similar entity feature for
the team id attribute would count how many
teammates of the player with id = 3 appear near
?Chris Johnson?. A high count for this teammate
feature is a strong clue that id is the correct referent
for m, while a high count for players of the same
position is a weak but still valuable clue.
4.3 Parameter Estimation via Distant
Supervision
Using string similarity, we can heuristically deter-
mine that three IDs with name attribute Chris
Johnson are highly likely to be the correct target
for a mention of ?Chris Johnson?. Our distant su-
pervision parameter estimation strategy is to move
as much probability mass as possible onto the set
of realistic referents obtained via string similarity.
Since our features rely on finding attributes and sim-
ilar entities, the side effect of this strategy is that
most of the probability mass for a particular mention
is moved onto the one target ID with high attribute
count and similar entity count features, thus disam-
biguating the entity. Although the string-similarity
heuristic is typically noisy, the strong information in
the database and the fact that many entity mentions
are typically not ambiguous allows the technique to
learn effectively from unlabeled text.
Let ?(m,DB) be a heuristic string-matching
function that returns a set of plausible ID values in
databaseDB for mentionm. The objective function
for this training procedure is a modified marginal log
likelihood (MLL) function that encourages probabil-
ity mass to be placed on the heuristically-matched
targets:
MLL(M,w) =
?
m?M
log
?
id??(m,DB)
P (id|m,w)
This objective is smooth but non-convex. We use
a gradient-based optimization procedure that finds a
local maximum. Our implementation uses an open-
source version of the LBFG-S optimization tech-
nique (Liu and Nocedal, 1989). The gradient of our
objective is given by
?LL(M,w)
?wi
=
?
m?M
Eid??(m,DB) [fi(m, id)]
?Eid?DB.S [fi(m, id)]
where the expectations are taken according to
P (id|m,w).
5 A Domain-Adaptation Strategy for
Open-DB NED
Our domain-adaptation strategy builds an Open-DB
NED system by training it on labeled examples from
an initial database or small set of initial databases.
Unlike traditional NED, however, the purpose in
Open-DB NED is to resolve to any database. Thus
the strategy must take care to build a model that
can transfer what it has learned to a new database,
without requiring additional labeled data for the new
database.
At first, the problem seems intractable ? just
because a system can disambiguate between ?Next
Door?, the 2005 Norwegian film, and ?Next Door?,
the 1975 short film by director Andrew Silver, that
seems to provide little benefit for disambiguating be-
tween different athletes named ?Andre Smith.? The
crux of the problem lies in the fact that database-
driven features are domain-specific. Counting how
many times the director of a movie appears is highly
120
useful in the movie domain, but worthless in the
sports domain.
Our solution works by re-defining the problem in
such a way that we can define domain-independent
and database-independent features. For example,
rather than counting how often the director of
a movie appears in the context around a movie
mention, we create a domain-independent Count
Att(m, s) feature function that counts how often any
attribute of s appears in the context of m. For
movies, Count Att will add together counts for ap-
pearances of a movie?s production year and IMDB
rating, among other attributes. In the sports domain,
Count Att will add together counts for appearances
of a player?s height, position, salary, etc.. But in ei-
ther domain, the feature is well-defined, and in either
domain, larger values of the feature indicate a better
match between m and s. Thus there is a hope for
training a model with domain-independent features
like Count Att on labeled data from one domain, say
movies, and producing a model that has high accu-
racy on the sports domain.
We first formalize the notion of a domain adap-
tation NED model, and then describe our algorithm
for producing such a model. We say that a domain
consists of a database DB as well as a distribution
D(M), whereM is the space of mentions. For in-
stance, the movie domain might consist of the Inter-
net Movie Database (IMDB) and a distribution that
places most probability mass on documents about
movies and Hollywood stars. In domain adapta-
tion, a system observes a set of training examples
(m, s, g(m, s)), where instances m ? M are drawn
from a source domain?s distribution DS and refer-
ents s are drawn from the source domain?s database
DBS . The labels g(m, s) are boolean values in-
dicating a correct or incorrect match between the
mention and referent. The system must then learn
a hypothesis for classifying examples (m, s) drawn
from a target domain?s distributionDT and database
DBT . Note that for domain adaptation, we can-
not use the more traditional problem formulation in
which the referent s is a label (i.e., s = g(m)) for the
mention, since the set of possible referents changes
from domain to domain, and therefore the output of
g would be completely different from one domain to
the next.
Table 1 lists the domain-independent features
Domain-Independent Feature Functions
Count Att:
?
i,j f
att
i,j (m, s)
Count Sim:
?
i,j f
sim
i,j (m, s)
Count All: Count Att + Count Sim
Count Unique:
?
i,j
{
0 if fatti,j (m, s) = 0,
1 if fatti,j (m, s) > 0.
Count Num:
?
i,j|jis a numeric att. f
att
i,j (m, s)
Table 1: Primary feature functions for a domain adapta-
tion approach to NED. These features made the biggest
difference in our experiments, but we also tested varia-
tions such as counting unique numeric attribute appear-
ances, counting unique similar entities, counting relation
name appearances, counting extended attributed appear-
ances, and others.
used in our domain adaptation model. These fea-
tures use the attribute counts and similar entity
counts from the distant supervision model as subrou-
tines. By aggregating over those domain-dependent
feature functions, the domain adaptation system ar-
rives at feature functions that can be defined for any
database, rather than for a specific database.
Note that there is a tradeoff between the do-
main adaptation technique and the distant super-
vision technique. The domain adaptation model
has access to labeled data, unlike the distant su-
pervision model. In addition, the domain adapta-
tion model requires no text whatsoever from the tar-
get domain, not even an unlabeled corpus, to set
weights for the target domain. Once trained, it is
ready for NED over any database that meets our as-
sumptions, out of the box. However, because the
model needs to be able to transfer to arbitrary new
domains, the domain adaptation model is restricted
to domain-independent features, which are ?coarser-
grained.? That is, the distant supervision model has
the ability to place more weight on attributes like
director rather than genre, or team rather than po-
sition, if those attributes are more discriminative.
The domain adaptation model cannot place differ-
ent weights on the different attributes, since those
weights would not transfer across databases.
As with distant supervision, the domain adapta-
tion strategy uses a log-linear model over these fea-
ture functions. We use standard techniques for train-
ing the model using labeled data from the source do-
121
main: conditional log likelihood (CLL) as the objec-
tive function, and LBFG-S for convex optimization.
CLL(L,w) =
?
(m,id,label)?L
logP (label|m, id,w)
The training algorithm is guaranteed to converge to
the globally optimal parameter setting for this objec-
tive function over the training data. The manually
annotated data contains only positive examples; to
generate negative examples, we use the same name-
matching heuristic ?(m,DB) to identify a set of po-
tentially confusing bad matches. On test data, we
use the trained model to choose the id for a given m
with the highest probability of being correct.
6 A Hybrid Model
The distant supervision and domain adaptation
strategies use two very different sources of evidence
for training a disambiguation classifier: the string-
matching heuristic and unlabeled text from the target
domain for the the distant supervision model, and
aggregate features over labeled text from a separate
domain for domain adaptation. This begs the ques-
tion, do these sources of evidence complement one
another? To address this question, we design a Hy-
brid model with features and training strategies from
both distant supervision and domain adaptation.
The training data consists of a set LS of labeled
mentions from a source domain, a source database
DBS , a set of unlabeled mentions MT from the tar-
get domain, and the target-domain database DBT .
The full feature set of the Hybrid model is the union
of the distant supervision feature functions for the
target domain and the domain-independent domain
adaptation feature functions. Note that the distant
supervision feature functions are domain-specific,
so they almost always will be uniformly zero on LS ,
but the domain adaptation feature functions will be
activated on both LS and MT . The combined train-
ing objective for the Hybrid model is:
LL(LS ,MT ,w) = CLL(LS ,w) +MLL(MT ,w)
7 Experiments
Our experiments compare our strategies for Open-
DB NED against one another, as well as against a
Wikipedia NED system from previous work, on two
domains: sports and movies.
7.1 Data
For the movie domain, we collected a set of
156 cult movie titles from an online movie site
(www.olivefilms.com). For each movie title, we ex-
ecuted a Web search using a commercial search en-
gine, and collected the top five documents for each
title from the search engine?s results. Nearly all top-
five results included at least one mention of an en-
tity not found in Wikipedia; overall, only 16% of the
mentions could be linked to Wikipedia. After strip-
ping javascript and html annotations, we removed
documents with fewer than 50 words, leaving a to-
tal of 770 documents. We select one occurrence of
any of the 156 movie titles from each document as
our set of mentions. Many titles are ambiguous not
just among different movies with the same name, but
also among novels, plays, geographical entities, and
assorted other types of entities. To provide labels for
these mentions, we use both a movie database and
Wikipedia. We downloaded the complete data dump
from the online Internet Movie Database (IMDB,
www.imdb.com). For our set of possible referents,
we use the set of all key values in IMDB, and the set
of all Wikipedia articles. Annotators manually la-
beled each mention using this set of referents. Table
2 shows summary statistics about this labeled data.
For the sports domain, we downloaded all player
data from Yahoo!, Inc.?s sports database for the
years 2011-2012 and two American sports leagues,
the National Football League (NFL) and Major
League Baseball (MLB). From the database, we ex-
tracted ambiguous player names and team names,
including names like ?Philadelphia? which may re-
fer to Philadelphia Eagles in the NFL data,
Philadelphia Phillies in the MLB data, or
the city of Philadelphia itself (in both types of
data). We then collected 1300 Yahoo! news arti-
cles which include a mention that partially matches
at least one of these database symbols. We manu-
ally labeled a random sample of 564 mentions from
this data, including 279 player name mentions and
285 city name mentions. Many player name and
place name mentions are ambiguous between the
two sports leagues, as well as with teams or play-
ers from other leagues. In order to focus on the
hardest cases, we specifically exclude mentions like
?Philadelphia? from the labeled data if any of their
122
domain |M | E|?(m,DB)| OOD Wiki
movies 770 2.6 13% 16%
sports 549 4.5 0% 100%
Table 2: Number of mentions, average number of refer-
ents per mention, % of mentions that are OOD, and %
of mentions that are in Wikipedia in our movie and sports
data.
unambiguous completions appears in the same arti-
cle (that is, if either of the team names ?Philadelphia
Eagles? or ?Philadelphia Phillies? appears in the
same article, we exclude the ?Philadelphia? men-
tion). As before, the set of possible referents in-
cludes the symbol OOD, key values from the sports
database, and Wikipedia articles, and a given men-
tion may be labeled with both a sports entity and a
Wikipedia article, if appropriate. All of our data is
available from the last author?s website.
7.2 Evaluation Metric
We report on a version of exact-match accuracy. The
system chooses the most likely label s? for each m.
This is judged correct if s? matches the correct label
s exactly, or (in cases where both a Wikipedia and a
database entity are considered correct) if one of the
labels matches s? exactly. This metric allows systems
to resolve against either reference, Wikipedia or an-
other database, without requiring it to match both if
the same entity appears in both references.
7.3 Exact or Partial Matching?
One important question in the design of our systems
is how to determine the ?match? between database
symbols and text. This question comes into play in
two components of our systems: it affects the com-
putation of feature functions that count how often a
match of some attribute is found in text, and it af-
fects which set of heuristically-determined database
entities are considered to be possible matches for a
given mention.
We experiment with two different matching
strategies between a symbol s and text t, exact
matching and partial matching. Exact matching
?exact(s, t) requires the sequence of characters in s
to appear exactly (modulo character encoding) in t.
For instance, the database value Chris Johnson
System Accuracy
No-Wikipedia Domain Adapt. 0.61
DocSim-Wikipedia Domain Adapt. 0.69
Table 3: Including a simple document-similarity feature
for comparing a mention?s context with a Wikipedia page
provides an 8% improvement over ignoring Wikipedia in-
formation.
would match ?Chris Johnson?, but not ?C. John-
son? or ?Johnson? in text. For partial matching,
we used different tests for numeric and textual en-
tities. For numeric entities, ?partial matched s and
t if the numeric value of one was within 10% of
the other, so that 5312 would match ?5,000.? We
made no attempt to convert numeric phrases, such
as ?3.6 million?, into numeric values. For textual
entities, ?partial matched s and t if at least one
token from each matched exactly. Thus Chris
Johnson matches both ?Chris? and ?C. Johnson?.
We found ?partial to be consistently superior for
computing ?(m,DB), since it has much better re-
call for mentions like ?Philadelphia?. On the other
hand, if we use ?partial for computing our models?
feature functions, like the Count Att(m, s) in the do-
main adaptation model, counts varied widely across
domains. A simple version of the domain adapta-
tion classifier (only the Count All and Count Unique
features) trained on sports data and tested on movies
achieved an accuracy of 24% using ?partial, com-
pared with 61% using ?exact. For all remaining
tests, we used ?exact for computing features, and
?partial for computing ?(m,DB).
7.4 Incorporating Wikipedia referents
Thus far, all of our features work on relational data,
not Wikipedia. In order to allow our systems to link
to Wikipedia, we create a single ?document simi-
larity? feature describing the similarity between the
text around a mention and the text appearing on a
Wikipedia page. We build a vector space model of
both the document containing the mention and the
Wikipedia page, remove stopwords, and use cosine
similarity to compute this feature.
To evaluate the effectiveness of this Wikipedia
feature, we tested two versions of our domain adap-
tation system, both trained on sports data and tested
123
0.13 
0.4 0.43 
0.54 0.65 
0.71 0.72 0.73 
0 
0.21 
0.33 
1 
0.54 0.63 0.62 
0.66 
00.1
0.20.3
0.40.5
0.60.7
0.80.9
1
Accu
racy
 
Open-DB NED Test 
Movies Sports
Figure 3: All three Open-DB NED strategies out-
perform a state-of-the-art Wikipedia NED system by
25% or more on sports and movies, and outperform
a Wikipedia NED system with oracle information by
14% or more on the movie data. Differences between
the Modified Zhou Wikifier and the Open-DB strategies
are statistically significant (p < 0.01, Fisher?s exact test)
on both domains.
on the movies domain. The first version involves
no Wikipedia information whatsoever, thus it has no
reason to select a Wikipedia article over OOD. The
second system includes the document similarity fea-
ture. Table 3 shows the results of these systems. En-
couragingly, our single document similarity feature
produces a significant improvement over the model
without Wikipedia information, so we use this fea-
ture in all of our systems tested below. More so-
phisticated use of Wikipedia is certainly possible,
and an important question for future work is how
to combine Open-DB NED more seamlessly with
Wikipedia NED.
7.5 Comparing Open-DB NED Strategies
For each domain, we compare our domain-
adaptation strategy, distant supervision, and hy-
brid strategies. The domain-adaptation model is
trained on the labeled data for sports when testing
on movies, and vice versa. We use a movies test set
of 180 mentions that is separate from the develop-
ment data used for the above tests. For the distant
supervision strategy, we use the entire collection of
texts from each domain as input (1300 articles for
sports, 770 articles for movies), with the labels re-
moved during training.
We compare against a state-of-the-art Wikipedia
NED system used in production by a major Web
company. This system is a modified version of the
system described by Zhou et al2010), where cer-
tain features have been removed for efficiency. We
refer to this as the Modified-Zhou Wikifier. This
system uses a gradient-boosted decision tree and
multiple local and global features for computing
the similarity between a mention?s context and a
Wikipedia article. We also test a hypothetical sys-
tem, Oracle Wikifier, which is given no information
about entities in IMDB, but is assumed to be able
to correctly resolve any mention that refers to an
entity found in Wikipedia. Thus, this system has
perfect accuracy on mentions that can be found in
Wikipedia, and accuracy similar to a baseline that
predicts randomly on all mentions that fall outside
of Wikipedia1. Oracle-Wikifier serves as an upper
bound on systems that have no access to a domain-
specific database. In addition, we compare against
two standard baselines: a classifier that always pre-
dicts OOD, and a classifier that chooses randomly.
Finally, we compare against a system that trains the
domain adaptation model using distant supervision
(?DA Trained with DS?).
Figure 3 shows our results. All three Open-DB
approaches outperform the baseline techniques on
this test by wide margins, with the Hybrid model in-
creasing by 30% or more over the random baseline.
On the movie domain, the Hybrid model outper-
forms the Oracle Wikifier by nearly 20%. Encour-
agingly, the Hybrid model consistently outperforms
both distant supervision and domain adaptation, sug-
gesting that the two sources of evidence are partially
complementary. Distant supervision performs better
on the movies test, whereas domain adaptation has
the advantage on sports. The differences among all
three Open-DB approaches is relatively small, com-
pared with the difference between these approaches
and Oracle Wikifier on the movie data.
The domain adaptation system outperforms DA
Trained with DS on both domains, suggesting
that labeled data from a separate domain is bet-
ter evidence for parameter estimates than unlabeled
data from the same domain. The distant super-
vision system also outperforms DA Trained with
1Alternatively, one could make the oracle system predict
OOD on all mentions that fall outside of Wikipedia. Random
predictions perform better on our data.
124
DS on both domains, suggesting that the fine-
grained, domain-specific features do in fact provide
more helpful information than the coarser-grained,
domain-independent features of the domain adapta-
tion model.
All of the Open-DB NED systems outperform the
Modified Zhou Wikifier on both data sets by a wide
margin. In fact the Modified Zhou Wikifier has sim-
ilar results on both domains, despite the fact that
Wikipedia has far greater coverage on sports than
movies. In part, the poor performance of the Modi-
fied Zhou Wikifier reflects the difficult nature of the
task. In previous experiments on an MSNBC news
test set it reached 85% accuracy, but a random clas-
sifier there achieved 60% accuracy compared with
21% on our sports data. Another difficulty with
the Modified Zhou Wikifier is its strong preference
for globally common entities. It consistently clas-
sifies mentions that are ambiguous between a city
and a team (like ?Chicago? in ?Chicago sweeps the
Red Sox?) as cities when they should be resolved
to teams, in large part because Chicago is a more
common referent in general text than either of the
baseball teams that play in that city. In sports arti-
cles, however, both meanings are common, and only
the surrounding context can help determine the cor-
rect referent.
Besides wikifiers, NED systems may also be
compared with dictionary-based word sense disam-
biguation techniques like the Lesk algorithm2 (Lesk,
1986). The Lesk algorithm is ?open? in the sense
that it works for arbitrary dictionaries, and it defines
a vector space model of the dictionary definitions
that may be likened to the attribute-value model in
our representation of entities in the database. Our
approach, however, estimates parameters for a sta-
tistical model from data, whereas the Lesk algorithm
uses an equal weight for all attributes. To make an
empirical comparison, we created a variant of the
Lesk algorithm for relational data: we took the dis-
ambiguation model from Eqn. 1, supplied all of
the features from the distant supervision model, and
manually set w = 1. This ?relational Lesk? model
achieves an accuracy of 0.11 on movies, and 0.15
on sports, significantly below the random baseline.
Giving equal weight to noisy attributes like genre
2We thank the reviewers for making this connection.
and more discriminative attributes like director
significantly hurts the performance.
For both the movie and sports domain, approx-
imately 80% of the Hybrid model?s errors are be-
cause of predicting database symbols, when the cor-
rect referent is a Wikipedia page or OOD. This
nearly always occurs because some words in the
context of a mention match an attribute of an in-
correct database referent. For instance, the crime
genre is an attribute for several movies, but it also
matches in contexts surrounding book titles and nu-
merous other entities. In the movie domain, most of
the remaining errors are incorrect OOD predictions
for mentions that should resolve to the database, but
the article contains no attributes or similar entities
to the database entity. In the sports domain, many
of the remaining errors were due to predicting in-
correct player referents. Quite often, this was be-
cause the document discusses a fantasy sports league
or team, where players from different professional
sports teams are mixed together on a ?fantasy team?
belonging to a fan of the sport. Since players in the
fantasy leagues have different teammates than they
do in the database, these articles consistently con-
fuse our methods.
8 Conclusion and Future Work
This paper introduces the task of Open-DB Named
Entity Disambiguation, and presents two distinct
strategies for solving this task. Experiments indicate
that a mixture of the two strategies significantly out-
performs a state-of-the-art Wikipedia NED system,
on a dataset where Wikipedia has good coverage and
on another dataset where Wikipedia has poor cover-
age. The results indicate that there is a significant
benefit to leveraging other sources of knowledge in
addition to Wikipedia, and that it is possible to lever-
age this knowledge without requiring labeled data
for each new source. The initial success of these
Open-DB NED approaches indicates that this task is
a promising area for future research, including ex-
citing extensions that link large numbers of domain-
specific databases to text.
Acknowledgments
This work was supported in part by a gift from Ya-
hoo!, Inc.
125
References
Kedar Bellare and Andrew McCallum. 2007. Learn-
ing extractors from unlabeled text using relevant data-
bases. In Sixth International Workshop on Information
Integration on the Web.
Kedar Bellare and Andrew McCallum. 2009. General-
ized Expectation Criteria for Bootstrapping Extractors
using Record-Text Alignment. In Empirical Methods
in Natural Language Processing (EMNLP-09).
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from different
domains. Machine Learning, 79:151?175.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal super-
vision. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL-
07).
R. Bunescu and M. Pasca. 2006. Using encyclope-
dic knowledge for named entity disambiguation. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-06).
Ying Chen and James Martin. 2007. Towards Ro-
bust Unsupervised Personal Name Disambiguation. In
EMNLP, pages 190?198.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
708?716.
Nilesh N. Dalvi, Ravi Kumar, Bo Pang, and Andrew
Tomkins. 2009. Matching Reviews to Objects using a
Language Model. In EMNLP, pages 609?618.
Nilesh N. Dalvi, Ravi Kumar, and Bo Pang. 2012. Object
matching in tweets with spatial models. In WSDM,
pages 43?52.
Hal Daume? III, Abhishek Kumar, and Avishek Saha.
2010. Frustratingly easy semi-supervised domain
adaptation. In Proceedings of the ACL Workshop on
Domain Adaptation (DANLP).
D. Downey, M. Broadhead, and O. Etzioni. 2007. Lo-
cating complex named entities in web text. In Procs.
of the 20th International Joint Conference on Artificial
Intelligence (IJCAI 2007).
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2009. Scaling wikipedia-based named entity disam-
biguation to arbitrary web text. In Proceedings of
the WikiAI 09 - IJCAI Workshop: User Contributed
Knowledge and Artificial Intelligence: An Evolving
Synergy.
Xianpei Han and Jun Zhao. 2009. Named entity dis-
ambiguation by leveraging Wikipedia semantic knowl-
edge. In Proceeding of the 18th ACM Conference
on Information and Knowledge Management (CIKM),
pages 215?224.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino,
Hagen Furstenau, Manfred Pinkal, Marc Spaniol,
Bilyana Taneva, Stefan Thater, and Gerhard Weikum1.
2011. Robust Disambiguation of Named Entities in
Text. In EMNLP, pages 782?792.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
Based Weak Supervision for Information Extraction of
Overlapping Relations. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics (ACL).
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised se-
quence labeling. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective annotation
of wikipedia entities in web text. In Proceedings of
the 15th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (KDD), pages
457?466.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater,
and Mark Steedman. 2011. Lexical Generalization
in CCG Grammar Induction for Semantic Parsing. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).
M.E. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
SIGDOC Conference.
Thomas Lin, Mausam, and Oren Etzioni. 2012. Entity
linking at web scale. In Knowledge Extraction Work-
shop (AKBC-WEKEX), 2012.
D.C. Liu and J. Nocedal. 1989. On the limited mem-
ory method for large scale optimization. Mathemati-
cal Programming B, 45(3):503?528.
G.S. Mann and D. Yarowsky. 2003. Unsupervised per-
sonal name disambiguation. In CoNLL.
Paul McNamee, Mark Dredze, Adam Gerber, Nikesh
Garera, Tim Finin, James Mayfield, Christine Pi-
atko, Delip Rao, David Yarowsky, and Markus Dreyer.
2009. HLTCOE Approaches to Knowledge Base Pop-
ulation at TAC 2009. In Text Analysis Conference.
Rada Mihalcea and Andras Csomai. 2007. Wikify!:
Linking documents to encyclopedic knowledge. In
126
Proceedings of the Sixteenth ACM Conference on
Information and Knowledge Management (CIKM),
pages 233?242.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics (ACL-2009), pages 1003?1011.
Patrick Pantel and Ariel Fuxman. 2011. Jigs and Lures:
Associating Web Queries with Structured Entities. In
ACL.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to wikipedia. In Proc. of the Annual Meeting of the
Association of Computational Linguistics (ACL).
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the Sixteenth Euro-
pean Conference on Machine Learning (ECML-2010),
pages 148?163.
Avi Silberschatz, Henry F. Korth, and S. Sudarshan.
2010. Database System Concepts. McGraw-Hill,
sixth edition.
Daniel S. Weld, Raphael Hoffmann, and Fei Wu. 2009.
Using Wikipedia to Bootstrap Open Information Ex-
traction. In ACM SIGMOD Record.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2010), pages 1013?1023.
Yiping Zhou, Lan Nie, Omid Rouhani-Kalleh, Flavian
Vasile, and Scott Gaffney. 2010. Resolving surface
forms to wikipedia topics. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (Coling), pages 1335?1343.
127
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1313?1323, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Biased Representation Learning for Domain Adaptation
Fei Huang , Alexander Yates
Temple University
Computer and Information Sciences
324 Wachman Hall
Philadelphia, PA 19122
{fhuang,yates}@temple.edu
Abstract
Representation learning is a promising tech-
nique for discovering features that allow su-
pervised classifiers to generalize from a source
domain dataset to arbitrary new domains. We
present a novel, formal statement of the rep-
resentation learning task. We argue that be-
cause the task is computationally intractable
in general, it is important for a representa-
tion learner to be able to incorporate expert
knowledge during its search for helpful fea-
tures. Leveraging the Posterior Regularization
framework, we develop an architecture for in-
corporating biases into representation learn-
ing. We investigate three types of biases, and
experiments on two domain adaptation tasks
show that our biased learners identify signif-
icantly better sets of features than unbiased
learners, resulting in a relative reduction in er-
ror of more than 16% for both tasks, with re-
spect to existing state-of-the-art representation
learning techniques.
1 Introduction
Supervised natural language processing (NLP) sys-
tems have been widely used and have achieved im-
pressive performance on many NLP tasks. Howev-
er, they exhibit a significant drop-off in performance
when tested on domains that differ from their train-
ing domains. (Gildea, 2001; Sekine, 1997; Pradhan
et al 2007) One major cause for poor performance
on out of-domain texts is the traditional representa-
tion used by supervised NLP systems (Ben-David et
al., 2007). Most systems depend on lexical features,
which can differ greatly between domains, so that
important words in the test data may never be seen
in the training data. The connection between word-
s and labels may also change across domains. For
instance, ?signaling? appears only as a present par-
ticiple (VBG) in WSJ text (as in, ?signaling that...?),
but predominantly as a noun (as in ?signaling path-
way?) in biomedical text.
Recently, several authors have found that learning
new features based on distributional similarity can
significantly improve domain adaptation (Blitzer et
al., 2006; Huang and Yates, 2009; Turian et al
2010; Dhillon et al 2011). This framework is at-
tractive for several reasons: experimentally, learned
features can yield significant improvements over s-
tandard supervised models on out-of-domain test-
s. Moreover, since the representation-learning tech-
niques are unsupervised, they can easily be applied
to arbitrary new domains. There is no need to supply
additional labeled examples for each new domain.
Traditional representations still hold one signif-
icant advantage over representation-learning, how-
ever: because features are hand-crafted, these rep-
resentations can readily incorporate the linguistic
or domain expert knowledge that leads to state-of-
the-art in-domain performance. In contrast, the on-
ly guide for existing representation-learning tech-
niques is a corpus of unlabeled text.
To address this shortcoming, we introduce
representation-learning techniques that incorporate
a domain expert?s preferences over the learned fea-
tures. For example, out of the set of all possi-
ble distributional-similarity features, we might pre-
fer those that help predict the labels in a labeled
training data set. To capture this preference, we
might bias a representation-learning algorithm to-
wards features with low joint entropy with the labels
in the training data. This particular biased form of
1313
representation learning is a type of semi-supervised
learning that allows our system to learn task-specific
representations from a source domain?s training da-
ta, rather than the single representation for all tasks
produced by current, unsupervised representation-
learning techniques.
We present a novel formal statement of represen-
tation learning, and demonstrate that it is computa-
tionally intractable in general. It is therefore criti-
cal for representation learning to be flexible enough
to incorporate the intuitions and knowledge of hu-
man experts, to guide the search for representations
efficiently and effectively. Leveraging the Posteri-
or Regularization framework (Ganchev et al 2010),
we present an architecture for learning representa-
tions for sequence-labeling tasks that allows for bi-
ases. In addition to a bias towards task-specific rep-
resentations, we investigate a bias towards repre-
sentations that have similar features across domain-
s, to improve domain-independence; and a bias to-
wards multi-dimensional representations, where d-
ifferent dimensions are independent of one another.
In this paper, we focus on incorporating the bias-
es with HMM-type representations (Hidden Markov
Model). However, this technique can also be ap-
plied to other graphical model-based representations
with little modification. Our experiments show that
on two different domain-adaptation tasks, our biased
representations improve significantly over unbiased
ones. In a part-of-speech tagging experiment, our
best model provides a 25% relative reduction in er-
ror over a state-of-the-art Chinese POS tagger, and
a 19% relative reduction in error over an unbiased
representation from previous work.
The next section describes background and previ-
ous work. Section 3 introduces our framework for
learning biased representations. Section 4 describes
how we estimate parameters for the biased objective
functions efficiently. Section 5 details our experi-
ments and results, and section 6 concludes and out-
lines directions for future work.
2 Background and Previous Work
2.1 Terminology and Notation
A representation is a set of features that describe da-
ta points. Formally, given an instance set X , it is a
functionR : X ? Y for some suitable space Y (of-
ten Rd), which is then used as the input space for a
classifier. For instance, a traditional representation
for POS tagging over vocabulary V would include
(in part) |V | dimensions, and would map a word to a
binary vector with a 1 in only one of the dimensions.
By a structured representation, we mean a function
R that incorporates some form of joint inference. In
this paper, we use Viterbi decoding of variants of
Hidden Markov Models (HMMs) for our structured
representations, although our techniques are appli-
cable to arbitrary (Dynamic) Bayes Nets. A domain
is a probability distribution D over the instance set
X ; R(D) denotes the induced distribution over Y .
In domain adaptation tasks, a learner is given sam-
ples from a source domain DS , and is evaluated on
samples from a target domain DT .
2.2 Theoretical Background
Ben-David et al(2010) give a theoretical analysis
of domain adaptation which shows that the choice
of representation is crucial. A good choice is one
that minimizes error on the training data, but equally
important is that the representation must make data
from the two domains look similar. Ben-David et al
show that for every hypothesis h, we can provably
bound the error of h on the target domain by its error
on the source domain plus a measure of the distance
between DS and DT :
Ex?DTL(x,R, f, h) ? Ex?DSL(x,R, f, h)
+ d1(R(DS), R(DT ))
where L is a loss function, f is the target function,
and the variation divergence d1 is given by
d1(D,D
?) = 2 sup
B?B
|PrD[B]? PrD? [B]| (1)
where B is the set of measurable sets under D,D?.
2.3 Problem Formulation
Ben-David et als theory provides learning bound-
s for domain adaptation under a fixed R. We now
reformulate this theory to define the task of repre-
sentation learning for domain adaptation as the fol-
lowing optimization problem: given a set of unla-
beled instances US drawn from the source domain
and unlabeled instances UT from the target domain,
as well as a set of labeled instances LS drawn from
1314
the source domain, identify a function R? from the
space of possible representationsR:
R? = argmin
R?R
{min
h?H
(Ex?DSL(x,R, f, h))
+ d1(R(DS), R(DT ))}
(2)
Unlike most learning problems, where the repre-
sentation R is fixed, this problem formulation in-
volves a search over the space of representation-
s and hypotheses. The equation also highlights an
important underlying tension: the best representa-
tion for the source domain would naturally include
domain-specific features, and allow a hypothesis to
learn domain-specific patterns. We are aiming, how-
ever, for the best general classifier, that happens to
be trained on training data from one or a few do-
mains. Domain-specific features would contribute
to distance between domains, and to classifier errors
on data taken from unseen domains. By optimizing
for this combined objective function, we allow the
optimization method to trade off between features
that are best for classifying source-domain data and
features that allow generalization to new domains.
Naturally, the objective function in Equation 2 is
completely intractable. Just finding the optimal hy-
pothesis for a fixed representation of the training da-
ta is intractable for many hypothesis classes. And
the d1 metric is intractable to compute from samples
of a distribution, although Ben-David et alpropose
some tractable bounds (2007; 2010). We view Equa-
tion 2 as a high-level goal rather than a computable
objective. We leverage prior knowledge to bias the
representation learner towards attractive regions of
the representations space R, and we develop effi-
cient, greedy optimization techniques for learning
effective representations.
2.4 Previous Work
There is a long tradition of research on representa-
tions for NLP, mostly falling into one of three cat-
egories: 1) vector space models and dimensionality
reduction techniques (Salton and McGill, 1983; Tur-
ney and Pantel, 2010; Sahlgren, 2005; Deerwester et
al., 1990; Honkela, 1997) 2) using structured repre-
sentations to identify clusters based on distributional
similarity, and using those clusters as features (Lin
and Wu, 2009; Candito and Crabbe?, 2009; Huang
and Yates, 2009; Ahuja and Downey, 2010; Turi-
an et al 2010; Huang et al 2011); 3) and struc-
tured representations that induce multi-dimensional
real-valued features (Dhillon et al 2011; Emami et
al., 2003; Morin and Bengio, 2005). Our work fall-
s into the second category, but builds on the pre-
vious work by demonstrating how to improve the
distributional-similarity clusters with prior knowl-
edge. To our knowledge, we are the first to apply
semi-supervised representation learning techniques
for structured NLP tasks.
Most previous work on domain adaptation has fo-
cused on the case where some labeled data is avail-
able in both the source and target domains (Daume?
III, 2007; Jiang and Zhai, 2007; Daume? III et al
2010). Learning bounds are known (Blitzer et al
2007; Mansour et al 2009). A few authors have
considered domain adaptation with no labeled data
from the target domain (Blitzer et al 2006; Huang
et al 2011) by using features based on distributional
similarity. We demonstrate empirically that incorpo-
rating biases into this type of representation-learning
process can significantly improve results.
3 Biased Representation Learning
As before, let US and UT be unlabeled data, and LS
be labeled data from the source domain only. Pre-
vious work on representation learning with Hidden
Markov Models (HMMs) (Huang and Yates, 2009)
has estimated parameters ? for the HMM from un-
labeled data alone, and then determined the Viterbi-
optimal latent states for training and test data to pro-
duce new features for a supervised classifier. The
objective function for HMM learning in this case is
marginal log-likelihood, optimized using the Baum-
Welch algorithm:
L(?) =
?
x?US?UT
log
?
y
p(x,Y = y|?) (3)
where x is a sentence, Y is the sequence of latent
random variables for the sentence, and y is an in-
stance of the latent sequence. The joint distribution
in an HMM factors into observation and transition
distributions, typically mixtures of multinomials:
p(x,y|?) = P (y1)P (x1|y1)
?
i?2
P (yi|yi?1)P (xi|yi)
1315
Innocent
bystanders are often the
JJ NNS RB VBP DT
y1
y2 y3 y4 y5
...
victims
y6
NNS
Innocent
bystanders are often the victims
...
E?
entropy
(Y,z)
P(Y)
p
1
p
2
p
3
p
m
p
n
KL(p
m
|| p
n
)
Monday, March 26, 12
Figure 1: Illustration of how the entropy bias is incor-
porated into HMM learning. The dotted oval shows the
space of desired distributions in the hidden space, which
have small or zero entropy with the real labels. The learn-
ing algorithm aims to maximize the log-likelihood of the
unlabeled data, and to minimize the KL divergence be-
tween the real distribution, pm, and the closest desired
distribution, pn.
Intuitively, this form of representation learning i-
dentifies clusters of distributionally-similar words:
those words with the same Viterbi-optimal latent s-
tate. The Viterbi-optimal latent states are then used
as features for the supervised classifier. Our previ-
ous work (2009) has shown that the features from
the learned HMM significantly improve the accura-
cy of POS taggers and chunkers on benchmark do-
main adaptation datasets.
We use the HMM model from our previous work
(2009) as our baseline. Our techniques follow the
same general setup, as it provides an efficient and
empirically-proven starting point for exploring (one
part of) the space of possible representations. Note,
however, that the HMM on its own does not provide
even an approximate solution to the objective func-
tion in our problem formulation (Eqn. 2), since it
makes no attempt to find the representation that min-
imizes loss on labeled data. To address this and other
concerns, we modify the objective function for HM-
M training. Specifically, we encode biases for rep-
resentation learning by defining a set of properties ?
that we believe a good representation function would
minimize. One possible bias is that the HMM states
should be predictive of the labels in labeled training
data. We can encode this as a property that computes
the entropy between the HMM states and the label-
s. For example, in Figure 1, we want to learn the
best HMM distribution for the sentence ?Innocen-
t bystanders are often the victims? for POS tagging
task. The hidden sequence y1, y2, y3, y4, y5, y6 can
have any distribution p1, p2, p3, ..., pm, ..., pn from
the latent space Y . Since we are doing POS tagging,
we want the distribution to learn the information en-
coded in the original POS labels ?JJ NNS RB VBP
DT NNS?. Therefore, by calculating the entropy be-
tween the hidden sequence and real labels, we can
identify a subset of desired distributions that have
low entropy, shown in the dotted oval. By minimiz-
ing the KL divergence between the learned distribu-
tion and the set of desired distributions, we can find
the best distribution which is the closest to our de-
sire.
The following subsections describe the specific
properties we investigate; here we show how to in-
corporate them into the objective function. Let z
be the sequence of labels in LS , and let ?(x,y, z)
be a property of the completed data that we wish
the learned representation to minimize, based on our
prior beliefs. Let Q be the subspace of the possible
distributions over Y that have a small expected val-
ue for ?: Q = {q(Y)|EY?q[?(x,Y, z)] ? ?}, for
some constant ?. We then add penalty terms to the
objective function (3) for the divergence between the
HMM distribution p and the ?good? distributions q,
as well as for ?:
L(?)?min
q,?
[KL(q(Y)||p(Y|x, ?)) + ?|?|] (4)
s.t. EY?q[?(x,Y, z)] ? ? (5)
where KL is the Kullback-Leibler divergence, and
? is a free parameter indicating how important the
bias is compared with the marginal log likelihood.
To incorporate multiple biases, we define a vec-
tor of properties ?, and we constrain each property
?i ? ?i. Everything else remains the same, except
that in the penalty term ?|?|, the absolute value is
replaced with a suitable norm: ? ???. To allow our-
selves to place weights on the relative importance
of the different biases, we use a norm of the form
?x?A =
?
(xtAx), where A is a diagonal matrix
whose diagonal entries Aii are free parameters that
provide weights on the different properties. For our
1316
experiments, we set the free parameters ? and Aii
using a grid search over development data, as de-
scribed in Section 5.1
3.1 A Bias for Task-specific Representations
Current representation learning techniques are unsu-
pervised, so they will generate the exact same repre-
sentation for different tasks. Yet it is exceedingly
rare that two state-of-the-art NLP systems for differ-
ent tasks share the same feature set, even if they do
tend to share some core set of lexical features.
Traditional non-learned (i.e., manually-
engineered) representations essentially always
include task-specific features. In response, we
propose to bias our representation learning such
that the learned representations are optimized for a
specific task. In particular, we propose a property
that measures how difficult it is to predict the labels
in training data, given the learned latent states.
Our entropy property uses conditional entropy of
the labels given the latent state as the measure of
unpredictability:
?entropy(y, z) = ?
?
i
P? (yi, zi) log P? (zi|yi) (6)
where P? is the empirical probability and i indicates
the ith position in the data. We can plug this feature
into Equation 5 to obtain a new version of Equation
4 as an objective function for task-specific represen-
tations. We refer to this model as HMM+E. Un-
like previous formulations for supervised and semi-
supervised dimensionality reduction (Zhang et al
2007; Yang et al 2006), our framework works effi-
ciently for structured representations.
3.2 A Bias for Domain-Independent Features
Following the theory in Section 2.2, we devise a bi-
ased objective to provide an explicit mechanism for
minimizing the distance between the source and tar-
get domain. As before, we construct a property of
the completed data:
?distance(y) = d1(P?S , P?T )
where P?S(Y ) is the empirical distribution over la-
tent state values estimated from source-domain la-
tent states, and similarly for P?T (Y ). Essentially,
1Note that ?, unlike A and ?, is not a free parameter. It is
explicitly minimized in the modified objective function.
minimizing this property will bias the the represen-
tation towards features that appear approximately as
often in the source domain as the target domain. We
refer to the model trained with a bias of minimiz-
ing ?distance as HMM+D, and the model with both
?distance and ?entropy biases as HMM+D+E.
3.3 A Bias for Multi-Dimensional
Representations
Words are multidimensional objects. In English,
words can be nouns or verbs, singular or plural,
count or mass, just to name a few dimensions along
which they may vary. Factorial HMMs (FHMM-
s) (Ghahramani and Jordan, 1997) can learn multi-
dimensional models, but inference and learning are
complex and computationally expensive even in su-
pervised settings. Our previous work (2010) creat-
ed a multi-dimensional representation called an ?I-
HMM? by training several HMM layers indepen-
dently; we showed that by finding several latent cat-
egories for each word, this representation can pro-
vide useful and domain-independent features for su-
pervised learners. In this work, we also learn a sim-
ilar multi-dimensional model (I-HMM+D+E), but
within each layer we add in the two biases described
above. While more efficient than FHMMs, the draw-
back of these I-HMM-based models is that there
is no mechanism to encourage the different HMM
models to learn different things. As a result, the lay-
ers may produce similar or equivalent features de-
scribing the dominant aspect of distributional sim-
ilarity in the data, but miss features that are less
strong, but still important, in the data.
To encourage learning a truly multi-dimensional
representation, we add a bias towards I-HMM mod-
els in which each layer is different from all previ-
ous layers. We define an entropy-based predictabili-
ty property that measures how predictable each pre-
vious layer is, given the current one. Formally, let
yli denote the hidden state at the ith position in lay-
er l of the model. For a given layer l, this proper-
ty measures the conditional entropy of ym given yl,
summed over layers m < l, and subtracts this from
the maximum possible entropy:
?predictl (y) = MAX+
?
i;m<l
P? (yli, y
m
i ) log P? (y
m
i |y
l
i)
The entropy between layer l and the previous layer-
1317
s m measures how unpredictable the previous lay-
ers are, given layer l. By biasing the model such
that MAX minus the entropy approaches zero, we
encourage layer l towards completely different fea-
tures from previous layers. We call the model with
this bias P-HMM+D+E.
4 Efficient Parameter Estimation
Several machine learning paradigms have been de-
veloped recently for incorporating biases and con-
straints into parameter estimation (Liang et al
2009; Chang et al 2007; Mann and McCallum,
2007). We leverage the Posterior Regularization
(PR) framework for our problem because of its flex-
ibility in handling different kinds of biases; we pro-
vide a brief overview of the technique here, but see
(Ganchev et al 2010) for full details.
4.1 Overview of PR
PR introduces a modified EM algorithm to handle
constrained objectives, like Equation 4. The modi-
fied E-step estimates a distribution q(Y) that is close
to the current estimate of p(Y|x, ?), but also close
to the ideal set of distributions that (in expectation)
have ? = 0 for each property ?. The M step re-
mains the same, except that it re-estimates parame-
ters with respect to expected latent states computed
with q rather than p.
E step:
qt+1 = argmin
q
min
?
KL(q(Y)||p(Y|x, ?t)) + ? ???
s.t. Eq[?(x,Y, z)] ? ?
M step:
?t+1 = argmax
?
Eqt+1 [log p(x,Y|?
t))]
To make the optimization task in the E-step more
tractable, PR transforms it to a dual problem:
max
??0,??????
? log
?
Y
p(Y|x, ?) exp{????(x,Y, z)}
where ???? is the dual norm of ???. The gradient of
this dual objective is ?Eq[?(x,Y, z)]. A projected
subgradient descent algorithm is used to perform the
optimization.
4.2 Modifying ? for Tractability
In unstructured settings, this optimization problem
is relatively straightforward. However, for struc-
tured representations, we need to ensure that the
dynamic programming algorithms needed for infer-
ence remain tractable for the biased objectives. For
efficient PR over structured models, the properties ?
need to be decomposed as a sum over the cliques in
the structured model. Unfortunately, the properties
we mention above do not decompose so nicely, so
we must resort to approximations.
In order to efficiently compute the expected val-
ue of the entropy property with respect to Y ? q,
we need to be able to compute each componen-
t EYi?q[?
entropy(Yi, zi)] separately. Yet P? depends
on the setting of other latent states Yj in the corpus.
To avoid this problem, we pre-compute the expected
empirical distributions over the completed data. For
each specific value y and z:
P?q(y, z) =
1
|LS |
?
x
|x|?
i=1
1[zi = z]q(Yi = y)
P?q(y) =
1
|LS |
?
x
|x|?
i=1
q(Yi = y)
These expected empirical distributions P?q can be
computed efficiently using standard inference algo-
rithms, such as the forward algorithm for HMMs.
Note that P?q depends on q, but unlike the original
P? from Equation 6, they do not depend on the data
completions y. Thus we can compute P?q once for
each qt, and then substitute it for P? for all values
of Y in the computation of EY?q?entropy(Y, z),
making this computation tractable. For the entropy-
based predictability properties, the calculation is
similar, but instead of using the label z, we use the
decoded states yli from previous layers.
For the distance property, Ben-David et als anal-
ysis depends on a particular notion of distance (E-
qn. 1) that is computationally intractable. They also
propose more tractable lower bounds, but these are
again incompatible with the PR framework. Since
no computationally feasible exact algorithm exists
for this distance feature, we resort to a crude but ef-
ficient approximation of this measure: for each pos-
1318
sible value y of the latent states, we define:
?disty (y) =
?
i|xi?US
1[yi = y]q(Yi = y)
|US |
?
?
i|xi?UT
1[yi = y]q(Yi = y)
|UT |
Each of these individual properties is tractable for
structured models. Combining these properties us-
ing the ???A norm results in a Euclidean distance
(weighted byA) between the frequencies of features
in each domain, rather than d1 distance.
5 Experiments
We tested the structured representations with biases
on two NLP tasks: Chinese POS tagging and En-
glish NER. In both cases, we use a domain adapta-
tion setting where no labeled data is available for the
target domain ? a particularly difficult setting, but
one that provides a strong test for an NLP system?s
ability to generalize . In our work (Huang and Yates,
2009), we used a plain HMM for domain adaptation
tasks in which there is labeled source data and un-
labeled source and target data, but no labeled target
data for training. Therefore, here, we use the HMM
technique as a baseline, and build on it by including
biases.
5.1 Chinese POS tagging
We use the UCLA Corpus of Written Chinese,
which is a part of The Lancaster Corpus of Man-
darin Chinese (LCMC). The UCLA Corpus consists
of 11,192 sentences of word-segmented and POS-
tagged text in 13 genres. We use gold-standard
word segmentation labels during training and test-
ing. The LCMC tagset consists of 50 Chinese POS
tags. Each genre averages 5284 word tokens, for a
total of 68,695 tokens among all genres. We use the
?news? genre as our source domain and randomly se-
lect 20% of every other genre as labeled test data. To
train our representation models, we use the ?news?
text, plus the remaining 80% of the texts from the
other genres. We use 90% of the labeled news text
for training, and 10% for development. We replace
hapax legomena in the unlabeled data with the spe-
cial symbol *UNKNOWN*, and also do the same
for word types in the labeled test sets that never ap-
pear in our unlabeled training texts.
0.888
0.893
0.898
0.903
0.908
0.913
0.918
0.923
0.928
0.1 1 10 100 1000
Ac
cu
rac
y 
? (log scale) 
News Domain (development data) 
alpha=0.01 alpha=0.1 alpha=1 alpha=10 alpha=100
Figure 2: Grid search for parameters on news text
Following our previous HMM setup in (Huang
and Yates, 2009) for consistency, we use an HMM
with 80 latent states. For our multi-layer models,
we use 7 layers of HMMs. We tuned the free pa-
rameters ? and A on development data. We varied
? from 0.1 to 1000. To tune A, we start by setting
the diagonal entry for ?entropy to 1, without loss of
generality. We then tie all the entries in A for ?disty
to a single parameter ?, and tie all of the entries for
?predicty to a parameter ?. We vary ? and ? over the
set {0.01,0.1,1,10,100}. Figure 2 shows our results
for ? and ? on news development data. A setting
of ? = 0.01 and ? = 100 performs best, with all
? = 100 doing reasonably well. Results for each
of these models on the general fiction test text con-
firm the general trends seen on development data ?
a comforting sign, since this indicates we can opti-
mize the free parameters on in-domain development
data, rather than requiring labeled data from the tar-
get domain. Our models tended to perform better
with increasing ? on development data, though with
diminishing returns. We pick the largest setting test-
ed, ? = 100, for our final models.
We use a linear-chain Conditional Random Field
(CRF) for our supervised classifier. To incorporate
the learned representations, we use the Viterbi Algo-
rithm to find the optimal latent state sequence from
each HMM-based model and then use the optimal
states as features in the CRF. Table 1 presents the
full list of features in the CRF. To handle Chinese,
we add in two features introduced in previous work
(Wang et al 2009): radical features and repeated
characters. A radical is a portion of a Chinese char-
acter that consists of a small number of pen or brush
strokes in a regular pattern.
1319
0.82
0.83
0.84
0.85
0.86
0.87
0.88
0.89
0.9
0.91
0.92
0.1 1 10 100 1000
Ac
cu
rac
y 
?ORJVFDOH 
General Fiction Domain (test data)  
alpha=0.01 alpha=0.1 alpha=1 alpha=10 alpha=100
Figure 3: Validating parameter settings on fiction text
CRF Feature Set
Transition
?z1[zj = z]
?z,z?1[zj = z and zj?1 = z?]
Word
?w,z1[xj = w and zj = z]
Radical
?z,r1[?c?xjradical(c) = r and zj = z]
Repeated Words
?A,B,z1[xj = AABB and zj = z]
?A,z1[(xj = AAor xj = AA/) and zj = z]
?A,B,z1[xj = ABAB and zj = z]
Features from Representation Learning
?y,l,z1[ylj = y and zj = z]
Table 1: Features used in our Chinese POS tagging
CRF systems. c represents a character within a word.
Table 2 shows our results. We compare against
the Baseline CRF without any additional representa-
tions and the unbiased HMM, a state-of-the-art do-
main adaptation technique from previous work, over
all 13 domains (source and target). We also com-
pare against a state-of-the-art Chinese POS tagger
for in-domain text, the CRF-based Stanford tagger
(Tseng et al 2005), retrained for this corpus. H-
MM+D+E outperforms the Stanford tagger on 10
out of 12 target domains and the unbiased HMM on
all domains, while the P-HMM+D+E outperform-
s the Stanford tagger (2.6% average improvement)
and HMM (1.7%) on all 12 target domains. The I-
HMM+D+E is slightly better than the HMM+D+E
(.3%), but incorporating the multi-dimensional bias
(P-HMM+D+E) adds an additional 0.6% improve-
ment.
Our interpretation for the success of I-
HMM+D+E and P-HMM+D+E is that the increase
in the state space of the models yields improved
performance. Because P-HMM+D+E biases against
redundant states found in I-HMM+D+E, it effective-
ly increases the state space beyond I-HMM+D+E.
Ahuja and Downey (2010) and our own work with
HMMs as representations (2010) have previously
shown that increasing the state space of the HMM
can significantly improve the representation, but
memory constraints eventually prevent further
progress this way. The I-HMM+D+E and P-
HMM+D+E models can provide similar benefits,
but because they split parameters across multiple
HMMs, they can accommodate much greater state
spaces in the same amount of memory.
We also tested the entropy and distance biases
separately. Figure 4 shows the result of the distance-
biased HMM+D on the general-fiction test text, as
we vary ? over the set {0.1,1,10,100,1000} (we ob-
served similar results for other domains). For all val-
ues of ?, the biased representation outperforms the
unbiased HMM. There is also a strong negative cor-
relation between the expected value of ??distance?
and the resulting accuracy, as expected from Ben-
David et als theoretical analysis. The HMM+E
model outperforms the HMM on the (source) news
domain by 0.3%, but actually performs worse for
most target domains. We suspect that the entropy
feature, which is learned only from labeled source-
domain data, makes the representation biased to-
wards features that are important in the source do-
main only. However, after we add in the distance
bias and a parameter to balance the weights from
both biases, the representation is able to capture the
label information as well as the target domain fea-
tures. Thus, the representation won?t solely depend
on source data. HMM+D+E, which combines both
biases, outperforms HMM+D, suggesting that task-
specific features for domain adaptation can be help-
ful, but only if there is some control for the domain-
independence of the features.
5.2 English Named Entity Recognition
To evaluate on a second task, we turn to Named En-
tity Recognition. We use the training data from the
1320
news (source) lore reli humour gen-fic essay mystery romance sci-fi skill science adv-fic report avg
words 9774 5428 3248 3326 4913 5214 5774 5489 3070 5464 5262 5071 6662 5284
CRF w/o HMM 93.8 85.0 80.0 85.4 85.0 83.8 84.7 86.0 82.8 78.2 82.2 77.1 85.3 84.5
HMM+E 97.1 88.2 83.1 87.5 87.4 89.2 89.5 87.1 86.7 82.1 87.2 79.4 91.7 88.3
Stanford 98.8 88.4 83.5 89.0 87.5 88.4 87.4 87.5 88.6 82.7 86.0 82.1 91.7 88.7
HMM 96.9 89.7 85.2 89.6 89.4 89.0 90.1 89.0 87.0 84.9 87.8 80.0 91.4 89.2
HMM+D 97.4 89.9 85.4 89.4 89.6 89.9 90.1 88.6 87.9 85.3 87.9 80.0 92.0 89.5
HMM+D+E 97.7 90.1 86.1 89.8 90.9 89.7 90.3 89.8 88.4 85.6 87.9 81.2 92.0 89.9
I-HMM+D+E 97.8 90.5 87.0 89.1 91.1 90.2 90.0 90.5 89.8 86.0 87.1 82.2 92.1 90.2
P-HMM+D+E 98.2 91.5 87.7 89.0 91.8 91.0 89.9 91.4 90.4 87.0 87.7 83.4 92.4 90.8
Table 2: POS tagging accuracy: The P-HMM+D+E tagger outperforms the unbiased HMM tagger and the
Stanford tagger on all target domains. The ?avg? column includes source-domain development data results. Differ-
ences between the P-HMM+D+E and the Stanford tagger are statistically significant at p < 0.01 on average and on 11
out of 12 target domain. We used the two-tailed Chi-square test with Yates? correction.
0.894
0.895
0.896
0.897
0.898
0.899
0.9
0.901
0.902
0.903
0.904
4.55E -05 4.60E-05 4.65E -05 4.70E-05 4.75E -05 4.80E-05
Ac
cu
rac
y 
?E q (?distance)? 
HMM+D on General Fiction Test  
?=1000  
?=100 
?=1  ?=0.1  
?=10  
Unconstrained HMM  
Figure 4: Greater distance between domains correlates
with worse target-domain tagging accuracy.
CoNLL 2003 shared task for our labeled training set,
consisting of 204k tokens from the newswire do-
main. We tested the system on the MUC7 formal
run test data, consisting of 59k tokens of stories on
the telecommunications and aerospace industries.
To train our representations, we use the CoNL-
L training data and the MUC7 training data without
labels. We again use a CRF, with features introduced
by Zhang and Johnson (2003) for our baseline. We
use the same setting of free parameters from our
POS tagging experiments.
Results are shown in Table 3. Our best biased
representation P-HMM+D+E outperformed the un-
biased HMM representation by 3.6%, and beats the
I-HMM+D+E by 1.6%. The domain-distance and
multi-dimensional biases help most, while the task-
specific bias helps somewhat, but only when the
domain-distance bias is included. The best sys-
System F1
CRF without HMM 66.15
HMM+E 74.25
HMM 75.06
HMM+D 75.75
HMM+D+E 76.03
I-HMM+D+E 77.04
P-HMM+D+E 78.62
Table 3: English Named Entity recognition results
tem tested on this dataset achieved a slightly bet-
ter F1 score (78.84) (Turian et al 2010), but used
a much larger training corpus (they use RCV1 cor-
pus which contains approximately 63 million token-
s). Other studies (Turian et al 2010; Huang et
al., 2011) have performed a detailed comparison be-
tween these types of systems, so we concentrate on
comparisons between biased and unbiased represen-
tations here.
5.3 Does the task-specific bias actually help?
In this section, we test whether the task-specific
bias (entropy bias) actually learns something task-
specific. We learn the entropy-biased representa-
tions for two tasks on the same set of sentences,
labeled differently for the two tasks: English POS
tagging and Named Entity Recognition. Then we
switch the representations to see whether they will
help or hurt the performance on the other task. We
randomly picked 500 sentences from WSJ section
1321
Representation/Task POS Accuracy NER F1
HMM 88.5 66.3
HMM+E(POS labels) 89.7 64.5
HMM+E(NER labels) 86.5 68.0
Table 4: Results of POS tagging and Named Entity
recognition tasks with different representations. With the
entropy-biased representation, the system has better per-
formance on the task which the bias is trained for, but
worse performance on the other task.
0-18 as our labeled training data and 500 sentences
from WSJ section 20-23 as testing data. Because
WSJ data does not have gold standard NER tags,
we manually labeled these sentences with NER tags.
For simplicity, we only use three types of NER tags:
person, organization and location. The result is
shown in Table 4. When the entropy bias uses la-
bels from the same task as the classifier, the perfor-
mance is improved: about 1.2% in accuracy on POS
tagging and 1.7% in F1 score on NER. Switching
the representations for the tasks actually hurts the
performance compared with the unbiased represen-
tation. The results suggest that the entropy bias does
indeed yield a task-specific representation.
6 Conclusion and Future Work
We introduce three types of biases into represen-
tation learning for sequence labeling using the PR
framework. Our experiments on POS tagging and
NER indicate domain-independent biases and multi-
dimensional biases significantly improve the repre-
sentations, while the task-specific bias improves per-
formance on out-of-domain data if it is combined
with the domain-independent bias. Our results indi-
cate the power of representation learning in building
domain-agnostic classifiers, but also the complexi-
ty of the task and the limitations of current tech-
niques, as even the best models still fall significantly
short of in-domain performance. Important consid-
erations for future work include identifying further
effective and tractable biases, and extending beyond
sequence-labeling to other types of NLP tasks.
Acknowledgments
This research was supported in part by NSF grant
IIS-1065397.
References
Arun Ahuja and Doug Downey. 2010. Improved extrac-
tion assessment through better language models. In
Proceedings of the Annual Meeting of the North Amer-
ican Chapter of the Association of Computational Lin-
guistics (NAACL-HLT).
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2007. Analysis of representations
for domain adaptation. In Advances in Neural Infor-
mation Processing Systems 20, Cambridge, MA. MIT
Press.
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from different
domains. Machine Learning, 79:151?175.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
John Blitzer, Koby Crammer, Alex Kulesza, Fernando
Pereira, and Jenn Wortman. 2007. Learning bounds
for domain adaptation. In Advances in Neural Infor-
mation Processing Systems.
M. Candito and B. Crabbe?. 2009. Improving generative
statistical parsing with semi-supervised word cluster-
ing. In IWPT, pages 138?141.
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In Pro-
ceedings of the ACL.
Hal Daume? III, Abhishek Kumar, and Avishek Saha.
2010. Frustratingly easy semi-supervised domain
adaptation. In Proceedings of the ACL Workshop on
Domain Adaptation (DANLP).
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In ACL.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society of
Information Science, 41(6):391?407.
Paramveer S. Dhillon, Dean Foster, and Lyle Ungar.
2011. Multi-view learning of word embeddings via
cca. In Neural Information Processing Systems (NIP-
S).
A. Emami, P. Xu, and F. Jelinek. 2003. Using a con-
nectionist model in a syntactical based language mod-
el. In Proceedings of the International Conference on
Spoken Language Processing, pages 372?375.
Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine
Learning Research, 11:10?49.
Zoubin Ghahramani and Michael I. Jordan. 1997. Facto-
rial hidden markov models. Machine Learning, 29(2-
3):245?273.
1322
Daniel Gildea. 2001. Corpus Variation and Parser Per-
formance. In Conference on Empirical Methods in
Natural Language Processing.
T. Honkela. 1997. Self-organizing maps of words for
natural language processing applications. In In Pro-
ceedings of the International ICSC Symposium on Soft
Computing.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised se-
quence labeling. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Fei Huang and Alexander Yates. 2010. Exploring
representation-learning approaches to domain adapta-
tion. In Proceedings of the ACL 2010 Workshop on
Domain Adaptation for Natural Language Processing
(DANLP).
Fei Huang, Alexander Yates, Arun Ahuja, and Doug
Downey. 2011. Language models as representation-
s for weakly supervised nlp tasks. In Conference on
Natural Language Learning (CoNLL).
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in NLP. In ACL.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
from measurements in exponential families. In Inter-
national Conference on Machine Learning (ICML).
D. Lin and X Wu. 2009. Phrase clustering for discrimi-
native learning. In ACL-IJCNLP, pages 1030?1038.
G. S. Mann and A. McCallum. 2007. Simple, robust,
scalable semi-supervised learning via expectation reg-
ularization. In In Proc. ICML.
Y. Mansour, M. Mohri, and A. Rostamizadeh. 2009. Do-
main adaptation with multiple sources. In Advances in
Neural Information Processing Systems.
F. Morin and Y. Bengio. 2005. Hierarchical probabilistic
neural network language model. In Proceedings of the
International Workshop on Artificial Intelligence and
Statistics, pages 246?252.
Sameer Pradhan, Wayne Ward, and James H. Martin.
2007. Towards robust semantic role labeling. In Pro-
ceedings of NAACL-HLT, pages 556?563.
M. Sahlgren. 2005. An introduction to random indexing.
In In Methods and Applications of Semantic Indexing
Workshop at the 7th International Conference on Ter-
minology and Knowledge Engineering (TKE).
G. Salton and M.J. McGill. 1983. Introduction to Mod-
ern Information Retrieval. McGraw-Hill.
Satoshi Sekine. 1997. The domain dependence of pars-
ing. In Proc. Applied Natural Language Processing
(ANLP), pages 96?102.
Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005. Morphological features help pos tagging
of unknown words across language varieties. In Pro-
ceedings of the Fourth SIGHAN Workshop on Chinese
Language Processing.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 384?394.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
Lijie Wang, Wanxiang Che, and Ting Liu. 2009. An
svmtool-based chinese pos tagger. In Journal of Chi-
nese Information Processing.
X. Yang, H. Fu, H. Zha, and J. Barlow. 2006. Semi-
supervised nonlinear dimensionality reduction. In
Proceedings of the 23rd International Conference on
Machine Learning.
T. Zhang and D. Johnson. 2003. A robust risk mini-
mization based named entity recognition system. In
CoNLL.
D. Zhang, Z.H. Zhou, and S. Chen. 2007. Semi-
supervised dimensionality reduction. In Proceedings
of the 7th SIAM International Conference on Data
Mining.
1323
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 1?9,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Improving Word Alignment Using Linguistic Code Switching Data
Fei Huang
?
and Alexander Yates
Temple University
Computer and Information Sciences
324 Wachman Hall
Philadelphia, PA 19122
{fei.huang,yates}@temple.edu
Abstract
Linguist Code Switching (LCS) is a
situation where two or more languages
show up in the context of a single
conversation. For example, in English-
Chinese code switching, there might
be a sentence like ???15?? k
?meeting (We will have a meeting in 15
minutes)?. Traditional machine translation
(MT) systems treat LCS data as noise,
or just as regular sentences. However, if
LCS data is processed intelligently, it can
provide a useful signal for training word
alignment and MT models. Moreover,
LCS data is from non-news sources which
can enhance the diversity of training data
for MT. In this paper, we first extract
constraints from this code switching data
and then incorporate them into a word
alignment model training procedure. We
also show that by using the code switching
data, we can jointly train a word alignment
model and a language model using co-
training. Our techniques for incorporating
LCS data improve by 2.64 in BLEU score
over a baseline MT system trained using
only standard sentence-aligned corpora.
1 Introduction
Many language users are competent in multiple
languages, and they often use elements of multiple
languages in conversations with other speakers
with competence in the same set of languages.
For example, native Mandarin speakers who
also speak English might use English words in
a Chinese sentence, like ?\???K
solution??(Do you know the solution to
this problem ?)?. This phenomenon of mixing
?
*The author is working at Raytheon BBN Technologies
now
languages within a single utterance is known as
Linguistic Code Switching (LCS). Examples of
these utterances are common in communities of
speakers with a shared competency in multiple
languages, such as Web forums for Chinese
emigr?es to the United States. For example, more
than 50% of the sentences we collected from a
Web forum (MITBBS.com) contains both Chinese
and English.
Traditional word alignment models take a
sentence-level aligned corpus as input and gener-
ate word-level alignments for each pair of parallel
sentences. Automatically-gathered LCS data
typically contains no sentence-level alignments,
but it still has some advantages for training
word alignment models and machine translation
(MT) systems which are worth exploring. First,
because it contains multiple languages in the same
sentence and still has a valid meaning, it will tell
the relationship between the words from different
languages to some extent. Second, most LCS
data is formed during people?s daily conversation,
and thus it contains a diversity of topics that
people care about, such as home furnishings,
cars, entertainment, etc, that may not show up in
standard parallel corpora. Moreover, LCS data is
easily accessible from Web communities, such as
MITBBS.com, Sina Weibo, Twitter, etc.
However, like most unedited natural language
text on the Web, LCS data contains symbols like
emotions, grammar and spelling mistakes, slang
and strongly idiomatic usage, and a variety of
other phenomena that are difficult to handle. LCS
data with different language pairs may also need
special handling. For instance, Sinha and Thakur
(2005) focus on words in mixed English and
Hindi texts where a single word contains elements
from both languages; they propose techniques
for translating such words into both pure English
and pure Hindi. Our study focuses on Chinese-
English LCS, where this is rarely a problem,
1
but for other language pairs, Sinha and Thakur?s
techniques may be required as preprocessing
steps. Primarily, though, LCS data requires
special-purpose algorithms to use it for word
alignment, since it contains no explicit alignment
labels.
In this paper, we investigate two approaches to
using LCS data for machine translation. The first
approach focuses exclusively on word alignment,
and uses patterns extracted from LCS data to guide
the EM training procedure for word alignment
over a standard sentence-aligned parallel corpus.
We focus on two types of patterns in the LCS
data: first, English words are almost never correct
translations for any Chinese word in the same
LCS utterance. Second, for sentences that are
mostly Chinese but with some English words, if
we propose substitutes for the English words using
a Chinese language model, those substitutes are
often good translations of the English words. We
incorporate these patterns into EM training via
the posterior regularization framework (Ganchev
et al., 2010).
Our second approach treats the alignment and
language model as two different and comple-
mentary views of the data. We apply the co-
training paradigm for semi-supervised learning
to incorporate the LCS data into the training
procedures for the alignment model and the
language model. From the translation table of
the alignment model, the training procedure finds
candidate translations of the English words in
the LCS data, and uses those to supplement the
language model training data. From the language
model, the training procedure identifies Chinese
words that complete the Chinese sentence with
high probability, and it uses the English word
paired with these completion words as additional
training points for translation probabilities. These
models are trained repeatedly until they converge
to similar predictions on the LCS data. In
combination with a larger phrase-based MT
system (Koehn et al., 2003), these two training
procedures yield an MT system that achieves a
BLEU score of 31.79 on an English-to-Chinese
translation task, an improvement of 2.64 in BLEU
score over a baseline MT system trained on only
our parallel corpora.
The rest of this paper is organized as follows.
The next section presents related work. Section 3
gives an overview of word alignment. Sections 4
and 5 detail our two algorithms. Section 6 presents
our experiments and discusses results, and Section
7 concludes and discusses future work.
2 Related Work
There has been a lot of research on LCS from
the theoretical and socio-linguistic communities
(Nilep, 2006; De Fina, 2007). Computational
research on LCS has studied how to identify
the boundaries of an individual language within
LCS data, or how to predict when an utterance
will switch to another language (Chan et al.,
2004; Solorio and Liu, 2008). Manandise and
Gdaniec (2011) analyzed the effect on machine
translation quality of LCS of Spanish-English and
showed that LCS degrades the performance of
the syntactic parser. Sinha and Thakur (2005)
translate mixed Hindi and English (Hinglish)
to pure Hindi and pure English by using two
morphological analyzers from both Hindi and
English. The difficulty in their problem is
that Hindi and English are often mixed into a
single word which uses only the English alphabet;
approaches based only on the character set cannot
tell these words apart from English words. Our
current study is for a language pair (English-
Chinese) where the words are easy to tell apart,
but for MT using code-switching data for other
language pairs (such as Hindi-English), we can
leverage some of the techniques from their work
to separate the tokens into source and target.
Like our proposed methods, other researchers
have used co-training before for MT (Callison-
Burch and Osborne, 2003). They use target
strings in multiple languages as different views on
translation. However, in our work, we treat the
alignment model and language model as different
views of LCS data.
In addition to co-training, various other semi-
supervised approaches for MT and word align-
ment have been proposed, but these have relied on
sentence alignments among multiple languages,
rather than LCS data. Kay (2000) proposes using
multiple target documents as a way of informing
subsequent machine translations. Kumar et al.
(2007) described a technique for word alignment
in a multi-parallel sentence-aligned corpus and
showed that this technique can be used to obtain
higher quality bilingual word alignments. Other
work like (Eisele, 2006) took the issue one step
further that they used bilingual translation systems
2
which share one or more common pivot languages
to build systems which non-parallel corpus is used.
Unlike the data in these techniques, LCS data
requires no manual alignment effort and is freely
available in large quantities.
Another line of research has attempted to
improve word alignment models by incorporating
manually-labeled word alignments in addition to
sentence alignments. Callison-Burch et al. (2004)
tried to give a higher weight on manually labeled
data compared to the automatic alignments. Fraser
and Marcu (2006) used a log-linear model with
features from IBM models. They alternated the
traditional Expectation Maximization algorithm
which is applied on a large parallel corpus with
a discriminative step aimed at increasing word-
alignment quality on a small, manually word-
aligned corpus. Ambati et al.(2010) tried to man-
ually correct the alignments which are informative
during the unsupervised training and applied them
to an active learning model. However, labeled
word alignment data is expensive to produce. Our
approach is complementary, in that we use mixed
data that has no word alignments, but still able to
learn constraints on word alignments.
Our techniques make use of posterior regular-
ization (PR) framework (Ganchev et al., 2010),
which has previously been used for MT (Graca
et al., 2008), but with very different constraints
on EM training and different goals. (Graca et
al., 2008) use PR to enforce the constraint that
one word should not translate to many words, and
that if a word s translates to a word t in one MT
system, then a model for translation in the reverse
direction should translate t to s. Both of these
constraints apply to sentence-aligned training data
directly, and complement the constraints that we
extract from LCS data.
3 Statistical Word Alignment
Statistical word alignment (Brown et al., 1994) is
the task identifying which words are translations
of each other in a bilingual sentence corpus. It
is primarily used for machine translation. The
input to an alignment system is a sentence-level
aligned bilingual corpus, which consists of pairs
of sentences in two languages. One language
is denoted as the target language, and the other
language as the source language.
We now introduce the baseline model for word
alignment and how we can incorporate the LCS
data to improve the model. IBM Model 1
(Brown et al., 1994) and the HMM alignment
model (Vogel et al., 1996) are cascaded to
form the baseline model for alignment. These
two models have a similar formulation L =
P (t, a|s) = P (a)
?
j
P (t
j
|s
a
j
) with a different
distortion probability P (a). s and t denote the
source and target sentences. a is the alignment,
and a
j
is the index of the source language word
that generates the target language word at position
j. The HMM model assumes the alignments have
a first-order Markov dependency, so that P (a) =
?
j
P (a
j
|a
j
? a
j?1
). IBM Model 1 ignores the
word position and uses a uniform distribution, so
P (a) =
?
j
P (a
j
) where P (a
j
) =
1
|t|
, where |t|
is the length of t.
Expectation Maximization (Dempster et al.,
1977) is typically used to train the alignment
model. It tries to maximize the marginal
likelihood of the sentence-level aligned pairs.
For the HMM alignment model, the forward-
backward algorithm can be used the optimize the
posterior probability of the hidden alignment a.
4 Learning Constraints for Word
Alignments from LCS Data
We observed that most LCS sentences are
predominantly in one language, which we call
the majority language, with just a small number
of words from another language, which we
call the minority language. The grammar of
each sentence appears to mirror the structure
of the majority language. Speakers appear to
be substituting primarily content words from the
minority language, especially nouns and verbs,
without changing the structure of the majority
language. In this section, we explain two types
of constraints we extract from the LCS data
that can be helpful for guiding the training of a
word alignment model, and we describe how we
incorporate those constraints into a full training
procedure.
4.1 Preventing bad alignments
After inspecting sentences in our LCS data, we
found that the words from the target language
occurring in the sentence are highly likely not to
be the translation of the remaining source word.
Figure 1 shows an example LCS sentence where
the speaker has replaced the Chinese word ????
with the corresponding English word ?request?.
3
?? ?? ?? ?? 
People request to change the Constitution 
   ?? request ?? ?? 
Chinese Translation: 
English Translation: 
LCS sentence:  
Figure 1: The upper sentence is the original LCS sentence. The bottom ones are its translation in pure Chinese and English.
Underlined words are the original words in the LCS sentence.
In most LCS utterances, the minority language
replaces or substitutes for words in the majority
language, and thus it does not serve as a translation
of any majority-language words in the sentence.
If we can enforce that a word alignment model
avoids pairing words that appear in the same
LCS sentence, we can significantly narrow down
the possible choices of the translation candidates
during word alignment training.
Formally, let t
LCS
be the set of target (Chinese)
words and s
LCS
be the source (English) words in
the same sentence of the LCS data. According to
our observation, each s
LCS
j
in s
LCS
should not
be aligned with any word t
LCS
i
in t
LCS
. We call
every target-source word pair (t
LCS
i
, s
LCS
j
) from
LCS data a blocked alignment. For a set of word
alignments WA = {(s
w
, t
w
)} produced by a word
alignment model, define
?
BA
=
?
(s
w
,t
w
)?WA
1[(s
w
, t
w
) ? BA] (1)
where BA is the set of blocked alignments
extracted from the LCS data. We want to minimize
?
BA
. Figure 2 shows a graphical illustration of this
constraint.
?? (People) ?? (change) ?? (constitution) 
request 
Figure 2: Illustration of the blocked alignment constraint.
4.2 Encouraging alignments with substitutes
proposed by a language model
Another perspective of using the LCS data is
that if we can find some target word set t
similar
from the target language which shares similar
contexts as the source word s
LCS
j
in the LCS
data, then we can encourage s
LCS
j
to be aligned
with the each word t
similar
m
in t
similar
. Figure
3 shows example phrases (??????U? ,
? ?????U?, ??????U? etc) that
appear in a Chinese language model and which
share the same left context and right context as
the word ?request.? Our second objective is to
encourage minority language words like ?request?
to align with possible substitutes from the majority
language?s language model. If we see any of
???, ??, ??? in the parallel corpus, we
should encourage the word ?request? to be aligned
with them. We call this target-source word pair
(t
similar
m
, s
LCS
j
) an encouraged alignment.
Formally, we define
?
EA
= |C| ?
?
(s
w
,t
w
)?WA
1[(s
w
, t
w
) ? EA] (2)
where |C| is the size of the parallel corpus and EA
is the encouraged alignment set. We define this
expression in such a way that if the optimization
procedure minimizes it, it will increase the number
of encouraged alignments.
?? (People)  ?? (change)  ?? (constitution)  request  
Trigrams  ??  ?? (r efuse)   ?? ??  ?? (r equest) ?? ??  ?? (suggest) ?? 
Figure 3: Illustration of the encouraged alignment
constraint. The dotted rectangle shows the candidate
translations of the English word from the tri-gram output
from the language model
Algorithm 1 shows the algorithm of calculating
t
similar
. (t
LCS
l
, s
LCS
j
, t
LCS
r
) is a (target, source,
target)word tuple contained in the LCS data. l
and r denote the left and right target words to the
source word. We use the language model output
from the target language. For each pair of contexts
t
l
and t
r
for the source word, we find the exact
match of this pair in the ngram. Then we extract
the middle word as the candidates for t
similar
.
Here, we only use 3 grams in our experiments, but
it is possible to extend this to 5grams, which might
lead to further improvements. The EA constraint
4
Algorithm 1: finding t
similar
1: Input: s
LCS
,t
LCS
, language model LM
2: Set t
similar
={}
3: Extract the 3 grams (t
l
, t
m
, t
r
) ? gram
3
from
LM
4: set S = {}
5: For j from 1 to size(gram
3
)
if (t
j
l
, t
j
r
) ? S
add t
j
m
into C
t
j
l
,t
j
r
else
put (t
j
l
, t
j
r
) into S
set C
t
j
l
,t
j
r
= {}
6: Extract tuple (t
LCS
l
, s
LCS
j
, t
LCS
r
)
if (t
LCS
l
, t
LCS
r
) ? S
add C
t
LCS
l
,t
LCS
r
into t
similar
7: Output: t
similar
is similar to a bilingual dictionary. However, in the
bilingual dictionary, each source word might have
several target translations (senses), so it might be
ambiguous. The candidate translations used in
EA are from language model (3 grams in this
paper, but it can be extended to 5 grams), which
will always match the contexts. Additionally,
the bilingual dictionary contains the standard
English/Chinese word pairs. But the LCS data
is generated from people.s daily conversation; it
reflects usage in a variety of domains, including
colloquial and figurative usages that may not
appear in a dictionary.
4.3 Constrained parameter estimation
We incorporate ?
BA
and ?
EA
into the EM
training procedure for the alignment model using
posterior regularization (PR) (Ganchev et al.,
2010). Formally, let x be the sentence pairs s and
t. During the E step, instead of using the posterior
p(a|x) to calculate the expected counts, the PR
framework tries to find a distribution q(a) which
is close to p(a|x), but which also minimizes the
properties ?(a,x):
min
q,?
[KL(q(a)||p(a|x, ?)) + ?||?||] (3)
s.t. E
a?q
[?(a,x)] ? ? (4)
where KL is the Kullback-Leibler divergence, ?
is a free parameter indicating how important the
constraints are compared with the marginal log
likelihood and ? is a small violation allowed in
?? ?? ?? ?? (0.025)  
?? ?? ?? ?? (0.05)  
?? ?? ?? ?? (0.009)  ?? 
Chinese Monolingual data  
?? ??  ?? (0.06)  
?? ??  ?? (0.002)  
?? ??  ?? (0.01)  
?? ??  ?? (0.04)  
?? 
?? 
?? 
Translation Table  
?? 
request ?? 0.025  
R equest ?? 0.05  request ?? 0.009  ?? Translation Table  
?? 
request ?? 0.06  
R equest ?? 0.0002  request ?? 0.01  
request ?? 0.04  Update Translation Table  
Update mixed data  
LM  
A M  
?? request ?? ?? (People request to change the constitution) 
Figure 4: The framework of co-training in word alignment.
AM represents alignment model and LM represents language
model. Green italic words are the encouraged translation and
red italic words are the discouraged translation.
the optimization. To impose multiple constraints,
we define a norm ||?||
A
=
?
(?
t
A?), where A
is a diagonal matrix whose diagonal entries A
ii
are free parameters that provide weights on the
different constraints. Since we only have two
constraints here from LCS data, A =
(
1 0
0 ?
)
where ? controls the relative importance of the
two constraints.
To make the optimization task in the E-step
more tractable, PR transforms it to a dual problem:
max
??0,???
?
??
? log
?
a
p(a|x, ?) exp{?? ??(a,x)}
where ???
?
is the dual norm of ???
A
. The gradient
of this dual objective is?E
q
[?(a,x)]. A projected
subgradient descent algorithm is used to perform
the optimization.
5 Co-training using the LCS data
The above approaches alter the translation and
distortion probabilities in the alignment model.
However, they leave the language model un-
changed. We next investigate a technique that
uses LCS data to re-estimate parameters for the
language model as well as the alignment model
simultaneously. Co-training (Blum and Mitchell,
1998) is a semi-supervised learning technique
that requires two different views of the data. It
assumes that each example can be described using
two different feature sets which are conditionally
independent. Also, each feature set of the data
should be sufficient to make accurate prediction.
5
The schema fits perfectly into our problem. We
can treat the alignment model and the language
model as two different views of the LCS data.
We use the same example ???request ?U
{? to show how co-training works, shown in
Figure 4. From the translation table generated
by the alignment model, we can get a set of
candidate translations of ?request?, such as ???
??,????,etc. We can find the candidate with the
highest probability as the translation. Similarly,
from the language model, we can extract all the
ngrams containing ? ??? and ??U? as the left
and right words and pick the words in the middle
such as ? ??, ??, ??? etc as the candidate
translations. We can then use the candidate
with the highest probability as the translation
for ?request?. Thus both models can predict
translations for the English (minority language) in
this example. Each model?s predictions can be
used as supplemental training data for the other
model.
Algorithm 2 shows the co-training algorithm for
word alignment. At each iteration, a language
model and an alignment model are trained. The
language model is trained on a Chinese-only
corpus plus a corpus of probabilistic LCS sen-
tences where the source words are replaced with
target candidates from the alignment model. The
alignment model is retrained using a translation
table which is updated according to the output
word pairs from the language model output and the
LCS data. In order to take the sentence probability
into consideration, we modify the language model
training procedure: when it counts the number of
times each ngram appears, instead of adding 1,
it adds the probability from the translation model
for ngrams in the LCS data that contain predicted
translations.
6 Experiments and Results
6.1 Experimental Setup
We evaluated our LCS-driven training algorithms
on an English-to-Chinese translation task. We
use Moses (Koehn et al., 2003), a phrase-
based translation system that learns from bilingual
sentence-aligned corpora as the MT system. We
supplement the baseline word alignment model in
Moses with our LCS data, constrained training
procedure, and co-training algorithm as well as
IBM 3 model. Because IBM 3 model is a
fertility based model which might also alleviate
Algorithm 2: Co-training for word alignment and
language modeling
1: Input: parallel data X
p
, LCS data X
LCS
,
language model training data X
l
2: Initialize translation table tb for IBM1 model
3: For iteration from 1 to MAX
tb? Train-IBM(X
p
)
tb
?
? Train-HMM(X
p
|tb)
4: For each sentence x
i
in X
LCS
:
For each source word s
j
in x
i
:
1) find the translation t
j
of s
j
with
with probability p
j
from tb
?
2) replace s
j
with t
j
and update
sentence?s probability p
s
= p
s
?p
j
X
new
l
? X
l
? x
i
5: LM? Train-LM(X
new
l
)
6: Extract the tri-gram gram
3
from LM
7: For each sentence x
i
in X
LCS
:
run Algorithm 1: finding t
similar
8: update tb
?
using (t
m
, s
j
) where
t
m
? t
similar
and s
j
? x
i
9: End For
10: Output: word alignment for X
p
and LM
some of the problems caused by LCS data. To
clarify, we use IBM1 model and HMM models in
succession for the baseline. We trained the IBM1
model first and used the resulting parameters
as the initial parameter values to train HMM
model. Parameters for the final MT system
are tuned with Minimum Error Rate Training
(MERT) (Och, 2003). The tuning set for MERT
is the NIST MT06 data set, which includes 1664
sentences. We test the system on NIST MT02
(878 sentences). To evaluate the word alignment
results, we manually aligned 250 sentences from
NIST MT02 data set. For simplicity, we only
have two types of labels for evaluating word
alignments: either two words are aligned together
or not. (Previous evaluation metrics also consider
a third label for ?possible? alignments.) Out of
the word-aligned data, we use 100 sentences as a
development set and the rest as our testing set.
Our MT training corpus contains 2,636,692
sentence pairs from two parallel corpora: Hong
Kong News (LDC2004T08) and Chinese English
News Magazine Parallel Text (LDC2005T10). We
use the Stanford Chinese segmenter to segment
the Chinese data. We use a ngram model
package called SRILM (Stolcke, 2002) to train
6
the language model. Because our modified
ngram counts contain factions, we used Witten-
Bell smoothing(Witten and Bell, 1991) which
supports fractional counts. The 3-gram language
model is trained on the Xinhua section of the
Chinese Gigaword corpus (LDC2003T09) as well
as the Chinese side of the parallel corpora. We
also removed the sentences in MT02 from the
Gigaword corpus if there is any to avoid the biases.
We gather the LCS data from ?MITBBS.com,?
a popular forum for Chinese people living in
the United States. This forum is separated by
discussion topic, and includes topics such as
?Travel?, ?News?, and ?Living style?. We extract
data from 29 different topics. To clean up the
LCS data, we get rid of HTML mark-up, and we
remove patterns that are commonly repeated in
forums, like ?Re:? (for ?reply? posts) and ?[=
1]? (for ?repost?). We change all English letters
written in Chinese font into English font. We stem
the English words in both the parallel training data
and the LCS data. After the cleaning step, we have
245,470 sentences in the LCS data. 120,922 of
them actually contain both Chinese and English in
the same sentence. 101,302 of them contain only
Chinese, and we add these into the language model
training data. We discard the sentences that only
contain English.
6.2 Word Alignment Results
In order to incorporate the two constraints during
the Posterior Regularization, we need to tune the
parameters ? which controls the weights between
the constraints and the marginal likelihood and
? which controls the relative importance between
two constraints on development data. We varied
? from 0.1 to 1000 and varied ? over the
set {0.01, 0.1, 1, 10, 100}. After testing the
25 different combinations of ? and ? on the
development data, we find that the setting with
? = 100 and ? = 0.1 achieves the best
performance. During PR training, we trained the
model 20 iterations for the dual optimization and
5 iterations for the modified EM.
Table 1 shows the word alignment results. We
can see that incorporating the LCS data into
our alignment model improves the performance.
Our best co-training+PR
+
system outperforms
the baseline by 8 points. Figure 5 shows an
example of how BA is extracted from LCS data
can help the word alignment performance. The
System F1
Baseline 0.68
IBM 3 0.70
PR+BA 0.71
PR+EA 0.70
PR
+
0.73
co-training 0.74
co-training+PR
+
0.76
Table 1: Word alignment results (PR
+
means PR+BA+EA).
upper figure shows that alignment by the baseline
system. We can see that the word ?badminton?
is aligned incorrectly with word ?>??(Taufik)?
. However, in the LCS data, we see that ? >?
?(Taufik)? and ?badminton? appear in the same
sentence ?>??badmintonx?
(Taufik
plays badminton so well)? and by adding the
blocked constraint into the alignment model, it
correctly learns that ? >??(Taufik)? should be
aligned with something else, and it finds ?Taufik?
at end. Table 2 shows some of the translations
of ?badminton? before and after incorporating the
LCS data. We can see that it contains some wrong
translations like ??	??(pingpong room)?,?>
??(Taufik)?etc using baseline model. After
using the LCS data as constraints and the co-
training framework, these wrong alignments are
eliminated and the translation ?? ?(another
way of expressing badminton)? get a higher
probability. We found that IBM 3 model can
also correct this specific case. However, our
co-training+PR
+
system still outperforms it by 6
points.
Figure 6 shows an example of how EA is
extracted from LCS data can help the word
alignment. The solid lines show the alignment
by the baseline model and we can see that
the word ?compiled? is not aligned with any
Chinese word. After using the LCS data and the
language model, we find that ?8B(compile)?
shows up in the same context ??(book) ?
5(up)?as ?compile? along with ?C?(staple)?
and ??(staple)?, therefore ?(compile,8B)? will
be an encouraged alignment. After adding the EA
constraint, the model learns that ?compile? should
be aligned with ?8B?.
6.3 Phrase-based machine translation
In this section, we investigated whether improved
alignments can improve MT performance. We
7
?? ??? ?? ?? ??? ? ?? ??? 
Indonesia badminton experts think Taufik?s ranking favorable  
?? ??? ?? ?? ??? ? ?? ??? 
Indonesia badminton experts think Taufik?s ranking favorable  
Baseline:  
PR+BA:  
Figure 5: After incorporating the BA constraint from the LCS data, the word ?Taufik(>??)? is aligned correctly.
Baseline PR+co-training
Translation Probability Translation Probability
?f?(badminton) 0.500 ?f?(badminton) 0.500
W	?(pingpong)?(room) 0.500 ??(two of the three characters in badminton) 0.430
?(play)?f(feather) 0.250 ?(play)?f(feather) 0.326
?f?(shuttlecock)?(head) 0.125 ?f?(shuttlecock)?(head) 0.105
... ... ... ...
>??(Taufik) 0.005 ??(racket) 0.002
Table 2: Translation tables of ?badminton? before and after incorporation of LCS data.
? ?? ? ? ?? ?? ? ??
Winning entries after the review will be compiled
?? ? ? compile ???
(How to compile the book ?)
Trigrams
?(book) ??(compile)  ??(up)
?(book)  ??(staple)    ??(up)
?(book)    ?(staple)      ??
(up)
...
Wednesday, October 16, 13
Figure 6: After incorporating the EA constraint from the
LCS data, the word ?compiled(8B)? is aligned correctly.
use different word alignment models? outputs as
the first step for Moses and keep the rest of
Moses system the same. We incorporate Moses?s
eight standard features as well as the lexicalized
reordering model. We also use the grow-diag-final
and alignment symmetrization heuristic.
Table 3 shows the machine translation results.
We can see that 3 techniques we proposed for word
alignment all improve the machine translation
result over the baseline system as well as the
IBM 3 model. However, although co-training
has a bigger improvement on the word alignment
compared with PR
+
, it actually has a lower
BLEU score. This phenomenon shows that the
improvement in the word alignment does not
necessarily lead to the improvement on machine
translation. After combining the co-training
and the PR
+
together, co-training+PR
+
improved
slightly over PR
+
for MT.
System BLEU score
Baseline 29.15
IBM 3 30.24
PR
+
31.59*
co-training 31.04*
co-training+PR
+
31.79*
Table 3: Machine translation results. All entries marked
with an asterisk are better than the baseline with 95%
statistical significance computed using paired bootstrap
resampling (Koehn, 2004).
7 Conclusion and Future Work
In this paper, we explored two different ways to
use LCS data in a MT system: 1) PR framework
to incorporate with Blocked Alignment and
Encouraged Alignment constraints. 2) A semi-
supervised co-training procedure. Both techniques
improve the performance of word alignment and
MT over the baseline. Our techniques are
currently limited to sentences where the LCS data
contains very short (usually one word) phrases
from a minority language. An important line of
investigation for generalizing these approaches is
to consider techniques that cover longer phrases in
the minority language; this can help add more of
the LCS data into training.
Acknowledgements
This work was supported in part by NSF awards
1065397 and 1218692.
8
References
S.and Carbonell J. Ambati, V.and Vogel. 2010.
Active semi-supervised learning for improving word
alignment. In In Proceedings of the Active Learning
for NLP Workshop, NAACL.
Avrim Blum and Tom Mitchell. 1998. Combining
labeled and unlabeled data with co-training. In
Annual Conference on Computational Learning
Theaory.
P. F. Brown, S. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1994. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
Chris Callison-Burch and Miles Osborne. 2003. Co-
training for statistical machine translation. In In
Proceedings of the 6th Annual CLUK Research
Colloquium.
Chris Callison-Burch, David Talbot, and Miles Os-
borne. 2004. Statistical machine translation with
word- and sentence-aligned parallel corpora. In In
Proceedings of ACL.
J. Y. C. Chan, P. C. Ching, and H. M. LEE, T.and Meng.
2004. Detection of language boundary in code-
switching utterances by bi-phone probabilities. In
In Proceedings of the International Symposium on
Chinese Spoken Language Processing.
A De Fina. 2007. Code-switching and the construction
of ethnic identity in a community of practice. In
Language in Society, volume 36.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. In Royal Statistical Society, Ser,
volume 39.
Andreas Eisele. 2006. Parallel corpora and
phrase-based statistical machine translation for new
language pairs via multiple intermediaries. In
International Conference on Language Resources
and Evaluation.
Alex Fraser and Daniel Marcu. 2006. Semi-supervised
training for statistical word alignment. In In
Proceedings of ACL.
Kuzman Ganchev, J. Graca, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for
structured latent variable models. In Journal of
Machine Learning Research, volume 11.
J. Graca, K. Ganchev, and B. Taskar. 2008.
Expectation maximization and posterior constraints.
In NIPS.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL-HLT.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In In Proceedings of
EMNLP.
Shankar Kumar, Franz Josef Och, and Wolfgang
Macherey. 2007. Improving word alignment with
bridge languages. In EMNLP.
Esme Manandise and Claudia Gdaniec. 2011. Mor-
phology to the rescue redux: Resolving borrowings
and code-mixing in machine translation. In SFCM.
C. Nilep. 2006. Code switching in sociocultural
linguistics. In Colorado Research in Linguistics,
volume 19.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In ACL.
R.M.K. Sinha and A. Thakur. 2005. Machine
translation of bi-lingual hindi-english (hinglish)
text. In In Proceedings of the 10th Conference on
Machine Translation.
T. Solorio and Y. Liu. 2008. Learning to predict
code-switching points. In In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
A. Stolcke. 2002. An extensible language modeling
toolkit. In Proc. Intl. Conf. on Spoken Language
Processing, volume 2, pages 901?904.
S. Vogel, H. Ney, and C. Tillmann. 1996. Hmm-
based word alignment in statistical translation. In
In Proc.COLING.
Ian H. Witten and Timothy C. Bell. 1991. The zero-
frequency problem: Estimating the probabil- ities of
novel events in adaptive text compression. In IEEE
Transactions on Information Theory, volume 4,
pages 1085?1094.
9
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 348?357,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Empirically-motivated Generalizations of CCG Semantic Parsing
Learning Algorithms
Jesse Glass
Temple University
1801 N Broad Street
Philadelphia, PA 19122, USA
jglassemc2@gmail.com
Alexander Yates
Temple University
1801 N Broad Street
Philadelphia, PA 19122, USA
ayates@gmail.com
Abstract
Learning algorithms for semantic parsing
have improved drastically over the past
decade, as steady improvements on bench-
mark datasets have shown. In this pa-
per we investigate whether they can gen-
eralize to a novel biomedical dataset that
differs in important respects from the tra-
ditional geography and air travel bench-
mark datasets. Empirical results for two
state-of-the-art PCCG semantic parsers in-
dicates that learning algorithms are sensi-
tive to the kinds of semantic and syntac-
tic constructions used in a domain. In re-
sponse, we develop a novel learning algo-
rithm that can produce an effective seman-
tic parser for geography, as well as a much
better semantic parser for the biomedical
dataset.
1 Introduction
Semantic parsing is the task of converting nat-
ural language utterances into formal representa-
tions of their meaning. In this paper, we consider
in particular a grounded form of semantic pars-
ing, in which the meaning representation language
takes its logical constants from a given, fixed on-
tology. Several recent systems have demonstrated
the ability to learn semantic parsers for domains
like the GeoQuery database containing geography
relations, or the ATIS database of air travel infor-
mation. In these settings, existing systems can
produce correct meaning representations with F1
scores approaching 0.9 (Wong and Mooney, 2007;
Kwiatkowski et al., 2011).
These benchmark datasets have supported a di-
verse and influential line of research into semantic
parsing learning algorithms for sophisticated se-
mantic constructions, with continuing advances in
accuracy. However, the focus on these datasets
leads to a natural question ? do other natural
datasets have similar syntax and semantics, and if
not, can existing algorithms handle the variability
in syntax and semantics?
In an effort to investigate and improve the
generalization capacity of existing learning algo-
rithms for semantic parsing, we develop a novel,
natural experimental setting, and we test whether
current semantic parsers generalize to the new set-
ting. For our datset, we use descriptions of clin-
ical trials of experimental drugs in the United
States, available from the U.S. National Insti-
tutes of Health
1
. Much of the text in the de-
scription of these clinical trials can be mapped
neatly onto biomedical ontologies, thus permitting
grounded semantic analysis. Crucially, the dataset
was not designed specifically with semantic pars-
ing or question-answering in mind, and as a re-
sult, it provides a natural source for the variety
and complexity of utterances that humans use in
this domain. As an added benefit, a successful
semantic parser in this domain could yield a va-
riety of useful bioinformatics applications by per-
mitting comparisons between and across clinical
trials using structured representations of the data,
rather than unstructured text.
In this initial investigation of semantic parsing
in this context, we ask:
? Can existing semantic parsing learning al-
gorithms handle the variety and complexity
of the clinical trials dataset? We show that
two representative learning algorithms fare
poorly on the clinical trials data: the best one
achieves a 0.41 F1 in our tests.
? What types of constructions are the major
cause of errors on the clinical trials dataset,
1
clinicaltrials.gov
348
and can semantic parsers be extended to han-
dle them? While this initial investigation
does not cover all types of constructions, we
identify three important types of construc-
tions that existing learning algorithms do not
handle. We propose a new learning algorithm
that can handle these types of constructions,
and we demonstrate empirically that the new
algorithm produces a semantic parser that im-
proves by over 23 points in F1 on the clinical
trials dataset compared with existing parsers.
The rest of this paper is organized as follows.
The next section provides background information
on CCG and semantic parsing. Section 3 describes
the text and ontology that form the new clinical
trials dataset for semantic parsing, as well as some
of the problems that exising approaches have on
this dataset. Sections 4 describes our semantic
parsing model, and learning and inference algo-
rithms. Section 5 presents our experiments and re-
sults, and Section 6 concludes.
2 Background on Semantic Parsing with
CCG
Our approach to learning a semantic parser
falls into the general framework of context-free
Probabilistic Combinatory Categorial Grammars
(PCCG) (Zettlemoyer and Collins, 2005) with
typed lambda calculus expressions for the seman-
tics. PCCG grammars involve lexical entries,
which are weighted unary rewrite rules of the form
Syntax : Semantics? Phrase. For example:
Example Lexical Entries
NP : melanoma? skin cancer
S\NP : ?p?d.has condition(p, d)?
patients with
In addition to lexical rules, PCCG grammars in-
volve weighted binary rewrite rules like the fol-
lowing:
Example CCG Grammar Rules
X : f(g)? X/Y : f Y : g (function application)
X : f(g)? Y : g X\Y : f (backward application)
These rules apply for any syntactic categories X
and Y , and any logical forms f and g. The rules
specify mechanisms for deducing syntactic cate-
gories for whole phrases based on their constituent
parts. They also specify mechanisms for identify-
ing semantics (logical forms) for phrases and sen-
tences based on combinations of the semantics for
the constituent parts. Besides function application,
other ways to combine the semantics of children
typically include conjunction, disjunction, func-
tion composition, and substitution, among others.
Inference algorithms for PCCG can identify the
best parse and logical form for a given sentence us-
ing standard dynamic programming algorithms for
context-free grammars (Clark and Curran, 2007).
As a baseline in our experiments, we use a
learning algorithm for semantic parsing known as
Unification Based Learning (UBL) (Kwiatkowski
et al., 2010). Source code for UBL is freely
available. Its authors found that the semantic
parsers it learns achieve results competitive with
the state-of-the-art on a variety of standard se-
mantic parsing data sets, including GeoQuery
(0.882 F1). UBL uses a log-linear probabilis-
tic model P (L, T |S) over logical forms L and
parse tree derivations T , given sentences S. Dur-
ing training, only S and L are observed, and
UBL?s gradient-based parameter estimation algo-
rithm tries to maximize
?
T
P (L, T |S) over the
training dataset. To learn lexicon entries, it adopts
a search procedure that involves unification in
higher-order logic. The objective of the search
procedure is to identify lexical entries for the
words in a sentence that, when combined with the
lexical entries for other words in the sentence, will
produce the observed logical form in the training
data. For each training sentence, UBL heuristi-
cally explores the space of all possible lexical en-
tries to produce a set of promising candidates, and
adds them to the lexicon.
Our second baseline is an extension of this
work, called Factored Unification Based Learning
(FUBL) (Kwiatkowski et al., 2011). Again, source
code is freely available. FUBL factors the lexicon
into a set of base lexical entries, and a set of tem-
plates that can construct more complex lexical en-
tries from the base entries. This allows for a signif-
icantly more compact lexicon, as well as the abil-
ity to handle certain linguistic constructions, like
ellipsis, that appear frequently in the ATIS dataset
and which UBL struggles with. FUBL achieves an
F1 of 0.82 on ATIS (compared with 66.3 for UBL),
and an F1 of 0.886 on GeoQuery; both results are
at or very near the best-reported results for those
datasets.
2.1 Previous Work
Many supervised learning frameworks have been
applied to the task of learning a semantic parser,
including inductive logic programming (Zelle and
Mooney, 1996; Thompson and Mooney, 1999;
Thompson and Mooney, 2003), support vec-
349
tor machine-based kernel approaches (Kate et
al., 2005; Kate and Mooney, 2006; Kate and
Mooney, 2007), machine translation-style syn-
chronous grammars (Wong and Mooney, 2007),
and context-free grammar-based approaches like
probabilistic Combinatory Categorial Grammar
(Zettlemoyer and Collins, 2005; Zettlemoyer and
Collins, 2007; Zettlemoyer and Collins, 2009;
Kwiatkowski et al., 2010; Kwiatkowski et al.,
2011; Lu et al., 2008) and discriminative reranking
(Ge and Mooney, 2006; Ge and Mooney, 2009).
These approaches have yielded steady improve-
ments on standard test sets like GeoQuery. As far
as we are aware, such systems have not been tested
on domains besides ATIS and GeoQuery.
Because of the complexity involved in build-
ing a training dataset for a supervised semantic
parser, there has been a recent push towards de-
veloping techniques which reduce the annotation
cost or the data complexity of the models. Mod-
els have been developed which can handle some
ambiguity in terms of which logical form is the
correct label for each training sentence (Chen et
al., 2010; Liang et al., 2009). Another set of ap-
proaches have investigated the case where no log-
ical forms are provided, but instead some form of
feedback or response from the world is used as ev-
idence for what the correct logical form must have
been (Clarke et al., 2010; Liang et al., 2011; Artzi
and Zettlemoyer, 2011). Several projects have in-
vestigated unsupervised (Goldwasser et al., 2011;
Poon, 2013; Krishnamurthy and Mitchell, 2012)
and semi-supervised (Yahya et al., 2012; Cai and
Yates, 2013) approaches. These techniques tend
to handle either the same benchmark domains, or
simpler questions over larger ontologies. While
such techniques are important, their (unlabeled
and labeled) sample complexity is higher than it
could be, because the underlying grammars in-
volved are not as general as they could be. Our
work investigates techniques that will reduce this
sample complexity.
3 The Clinical Trials Dataset
Clinical trials are scientific experiments that mea-
sure the effects of a medical procedure, instru-
ment, or product on humans. Since September
2009 in the United States, any clinical trial that
is funded by the federal government must make
its results publicly available online at clinicaltri-
als.gov. This site provides a wealth of biomedical
text and structured data, which we use to produce
a novel test set for semantic parsing.
3.1 The text and ontology
We collected our utterances from a set of 47 ran-
dom documents from clinicaltrials.gov. Many as-
pects of each study are reported in structured for-
mat; for example, the number of participants who
were given a placebo and the number of partici-
pants who were given the intervention under con-
sideration are both reported in a table in a stan-
dard format. However, certain crucial aspects of
each study are reported only in text. Perhaps the
most critical aspect of each study that is described
only in text is the set of criteria for deciding who
will be admitted to the study and who cannot be;
these criteria are called inclusion criteria and ex-
clusion criteria. We focus our semantic parsing
tests on these criteria because they often form the
longest portion of unstructured text for a given
clinical trial report; because their meaning can be
represented using a concise set of logical constants
from a biomedical ontology; and because the cri-
teria have a great deal of significance in the clin-
ical trials domain. For example, these criteria are
crucial for understanding why the results of two
related studies about the same intervention might
differ.
The criteria for a study can be logically repre-
sented as a function of candidate test subjects that
returns true if they match the study criteria, and
false otherwise. We use a variant of lambda calcu-
lus over a typed ontology to represent each inclu-
sion and exclusion criterion in our dataset. We ran-
domly collected 803 utterances and manually la-
beled each using our representation language. 401
were used for training, 109 for development, and
293 for our final tests.
To keep our semantic parsing study simple, we
eschewed existing ontologies like UMLS (Boden-
reider, 2004) that are large and overly-complex for
this problem. We instead developed an ontology
of 10 types, 38 relations and functions, and a dic-
tionary of 591 named-entities to build the logical
forms. The five most common types and relations
in our dataset are listed in Table 1. On average,
the logical forms in our dataset involved 3.7 rela-
tions per logical form, typically joined with con-
junction, implication, or disjunction. If accepted,
both the full ontology and dataset will be made
publicly available.
3.2 Problems with semantic parsing the
clinical trials data
We applied two state-of-the-art learning algo-
rithms for learning PCCG semantic parsers ?
UBL and its extension, FUBL? to our training
350
Example Types Example Functions
(p)erson t has condition(p, d)
(d)isease t complication(d, d)
(t)est i result(p, t)
(tr)eatment t treated with(p, tr, date)
(bo)dy-part t located(d, bo)
Table 1: Common types and functions in our on-
tology. In the example functions, t indicates
boolean type, i indicates real values, p indicates
person, d disease, and so on.
patients with acute lymphoma
?p . has condition(p, acute(lymphoma))
hypertension
(i.e., include patients with hypertension)
?p . has condition(p, hypertension)
AST > 3 mg
(i.e., include patients with a level of the AST en-
zyme in the blood of greater than 3 mg)
?p . > (result(p, AST), unit(3, mg))
Table 2: Example utterances from the clinical tri-
als dataset, and their logical forms. Paraphrases in
parentheses do not appear in the actual data.
data and tested the resulting parsers on develop-
ment data. Results indicate that both systems have
difficulty with the clinical trials datasets: FUBL
achieves an F1 of 0.413, and UBL of just 0.281.
To help understand why state-of-the-art sys-
tems? performance differs so much from perfor-
mance on benchmark datasets like GeoQuery, we
performed an error analysis. Table 3 describes the
most common errors we observed. The most com-
mon errors occurred on sentences containing co-
ordination constructions, nested function applica-
tions, and for UBL, ellipsis, although a long tail
of less common errors exists. FUBL manages to
handle the elliptical constructions in Clinical Tri-
als well, but not coordination or nested functions.
Both systems tend to learn many, overly-specific
lexical entries that include too much of the logical
form in one lexical entry. For instance, from the
coordination example in Table 3, UBL learns a lex-
ical entry for the word ?or? that includes the log-
ical form ?p?d1?d2 . or(has condition(p, d1),
has condition(p, d2)). While this entry works
well when coordinating two diseases or conditions
that patients must have, it will not work for coor-
dinations between treatments or dates, or coordi-
nations between diseases that patients should not
have. UBL learns over 250 lexical entries for the
word ?or? from our training dataset of 401 sen-
tences, each one with limited applicability to de-
velopment sentences.
Based on these observed error types, we next
develop novel learning procedures that properly
handle coordination, nested function construc-
tions, and ellipsis.
4 Learning to Handle Complex
Constructions in Clinical Trials Data
4.1 Model and Inference
We introduce the GLL system for learning a se-
mantic parser that generalizes to both GeoQuery
and Clinical Trials data. The semantic parsing
model involves a grammar that consists of a fixed
set of binary CCG rewrite rules, a learned lexicon
?, and a new set T of learned templates for con-
structing unary type-raising rules. We call these
templates for type-raising rules T-rules; these are
described below in Section 4.4.
Following Kwiatkowsi et al.(2010), we as-
sign a probability P (L, T |S,
~
?,G) to a logical
form and parse tree for a sentence licensed by
grammar G using a log-linear model with pa-
rameters
~
?. We use a set of feature functions
~
F (L, T, S) = (f
1
(L, T, S), . . . , f
K
(L, T, S)),
where each f
i
counts the number of times that the
ith grammar rule is used in the derivation of T
and S. The probability of a particular logical form
given a sentence and
~
? is given by:
P (L|S,
~
?,G) =
?
T
exp(
~
? ?
~
F (L, T, S))
?
T
?
,L
?
exp(
~
? ?
~
F (L
?
, T
?
, S))
(1)
where the trees T (and T
?
) are restricted to those
that are licensed by G and which produce L (L?) as
the logical form for the parent node of the tree. In-
ference is performed using standard dynamic pro-
gramming algorithms for context-free parsing.
4.2 Learning
The input for the task of learning a semantic parser
is a set of sentences
~
S, where each S
i
?
~
S has
been labeled with a logical form L(S
i
). We as-
sume a fixed set of binary grammar productions,
and use the training data to learn lexical entries,
T-rules, and parameters. The training objective is
to maximize the likelihood of the observed logical
351
Error Type Freq. Example Description
Nested Funcs. 27% patients > 18 years of age
?p . > (result(age, p), unit(18, year))
Many logical forms involve functions as
arguments to other functions or relations.
Ellipsis 26% diabetes
?p . has condition(p, diabetes)
Many examples in the inclusion (exclu-
sion) criteria simply list a disease or treat-
ment, with the understanding that a patient
p should be included (excluded) if p has
the disease or is undergoing the treatment.
Coordination 16% patient is pregnant or lactating
?p . or(has condition(p, pregnant),
has condition(p, lactating))
Clinical trials data has more coordina-
tion, especially noun phrase and adjective
phrase coordination, than GeoQuery.
Table 3: Three common kinds of utterances in the clinical trials development set that caused UBL and
FUBL to make errors. Frequency indicates the percentage of all development examples that exhibited
that type of construction.
Input: set of labeled sentences {(S
i
, L(S
i
))}, ini-
tial grammar G
0
, number of iterations MAX ,
learning rate ?
?? ?
?i : ?? ? ? {S : L(S
i
)? S
i
}
G? G
0
? ?
~
? ?
~
0
For iteration := 1 to MAX:
TR? TRLEARN(G)
Add dimension ?
t
to
~
? for t ? TR?G
G? G ? TR
For each sentence S
i
:
?? LEXENTLEARN(S
i
, L(S
i
), G)
Add dimension ?
?
to
~
? for all ? ? ??G
G? G ? ?
~
? ?
~
? + ??
i
CLL
Return G,
~
?
Figure 1: The GLL Learning Algorithm. ?
i
CLL
indicates the local gradient of the conditional log
likelihood at sentence S
i
.
forms, or to find G
?
and
~
?
?
such that:
G
?
,
~
?
?
= arg max
G,
~
?
?
i
P (L(S
i
)|S
i
,
~
?,G)
This is a non-convex optimization problem. We
use a greedy optimization procedure that iter-
atively updates G and
~
?. Figure 1 shows an
overview of the full algorithm.
We use stochastic gradient updates to estimate
parameters (LeCun et al., 1998). For each exam-
ple sentence S
i
in training, we compute the local
gradient of the conditional log likelihood function
CLL = logP (L(S
i
)|S
i
,
~
?,G), and update
~
? by
a step in the direction of this local gradient. The
partial derivatives for this local gradient are:
?CLL
??
j
= E
P (T |L(S
i
),S
i
,
~
?,G)
f
j
(L(S
i
), T, S
i
)?
E
P (T |S
i
,
~
?,G)
f
j
(L(S
i
), T, S
i
)
4.3 Learning Lexical Entries with Inverse
Function Composition
We adopt a greedy approach to learning new lex-
ical entries. We first identify in our current parse
any high-scoring lexical entries that cover multiple
words, and then look for new lexical rules for the
sub-phrases covered by these lexical entries that
could combine to create the current parse chart en-
try using the existing grammar rules. This requires
searching through the grammar rules to find chil-
dren nodes that the nonterminal could be the par-
ent of. In general, this produces an intractably
large set, because it requires taking the inverse
of function application and function composition
for forming the semantics of the nonterminal, and
those inverses are intractably large.
Figure 2 shows our algorithm for learning lex-
ical entries, and Figure 3 shows the details of the
critical component that generates the semantics of
new potential lexical entries. For brevity, we omit
the details of how we learn the syntax and map-
pings from semantics to words or phrases for new
lexical entries, but these are borrowed from the ex-
isting techniques in UBL. The crucial difference
from existing techniques is that the SPLITLEARN
algorithm focuses on inverse function composi-
tion, while existing techniques focus on inverse
352
Input: training sentence Sent, its logical form L,
current grammar G
Initialize:
PC ? parse chart from parsing Sent with G
splits? ?
For len := length(Sent) to 1:
For pos := 0 to length(Sent)? len:
e = arg max
entry?PC[len][pos]
entry.score
if e?s only derivation is a lexical rule in G:
(score,?)? SPLITLEARN(e, PC)
splits? splits ? {(score,?)}
split
?
? arg max
split?splits
split.score
Return split
?
.?
Figure 2: LEXENTLEARN Algorithm for learning
lexical entries
function application. While a priori both tech-
niques are reasonable choices (and both work well
on GeoQuery), our empirical results show that in-
verse function composition can learn the same se-
mantic forms as inverse function application, but
in addition can handle nested functions (which
are function compositions) and coordination ? a
form of function composition if one views logical
connectives like or as boolean functions.
The SPLITLEARN algorithm uses a GET-
SUBEXPR subroutine to heuristically select only
certain subexpressions of the input logical form
for computing inverse composition. This is to
avoid a combinatorial explosion in the number
of learned splits of the input semantics. Mostly
we consider any subexpression that forms an ar-
gument to some function in le.sem, but we take
care to also include abstracted versions of these
subexpressions, in which some of their arguments
are in turn replaced by variables. The subrou-
tine FREEVARS identifies all variables in a logical
form that have no quantifier; REPEATVARS iden-
tifies all variables that appear at least twice. PC-
SCORE looks for any entry in the parse chart that
has a matching semantics and returns the score of
that entry, or 0 if no matches are found. We use
PCSCORE to measure the improvement (delta) in
the score of the parse if it uses the two new lexical
entries, rather than the previous single lexical en-
try. SPLITLEARN returns the set of lexical entries
that tie for the largest improvement in the score of
the parse.
Figures 4 and 5 illustrate the difference be-
Input: lexical entry le, parse chart PC
Entries? ?
For s ?GETSUBEXPR(le.sem):
t? copy of s
sem
?
? copy of le.sem
Apply[t]? ?
For v ? FREEVARS(s)? REPEATVARS(sem
?
):
Create variable v
?
, t? t
v
?
sub for v
Concatenate ??v
?
? onto front of t
Apply[t]? Apply[t] ? {v}
For v ? FREEVARS(t):
Remove ??v? from front of sem
?
Concatenate ??v? onto front of t
Create new variable w
sub? ?(w? + each a ? Apply[t] + ?)?
sem
?
? sem
?
sub sub for s
Concatenate ??w? onto front of sem
?
Entries? Entries ? {t, sem
?
}
delta[t], delta[sem
?
]? PCSCORE(t) +
PCSCORE(sem
?
) - PCSCORE(le)
max? max
x
delta[x]
Returnmax, {s ? Entries | delta[s] = max}
Figure 3: SPLITLEARN Algorithm for generating
(the semantics of) new lexical entries.
tween SPLITLEARN and lexical entry learning for
UBL and FUBL. For both example sentences,
there is a point in the learning process where
a logical form must be split using inverse func-
tion composition in order for useful lexical en-
tries to be learned. At those points, UBL and
FUBL split the logical forms using inverse func-
tion application, resulting in splits where the se-
mantics of different lexemes are mixed together
in the two resulting subexpressions. In Figure 4,
all three systems take the logical form ?u.?p. >
(result(p, bilirubin), unit(1.5, u)) and split
it by removing some aspect of the final argu-
ment, unit(1.5, u), from the full expression. In
UBL and FUBL, the term that is left behind in
the full expression is something that unifies with
?u.unit(1.5, u). In GLL, however, only a vari-
able is left behind, since that variable can be re-
placed by ?u. unit(1.5, u) through function com-
position to obtain the original expression. Thus
GLL?s split yields one significantly simpler subex-
pression, which in the end yields simpler lexical
entries. In both figures, and in general for most
parses we have observed, inverse function compo-
sition yields simpler and cleaner subexpressions.
353
  total > 1.5 mg/dL
mg/dl:u
bilirubin:q
?p.>(result(p,bilirubin),unit(1.5,mg/dl))
?p.result(p,bilirubin)
bilirubin
?u.unit(1.5,u)
F: u ? iG: p ? i?i.?u.?p.>(result(p,bilirubin),unit(i,u))?F.?u.?p.>(result(p,bilirubin),F(u))
?G.?F.?u.?p.>(G(p),F(u))
?i.?p.>(result(p,bilirubin),i)
1.5:i?q.?i.?u.?p.>(result(p,q),unit(i,u))?i'.?i.>(i',i)
UBLFUBLGLL
?u.?p.>(result(p,bilirubin),unit(1.5,u))
Figure 4: An example of a sentence with nested-
function semantics. GLL?s lexical entry learning
procedure correctly identifies the most general se-
mantics for the lexeme >, while UBL and FUBL
learn more specific and complex semantics.
4.4 Learning T-rules
We use T-rules to handle elliptical constructions.
They are essentially a simplification of the fac-
tored lexicon used in FUBL that yields very sim-
ilar results. Each T-rule ? ? T is a function of the
form ?e . if type(e) then return Syn : f(e)?
Syn
?
: e, where type is a type from our ontology,
Syn and Syn
?
are two syntactic CCG categories
or variables, and f is an arbitrary lambda calcu-
lus expression. For example, consider the T-rule
? = (?e . if disease(e) then return S\N :
?p . has condition(p, e) ? N : e).
When applied to the entity diabetes, this T-
rule results in an ordinary CCG rule: S\N :
?p . has condition(p, diabetes) ? N :
diabetes. Thus each T-rule is a template for con-
structing unary (type-raising) CCG grammar rules
from an entity of the appropriate type.
TRLEARN works by first identifying a set
of entity symbols E that appear in multiple
lexical entries in the input grammar G. Let
the lexical entries for entity e ? E be denoted
by ?(e); thus, E consists of all entities where
|?(e)| ? 2. TRLEARN then looks for patterns
in each of these sets of lexical entries. If one
of the lexical entries in ?(e) has a semantics
that consists of just e (for example, the lexical
entry N : diabetes ? diabetes), we create
candidate T-rules from every other lexical entry
l
?
? ?(e) that has the same child, such as
S\N : ?p . has condition(p, diabetes) ?
diabetes. From this lexical entry,
we create the candidate T-rule ? =
  toal>1a l. t5>m1o1a g5 /odaoal1m
Lu:Lq:bo.idg1rlalg1uqnu?t5>m1o1a(u /odaoal1m(u
Lq:g5ubo.idg1rlalg1uqnt5>m1o1a?nibo.idg1rlalg1uqn/odaoal1m??
Lue:Lp:Lu:Lq:g5ubo.idg1rlalg1uqnuF?npuqnu??
Lp:Lu:Lq:g5ubo.idg1rlalg1uqnt5>m1o1a?npuqnu??
?ip.?' .?i .>(r' ripes' ri ee
p(iuqnu?isi iG(iuisi i
?i .?? .ult,b>in)G)>ir ? si e
u,?) ?L??Lu:Lq:g5ubo.idg1rlalg1uqnt5>m1o1a?nibo.idg1rlalg1uqn ??
?' .?i .>(r' r?(??iliGes ' ri ee
Figure 5: An example of a sentence with coor-
dination semantics. GLL?s lexical entry learning
procedure correctly identifies the semantics for the
lexeme or, while UBL and FUBL learn incorrect
semantics.
(?x . if disease(x) then return S\N :
?p . has condition(p, x) ? N : x). In general,
the test in the if statement in the T-rule contains
a check for the type of entity e. The right-hand
side of the implication contains a unary grammar
rule whose parent matches the parent of the rule
in l
?
, except that entity e has been replaced by a
variable x. The child of the grammar rule matches
the parent of the basic lexical entry N : e, except
again that the entity e has been replaced by the
variable x.
Having constructed a set of candidate T-rules
from this process, TRLEARN must select the ones
that will actually be added to the grammar. We
use a test of selecting T-rules that cover at least
MIN existing grammar rules in the input gram-
mar G. In our implementation, we set MIN = 2.
When parsing a sentence, the parser checks any
parse chart entry for semantics that consist solely
of an entity; for any such entry, it looks in a hash-
based index for applicable T-rules, applies them to
the entity to construct new unary grammar rules,
and then applies the unary grammar rules to the
parse chart entry to create new nonterminal nodes.
5 Experiments
In our experiments, we test the generality of our
learning algorithm by testing its ability to handle
both GeoQuery and the Clinical Trials datasets.
5.1 Experimental setup
The clinical trials dataset is described above in
Section 3. GeoQuery consists of a database of
354
System Precision Recall F1
UBL 87.9 88.5 88.2
FUBL 88.6 88.6 88.6
GLL 84.6 86.1 85.5
Table 4: GLL performs comparably to two state-
of-the-art learning algorithms for PCCG semantic
parsing on the benchmark GeoQuery dataset.
System Precision Recall F1
UBL 20.3 19.9 20.1
FUBL 42.3 39.7 40.8
GLL 65.3 63.2 64.1
Table 5: On the clinical trials dataset, GLL outper-
forms UBL and FUBL by more than 23 points in
F1, for a reduction in error (i.e., 1-F1) of nearly
40% over FUBL.
2400 geographical entities, such as nations, rivers,
and mountains, as well as 8 geography relations,
such as the location of a mountain, and whether
one state borders another. The text for semantic
parsing consists of a set of 880 geography ques-
tions, labeled with a lambda-calculus representa-
tion of the sentence?s meaning. We follow the pro-
cedure described by Kwiatkowski et al.. (2010)
in splitting these sentences into training, develop-
ment, and test sentences. This dataset allows us to
provide a comparison with other semantic parsers
on a well-known dataset. We measured perfor-
mance based on exact-match of the full logical
form, modulo re-ordering of arguments to sym-
metric relations (like conjunction and disjunction).
5.2 Results and Discussion
Tables 4 and 5 show the results of semantic parsers
learned by the UBL, FUBL, and GLL learning
algorithms on the GeoQuery and clinical trials
datasets, respectively. On the GeoQuery dataset,
all three parsers perform very similarly, although
GLL?s performance is slightly worse. However, on
the clinical trials dataset, GLL significantly out-
performs both UBL and FUBL in terms of preci-
sion, recall, and F1. Of course, there clearly re-
main many syntactic and semantic constructions
that none of these algorithms can currently han-
dle, as all systems perform significantly worse on
clinical trials than on GeoQuery.
Tables 6 shows the overall size of UBL?s and
GLL?s learned lexicons, and Table 7 shows the
number of learned entries for selected lexical
Lexicon Size
System GeoQuery Clinical Trials
UBL 5,149 49,635
GLL 4,528 36,112
Table 6: GLL learns a lexicon that is 27% smaller
than UBL?s lexicon on clinical trials data.
Lexeme UBL meanings GLL meanings
> 36 2
< 28 2
= 35 2
and 6 4
or 254 9
Table 7: For certain common and critical lexical
items in the clinical trials dataset, GLL learns far
fewer (but more general) lexical entries; for the
word ?or?, GLL learns only 3.5% of the entries
that UBL learns.
items that appear frequently in the clinical trials
corpus. FUBL uses a factored lexicon in which
the semantics of a logical form is split across two
data structures. As a result, FUBL?s lexicon is not
directly comparable to the other systems, so for
these comparisons we restrict our attention to UBL
and GLL. UBL tends to learn far more lexical en-
tries than GLL, particularly for words that appear
in multiple sentences. Yet the poorer performance
of UBL on clinical trials is an indication that these
lexical entries are overly specific.
6 Conclusion
We have introduced the clinical trials dataset,
a naturally-occurring set of text where existing
learning algorithms for semantic parsing struggle.
Our new GLL algorithm uses a novel inverse func-
tion composition algorithm to handle coordina-
tion and nested function constructions, and pattern
learning to handle elliptical constructions. These
innovations allow GLL to handle GeoQuery and
improve on clinical trials. Many sources of er-
ror on clinical trials remain for future research,
including long-distance dependencies, attachment
ambiguities, and coreference. In addition, further
investigation is necessary to test how these algo-
rithms handle additional domains and other types
of natural linguistic constructions.
355
Acknowledgments
This work was supported by National Science
Foundation grant 1218692. The authors appreciate
the help that Anjan Nepal, Qingqing Cai, Avirup
Sil, and Fei Huang provided.
References
Yoav Artzi and Luke Zettlemoyer. 2011. Bootstrap-
ping Semantic Parsers from Conversations. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Olivier Bodenreider. 2004. The Unified Medical Lan-
guage System (UMLS): integrating biomedical ter-
minology. Nucleic Acids Research, 32:D267?D270.
Qingqing Cai and Alexander Yates. 2013. Large-scale
Semantic Parsing via Schema Matching and Lexi-
con Extension. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
David L. Chen, Joohyun Kim, and Raymond J.
Mooney. 2010. Training a Multilingual
Sportscaster: Using Perceptual Context to Learn
Language. Journal of Artificial Intelligence Re-
search, 37:397?435.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with ccg and
log-linear models. Computational Linguistics,
33(4):493?552.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world?s re-
sponse. In Computational Natural Language Learn-
ing (CoNLL).
Ruifang Ge and Raymond J. Mooney. 2006. Discrim-
inative Reranking for Semantic Parsing. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING/ACL-06).
Ruifang Ge and Raymond J. Mooney. 2009. Learning
a Compositional Semantic Parser using an Existing
Syntactic Parser. In Joint Conference of the 47th An-
nual Meeting of the Association for Computational
Linguistics and the 4th International Joint Confer-
ence on Natural Language Processing of the Asian
Federation of Natural Language Processing (ACL-
IJCNLP 2009).
D. Goldwasser, R. Reichart, J. Clarke, and D. Roth.
2011. Confidence driven unsupervised semantic
parsing. In Association for Computational Linguis-
tics (ACL).
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing String-Kernels for Learning Semantic Parsers.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the ACL.
Rohit J. Kate and Raymond J. Mooney. 2007. Semi-
Supervised Learning for Semantic Parsing using
Support Vector Machines. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, Short Papers (NAACL/HLT-
2007).
Rohit J. Kate, Yuk Wah Wong, and Raymond J.
Mooney. 2005. Learning to Transform Natural to
Formal Languages. In Proceedings of the Twen-
tieth National Conference on Artificial Intelligence
(AAAI-05).
Jayant Krishnamurthy and Tom Mitchell. 2012.
Weakly Supervised Training of Semantic Parsers. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing Probabilis-
tic CCG Grammars from Logical Form with Higher-
order Unification. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2011. Lexical Gen-
eralization in CCG Grammar Induction for Seman-
tic Parsing. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. 1998.
Gradient-based learning applied to document recog-
nition. In Proceedings of the IEEE.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Association for Computational Linguistics and In-
ternational Joint Conference on Natural Language
Processing (ACL-IJCNLP).
P. Liang, M. I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
Association for Computational Linguistics (ACL).
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A Generative Model for Parsing
Natural Language to Meaning Representations. In
Proceedings of The Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Hoifung Poon. 2013. Grounded Unsupervised Se-
mantic Parsing. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
C.A. Thompson and R.J. Mooney. 1999. Automatic
construction of semantic lexicons for learning natu-
ral language interfaces. In Proc. 16th National Con-
ference on Artificial Intelligence (AAAI-99), pages
487?493.
356
Cynthia A. Thompson and Raymond J. Mooney. 2003.
Acquiring Word-Meaning Mappings for Natural
Language Interfaces. Journal of Artificial Intelli-
gence Research (JAIR), 18:1?44.
Yuk Wah Wong and Raymond J. Mooney. 2007.
Learning Synchronous Grammars for Semantic
Parsing with Lambda Calculus. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics (ACL-2007).
Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural Language Questions for the
Web of Data. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to Parse Database Queries using Inductive Logic
Programming. In AAAI/IAAI, pages 1050?1055.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to Map Sentences to Logical Form: Struc-
tured Classification with Probabilistic Categorial
Grammars. In Proceedings of the Twenty First
Conference on Uncertainty in Artificial Intelligence
(UAI).
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line Learning of Relaxed CCG Grammars for Pars-
ing to Logical Form. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).
Luke S. Zettlemoyer and Michael Collins. 2009.
Learning Context-dependent Mappings from Sen-
tences to Logical Form. In Proceedings of the Joint
Conference of the Association for Computational
Linguistics and International Joint Conference on
Natural Language Processing (ACL-IJCNLP).
357
Learning Representations for Weakly
Supervised Natural Language
Processing Tasks
Fei Huang?
Temple University
Arun Ahuja??
Northwestern University
Doug Downey?
Northwestern University
Yi Yang?
Northwestern University
Yuhong Guo?
Temple University
Alexander Yates?
Temple University
Finding the right representations for words is critical for building accurate NLP systems when
domain-specific labeled data for the task is scarce. This article investigates novel techniques for
extracting features from n-gram models, Hidden Markov Models, and other statistical language
models, including a novel Partial Lattice Markov Random Field model. Experiments on part-
of-speech tagging and information extraction, among other tasks, indicate that features taken
from statistical language models, in combination with more traditional features, outperform
traditional representations alone, and that graphical model representations outperform n-gram
models, especially on sparse and polysemous words.
? 1805 N. Broad St., Wachman Hall 324, Philadelphia, PA 19122, USA.
E-mail: {fei.huang,yuhong,yates}@temple.edu.
?? 2133 Sheridan Road, Evanston, IL, 60208. E-mail: ahuja@eecs.northwestern.edu.
? 2133 Sheridan Road, Evanston, IL, 60208. E-mail: ddowney@eecs.northwestern.edu.
? 2133 Sheridan Road, Evanston, IL, 60208. E-mail: yya518@eecs.northwestern.edu.
Submission received: 13 June 2012; revised submission received: 25 November 2012, accepted for publication:
15 January 2013.
doi:10.1162/COLI a 00167
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 1
1. Introduction
NLP systems often rely on hand-crafted, carefully engineered sets of features to achieve
strong performance. Thus, a part-of-speech (POS) tagger would traditionally use a
feature like, ?the previous token is the? to help classify a given token as a noun
or adjective. For supervised NLP tasks with sufficient domain-specific training data,
these traditional features yield state-of-the-art results. However, NLP systems are in-
creasingly being applied to the Web, scientific domains, personal communications like
e-mails and tweets, among many other kinds of linguistic communication. These texts
have very different characteristics from traditional training corpora in NLP. Evidence
from POS tagging (Blitzer, McDonald, and Pereira 2006; Huang and Yates 2009), parsing
(Gildea 2001; Sekine 1997; McClosky 2010), and semantic role labeling (SRL) (Pradhan,
Ward, and Martin 2007), among other NLP tasks (Daume? III and Marcu 2006; Chelba
and Acero 2004; Downey, Broadhead, and Etzioni 2007; Chan and Ng 2006; Blitzer,
Dredze, and Pereira 2007), shows that the accuracy of supervised NLP systems degrades
significantly when tested on domains different from those used for training. Collecting
labeled training data for each new target domain is typically prohibitively expensive.
In this article, we investigate representations that can be applied to weakly supervised
learning, that is, learning when domain-specific labeled training data are scarce.
A growing body of theoretical and empirical evidence suggests that traditional,
manually crafted features for a variety of NLP tasks limit systems? performance in this
weakly supervised learning for two reasons. First, feature sparsity prevents systems
from generalizing accurately, because many words and features are not observed in
training. Also because word frequencies are Zipf-distributed, this often means that there
is little relevant training data for a substantial fraction of parameters (Bikel 2004b), espe-
cially in new domains (Huang and Yates 2009). For example, word-type features form
the backbone of most POS-tagging systems, but types like ?gene? and ?pathway? show
up frequently in biomedical literature, and rarely in newswire text. Thus, a classifier
trained on newswire data and tested on biomedical data will have seen few training
examples related to sentences with features ?gene? and ?pathway? (Blitzer, McDonald,
and Pereira 2006; Ben-David et al. 2010).
Further, because words are polysemous, word-type features prevent systems from
generalizing to situations in which words have different meanings. For instance, the
word type ?signaling? appears primarily as a present participle (VBG) in Wall Street
Journal (WSJ) text, as in, ?Interest rates rose, signaling that . . . ? (Marcus, Marcinkiewicz,
and Santorini 1993). In biomedical text, however, ?signaling? appears primarily in the
phrase ?signaling pathway,? where it is considered a noun (NN) (PennBioIE 2005); this
phrase never appears in the WSJ portion of the Penn Treebank (Huang and Yates 2010).
Our response to the sparsity and polysemy challenges with traditional NLP repre-
sentations is to seek new representations that allow systems to generalize to previously
unseen examples. That is, we seek representations that permit classifiers to have close
to the same accuracy on examples from other domains as they do on the domain of the
training data. Our approach depends on the well-known distributional hypothesis,
which states that a word?s meaning is identified with the contexts in which it appears
(Harris 1954; Hindle 1990). Our goal is to develop probabilistic statistical language
models that describe the contexts of individual words accurately. We then construct
representations, or mappings from word tokens and types to real-valued vectors,
from statistical language models. Because statistical language models are designed to
model words? contexts, the features they produce can be used to combat problems
with polysemy. And by careful design of the statistical language models, we can limit
86
Huang et al. Computational Linguistics
the number of features that they produce, controlling how sparse those features are in
training data.
Our specific contributions are as follows:
1. We show how to generate representations from a variety of language
models, including n-gram models, Brown clusters, and Hidden Markov
Models (HMMs). We also introduce a Partial-Lattice Markov Random
Field (PL-MRF), which is a tractable variation of a Factorial Hidden
Markov Model (Ghahramani and Jordan 1997) for language modeling,
and we show how to produce representations from it.
2. We quantify the performance of these representations in experiments
on POS tagging in a domain adaptation setting, and weakly supervised
information extraction (IE). We show that the graphical models outperform
n-gram representations, even when the n-gram models leverage larger
corpora for training. The PL-MRF representation achieves a state-of-the-art
93.8% accuracy on a biomedical POS tagging task, which represents a
5.5 percentage point absolute improvement over more traditional POS
tagging representations, a 4.8 percentage point improvement over a tagger
using an n-gram representation, and a 0.7 percentage point improvement
over a tagger with an n-gram representation using several orders of
magnitude more training data. The HMM representation improves
over the n-gram model by 7 percentage points on our IE task.
3. We analyze how sparsity, polysemy, and differences between domains
affects the performance of a classifier using different representations.
Results indicate that statistical language model representations, and
especially graphical model representations, provide the best features
for sparse and polysemous words.
The next section describes background material and related work on representation
learning for NLP. Section 3 presents novel representations based on statistical language
models. Sections 4 and 5 discuss evaluations of the representations, first on sequence-
labeling tasks in a domain adaptation setting, and second on a weakly supervised set-
expansion task. Section 6 concludes and outlines directions for future work.
2. Background and Previous Work on Representation Learning
2.1 Terminology and Notation
In a traditional machine learning task, the goal is to make predictions on test data using
a hypothesis that is optimized on labeled training data. In order to do so, practitioners
predefine a set of features and try to estimate classifier parameters from the observed
features in the training data. We call these feature sets representations of the data.
Formally, let X be an instance space for a learning problem. Let Z be the space of
possible labels for an instance, and let f : X ? Z be the target function to be learned.
A representation is a function R: X ? Y , for some suitable feature space Y (such as Rd).
We refer to dimensions of Y as features, and for an instance x ? X we refer to values
for particular dimensions of R(x) as features of x. Given a set of training examples, a
learning machine?s task is to select a hypothesis h from the hypothesis space H, a subset
of ZR(X ). Errors by the hypothesis are measured using a loss function L(x, R, f, h) that
87
Computational Linguistics Volume 40, Number 1
measures the cost of the mismatch between the target function f (x) and the hypothesis
h(R(x)).
As an example, the instance set for POS tagging in English is the set of all English
sentences, and Z is the space of POS sequences containing labels like NN (for noun) and
VBG (for present participle). The target function f is the mapping between sentences
and their correct POS labels. A traditional representation in NLP converts sentences
into sequences of vectors, one for each word position. Each vector contains values for
features like, ?+1 if the word at this position ends with -tion, and 0 otherwise.? A
typical loss function would count the number of words that are tagged differently by
f (x) and h(R(x)).
2.2 Representation-Learning Problem Formulation
Machine learning theory assumes that there is a distribution D over X from which
data is sampled. Given a training set S = {(x1, z1), . . . , (xN, zN )} ? (D(X ),Z )N, a fixed
representation R, a hypothesis space H, and a loss function L, a machine learning
algorithm seeks to identify the hypothesis in H that will minimize the expected loss
over samples from distribution D:
h? = argmin
h?H
Ex?D(X )L(x, R, f, h) (1)
The representation-learning paradigm breaks the traditional notion of a fixed rep-
resentation R. Instead, we allow a space of possible representations R. The full learning
problem can then be formulated as the task of identifying the best R ? R and h ? H
simultaneously:
R?, h? = argmin
R?R,h?H
Ex?D(X )L(x, R, f, h) (2)
The representation-learning problem formulation in Equation (2) can in fact be
reduced to the general learning formulation in Equation (1) by setting the fixed rep-
resentation R to be the identity function, and setting the hypothesis space to be R?H
from the representation-learning task. We introduce the new formulation primarily as
a way of changing the perspective on the learning task: most NLP systems consider
a fixed, manually crafted transformation of the original data to some new space, and
investigate hypothesis classes over that space. In the new formulation, systems learn
the transformation to the feature space, and then apply traditional classification or
regression algorithms.
2.3 Theory on Domain Adaptation
We refer to the distribution D over the instance space X as a domain. For example,
the newswire domain is a distribution over sentences that gives high probability to
sentences about governments and current events; the biomedical literature domain
gives high probability to sentences about proteins and regulatory pathways. In domain
adaptation, a system observes a set of training examples (R(x), f (x)), where instances
x ? X are drawn from a source domain DS, to learn a hypothesis for classifying ex-
amples drawn from a separate target domain DT. We assume that large quantities of
unlabeled data are available for the source domain and target domain, and call these
88
Huang et al. Computational Linguistics
samples US and UT, respectively. For any domain D, let R(D) represent the induced
distribution over the feature space Y given by PrR(D)[y] = PrD[{x such that R(x) = y}].
Previous work by Ben-David et al. (2007, 2010) proves theoretical bounds on an
open-domain learning machine?s performance. Their analysis shows that the choice of
representation is crucial to domain adaptation. A good choice of representation must
allow a learning machine to achieve low error rates on the source domain. Just as
important, however, is that the representation must simultaneously make the source
and target domains look as similar to one another as possible. That is, if the labeling
function f is the same on the source and target domains, then for every h ? H, we can
provably bound the error of h on the target domain by its error on the source domain
plus a measure of the distance between DS and DT:
Ex?DTL(x, R, f, h) ? Ex?DSL(x, R, f, h) + d1(R(DS), R(DT )) (3)
where the variation divergence d1 is given by
d1(D,D?) = 2 sup
B?B
|PrD[B] ? PrD? [B]| (4)
where B is the set of measurable sets under D and D? (Ben-David et al. 2007, 2010).
Crucially, the distance between domains depends on the features in the representa-
tion. The more that features appear with different frequencies in different domains, the
worse this bound becomes. In fact, one lower bound for the d1 distance is the accuracy
of the best classifier for predicting whether an unlabeled instance y = R(x) belongs to
domain S or T (Ben-David et al. 2010). Thus, if R provides one set of common features for
examples from S, and another set of common features for examples from T, the domain
of an instance becomes easy to predict, meaning the distance between the domains
grows, and the bound on our classifier?s performance grows worse.
In light of Ben-David et al.?s theoretical findings, traditional representations in
NLP are inadequate for domain adaptation because they contribute to the d1 distance
between domains. Although many previous studies have shown that lexical features
allow learning systems to achieve impressively low error rates during training, they also
make texts from different domains look very dissimilar. For instance, a feature based on
the word ?bank? or ?CEO? may be common in a domain of newswire text, but scarce
or nonexistent in, say, biomedical literature. Ben David et al.?s theory predicts greater
variance in the error rate of the target domain classifier as the distance grows.
At the same time, traditional representations contribute to data sparsity, a lack of
sufficient training data for the relevant parameters of the system. In traditional super-
vised NLP systems, there are parameters for each word type in the data, or perhaps
even combinations of word types. Because vocabularies can be extremely large, this
leads to an explosion in the number of parameters. As a consequence, for many of their
parameters, supervised NLP systems have zero or only a handful of relevant labeled
examples (Bikel 2004a, 2004b). No matter how sophisticated the learning technique, it
is difficult to estimate parameters without relevant data. Because vocabularies differ
across domains, domain adaptation greatly exacerbates this issue of data sparsity.
2.4 Problem Formulation for the Domain Adaptation Setting
Formally, we define the task of representation learning for domain adaptation as the
following optimization problem: Given a set of unlabeled instances US drawn from the
89
Computational Linguistics Volume 40, Number 1
source domain and unlabeled instances UT from the target domain, as well as a set of
labeled instances LS drawn from the source domain, identify a function R? from the
space of possible representations R that minimizes
R?, h? = argmin
R?R,h?H
(
Ex?DSL(x, R, f, h)
)
+ ?d1(R(DS), R(DT )) (5)
where ? is a free parameter.
Note that there is an underlying tension between the two terms of the objec-
tive function: The best representation for the source domain would naturally include
domain-specific features, and allow a hypothesis to learn domain-specific patterns.
We are aiming, however, for the best general classifier, which happens to be trained
on training data from one domain (or a few domains). The domain-specific features
contribute to distance between domains, and to classifier errors on data taken from
domains not seen in training. By optimizing for this combined objective function, we
allow the optimization method to trade off between features that are best for classifying
source-domain data and features that allow generalization to new domains.
Unlike the representation-learning problem-formulation in Equation (2), Equa-
tion (5) does not reduce to the standard machine-learning problem (Equation (1)). In
a sense, the d1 term acts as a regularizer on R, which also affects H. Representation
learning for domain adaptation is a fundamentally novel learning task.
2.5 Tractable Representation Learning: Statistical Language Models
as Representations
For most hypothesis classes and any interesting space of representations, Equations (2)
and (5) are completely intractable to optimize exactly. Even given a fixed representation,
it is intractable to compute the best hypothesis for many hypothesis classes. And the d1
metric is intractable to compute from samples of a distribution, although Ben-David
et al. (2007, 2010) propose some tractable bounds. We view these problem formulations
as high-level goals rather than as computable objectives.
As a tractable objective, in this work we describe an investigation into the use of
statistical language models as a way to represent the meanings of words. This approach
depends on the well-known distributional hypothesis, which states that a word?s
meaning is identified with the contexts in which it appears (Harris 1954; Hindle 1990).
From this hypothesis, we can formulate the following testable prediction, which we call
the statistical language model representation hypothesis, or LMRH:
To the extent that a model accurately describes a word?s possible contexts, parameters
of that model are highly informative descriptors of the word?s meaning, and are
therefore useful as features in NLP tasks like POS tagging, chunking, NER, and
information extraction.
The LMRH says, essentially, that for NLP tasks, we can decouple the task of optimiz-
ing a representation from the task of optimizing a hypothesis. To learn a representation,
we can train a statistical language model on unlabeled text, and then use parameters
or latent states from the statistical language model to create a representation function.
Optimizing a hypothesis then follows the standard learning framework, using the
representation from the statistical language model.
90
Huang et al. Computational Linguistics
The LMRH is similar to the manifold and cluster assumptions behind other semi-
supervised approaches to machine learning, such as Alternating Structure Optimization
(ASO) (Ando and Zhang 2005) and Structural Correspondence Learning (SCL) (Blitzer,
McDonald, and Pereira 2006). All three of these techniques use predictors built on
unlabeled data as a way to harness the manifold and cluster assumptions. However,
the LMRH is distinct from at least ASO and SCL in important ways. Both ASO and SCL
create multiple ?synthetic? or ?pivot? prediction tasks using unlabeled data, and find
transformations of the input feature space that perform well on these tasks. The LMRH,
on the other hand, is more specific ? it asserts that for language problems, if we opti-
mize word representations on a single task (the language modeling task), this will lead
to strong performance on weakly supervised tasks. In reported experiments on NLP
tasks, both ASO and SCL use certain synthetic predictors that are essentially language
modeling tasks, such as the task of predicting whether the next token is of word type w.
To the extent that these techniques? performance relies on language-modeling tasks as
their ?synthetic predictors,? they can be viewed as evidence in support of the LMRH.
One significant consequence of the LMRH is that it allows us to leverage well-
developed techniques and models from statistical language modeling. Section 3
presents a series of statistical language models that we investigate for learning repre-
sentations for NLP.
2.6 Previous Work
There is a long tradition of NLP research on representations, mostly falling into one of
four categories: 1) vector space models of meaning based on document-level lexical co-
occurrence statistics (Salton and McGill 1983; Sahlgren 2006; Turney and Pantel 2010);
2) dimensionality reduction techniques for vector space models (Deerwester et al. 1990;
Ritter and Kohonen 1989; Honkela 1997; Kaski 1998; Sahlgren 2001, 2005; Blei, Ng, and
Jordan 2003; Va?yrynen and Honkela 2004, 2005; Va?yrynen, Honkela, and Lindqvist
2007); 3) using clusters that are induced from distributional similarity (Brown et al.
1992; Pereira, Tishby, and Lee 1993; Martin, Liermann, and Ney 1998) as non-sparse
features (Miller, Guinness, and Zamanian 2004; Ratinov and Roth 2009; Lin and Wu
2009; Candito and Crabbe 2009; Koo, Carreras, and Collins 2008; Suzuki et al. 2009; Zhao
et al. 2009); and, recently, 4) neural network statistical language models (Bengio 2008;
Bengio et al. 2003; Morin and Bengio 2005; Mnih, Yuecheng, and Hinton 2009; Mnih
and Hinton 2007, 2009) as representations (Weston, Ratle, and Collobert 2008; Collobert
and Weston 2008; Bengio et al. 2009). Our work is a form of distributional clustering
for representations, but where previous work has used bigram and trigram statistics to
form clusters, we build sophisticated models that attempt to capture the context of a
word, and hence its similarity to other words, more precisely. Our experiments show
that the new graphical models provide representations that outperform those from
previous work on several tasks.
Neural network statistical language models have recently achieved state-of-the-art
perplexity results (Mnih and Hinton 2009), and representations based on them have im-
proved in-domain chunking, NER, and SRL (Weston, Ratle, and Collobert 2008; Turian,
Bergstra, and Bengio 2009; Turian, Ratinov, and Bengio 2010). As far as we are aware,
Turian, Ratinov, and Bengio (2010) is the only other work to test a learned representation
on a domain adaptation task, and they show improvement on out-of-domain NER
with their neural net representations. Though promising, the neural network models
are computationally expensive to train, and these statistical language models work
only on fixed-length histories (n-grams) rather than full observation sequences. Turian,
91
Computational Linguistics Volume 40, Number 1
Ratinov, and Bengio?s (2010) tests also show that Brown clusters perform as well or
better than neural net models on all of their chunking and NER tests. We concentrate on
probabilistic graphical models with discrete latent states instead. We show that HMM-
based and other representations significantly outperform the more commonly used
Brown clustering (Brown et al. 1992) as a representation for domain adaptation settings
of sequence-labeling tasks.
Most previous work on domain adaptation has focused on the case where some
labeled data are available in both the source and target domains (Chan and Ng 2006;
Daume? III and Marcu 2006; Blitzer, Dredze, and Pereira 2007; Daume? III 2007; Jiang
and Zhai 2007a, 2007b; Dredze and Crammer 2008; Finkel and Manning 2009; Dredze,
Kulesza, and Crammer 2010). Learning bounds for this domain-adaptation setting are
known (Blitzer et al. 2007; Mansour, Mohri, and Rostamizadeh 2009). Approaches to this
problem setting have focused on appropriately weighting examples from the source and
target domains so that the learning algorithm can balance the greater relevance of the
target-domain data with the larger source-domain data set. In some cases, researchers
combine this approach with semi-supervised learning to include unlabeled examples
from the target domain as well (Daume? III, Kumar, and Saha 2010). These techniques
do not handle open-domain corpora like the Web, where they require expert input to
acquire labels for each new single-domain corpus, and it is difficult to come up with
a representative set of labeled training data for each domain. Our technique requires
only unlabeled data from each new domain, which is significantly easier and cheaper to
acquire. Where target-domain labeled data is available, however, these techniques can
in principle be combined with ours to improve performance, although this has not yet
been demonstrated empirically.
A few researchers have considered the more general case of domain adaptation
without labeled data in the target domain. Perhaps the best known is Blitzer, McDonald,
and Pereira?s (2006) Structural Correspondence Learning (SCL). SCL uses ?pivot? words
common to both source and target domains, and trains linear classifiers to predict these
pivot words from their context. After an SVD reduction of the weight vectors for these
linear classifiers, SCL projects the original features through these weight vectors to
obtain new features that are added to the original feature space. Like SCL, our language
modeling techniques attempt to predict words from their context, and then use the
output of these predictions as new features. Unlike SCL, we attempt to predict all words
from their context, and we rely on traditional probabilistic methods for language mod-
eling. Our best learned representations, which involve significantly different techniques
from SCL, especially latent-variable probabilistic models, significantly outperform SCL
in POS tagging experiments.
Other approaches to domain adaptation without labeled data from the target do-
main include Satpal and Sarawagi (2007), who show that by changing the optimization
function during conditional random field (CRF) training, they can learn classifiers that
port well to new domains. Their technique selects feature subsets that minimize the
distance between training text and unlabeled test text, but unlike our techniques, theirs
cannot learn representations with features that do not appear in the original feature set.
In contrast, we learn hidden features through statistical language models. McClosky,
Charniak, and Johnson (2010) use classifiers from multiple source domains and features
that describe how much a target document diverges from each source domain to deter-
mine an optimal weighting of the source-domain classifiers for parsing the target text.
However, it is unclear if this ?source-combination? technique works well on domains
that are not mixtures of the various source domains. Dai et al. (2007) use KL-divergence
between domains to directly modify the parameters of their naive Bayes model for a
92
Huang et al. Computational Linguistics
text classification task trained purely on the source domain. These last two techniques
are not representation learning, and are complementary to our techniques.
Our representation-learning approach to domain adaptation is an instance of
semi-supervised learning. Of the vast number of semi-supervised approaches to
sequence labeling in NLP, the most relevant ones here include Suzuki and Isozaki?s
(2008) combination of HMMs and CRFs that uses over a billion words of unlabeled text
to achieve the current best performance on in-domain chunking, and semi-supervised
approaches to improving in-domain SRL with large quantities of unlabeled text
(Weston, Ratle, and Collobert 2008; Deschacht and Moens 2009; and Fu?rstenau and
Lapata 2009). Ando and Zhang?s (2005) semi-supervised sequence labeling technique
has been tested on a domain adaptation task for POS tagging (Blitzer, McDonald, and
Pereira 2006); our representation-learning approaches outperform it. Unlike most semi-
supervised techniques, we concentrate on a particularly simple task decomposition: un-
supervised learning for new representations, followed by standard supervised learning.
In addition to our task decomposition being simple, our learned representations are also
task-independent, so we can learn the representation once, and then apply it to any task.
One of the best-performing representations that we consider for domain adaptation
is based on the HMM (Rabiner 1989). HMMs have of course also been used for super-
vised, semi-supervised, and unsupervised POS tagging on a single domain (Banko and
Moore 2004; Goldwater and Griffiths 2007). Recent efforts on improving unsupervised
POS tagging have focused on incorporating prior knowledge into the POS induction
model (Grac?a et al. 2009; Toutanova and Johnson 2007), or on new training techniques
like contrastive estimation (Smith and Eisner 2005) for alternative sequence models.
Despite the fact that completely connected, standard HMMs perform poorly at the POS
induction task (Johnson 2007), we show that they still provide very useful features
for a supervised POS tagger. Experiments in information extraction have previously
also shown that HMMs provide informative features for this quite different, semantic
processing task (Downey, Schoenmackers, and Etzioni 2007; Ahuja and Downey 2010).
This article extends our previous work on learning representations for do-
main adaptation (Huang and Yates 2009, 2010) by investigating new language
representations?the naive Bayes representation and PL-MRF representation (Huang
et al. 2011)?by analyzing results in terms of polysemy, sparsity, and domain diver-
gence; by testing on new data sets including a Chinese POS tagging task; and by pro-
viding an empirical comparison with Brown clusters as representations.
3. Learning Representations of Distributional Similarity
In this section, we will introduce several representation learning models.
3.1 Traditional POS-Tagging Representations
As an example of our terminology, we begin by describing a representation used in
traditional POS taggers (this representation will later form a baseline for our POS
tagging experiments). The instance set X is the set of English sentences, and Z is the set
of POS tag sequences. A traditional representation TRAD-R maps a sentence x ? X to a
sequence of boolean-valued vectors, one vector per word xi in the sentence. Dimensions
for each latent vector include indicators for the word type of xi and various orthographic
features. Table 1 presents the full list of features in TRAD-R. Because our IE task classifies
word types rather than tokens, this baseline is not appropriate for that task. Herein, we
93
Computational Linguistics Volume 40, Number 1
Table 1
Summary of features provided by our representations. ?a1[g(a)] represents a set of boolean
features, one for each value of a, where the feature is true iff g(a) is true. xi represents a token at
position i in sentence x, w represents a word type, Suffixes = {-ing,-ogy,-ed,-s,-ly,-ion,-tion,-ity},
k (and k) represents a value for a latent state (set of latent states) in a latent-variable model, y?
represents the maximum a posteriori sequence of states y for x, yi is the latent variable for xi, and
yi,j is the latent variable for xi at layer j. prefix(y,p) is the p-length prefix of the Brown cluster y.
Representation Features
TRAD-R ?w1[xi = w]
?s?Suffixes1[xi ends with s]
1[xi contains a digit]
n-GRAM-R ?w? ,w??P(w?ww??)/P(w)
LSA-R ?w,j{v?left(w)}j
?w,j{v?right(w)}j
NB-R ?k1[y?i = k]
HMM-TOKEN-R ?k1[y?i = k]
HMM-TYPE-R ?kP(y = k|x = w)
I-HMM-TOKEN-R ?j,k1[y?i,j = k]
I-HMM-TYPE-R ?j,kP(y.,j = k|x = w)
BROWN-TOKEN-R ?j?{?2,?1,0,1,2}
?p?{4,6,10,20} prefix(yi+j, p)
BROWN-TYPE-R ?p prefix(y, p)
LATTICE-TOKEN-R ?j,k1[y?i,j = k]
LATTICE-TYPE-R ?kP(y = k|x = w)
describe how we can learn representations R by using a variety of statistical language
models, for use in both our IE and POS tagging tasks. All representations for POS
tagging inherit the features from TRAD-R; all representations for IE do not.
3.2 n-gram Representations
n-gram representations, which we call n-GRAM-R, model a word type w in terms of the
n-gram contexts in which w appears in a corpus. Specifically, for word w we generate
the vector P(w?ww??)/P(w), the conditional probability of observing the word sequence
w? to the left and w?? to the right of w. Each dimension in this vector represents a com-
bination of the left and right words. The experimental section describes the particular
corpora and statistical language modeling methods used for estimating probabilities.
Note that these features depend only on the word type w, and so for every token xi = w,
n-GRAM-R provides the same set of features regardless of local context.
One drawback of n-GRAM-R is that it does not handle sparsity well?the features
are as sparsely observed as the lexical features in TRAD-R, except that n-GRAM-R fea-
tures can be obtained from larger corpora. As an alternative, we apply latent semantic
analysis (LSA) (Deerwester et al. 1990) to compute a reduced-rank representation. For
word w, let vright(w) represent the right context vector of w, which in each dimension
contains the value of P(ww??)/P(w) for some word w??, as observed in the n-gram
model. Similarly, let vleft(w) be the left context vector of w. We apply LSA to the set
94
Huang et al. Computational Linguistics
   
 


	 	 	 	
 	


 
Figure 1
A graphical representation of the naive Bayes statistical language model. The B and E are special
dummy words for the beginning and end of the sentence.
of right context vectors and the set of left context vectors separately,1 to find reduced-
rank versions v?right(w) and v
?
left(w), where each dimension represents a combination
of several context word types. We then use each component of v?right(w) and v
?
left(w)
as features. After experimenting with different choices for the number of dimensions to
reduce our vectors to, we choose a value of 10 dimensions as the one that maximizes
the performance of our supervised sequence labelers on held-out data. We call this
model LSA-R.
3.3 A Context-Dependent Representation Using Naive Bayes
The n-GRAM-R and LSA-R representations always produce the same features F for a
given word type w, regardless of the local context of a particular token xi = w. Our
remaining representations are all context-dependent, in the sense that the features
provided for token xi depend on the local context around xi. We begin with a statis-
tical language model based on the Naive Bayes model with categorical latent states
S = {1, . . . , K}. First, we form trigrams from our sentences. For each trigram, we form a
separate Bayes net in which each token from the trigram is conditionally independent
given the latent state. For tokens xi?1, xi, and xi+1, the probability of this trigram given
latent state Yi = y is given by:
P(xi?1, xi, xi+1|yi) = Pleft(xi?1|yi)Pmid(xi|yi)Pright(xi+1|yi) (6)
where Pleft, Pmid, and Pright are multinomial distributions conditioned on the latent state.
The probability of a whole sentence is then given by the product of the probabilities
of its trigrams. Figure 1 shows a graphical representation of this model. We train our
models using standard expectation-maximization (Dempster, Laird, and Rubin 1977)
with random initialization of the parameters.
Because our factorization of the sentence does not take into account the fact that the
trigrams overlap, the resulting statistical language model is mass-deficient. Worse still,
it is throwing away information from the dependencies among trigrams which might
help make better clustering decisions. Nevertheless, this model closely mirrors many
of the clustering algorithms used in previous approaches to representation learning for
sequence labeling (Ushioda 1996; Miller, Guinness, and Zamanian 2004; Koo, Carreras,
1 Compare with Dhillon, Foster, and Ungar (2011), who use canonical correlation analysis to find a
simultaneous reduction of the left and right context vectors, a significantly more complex undertaking.
95
Computational Linguistics Volume 40, Number 1
and Collins 2008; Lin and Wu 2009; Ratinov and Roth 2009), and therefore serves as an
important benchmark.
Given a naive Bayes statistical language model, we construct an NB-R representa-
tion that produces |S| boolean features Fs(xi) for each token xi and each possible latent
state s ? S:
Fs(xi) =
{
true if s = arg maxs??SP(xi?1, xi, xi+1|yi = s?),
false otherwise.
For a reasonable choice of S (i.e., |S|  |V|), each feature should be observed often
in a sufficiently large training data set. Therefore, compared with n-GRAM-R, NB-R
produces far fewer features. On the other hand, its features for xi depend not just on
the contexts in which xi has appeared in the statistical language model?s training data,
but also on xi?1 and xi+1 in the current sentence. Furthermore, because the range of
the features is much more restrictive than real-valued features, it is less prone to data
sparsity or variations across domains than real-valued features.
3.4 Context-Dependent, Structured Representations: The Hidden Markov Model
In previous work, we have implemented several representations based on hidden
Markov models (Rabiner 1989), which we used for both sequential labeling (like POS
tagging [Huang et al. 2011] and NP chunking [Huang and Yates 2009]) and IE (Downey,
Schoenmackers, and Etzioni 2007). Figure 2 shows a graphical model of an HMM. An
HMM is a generative probabilistic model that generates each word xi in the corpus
conditioned on a latent variable yi. Each yi in the model takes on integral values from 1
to K, and each one is generated by the latent variable for the preceding word, yi?1. The
joint distribution for a corpus x = (x1, . . . , xN ) and a set of state vectors y = (y1, . . . , yN )
is given by: P(x, y) =
?
i P(xi|yi)P(yi|yi?1). Using expectation-maximization (EM)
(Dempster, Laird, and Rubin 1977), it is possible to estimate the distributions for
P(xi|yi) and P(yi|yi?1) from unlabeled data.
We construct two different representations from HMMs, one for sequence-labeling
tasks and one for IE. For sequence labeling, we use the Viterbi algorithm to produce the
optimal setting y? of the latent states for a given sentence x, or y? = argmax
y
P(x, y). We
use the value of y?i as a new feature for xi that represents a cluster of distributionally
similar words. For IE, we require features for word types w, rather than tokens xi.
Applying Bayes? rule to the HMM parameters, we compute a distribution P(Y|x = w),
where Y is a single latent node, x is a single token, and w is its word type. We then use
each of the K values for P(Y = k|x = w), where k ranges from 1 to K, as features. This set
   
 

	 	 	 	
 	

Figure 2
The Hidden Markov Model.
96
Huang et al. Computational Linguistics
of features represents a ?soft clustering? of w into K different clusters. We refer to these
representations as HMM-TOKEN-R and HMM-TYPE-R, respectively.
We also compare against a multi-layer variation of the HMM from our previous
work (Huang and Yates 2010). This model trains an ensemble of M independent HMM
models on the same corpus, initializing each one randomly. We can then use the Viterbi-
optimal decoded latent state of each independent HMM model as a separate feature for
a token, or the posterior distribution for P(Y|x = w) from each HMM as a separate set
of features for each word type. We refer to this statistical language model as an I-HMM,
and the representations as I-HMM-TOKEN-R and I-HMM-TYPE-R, respectively.
Finally, we compare against Brown clusters (Brown et al. 1992) as learned features.
Although not traditionally described as such, Brown clustering involves constructing
an HMM model in which each word type is restricted to having exactly one latent state
that may generate it. Brown et al. describe a greedy agglomerative clustering algorithm
for training this model on unlabeled text. Following Turian, Ratinov, and Bengio (2010),
we use Percy Liang?s implementation of this algorithm for our comparison, and we test
runs with 100, 320, 1,000 and 3,200 clusters. We use features from these clusters identical
to Turian et al.?s.2 Turian et al. have shown that Brown clusters match or exceed the
performance of neural network-based statistical language models in domain adaptation
experiments for named-entity recognition, as well as in-domain experiments for NER
and chunking.
Because HMM-based representations offer a small number of discrete states as
features, they have a much greater potential to combat sparsity than do n-gram mod-
els. Furthermore, for token-based representations, these models can potentially handle
polysemy better than n-gram statistical language models by providing different features
in different contexts.
3.5 A Novel Lattice Statistical Language Model Representation
Our final statistical language model is a novel latent-variable statistical language model,
called a Partial Lattice MRF (PL-MRF), with rich latent structure, shown in Figure 3. The
model contains a lattice of M ? N latent states, where N is the number of words in a
sentence and M is the number of layers in the model. The dotted and solid lines in the
figure together form a complete lattice of edges between these nodes; the PL-MRF uses
only the solid edges. Formally, let c = 	N2 
, where N is the length of the sentence; let i
denote a position in the sentence, and let j denote a layer in the lattice. If i < c and j is
odd, or if j is even and i > c, we delete edges between yi,j and yi,j+1 from the complete
lattice. The same set of nodes remains, but the partial lattice contains fewer edges and
paths between the nodes. A central ?trunk? at i = c connects all layers of the lattice, and
branches from this trunk connect either to the branches in the layer above or the layer
below (but not both).
The result is a model that retains most of the edges of the complete lattice, but
unlike the complete lattice, it supports tractable inference. As M, N ? ?, five out of
every six edges from the complete lattice appear in the PL-MRF. However, the PL-MRF
makes the branches conditionally independent from one another, except through the
trunk. For instance, the left branch between layers 1 and 2 ((y1,1, y1,2) and (y2,1, y2,2)) in
Figure 3 are disconnected; similarly, the right branch between layers 2 and 3 ((y4,2, y4,3)
and (y5,2, y5,3)) are disconnected, except through the trunk and the observed nodes. As
2 Percy Liang?s implementation is available at http://metaoptimize.com/projects/wordreprs/.
97
Computational Linguistics Volume 40, Number 1
y4,1
y3,1
y4,2
y3,2
y4,3
y3,3
y4,4
y3,4
y4,5
y3,5
x1
y2,1
y1,1
x2
y2,2
y1,2
x3
y2,3
y1,3
x4
y2,4
y1,4
x5
y2,5
y1,5
Figure 3
The PL-MRF model for a five-word sentence and a four-layer lattice. Dashed gray edges are part
of a complete lattice, but not part of the PL-MRF.
a result, excluding the observed nodes, this model has a low tree-width of 2 (excluding
observed nodes), and a variety of efficient dynamic programming and message-passing
algorithms for training and inference can be readily applied (Bodlaender 1988). Our
inference algorithm passes information from the branches inwards to the trunk, and
then upward along the trunk, in time O(K4MN). In contrast, a fully connected lattice
model has tree-width = min(M, N), making inference and learning intractable (Sutton,
McCallum, and Rohanimanesh 2007), partly because of the difficulty in enumerating
and summing over the exponentially-many configurations y for a given x.
We can justify the choice of this model from a linguistic perspective as a way to
capture the multi-dimensional nature of words. Linguists have long argued that words
have many different features in a high dimensional space: They can be separately
described by part of speech, gender, number, case, person, tense, voice, aspect, mass
vs. count, and a host of semantic categories (agency, animate vs. inanimate, physical vs.
abstract, etc.), to name a few (Sag, Wasow, and Bender 2003). In the PL-MRF, each layer
of nodes is intended to represent some latent dimension of words.
We represent the probability distribution for PL-MRFs as log-linear models that
decompose over cliques in the MRF graph. Let Cliq(x, y) represent the set of all maximal
cliques in the graph of the MRF model for x and y. Expressing the lattice model in log-
linear form, we can write the marginal probability P(x) of a given sentence x as:
?
y
?
c?Cliq(x,y) score(c, x, y)
?
x?,y?
?
c?Cliq(x?,y? ) score(c, x
?, y?)
where score(c, x, y) = exp(?c ? fc(xc, yc)). Our model includes parameters for transitions
between two adjacent latent variables on layer j: ?transi,s,i+1,s?,j for yi,j = s and yi+1,j = s
?. It
also includes observation parameters for latent variables and tokens, as well as for pairs
of adjacent latent variables in different layers and their tokens: ?obsi,j,s,w and ?
obs
i,j,s,j+1,s?,w for
yi,j = s, yi,j+1 = s?, and xi = w.
98
Huang et al. Computational Linguistics
As with our HMM models, we create two representations from PL-MRFs, one for
tokens and one for types. For tokens, we decode the model to compute y?, the matrix of
optimal latent state values for sentence x. For each layer j and and each possible latent
state value k, we add a boolean feature for token xi that is true iff y?i,j = k. For word
types, we compute distributions over the latent state space. Let y be a column vector of
latent variables for word type w. For a PL-MRF model with M layers of binary variables,
there are 2M possible values for y. Our type representation computes a probability
distribution over these 2M possible values, and uses each probability as a feature for
w.3 We refer to these two representations as LATTICE-TOKEN-R and LATTICE-TYPE-R,
respectively.
We train the PL-MRF using contrastive estimation (Smith and Eisner 2005), which
iteratively optimizes the following objective function on a corpus X:
?
x?X
log
?
y
?
c?Cliq(x,y) score(c, x, y)
?
x??N (x),y?
?
c?Cliq(x?,y? ) score(c, x
?, y?)
(7)
where N (x), the neighborhood of x, indicates a set of perturbed variations of the original
sentence x. Contrastive estimation seeks to move probability mass away from the per-
turbed neighborhood sentences and onto the original sentence. We use a neighborhood
function that includes all sentences which can be obtained from the original sentence by
swapping the order of a consecutive pair of words. Training uses gradient descent over
this non-convex objective function with a standard software package (Liu and Nocedal
1989) and converges to a local maximum or saddle point.
For tractability, we modify the training procedure to train the PL-MRF one layer
at a time. Let ?i represent the set of parameters relating to features of layer i, and let
??i represent all other parameters. We fix ??0 = 0, and optimize ?0 using contrastive
estimation. After convergence, we fix ??1, and optimize ?1, and so on. For training each
layer, we use a convergence threshold of 10?6 on the objective function in Equation (7),
and each layer typically converges in under 100 iterations.
4. Domain Adaptation with Learned Representations
We evaluate the representations described earlier on POS tagging and NP chunking
tasks in a domain adaptation setting.
4.1 A Rich Problem Setting for Representation Learning
Existing supervised NLP systems are domain-dependent: There is a substantial drop in
their performance when tested on data from a new domain. Domain adaptation is the
task of overcoming this domain dependence. The aim is to build an accurate system for
3 This representation is only feasible for small numbers of layers, and in our experiments that require type
representations, we used M = 10. For larger values of M, other representations are also possible. We also
experimented with a representation which included only M possible values: For each layer l, we included
P(yl = 0|w) as a feature. We used the less-compact representation in our experiments because results
were better.
99
Computational Linguistics Volume 40, Number 1
a target domain by training on labeled examples from a separate source domain. This
problem is sometimes also called transfer learning (Raina et al. 2007).
Two of the challenges for NLP representations, sparsity and polysemy, are exacer-
bated by domain adaptation. New domains come with new words and phrases that
appear rarely (or even not at all) in the training domain, thus increasing problems
with data sparsity. And even for words that do appear commonly in both domains, the
contexts around the words will change from the training domain to the target domain.
As a result, domain adaptation adds to the challenge of handling polysemous words,
whose meaning depends on context.
In short, domain adaptation is a challenging setting for testing NLP representations.
We now present several experiments testing our representations against state-of-the-
art POS taggers in a variety of domain adaptation settings, showing that the learned
representations surpass the previous state-of-the-art, without requiring any labeled data
from the target domain.
4.2 Experimental Set-up
For domain adaptation, we test our representations on two sequence labeling tasks:
POS tagging and chunking. To incorporate learned representation into our models, we
follow this general procedure, although the details vary by experiment and are given in
the following sections. First, we collect a set of unannotated text from both the training
domain and test domain. Second, we learn representations on the unannotated text.
We then automatically annotate both the training and test data with features from the
learned representation. Finally, we train a supervised linear-chain CRF model on the
annotated training set and apply it to the test set.
A linear-chain CRF is a Markov random field (Darroch, Lauritzen, and Speed 1980)
in which the latent variables form a path with edges only between consecutive nodes in
the path, and all latent variables are globally conditioned on the observations. Let X be a
random variable over data sequences, and Z be a random variable over corresponding
label sequences. The conditional distribution over the label sequence Z given X has the
form
p?(Z = z|X = x) ? exp
?
?
?
i
?
j
?j fj(zi?1, zi, x, i)
?
? (8)
where fj(zi?1, zi, x, i) is a real-valued feature function of the entire observation sequence
and the labels at positions i and i ? 1 in the label sequence, and ?j is a parameter to be
estimated from training data.
We use an open source CRF software package designed by Sunita Sarawagi to train
and apply our CRF models.4 As is standard, we use two kinds of feature functions:
transition and observation. Transition feature functions indicate, for each pair of labels
l and l?, whether zi = l and zi?1 = l?. Boolean observation feature functions indicate, for
each label l and each feature f provided by a representation, whether zi = l and xi has
feature f . For each label l and each real-valued feature f in representation R, real-valued
observation feature functions have value f (x) if zi = l, and are zero otherwise.
4 Available from http://sourceforge.net/projects/crf/.
100
Huang et al. Computational Linguistics
4.3 Domain Adaptation for POS Tagging
Our first experiment tests the performance of all the representations we introduced
earlier on an English POS tagging task, trained on newswire text, to tag biomedical re-
search literature. We follow Blitzer et al.?s experimental set-up. The labeled data consists
of the WSJ portion of the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993)
as source domain data, and 561 labeled sentences (9,576 tokens) from the biomedical
research literature database MEDLINE as target domain data (PennBioIE 2005). Fully
23% of the tokens in the labeled test text are never seen in the WSJ training data. The
unlabeled data consists of the WSJ text plus 71,306 additional sentences of MEDLINE
text (Blitzer, McDonald, and Pereira 2006). As a preprocessing step, we replace hapax
legomena (defined as words that appear once in our unlabeled training data) with
the special symbol *UNKNOWN*, and do the same for words in the labeled test sets that
never appeared in any of our unlabeled training text.
For representations, we tested TRAD-R, n-GRAM-R, LSA-R, NB-R, HMM-TOKEN-
R, I-HMM-TOKEN-R (between 2 and 8 layers), and LATTICE-TOKEN-R (8, 12, 16,
and 20 layers). Each latent node in the I-HMMs had 80 possible values, creating
808 ? 1015 possible configurations of the eight-layer I-HMM for a single word. Each
node in our PL-MRF is binary, creating a much smaller number (220 ? 106) of possible
configurations for each word in a 20-layer representation. To give the n-gram model
the largest training data set available, we trained it on the Web 1Tgram corpus (Brants
and Franz 2006). We included the top 500 most common n-grams for each word type,
and then used mutual information on the training data to select the top 10,000 most
relevant n-gram features for all word types, in order to keep the number of features
manageable. We incorporated n-gram features as binary values indicating whether xi
appeared with the n-gram or not. For comparison, we also report on the performance of
Brown clusters (100, 320, 1,000, and 3,200 possible clusters), following Turian, Ratinov,
and Bengio (2010). Finally, we compare against Blitzer, McDonald, and Pereira (2006)
SCL technique, described in Section 2.6, and the standard semi-supervised learning
algorithm ASO (Ando and Zhang 2005), whose results on this task were previously
reported by Blitzer, McDonald, and Pereira (2006).
Table 2 shows the results for the best variation of each kind of model?20 layers for
the PL-MRF, 7 layers for the I-HMM, and 3,200 clusters for the Brown clustering. All
statistical language model representations outperform the TRAD-R baseline.
In nearly all cases, learned representations significantly outperformed TRAD-R. The
best representation, the 20-layer LATTICE-TOKEN-R, reduces error by 47% (35% on
OOV) relative to the baseline TRAD-R, and by 44% (24% on out-of-vocabulary words
(OOV)) relative to the benchmark SCL system. For comparison, this model achieved a
96.8% in-domain accuracy on Sections 22?24 of the Penn Treebank, about 0.5 percentage
point shy of a state-of-the-art in-domain system with more sophisticated supervised
learning (Shen, Satta, and Joshi 2007). The BROWN-TOKEN-R representation, which
Turian, Ratinov, and Bengio (2010) demonstrated performed as well or better than
a variety of neural network statistical language models as representations, achieved
accuracies between the SCL system and the HMM-TOKEN-R. The WEB1T-n-GRAM-R,
I-HMM-TOKEN-R, and LATTICE-TOKEN-R all performed quite close to one another,
but the I-HMM-TOKEN-R and LATTICE-TOKEN-R were trained on many orders of
magnitude less text. The LSA-R and NB-R outperformed the TRAD-R baseline but
not the SCL system. The n-GRAM-R, which was trained on the same text as the
other representations except the WEB1T-n-GRAM-R, performed far worse than the
WEB1T-n-GRAM-R.
101
Computational Linguistics Volume 40, Number 1
Table 2
Learned representations, and especially latent-variable statistical language model
representations, significantly outperform a traditional CRF system on domain adaptation for
POS tagging. Percent error is shown for all words and out-of-vocabulary (OOV) words. The
SCL+500bio system was given 500 labeled training sentences from the biomedical domain.
1.8% of tokens in the biomedical test set had POS tags like ?HYPHENATED?, which are not
part of the tagset for the training data, and were labeled incorrectly by all systems without
access to labeled data from the biomedical domain. As a result, an error rate of 1.8 + 3.9 = 5.7
serves as a reasonable lower bound for a system that has never seen labeled examples from
the biomedical domain.
Model All words OOV words
TRAD-R 11.7 32.7
n-GRAM-R 11.7 32.2
LSA-R 11.6 31.1
NB-R 11.6 30.7
ASO 11.6 29.1
SCL 11.1 28
BROWN-TOKEN-R 10.0 25.2
HMM-TOKEN-R 9.5 24.8
WEB1T-n-GRAM-R 6.9 24.4
I-HMM-TOKEN-R 6.7 24
LATTICE-TOKEN-R 6.2 21.3
SCL+500bio 3.9 ?
The amount of unlabeled training data has a significant impact on the performance
of these representations. This is apparent in the difference between WEB1T-n-GRAM-
R and n-GRAM-R, but it is also true for our other representations. Figure 4 shows the
accuracy of a representative subset of our taggers on words not seen in labeled training
data, as we vary the amount of unlabeled training data available to the language
Figure 4
Learning curve for representations: target domain accuracy of our taggers on OOV words
(not seen in labeled training data), as a function of the number of unlabeled examples given
to the language models.
102
Huang et al. Computational Linguistics
models. Performance grows steadily for all representations we measured, and none
of the learning curves appears to have peaked. Furthermore, the margin between the
more complex graphical models and the simpler n-gram models grows with increasing
amounts of training data.
4.3.1 Sparsity and Polysemy. We expected that statistical language model represen-
tations would perform well in part because they provide meaningful features for
sparse and polysemous words. For sparse tokens, these trends are already evident
in the results in Table 2: Models that provide a constrained number of features, like
HMM-based models, tend to outperform models that provide huge numbers of fea-
tures (each of which, on average, is only sparsely observed in training data), like
TRAD-R.
As for polysemy, HMM models significantly outperform naive Bayes models and
the n-GRAM-R. The n-GRAM-R?s features do not depend on a token type?s context at all,
and the NB-R?s features depend only on the tokens immediately to the right and left of
the current word. In contrast, the HMM takes into account all tokens in the surrounding
sentence (although the strength of the dependence on more distant words decreases
rapidly). Thus the performance of the HMM compared with n-GRAM-R and NB-R,
as well as the performance of the LATTICE-TOKEN-R compared with the WEB1T-n-
GRAM-R, suggests that representations that are sensitive to the context of a word
produce better features.
To test these effects more rigorously, we selected 109 polysemous word types from
our test data, along with 296 non-polysemous word types. The set of polysemous word
types was selected by filtering for words in our labeled data that had at least two
POS tags that began with distinct letters (e.g., VBZ and NNS). An initial set of non-
polysemous word types was selected by filtering for types that appeared with just
one POS tag. We then manually inspected these initial selections to remove obvious
cases of word types that were in fact polysemous within a single part-of-speech, such
as ?bank.? We further define sparse word types as those that appear five times or
fewer in all of our unlabeled data, and we define non-sparse word types as those that
appear at least 50 times in our unlabeled data. Table 3 shows our POS tagging results
on the tokens of our labeled biomedical data with word types matching these four
categories.
As expected, all of our statistical language models outperform the baseline by
a larger margin on polysemous words than on non-polysemous words. The margin
between graphical model representations and the WEB1T-n-GRAM-R model also in-
creases on polysemous words, except for the NB-R. The WEB1T-n-GRAM-R uses none
of the local context to decide which features to provide, and the NB-R uses only the
immediate left and right context, so both models ignore most of the context. In contrast,
the remaining graphical models use Viterbi decoding to take into account all tokens
in the surrounding sentence, which helps to explain their relative improvement over
WEB1T-n-GRAM-R on polysemous words.
The same behavior is evident for sparse words, as compared with non-sparse
words: All of the statistical language model representations outperform the baseline
by a larger margin on sparse words than not-sparse words, and all of the graphical
models perform better relative to the WEB1T-n-GRAM-R on sparse words than not-
sparse words. By reducing the feature space from millions of possible n-gram fea-
tures to L categorical features, these models ensure that each of their features will
be observed often in a reasonably sized training data set. Thus representations based
103
Computational Linguistics Volume 40, Number 1
Table 3
Graphical models consistently outperform n-gram models by a larger margin on sparse words
than not-sparse words, and by a larger margin on polysemous words than not-polysemous
words. One exception is the NB-R, which performs worse relative to WEB1T-n-GRAM-R on
polysemous words than non-polysemous words. For each graphical model representation,
we show the difference in performance between that representation and WEB1T-n-GRAM-R
in parentheses. For each representation, differences in accuracy on polysemous and
non-polysemous subsets were statistically significant at p < 0.01 using a two-tailed
Fisher?s exact test. Likewise for performance on sparse vs. non-sparse categories.
polysemous not polysemous sparse not sparse
tokens 159 4,321 463 12,194
TRAD-R 59.5 78.5 52.5 89.6
WEB1T-n-GRAM-R 68.2 85.3 61.8 94.0
NB-R 64.5 88.7 57.8 89.4
(-WEB1T-n-GRAM-R) (?3.7) (+3.4) (?4.0) (?4.6)
HMM-TOKEN-R 67.9 83.4 60.2 91.6
(-WEB1T-n-GRAM-R) (?0.3) (?1.9) (?1.6) (?2.4)
I-HMM-TOKEN-R 75.6 85.2 62.9 94.5
(-WEB1T-n-GRAM-R) (+7.4) (?0.1) (+1.1) (+0.5)
LATTICE-TOKEN-R 70.5 86.9 65.2 94.6
(-WEB1T-n-GRAM-R) (+2.3) (+1.6) (+3.4) (+0.6)
on graphical models help address two key issues in building representations for POS
tagging.
4.3.2 Domain Divergence. Besides sparsity and polysemy, Ben-David et al.?s (2007, 2010)
theoretical analysis of domain adaptation shows that the distance between two domains
under a representation R of the data is crucial for a good representation. We test their
predictions using learned representations.
Ben-David et al.?s (2007, 2010) analysis depends on a particular notion of distance,
the d1 divergence, that is computationally intractable to calculate. For our analysis, we
resort instead to two different computationally efficient approximations of this measure.
The first uses a more standard notion of distance: the Jensen-Shannon Divergence (dJS),
a distance metric for probability distributions:
dJS(p||q) = 12
?
i
[
pilog
( pi
mi
)
+ qilog
( qi
mi
)]
where mi =
pi+qi
2 .
Intuitively, we aim to measure the distance between two domains by measuring
whether features appear more commonly in one domain than in the other. For instance,
the biomedical domain is far from the newswire domain under the TRAD-R repre-
sentation because word-based features like protein, gene, and pathway appear far more
commonly in the biomedical domain than the newswire domain. Likewise, bank and
president appear far more commonly in newswire text. Since the d1 distance is related
to the optimal classifier for distinguishing two domains, it makes sense to measure the
distance by comparing the frequencies of these features: a classifier can easily use the
occurrence of words like bank and protein to accurately predict whether a given sentence
belongs to the newswire or biomedical domain.
104
Huang et al. Computational Linguistics
More formally, let S and T be two domains, and let f be a feature5 in representation
R?that is, a dimension of the image space of R. Let V be the set of possible values
that f can take on. Let US be an unlabeled sample drawn from S, and likewise for
UT. We first compute the relative frequencies of the different values of f in R(US) and
R(UT ), and then compute dJS between these empirical distributions. Let pf represent the
empirical distribution over V estimated from observations of feature f in R(US), and let
qf represent the same distribution estimated from R(UT ).
Definition 1
JS domain divergence for a feature or df (US, UT ) is the domain divergence between
domains S and T under feature f from representation R, and is given by
df (US, UT ) = dJS(pf ||qf )
For a multidimensional representation, we compute the full domain divergence as a
weighted sum over the domain divergences for its features. Because individual features
may vary in their relevance to a sequence-labeling task, we use weights to indicate
their importance to the overall distance between the domains. We set the weight wf
for feature f proportional to the L1 norm of CRF parameters related to f in the trained
POS tagger. That is, let ? be the CRF parameters for our trained POS tagger, and let
?f = {?l,v|l be the state for zi and v be the value for f}. We set wf =
||?f ||1
||?||1 .
Definition 2
JS Domain Divergence or dR(US, UT ), is the distance between domains S and T under
representation R, and is given by
dR(US, UT ) =
?
f
wf df (US, UT )
Blitzer (2008) uses a different notion of domain divergence to approximate the d1
divergence, which we also experimented with. He trains a CRF classifier on examples
labeled with a tag indicating which domain the example was drawn from. We refer to
this type of classifier as a domain classifier. Note that these should not be confused
with our CRFs used for POS tagging, which take as input examples which are labeled
with POS sequences. For the domain classifier, we tag every token from the WSJ domain
as 0, and every token from the biomedical domain as 1. Blitzer then uses the accuracy
of his domain classifier on a held-out test set as his measure of domain divergence. A
high accuracy for the domain classifier indicates that the representation makes the two
domains easy to separate, and thus high accuracy signifies a high domain divergence. To
measure domain divergence using a domain classifier, we trained our representations
on all of the unlabeled data for this task, as before. We then used 500 randomly sampled
sentences from the WSJ domain, and 500 randomly sampled biomedical sentences, and
labeled these with 0 for the WSJ data and 1 for the biomedical data. We measured
the error rate of our domain-classifier CRF as the average error rate across folds when
performing three-fold cross-validation on these 1,000 sentences.
5 For simplicity, the definition we provide here works only for discrete features, although it is possible to
extend this definition to continuous-valued features.
105
Computational Linguistics Volume 40, Number 1
0.88
0.89
0.9
0.91
0.92
0.93
0.94
0.32 0.37 0.42 0.47
Ta
rg
et
 D
om
ai
n 
 
Ta
gg
in
g 
Ac
cu
ra
cy
 
Domain Divergence under the Representation 
LATTICE-R
I-HMM-R
Trad-R
Ngram-R
1 HMM 
7 HMMs 
8 layer LATTICE 
20 layer LATTICE 
Figure 5
Target-domain POS tagging accuracy for a model developed using a representation R correlates
strongly with lower JS domain divergence between WSJ and biomedical text under each
representation R. The correlation coefficients r2 for the linear regressions drawn in the
figure are both greater than 0.97.
Figure 5 plots the accuracies and JS domain divergences for our POS taggers.
Figure 6 shows the difference between target-domain error and source-domain error
as a function of JS domain divergence. Figures 7 and 8 show the same information,
except that the x axis plots the accuracy of a domain classifier as the way of mea-
suring domain divergence. These results give empirical support to Ben-David et al.?s
(2007, 2010) theoretical analysis: Smaller domain divergence?whether measured by
JS domain divergence or by the accuracy of a domain classifier?correlates strongly
with better target-domain accuracy. Furthermore, smaller domain divergence correlates
strongly with a smaller difference in the accuracy of the taggers on the source and
target domains.
Figure 6
Smaller JS domain divergence correlates with a smaller difference between target-domain error
and source-domain error.
106
Huang et al. Computational Linguistics
0.88
0.89
0.9
0.91
0.92
0.93
0.94
0.95
0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98
Ta
rg
et
 D
om
ai
n 
 
Ta
gg
in
g 
Ac
cu
ra
cy
 
Domain Classification Accuracy 
Trad Rep
I-HMM
Ngram
PL-MRF
1 HMM 
7 HMMs 
20 layer PL-MRF 
8 layer PL-MRF 
Figure 7
Target-domain tagging accuracy decreases with the accuracy of a CRF domain classifier.
Intuitively, this means that training data from a source domain is less helpful for tagging in
a target domain when source-domain data is easy to distinguish from target-domain data.
Figure 8
Better domain classification correlates with a larger difference between target-domain error and
source-domain error.
Although both the JS domain divergence and the domain classifier provide only
approximations of the d1 metric for domain divergence, they agree very strongly:
In both cases, the LATTICE-TOKEN-R representations had the lowest domain diver-
gence, followed by the I-HMM-TOKEN-R representations, followed by TRAD-R, with
n-GRAM-R somewhere between LATTICE-TOKEN-R and I-HMM-TOKEN-R. The main
difference between the two metrics appears to be that the JS domain divergence gives
a greater domain divergence to the eight-layer LATTICE-TOKEN-R model and the
n-GRAM-R, placing them past the four- through eight-layer I-HMM-TOKEN-R represen-
tations. The domain classifier places these models closer to the other LATTICE-TOKEN-R
representations, just past the seven-layer I-HMM-TOKEN-R representation.
107
Computational Linguistics Volume 40, Number 1
The domain divergences of all models, using both techniques for measuring diver-
gence, remain significantly far from zero, even under the best representation. As a result,
there is ample room to experiment with even less-divergent representations of the two
domains, to see if they might yield ever-increasing target-domain accuracies. Note that
this is not simply a matter of adding more layers to the layered models. The I-HMM-
TOKEN-R model performed best with seven layers, and the eight-layer representation
had about the same accuracy and domain divergence as the five-layer model. This
may be explained by the fact that the I-HMM layers are trained independently, and so
additional layers may be duplicating other ones, and causing the supervised classifier
to overfit. But it also shows that our current methodology has no built-in technique
for constraining the domain divergence in our representations?the decrease in domain
divergence from our more sophisticated representations is a coincidental byproduct of
our training methodology, but there is no guarantee that our current mechanisms will
continue to decrease domain divergence simply by increasing the number of layers. An
important consideration for future research is to devise explicit learning mechanisms
that guide representations towards smaller domain divergences.
4.4 Domain Adaptation for Noun-Phrase Chunking and Chinese POS Tagging
We test the generality of our representations by using them for other tasks, domains, and
languages. Here, we report on further sequence-labeling tasks in a domain adaptation
setting: noun phrase chunking for adaptation from news text to biochemistry journals,
and POS tagging in Mandarin for a variety of domains. In the next section, we describe
the use of our representations in a weakly supervised information extraction task.
For chunking, the training set consists of the CoNLL 2000 shared task data for
source-domain labeled data (Sections 15?18 of the WSJ portion of the Penn Treebank,
labeled with chunk tags) (Tjong, Sang, and Buchholz 2000). For test data, we used
biochemistry journal data from the Open American National Corpus6 (OANC). One
of the authors manually labeled 198 randomly selected sentences (5,361 tokens) from
the OANC biochemistry text with noun-phrase chunk information.7 We focus on noun
phrase chunks because they are relatively easy to annotate manually, but contain a large
variety of open-class words that vary from domain to domain. The labeled training set
consists of 8,936 sentences and 211,726 tokens. Twenty-three percent of chunks in the
test set begin with an OOV word (especially adjective-noun constructions like ?aqueous
formation? and ?angular recess?), and 29% begin with a word seen at most twice in
training data; we refer to these as OOV chunks and rare chunks. For our unlabeled
data, we use 15,000 sentences (358,000 tokens; Sections 13?19) of the Penn Treebank
and 45,000 sentences (1,083,000 tokens) from the OANC?s biochemistry section. We
tested TRAD-R (augmented with features for automatically generated POS tags), LSA-R,
n-GRAM-R, NB-R, HMM-TOKEN-R, I-HMM-TOKEN-R (7 layers, which performed best
for POS tagging) and LATTICE-TOKEN-R (20 layers) representations.
Figure 9 shows our NP chunking results for this domain adaptation task. The
performance improvements for the HMM-based chunkers are impressive: LATTICE-
TOKEN-R reduces error by 57% with respect to TRAD-R, and comes close to state-of-the-
art results for chunking on newswire text. The results suggest that this representation
allows the CRF to generalize almost as well to out-of-domain text as in-domain text.
6 Available from http://www.anc.org/OANC/.
7 The labeled data for this experiment are available from the first author?s Web site.
108
Huang et al. Computational Linguistics
F1
on
B
io
ch
em
is
tr
y
Te
xt
0.72 0.74 
0.75 0.76 
0.84 
0.87 0.89 0.86 0.87 0.87 0.88 
0.91 
0.94 0.94 
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Trad-R Ngram-R LSA-R NB-R HMM-R I-HMM-R LATTICE-R
OOV ALL
Freq: 0 1 2 all
Chunks: 284 39 39 1,258
R P R P R P R P
TRAD-R .74 .70 .85 .87 .79 .86 .86 .87
n-GRAM-R .74 .74 .85 .85 .79 .86 .87 .87
LSA-R .76 .74 .82 .83 .78 .85 .87 .88
NB-R .73 .78 .86 .73 .86 .75 .88 .88
HMM-TOKEN-R .80 .89 .92 .88 .92 .90 .91 .90
I-HMM-TOKEN-R .90 .86 .92 .95 .87 .97 .95 .92
LATTICE-TOKEN-R .92 .85 .94 .95 .87 .97 .95 .93
Figure 9
On biomedical journal data from the OANC, our best NP chunker outperforms the baseline
CRF chunker by 0.17 F1 on chunks that begin with OOV words, and by 0.08 on all chunks. The
table shows performance breakdowns (recall and precision) for chunks whose first word has
frequency 0, 1, and 2 in training data, and the number of chunks in test data that fall into each
of these categories.
Improvements are greatest on OOV and rare chunks, where LATTICE-TOKEN-R made
absolute improvements over TRAD-R by 0.17 and 0.09 F1, respectively. Improvements
for the single-layer HMM-TOKEN-R were smaller but still significant: 36% relative re-
duction in error overall, and 32% for OOV chunks.
The improved performance from our HMM-based chunker caused us to wonder
how well the chunker could work without some of its other features. We removed all
tag features and orthographic features and all features for word types that appear fewer
than 20 times in training. This chunker still achieves 0.91 F1 on OANC data, and 0.93
F1 on WSJ data (Section 20), outperforming the TRAD-R system in both cases. It has
only 20% as many features as the baseline chunker, greatly improving its training time.
Thus these features are more valuable to the chunker than features from automatically
produced tags and features for all but the most common words.
For Chinese POS tagging, we use text from the UCLA Corpus of Written Chinese
(Tao and Xiao 2007), which is part of the Lancaster Corpus of Mandarin Chinese
(LCMC). The UCLA Corpus consists of 11,192 sentences of word-segmented and POS-
tagged text in 13 genres (see Table 4). We use gold-standard word segmentation labels
for training and testing. The LCMC tagset consists of 50 Chinese POS tags. On average,
each genre contains 5,284 word tokens, for a total of 68,695 tokens among all genres. We
use the ?news? genre as our source domain, which we use for training and development
109
Computational Linguistics Volume 40, Number 1
Table 4
POS tagging accuracy: the LATTICE-TOKEN-R and other graphical model representations
outperform TRAD-R and state-of-the-art Chinese POS taggers on all target domains. For target
domains, * indicates the performance is statistically significantly better than the Stanford and
TRAD-R baselines at p < 0.05, using a two-tailed ?2 test; ** indicates significance at p < 0.01.
On the news domain, the Stanford tagger is significantly different from all other systems
using a two-tailed ?2 test with p < 0.01.
Domain Stanford TRAD NGR LSA NB HMM I-H LAT
lore 88.4 84.0 84.2 85.3 85.3 89.7 89.9 90.1*
religion 83.5 79.1 79.4 79.8 80.0 85.2 85.6 85.9*
humour 89.0 84.2 84.5 86.2 86.8 89.6 89.6 89.9*
general-fic 87.5 84.5 85.0 85.3 85.7 89.4 89.7 89.9*
essay 88.4 83.2 83.7 84.0 84.3 89.0 89.1 90.1*
mystery 87.4 82.4 83.4 84.3 85.3 90.1 91.1 91.3**
romance 87.5 84.2 84.5 85.3 86.1 89.0 89.5 89.8**
science-fic 88.6 82.1 82.5 83.0 83.0 87.0 88.3 88.6
skills 82.7 77.3 77.7 78.2 78.4 84.9 85.0 85.1**
science 86.0 82.0 82.3 82.4 82.4 87.8 87.8 87.9*
adventure-fic 82.1 74.3 75.2 76.1 77.8 81.7 82.0 82.2
report 91.7 84.2 85.1 85.3 86.1 91.9 91.9 91.9
news 98.8** 96.9 92.3 93.4 94.3 94.2 97.0 97.1
all but news 87.0 81.2 82.0 82.8 83.6 88.1 88.4 88.8**
all domains 88.7 83.2 83.6 84.4 85.5 89.5 89.7 90.0**
data. For test data, we randomly select 20% of every other genre. For our unlabeled
data, we use all of the ?news? text, plus the remaining 80% of the texts from the other
genres. As before, we replace hapax legomena in the unlabeled data with the special
symbol *UNKNOWN*, and do the same for word types in the labeled test sets that never
appear in our unlabeled training texts. We compare against a state-of-the-art Chinese
POS tagger for in-domain text, the CRF-based Stanford tagger (Tseng, Jurafsky, and
Manning 2005). We obtained the code for this tagger,8 and retrained it on our training
data set.
The Chinese POS tagging results are shown in Table 4. The LATTICE-TOKEN-R
outperforms the state-of-the-art Stanford tagger on all target domains. Overall, on all
out-of-domain tests, LATTICE-TOKEN-R provides a relative reduction in error of 13.8%
compared with the Stanford tagger. The best performance is on the ?mystery? domain,
where the LATTICE-TOKEN-R model reaches 91.3% accuracy, a 3.9 percentage points
improvement over the Stanford tagger. Its performance on the in-domain ?news? test set
is significantly worse (1.7 percentage points) than the Stanford tagger, suggesting that
the Stanford tagger relies on domain-dependent features that are helpful for tagging
news, but not for tagging in general. The LATTICE-TOKEN-R?s accuracy is still signifi-
cantly worse on out-of-domain text than in-domain text, but the gap between the two
(8.3 percentage points) is better than the gap for the Stanford tagger (11.8 percentage
points). We believe that the lower out-of-domain performance of our Chinese POS
tagger, compared with our English POS tagger and our chunker, was at least in part
due to having far less unlabeled text available for this task.
8 Available at http://nlp.stanford.edu/software/tagger.shtml.
110
Huang et al. Computational Linguistics
5. Information Extraction Experiments
In this section, we evaluate our learned representations on their ability to capture
semantic, rather than syntactic, information. Specifically, we investigate a set-expansion
task in which we?re given a corpus and a few ?seed? noun phrases from a semantic
category (e.g., Superheroes), and our goal is to identify other examples of the category
in the corpus. This is a different type of weakly supervised task from the earlier domain
adaptation tasks because we are given only a handful of positive examples from a cate-
gory, rather than a large sample of positively and negatively labeled training examples
from a separate domain.
Existing set-expansion techniques utilize the distributional hypothesis: Candidate
noun phrases for a given semantic class are ranked based on how similar their contex-
tual distributions are to those of the seeds. Here, we measure how performance on the
set-expansion task varies when we employ different representations for the contextual
distributions.
5.1 Methods
The set-expansion task we address is formalized as follows. Given a corpus, a set of
seeds from some semantic category C, and a separate set of candidate phrases P, output
a ranking of the phrases in P in decreasing order of likelihood of membership in the
semantic category C.
For any given representation R, the set-expansion algorithm we investigate is
straightforward: We rank candidate phrases in increasing order of the distance between
their feature vectors and those of the seeds. The particular distance metrics utilized are
detailed subsequently.
Because set expansion is performed at the level of word types rather than to-
kens, it requires type-based representations. We compare HMM-TYPE-R, n-GRAM-R,
LATTICE-TYPE-R, and BROWN-TYPE-R in this experiment. We used a 25-state HMM,
and the LATTICE-TYPE-R as described in the previous section. Following previous set-
expansion experiments with n-grams (Ahuja and Downey 2010), we use a trigram
model with Kneser-Ney smoothing for n-GRAM-R.
The distances between the candidate phrases and the seeds for HMM-TYPE-R,
n-GRAM-R, and LATTICE-TYPE-R representations are calculated by first creating a
prototypical ?seed feature vector? equal to the mean of the feature vectors for each
of the seeds in the given representation. Then, we rank candidate phrases in order
of increasing distance between their feature vector and the seed feature vector. As a
distance measure between vectors (in this case, probability distributions), we compute
the average of five standard distance measures, including KL and JS divergence, and
cosine, Euclidean, and L1 distance. In experiments, we found that improving upon
this simple averaging was not easy?in fact, tuning a weighted average of the distance
measures for each representation did not improve results significantly on held-out data.
For Brown clusters, we use prefixes of all possible lengths as features. We define
the similarity between two Brown representation feature vectors to be the number of
features they share in common (this is equivalent to the length of the longest common
prefix between the two original Brown cluster labels). The candidate phrases are then
ranked in decreasing order of the sum of their similarity scores to each of the seeds. We
experimented with normalizing the similarity scores by the longer of the two vector
lengths, and found this to decrease results slightly. We use unnormalized (integer)
similarity scores for Brown clusters in our experiments.
111
Computational Linguistics Volume 40, Number 1
5.2 Data Sets
We utilized a set of approximately 100,000 sentences of Web text, joining multi-word
named entities in the corpus into single tokens using the Lex algorithm (Downey,
Broadhead, and Etzioni 2007). This process enables each named entity (the focus of the
set-expansion experiments) to be treated as a single token, with a single representation
vector for comparison. We developed all word type representations using this corpus.
To obtain examples of multiple semantic categories, we utilized selected Wikipedia
?listOf? pages from Pantel et al. (2009) and augmented these with our own manually
defined categories, such that each list contained at least ten distinct examples occurring
in our corpus. In all, we had 432 examples across 16 distinct categories such as Coun-
tries, Greek Islands, and Police TV Dramas.
5.3 Results
For each semantic category, we tested five different random selections of five seed
examples, treating the unselected members of the category as positive examples, and
all other candidate phrases as negative examples. We evaluate using the area under the
precision-recall curve (AUC) metric.
The results are shown in Table 5. All representations improve performance over
a random baseline, equal to the average AUC over five random orderings for each
category, and the graphical models outperform the n-gram representation. I-HMM-
TYPE-R and Brown clustering in the particular case of 1,000 clusters perform best, with
HMM-TYPE-R performing nearly as well. Brown clusters give somewhat lower results
as the number of clusters varies.
As with POS tagging, we expect that language model representations improve
performance on the IE task by providing informative features for sparse word types.
However, because the IE task classifies word types rather than tokens, we expect the rep-
resentations to provide less benefit for polysemous word types. To test these hypotheses,
we measured how IE performance changed in sparse or polysemous settings. We identi-
fied polysemous categories as those for which fewer than 90% of the category members
had the category as a clear dominant sense (estimated manually); other categories were
considered non-polysemous. Categories whose members had a median number of
occurrences in the corpus of less than 30 were deemed sparse, and others non-sparse.
Table 5
I-HMM-TYPE-R outperforms the other methods, improving performance over a random
baseline by twice as much as either n-GRAM-R or LATTICE-TYPE-R.
model AUC
I-HMM-TYPE-R 0.18
HMM-TYPE-R 0.17
BROWN-TYPE-R-3200 0.16
BROWN-TYPE-R-1000 0.18
BROWN-TYPE-R-320 0.15
BROWN-TYPE-R-100 0.13
LATTICE-TYPE-R 0.11
n-GRAM-R baseline 0.10
Random baseline 0.10
112
Huang et al. Computational Linguistics
Table 6
Graphical models as representations for IE consistently perform better relative to n-gram models
on sparse words, but not necessarily polysemous words.
polysemous not-polysemous sparse not-sparse
types 222 210 266 166
categs. 12 4 13 3
n-GRAM-R 0.07 0.17 0.06 0.25
LATTICE-TYPE-R 0.09 0.15 0.1 0.19
-n-GRAM-R +0.02 ?0.02 +0.04 ?0.06
HMM-TYPE-R 0.14 0.26 0.15 0.32
-n-GRAM-R +0.07 +0.09 +0.09 +0.07
IE performance on these subsets of the data are shown in Table 6. Both graphical
model representations outperform the n-gram representation more on sparse words, as
expected. For polysemy, the picture is mixed: The LATTICE-TYPE-R outperforms
n-GRAM-R on polysemous categories, whereas HMM-TYPE-R?s performance advan-
tage over n-GRAM-R decreases.
One surprise on the IE task is that the LATTICE-TYPE-R performs significantly less
well than the HMM-TYPE-R, whereas the reverse is true on POS tagging. We suspect
that the difference is due to the issue of classifying types vs. tokens. Because of their
more complex structure, PL-MRFs tend to depend more on transition parameters than
do HMMs. Furthermore, our decision to train the PL-MRFs using contrastive estimation
with a neighborhood that swaps consecutive pairs of words also tends to emphasize
transition parameters. As a result, we believe the posterior distribution over latent states
given a word type is more informative in our HMM model than the PL-MRF model.
We measured the entropy of these distributions for the two models, and found that
H(PPL-MRF(y|x = w)) = 9.95 bits, compared with H(PHMM(y|x = w)) = 2.74 bits, which
supports the hypothesis that the drop in the PL-MRF?s performance on IE is due to its
dependence on transition parameters. Further experiments are warranted to investigate
this issue.
5.4 Testing the Language Model Representation Hypothesis in IE
The language model representation hypothesis (Section 2) suggests that all else being
equal, more accurate language models will provide features that lead to better perfor-
mance on NLP tasks. Here, we test this hypothesis on the set expansion IE task.
Figures 10 and 11 show how the performance of the HMM-TYPE-R varies with the
language modeling accuracy of the underlying HMM. Language modeling accuracy
is measured in terms of perplexity on held-out text. Here, we use set expansion data
sets from previous work (Ahuja and Downey 2010). The first two are composed of
extractions from the TextRunner information extraction system (Banko et al. 2007) and
are denoted as Unary (361 examples) and Binary (265 examples). The second, Wikipedia
(2,264 examples), is a sample of Wikipedia concept names. We evaluate the performance
of several different trained HMMs with numbers of latent states K ranging from 5 to
1,600 (to help illustrate how IE and LM performance varies even when model capacity
is fixed, we include three distinct models with K = 100 states trained separately over
the full corpus). We used a distributed implementation of HMM training and corpus
113
Computational Linguistics Volume 40, Number 1
K = 5
K = 10
K = 25 K = 50
K = 100
K = 100
K = 100
K = 200
K = 400
Figure 10
Information extraction (IE) performance of HMM-TYPE-R as the language modeling accuracy of
the HMM varies, on TextRunner data sets. IE accuracy (in terms of area under the precision-recall
curve) tends to increase as language modeling accuracy improves (i.e., perplexity decreases).
5 10
25 50
100
100
100
200 400
5
5
25
2550
50
100
200
100
200
800
1600 400
Figure 11
Information extraction (IE) performance of HMM-TYPE-R as the language modeling accuracy
of the HMM varies on the Wikipedia data set. Number labels indicate the number of latent
states K, and performance is shown for three training corpus sizes (the full corpus consists of
approximately 60 million tokens). IE accuracy (in terms of area under the precision-recall curve)
tends to increase as language modeling accuracy improves (i.e., perplexity decreases).
partitioning techniques (Yang, Yates, and Downey 2013) to enable training of our larger
capacity HMM models on large data sets.
The results provide support for the language model representation hypothesis,
showing that IE performance does tend to improve as language model perplexity
decreases. On the smaller Unary and Binary sets (Figure 10), although IE accuracy
114
Huang et al. Computational Linguistics
does decrease for the lowest-perplexity models, overall language model perplexity
exhibits a negative correlation with IE area under the precision-recall curve (the Pearson
correlation coefficient is ?0.18 for Unary, and ?0.28 for Binary). For Wikipedia (Fig-
ure 11), the trend is more consistent, with IE performance increasing monotonically
as perplexity decreases for models trained on the full training corpus (the Pearson
correlation coefficient is ?0.90).
Figure 11 also illustrates how LM and IE performance changes as the amount
of training text varies. In general, increasing the training corpus size increases IE
performance and decreases perplexity. Over all data points in the figure, IE perfor-
mance correlates most strongly with model perplexity (?0.68 Pearson correlation, ?0.88
Spearman correlation), followed by corpus size (0.66, 0.71) and model capacity (?0.05,
0.38). The small negative Pearson correlation between model capacity and IE perfor-
mance is primarily due to the model with 1,600 states trained on 4% of the corpus.
This model has a large parameter space and sparse training data, and thus suffers from
overfitting in terms of both model perplexity and IE performance. If we ignore this
overfit model, the Pearson correlation between model capacity and IE performance for
the other models in the Figure is 0.24.
Our results show that IE based on distributional similarity tends to improve as the
quality of the latent variable model used to measure distributional similarity improves.
A similar trend was exhibited in our previous work (Ahuja and Downey 2010); here, we
extend the previous results to models with more latent states and a larger, more reliable
test set (Wikipedia). The results suggest that scaling up the training of latent variable
models to utilize larger training corpora and more latent states may be a promising
direction for improving IE capabilities.
6. Conclusion and Future Work
Our study of representation learning demonstrates that by using statistical language
models to aggregate information across many unannotated examples, it is possible to
find accurate distributional representations that can provide highly informative features
to weakly supervised sequence labelers and named-entity classifiers. For both domain
adaptation and weakly supervised set expansion, our results indicate that graphical
models outperform n-gram models as representations, in part for their greater ability to
handle sparsity and polysemy. Our IE task provides important evidence to support the
Language Model Representation Hypothesis, showing that the AUC of the IE system
correlates more with language model perplexity than the size of the training data or
the capacity of the language model. Finally, our sequence labeling experiments provide
empirical evidence in support of theoretical work on domain adaptation, showing that
target-domain tagging accuracy is highly correlated with two different measures of
domain divergence.
Representation learning remains a promising area for finding further improve-
ments in various NLP tasks. The representations we have described are trained in
an unsupervised fashion, so a natural extension is to investigate supervised or semi-
supervised representation-learning techniques. As mentioned previously, our current
techniques have no built-in methods for enforcing that they provide similar features in
different domains; devising a mechanism that enforces this could allow for less domain-
divergent and potentially more accurate representations. We have considered sequence
labeling, but another promising direction is to apply these techniques to more complex
structured prediction tasks, like parsing or relation extraction. Our current approach
to sequence labeling requires retraining of a CRF for every new domain; incremental
115
Computational Linguistics Volume 40, Number 1
retraining techniques for new domains would speed up the process. Finally, models
that combine our representation learning approach with instance weighting and other
forms of supervised domain adaptation may take better advantage of labeled data in
target domains, when it is available.
Acknowledgments
This material is based on work supported
by the National Science Foundation under
grant no. IIS-1065397.
References
Ahuja, Arun and Doug Downey. 2010.
Improved extraction assessment through
better language models. In Proceedings of
the Annual Meeting of the North American
Chapter of the Association of Computational
Linguistics (NAACL-HLT), pages 225?228,
Los Angeles, CA.
Ando, Rie Kubota and Tong Zhang. 2005.
A high-performance semi-supervised
learning method for text chunking.
In Proceedings of the ACL, pages 1?9,
Ann Arbor, MI.
Banko, Michele, Michael J. Cafarella,
Stephen Soderland, Matt Broadhead, and
Oren Etzioni. 2007. Open information
extraction from the web. In Proceedings of
the IJCAI, pages 2670?2676, Hyderabad.
Banko, Michele and Robert C. Moore.
2004. Part of speech tagging in context.
In Proceedings of the COLING, pages
556?561, Geneva.
Ben-David, Shai, John Blitzer, Koby
Crammer, Alex Kulesza, Fernando Pereira,
and Jenn Wortman. 2010. A theory of
learning from different domains. Machine
Learning, 79:151?175.
Ben-David, Shai, John Blitzer, Koby
Crammer, and Fernando Pereira. 2007.
Analysis of representations for domain
adaptation. In Advances in Neural
Information Processing Systems 20,
pages 127?144, Vancouver.
Bengio, Yoshua. 2008. Neural net language
models. Scholarpedia, 3(1):3,881.
Bengio, Yoshua, Re?jean Ducharme, Pascal
Vincent, and Christian Janvin. 2003.
A neural probabilistic language model.
Journal of Machine Learning Research,
3:1,137?1,155.
Bengio, Yoshua, Jerome Louradour,
Ronan Collobert, and Jason Weston.
2009. Curriculum learning. In Proceedings
of the International Conference on Machine
Learning (ICML), pages 41?48,
Montreal.
Bikel, Daniel M. 2004a. A distributional
analysis of a lexicalized statistical
parsing model. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing, pages 182?189,
Barcelona.
Bikel, Daniel M. 2004b. Intricacies of Collins?
parsing model. Computational Linguistics,
30(4):479?511.
Blei, David M., Andrew Y. Ng, and Michael I.
Jordan. 2003. Latent Dirichlet allocation.
Journal of Machine Learning Research,
3:993?1,022.
Blitzer, John. 2008. Domain Adaptation of
Natural Language Processing Systems.
Ph.D. thesis, University of Pennsylvania,
Philadelphia, PA.
Blitzer, John, Koby Crammer, Alex Kulesza,
Fernando Pereira, and Jenn Wortman.
2007. Learning bounds for domain
adaptation. In Advances in Neural
Information Processing Systems,
pages 129?136, Vancouver.
Blitzer, John, Mark Dredze, and Fernando
Pereira. 2007. Biographies, Bollywood,
boom-boxes and blenders: Domain
adaptation for sentiment classification.
In Association for Computational Linguistics
(ACL), pages 40?47, Prague.
Blitzer, John, Ryan McDonald, and
Fernando Pereira. 2006. Domain
adaptation with structural correspondence
learning. In Proceedings of the EMNLP,
pages 120?128, Sydney.
Bodlaender, Hans L. 1988. Dynamic
programming on graphs with bounded
treewidth. In Proceedings of the 15th
International Colloquium on Automata,
Languages and Programming,
pages 105?118, Tampere.
Brants, Thorsten and Alex Franz. 2006.
Web 1t 5-gram version 1. www.ldc.
upenn.edu/catalog/.
Brown, Peter F., Vincent J. Della Pietra,
Peter V. deSouza, Jenifer C. Lai, and
Robert L. Mercer. 1992. Class-based
n-gram models of natural language.
Computational Linguistics, 18:467?479.
Candito, Marie and Benoit Crabbe. 2009.
Improving generative statistical parsing
with semi-supervised word clustering.
In Proceedings of the IWPT, pages 138?141,
Paris.
116
Huang et al. Computational Linguistics
Chan, Yee Seng and Hwee Tou Ng. 2006.
Estimating class priors in domain
adaptation for word sense disambiguation.
In Proceedings of the Association for
Computational Linguistics (ACL),
pages 89?96, Sydney.
Chelba, Ciprian and Alex Acero. 2004.
Adaptation of maximum entropy
classifier: Little data can help a lot.
In Proceedings of the EMNLP,
pages 285?292, Barcelona.
Collobert, Robert and Jason Weston. 2008. A
unified architecture for natural language
processing: Deep neural networks with
multitask learning. In Proceedings of the
International Conference on Machine Learning
(ICML), pages 160?167, Helsinki.
Dai, Wenyuan, Gui-Rong Xue, Qiang Yang,
and Yong Yu. 2007. Transferring naive
Bayes classifiers for text classification.
In Proceedings of the National Conference
on Artificial Intelligence (AAAI),
pages 540?545, Vancouver.
Darroch, J. N., S. L. Lauritzen, and
T. P. Speed. 1980. Markov fields and
log-linear interaction models for
contingency tables. The Annals of
Statistics, 8(3):522?539.
Daume? III, Hal. 2007. Frustratingly easy
domain adaptation. In Proceedings of the
ACL, pages 256?263, Prague.
Daume? III, Hal, Abhishek Kumar, and
Avishek Saha. 2010. Frustratingly easy
semi-supervised domain adaptation.
In Proceedings of the ACL Workshop
on Domain Adaptation (DANLP),
pages 53?59, Uppsala.
Daume? III, Hal and Daniel Marcu. 2006.
Domain adaptation for statistical
classifiers. Journal of Artificial Intelligence
Research, 26:101?126.
Deerwester, Scott, Susan T. Dumais,
George W. Furnas, Thomas K. Landauer,
and Richard Harshman. 1990. Indexing by
latent semantic analysis. Journal of the
American Society of Information Science,
41(6):391?407.
Dempster, Arthur, Nan Laird, and Donald
Rubin. 1977. Likelihood from incomplete
data via the EM algorithm. Journal of
the Royal Statistical Society, Series B,
39(1):1?38.
Deschacht, Koen and Marie-Francine Moens.
2009. Semi-supervised semantic role
labeling using the latent words language
model. In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 21?29,
Singapore.
Dhillon, Paramveer S., Dean Foster, and
Lyle Ungar. 2011. Multi-View Learning of
Word Embeddings via CCA. In Proceedings
of the Advances in Neural Information
Processing Systems (NIPS), volume 24,
pages 886?874, Granada.
Downey, Doug, Matthew Broadhead, and
Oren Etzioni. 2007. Locating complex
named entities in web text. In Proceedings
of the 20th International Joint Conference
on Artificial Intelligence (IJCAI 2007),
pages 2,733?2,739, Hyderabad.
Downey, Doug, Stefan Schoenmackers, and
Oren Etzioni. 2007. Sparse information
extraction: Unsupervised language models
to the rescue. In Proceedings of the ACL,
pages 696?703, Prague.
Dredze, Mark and Koby Crammer. 2008.
Online methods for multi-domain learning
and adaptation. In Proceedings of EMNLP,
pages 689?697, Honolulu, HI.
Dredze, Mark, Alex Kulesza, and Koby
Crammer. 2010. Multi-domain learning
by confidence weighted parameter
combination. Machine Learning,
79:123?149.
Finkel, Jenny Rose and Christopher D.
Manning. 2009. Hierarchical Bayesian
domain adaptation. In Proceedings of
HLT-NAACL, pages 602?610, Boulder, CO.
Fu?rstenau, Hagen and Mirella Lapata. 2009.
Semi-supervised semantic role labeling.
In Proceedings of the 12th Conference of the
European Chapter of the ACL, pages 220?228,
Athens.
Ghahramani, Zoubin and Michael I. Jordan.
1997. Factorial hidden Markov models.
Machine Learning, 29(2-3):245?273.
Gildea, Daniel. 2001. Corpus variation and
parser performance. In Conference on
Empirical Methods in Natural Language
Processing, pages 167?202, Pittsburgh, PA.
Goldwater, Sharon and Thomas L. Griffiths.
2007. A fully Bayesian approach to
unsupervised part-of-speech tagging.
In Proceedings of the ACL, pages 744?751,
Prague.
Grac?a, Joa?o V., Kuzman Ganchev, Ben Taskar,
and Fernando Pereira. 2009. Posterior vs.
parameter sparsity in latent variable
models. In Proceedings of the Neural
Information Processing Systems Conference
(NIPS), pages 664?672, Vancouver.
Harris, Z. 1954. Distributional structure.
Word, 10(23):146?162.
Hindle, Donald. 1990. Noun classification
from predicage-argument structures.
In Proceedings of the ACL, pages 268?275,
Pittsburgh, PA.
117
Computational Linguistics Volume 40, Number 1
Honkela, Timo. 1997. Self-organizing
maps of words for natural language
processing applications. In Proceedings
of the International ICSC Symposium on
Soft Computing, pages 401?407, Millet,
Alberta.
Huang, Fei and Alexander Yates. 2009.
Distributional representations for
handling sparsity in supervised sequence
labeling. In Proceedings of the Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 495?503,
Singapore.
Huang, Fei and Alexander Yates. 2010.
Exploring representation-learning
approaches to domain adaptation. In
Proceedings of the ACL 2010 Workshop on
Domain Adaptation for Natural Language
Processing (DANLP), pages 23?30, Uppsala.
Huang, Fei, Alexander Yates, Arun Ahuja,
and Doug Downey. 2011. Language
models as representations for weakly
supervised NLP tasks. In Proceedings
of the Conference on Natural Language
Learning (CoNLL), pages 125?134,
Portland, OR.
Jiang, Jing and ChengXiang Zhai. 2007a.
Instance weighting for domain
adaptation in NLP. In Proceedings
of ACL, pages 264?271, Prague.
Jiang, Jing and ChengXiang Zhai. 2007b. A
two-stage approach to domain adaptation
for statistical classifiers. In Proceedings of
the Conference on Information and Knowledge
Management (CIKM), pages 401?410, Lisbon.
Johnson, Mark. 2007. Why doesn?t EM find
good HMM POS-taggers. In Proceedings of
the EMNLP, pages 296?305, Prague.
Kaski, S. 1998. Dimensionality reduction
by random mapping: Fast similarity
computation for clustering. In
Proceedings of the IJCNN, pages 413?418,
Washington, DC.
Koo, Terry, Xavier Carreras, and Michael
Collins. 2008. Simple semi-supervised
dependency parsing. In Proceedings of
the Annual Meeting of the Association of
Computational Linguistics (ACL),
pages 595?603, Columbus, OH.
Lin, Dekang and Xiaoyun Wu. 2009. Phrase
clustering for discriminative learning.
In Proceedings of the ACL-IJCNLP,
pages 1,030?1,038, Singapore.
Liu, Dong C. and Jorge Nocedal. 1989. On
the limited memory method for large scale
optimization. Mathematical Programming B,
45(3):503?528.
Mansour, Y., M. Mohri, and
A. Rostamizadeh. 2009. Domain
adaptation with multiple sources.
In Proceedings of the Advances in Neural
Information Processing Systems,
pages 1,041?1,048, Vancouver.
Marcus, Mitchell P., Mary Ann
Marcinkiewicz, and Beatrice Santorini.
1993. Building a large annotated corpus of
English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Martin, Sven, Jorg Liermann, and Hermann
Ney. 1998. Algorithms for bigram and
trigram word clustering. Speech
Communication, 24:19?37.
McClosky, David. 2010. Any Domain Parsing:
Automatic Domain Adaptation for Parsing.
Ph.D. thesis, Brown University,
Providence, RI.
McClosky, David, Eugene Charniak, and
Mark Johnson. 2010. Automatic domain
adaptation for parsing. In North American
Chapter of the Association for Computational
Linguistics - Human Language Technologies
2010 Conference (NAACL-HLT 2010),
pages 28?36, Los Angeles, CA.
Miller, Scott, Jethran Guinness, and
Alex Zamanian. 2004. Name tagging with
word clusters and discriminative training.
In Proceedings of the Annual Meeting of the
North American Chapter of the Association of
Computational Linguistics (HLT-NAACL),
pages 337?342, Boston, MA.
Mnih, Andriy and Geoffrey Hinton. 2007.
Three new graphical models for statistical
language modelling. In Proceedings of
the 24th International Conference on
Machine Learning, pages 641?648,
Corvallis, OR.
Mnih, Andriy and Geoffrey Hinton. 2009.
A scalable hierarchical distributed
language model. In Proceedings of the
Neural Information Processing Systems
(NIPS), pages 1,081?1,088, Vancouver.
Mnih, Andriy, Zhang Yuecheng, and
Geoffrey Hinton. 2009. Improving a
statistical language model through
non-linear prediction. Neurocomputing,
72(7-9):1414?1418.
Morin, Frederic and Yoshua Bengio. 2005.
Hierarchical probabilistic neural network
language model. In Proceedings of the
International Workshop on Artificial
Intelligence and Statistics, pages 246?252,
Barbados.
Pantel, Patrick, Eric Crestan, Arkady
Borkovsky, Ana-Maria Popescu,
and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set
expansion. In Proceedings of the EMNLP,
pages 938?947, Singapore.
118
Huang et al. Computational Linguistics
PennBioIE. 2005. Mining the bibliome
project. http://bioie.ldc.upenn.edu/.
Pereira, Fernando, Naftali Tishby, and
Lillian Lee. 1993. Distributional clustering
of English words. In Proceedings of the
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 183?190, Columbus, OH.
Pradhan, Sameer, Wayne Ward, and James H.
Martin. 2007. Towards robust semantic role
labeling. In Proceedings of NAACL-HLT,
pages 556?563, Rochester, NY.
Rabiner, Lawrence R. 1989. A tutorial on
hidden Markov models and selected
applications in speech recognition.
Proceedings of the IEEE, 77(2):257?285.
Raina, Rajat, Alexis Battle, Honglak Lee,
Benjamin Packer, and Andrew Y. Ng.
2007. Self-taught learning: Transfer
learning from unlabeled data.
In Proceedings of the 24th International
Conference on Machine Learning,
pages 759?766, Corvallis, OR.
Ratinov, Lev and Dan Roth. 2009. Design
challenges and misconceptions in named
entity recognition. In Proceedings of the
Conference on Natural Language Learning
(CoNLL), pages 147?155, Boulder, CO.
Ritter, H. and T. Kohonen. 1989.
Self-organizing semantic maps.
Biological Cybernetics, 61(4):241?254.
Sag, Ivan A., Thomas Wasow, and Emily M.
Bender. 2003. Synactic Theory: A Formal
Introduction. CSLI Publications, Stanford,
CA, second edition.
Sahlgren, Magnus. 2001. Vector-based
semantic analysis: Representing word
meanings based on random labels.
In Proceedings of the Semantic Knowledge
Acquisition and Categorization Workshop,
pages 1?12, Helsinki.
Sahlgren, Magnus. 2005. An introduction
to random indexing. In Methods and
Applications of Semantic Indexing Workshop
at the 7th International Conference on
Terminology and Knowledge Engineering
(TKE), 87:1?9.
Sahlgren, Magnus. 2006. The word-space
model: Using distributional analysis to
represent syntagmatic and paradigmatic
relations between words in high-dimensional
vector spaces. Ph.D. thesis, Stockholm
University.
Salton, Gerard and Michael J. McGill. 1983.
Introduction to Modern Information Retrieval.
McGraw-Hill.
Satpal, Sandeep and Sunita Sarawagi.
2007. Domain adaptation of conditional
probability models via feature subsetting.
In Proceedings of ECML/PKDD,
pages 224?235, Warsaw.
Sekine, Satoshi. 1997. The domain
dependence of parsing. In Proceedings of
Applied Natural Language Processing
(ANLP), pages 96?102, Washington, DC.
Shen, Libin, Giorgio Satta, and Aravind K.
Joshi. 2007. Guided learning for
bidirectional sequence classification.
In Proceedings of the ACL, pages 760?767,
Prague.
Smith, Noah A. and Jason Eisner. 2005.
Contrastive estimation: Training
log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 354?362,
Ann Arbor, MI.
Sutton, Charles, Andrew McCallum, and
Khashayar Rohanimanesh. 2007. Dynamic
conditional random fields: Factorized
probabilistic models for labeling and
segmenting sequence data. Journal of
Machine Learning Research, 8:693?723.
Suzuki, Jun and Hideki Isozaki. 2008.
Semi-supervised sequential labeling and
segmentation using giga-word scale
unlabeled data. In Proceedings of the
Annual Meeting of the Association for
Computational Linguistics (ACL-HLT),
pages 665?673, Columbus, OH.
Suzuki, Jun, Hideki Isozaki, Xavier Carreras,
and Michael Collins. 2009. An empirical
study of semi-supervised structured
conditional models for dependency
parsing. In Proceedings of the EMNLP,
pages 551?560, Singapore.
Tao, Hongyin and Richard Xiao. 2007.
The UCLA Chinese corpus. UCREL.
www.lancaster.ac.uk/fass/projects/
corpus/UCLA/.
Tjong, Erik F., Kim Sang, and Sabine
Buchholz. 2000. Introduction to the
CoNLL-2000 shared task: Chunking.
In Proceedings of the 4th Conference
on Computational Natural Language
Learning, pages 127?132, Lisbon.
Toutanova, Kristina and Mark Johnson.
2007. A Bayesian LDA-based model for
semi-supervised part-of-speech
tagging. In Proceedings of the NIPS,
pages 1,521?1,528, Vancouver.
Tseng, Huihsin, Daniel Jurafsky, and
Christopher Manning. 2005.
Morphological features help POS
tagging of unknown words across
language varieties. In Proceedings
of the Fourth SIGHAN Workshop,
pages 32?39, Jeju Island.
119
Computational Linguistics Volume 40, Number 1
Turian, Joseph, James Bergstra, and
Yoshua Bengio. 2009. Quadratic
features and deep architectures for
chunking. In Proceedings of the North
American Chapter of the Association for
Computational Linguistics - Human
Language Technologies (NAACL HLT),
pages 245?248, Boulder, CO.
Turian, Joseph, Lev Ratinov, and Yoshua
Bengio. 2010. Word representations:
A simple and general method for
semi-supervised learning. In Proceedings
of the Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 384?394, Uppsala.
Turney, Peter D. and Patrick Pantel. 2010.
From frequency to meaning: Vector
space models of semantics. Journal of
Artificial Intelligence Research, 37:141?188.
Ushioda, Akira. 1996. Hierarchical clustering
of words. In Proceedings of the International
Conference on Computational Linguistics
(COLING), pages 1,159?1,162, Copenhagen.
Va?yrynen, Jaakko and Timo Honkela. 2004.
Word category maps based on emergent
features created by ICA. In Proceedings of
the STePs 2004 Cognition + Cybernetics
Symposium, pages 173?185, Tikkurila.
Va?yrynen, Jaakko and Timo Honkela. 2005.
Comparison of independent component
analysis and singular value decomposition
in word context analysis. In Proceedings
of the International and Interdisciplinary
Conference on Adaptive Knowledge
Representation and Reasoning (AKRR),
pages 135?140, Espoo.
Va?yrynen, Jaakko, Timo Honkela, and
Lasse Lindqvist. 2007. Towards explicit
semantic features using independent
component analysis. In Proceedings of the
Workshop Semantic Content Acquisition
and Representation (SCAR), pages 20?27,
Stockholm.
Weston, Jason, Frederic Ratle, and
Ronan Collobert. 2008. Deep learning
via semi-supervised embedding.
In Proceedings of the 25th International
Conference on Machine Learning,
pages 1,168?1,175, Helsinki.
Yang, Yi, Alexander Yates, and Doug
Downey. 2013. Overcoming the memory
bottleneck in distributed training
of latent variable models of text.
In Proceedings of the NAACL-HLT,
pages 579?584, Atlanta, GA.
Zhao, Hai, Wenliang Chen, Chunyu Kit,
and Guodong Zhou. 2009. Multilingual
dependency learning: A huge feature
engineering method to semantic
dependency parsing. In Proceedings of the
CoNLL 2009 Shared Task, pages 55?60,
Boulder, CO.
120
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 627?635,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Extracting Glosses to Disambiguate Word Senses
Weisi Duan
Carnegie Mellon University
Language Technologies Institute
5000 Forbes Ave.
Gates Hillman Complex 5407
Pittsburgh, PA 15213
wduan@cs.cmu.edu
Alexander Yates
Temple University
Computer and Information Sciences
1805 N. Broad St.
Wachman Hall 303A
Philadelphia, PA 19122
yates@temple.edu
Abstract
Like most natural language disambiguation
tasks, word sense disambiguation (WSD) re-
quires world knowledge for accurate predic-
tions. Several proxies for this knowledge
have been investigated, including labeled cor-
pora, user-contributed knowledge, and ma-
chine readable dictionaries, but each of these
proxies requires significant manual effort to
create, and they do not cover all of the ambigu-
ous terms in a language. We investigate the
task of automatically extracting world knowl-
edge, in the form of glosses, from an unlabeled
corpus. We demonstrate how to use these
glosses to automatically label a training cor-
pus to build a statistical WSD system that uses
no manually-labeled data, with experimental
results approaching that of a supervised SVM-
based classifier.
1 Introduction
For many semantic natural language processing
tasks, systems require world knowledge to disam-
biguate language utterances. Word sense disam-
biguation (WSD) is no exception ? systems for
WSD require world knowledge to figure out which
aspects of a word?s context indicate one sense over
another. A fundamental problem for WSD is that the
required knowledge is open-ended. That is, for ev-
ery ambiguous term, new kinds of information about
the world become important, and the knowledge that
a system may have acquired for previously-studied
ambiguous terms may have little or no impact on the
next ambiguous term. Thus open-ended knowledge
acquisition is a fundamental obstacle to strong per-
formance for this disambiguation task.
Researchers have investigated a variety of tech-
niques that address this knowledge acquisition bot-
tleneck in different ways. Supervised WSD tech-
niques, for instance, can learn to associate features
in the context of a word with a particular sense of
that word. Knowledge-based techniques rely on
machine-readable dictionaries or lexical resources
like WordNet (Fellbaum, 1998) to provide the nec-
essary knowledge. And most recently, systems
have used resources like Wikipedia, which contain
user-contributed knowledge in the form of sense-
disambiguated links, to acquire world knowledge for
WSD. Yet each of these approaches is limited by the
amount of manual effort that is needed to build the
necessary resources, and as a result the techniques
are limited to a subset of English words for which
the manually-constructed resources are available.
In this work we investigate an alternative ap-
proach that attacks the problem of knowledge acqui-
sition head-on. We use information extraction (IE)
techniques to extract glosses, or short textual char-
acterizations of the meaning of one sense of a word.
In the ideal case, we would extract full logical forms
to define word senses, but here we instead focus on
a more feasible, but still very useful, sub-task: for a
given word sense, extract a collection of terms that
are highly correlated with that sense and no other
sense of the ambiguous word. Our system requires
as input only an unlabeled corpus of documents that
each contain the ambiguous term of interest.
In experiments, we demonstrate that our gloss
extraction system can often determine key aspects
of a word?s senses. In one experiment our sys-
tem was able to extract glosses with 60% precision
for 20 ambiguous biomedical terms, while discov-
ering 7 senses of those terms that never appeared
in a widely-used dictionary of biomedical terminol-
ogy. In addition, we demonstrate that our extracted
glosses are useful for real WSD problems: our sys-
627
tem outperforms a state-of-the-art unsupervised sys-
tem, and it comes close to the performance of a su-
pervised WSD system on a challenging dataset.
In the next section, we describe previous work. In
Section 3, we formally define the gloss extraction
task and refine it into a sub-task that is feasible for
an IE approach, and Section 5 presents our technique
for using extracted glosses in a WSD task. Section
6 discusses our experiments and results.
2 Previous Work
Many previous systems (Cui et al, 2007; Androut-
sopoulos and Galanis, 2005) have studied the re-
lated task of answering definitional questions on the
Web, such as ?What does cold mean??. Such sys-
tems are focused on information retrieval for human
consumption, and especially on recall of definitional
information (Velardi et al, 2008). They generally
do not consider the problem of how to merge the
large number of similar extracted definitions into a
single item (Fujii and Ishikawa, 2000), so that the
overall result contains one definition per sense of
the word. A separate approach (Pasca, 2005) relies
on the WordNet lexical database to supply the set of
senses, and extracts alternate glosses for the senses
that have already been defined. When glosses are to
be used by computational methods, as in a WSD sys-
tem in our case, it becomes critical that the system
extract one coherent gloss per sense. As far as we
are aware, no previous system has extracted glosses
for word sense disambiguation.
Gloss extraction is related to the task of ontol-
ogy extraction, in which systems extract hierarchies
of word classes (Snow et al, 2006; Popescu et al,
2004). Gloss extraction differs from ontology ex-
traction in that it extracts definitional information
characterizing senses of a single word, rather than
trying to place a word in a hierarchy of other words.
Most WSD systems have relied on hand-labeled
training examples (Leroy and Rindflesch, 2004;
Joshi et al, 2005; Mohammad and Pedersen, 2004)
or on dictionary glosses (Lesk, 1986; Stevenson
and Wilks, 2001) or the WordNet hierarchy (Boyd-
Graber et al, 2007) to help make disambiguation
choices. In recent coarse-grained evaluations, such
systems have achieved accuracies of close to 90%
(Pradhan et al, 2007; Agirre and Soroa, 2007; Schi-
jvenaars et al, 2005). However, by some estimates,
English contains over a million word types, and new
words and new senses are added to the language ev-
ery day. It is unreasonable to expect that any system
will have access to hand-labeled training examples
or useful dictionary glosses for each of them.
More recent techniques based on user-contributed
knowledge (Mihalcea, 2007; Chklovski and Mihal-
cea, 2002; Milne and Witten, 2008), such as that
found in Wikipedia, suffer from similar problems ?
Wikipedia contains many articles on well known en-
tities, categories, and events, but very few articles
that disambiguate verbs, adjectives, adverbs, and
certain kinds of nouns which are poorly represented
in an encyclopedia.
On the other hand, word usages in large corpora
like the Web reflect nearly all of the word senses
in use in English today, albeit without manually-
supplied labels. Unsupervised approaches to WSD
use clustering techniques to group instances of
words into clusters that correspond to different
senses (Pantel and Lin, 2002). While such systems
are more general than supervised and dictionary-
based approaches in that they can handle any word
type and word sense, they have lagged behind other
approaches in terms of accuracy thus far ? for ex-
ample, the best system in the recent word sense in-
duction task of Semeval 2007 (Agirre and Soroa,
2007) achieved an F1 score of 78.7, slightly below
the baseline (78.9) in which all instances of a word
are part of a single cluster. Part of the problem
is that the clustering techniques operate in a bag-
of-words-like representation. This is an extremely
high-dimensional space, and it is difficult in such
a space to determine which dimensions are noise
and which ones correlate with different senses. Our
gloss extraction technique helps to address this curse
of dimensionality by reducing the large vocabulary
of a corpus to a much smaller set of terms that are
highly relevant for WSD. Others (Kulkarni and Ped-
ersen, 2005) have used feature selection techniques
like mutual information to reduce dimensionality,
but so far these techniques have only been able to
find features that correlate with an ambiguous term.
With gloss extraction, we are able to find features
that correlate with individual senses of a term.
3 Overview: The Gloss Extraction Task
Given an input corpus C of documents where each
document contains at least one instance of a keyword
k, a Gloss Extraction system should produce a set of
glosses G = {gi}, where each gi is a logical expres-
sion defining the meaning of a particular sense si of
628
Glosses:
1. cold(a) ? isA(a, b) ? disease(b) ? symptom(a, c) ? possibly-includes(c, d) ? fever(d)
2. cold(a) ? isA(a, b) ? physical-entity(b) ? temperature(a, c) ? less-than(c, 25C)
Sense Indicators:
1. common cold, virus, symptom, fever
2. hot, ice cold, lukewarm, cold room, room temperature
Figure 1: Example glosses and sense indicators for two senses of the word cold.
k, to the exclusion of all other senses of k. Note that
the system must discover the appropriate number of
senses in addition to the gloss for each sense.
While extraction technology has made impressive
advancements, it is not yet at a stage where it can
produce full logical forms for sense glosses. As a
first step towards this goal, we introduce the task of
Sense Indicator Extraction, in which each gloss gi
consists of a set of features that, when present in the
context of an instance of k, strongly indicate that the
instance has sense si, and no other sense. Exam-
ples of both tasks are given in Figure 1. The Sense
Indicator Extraction task represents a nontrivial ex-
traction challenge, but it is much more feasible than
full Gloss Extraction. And the task preserves key
properties of Gloss Extraction: the results are quite
useful for word sense disambiguation. The results
are also readily interpreted upon inspection, making
it easy to monitor a system?s accuracy.
4 Extracting Word Sense Glosses
We present the GLOSSY system, an unsupervised in-
formation extraction system for Sense Indicator Ex-
traction. GLOSSY proceeds in two phases: a col-
location detection phase, in which the system de-
tects components of the glosses, and an arrangement
phase, in which the system decides how many dis-
tinct senses there are, and puts together the compo-
nents of the glosses.
4.1 Collocation Detection
The first major challenge to a Gloss Extraction sys-
tem is that the space of possible features is enor-
mous, and almost all of them are irrelevant to the
task at hand. Supervised techniques can use la-
beled examples to provide clues, but in an unsu-
pervised setting the curse of dimensionality can be
overwhelming. Indeed, unsupervised WSD tech-
niques suffer from exactly this problem.
GLOSSY?s answer to this problem is based on the
following observation: pairs of potential features
which rarely or never co-occur in the same docu-
ment in a large corpus are likely to represent fea-
tures for two distinct senses. The well-known obser-
vation that words rarely exhibit more than one sense
per discourse (Yarowsky, 1995) implies that features
closely associated with a particular sense have a low
probability of appearing in the same document as
features associated with another sense. Features that
are independent of any particular sense of the key-
word, on the other hand, have no such restriction,
and are just as likely to appear in the context of one
sense as any other. As a consequence, a low count
for the co-occurrence of two potential features over
a large corpus of documents for keyword k is a re-
liable indicator that the two features are part of the
glosses of two distinct senses of k.
GLOSSY?s collocation detector begins by index-
ing the corpus and counting the frequency of each
vocabulary word. Using the index, the collocation
detector determines all pairs of potential features
such that each feature appears at least T times, and
the pair of features never co-occurs in the same doc-
ument. We call the pairs that this step finds the ?non-
overlapping? features. Finally, we rank the feature
pairs according to the total number of documents
they appear in, and choose the most frequent N
pairs. This excludes non-overlapping pairs that have
not been seen often enough to provide reliable evi-
dence that they are features of different senses, and
it cuts down on processing time for the next phase of
the algorithm. The collocation detector outputs the
set of features F = {f |?f ?(f, f ?) or (f ?, f) is one
of the top N non-overlapping pairs}. The GLOSSY
system uses stems, words, and bigrams as potential
features. We use N = 100 and T = 50 in our ex-
periments. Figure 2 shows an example corpus and
the set of features that the collocation detector would
output.
629
Corpus of documents for term cold:
DOCUMENT 1: ?Symptoms of the common cold may include fever, headache, sore throat, and coughing.?
DOCUMENT 2: ?Hibernation is a common response to the cold winter weather of temperate climates.?
Non-overlapping feature pairs:
(symptoms,temperate) (headache, climate) (cold winter, common cold) (response, headache)
Detected collocations:
symptoms, temperate, headache, climate, cold winter, common cold, response
Arranged glosses:
cold 1: symptoms, common cold, headache
cold 2: temperate, climate, cold winter
Figure 2: Example operation of the GLOSSY extraction system. The collocation detector finds potential features
using its non-overlapping pair heuristic. The arranger selects a subset of the potential features (in this example, it
drops the feature response) and clusters them to produces glosses containing sense indicators.
4.2 Arranging Glosses
Given the corpus C for keyword k and the features F
that GLOSSY?s collocation detector has discovered,
the arrangement phase groups these features into co-
herent sense glosses. Figure 2 shows an example of
how the features found during collocation detection
may be arranged to form coherent glosses for two
senses of the word ?cold.?
GLOSSY?s Arranger component uses a combina-
tion of a small set of statistics to determine whether
a particular arrangement of the features into glosses
is warranted, based on the given corpus. Let A ? 2F
be an arrangement of the features into clusters rep-
resenting glosses. We require that clusters in A be
disjoint, but we do not require every feature in F to
be included in a cluster in A ? in other words, A
is a partition of a subset of F . We define a scoring
function S that is a linear interpolation of several
statistics of the arrangement A and the corpus C:
S(A|C,w) =
?
i
wifi(A, C) (1)
After experimenting with a number of options, we
settled on the following for our statistics fi:
NUMCLUSTERS: the number of clusters in A. We
use a negative weight for this statistic to favor fewer
senses and encourage clustering.
DOCSCOVERED: the total number of documents in
C in which at least one feature from A appears. We
use this statistic to encourage the Arranger to find an
arrangement that explains the sense of as many ex-
amples of the keyword as possible.
BADOVERLAPS: the number of pairs of features
that co-occur in at least one document in C, and that
belong to different clusters of A. A negative weight
for this statistic encourages overlapping feature pairs
to be placed in the same cluster.
BADNONOVERLAPS: the number of pairs of fea-
tures that never co-occur in C, and that belong to the
same cluster in A. A negative weight for this statis-
tic encourages non-overlapping feature pairs to be
placed in different clusters.
Given such an optimization function, the Ar-
ranger attempts to maximize its value by search-
ing for an optimal A. Note that this is a struc-
tured prediction task in which the choice for some
sub-component of A can greatly affect the choice of
other clusters and features. GLOSSY addresses this
optimization problem with a greedy hill-climbing
search with random restarts. Each round of hill-
climbing is initialized with a randomly chosen sub-
set of features, which are then all assigned to a sin-
gle cluster. Using a randomly chosen search opera-
tor from a pre-defined set, the search procedure at-
tempts to move to a new arrangement A?. It accepts
the move to A? if the optimization function gives a
higher value than at the previous state; otherwise, it
continues from the previous state. Our set of search
operators include a move that splits a cluster; a move
that joins two clusters; a move that swaps a feature
from one cluster to another; a move that removes a
feature from the arrangement altogether; and a move
630
that adds a feature from the pool of unused features.
We used 100 rounds of hill-climbing, and found that
each round converged in fewer than 1000 moves.
To estimate the weights wi for each of the four
features of the Arranger, we use a development cor-
pus consisting of 185 documents each containing the
same ambiguous term, and each labeled with sense
information. Because of the small number of pa-
rameters, we performed a grid search on the devel-
opment data for the optimal values of the weights.
5 A Bootstrapping WSD System
Yarowsky (1995) first recognized that it is possi-
ble to use a small number of features for different
senses to bootstrap an unsupervised word sense dis-
ambiguation system. In Yarowsky?s work, his sys-
tem requires an initial, manually-supplied colloca-
tion as a feature for each sense of a keyword. In con-
trast, we can use GLOSSY?s extracted glosses to sup-
ply starter features fully automatically, using only an
unlabeled corpus. Thus GLOSSY complements the
efforts of Yarowsky and other bootstrapping tech-
niques for WSD (Diab, 2004; Mihalcea, 2002).
Building on their efforts, we design a boot-
strapping WSD system using GLOSSY?s extracted
glosses as follows. Let A be the arranged features
representing glosses for a keyword. We first retrieve
all the documents from our unlabeled corpus which
contain features in A. We then label appearances
of the target word according to the cluster of the
features that appear in that document. If features
for more than one cluster appear in the same docu-
ment, we discard it. The result is an automatically
labeled corpus containing examples of all the ex-
tracted senses.
We use this automatically labeled ?bootstrap cor-
pus? to perform supervised WSD. This allows our
system a great deal of flexibility once the bootstrap
corpus is created: we can use any features of the
corpus, plus the labels, in our classifier. Importantly,
this means we do not need to rely on just the features
in the extracted glosses. We use a multi-class SVM
classifier with a linear kernel and default parameter
settings. We use LibSVM (Chang and Lin, 2001) for
all of our experiments. We use standard features for
supervised WSD (Liu et al, 2004): all stems, words,
bigrams, and trigrams within a context window of 20
words surrounding the ambiguous term.
6 Experiments
We ran two types of experiments, one to measure
the accuracy of our sense gloss extractor, and one to
measure the usefulness of the extracted knowledge
for word sense disambiguation.
6.1 Data
We use a dataset of biomedical literature abstracts
from Duan et al(2009). The data contains a set of
documents for 21 ambiguous terms. We reserved
one of these terms (?MCP?) for setting parameters,
and ran our algorithms on the remaining keywords.
The ambiguous terms vary from acronyms (7 terms),
which are common and important in biomedical lit-
erature, to ambiguous biomedical terminology (3
terms), to terms like ?culture? and ?mole? that have
some biomedical senses and some senses that are
part of the general lexicon (11 terms). There were on
average 271 labeled documents per term; the small-
est number of documents for a term is 125, and
the largest is 503. For every ambiguous term, we
added on average 9625 (minimum of 1544, maxi-
mum of 15711) unlabeled documents to our collec-
tion by searching for the term on PubMed Central
and downloading additional PubMed abstracts.
6.2 Extracting Glosses
We measured the performance of GLOSSY?s gloss
extraction by comparing the extracted glosses with
definitions contained in the Unified Medical Lan-
guage System (UMLS) Metathesaurus. First, for
each ambiguous term, we looked up the set of ex-
act matches for that term in the Metathesaurus, and
downloaded definitions for all of the different senses
listed under that term. Wherever possible, we used
the MeSH definition of a sense; when that was un-
available, we used the definition from the NCI The-
saurus; and when both were unavailable, we used the
definition from the resource listed first. 34 senses
(40%) had no available definitions at all, but in all
cases, the Metathesaurus lists a short (usually 1-3
word) gloss of the sense, which we used instead.
We manually aligned extracted glosses with
UMLS senses in a way that maximizes the number
of matched senses for every ambiguous term. We
consider an extracted gloss to match a UMLS sense
when the extracted gloss unambiguously refers to
a single sense of the ambiguous term, and that
sense matches the definition in UMLS. Typically,
this means that the extracted features in the gloss
631
overlap content words in the UMLS definition (e.g.,
the extracted feature ?symptoms? for the ?common
cold? sense of the term ?cold?). In some cases, how-
ever, there was no strict overlap in content words
between the extracted gloss and the UMLS defini-
tion, but the sense of the extracted gloss still unam-
biguously matched a unique UMLS sense: e.g., for
the term ?transport,? the extracted gloss ?intracel-
lular transport? was matched with the UMLS sense
of ?Molecular Transport,? which the NCI Thesaurus
defines as, ?Any subcellular or molecular process in-
volved in translocation of a biological entity, such
as a macromolecule, from one site or compartment
to another.? In the end, such matchings were deter-
mined by hand. Table 1 shows extracted glosses and
UMLS definitions for the term ?mole.?
For each ambiguous term, we measure the num-
ber of extracted glosses, the number of UMLS
senses, and the number of matches between the
two. We report on the precision (number of matches
/ number of extracted glosses), recall (number of
matches / number of UMLS senses), and F1 score
(harmonic mean of precision and recall). Table 2
shows the average of the precision and recall num-
bers over all terms. Since these terms have different
numbers of senses, we can compute this average in
two different ways: a Macro average, in which each
term has equal weight in the average; and a Micro
average, in which each term?s weight in the average
is proportional to the number of senses (extracted
senses for the precision, and UMLS senses for the
recall). We report on both.
A strict matching between GLOSSY?s glosses and
UMLS senses is potentially unfair to GLOSSY in
several ways: GLOSSY may discover valid senses
that happen not to appear in UMLS; UMLS senses
may overlap one another, and so multiple UMLS
senses may match a single GLOSSY gloss; and the
two sets of senses may differ in granularity. For the
sake of repeatable experiments, in this evaluation we
make no attempt to change existing UMLS senses.
However, to highlight one particular strength of
the Gloss Extraction paradigm, we do consider a
separate evaluation that allows for new senses that
GLOSSY discovers, but do not appear in UMLS.
For instance, ?biliopancreatic diversion? and ?bipo-
lar disorder? are both valid senses for the acronym
?BPD.? GLOSSY discovers both, but UMLS does
not contain entries for either, so in our original eval-
uation both senses would count against GLOSSY?s
precision. To correct for this, our second evalua-
tion adds senses to the list of UMLS senses when-
ever GLOSSY discovers valid entries missing from
the Metathesaurus. The last five columns of Table 2
show our results under these conditions.
Despite the difficulty of the task, GLOSSY is able
to find glosses with 53% precision and 47% re-
call (Macro average, no discovered senses) using
only unlabeled corpora as input, and it is extract-
ing roughly the right number of senses for each am-
biguous term. In addition, GLOSSY is able to iden-
tify 7 valid senses missing from UMLS for the 20
terms in our evaluation. Including these senses in
the evaluation increases GLOSSY?s F1 by 6.2 points
Micro (4.7 Macro). We are quite encouraged by
the results, especially because they hold promise for
WSD. Note that in order to improve upon a WSD
baseline which tags all instances of a word as the
same sense, GLOSSY only needs to be able to sep-
arate one sense from the rest. GLOSSY is finding
between 1.85 and 2.2 correct glosses per term, more
than enough to help with WSD.
6.3 WSD with Extracted Glosses
While extracting glosses is an important application
in its own right, we also aim to show that this ex-
tracted knowledge is useful for an established ap-
plication: namely, word sense disambiguation. Our
next experiment compares the performance of our
WSD system with an established unsupervised al-
gorithm, and with a supervised technique ? support
vector machines (SVMs).
Using the same dataset as above, we trained
GLOSSY on the ambiguous term ?MCP?, and tested
it on the remaining ones. For comparison, we also
report the state-of-the-art results of Duan et al?s
(2009) SENSATIONAL system, and the results of a
BASELINE system that lumps all documents into
a single cluster. SENSATIONAL is a fast cluster-
ing system based on minimum spanning trees and
a pruning mechanism that eliminates noisy points
from consideration during clustering. Since SEN-
SATIONAL uses both ?MCP? and ?white? to train a
small set of parameters, we leave ?white? out of our
comparison as well. We measure accuracy by align-
ing each system?s clusters with the gold standard
clusters in such a way as to maximize the number
of elements that belong to aligned clusters. We use
an implementation of the MaxFlow algorithm to de-
termine this alignment. We then compute accuracy
632
GLOSSY UMLS
1. choriocarcinoma,
invasive, complete,
hydatidiform mole,
hydatidiform
1. Hydatidiform Mole ? Trophoblastic hyperplasia associated with normal gestation,
or molar pregnancy. . . . Hydatidiform moles or molar pregnancy may be catego-
rized as complete or partial based on their gross morphology, histopathology, and
karyotype.
2. grams per mole 2. Mole, unit of measurement ? A unit of amount of substance, one of the seven
base units of the International System of Units. It is the amount of substance that
contains as many elementary units as there are atoms in 0.012 kg of carbon-12.
3. mole fractions -
- 3. Nevus ? A circumscribed stable malformation of the skin . . . .
- 4. Talpidae ? Any of numerous burrowing mammals found in temperate regions . . .
Table 1: GLOSSY?s extracted glosses and UMLS dictionary entries for the example term ?mole?.
Without Discovered Senses With Discovered Senses
GLOSSY UMLS UMLS
Senses Senses Matches P R F1 Senses Matches P R F1
Macro Avg 4.35 4.25 1.85 53.1 47.1 49.9 4.6 2.2 60.6 49.7 54.6
Micro Avg N/A N/A N/A 42.5 43.5 43.0 N/A N/A 50.6 47.8 49.2
Table 2: GLOSSY can automatically discover glosses that match definitions in an online dictionary. ?Without
Discovered Senses? counts only the senses that are listed in the UMLS Metathesaurus; ?With Discovered Senses?
enhances the Metathesaurus with 7 new senses that GLOSSY has automatically discovered.
as the percentage of elements that belong to aligned
clusters. This metric is very similar to the so-called
?supervised? evaluation of Agirre et al (2006).
The first four columns of Table 3 show our results.
Clearly, both SENSATIONAL and GLOSSY outper-
form the BASELINE significantly, and traditionally
this is a difficult baseline for unsupervised WSD sys-
tems to beat. SENSATIONAL outperforms GLOSSY
by approximately 6%. There appear to be two rea-
sons for this. In other experiments, SENSATIONAL
has been shown to be competitive with supervised
systems, but only when the corpus consists mostly
of two, fairly well-balanced senses, as is true for
this particular dataset, where the two most common
senses always covered at least 70% of the examples
for every ambiguous term.
A more serious problem for GLOSSY is that the
unlabeled corpus that it extracts glosses from may
not match well with the labeled test data. If the rela-
tive frequency of senses in the unlabeled documents
does not match the relative frequency of senses in
the labeled test set, GLOSSY may not extract the
right set of glosses. Manual inspection of the ex-
tracted glosses shows that this is indeed a problem:
for example, the labeled data contains two senses of
the word ?mole?: a discolored area of skin (78%),
and a burrowing mammal (22%); our unlabeled data
contains both of these senses, but the additional
sense of ?mole? as a unit of measurement is by far
predominant. GLOSSY manages to extract glosses
for ?skin? and ?unit of measurement,? but misses out
on ?mammal? as a result of the skew in the data.
Note that this problem, though serious for our ex-
periments, is largely artificial from the point of view
of applications. In a typical usage of a WSD system,
there is a supply of data that the system needs to dis-
ambiguate, and accuracy is measured on a labeled
sample of this data. Here, we started from a sample
of labeled data, constructed a larger corpus that does
not necessarily match it, and then ran our algorithm.
To correct for the artificial bias in our experiment,
we ran a second test in which we manually labeled a
random sample of 100 documents for each ambigu-
ous term from the larger unlabeled corpus. We used
a subset of 14 of the 21 keywords in the original
dataset. As before, we compared our system against
SENSATIONAL and the most-frequent-sense BASE-
LINE. We also compare against an SVM system us-
ing 3-fold cross-validation. We use a linear kernel
SVM, with the same set of features that are available
633
Duan et al(2009) Data Sampled Data
Num. BASE- SENSE- Num. BASE- SENSE-
Keyword senses LINE GLOSSY ATIONAL senses LINE ATIONAL GLOSSY SVM
ANA 2 63.1 87.9 100 13 75 79 74 75.8
BPD 3 39.8 71.6 52.9 7 33 48 85 66.7
BSA 2 50.1 77.9 94.7 5 97 53 89 87.9
CML 2 55.0 99.2 89.5 4 81 75 84 75.8
MAS 2 50.0 100 100 35 46 90 67 66.7
VCR 2 79.2 79.2 64.0 8 72 32 72 75.8
cold 3 37.1 73.3 66.8 3 87 81 44 90.9
culture 2 52.0 67.1 81.7 3 74 39 62 66.7
discharge 2 66.3 82.4 95.1 5 57 41 84 54.5
fat 2 50.6 50.1 53.2 2 97 60 97 97.0
mole 2 78.3 71.3 95.8 7 78 47 57 84.8
pressure 2 52.1 69.8 86.4 5 47 60 65 75.8
single 2 50.0 59.7 99.5 4 53 63 37 45.4
white - - - - 7 32 33 58 51.5
fluid 2 64.3 83.5 99.6 - - - - -
glucose 2 50.5 64.5 50.5 - - - - -
inflammation 3 35.5 52.8 50.4 - - - - -
inhibition 2 50.4 50.4 54.2 - - - - -
nutrition 3 38.8 53.8 54.9 - - - - -
transport 2 50.6 41.1 56.8 - - - - -
AVERAGE 2.16 53.4 70.3 76.1 7.71 66.3 57.2 69.6 72.5
Diff from BL - 0.0 +16.9 +22.7 - 0.0 -9.1 +3.3 +6.2
Table 3: GLOSSY?s extracted glosses can be used to create an unsupervised WSD system that achieves an accu-
racy within 3% of a supervised system. Our WSD system outperforms our BASELINE system, widely recognized
as a difficult baseline for unsupervised WSD, by 16.9% and 3.3% on two different datasets.
to the SVM in the GLOSSY system. We run our un-
supervised systems on all of the unlabeled data, and
then intersect the resulting clusters with the docu-
ment set that we randomly sampled.
The last four columns of Table 3 show our results.
The sampled data set appears to be a significantly
harder test, since even the supervised SVM achieves
only a 6% gain over the BASELINE. The SEN-
SATIONAL system does significantly worse on this
data, where there is a wider variation in the distri-
bution of senses. The GLOSSY system outperforms
both the SENSATIONAL system and the BASELINE.
7 Conclusion and Future Work
Gloss Extraction is an important, and difficult task of
extracting definitions of words from unlabeled text.
The GLOSSY system succeeds at a more feasible re-
finement of this task, the Sense Indicator Extrac-
tion task. GLOSSY?s extractions have proven use-
ful as seed definitions in an unsupervised WSD task.
There is a great deal of room for future work in ex-
panding the ability of Gloss Extraction systems to
extract sense glosses that more closely match the
meanings of a word. An important first step in this
direction is to extract relations, rather than ngrams,
that make up the definition a word?s senses.
Acknowledgments
Presentation of this work was supported by the Insti-
tute of Education Sciences, U.S. Department of Ed-
ucation, through Grant R305A080157 to Carnegie
Mellon University. The opinions expressed are those
of the authors and do not necessarily represent the
views of the Institute or the U.S. Department of Edu-
cation. The authors thank the anonymous reviewers
for their helpful suggestions and comments.
634
References
Eneko Agirre and Aitor Soroa. 2007. Semeval 2007 task
02: Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval),
pages 7?12.
E. Agirre, O. Lopez de Lacalle, D. Martinez, and
A. Soroa. 2006. Evaluating and optimizing the param-
eters of an unsupervised graph-based WSD algorithm.
In Proceedings of the NAACL Textgraphs Workshop.
I. Androutsopoulos and D. Galanis. 2005. A practi-
cally unsupervised learning method to identify single-
snippet answers to definition questions on the web. In
Proceedings of HLT-EMNLP, pages 323?330.
Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambiguation.
In Empirical Methods in Natural Language Process-
ing.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines.
T. Chklovski and R. Mihalcea. 2002. Building a sense
tagged corpus with Open Mind Word Expert. In Pro-
ceedings of the Workshop on Word Sense Disambigua-
tion: Recent Successes and Future Directions.
H. Cui, M.K. Kan, and T.S. Chua. 2007. Soft pattern
matching models for definitional question answering.
ACM Trans. Information Systems, 25(2):1?30.
Mona Diab. 2004. Relieving the data acquisition bottle-
neck in word sense disambiguation. In Proceedings of
the ACL.
Weisi Duan, Min Song, and Alexander Yates. 2009. Fast
max-margin clustering for unsupervised word sense
disambiguation in biomedical texts. BMC Bioinfor-
matics, 10(S3)(S4).
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Bradford Books.
A. Fujii and T. Ishikawa. 2000. Utilizing the world wide
web as an encyclopedia: Extracting term descriptions
from semi-structured texts. In Proceedings of ACL,
pages 488?495.
M. Joshi, T. Pedersen, and R. Maclin. 2005. A compar-
ative study of support vector machines applied to the
supervised word sense disambiguation problem in the
medical domain. In Proceedings of the Second Indian
International Conference on Artificial Intelligence.
Anagha Kulkarni and Ted Pedersen. 2005. Name dis-
crimination and email clustering using unsupervised
clustering and labeling of similar contexts. In Pro-
ceedings of the Second Indian International Confer-
ence on Artificial Intelligence, pages 703?722.
Gondy Leroy and Thomas C. Rindflesch. 2004. Us-
ing symbolic knowledge in the umls to disambiguate
words in small datasets with a naive bayes classifier.
In MEDINFO.
M.E. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
SIGDOC Conference.
Hongfang Liu, Virginia Teller, and Carol Friedman.
2004. A multi-aspect comparison study of supervised
word sense disambiguation. Journal of the American
Medical Informatics Association, 11:320?331.
Rada Mihalcea. 2002. Bootstrapping large sense-tagged
corpora. In International Conference on Languages
Resources and Evaluations (LREC).
Rada Mihalcea. 2007. Using wikipedia for automatic
word sense disambiguation. In Proceedings of the
NAACL.
David Milne and Ian H. Witten. 2008. Learning to link
with wikipedia. In Proceedings of the 17th Conference
on Information and Knowledge Management (CIKM).
S. Mohammad and Ted Pedersen. 2004. Combining lex-
ical and syntactic features for supervised word sense
disambiguation. In Proceedings of CoNLL.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Procs. of ACM Conference on Knowl-
edge Discovery and Data Mining (KDD-02).
Marius Pasca. 2005. Finding instance names and alterna-
tive glosses on the web: WordNet reloaded. In Com-
putational Linguistics and Intelligent Text Processing,
pages 280?292. Springer Berlin / Heidelberg.
Ana-Maria Popescu, Alexander Yates, and Oren Etzioni.
2004. Class extraction from the world wide web. In
AAAI-04 ATEM Workshop, pages 65?70.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: English
lexical sample, srl and all words. In Proceedings of
the Fourth International Workshop on Semantic Eval-
uations (SemEval).
B.J. Schijvenaars, B. Mons, M. Weeber, M.J. Schuemie,
E.M. van Mulligen, H.M. Wain, and J.A. Kors. 2005.
Thesaurus-based disambiguation of gene symbols.
BMC Bioinformatics, 6.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
COLING/ACL.
M. Stevenson and Yorick Wilks. 2001. The interaction
of knowledge sources in word sense disambiguation.
Computational Linguistics, 27(3):321?349.
Paola Velardi, Roberto Navigli, and Pierluigi D?Amadio.
2008. Mining the web to create specialized glossaries.
IEEE Intelligent Systems, 23(5):18?25.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the ACL.
635
Proceedings of NAACL-HLT 2013, pages 579?584,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Overcoming the Memory Bottleneck in Distributed Training of
Latent Variable Models of Text
Yi Yang
Northwestern University
Evanston, IL
yiyang@eecs.northwestern.edu
Alexander Yates
Temple University
Philadelphia, PA
yates@temple.edu
Doug Downey
Northwestern University
Evanston, IL
ddowney@eecs.northwestern.edu
Abstract
Large unsupervised latent variable models
(LVMs) of text, such as Latent Dirichlet Al-
location models or Hidden Markov Models
(HMMs), are constructed using parallel train-
ing algorithms on computational clusters. The
memory required to hold LVM parameters
forms a bottleneck in training more powerful
models. In this paper, we show how the mem-
ory required for parallel LVM training can
be reduced by partitioning the training corpus
to minimize the number of unique words on
any computational node. We present a greedy
document partitioning technique for the task.
For large corpora, our approach reduces mem-
ory consumption by over 50%, and trains the
same models up to three times faster, when
compared with existing approaches for paral-
lel LVM training.
1 Introduction
Unsupervised latent variable models (LVMs) of text
are utilized extensively in natural language process-
ing (Griffiths and Steyvers, 2004; Ritter et al, 2010;
Downey et al, 2007; Huang and Yates, 2009; Li and
McCallum, 2005). LVM techniques include Latent
Dirichlet Allocation (LDA) (Blei et al, 2003), Hid-
den Markov Models (HMMs) (Rabiner, 1989), and
Probabilistic Latent Semantic Analysis (Hofmann,
1999), among others.
LVMs become more predictive as they are trained
on more text. However, training LVMs on mas-
sive corpora introduces computational challenges, in
terms of both time and space complexity. The time
complexity of LVM training has been addressed
through parallel training algorithms (Wolfe et al,
2008; Chu et al, 2006; Das et al, 2007; Newman
et al, 2009; Ahmed et al, 2012; Asuncion et al,
2011), which reduce LVM training time through the
use of large computational clusters.
However, the memory cost for training LVMs re-
mains a bottleneck. While LVM training makes se-
quential scans of the corpus (which can be stored on
disk), it requires consistent random access to model
parameters. Thus, the model parameters must be
stored in memory on each node. Because LVMs in-
clude a multinomial distribution over words for each
latent variable value, the model parameter space in-
creases with the number of latent variable values
times the vocabulary size. For large models (i.e.,
with many latent variable values) and large cor-
pora (with large vocabularies), the memory required
for training can exceed the limits of the commod-
ity servers comprising modern computational clus-
ters. Because model accuracy tends to increase with
both corpus size and model size (Ahuja and Downey,
2010; Huang and Yates, 2010), training accurate lan-
guage models requires that we overcome the mem-
ory bottleneck.
We present a simple technique for mitigating the
memory bottleneck in parallel LVM training. Ex-
isting parallelization schemes begin by partitioning
the training corpus arbitrarily across computational
nodes. In this paper, we show how to reduce mem-
ory footprint by instead partitioning the corpus to
minimize the number of unique words on each node
(and thereby minimize the number of parameters the
node must store). Because corpus partitioning is
a pre-processing step in parallel LVM training, our
579
technique can be applied to reduce the memory foot-
print of essentially any existing LVM or training ap-
proach. The accuracy of LVM training for a fixed
model size and corpus remains unchanged, but in-
telligent corpus partitioning allows us to train larger
and typically more accurate models using the same
memory capacity.
While the general minimization problem we en-
counter is NP-hard, we develop greedy approxima-
tions that work well. In experiments with both
HMM and LDA models, we show that our technique
offers large advantages over existing approaches in
terms of both memory footprint and execution time.
On a large corpus using 50 nodes in parallel, our best
partitioning method can reduce the memory required
per node to less than 1/10th that when training with-
out corpus partitioning, and to half that of a random
partitioning. Further, our approach reduces the train-
ing time of an existing parallel HMM codebase by
3x. Our work includes the release of our partitioning
codebase, and an associated codebase for the paral-
lel training of HMMs.1
2 Problem Formulation
In a distributed LVM system, a training corpus D =
{d1, d2, . . . , dN} of documents is distributed across
T computational nodes. We first formalize the mem-
ory footprint on each node nt, where t = {1, ..., T}.
Let Dt ? D denote the document collection on node
nt, and Vt be the number of word types (i.e., the
number of unique words) in Dt. Let K be the num-
ber of latent variable values in the LVM.
With these quantities, we can express how many
parameters must be held in memory on each com-
putational node for training LVMs in a distributed
environment. In practice, the LVM parameter space
is dominated by an observation model: a condi-
tional distribution over words given the latent vari-
able value. Thus, the observation model includes
K(Vt? 1) parameters. Different LVMs include var-
ious other parameters to specify the complete model.
For example, a first-order HMM includes additional
distributions for the initial latent variable and latent
variable transitions, for a total of K(Vt ? 1) + K2
parameters. LDA, on the other hand, includes just a
1https://code.google.com/p/
corpus-partition/
single multinomial over the latent variables, making
a total of K(Vt ? 1) + K ? 1 parameters.
The LVM parameters comprise almost all of the
memory footprint for LVM training. Further, as the
examples above illustrate, the number of parame-
ters on each node tends to vary almost linearly with
Vt (in practice, Vt is typically larger than K by an
order of magnitude or more). Thus, in this paper
we attempt to minimize memory footprint by lim-
iting Vt on each computational node. We assume
the typical case in a distributed environment where
nodes are homogeneous, and thus our goal is to par-
tition the corpus such that the maximum vocabulary
size Vmax = maxTt=1Vt on any single node is mini-
mized. We define this task formally as follows.
Definition CORPUSPART : Given a corpus of
N documents D = {d1, d2, . . . , dN}, and T nodes,
partition D into T subsets D1, D2, . . . , DT , such
that Vmax is minimized.
For illustration, consider the following small ex-
ample. Let corpus C contain three short docu-
ments {c1=?I live in Chicago?, c2=?I am studying
physics?, c3=?Chicago is a city in Illinois?}, and
consider partitioning C into 2 non-empty subsets,
i.e., T = 2. There are a total of three possibilities:
? {{c1, c2}, {c3}}. Vmax = 7
? {{c1, c3}, {c2}}. Vmax = 8
? {{c2, c3}, {c1}}. Vmax = 10
The decision problem version of
CORPUSPART is NP-Complete, by a re-
duction from independent task scheduling (Zhu and
Ibarra, 1999). In this paper, we develop greedy
algorithms for the task that are effective in practice.
We note that CORPUSPART has a submodu-
lar problem structure, where greedy algorithms are
often effective. Specifically, let |S| denote the vo-
cabulary size of a set of documents S, and let S? ?
S. Then for any document c the following inequality
holds.
|S? ? c| ? |S?| ? |S ? c| ? |S|
That is, adding a document c to the subset S? in-
creases vocabulary size at least as much as adding
c to S; the vocabulary size function is submodular.
The CORPUSPART task thus seeks a partition
of the data that minimizes the maximum of a set of
submodular functions. While formal approximation
580
guarantees exist for similar problems, to our knowl-
edge none apply directly in our case. For example,
(Krause et al, 2007) considers maximizing the mini-
mum over a set of monotonic submodular functions,
which is the opposite of our problem. The distinct
task of minimizing a single submodular function has
been investigated in e.g. (Iwata et al, 2001).
It is important to emphasize that data partition-
ing is a pre-processing step, after which we can em-
ploy precisely the same Expectation-Maximization
(EM), sampling, or variational parameter learning
techniques as utilized in previous work. In fact,
for popular learning techniques including EM for
HMMs (Rabiner, 1989) and variational EM for LDA
(Wolfe et al, 2008), it can be shown that the param-
eter updates are independent of how the corpus is
partitioned. Thus, for those approaches our parti-
tioning is guaranteed to produce the same models as
any other partitioning method; i.e., model accuracy
is unchanged.
Lastly, we note that we target synchronized LVM
training, in which all nodes must finish each train-
ing iteration before any node can proceed to the
next iteration. Thus, we desire balanced partitions to
help ensure iterations have similar durations across
nodes. We achieve this in practice by constraining
each node to hold at most 3% more than Z/T to-
kens, where Z is the corpus size in tokens.
3 Corpus Partitioning Methods
Our high-level greedy partitioning framework is
given in Algorithm 1. The algorithm requires an-
swering two key questions: How do we select which
document to allocate next? And, given a document,
on which node should it be placed? We present al-
ternative approaches to each question below.
Algorithm 1 Greedy Partitioning Framework
INPUT: {D, T}
OUTPUT: {D1, . . . , DT }
Objective: Minimize Vmax
Initialize each subset Dt = ? for T nodes
repeat
document selection:Select document d from D
node selection: Select node nt, and add d to Dt
Remove d from D
until all documents are allocated
A baseline partitioning method commonly used
in practice simply distributes documents across
nodes randomly. As our experiments show, this
baseline approach can be improved significantly.
In the following, set operations are interpreted as
applying to the set of unique words in a document.
For example, |d?Dt| indicates the number of unique
word types in node nt after document d is added to
its document collection Dt.
3.1 Document Selection
For document selection, previous work (Zhu and
Ibarra, 1999) proposed a heuristic DISSIMILARITY
method that selects the document d that is least sim-
ilar to any of the node document collections Dt,
where the similarity of d and Dt is calculated as:
Sim(d,DT ) = |d ? Dt|. The intuition behind the
heuristic is that dissimilar documents are more likely
to impact future node selection decisions. Assigning
the dissimilar documents earlier helps ensure that
more greedy node selections are informed by these
impactful assignments.
However, DISSIMILARITY has a prohibitive time
complexity of O(TN2), because we must compare
T nodes to an order of N documents for a total of
N iterations. To scale to large corpora, we propose
a novel BATCH DISSIMILARITY method. In BATCH
DISSIMILARITY, we select the top L most dissim-
ilar documents in each iteration, instead of just the
most dissimilar. Importantly, L is altered dynami-
cally: we begin with L = 1, and then increase L by
one for iteration i+1 iff using a batch size of L+1 in
iteration i would not have altered the algorithm?s ul-
timate selections (that is, if the most dissimilar doc-
ument in iteration i + 1 is in fact the L + 1st most
dissimilar in iteration i). In the ideal case where L
is incremented each iteration, BATCH DISSIMILAR
will have a reduced time complexity of O(TN3/2).
Our experiments revealed two key findings re-
garding document selection. First, BATCH DISSIM-
ILARITY provides a memory reduction within 0.1%
of that of DISSIMILARITY (on small corpora where
running DISSIMILARITY is tractable), but partitions
an estimated 2,600 times faster on our largest eval-
uation corpus. Second, we found that document se-
lection has relatively minor impact on memory foot-
print, providing a roughly 5% incremental benefit
over random document selection. Thus, although
581
we utilize BATCH DISSIMILARITY in the final sys-
tem we evaluate, simple random document selection
may be preferable in some practical settings.
3.2 Node Selection
Given a selected document d, the MINIMUM
method proposed in previous work selects node nt
having the minimum number of word types after al-
location of d to nt (Zhu and Ibarra, 1999). That is,
MINIMUM minimizes |d ?Dt|. Here, we introduce
an alternative node selection method JACCARD that
selects node nt maximizing the Jaccard index, de-
fined here as |d ?Dt|/|d ?Dt|.
Our experiments showed that our JACCARD node
selection method outperforms the MINIMUM selec-
tion method. In fact, for the largest corpora used
in our experiments, JACCARD offered an 12.9%
larger reduction in Vmax than MINIMUM. Our
proposed system, referred to as BJAC, utilizes
our best-performing strategies for document selec-
tion (BATCH DISSIMILARITY) and node selection
(JACCARD).
4 Evaluation of Partitioning Methods
We evaluate our partitioning method against the
baseline and Z&I, the best performing scalable
method from previous work, which uses random
document selection and MINIMUM node selection
(Zhu and Ibarra, 1999). We evaluate on three cor-
pora (Table 1): the Brown corpus of newswire text
(Kucera and Francis, 1967), the Reuters Corpus Vol-
ume1 (RCV1) (Lewis et al, 2004), and a larger Web-
Sent corpus of sentences gathered from the Web
(Downey et al, 2007).
Corpus N V Z
Brown 57339 56058 1161183
RCV1 804414 288062 99702278
Web-Sent 2747282 214588 58666983
Table 1: Characteristics of the three corpora. N = #
of documents, V = # of word types, Z = # of tokens.
We treat each sentence as a document in the Brown
and Web-Sent corpora.
Table 2 shows how the maximum word type size
Vmax varies for each method and corpus, for T = 50
nodes. BJAC significantly decreases Vmax over the
Corpus baseline Z&I BJAC
Brown 6368 5714 4369
RCV1 49344 32136 24923
Web-Sent 72626 45989 34754
Table 2: Maximum word type size Vmax for each
partitioning method, for each corpus. For the larger
corpora, BJAC reduces Vmax by over 50% compared
to the baseline, and by 23% compared to Z&I.
random partitioning baseline typically employed in
practice. Furthermore, the advantage of BJAC over
the baseline is maintained as more computational
nodes are utilized, as illustrated in Figure 1. BJac
reduces Vmax by a larger factor over the baseline as
more computational nodes are employed.
  0
  20,000
  40,000
  60,000
  80,000
  100,000
  120,000
  140,000
10 20 30 40 50 60 70 80 90 100
nu
m
ber
 of 
wo
rd t
ype
s
 number of nodes
Vmax by baselineVmax by BJac
Figure 1: Effects of partitioning as the number of
computational nodes increases (Web-Sent corpus).
With 100 nodes, BJac?s Vmax is half that of the base-
line, and 1/10th of the full corpus vocabulary size.
5 Evaluation in Parallel LVM Systems
We now turn to an evaluation of our corpus parti-
tioning within parallel LVM training systems.
Table 3 shows the memory footprint required for
HMM and LDA training for three different partition-
ing methods. We compare BJAC with the random
partitioning baseline, Zhu?s method, and with all-
words, the straightforward approach of simply stor-
ing parameters for the entire corpus vocabulary on
every node (Ahuja and Downey, 2010; Asuncion et
al., 2011). All-words has the same memory footprint
as when training on a single node.
For large corpora, BJAC reduces memory size
per node by approximately a factor of two over the
random baseline, and by a factor of 8-11 over all-
582
LVM Corpus all-words baseline BJAC
HMM
Brown 435.3 56.2 40.9
RCV1 2205.4 384.1 197.8
Web-Sent 1644.8 561.7 269.7
LDA
Brown 427.7 48.6 33.3
RCV1 2197.7 376.5 190.1
Web-Sent 1637.2 554.1 262.1
Table 3: Memory footprint of computational nodes
in megabytes(MB), using 50 computational nodes.
Both models utilize 1000 latent variable values.
words. The results demonstrate that in addition to
the well-known savings in computation time offered
by parallel LVM training, distributed computation
also significantly reduces the memory footprint on
each node. In fact, for the RCV1 corpus, BJAC re-
duces memory footprint to less than 1/10th that of
training with all words on each computational node.
We next evaluate the execution time for an itera-
tion of model training. Here, we use a parallel im-
plementation of HMMs, and measure iteration time
for training on the Web-sent corpus with 50 hidden
states as the number of computational nodes varies.
We compare against the random baseline and against
the all-words approach utilized in an existing paral-
lel HMM codebase (Ahuja and Downey, 2010). The
results are shown in Table 4. Moving beyond the all-
words method to exploit corpus partitioning reduces
training iteration time, by a factor of two to three.
However, differences in partitioning methods have
only small effects in iteration time: BJAC has essen-
tially the same iteration time as the random baseline
in this experiment.
It is also important to consider the additional time
required to execute the partitioning methods them-
selves. However, in practice this additional time
is negligible. For example, BJAC can partition the
Web-sent corpus in 368 seconds, using a single com-
putational node. By contrast, training a 200-state
HMM on the same corpus requires over a hundred
CPU-days. Thus, BJAC?s time to partition has a neg-
ligible impact on total training time.
6 Related Work
The CORPUSPART task has some similarities
to the graph partitioning task investigated in other
T all-words baseline BJAC
25 4510 1295 1289
50 2248 740 735
100 1104 365 364
200 394 196 192
Table 4: Average iteration time(sec) for training an
HMM with 50 hidden states on Web-Sent. Partition-
ing with BJAC outperforms all-words, which stores
parameters for all word types on each node.
parallelization research (Hendrickson and Kolda,
2000). However, our LVM training task differs sig-
nificantly from those in which graph partitioning is
typically employed. Specifically, graph partitioning
tends to be used for scientific computing applica-
tions where communication is the bottleneck. The
graph algorithms focus on creating balanced parti-
tions that minimize the cut edge weight, because
edge weights represent communication costs to be
minimized. By contrast, in our LVM training task,
memory consumption is the bottleneck and commu-
nication costs are less significant.
Zhu & Ibarra (1999) present theoretical results
and propose techniques for the general partitioning
task we address. In contrast to that work, we fo-
cus on the case where the data to be partitioned is a
large corpus of text. In this setting, we show that our
heuristics partition faster and provide smaller mem-
ory footprint than those of (Zhu and Ibarra, 1999).
7 Conclusion
We presented a general corpus partitioning tech-
nique which can be exploited in LVM training to re-
duce memory footprint and training time. We eval-
uated the partitioning method?s performance, and
showed that for large corpora, our approach reduces
memory consumption by over 50% and learns mod-
els up to three times faster when compared with ex-
isting implementations for parallel LVM training.
Acknowledgments
This work was supported in part by NSF Grants
IIS-101675 and IIS-1065397, and DARPA contract
D11AP00268.
583
References
Amr Ahmed, Moahmed Aly, Joseph Gonzalez, Shra-
van Narayanamurthy, and Alexander J. Smola. 2012.
Scalable inference in latent variable models. In Pro-
ceedings of the fifth ACM international conference on
Web search and data mining, WSDM ?12, pages 123?
132, New York, NY, USA. ACM.
Arun Ahuja and Doug Downey. 2010. Improved extrac-
tion assessment through better language models. In
Human Language Technologies: Annual Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL HLT).
Arthur U. Asuncion, Padhraic Smyth, and Max Welling.
2011. Asynchronous distributed estimation of topic
models for document analysis. Statistical Methodol-
ogy, 8(1):3 ? 17. Advances in Data Mining and Statis-
tical Learning.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Cheng T. Chu, Sang K. Kim, Yi A. Lin, Yuanyuan Yu,
Gary R. Bradski, Andrew Y. Ng, and Kunle Olukotun.
2006. Map-Reduce for machine learning on multicore.
In Bernhard Scho?lkopf, John C. Platt, and Thomas
Hoffman, editors, NIPS, pages 281?288. MIT Press.
Abhinandan S. Das, Mayur Datar, Ashutosh Garg, and
Shyam Rajaram. 2007. Google news personaliza-
tion: scalable online collaborative filtering. In Pro-
ceedings of the 16th international conference on World
Wide Web, WWW ?07, pages 271?280, New York, NY,
USA. ACM.
D. Downey, S. Schoenmackers, and O. Etzioni. 2007.
Sparse information extraction: Unsupervised language
models to the rescue. In Proc. of ACL.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101(Suppl. 1):5228?5235, April.
Bruce Hendrickson and Tamara G Kolda. 2000. Graph
partitioning models for parallel computing. Parallel
computing, 26(12):1519?1534.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual inter-
national ACM SIGIR conference on Research and de-
velopment in information retrieval, SIGIR ?99, pages
50?57, New York, NY, USA. ACM.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised se-
quence labeling. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Fei Huang and Alexander Yates. 2010. Exploring
representation-learning approaches to domain adapta-
tion. In Proceedings of the ACL 2010 Workshop on
Domain Adaptation for Natural Language Processing
(DANLP).
Satoru Iwata, Lisa Fleischer, and Satoru Fujishige. 2001.
A combinatorial strongly polynomial algorithm for
minimizing submodular functions. J. ACM, 48:761?
777.
Andreas Krause, H. Brendan Mcmahan, Google Inc, Car-
los Guestrin, and Anupam Gupta. 2007. Selecting
observations against adversarial objectives. Technical
report, In NIPS, 2007a.
H. Kucera and W. N. Francis. 1967. Computational
analysis of present-day American English. Brown
University Press, Providence, RI.
David D. Lewis, Yiming Yang, Tony G. Rose, Fan Li,
G. Dietterich, and Fan Li. 2004. Rcv1: A new bench-
mark collection for text categorization research. Jour-
nal of Machine Learning Research, 5:361?397.
Wei Li and Andrew McCallum. 2005. Semi-supervised
sequence modeling with syntactic topic models. In
Proceedings of the 20th national conference on Artifi-
cial intelligence - Volume 2, AAAI?05, pages 813?818.
AAAI Press.
David Newman, Arthur Asuncion, Padhraic Smyth, and
Max Welling. 2009. Distributed algorithms for
topic models. Journal of Machine Learning Research,
10:1801?1828.
L. R. Rabiner. 1989. A tutorial on Hidden Markov
Models and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2):257?286.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, HLT ?10, pages 172?180,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Jason Wolfe, Aria Haghighi, and Dan Klein. 2008. Fully
distributed EM for very large datasets. In Proceed-
ings of the 25th international conference on Machine
learning, ICML ?08, pages 1184?1191, New York,
NY, USA. ACM.
Huican Zhu and Oscar H. Ibarra. 1999. On some ap-
proximation algorithms for the set partition problem.
In Proceedings of the 15th Triennial Conf. of Int. Fed-
eration of Operations Research Society.
584
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 968?978,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Open-Domain Semantic Role Labeling by Modeling Word Spans
Fei Huang
Temple University
1805 N. Broad St.
Wachman Hall 318
fei.huang@temple.edu
Alexander Yates
Temple University
1805 N. Broad St.
Wachman Hall 303A
yates@temple.edu
Abstract
Most supervised language processing sys-
tems show a significant drop-off in per-
formance when they are tested on text
that comes from a domain significantly
different from the domain of the training
data. Semantic role labeling techniques
are typically trained on newswire text, and
in tests their performance on fiction is
as much as 19% worse than their perfor-
mance on newswire text. We investigate
techniques for building open-domain se-
mantic role labeling systems that approach
the ideal of a train-once, use-anywhere
system. We leverage recently-developed
techniques for learning representations of
text using latent-variable language mod-
els, and extend these techniques to ones
that provide the kinds of features that are
useful for semantic role labeling. In exper-
iments, our novel system reduces error by
16% relative to the previous state of the art
on out-of-domain text.
1 Introduction
In recent semantic role labeling (SRL) competi-
tions such as the shared tasks of CoNLL 2005 and
CoNLL 2008, supervised SRL systems have been
trained on newswire text, and then tested on both
an in-domain test set (Wall Street Journal text)
and an out-of-domain test set (fiction). All sys-
tems tested on these datasets to date have exhib-
ited a significant drop-off in performance on the
out-of-domain tests, often performing 15% worse
or more on the fiction test sets. Yet the baseline
from CoNLL 2005 suggests that the fiction texts
are actually easier than the newswire texts. Such
observations expose a weakness of current super-
vised natural language processing (NLP) technol-
ogy for SRL: systems learn to identify semantic
roles for the subset of language contained in the
training data, but are not yet good at generalizing
to language that has not been seen before.
We aim to build an open-domain supervised
SRL system; that is, one whose performance on
out-of-domain tests approaches the same level of
performance as that of state-of-the-art systems on
in-domain tests. Importantly, an open-domain sys-
tem must not use any new labeled data beyond
what is included in the original training text when
running on a new domain. This allows the sys-
tem to be ported to any new domain without any
manual effort. In particular, it ought to apply to
arbitrary Web documents, which are drawn from a
huge variety of domains.
Recent theoretical and empirical evidence sug-
gests that the fault for poor performance on out-of-
domain tests lies with the representations, or sets
of features, traditionally used in supervised NLP.
Building on recent efforts in domain adaptation,
we develop unsupervised techniques for learning
new representations of text. Using latent-variable
language models, we learn representations of texts
that provide novel kinds of features to our su-
pervised learning algorithms. Similar represen-
tations have proven useful in domain-adaptation
for part-of-speech tagging and phrase chunking
(Huang and Yates, 2009). We demonstrate how
to learn representations that are effective for SRL.
Experiments on out-of-domain test sets show that
our learned representations can dramatically im-
prove out-of-domain performance, and narrow the
gap between in-domain and out-of-domain perfor-
mance by half.
The next section provides background informa-
tion on learning representations for NLP tasks us-
ing latent-variable language models. Section 3
presents our experimental setup for testing open-
domain SRL. Sections 4, 5, 6 describe our SRL
system: first, how we identify predicates in open-
domain text, then how our baseline technique
968
identifies and classifies arguments, and finally how
we learn representations for improving argument
identification and classification on out-of-domain
text. Section 7 presents previous work, and Sec-
tion 8 concludes and outlines directions for future
work.
2 Open-Domain Representations Using
Latent-Variable Language Models
Let X be an instance set for a learning problem;
for SRL, this is the set of all (sentence,predicate)
pairs. Let Y be the space of possible labels for an
instance, and let f : X ? Y be the target func-
tion to be learned. A representation is a func-
tion R: X ? Z , for some suitable feature space
Z (such as Rd). A domain is defined as a dis-
tribution D over the instance set X . An open-
domain system observes a set of training examples
(R(x), f(x)), where instances x ? X are drawn
from a source domain, to learn a hypothesis for
classifying examples drawn from a separate target
domain.
Previous work by Ben-David et al (2007; 2009)
uses Vapnik-Chervonenkis (VC) theory to prove
theoretical bounds on an open-domain learning
machine?s performance. Their analysis shows that
the choice of representation is crucial to open-
domain learning. As is customary in VC the-
ory, a good choice of representation must allow
a learning machine to achieve low error rates dur-
ing training. Just as important, however, is that
the representation must simultaneously make the
source and target domains look as similar to one
another as possible.
For open-domain SRL, then, the traditional rep-
resentations are problematic. Typical represen-
tations in SRL and NLP use features of the lo-
cal context to produce a representation. For in-
stance, one dimension of a traditional represen-
tation R might be +1 if the instance contains the
word ?bank? as the head of a noun-phrase chunk
that occurs before the predicate in the sentence,
and 0 otherwise. Although many previous studies
have shown that these features allow learning sys-
tems to achieve impressively low error rates dur-
ing training, they also make texts from different
domains look very dissimilar. For instance, a fea-
ture based on the word ?bank? or ?CEO? may be
common in a domain of newswire text, but scarce
or nonexistent in, say, biomedical literature.
In our recent work (Huang and Yates, 2009) we
show how to build systems that learn new rep-
resentations for open-domain NLP using latent-
variable language models like Hidden Markov
Models (HMMs). An HMM is a generative prob-
abilistic model that generates each word xi in the
corpus conditioned on a latent variable Yi. Each
Yi in the model takes on integral values from 1 to
K, and each one is generated by the latent variable
for the preceding word, Yi?1. The distribution for
a corpus x = (x1, . . . , xN ) and a set of state vec-
tors s = (s1, . . . , sN ) is given by:
P (x, s) =
?
i
P (xi|si)P (si|si?1)
Using Expectation-Maximization (Dempster et
al., 1977), it is possible to estimate the distribu-
tions for P (xi|si) and P (si|si?1) from unlabeled
data. The Viterbi algorithm (Rabiner, 1989) can
then be used to produce the optimal sequence of
latent states si for a given instance x. The output
of this process is an integer (ranging from 1 to K)
for every word xi in the corpus. We use the inte-
ger value of si as a new feature for every xi in the
sentence.
In POS-tagging and chunking experiments,
these learned representations have proven to meet
both of Ben-David et al?s criteria for open-domain
representations: first, they are useful in making
predictions on the training text because the HMM
latent states categorize tokens according to dis-
tributional similarity. And second, it would be
difficult to tell two domains apart based on their
HMM labels, since the same HMM state can gen-
erate similar words from a variety of domains.
In what follows, we adapt these representation-
learning concepts to open-domain SRL.
3 Experimental Setup
We test our open-domain semantic role labeling
system using data from the CoNLL 2005 shared
task (Carreras and Ma`rquez, 2005). We use the
standard training set, consisting of sections 02-21
of the Wall Street Journal (WSJ) portion of the
Penn Treebank, labeled with PropBank (Palmer
et al, 2005) annotations for predicates and argu-
ments. We perform our tests on the Brown corpus
(Kucera and Francis, 1967) test data from CoNLL
2005, consisting of 3 sections (ck01-ck03) of
propbanked Brown corpus data. This test set con-
sists of 426 sentences containing 7,159 tokens,
804 propositions, and 2,177 arguments. While the
969
training data contains newswire text, the test sen-
tences are drawn from the domain of ?general fic-
tion,? and contain an entirely different style (or
styles) of English. The data also includes a sec-
ond test set of in-domain text (section 23 of the
Treebank), which we refer to as the WSJ test set
and use as a reference point.
Every sentence in the dataset is automatically
annotated with a number of NLP pipeline systems,
including part-of-speech (POS) tags, phrase chunk
labels (Carreras and Ma`rquez, 2003), named-
entity tags, and full parse information by multiple
parsers. These pipeline systems are important for
generating features for SRL, and one key reason
for the poor performance of SRL systems on the
Brown corpus is that the pipeline systems them-
selves perform worse. The Charniak parser, for
instance, drops from an F1 of 88.25 on the WSJ
test to a F1 of 80.84 on the Brown corpus. For
the chunker and POS tagger, the drop-offs are less
severe: 94.89 to 91.73, and 97.36 to 94.73.
Toutanova et al (2008) currently have the best-
performing SRL system on the Brown corpus test
set with an F1 score of 68.81 (80.8 for the WSJ
test). They use a discriminative reranking ap-
proach to jointly predict the best set of argu-
ment boundaries and the best set of argument la-
bels for a predicate. Like the best systems from
the CoNLL 2005 shared task (Punyakanok et al,
2008; Pradhan et al, 2005), they also use features
from multiple parses to remain robust in the face
of parser error. Owing to the established difficulty
of the Brown test set and the different domains of
the Brown test and WSJ training data, this dataset
makes for an excellent testbed for open-domain
semantic role labeling.
4 Predicate Identification
In order to perform true open-domain SRL, we
must first consider a task which is not formally
part of the CoNLL shared task: the task of iden-
tifying predicates in a given sentence. While this
task is almost trivial in the WSJ test set, where
all but two out of over 5000 predicates can be ob-
served in the training data, it is significantly more
difficult in an open-domain setting. In the Brown
test set, 6.1% of the predicates do not appear in the
training data, and 11.8% of the predicates appear
at most twice in the training data (c.f. 1.5% of the
WSJ test predicates that appear at most twice in
training). In addition, many words which appear
Baseline HMM
Freq P R F1 P R F1
0 89.1 80.4 84.5 93.5 84.3 88.7
0-2 87.4 84.7 86.0 91.6 88.8 90.2
all 87.8 92.5 90.1 90.8 96.3 93.5
Table 1: Using HMM features in predicate iden-
tification reduces error in out-of-domain tests by
34.3% overall, and by 27.1% for OOV predicates.
?Freq? refers to frequency in the training data.
There were 831 predicates in total; 51 never ap-
peared in training and 98 appeared at most twice.
as predicates in training may not be predicates in
the test set. In an open-domain setting, therefore,
we cannot rely solely on a catalog of predicates
from the training data.
To address the task of open-domain predicate
identification, we construct a Conditional Random
Field (CRF) (Lafferty et al, 2001) model with tar-
get labels of B-Pred, I-Pred, and O-Pred (for the
beginning, interior, and outside of a predicate).
We use an open source CRF software package to
implement our CRF models.1 We use words, POS
tags, chunk labels, and the predicate label at the
preceding and following nodes as features for our
Baseline system. To learn an open-domain repre-
sentation, we then trained an 80 state HMM on the
unlabeled texts of the training and Brown test data,
and used the Viterbi optimum states of each word
as categorical features.
The results of our Baseline and HMM systems
appear in Table 1. For predicates that never or
rarely appear in training, the HMM features in-
crease F1 by 4.2, and they increase the overall F1
of the system by 3.5 to 93.5, which approaches
the F1 of 94.7 that the Baseline system achieves
on the in-domain WSJ test set. Based on these re-
sults, we were satisfied that our system could find
predicates in open-domain text. In all subsequent
experiments, we fall back on the standard evalua-
tion in which it is assumed that the boundaries of
the predicate are given. This allows us to compare
with previous work.
5 Semantic Role Labeling with
HMM-based Representations
Following standard practice, we divide the SRL
task into two parts: argument identification and
1Available from http://sourceforge.net/projects/crf/
970
argument classification. We treat both sub-tasks
as sequence-labeling problems. During argument
identification, the system must label each token
with labels that indicate either the beginning or in-
terior of an argument (B-Arg or I-Arg), or a label
that indicates the token is not part of an argument
(O-Arg). During argument classification, the sys-
tem labels each token that is part of an argument
with a class label, such as Arg0 or ArgM. Follow-
ing argument classification, multi-word arguments
may have different classification labels for each to-
ken. We post-process the labels by changing them
to match the label of the first token. We use CRFs
as our models for both tasks (Cohn and Blunsom,
2005).
Most previous approaches to SRL have relied
heavily on parsers, and especially constituency
parsers. Indeed, when SRL systems use gold stan-
dard parses, they tend to perform extremely well
(Toutanova et al, 2008). However, as several pre-
vious studies have noted (Gildea, 2001; Pradhan
et al, 2007), using parsers can cause problems for
open-domain SRL. The parsers themselves may
not port well to new domains, or the features they
generate for SRL may not be stable across do-
mains, and therefore may cause sparse data prob-
lems on new domains. Our first step is therefore
to build an SRL system that relies on partial pars-
ing, as was done in CoNLL 2004 (Carreras and
Ma`rquez, 2004). We then gradually add in less-
sparse alternatives for the syntactic features that
previous systems derive from parse trees.
During argument identification we use the fea-
tures below to predict the label Ai for token wi:
? words: wi, wi?1, and wi+1
? parts of speech (POS): POS tags ti, ti?1,
and ti+1
? chunk labels: (e.g., B-NP, I-VP, or O)
chunk tags ci, ci?1, and ci+1
? combinations: citi, tiwi, citiwi
? NE: the named entity type ni of wi
? position: whether the word occurs before
or after the predicate
? distance: the number of intervening
tokens between wi and the target predicate
? POS before, after predicate: the POS tag
of the tokens immediately preceding and
following the predicate
? Chunk before, after predicate: the chunk
type of the tokens immediately preceding
and following the predicate
? Transition: for prediction node Ai, we use
Ai?1and Ai+1 as features
For argument classification, we add the features
below to those listed above:
? arg ID: the labels Ai produced by arg.
identification (B-Arg, I-Arg, or O)
? combination: predicate + first argument
word, predicate+ last argument word,
predicate + first argument POS, predicate
+ last argument POS
? head distance: the number of tokens
between the first token of the argument
phrase and the target predicate
? neighbors: the words immediately before
and after the argument.
We refer to the CRF model with these features as
our Baseline SRL system; in what follows we ex-
tend the Baseline model with more sophisticated
features.
5.1 Incorporating HMM-based
Representations
As a first step towards an open-domain representa-
tion, we use an HMM with 80 latent state values,
trained on the unlabeled text of the training and
test sets, to produce Viterbi-optimal state values
si for every token in the corpus. We then add the
following features to our CRFs for both argument
identification and classification:
? HMM states: HMM state values si, si?1,
and si+1
? HMM states before, after predicate: the
state value of the tokens immediately
preceding and following the predicate
We call the resulting model our Baseline+HMM
system.
5.2 Path Features
Despite all of the features above, the SRL sys-
tem has very little information to help it determine
the syntactic relationship between a target predi-
cate and a potential argument. For instance, these
baseline features provide only crude distance in-
formation to distinguish between multiple argu-
ments that follow a predicate, and they make it
difficult to correctly identify clause arguments or
arguments that appear far from the predicate. Our
system needs features that can help distinguish
between different syntactic relationships, without
being overly sensitive to the domain.
As a step in this direction, we introduce path
features: features for the sequence of tokens be-
971
System P R F1
Baseline 63.9 59.7 61.7
Baseline+HMM 68.5 62.7 65.5
Baseline+HMM+Paths 70.0 65.6 67.7
Toutanova et al (2008) NR NR 68.8
Table 2: Na??ve path features improve our base-
line, but not enough to match the state-of-the-art.
Toutanova et al do not report (NR) separate val-
ues for precision and recall on this dataset. Dif-
ferences in both precision and recall between the
baseline and the other systems are statistically sig-
nificant at p < 0.01 using the two-tailed Fisher?s
exact test.
tween a predicate and a potential argument. In
standard SRL systems, these path features usually
consist of a sequence of constituent parse nodes
representing the shortest path through the parse
tree between a word and the predicate (Gildea and
Jurafsky, 2002). We substitute paths that do not
depend on parse trees. We use four types of paths:
word paths, POS paths, chunk paths, and HMM
state paths. Given an input sentence labeled with
POS tags, and chunks, we construct path features
for a token wi by concatenating words (or tags or
chunk labels) between wi and the predicate. For
example, in the sentence ?The HIV infection rate
is expected to peak in 2010,? the word path be-
tween ?rate? and predicate ?peak? would be ?is
expected to?, and the POS path would be ?VBZ
VBD TO.?
Since word, POS, and chunk paths are all sub-
ject to data sparsity for arguments that are far from
the predicate, we build less-sparse path features by
using paths of HMM states. If we use a reason-
able number of HMM states, each category label
is much more common in the training data than
the average word, and paths containing the HMM
states should be much less sparse than word paths,
and even chunk paths. In our experiments, we use
80-state HMMs.
We call the result of adding path features to
our feature set the Baseline+HMM+Paths sys-
tem((BL). Table 2 shows the performance of our
three baseline systems. In this open-domain SRL
experiment, path features improve over the Base-
line?s F1 by 6 points, and by 2.2 points over
Baseline+HMM, although the improvement is not
enough to match the state-of-the-art system by
Toutanova et al
Y1 Y2 Y6
The is expected to peak in 2010
Y3 Y4 Y5 Y7 Y8
HIV infection rate
Figure 1: The Span-HMM over the sentence. It
shows the span of length 3.
6 Representations for Word Spans
Despite partial success in improving our baseline
SRL system with path features, these features still
suffer from data sparsity ? many paths in the
test set are never or very rarely observed during
training, so the CRF model has little or no data
points from which to estimate accurate parameters
for these features. In response, we introduce la-
tent variable models of word spans, or sequences
of words. As with the HMM models above, the
latent states for word spans can be thought of as
probabilistic categories for the spans. And like the
HMM models, we can turn the word span models
into representations by using the state value for a
span as a feature in our supervised SRL system.
Unlike path features, the features from our models
of word spans consist of a single latent state value
rather than a concatenation of state values, and as
a consequence they tend to be much less sparse in
the training data.
6.1 Span-HMM Representations
We build our latent-variable models of word spans
using variations of Hidden Markov Models, which
we call Span-HMMs. Figure 1 shows a graphi-
cal model of a Span-HMM. Each Span-HMM be-
haves just like a regular HMM, except that it in-
cludes one node, called a span node, that can gen-
erate an entire span rather than a single word. For
instance, in the Span-HMM of Figure 1, node y5 is
a span node that generates a span of length 3: ?is
expected to.?
Span-HMMs can be used to provide a single
categorical value for any span of a sentence us-
ing the usual Viterbi algorithm for HMMs. That
is, at test time, we generate a Span-HMM feature
for word wj by constructing a Span-HMM that has
a span node for the sequence of words between wj
and the predicate. We determine the Viterbi opti-
mal state of this span node, and use that state as the
value of the new feature. In our example in Figure
1, the value of span node y5 is used as a feature for
972
the token ?rate?, since y5 generates the sequence
of words between ?rate? and the predicate ?peak.?
Notice that by using Span-HMMs to provide
these features, we have condensed all paths in our
data into a small number of categorical values.
Whereas there are a huge number of variations to
the spans themselves, we can constrain the number
of categories for the Span-HMM states to a rea-
sonable number such that each category is likely to
appear often in the training data. The value of each
Span-HMM state then represents a cluster of spans
with similar delimiting words; some clusters will
correlate with spans between predicates and argu-
ments, and others with spans that do not connect
predicates and arguments. As a result, Span-HMM
features are not sparse, and they correlate with the
target function, making them useful in learning an
SRL model.
6.2 Parameter Estimation
We use a variant of the Baum-Welch algorithm to
train our Span-HMMs on unlabeled text. In order
for this to work, we need to provide Baum-Welch
with a modified view of the data so that span nodes
can generate multiple consecutive words in a sen-
tence. First, we take every sentence S in our train-
ing data and generate the set Spans(S) of all valid
spans in the sentence. For efficiency?s sake, we use
only spans of length less than 15; approximately
95% of the arguments in our dataset were within
15 words of the predicate, so even with this re-
striction we are able to supply features for nearly
all valid arguments. The second step of our train-
ing procedure is to create a separate data point for
each span of S. For each span t ? Spans(S), we
construct a Span-HMM with a regular node gen-
erating each element of S, except that a span node
generates all of t. Thus, our training data contains
many different copies of each sentence S, with a
different Span-HMM generating each copy.
Intuitively, running Baum-Welch over this data
means that a span node with state k will be likely
to generate two spans t1 and t2 if t1 and t2 tend to
appear in similar contexts. That is, they should
appear between words that are also likely to be
generated by the same latent state. Thus, certain
values of k will tend to appear for spans between
predicates and arguments, and others will tend
to appear between predicates and non-arguments.
This makes the value k informative for both argu-
ment identification and argument classification.
6.3 Memory Considerations
Memory usage is a major issue for our Span-
HMM models. We represent emission distribu-
tions as multinomials over discrete observations.
Since there are millions of different spans in our
data, a straightforward implementation would re-
quire millions of parameters for each latent state
of the Span-HMM.
We use two related techniques to get around this
problem. In both cases, we use a second HMM
model, which we call the base HMM to distin-
guish from our Span-HMM, to back-off from the
explicit word sequence. We use the largest num-
ber of states for HMMs that can be fit into mem-
ory. Let S be a sentence, and let s? be the sequence
of optimal latent state values for S produced by
our base HMM. Our first approach trains the Span-
HMM on Spans(s?), rather than Spans(S). If
we use a small enough number of latent states in
the base HMM (in experiments, we use 10 latent
states), we drastically reduce the number of differ-
ent spans in the data set, and therefore the num-
ber of parameters required for our model. We call
this representation Span-HMM-Base10. As with
our other HMM-based models, we use the largest
number of latent states that will allow the result-
ing model to fit in our machine?s memory ? our
previous experiments on representations for part-
of-speech tagging suggest that more latent states
are usually better.
While our first technique solves the memory is-
sue, it also loses some of the power of our orig-
inal Span-HMM model by using a very coarse-
grained base HMM clustering of the text into 10
categories. Our second approach trains a separate
Span-HMM model for spans of different lengths.
Since we need only one model in memory at a
time, this allows each one to consume more mem-
ory. We therefore use base HMM models with
more latent states (up to 20) to annotate our sen-
tences, and then train on the resulting Spans(s?)
as before. With this technique, we produce fea-
tures that are combinations of the state value for
span nodes and the length of the span, in order
to indicate which of our Span-HMM models the
state value came from. We call this representation
Span-HMM-BaseByLength.
6.4 Combining Multiple Span-HMMs
So far, our Span-HMM models produce one new
feature for every token during argument identifi-
973
System P R F1
Baseline+HMM+Paths 70.0 65.6 67.7
Toutanova et al NR NR 68.8
Span-HMM-Base10 74.5 69.3 71.8
Span-HMM-BaseByLength 76.3 70.2 73.1
Multi-Span-HMM 77.0 70.9 73.8
Table 3: Span-HMM features significantly im-
prove over state-of-the-art results in out-of-
domain SRL. Differences in both precision and re-
call between the baseline and the Span-HMM sys-
tems are statistically significant at p < 0.01 using
the two-tailed Fisher?s exact test.
cation and classification. While these new fea-
tures may be very helpful, ideally we would like
our learned representations to produce multiple
useful features for the CRF model, so that the
CRF can combine the signals from each feature
to learn a sophisticated model. Towards this goal,
we train N independent versions of our Span-
HMM-BaseByLength models, each with a ran-
dom initialization for the Baum-Welch algorithm.
Since Baum-Welch is a hill-climbing algorithm,
it should find local, but not necessarily global,
optima for the parameters of each Span-HMM-
BaseByLength model. When we decode each of
the models on training and test texts, we will ob-
tain N different sequences of latent states, one
for each locally-optimized model. Thus we obtain
N different, independent sources of features. We
call the CRF model with these N Span-HMM fea-
tures the Multi-Span-HMM model(MSH); in ex-
periments we use N = 5.
6.5 Results and Discussion
Results for the Span-HMM models on the CoNLL
2005 Brown corpus are shown in Table 3. All three
versions of the Span-HMM outperform Toutanova
et al?s system on the Brown corpus, with the
Multi-Span-HMM gaining 5 points in F1. The
Multi-Span-HMM model improves over the Base-
line+HMM+Paths model by 7 points in precision,
and 5.3 points in recall. Among the Span-HMM
models, the use of more states in the Span-HMM-
BaseByLength model evidently outweighed the
cost of splitting the model into separate versions
for different length spans. Using multiple in-
dependent copies of the Span-HMMs provides a
small (0.7) gain in precision and recall. Dif-
ferences among the different Span-HMM models
System WSJ Brown Diff
Multi-Span-HMM 79.2 73.8 5.4
Toutanova et al (2008) 80.8 68.8 12.0
Pradhan et al (2005) 78.6 68.4 10.2
Punyakanok et al (2008) 79.4 67.8 11.6
Table 4: Multi-Span-HMM has a much smaller
drop-off in F1 than comparable systems on out-
of-domain test data vs in-domain test data.
were not statistically significant, except that the
difference in precision between the Multi-Span-
HMM and the Span-HMM-Base10 is significant
at p < .1.
Table 4 shows the performance drop-off for top
SRL systems when applied to WSJ test data and
Brown corpus test data. The Multi-Span-HMM
model performs near the state-of-the-art on the
WSJ test set, and its F1 on out-of-domain data
drops only about half as much as comparable sys-
tems. Note that several of the techniques used
by other systems, such as using features from k-
best parses or jointly modeling the dependencies
among arguments, are complementary to our tech-
niques, and may boost the performance of our sys-
tem further.
Table 5 breaks our results down by argument
type. Most of our improvement over the Baseline
system comes from the core arguments A0 and
A1, but also from a few adjunct types like AM-
TMP and AM-LOC. Figure 2 shows that when the
argument is close to the predicate, both systems
perform well, but as the distance from the predi-
cate grows, our Multi-Span-HMM system is bet-
ter able to identify and classify arguments than the
Baseline+HMM+Paths system.
Table 6 provides results for argument identifi-
cation and classification separately. As Pradhan et
al.previously showed (Pradhan et al, 2007), SRL
systems tend to have an easier time with porting
argument identification to new domains, but are
less strong at argument classification on new do-
mains. Our baseline system decreases in F-score
from 81.5 to 78.9 for argument identification, but
suffers a much larger 8% drop in argument classi-
fication. The Multi-Span-HMM model improves
over the Baseline in both tasks and on both test
sets, but the largest improvement (6%) is in argu-
ment classification on the Brown test set.
To help explain the success of the Span-HMM
techniques, we measured the sparsity of our path
974
Overall A0 A1 A2 A3 A4 ADV DIR DIS LOC MNR MOD NEG PNC TMP R-A0 R-A1
Num 2177 566 676 147 12 15 143 53 22 85 110 91 50 17 112 25 21
BL 67.7 76.2 70.6 64.8 59.0 71.2 52.7 54.8 71.9 67.5 58.3 90.9 90.0 50.0 76.5 76.5 71.3
MSH 73.8 82.5 73.6 63.9 60.3 73.3 50.8 52.9 70.0 70.3 52.7 94.2 92.9 51.6 81.6 84.4 75.7
Table 5: SRL results (F1) on the Brown test corpus broken down by role type. BL is the Base-
line+HMM+Paths model, MSH is the Multi-Span-HMM model. Column 8 to 16 are all adjuncts (AM-).
We omit roles with ten or fewer examples.
50
55
60
65
70
75
80
85
90
F1
 sc
or
e
Words between predicate and argument 
MSH
BL
Figure 2: The Multi-Span-HMM (MSH) model
is better able to identify and classify arguments
that are far from the predicate than the Base-
line+HMM+Paths (BL) model.
Test Id.F1 Accuracy
BL WSJ 81.5 93.7
Brown 78.9 85.8
MSH WSJ 83.9 94.4
Brown 80.3 91.9
Table 6: Baseline (BL) and Multi-Span-HMM
(MSH) performance on argument identification
(Id.F1) and argument classification.
and Span-HMM features. Figure 3 shows the per-
centage of feature values in the Brown corpus that
appear more than twice, exactly twice, or exactly
once in the training data. While word path fea-
tures can be highly valuable when there is train-
ing data available for them, only about 11% of the
word paths in the Brown test set alo appeared at
all in the training data. POS and chunk paths fared
a bit better (22% and 33% respectively), but even
then nearly 70% of all feature values had no avail-
able training data. HMM and Span-HMM-Base10
paths achieved far better success in this respect.
Importantly, the improvement is mostly due to fea-
tures that are seen often in training, rather than fea-
tures that were seen just once or twice. Thus Span-
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Fr
ac
tio
n 
of
 Fe
at
ur
e 
Va
lu
es
 in
 
Br
ow
n 
Co
rp
us
Occurs 1x 
in WSJ
Occurs 2x 
in WSJ
Occurs 3x 
or more in 
WSJ
Fr
ac
tio
n 
of
 Fe
at
ur
e 
Va
lu
es
 in
 
Br
ow
n 
Co
rp
us
Figure 3: HMM path and Span-HMM features are
far more likely to appear often in training data than
the word, POS, and chunk path features. Over
70% of Span-HMM-Base10 features in the Brown
corpus appear at least three times during training;
in contrast, fewer than 33% of chunk path features
in the Brown corpus appear at all during training.
HMMs derive their power as representations for
open-domain SRL from the fact that they provide
features that are mostly the same across domains;
80% of the features of our Span-HMM-Base10 in
the Brown corpus were observed at least once in
the training data.
Table 7 shows examples of spans that were
clustered into the same Span-HMM state, along
with word to either side. All four examples
are cases where the Span-HMM-Base10 model
correctly tagged the following argument, but the
Baseline+HMM+Paths model did not. We can see
that the paths of these four examples are com-
pletely different, but the words surrounding them
are very similar. The emission from a span node
are very sparse, so the Span-HMM has unsurpris-
ingly learned to cluster spans according to the
HMM states that precede and follow the span
node. This is by design, as this kind of distri-
butional clustering is helpful for identifying and
classifying arguments. One potentially interesting
975
Predicate Span B-Arg
picked the things up from
passed through the barbed wire at
come down from Sundays to
sat over his second rock in
Table 7: Example spans labeled with the same
Span-HMM state. The examples are taken from
sentences where the Span-HMM-Base10 model
correctly identified the argument on the right, but
the Baseline+HMM+Paths model did not.
question for future work is whether a less sparse
model of the spans themselves, such as a Na??ve
Bayes model for the span node, would yield a bet-
ter clustering for producing features for semantic
role labeling.
7 Previous Work
Deschact and Moens (2009) use a latent-variable
language model to provide features for an SRL
system, and they show on CoNLL 2008 data that
they can significantly improve performance when
little labeled training data is available. They do
not report on out-of-domain tests. They use HMM
language models trained on unlabeled text, much
like we use in our baseline systems, but they do not
consider models of word spans, which we found to
be most beneficial. Downey et al (2007b) also in-
corporate HMM-based representations into a sys-
tem for the related task of Web information extrac-
tion, and are able to show that the system improves
performance on rare terms.
Fu?rstenau and Lapata (2009b; 2009a) use semi-
supervised techniques to automatically annotate
data for previously unseen predicates with seman-
tic role information. This task differs from ours
in that it focuses on previously unseen predicates,
which may or may not be part of text from a new
domain. Their techniques also result in relatively
lower performance (F1 between 15 and 25), al-
though their tests are on a more difficult and very
different corpus. Weston et al (2008) use deep
learning techniques based on semi-supervised em-
beddings to improve an SRL system, though their
tests are on in-domain data. Unsupervised SRL
systems (Swier and Stevenson, 2004; Grenager
and Manning, 2006; Abend et al, 2009) can natu-
rally be ported to new domains with little trouble,
but their accuracy thus far falls short of state-of-
the-art supervised and semi-supervised systems.
The disparity in performance between in-
domain and out-of-domain tests is by no means
restricted to SRL. Past research in a variety of
NLP tasks has shown that parsers (Gildea, 2001),
chunkers (Huang and Yates, 2009), part-of-speech
taggers (Blitzer et al, 2006), named-entity tag-
gers (Downey et al, 2007a), and word sense dis-
ambiguation systems (Escudero et al, 2000) all
suffer from a similar drop-off in performance on
out-of-domain tests. Numerous domain adapta-
tion techniques have been developed to address
this problem, including self-training (McClosky et
al., 2006) and instance weighting (Bacchiani et al,
2006) for parser adaptation and structural corre-
spondence learning for POS tagging (Blitzer et al,
2006). Of these techniques, structural correspon-
dence learning is closest to our technique in that it
is a form of representation learning, but it does not
learn features for word spans. None of these tech-
niques have been successfully applied to SRL.
8 Conclusion and Future Work
We have presented novel representation-learning
techniques for building an open-domain SRL sys-
tem. By incorporating learned features from
HMMs and Span-HMMs trained on unlabeled
text, our SRL system is able to correctly iden-
tify predicates in out-of-domain text with an F1
of 93.5, and it can identify and classify argu-
ments to predicates with an F1 of 73.8, out-
performing comparable state-of-the-art systems.
Our successes so far on out-of-domain tests bring
hope that supervised NLP systems may eventually
achieve the ideal where they no longer need new
manually-labeled training data for every new do-
main. There are several potential avenues for fur-
ther progress towards this goal, including the de-
velopment of more portable SRL pipeline systems,
and especially parsers. Developing techniques that
can incrementally adapt to new domains without
the computational expense of retraining the CRF
model every time would help make open-domain
SRL more practical.
Acknowledgments
We wish to thank the anonymous reviewers for
their helpful comments and suggestions.
976
References
Omri Abend, Roi Reichart, and Ari Rappoport. 2009.
Unsupervised argument identification for semantic
role labeling. In Proceedings of the ACL.
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41?68.
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2007. Analysis of representations
for domain adaptation. In Advances in Neural In-
formation Processing Systems 20, Cambridge, MA.
MIT Press.
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jenn Wortman.
2009. A theory of learning from different domains.
Machine Learning, (to appear).
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
Xavier Carreras and Llu??s Ma`rquez. 2003. Phrase
recognition by filtering and ranking with percep-
trons. In Proceedings of RANLP-2003.
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduc-
tion to the CoNLL-2004 shared task: Semantic role
labeling. In Proceedings of the Conference on Nat-
ural Language Learning (CoNLL).
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling. In Proceedings of the Conference on Nat-
ural Language Learning (CoNLL).
Trevor Cohn and Phil Blunsom. 2005. Semantic role
labelling with tree conditional random fields. In
Proceedings of CoNLL.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Likelihood from incomplete data via the EM algo-
rithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1?38.
Koen Deschacht and Marie-Francine Moens. 2009.
Semi-supervised semantic role labeling using the la-
tent words language model. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
D. Downey, M. Broadhead, and O. Etzioni. 2007a. Lo-
cating complex named entities in web text. In Procs.
of the 20th International Joint Conference on Artifi-
cial Intelligence (IJCAI 2007).
Doug Downey, Stefan Schoenmackers, and Oren Et-
zioni. 2007b. Sparse information extraction: Unsu-
pervised language models to the rescue. In ACL.
G. Escudero, L. Ma?rquez, and G. Rigau. 2000. An
empirical study of the domain dependence of su-
pervised word sense disambiguation systems. In
EMNLP/VLC.
Hagen Fu?rstenau and Mirella Lapata. 2009a. Graph
alignment for semi-supervised semantic role label-
ing. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 11?20.
Hagen Fu?rstenau and Mirella Lapata. 2009b. Semi-
supervised semantic role labeling. In Proceedings
of the 12th Conference of the European Chapter of
the ACL, pages 220?228.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Daniel Gildea. 2001. Corpus Variation and Parser Per-
formance. In Conference on Empirical Methods in
Natural Language Processing.
Trond Grenager and Christopher D Manning. 2006.
Unsupervised discovery of a statistical verb lexi-
con. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised
sequence labeling. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.
H. Kucera and W.N. Francis. 1967. Computational
Analysis of Present-Day American English. Brown
University Press.
J. Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data.
In Proceedings of the International Conference on
Machine Learning.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Reranking and self-training for parser
adaptation. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the ACL, pages 337?344.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: A corpus annotated with se-
mantic roles. Computational Linguistics Journal,
31(1).
Sameer Pradhan, Kadri Hacioglu, Wayne Ward,
James H. Martin, and Daniel Jurafsky. 2005. Se-
mantic role chunking combining complementary
syntactic views. In Proc. of the Annual Confer-
ence on Computational Natural Language Learning
(CoNLL).
Sameer Pradhan, Wayne Ward, and James H. Martin.
2007. Towards robust semantic role labeling. In
Proceedings of NAACL-HLT, pages 556?563.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257?287.
977
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?
285.
Robert S. Swier and Suzanne Stevenson. 2004. Unsu-
pervised semantic role labelling. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 95?102.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for se-
mantic role labeling. Computational Linguistics,
34(2):161?191.
Jason Weston, Frederic Ratle, and Ronan Collobert.
2008. Deep learning via semi-supervised embed-
ding. In Proceedings of the 25th International Con-
ference on Machine Learning.
978
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 329?334,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Types of Common-Sense Knowledge
Needed for Recognizing Textual Entailment
Peter LoBue and Alexander Yates
Temple University
Broad St. and Montgomery Ave.
Philadelphia, PA 19130
{peter.lobue,yates}@temple.edu
Abstract
Understanding language requires both linguis-
tic knowledge and knowledge about how the
world works, also known as common-sense
knowledge. We attempt to characterize the
kinds of common-sense knowledge most often
involved in recognizing textual entailments.
We identify 20 categories of common-sense
knowledge that are prevalent in textual entail-
ment, many of which have received scarce at-
tention from researchers building collections
of knowledge.
1 Introduction
It is generally accepted that knowledge about how
the world works, or common-sense knowledge, is
vital for natural language understanding. There
is, however, much less agreement or understanding
about how to define common-sense knowledge, and
what its components are (Feldman, 2002). Existing
large-scale knowledge repositories, like Cyc (Guha
and Lenat, 1990), OpenMind (Stork, 1999), and
Freebase1, have steadily gathered together impres-
sive collections of common-sense knowledge, but
no one yet believes that this job is done. Other da-
tabases focus on exhaustively cataloging a specific
kind of knowledge ? e.g., synonymy and hyper-
nymy in WordNet (Fellbaum, 1998). Likewise, most
knowledge extraction systems focus on extracting
one specific kind of knowledge from text, often fac-
tual relationships (Banko et al, 2007; Suchanek et
al., 2007; Wu and Weld, 2007), although other spe-
cialized extraction techniques exist as well.
1http://www.freebase.com/
If we continue to build knowledge collections fo-
cused on specific types, will we collect a sufficient
store of common sense knowledge for understand-
ing language? What kinds of knowledge might lie
outside the collections that the community has fo-
cused on building? We have undertaken an empir-
ical study of a natural language understanding task
in order to help answer these questions. We focus
on the Recognizing Textual Entailment (RTE) task
(Dagan et al, 2006), which is the task of recogniz-
ing whether the meaning of one text, called the Hy-
pothesis (H), can be inferred from another, called
the Text (T). With the help of five annotators, we
have investigated the RTE-5 corpus to determine the
types of knowledge involved in human judgments of
RTE. We found 20 distinct categories of common-
sense knowledge that featured prominently in RTE,
besides linguistic knowledge, hyponymy, and syn-
onymy. Inter-annotator agreement statistics indicate
that these categories are well-defined. Many of the
categories fall outside of the realm of all but the most
general knowledge bases, like Cyc, and differ from
the standard relational knowledge that most auto-
mated knowledge extraction techniques try to find.
The next section outlines the methodology of our
empirical investigation. Section 3 presents the cate-
gories of world knowledge that we found were most
prominent in the data. Section 4 discusses empirical
results of our survey.
2 Methodology
We follow the methodology outlined in Sammons et
al. (2010), but unlike theirs and other previous stud-
ies (Clark et al, 2007), we concentrate on the world
329
#56 - ENTAILMENT
T: (CNN) Nadya Suleman, the Southern Cali-
fornia woman who gave birth to octuplets in Jan-
uary, [...] She now has four of the octuplets at
home, along with her six other children.
1) ?octuplets? are 8 children (definitional)
2) 8 + 6 = 14 children (arithmetic)
H: Nadya Suleman has 14 children.
Figure 1: An example RTE label, Text, a condensed
?proof? (with knowledge categories for the back-
ground knowledge) and Hypothesis.
knowledge rather than linguistic knowledge required
for RTE. First, we manually selected a set of RTE
data that could not be solved using linguistic knowl-
edge and WordNet alne. We then sketched step-
by-step inferences needed to show ENTAILMENT
or CONTRADICTION of the hypothesis. We iden-
tified prominent categories of world knowledge in-
volved in these inferences, and asked five annotators
to label the knowledge with the different categories.
We judge the well-definedness of the categories by
inter-annotator agreement, and their relative impor-
tance according to frequency in the data.
To select an appropriate subset of the RTE data,
we discarded RTE pairs labeled as UNKNOWN.
We also discarded RTE pairs with ENTAILMENT
and CONTRADICTION labels, if the decision re-
lies mostly or entirely on a combination of linguistic
knowledge, coreference decisions, synonymy, and
hypernymy. These phenomena are well-known to
be important to language understanding and RTE
(Mirkin et al, 2009; Roth and Sammons, 2007).
Many synonymy and hypernymy databases already
exist, and although coreference decisions may them-
selves depend on world knowledge, it is difficult to
separate the contribution of world knowledge from
the contribution of linguistic cues for coreference.
Some sample phenomena that we explicitly chose
to disregard include: knowledge of syntactic vari-
ations, verb tenses, apposition, and abbreviations.
From the 600 T and H pairs in RTE-5, we selected
108 that did not depend only on these phenomena.
For each of the 108 pairs in our data, we created
proofs, or a step-by-step sketch of the inferences that
lead to a decision about entailment of the hypothesis.
Figure 1 shows a sample RTE pair and (condensed)
proof. Each line in the proof indicates either a new
piece of background knowledge brought to bear, or
a modus ponens inference from the information in
the text or previous lines of the proof. This labor-
intensive process was conducted by one author over
more than three months. Note that the proofs may
not be the only way of reasoning from the text to an
entailment decision about the hypothesis, and that
alternative proofs might require different kinds of
common-sense knowledge. This caveat should be
kept in mind when interpreting the results, but we
believe that by aggregating over many proofs, we
can counter this effect.
We created 20 categories to classify the 221 di-
verse statements of world knowledge in our proofs.
These categories are described in the next section.2
In some cases, categories overlap (e.g., ?Canberra is
part of Australia? could be in the Geography cate-
gory or the part of category). In cases where we
foresaw the overlaps, we manually specified which
category should take precedence; in the above exam-
ple, we gave precedence to the Geography category,
so that statements of this kind would all be included
under Geography. This approach has the drawback
of biasing somewhat the frequencies in our data set
towards the categories that take precedence. How-
ever, this simplification significantly reduces the an-
notation effort of our survey participants, who al-
ready face a complicated set of decisions.
We evaluate our categorization to determine how
well-defined and understandable the categories are.
We conducted a survey of five undergraduate stu-
dents, who were all native English speakers but oth-
erwise unfamiliar with NLP. The 20 categories were
explained using fabricated examples (not part of the
survey data). Annotators kept these fabricated ex-
amples as references during the survey. Each anno-
tator labeled each of the pieces of world knowledge
from the proofs using one of the 20 categories. From
this data we calculate Fleiss?s ? for inter-annotator
agreement3 in order to measure how well-defined
the categories are. We compute ? once over all ques-
2The RTE pairs, proofs, and category judgments from our
study are available at
http://www.cis.temple.edu/?yates/data/rte-study-data.zip
3Fleiss?s ? handles more than two annotators, unlike the
more familiar Cohen?s ?.
330
tions and all categories. Separately, we also compute
? once for each category C, by treating all annota-
tions for categories C ? 6= C as the same.
3 Categories of Knowledge
By manual inspection, we arrived at the following
20 prominent categories of world knowledge in our
subset of the RTE-5 data. For each category, we give
a brief definition and example, along with the ID of
an RTE pair whose proof includes the example. Our
categories can be loosely organized into form-based
categories and content-based categories. Note that,
as with most common-sense knowledge, our exam-
ples are intended as rules that are usually or typically
true, rather than categorically or universally true.
3.1 Form-based Categories
The following categories are defined by how the
knowledge can be described in a representation lan-
guage, such as logic.
1. Cause and Effect: Statements in this category re-
quire that a predicate p holds true after an event or
action A.
#542: Once a person is welcomed into an organiza-
tion, they belong to that organization.
2. Preconditions: For a given action or event A at
time t, a precondition p is a predicate that must hold
true of the world before time t, in order for A to have
taken place.
#372: To become a naturalized citizen of a place,
one must not have been born there.
3. Simultaneous Conditions: Knowledge in this cat-
egory indicates that a predicate p must hold true at
the same time as an event or second predicate p?.
#240: When a person is an employee of an organi-
zation, that organization pays his or her salary.
4. Argument Types: Knowledge in this category
specifies the types or selectional preferences for ar-
guments to a relationship.
#311: The type of thing that adopts children is the
type person.
5. Prominent Relationship: Texts often specify that
there exists some relationship between two entities,
without specifying which relationship. Knowledge
in this category specifies which relationship is most
likely, given the types of the entities involved.
#42: If a painter is related to a painting somehow
(e.g., ?da Vinci?s Mona Lisa?), the painter most
likely painted the painting.
6. Definition: Any explanation of a word or phrase.
#163: A ?seat? is an object which holds one person.
7. Functionality: This category lists relation-
ships R which are functional; i.e., ?x,y,y?R(x, y) ?
R(x, y?) ? y = y?.
#493: fatherOf is functional ? a person can have
only one father.
8. Mutual Exclusivity: Related to functionality, mu-
tual exclusivity knowledge indicates types of things
that do not participate in the same relationship.
#229: Government and media sectors usually do not
employ the same person at the same time.
9. Transitivity: If we know that R is transitive, and
that R(a, b) and R(b, c) are true, we can infer that
R(a, c) is true.
#499: The supports relation is transitive. Thus, be-
cause Putin supports the United Russia party, and
the United Russia party supports Medvedev, we can
infer that Putin supports Medvedev.
3.2 Content-based Categories
The following categories are defined by the content,
topic, or domain of the knowledge in them.
10. Arithmetic: This includes addition and subtrac-
tion, as well as comparisons and rounding.
#609: 115 passengers + 6 crew = 121 people
11. Geography: This includes knowledge such as
?Australia is a place,? ?Sydney is in Australia,? and
?Canberra is the capital of Australia.?
12. Public Entities: This category is for well-known
properties of highly-recognizable named-entities.
#142: Berlusconi is prime minister of Italy.
13. Cultural/Situational: This category includes
knowledge of or shared by a particular culture.
#207: A ?half-hour drive? is ?near.?
14. is member of: Statements of this category indi-
cate that an entity belongs to a larger organization.
#374: A minister is part of the government.
15. has parts: This category expresses what compo-
nents an object or situation is comprised of.
#463: Forests have trees.
16. Support/Opposition: This includes knowledge
of the kinds of actions or relationships toward X that
indicate positive or negative feeling toward X .
#357: P founds X ? P supports X
331
17. Accountability: This includes any knowledge
that is helpful for determining who or what is re-
sponsible for an action or event.
#158: A nation?s military is responsible for that na-
tion?s bombings.
18. Synecdoche: Synecdoche is knowledge that a
person or thing can represent or speak for an organi-
zation or structure he or she is a part of.
#410: The president of Russia represents Russia.
3.3 Miscellaneous Categories
19. Probabilistic Dependency: Multiple phrases in
the text may contribute to the hypothesis being more
or less likely to be true, although each phrase on its
own might not be sufficient to support the hypothe-
sis. Knowledge in this category indicates that these
separate pieces of evidence can combine in a proba-
bilistic, noisy-or fashion to increase confidence in a
particular inference.
#437: Stocks on the ?Nikkei 225? exchange and
Toyota?s stock both fell, which independently sug-
gest that Japan?s economy might be struggling,
but in combination they are stronger evidence that
Japan?s economy is floundering.
20. Omniscience: Certain RTE judgments are only
possible if we assume that the text includes all in-
formation pertinent to the story, so that we may dis-
credit statements that were not mentioned.
#208: T states that ?Fitzpatrick pleaded guilty to
fraud and making a false report.? H, which is marked
as a CONTRADICTION, states that ?Fitzpatrick is
accused of robbery.? In order to prove the falsehood
of H, we had to assume that no charges were made
other than the ones described in T.
4 Results and Discussion
Our headline result is that the above twenty cat-
egories overall are well-defined, with a Fleiss?s ?
score of 0.678, and that they cover the vast majority
of the world knowledge used in our proofs. This has
important implications, as it suggests that concen-
trating on collecting these kinds of world knowledge
will make a large difference to RTE, and hopefully to
language understanding in general. Naturally, more
studies of this issue are warranted for validation.
Many of the categories ? has parts, member of,
geography, cause and effect, public entities, and
Category Occurrences ?
Functionality 19.2 (8.7%) 0.663
Definitions 17.2 (7.8%) 0.633
Preconditions 15.8 (7.1%) 0.775
Cause and Effect 10.8 (4.9%) 0.591
Prominent Relationship 8.4 (3.8%) 0.145
Argument Types 6.8 (3.1%) 0.180
Simultaneous Conditions 6.2 (2.8%) 0.203
Mutual Exclusivity 6 (2.7%) 0.640
Transitivity 3 (1.4%) 0.459
Geography 36.4 (16.5%) 0.927
Support/Opposition 14.6 (6.6%) 0.684
Arithmetic 13.4 (6.1%) 0.968
is member of 11.6 (5.2%) 0.663
Synecdoche 9.8 (4.4%) 0.829
has parts 8.8 (4.0%) 0.882
Accountability 7.2 (3.3%) 0.799
Cultural/Situational 4.6 (2.1%) 0.267
Public Entities 3.2 (1.4%) 0.429
Omniscience 7.2 (3.3%) 0.828
Probabilistic Dependency 4.8 (2.2%) 0.297
All 215 (97%) 0.678
Table 1: Frequency and inter-annotator agreement for
each category of world knowledge in the survey. Fre-
quencies are averaged over the five annotators, and agree-
ment is calculated using Fleiss?s ?.
support/opposition ? will be familiar to NLP re-
searchers from resources like WordNet, gazetteers,
and text mining projects for extracting causal knowl-
edge, properties of named entities, and opinions. Yet
these familiar categories make up only about 40%
of the world knowledge used in our proofs. Com-
mon knowledge types, like definitional knowledge,
arithmetic, and accountability, have for the most part
been ignored by research on automated knowledge
collection. Others have only earned very scarce and
recent attention, like preconditions (Sil et al, 2010)
and functionality (Ritter et al, 2008).
Several interesting form-based categories, in-
cluding Prominent relationships, Argument Types,
and Simultaneous Conditions, had quite low inter-
annotator agreement. We continue to believe that
these are well-defined categories, and suspect that
332
further studies with better training of the annotators
will support this. One issue during annotation was
that certain pieces of knowledge could be labeled as
a content category or a form category, and instruc-
tions may not have been clear enough on which is
appropriate under these circumstances. Neverthe-
less, considering the number of annotators and the
uneven distribution of data points across the cate-
gories (both of which tend to decrease ?), ? scores
are overall quite high.
In an effort to discover if some of the categories
overlap enough to justify combining them into a sin-
gle category, we tried combining categories which
annotators frequently confused with one another.
While we could not find any combination that sig-
nificantly improved the overall ? score, several com-
binations provided minor improvements. As an ex-
ample of a merge that failed, we tried merging Ar-
gument Types and Mutual Exclusivity, with the idea
that if a system knows about the selectional prefer-
ences of different relationships, it should be able to
deduce which relationships or types are mutually ex-
clusive. However, the ? score for this combined cat-
egory was 0.410, significantly below the ? of 0.640
for Mutual Exclusivity on its own. One merge that
improves ? is a combination of Prominent Relation-
ship with Argument Types (combined ? of 0.250, as
compared with 0.145 for Prominent Relationship and
0.180 for Argument Types). However, we believe
this is due to unclear wording in the proofs, rather
than a real overlap between the two categories. For
instance, ?Painters paint paintings? is an example
of the Prominent Relationship category, and it looks
very similar to the Argument Types example, ?Peo-
ple adopt children.? The knowledge in the first case
is more properly described as, ?If there exists an
unspecified relationship R between a painter and a
painting, then R is the relationship ?painted?.? In
the second case, the knowledge is more properly
described as, ?If x participates in the relationship
?adopts children?, then x is of type ?person?.? Stated
in this way, these kinds of knowledge look quite dif-
ferent. If one reads our proofs from start to finish,
the flow of the argument indicates which of these
forms is intended, but for annotators quickly read-
ing through the proofs, the two kinds of knowledge
can look superficially very similar, and the annota-
tors can become confused.
The best category combination that we discovered
is a combination of Functionality and Mutual Exclu-
sivity (combined ? of 0.784, compared with 0.663
for Functionality and 0.640 for Mutual Exclusivity).
This is a potentially valid alternative to our classi-
fication of the knowledge. Functional relationships
R imply that if x and x? have different values y and
y?, then x and x? must be distinct, or mutually exclu-
sive. We intended that Mutual Exclusivity apply to
sets rather than individual items, but annotators ap-
parently had trouble distinguishing between the two
categories, so in future we may wish to revise our
set of categories. Further surveys would be required
to validate this idea.
The 20 categories of knowledge covered 215
(97%) of the 221 statements of world knowledge
in our proofs. Of the remaining 6 statements, two
were from recognizable categories, like knowledge
for temporal reasoning (#355) and an application of
the frame axiom (#265). We left these out of the sur-
vey to cut down on the number of categories that an-
notators had to learn. The remaining four statements
were difficult to categorize at all. For instance,
#177: ?Motorcycle manufacturers often sponsor
teams in motorcycle sports.? The other three of these
difficult-to-categorize statements came from proofs
for #265, #336, and #432. We suspect that if future
studies analyze more data for common-sense knowl-
edge types, more categories will emerge as impor-
tant, and more facts that lie outside of recognizable
categories will also appear. Fortunately, however, it
appears that at least a very large fraction of common-
sense knowledge can be captured by the sets of cate-
gories we describe here. Thus these categories serve
to point out promising areas for further research in
collecting common-sense knowledge.
References
M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. In IJCAI.
Peter Clark, William R. Murray, John Thompson, Phil
Harrison, Jerry Hobbs, and Christiane Fellbaum.
2007. On the role of lexical and world knowledge in
rte3. In Proceedings of the ACL-PASCAL Workshop on
Textual Entailment and Paraphrasing, RTE ?07, pages
54?59, Morristown, NJ, USA. Association for Com-
putational Linguistics.
333
I. Dagan, O. Glickman, and B. Magnini. 2006. The PAS-
CAL Recognising Textual Entailment Challenge. Lec-
ture Notes in Computer Science, 3944:177?190.
Richard Feldman. 2002. Epistemology. Prentice Hall.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Bradford Books.
R.V. Guha and D.B. Lenat. 1990. Cyc: a mid-term re-
port. AI Magazine, 11(3).
V. Vydiswaran M. Sammons and D. Roth. 2010. Ask
not what textual entailment can do for you... In Proc.
of the Annual Meeting of the Association of Computa-
tional Linguistics (ACL), Uppsala, Sweden, 7. Associ-
ation for Computational Linguistics.
Shachar Mirkin, Ido Dagan, and Eyal Shnarch. 2009.
Evaluating the inferential utility of lexical-semantic re-
sources. In EACL.
Alan Ritter, Doug Downey, Stephen Soderland, and Oren
Etzioni. 2008. It?s a contradiction ? No, it?s not:
A case study using functional relations. In Empirical
Methods in Natural Language Processing.
Dan Roth and Mark Sammons. 2007. Semantic and log-
ical inference model for textual entailment. In Pro-
ceedings of ACL-WTEP Workshop.
Avirup Sil, Fei Huang, and Alexander Yates. 2010. Ex-
tracting action and event semantics from web text. In
AAAI Fall Symposium on Common-Sense Knowledge
(CSK).
D. G. Stork. 1999. The OpenMind Initiative. IEEE Ex-
pert Systems and Their Applications, 14(3):19?20.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A core of semantic knowledge.
In Proceedings of the 16th International Conference
on the World Wide Web (WWW).
Fei Wu and Daniel S. Weld. 2007. Automatically se-
mantifying wikipedia. In Sixteenth Conference on In-
formation and Knowledge Management (CIKM-07).
334
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 423?433,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Large-scale Semantic Parsing via Schema Matching and Lexicon
Extension
Qingqing Cai
Temple University
Computer and Information Sciences
qingqing.cai@temple.edu
Alexander Yates
Temple University
Computer and Information Sciences
yates@temple.edu
Abstract
Supervised training procedures for seman-
tic parsers produce high-quality semantic
parsers, but they have difficulty scaling
to large databases because of the sheer
number of logical constants for which
they must see labeled training data. We
present a technique for developing seman-
tic parsers for large databases based on
a reduction to standard supervised train-
ing algorithms, schema matching, and pat-
tern learning. Leveraging techniques from
each of these areas, we develop a semantic
parser for Freebase that is capable of pars-
ing questions with an F1 that improves by
0.42 over a purely-supervised learning al-
gorithm.
1 Introduction
Semantic parsing is the task of translating natural
language utterances to a formal meaning represen-
tation language (Chen et al, 2010; Liang et al,
2009; Clarke et al, 2010; Liang et al, 2011; Artzi
and Zettlemoyer, 2011). There has been recent in-
terest in producing such semantic parsers for large,
heterogeneous databases like Freebase (Krishna-
murthy and Mitchell, 2012; Cai and Yates, 2013)
and Yago2 (Yahya et al, 2012), which has driven
the development of semi-supervised and distantly-
supervised training methods for semantic parsing.
Previous purely-supervised approaches have been
limited to smaller domains and databases, such as
the GeoQuery database, in part because of the cost
of labeling enough samples to cover all of the log-
ical constants involved in a domain.
This paper investigates a reduction of the prob-
lem of building a semantic parser to three stan-
dard problems in semantics and machine learning:
supervised training of a semantic parser, schema
matching, and pattern learning. Figure 1 provides
a visualization of our system architecture. We
apply an existing supervised training algorithm
for semantic parsing to a labeled data set. We
(sentence, logical form) Training data 
Test questions 
Freebase Web Relations Extracted from Web 
Supervised Semantic Parser Learning MATCHER 
LEXTENDER Freebase PCCG Grammar and Lexicon (word, Freebase symbol) correspondences 
High-Coverage Freebase PCCG Grammar and Lexicon 
Figure 1: We reduce the task of learning a large-
scale semantic parser to a combination of 1) a
standard supervised algorithm for learning seman-
tic parsers; 2) our MATCHER algorithm for find-
ing correspondences between words and database
symbols; and 3) our LEXTENDER algorithm for
integrating (word, database symbol) matches into
a semantic parsing lexicon.
apply schema matching techniques to the prob-
lem of finding correspondences between English
words w and ontological symbols s. And we ap-
ply pattern learning techniques to incorporate new
(w, s) pairs into the lexicon of the trained seman-
tic parser.
This reduction allows us to apply standard tech-
niques from each problem area, which in com-
bination provide a large improvement over the
purely-supervised approaches. On a dataset of
917 questions taken from 81 domains of the Free-
base database, a standard learning algorithm for
semantic parsing yields a parser with an F1 of
0.21, in large part because of the number of log-
ical symbols that appear during testing but never
appear during training. Our techniques can extend
this parser to new logical symbols through schema
matching, and yield a semantic parser with an F1
of 0.63 on the same task. On a more challenging
task where training and test data are divided so that
all logical constants in test are never observed dur-
423
ing training, our approach yields a semantic parser
with an F1 of 0.6, whereas the purely supervised
approach cannot parse a single test question cor-
rectly. These results indicate that it is possible to
automatically extend semantic parsers to symbols
for which little or no training data has been ob-
served.
The rest of this paper is organized as follows.
The next section discusses related work. Section 3
describes our MATCHER algorithm for performing
schema matching between a knowledge base and
text. Section 4 explains how we use MATCHER?s
schema matching to extend a standard semantic
parser to logical symbols for which it has seen no
labeled training data. Section 5 analyzes the per-
formance of MATCHER and our semantic parser.
Section 6 concludes.
2 Previous Work
Two existing systems translate between natural
language questions and database queries over
large-scale databases. Yahya et al (2012) re-
port on a system for translating natural language
queries to SPARQL queries over the Yago2 (Hof-
fart et al, 2013) database. Yago2 consists of
information extracted from Wikipedia, WordNet,
and other resources using manually-defined ex-
traction patterns. The manual extraction patterns
pre-define a link between natural language terms
and Yago2 relations. Our techniques automate
the process of identifying matches between tex-
tual phrases and database relation symbols, in or-
der to scale up to databases with more relations,
like Freebase. A more minor difference between
Yahya et al?s work and ours is that their system
handles SPARQL queries, which do not handle ag-
gregation queries like argmax and count. We
rely on an existing semantic parsing technology
to learn the language that will translate into such
aggregation queries. On the other hand, their test
questions involve more conjunctions and complex
semantics than ours. Developing a dataset with
more complicated semantics in the queries is part
of our ongoing efforts.
Krishnamurthy and Mitchell (2012) also cre-
ate a semantic parser for Freebase covering 77
of Freebase?s over 2000 relations. Like our
work, their technique uses distant supervision to
drive training over a collection of sentences gath-
ered from the Web, and they do not require any
manually-labeled training data. However, their
technique does require manual specification of
rules that construct CCG lexical entries from de-
pendency parses. In comparison, we fully auto-
mate the process of constructing CCG lexical en-
tries for the semantic parser by making it a pre-
diction task. We also leverage synonym-matching
techniques for comparing relations extracted from
text with Freebase relations. Finally, we test our
results on a dataset of 917 questions covering
over 600 Freebase relations, a more extensive test
than the 50 questions used by Krishnamurthy and
Mitchell.
Numerous methods exist for comparing two re-
lations based on their sets of tuples. For instance,
the DIRT system (Lin and Pantel, 2001) uses the
mutual information between the (X,Y ) argument
pairs for two binary relations to measure the sim-
ilarity between them, and clusters relations ac-
cordingly. More recent examples of similar tech-
niques include the Resolver system (Yates and Et-
zioni, 2009) and Poon and Domingos?s USP sys-
tem (Poon and Domingos, 2009). Our techniques
for comparing relations fit into this line of work,
but they are novel in their application of these
techniques to the task of comparing database re-
lations and relations extracted from text.
Schema matching (Rahm and Bernstein, 2001;
Ehrig et al, 2004; Giunchiglia et al, 2005) is a
task from the database and knowledge representa-
tion community in which systems attempt to iden-
tify a ?common schema? that covers the relations
defined in a set of databases or ontologies, and the
mapping between each individual database and the
common schema. Owing to the complexity of the
general case, researchers have resorted to defining
standard similarity metrics between relations and
attributes, as well as machine learning algorithms
for learning and predicting matches between rela-
tions (Doan et al, 2004; Wick et al, 2008b; Wick
et al, 2008a; Nottelmann and Straccia, 2007;
Berlin and Motro, 2006). These techniques con-
sider only matches between relational databases,
whereas we apply these ideas to matches between
Freebase and extracted relations. Schema match-
ing in the database sense often considers com-
plex matches between relations (Dhamanka et al,
2004), whereas as our techniques are currently re-
stricted to matches involving one database relation
and one relation extracted from text.
424
3 Textual Schema Matching
3.1 Problem Formulation
The textual schema matching task is to identify
natural language words and phrases that corre-
spond with each relation and entity in a fixed
schema for a relational database. To formalize this
task, we first introduce some notation.
A schema S = (E,R,C, I) consists of a
set of entities E, a set of relations R, a set of
categories C, and a set of instances I . Categories
are one-argument predicates (e.g., film(e)), and
relations are two- (or more-) argument predicates
(e.g., directed by(e1, e2)). Instances are
known tuples of entities that make a relation
or category true, such as film(Titanic)
or directed by(Titanic, James
Cameron). For a given r ? R (or c ? C),
IS(r) indicates the set of known instances of r in
schema S (and likewise for IS(c)). Examples of
such schemas include Freebase (Bollacker et al,
2008) and Yago2 (Hoffart et al, 2013). We say a
schema is a textual schema if it has been extracted
from free text, such as the Nell (Carlson et al,
2010) and ReVerb (Fader et al, 2011) extracted
databases.
Given a textual schema T and a database
schema D, the textual schema matching task is to
identify an alignment or matching M ? RT ?RD
such that (rT , rD) ? M if and only if rT can
be used to refer to rD in normal language usage.
The problem would be greatly simplified if
M were a 1-1 function, but in practice most
database relations can be referred to in many
ways by natural language users: for instance,
film actor can be referenced by the English
verbs ?played,? ?acted,? and ?starred,? along
with morphological variants of them. In addi-
tion, many English verbs can refer to several
different relations in Freebase: ?make? can refer
to computer processor manufacturer
or distilled spirits producer, among
many others. Our MATCHER algorithm for textual
schema matching handles this by producing a
confidence score for every possible (rT , rD) pair,
which downstream applications can then use to
reason about the possible alignments.
Even worse than the ambiguities in alignment,
some textual relations do not correspond with
any database relation exactly, but instead they
correspond with a projection of a relation, or a
join between multiple relations, or another com-
plex view of a database schema. As a sim-
ple example, ?actress? corresponds to a subset
of the Freebase film actor relation that inter-
sects with the set {x: gender(x, female)}.
MATCHER can only determine that ?actress?
aligns with film actor or not; it cannot pro-
duce an alignment between ?actress? and a join of
film actor and gender. These more complex
alignments are an important consideration for fu-
ture work, but as our experiments will show, quite
useful alignments can be produced without han-
dling these more complex cases.
3.2 Identifying candidate matches
MATCHER uses a generate-and-test architecture
for determining M . It uses a Web search engine
to issue queries for a database relation rD consist-
ing of all the entities in a tuple t ? ID(rD). 1000
tuples for each rD are randomly chosen for issu-
ing queries. The system then retrieves matching
snippets from the search engine results. It uses
the top 10 results for each search engine query. It
then counts the frequency of each word type in the
set of retrieved snippets for rD. The top 500 non-
stopword word types are chosen as candidates for
matches with rD. We denote the candidate set for
rD as C(rD).
MATCHER?s threshold of 500 candidates for
C(rD) results in a maximum possible recall of just
less than 0.8 for the alignments in our dataset, but
even if we double the threshold to 1000, the re-
call improves only slightly to 0.82. We therefore
settled on 500 as a point with an acceptable upper
bound on recall, while also producing an accept-
able number of candidate terms for further pro-
cessing.
3.3 Pattern-based match selection
The candidate pool C(rD) of 500 word types is
significantly smaller than the set of all textual re-
lations, but it is also extremely noisy. The can-
didates may include non-relation words, or other
frequent but unrelated words. They may also in-
clude words that are highly related to rD, but not
actually corresponding textual relations. For in-
stance, the candidate set for film director in
Freebase includes words like ?directed,? but also
words like ?film,? ?movie,? ?written,? ?produced,?
and ?starring.? We use a series of filters based on
synonym-detection techniques to help select the
true matching candidates from C(rD).
425
Pattern Condition Example
1. ?rT in E? rT ends with ?-ed? and E has
type datetime or location
?founded in 1989?
2. ?rT by E? rT ends with ?-ed? ?invented by Edison?
3. ?rT such as E? rT ends with ?-s? ?directors such as Tarantino?
4. ?E is a(n) rT ? all cases ?Paul Rudd is an actor?
Table 1: Patterns used by MATCHER as evidence of a match between rD and rT . E represents an entity
randomly selected from the tuples in ID(rD).
The first type of evidence we consider for
identifying true matches from C(rD) consists of
pattern-matching. Relation words that express rD
will often be found in complex grammatical con-
structions, and often they will be separated from
their entity arguments by long-distance dependen-
cies. However, over a large corpus, one would ex-
pect that in at least some cases, the relation word
will appear in a simple, relatively-unambiguous
grammatical construction that connects rT with
entities from rD. For instance, entities e from the
relationship automotive designer appear in
the pattern ?designed by e? more than 100 times
as often as the next most-common patterns, ?con-
sidered by e? and ?worked by e.?
MATCHER use searches over the Web to count
the number of instances where a candidate rT ap-
pears in simple patterns that involve entities from
rD. Greater counts for these patterns yield greater
evidence of a correct match between rD and rT .
Table 1 provides a list of patterns that we consider.
For each rD and each rT ? C(rD), MATCHER
randomly selects 10 entities from rD?s tuples to
include in its pattern queries. Two of the patterns
are targeted at past-tense verbs, and the other two
patterns at nominal relation words.
MATCHER computes statistics similar to point-
wise mutual information (PMI) (Turney, 2001) to
measure how related rD and rT are, for each pat-
tern p. Let c(p, rD, rT ) indicate the sum of all the
counts for a particular pattern p, database relation,
and textual relation:
fp(rT , rD) =
c(p, rD, rT )?
r?D
c(p, r?D, rT ) ?
?
r?T
c(p, rD, r?T )
For the sum over all r?D, we use all r?D in Freebase
for which rT was extracted as a candidate.
One downside of the pattern-matching evidence
is the sheer number of queries it requires. Freebase
currently has over 2,000 relations. For each rD,
we have up to 500 candidate rT , up to 4 patterns,
and up to 10 entities per pattern. To cover all of
Freebase, MATCHER needs 2, 000?500?4?10 =
40 million queries, or just over 1.25 years if it
issues 1 query per second (we covered approxi-
mately one-quarter of Freebase?s relations in our
experiments). Using more patterns and more en-
tities per pattern are desirable for accumulating
more evidence about candidate matches, but there
is a trade-off with the time required to issue the
necessary queries.
3.4 Comparing database relations with
extracted relations
Open Information Extraction (Open IE) systems
(Banko et al, 2007) can often provide a large set of
extracted tuples for a given rT , which MATCHER
can then use to make much more comprehensive
comparisons with the full tuple set for rD than the
pattern-matching technique allows.
MATCHER employs a form of PMI to compute
the degree of relatedness between rD and rT . In
its simplest form, MATCHER computes:
PMI(rT , rD) =
|ID(rD) ? IT (rT )|
|ID(rD)| ? |IT (rT )|
(1)
While this PMI statistic is already quite useful, we
have found that in practice there are many cases
where an exact match between tuples in ID(rD)
and tuples in IT (rT ) is too strict of a criterion.
MATCHER uses a variety of approximate matches
to compute variations of this statistic. Considered
as predictors for the true matches inM , these vari-
ations of the PMI statistic have a lower precision,
in that they are more likely to have high values
for incorrect matches. However, they also have a
higher recall: that is, they will have a high value
for correct candidates in C(rD) when the strict
version of PMI does not. Table 2 lists all the vari-
ations used by MATCHER.
426
Statistics for (rT , rD)
s?(rT , rD) =
?
tD?ID(rD)
?
tT?IT (rT )
?(tD, tT )
|ID(rD)|?|IT (rT )|
s??(rT , rD) = s?(rT ,rD)?
r?D
s?(r?D, rT )
s??(rT , rD) = |IT (rT )||ID(rD)|
Table 2: MATCHER statistics: for each ? func-
tion for comparing two tuples (given in Table 3),
MATCHER computes the statistics above to com-
pare rD and rT . The PMI statistic in Equation
1 corresponds to s? where ? =strict match over
? =full tuples.
?(t1, t2) for comparing tuples t1, t2
strict match:
{
1, if ?(t1) = ??(t2)
0, otherwise.
type match:
?
??
??
1, if ?kcat(?(t1)k)
= cat(??(t2)k)
0, otherwise.
Table 3: MATCHER?s ? functions for computing
whether two tuples are similar. cat maps an entity
to a category (or type) in the schema. MATCHER
has a different ? function for each possible com-
bination of ? and ?? functions, which are given in
Table 4.
MATCHER uses an API for the ReVerb Open
IE system1 (Fader et al, 2011) to collect I(rT ),
for each rT . The API for ReVerb allows for rela-
tional queries in which some subset of the entity
strings, entity categories, and relation string are
specified. The API returns all matching triples;
types must match exactly, but relation or argument
strings in the query will match any relation or ar-
gument that contains the query string as a sub-
string. MATCHER queries ReVerb with three dif-
ferent types of queries for each rT , specifying the
types for both arguments, or just the type of the
first argument, or just the second argument. Types
for arguments are taken from the types of argu-
ments for a potentially matching rD in Freebase.
To avoid overwhelming the ReVerb servers, for
our experiments we limited MATCHER to queries
1http://openie.cs.washington.edu/
?(t) for tuple t = (e1, . . . , en)
?iei (projection to one dimension)
(e1, . . . , en) (full tuple)
??(?)(e?(1), . . . , e?(n)) (permutation)
Table 4: MATCHER?s ? functions for projecting
or permuting a tuple. ? indicates a permutation of
the indices.
for the top 80 rT ? C(rD), when they are ranked
according to frequency during the candidate iden-
tification process.
3.5 Regression models for scoring candidates
Pattern statistics, the ReVerb statistics from Ta-
ble 2, and the count of rT during the candidate
identification step all provide evidence for correct
matches between rD and rT . MATCHER uses a re-
gression model to combine these various statistics
into a score for (rT , rD). The regression model
is a linear regression with least-squares parameter
estimation; we experimented with support vector
regression models with non-linear kernels, with
no significant improvements in accuracy. Section
5 explains the dataset we use to train this model.
Unlike a classifier, MATCHER does not output any
single matching M . However, downstream appli-
cations can easily convert MATCHER?s output into
a matchingM by, for instance, selecting the topK
candidate rT values for each rD, or by selecting all
(rT , rD) pairs with a score over a chosen thresh-
old. Our experiments analyze MATCHER?s suc-
cess by comparing its performance across a range
of different values for the number of rT matches
for each rD.
4 Extending a Semantic Parser Using a
Schema Alignment
An alignment between textual relations and
database relations has many possible uses: for ex-
ample, it might be used to allow queries over a
database to be answered using additional infor-
mation stored in an extracted relation store, or
it might be used to deduce clusters of synony-
mous relation words in English. Here, we de-
scribe an application in which we build a question-
answering system for Freebase by extending a
standard learning technique for semantic parsing
with schema alignment information.
As a starting point, we used the UBL system
427
developed by Kwiatkowski et al (2010) to learn
a semantic parser based on probabilistic Com-
binatory Categorial Grammar (PCCG). Source
code for UBL is freely available. Its authors
found that it achieves results competitive with the
state-of-the-art on a variety of standard semantic
parsing data sets, including Geo250 English (0.85
F1). Using a fixed CCG grammar and a procedure
based on unification in second-order logic, UBL
learns a lexicon ? from the training data which
includes entries like:
Example Lexical Entries
New York City ` NP : new york
neighborhoods in `
S\NP/NP : ?x?y.neighborhoods(x, y)
Example CCG Grammar Rules
X/Y : f Y : g ? X : f(g)
Y : g X\Y : f ? X : f(g)
Using ?, UBL selects a logical form z
for a sentence S by selecting the z with the
most likely parse derivations y: h(S) =
arg maxz
?
y p(y, z|x; ?,?). The probabilistic
model is a log-linear model with features for lex-
ical entries used in the parse, as well as indi-
cator features for relation-argument pairs in the
logical form, to capture selectional preferences.
Inference (parsing) and parameter estimation are
driven by standard dynamic programming algo-
rithms (Clark and Curran, 2007), while lexicon
induction is based on a novel search procedure
through the space of possible higher-order logic
unification operations that yield the desired logi-
cal form for a training sentence.
Our Freebase data covers 81 of the 86 core do-
mains in Freebase, and 635 of its over 2000 re-
lations, but we wish to develop a semantic parser
that can scale to all of Freebase. UBL gets us part
of the way there, by inducing a PCCG grammar, as
well as lexical entries for function words that must
be handled in all domains. It can also learn lexical
entries for relations rD that appear in the training
data. However, UBL has no way to learn lexical
entries for the many valid (rT , rD) pairs that do
not appear during training.
We use MATCHER?s learned alignment to ex-
tend the semantic parser that we get from UBL
by automatically adding in lexical entries for Free-
base relations. Essentially, for each (rT , rD) from
MATCHER?s output, we wish to construct a lexi-
cal entry that states that rT ?s semantics resembles
?x?y.rD(x, y). However, this simple process is
complicated by the fact that the semantic parser re-
quires two additional types of information for each
lexical entry: a syntactic category, and a weight.
Furthermore, for many cases the appropriate se-
mantics are significantly more complex than this
pattern.
To extend the learned semantic parser to a se-
mantic parser for all of Freebase, we introduce a
prediction task, which we call semantic lexicon ex-
tension: given a matching M together with scores
for each pair in M , predict the syntactic category
Syn, lambda-calculus semantics Sem, and weight
W for a full lexical entry for each (rT , rD) ? M .
One advantage of the reduction approach to learn-
ing a semantic parser is that we can automatically
construct training examples for this prediction task
from the other components in the reduction. We
use the output lexical entries learned by UBL as
(potentially noisy) examples of true lexical entries
for (rT , rD) pairs where rT matches the word in
one of UBL?s lexical entries, and rD forms part
of the semantics in the same lexical entry. For
(rT , rD) pairs in M where rD occurs in UBL?s
lexical entries, but not paired with rT , we create
dummy ?negative? lexical entries with very low
weights, one for each possible syntactic category
observed in all lexical entries. Note that in or-
der to train LEXTENDER, we need the output of
MATCHER for the relations in UBL?s training data,
as well as UBL?s output lexicon from the training
data.
Our system for this prediction task, which we
call LEXTENDER (for Lexicon eXtender), factors
into three components: P (Sem|rD, rT , score),
P (Syn|Sem, rD, rT , score), and
P (W |Syn, Sem, rD, rT , score). This factoriza-
tion is trivial in that it introduces no independence
assumptions, but it helps in designing models
for the task. We set the event space for random
variable Sem to be the set of all lambda calculus
expressions observed in UBL?s output lexicon,
modulo the names of specific Freebase relations.
For instance, if the lexicon includes two entries
whose semantics are ?x?y . film actor(x, y) and
?x?y . book author(x, y), the event space would
include the single expression in which relations
film actor and book author were replaced by
428
a new variable: ?p?x?y.p(x, y). The final
semantics for a lexical entry is then constructed
by substituting rD for p, or more formally, by a
function application Sem(rD). The event space
for Syn consists of all syntactic categories in
UBL?s output lexicon, and W ranges over R.
LEXTENDER?s model for Sem and Syn are
Na??ve Bayes classifiers (NBC), with features for
the part-of-speech for rT (taken from a POS tag-
ger), the suffix of rT , the number of arguments of
rD, and the argument types of rD. For Syn, we
add a feature for the predicted value of Sem. For
W , we use a linear regression model whose fea-
tures are the score from MATCHER, the probabili-
ties from the Syn and Sem NBC models, and the
average weight of all lexical entries in UBL with
matching syntax and semantics. Using the pre-
dictions from these models, LEXTENDER extends
UBL?s learned lexicon with all possible lexical en-
tries with their predicted weights, although typi-
cally only a few lexical entries have high enough
weight to make a difference during parsing. Prun-
ing entries with low weights could improve the
memory and time requirements for parsing, but
these were not an issue in our experiments, so we
did not investigate this further.
5 Experiments
We conducted experiments to test the ability of
MATCHER and LEXTENDER to produce a se-
mantic parser for Freebase. We first analyze
MATCHER on the task of finding matches between
Freebase relations and textual relations. We then
compare the performance of the semantic parser
learned by UBL with its extension provided by
LEXTENDER on a dataset of English questions
posed to Freebase.
5.1 Experimental Setup
Freebase (Bollacker et al, 2008) is a free,
online, user-contributed, relational database
(www.freebase.com) covering many different
domains of knowledge. The full schema and
contents are available for download. The ?Free-
base Commons? subset of Freebase, which is our
focus, consists of 86 domains, an average of 25
relations per domain (total of 2134 relations),
and 615,000 known instances per domain (53
million instances total). As a reference point,
the GeoQuery database ? which is a standard
benchmark database for semantic parsing ?
Examples
1. What are the neighborhoods in New
York City?
?x . neighborhoods(new york, x)
2. How many countries use the rupee?
count(x) . countries used(rupee, x)
3. How many Peabody Award winners are
there?
count(x) . ?y . award honor(y) ?
award winner(y, x) ?
award(y, peabody award)
Figure 2: Example questions with their logical
forms. The logical forms make use of Freebase
symbols as logical constants, as well as a few ad-
ditional symbols such as count and argmin, to
allow for aggregation queries.
contains a single domain (geography), 8 relations,
and 880 total instances.
Our dataset contains 917 questions (on aver-
age, 6.3 words per question) and a meaning repre-
sentation for each question written in a variant of
lambda calculus2. 81 domains are represented in
the data set, and the lambda calculus forms contain
635 distinct Freebase relations. The most com-
mon domains, film and business, each took
up no more than 6% of the overall dataset. Sev-
eral examples are listed in Fig. 2. The ques-
tions were provided by two native English speak-
ers. No restrictions were placed on the type of
questions they should produce, except that they
should produce questions for multiple domains.
By inspection, a large majority of the questions
appear to be answerable from Freebase, although
no instructions were given to restrict questions
to this sort. We also created a dataset of align-
ments from these annotated questions by creating
an alignment for each Freebase relation mentioned
in the logical form for a question, paired with a
manually-selected word from the question.
5.2 Alignment Tests
We measured the precision and recall of
MATCHER?s output against the manually la-
beled data. Let M be the set of (rT , rD) matches
produced by the system, and G the set of matches
in the gold-standard manual data. We define
2The data is available from the second author?s website.
429
0
0.1
0.2
0.3
0.4
0.5
0.6
0 0.2 0.4 0.6 0.8 1
Pre
cisi
on 
Recall  
Alignment Predictions  
Matcher
Extractions
Pattern
Frequency
Figure 3: MATCHER?s Pattern features and Extrac-
tions features complement one another, so that in
combination they outperform either subset on its
own, especially at the high-recall end of the curve.
precision and recall as:
P = |M ?G||M | , R =
|M ?G|
|G|
Figure 3 shows a Precision-Recall (PR) curve
for MATCHER and three baselines: a ?Frequency?
model that ranks candidate matches for rD by their
frequency during the candidate identification step;
a ?Pattern? model that uses MATCHER?s linear re-
gression model for ranking, but is restricted to
only the pattern-based features; and an ?Extrac-
tions? model that similarly restricts the ranking
model to ReVerb features. We have three folds in
our data; the alignments for relation rD in one fold
are predicted by models trained on the other two
folds. Once all of the alignments in all three folds
are scored, we generate points on the PR curve by
applying a threshold to the model?s ranking, and
treating all alignments above the threshold as the
set of predicted alignments.
All regression models for learning alignments
outperform the Frequency ranking by a wide mar-
gin. The Pattern model outperforms the Extrac-
tions model at the high-precision, low-recall end
of the curve. At the high-recall points, the Pat-
tern model drops quickly in precision. However,
the combination of the two kinds of features in
MATCHER yields improved precision at all levels
of recall.
5.3 Semantic Parsing Tests
While our alignment tests can tell us in relative
terms how well different models are performing,
it is difficult to assess these models in absolute
terms, since alignments are not typical applica-
tions that people care about in their own right. We
now compare our alignments on a semantic pars-
ing task for Freebase.
In a first semantic parsing experiment, we train
UBL, MATCHER, and LEXTENDER on a random
sample of 70% of the questions, and test them
on the remaining 30%. In a second test, we fo-
cus on the hard case where all questions from the
test set contain logical constants that have never
been seen before during training. We split the
data into 3 folds, making sure that no Freebase do-
main has symbols appearing in questions in more
than one fold. We then perform 3-fold cross-
validation for all of our supervised models. We
varied the number of matches that the alignment
model (MATCHER, Pattern, Extractions, or Fre-
quency) could make for each Freebase relation,
and measured semantic parsing performance as a
function of the number of matches.
Figure 4 shows the F1 scores for these se-
mantic parsers, judged by exact match between
the top-scoring logical form from the parser and
the manually-produced logical form. Exact-match
tests are overly-strict, in the sense that the sys-
tem may be judged incorrect even when the log-
ical form that is produced is logically equivalent
to the correct logical form. However, by inspec-
tion such cases appear to be very rare in our data,
and the exact-match criterion is often used in other
semantic parsing experimental settings.
The semantic parsers produced by
MATCHER+LEXTENDER and the other alignment
techniques significantly outperform the baseline
semantic parser learned by UBL, which achieves
an overall F1 of 0.21 on these questions in the
70/30 split of the data, and an F1 of 0 in the
cross-domain experiment. Purely-supervised
approaches to this data are severely limited, since
they have almost no chance of correctly parsing
questions that refer to logical symbols that never
appeared during training. However, MATCHER
and LEXTENDER combine with UBL to produce
an effective semantic parser. The best semantic
parser we tested, which was produced by UBL,
MATCHER, and LEXTENDER with 9 matches per
Freebase relation, had a precision of 0.67 and a
recall of 0.59 on the 70/30 split experiment.
The difference in alignment performance be-
tween MATCHER, Pattern, and Extractions carries
over to semantic parsing. MATCHER drops in F1
with more matches as additional matches tend to
be low-quality and low-probability, whereas Pat-
430
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0 5 1 0 1 5 2 0 2 5 3 0
F1 fo
r ex
act m
atch
 
 
of lo
gica
l form
s
 
Number of Matches per Freebase Relation  
Semantic Parsing (70/30 Split)  
Matcher
Pattern
Extractions
Frequency
Baseline
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0 5 1 0 1 5 2 0 2 5 3 0
F1 fo
r ex
act m
atch
 
 
of lo
gica
l form
s
 
Number of Matches per Freebase Relation  
Semantic Parsing (Cross -Domain)  
Matcher
Extractions
Pattern
Frequency
Figure 4: Semantic parsers produced by UBL+MATCHER+LEXTENDER outperform the purely-
supervised baseline semantic parser on a random 70/30 split of the data (left) by as much as
0.42 in F1. In the case of this split and in the case of a cross-domain experiment (right),
UBL+MATCHER+LEXTENDER outperforms UBL+Pattern+LEXTENDER by as much as 0.06 in F1.
tern and Extractions keep improving as more low-
probability alignments are added. Interestingly,
the Extractions model begins to overtake the Pat-
tern model in F1 at higher numbers of matches,
and all three models trend toward convergence in
F1 with increasing numbers of matches. Neverthe-
less, MATCHER clearly improves over both, and
reaches a higher F1 than either Pattern or Extrac-
tions using a small number of matches, which cor-
responds to a smaller lexicon and a leaner model.
To place these results in context, many different
semantic parsers for databases like GeoQuery and
ATIS (including parsers produced by UBL) have
achieved F1 scores of 0.85 and higher. However,
in all such tests, the test questions refer to logi-
cal constants that also appeared during training, al-
lowing supervised techniques for learning seman-
tic parsers to achieve strong accuracy. As we have
argued, Freebase is large enough that is difficult
to produce enough labeled training data to cover
all of its logical constants. An unsupervised se-
mantic parser for GeoQuery has achieved an F1
score of 0.66 (Goldwasser et al, 2011), impres-
sive in its own right and slightly better than our F1
score. However, this parser was given questions
which it knew a priori to contain words that re-
fer to the logical constants in the database. Our
MATCHER and LEXTENDER systems address a
different challenge: how to learn a semantic parser
for Freebase given the Web and a set of initial la-
beled questions.
6 Conclusion
Scaling semantic parsing to large databases re-
quires an engineering effort to handle large
datasets, but also novel algorithms to extend se-
mantic parsing models to testing examples that
look significantly different from labeled training
data. The MATCHER and LEXTENDER algo-
rithms represent an initial investigation into such
techniques, with early results indicating that se-
mantic parsers can handle Freebase questions on a
large variety of domains with an F1 of 0.63.
We hope that our techniques and datasets will
spur further research into this area. In particu-
lar, more research is needed to handle more com-
plex matches between database and textual rela-
tions, and to handle more complex natural lan-
guage queries. As mentioned in section 3.1, words
like ?actress? cannot be addressed by the cur-
rent methodology, since MATCHER assumes that
a word maps to a single Freebase relation, but
the closest Freebase equivalent to the meaning of
?actress? involves the two relations film actor
and gender. Another limitation is that our cur-
rent methodology focuses on finding matches for
nouns and verbs. Other important limitations of
the current methodology include:
? the assumption that function words have no
domain-specific meaning, which prepositions
in particular can violate;
? low accuracy when there are few relevant re-
sults among the set of extracted relations;
? and the restriction to a single database (Free-
base) for finding answers.
While significant challenges remain, the reduction
of large-scale semantic parsing to a combination
of schema matching and supervised learning of-
fers a new path toward building high-coverage se-
mantic parsers.
431
References
Yoav Artzi and Luke Zettlemoyer. 2011. Bootstrap-
ping Semantic Parsers from Conversations. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP).
M. Banko, M. J. Cafarella, S. Soderland, M. Broad-
head, and O. Etzioni. 2007. Open information ex-
traction from the web. In IJCAI.
Jacob Berlin and Amihai Motro. 2006. Database
schema matching using machine learning with fea-
ture selection. In Advanced Information Systems
Engineering. Springer.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the Interna-
tional Conference on Management of Data (SIG-
MOD), pages 1247?1250.
Qingqing Cai and Alexander Yates. 2013. Semantic
Parsing Freebase: Towards Open-Domain Semantic
Parsing. In Second Joint Conference on Lexical and
Computational Semantics.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an Architecture for Never-
Ending Language Learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010).
David L. Chen, Joohyun Kim, and Raymond J.
Mooney. 2010. Training a Multilingual
Sportscaster: Using Perceptual Context to Learn
Language. Journal of Artificial Intelligence Re-
search, 37:397?435.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with ccg and
log-linear models. Computational Linguistics,
33(4):493?552.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world?s re-
sponse. In Computational Natural Language Learn-
ing (CoNLL).
R. Dhamanka, Y. Lee, A. Doan, A. Halevy, and
P. Domingos. 2004. iMAP: Discovering Complex
Semantic Matches between Database Schemas. In
SIGMOD.
A. Doan, J. Madhavan, P. Domingos, and A. Halevy.
2004. Ontology Matching: A Machine Learning
Approach. In S. Staab and R. Studer, editors, Hand-
book on Ontologies in Information Systems, pages
397?416. Springer-Verlag.
M. Ehrig, P. Haase, N. Stojanovic, and M. Hefke.
2004. Similarity for ontologies-a comprehensive
framework. In Workshop Enterprise Modelling and
Ontology: Ingredients for Interoperability, PAKM.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying Relations for Open Information
Extraction. In Conference on Empirical Methods in
Natural Language Processing (EMNLP).
Fausto Giunchiglia, Pavel Shvaiko, and Mikalai
Yatskevich. 2005. Semantic schema matching.
On the Move to Meaningful Internet Systems 2005:
CoopIS, DOA, and ODBASE, pages 347?365.
D. Goldwasser, R. Reichart, J. Clarke, and D. Roth.
2011. Confidence driven unsupervised semantic
parsing. In Association for Computational Linguis-
tics (ACL).
Johannes Hoffart, Fabian M. Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. YAGO2:
A Spatially and Temporally Enhanced Knowl-
edge Base from Wikipedia. Artificial Intelligence,
194:28?61, January.
Jayant Krishnamurthy and Tom Mitchell. 2012.
Weakly Supervised Training of Semantic Parsers. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing Probabilis-
tic CCG Grammars from Logical Form with Higher-
order Unification. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Association for Computational Linguistics and In-
ternational Joint Conference on Natural Language
Processing (ACL-IJCNLP).
P. Liang, M. I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
Association for Computational Linguistics (ACL).
D. Lin and P. Pantel. 2001. DIRT ? Discovery of In-
ference Rules from Text. In KDD.
Henrik Nottelmann and Umberto Straccia. 2007. In-
formation retrieval and machine learning for proba-
bilistic schema matching. Information processing &
management, 43(3):552?576.
Hoifung Poon and Pedro Domingos. 2009. Unsu-
pervised semantic parsing. In Proceedings of the
2009 Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP ?09, pages 1?10,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
E. Rahm and P.A. Bernstein. 2001. A survey of ap-
proaches to automatic schema matching. The VLDB
Journal, 10:334?350.
P. D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Procs. of the
Twelfth European Conference on Machine Learning
(ECML), pages 491?502, Freiburg, Germany.
432
M. Wick, K. Rohanimanesh, A. McCallum, and A.H.
Doan. 2008a. A discriminative approach to ontol-
ogy mapping. In International Workshop on New
Trends in Information Integration (NTII) at VLDB
WS.
M.L. Wick, K. Rohanimanesh, K. Schultz, and A. Mc-
Callum. 2008b. A unified approach for schema
matching, coreference and canonicalization. In Pro-
ceeding of the 14th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing.
Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural Language Questions for the
Web of Data. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised methods for determining object and relation
synonyms on the web. Journal of Artificial Intelli-
gence Research (JAIR), 34:255?296, March.
433
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 328?338, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Semantic Parsing Freebase: Towards Open-domain Semantic Parsing
Qingqing Cai
Temple University
Computer and Information Sciences
qingqing.cai@temple.edu
Alexander Yates
Temple University
Computer and Information Sciences
yates@temple.edu
Abstract
Existing semantic parsing research has
steadily improved accuracy on a few domains
and their corresponding databases. This paper
introduces FreeParser, a system that trains on
one domain and one set of predicate and con-
stant symbols, and then can parse sentences
for any new domain, including sentences that
refer to symbols never seen during training.
FreeParser uses a domain-independent archi-
tecture to automatically identify sentences
relevant to each new database symbol, which
it uses to supplement its manually-annotated
training data from the training domain. In
cross-domain experiments involving 23
domains, FreeParser can parse sentences for
which it has seen comparable unannotated
sentences with an F1 of 0.71.
1 Introduction
Semantic parsing is the task of converting a sentence
into a representation of its meaning, usually in a log-
ical form grounded in the symbols of some fixed
ontology or relational database (Zelle and Mooney,
1996; Zettlemoyer and Collins, 2005; Kate and
Mooney, 2006). A growing body of research on
semantic parsing has yielded consistent improve-
ments in parsing accuracy. Yet existing semantic
parsers have always been limited by the need for
significant amounts of manually-annotated training
data for each domain of discourse, or for each new
database. As a result, current semantic parsers have
been constrained to small domains, like answering
geography questions.
In an effort to break out of these narrowly-
constrained domains, we investigate semantic
parsers for Freebase, an online database of user-
contributed facts divided into 86 domains, includ-
ing everything from architecture to zoos. Freebase is
much larger than standard benchmark databases for
semantic parsing; for example, it contains 300 times
as many relations, and 75,000 times as many in-
stances, as the GeoQuery database. On average, the
benchmark GeoQuery dataset has 125 training sen-
tences per relation. An equivalent dataset for Free-
base would require labeling close to 40,000 training
sentences, an expensive undertaking.
The size and diversity of data in Freebase forces
us to consider a new task of open-domain seman-
tic parsing. We introduce FreeParser, which trains
on labeled examples from a select group of initial
domains. It also uses the information in Freebase
to automatically find unlabeled training sentences
from Wikipedia for every Freebase relation. Using
a self-supervised architecture, FreeParser automat-
ically labels these sentences, and then trains a se-
mantic parser for all of Freebase. The current re-
striction to Wikipedia has a downside: 44% of the
test questions in our dataset contained a word that
never appeared in our set of automatically-collected
sentences, suggesting that significant further gains
could be had by scaling to a larger corpus. However,
FreeParser is able to find correct parses for 70% of
the questions from new domains where it could find
relevant sentences in Wikipedia, at a precision of
72%.
The next section provides background on se-
mantic parsing for Freebase, and discusses related
work. Section 3 describes the main modules of the
FreeParser system. Section 4 analyzes the perfor-
mance of FreeParser on an open-domain semantic
parsing task. Section 5 concludes.
328
domain num. queries % of total
film 49 12
business 46 11
tv 34 8
location 32 8
award 32 8
people 30 7
medicine 25 6
organization 24 6
finance 21 5
book 21 5
et al 89 22
total 403 100
Table 1: Breakdown of our Freebase data set into do-
mains. Several questions used symbols from multiple
Freebase domains, in which cases human judges selected
the best domain they could for that question?s category.
2 Background and Previous Work
2.1 Freebase Dataset
Freebase is a free, online, user-contributed, rela-
tional database (www.freebase.com) covering many
different domains of knowledge. The full schema
and contents are available for download.
Freebase has a number of advantages for build-
ing an open-domain semantic parser. Most obvi-
ously, it provides a much tougher test for seman-
tic parsing than traditional benchmark databases like
GeoQuery. It also provides a testbed for semantic
parsing across domains. As a reference point, the
GeoQuery database contains a single domain (ge-
ography), 8 relations, and 698 total instances. The
?Freebase Commons? subset of Freebase, which is
our focus, consists of 86 domains, an average of 25
relations per domain (total of 2134 relations), and
615,000 known instances per domain (53 million in-
stances total). By dividing Freebase into different
sub-databases according to domain, we can readily
test the portability of our parser across domains, and
its ability to handle relations and symbols that never
occur in manually-labeled training data.
Our dataset contains 403 questions and a meaning
representation for each question, written in a variant
of lambda calculus1. We believe the dataset in it-
self is an important contribution to the field, as it
1The data is available from the second author?s webpage.
Examples
1. What are the neighborhoods in New
York City?
?x . neighborhoods(new york, x)
2. How many countries use the rupee?
count(x) . countries used(rupee, x)
3. How many Peabody Award winners are
there?
count(x) . ?y . award honor(y) ?
award winner(y, x) ?
award(y, peabody award)
Figure 1: Example questions with their logical forms.
provides a testbed for semantic parsing across mul-
tiple domains. Several examples are listed in Fig. 1,
and Table 1 provides a breakdown of the domains in
our data. The questions were provided by two na-
tive English speakers, one high school student and
one computer science undergraduate student. Each
contributor was introduced to the Freebase website,
and asked to come up with English questions that
they would like to have answered. No restrictions
were placed on the type of questions they should
produce, except that they should produce questions
for multiple domains. 23 domains are represented in
the data set. Inspection of the dataset indicates that
most questions have relatively simple and regular
syntax, compared with the more complex construc-
tions observed in datasets like GeoQuery. Collecting
more complex questions for open-domain tests is an
ongoing project, but the existing dataset is already
a significant challenge for current semantic parsing
learning algorithms.
2.2 Challenges for a Freebase Semantic Parser
To provide a benchmark for comparison, we applied
the PCCG-based semantic parser called UBL, de-
veloped by Kwiatkowski et al (2010). Source code
for UBL is freely available. Its authors found that it
achieves results competitive with the state-of-the-art
on a variety of standard semantic parsing data sets,
including Geo250 English (0.85 F1). Using a fixed
CCG grammar and a procedure based on unification
in second-order logic, UBL learns a lexicon ? from
the training data which includes entries like:
329
Example Lexical Entries
New York City ` NP : new york
neighborhoods in `
S\NP/NP : ?x?y.neighborhoods(x, y)
Example CCG Grammar Rules
X/Y : f Y : g ? X : f(g)
Y : g X\Y : f ? X : f(g)
Using ?, UBL selects a logical form z for a sen-
tence S by selecting the z with the most likely parse
derivations y: h(S) = arg maxz
?
y p(y, z|x; ?,?).
The probabilistic model is a log-linear model with
features for lexical entries used in the parse, as
well as indicator features for relation-argument pairs
in the logical form, to capture selectional prefer-
ences. Inference (parsing) and parameter estimation
are driven by standard dynamic programming algo-
rithms (Clark and Curran, 2007; Wilks et al, 1990),
using a context-free, combinatory categorial gram-
mar that includes rules for forward application and
composition.
In a standard experimental setup on our dataset,
UBL provides an F1 of 0.35. We took a random split
of 70% of the data for training, 30% for test. An
F1 of 0.35 is significantly worse than UBL?s perfor-
mance on GeoQuery data (F1 of 0.85) but within the
bounds of reason, given that our data has over 200
relation symbols that need to be learned using less
than 300 training sentences, compared with the 8 re-
lations and 250 sentences that make up the Geo250
English dataset.
However, UBL is not designed for open-domain
semantic parsing, and after training on the training
set above, it is not be able to handle questions for
any of the remaining 63 domains in Freebase. In
open-domain tests, it achieves an F1 of 0.0, and for
most sentences, it cannot produce a parse. As one
example, we created a test set from the business
and finance domains, and separated the remain-
ing domains for training. Every test example has a
predicate symbol that has never been observed be-
fore in training. The F1 of 0.0 on this dataset is not a
fault of UBL, but rather it shows the difficulty of the
task. Porting a system across domains often results
in substantial loss of accuracy for many natural lan-
guage processing tasks (Huang et al, 2011), but usu-
ally the drop in accuracy is no more than 10-20%.
Open-domain semantic parsing is an even starker
challenge; it involves not just new natural language
words in the new domains, but also new database
symbols, which existing technology cannot handle.
2.3 Previous Work
Krishnamurthy and Mitchell (2012) also create a se-
mantic parser for Freebase, covering 77 of Free-
base?s over 2000 relations. Like our work, their
technique uses distant supervision to drive training
over a collection of sentences gathered from the
Web, and they do not require any manually-labeled
training data. However, their technique does require
manual specification of rules that construct CCG
lexical entries from dependency parses. In compar-
ison, we fully automate the process of constructing
CCG lexical entries for the semantic parser by mak-
ing it a learning task. We test our results on a dataset
of over 400 questions covering over 200 Freebase re-
lations, a more extensive test than the 50 questions
used by Krishnamurthy and Mitchell.
Yahya et al (2012) report on a system for
translating natural language queries to SPARQL
queries over the Yago2 (Hoffart et al, 2013)
database. Yago2 consists of information extracted
from Wikipedia, WordNet, and other resources us-
ing manually-defined extraction patterns. The man-
ual extraction patterns pre-define a link between nat-
ural language terms and Yago2 relations. Our tech-
niques automate the process of identifying matches
between textual phrases and database relation sym-
bols, in order to scale up to databases with more
relations, like Freebase. A more minor difference
between Yahya et al?s work and ours is that their
system handles SPARQL queries, which do not han-
dle aggregation queries like argmax and count.
We rely on an existing semantic parsing technology
to learn the language that will translate into such
aggregation queries. On the other hand, their test
questions involve more conjunctions and complex
semantics than ours. Developing a dataset with more
complicated semantics in the queries is part of our
ongoing efforts.
Goldwasser et al?s self-supervised, grounded se-
mantic parser (2011) relies on co-training between
two different semantic parsing models, one being a
simple machine-translation model and the other a
more complex structured-prediction model. They
achieve an impressive F1 of 0.66 on the bench-
mark GeoQuery 250 (English) dataset, compared
with state-of-the-art supervised models that achieve
330
accuracies around 0.85. Unlike semantic parsers
for Freebase, Goldwasser et al?s work assumes that
a dataset of unlabeled geography questions already
exists, for use in unsupervised training. FreeParser
answers orthogonal questions: how can we auto-
matically acquire a dataset containing the right key-
words and phrases, given only the database itself,
and how can we ensure that the acquired sentences
are relevant to the relations in the database, with-
out manual supervision? Also, unlike Goldwasser
et al?s experiments, FreeParser is tested in a signif-
icantly more challenging setting, with far more do-
mains, relations, and entities to be learned.
Many supervised learning frameworks have been
applied, including inductive logic programming
(Zelle and Mooney, 1996; Thompson and Mooney,
1999; Thompson and Mooney, 2003), support vec-
tor machine-based kernel approaches (Kate et al,
2005; Kate and Mooney, 2006; Kate and Mooney,
2007), machine translation-style synchronous gram-
mars (Wong and Mooney, 2007), and context-
free grammar-based approaches like probabilistic
Combinatory Categorial Grammar (Zettlemoyer and
Collins, 2005; Zettlemoyer and Collins, 2007;
Zettlemoyer and Collins, 2009; Kwiatkowski et al,
2010; Kwiatkowski et al, 2011; Lu et al, 2008) and
discriminative reranking (Ge and Mooney, 2006; Ge
and Mooney, 2009). These approaches have yielded
steady improvements on standard test sets like Geo-
Query, but are difficult to apply to Freebase because
of their built-in assumption that relation symbols
will be observed during training.
There has been a recent push towards develop-
ing techniques which reduce the annotation cost or
the data complexity of the models. Models have
been developed which can handle some ambiguity
in terms of which logical form is the correct label
for each training sentence (Chen et al, 2010; Liang
et al, 2009). Another set of approaches has investi-
gated the case where no logical forms are provided,
but instead some form of feedback or response from
the world is used as evidence for what the correct
logical form must have been (Clarke et al, 2010;
Liang et al, 2011; Artzi and Zettlemoyer, 2011).
While such techniques are important, they can only
reduce the annotation cost per domain, and annota-
tion efforts would still be required for each new do-
main that contains new database symbols. The goal
of the Freebase semantic parser, in contrast, is to
program actor role 
Party Down Ryan Hansen Kyle Bradway 
structure owner 
CN Tower Canada Lands Co. 
particle sub-particle number 
Proton Up Quark 2 
TV domain 
cast member table 
Architecture domain 
ownership table 
Physics domain 
particle composition 
Figure 2: Example Freebase relations (tables) and in-
stances for three domains.
port to all domains automatically, without any new
manually-labeled data per domain.
3 FreeParser
We introduce FreeParser, an automated system for
converting natural language sentences into represen-
tations of their meaning, where the relation and con-
stant symbols for the meaning representations are
taken from Freebase. FreeParser?s modules are de-
scribed below.
Sentence Retrieval Engine: This module con-
structs keyword queries for sentences that are likely
to express the same relationships as the ones ob-
served in Freebase. It uses an index over a large
corpus, currently a snapshot of English Wikipedia,
to identify sentences that match the query. Each sen-
tence, along with the Freebase relation r and query
q that generated it, is then fed to the Auto-Labeler.
Auto-Labeler: The Auto-Labeler uses knowledge
of the relation and query for a sentence to automati-
cally generate a simple logical form for the sentence.
The automatically-labeled sentences are then sent to
the Assessor.
Assessor: Using a set of domain-independent fea-
tures, the Assessor filters out sentences that are un-
suitable for training the semantic parser. These in-
clude sentences that are too long or complex, and
sentences where the label from the Auto-Labeler ap-
pears to be incorrect. The sentences that pass this
filter are added to the training data for FreeParser?s
semantic parser.
Open-domain Regularizer: FreeParser relies on an
existing semantic parser, but with a novel regularizer
that helps it learn more appropriate lexical entries for
domain-independent function words.
3.1 Sentence Retrieval Engine
The Sentence Retrieval Engine is FreeParser?s open-
domain technique for retrieving sentences from a
331
Input: Freebase relation r, unlabeled corpus C
Output: Sent, a set of sentences relevant to r
1. Initialize Sent? ?
2. E ?M random instances from r,
each projected onto two random attributes
3. E? ? N pairs (e1, e2) ? E with smallest
relation-count(e1, e2)
4. For each (e1, e2) ? E?:
S2? {sentences in C containing e1 and e2}
S1? {sents containing e2, in docs with e1}
Sent? Sent ? S1 ? S2
5. Return Sent
Figure 3: The Sentence Retrieval Engine algorithm
corpus that are relevant to a particular relation in the
database.
Definition relevance: We say that a sentence s is
relevant to a relation r in Freebase if there exist
database symbols a1, . . . , ak such that r(a1, . . . , ak)
appears in Freebase, and r(a1, . . . , ak) forms part of
the meaning of s, if the meaning were written in a
logical form.
For example, for the cast member relation in the
Freebase sample shown in Figure 2, the sentence
Hansen also played Kyle Bradway on the
Starz show Party Down.
would be relevant, since the sentence expresses a
known instance of cast member.
Of course, the corpus given as input to the Sen-
tence Retrieval Engine contains only sentences, not
the logical forms required to determine relevance ac-
cording to our definition. FreeParser?s strategy is
to generate keyword queries that list several named
entities that belong to a particular relation. For in-
stance, one query that the Sentence Retrieval Engine
might generate for the cast member relation is
?Ryan Hansen Kyle Bradway,? and another might
be ?Kyle Bradway Party Down.?
Figure 3 shows the algorithm for the Sentence
Retrieval Engine. In our experiments, we create
M = 1000 candidate entity pairs, and we select
N = 50 for queries. We use the open-source Apache
Lucene software for constructing an index over the
Wikipedia corpus and retrieving relevant sentences.
We have found that selecting good queries is in
fact quite tricky, and our experiments in Section 4.4
indicate how badly things can go wrong if it is not
done carefully. Two important lessons stand out:
First, for reasonable recall, we limit queries to just
one or two names. Queries with two names (we call
these 2-entity queries) are very often highly relevant,
but there are not enough sentences in Wikipedia that
match such queries for all relations. We therefore
also include queries (which we refer to as 1-entity
queries) that first identify Wikipedia articles for one
name from a relation, such as articles that mention
?Ryan Hansen?, and then within this resulting docu-
ment set, we select sentences that match a second
name, such as ?Party Down?. The resulting sen-
tences therefore always contain one name from the
relation, and appear near (within the same document
as) a second name. These sentences are noisier than
sentences selected with two names, but there are far
more matches of such sentences within Wikipedia.
The second lesson for sentence retrieval is that we
need to select queries that are not ambiguous. For
instance, ?James Cameron Avatar? retrieves many
sentences for the relation directed by. Un-
fortunately, this same query also produces many
sentences for the relations written by and won
award for. The two entities are not enough to un-
ambiguously identify the relationship between them.
To combat this problem, FreeParser scores candidate
queries according to relation-count, the number of
relations in Freebase that hold between the names
in the query, and keeps the top-N least ambiguous
queries, breaking ties randomly.
3.2 Auto-Labeler
The Auto-Labeler automatically generates a log-
ical form label for every sentence in our data
set. It provides a form of ?distant supervision?
(Bunescu and Mooney, 2007). As an example, if
the sentence above were generated from the cast
member relation using the query ?Hansen Brad-
way?, and ?Hansen? and ?Bradway? are names
for the database symbols Ryan Hansen and
Kyle Bradway, then the Auto-Labeler produces
?pcast member(p,Ryan Hansen,Kyle Bradway) as a la-
bel for the sentence. The existentially quantified p
variable is necessary to supply enough arguments for
the cast member relation.
For the general case, let s be a sentence generated
from a relation r of arity n via queries involving the
entities e = (e1, . . . , em), and let a = (a1, . . . , am)
be the sequence of attribute indices of r such that
332
Input: auto-labeled sentences S for relation r
Output: S? ? S, a high-quality training dataset
1. For each (s, l) ? S:
C[s, l]? complexity-score(s, l)
2. Sort S in descending order according to C
3. T ? top 100 examples from S
4. CW ? critical-words(T )
5. result? ?
6. For each cw ? CW :
Scw ? top two (s, l) ? (S ? result)
such that s contains cw
result? result ? Scw
7. Return result
Figure 4: The Assessor algorithm
ei is a value for r?s attribute ai. The Auto-Labeler
produces the following logical form for s:
?v1, . . . vn s.t. r(v1, . . . , vn)?
va1 = e1 ? . . . ? vam = em
3.3 Assessor
Automatically retrieving training sentences from an
unlabeled corpus is a noisy process. In order to
improve its precision, FreeParser automatically as-
sesses whether each sentence from the Sentence Re-
trieval Engine is relevant and useful for training. Its
goal is to select, for each relation r, a set of sen-
tences that are all structurally simple; that include a
variety of ways of expressing r in English; and that
do not include any sentences about other relations
r?. The full Assessor algorithm is given in Figure 4.
The Assessor uses two sources of evidence. The
first is the complexity of the sentence. After ex-
perimenting with numerous features for measur-
ing complexity, we have found that a few types
of word counts are the most helpful. Specifically,
the most helpful features include: the number of
words between two named entities (for two-entity
queries), the number of words before the named
entity that was part of the query (for one-entity
queries), and the total number of non-named-entity,
non-stopword words in the sentence. Our imple-
mentation uses a list of 200 common stopwords. We
trained a maximum-entropy classifier over the com-
plexity features to predict the probability that a sen-
tence is simple enough for training. We manually
labeled a small sample of 50 sentences, which were
retrieved for relations not found in any of our test
sentences. Sentences that truly expressed the rela-
tions in the logical form and no other relation were
labeled as positive, and all others were labeled neg-
ative. The Assessor uses the probability from this
classifier to rank all sentences for a relation, and se-
lects the top 100 sentences for further processing.
Complexity statistics alone are not sufficient for
selecting good training sentences. For instance,
??Being Spiderman is a dream come true,? says An-
drew Garfield? is a short sentence mentioning two
entities that participate in the acted in relation.
However, none of the words in the sentence are par-
ticularly indicative of acted in, and if FreeParser
were to use this as a training sentence, it would most
likely learn a wrong lexical entry.
The Assessor additionally weeds out sentences
which do not include words strongly associated with
a database relation. Previous work has used statisti-
cal machine translation models like IBM Model 1
(Brown et al, 1993) as a method for initially de-
termining which words should be associated with
which database symbols. After experimenting with
this and other models, as implemented in GIZA++
(Och and Ney, 2003), we have found that a simpler
procedure is more effective for finding the words
which are most indicative of a database relation.
Taking the set T of top 100 sentences for r from the
complexity ranker, we preprocess the sentences by
discarding stopwords and applying stemming. We
then count all the remaining word types v ? V
appearing in T , and rank them by frequency. We
select the top K as word stems that are highly in-
dicative of relation r; we call these word stems the
critical words for r. For example, for the relation
date founded, this technique produces the crit-
ical words ?found,? ?establish,? and ?settl,? among
others. Sentences which do not contain some vari-
ant of one of these critical words are unlikely to be
good training examples. To obtain a set of diverse
but relevant sentences, the Assessor selects at most
two sentences for each of the K critical words, tak-
ing care not to select any sentence twice. In practice
we found that using more than 2 sentences per criti-
cal word has no effect on parsing accuracy, but slows
the parser training procedure significantly. We tuned
K on development data, and set it to K = 7.
333
Example lexical entries for ?is? learned by UBL
S|NP : ?x . religion(x)
S|NP |NP : ?x?y . person(x)?
appearance type(x, newscaster)
S|NP |NP : ?x?y . brand(x, y)?
company brand relationship(x)
Table 2: Overly-specific lexical entries for the function
word ?is,? as learned by a state-of-the-art PCCG se-
mantic parser on our Freebase data set. All entries
shown have significant positive weight in the learned lex-
icon.
3.4 Initializing the Lexicon for Learning a
Semantic Parser
Existing semantic parsing technology requires some
initial knowledge in order to learn a full parser. Typ-
ically, this knowledge includes lexical entries for
named entities and the database symbols to which
they correspond, a small number of additional en-
tries for important function words, and a procedure
for initializing the weights for learned lexical en-
tries. For instance, UBL uses GIZA++ (Och and
Ney, 2003) to initialize the weight of learned lexi-
cal entries.
FreeParser includes initial lexical entries for all
named entities in our dataset, as well as 29 hand-
crafted lexical entries for the words ?who,? ?what,?
?when,? and ?where.? These helped to combat
the problem of learning a semantic parser from
small numbers of questions and large numbers of
automatically-retrieved sentences that were almost
all declarative statements rather than questions. Fol-
lowing Kwiatkowski et al, these hand-crafted lexi-
cal entries are assigned a fixed positive initial weight
of 10. We found the following procedure more effec-
tive than GIZA++ for initializing the lexicon weights
for learned lexical entries in practice: for each crit-
ical word and relation pair (v, r) in the sentences
from the Assessor, we found a maximum likelihood
estimate of P (v|r), the probability of observing the
critical word v, given that a sentence expresses the
relation r. We then created initial learned lexical en-
tries that pair v and r, with a weight equal to P (v|r).
3.5 New Learning Component for Semantic
Parsing: An Open-Domain Regularizer
Training FreeParser?s semantic parsing component
on the automatically-labeled sentences, as the sys-
tem has been described thus far, results in disap-
pointing performance. This is in large part because
the UBL semantic parser learns highly domain-
specific meanings for function words. Table 2 shows
example lexical entries learned for the word ?is?.
These types of learned meanings are the rule, not
the exception, in the existing semantic parser. For
single-domain tests, they pose no particular diffi-
culties, even though intuitively they are bad repre-
sentations of the meaning of a function word. For
open-domain semantic parsing, however, it becomes
nearly impossible to parse sentences correctly on
a new domain, if the only meanings for function
words include relations from training domains.
To overcome this problem, we devised a novel
regularization technique to encourage the parser to
learn domain-independent meanings for function
words. Unlike most of FreeParser, this technique
is specific to the log-linear CCG semantic pars-
ing technique used by Kwiatkowski et al How-
ever, similar mechanisms could potentially be de-
vised for other semantic parsing frameworks. The
Kwiatkowski et al model includes a feature func-
tion fw,l for every lexical entry mapping a wordw to
a logical form l. Our novel regularizer R(?) over the
parameters ?, which we call an open-domain regu-
larizer, penalizes parameters for lexical entries map-
ping function words to any domain-specific lambda
calculus expression. Formally, let F be a set of func-
tion words, and P a set of domain-specific predi-
cates from Freebase:
R(?) =
?
w,l
{
?2w,l if w ? F ? ?p ? P.p ? l
0 otherwise.
In our implementation, we added all relations in
Freebase that are not part of its common domain to
P , and collected a set of 282 common English func-
tion words for F .
4 Experiments
We now test FreeParser?s ability to provide semantic
parses in domains where it has seen no manually-
labeled training data. We also empirically analyze
the design decisions for FreeParser.
4.1 Experimental Setup
All of our experiments are conducted on the Free-
base dataset described in Section 2.1. To create
334
Q: What is ?Big Daddy? rated?
Movie ratings are stored as special codes in Free-
base, and are rarely observed ?as is? in text.
Q: Who is the CEO of Apple?
Wikipedia regularly uses the full form ?Chief Ex-
ecutive Officer?; no retrieved sentence had ?CEO?
together with the executive?s name and company
name.
Q: When did Jack Albertson die?
Many sentences contain ?person died on date?, but
no retrieved sentence contained the morphological
variant ?(did) die.?
Table 3: Example infeasible questions, and why
FreeParser had difficulty finding sentences in Wikipedia
that contain the relevant keywords from the question.
manually-labeled training and test sets for domain
adaptation, we divide the dataset into three groups
of nearly-equal size by placing similar domains to-
gether in the same group. No domain has ques-
tions in more than one group. We then perform 3-
fold cross-validation across these three groups. We
run FreeParser?s Sentence Retrieval Engine, Auto-
Labeler, and Assessor for all relations that appear
in our dataset, and we include the automatically-
labeled data in the training data.
4.2 Testing the Sentence Retrieval Engine
179 of the 403 questions (44%) in our dataset in-
cluded critical words which could not be found us-
ing the Sentence Retrieval Engine?s queries over
Wikipedia. Table 3 lists example infeasible ques-
tions. One obvious improvement is to open the re-
trieval engine to sentences from the Web, for greater
recall; this is an important task for future work. For
now, this is FreeParser?s biggest source of errors.
However, note that without this component, the se-
mantic parser parses none of the test data correctly.
4.3 Open-domain semantic parsing tests
We now turn to an experiment that assesses the full
FreeParser system on open-domain semantic pars-
ing. For the current experiments, we concentrate
on the 224 questions (56% of the full dataset) for
which all of the words (except named entities) could
be found in at least one of the auto-labeled sentences
returned by the Sentence Retrieval Engine. We call
these 224 questions the feasible questions. For the
remaining infeasible questions, FreeParser almost
never produces a correct logical form.
Figure 5 shows FreeParser?s performance on fea-
sible questions in all test domains, as well as for each
of the seven most-common test domains. FreeParser
performs quite well, achieving an overall F1 of
0.71, which represents a huge improvement over the
F1 of 0.0 for the supervised UBL semantic parser
in a domain adaptation setting. An unsupervised
parser, which uses only the initial lexical entries
from FreeParser and the auto-labeled training data,
achieves an F1 of 0.43. Precision and recall differ-
ences between FreeParser and these two baselines
are statistically significant (p < 0.01) using Fisher?s
exact test. Including both feasible and infeasible
questions, FreeParser?s F1 is 0.37 because of the low
recall on infeasible questions, but as more unlabeled
text becomes available to FreeParser, it should have
fewer and fewer infeasible questions.
4.4 Testing Critical Design Components
We tested FreeParser with different choices for key
parts of the design, to measure their impact. Ta-
ble 4 presents precision, recall, and F1 scores for
four variations of FreeParser, where each variation
is missing a critical component of the design. In the
first variation, the Sentence Retrieval Engine only
issues two-entity queries; it is missing the ability to
issue the less-precise single-entity queries. In the
second variation, the Assessor uses only the critical
words to select sentences for training; it is missing
the ability to rank sentences based on their complex-
ity. In the third variation, the Assessor selects the
top 2K, or 14, sentences based on the complexity
ranking; it ignores the critical words test. Finally,
the last variation shows FreeParser?s performance
when UBL?s training procedure has not been mod-
ified with the open-domain regularizer.
Deleting any one of these critical design ele-
ments substantially degrades FreeParser?s perfor-
mance, but the 1-entity queries appear to be the most
critical design choice, followed by the critical words
test and open-domain regularizer. Removing the 1-
entity queries surprisingly hurts both precision and
recall. The 2-entity queries do tend to retrieve bet-
ter sentences on average than 1-entity queries, but
because they retrieve so few, the Assessor has more
difficulty selecting good critical words.
Error analysis showed that incorrect or missing
335
.72 .68 
.29 
.82 
.93 
.57 
.89 
.27 
.70 .68 
.27 
.82 .81 
.53 
.67 
.27 
.71 .68 
.28 
.82 
.87 
.55 
.76 
.27 
.00
.20
.40
.60
.80
1.00
all (224) film (31) tv (22) location (22) medicine(16) business (15) people (12) award (11)
Open-Domain Semantic Parsing on Feasible Questions 
precision recall f1
Figure 5: FreeParser achieves an overall F1 of 0.71, in a test where every correct logical form has some element
never seen in manually-labeled training data. Results across different domains vary, but FreeParser performs well
in a variety of domains. Numbers next to domain labels indicate the number of feasible test questions. Results for
?all? domains are the micro-average across our three cross-validation folds.
Model P R F1
?1-entity queries .29 .29 .29
?complexity ranking .59 .53 .56
?critical words test .47 .41 .44
?open-domain regul. .50 .45 .47
FreeParser .72 .70 .71
Table 4: FreeParser compared with variations that are
missing critical design components. All precision and
recall differences between the full system and its varia-
tions are statistically significant (p < 0.01) using a two-
tailed Fisher?s exact test.
lexical entries for critical words were responsible for
most (68%) of the 67 incorrect or missing parses for
feasible test questions. Many of the incorrect en-
tries mapped critical words like ?directed? to related
but incorrect predicates, like written by. Miss-
ing lexical entries were often because the Assessor
incorrectly weeded out good auto-labeled examples.
The remaining 32% of the errors were mostly due to
complex syntax in the questions, or vague questions
that require significant reasoning to come up with a
valid interpretation.
5 Conclusion and Future Work
Most work on semantic parsing focuses on improv-
ing parser accuracy on a small number of relations in
a single domain. FreeParser is an exploration of the
possibility of automated semantic parsing for arbi-
trary domains. Among the lessons from our experi-
ence in designing FreeParser, these stand out: First,
finding training sentences that cover all of the dif-
ferent ways a person may refer to a database ele-
ment is difficult, and requires carefully constructed
retrieval mechanisms for sufficient recall. Second,
simple measures of sentence complexity and cooc-
currence statistics are effective techniques for iden-
tifying good training sentences. And third, standard
semantic parsing algorithms require modification for
open-domain semantic parsing, to enforce that func-
tion words are not mapped to domain-specific logi-
cal forms. We report results that help in understand-
ing FreeParser?s current strengths and weaknesses,
and that also serve as a baseline for future open-
domain semantic parsers.
Significant work remains: ideally, a system would
be able to incorporate relational data from multi-
ple schemas, and could leverage much larger cor-
pora for learning alignments. Also, FreeParser cur-
rently maps English words only to individual Free-
base symbols; more sophisticated algorithms and
representations are necessary for learning how to
map to conjunctions, disjunctions, and more com-
plex combinations of Freebase symbols.
Acknowledgements
This material is based upon work supported by the
National Science Foundation under Grant No. IIS-
1218692. We wish to thank Sophia Kohlhaas and
Ragine Williams for providing data for the project.
336
References
Yoav Artzi and Luke Zettlemoyer. 2011. Bootstrapping
Semantic Parsers from Conversations. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP).
P. F. Brown, S. D. Pietra, V. J. D. Pietra, and R. L. Mercer.
1993. The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Linguis-
tics, 19(2):263?311.
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal super-
vision. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL-
07).
David L. Chen, Joohyun Kim, and Raymond J. Mooney.
2010. Training a Multilingual Sportscaster: Using
Perceptual Context to Learn Language. Journal of Ar-
tificial Intelligence Research, 37:397?435.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with ccg and log-
linear models. Computational Linguistics, 33(4):493?
552.
J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world?s re-
sponse. In Computational Natural Language Learn-
ing (CoNLL).
Ruifang Ge and Raymond J. Mooney. 2006. Discrimina-
tive Reranking for Semantic Parsing. In Proceedings
of the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics (COLING/ACL-
06).
Ruifang Ge and Raymond J. Mooney. 2009. Learning a
Compositional Semantic Parser using an Existing Syn-
tactic Parser. In Joint Conference of the 47th Annual
Meeting of the Association for Computational Linguis-
tics and the 4th International Joint Conference on Nat-
ural Language Processing of the Asian Federation of
Natural Language Processing (ACL-IJCNLP 2009).
D. Goldwasser, R. Reichart, J. Clarke, and D. Roth.
2011. Confidence driven unsupervised semantic pars-
ing. In Association for Computational Linguistics
(ACL).
Johannes Hoffart, Fabian M. Suchanek, Klaus Berberich,
and Gerhard Weikum. 2013. YAGO2: A Spa-
tially and Temporally Enhanced Knowledge Base from
Wikipedia. Artificial Intelligence, 194:28?61, Jan-
uary.
Fei Huang, Alexander Yates, Arun Ahuja, and Doug
Downey. 2011. Language Models as Representations
for Weakly Supervised NLP Tasks. In Conference on
Computational Natural Language Learning (CoNLL).
Rohit J. Kate and Raymond J. Mooney. 2006. Using
String-Kernels for Learning Semantic Parsers. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting of
the ACL.
Rohit J. Kate and Raymond J. Mooney. 2007. Semi-
Supervised Learning for Semantic Parsing using Sup-
port Vector Machines. In Proceedings of the Human
Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, Short Papers (NAACL/HLT-2007).
Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney.
2005. Learning to Transform Natural to Formal Lan-
guages. In Proceedings of the Twentieth National
Conference on Artificial Intelligence (AAAI-05).
Jayant Krishnamurthy and Tom Mitchell. 2012. Weakly
Supervised Training of Semantic Parsers. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP).
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing Probabilis-
tic CCG Grammars from Logical Form with Higher-
order Unification. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP).
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater,
and Mark Steedman. 2011. Lexical Generalization
in CCG Grammar Induction for Semantic Parsing. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
semantic correspondences with less supervision. In
Association for Computational Linguistics and Inter-
national Joint Conference on Natural Language Pro-
cessing (ACL-IJCNLP).
P. Liang, M. I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. In Asso-
ciation for Computational Linguistics (ACL).
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-
moyer. 2008. A Generative Model for Parsing Natural
Language to Meaning Representations. In Proceed-
ings of The Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP).
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
C.A. Thompson and R.J. Mooney. 1999. Automatic con-
struction of semantic lexicons for learning natural lan-
guage interfaces. In Proc. 16th National Conference
on Artificial Intelligence (AAAI-99), pages 487?493.
Cynthia A. Thompson and Raymond J. Mooney. 2003.
Acquiring Word-Meaning Mappings for Natural Lan-
guage Interfaces. Journal of Artificial Intelligence Re-
search (JAIR), 18:1?44.
337
Y. Wilks, D. Fass, C. Guo, J. MacDonald, T. Plate, and
B. Slator. 1990. Providing Machine Tractable Dictio-
nary Tools. MIT Press.
Yuk Wah Wong and Raymond J. Mooney. 2007. Learn-
ing Synchronous Grammars for Semantic Parsing with
Lambda Calculus. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-2007).
Mohamed Yahya, Klaus Berberich, Shady Elbassuoni,
Maya Ramanath, Volker Tresp, and Gerhard Weikum.
2012. Natural Language Questions for the Web of
Data. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP).
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to Parse Database Queries using Inductive Logic
Programming. In AAAI/IAAI, pages 1050?1055.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to Map Sentences to Logical Form: Structured
Classification with Probabilistic Categorial Grammars.
In Proceedings of the Twenty First Conference on Un-
certainty in Artificial Intelligence (UAI).
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line Learning of Relaxed CCG Grammars for Parsing
to Logical Form. In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL).
Luke S. Zettlemoyer and Michael Collins. 2009. Learn-
ing Context-dependent Mappings from Sentences to
Logical Form. In Proceedings of the Joint Conference
of the Association for Computational Linguistics and
International Joint Conference on Natural Language
Processing (ACL-IJCNLP).
338
Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 23?30,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Exploring Representation-Learning Approaches to Domain Adaptation
Fei Huang and Alexander Yates
Temple University
Computer and Information Sciences
324 Wachman Hall
Philadelphia, PA 19122
{fei.huang,yates}@temple.edu
Abstract
Most supervised language processing sys-
tems show a significant drop-off in per-
formance when they are tested on text
that comes from a domain significantly
different from the domain of the training
data. Sequence labeling systems like part-
of-speech taggers are typically trained on
newswire text, and in tests their error
rate on, for example, biomedical data can
triple, or worse. We investigate techniques
for building open-domain sequence label-
ing systems that approach the ideal of a
system whose accuracy is high and con-
stant across domains. In particular, we in-
vestigate unsupervised techniques for rep-
resentation learning that provide new fea-
tures which are stable across domains, in
that they are predictive in both the train-
ing and out-of-domain test data. In exper-
iments, our novel techniques reduce error
by as much as 29% relative to the previous
state of the art on out-of-domain text.
1 Introduction
Supervised natural language processing (NLP)
systems exhibit a significant drop-off in perfor-
mance when tested on domains that differ from
their training domains. Past research in a vari-
ety of NLP tasks, like parsing (Gildea, 2001) and
chunking (Huang and Yates, 2009), has shown that
systems suffer from a drop-off in performance on
out-of-domain tests. Two separate experiments
with part-of-speech (POS) taggers trained on Wall
Street Journal (WSJ) text show that they can reach
accuracies of 97-98% on WSJ test sets, but achieve
accuracies of at most 90% on biomedical text
(R.Codena et al, 2005; Blitzer et al, 2006).
The major cause for poor performance on out-
of-domain texts is the traditional representation
used by supervised NLP systems. Most systems
depend to varying degrees on lexical features,
which tie predictions to the words observed in
each example. While such features have been used
in a variety of tasks for better in-domain perfor-
mance, they are pitfalls for out-of-domain tests for
two reasons: first, the vocabulary can differ greatly
between domains, so that important words in the
test data may never be seen in the training data.
And second, the connection between words and
labels may also change across domains. For in-
stance, ?signaling? appears only as a present par-
ticiple (VBG) in WSJ text (as in, ?signaling that
...?), but predominantly as a noun (as in ?signaling
pathway?) in biomedical text.
Representation learning is a promising new ap-
proach to discovering useful features that are sta-
ble across domains. Blitzer et al (2006) and our
previous work (2009) demonstrate novel, unsu-
pervised representation learning techniques that
produce new features for domain adaptation of a
POS tagger. This framework is attractive for sev-
eral reasons: experimentally, learned features can
yield significant improvements over standard su-
pervised models on out-of-domain tests. Since
the representation learning techniques are unsu-
pervised, they can be applied to arbitrary new do-
mains to yield the best set of features for learning
on WSJ text and predicting on the new domain.
There is no need to supply additional labeled ex-
amples for each new domain. This reduces the ef-
fort for domain adaptation, and makes it possible
to apply systems to open-domain text collections
like the Web, where it is prohibitively expensive
to collect a labeled sample that is truly representa-
tive of all domains.
Here we explore two novel directions in the
representation-learning framework for domain
adaptation. Specifically, we investigate empiri-
cally the effects of representation learning tech-
niques on POS tagging to answer the following:
23
1. Can we produce multi-dimensional represen-
tations for domain adaptation? Our previous ef-
forts have provided only a single new feature in
the learned representations. We now show how
we can perform a multi-dimensional clustering
of words such that each dimension of the clus-
tering forms a new feature in our representation;
such multi-dimensional representations dramati-
cally reduce the out-of-domain error rate of our
POS tagger from 9.5% to 6.7%.
2. Can maximum-entropy models be used to pro-
duce representations for domain adaptation? Re-
cent work on contrastive estimation (Smith and
Eisner, 2005) has shown that maximum-entropy-
based latent variable models can yield more accu-
rate clusterings for POS tagging than more tradi-
tional generative models trained with Expectation-
Maximization. Our preliminary results show that
such models can be used effectively as represen-
tations for domain adaptation as well, matching
state-of-the-art results while using far less data.
The next section provides background informa-
tion on learning representations for NLP tasks us-
ing latent-variable language models. Section 3 de-
scribes our experimental setup. In Sections 4 and
5, we empirically investigate our two questions
with a series of representation-learning methods.
Section 6 analyzes our best learned representation
to help explain its effectiveness. Section 7 presents
previous work, and Section 8 concludes and out-
lines directions for future work.
2 Open-Domain Sequence Labeling by
Learning Representations
Let X be an instance set for a learning problem;
for POS tagging, for instance, this could be the set
of all English sentences. Let Y be the space of
possible labels for an instance, and let f : X ? Z
be the target function to be learned. A represen-
tation is a function R: X ? Y , for some suitable
feature space Y (such as Rd). A domain is defined
as a distribution D over the instance set X . An
open-domain system observes a set of training ex-
amples (R(x), f(x)), where instances x ? X are
drawn from a source domain, to learn a hypothe-
sis for classifying examples drawn from a separate
target domain.
Previous work by Ben-David et al (2007) uses
Vapnik-Chervonenkis (VC) theory to show that
the choice of representation is crucial to open-
domain learning. As is customary in VC the-
ory, a good choice of representation must allow
a learning machine to achieve low error rates dur-
ing training. Just as important, however, is that
the representation must simultaneously make the
source and target domains look as similar to one
another as possible.
For open-domain sequence-labeling, then, the
traditional representations are problematic. Typ-
ical representations in NLP use functions of the
local context to produce features. Although many
previous studies have shown that such lexical
features allow learning systems to achieve im-
pressively low error rates during training, they
also make texts from different domains look very
dissimilar. For instance, a sentence containing
?bank? is almost certainly from the WSJ rather
than biomedical text; a sentence containing ?path-
way? is almost certainly from a biomedical text
rather than from the WSJ.
Our recent work (2009) shows how to build
systems that learn new representations for open-
domain NLP using latent-variable language mod-
els like Hidden Markov Models (HMMs). In POS-
tagging and chunking experiments, these learned
representations have proven to meet both of Ben-
David et al?s criteria for representations. They
help discriminate among classes of words, since
HMMs learn distributional similarity classes of
words that often correlate with the labels that need
to be predicted. Moreover, it would be difficult to
tell apart two domains based on the set of HMM
states that generated the texts, since a given HMM
state may generate words from any number of do-
mains.
In the rest of this paper, we investigate ways to
improve the predictive power of the learned rep-
resentations, without losing the essential property
that the features remain stable across domains. We
stay within the framework of using graphical mod-
els to learn representations, and demonstrate sig-
nificant improvements on our original technique.
3 Experimental Setup
We use the same experimental setup as Blitzer
et al (2006): the Penn Treebank (Marcus et al,
1993) Wall Street Journal portion for our labeled
training data; 561 MEDLINE sentences (9576
words) from the Penn BioIE project (PennBioIE,
2005) for our labeled test set; and all of the un-
labeled text from the Penn Treebank WSJ portion
plus Blitzer et al?s MEDLINE corpus of 71,306
24
unlabeled sentences to train our latent variable
models. The two texts come from two very dif-
ferent domains, making this data a tough test for
domain adaptation. 23% of the word types in the
test text are Out-Of-Vocabulary (OOV), meaning
that they are never observed in the training data.
We use a number of unsupervised representa-
tion learning techniques to discover features from
our unlabeled data, and a supervised classifier to
train on the training set annotated with learned fea-
tures. We use an open source Conditional Random
Field (CRF) (Lafferty et al, 2001) software pack-
age1 designed by Sunita Sajarwal and William W.
Cohen to implement our supervised models. We
refer to the baseline system with feature set fol-
lowing our previous work (2009) as PLAIN-CRF.
Our learned features will supplement this set.
For comparison, we also report on the perfor-
mance of Blitzer et al?s Structural Correspon-
dence Learning (SCL) (2006), our HMM-based
model (2009)(HY09), and two other baselines:
? TEST-CRF: Our baseline model, trained and
tested on the test data. This is our upper
bound.
? SELF-CRF: Following the self-training
paradigm (e.g., (McClosky et al, 2006b;
McClosky et al, 2006a)), we train our
baseline first on the training set, then apply it
to the test set, then retrain it on the training
set plus the automatically labeled test set.
We perform only one iteration of retraining,
although in general multiple iterations are
possible, usually with diminishing marginal
returns.
4 Multi-dimensional Representations
From a linguistic perspective, words are multi-
dimensional objects. For instance, the word ?we?
in ?We like doing domain adaptation research? is a
pronoun, a subject, first person, and plural, among
other things. Each of these properties is a sepa-
rate feature of this word, which can be changed
without changing the other features. For exam-
ple, if ?we? is changed to ?they? in the previ-
ous example, it is exactly the same as ?we? in
all aspects, except that it is third person; if ?we?
is changed to ?us?, then it changes from subject
case to object case. In morphologically rich lan-
guages, many syntactic distinctions are marked in
1Available from http://sourceforge.net/projects/crf/
the surface forms of words; in more analytic or
isolating languages like English, the distinctions
are still there, but must often be inferred from con-
text rather than word form. Beyond syntactic di-
mensions, numerous semantic properties can also
distinguish words, such as nouns that refer to cog-
nitive agents versus nouns that refer to materials
and tools.
We seek to learn multidimensional representa-
tions of words. Our HMM-based model is able to
categorize words in one dimension, by assigning
a single HMM latent state to each word. Since
the HMM is trained on unlabeled data, this di-
mension may partially reflect POS categories, but
more likely represents a mixture of many different
word dimensions. By adding in multiple hidden
layers to our sequence model, we aim to learn a
multi-dimensional representation that may help us
to capture word features from multiple perspec-
tives. The supervised CRF system can then sort
out which dimensions are relevant to the sequence-
labeling task at hand.
A Factorial HMM (FHMM) can be used to
model multiple hidden dimensions of a word.
However, the memory requirements of an FHMM
increase exponentially with the number of lay-
ers in the graphical model, making it hard to use
(see Table 1). Although other parameterizations
may require much less memory, like using a log-
linear output distribution conditioned on the fac-
tors, exact inference is still computationally in-
tractable; exploring FHMMs with approximate in-
ference and learning is an interesting area for fu-
ture work. Here, we choose to create several
single-layer HMMs separately. Figure 1 shows
our Independent-HMM model (I-HMM). I-HMM
has several copies of the observation sequence and
each copy is associated with its own hidden label
sequence. To encourage each layer of the I-HMM
model to find a different local maximum in pa-
rameter space during training (and thus a different
model of the observation sequence), we initialize
the parameters randomly.
Suppose there are L independent layers in an I-
HMM model for corpus x = (x1, . . . , xN ), and
each layer is (yl1,yl2,...ylN ), where l = 1...L and
each y can have K states. The distribution of the
corpus and one hidden layer l is
P (x,yl) =
?
i
P (xi|yli)P (yli|yli?1)
For each layer l, for each position i, each HMM
25
XN
?
?
?
XN
X2 XN
X1 X2
X1
X1
X2
Y11
Y12
Y1L
Y21
Y22
Y2L YNL
YN2
YN1
Figure 1: Graphical models of an Independent Hidden
Markov Model. The dash line rectangle indicates that they
are copies of the observation sequence
Model Number of Memorylayers words states
HMM 1 W K O(WK + K2)
FHMM L W K O(WKL + LK2)
I-HMM L W K O(WKL + LK2)
Table 1: The memory requirement for HMM, FHMM, and
I-HMM models.
state y and each POS tag z, we add a new boolean
feature to our CRF system that indicates whether
Y li = y and Zi=z.
We experiment with two versions of I-HMM:
first, we fix the number of states in each layer at
80 states, and increase the number of HMM lay-
ers from 1 to 8 (I-HMM(80)). Second, to provide
greater encouragement for each layer to represent
separate information, we vary the number of states
in each layer (I-HMM(vary)). The detailed config-
uration for this model is shown in Table 2.
The results for our two models are shown in Fig-
ure 2. We can see that the accuracy of I-HMM(80)
model keeps increasing from 90.5% to 93.3% until
7 layers of HMM features (we call this 7-layer rep-
resentation I-HMM*). This is a dramatic 29% de-
crease in the best reported error rate for this dataset
when no labeled data from the biomedical domain
is used. Unlike with an FHMM, there is no guar-
antee that the different layers of an I-HMM will
model different aspects of the observation signal,
but our results indicate that for at least several lay-
ers, the induced models are complementary. After
7 layers, results begin to decrease, most likely be-
cause the added layer is no longer complementary
to the existing latent-variable models and is caus-
ing the supervised CRF to overfit the training data.
For the I-HMM(vary) model with up to 5 lay-
Number Number of States
of Layers in each Layer
1 10
2 10 20
3 10 20 40
4 10 20 40 60
5 10 20 40 60 80
Table 2: The configuration of HMM layers and HMM states
for the I-HMM(vary) model
86
87
88
89
90
91
92
93
94
1 2 3 4 5 6 7 8
Ac
cu
ra
cy
Number of HMM layers
Accuracy on different number of 
HMM layers
I-HMM(80)
I-HMM(vary)
HY09(90.5%)
Figure 2: Our best multi-dimensional smoothed-HMM tag-
ger with 7 layers reaches 93.3% accuracy, a drop of nearly 3%
in the error rate from the previous state of the art (HY09).
ers, the accuracy is not as good as I-HMM(80), al-
though the 5-layer model still outperforms HY09.
Individually, HMM models with fewer than 80
states perform worse than the 80-state model (a
model with 40 states achieved 89.4% accuracy,
and a model with 20 states achieved 88.9%). We
had hoped that by using layers with different
numbers of states, we could force the layers to
learn complementary models, but the results indi-
cate that any benefit from complementarity is out-
weighed by the lower performance of the individ-
ual layers.
5 Learning Representations with
Contrastive Estimation
In recent years, many NLP practitioners have be-
gun using discriminative models, and especially
maximum-entropy-based models like CRFs, be-
cause they allow the modeler to incorporate ar-
bitrary, interacting features of the observation se-
quence while still providing tractable inference.
To see if the same benefit can carry over to our rep-
resentation learning, we aim to build maximum-
entropy-based linear-chain models that, unlike
26
most discriminative models, train on unannotated
data. We follow Smith and Eisner (2005) in
training our models using a technique called con-
trastive estimation, which we explain below. We
call the resulting model the Smith and Eisner
Model (SEM).
The key to SEM is that the contrastive estima-
tion training procedure forces the model to explain
why the given training data are better than per-
turbed versions of the data, called neighbor points.
For example, the sentence ?We like doing domain
adaptation research? is a valid sentence, but if we
switched ?like? and ?doing?, the new sentence
?We doing like domain adaptation research? is not
valid. SEM learns a model of the original sen-
tence by contrasting it with the invalid neighbor
sentences.
Let ~x =< x1, x2, ..., xN > be the observed ex-
ample sentences, and let Y be the space of possible
hidden structures for xi. Let N (xi) be a ?neigh-
borhood? for xi, or a set of negative examples ob-
tained by perturbing xi, plus xi itself. Given a vec-
tor of feature functions ~f(x, y), SEM tries to find
a set of weights ~? that maximize a log-likelihood
function:
LN (~?) = log
?
i
?
y?Y u(xi, y|~?)
?
(x,y)?N (xi)?Y u(x, y|~?)
where u(x, y|~?) = exp(~? ? ~f(x, y)) is the ?un-
normalized probability? of an (example, hidden
structure) pair (x,y). Following Smith and Eisner,
we use the best performing neighborhood, called
TRANS1, to conduct our experiments. TRANS1
is the set of sentences resulting from transposing
any pair of adjacent words for any given training
example.
The base feature space for SEM includes two
kinds of boolean features analogous to HMM
emission and transition probabilities. For an ob-
servation sequence x1, . . . , xT and a label se-
quence y1, . . . , yT , a boolean emission feature in-
dicates whether xt = x and yt = y for all possible
t, x, and y. A boolean transition feature indicates
whether yt?1 = y and yt = y? for all possible t, y,
and y?.
Because contrastive estimation is a computa-
tionally expensive training procedure, we take two
steps to reduce the computational cost: we reduce
the unlabeled data set, and we prune the feature
set of SEM. For our training data, we use only the
sentences with length less than or equal to 10. We
also get rid of punctuation and the corresponding
tags, change all words to lowercase and change all
numbers into a single symbol.
To reduce the feature space, we create a tag-
ging dictionary from Penn Treebank sections 02-
21: for every word in these sections, the dictionary
records the set of POS tags that were ever asso-
ciated with that word. We then prune the emis-
sion features for words that appear in this dic-
tionary to include only the features that associate
words with their corresponding POS tags in the
dictionary. For the words that don?t appear in the
Penn Treebank, they are associated with all pos-
sible POS tags. This procedure reduces the total
number of features in our SEM model from over
500,000 to just over 60,000.
After we train the model, we use a Viterbi-like
algorithm to decode it on the testing set. Unlike
the HMM model, the decoded states of SEM are
already meaningful POS tags, so we can use these
decoded states as POS tags (PLAIN-SEM), or use
them as features for a CRF model (SEM-CRF).
We show the result of both models, as well as
several comparison models, in Table 3. From the
result, we can see that the unsupervised PLAIN-
SEM outperforms the supervised PLAIN-CRF on
both all words and OOV words. This impres-
sive performance results from its ability to adapt
to the new domain through the unlabeled train-
ing examples and the contrastive estimation train-
ing procedure. In addition, the SEM-CRF model
significantly outperforms the SCL model (88.9%)
and the HMM-based CRF with 40 hidden states
(89.4%) while using only 36 hidden states, al-
though it does not quite reach the performance
of HY09. These results, which use a subset of
the available unlabeled training text, suggest that
maximum-entropy-style representation learning is
a promising area for further investigation.
6 Analysis
As we mention in Section 2, the choice of repre-
sentation is crucial to open-domain learning. In
Sections 4 and 5, we demonstrate empirically that
learned representations based on latent-variable
graphical models can significantly improve the ac-
curacy of a POS tagger on a new domain, com-
pared with using the traditional word-level repre-
sentations. We now examine our best representa-
tion, I-HMM*, in light of the theoretical predic-
tions made by VC theory.
27
All OOV
Model words words
PLAIN-CRF 88.3 67.3
SELF-CRF 88.5 70.4
PLAIN-SEM 88.5 69.8
SCL 88.9 72.0
SEM-CRF 90.0 71.9
HY09 90.5 75.2
I-HMM* 93.3 76.3
TEST-CRF 98.9 NA
Table 3: SEM-CRF reduces error compared with
SCL by 1.1% on all words; I-HMM* closes 33%
of the gap between the state-of-the-art HY09 and
the upper-bound, TEST-CRF.
In particular, Ben-David et al?s analysis shows
that the distance between two domains under a
representation R of the data is crucial to domain
adaptation. However, their analysis depends on
a particular notion of distance, the H-divergence,
that is computationally intractable to calculate.
For our analysis, we resort instead to a crude
but telling approximation of this measure, using a
more standard notion of distance: Jensen-Shannon
Divergence (DJS).
To calculate the distance between domains un-
der a representation R, we represent a domain D
as a multinomial probability distribution over the
set of features in R. We take maximum-likelihood
estimates of this distribution using our samples
from the WSJ and MEDLINE domains. We then
measure the Jensen-Shannon Divergence between
the two distributions, which for discrete distribu-
tions is calculated as
DJS(p||q) =
1
2
?
i
[
pilog
(
pi
mi
)
+ qilog
(
qi
mi
)]
where m = p+q2 .
Figure 3 shows the divergence between these
two domains under purely lexical features, and un-
der only HMM-based features. OOV words make
up a substantial portion of the divergence between
the two domains under the lexical representation,
but even if we ignore them the HMM features are
substantially less variable across the two domains,
which helps to explain their ability to provide su-
pervised classifiers with stable features for domain
adaptation. Because there are so few HMM states
compared with the number of word types, there is
no such thing as an OOV HMM state, and the word
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Words I-HMM* States
Di
st
an
ce
 b
et
w
ee
n 
Do
m
ai
ns
OOV
Non-OOV
Figure 3: The Jensen-Shannon Divergence be-
tween the newswire domain and the biomedical
domain, according to a word-based representation
of the domains and a HMM-based representation.
The portion of the distance that is due to words
which appear in the biomedical domain but not the
newswire domain is shown in gray.
states that appear in training data appear roughly
as often in test data. This means that any asso-
ciations that the CRF might learn between HMM
states and predicted outcomes is likely to remain
useful on the test data, but associations between
words and outcomes are less likely to be useful.
7 Previous Work
Previous work on artificial neural networks
(ANNs) (Fahlman and Lebiere, 1990) has shown
that it is possible to learn effectively by adding
more hidden units to the neural network that cor-
relate with the residual error of the existing hidden
units (Cascade-Correlation learning). Like our I-
HMM technique, this work aims to build a multi-
dimensional model, and it is capable of learning
the number of appropriate dimensions. Unlike
the ANN scenario, our multi-dimensional learn-
ing techniques must handle unlabeled data, and
they rely on the sequential structure of language
to learn effectively, whereas Cascade-Correlation
learning assumes samples are independent and
identically distributed. Our techniques do not (yet)
automatically determine the best number of layers
in the model.
Unlike our techniques for domain adaptation, in
most cases researchers have focused on the sce-
nario where labeled training data is available in
both the source and the target domain (e.g., (Bac-
chiani et al, 2006; Daume? III, 2007; Chelba and
28
Acero, 2004; Daume? III and Marcu, 2006; Blitzer
et al, 2007)). Our techniques use only raw text
from the target domain. This reduces the cost
of domain adaptation and makes the techniques
more widely applicable to new domains like web
processing, where the domain and vocabulary is
highly variable, and it is extremely difficult to
obtain labeled data that is representative of the
test distribution. When labeled target-domain data
is available, instance weighting and similar tech-
niques can potentially be used in combination with
our techniques to improve our results further.
Several researchers have previously studied
methods for using unlabeled data for sequence la-
beling, either alone or as a supplement to labeled
data. Ando and Zhang develop a semi-supervised
chunker that outperforms purely supervised ap-
proaches on the CoNLL 2000 dataset (Ando and
Zhang, 2005). Recent projects in semi-supervised
(Toutanova and Johnson, 2007) and unsupervised
(Biemann et al, 2007; Smith and Eisner, 2005)
tagging also show significant progress. HMMs
have been used many times for POS tagging in
supervised, semi-supervised, and in unsupervised
settings (Banko and Moore, 2004; Goldwater and
Griffiths, 2007; Johnson, 2007). The REALM sys-
tem for sparse information extraction has also used
unsupervised HMMs to help determine whether
the arguments of a candidate relation are of the
appropriate type (Downey et al, 2007). Schu?tze
(1994) has presented an algorithm that categorizes
word tokens in context instead of word types for
tagging words. We take a novel perspective on the
use of unsupervised latent-variable models by us-
ing them to compute features of each token that
represent the distribution over that token?s con-
texts. These features prove to be highly useful
for supervised sequence labelers in out-of-domain
tests.
In the deep learning (Bengio, 2009) paradigm,
researchers have investigated multi-layer latent-
variable models for language modeling, among
other tasks. While n-gram models have tradition-
ally dominated in language modeling, two recent
efforts develop latent-variable probabilistic mod-
els that rival and even surpass n-gram models in
accuracy (Blitzer et al, 2005; Mnih and Hinton,
2007). Several authors investigate neural network
models that learn a vector of latent variables to
represent each word (Bengio et al, 2003; Emami
et al, 2003; Morin and Bengio, 2005). And facto-
rial Hidden Markov Models (Ghahramani and Jor-
dan, 1997) are a multi-layer variant of the HMM
that has been used in speech recognition, among
other things. We use simpler mixtures of single-
layer models for the sake of memory-efficiency,
and we use our models as representations in a su-
pervised task, rather than as language models.
8 Conclusion and Future Work
Our representation learning approach to domain
adaptation yields state-of-the-art results in POS
tagging experiments. Our best models use multi-
dimensional clustering to find several latent cate-
gories for each word; the latent categories serve
as useful and domain-independent features for
our supervised learner. Our exploration has
yielded significant progress already, but it has only
scratched the surface of possible models for this
task. The current representation learning tech-
niques we use are unsupervised, meaning that they
provide the same set of categories, regardless of
what task they are to be used for. Semi-supervised
learning approaches could be developed to guide
the representation learning process towards fea-
tures that are best-suited for a particular task, but
are still useful across domains. Our current ap-
proach also requires retraining of a CRF for every
new domain; incremental retraining techniques for
new domains would speed up the process and
make domain adaptation much more accessible.
Finally, there are cases where small amounts of la-
beled data are available for new domains; models
that combine our representation learning approach
with instance weighting and other forms of super-
vised domain adaptation may take better advan-
tage of these cases.
Acknowledgments
We wish to thank the anonymous reviewers for
their helpful comments and suggestions.
References
Rie Kubota Ando and Tong Zhang. 2005. A high-
performance semi-supervised learning method for
text chunking. In ACL.
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41?68.
Michele Banko and Robert C. Moore. 2004. Part of
speech tagging in context. In COLING.
29
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2007. Analysis of representations
for domain adaptation. In Advances in Neural In-
formation Processing Systems 20, Cambridge, MA.
MIT Press.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
Y. Bengio. 2009. Learning deep architectures for AI.
Foundations and Trends in Machine Learning, 2.
C. Biemann, C. Giuliano, and A. Gliozzo. 2007. Un-
supervised pos tagging supporting supervised meth-
ods. Proceeding of RANLP-07.
J. Blitzer, A. Globerson, and F. Pereira. 2005. Dis-
tributed latent variable models of lexical cooccur-
rences. In Proceedings of the Tenth International
Workshop on Artificial Intelligence and Statistics.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
John Blitzer, Koby Crammer, Alex Kulesza, Fernando
Pereira, and Jenn Wortman. 2007. Learning bounds
for domain adaptation. In Advances in Neural Infor-
mation Processing Systems.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy classifier: Little data can help a
lot. In EMNLP.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In ACL.
Doug Downey, Stefan Schoenmackers, and Oren Et-
zioni. 2007. Sparse information extraction: Unsu-
pervised language models to the rescue. In ACL.
A. Emami, P. Xu, and F. Jelinek. 2003. Using a
connectionist model in a syntactical based language
model. In Proceedings of the International Confer-
ence on Spoken Language Processing, pages 372?
375.
Scott E. Fahlman and Christian Lebiere. 1990. The
cascade-correlation learning architecture. Advances
in Neural Information Processing Systems 2.
Zoubin Ghahramani and Michael I. Jordan. 1997. Fac-
torial hidden markov models. Machine Learning,
29(2-3):245?273.
Daniel Gildea. 2001. Corpus Variation and Parser Per-
formance. In Conference on Empirical Methods in
Natural Language Processing.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully bayesian approach to unsupervised part-of-
speech tagging. In ACL.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised
sequence labeling. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers. In EMNLP.
J. Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data.
In Proceedings of the International Conference on
Machine Learning.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn Treebank. Com-
putational Linguistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006a. Effective self-training for parsing. In
Proc. of HLT-NAACL.
David McClosky, Eugene Charniak, and Mark John-
son. 2006b. Reranking and self-training for parser
adaptation. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the ACL, pages 337?344.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th International Conference
on Machine Learning, pages 641?648, New York,
NY, USA. ACM.
F. Morin and Y. Bengio. 2005. Hierarchical probabilis-
tic neural network language model. In Proceedings
of the International Workshop on Artificial Intelli-
gence and Statistics, pages 246?252.
PennBioIE. 2005. Mining the bibliome project.
http://bioie.ldc.upenn.edu/.
Anni R.Codena, Serguei V.Pakhomovb, Rie K.Andoa,
Patrick H.Duffyb, and Christopher G.Chute. 2005.
Domain-specific language models and lexicons
for tagging. Journal of Biomedical Informatics,
38(6):422?430.
Hinrich Schu?tze. 1994. Distributional part-of-speech
tagging. In Proceedings of the 7th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL).
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 354?362, Ann Arbor, Michigan, June.
Kristina Toutanova and Mark Johnson. 2007. A
bayesian LDA-based model for semi-supervised
part-of-speech tagging. In NIPS.
30
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 125?134,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Language Models as Representations for Weakly-Supervised NLP Tasks
Fei Huang and Alexander Yates
Temple University
Broad St. and Montgomery Ave.
Philadelphia, PA 19122
fei.huang@temple.edu
yates@temple.edu
Arun Ahuja and Doug Downey
Northwestern University
2133 Sheridan Road
Evanston, IL 60208
a-ahuja@northwestern.edu
ddowney@eecs.northwestern.edu
Abstract
Finding the right representation for words is
critical for building accurate NLP systems
when domain-specific labeled data for the
task is scarce. This paper investigates lan-
guage model representations, in which lan-
guage models trained on unlabeled corpora
are used to generate real-valued feature vec-
tors for words. We investigate ngram mod-
els and probabilistic graphical models, includ-
ing a novel lattice-structured Markov Random
Field. Experiments indicate that language
model representations outperform traditional
representations, and that graphical model rep-
resentations outperform ngram models, espe-
cially on sparse and polysemous words.
1 Introduction
NLP systems often rely on hand-crafted, carefully
engineered sets of features to achieve strong perfor-
mance. Thus, a part-of-speech (POS) tagger would
traditionally use a feature like, ?the previous token
is the? to help classify a given token as a noun or
adjective. For supervised NLP tasks with sufficient
domain-specific training data, these traditional fea-
tures yield state-of-the-art results. However, NLP
systems are increasingly being applied to texts like
the Web, scientific domains, and personal commu-
nications like emails, all of which have very differ-
ent characteristics from traditional training corpora.
Collecting labeled training data for each new target
domain is typically prohibitively expensive. We in-
vestigate representations that can be applied when
domain-specific labeled training data is scarce.
An increasing body of theoretical and empirical
evidence suggests that traditional, manually-crafted
features limit systems? performance in this setting
for two reasons. First, feature sparsity prevents sys-
tems from generalizing accurately to words and fea-
tures not seen during training. Because word fre-
quencies are Zipf distributed, this often means that
there is little relevant training data for a substantial
fraction of parameters (Bikel, 2004), especially in
new domains (Huang and Yates, 2009). For exam-
ple, word-type features form the backbone of most
POS-tagging systems, but types like ?gene? and
?pathway? show up frequently in biomedical liter-
ature, and rarely in newswire text. Thus, a classifier
trained on newswire data and tested on biomedical
data will have seen few training examples related to
sentences with features ?gene? and ?pathway? (Ben-
David et al, 2009; Blitzer et al, 2006).
Further, because words are polysemous, word-
type features prevent systems from generalizing to
situations in which words have different meanings.
For instance, the word type ?signaling? appears pri-
marily as a present participle (VBG) in Wall Street
Journal (WSJ) text, as in, ?Interest rates rose, sig-
naling that . . . ? (Marcus et al, 1993). In biomedical
text, however, ?signaling? appears primarily in the
phrase ?signaling pathway,? where it is considered
a noun (NN) (PennBioIE, 2005); this phrase never
appears in the WSJ portion of the Penn Treebank
(Huang and Yates, 2010a).
Our response to these problems with traditional
NLP representations is to seek new representations
that allow systems to generalize more accurately to
previously unseen examples. Our approach depends
on the well-known distributional hypothesis, which
states that a word?s meaning is identified with the
contexts in which it appears (Harris, 1954; Hin-
dle, 1990). Our goal is to develop probabilistic lan-
125
guage models that describe the contexts of individ-
ual words accurately. We then construct represen-
tations, or mappings from word tokens and types
to real-valued vectors, from these language models.
Since the language models are designed to model
words? contexts, the features they produce can be
used to combat problems with polysemy. And by
careful design of the language models, we can limit
the number of features that they produce, controlling
how sparse those features are in training data.
In this paper, we analyze the performance
of language-model-based representations on tasks
where domain-specific training data is scarce. Our
contributions are as follows:
1. We introduce a novel factorial graphical model
representation, a Partial-Lattice Markov Random
Field (PL-MRF), which is a tractable variation of
a Factorial Hidden Markov Model (HMM) for lan-
guage modeling.
2. In experiments on POS tagging in a domain adap-
tation setting and on weakly-supervised informa-
tion extraction (IE), we quantify the performance of
representations derived from language models. We
show that graphical models outperform ngram rep-
resentations. The PL-MRF representation achieves a
state-of-the-art 93.8% accuracy on the POS tagging
task, while the HMM representation improves over
the ngram model by 10% on the IE task.
3. We analyze how the performance of the different
representations varies due to the fundamental chal-
lenges of sparsity and polysemy.
The next section discusses previous work. Sec-
tions 3 and 4 present the existing representations we
investigate and the new PL-MRF, respectively. Sec-
tions 5 and 6 describe our two tasks and the results
of using our representations on each of them. Sec-
tion 7 concludes.
2 Previous Work
There is a long tradition of NLP research on rep-
resentations, mostly falling into one of four cate-
gories: 1) vector space models of meaning based
on document-level lexical cooccurrence statistics
(Salton and McGill, 1983; Turney and Pantel, 2010;
Sahlgren, 2006); 2) dimensionality reduction tech-
niques for vector space models (Deerwester et al,
1990; Honkela, 1997; Kaski, 1998; Sahlgren, 2005;
Blei et al, 2003; Va?yrynen et al, 2007); 3) using
clusters that are induced from distributional similar-
ity (Brown et al, 1992; Pereira et al, 1993; Mar-
tin et al, 1998) as non-sparse features (Lin and Wu,
2009; Candito and Crabbe, 2009; Koo et al, 2008;
Zhao et al, 2009); 4) and recently, language models
(Bengio, 2008; Mnih and Hinton, 2009) as represen-
tations (Weston et al, 2008; Collobert and Weston,
2008; Bengio et al, 2009), some of which have al-
ready yielded state of the art performance on domain
adaptation tasks (Huang and Yates, 2009; Huang and
Yates, 2010a; Huang and Yates, 2010b; Turian et al,
2010) and IE (Ahuja and Downey, 2010; Downey et
al., 2007b). In contrast to this previous work, we de-
velop a novel Partial Lattice MRF language model
that incorporates a factorial representation of latent
states, and demonstrate that it outperforms the pre-
vious state-of-the-art in POS tagging in a domain
adaptation setting. We also analyze the novel PL-
MRF representation on an IE task, and several repre-
sentations along the key dimensions of sparsity and
polysemy.
Most previous work on domain adaptation has fo-
cused on the case where some labeled data is avail-
able in both the source and target domains (Daume?
III, 2007; Jiang and Zhai, 2007; Daume? III and
Marcu, 2006; Finkel and Manning, 2009; Dredze
et al, 2010; Dredze and Crammer, 2008). Learn-
ing bounds are known (Blitzer et al, 2007; Man-
sour et al, 2009). Daume? III et al (2010) use semi-
supervised learning to incorporate labeled and unla-
beled data from the target domain. In contrast, we
investigate a domain adaptation setting where no la-
beled data is available for the target domain.
3 Representations
A representation is a set of features that describe
instances for a classifier. Formally, let X be an
instance set, and let Z be the set of labels for a
classification task. A representation is a function
R : X ? Y for some suitable feature space Y (such
as Rd). We refer to dimensions of Y as features, and
for an instance x ? X we refer to values for partic-
ular dimensions of R(x) as features of x.
3.1 Traditional POS-Tagging Representations
As a baseline for POS tagging experiments and an
example of our terminology, we describe a repre-
sentation used in traditional supervised POS taggers.
The instance set X is the set of English sentences,
and Z is the set of POS tag sequences. A traditional
representation TRAD-R maps a sentence x ? X to a
sequence of boolean-valued vectors, one vector per
126
Representation Feature
TRAD-R ?w1[xi = w]
?s?Suffixes1[xi ends with s]
1[xi contains a digit]
NGRAM-R ?w?,w??P (w?ww??)/P (w)
HMM-TOKEN-R ?k1[yi? = k]
HMM-TYPE-R ?kP (y = k|x = w)
I-HMM-TOKEN-R ?j,k1[yi,j? = k]
BROWN-TOKEN-R ?j?{?2,?1,0,1,2}
?p?{4,6,10,20} prefix(yi+j , p)
BROWN-TYPE-R ?p prefix(y, p)
LATTICE-TOKEN-R ?j,k1[yi,j? = k]
LATTICE-TYPE-R ?kP (y = k|x = w)
Table 1: Summary of features provided by our repre-
sentations. ?a1[g(a)] represents a set of boolean fea-
tures, one for each value of a, where the feature is
true iff g(a) is true. xi represents a token at position
i in sentence x, w represents a word type, Suffixes =
{-ing,-ogy,-ed,-s,-ly,-ion,-tion,-ity}, k (and k) represents
a value for a latent state (set of latent states) in a latent-
variable model, y? represents the optimal setting of latent
states y for x, yi is the latent variable for xi, and yi,j is
the latent variable for xi at layer j. prefix(y,p) is the p-
length prefix of the Brown cluster y.
word xi in the sentence. Dimensions for each latent
vector include indicators for the word type of xi and
various orthographic features. Table 1 presents the
full list of features in TRAD-R. Since our IE task
classifies word types rather than tokens, this base-
line is not appropriate for that task. Below, we de-
scribe how we can learn representations R by using
a variety of language models, for use in both our IE
and POS tagging tasks. All representations for POS
tagging inherit the features from TRAD-R; all repre-
sentations for IE do not.
3.2 Ngram Representations
N-gram representations model a word type w in
terms of the n-gram contexts in which w appears
in a corpus. Specifically, for word w we generate
the vector P (w?ww??)/P (w), the conditional prob-
ability of observing the word sequence w? to the left
and w?? to the right of w. The experimental section
describes the particular corpora and language mod-
eling methods used for estimating probabilities.
3.3 HMM-based Representations
In previous work, we have implemented several
representations based on HMMs (Rabiner, 1989),
which we used for both POS tagging (Huang and
Yates, 2009) and IE (Downey et al, 2007b). An
HMM is a generative probabilistic model that gen-
erates each word xi in the corpus conditioned on a
latent variable yi. Each yi in the model takes on in-
tegral values from 1 to K, and each one is generated
by the latent variable for the preceding word, yi?1.
The joint distribution for a corpus x = (x1, . . . , xN )
and a set of state vectors y = (y1, . . . , yN ) is
given by: P (x,y) = ?i P (xi|yi)P (yi|yi?1). Us-
ing Expectation-Maximization (EM) (Dempster et
al., 1977), it is possible to estimate the distributions
for P (xi|yi) and P (yi|yi?1) from unlabeled data.
We construct two different representations from
HMMs, one for POS tagging and one for IE. For
POS tagging, we use the Viterbi algorithm to pro-
duce the optimal setting y? of the latent states for a
given sentence x, or y? = arg maxy P (x,y). We
use the value of yi? as a new feature for xi that repre-
sents a cluster of distributionally-similar words. For
IE, we require features for word types w, rather than
tokens xi. We use the K-dimensional vector that
represents the distribution P (y|x = w) as the fea-
ture vector for word type w. This set of features
represents a ?soft clustering? of w into K different
clusters. We refer to these representations as HMM-
TOKEN-R and HMM-TYPE-R, respectively.
Because HMM-based representations offer a
small number of discrete states as features, they have
a much greater potential to combat feature sparsity
than do ngram models. Furthermore, for token-
based representations, these models can potentially
handle polysemy better than ngram language models
by providing different features in different contexts.
We also compare against a variation of the HMM
from our previous work (Huang and Yates, 2010a),
henceforth HY10. This model independently trains
M separate HMM models on the same corpus, ini-
tializing each one randomly. We can then use the
Viterbi-optimal decoded latent state of each inde-
pendent HMM model as a separate feature for a to-
ken. We refer to this language model as an I-HMM,
and the representation as I-HMM-TOKEN-R.
Finally, we compare against Brown clusters
(Brown et al, 1992) as learned features. Although
not traditionally described as such, Brown cluster-
ing involves constructing an HMM model in which
127
each type is restricted to having exactly one latent
state that may generate it. Brown et al describe a
greedy agglomerative clustering algorithm for train-
ing this model on unlabeled text. Following Turian
et al (2010), we use Percy Liang?s implementation
of this algorithm for our comparison, and we test
runs with 100, 320, and 1000 clusters. We use fea-
tures from these clusters identical to Turian et al?s.1
Turian et al have shown that Brown clusters match
or exceed the performance of neural network-based
language models in domain adaptation experiments
for named-entity recognition, as well as in-domain
experiments for NER and chunking.
4 A Novel Lattice Language Model
Representation
Our final language model is a novel latent-variable
language model with rich latent structure, shown in
Figure 1. The model contains a lattice of M ?N la-
tent states, where N is the number of words in a sen-
tence and M is the number of layers in the model.
We can justify the choice of this model from a lin-
guistic perspective as a way to capture the multi-
dimensional nature of words. Linguists have long
argued that words have many different features in a
high dimensional space: they can be separately de-
scribed by part of speech, gender, number, case, per-
son, tense, voice, aspect, mass vs. count, and a host
of semantic categories (agency, animate vs. inani-
mate, physical vs. abstract, etc.), to name a few (Sag
et al, 2003). Our model seeks to capture a multi-
dimensional representation of words by creating a
separate layer of latent variables for each dimension.
The values of the M layers of latent variables for a
single word can be used as M distinct features in
our representation. The I-HMM attempts to model
the same intuition, but unlike a lattice model the I-
HMM layers are entirely independent, and as a re-
sult there is no mechanism to enforce that the layers
model different dimensions. Duh (2005) previously
used a 2-layer lattice for tagging and chunking, but
in a supervised setting rather than for representation
learning.
Let Cliq(x,y) represent the set of all maximal
cliques in the graph of the MRF model for x and y.
1Percy Liang?s implementation is available at
http://metaoptimize.com/projects/wordreprs/. Turian et al
also tested a run with 3200 clusters in their experiments, which
we have been training for months, but which has not finished in
time for publication.
y4,1
y3,1
y
y4,2
y3,2
y
y4,3
y3,3
y
y4,4
y3,4
y
y4,5
y3,5
y
x1
2,1
y1,1
x2
2,2
y1,2
x3
2,3
y1,3
x4
2,4
y1,4
x5
2,5
y1,5
Figure 1: The Partial Lattice MRF (PL-MRF) Model for a
5-word sentence and a 4-layer lattice. Dashed gray edges
are part of a full lattice, but not the PL-MRF.
Expressing the lattice model in log-linear form, we
can write the marginal probability P (x) of a given
sentence x as:
?
y
?
c?Cliq(x,y) score(c,x,y)
?
x?,y?
?
c?Cliq(x?,y?) score(c,x?,y?)
where score(c,x,y) = exp(?c ? fc(xc,yc)). Our
model includes parameters for transitions between
two adjacent latent variables on layer j: ?transi,s,i+1,s?,j
for yi,j = s and yi+1,j = s?. It also includes obser-
vation parameters for latent variables and tokens, as
well as for pairs of adjacent latent variables in differ-
ent layers and their tokens: ?obsi,j,s,w and ?obsi,j,s,j+1,s?,w
for yi,j = s, yi,j+1 = s?, and xi = w.
Computationally, the lattice MRF is preferable to
a na??ve Factorial HMM (Ghahramani and Jordan,
1997) representation, which would require O(2M )
parameters for an M -layer model. However, ex-
act training and inference in supervised settings are
still intractable for this model (Sutton et al, 2007),
and thus it has not yet been explored as a language
model, which requires even more difficult, unsuper-
vised training. Training is intractable in part because
of the difficulty in enumerating and summing over
the exponentially-many configurations y for a given
x. We address this difficulty in two ways: by modi-
fying the model, and by modifying the training pro-
cedure.
4.1 Partial Lattice MRF
Instead of the full lattice model, we construct a
Partial Lattice MRF (PL-MRF) model by deleting
128
certain edges between latent layers of the model
(dashed gray edges in Figure 1). Let c = bN2 c,
where N is the length of the sentence. If i < c
and j is odd, or if j is even and i > c, we delete
edges between yi,j and yi,j+1. The same lattice of
nodes remains, but fewer edges and paths. A cen-
tral ?trunk? at i = c connects all layers of the lat-
tice, and branches from this trunk connect either to
the branches in the layer above or the layer below
(but not both). The result is a model that retains
most2 of the edges of the full model. Additionally,
the pruned model makes the branches conditionally
independent from one another, except through the
trunk. For instance, the right branch at layers 1
and 2 in Figure 1 (y1,4, y1,5, y2,4, and y2,5) are dis-
connected from the right branch at layers 3 and 4
(y3,4, y3,5, y4,4, and y4,5), except through the trunk
and the observed nodes. As a result, excluding the
observed nodes, this model has a low tree-width of
2 (excluding observed nodes), and a variety of ef-
ficient dynamic programming and message-passing
algorithms for training and inference can be readily
applied (Bodlaender, 1988).3 Our inference algo-
rithm passes information from the branches inwards
to the trunk, and then upward along the trunk, in
time O(K4MN).
As with our HMM models, we create two repre-
sentations from PL-MRFs, one for tokens and one
for types. For tokens, we decode the model to com-
pute y?, the matrix of optimal latent state values for
sentence x. For each layer j and and each possi-
ble latent state value k, we add a boolean feature
for token xi that is true iff y?i,j = k. For types,
we compute distributions over the latent state space.
Let y be the column vector of latent variables for
word x. For each possible configuration of values k
of the latent variables y, we add a real-valued fea-
tures for x given by P (y = k|x = w). We refer
to these two representations as LATTICE-TOKEN-R
and LATTICE-TYPE-R, respectively.
4.2 Parameter Estimation
We train the PL-MRF using contrastive estimation,
which iteratively optimizes the following objective
function on a corpus X:
?
x?X
log
?
y
?
c?Cliq(x,y) score(c,x,y)
?
x??N (x),y?
?
c?Cliq(x?,y?) score(c,x?,y?)
2As M, N ??, 5 out of every 6 edges are kept.
3c.f. a tree-width of min(M ,N ) for the unpruned model
where N (x), the neighborhood of x, indicates a
set of perturbed variations of the original sentence
x. Contrastive estimation seeks to move probability
mass away from the perturbed neighborhood sen-
tences and onto the original sentence. We use a
neighborhood function that includes all sentences
which can be obtained from the original sentence by
swapping the order of a consecutive pair of words.
Training uses gradient descent over this non-convex
objective function with a standard software package
(Liu and Nocedal, 1989) and converges to a local
maximum (Smith and Eisner, 2005).
For tractability, we modify the training procedure
to train the PL-MRF one layer at a time. Let ?i rep-
resent the set of parameters relating to features of
layer i, and let ??i represent all other parameters.
We fix ??0 = 0, and optimize ?0 using contrastive
estimation. After convergence, we fix ??1, and opti-
mize ?1, and so on. We use a convergence threshold
of 10?6, and each layer typically converges in under
100 iterations.
5 Domain Adaptation for a POS Tagger
We evaluate the representations described above on
a POS tagging task in a domain adaptation setting.
5.1 Experimental Setup
We use the same experimental setup as in HY10:
the Penn Treebank (Marcus et al, 1993) Wall Street
Journal portion for our labeled training data; 561
MEDLINE sentences (9576 types, 14554 tokens,
23% OOV tokens) from the Penn BioIE project
(PennBioIE, 2005) for our labeled test set; and all of
the unlabeled text from the Penn Treebank WSJ por-
tion plus a MEDLINE corpus of 71,306 unlabeled
sentences to train our language models. The two
texts come from two very different domains, mak-
ing this data a tough test for domain adaptation.
We use an open source Conditional Random Field
(CRF) (Lafferty et al, 2001) software package4 de-
signed by Sunita Sarawagi and William W. Cohen
to implement our supervised models. Let X be a
training corpus, Z the corresponding labels, and R
a representation function. For each token xi in X,
we include a parameter in our CRF model for all
features R(xi) and all possible labels in Z. Further-
more, we include transition parameters for pairs of
consecutive labels zi, zi+1.
4Available from http://sourceforge.net/projects/crf/
129
For representations, we tested TRAD-R,
NGRAM-R, HMM-TOKEN-R, I-HMM-TOKEN-R
(between 2 and 8 layers), and LATTICE-TOKEN-R
(8, 12, 16, and 20 layers). Following HY10, each
latent node in the I-HMMs have 80 possible values,
creating 808 ? 1015 possible configurations of the
8-layer I-HMM for a single word. Each node in
the PL-MRF is binary, creating a much smaller
number (220 ? 106) of possible configurations for
each word in a 20-layer representation. NGRAM-R
was trained using an unsmoothed trigram model on
the Web 1Tgram corpus. To keep the feature set
manageable, we included the top 500 most common
ngrams for each word type, and then used mutual
information on the training data to select the top
10,000 most relevant ngram features for all word
types. We incorporated ngram features as binary
values indicating whether xi appeared with the
ngram or not. We also report on the performance
of Brown clusters and Blitzer et al?s Structural
Correspondence Learning (SCL) (2006) technique,
which uses manually-selected ?pivot? words (like
?of?, ?the?) to learn domain-independent features.
Finally, we compare against the self-training CRF
technique from HY10.
5.2 Results and Discussion
For each representation, we measured the accuracy
of the POS tagger on the biomedical test text. Ta-
ble 2 shows the results for the best variation of each
kind of model ? 20 layers for the PL-MRF, 7 lay-
ers for the I-HMM, and 1000 clusters for the Brown
clustering. All language model representations sig-
nificantly outperform the SCL model and the TRAD-
R baseline. The novel PL-MRF model outperforms
the previous state of the art, the I-HMM model, and
much of the performance increase comes from a
11.3% relative reduction in error on words that ap-
pear in biomedical texts but not in newswire texts.
Both graphical model representations significantly
outperform the ngram model, which is trained on far
more text. For comparison, our best model, the PL-
MRF, achieved a 96.8% in-domain accuracy on sec-
tions 22-24 of the Penn Treebank, about 0.5% shy
of a state-of-the-art in-domain system (Shen et al,
2007) with more sophisticated supervised learning.
We expected that language model representations
perform well in part because they provide meaning-
ful features for sparse and polysemous words. To
test this, we selected 109 polysemous word types
model % error OOV % error
TRAD-R 11.7 32.7
TRAD-R+self-training 11.5 29.6
SCL 11.1 -
BROWN-TOKEN-R 10.8 25.4
HMM-TOKEN-R 9.5 24.8
NGRAM-R 6.9 24.4
I-HMM-TOKEN-R 6.7 24
LATTICE-TOKEN-R 6.2 21.3
SCL+500bio 3.9 -
Table 2: PL-MRF representations reduce error by 7.5%
relative to the previous state-of-the-art I-HMM, and ap-
proach within 2.3% absolute error a SCL+500bio model
with access to 500 labeled sentences from the target do-
main. 1.8% of the tags in the test set are new tags that
do not occur in the WSJ training data, so an error rate of
3.9+1.8 = 5.7% error is a reasonable bound for the best
possible performance of a model that has seen no exam-
ples from the target domain.
from our test data, along with 296 non-polysemous
word types, chosen based on POS tags and manual
inspection. We further define sparse word types as
those that appear 5 times or fewer in all of our unla-
beled data, and non-sparse word types as those that
appear at least 50 times in our unlabeled data. Table
3 shows results on these subsets of the data.
As expected, all of our language models outper-
form the baseline by a larger margin on polysemous
words than on non-polysemous words. The mar-
gin between graphical model representations and the
ngram model also increases on polysemous words,
presumably because the Viterbi decoding of these
models takes into account the tokens in the sur-
rounding sentence. The same behavior is evident for
sparsity: all of the language model representations
outperform the baseline by a larger margin on sparse
words than not-sparse words, and all of the graphical
models perform better relative to the ngram model
on sparse words as well. Thus representations based
on graphical models address two key issues in build-
ing representations for POS tagging.
6 Information Extraction Experiments
In this section, we evaluate our learned representa-
tions on a different task that investigates the abil-
ity of each representation to capture semantic, rather
than syntactic, information. Specifically, we inves-
130
POS Tagging Information Extraction
polys. not polys. sparse not sparse polys. not polys. sparse not sparse
tokens/types 159 4321 463 12194 222 210 266 166
categories - - - - 12 4 13 3
TRAD-R 59.5 78.5 52.5 89.6 - - - -
Ngram 68.2 85.3 61.8 94.0 0.07 0.17 0.06 0.25
HMM 67.9 83.4 60.2 91.6 0.14 0.26 0.15 0.32
(-Ngram) (-0.3) (-1.9) (-1.6) (-2.4) (+0.07) (+0.09) (+0.09) (+0.07)
I-HMM 75.6 85.2 62.9 94.5 - - - -
(-Ngram) (+7.4) (-0.1) (+1.1) (+0.5) - - - -
PL-MRF 70.5 86.9 65.2 94.6 0.09 0.15 0.1 0.19
(-Ngram) (+2.3) (+1.6) (+3.4) (+0.6) (+0.02) (-0.02) (+0.04) (-0.06)
Table 3: Graphical models consistently outperform ngram models by a larger margin on sparse words than not-sparse
words. On polysemous words, the difference between graphical model performance and ngram performance grows
for POS tagging, where the context surrounding polysemous words is available to the language model, but not for
information extraction. For tagging, we show number of tokens and accuracies. For IE, we show number of types,
categories, and AUCs.
tigate a set-expansion task in which we?re given a
corpus and a few ?seed? noun phrases from a se-
mantic category (e.g. Superheroes), and our goal is
to identify other examples of the category in the cor-
pus. This is a weakly-supervised task because we are
given only a handful of examples of the category,
rather than a large sample of positively and nega-
tively labeled training examples.
Existing set-expansion techniques utilize the dis-
tributional hypothesis: candidate noun phrases for a
given semantic class are ranked based on how sim-
ilar their contextual distributions are to those of the
seeds. Here, we measure how performance on the
set-expansion task varies when we employ different
representations for the contextual distributions.
6.1 Methods
The set-expansion task we address is formalized as
follows: given a corpus, a set of seeds from some
semantic category C, and a separate set of candidate
phrases P , output a ranking of the phrases in P in
decreasing order of likelihood of membership in C.
For any given representation R, the set-expansion
algorithm we investigate is straightforward: we cre-
ate a prototypical ?seed representation vector? equal
to the mean of the representation vectors for each
of the seeds. Then, we rank candidate phrases in
increasing order of the distance between the candi-
date phrase representation and the seed representa-
tion vector. As a measure of distance between rep-
resentations, we compute the average of five stan-
dard distance measures, including KL and Jensen-
Shannon divergence, and cosine, Euclidean, and L1
distance. In experiments, we found that improving
upon this simple averaging was not easy?in fact,
tuning a weighted average of the distance measures
for each representation did not improve results sig-
nificantly on held-out data.
Because set expansion is performed at the level
of word types rather than tokens, it requires type-
based representations. We compare HMM-TYPE-
R, NGRAM-R, LATTICE-TYPE-R, and BROWN-
TYPE-R in this experiment. We used a 25-state
HMM, and the same PL-MRF as in the previous
section. Following previous set-expansion experi-
ments with n-grams (Ahuja and Downey, 2010), we
employ a trigram model with Kneser-Ney smooth-
ing for NGRAM-R. For Brown clusters, instead of
distance metrics like KL divergence (which assume
distributions), we rank extractions by the number
of matches between a word?s BROWN-TYPE-R fea-
tures and seed features.
6.2 Data Sets
We utilized a set of approximately 100,000 sen-
tences of Web text, joining multi-word named enti-
ties in the corpus into single tokens using the Lex
algorithm (Downey et al, 2007a). This process
enables each named entity (the focus of the set-
expansion experiments) to be treated as a single to-
ken, with a single representation vector for compar-
ison. We developed all word type representations
131
model AUC
HMM-TYPE-R 0.18
BROWN-TYPE-R 0.16
LATTICE-TYPE-R 0.11
NGRAM-R 0.10
Random baseline 0.10
Table 4: HMM-TYPE-R outperforms the other methods,
improving performance by 12.5% over Brown clusters,
and by 80% over the traditional NGRAM-R.
using this corpus.
To obtain examples of multiple semantic cat-
egories, we utilized selected Wikipedia ?listOf?
pages from (Pantel et al, 2009) and augmented these
with our own manually defined categories, such that
each list contained at least ten distinct examples oc-
curring in our corpus. In all, we had 432 exam-
ples across 16 distinct categories such as Countries,
Greek Islands, and Police TV Dramas.
6.3 Results
For each semantic category, we tested five differ-
ent random selections of five seed examples, treating
the unselected members of the category as positive
examples, and all other candidate phrases as nega-
tive examples. We evaluate using the area under the
precision-recall curve (AUC) metric.
The results are shown in Table 4. All represen-
tations improve performance over a random base-
line, equal to the average AUC over five random or-
derings for each category, and the graphical models
outperform the ngram representation. HMM-TYPE-
R performs the best overall, and Brown clustering
with 1000 clusters is comparable (320 and 100 clus-
ter perform slightly worse).
As with POS tagging, we expect that language
model representations improve performance on the
IE task by providing informative features for sparse
word types. However, because the IE task classifies
word types rather than tokens, we expect the rep-
resentations to provide less benefit for polysemous
word types. To test these hypotheses, we measured
how IE performance changed in sparse or polyse-
mous settings. We identified polysemous categories
as those for which fewer than 90% of the category
members had the category as a clear dominant sense
(estimated manually); other categories were consid-
ered non-polysemous. Categories whose members
had a median number of occurrences in the cor-
pus less than 30 were deemed sparse, and others
non-sparse. IE performance on these subsets of the
data are shown in Table 3. Both graphical model
representations outperform the ngram representation
more on sparse words, as expected. For polysemy,
the picture is mixed: the PL-MRF outperform n-
grams on polysemous categories, whereas HMM?s
performance advantage over n-grams decreases.
One surprise on the IE task is that the LATTICE-
TYPE-R performs significantly less well than the
HMM-TYPE-R, whereas the reverse is true on POS
tagging. We suspect that the difference is due to the
issue of classifying types vs. tokens. Because of
their more complex structure, PL-MRFs tend to de-
pend more on transition parameters than do HMMs.
Furthermore, our decision to train the PL-MRFs
using contrastive estimation with a neighborhood
that swaps consecutive pairs of words also tends to
emphasize transition parameters. As a result, we
believe the posterior distribution over latent states
given a word type is more informative in our HMM
model than the PL-MRF model. We measured the
entropy of these distributions for the two models,
and found that H(PPL-MRF(y|x = w)) = 9.95 bits,
compared with H(PHMM(y|x = w)) = 2.74 bits,
which supports the hypothesis that the drop in the
PL-MRF?s performance on IE is due to its depen-
dence on transition parameters. Further experiments
are warranted to investigate this issue.
7 Conclusion and Future Work
Our investigation into language models as represen-
tations shows that graphical models can be used to
combat polysemy and, especially, sparsity in rep-
resentations for weakly-supervised classifiers. Our
novel factorial graphical model yields a state-of-the-
art POS tagger for domain adaptation, and HMMs
improve significantly over all other representations
in an information extraction task. Important direc-
tions for future research include models for han-
dling polysemy in IE, and richer language models
that incorporate more linguistic intuitions about how
words interact with their contexts.
Acknowledgments
This research was supported in part by NSF grant
IIS-1065397 and a Microsoft New Faculty Fellow-
ship.
132
References
Arun Ahuja and Doug Downey. 2010. Improved extrac-
tion assessment through better language models. In
Proceedings of the Annual Meeting of the North Amer-
ican Chapter of the Association of Computational Lin-
guistics (NAACL-HLT).
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jenn Wortman. 2009.
A theory of learning from different domains. Machine
Learning, (to appear).
Y. Bengio, J. Louradour, R. Collobert, and J. Weston.
2009. Curriculum learning. In International Confer-
ence on Machine Learning (ICML).
Yoshua Bengio. 2008. Neural net language models.
Scholarpedia, 3(1):3881.
Daniel M. Bikel. 2004. Intricacies of Collins Parsing
Model. Computational Linguistics, 30(4):479?511.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022, January.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
John Blitzer, Koby Crammer, Alex Kulesza, Fernando
Pereira, and Jenn Wortman. 2007. Learning bounds
for domain adaptation. In Advances in Neural Infor-
mation Processing Systems.
Hans L. Bodlaender. 1988. Dynamic programming on
graphs with bounded treewidth. In Proc. 15th Interna-
tional Colloquium on Automata, Languages and Pro-
gramming, pages 105?118.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. D. Pietra,
and J. C. Lai. 1992. Class-based n-gram models of
natural language. Computational Linguistics, pages
467?479.
M. Candito and B. Crabbe. 2009. Improving generative
statistical parsing with semi-supervised word cluster-
ing. In IWPT, pages 138?141.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks
with multitask learning. In International Conference
on Machine Learning (ICML).
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26.
Hal Daume? III, Abhishek Kumar, and Avishek Saha.
2010. Frustratingly easy semi-supervised domain
adaptation. In Proceedings of the ACL Workshop on
Domain Adaptation (DANLP).
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In ACL.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society of
Information Science, 41(6):391?407.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Likelihood from incomplete data via the EM algo-
rithm. Journal of the Royal Statistical Society, Series
B, 39(1):1?38.
D. Downey, M. Broadhead, and O. Etzioni. 2007a. Lo-
cating complex named entities in web text. In Procs.
of the 20th International Joint Conference on Artificial
Intelligence (IJCAI 2007).
Doug Downey, Stefan Schoenmackers, and Oren Etzioni.
2007b. Sparse information extraction: Unsupervised
language models to the rescue. In ACL.
Mark Dredze and Koby Crammer. 2008. Online methods
for multi-domain learning and adaptation. In Proceed-
ings of EMNLP, pages 689?697.
Mark Dredze, Alex Kulesza, and Koby Crammer. 2010.
Multi-domain learning by confidence weighted param-
eter combination. Machine Learning, 79.
Kevin Duh. 2005. Jointly labeling multiple sequences: A
Factorial HMM approach. In 43rd Annual Meeting of
the Assoc. for Computational Linguistics (ACL 2005),
Student Research Workshop.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical bayesian domain adaptation. In Proceed-
ings of HLT-NAACL, pages 602?610.
Zoubin Ghahramani and Michael I. Jordan. 1997. Facto-
rial hidden markov models. Machine Learning, 29(2-
3):245?273.
Z. Harris. 1954. Distributional structure. Word,
10(23):146?162.
D. Hindle. 1990. Noun classification from predicage-
argument structures. In ACL.
T. Honkela. 1997. Self-organizing maps of words for
natural language processing applications. In Proceed-
ings of the International ICSC Symposium on Soft
Computing.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised se-
quence labeling. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Fei Huang and Alexander Yates. 2010a. Exploring
representation-learning approaches to domain adapta-
tion. In Proceedings of the ACL 2010 Workshop on
Domain Adaptation for Natural Language Processing
(DANLP).
Fei Huang and Alexander Yates. 2010b. Open-domain
semantic role labeling by modeling word spans. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL).
133
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in NLP. In ACL.
S. Kaski. 1998. Dimensionality reduction by random
mapping: Fast similarity computation for clustering.
In IJCNN, pages 413?418.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proceedings of the
Annual Meeting of the Association of Computational
Linguistics (ACL), pages 595?603.
J. Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the International Conference on Machine
Learning.
D. Lin and X Wu. 2009. Phrase clustering for discrimi-
native learning. In ACL-IJCNLP, pages 1030?1038.
D.C. Liu and J. Nocedal. 1989. On the limited mem-
ory method for large scale optimization. Mathemati-
cal Programming B, 45(3):503?528.
Y. Mansour, M. Mohri, and A. Rostamizadeh. 2009. Do-
main adaptation with multiple sources. In Advances in
Neural Information Processing Systems.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
S. Martin, J. Liermann, and H. Ney. 1998. Algorithms
for bigram and trigram word clustering. Speech Com-
munication, 24:19?37.
A. Mnih and G. E. Hinton. 2009. A scalable hierarchi-
cal distributed language model. In Neural Information
Processing Systems (NIPS), pages 1081?1088.
P. Pantel, E. Crestan, A. Borkovsky, A. M. Popescu, and
V. Vyas. 2009. Web-scale distributional similarity and
entity set expansion. In Proc. of EMNLP.
PennBioIE. 2005. Mining the bibliome project.
http://bioie.ldc.upenn.edu/.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional
clustering of English words. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 183?190.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?285.
Ivan A. Sag, Thomas Wasow, and Emily M. Bender.
2003. Synactic Theory: A Formal Introduction. CSLI,
Stanford, CA, second edition.
M. Sahlgren. 2005. An introduction to random index-
ing. In Methods and Applications of Semantic Index-
ing Workshop at the 7th International Conference on
Terminology and Knowledge Engineering (TKE).
M. Sahlgren. 2006. The word-space model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stockholm
University.
G. Salton and M.J. McGill. 1983. Introduction to Mod-
ern Information Retrieval. McGraw-Hill.
L. Shen, G. Satta, and A. Joshi. 2007. Guided learn-
ing for bidirectional sequence classification. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics (ACL 2007), pages
760?767.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
354?362, Ann Arbor, Michigan, June.
Charles Sutton, Andrew McCallum, and Khashayar Ro-
hanimanesh. 2007. Dynamic conditional random
fields: Factorized probabilistic models for labeling
and segmenting sequence data. J. Mach. Learn. Res.,
8:693?723.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 384?394.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
J. J. Va?yrynen, T. Honkela, and L. Lindqvist. 2007.
Towards explicit semantic features using independent
component analysis. In Proceedings of the Work-
shop Semantic Content Acquisition and Representa-
tion (SCAR).
Jason Weston, Frederic Ratle, and Ronan Collobert.
2008. Deep learning via semi-supervised embedding.
In Proceedings of the 25th International Conference
on Machine Learning.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009. Multilingual dependency learning: A
huge feature engineering method to semantic depen-
dency parsing. In CoNLL 2009 Shared Task.
134
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 22?32,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Automatic Grading of Scientific Inquiry
Avirup Sil
Computer and Information Sciences
Temple University
Philadelphia, PA
avirup.sil@temple.edu
Angela Shelton
College of Education
Temple University
Philadelphia, PA
angi@temple.edu
Diane Jass Ketelhut
Teaching and Learning, Policy and Leadership
University of Maryland
College Park, MD
djk@umd.edu
Alexander Yates
Computer and Information Sciences
Temple University
Philadelphia, PA
yates@temple.edu
Abstract
The SAVE Science project is an attempt to ad-
dress the shortcomings of current assessments
of science. The project has developed two vir-
tual worlds that each have a mystery or natu-
ral phenomenon requiring scientific explana-
tion; by recording students? behavior as they
investigate the mystery, these worlds can be
used to assess their understanding of the scien-
tific method. Currently, however, the scoring
of the assessment depends either on manual
grading of students? written responses, or, on
multiple choice questions. This paper presents
an automated grader that can combine with
SAVE Science?s virtual worlds to provide a
cheap mechanism for assessments of the abil-
ity to apply scientific methodology. In experi-
ments on over 300 middle school students, our
best automated grader improves by over 50%
relative to the closest system from previous
work in predicting grades supplied by human
judges.
1 Introduction
Education researchers criticize current standardized
tests of science on many grounds. First, they lack
context (Behrens et al, 2007), which complicates a
student?s task of applying classroom-based learning,
as the theory of situated cognition suggests (Brown
et al, 1989). Second, many have criticized such
tests for failing to engage students long enough to
apply their understanding to the question. Further-
more and perhaps worst of all, standardized tests fail
to assess scientific inquiry?the ability of students
to apply the scientific method?authentically rather
than as scientific content (National Research Coun-
cil, 2005; Singley and Taft, 1995).
We consider an assessment conducted by the
Situated Assessment using Virtual Environments
for Science Content and Inquiry (SAVE Science)
project (Ketelhut et al, 2010; Ketelhut et al, 2009),
whose long-term goal is to address the shortcomings
of current standardized tests of science. The assess-
ments from SAVE Science have produced an abun-
dance of data on how students interact with a vir-
tual world, when trying to conduct scientific inquiry.
Observing student behavior in virtual environments
offers the potential for new insights into both how
students learn and what they know. However, this
benefit can only be realized if we can make sense of
the stream of data and text produced by the students.
In this paper, we attempt to automate the process
of grading students in SAVE Science assessments, to
make the evaluations as cost-effective as standard-
ized tests. Unlike most previous systems for au-
tomated grading (Sukkarieh and Stoyanchev, 2009;
Sukkarieh et al, 2004; Higgins et al, 2004; Wang
et al, 2008), the data for this task includes a short
paragraph (usually 50-60 words) natural language
response stating a hypothesis and evidence in sup-
port of it. In addition, there is a wealth of relational
data about student behavior in a virtual environment.
We develop novel predictors for automatically grad-
ing the written responses using a wide variety of nat-
ural language features, as well as features from the
data on student behavior in the virtual world. On
student data from two virtual worlds, our best auto-
mated grader has correlations of r = 0.58 and 0.44
with human judgments, improving over the closest
22
technique from previous work by 56% for the first
world, and by 120% for the second.
The rest of the paper is organized as follows.
The next section contrasts this project with previ-
ous work. Section 3 describes the SAVE Science
project and the student data it has produced. Section
4 details our automated grading models. Section 5
reports on experiments, and Section 6 concludes.
2 Previous Work
Wang et al (2008) have previously conducted a
study on assessing creative problem-solving in sci-
ence education by automatically grading student es-
says. Our techniques improve substantially over
theirs, as we demonstrate empirically. In part, we
improve by including more sophisticated language-
processing features in our model than the unigram
and bigram features they use; as others have noted,
bag-of-words representations and latent semantic
indexing become less useful as word order and
causal relationships become important for judging
an essay?s quality (Malatesta et al, 2002; Wiemer-
Hastings et al, 2005). A secondary reason for our
improvement is that we also have access to non-
linguistic data about the students that we can mine
for additional patterns.
Most previous research on automated grading of
written text focuses on short, factual text (Wiemer-
Hastings et al, 1999; Mohler and Mihalcea, 2009;
Leacock and Chodorow, 2003; Sukkarieh and Stoy-
anchev, 2009; Sukkarieh et al, 2004; Mitchell et al,
2002; Pulman and Sukkarieh, 2005), whereas SAVE
Science?s texts are only partly factual. Responses
are meant to convey a scientific explanation of a
mystery, and therefore, correct responses contain in-
ferences, observations of the world, and causal links
between observations and inferences.
Automatic systems for grading longer responses
typically grade essays for coherence and discourse
structure (Burstein et al, 2001; Higgins et al, 2004),
but these global discourse criteria are only partially
indicative of the quality of a student?s response to the
SAVE Science assessments. To be considered fully
correct in these tests, student responses must contain
factually correct information, as well as causal rela-
tionships that justify the student?s inferences, such
as ?The balls don?t bounce outside because it?s cold,
and lower temperatures decrease pressure.?
3 Assessing Scientific Inquiry Using
Virtual Worlds
We now give a brief overview of SAVE Science,
which aims to complement (or even replace) cur-
rent standardized tests for evaluating students? un-
derstanding of science. We first present the project?s
goals and methodology, and then describe the chal-
lenges involved in creating an automated evalua-
tion of student performance for this new assessment
paradigm.
3.1 The SAVE Science Project
SAVE Science (Ketelhut et al, 2010; Ketelhut et al,
2009; Ketelhut et al, 2012) is a novel project for
evaluating students? understanding of the scientific
method ? problem identification, gathering data,
analyzing data, developing a hypothesis, and com-
municating results ? by asking students to solve
a mystery in a virtual world through the applica-
tion of the scientific method to a content-based prob-
lem. Using immersive virtual environments for as-
sessments is a current area of focus among educa-
tion researchers (Clarke-Midura, 2010); SAVE Sci-
ence is unique in its attempt to assess understand-
ing of both inquiry as well as content. That is, the
test is designed to assess students? ability to apply
their knowledge of the scientific inquiry processes
to a problem they have never seen before, but within
a content area they have just studied. To be suc-
cessful, students must explore a virtual environment,
collect appropriate data about it, and find evidence
that supports their inference about the cause of the
mystery. Part of the reasoning for a particular con-
clusion draws on scientific knowledge learned in the
classroom, but for these mysteries such knowledge
of scientific content is insufficient. Students must
also be able to explore the virtual world and create a
hypothesis about the cause of the problem, based on
their observations and analysis of collected data.
For this study, we concentrate on two virtual
worlds produced by the SAVE Science project team,
Basketball and Weather Trouble. Screenshots of
the two virtual worlds are shown in Figure 1. Stu-
dents are represented by an avatar, or virtual char-
acter, whom they can control in the virtual world
23
Figure 1: Screenshots from SAVE Science?s virtual environments. Left: the Basketball module. Right: the Weather
Trouble module. The bar of icons along the bottom of the screen shows various tools that students may choose to use
in the world, including a map, compass, graphing tool, note pad, and instruments like a barometer and thermometer,
among others. Glowing green arrows indicate ?objects? (sometimes including people) with which the student?s avatar
may interact, by making observations, by taking measurements, or through conversation.
with a mouse or key presses. When the test be-
gins, one character in the world informs the student
of a mystery that the student needs to explain. In
the Weather Trouble world, citizens of Scientopolis
are concerned with the lack of rain recently, and ask
the avatar to determine whether it will rain soon. In
the Basketball world, a basketball tournament staffer
is concerned that students cannot play basketball on
the outdoor playground, because the balls will not
bounce high enough outdoors, even though the same
balls bounce just fine indoors.
Once informed of the mission, the student
(through her or his avatar) explores the world, and
interacts with objects or other characters in the vir-
tual world by ?colliding? with them. Interactions
with characters mostly involve the character telling
the avatar some part of the story of the world through
their eyes (e.g., ?It hasn?t rained here in weeks; I
hope it rains soon!?). The conversation may yield
useful clues, or it may be ?folk science? (e.g., ?The
sheep are lying down, so it is probably going to rain
soon?). When the avatar interacts with an object, the
student can choose from a set of tools to determine
measurements of the object. Measurements that a
student deems interesting can be recorded in the stu-
dent?s clipboard, and a graphing tool allows students
to construct charts from the data in the clipboard.
Once students have finished exploring, collect-
ing data, and analyzing the data, they are asked to
communicate the results by writing a brief expla-
nation for the cause of the mystery for the world.
In addition, students are asked to provide what they
consider to be the top three pieces of evidence for
their explanation. Both the explanation and the
ranked evidence are written in freeform text, con-
sisting of 48.5 words on average for Basketball, and
62.4 for Weather Trouble. We refer to the expla-
nation and ranked evidence collectively as the stu-
dent?s freeform response. These texts are critical
components of the overall data about the student, as
they can be used to assess the student?s ability to
communicate findings.
3.2 Assessing the ability to make scientific
inquiries
The virtual worlds from SAVE Science provide an
abundance of data about each student?s ability to
apply the scientific method, as well as their un-
derstanding of content, but the current assessment
scheme involves either manual grading of freeform
responses, or multiple choice questions. The first
is problematic because of the effort and expense in-
volved; the second is problematic because of the dif-
ficulty in designing multiple choice questions that
accurately assess everything a student has learned
(Wang et al, 2008; Chang and Chiu, 2005; Singley
and Taft, 1995). The focus of this paper is to pro-
vide an automated way of assessing students? ability
to perform scientific inquiry based on their behav-
ior in the virtual world and their freeform responses.
We first describe the current assessment mechanisms
available in SAVE Science?s data, which we then use
24
Score Criteria
4 Provides a correct hypothesis with supporting
data gathered from within the world
3 Provides a correct hypothesis with only folk
or incorrect evidence
2 Provides a somewhat correct answer
1 Provides a hypothesis
0 No hypothesis, or nonsense
Table 1: Rubric for manual scoring of freeform re-
sponses.
Score Example
3 it?s because the air outside is more colder
than the air inside here the cold air causes
the air molecules to gather up toghter tight
toghter causeing the ball to deflate and have
less bounce . . .
1 the wieght isnt up to regulations but the bouce
is ok everyball i bouce it bouced according
to regulartion but almost every ball has the
weight of 1.25 . . .
Figure 2: Example portions of two freeform responses
from Basketball, presented as written by the students.
below as gold standards for automated predictors for
assessment.
Manual grading of the freeform responses uses a
rubric of integer scores from 0 to 4. Guidelines for
the rubric scores are shown in Table 1, and two ex-
ample responses are shown in Figure 2. Two anno-
tators, the first holding a PhD in education and the
second a PhD student in computer science, indepen-
dently judged each response, achieving a high inter-
annotator agreement ? for Basketball, Cohen?s ? =
0.95, Pearson?s ? = 0.98; and for Weather Trouble
? = 0.8, ? = 0.93. For our experiments, we use
the judgments of the first annotator, who helped de-
sign the virtual worlds and has experience in grading
student essays, but the choice of which annotator?s
judgments to use makes little difference to the re-
sults.
The multiple choice questions, which we call quiz
questions, consist of two types, as shown in Table
2. The first type, which we call contextualized ques-
tions, directly test students? understanding of the sci-
entific issues that arise in the virtual environment
of the module. Non-contextualized questions are re-
lated to the topic of the module, but they can be an-
swered correctly using general scientific knowledge
rather than specific knowledge gleaned from explo-
ration of the virtual world. The non-contextualized
questions are taken from the benchmark exams of a
major urban school district.
4 Predictors for Scientific Inquiry Grades
We now focus on the task of building automated pre-
dictors for assessing students? ability to make scien-
tific inquiries. To do this, we turn the grading task
into a classical machine learning problem, in which
the system must learn from a set of training data
(students and their grades) how to predict a grade
for new students included in separate test data. We
focus on two main types of models: ones that can
grade by predicting how many multiple-choice ques-
tions (contextualized, non-contextualized, or both)
a student will answer correctly, and ones that can
predict the manual grade assigned to a freeform re-
sponse.
Unlike typical automated-grading systems for
grading written or spoken natural language, our task
includes a large additional source of evidence for the
predictions: data about the students? behavior in the
virtual world. Our prediction models therefore make
extensive use of both the freeform response and data
from the students? behavior in the world, which we
refer to as world data.
4.1 Models
We use Support Vector Machines with Radial Ba-
sis Function kernels (RBF-SVM) (Pang-Ning et al,
2006; Smola and Scho?lkopf, 1998) for learning
non-linear regression models of grading. Let S be
the set of students evaluated through SAVE Sci-
ence?s virtual environment, and let f : S ? Rn be
a vector-valued feature function providing n real-
valued features for each student, based on the stu-
dent?s freeform response and behavior in the virtual
world. Let g : S ? R be the target grading func-
tion, which provides a real-valued grade for each
student. The hypothesis spaceH for RBF-SVMs in-
cludes functions h : S ? R of the form
h(s) =
m?
i=1
?iK(xi, f(s)) + b (1)
25
Contextualized Questions Non-Contextualized Questions
What variable would you change to cor-
rect this basketball problem?
1. Temperature
A. Make it 75?F
B. Make it 55?F
C. Make it 35?F
2. Court Type
A. Concrete only
B. Wood only
C. Court Type makes little to no differ-
ence
3. Basketball used
A. Replace one Wade Park ball with one
Jordan Gym ball
B. Purchase a new set of balls for Wade
Park
C. New basketballs will not help this
problem
1. A child riding a bicycle notices that the tires are more in-
flated on hot days than on cold days, even though no air is
being added or removed. How can this be explained?
A. A higher temperature of the air in the tires causes the par-
ticles in the air to stick together and take up more space.
B. A higher temperature of the air in the tires causes the num-
ber of particles in the air to increase.
C. A higher temperature of the air in the tires causes the pres-
sure of the air to drop and the volume of the air to increase.
D. A higher temperature of the air in the tires causes both
the pressure and volume of the air to increase.
2. A sample of oxygen is being stored in a closed container
at a constant temperature. What will happen to the gas if
it is transferred to a container with a smaller volume?
A. Its weight will increase
B. Its weight will decrease
C. Its pressure will increase
D. The size of its particles will decrease
Table 2: Complete list of Basketball contextualized and non-contextualized quiz questions. Bold indicates the correct
answer.
where the xi are the support vectors, and K is the
RBF kernel function, given by:
K(x,x?) = exp(???x? x??2) (2)
Here, ?i, b, ? ? R are parameters to be learned from
the training data. We use the Weka (Hall et al, 2009)
toolkit for running standard training and prediction
algorithms with the SVM.
We train models for four distinct prediction tasks,
each defined by a different grading function g(s):
1) g(s) is the manually-assessed grade on stu-
dent s?s freeform responses; 2) g(s) is the num-
ber of correctly-answered contextualized questions;
3) g(s) is the number of correctly-answered non-
contextualized questions; and 4) g(s) is the total
number of correctly-answered quiz questions (the
sum of g(s) from 2 and 3). We use the same feature
function f for all models, which we describe next.
4.2 World Features
From the database that records a student?s activity in
the immersive virtual environment, we extract fea-
tures describing the frequency and types of activi-
ties in which students engaged. For both modules,
we include features for the number of object interac-
tions, the number of distinct objects interacted with,
the total number of measurements made, the number
of measurements saved in the student?s clipboard,
and the number of graphs made. We also include
module-specific features: for example, in the Bas-
ketball assessment module, we counted how many
distinct basketballs were interacted with, how many
measurements were made using each type of tool
available in the Basketball world, whether a given
student created graphs of temperature inside vs. out-
side, or graphs of temperature vs. pressure, etc. In
total, the model contains 69 world features in the
Weather module, and 65 in the Basketball module.
All features conform to the pattern of counts over
particular types of actions the avatar might take. We
call the features from the virtual environment world
features.
We note that the relational data in this world is
large and complex, containing temporal and sequen-
tial information which these features currently ig-
nore. This feature set serves as an initial exploration
of the world data, but we fully expect that future in-
vestigation will improve on this representation. For
26
this paper we are primarily interested in features of
the freeform responses, which we now turn to.
4.3 Natural Language Features
We investigate standard text mining features from
bag-of-words representations and Latent Semantic
Analysis, as well as a variety of features tailored to
the grading task. Spelling is a major problem for
this type of prediction task, but spelling-correctors
are investigated elsewhere (Kernighan et al, 1990)
and are not a focus of this research. We therefore
manually corrected spelling errors throughout the
texts before extracting features and conducting ex-
periments. No correction of grammar or punctuation
was performed.
4.3.1 Latent Semantic Analysis Features
After removing 34 common stopwords, we
extract a bag-of-words representation from the
freeform responses (Manning and Schu?tze, 1999).
We apply Latent Semantic Analysis (LSA) (Lan-
dauer and Dumais, 1997; Steyvers and Griffiths,
2006) to this set of features to produce a smaller
set of 72 latent features for Basketball, and 94 for
Weather Trouble, based on a threshold of retaining
90% of the variance in the data.
4.3.2 Features from Hidden Markov Models
LSA and other topic models identify latent struc-
ture based on document-level cooccurrence statis-
tics, but the ?documents? in our data are short for
topic-modeling purposes, and we have less than
200 of them for each world. As a result, stan-
dard topic modeling techniques may have difficulty
identifying the appropriate structure. We therefore
also consider Hidden Markov Models (HMMs) (Ra-
biner, 1989), generative models which rely both on
cooccurrence within a sentence and on sequence in-
formation for determining model parameters. Fol-
lowing recent work by Huang et al (2011) on
using HMMs to build representations, we esti-
mate parameters for a fully-connected HMM with
100 latent states over the freeform responses us-
ing Expectation-Maximization. We then decode the
HMM over the corpus to produce a Viterbi-optimal
latent state for each word. Finally, we use counts of
these 100 latent states to produce 100 new features
for each freeform response.
4.3.3 Detecting disengagement
A small number of students show little enthusi-
asm for the test, and their responses and general per-
formance are quite poor. Often their freeform re-
sponses are short, or they repeat the same text mul-
tiple times. We include three features that help iden-
tify such cases: the overall length of the response,
the number of times a full sentence is repeated ex-
actly, and the number of tokens that are repeated
across multiple sentences.
4.3.4 Ngram and Pattern Features
While HMM and LSA features help combat spar-
sity in the predictive model, they may ignore the
strong signal from a few expressions that are par-
ticularly important for a domain. By soliciting ad-
vice from domain experts, we selected important
unigrams, bigrams, and trigrams for each module,
and created features that count each of these. Like-
wise, we selected important two-word and three-
word sets, which we call loose patterns, that weakly
indicate that a student understood the problem, if
they all occur in the same response but not neces-
sarily near one another. Again, these words were se-
lected as a result of combination of empirical obser-
vations and expert domain knowledge from design-
ers. For instance, if a response contains the three
words ?temperature,? ?pressure,? and ?because,? it
would match one of these loose patterns. For each
pattern, we create a feature to count the number of
matches in a response.
The selected patterns and ngrams both consist of
three kinds of words: ones that indicate types of
measurable phenomena or properties (e.g., ?temper-
ature?), locations (e.g., ?outside?), or causal or com-
parative words (e.g., ?causes,? ?higher,? ?than,? or
?decrease?). Because the responses discuss numer-
ical observations like temperature and pressure val-
ues, we also allow a wildcard for matching any num-
ber as part of the loose patterns.
4.3.5 Semantic Features
We use the Senna1 semantic role labeling (SRL)
system (Collobert et al, 2011) to automatically iden-
tify predicate-argument relationships in the freeform
responses. In general, the SRL system is only able
1http://ml.nec-labs.com/senna/
27
to identify predicate-argument structures in well-
crafted sentences, which on its own is a good indi-
cator that the student will do well in the evaluation.
In addition, we extract semantic features (SFs) that
count how often certain predicate-argument struc-
tures appear which are indicative of a good answer:
SF1 Count how often the freeform response con-
tains any predicate.
SF2 Count how often the response contains predi-
cates that involve causality, such as ?causes? or
change-of-value predicates like ?increase.?
SF3 Count how often measurement words (e.g.,
temperature, pressure) appear as arguments to
any predicate.
SF4 Count how often measurement words appear as
arguments to the predicates related to causality.
4.4 Feature Selection
We perform feature selection using a correlation-
based technique that tries to identify maximally-
relevant and minimally-redundant features (Hall,
1998; Deng and Moore, 1998). The algorithm eval-
uates the value of a subset of features by considering
the individual correlation between each feature and
the gold standard, as well as the correlation between
features. We use the default parameter settings for
feature selection, as specified in Weka.
5 Experiments
5.1 Experimental Setup
We use a dataset collected by the SAVE Science
project, consisting of the world data, freeform re-
sponses, and quiz answers from public middle-
school students in a major urban area of the United
States. 120 students completed the Weather Trou-
ble module, and 184 students completed Basket-
ball. After manually correcting spelling errors in
the freeform responses, we extracted features as de-
scribed above.
Following Wang et al (2008), we evaluate our re-
gression models using Pearson correlation between
the predicted outcome and the gold standard out-
come. Four different gold standards are consid-
ered for each module: manually-assigned grades for
the freeform text, and three versions of the num-
ber of correctly-answered quiz questions (contextu-
alized only, non-contextualized only, and all). We
use a ?2 test with a threshold of p < 0.05 to deter-
mine statistical significance. We train and test mod-
els using 10-fold cross-validation to reduce variabil-
ity, and the results are averaged over the folds.
We evaluate several variants of our system, in-
cluding a World variant that only includes features
from the world data; an NLP variant that only in-
cludes features from the freeform responses; and a
combined World+NLP variant that includes all fea-
tures before feature selection is performed.
Our evaluation compares against the essay grad-
ing technique by Wang et al Like ours, their sys-
tem uses RBF-SVM regression with default param-
eter settings as implemented in Weka, and like ours
the system is trained on student texts proposing so-
lutions to a science problem (in their case, a high
school chemistry problem). The system is trained
on human judgments of the quality of the student
answers. The major difference between our tech-
nique and theirs lies in the representation of the data;
Wang et al use two types of features: unigrams, and
bigrams that occur at least five times during train-
ing. In our implementation of their technique, we
use a lower threshold for bigrams ? they must oc-
cur at least twice. This is because we have less text
to work with, and the higher threshold yields too
few bigrams. Using the lower threshold improved
performance slightly, so we report only those results
below.
5.2 Results and Discussion
The full system for automatic grading is accurate,
across both worlds and all gold standards. Figure
3 shows the results of predicting human judgments
of the freeform responses, where the World+NLP
system achieves a correlation of 0.58 for Basket-
ball and 0.44 for Weather Trouble. The same sys-
tem achieves 0.55 and 0.54 on the World ques-
tions of Basketball and Weather Trouble, respec-
tively (Figures 4 and 5). Our best models are sta-
tistically significantly different from the Wang et al
model (for predicting contextualized questions for
basketball: p = .009, ?2 = 6.87162; for grading
freeform responses: p ? 0, ?2 = 14.21725). Cor-
relations from World+NLP for other quiz types ?
28
0.26 
0.37 
0.58 0.58 
0.15 0.20 
0.43 0.44 
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
World Wang et al NLP World+NLP
Corr
elat
ion 
Coe
ffici
ent 
Correlation: Predicted vs Human Rubric Scores 
Basketball
Weather
Figure 3: Our NLP features dramatically improve predic-
tion over the Wang et al model for grading freeform sci-
ence essays, by a margin of 0.21 on Basketball and 0.23
on Weather Trouble.
0.33 0.34 
0.45 
0.55 
0.09  0.12 
0.34 0.40 
0.20 
0.33 
0.40 0.46 
0
0.1
0.2
0.3
0.4
0.5
0.6
Wang et al World NLP World+NLP
Corr
elat
ion 
Coe
ffici
ent 
Automatic Grading  of Basketball Quiz Answers  
Contextualized Non-contextualized All
Figure 4: The World+NLP model outperforms both
World and NLP, and substantially outperforms the Wang
et al system.
non-contextualized and all questions ? were some-
what lower, but still statistically significant (p =
.002, ?2 = 10.05986).
The language features are currently the major fac-
tor in the predictive models for automated grad-
ing. The NLP model substantially outperforms both
the simpler Wang et al model and the World-only
model in predicting quiz answers for both worlds.
It achieves correlations that are statistically signifi-
cantly different from the baseline, for all gold stan-
dards and both worlds.
The story in the case of grading freeform essays
is similar. Our NLP model beats the Wang et al
model and the World-only model. Our full model
World+NLP, however, outperforms the NLP model
by only a small fraction. Also, the Wang et al model
performs slightly better than the World-only model
on freeform responses. For Basketball, the correla-
tion coefficient of their model is greater by 0.11 and
for Weather by 0.05. We believe that the NLP-based
0.13 
0.06 
0.53 0.54 
0.00 0.06 
0.46 
0.30 
0.10 0.12 
0.38 0.41 
-0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
Wang et al World NLP World+NLPCorr
elat
ion 
Coe
ffici
ent 
Automatic Grading  of Weather Trouble Quiz Answers  
Contextualized Non-contextualized All
Figure 5: The NLP model substantially outperforms
World and Wang et al on predicting quiz questions for
Weather Trouble, and the combined World+NLP model
achieves a 0.54 correlation for contextualized questions.
models, including Wang et al?s, are outperforming
the World model because the current representation
of the World data fails to capture all of the pertinent
information from students? behavior in the virtual
environments. Our plans for future work include the
development of features that can capture temporal
patterns in student activity.
Each type of language feature appears to pro-
vide a beneficial and complementary source of ev-
idence. We tested the model using only individual
subsets of the NLP features, such as HMM features
only, LSA features only, ngrams and loose patterns
only, and features from semantic role labeling only.
On their own, each set of features provides only a
small improvement over the mean predictor. When
combined with the world features, each subset of
the NLP features again provides only a small im-
provement over the World-only model. For exam-
ple, for predicting Basketball world quiz questions,
World features achieve r = 0.34, World+HMM and
World+LSA achieve 0.35, and World+(ngrams and
loose patterns) achieves 0.39. The relative ranking
of these subsets of features is not consistent across
different tasks; for Weather contextualized ques-
tions, World+HMM is best, and for Weather non-
contextualized questions, World+LSA is best. Fea-
tures selected by the feature selection algorithm also
indicate that the different types of language features
complement one another. The feature selection al-
gorithm for the World+NLP model selects some fea-
tures for every different type we presented, although
the HMM, LSA, loose pattern, and unigram fea-
tures dominate. We believe that the best procedure
29
for developing grading systems for science essays
is therefore to construct a large number of possible
features using a variety of techniques, and then train
a model for a particular task and gold standard. In-
cluding significantly more varieties of features, per-
haps from additional kinds of language models or
NLP pipeline tools, is an important future direction
for further improving the grading accuracy.
While the accuracies of the models for contextu-
alized and non-contextualized questions are broadly
similar, the models themselves are not. For the con-
textualized questions, 4 important world behavior
features were deemed important and non-redundant
by the feature selection algorithm: the number of
distinct collisions, the number of people collided
with, the number of distinct objects (basketballs or
balloons) whose pressure was measured, and the
number of distinct temperature measurements that
were recorded into clipboards. The essential task
in this virtual world is to discover that a decrease
in the temperature of several gas systems (basket-
balls and balloons filled with air) is causing their
pressure to decrease. The model for the contex-
tualized questions thus includes variables that are
highly relevant to a student?s understanding of the
core problem in the world, which in turn indicates
that automated data mining techniques are capable
of identifying when students are learning to prac-
tice the scientific method, by observing student be-
havior. On the other hand, the model for the non-
contextualized questions includes only 2 world fea-
tures: The number of collisions made and number
of different objects whose circumference was mea-
sured. The first one is an indicator of the activity
level of a student and the second variable is an indi-
cator for whether the student has identified the prob-
lem (the basketballs are not bouncing because they
are deflated), but not for the underlying cause of
the problem (the outside temperature causes a drop
in pressure, which causes the basketball circumfer-
ence to decrease). Thus the model that predicts non-
contextualized questions very accurately has little
information about whether the student understood
the core problem of the world or not; instead, it has
information about whether the student is active in
the world. These observations lend some support to
the criticism that the standardized tests are not prop-
erly assessing inquiry.
Performance on the Weather Trouble module is
consistently lower than on Basketball. In part, this
reflects the increased difficulty of this world; human
inter-annotator agreement is a bit lower (? = 0.8
vs. 0.95 on Basketball). However, another large
part of the difference is that the world features pro-
vide far less information in Weather Trouble ? the
World-only model has less than half the correlation
on Weather than on Basketball, for all quiz ques-
tion types. We suspect that the cause is the nature
of the task on the Weather Trouble world, where
temporal information plays a bigger role as measure-
ments of air pressure and wind direction may change
over time. Investigating world features that can dis-
tinguish different patterns of student behavior over
time is an important area for further investigation.
6 Conclusion
Our automated grader uses a wide variety of NLP
pipeline tools to produce features for students? es-
says on the answers to scientific mysteries. The
grader achieves significant correlation with human
judges and multiple choice quiz evaluations, sub-
stantially outperforming a simpler grader from prior
work. The findings of this research suggest that au-
thentic assessments of scientific inquiry through vir-
tual environments can be graded purely automati-
cally, like high stakes multiple choice tests. Ongoing
work on SAVE Science is investigating the differ-
ences in how students respond to standard multiple-
choice tests and tests based on virtual environments.
But the contextualized assessments from SAVE Sci-
ence provide evaluation of scientific inquiry that
multiple choice tests currently do not, and they can
now be graded just as cheaply.
Acknowledgments
This material is based upon work supported
under National Science Foundation Grant No.
0822308/1157534. Any opinions, findings, and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not necessarily
reflect the views of the NSF. We would like to thank
Catherine Schifter and the rest of the SAVE Sci-
ence team for their help and support. We also wish
to thank the anonymous reviewers for their helpful
comments.
30
References
J. T. Behrens, D. Frezzo, R. Mislevy, M. Kroopnick, and
D. Wise. 2007. Structural, Functional, and Semiotic
Symmetries in Simulation-based Games and Assess-
ments. In E. Baker, J. Dickieson, W. Wulfeck, and
H. O?Neil, editors, Assessment of Problem Solving Us-
ing Simulations. Lawrence Erlbaum Associates.
J. S. Brown, A. Collins, , and P. Duguid. 1989. Situated
cognition and the culture of learning. Educational Re-
searcher, 18(1):32?41.
J. Burstein, C. Leacock, and R. Swartz. 2001. Auto-
mated evaluation of essays and short answers. In 5th
International Computer Assisted Assessment Confer-
ence. Loughborough University.
S.-N. Chang and M.-H. Chiu. 2005. The development of
authentic assessment to investigate ninth graders sci-
entific literacy: In the case of scientific cognition con-
cerning the concepts of chemistry and physics. Inter-
national Journal of Science and Mathematics Educa-
tion, 3:117?140.
J. Clarke-Midura. 2010. The Role of Technology in
Science Assessments. Better: Evidence-based Edu-
cation, 3(1).
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12:2493?2537.
Kan Deng and Andrew Moore. 1998. On the greediness
of feature selection algorithms. In Proc. International
Conference on Machine Learning (ICML), June 1998.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11(1).
M. A. Hall. 1998. Correlation-based feature subset
selection for machine learning. In Hamilton, New
Zealand.
D. Higgins, J. Burstein, D. Marcu, and C. Gentile. 2004.
Evaluating multiple aspects of coherence in student
essays. In Proceedings of the annual meeting of the
North American Chapter of the Association for Com-
putational Linguistics, Boston, MA.
Fei Huang, Alexander Yates, Arun Ahuja, and Doug
Downey. 2011. Language Models as Representations
for Weakly Supervised NLP Tasks. In Conference on
Computational Natural Language Learning (CoNLL).
Mark D. Kernighan, Kenneth W. Church, and William A.
Gale. 1990. A spelling correction program based on
a noisy channel model. In Proceedings of the 13th
Conference on Computational Linguistics, pages 205?
210.
D.J. Ketelhut, B. Nelson, and C. Schifter. 2009. Virtual
Environments for Situated Science Assessment. In
Proceedings of the International Conference on Cog-
nition and Exploratory Learning in the Digital Age,
pages 507?508.
D.J. Ketelhut, B. Nelson, C. Schifter, and Y. Kim. 2010.
Using Immersive Virtual Environments to Assess Sci-
ence Content Understanding: The Impact of Context.
In D. G. Kinshuk, J. M. Sampson, P. Spector, D. Isaas,
Ifenthaler, and R. Vasiu, editors, Proceedings of the
IADIS International Conference on Cognition and Ex-
ploratory Learning in the Digital Age (CELDA), pages
227?230.
Diane Jass Ketelhut, Alexander Yates, Avirup Sil, and
Michael Timms. 2012. Applying Educational Data
mining in E-learning environments. In Section within
the New Measurement Paradigm Report, p 47-52.
T.K. Landauer and S.T. Dumais. 1997. A solution to
Platos problem: The latent semantic analysis theory
of acquisition, induction, and representation of knowl-
edge. In Psychological Review, 104.
C. Leacock and M. Chodorow. 2003. C-rater: Auto-
mated scoring of short-answer questions. In Comput-
ers and the Humanities, 37(4):389405.
K.I. Malatesta, P. Wiemer-Hastings, and J. Robertson.
2002. Beyond the short answer question with research
methods tutor. In Proceedings of the Intelligent Tutor-
ing Systems Conference.
Chris Manning and Hinrich Schu?tze. 1999. Foundations
of Statistical Natural Language Processings. MIT
Press.
T. Mitchell, T. Russell, P. Broomhead, and N. Aldridge.
2002. Towards robust computerised marking of free-
text responses. In Proceedings of the 6th International
Computer Assisted Assessment (CAA) Conference.
Michael Mohler and Rada Mihalcea. 2009. Text-to-text
semantic similarity for automatic short answer grad-
ing. In Proceedings of the 12th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, EACL.
National Research Council. 2005. America?s Lab Re-
port: Investigations in High School Science. National
Academies Press.
T. Pang-Ning, M. Steinbach, and V. Kumar. 2006. Intro-
duction to Data Mining. Pearson Addison-Wesley.
S.G. Pulman and J.Z. Sukkarieh. 2005. Automatic
short answer marking. In Proceedings of the Second
workshop on Building Educational Applications Using
NLP.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?285.
M.K. Singley and H.L. Taft. 1995. Open-ended
approaches to science assessment using computers.
Journal of Science Education and Technology, 4(1):7?
20.
31
A. Smola and B. Scho?lkopf. 1998. A tutorial on support
vector regression. Technical report, Royal Holloway
College, University of London, UK.
Mark Steyvers and Tom Griffiths. 2006. Probabilistic
topic models. In T. Landauer, D. McNamara, S. Den-
nis, and W. Kintsch, editors, Latent Semantic Analy-
sis: A Road to Meaning, pages 427?448. Lawrence
Erlbaum Associates.
J. Sukkarieh and S. Stoyanchev. 2009. Automating
model building in C-rater. In Proceedings of the 2009
Workshop on Applied Textual Inference, pages 6169,
Suntec, Singapore, August.
J.Z. Sukkarieh, S.G. Pulman, and N. Raikes. 2004. Auto-
marking 2: An update on the ucles-oxford university
research into using computational linguistics to score
short, free text responses. In International Association
of Educational Assessment, Philadephia.
H-C. Wang, C-Y. Chang, and T-Y Li. 2008. Assessing
creative problem solving with automated text grading.
In Computers and Education.
P. Wiemer-Hastings, K. Wiemer-Hastings, and
A. Graesser. 1999. Improving an intelligent tu-
tors comprehension of students with latent semantic
analysis. In Artificial Intelligence in Education, pages
535542.
P. Wiemer-Hastings, E. Arnott, and D. Allbritton. 2005.
Initial results and mixed directions for research meth-
ods tutor. In AIED2005 - Supplementary Proceedings
of the 12th International Conference on Artificial In-
telligence in Education, Amsterdam.
32
