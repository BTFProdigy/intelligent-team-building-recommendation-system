Automatic Refinement of a POS Tagger 
Using a Reliable Parser and Plain Text Corpora 
Hideki Hirakawa, Kenji Ono, Yulniko Yoshimura 
Human Interface Laboratory 
Corporate Research & Development Center 
Toshiba Corporation 
Konmkai-Toshiba-cho 1, Saiwai-ku, Kawasaki, 212-8582, Japan 
{hideki.hirakawa, kenj i2.ono, yulniko.yoshimura} @toshiba.co.jp 
Abstract 
This paper proposes a new unsupervised learning method for obtaining English part-of- 
specch(POS) disambiguation rules which would improve thc accuracy of a POS tagger. This 
method has been implemented in the experimental system APRAS (Automatic POS Rule 
Acquisition System), which extracts POS disambiguation rules fl'om plain text corpora by 
utilizing different ypes of coded linguistic knowledge, i.e., POS tagging rules and syntactic 
parsing rules, which arc already stored in a fully implemented MT system. 
In our ext)eriment , he obtained rules were applied to 1.7% of the sentences in a non-training 
corpus. For this group of sentences, 78.4% of the changes made in tagging results were an 
improvement. We also saw a 15.5 % improvement in tagging and parsing speed and an 8.0 % 
increase of parsable sentences. 
1 Int roduct ion 
Much research has been donc Oll knowledge 
acquisition fiom large-scalc annotated corpora 
as a rich source of linguistic knowledge. M~tior 
works done to create English POS taggers 
(henceforth, "taggers"), for example, include 
(Church 1988), (Kupicc 1992), (Brill 1992)and 
(Voutilaincn et al 1992). The problem with this 
framework, however, is that such reliable 
corpora are hardly awdlable duc to a huge 
amount of the labor-intensive work required. In 
case of the acquisition of non-core knowledge, 
such as specific, lexically or dolnain dependent 
knowledge, preparation of annotated corpora 
becomes more serious problem. 
One viable approach then is to utilize plain text 
corpora instead, as in (Mikheev 1996). But The 
method proposed by (Mikheev 1996) has its 
own weaknesses, in that it is restricted in scope. 
That is, it aims to acquire rules for unknown 
words in corpora fi'om their ending characters 
without looking at the context. In the meantime, 
(Brill 1995a) (Brill 1995b) proposed a method to 
acquire contcxt-dcpendent POS disambiguation 
rules and created an accurate tagger, even from a 
very small aunotated text by combining 
supervised and tmsupcrviscd learning. The 
wcakness of his method is that the effect of 
unsupervised learning decreases as the training 
corpus size increases. 
The problem in using plain text corpora for 
knowledge acquisition is that we need a human 
supervisor who can evaluate and sift the 
obtained knowledge. An alternative to this 
would be to use a number of modules of a well- 
developed NLP system which stores most of thc 
highly reliable general rules. Here, one module 
fimctions as a supervisor for other modules, 
since all these modules arc designed to work 
cooperatively and the knowledgcs tored in each 
module are correlated. 
Keeping this idea in mind, we propose a new 
unsupervised learning method for obtaining 
linguistic rules fi'om plain text corpora using the 
existing linguistic knowledge. This method has 
been implemented in the rule extraction system 
APRAS (Automatic POS Rule Acquisition 
313 
System), which automatically acquires rnles for 
refining the morphological analyzer (taggcr) in 
our English-Japanese MT system ASTRANSAC 
(Hirakawa ct al. 1991) through the interaction 
between the system's tagger and parser on the 
assumption that they are considerably accurate. 
This paper is organized as follows: Section 2 
illustrates the basic idea of our method; Section 
3 gives the outline of APRAS; Sections 4 and 5 
describe our experiments. 
2 Basic Idea 
Our MT system has a tagger which can generate 
ranked POS sequences of input sentences 
according to their plausibility and also a parser 
which judges the parsability of the derived POS 
sequences one by one until a parsable one is 
found ~ . in our framework, this tagger can be 
viewed as a POS candidate generator, and the 
parser as a sifter. 
Now sentences can be categorized into the 
following three: 
(P )  a balanced sentence, whose top ranked 
sequence, or initial POS sequence, is 
parsable, 
(Q) a conflicting sentence, in which the top 
ranked scquencc is unparsable, but there are 
parsable ones in the rest of the sequences; 
and 
(R)  an unparsable sentence, in which all the 
POS sequences are unparsable. 
Before going on to our main discussion, we will 
briefly explain the terminology used in this 
paper. Here we call a highest-ranking parsable 
POS sequence as the "Most Preferable _PParsable 
POS sequence," or simply "MPP POS 
sequence." For our purposes, we will make use 
of balanced sentences and conflicting sentences. 
We call the POS of a word in the initial POS 
sequence as its "initially tagged POS" and that in 
the MPP POS sequence as its "parsable POS." 
We call the word whose initially tagged POS 
and parsable POS differ as a "focus word." Since 
the tagger is accurate, we can expect only a few 
POS differences between the initial and MPP 
POS sequences for a sentence. Finally, let us call 
Here only top-N POS scquenccs are tried, where N 
is a pre-defined constant to limit parsing time. 
the POS's of the preceding and succeeding 
words as the "POS context of the focus word." 
Conflicting sentences, and their initial POS 
sequences, parsable POS sequences, and focus 
words can be automatically cxtracted. Through 
extraction out of a large amount of plain text 
corpora combined with statistical filtering, it 
would be possible to automatically select the 
proper POS conditions that could determine 
POS's of focus words. Then, we extract "POS 
Adjusting rules" or "PA rules" defined as below: 
PA rule: W(IPOS) ---> W(PPOS): C 
C: Context 
W :Word 
IPOS: Initially tagged POS 
PPOS: Parsable POS 
Means "Give priority to the parsable POS 
over the initially tagged POS in a particular 
context shown as 'C'." 
PA rules do not determine POS's of words from 
their context, but change the judgement made by 
the tagger in a particular context. Extracted PA 
rules arc independent rules to the tagger and the 
parser used in the extraction. At the samc time, 
these rules are optimized for the tagger and the 
parser, since they are derived only from 
conflicting sentences, not from balanced 
sentences. Hence, the knowledge already coded 
in the system will not be extracted. 
I11 the following section, we give the outline of 
APRAS focusing on its two modules. 
3 Outline of APRAS 
Fig. 1 shows the application of APRAS to an 
MT system. APRAS works in two phases, a rule 
extraction phase and a rule application phase. 
Note that the same tagger and the parser of the 
MT system are used throughout. 
314 
I'A rule extraction phase PA rule application phase --- 
qYaining Coqms 
Mf Systcm ~- APP, AS 
\[ GenorationJ 
Rule caMidatc 
gctlcralion J 
M'I' Systm~ ~ 
\[ Generation \] 
Figure 1 : Application of APRAS to an MT System 
In the rule extraction phase, the tagger analyzes 
each sentence in a training corpus and produces 
plausible POS sequences. The parser then judges 
the parsability of each POS sequence. Whenever 
a conflicting sentence appears, the role 
generation module outputs the candidates of PA 
rules. 
After all PA rule candidates for this training 
corpus are generated, the rule filtering module 
statistically weighs the validity of obtained PA 
rule candidates, and filters out unreliable rules. 
Sentences in the training corpus are not 
mmslated in this phase. 
In the rule application phase, both the already 
installed POS rulcs and the acquired PA rules 
are used for tagging. A sentence is parsed and 
then translated into target language. PA rules 
basically act to avoid the taggcr's wasteful 
generation of POS sequences. This would 
improve the ranking of POS sequences the 
tagger outputs and also increase the chances that 
the parser will find a parsable or better POS 
sequence in the improved ranking. 
3.1 Rule Generation Module 
PA rule candidates are generated fronl 
conflicting sentences. Balanced and unparsable 
sentences generate no PA rule candidate. The 
words in balanced sentences arc recorded along 
with their POS's and POS contexts to be used in 
the rule filtering module. Whenever the system 
enconllters a conflicting sentence in a training 
corpus, the system compares the initial POS 
sequence with the MPP POS sequence of the 
scntcncc and picks up focus words. Then, for 
every focus word, the system generates a PA 
rule candidate which consists of a focus word, 
its initially tagged POS, parsable POS, and the 
POS context, i.e., the preceding POS's and the 
succeeding POS's. 
t:ig. 2 illustrates how a PA rulc candidate is 
generated. The focus word is 'rank', its initially 
tagged POS is "(verb)', its parsable POS is 
"(noun)', and the POS context is "(verb)- 
(determiner)-$-'in'-(dcterminer)", where "$' 
denotes the focus word. The POS context is 
composed of preceding two POS's and 
succeeding two POS's. ttere surface words can 
be used instead of POS, like "in' in the example. 
The generated PA rule candidate can be read as: 
If the word 'rank' appears in a POS context 
"(verb)-(determiner)-$-'in'-(determiner)", then 
give priority to "(noun)' over "(verb)'. 
In this rule generation module, two important 
factors should be taken into account: namely, 
context size and levels of abstraction. If we 
expand the context of a focus word, the PA rule 
should gain accuracy. But its frequency in the 
training corpus would drop, thereby making it 
difficult to perform statistical filtering. To 
ensure statistical reliability, we need a large- 
315 
Input sentence 
1 
? ? ? n \ ]ove  I 
I 
(Focus word) 
,, ,, ,, ,, 
the ', rank  ', in ', the', . . .  
I I I I 
Initial POS sequence 
o ? ?  
. , ?  
MPP POS sequence 
PA rule candidate: 
POS tagger output Parser output 
Q O 4 
Q ? J 
6 ? O 
D O g 
V 
, ? ?  
V 
det 
o ? ?  
? o ,  
det 
vti 
, ? ?  
? , ,  
11 
in det . . .  
? ? ?  ? o ?  * * ?  
? ? ?  . ? ,  * * *  
in det . . .  
PA rule generation 
unparsable 
? ? ?  
unparsable 
parsable 
rank(verb) -+ rank(norm) : (verb)-(determiner)-$-'in'-(determiner) 
Initially tagged POS Parsable POS POS context 
Figure 2: PA Rule 
sized training corpus. At present we set the 
context size to be two words. 
In choosing adequate levels of abstraction or 
specification of POS in the context, we grouped 
together those POS tags which influence the 
choice of POS of a focus word in a similar 
lnanner as one super-POS tag, as in (Haruno & 
Matsumoto 1997). We also changed some POS 
tags for functional words like prepositions and 
words such as "be' and "have' to tags which 
denote their literal forms, because the choice of 
POS of a focus word is highly dependent on the 
word itsclf. As a result, we obtained 513 POS 
tags including 16 POS tags for nouns, 17 for 
verbs, 410 for prepositions and phrasal 
prepositions, and 70 for adjectives and adverbs. 
3.2 Rule Filtering Module 
This section deals with how to statistically filter 
out inappropriate rules from the generated PA 
rule candidates. For this purpose, we introduce 
what we call "adjustment ratios." 
Table 1 shows the parsing process of a sentence 
in which word W appears in POS context C: P1- 
P2-$-P3-P4. In this context, the word W has two 
possible POS's, X and Y. Case A shows the case 
of balanced sentences where the tagger first 
tagged W with X and the parser found it 
parsable. Case B shows the case of conflicting 
sentences where the tagger first tagged W with 
Candidate Generation 
unparsable X and then with Y which proved to 
be parsable 2.
Let N,, and Nb be the number of semcnces in 
cases A and B, respectively. Assume the parser 
is accurate nough to be able to judge a majority 
of sentences with correct POS contexts to be 
parsable 3, and those with incorrect POS 
unparsable. 
Table 1 : Transition of POS of W 
in Parsing Process for Context C 
POSw, cX I A 
POSI~cX ---> POS,~cY B 
Then, adjustment ratios can be fornmlated as 
follows : 
2 Here only two possibilities, namely X and Y, arc 
considered. However it is easy to generalize the 
transition process for cases where focus words have 
more than two POS candidates. 
3 The accuracy of POS sequences accepted by our 
parser is more than 99% (Yoshimura 1995). 
4 Financial Times(1992-1994, approx. 210,000 
documents) in NIST Standard Reference Data TREC 
Document Database: Disk4 (Special No. 22), 
National Institute of Standards and Technology, U.S. 
Department of Commerce (http://www.nist.gov/srd). 
316 
N b adjustment ratio,:.lc (X --> Y ) - 
N, +N h 
When the value is high, the tagger should 
changc the POS from X to Y, whereas when the 
value is low, the tagger should not changc the 
POS in the given context. Thus, based on the 
statistics of an accurate parscr's judgement, 
adjustment ratios can be a criterion for the 
validity of PA rules. The rules whose adjustment 
ratios are above the threshold are extracted and 
output as PA rules. The threshold is fixed by 
examining PA rule candidates as will be 
lnentioned in the next section. More importantly, 
PA rules are considered to be "optimized' to the 
parser. First, the selection and application of 
inappropriate PA rules do not ilnmcdiately 
deteriorate he parser output, since PA rules only 
serve to eliminate wasteful generation of POS 
sentences. Second, the existence of inappropriate 
PA rulcs eventually shortens the processing time 
for those sentences for which the parser 
produces an errorneous syntactic structure due to 
a lack of syntactic knowledge. 
4 Rule Extraction Experiment 
We applied the method described in Section 3.2 
to English news articles (6,684,848 sentences, 
530MB) 4 as a training corpus and obtained 
300,438 different PA rule candidates. Since 
rules with low ficquencics do not have reliable 
adjustment ratios, we omitted rules with a 
frequency below 6 and thus obtained 17,731 
rules. 
Table 2: Adjustment Ratios and the Validity of Extracted 
Rules 
Adjustment I Total Valid (%) 
ratio(%) 
2O 
25 
24 
16 
15 
15 
15 
17 
16 
18 
29 
I 21ol 
0-9 
10-19 
20-29 
30-39 
40-49 
50-59 
60-69 
70-79 
80-89 
90-99 
100 
total 
Invalid (%) 
To verify the validity of adjustment ratio-based 
rule selection method described ill Section 3.2, 
we examined some of the obtained PA rules 
whose frequencies are 10, 20, and 30, referring 
to the original sentences froln which they were 
generated, and classified the rules into the 
following three categories. 
(p )  Valid: applicable to any sentence. 
(O)lnvalid: inapplicable to every sentence. 
This type of rule is derived when an 
incorrect POS sequence was judged to be 
parsable, due to a lack of coverage of 
parsing rules in the parser. 
(R)  Undecidable: The derived rule is neither 
valid nor invalid, either because the POS 
context or POS specifications are 
insufficient to uniquely determine the POS 
of the focus word, or because both the 
initially tagged POS and the parsable POS 
are inadequate for the POS context. 
An example of (3) is: 
trading(preseut particle) ---> trading(noun): 
(noun)-'oP-$-(dcterminer)-(noun) 
The word "trading" is a prcscnt pallicle in 
sentences like ".. index features represent a more 
convenient and liquid way of trading all index 
basket han ...," while it is a noun ill selltcnecs 
like "By the close of trading the deal was quoted 
at 99.82 bid." 
Table 2 shows the result of the classification. As 
is clear in the table, for adjustmcnt ratios bclow 
30 %, there arc more invalid rules than valid 
rulcs, and for adjustment ratios above 30 %, the 
converse is true. The percentage of 
invalid rules is small above 60 %. 
These results prove the validity of our 
adjustment ratio-based rule selection 
1 framework. By setting the threshold to 
5 60%, we can extract in an unsupervised 
10 manner PA rules of which 86% are 
6 valid and 7% invalid, but the presence 
4 of such invalid PA rules are unlikely to 
4 cause a serious deterioration, as 
0 mentioned previously. Incidentally, 
1 rules whose adjustment ratio is below 
0 30 % could be used as prohibition rules 
1 to be applied in the given POS contexts. 
4 These rules are not used in the next 
experiment. 
Undccid 
-able 
0 (0) 19 (95) 
3 (12) 17 (68) 
4 (17) 10 (42) 
8 (50) 2 (13) 
10 (67) 1 (7) 
7 (47) 4 (27) 
15 (1oo) o (o) 
15 (88) 1 (6) 
15 (94) 1 (6) 
14 (78) 3 (17) 
23 (79) 2 (7) 
114 60 36 
317 
Thus, we eliminated the extracted 17,731 rules 
whose adjustment ratio are below 60% and 
obtained 4,494 rules such as : 
group(V) --~ group(N): 
(ADV)-(N)-$-(NAME)-(CC) 
report(N) ---> report(V): 
(ADV)-','-$ -(NAME)-(NAME) 
related(VP) ---> related(PP) : 
(NAME)-(CC)-$-(N)-(PNC) 
open(V) --> open(ADJ) : 
(N)-','-$-(N)-',' 
further(V) --;, further(ADV) : 
'to'-(V)-$-(NU)-(DEM) 
where ADJ=adjective, ADV=adverb, 
CC=coordinate conjuction, DET=determiner, 
DEM=demonstrative, NAMEP=place name, 
N=noun, NAME=proper noun, NT=noun 
meaning "time", NU=noun meaning "unit", 
PP=past particle, PNC=punctuation mark other 
than commas, V=verb (other than past form), 
VP=verb (past form). 
5 Rule Application Experiment 
By using PA rules, we can expect hat: 
( P) the process time would be reduced by 
obtaining a parsable POS sequence at an 
earlier stage, and 
(O) both tagging precision and parsing accuracy 
would improve. 
To prove the above statements, we applied the 
3,921 PA rules 5 extracted in the previous 
experiment for tagging entirely different English 
news articles (146,229 sentences; 2.26M 
Words ) from the training corpus. Among them, 
2,421 sentences (1.7%) or 2,476 words 
(0.11%) satisfied the conditions of these PA 
rules, which were then tagged and parsed with 
and without the PA rules. We measured the 
difference in the elapsed time 6 and the number 
5 Out of 4,494, 573 rules have been eliminated in this 
experiment. These cases involved distinction between 
compound words (cx. "that is'(adverb)) and non- 
compound words (ex. "that(pronoun)+is(vex'b)'). This 
accompanies changes in the window of context, 
which requires further esearch. 
6 The elapsed time is measured on WorkStation SUN 
counted of successfully parsed sentences. The 
result is shown in Table 3. The tagging time was 
extended by 11.5%, but the parsing time and the 
total processing time were reduced by 24% and 
15.5%, respectively, while the ratio of 
successfully parsed sentences improved by 
8.0%. 
We also examined 524 POS differences out of 
all the resulting differences in the tagger's 
outputs made by the PA rules, and obtained the 
following figures. 
- Improved: 411 (78.4%) 
- Worse: 84 (16.0%) 
- Neither improved nor worse 29 (5.5%) 
Out of the 84 worsened cases, 43 were due to 
invalid rules acquired through wrong parsing 
because of a lack of sufficient parsing rules. 
There are highly fiequent expressions 
characteristic of financial reports which our 
parser cannot parse. However, again, this kind of 
invalid rules would not make a significant 
difference in the final output of the parser. The 
remaining 32 cases were due to learning fi'om 
wrongly segmented sets of words and also fiom 
distinct header expressions like "FT 14 MAY 91 
/ World News in Brief". These errors can be 
easily eliminated by not learning from these data. 
Adopting the rule accuracy obtained fiom the 
above examination, we can expect 62.4% 
(78.4% - 16.0%) improvement for words with 
PA-rule applied. Since PA-mles are applied to 
0.11% of the words in corpus, 0.07% 
improvement of POS tagging is expected. Wc 
measured the tagging precision with and without 
the acquired PA rules for a test corpus 
containging 5,630 words, and observed that the 
precision rose to 98.65% fi'om the initial 98.60%, 
i.e. 0.05% improvement. Since PA rules are 
lexically based rules, the ratio of sentences 
which satisfied the rule conditions is rather low, 
but the number of those sentences would 
increase in proportion to the number of PA rules 
acquired. 
If we expand the size of a training corpus, we 
could obtain much more PA rules. In fact, we 
observed many valid rules in the eliminated PA 
rule candidates whose frequency is immediately 
Ultra U 1E/200. 
318 
Table 3 : Processing Time and Parsable Sentence Ratio 
Tagging time(sec.) 
Parsing time(sec.) 
Total processing time(sec.) 
Parsable sentence ratio 
Without PA rules 
79.40 
252.17 
331.57 
64.0% 
With PA rules 
88.49(+9.09, + 11.5%) 
191.73(-60.44, -24.0%) 
280.22(-51.35, - 15.5%) 
72.0% 
below the threshold. Since the observed 
frequency distribution of PA rules was 
exponential, we can expect PA rules would 
increase xponentially by expanding the size of 
a training corpus. 
This expansion also enables us to specify POS 
context in detail, like widening the context 
window, subcategorizing POS tags employed in 
context, assigning onc surface fimctional word 
to a lexical tag, etc. To make detailed 
classification fully effective, we will need to 
generalize specific rules to the level that reflects 
the maximum distinction of individual 
examples. 
6 Conclusion 
In this paper we presented a new approach to 
acquiring linguistic knowledge automatically 
from plain text corpora, and proved its 
feasibility by the experiment. Our method 
utilizes well-developed modules in a NLP 
system, including a tagger and a parser, and 
enables us to extract wflid rules with high 
accuracy. It is robust in that the application of 
the extracted incorrect knowledge does not 
cause a serious performance deterioration. 
As a first step to obtaining lexically dependent 
knowledge, we examined the validity of 
obtained POS rules to measure the viability of 
our unsupervised learning method fi'om plain 
text corpora. In the future we will expand the 
size of training corpora and make use of invalid 
PA rules with a low adjustment ratio. 
References 
Brill, Eric. 1992: A Shlq)le Rule-Based Part of 
Speech Tagger, in Proceedings of the Third 
Conference on Applied Natural Language 
Processing, pp. 152-155. 
Brill, Eric. 1995a: 7)'an.y/brmation-Based Error- 
Driven Learning and Natural Language 
PJvcexsing: A Case Study in Part-oJ'-Speech 
Tagging, in Computational Linguistics, Volume 21, 
Number 4. 
Brill, Eric. 1995b: Unmq~ervised Learning of 
DLs'ambiguation Rules Jbr Part of Speech Tagging, 
Workshop on Very Large Corpora. 
Church, Kenneth. 1988: A Stochastic Parts Program 
and Noun Phrase Parser Jbr Um'estricted Text, in 
Proceedings of the Second Conference on Applied 
Natural Language Processing, Austin, Texas, 
pp.126-143. 
Haruno, Masahiko and Yuji Matsumoto. 1997: 
Mistalce-Driven Mixtm'e of HielzHvhical Tag 
Context Trees, in Proceedings of the 35th Annual 
Meeting of the Association of Compntational 
Linguistics, Madrid, Spain. 
llirakawa, tlideki, Hiroyasu Nogami and Shin'ya 
Amano. 1991 : E.I/.IE Machine TranslatioJl System 
ASTRANSAC-Extensions toward l'crsonalization, 
in Proceedings of MT SUMMIT-Ill, Washington, 
D.C., 1991, pp.73-80. 
Kupicc, Julian. 1992: Robust Part-of Speech Tagging 
Using a llidden Markov Model, Computer Speech 
& Language, 6(3), pp.225-242. 
Mikhecv, Andrei. 1996: Unsupervived Lcarnit~g qf 
Word-Categow Guessit N Rules, in Proceedings of 
the 34th Annual Meeting of the Association of 
Computational Linguistics, Santa Cruz, California. 
Voutilainen, Atro, Juha tteikkilfi and Arto Anttila. 
1992: CONSTRAINT GRAMMAR OF ENGLISII - 
A PelJbrmance-Oriented Intlwduction, Publications 
of the Department of General Linguistics, 
University of Helsinki, No.21. 
Yoshimura, Yumiko. 1995: Selection of Englis'h 
Part-of Speech Strings Uxing Syntactic Analysis 
hTJbrmation, in Proceedings of the 50th Annual 
Convention of IPS Japan, 3-65, March (in 
Japanese). 
319 
Kana-Kanji Conversion System with Input Support 
Based on Prediction 
Yumi ICHIMURA,  Yoshimi SAITO, Kazuhiro K IMURA and Hideki HIRAKAWA 
I Iuman Interface l ,aboratory  
Corporate  Research & Development  Center, TOSt t IBA  Corp. 
1, Komukai-Toshiba-cho,  Saiwai-ku, Kawasaki-shi ,  
Kanagawa,  212-8582 Japan 
yumi. i chimura@t oshiba, co. j p 
Abst ract  
We propose ~ kana-ka'~ji conversion systeln with 
input support based on prediction. This sys- 
tem is composed o\[' l,wo pm'ts: prediction of 
succeeding ka'1~:ji charac/;er st, rings from l,yped 
l,:ana ones, and ordinary k(vna-lca'l~(\]i conversion. 
It automatically shows candidates of kanji char- 
acter strings which the user intends to input. 
Ore' prediction method features: (i)Arbitrary 
positions of typed t,:ana character slarings are re- 
gm'ded as l,he top of words. (ii)A system dictio- 
nary and a user dictionary are used, and eadl 
entry in the systcln dictionary has (:erl, ai.nly fac- 
tor calculated fl'om the frequency of' words in 
corpora. (ill)Candidates are estimated by cer- 
tainty factor and 'us@l, lness factor, and likely 
ones with greater \['actors than l;hresholds are 
shown. The proposed system could reduce the 
user's key inlmt operations to 78% from the 
original ones in ore' experinmnts. 
1 In t roduct ion  
TOSHIBA developed the world's first Japanese 
word processor in 1978. Unlike languages based 
on an alphabet, Japanese uses /,housands of 
Ica nji characters of varying comp\]exity. Hence, 
l,o arrange all of l~a'~:ii chm'acl;ers on keyboard 
is; difficult. On the other hand, kana dlarac- 
ters which are phonetic scripl,s of Japanese have 
83 variations; these can be arranged on key- 
boa '& As a result, conversion from kana no- 
tations to kanji ones, whal, is called kana-ka,~:ji 
conversion, has been used. Since Japanese is 
not writl,en separately by words, segmental,ion 
of typed kana characl,er strings has ambiguil,y. 
And ~m ambiguil,y in conversion exists, too; a 
kana notation may correspond l,o some different 
t:a, nji notations. These make t,:ana-kanji conver- 
sion challenging. 
We have made efforts l,o raise a precision of 
kana-kaTt:ji conversion, thinking l,h~t high pre- 
cision can provide a better input enviromnent 
for the user. A t)rccision of our kana-kanji con- 
version system reaches 95-98% for several kinds 
o\[' texts in our previous experiments. Neverthe- 
\]ess, this approach is not; enough in the situa- 
tions where fasl; typing is hard, e.g., lbr begin- 
ners who m'e not familiar wil,h keyboard or for 
palm-size comlmters. Thus, new method to re- 
duce key input; operations is needed. 
We propose a k(vna-kanji conversion system 
witll input support based on prediction. This 
system is composed o\[' two parts: prediction of 
succeeding ka'nji character strings t\]'om typed 
kana ones, and ordinary hrvna-kanji conversion. 
It; automatically shows candidates of ka'nji char- 
acter strings which the user intends to input. 
The candidates change as /;lie usc, r inputs ka.na 
characters. If no approt)riate ctloice is pre- 
senl;e(t, the candidates aul;Olnatictd\]y disappear 
when the ne?t kana character is entered. Our 
system, l,here\['ore, can be used in the same man- 
ner as an ordinary ka'na-kanji conversion sys- 
tem, and allows the user to save time and efforts 
for key input without learning new key opera- 
tions. 
We have been considered two issues to gener- 
ate accurate candidates: 
(i) How Co determine where t;yped kana char- 
acter strings are segmented; since Japanese is 
not written separately by words, determination 
of positions where words start is needed to re- 
trieve dictionaries. 
(ii) How to determine when prediction candi- 
dates are presented; if all of retrieval results are 
always shown, a sysl,em cannot be convenielfl,. 
Surveying previous works Dora the view on 
above issues, we found l,hal, the Reacl,ive Key- 
board has been proposed (Darragh el, al., 1980). 
It accelerates typewritten communication with 
341 
a computer system by predicting what the user 
is going to type next. In this system, the top 
of typed ct~aracter strings is regarded as the top 
of words, because English is written separately 
by words; the issue of segmentation f character 
strings does not occur. 
On the other hand, in previous works for 
Japanese, a predictive pen-based text input 
method has been proposed (Fukushima nd Ya- 
lnada, 1.996). In this system, character strings 
are input by h~md-writing on LCD panel. Since 
the user usually inputs not only by kana but 
also by t~anji and an alphabet, entered charac- 
ter strings are segmented with the help of the 
wu-iety of characters. Thus, the issue of segmen- 
tation is not considered. 
The POBox (Pen-Operations Bused On eX- 
ample) which is a text input method tbr pen- 
based computers has also been proposed (Ma- 
sui, 1998). It shows succeeding candidates from 
cl~aracter strings input by software keyboard. 
Arbitrary positions of input character strings 
can be regarded as the top of words, and re- 
trieval results are always shown as candidates; 
the prediction ordering is based on the user's 
previous choice. Since input speed by pen is 
not faster than that by keyboard, time to choose 
candidates i shorter than that to input c.harac- 
ters. Hence, even if many candidates are shown, 
this n~ethod is effective for pen-based comput- 
ers. It is, however, inefficient for ordinary key- 
board. 
We propose a system with following fea- 
tures: 
(i) Arbitrary positions of typed kana character 
strings are regarded as the top of words. 
(ii) A system dictionary and a user dictionary 
are used, aud each entry in the system dictio- 
nary has certainty factor calculated from the 
frequency of words in corpora. 
(iii) Candidates are estimated by certainty fac- 
tor and useful'hess factor; and likely ones with 
greater factors than thresholds are shown. 
These features provide an cfl3cicnt Japanese text 
input environment for ordinary keyboard with- 
out learning new key operations. 
Section 2 shows an example of text input us- 
ing the proposed system. Section 3 explains an 
input support method based on prediction. Sec- 
tion 4 shows efficiency of our system by means 
of experiments. Section 5 describes conclusions. 
2 Example of Text  Input  
Figure 1 shows an example of text input using 
tile proposed system. Suppose that tile user in- 
tends to input a sentencc , , , ,v,~,~ 
2- co -e ~" e$ & N ~ ~ ~ ~- (we rcqucst yore" atten- 
dance at the following meeting)", typing ha.ha 
/u b ~ 9 ~ ~ ~. ~- (katcino kaigiwo kaisai shi- 
maswnodc gosanshuu ncgaimasu)". When the 
user types "\]~' (ha)", "~ (hi)",  and "CO (no)" 
keys, the system antomatically opens a predic- 
tion menu window just below the typed char- 
actcrs, and shows two candidates in the menu 
window (Fig.l(a)): 
(at the following address, modest .... ) 
(al. the following address, modest .... ) 
The first candidate is high\]ighted. If the menu 
window contains an appropriate candidate, the 
user can choose it by cursor; otherwise the user 
can continue ntering the next characters. Sub- 
sequently, when "\]0, (lea)" key is typed, the 
predic.tion menu window disN)pears (Fig.l(b)). 
When "~ (i)" and "~" (gi)" keys are typed, the 
system automatically opens a prediction menu 
window again, and shows %ur new candidates 
(Fig.l (e)): 
(we request your a.ttenda.nce a.t the meeting) 
(we hold the meeting) 
(we hold the meeting) 
(we hold the meeting) 
Here the first one is what the user just wants; 
the user enters select key, thcn the prcdiction 
menu window disappears, and dmsen candidate 
is insertcd in the cdit area. If remaining tcana 
charactcr string wtfich was not included in the 
chosen cundidate xists, l~a,na-l~anji conversion 
starts automatically; the first three Icana c.har- 
acters of this sentence "?a~  ~ (l~ahino)" is con- 
verted to tcanji notation 'q'~,co (the following)" 
(Fig.l(d)). This is the first result of t~ana-kanji 
conversion, so that the user can d~ange it to 
others. An overline of thc conversion rcsult in 
Fig.l(d) shows that this result is not fixed yet. 
In above example, while 27 ~ana d~aracters 
are needed to input in ordinary l~ana-t~anji con- 
342 
(a) 
I~A l : : l -w~ ~?j  U4 118 0 I0 1- l -  I 
?o) 
~?.~ 1/4 1/8 0 10 1- | -  i 
(c) 
NA ~- :z -~ ~.?  I/4 I/8 0 I0 I .  I -  1 
(d) 
NA H-w~ f~3 114 1/8 0 10 t - l - I  
Figure 1: Example of Japanese text input using 
word processor with input support. (a)";~J '', "-~", 
and "03" keys are typed. (b)"~ ~'' key is typed, sub- 
sequently. (e)"~ ~'' and "{s'" keys are typed, subse- 
quently. (d)The first candidate in (c) is chosen. 
version, our system can reduce the input of 21 
~:ana characters, "*L ~b' ~ ~ t~ ~ ~ b ~ J-c0 ~ "~" ~ \]u b e~ 
9 talo~  ~ J- (wo kaisai shimasunode gosanshuu 
negaimasu)"; only 6 kana characters are needed 
to input. 
3 Input Support Method Based on 
Prediction 
In this section, an overview of the system is 
shown. Then dictionaries used in the system, 
factors for estimation of candidates, and user 
learning are described. 
. . . . . . . . . . . .  -\[ InLut Character St ings 1 
Kana-Kemji Coi~,'elsiot~ 
Ka,~a-Kanji Conve,sion '~ l -~~. '~~ 
tJ_ Input Prediction U it 
Chomc ter Sub-strings \] ~_ __~ 
F dictionCandidates I I 
"1 Estimation u~t I ~ . . . . . . . . .  Y -.-J 
-~  UserLeanmlgUldt } - - J  
(Kam-K~ji Conversion \] Adopted byUs2;-"' \[ C~ida~s ) 
Figure 2: Diagram of the proposed system? 
3.1 Overv iew of  the  sys tem 
Figure 2 shows a d iagram of the proposed sys- 
tem. It is composed of a kana-hanji conversion 
unit and au input prediction unit, and the latter 
has following four sub-units: 
Character Sub-strings Generation Unit(a) gener- 
ates character sub-strings obtained from seg- 
mentation of typed kana character strings. 
Dictionary Retrieval Unit(b) retrieves prediction 
dictionaries using character sub-strings gener- 
ated by Unit(a). 
Prediction Candidates Estimation Unit(c) calcu- 
lates certainty factor and usefulness factor for 
all of retrieved results by Unit(b) to estimate 
candidates? 
User Learning Unit(d) extracts phrases adopted 
by the user, and automatically registers them 
into the user dictionary. 
3.2 P red ic t ion  D ic t ionary  
Two kinds of dictionaries are used as a predic- 
tion source: 
(i) System Dic t ionary  consists of high frequent 
phrases. 
343 
(ii) User Dictionary consists of phrases learned 
fl'om texts which the user typed before. 
Ead~ dictionm'y includes words and phrases 
without distinction. This is because Japanese 
is not written separately by words, and high 
fl'equent phrases consist of various grammatical 
forms, sud~ as single word or two words. And 
eadt entry has kana notation (phonetic script) 
and kanji one. 
3.3 Est imat ion of  Predict ion 
Candidates 
Two kinds of factors are used to estimate can- 
didates: 
(i) Certainty Factor indicates how certain a can- 
didate is. 
(ii) Usefulness Factor indicates how useful a can- 
didate is. 
These two factors vary as the user inputs a 
character. Retrieval results are sorted in order 
of these factors, and only ones with greater fac- 
tors than thresholds are shown as candidates. 
3.4 Calculat ion of  Certainty Factor 
Certainty factors fox" entries in the system dic- 
tionary and the user dictionary are calculated 
in different manner. 
First we make some notational conventions. 
A typed kana character string is denoted by S, 
which has right sub-strings S; (1 < i < L(S)). 
L(x) is the length of a string x. An entry in 
the dictionary is denoted by W, which has kanji 
notation WH and kana notation Wy. 
3.4.1 Ent ry  of  System D ic t ionary  
When S is typed, certainty factor fox' W in the 
system dictionary is calculated as follows: 
Certainty f actor(WlS ) = 
Ft~(WH) 
F l ( (S i )  ' 
O, 
when S has a right sub-string Si 
whid~ partiMly matches with the 
head of Wy 
otherwise 
where F~:(WH) is the frequency of WH in kanji 
notation corpus, and FK(&) is the fl'equency 
of Si in kana notation corpus corresponding to
kanji one. 
For example, certainty factor :\['or "~' ~ ~-q':'~ 
~ (ka,na-kanji conversion)" is calculated using 
the frequency in Table 1: 
Table 1: Frequency for "\]a~ta:f~q':'~f~" in two 
corresponding corpora: kanji notation corpus with 
155,000 characters, and kana one with 227,000 char- 
acters. 
Character strings Frequency in 
kana notation corpus 
iJ~ 6,720 
~a~ 191 
ia~ Y'a: Yo~  12, 87 
\]a' ~a: \]o' tu 12, ',tu 77 
IJ'/a Io' A~ D ~,AJ IJ' 76 
iJ' ~a: io' k~ 15 ",tu lo' /u 76 
Clmrac/er strings Frequency in 
kanji nota.tion corpus 
Certainty f ac lo r (~a '~.~f4  I ?J't*. ) 
= 70/191. = 0.366 
Ccrl, ai'n, ty .f actor(ia' ~ "?~-~:~~ I io' re: "h' ) 
= 70/114 = 0 .614  
The values of certainty factor corresponding 
to every character sub-strings are described in 
tim system dictionary, and are read out at re- 
tri eval. 
3.4.2 Entry of  User Dict ionary 
Since t.he system cannot infer which phrases 
would be registered into the user dicl, ionax'y, cal- 
culation of certainty factor \['or an entry in the 
user dictionary from corpora may 10e impossible. 
Hence, when S is typed, certainty factor fox' W 
in the user dictionary is calculated as follows: 
Certainty f actor (W IS) = 
O, 
when S has a right sub-string & 
which partially m attires with the 
head of Wy 
otherwise 
where N(Si) is the number of entries whose 
kana notations tart from Si in the user dictio- 
nary, and c~ is a constant o give greater factor 
tbr entries in tile user dictionary than that in 
the system dictionary; i.e., tile user dictionary 
has priority. 
344 
3.5 Ca lcu la t ion  o f  Use f idness  Factor  
An increase in tile length of typed tca'n,a d~m'- 
acter strings raises the certainty on prediction, 
but  lessens the usefulness. Hence, 'us@d'n, css 
factor is introduced in addit ion to certain, ty fac- 
tor. When S is typed, "us@tl'ness factor for W 
is calculated as tbllows: 
Use f uI,n.ess f actor(WlS ) = 
{ 
O, 
when S has a right sub- 
string & which partial ly 
matches with the head 
of Wy 
otlmrwise 
3.6 User  Learn ing  
Aft, re' the user adopts prediction or t:a~ta-ha.n:ji 
conversion cmldidates, words wit;ll longer length 
than threshold and phrases which sat, isfy given 
grammatical  conditions are extracted; these are 
automaticMly registered int, o the user dictio- 
nm'y. 
Por example, suppose, that  the user intends 
(;() input a phrase """*'~ 
mcet,ing)", typing 1,:a'na chm'acl, ers "#' v~g V- b 
r;,o ~ ~ g- ~ (ko.igi'ni sh,'ussetci sn, ru,)". When "#' 
(ka)", %~ (i)", and "~" (gi)" keys ~we typed, \['om' 
candidates are sllowu in the predict.ion metal 
window (Fig.l (c) ). IIcre tlm predict;ion menu 
window dose not cont:ain a candidate wlficll 
the user wants, the, ll tlm user conl;imms enter- 
tug the next; l~:a'n,a characters "k  b r;,v ~ ~ 3- ,5 
('hi sh'ussehi s'uru,)" and l,:a'n,a,-/,:an:ji cmwersion 
key. As a result, bb' ~ ~-~" V- b r;,-o -g ~ 3- ~ (l,;a, igini 
,stu~ssehi suru)" is convcrt, ed to ~,aL,l+),~,3-~ 
(attend 31; the meeting)".  
When this conversion candidate is adopted, 
two words ml(t a phrase are regist, ered into the 
user dictionary: ~N (meeting)", "/15/,'i~g-~ (ag- 
~_~'~L ,I,))I,~Y ~ (M;tend at, the meet- tend)",  ~md "  - " "" 
ing)". 711" 7~, (lea)", %' (i)", and "~" ((.It)" keys 
I I I 't;  ')~ are typed after this learning, "~q~V-,bI, u 3-~ is 
contained in the predict ion menu window. 
4: Exper iment ,  s 
Efficiency of the proposed system is shown by 
means of' experilnents. 
4.1 Eva luat ion  Measures  
Neither st;art key for tn'ediction or cancel key 
for prediction candidates are needed. And st- 
leer, key to ~dopt. candidates i needed in both of 
t)rediction mid ordinary ha.ha-ha,nit conversion; 
we need not; tn~ke into account of l;he input of 
select; key. tIence, the length of complemented 
hana c, haracters is just a decrease in key input; 
operations. Two evaluation measures, an opera- 
tion ratio and a precision, are defined as follows: 
P-Q 
Operation, ratio = p x 100 (%) 
R 
Precision, = S x 100 (%) 
where P is the length of the original ha'ha 
text;, Q is the length of ha'ha chm'acters com- 
plemented by prediction~ R is the number of 
sllown prediction menu windows contMning ap- 
propriate choices, and S is tile number of ~dl of 
shown prediction menu windows. 
4.2 Data  and  Cond i t ions  
Two kind of texts, a paper on natm'al anguage 
processing and a let;ter, were used in ore" ex- 
tmriments; these l, exl, s were not included in the 
corpora used to calculate certainty factor: A 
system dictionary with 37,926 entries was used. 
Thresholds of certai'nl, ll factor mid 'us@d.ness 
fa, ctor were 0.1 and 2. q~he numl mr of candidates 
present.ed in ~ predict;ion mmm window was five 
or less. If ~ prediction menu window contained 
ml ~l)propri~t,e ('hoice~ it, was ~lw~ys adopted. 
With ~v view to ex~mdnin 8 eacl~ contri lmtion o\[' 
the system (tict;ionary and user learning, exper- 
iments were carried out in three cases: 
(i) Only the systeln dictionary was used. 
(it) Only user learning was used. 
(iii) Bot;h t\]tc syst, em dict ionary mid user learn- 
ing were used. 
~Ve calculated the lengt;h of complemented ha'ha 
du~r~,ct;ers automaticMly. An operation ratio 
and a precision were recorded at; every input 
of 4,500 lcana characters. 
4.3 I I ,esults 
Pigm'e 3 shows experimental results. 
Decrease in key input operations: Using both the 
system dictionary and user learning, for the pa- 
per, an operation ratio was 97.3-78.6% (line 
(r3) in Fig.3(a)) and a precision was 20.0 -26.7% 
(line (p3) in Fig.3(c)); for the letter, an opera- 
tion rat;k, was 80.7 78.1% (line (r3) in Fig.3(b)) 
and a precision was 26.1--29.6% (line (p3) in 
345 
100% 
95% 
,.--., 
90% 
85% 
0 
0 
~'~ 65% 
0 60% 
55% 
50% i t i t i T T t 
o (~ o o o o o o o o 
o o o o o o o o o o 
00Pal)er ? the llUlllber of  
input characters 
o 
0 
t~ 
? 
100% 
95% 
90% 
85% 
80% 
75% 
70% 
65% 
60% 
55% 
50% 
,' "~- -~.  - - . .  
r , r i i i i i 
(b) Let ter  the nttmber of 
input chracters 
. . . .  ( r l )Sys tem Dict i0naO" 
. . . . . . .  ( r2 )U  ser  Learn ing  
- -  ( r3 )System D ic t ionary  + 
U set  Learn ing  
50% 
45% 
t0% 
"--" 35% 
30% 
0 25% 
"~ 20% 
10% 
5% 
f--2__t=--_==~ 
, /  
50% 
45% 
40% 
/- -"  35% 
30% 
,~ 25% 
G'2 
"~ 205'; 
'.I) 
t 15% 
10% 
5 % 
-- . . . . .  - "  _ : .  - . . . . . .  
0% t r r , ~ ~ , r 0% , ~ , ~ , r , , 
e3 ~ ~3 o u~ ~ v5  o ~3 o e5  o v5  o o ~ c ,  e5  cD 
"* ~" ?21 ~ ~ ~q ~ ~ ~ ~ ~ ~ +.; ~ +~, ,-, .+ .t  
(c) l?~q)er " the mtmber of  (d) Letter " the munber of  
input characters input characters 
. . . .  (p  I )Sys tem D ic t ionary  
. . . . . . .  (p2)User  Learn ing  
- -  (p 3 )System D ic t ionary  + 
User  Learn ing  
Pigure 3: Experimental Results. (a.)Operation ratio for the paper. (b)Operation ratio for the letter. 
(c)Precision for the paper. (d)Precision for the letter. 
Fig.3(d)). When 45,000 ka~a characters were 
typed, an average of the operation ratio was 
78%, that is, a 22% decrease in the original op- 
erations was obtained; an average of the pre- 
cision was 25~)~, that is, a quarter of shown 
prediction menn windows contained appropri- 
ate choices. This precision was enough to realize 
comfortable operations. 
Contribution of the system dictionary: Using only 
the system dictionary, for the paper, an opera- 
tion ratio was 97.6-96.6% (line (rl) in Pig.3(a)), 
that is, a 2.4-3.4% decrease in the original op- 
erations was obtained; for the letter, an opera- 
tion ratio was 90.6-78.8% (line (rl) in Pig.3(b)), 
that is, a 9.4-21..2% decrease in the original op- 
erations was obtained. As a result, the sys- 
tem dictionary is effective for a text like a let- 
ter with idioms or common phrases, because 
346 
the system dictionary includes a lot; of such 
phrases. Furthermore, eomt)ared a t)recisiou s- 
ing both the system dictionm'y and user lewcning 
wi/;h that using only user learning, the \['orlner 
was worst for the paper (lines (t)2) and (p3) in 
Fig.ate) (d)). As a result, for some kind of texts, 
the system dictionary not only is ineffective but 
also reduces a prec.ision. 
Contribution of user learning: User learning had 
an effect tbv an operation ratio aI'te.v more than 
9,000 ka'na chm'acters were typed (lines (r2) in 
Fig.(a)(b)). In fact, if the user types about ten 
pages of texts, a 15 20% decrease in the original 
ot)erations can be obtained. 
5 Conc lus ions  
We proposed a l~ana-ka'nji (:onversion system 
with input sut)port based (m prediction. Ore' 
system thatures: 
(i) It; shows prediction cmldid~tes of 1,:a'@i char- 
acter strings which the 11sev int, ends to input 
without any special key operation. I\[' no apt)ro- 
priate choice is presented, the candidates disap- 
pear automatically when the next l,:a'na charac- 
ter is entered. 
(it) Arbit;rary positions o\[' typed l:ana cJlavacter 
str i l lgS are  reg~w(le(t as the top of words. 
(iii) A system dietio1~avy and a user dictionary 
ave used, and each entry in the system (ticLio- 
nary has certainty factor calculated :from the 
frequency of words in corpora. 
(iv) Candidates ave estimated by certainty fac- 
tor and use.fill.hess factor, and likely ones with 
greater \['actors than thresholds rare shown. 
(v) ~Zords and phrases are extracted \['rein typed 
texts, and registered into the user dictionary au- 
tomatically. 
The proposed system could reduce the. user's 
key input operations to 78% h'om the origi- 
nal ones in the experiments using two kinds of 
texts. This system was built into the TOSII IBA 
Japanese word processor, the JW-8020, which 
was released in autumn 1.998 (Fig.4). 
To raise a precision by field information and 
context of texts is our lhtm'e work. 
Figm'e 4.: TOSHIBA Japanese word processor, the 
JW-8020. where the proposed systenl was built. 
Profet, a new generation of word prediction: '\n 
eva.luation study. ill Proceedings of I,h,e A CL 
Wo'rt,;shop on Natu, ra, l Language Proccssin 9 fin, 
Co'm, munication Aids, pages 23-28, July. 
John J. Darragh, Ian H.Witten, and Ma.rk L. Ja.mes. 
1980. The reactive keyboa.rd: A predictive typing 
aid. IEEE Corn.purer, 23(11):41-49, November. 
Toshikazu Fukushinm a.nd Hiroshi "~amada. 1996. A 
predictive pen-based ja.pa.nese text input method 
and its (,valuation (in japanese). Journal of \['Jhlbr- 
"m.atio'n Processing Socie*y of ,Japan,, 37(1):23 30. 
Nestor Garay-Vitoria. and .lulio G. Abas(-al. 1997. 
\?ord prediction for inflected languages, a.pplica- 
lion to basque la.nguage. In Prtmcedi.nfls of the 
ACL Worl,:sh.op on Natural Lan:f~m,.qe Proeessin:l 
for Co'm,'m,u'nication Aids, pages ')9 35, July. 
Toshiyuki Masui. 1998. An efficient text input 
nlelhod for pen-based computers. In Proceed- 
i'ngs of lh.c A CM Conference on Hum, an Factors 
in Comgrttli'ng Systcm, s (CHI'98), pages 328-335. 
.:\ ddison-\?esley, April. 
Masa.ka.l.su Sugimoto. 1997. Input scheme for 
ja.panese text with shk keycard (in japa.nese). In 
\[PSJ S\[GMBL Report No.l, pages 1-6, May. 
Re ferences  
Alice Ca.rlberger, Joh~m C~rlberger, Timt Ma.gnu- 
son, Sira, E. Palazuelos-Cagigas, M. Sha.ron Hun- 
nicutt, and Santiago Aguilera Naya.rro. 1997. 
347 
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 361?368,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Graph Branch Algorithm: An Optimum Tree Search Method for Scored
Dependency Graph with Arc Co-occurrence Constraints
Hideki Hirakawa
Toshiba R&D Center
1 Komukai Toshiba-cho, Saiwai-ku,
Kawasaki 210, JAPAN
hideki.hirakawa@toshiba.co.jp
Abstract
Various kinds of scored dependency
graphs are proposed as packed shared data
structures in combination with optimum
dependency tree search algorithms. This
paper classifies the scored dependency
graphs and discusses the specific features
of the ?Dependency Forest? (DF) which is
the packed shared data structure adopted
in the ?Preference Dependency Grammar?
(PDG), and proposes the ?Graph Branch
Algorithm? for computing the optimum
dependency tree from a DF. This paper
also reports the experiment showing the
computational amount and behavior of the
graph branch algorithm.
1 Introduction
The dependency graph (DG) is a packed shared
data structure which consists of the nodes corre-
sponding to the words in a sentence and the arcs
showing dependency relations between the nodes.
The scored DG has preference scores attached to
the arcs and is widely used as a basis of the opti-
mum tree search method. For example, the scored
DG is used in Japanese Kakari-uke analysis1
to represent all possible kakari-uke(dependency)
trees(Ozeki, 1994),(Hirakawa, 2001). (McDon-
ald et al, 2005) proposed a dependency analysis
method using a scored DG and some maximum
spanning tree search algorithms. In this method,
scores on arcs are computed from a set of features
obtained from the dependency trees based on the
1Kakari-uke relation, widely adopted in Japanese sen-
tence analysis, is projective dependency relation with a con-
straint such that the dependent word is located at the left-hand
side of its governor word.
optimum parameters for scoring dependency arcs
obtained by the discriminative learning method.
There are various kinds of dependency analy-
sis methods based on the scored DGs. This pa-
per classifies these methods based on the types
of the DGs and the basic well-formed constraints
and explains the features of the DF adopted in
PDG(Hirakawa, 2006). This paper proposes the
graph branch algorithm which searches the opti-
mum dependency tree from a DF based on the
branch and bound (B&B) method(Ibaraki, 1978)
and reports the experiment showing the computa-
tional amount and behavior of the graph branch
algorithm. As shown below, the combination of
the DF and the graph branch algorithm enables the
treatment of non-projective dependency analysis
and optimum solution search satisfying the single
valence occupation constraint, which are out of the
scope of most of the DP(dynamic programming)-
based parsing methods.
2 Optimum Tree Search in a Scored DG
2.1 Basic Framework
Figure 1 shows the basic framework of the opti-
mum dependency tree search in a scored DG. In
general, nodes in a DG correspond to words in
the sentence and the arcs show some kind of de-
pendency relations between nodes. Each arc has
 
Scored Dependency 
Graph
Dependency
Tree
Set of Scored Well-
formed Dependency 
Trees
Well-formed 
dependency tree 
constraint
Optimum Tree
Search 
Algorithm
Well-formed Dependency 
Tree with the highest score
s1
s2
s3
s4 s5
(score=s1+s2+s3+s4+s5 )
Figure 1: The optimum tree search in a scored DG
361
a preference score representing plausibility of the
relation. The well-formed dependency tree con-
straint is a set of well-formed constraints which
should be satisfied by all dependency trees repre-
senting sentence interpretations. A DG and a well-
formed dependency tree constraint prescribe a set
of well-formed dependency trees. The score of a
dependency tree is the sum total of arc scores. The
optimum tree is a dependency tree with the highest
score in the set of dependency trees.
2.2 Dependency Graph
DGs are classified into some classes based on the
types of nodes and arcs. This paper assumes three
types of nodes, i.e. word-type, WPP-type2 and
concept-type3. The types of DGs are called a word
DG, a WPP DG and a concept DG, respectively.
DGs are also classified into non-labeled and la-
beled DGs. There are some types of arc labels
such as syntactic label (ex. ?subject?,?object?)
and semantic label (ex. ?agent?,?target?). Var-
ious types of DGs are used in existing sys-
tems according to these classifications, such as
non-label word DG(Lee and Choi, 1997; Eisner,
1996; McDonald et al, 2005)4, syntactic-label
word DG (Maruyama, 1990), semantic-label word
DG(Hirakawa, 2001), non-label WPP DG(Ozeki,
1994; Katoh and Ehara, 1989), syntactic-label
WPP DG(Wang and Harper, 2004), semantic-label
concept DG(Harada and Mizuno, 2001).
2.3 Well-formedness Constraints and Graph
Search Algorithms
There can be a variety of well-formedness con-
straints from very basic and language-independent
constraints to specific and language-dependent
constraints. This paper focuses on the following
four basic and language-independent constraints
which may be embedded in data structure and/or
the optimum tree search algorithm.
(C1) Coverage constraint: Every input word has
a corresponding node in the tree
(C2) Single role constraint(SRC): No two nodes
in a dependency tree occupy the same input
position
2WPP is a pair of a word and a part of speech (POS). The
word ?time? has WPPs such as ?time/n? and ?time/v?.
3One WPP (ex. ?time/n?) can be categorized into one or
more concepts semantically (ex. ?time/n/period time? and
?time/n/clock time?).
4This does not mean that these algorithms can not handle
labeled DGs.
(C3) Projectivity constraint(PJC): No arc crosses
another arc5
(C4) Single valence occupation constraint(SVOC):
No two arcs in a tree occupy the same valence
of a predicate
(C1) and (C2), collectively referred to as ?cover-
ing constraint?, are basic constraints adopted by
almost all dependency parsers. (C3) is adopted
by the majority of dependency parsers which are
called projective dependency parsers. A projective
dependency parser fails to analyze non-projective
sentences. (C4) is a basic constraint for valency
but is not adopted by the majority of dependency
parsers.
Graph search algorithms, such as the Chu-
Liu-Edmonds maximum spanning tree algorithm
(Chu and Liu, 1965; Edmonds, 1967), algorithms
based on the dynamic programming (DP) princi-
ple (Ozeki, 1994; Eisner, 1996) and the algorithm
based on the B&B method (Hirakawa, 2001), are
used for the optimum tree search in scored DGs.
The applicability of these algorithms is closely re-
lated to the types of DGs and/or well-formedness
constraints. The Chu-Liu-Edmonds algorithm is
very fast (O(n2) for sentence length n), but it
works correctly only on word DGs. DP-based al-
gorithms can satisfy (C1)-(C3) and run efficiently,
but seems not to satisfy (C4) as shown in 2.4.
(C2)-(C4) can be described as a set of co-
occurrence constraints between two arcs in a DG.
As described in Section 2.6, the DF can represent
(C2)-(C4) and more precise constraints because it
can handle co-occurrence constraints between two
arbitrary arcs in a DG. The graph branch algorithm
described in Section 3 can find the optimum tree
from the DF.
2.4 SVOC and DP
(Ozeki and Zhang, 1999) proposed the minimum
cost partitioning method (MCPM) which is a parti-
tioning computation based on the recurrence equa-
tion where the cost of joining two partitions is
the cost of these partitions plus the cost of com-
bining these partitions. MCPM is a generaliza-
tion of (Ozeki, 1994) and (Katoh and Ehara, 1989)
which compute the optimum dependency tree in a
scored DG. MCPM is also a generalization of the
probabilistic CKY algorithm and the Viterbi algo-
5Another condition for projectivity, i.e. ?no arc covers top
node? is equivalent to the crossing arc constraint if special
root node , which is a governor of top node, is introduced at
the top (or end) of a sentence.
362
 

agent1,15
Isha-mo
(doctor)
Wakaranai
(not_know)
Byouki-no
(sickness)
Kanja
(patient)
target2,10
agent3,5
target4,7
in-state7,10
agent5,15
target6,5
OS1[15]: (agent1,15)
OS3[22]:  (agent1,15)   +   (target4,7)
OS2[10]:  (in-state7,10)
OS4[25]:  (agent5,15)  +  (in-state7,10)NOS1[10]: (target2,10)
NOS2[20]:  (target4,10) + (in-state7,10)OS1[15]: (agent1,15)
OS4[25]:  (agent5,15)   +  (in-state7,10)
Well-formed optimum solutions for covering whole phrase
Figure 2: Optimum tree search satisfying SVOC
rithm6. The minimum cost partition of the whole
sentence is calculated very efficiently by the DP
principle. The optimum partitioning obtained by
MCPM constitutes a tree covering the whole sen-
tence satisfying the SRC and PJC. However, it is
not assured that the SVOC is satisfied by MCPM.
Figure 2 shows a DG for the Japanese phrase
?Isha-mo Wakaranai Byouki-no Kanja? encom-
passing dependency trees corresponding to ?a pa-
tient suffering from a disease that the doctor
doesn?t know?, ?a sick patient who does not know
the doctor?, and so on. OS
1
-OS
4
represent the op-
timum solutions for the phrases specified by their
brackets computed based on MCPM. For exam-
ple, OS
3
gives an optimum tree with a score of 22
(consisting of agent1 and target4) for the phrase
?Isha-mo Wakaranai Byouki-no?. The optimum
solution for the whole phrase is either OS
1
+OS
4
or OS
3
+OS
2
due to MCPM. The former has the
highest score 40(= 15 + 25) but does not satisfy
the SVOC because it has agent1 and agent5 si-
multaneously. The optimum solutions satisfying
the SVOC are NOS
1
+ OS
4
and OS
1
+ NOS
2
shown at the bottom of Figure 2. NOS
1
and
NOS
2
are not optimum solutions for their word
coverages. This shows that it is not assured that
MCPM will obtain the optimum solution satisfy-
ing the SVOC.
On the contrary, it is assured that the graph
branch algorithm computes the optimum solu-
tion(s) satisfying the SVOC because it com-
putes the optimum solution(s) satisfying any co-
occurrence constraints in the constraint matrix. It
is an open problem whether an algorithm based
on the DP framework exists which can handle the
SVOC and arbitrary arc co-occurrence constraints.
6Specifically, MTCM corresponds to probabilistic CKY
and the Viterbi algorithm because it computes both the opti-
mum tree score and its structure.



Constraint
Matrix
Dependency 
Graph
Meaning of Arc Name
sub : subject
obj : object
npp : noun-preposition
vpp : verb-preposition
pre : preposition
nc : noun compound
det : determiner
rt : root
	
	
	


	


 




npp19
det14
pre15vpp20
vpp18sub24
sub23obj4
nc2 obj16
0,time/n 1,fly/v
0,time/v 1,fly/n
2,like/p
2,like/v
3,an/det 4,arrow/n
root
rt29
rt32
rt31
         

  
     
     
      
     
     
     
     
             
     


        
     
     
      
Figure 3: Scored dependency forest
2.5 Semantic Dependency Graph (SDG)
The SDG is a semantic-label word DG designed
for Japanese sentence analysis. The optimum tree
search algorithm searches for the optimum tree
satisfying the well-formed constraints (C1)-(C4)
in a SDG(Hirakawa, 2001). This method is lack-
ing in terms of generality in that it cannot handle
backward dependency and multiple WPP because
it depends on some linguistic features peculiar to
Japanese. Therefore, this method is inherently in-
applicable to languages like English that require
backward dependency and multiple POS analysis.
The DF described below can be seen as the ex-
tension of the SDG. Since the DF has none of the
language-dependent premises that the SDG has, it
is applicable to English and other languages.
2.6 Dependency Forest (DF)
The DF is a packed shared data structure en-
compassing all possible dependency trees for a
sentence adopted in PDG. The DF consists of a
dependency graph (DG) and a constraint matrix
(CM). Figure 3 shows a DF for the example sen-
tence ?Time flies like an arrow.? The DG consists
of nodes and directed arcs. A node represents a
WPP and an arc shows the dependency relation
between nodes. An arc has its ID and preference
score. CM is a matrix whose rows and columns
are a set of arcs in DG and prescribes the co-
occurrence constraint between arcs. Only when
CM(i,j) is , ar
i
and ar
j
are co-occurrable in
one dependency tree.
The DF is generated by using a phrase structure
parser in PDG. PDG grammar rule is an extended
CFG rule, which defines the mapping between
a sequence of constituents (the body of a CFG
rule) and a set of arcs (a partial dependency tree).
363
The generated CM assures that the parse trees in
the parse forest and the dependency trees in the
DF have mutual correspondence(Hirakawa, 2006).
CM can represent (C2)-(C4) in 2.3 and more pre-
cise constraints. For example, PDG can generate
a DF encompassing non-projective dependency
trees by introducing the grammar rules defining
non-projective constructions. This is called the
controlled non-projectivity in this paper. Treat-
ment of non-projectivity as described in (Kanahe
et al, 1998; Nivre and Nilsson, 2005) is an impor-
tant topic out of the scope of this paper.
3 The Optimum Tree Search in DF
This section shows the graph branch algorithm
based on the B&B principle, which searches for
the optimum well-formed tree in a DF by apply-
ing problem expansions called graph branching.
3.1 Outline of B&B Method
The B&B method(Ibaraki, 1978) is a principle
for solving computationally hard problems such
as NP-complete problems. The basic strategy is
that the original problem is decomposed into eas-
ier partial-problems (branching) and the original
problem is solved by solving them. Pruning called
a bound operation is applied if it turns out that the
optimum solution to a partial-problem is inferior
to the solution obtained from some other partial-
problem (dominance test)7, or if it turns out that
a partial-problem gives no optimum solutions to
the original problem (maximum value test). Usu-
ally, the B&B algorithm is constructed to mini-
mize the value of the solution. The graph branch
algorithm in this paper is constructed to maximize
the score of the solution because the best solution
is the maximum tree in the DF.
3.2 Graph Branch Algorithm
The graph branch algorithm is obtained by defin-
ing the components of the original B&B skeleton
algorithm, i.e. the partial-problem, the feasible so-
lution, the lower bound value, the upper bound
value, the branch operation, and so on(Ibaraki,
1978). Figure 4 shows the graph branch algorithm
which has been extended from the original B&B
skeleton algorithm to search for all the optimum
trees in a DF. The following sections explain the
B&B components of the graph branch algorithm.
7The dominance test is not used in the graph branch algo-
rithm.
   
    	 
            
  	 
       
     	   
  	
         	 
    	 