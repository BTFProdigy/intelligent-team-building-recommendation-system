Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 118?119,
New York City, June 2006. c?2006 Association for Computational Linguistics
Rapid Adaptation of POS Tagging for Domain Specific Uses John E. Miller1 Michael Bloodgood1 Manabu Torii2 K. Vijay-Shanker1 1Computer & Information Sciences 2Biostatistics, Bioinformatics and Biomathematics University of Delaware Georgetown University Medical Center Newark, DE 19716 Washington, DC 20057 {jmiller,bloodgoo,vijay}@cis.udel.edu mt352@georgetown.edu 
1 Introduction Part-of-speech (POS) tagging is a fundamental component for performing natural language tasks such as parsing, information extraction, and ques-tion answering.  When POS taggers are trained in one domain and applied in significantly different domains, their performance can degrade dramati-cally.  We present a methodology for rapid adapta-tion of POS taggers to new domains.  Our technique is unsupervised in that a manually anno-tated corpus for the new domain is not necessary.  We use suffix information gathered from large amounts of raw text as well as orthographic infor-mation to increase the lexical coverage. We present an experiment in the Biological domain where our POS tagger achieves results comparable to POS taggers specifically trained to this domain.   Many machine-learning and statistical tech-niques employed for POS tagging train a model on an annotated corpus, such as the Penn Treebank (Marcus et al 1993). Most state-of-the-art POS taggers use two main sources of information: 1) Information about neighboring tags, and 2) Infor-mation about the word itself. Methods using both sources of information for tagging are: Hidden Markov Modeling, Maximum Entropy modeling, and Transformation Based Learning (Brill, 1995).  In moving to a new domain, performance can degrade dramatically because of the increase in the unknown word rate as well as domain-specific word use. We improve tagging performance by attacking these problems. Since our goal is to em-ploy minimal manual effort or domain-specific knowledge, we consider only orthographic, inflec-tional and derivational information in deriving POS. We bypass the time, cost, resource, and con-tent expert intensive approach of annotating a cor-pus for a new domain. 
2 Methodology and Experiment The initial components in our POS tagging process are a lexicon and part of speech (POS) tagger trained on a generic domain corpus. The lexicon is updated to include domain specific information based on suffix rules applied to an un-annotated corpus. Documents in the new domain are POS tagged using the updated lexicon and orthographic information. So, the POS tagger uses the domain specific updated lexicon, along with what it knows from generic training, to process domain specific text and output POS tags. In demonstrating feasibility of the approach, we used the fnTBL-1.0 POS tagger (Ngai and Florian, 2001) based on Brill?s Transformation Based Learning (Brill, 1995) along with its lexicon and contextual rules trained on the Wall Street Journal corpus.  To update the lexicon, we processed 104,322 abstracts from five of the 500 compressed data files in the 2005 PubMed/Medline database (Smith et al 2004). As a result of this update, coverage of words with POS tags from the lexicon increased from 73.0% to 89.6% in our test corpus. Suffix rules were composed based on informa-tion from Michigan State University?s Suffixes and Parts of Speech web page for Graduate Record Exams (DeForest, 2000). The suffix endings indi-cate the POS used for new words. However, as seen in the table of suffix examples below, there can be significant lack of precision in assigning POS based just on suffixes.  Suffix POS #uses/ %acc ize; izes VB VBP; VBZ 23/100% ous JJ 195/100% er, or; ers, ors NN; NNS 1471/99.5% ate; ates VB VBP 576/55.7% 
118
Most suffixes did well in determining the actual POS assigned to the word. Some such as ?-er? and ?-or? had very broad use as well. ?-ate? typically forms a verb from a noun or adjective in a generic domain. However in scientific domains it often indicates a noun or adjective word form. (In work just begun, we add POS assignment confirmation tests to suffix rules so as to confirm POS tags while maintaining our domain independent and unsupervised analysis of un-annotated corpora.) Since the fnTBL POS tagger gives preliminary assignment of POS tags based on the first POS listed for that word in the lexicon, it is vital that the first POS tag for a common word be correct. Words ending in ?-ing? can be used in a verbal (VBG), adjectival (JJ) or noun (NN) sense. Our intuition is that the ?-ed? form should also appear often when the verbal sense dominates. In contrast, if the ratio heavily favors the ?-ing? form then we expect the noun sense to dominate.  We incorporated this reasoning into a computa-tionally defined process which assigned the NN tag first to the following words: binding, imaging, learning, nursing, processing, screening, signal-ing, smoking, training, and underlying. Only un-derlying seems out of place in this list.  In addition to inflectional and derivational suf-fixes, we used rules based on orthographic charac-teristics. These rules defined proper noun and number or code categories. 3 Results and Conclusion For testing purposes, we used approximately half the abstracts of the GENIA corpus (version 3.02) described in (Tateisi et al 2003). As the GENIA corpus does not distinguish between common and proper nouns we dropped that distinction in evalu-ating tagger performance.  POS tagging accuracy on our GENIA test set (second half of abstracts) consisting of 243,577 words is shown in the table below. Source Accuracy Original fnTBL lexicon 92.58% Adapted lexicon (Rapid) 94.13% MedPost 94.04% PennBioIE1 93.98%                                                            1 Note that output from the tagger is not fully compatible with GENIA annotation. 
The original fnTBL tagger has an accuracy of 92.58% on the GENIA test corpus showing that it deals well with unknown words from this domain. Our rapid adaptation tagger achieves a modest 1.55% absolute improvement in accuracy, which equates to a 21% error reduction.   There is little difference in performance be-tween our rapid adaptation tagger and the MedPost (Smith et al 2004) and PennBioIE (Kulick et al 2004) taggers. The PennBioIE tagger employs maximum entropy modeling and was developed using 315 manually annotated Medline abstracts. The MedPost tagger also used domain-specific annotated corpora and a 10,000 word lexicon, manually updated with POS tags. We have improved the accuracy of the fnTBL-1.0 tagger for a new domain by adding words and POS tags to its lexicon via unsupervised methods of processing raw text from the new domain. The accuracy of the resulting tagger compares well to those that have been trained to this domain using annotation effort and domain-specific knowledge.  References Brill, E. 1995. Transformation-based error-driven learn-ing and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4):543-565.   DeForest, Jessica. 2000. Graduate Record Exam Suffix web page. Michigan State University. http:// www.msu.edu/~defores1/gre/sufx/gre_suffx.htm.  Kulick, S., Bies, A., Liberman, M., Mandel, M., McDonald, R., Palmer, M., Schein, A., Ungar, L. 2004. Integrated annotation for biomedical informa-tion extraction. HLT/NAACL-2004: 61-68. Marcus, M., Santorini, B., Marcinkiewicz, M.A. 1993. Building a large annotated corpus of English: The Penn Treebank.  Computational Linguistics, 19:313-330.   Ngai, G. and Florian, R. 2001.  Transformation-based learning in the fast lane.  In Proceedings of North America ACL 2001(June): 40-47.   Smith, L., Rindflesch, T., Wilbur, W.J. 2004. MedPost: a part-of-speech tagger for bioMedical text.  Bioin-formatics 20 (14):2320-2321.   Tateisi, Y., Ohta, T., dong Kim, J., Hong, H., Jian, S., Tsujii, J. 2003. The GENIA corpus: Medline ab-stracts annotated with linguistic information.  In: Third meeting of SIG on Text Mining, Intelligent Systems for Molecular Biology (ISMB).   
119
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 104?105,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
An Approach to Reducing Annotation Costs for BioNLP 
Michael Bloodgood 
Computer and Information Sciences 
University of Delaware 
Newark, DE 19716 
bloodgoo@cis.udel.edu 
K. Vijay-Shanker 
Computer and Information Sciences 
University of Delaware 
Newark, DE 19716 
vijay@cis.udel.edu 
1 Introduction 
There is a broad range of BioNLP tasks for which 
active learning (AL) can significantly reduce anno-
tation costs and a specific AL algorithm we have 
developed is particularly effective in reducing an-
notation costs for these tasks. We have previously 
developed an AL algorithm called ClosestInitPA 
that works best with tasks that have the following 
characteristics: redundancy in training material, 
burdensome annotation costs, Support Vector Ma-
chines (SVMs) work well for the task, and imbal-
anced datasets (i.e. when set up as a binary 
classification problem, one class is substantially 
rarer than the other). Many BioNLP tasks have 
these characteristics and thus our AL algorithm is a 
natural approach to apply to BioNLP tasks.   
2 Active Learning Algorithm 
ClosestInitPA uses SVMs as its base learner. This 
fits well with many BioNLP tasks where SVMs 
deliver high performance (Giuliano et al, 2006; 
Lee et al, 2004). ClosestInitPA is based on the 
strategy of selecting the points which are closest to 
the current model?s hyperplane (Tong and Koller, 
2002) for human annotation. ClosestInitPA works 
best in situations with imbalanced data, which is 
often the case for BioNLP tasks. For example, in 
the AIMed dataset annotated with protein-protein 
interactions, the percentage of pairs of proteins in 
the same sentence that are annotated as interacting 
is only 17.6%.  
SVMs (Vapnik, 1998) are learning systems that 
learn linear functions for classification. A state-
ment of the optimization problem solved by soft-
margin SVMs that enables the use of asymmetric 
cost factors is the following: 
Minimize: ??
?=
?
+=
+
++
1:1:
2||||
2
1
yy
w
ji
j
j
i
i CC ??r  (1) 
Subject to: kkk bxwyk ???+?? 1][: rr          (2) 
where ),( bwr  represents the hyperplane that is 
learned, kx
r
is the feature vector for example k, yk 
in {+1,-1} is the label for example k, 
])[1,0max( bxwy kkk +??=
rr?
 
is the slack vari-
able for example k, and C+ and C- are user-defined 
cost factors that trade off separating the data with a 
large margin and misclassifying training examples. 
Let PA=C+/C-. PA stands for ?positive amplifi-
cation.? We use this term because as the PA is in-
creased, the importance of positive examples is 
amplified. ClosestInitPA is described in Figure 3. 
We have previously shown that setting PA based 
on a small initial set of data outperforms the more 
obvious approach of using the current labeled data 
to estimate PA. 
 
 
Figure 3. ClosestInitPA algorithm. 
 
We have previously developed a stopping crite-
rion called staticPredictions that is based on stop-
ping when we detect that the predictions of our 
models on some unlabeled data have stabilized. All 
of the automatic stopping points in our results are 
determined using staticPredictions.  
Initialization: 
? L = small initial set of labeled data 
? U = large pool of unlabeled data 
 
L
LPA
in  examples pos #
in  examples neg #
=  
Loop until stopping criterion is met: 
1. Train an SVM with parameters C+ 
and C
-
 set such that C+/C- = PA. 
2. batch = select k points from U that 
are closest to the hyperplane learned 
in step 1. 
U = U ? batch 
L = L U batch 
104
3 Experiments 
Protein-Protein Interaction Extraction: We used 
the AImed corpus, which was previously used for 
training protein interaction extraction systems in 
(Giuliano et al, 2006). We cast RE as a binary 
classification task as in (Giuliano et al, 2006). 
We do 10-fold cross validation and use what is 
referred to in (Giuliano et al, 2006) as the KGC 
kernel with SVMlight (Joachims, 1999) in our ex-
periments. Table 1 reports the results. 
F Measure StoppingPoint Average # Labels 
Random AL 
20% 1012 48.33 54.34 
30% 1516 49.76 54.52 
40% 2022 53.11 56.39 
100% 5060 57.54 57.54 
AutoStopPoint 1562 51.25 55.34 
Table 1. AImed Stopping Point Performance. ?AutoS-
topPoint? is when the stopping criterion says to stop. 
 
Medline Text Classification: We use the Oh-
sumed corpus (Hersh, 1994) and a linear kernel 
with SVMlight with binary features for each word 
that occurs in the training data at least three times. 
Results for the five largest categories for one ver-
sus the rest classification are in Table 2. 
F Measure StoppingPoint Average # Labels 
Random AL 
20% 1260 49.99 61.49 
30% 1880 54.18 62.72 
40% 2500 57.46 63.75 
100% 6260 65.75 65.75 
AutoStopPoint 1204 47.06 60.73 
Table 2. Ohsumed stopping point performance. ?AutoS-
topPoint? is when the stopping criterion says to stop. 
 
GENIA NER: We assume a two-phase model 
(Lee et al, 2004) where boundary identification of 
named entities is performed in the first phase and 
the entities are classified in the second phase. As in 
the semantic classification evaluation of (Lee et al, 
2004), we assume that boundary identification has 
been performed. We use features based on those 
from (Lee et al, 2004), a one versus the rest setup 
and 10-fold cross validation. Tables 3-5 show the 
results for the three most common types in 
GENIA. 
 
F Measure StoppingPoint Average # Labels 
Random AL 
20% 13440 86.78 90.16 
30% 20120 87.81 90.27 
40% 26900 88.55 90.32 
100% 67220 90.28 90.28 
AutoStopPoint 8720 85.41 89.24 
Table 3. Protein stopping points performance. ?AutoS-
topPoint? is when the stopping criterion says to stop. 
 
F Measure StoppingPoint Average # Labels 
Random AL 
20% 13440 79.85 82.06 
30% 20120 80.40 81.98 
40% 26900 80.85 81.84 
100% 67220 81.68 81.68 
AutoStopPoint 7060 78.35 82.29 
Table 4. DNA stopping points performance. ?AutoS-
topPoint? is when the stopping criterion says to stop. 
 
F Measure StoppingPoint Average # Labels 
Random AL 
20% 13440 84.01 86.76 
30% 20120 84.62 86.63 
40% 26900 85.25 86.45 
100% 67220 86.08 86.08 
AutoStopPoint 4200 81.32 86.31 
 Table 5. Cell Type stopping points performance. ?Au-
toStopPoint? is when the stopping criterion says to stop. 
4 Conclusions 
ClosestInitPA is well suited to many BioNLP 
tasks. In experiments, the annotation savings are 
practically significant for extracting protein-protein 
interactions, classifying Medline text, and perform-
ing biomedical named entity recognition.  
References 
 
Claudio Giuliano, Alberto Lavelli, and Lorenza Roma-
no. 2006. Exploiting Shallow Linguistic Information 
for Relation Extraction from Biomedical Literature. 
In Proceedings of the EACL, 401-408. 
 William Hersh, Buckley, C., Leone, T.J., and Hickman, 
D. (1994). Ohsumed: an interactive retrieval evalua-
tion and new large text collection for research. ACM 
SIGIR.  
Thorsten Joachims. 1999. Making large-Scale SVM 
Learning Practical. Advances in Kernel Methods - 
Support Vector Learning, MIT-Press, 169-184. 
Ki-Joong Lee, Young-Sook Hwang, Seonho Kim, and 
Hae-Chang Rim. 2004. Biomedical named entity 
recognition using two-phase model based on SVMs. 
Journal of Biomedical Informatics, Vol 37, 436?447. 
Simon Tong and Daphne Koller. 2002. Support vector 
machine active learning with applications to text 
classification. JMLR 2: 45-66. 
Vladimir Vapnik. 1998. Statistical Learning Theory. 
John Wiley & Sons, New York, NY, USA.  
 
105
Proceedings of NAACL HLT 2009: Short Papers, pages 137?140,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Taking into Account the Differences between Actively and Passively
Acquired Data: The Case of Active Learning with Support Vector Machines
for Imbalanced Datasets
Michael Bloodgood?
Human Language Technology
Center of Excellence
Johns Hopkins University
Baltimore, MD 21211 USA
bloodgood@jhu.edu
K. Vijay-Shanker
Computer and Information
Sciences Department
University of Delaware
Newark, DE 19716 USA
vijay@cis.udel.edu
Abstract
Actively sampled data can have very different
characteristics than passively sampled data.
Therefore, it?s promising to investigate using
different inference procedures during AL than
are used during passive learning (PL). This
general idea is explored in detail for the fo-
cused case of AL with cost-weighted SVMs
for imbalanced data, a situation that arises for
many HLT tasks. The key idea behind the
proposed InitPA method for addressing im-
balance is to base cost models during AL on
an estimate of overall corpus imbalance com-
puted via a small unbiased sample rather than
the imbalance in the labeled training data,
which is the leading method used during PL.
1 Introduction
Recently there has been considerable interest in us-
ing active learning (AL) to reduce HLT annotation
burdens. Actively sampled data can have differ-
ent characteristics than passively sampled data and
therefore, this paper proposes modifying algorithms
used to infer models during AL. Since most AL re-
search assumes the same learning algorithms will be
used during AL as during passive learning1 (PL),
this paper opens up a new thread of AL research that
accounts for the differences between passively and
actively sampled data.
The specific case focused on in this paper is
that of AL with SVMs (AL-SVM) for imbalanced
?This research was conducted while the first author was a
PhD student at the University of Delaware.
1Passive learning refers to the typical supervised learning
setup where the learner does not actively select its training data.
datasets2. Collectively, the factors: interest in AL,
widespread class imbalance for many HLT tasks, in-
terest in using SVMs, and PL research showing that
SVM performance can be improved substantially by
addressing imbalance, indicate the importance of the
case of AL with SVMs with imbalanced data.
Extensive PL research has shown that learning
algorithms? performance degrades for imbalanced
datasets and techniques have been developed that
prevent this degradation. However, to date, rela-
tively little work has addressed imbalance during AL
(see Section 2). In contrast to previous work, this
paper advocates that the AL scenario brings out the
need to modify PL approaches to dealing with im-
balance. In particular, a new method is developed
for cost-weighted SVMs that estimates a cost model
based on overall corpus imbalance rather than the
imbalance in the so far labeled training data. Sec-
tion 2 discusses related work, Section 3 discusses
the experimental setup, Section 4 presents the new
method called InitPA, Section 5 evaluates InitPA,
and Section 6 concludes.
2 Related Work
A problem with imbalanced data is that the class
boundary (hyperplane) learned by SVMs can be too
close to the positive (pos) examples and then recall
suffers. Many approaches have been presented for
overcoming this problem in the PL setting. Many
require substantially longer training times or ex-
2This paper focuses on the fundamental case of binary clas-
sification where class imbalance arises because the positive ex-
amples are rarer than the negative examples, a situation that nat-
urally arises for many HLT tasks.
137
tra training data to tune parameters and thus are
not ideal for use during AL. Cost-weighted SVMs
(cwSVMs), on the other hand, are a promising ap-
proach for use with AL: they impose no extra train-
ing overhead. cwSVMs introduce unequal cost fac-
tors so the optimization problem solved becomes:
Minimize:
1
2?~w?
2 + C+
?
i:yi=+1
?i + C?
?
i:yi=?1
?i (1)
Subject to:
?k : yk [~w ? ~xk + b] ? 1? ?k, (2)
where (~w, b) represents the learned hyperplane, ~xk
is the feature vector for example k, yk is the label
for example k, ?k = max(0, 1 ? yk(~wk ? ~xk + b))
is the slack variable for example k, and C+ and C?
are user-defined cost factors.
The most important part for this paper are the cost
factors C+ and C?. The ratio C+C? quantifies the
importance of reducing slack error on pos train ex-
amples relative to reducing slack error on negative
(neg) train examples. The value of the ratio is cru-
cial for balancing the precision recall tradeoff well.
(Morik et al, 1999) showed that during PL, set-
ting C+C? = # of neg training examples# of pos training examples is an effec-
tive heuristic. Section 4 explores using this heuris-
tic during AL and explains a modified heuristic that
could work better during AL.
(Ertekin et al, 2007) propose using the balancing
of training data that occurs as a result of AL-SVM
to handle imbalance and do not use any further mea-
sures to address imbalance. (Zhu and Hovy, 2007)
used resampling to address imbalance and based the
amount of resampling, which is the analog of our
cost model, on the amount of imbalance in the cur-
rent set of labeled train data, as PL approaches do.
In contrast, the InitPA approach in Section 4 bases
its cost models on overall (unlabeled) corpus imbal-
ance rather than the amount of imbalance in the cur-
rent set of labeled data.
3 Experimental Setup
We use relation extraction (RE) and text classifica-
tion (TC) datasets and SVMlight (Joachims, 1999)
for training the SVMs. For RE, we use AImed,
previously used to train protein interaction extrac-
tion systems ((Giuliano et al, 2006)). As in previ-
ous work, we cast RE as a binary classification task
Figure 1: Hyperplane B was trained with a higher C+C?
ratio than hyperplane A was trained with.
(14.94% of the examples in AImed are positive). We
use the KGC kernel from (Giuliano et al, 2006), one
of the highest-performing systems on AImed to date
and perform 10-fold cross validation. For TC, we
use the Reuters-21578 ModApte split. Since a doc-
ument may belong to more than one category, each
category is treated as a separate binary classification
problem, as in (Joachims, 1998). As in (Joachims,
1998), we use the ten largest categories, which have
imbalances ranging from 1.88% to 29.96%.
4 AL-SVM Methods for Addressing Class
Imbalance
The key question when using cwSVMs is how to set
the ratio C+C? . Increasing it will typically shift the
learned hyperplane so recall is increased and preci-
sion is decreased (see Figure 1 for a hypothetical ex-
ample). Let PA= C+C? .3 How should the PA be set
during AL-SVM?
We propose two approaches: one sets the PA
based on the level of imbalance in the labeled train-
ing data and one aims to set the PA based on an es-
timate of overall corpus imbalance, which can dras-
tically differ from the level of imbalance in actively
sampled training data. The first method is called
CurrentPA, depicted in Figure 2. Note that in step
0 of the loop, PA is set based on the distribution of
positive and negative examples in the current set of
labeled data. However, observe that during AL the
ratio # neg labeled examples# pos labeled examples in the current set of la-
beled data gets skewed from the ratio in the entire
3PA stands for positive amplification and gives us a concise
way to denote the fraction C+C? , which doesn?t have a standard
name.
138
Input:
L = small initial set of labeled data
U = large pool of unlabeled data
Loop until stopping criterion is met:
0. Set PA = |{x?Labeled:f(x)=?1}||{x?L:f(x)=+1}|
where f is the function we desire to learn.
1. Train an SVM with C+ and C? set such
that C+C? = PA and obtain hyperplane h .4
2. batch? select k points from U that are
closest to h and request their labels.5
3. U = U ? batch .
4. L = L ? batch .
End Loop
Figure 2: The CurrentPA algorithm
0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000
1.5
2
2.5
3
3.5
4
4.5
5
5.5
Empirical Evidence of CurrentPA creating a Skewed Distribution (Fold Avg)
Number of Points for which Annotations Have Been Requested
Ra
tio
 of
 # 
of 
Ne
ga
tiv
e t
o #
 of
 Po
sit
ive
 Po
int
s
 
 
Ratio with CurrentPA
Ratio with Entire Set
Figure 3: Illustration of AL skewing the distribution of
pos/neg points on AImed.
corpus because AL systematically selects the exam-
ples that are closest to the current model?s hyper-
plane and this tends to select more positive exam-
ples than random selection would select (see also
(Ertekin et al, 2007)).
Empirical evidence of this distribution skew is il-
lustrated in Figure 3. The trend toward balanced
datasets during AL could mislead and cause us to
underestimate the PA.
Therefore, our next algorithm aims to set the PA
based on the ratio of neg to pos instances in the en-
tire corpus. However, since we don?t have labels for
the entire corpus, we don?t know this ratio. But by
using a small initial sample of labeled data, we can
4We use SVMlight?s default value for C?.
5In our experiments, batch size is 20.
0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000
20
25
30
35
40
45
50
55
60
AImed Average F Measure versus Number of Annotations
Number of Points for which Annotations Have Been Requested
Pe
rfo
rm
an
ce
 (F 
Me
asu
re)
 
 
InitPA
Oversampling(Zhu and Hovy,2007)
CurrentPA
EHG2007(Ertekin et al 2007)
Figure 4: AImed learning curves. y-axis is from 20% to
60%.
estimate this ratio with high confidence. This esti-
mate can then be used for setting the PA throughout
the AL process. We call this method of setting the
PA based on a small initial set of labeled data the
InitPA method. It is like CurrentPA except we move
Step 0 to be executed one time before the loop and
then use that same PA value on each iteration of the
AL loop.
To guide what size to make the initial set of la-
beled data, one can determine the sample size re-
quired to estimate the proportion of positives in a
finite population to within sampling error e with a
desired level of confidence using standard statisti-
cal techniques found in many college-level statistics
references such as (Berenson et al, 1988). For ex-
ample, carrying out the computations on the AImed
dataset shows that a size of 100 enables us to be
95% confident that our proportion estimate is within
0.0739 of the true proportion. In our experiments,
we used an initial labeled set of size 100.
5 Evaluation
In addition to InitPA and CurrentPA, we also imple-
mented the methods from (Ertekin et al, 2007; Zhu
and Hovy, 2007). We implemented oversampling by
duplicating points and by BootOS (Zhu and Hovy,
2007). To avoid cluttering the graphs, we only show
the highest-performing oversampling variant, which
was by duplicating points. Learning curves are pre-
sented in Figures 4 and 5.
Note InitPA is the highest-performing method for
all datasets, especially in the practically important
area of where the learning curves begin to plateau.
139
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
76
77
78
79
80
81
82
83
Reuters Average F Measure versus Number of Annotations
Number of Points for which Annotations Have Been Requested
Pe
rfo
rm
an
ce
 (F
 Me
asu
re)
 
 
InitPA
Oversampling(Zhu and Hovy,2007)
CurrentPA
EHG2007(Ertekin et al 2007)
Figure 5: Reuters learning curves. y-axis is from 76% to
83%.
This area is important because this is around where
we would want to stop AL (Bloodgood and Vijay-
Shanker, 2009).
Observe that the gains of InitPA over CurrentPA
are smaller for Reuters. For some Reuters cate-
gories, InitPA and CurrentPA have nearly identical
performance. Applying the models learned by Cur-
rentPA at each round of AL on the data used to
train the model reveals that the recall on the train-
ing data is nearly 100% for those categories where
InitPA/CurrentPA perform similarly. Increasing the
relative penalty for slack error on positive training
points will not have much impact if (nearly) all of
the pos train points are already classified correctly.
Thus, in situations where models are already achiev-
ing nearly 100% recall on their train data, InitPA is
not expected to outperform CurrentPA.
The hyperplanes learned during AL-SVM serve
two purposes: sampling - they govern which unla-
beled points will be selected for human annotation,
and predicting - when AL stops, the most recently
learned hyperplane is used for classifying test data.
Although all AL-SVM approaches we?re aware of
use the same hyperplane at each round of AL for
both of these purposes, this is not required. We com-
pared InitPA with hybrid approaches where hyper-
planes trained using an InitPA cost model are used
for sampling and hyperplanes trained using a Cur-
rentPA cost model are used for predicting, and vice-
versa, and found that InitPA performed better than
both of these hybrid approaches. This indicates that
the InitPA cost model yields hyperplanes that are
better for both sampling and predicting.
6 Conclusions
We?ve made the case for the importance of AL-SVM
for imbalanced datasets and showed that the AL sce-
nario calls for modifications to PL approaches to ad-
dressing imbalance. For AL-SVM, the key idea be-
hind InitPA is to base cost models on an estimate of
overall corpus imbalance rather than the class imbal-
ance in the so far labeled data. The practical utility
of the InitPA method was demonstrated empirically;
situations where InitPA won?t help that much were
made clear; and analysis showed that the sources of
InitPA?s gains were from both better sampling and
better predictive models.
InitPA is an instantiation of a more general idea
of not using the same inference algorithms during
AL as during PL but instead modifying inference al-
gorithms to suit esoteric characteristics of actively
sampled data. This is an idea that has seen relatively
little exploration and is ripe for further investigation.
References
Mark L. Berenson, David M. Levine, and David Rind-
skopf. 1988. Applied Statistics. Prentice-Hall, Engle-
wood Cliffs, NJ.
Michael Bloodgood and K. Vijay-Shanker. 2009. A
method for stopping active learning based on stabiliz-
ing predictions and the need for user-adjustable stop-
ping. In CoNLL.
Seyda Ertekin, Jian Huang, Le?on Bottou, and C. Lee
Giles. 2007. Learning on the border: active learning
in imbalanced data classification. In CIKM.
Claudio Giuliano, Alberto Lavelli, and Lorenza Romano.
2006. Exploiting shallow linguistic information for re-
lation extraction from biomedical literature. In EACL.
Thorsten Joachims. 1998. Text categorization with su-
port vector machines: Learning with many relevant
features. In ECML, pages 137?142.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Advances in Kernel Methods ?
Support Vector Learning, pages 169?184.
Katharina Morik, Peter Brockhausen, and Thorsten
Joachims. 1999. Combining statistical learning with a
knowledge-based approach - a case study in intensive
care monitoring. In ICML, pages 268?277.
Jingbo Zhu and Eduard Hovy. 2007. Active learning for
word sense disambiguation with methods for address-
ing the class imbalance problem. In EMNLP-CoNLL.
140
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 39?47,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Method for Stopping Active Learning Based on Stabilizing Predictions
and the Need for User-Adjustable Stopping
Michael Bloodgood?
Human Language Technology
Center of Excellence
Johns Hopkins University
Baltimore, MD 21211 USA
bloodgood@jhu.edu
K. Vijay-Shanker
Computer and Information
Sciences Department
University of Delaware
Newark, DE 19716 USA
vijay@cis.udel.edu
Abstract
A survey of existing methods for stopping ac-
tive learning (AL) reveals the needs for meth-
ods that are: more widely applicable; more ag-
gressive in saving annotations; and more sta-
ble across changing datasets. A new method
for stopping AL based on stabilizing predic-
tions is presented that addresses these needs.
Furthermore, stopping methods are required
to handle a broad range of different annota-
tion/performance tradeoff valuations. Despite
this, the existing body of work is dominated
by conservative methods with little (if any) at-
tention paid to providing users with control
over the behavior of stopping methods. The
proposed method is shown to fill a gap in the
level of aggressiveness available for stopping
AL and supports providing users with control
over stopping behavior.
1 Introduction
The use of Active Learning (AL) to reduce NLP an-
notation costs has generated considerable interest re-
cently (e.g. (Bloodgood and Vijay-Shanker, 2009;
Baldridge and Osborne, 2008; Zhu et al, 2008a)).
To realize the savings in annotation efforts that AL
enables, we must have a mechanism for knowing
when to stop the annotation process.
Figure 1 is intended to motivate the value of stop-
ping at the right time. The x-axis measures the num-
ber of human annotations that have been requested
and ranges from 0 to 70,000. The y-axis measures
? This research was conducted while the first author was a
PhD student at the University of Delaware.
0 1 2 3 4 5 6 7
x 104
65
70
75
80
85
90
Active Learning Curve (F Measure vs Number of Annotations)
Number of Points for which     
Annotations Have Been Requested
Pe
rfo
rm
an
ce
 (F
 M
ea
su
re)
 
 
stop point 1: 
stops too early;
results in lower 
performing model
stop point 2: 
good place to stop
stop point 3: 
stops too late; 
wastes around 
30,000 human 
annotations
Figure 1: Hypothetical Active Learning Curve with hy-
pothetical stopping points.
performance in terms of F-Measure. As can be seen
from the figure, the issue is that if we stop too early
while useful generalizations are still being made, we
wind up with a lower performing system but if we
stop too late after all the useful generalizations have
been made, we just wind up wasting human annota-
tion effort.
The terms aggressive and conservative will be
used throughout the rest of this paper to describe the
behavior of stopping methods. Conservative meth-
ods tend to stop further to the right in Figure 1.
They are conservative in the sense that they?re very
careful not to risk losing significant amounts of F-
measure, even if it means annotating many more ex-
amples than necessary. Aggressive methods, on the
other hand, tend to stop further to the left in Figure 1.
They are aggressively trying to reduce unnecessary
annotations.
There has been a flurry of recent work tackling the
39
problem of automatically determining when to stop
AL (see Section 2). There are three areas where this
body of work can be improved:
applicability Several of the leading methods are re-
stricted to only being used in certain situations,
e.g., they can?t be used with some base learn-
ers, they have to select points in certain batch
sizes during AL, etc. (See Section 2 for dis-
cussion of the exact applicability constraints of
existing methods.)
lack of aggressive stopping The leading methods
tend to find stop points that are too far to the
right in Figure 1. (See Section 4 for empirical
confirmation of this.)
instability Some of the leading methods work well
on some datasets but then can completely break
down on other datasets, either stopping way too
late and wasting enormous amounts of annota-
tion effort or stopping way too early and losing
large amounts of F-measure. (See Section 4 for
empirical confirmation of this.)
This paper presents a new stopping method based
on stabilizing predictions that addresses each of
these areas and provides user-adjustable stopping
behavior. The essential idea behind the new method
is to test the predictions of the recently learned mod-
els (during AL) on examples which don?t have to
be labeled and stop when the predictions have sta-
bilized. Some of the main advantages of the new
method are that: it requires no additional labeled
data, it?s widely applicable, it fills a need for a
method which can aggressively save annotations, it
has stable performance, and it provides users with
control over how aggressively/conservatively to stop
AL.
Section 2 discusses related work. Section 3 ex-
plains our Stabilizing Predictions (SP) stopping cri-
terion in detail. Section 4 evaluates the SP method
and discusses results. Section 5 concludes.
2 Related Work
Laws and Schu?tze (2008) present stopping criteria
based on the gradient of performance estimates and
the gradient of confidence estimates. Their tech-
nique with gradient of performance estimates is only
applicable when probabilistic base learners are used.
The gradient of confidence estimates method is more
generally applicable (e.g., it can be applied with
our experiments where we use SVMs as the base
learner). This method, denoted by LS2008 in Tables
and Figures, measures the rate of change of model
confidence over a window of recent points and when
the gradient falls below a threshold, AL is stopped.
The margin exhaustion stopping criterion was de-
veloped for AL with SVMs (AL-SVM). It says to
stop when all of the remaining unlabeled examples
are outside of the current model?s margin (Schohn
and Cohn, 2000) and is denoted as SC2000 in Ta-
bles and Figures. Ertekin et al (2007) developed a
similar technique that stops when the number of sup-
port vectors saturates. This is equivalent to margin
exhaustion in all of our experiments so this method
is not shown explicitly in Tables and Figures. Since
we use AL with SVMs, we will compare with mar-
gin exhaustion in our evaluation section. Unlike our
SP method, margin exhaustion is only applicable for
use with margin-based methods such as SVMs and
can?t be used with other base learners such as Maxi-
mum Entropy, Naive Bayes, and others. Schohn and
Cohn (2000) show in their experiments that margin
exhaustion has a tendency to stop late. This is fur-
ther confirmed in our experiments in Section 4.
The confidence-based stopping criterion (here-
after, V2008) in (Vlachos, 2008) says to stop when
model confidence consistently drops. As pointed out
by (Vlachos, 2008), this stopping criterion is based
on the assumption that the learner/feature represen-
tation is incapable of fully explaining all the exam-
ples. However, this assumption is often violated and
then the performance of the method suffers (see Sec-
tion 4).
Two stopping criteria (max-conf and min-err) are
reported in (Zhu and Hovy, 2007). The max-conf
method indicates to stop when the confidence of the
model on each unlabeled example exceeds a thresh-
old. In the context of margin-based methods, max-
conf boils down to be simply a generalization of the
margin exhaustion method. Min-err, reported to be
superior to max-conf, says to stop when the accu-
racy of the most recent model on the current batch of
queried examples exceeds some threshold (they use
0.9). Zhu et al (2008b) proposes the use of multi-
criteria-based stopping to handle setting the thresh-
40
old for min-err. They refuse to stop and they raise
the min-err threshold if there have been any classi-
fication changes on the remaining unlabeled data by
consecutive actively learned models when the cur-
rent min-err threshold is satisfied. We denote this
multi-criteria-based strategy, reported to work better
than min-err in isolation, by ZWH2008. As seen in
(Zhu et al, 2008a), sometimes min-err indeed stops
later than desired and ZWH2008 must (by nature
of how it operates) stop at least as late as min-err
does. The susceptibility of ZWH2008 to stopping
late is further shown emprically in Section 4. Also,
ZWH2008 is not applicable for use with AL setups
that select examples in small batches.
3 A Method for Stopping Active Learning
Based on Stabilizing Predictions
To stop active learning at the point when annotations
stop providing increases in performance, perhaps the
most straightforward way is to use a separate set of
labeled data and stop when performance begins to
level off on that set. But the problem with this is that
it requires additional labeled data which is counter
to our original reason for using AL in the first place.
Our hypothesis is that we can sense when to stop AL
by looking at (only) the predictions of consecutively
learned models on examples that don?t have to be
labeled. We won?t know if the predictions are cor-
rect or not but we can see if they have stabilized. If
the predictions have stabilized, we hypothesize that
the performance of the models will have stabilized
and vice-versa, which will ensure a (much-needed)
aggressive approach to saving annotations.
SP checks for stabilization of predictions on a set
of examples, called the stop set, that don?t have to
be labeled. Since stabilizing predictions on the stop
set is going to be used as an indication that model
stabilization has occurred, the stop set ought to be
representative of the types of examples that will be
encountered at application time. There are two con-
flicting factors in deciding upon the size of the stop
set to use. On the one hand, a small set is desir-
able because then SP can be checked quickly. On
the other hand, a large set is desired to ensure we
don?t make a decision based on a set that isn?t repre-
sentative of the application space. As a compromise
between these factors, we chose a size of 2000. In
Section 4, sensitivity analysis to stop set size is per-
formed and more principled methods for determin-
ing stop set size and makeup are discussed.
It?s important to allow the examples in the stop
set to be queried if the active learner selects them
because they may be highly informative and ruling
them out could hurt performance. In preliminary ex-
periments we had made the stop set distinct from the
set of unlabeled points made available for querying
and we saw performance was qualitatively the same
but the AL curve was translated down by a few F-
measure points. Therefore, we allow the points in
the stop set to be selected during AL.1
The essential idea is to compare successive mod-
els? predictions on the stop set to see if they have
stabilized. A simple way to define agreement be-
tween two models would be to measure the percent-
age of points on which the models make the same
predictions. However, experimental results on a sep-
arate development dataset show then that the cutoff
agreement at which to stop is sensitive to the dataset
being used. This is because different datasets have
different levels of agreement that can be expected by
chance and simple percent agreement doesn?t adjust
for this.
Measurement of agreement between human anno-
tators has received significant attention and in that
context, the drawbacks of using percent agreement
have been recognized (Artstein and Poesio, 2008).
Alternative metrics have been proposed that take
chance agreement into account. In (Artstein and
Poesio, 2008), a survey of several agreement met-
rics is presented. Most of the agreement metrics are
of the form:
agreement = Ao ? Ae1 ? Ae , (1)
where Ao = observed agreement, and Ae = agree-
ment expected by chance. The different metrics dif-
fer in how they compute Ae.
The Kappa statistic (Cohen, 1960) measures
agreement expected by chance by modeling each
coder (in our case model) with a separate distribu-
tion governing their likelihood of assigning a partic-
ular category. Formally, Kappa is defined by Equa-
1They remain in the stop set if they?re selected.
41
tion 1 with Ae computed as follows:
Ae =
?
k?{+1,?1}
P (k|c1) ? P (k|c2), (2)
where each ci is one of the coders (in our case,
models), and P (k|ci) is the probability that coder
(model) ci labels an instance as being in category k.
Kappa estimates P (k|ci) based on the proportion of
observed instances that coder (model) ci labeled as
being in category k.
We have found Kappa to be a robust parameter
that doesn?t require tuning when moving to a new
dataset. On a separate development dataset, a Kappa
cutoff of 0.99 worked well. All of the experiments
(except those in Table 2) in the current paper used an
agreement cutoff of Kappa = 0.99 with zero tuning
performed. We will see in Section 4 that this cutoff
delivers robust results across all of the folds for all
of the datasets.
The Kappa cutoff captures the intensity of the
agreement that must occur before SP will conclude
to stop. Though an intensity cutoff of K=0.99 is
an excellent default (as seen by the results in Sec-
tion 4), one of the advantages of the SP method is
that by giving users the option to vary the intensity
cutoff, users can control how aggressive SP will be-
have. This is explored further in Section 4.
Another way to give users control over stopping
behavior is to give them control over the longevity
for which agreement (at the specified intensity) must
be maintained before SP concludes to stop. The sim-
plest implementation would be to check the most
recent model with the previous model and stop if
their agreement exceeds the intensity cutoff. How-
ever, independent of wanting to provide users with
a longevity control, this is not an ideal approach be-
cause there?s a risk that these two models could hap-
pen to highly agree but then the next model will not
highly agree with them. Therefore, we propose us-
ing the average of the agreements from a window
of the k most recent pairs of models. If we call the
most recent model Mn, the previous model Mn?1
and so on, with a window size of 3, we average the
agreements between Mn and Mn?1, between Mn?1
and Mn?2, and between Mn?2 and Mn?3. On sepa-
rate development data a window size of k=3 worked
well. All of the experiments (except those in Ta-
ble 3) in the current paper used a longevity window
size of k=3 with zero tuning performed. We will
see in Section 4 that this longevity default delivers
robust results across all of the folds for all of the
datasets. Furthermore, Section 4 shows that varying
the longevity requirement provides users with an-
other lever for controlling how aggressively SP will
behave.
4 Evaluation and Discussion
4.1 Experimental Setup
We evaluate the Stabilizing Predictions (SP) stop-
ping method on multiple datasets for Text Classifi-
cation (TC) and Named Entity Recognition (NER)
tasks. All of the datasets are freely and publicly
available and have been used in many past works.
For Text Classification, we use two publicly avail-
able spam corpora: the spamassassin corpus used in
(Sculley, 2007) and the TREC spam corpus trec05p-
1/ham25 described in (Cormack and Lynam, 2005).
For both of these corpora, the task is a binary clas-
sification task and we perform 10-fold cross valida-
tion. We also use the Reuters dataset, in particular
the Reuters-21578 Distribution 1.0 ModApte split2.
Since a document may belong to more than one cat-
egory, each category is treated as a separate binary
classification problem, as in (Joachims, 1998; Du-
mais et al, 1998). Consistent with (Joachims, 1998;
Dumais et al, 1998), results are reported for the ten
largest categories. Other TC datasets we use are the
20Newsgroups3 newsgroup article classification and
the WebKB web page classification datasets. For
WebKB, as in (McCallum and Nigam, 1998; Zhu et
al., 2008a; Zhu et al, 2008b) we use the four largest
categories. For all of our TC datasets, we use binary
features for every word that occurs in the training
data at least three times.
For NER, we use the publicly available GENIA
corpus4. Our features, based on those from (Lee et
al., 2004), are surface features such as the words in
2http://www.daviddlewis.com/resources/
testcollections/reuters21578
3We used the ?bydate? version of the dataset downloaded
from http://people.csail.mit.edu/jrennie/20Newsgroups/. This
version is recommended since it makes cross-experiment com-
parison easier since there is no randomness in the selection of
train/test splits.
4http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/
wiki.cgi?page=GENIA+Project
42
the named entity and two words on each side, suf-
fix information, and positional information. We as-
sume a two-phase model where boundary identifica-
tion has already been performed, as in (Lee et al,
2004).
SVMs deliver high performance for the datasets
we use so we employ SVMs as our base learner
in the bulk of our experiments (maximum entropy
models are used in Subsection 4.3). For selection of
points to query, we use the approach that was used
in (Tong and Koller, 2002; Schohn and Cohn, 2000;
Campbell et al, 2000) of selecting the points that are
closest to the current hyperplane. We use SVMlight
(Joachims, 1999) for training the SVMs. For the
smaller datasets (less than 50,000 examples in total),
a batch size of 20 was used with an initial training
set of size 100 and for the larger datasets (greater
than 50,000 examples in total), a batch size of 200
was used with an initial training set of size 1000.
4.2 Main Results
Table 1 shows the results for all of our datasets. For
each dataset, we report the average number of anno-
tations5 requested by each of the stopping methods
as well as the average F-measure achieved by each
of the stopping methods.6
There are two facts worth keeping in mind. First,
the numbers in Table 1 are averages and therefore,
sometimes two methods could have very similar
average numbers of annotations but wildly differ-
ent average F-measures (because one of the meth-
ods was consistently stopping around its average
whereas the other was stopping way too early and
way too late). Second, sometimes a method with a
higher average number of annotations has a lower
5Better evaluation metrics would use more refined measures
of annotation effort than the number of annotations because not
all annotations require the same amount of effort to annotate but
lacking such a refined model for our datasets, we use number of
annotations in these experiments.
6Tests of statistical significance are performed using
matched pairs t tests at a 95% confidence level.
7(Vlachos, 2008) suggests using three drops in a row to de-
tect a consistent drop in confidence so we do the same in our
implementation of the method from (Vlachos, 2008).
8Following (Zhu et al, 2008b), we set the starting accuracy
threshold to 0.9 when reimplementing their method.
9(Laws and Schu?tze, 2008) uses a window of size 100
and a threshold of 0.00005 so we do the same in our re-
implementation of their method.
average F-measure than a method with a lower aver-
age number of annotations. This can be caused be-
cause of the first fact just mentioned about the num-
bers being averages and/or this can also be caused
by the ?less is more? phenomenon in active learn-
ing where often with less data, a higher-performing
model is learned than with all the data; this was
first reported in (Schohn and Cohn, 2000) and sub-
sequently observed by many others (e.g., (Vlachos,
2008; Laws and Schu?tze, 2008)).
There are a few observations to highlight regard-
ing the performance of the various stopping meth-
ods:
? SP is the most parsimonious method in terms
of annotations. It stops the earliest and remark-
ably it is able to do so largely without sacrific-
ing F-measure.
? All the methods except for SP and SC2000 are
unstable in the sense that on at least one dataset
they have a major failure, either stopping way
too late and wasting large numbers of anno-
tations (e.g. ZWH2008 and V2008 on TREC
Spam) or stopping way too early and losing
large amounts of F-measure (e.g. LS2008 on
NER-Protein) .
? It?s not always clear how to evaluate stopping
methods because the tradeoff between the value
of extra F-measure versus saving annotations is
not clearly known and will be different for dif-
ferent applications and users.
This last point deserves some more discussion. In
some cases it is clear that one stopping method is
the best. For example, on WKB-Project, the SP
method saves the most annotations and has the high-
est F-measure. But which method performs the
best on NER-DNA? Arguments can reasonably be
made for SP, SC2000, or ZWH2008 being the best
in this case depending on what exactly the anno-
tation/performance tradeoff is. A promising direc-
tion for research on AL stopping methods is to de-
velop user-adjustable stopping methods that stop as
aggressively as the user?s annotation/performance
preferences dictate.
One avenue of providing user-adjustable stopping
is that if some methods are known to perform con-
sistently in an aggressive manner against annotating
43
Task-Dataset SP V20087 SC2000 ZWH20088 LS20089 All
TREC-SPAM 2100 56000 3900 29220 3160 56000
(10-fold AVG) 98.33 98.47 98.41 98.44 96.63 98.47
20Newsgroups 678 181 1984 1340 1669 11280
(20-cat AVG) 60.85 18.06 55.43 60.72 54.79 54.81
Spamassassin 326 4362 862 398 1176 5400
(10-fold AVG) 94.57 95.00 95.53 95.94 95.62 95.63
NER-protein 8720 67220 17680 18580 2360 67220
(10-fold AVG) 89.48 90.28 90.38 90.31 76.47 90.28
NER-DNA 4020 67220 10640 7200 3900 67220
(10-fold AVG) 82.40 84.31 84.73 84.51 74.74 84.31
NER-cellType 3840 29600 5540 11580 4580 67220
(10-fold AVG) 86.15 86.87 87.19 87.32 85.65 87.83
Reuters 484 6762 1196 650 1272 9580
(10-cat AVG) 74.29 65.81 73.88 76.77 74.00 75.64
WKB-Course 790 184 1752 912 1740 7420
(10-fold AVG) 83.12 30.34 80.47 83.16 80.55 80.19
WKB-Faculty 808 892 1932 1062 1818 7420
(10-fold AVG) 81.53 40.14 81.79 81.64 81.99 82.36
WKB-Project 646 916 1358 794 1482 7420
(10-fold AVG) 63.30 25.33 58.11 61.82 59.30 61.19
WKB-Student 1258 894 2400 1468 2150 7420
(10-fold AVG) 84.70 50.66 83.46 84.39 83.19 83.30
Average 2152 21294 4477 6655 2301 28509
(macro-avg) 81.70 62.30 80.85 82.27 78.45 81.27
Table 1: Methods for stopping AL. For each dataset, the average number of annotations at the automatically determined
stopping points and the average F-measure at the automatically determined stopping points are displayed. Bold entries
are statistically significantly different than SP (and non-bold entries are not). The Average row is simply an unweighted
macro-average over all the datasets. The final column (labeled ?All?) represents standard fully supervised passive
learning with the entire set of training data.
too much while others are known to perform consis-
tently in a conservative manner, then users can pick
the stopping criterion that?s more suitable for their
particular annotation/performance valuation. For
this purpose, SP fills a gap as the other stopping cri-
teria seem to be conservative in the sense defined
in Section 1. SP, on the other hand, is more of an
aggressive stopping criterion and is less likely to an-
notate data that is not needed.
A second avenue for providing user-adjustable
stopping is a single stopping method that is itself ad-
justable. To this end, Section 4.3 shows how inten-
sity and longevity provide levers that can be used to
control the behavior of SP in a controlled fashion.
Sometimes viewing the stopping points of the var-
ious criteria on a graph with the active learning curve
can help one visualize how the methods perform.
Figure 2 shows the graph for a representative fold.10
The x-axis measures the number of human annota-
tions that have been requested so far. The y-axis
measures performance in terms of F-Measure. The
vertical lines are where the various stopping meth-
ods would have stopped AL if we hadn?t continued
the simulation. The figure reinforces and illustrates
what we have seen in Table 1, namely that SP stops
more aggressively than existing criteria and is able
10It doesn?t make sense to show a graph for the average over
cross validation because the average number of annotations at
the stopping point may cross the learning curve at a completely
misleading point. Consider a method that stops way too early
and way too late at times.
44
0 1 2 3 4 5 6 7
x 104
60
65
70
75
80
85
90
Number of Human Annotations Requested
Pe
rfo
rm
an
ce
 (F
?M
eas
ure
)
DNA Fold 1
SC2000
LS2008
ZWH2008
V2008
SP
Figure 2: Graphic with stopping criteria in action for fold
1 of NER of DNA from the GENIA corpus. The x-axis
ranges from 0 to 70,000.
to do so without sacrificing performance.
4.3 Additional Experiments
All of the additional experiments in this subsection
were conducted on our least computationally de-
manding dataset, Spamassassin. The results in Ta-
bles 2 and 3 show how varying the intensity cut-
off and the longevity requirement, respectively, of
SP enable a user to control stopping behavior. Both
methods enable a user to adjust stopping in a con-
trolled fashion (without radical changes in behav-
ior). Areas of future work include: combining the
intensity and longevity methods for controlling be-
havior; and developing precise expectations on the
change in behavior corresponding to changes in the
intensity and longevity settings.
The results in Table 4 show results for different
stop set sizes. Even with random selection of a stop
set as small as 500, SP?s performance holds fairly
steady. This plus the fact that random selection of
stop sets of size 2000 worked across all the folds of
all the datasets in Table 1 show that in practice per-
haps the simple heuristic of choosing a fairly large
random set of points works well. Nonetheless, we
think the size necessary will depend on the dataset
and other factors such as the feature representation
so more principled methods of determining the size
and/or the makeup of the stop set are an area for
future work. For example, construction techniques
Intensity Annotations F-Measure
K=99.5 364 96.01
K=99.0 326 94.57
K=98.5 304 95.59
K=98.0 262 93.75
K=97.5 242 93.35
K=97.0 224 90.91
Table 2: Controlling the behavior of stopping through the
use of intensity. For Kappa intensity levels in {97.0, 97.5,
98.0, 98.5, 99.0, 99.5}, the 10-fold average number of an-
notations at the automatically determined stopping points
and the 10-fold average F-measure at the automatically
determined stopping points are displayed for the Spamas-
sassin dataset.
Longevity Annotations F-Measure
k=1 284 95.17
k=2 318 94.95
k=3 326 94.57
k=4 336 95.40
k=5 346 96.41
k=6 366 94.53
Table 3: Controlling the behavior of stopping through the
use of longevity. For window length k longevity levels in
{1, 2, 3, 4, 5, 6}, the 10-fold average number of annota-
tions at the automatically determined stopping points and
the 10-fold average F-measure at the automatically deter-
mined stopping points are displayed for the Spamassassin
dataset.
could be developed to create stop sets with high rep-
resentativeness (in terms of feature space) density
(meaning representativeness of stop set divided by
size of stop set). For example, a possibility is to
cluster examples before AL begins and then make
sure the stop set contains examples from each of the
clusters. Another possibility is to use a greedy algo-
rithm where the stop set is iteratively grown where
on each iteration the center of mass of the stop set
in feature space is computed and an example in the
unlabeled pool that is maximally far in feature space
from this center of mass is selected for inclusion in
the stop set. This could be useful for efficiency (in
terms of getting the same stopping performance with
a smaller stop set as could be achieved with a larger
stop set) and also as a way to ensure adequate repre-
sentation of the task space. The latter can be accom-
45
Task-Dataset SP V2008 ZWH2008 LS2008 All
Spamassassin 286 1208 386 756 5400
(10-fold AVG) 94.92 89.89 95.31 96.40 91.74
Table 5: Methods for stopping AL with maximum entropy as the base learner. For each stopping method, the average
number of annotations at the automatically determined stopping point and the average F-measure at the automatically
determined stopping point are displayed. Bold entries are statistically significantly different than SP (and non-bold
entries are not). SC2000, the margin exhaustion method, is not shown since it can?t be used with a non-margin-based
learner. The final column (labeled ?All?) represents standard fully supervised passive learning with the entire set of
training data.
Stop Set Size Annotations F-Measure
2500 326 95.58
2000 326 94.57
1500 314 95.00
1000 328 95.73
500 314 94.57
Table 4: Investigating the sensitivity to stop set size. For
stop set sizes in {2500, 2000, 1500, 1000, 500}, the 10-
fold average number of annotations at the automatically
determined stopping points and the 10-fold average F-
measure at the automatically determined stopping points
are displayed for the Spamassassin dataset.
plished by perhaps continuing to add examples to
the stop set until adding new examples is no longer
increasing the representativeness of the stop set.
As one of the advantages of SP is that it?s widely
applicable, Table 5 shows the results when using
maximum entropy models as the base learner dur-
ing AL (the query points selected are those which
the model is most uncertain about). The results re-
inforce our conclusions from the SVM experiments,
with SP performing aggressively and all statistically
significant differences in performance being in SP?s
favor. Figure 3 shows the graph for a representative
fold.
5 Conclusions
Effective methods for stopping AL are crucial for re-
alizing the potential annotation savings enabled by
AL. A survey of existing stopping methods identi-
fied three areas where improvements are called for.
The new stopping method based on Stabilizing Pre-
dictions (SP) addresses all three areas: SP is widely
applicable, stable, and aggressive in saving annota-
tions.
0 1000 2000 3000 4000 5000 6000
50
60
70
80
90
100
Number of Human Annotations Requested
Pe
rf
or
m
an
ce
 (F
?M
ea
su
re)
AL?MaxEnt: Spamassassin Fold 5
SP
ZWH2008
LS2008
V2008
Figure 3: Graphic with stopping criteria in action for fold
5 of TC of the spamassassin corpus. The x-axis ranges
from 0 to 6,000.
The empirical evaluation of SP and the existing
methods was informative for evaluating the crite-
ria but it was also informative for demonstrating the
difficulties for rigorous objective evaluation of stop-
ping criteria due to different annotation/performance
tradeoff valuations. This opens up a future area for
work on user-adjustable stopping. Two potential
avenues for enabling user-adjustable stopping are a
single criterion that is itself adjustable or a suite of
methods with consistent differing levels of aggres-
siveness/conservativeness from which users can pick
the one(s) that suit their annotation/performance
tradeoff valuation. SP substantially widens the range
of behaviors of existing methods that users can
choose from. Also, SP?s behavior itself can be ad-
justed through user-controllable parameters.
46
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Jason Baldridge and Miles Osborne. 2008. Active learn-
ing and logarithmic opinion pools for hpsg parse se-
lection. Nat. Lang. Eng., 14(2):191?222.
Michael Bloodgood and K. Vijay-Shanker. 2009. Taking
into account the differences between actively and pas-
sively acquired data: The case of active learning with
support vector machines for imbalanced datasets. In
NAACL.
Colin Campbell, Nello Cristianini, and Alex J. Smola.
2000. Query learning with large margin classifiers.
In ICML ?00: Proceedings of the Seventeenth Interna-
tional Conference on Machine Learning, pages 111?
118, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20:37?46.
Gordon Cormack and Thomas Lynam. 2005. Trec 2005
spam track overview. In TREC-14.
Susan Dumais, John Platt, David Heckerman, and
Mehran Sahami. 1998. Inductive learning algorithms
and representations for text categorization. In CIKM
?98: Proceedings of the seventh international con-
ference on Information and knowledge management,
pages 148?155, New York, NY, USA. ACM.
Seyda Ertekin, Jian Huang, Le?on Bottou, and C. Lee
Giles. 2007. Learning on the border: active learn-
ing in imbalanced data classification. In Ma?rio J.
Silva, Alberto H. F. Laender, Ricardo A. Baeza-Yates,
Deborah L. McGuinness, Bj?rn Olstad, ?ystein Haug
Olsen, and Andre? O. Falca?o, editors, Proceedings of
the Sixteenth ACM Conference on Information and
Knowledge Management, CIKM 2007, Lisbon, Portu-
gal, November 6-10, 2007, pages 127?136. ACM.
Thorsten Joachims. 1998. Text categorization with su-
port vector machines: Learning with many relevant
features. In ECML, pages 137?142.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Advances in Kernel Methods ?
Support Vector Learning, pages 169?184.
Florian Laws and Hinrich Schu?tze. 2008. Stopping crite-
ria for active learning of named entity recognition. In
Proceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 465?
472, Manchester, UK, August. Coling 2008 Organiz-
ing Committee.
Ki-Joong Lee, Young-Sook Hwang, Seonho Kim, and
Hae-Chang Rim. 2004. Biomedical named entity
recognition using two-phase model based on svms.
Journal of Biomedical Informatics, 37(6):436?447.
Andrew McCallum and Kamal Nigam. 1998. A compar-
ison of event models for naive bayes text classification.
In Proceedings of AAAI-98, Workshop on Learning for
Text Categorization.
Greg Schohn and David Cohn. 2000. Less is more: Ac-
tive learning with support vector machines. In Proc.
17th International Conf. on Machine Learning, pages
839?846. Morgan Kaufmann, San Francisco, CA.
D. Sculley. 2007. Online active learning methods for fast
label-efficient spam filtering. In Conference on Email
and Anti-Spam (CEAS), Mountain View, CA, USA.
Simon Tong and Daphne Koller. 2002. Support vec-
tor machine active learning with applications to text
classification. Journal of Machine Learning Research
(JMLR), 2:45?66.
Andreas Vlachos. 2008. A stopping criterion for active
learning. Computer Speech and Language, 22(3):295?
312.
Jingbo Zhu and Eduard Hovy. 2007. Active learning
for word sense disambiguation with methods for ad-
dressing the class imbalance problem. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
783?790.
Jingbo Zhu, Huizhen Wang, and Eduard Hovy. 2008a.
Learning a stopping criterion for active learning for
word sense disambiguation and text classification. In
IJCNLP.
Jingbo Zhu, Huizhen Wang, and Eduard Hovy. 2008b.
Multi-criteria-based strategy to stop active learning for
data annotation. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 1129?1136, Manchester, UK, Au-
gust. Coling 2008 Organizing Committee.
47
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 202?210,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Translation Memory Retrieval Methods
Michael Bloodgood
Center for Advanced Study of Language
University of Maryland
College Park, MD 20742 USA
meb@umd.edu
Benjamin Strauss
Center for Advanced Study of Language
University of Maryland
College Park, MD 20742 USA
bstrauss@umd.edu
Abstract
Translation Memory (TM) systems are
one of the most widely used translation
technologies. An important part of TM
systems is the matching algorithm that de-
termines what translations get retrieved
from the bank of available translations
to assist the human translator. Although
detailed accounts of the matching algo-
rithms used in commercial systems can?t
be found in the literature, it is widely
believed that edit distance algorithms are
used. This paper investigates and eval-
uates the use of several matching algo-
rithms, including the edit distance algo-
rithm that is believed to be at the heart
of most modern commercial TM systems.
This paper presents results showing how
well various matching algorithms corre-
late with human judgments of helpfulness
(collected via crowdsourcing with Ama-
zon?s Mechanical Turk). A new algorithm
based on weighted n-gram precision that
can be adjusted for translator length pref-
erences consistently returns translations
judged to be most helpful by translators for
multiple domains and language pairs.
1 Introduction
The most widely used computer-assisted transla-
tion (CAT) tool for professional translation of spe-
cialized text is translation memory (TM) technol-
ogy (Christensen and Schjoldager, 2010). TM
consists of a database of previously translated ma-
terial, referred to as the TM vault or the TM bank
(TMB in the rest of this paper). When a trans-
lator is translating a new sentence, the TMB is
consulted to see if a similar sentence has already
been translated and if so, the most similar pre-
vious translation is retrieved from the bank to
help the translator. The main conceptions of TM
technology occurred in the late 1970s and early
1980s (Arthern, 1978; Kay, 1980; Melby and oth-
ers, 1981). TM has been widely used since the
late 1990s and continues to be widely used to-
day (Bowker and Barlow, 2008; Christensen and
Schjoldager, 2010; Garcia, 2007; Somers, 2003).
There are a lot of factors that determine how
helpful TM technology will be in practice. Some
of these include: quality of the interface, speed of
the back-end database lookups, speed of network
connectivity for distributed setups, and the com-
fort of the translator with using the technology.
A fundamentally important factor that determines
how helpful TM technology will be in practice is
how well the TM bank of previously translated
materials matches up with the workload materials
to be translated. It is necessary that there be a high
level of match for the TM technology to be most
helpful. However, having a high level of match is
not sufficient. One also needs a successful method
for retrieving the useful translations from the (po-
tentially large) TM bank.
TM similarity metrics are used for both evalu-
ating the expected helpfulness of previous transla-
tions for new workload translations and the met-
rics also directly determine what translations get
provided to the translator during translation of new
materials. Thus, the algorithms that compute the
TM similarity metrics are not only important, but
they are doubly important.
The retrieval algorithm used by commercial TM
systems is typically not disclosed (Koehn and
Senellart, 2010; Simard and Fujita, 2012; Why-
man and Somers, 1999). However, the best-
performing method used in current systems is
widely believed to be based on edit distance (Bald-
win and Tanaka, 2000; Simard and Fujita, 2012;
Whyman and Somers, 1999; Koehn and Senellart,
2010; Christensen and Schjoldager, 2010; Man-
dreoli et al., 2006; He et al., 2010). Recently
202
Simard and Fujita (2012) have experimented with
using MT (machine translation) evaluation metrics
as TM fuzzy match, or similarity, algorithms. A
limitation of the work of (Simard and Fujita, 2012)
was that the evaluation of the performance of the
TM similarity algorithms was also conducted us-
ing the same MT evaluation metrics. Simard
and Fujita (2012) concluded that their evalua-
tion of TM similarity functions was biased since
whichever MT evaluation metric was used as the
TM similarity function was also likely to obtain
the best score under that evaluation metric.
The current paper explores various TM fuzzy
match algorithms ranging from simple baselines
to the widely used edit distance to new methods.
The evaluations of the TM fuzzy match algorithms
use human judgments of helpfulness. An algo-
rithm based on weighted n-gram precision consis-
tently returns translations judged to be most help-
ful by translators for multiple domains and lan-
guage pairs. In addition to being able to retrieve
useful translations from the TM bank, the fuzzy
match scores ought to be indicative of how helpful
a translation can be expected to be. Many transla-
tors find it counter-productive to use TM when the
best-matching translation from the TM is not simi-
lar to the workload material to be translated. Thus,
many commercial TM products offer translators
the opportunity to set a fuzzy match score thresh-
old so that only translations with scores above the
threshold will ever be returned. It seems to be a
widely used practice to set the threshold at 70%
but again it remains something of a black-box as to
why 70% ought to be the setting. The current pa-
per uncovers what expectations of helpfulness can
be given for different threshold settings for various
fuzzy match algorithms.
The rest of this paper is organized as follows.
Section 2 presents the TM similarity metrics that
will be explored; section 3 presents our experi-
mental setup; section 4 presents and analyzes re-
sults; and section 5 concludes.
2 Translation Memory Similarity
Metrics
In this section we define the methods for measur-
ing TM similarity for which experimental results
are reported in section 4. All of the metrics com-
pute scores between 0 and 1, with higher scores
indicating better matches. All of the metrics take
two inputs: M and C, where M is a workload sen-
tence from the MTBT (Material To Be Translated)
and C is the source language side of a candidate
pre-existing translation from the TM bank. The
metrics range from simple baselines to the sur-
mised current industrial standard to new methods.
2.1 Percent Match
Perhaps the simplest metric one could conceive of
being useful for TM similarity matching is percent
match (PM), the percent of tokens in the MTBT
segment found in the source language side of the
candidate translation pair from the TM bank.
Formally,
PM(M,C) =
|M
unigrams
?
C
unigrams
|
|M
unigrams
|
, (1)
where M is the sentence from the MTBT that is
to be translated, C is the source language side
of the candidate translation from the TM bank,
M
unigrams
is the set of unigrams in M , and
C
unigrams
is the set of unigrams in C.
2.2 Weighted Percent Match
A drawback of PM is that it weights the match-
ing of each unigram in an MTBT segment equally,
however, it is not the case that the value of assis-
tance to the translator is equal for each unigram
of the MTBT segment. The parts that are most
valuable to the translator are the parts that he/she
does not already know how to translate. Weighted
percent match (WPM) uses inverse document fre-
quency (IDF) as a proxy for trying to weight words
based on how much value their translations are ex-
pected to provide to translators. The use of IDF-
based weighting is motivated by the assumption
that common words that permeate throughout the
language will be easy for translators to translate
but words that occur in relatively rare situations
will be harder to translate and thus more valuable
to match in the TM bank. For our implementa-
tion of WPM, each source language sentence in
the parallel corpus we are experimenting with is
treated as a ?document? when computing IDF.
Formally,
WPM(M,C) =
?
u?{M
unigrams
T
C
unigrams
}
idf(u,D)
?
u?M
unigrams
idf(u,D)
, (2)
where M , C, M
unigrams
, and C
unigrams
are as
defined in Eq. 1, D is the set of all source language
203
sentences in the parallel corpus, and idf(x,D) =
log(
|D|
|{d?D:x?d}|
).
2.3 Edit Distance
A drawback of both the PM and WPM metrics
are that they are only considering coverage of the
words from the workload sentence in the candi-
date sentence from the TM bank and not taking
into account the context of the words. However,
words can be translated very differently depending
on their context. Thus, a TM metric that matches
sentences on more than just (weighted) percentage
coverage of lexical items can be expected to per-
form better for TM bank evaluation and retrieval.
Indeed, as was discussed in section 1, it is widely
believed that most TM similarity metrics used in
existing systems are based on string edit distance.
Our implementation of edit distance (Leven-
shtein, 1966), computed on a word level, is sim-
ilar to the version defined in (Koehn and Senellart,
2010).
Formally, our TM metric based on Edit Dis-
tance (ED) is defined as
ED = max
(
1?
edit-dist(M,C)
|M
unigrams
|
, 0
)
, (3)
where M , C, and M
unigrams
are as defined in
Eq. 1, and edit-dist(M,C) is the number of word
deletions, insertions, and substitutions required to
transform M into C.
2.4 N-Gram Precision
Although ED takes context into account, it does
not emphasize local context in matching certain
high-value words and phrases as much as metrics
that capture n-gram precision between the MTBT
workload sentence and candidate source-side sen-
tences from the TMB. We note that n-gram preci-
sion forms a fundamental subcomputation in the
computation of the corpus-level MT evaluation
metric BLEU score (Papineni et al., 2002). How-
ever, although TM fuzzy matching metrics are re-
lated to automated MT evaluation metrics, there
are some important differences. Perhaps the most
important is that TM fuzzy matching has to be able
to operate at a sentence-to-sentence level whereas
automated MT evaluation metrics such as BLEU
score are intended to operate over a whole cor-
pus. Accordingly, we make modifications to how
we use n-gram precision for the purpose of TM
matching than how we use it when we compute
BLEU scores. The rest of this subsection and the
next two subsections describe the innovations we
make in adapting the notion of n-gram precision to
the TM matching task.
Our first metric along these lines, N-Gram Pre-
cision (NGP), is defined formally as follows:
NGP =
N
?
n=1
1
N
p
n
, (4)
where the value of N sets the upper bound on the
length of n-grams considered
1
, and
p
n
=
|M
n-grams
? C
n-grams
|
Z ? |M
n-grams
|+ (1? Z) ? |C
n-grams
|
, (5)
where M and C are as defined in Eq. 1, M
n-grams
is the set of n-grams in M , C
n-grams
is the set of
n-grams in C, and Z is a user-set parameter that
controls how the metric is normalized.
2
As seen by equation 4, we use an arithmetic
mean of precisions instead of the geometric mean
that BLEU score uses. An arithmetic mean is bet-
ter than a geometric mean for use in translation
memory metrics since translation memory metrics
are operating at a segment level and not at the
aggregate level of an entire test set. At the ex-
treme, the geometric mean will be zero if any of
the n-gram precisions p
n
are zero. Since large n-
gram matches are unlikely on a segment level, us-
ing a geometric mean can be a poor method to use
for matching on a segment level, as has been de-
scribed for the related task of MT evaluation (Dod-
dington, 2002; Lavie et al., 2004). Additionally,
for the related task of MT evaluation at a segment
level, Lavie et al. (2004) have found that using
an arithmetic mean correlates better with human
judgments than using a geometric mean.
Now we turn to discussing the parameter Z for
controlling how the metric is normalized. At one
extreme, setting Z=1 will correspond to having no
penalty on the length of the candidate retrieved
from the TMB and leads to getting longer trans-
lation matches retrieved. At the other extreme,
1
We used N = 4 in our experiments.
2
Note that the n in n-grams is intended to be substituted
with the corresponding integer. Accordingly, for p
1
, n = 1
and therefore M
n-grams
= M
1-grams
is the set of unigrams
in M and C
n-grams
= C
1-grams
is the set of unigrams in C;
for p
2
, n = 2 and therefore M
n-grams
= M
2-grams
is the
set of bigrams in M and C
n-grams
= C
2-grams
is the set of
bigrams in C; and so on.
204
setting Z=0 will correspond to a normalization
that penalizes relatively more for length of the
retrieved candidate and leads to shorter transla-
tion matches being retrieved. There is a preci-
sion/recall tradeoff in that one wants to retrieve
candidates from the TMB that have high recall
in the sense of matching what is in the MTBT
sentence yet one also wants the retrieved candi-
dates from the TMB to have high precision in the
sense of not having extraneous material not rele-
vant to helping with the translation of the MTBT
sentence. The optimal setting of Z may differ
for different scenarios based on factors like the
languages, the corpora, and translator preference.
We believe that for most TM applications there
will usually be an asymmetric valuation of pre-
cision/recall in that recall will be more important
since the value of getting a match will be more
than the cost of extra material up to a point. There-
fore, we believe a Z setting in between 0.5 and 1.0
will be an optimal default. We use Z=0.75 in all
of our experiments described in section 3 and re-
ported on in section 4 except for the experiments
explicitly showing the impact of changing the Z
parameter.
2.5 Weighted N-Gram Precision
Analogous to how we improved PM with WPM,
we seek to improve NGP in a similar fashion. As
can be seen from the numerator of Equation 5,
NGP is weighting the match of all n-grams as
uniformly important. However, it is not the case
that each n-gram is of equal value to the transla-
tor. Similar to WPM, we use IDF as the basis of
our proxy for weighting n-grams according to the
value their translations are expected to provide to
translators. Specifically, we define the weight of
an n-gram to be the sum of the IDF values for each
constituent unigram that comprises the n-gram.
Accordingly, we formally define method
Weighted N-Gram Precision (WNGP) as follows:
WNGP =
N
?
n=1
1
N
wp
n
, (6)
where N is as defined in Equation 4, and
wp
n
=
?
i?{M
n-grams
? C
n-grams
}
w(i)
Z
[
?
i?M
n-grams
w(i)
]
+ (1? Z)
[
?
i?C
n-grams
w(i)
]
,
(7)
where Z, M
n-grams
, and C
n-grams
are as defined
in Equation 5, and
w(i) =
?
1-gram?i
idf(1-gram,D), (8)
where i is an n-gram and idf(x,D) is as defined
above for Equation 2.
2.6 Modified Weighted N-gram Precision
Note that in Equation 6 each wp
n
contributes
equally to the average. Modified Weighted N-
Gram Precision (MWNGP) improves on WNGP
by weighting the contribution of each wp
n
so that
shorter n-grams contribute more than longer n-
grams. The intuition is that for TM settings, get-
ting more high-value shorter n-gram matches at
the expense of fewer longer n-gram matches will
be more helpful since translators will get relatively
more assistance from seeing new high-value vo-
cabulary. Since the translators already presumably
know the rules of the language in terms of how
to order words correctly, the loss of the longer n-
gram matches will be mitigated.
Formally we define MWNGP as follows:
MWNGP =
2
N
2
N
? 1
N
?
n=1
1
2
n
wp
n
, (9)
where N and wp
n
are as they were defined for
Equation 6.
3 Experimental Setup
We performed experiments on two corpora from
two different technical domains with two language
pairs, French-English and Chinese-English. Sub-
section 3.1 discusses the specifics of the corpora
and the processing we performed. Subsection 3.2
discusses the specifics of our human evaluations of
how helpful retrieved segments are for translation.
205
3.1 Corpora
For Chinese-English experiments, we used the
OpenOffice3 (OO3) parallel corpus (Tiedemann,
2009), which is OO3 computer office productiv-
ity software documentation. For French-English
experiments, we used the EMEA parallel cor-
pus (Tiedemann, 2009), which are medical docu-
ments from the European Medecines Agency. The
corpora were produced by a suite of automated
tools as described in (Tiedemann, 2009) and come
sentence-aligned.
The first step in our experiments was to pre-
process the corpora. For Chinese corpora we to-
kenize each sentence using the Stanford Chinese
Word Segmenter (Tseng et al., 2005) with the Chi-
nese Penn Treebank standard (Xia, 2000). For all
corpora we remove all segments that have fewer
than 5 tokens or more than 100 tokens. We call
the resulting set the valid segments. For the pur-
pose of computing match statistics, for French cor-
pora we remove all punctuation, numbers, and sci-
entific symbols; we case-normalize the text and
stem the corpus using the NLTK French snowball
stemmer. For the purpose of computing match
statistics, for Chinese corpora we remove all but
valid tokens. Valid tokens must include at least
one Chinese character. A Chinese character is de-
fined as a character in the Unicode range 0x4E00-
0x9FFF or 0x4000-0x4DFF or 0xF900-0xFAFF.
The rationale for removing these various tokens
from consideration for the purpose of comput-
ing match statistics is that translation of numbers
(when they?re written as Arabic numerals), punc-
tuation, etc. is the same across these languages
and therefore we don?t want them influencing the
match computations. But once a translation is se-
lected as being most helpful for translation, the
original version (that still contains all the numbers,
punctuation, case markings, etc.) is the version
that is brought back and displayed to the transla-
tor.
For the TM simulation experiments, we ran-
domly sampled 400 translations from the OO3
corpus and pretended that the Chinese sides of
those 400 translations constitute the workload
Chinese MTBT. From the rest of the corpus we
randomly sampled 10,000 translations and pre-
tended that that set of 10,000 translations consti-
tutes the Chinese-English TMB. We also did simi-
lar sampling from the EMEA corpus of a workload
French MTBT of size 300 and a French-English
TMB of size 10,000.
After the preprocessing and selection of the
TMB and MTBT, we found the best-matching
segment from the TMB for each MTBT seg-
ment according to each TM retrieval metric de-
fined in section 2.
3
The resulting sets of
(MTBT segment,best-matching TMB segment)
pairs formed the inputs on which we conducted
our evaluations of the performance of the various
TM retrieval metrics.
3.2 Human Evaluations
To conduct evaluations of how helpful the transla-
tions retrieved by the various TM retrieval metrics
would be for translating the MTBT segments, we
used Amazon Mechanical Turk, which has been
used productively in the past for related work in
the context of machine translation (Bloodgood and
Callison-Burch, 2010b; Bloodgood and Callison-
Burch, 2010a; Callison-Burch, 2009).
For each (MTBT segment,best-matching TMB
segment) pair generated as discussed in subsec-
tion 3.1, we collected judgments from Turkers
(i.e., the workers on MTurk) on how helpful
the TMB translation would be for translating the
MTBT segment on a 5-point scale. The 5-point
scale was as follows:
? 5 = Extremely helpful. The sample is so sim-
ilar that with trivial modifications I can do the
translation.
? 4 = Very helpful. The sample included a large
amount of useful words or phrases and/or
some extremely useful words or phrases that
overlapped with the MTBT.
? 3 = Helpful. The sample included some use-
ful words or phrases that made translating the
MTBT easier.
? 2 = Slightly helpful. The sample contained
only a small number of useful words or
phrases to help with translating the MTBT.
? 1 = Not helpful or detrimental. The sample
would not be helpful at all or it might even be
harmful for translating the MTBT.
After a worker rated a (MTBT segment,TMB
segment) pair the worker was then required to give
3
If more than one segment from the TMB was tied for
being the highest-scoring segment, the segment located first
in the TMB was considered to be the best-matching segment.
206
metric PM WPM ED NGP WNGP MWNGP
PM 100.0 69.5 23.0 32.0 31.5 35.5
WPM 69.5 100.0 25.8 37.0 39.0 44.2
ED 23.0 25.8 100.0 41.5 35.8 35.0
NGP 32.0 37.0 41.5 100.0 77.8 67.0
WNGP 31.5 39.0 35.8 77.8 100.0 81.2
MWNGP 35.5 44.2 35.0 67.0 81.2 100.0
Table 1: OO3 Chinese-English: The percent of the
time that each pair of metrics agree on the most
helpful TM segment
metric PM WPM ED NGP WNGP MWNGP
PM 100.0 64.7 30.3 40.3 38.3 41.3
WPM 64.7 100.0 32.0 46.3 47.0 54.3
ED 30.3 32.0 100.0 42.3 40.3 39.3
NGP 40.3 46.3 42.3 100.0 76.3 67.7
WNGP 38.3 47.0 40.3 76.3 100.0 81.3
MWNGP 41.3 54.3 39.3 67.7 81.3 100.0
Table 2: EMEA French-English: The percent of
the time that each pair of metrics agree on the most
helpful TM segment
an explanation for their rating. These explanations
proved quite helpful as discussed in section 4. For
each (MTBT segment,TMB segment) pair, we col-
lected judgments from five different Turkers. For
each (MTBT segment,TMB segment) pair these
five judgments were then averaged to form a mean
opinion score (MOS) on the helpfulness of the re-
trieved TMB translation for translating the MTBT
segment. These MOS scores form the basis of our
evaluation of the performance of the different TM
retrieval metrics.
4 Results and Analysis
4.1 Main Results
Tables 1 and 2 show the percent of the time that
each pair of metrics agree on the choice of the
most helpful TM segment for the Chinese-English
OO3 data and the French-English EMEA data, re-
spectively. A main observation to be made is that
the choice of metric makes a big difference in
the choice of the most helpful TM segment. For
example, we can see that the surmised industrial
standard ED metric agrees with the new MWNGP
metric less than 40% of the time on both sets of
data (35.0% on Chinese-English OO3 and 39.3%
on French-English EMEA data).
Tables 3 and 4 show the number of times each
metric found the TM segment that the Turkers
judged to be the most helpful out of all the TM
segments retrieved by all of the different metrics.
From these tables one can see that the MWNGP
Metric Found Best Total MTBT Segments
PM 178 400
WPM 200 400
ED 193 400
NGP 251 400
WNGP 271 400
MWNGP 282 400
Table 3: OO3 Chinese-English: The number of
times that each metric found the most helpful TM
segment (possibly tied).
Metric Found Best Total MTBT Segments
PM 166 300
WPM 184 300
ED 148 300
NGP 188 300
WNGP 198 300
MWNGP 201 300
Table 4: EMEA French-English: The number of
times that each metric found the most helpful TM
segment (possibly tied).
method consistently retrieves the best TM segment
more often than each of the other metrics. Scat-
terplots showing the exact performance on every
MTBT segment of the OO3 dataset for various
metrics are shown in Figures 1, 2, and 3. To con-
serve space, scatterplots are only shown for met-
rics PM (baseline metric), ED (strong surmised
industrial standard metric), and MWNGP (new
highest-performing metric). For each MTBT seg-
ment, there is a point in the scatterplot. The y-
coordinate is the value assigned by the TM metric
to the segment retrieved from the TM bank and
the x-coordinate is the MOS of the five Turkers
on how helpful the retrieved TM segment would
be for translating the MTBT segment. A point
is depicted as a dark blue diamond if none of
the other metrics retrieved a segment with higher
MOS judgment for that MTBT segment. A point
is depicted as a yellow circle if another metric re-
trieved a different segment from the TM bank for
that MTBT segment that had a higher MOS.
A main observation from Figure 1 is that PM is
failing as evidenced by the large number of points
in the upper left quadrant. For those points, the
metric value is high, indicating that the retrieved
segment ought to be helpful. However, the MOS
is low, indicating that the humans are judging it
to not be helpful. Figure 2 shows that the ED
207
metric does not suffer from this problem. How-
ever, Figure 2 shows that ED has another prob-
lem, which is a lot of yellow circles in the lower
left quadrant. Points in the lower left quadrant are
not necessarily indicative of a poorly performing
metric, depending on the degree of match of the
TMB with the MTBT workload. If there is noth-
ing available in the TMB that would help with
the MTBT, it is appropriate for the metric to as-
sign a low value and the humans to correspond-
ingly agree that the retrieved sentence is not help-
ful. However, the fact that so many of ED?s points
are yellow circles indicates that there were better
segments available in the TMB that ED was not
able to retrieve yet another metric was able to re-
trieve them. Observing the scatterplots for ED and
those for MWNGP one can see that both methods
have the vast majority of points concentrated in
the lower left and upper right quadrants, solving
the upper left quadrant problem of PM. However,
MWNGP has a relatively more densely populated
upper right quadrant populated with dark blue di-
amonds than ED does whereas ED has a more
densely populated lower left quadrant with yel-
low circles than MWNGP does. These results and
trends are consistent across the EMEA French-
English dataset so those scatterplots are omitted
to conserve space.
Examining outliers where MWNGP assigns a
high metric value yet the Turkers indicated that the
translation has low helpfulness such as the point
in Figure 3 at (1.6,0.70) is informative. Looking
only at the source side, it looks like the translation
retrieved from the TMB ought to be very help-
ful. The Turkers put in their explanation of their
scores that the reason they gave low helpfulness
is because the English translation was incorrect.
This highlights that a limitation of MWNGP, and
all other TM metrics we?re aware of, is that they
only consider the source side.
4.2 Adjusting for length preferences
As discussed in section 2, the Z parameter can be
used to control for length preferences. Table 5
shows how the average length, measured by num-
ber of tokens of the source side of the translation
pairs returned by MWNGP, changes as the Z pa-
rameter is changed.
Table 6 shows an example of how the opti-
mal translation pair returned by MWNGP changes
from Z=0.00 to Z=1.00. The example illustrates
1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0MOS
0.0
0.2
0.4
0.6
0.8
1.0
Met
ric 
Val
ue
Figure 1: OO3 PM scatterplot
1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0MOS
0.0
0.2
0.4
0.6
0.8
1.0
Met
ric 
Val
ue
Figure 2: OO3 ED scatterplot
1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0MOS
0.0
0.2
0.4
0.6
0.8
1.0
Met
ric 
Val
ue
Figure 3: OO3 MWNGP scatterplot
208
MTBT French: Ne pas utiliser durant la gestation et la lactation, car l? innocuit?e du
m?edicament v?et?erinaire n? a pas ?et?e ?etablie pendant la gestation ou
la lactation.
English: Do not use during pregnancy and lactation because the safety of the
veterinary medicinal product has not been established during
pregnancy and lactation.
MWNGP French: Peut ?etre utilis?e pendant la gestation et la lactation.
(Z=0.00) English: Can be used during pregnancy and lactation.
MWNGP French: Ne pas utiliser chez l? animal en gestation ou en p?eriode de lactation,
(Z=1.00) car la s?ecurit?e du robenacoxib n? a pas ?et?e ?etablie chez les femelles gestantes ou
allaitantes ni chez les chats et chiens utilis?es pour la reproduction.
English: Do not use in pregnant or lactating animals because the safety of
robenacoxib has not been established during pregnancy and lactation or in cats
and dogs used for breeding.
Table 6: This table shows for an example MTBT workload sentence from the EMEA French-English data
how the optimal translation pair returned by MWNGP changes when going from Z = 0.00 to Z = 1.00.
We provide the English translation of the MTBT workload sentence for the convenience of the reader
since it was available from the EMEA parallel corpus. Note that in a real setting it would be the job of
the translator to produce the English translation of the MTBT-French sentence using the translation pairs
returned by MWNGP as help.
Z Value Avg Length
0.00 9.9298
0.25 13.204
0.50 16.0134
0.75 19.6355
1.00 27.8829
(a) EMEA French-English
Z Value Avg Length
0.00 7.2475
0.25 9.5600
0.50 11.1250
0.75 14.1825
1.00 25.0875
(b) OO3 Chinese-English
Table 5: Average TM segment length, measured
by number of tokens of the source side of the trans-
lation pairs returned by MWNGP, for varying val-
ues of the Z parameter
the impact of changing the Z value on the na-
ture of the translation matches that get returned
by MWNGP. As discussed in section 2, smaller
settings of Z are appropriate for preferences for
shorter matches that are more precise in the sense
that a larger percentage of their content will be
relevant. Larger settings of Z are appropriate for
preferences for longer matches that have higher re-
call in the sense that they will have more matches
with the content in the MTBT segment overall, al-
though at the possible expense of having more ir-
relevant content as well.
5 Conclusions
Translation memory is one of the most widely
used translation technologies. One of the most
important aspects of the technology is the system
for assessing candidate translations from the TM
bank for retrieval. Although detailed descriptions
of the apparatus used in commercial systems are
lacking, it is widely believed that they are based
on an edit distance approach. We have defined
and examined several TM retrieval approaches, in-
cluding a new method using modified weighted n-
gram precision that performs better than edit dis-
tance according to human translator judgments of
helpfulness. The MWNGP method is based on the
following premises: local context matching is de-
sired; weighting words and phrases by expected
helpfulness to translators is desired; and allowing
shorter n-gram precisions to contribute more to the
final score than longer n-gram precisions is de-
sired. An advantage of the method is that it can be
adjusted to suit translator length preferences of re-
turned matches. A limitation of MWNGP, and all
other TM metrics we are aware of, is that they only
consider the source language side. Examples from
our experiments reveal that this can lead to poor
retrievals. Therefore, future work is called for to
examine the extent to which the target language
sides of the translations in the TM bank influence
TM system performance and to investigate ways
to incorporate target language side information to
improve TM system performance.
209
References
Peter J Arthern. 1978. Machine translation and com-
puterized terminology systems: a translator?s view-
point. In Translating and the Computer: Proceed-
ings of a Seminar, pages 77?108.
Timothy Baldwin and Hozumi Tanaka. 2000. The ef-
fects of word order and segmentation on translation
retrieval performance. In Proceedings of the 18th
conference on Computational linguistics-Volume 1,
pages 35?41. Association for Computational Lin-
guistics.
Michael Bloodgood and Chris Callison-Burch. 2010a.
Bucking the trend: Large-scale cost-focused active
learning for statistical machine translation. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 854?864.
Association for Computational Linguistics.
Michael Bloodgood and Chris Callison-Burch. 2010b.
Using mechanical turk to build machine translation
evaluation sets. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, pages 208?
211. Association for Computational Linguistics.
Lynne Bowker and Michael Barlow. 2008. A
comparative evaluation of bilingual concordancers
and translation memory systems. Topics in Lan-
guage Resources for Translation and Localization,
?
Amsterdam-Filadelfia: John Benjamins, pages 1?22.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: Evaluating translation quality using Amazon?s
Mechanical Turk. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 286?295, Singapore, August. As-
sociation for Computational Linguistics.
Tina Paulsen Christensen and Anne Gram Schjoldager.
2010. Translation-memory (tm) research: what do
we know and how do we know it? Hermes, 44:89?
101.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Language
Technology Research, HLT ?02, pages 138?145, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Ignacio Garcia. 2007. Power shifts in web-based trans-
lation memory. Machine Translation, 21(1):55?68.
Yifan He, Yanjun Ma, Andy Way, and Josef Van Gen-
abith. 2010. Integrating n-best smt outputs into a
tm system. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
pages 374?382. Association for Computational Lin-
guistics.
Martin Kay. 1980. The proper place of men and ma-
chines in language translation. In Research Report
CSL-80-11, Xerox PARC, Palo Alto, CA. Reprinted
in Machine Translation 12, 3-23, 1997.
Philipp Koehn and Jean Senellart. 2010. Convergence
of translation memory and statistical machine trans-
lation. In Proceedings of AMTA Workshop on MT
Research and the Translation Industry, pages 21?31.
Alon Lavie, Kenji Sagae, and Shyamsundar Jayara-
man. 2004. The significance of recall in auto-
matic metrics for mt evaluation. In In Proceedings
of the 6th Conference of the Association for Machine
Translation in the Americas (AMTA-2004.
Vladimir I Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. In
Soviet physics doklady, volume 10, page 707.
Federica Mandreoli, Riccardo Martoglia, and Paolo
Tiberio. 2006. Extra: a system for example-
based translation assistance. Machine Translation,
20(3):167?197.
Alan K Melby et al. 1981. A bilingual concordance
system and its use in linguistic studies. In The
Eighth Lacus Forum, pages 541?549, Columbia, SC.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311?318. Association for
Computational Linguistics.
Michel Simard and Atsushi Fujita. 2012. A poor man?s
translation memory using machine translation eval-
uation metrics. In Conference of the Association
for Machine Translation in the Americas 2012, San
Diego, California, USA, October.
Harold L Somers. 2003. Computers and translation:
a translator?s guide, volume 35. John Benjamins
Publishing Company.
J?org Tiedemann. 2009. News from OPUS - A col-
lection of multilingual parallel corpora with tools
and interfaces. In N. Nicolov, K. Bontcheva,
G. Angelova, and R. Mitkov, editors, Recent
Advances in Natural Language Processing, vol-
ume V, pages 237?248. John Benjamins, Amster-
dam/Philadelphia, Borovets, Bulgaria.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, volume
171. Jeju Island, Korea.
Edward K. Whyman and Harold L. Somers. 1999.
Evaluation metrics for a translation memory system.
Software-Practice and Experience, 29:1265?1284.
Fei Xia. 2000. The segmentation guidelines for
the penn chinese treebank (3.0). Technical Report
IRCS-00-06, University of Pennsylvania.
210
Use of Modality and Negation in
Semantically-Informed Syntactic MT
Kathryn Baker?
U.S. Department of Defense
Michael Bloodgood??
University of Maryland
Bonnie J. Dorr?
University of Maryland
Chris Callison-Burch?
Johns Hopkins University
Nathaniel W. Filardo?
Johns Hopkins University
Christine Piatko?
Johns Hopkins University
Lori Levin||
Carnegie Mellon University
Scott Miller#
BBN Technologies
? U.S. Department of Defense, 9800 Savage Rd., Suite 6811, Fort Meade, MD 20755.
E-mail: kathrynlb@gmail.com.
?? Center for Advanced Study of Language, University of Maryland, 7005 52nd Avenue, College Park, MD
20742. E-mail: meb@umd.edu.
? Department of Computer Science and UMIACS, University of Maryland, AV Williams Building 3153,
College Park, MD 20742. E-mail: bonnie@umiacs.umd.edu.
? Center for Language and Speech Processing, Johns Hopkins University, 3400 N. Charles Street,
Hackerman Hall 320, Baltimore MD 21218. E-mail: {ccb,nwf}@cs.jhu.edu.
? Applied Physics Laboratory, Johns Hopkins University, 11000 Johns Hopkins Rd., Laurel, MD 20723.
E-mail: christine.piatko@jhuapl.edu.
|| Carnegie Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213.
E-mail: lsl@cs.cmu.edu.
# BNN Technologies, 10 Moulton Street, Cambridge, MA 02138. E-mail: smiller@bbn.com.
Submission received: 27 March 2011; revised submission received: 28 September 2011; accepted for
publication: 30 November 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 2
This article describes the resource- and system-building efforts of an 8-week Johns Hopkins
University Human Language Technology Center of Excellence Summer Camp for Applied Lan-
guage Exploration (SCALE-2009) on Semantically Informed Machine Translation (SIMT). We
describe a new modality/negation (MN) annotation scheme, the creation of a (publicly available)
MN lexicon, and two automated MN taggers that we built using the annotation scheme and
lexicon. Our annotation scheme isolates three components of modality and negation: a trigger
(a word that conveys modality or negation), a target (an action associated with modality or
negation), and a holder (an experiencer of modality). We describe how our MN lexicon was
semi-automatically produced and we demonstrate that a structure-based MN tagger results in
precision around 86% (depending on genre) for tagging of a standard LDC data set.
We apply our MN annotation scheme to statistical machine translation using a syntactic
framework that supports the inclusion of semantic annotations. Syntactic tags enriched with
semantic annotations are assigned to parse trees in the target-language training texts through
a process of tree grafting. Although the focus of our work is modality and negation, the tree
grafting procedure is general and supports other types of semantic information. We exploit this
capability by including named entities, produced by a pre-existing tagger, in addition to the MN
elements produced by the taggers described here. The resulting system significantly outperformed
a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the
NIST 2009 Urdu?English test set. This finding supports the hypothesis that both syntactic and
semantic information can improve translation quality.
1. Introduction
This article describes the resource- and system-building efforts of an 8-week Johns
Hopkins Human Language Technology Center of Excellence Summer Camp for Ap-
plied Language Exploration (SCALE-2009) on Semantically InformedMachine Translation
(SIMT) (Baker et al 2010a, 2010b, 2010c, 2010d). Specifically, we describe our modal-
ity/negation (MN) annotation scheme, a (publicly available) MN lexicon, and two
automated MN taggers that were built using the lexicon and annotation scheme.
Our annotation scheme isolates three components of modality and negation: a
trigger (a word that conveys modality or negation), a target (an action associated with
modality or negation), and a holder (an experiencer of modality). Two examples of MN
tagging are shown in Figure 1.
Note that modality and negation are unified into single MN tags (e.g., the ?Able?
modality tag is combined with ?NOT? to form the ?NOTAble? tag) and also that
Figure 1
Modality/negation tagging examples.
412
Baker et al Modality and Negation in SIMT
MN tags occur in pairs of triggers (e.g., TrigAble and TrigNegation) and targets (e.g.,
TargNOTAble).
We apply our modality and negation mechanism to the problem of Urdu?English
machine translation using a technique that we call tree grafting. This technique incorpo-
rates syntactic labels and semantic annotations in a unified and coherent framework for
implementing semantically informed machine translation. Our framework is not lim-
ited to the semantic annotations produced by the MN taggers that are the subject of this
article and we exploit this capability to additionally include named-entity annotations
produced by a pre-existing tagger. By augmenting hierarchical phrase-based translation
rules with syntactic labels that were extracted from a parsed parallel corpus, and further
augmenting the parse trees with markers for modality, negation, and entities (through
the tree grafting process), we produced a better model for translating Urdu and English.
The resulting system significantly outperformed the linguistically naive baseline Hiero
model, and reached the highest scores yet reported on the NIST 2009 Urdu?English
translation task.
We note that although our largest gains were from syntactic enrichments to the
model, smaller (but significant) gains were achieved by injecting semantic knowledge
into the syntactic paradigm. Verbal semantics (modality and negation) contributed
slightly more gains than nominal semantics (named entities) and their combined gains
were the sum of their individual contributions.
Of course, the limited semantic types we explored (modality, negation, and en-
tities) are only a small piece of the much larger semantic space, but demonstrating
success on these semantic aspects of language, the combination of which has been
unexplored by the statistical machine translation community, bodes well for (larger)
improvements based on the incorporation of other semantic aspects (e.g., relations and
temporal knowledge). Moreover, we believe this syntactic framework to be well suited
for further exploration of the impact of many different types of semantics on the quality
of machine-translation (MT) output. Indeed, it would not have been possible to initiate
the current study without the foundational work that gave rise to a syntactic paradigm
that could support these semantic enrichments.
In the SIMT paradigm, semantic elements (e.g., modality/negation) are identified
in the English portion of a parallel training corpus and projected to the source language
(in our case, Urdu) during a process of syntactic alignment. These semantic elements are
subsequently used in the translation rules that are extracted from the parallel corpus.
The goal of adding them to the translation rules is to constrain the space of possible
translations to more grammatical and more semantically coherent output. We explored
whether including such semantic elements could improve translation output in the face
of sparse training data and few source language annotations. Results were encouraging.
Translation quality, as measured by the Bleu metric (Papineni et al 2002), improved
when the training process for the Joshua machine translation system (Li et al 2009)
used in the SCALE workshop included MN annotation.
We were particularly interested in identifying modalities and negation because
they can be used to characterize events in a variety of automated analytic processes.
Modalities and negation can distinguish realized events from unrealized events, beliefs
from certainties, and can distinguish positive and negative instances of entities and
events. For example, the correct identification and retention of negation in a particular
language?such as a single instance of the word ?not??is very important for a correct
representation of events and likewise for translation.
The next two sections examine related work and the motivation behind the SIMT
approach. Section 4 defines the theoretical framework for ourMN lexicon and automatic
413
Computational Linguistics Volume 38, Number 2
MN taggers. Section 5 presents the MN annotation scheme used by our human annota-
tors and describes the creation of a MN lexicon based on this scheme. Section 6 presents
two types of MN taggers?one that is string-based and one that is structure-based?
and evaluates the effectiveness of the structure-based tagger. Section 7 then presents
implementation details of the semantically informed syntactic system and describes
the results of its application. Finally, Section 8 presents conclusions and future work.
2. Related Work
The development of annotation schemes has become an area of computational lin-
guistics development in its own right, often separate from machine learning applica-
tions. Many projects began as strictly linguistic projects that were later adapted for
computational linguistics. When an annotation scheme is consistent and well devel-
oped, its subsequent application to NLP systems is most effective. For example, the
syntactic annotation of parse trees in the Penn Treebank (Marcus, Marcinkiewicz, and
Santorini 1993) had a tremendous effect on parsing and onNatural Language Processing
in general.
In the case of semantic annotations, each tends to have its unique area of focus.
Although the labeling conventions may differ, a layer of modality annotation over
verb role annotation, for example, can have a complementary effect of providing more
information, rather than being viewed as a competing scheme. We review some of the
major semantic annotation efforts here.
Propbank (Palmer, Gildea, and Kingsbury 2005) is a set of annotations of predicate?
argument structure over parse trees. First annotated as an overlay to the Penn
Treebank, Propbank annotation now exists for other corpora. Propbank annotation aims
to answer the question Who did what to whom? for individual predicates. It is tightly
coupled with the behavior of individual verbs. FrameNet (Baker, Fillmore, and Lowe
1998), a frame-based lexical database that associates each word in the database with
a semantic frame and semantic roles, is also associated with annotations at the lexical
level.WordNet (Fellbaum 1998) is a verywidely used online lexical taxonomywhich has
been developed in numerous languages.WordNet nouns, verbs, adjectives, and adverbs
are organized into synonym sets. PropBank, FrameNet, and WordNet cover the word
senses and argument-taking properties of many modal predicates.
The Prague Dependency Treebank (Hajic? et al 2001; Bo?hmova?, Cinkova?, and
Hajic?ova? 2005) (PDT) is a multi-level system of annotation for texts in Czech and other
languages, with its roots in the Prague school of linguistics. Besides a morphological
layer and an analytical layer, there is a Tectogrammatical layer. The Tectogrammatical
layer includes functional relationships, dependency relations, and co-reference. The
PDT also integrates propositional and extra-propositional meanings in a single anno-
tation framework.
The Penn Discourse Treebank (PDTB) (Webber et al 2003; Prasad et al 2008)
annotates discourse connectives and their arguments over a portion of the Penn
Treebank. Within this framework, senses are annotated for the discourse connectives
in a hierarchical scheme. Relevant to the current work, one type of tag in the scheme is
the Conditional tag, which includes hypothetical, general, unreal present, unreal past,
factual present, and factual past arguments.
The PDTB work is related to that of Wiebe, Wilson, and Cardie (2005) for estab-
lishing the importance of attributing a belief or assertion expressed in text to its agent
(equivalent to the notion of holder in our scheme). The annotation scheme is designed to
capture the expression of opinions and emotions. In the PDTB, each discourse relation
414
Baker et al Modality and Negation in SIMT
and its two arguments are annotated for attribution. The attribute features are the
Source or agent, the Type (assertion propositions, belief propositions, facts, and eventu-
alities), scopal polarity, and determinacy. Scopal polarity is annotated on relations and
their arguments to identify cases when verbs of attribution are negated on the surface
but the negation takes scope over the embedded clause. An example is the sentence
?Having the dividend increases is a supportive element in the market outlook but I don?t
think it?s a main consideration.? Here, the second argument (the clause following but) is
annotated with a ?Neg? marker, meaning ?I think it?s not a main consideration.?
Wilson, Wiebe, and Hoffman (2009) describe the importance of correctly inter-
preting polarity in the context of sentiment analysis, which is the task of identifying
positive and negative opinions, emotions, and evaluations. The authors have estab-
lished a set of features to distinguish between positive and negative polarity and discuss
the importance of correctly analyzing the scope of the negation and the modality (e.g.,
whether the proposition is asserted to be real or not real).
A major annotation effort for temporal and event expressions is the TimeML spec-
ification language, which has been developed in the context of reasoning for question
answering (Saur??, Verhagen, and Pustejovsky 2006). TimeML, which includes modality
annotation on events, is the basis for creating the TimeBank and FactBank corpora
(Pustejovsky et al 2006; Saur?? and Pustejovsky 2009). In FactBank, event mentions are
marked with their degree of factuality.
Recent work incorporating modality annotation includes work on detecting cer-
tainty and uncertainty. Rubin (2007) describes a scheme for five levels of certainty,
referred to as Epistemic modality, in news texts. Annotators identify explicit certainty
markers and also take into account Perspective, Focus, and Time. Focus separates
certainty into facts and opinions, to include attitudes. In our scheme, Focus would be
covered by want and belief modality. Also, separating focus and uncertainty can allow
the annotation of both on one trigger word. Prabhakaran, Rambow, and Diab (2010)
describe a scheme for automatic committed belief tagging. Committed belief indicates
the writer believes the proposition. The authors use a previously annotated corpus of
committed belief, non-committed belief, and not applicable (Diab et al 2009), and derive
features for machine learning from parse trees. The authors desire to combine their
work with FactBank annotation.
The CoNLL-2010 shared task (Farkas et al 2010) was about the detection of cues
for uncertainty and their scope. The task was described as ?hedge detection,? that is,
finding statements which do not or cannot be backed up with facts. Auxiliary verbs
such as may, might, can, and so forth, are one type of hedge cue. The training data for
the shared task included the BioScope corpus (Szarvas et al 2008), which is manually
annotated with negation and speculation cues and their scope, and paragraphs from
Wikipedia possibly containing hedge information. Our scheme also identifies cues in
the form of triggers, but our desired outcome is to cover the full range of modalities
and not just certainty and uncertainty. To identify scope, we use syntactic parse trees,
as was allowed in the CoNLL task.
The textual entailment literature includes modality annotation schemes. Identifying
modalities is important to determine whether a text entails a hypothesis. Bar-Haim et al
(2007) include polarity based rules and negation and modality annotation rules. The
polarity rules are based on an independent polarity lexicon (Nairn, Condorovdi, and
Karttunen 2006). The annotation rules for negation andmodality of predicates are based
on identifying modal verbs, as well as conditional sentences and modal adverbials.
The authors read the modality off parse trees directly using simple structural rules for
modifiers.
415
Computational Linguistics Volume 38, Number 2
Earlier work describing the difficulty of correctly translating modality using ma-
chine translation includes Sigurd and Gawro?nska (1994) andMurata et al (2005). Sigurd
and Gawro?nska (1994) write about rule based frameworks and how using alternate
grammatical constructions such as the passive can improve the rendering of the modal
in the target language. Murata et al (2005) analyze the translation of Japanese into
English by several systems, showing they often render the present incorrectly as the
progressive. The authors trained a support vector machine to specifically handle modal
constructions, whereas our modal annotation approach is a part of a full translation
system.
We now consider other literature, relating to tree-grafting and machine translation.
Our tree-grafting approach builds on a technique used for tree augmentation in Miller
et al (2000), where parse-tree nodes are augmented with semantic categories. In that
earlier work, tree nodes were augmented with relations, whereas we augmented tree
nodes with modality and negation. The parser is subsequently retrained for both
semantic and syntactic processing. The semantic annotations were done manually by
students who were provided a set of guidelines and then merged with the syntactic
trees automatically. In our work we tagged our corpus with entities, modality, and
negation automatically and then grafted them onto the syntactic trees automatically,
for the purpose of training a statistical machine translation system. An added benefit of
the extracted translation rules is that they are capable of producing semantically tagged
Urdu parses, despite the fact that the training data were processed by only an English
parser and tagger.
Related work in syntax-based MT includes that of Huang and Knight (2006), where
a series of syntax rules are applied to a source language string to produce a target
language phrase structure tree. The Penn English Treebank (Marcus, Marcinkiewicz,
and Santorini 1993) is used as the source for the syntactic labels and syntax trees are
relabeled to improve translation quality. In this work, node-internal and node-external
information is used to relabel nodes, similar to earlier work where structural context
was used to relabel nodes in the parsing domain (Klein and Manning 2003). Klein
and Manning?s methods include lexicalizing determiners and percent markers, making
more fine-grained verb phrase (VP) categories, and marking the properties of sister
nodes on nodes. All of these labels are derivable from the trees themselves and not
from an auxiliary source. Wang et al (2010) use this type of node splitting in machine
translation and report a small increase in BLEU score.
We use the methods described in Zollmann and Venugopal (2006) and Venugopal,
Zollmann, and Vogel (2007) to induce synchronous grammar rules, a process which
requires phrase alignments and syntactic parse trees. Venugopal, Zollmann, and Vogel
(2007) use generic non-terminal category symbols, as in Chiang (2005), as well as gram-
matical categories from the Stanford parser (Klein and Manning 2003). Their method
for rule induction generalizes to any set of non-terminals. We further refine this process
by adding semantic notations onto the syntactic non-terminals produced by a Penn
Treebank trained parser, thus making the categories more informative.
In the parsing domain, the work of Petrov and Klein (2007) is related to the current
work. In their work, rule splitting and rule merging are applied to refine parse trees
during machine learning. Hierarchical splitting leads to the creation of learned cate-
gories that have linguistic relevance, such as a breakdown of a determiner category into
two subcategories of determiners by number, that is, this and that group together as do
some and these. We augment parse trees by category insertion in cases where a semantic
category is inserted as a node in a parse tree, after the English side of the corpus has
been parsed by a statistical parser.
416
Baker et al Modality and Negation in SIMT
3. SIMTMotivation
As in many of the frameworks described herein, the aim of the SIMT effort was to
provide a generalized framework for representing structured semantic information,
such as modality and negation. Unlike many of the previous semantic annotation efforts
(where the emphasis tends to be on English), however, our approach is designed to
be directly integrated into a translation engine, with the goal of translating highly
divergent language pairs, such as Urdu and English. As such, our choice of annotation
scheme?illustrated in the trigger-target example shown in Figure 1?was based on a
simplified structural representation that is general enough to accommodate divergent
modality/negation phenomena, easy for language experts to follow, and straightfor-
ward to integrate into a tree-grafting mechanism for MT. Our objective is to investigate
whether incorporating this sort of information into machine translation systems could
produce better translations, particularly in settings where only small parallel corpora
are available.
It is informative to look at an example translation to understand the challenges of
translating important semantic elements when working with a low-resource language
pair. Figure 2 shows an example taken from the 2008 NIST Urdu?English translation
task, and illustrates the translation quality of a state-of-the-art Urdu?English system
(prior to the SIMT effort). The small amount of training data for this language pair (see
Figure 2
An example of Urdu?English translation. Shown are an Urdu source document, a reference
translation produced by a professional human translator, and MT output from a phrase-based
model (Moses) without linguistic information, which is representative of state-of-the-art MT
quality before the SIMT effort.
417
Computational Linguistics Volume 38, Number 2
Table 1
The size of the various data sets used for the experiments in this article including the training,
development (dev), incremental test set (devtest), and blind test set (test). The dev/devtest was a
split of the NIST08 Urdu?English test set, and the blind test set was NIST09.
Urdu English
set lines tokens types tokens types
training 202k 1.7M 56k 1.7M 51k
dev 981 21k 4k 19k 4k
devtest 883 22k 4k 19?20k 4k
test 1,792 42k 6k 38?41k 5k
Table 1) results in significantly degraded translation quality compared, for example, to
an Arabic?English system that has more than 100 times the amount of training data.
The output in Figure 2 was produced using Moses (Koehn et al 2007), a state-of-
the-art phrase-based MT system that by default does not incorporate any linguistic
information (e.g., syntax or morphology or transliteration knowledge). As a result,
words that were not directly observed in the bilingual training data were untranslatable.
Names, in particular, are problematic. For example, the lack of translation for Nagaland
and Nagas induces multiple omissions throughout the translated text, thus producing
several instances where the holder of a claim (or belief ) is missing. This is because out-of-
vocabulary words are deleted from the Moses output.
We use syntactic and semantic tags as higher-order symbols inside the translation
rules used by the translation models. Generic symbols in translation rules (i.e., the
non-terminal symbol ?X?) were replaced with structured information at multiple levels
of abstraction, using a tree-grafting approach that we describe subsequently. Figure 3
Figure 3
The evolution of a semantically informed approach to our synchronous context-free grammars.
At the start of the 8 weeks the decoder used translation rules with a single generic non-terminal
symbol. Later syntactic categories were used, and by the end of the workshop the translation
rules included semantic elements such as modalities and negation, as well as named entities.
418
Baker et al Modality and Negation in SIMT
illustrates the evolution of the translation rules that we used, first replacing ?X? with
grammatical categories and then with categories corresponding to semantic units.
The semantic units that we examined in this effort weremodalities and negation (in-
dications that a statement represents something that has/hasn?t taken place or is/isn?t
a belief or an intention) and named entities (such as people or organizations). Other
semantic units, such as relations between entities and events, were not part of this effort
but we believe they could be similarly incorporated into the framework. We chose to
examine semantic units that canonically exhibit two different syntactic types: verbal, in
the case of modality and negation, and nominal, in the case of named entities.
Although used in this effort, named entities were not the focus of our research
efforts in SIMT. Rather, we focused on the development of an annotation scheme
for modality and negation and its use in MT, while relying on a pre-existing hidden
Markov model (HMM)-based tagger derived from Identifinder (Bikel, Schwartz, and
Weischedel 1999) to produce entity tags. Thus, the remainder of this article will focus
on our MN annotation scheme, two MN taggers produced by the effort, and on the
integration of semantics in the SIMT paradigm.
4. Modality and Negation
Modality is an extra-propositional component of meaning. In John may go to NY, the
basic proposition is John go to NY and the word may indicates modality and is called the
trigger in our work. van der Auwera and Amman (2005) define core cases of modality:
John must go to NY (epistemic necessity), John might go to NY (epistemic possibility),
John has to leave NY now (deontic necessity), and John may leave NY now (deontic pos-
sibility). Larreya (2009) defines the core cases slightly differently as root and epistemic.
Root modality in Larreya?s taxonomy includes physical modality (He had to stop. The
road was blocked) and deontic modality (You have to stop). Epistemic modality includes
problematic modality (You must be tired) and implicative modality (You have to be mad to
do that). Many semanticists (Kratzer 1991, von Fintel and Iatridou 2006) define modality
as quantification over possible worlds. John might leave NY means that there exist some
possible worlds in which John leaves NY. Another view of modality relates more to a
speaker?s attitude toward a proposition (McShane, Nirenburg, and Zacharski).
We incorporate negation as an inextricably intertwined component of modality,
using the term ?modality/negation (MN)? to refer to our resources (lexicons) and
processes (taggers). We adopt the view that modality includes several types of attitudes
that a speaker might have (or not have) toward an event or state. From the point of
view of the reader or listener, modality might indicate factivity, evidentiality, or senti-
ment. Factivity is related to whether an event, state, or proposition happened or didn?t
happen. It distinguishes things that happened from things that are desired, planned,
or probable. Evidentiality deals with the source of information and may provide clues
to the reliability of the information. Did the speaker have first-hand knowledge of
what he or she is reporting, or was it hearsay or inferred from indirect evidence?
Sentiment deals with a speaker?s positive or negative feelings toward an event, state,
or proposition.
Our project was limited to modal words and phrases?and their negations?that
are related to factivity. Beyond the core cases of modality, however, we include some
aspects of speaker attitude such as intent and desire. We included these because they
are often not separable from the core cases of modality. For example, He had to go may
include the ideas that someone wanted him to go, that he might not have wanted to go,
419
Computational Linguistics Volume 38, Number 2
that at some point after coercion he intended to go, and that at some point he was able
to go (Larreya 2009).
Our focus was on the eight modalities in Figure 4, where P is a proposition (the
target of the triggering modality) and H is the holder (experiencer or cognizer of the
modality). Some of the eight factivity-related modalities may overlap with sentiment
or evidentiality. For example, want indicates that the proposition it scopes over may
not be a fact (it may just be desired), but it also expresses positive sentiment toward
the proposition it scopes over. We assume that sentiment and evidentiality are covered
under separate coding schemes, and that words like want would have two tags, one for
sentiment and one for factivity.
5. The Modality/Negation Annotation Scheme
The challenge of creating an MN annotation scheme was to deal with the complex
scoping ofmodalities with each other andwith negation, while at the same time creating
a simplified operational procedure that could be followed by language experts without
special training. Here we describe our MN annotation framework, including a set
of linguistic simplifications, and then we present our methodology for creation of a
publicly available MN lexicon. The modality annotation scheme is fully documented in
a set of guidelines that were written with English example sentences (Baker et al 2010c).
The guidelines can be used to derive hand-tagged evaluation data for English and they
also include a section that contains a set of Urdu trigger-word examples.
During the SCALE workshop, some Urdu speakers used the guidelines to annotate
a small corpus of Urdu by hand, which we reserved for future work. The Urdu corpus
could be useful as an evaluation corpus for automatically tagged Urdu, such as one
derived from rule projection in the Urdu?English MT system, a method we describe
further in Section 7. Also, although we did not annotate a very large Urdu corpus, more
data could be manually annotated to train an automatic Urdu tagger in the future.
5.1 Anatomy of Modality/Negation in Sentences
In sentences that express modality, we identify three components: a trigger, a target, and
a holder. The trigger is the word or string of words that expresses modality or negation.
The target is the event, state, or relation over which the modality scopes. The holder is
Figure 4
Eight modalities used for tagging. H = the holder of the modality; P = the proposition over
which the modality has scope.
420
Baker et al Modality and Negation in SIMT
the experiencer or cognizer of themodality. The trigger can be a word such as should, try,
able, likely, or want. It can also be a negative element such as not or n?t. Often, modality
or negation is expressed without a lexical trigger. For a typical declarative sentence
(e.g., John went to NY), the default modality is strong belief when no lexical trigger is
present. Modality can also be expressed constructionally. For example, Requirement can
be expressed in Urdu with a dative subject and infinitive verb followed by a verb that
means to happen or befall.
5.2 Linguistic Simplifications for Efficient Operationalization
Six linguistic simplifications were made for the sake of efficient operationalization of
the annotation task. The first linguistic simplification deals with the scope of modality
and negation. The first given sentence indicates scope of modality over negation. The
second sentence indicates scope of negation over modality:
 He tried not to criticize the president.
 He didn?t try to criticize the president.
The interaction of modality with negation is complex, but was operationalized eas-
ily in the menu of 13 choices shown in Figure 5. First consider the case where negation
scopes over modality. Four of the 13 choices are composites of negation scoping over
modality. For example, the annotators can choose try or not try as two separate modali-
ties. Five modalities (Require, Permit, Want, Firmly Believe, and Believe) do not have a
negated form. For three of these modalities (Want, Firmly Believe, and Believe), this is
because they are often transparent to negation. For example, I do not believe that he left NY
sometimes means the same as I believe he didn?t leave NY. Merging the two is obviously
a simplification, but it saves the annotators from having to make a difficult decision.
Figure 5
Thirteen menu choices for Modality/Negation annotation. H = the holder of the modality;
P = the proposition over which the modality has scope.
421
Computational Linguistics Volume 38, Number 2
The second linguistic simplification is related to a duality in meaning between
require and permit. Not requiring P to be true is similar in meaning to permitting P to
be false. Thus, annotators were instructed to label not require P to be true as Permit P to be
false. Conversely, not Permit P to be truewas labeled as Require P to be false.
After the annotator chooses the modality, the scoping of modality over negation
takes place as a second decision. For example, for the sentence John tried not to go to NY,
the annotator first identifies go as the target of a modality and then chooses try as the
modality. Finally, the annotator chooses false as the polarity of the target.
The third simplification relates to entailments between modalities. Many words
have complex meanings that include components of more than one modality. For ex-
ample, if one managed to do something, one tried to do it and one probably wanted to
do it. Thus, annotators were provided a specificity-ordered modality list as in Figure 5,
andwere asked to choose the first applicable modality. We note that this list corresponds
to two independent ?entailment groupings,? ordered by specificity:
 {requires ? permits}
 {succeeds ? tries ? intends ? is able ? wants}
Inside the entailment groupings, the ordering corresponds to an entailment relation:
For example, succeeds can only occur if tries has occurred. Also, the {requires ? . . . }
entailment grouping is taken to be more specific than (ordered before) the {succeeds ?
. . . } entailment grouping. Moreover, both entailment groupings are taken to be more
specific than believes, which is not in an entailment relation with any of the other
modalities.
The fourth simplification, already mentioned, is that sentences without an overt
trigger word are tagged as firmly believes. This heuristic works reasonably well for the
types of documents we were working with, although one could imagine genres such
as fiction in which many sentences take place in an alternate possible world (imagined,
conditional, or counterfactual) without explicit marking.
The fifth linguistic simplification is that we did not require annotators to mark
nested modalities. For a sentence like He might be able to go to NY the target word go
is marked as ability, but might is not annotated for Belief modality. This decision was
based on time limits on the annotation task; there was not enough time for annotators
to deal with syntactic scoping of modalities over other modalities.
Finally, we did not mark the holder H because of the short time frame for workshop
preparation. We felt that identifying the triggers and targets would be most beneficial
in the context of machine translation.
5.3 The English Modality/Negation Lexicon
Using the given framework, we created an MN lexicon that was incorporated into an
MN tagging scheme to be described in Section 6. Entries in the MN lexicon consist of:
(1) A string of one or more words: for example, should or have need of . (2) A part of
speech for each word: The part of speech helps us avoid irrelevant homophones such as
the noun can. (3) An MN designator: one of the 13 modality/negation cases described
previously. (4) A head word (or trigger): the primary phrasal constituent to cover
cases where an entry is a multi-word unit (e.g., the word hope in hope for). (5) One or
more subcategorization codes derived from the Longman Dictionary of Contemporary
English (LDOCE).
422
Baker et al Modality and Negation in SIMT
We produced the full English MN lexicon semi-automatically. First, we gathered a
small seed list of MN trigger words and phrases from our modality annotation manual
(Baker et al 2010c). Then, we expanded this small list of MN trigger words by running
an on-line search for each of the words, specifically targeting free on-line thesauri (e.g.,
thesaurus.com), to find both synonymous and antonymous words. From these we
manually selected the words we thought triggered modality (or their corresponding
negative variants) and filtered out words that we thought didn?t trigger modality. The
resulting list of MN trigger words and phrases contained about 150 lemmas.
We note that most intransitive (LDOCE) codes were not applicable to modality/
negation constructions. For example, hunger (in the Want modality class) has a modal
reading of ?desire? when combined with the preposition for (as in she hungered for a
promotion), but we do not consider it to be modal when it is used in the somewhat
archaic sentence He hungered, meaning that he did not have enough to eat. Thus the
LDOCE code I associated with the verb hungerwas hand-changed to I-FOR. There were
43 such cases. Once the LDOCE codes were hand-verified (and modified accordingly),
the mapping to subcategorization codes was applied.
The MN lexicon is publicly available at http://www.umiacs.umd.edu/?bonnie/
ModalityLexicon.txt. An example of an entry is given in Figure 6, for the verb need.
6. Automatic Modality/Negation Annotation
An MN tagger produces text or structured text in which modality or negation triggers
and/or targets are identified. Automatic identification of the holders of modalities was
beyond the scope of our project because the holder is often not explicitly stated in the
sentence in which the trigger and target occur. This section describes two types of MN
taggers?one that is string-based and one that is structure-based.
6.1 The String-Based English Modality/Negation Tagger
The string-based tagger operates on text that has been tagged with parts of speech
by a Collins-style statistical parser (Miller et al 1998). The tagger marks spans of
words/phrases that exactly match MN trigger words in the MN lexicon described
previously, and that exactly match the same parts of speech. This tagger identifies the
target of each modality/negation using the heuristic of tagging the next non-auxiliary
verb to the right of the trigger. Spans of words can be tagged multiple times with
different types of triggers and targets.
Figure 6
Modality lexicon entry for need.
423
Computational Linguistics Volume 38, Number 2
We found the string-based MN tagger to produce output that matched about 80%
of the sentence-level tags produced by our structure-based tagger, the results of which
are described next. Although string-based tagging is fast and reasonably accurate in
practice, we opted to focus on the indepth analysis of modality/negation of our SIMT
results using the more accurate structure-based tagger.
6.2 The Structure-Based English Modality/Negation Tagger
The structure-based MN tagger operates on text that has been parsed (Miller et al
1998). We used a version of the parser that produces flattened trees. In particular, the
flattener deletes VP nodes that are immediately dominated by VP or S and noun phrase
(NP) nodes that are immediately dominated by PP or NP. The parsed sentences are
processed by TSurgeon rules. Each TSurgeon rule consists of a pattern and an action.
The pattern matches part of a parse tree and the action alters the parse tree. More
specifically, the pattern finds an MN trigger word and its target and the action inserts
tags such as TrigRequire and TargRequire for triggers and targets for the modality
Require. Figure 7 shows output from the structure-based MN tagger. (Note that the
sentence is disfluent: Pakistan which could not reach semi-final, in a match against South
African team for the fifth position Pakistan defeated South Africa by 41 runs.) The example
shows that could is a trigger for the Ability modality and not is a trigger for negation.
Reach is a target for both Ability and Negation, which means that it is in the category of
?H is not able [to make P true/false]? in our coding scheme. Reach is also a trigger for
the Succeed modality and semi-final is its target.
The TSurgeon patterns are automatically generated from the verb class codes in
the MN lexicon along with a set of 15 templates. Each template covers one situation
such as the following: the target is the subject of the trigger; the target is the direct
object of the trigger; the target heads an infinitival complement of the trigger; the target
is a noun modified by an adjectival trigger, and so on. The verb class codes indicate
Figure 7
Sample output from the structure-based MN tagger.
424
Baker et al Modality and Negation in SIMT
which templates are applicable for each trigger word. For example, a trigger verb in the
transitive class may use two target templates, one in which the trigger is in active voice
and the target is a direct object (need tents) and one in which the trigger is in passive
voice and the target is a subject (tents are needed).
In developing the TSurgeon rules, we first conducted a corpus analysis for 40 of the
most common trigger words in order to identify and debug the most broadly applicable
templates. We then used LDOCE to assign verb classes to the remaining verbal triggers
in the MN lexicon, and we associated one or more debugged templates with each verb
class. In this way, the initial corpus work on a limited number of trigger words was
generalized to a longer list of trigger words. Because the TSurgeon patterns are tailored
to the flattened structures produced by our parser, it is not easily ported to new parser
outputs. The MN lexicon itself is portable, however. Switching parsers would entail
writing new TSurgeon templates, but the trigger words in the MN lexicon would still
be automatically assigned to templates based on their verb classes.
The following example shows an example of a TSurgeon pattern?action pair for a
sentence like They were required to provide tents. The pattern?action pair is intended to
be used after a pre-processing stage in which labels such as ?VoicePassive? and ?AUX?
have been assigned. ?VoicePassive? is inserted by a pre-processing TSurgeon pattern
because, in some cases, the target of a passive modality trigger word is in a different
location from the target of the corresponding active modality trigger word. ?AUX? is
inserted during pre-processing to distinguish auxiliary uses of have and be from their
uses as main verbs. The pattern portion of the pattern?action pair matches a node with
label VB that is not already tagged as a trigger and that is passive and dominates the
string ?required?. The VB node is also a sister to an S node, and the S node dominates a
VB that is not an auxiliary (provide in this case). The action portion of the pattern?action
pair inserts the string ?TargReq? as the second daughter of the second VB and inserts
the string ?TrigReq? as the second daughter of the first VB.
VB=trigger !< /^Trig/ < VoicePassive < required $..
(S < (VB=target !< AUX))
insert (TargReq) >2 target
insert (TrigReq) >2 trigger
Verb-specific patterns such as this one were generalized in order to gain coverage of
the whole modality lexicon. The specific lexical item, required, was replaced with a vari-
able, as were the labels ?TrigReq? and ?TargReq.? The pattern was then given a name,
V3-passive-basic, where V3 is a verb class tag from LDOCE (described in Section 5.3)
for verbs that take infinitive complements. We then looked up the LDOCE verb class
labels for all of the verbs in the modality lexicon. Using this information, we could then
generate a set of new, verb-specific patterns for each V3 verb in the modality lexicon.
6.3 Evaluating the Effectiveness of Structure-Based MN Tagging
We performed amanual inspection of the structure-based tagging output. We calculated
precision by examining 229 instances of modality triggers that were tagged by our
tagger from the English side of the NIST 09 MTEval training sentences. We analyzed
precision in two steps, first checking for the correct syntactic position of the target and
then checking the semantic correctness of the trigger and target. For 192 of the 229
triggers (around 84%), the targets were tagged in the correct syntactic location.
For example, for the sentence A solution must be found to this problem shown in
Figure 8, the word must is a modality trigger word, and the correct target is the first
425
Computational Linguistics Volume 38, Number 2
Figure 8
Example of embedded target head found inside VP must be found.
non-auxiliary verb heading a verb phrase that is contained in the syntactic complement
of must. The syntactic complement of must is the verb phrase be found to this problem.
The syntactic head of that verb phrase, be, is skipped because it is an auxiliary verb. The
correct (embedded) target found is the head of the syntactic complement of be.
The 192 modality instances with structurally correct targets do not all have seman-
tically correct tags. In this example, must is tagged as TrigBelief, where the correct tag
would be TrigRequire. Also, because theMN lexiconwas usedwithout respect to word
sense, words were sometimes erroneously identified as triggers. This includes non-
modal uses of work (work with refugees), reach (reach a destination), and attack (attack
a physical object), in constrast to modal uses of these words: work for peace (effort), reach
a goal (succeed), and attack a problem (effort). Fully correct tagging of modality would
need to include word sense disambiguation.
For 37 of the 229 triggers we examined, a target was not tagged in the correct syn-
tactic position. In 12 of 37 incorrectly tagged instances the targets are inside compound
nouns or coordinate structures (NP or VP), which are not yet handled by the modality
tagger. The remaining 25 of the 37 incorrectly tagged instances had targets that were lost
because the tagger does not yet handle all cases of nested modalities. Nested modalities
occur in sentences like They did not want to succeed in winning where the target words
want and succeed are also modality trigger words. Proper treatment of nested modalities
requires consideration of scope and compositional semantics.
Nesting was treated in two steps. First, the modality tagger marked each word as a
trigger and/or target. In They did not want to succeed in winning, not is marked as a trigger
for negation, want is marked as a target of negation and a trigger of wanting, succeed is
marked as a trigger of succeeding and a target of wanting, and win is marked as a target
of succeeding. The second step in the treatment of nested modalities occurs during tree
grafting, where the meanings of the nested modalities are composed. The tree grafting
program correctly composes some cases of nested modalities. For example, the tag
TrigAble composed with TrigNegation results in the target tag TargNOTAble, as shown
in Figure 9. In other cases, where compositional semantics are not yet accommodated,
the tree grafting program removed target labels from the trees, and those cases were
counted as incorrect for the purpose of this evaluation.
Figure 9
Example of modality composed with negation: TrigAble and TrigNegation combine to form
NOTAble.
426
Baker et al Modality and Negation in SIMT
In the 229 instances that we examined, there were 14 in which a light verb or noun
was the correct syntactic target, but not the correct semantic target. Decision would be
a better target than taken in The decision should be taken on delayed cases on the basis of
merit.We counted sentences with semantically light targets as correct in our evaluation
because our goal was to identify the syntactic head of the target. The semantics of the
target is a general issue, and we often find lexico-syntactic fluff between the trigger and
the most semantically salient target in sentences likeWe succeeded in our goal of winning
the war where ?success in war? is the salient meaning.
With respect to recall, the tagger primarily missed special forms of negation in
noun phrases and prepositional phrases: There was no place to seek shelter; The buildings
should be reconstructed, not with RCC, but with the wood and steel sheets. More complex
constructional and phrasal triggers were also missed: President Pervaiz Musharraf has said
that he will not rest unless the process of rehabilitation is completed. Finally, we discovered
some omissions from our MN lexicon: It is not possible in the middle of winter to re-open
the roads. Further annotation experiments are planned, which will be analyzed to close
such gaps and update the lexicon as appropriate.
Providing a quantitative measure of recall was beyond the scope of this project.
At best we could count instances of sentences containing trigger words that were not
tagged. We are also aware of many cases of modality that were not covered such as
the modal uses of the future tense auxiliary will as in That?ll be John (conjecture), I?ll do
the dishes (volition), He won?t do it (non-volition), and It will accommodate five (ability)
(Larreya 2009). Because of the complexity and subtlety of modality and negation, how-
ever, it would be impractical to count every clause (such as the not rest unless clause
above) that had a nuance of non-factivity.
7. Semantically Informed Syntactic MT
This section describes the incorporation of our structured-based MN tagging into an
Urdu?English machine-translation system using tree grafting for combining syntactic
symbols with semantic categories (e.g., modality/negation). We note that a de facto
Urdu MN tagger resulted from identifying the English MN trigger and target words in
a parallel English?Urdu corpus, and then projecting the trigger and target labels to the
corresponding words in Urdu syntax trees.
7.1 Refinement of Translation Grammars with Semantic Categories
We used synchronous context-free grammars (SCFGs) as the underlying formalism
for our statistical models of translation. SCFGs provide a convenient and theoretically
grounded way of incorporating linguistic information into statistical models of transla-
tion, by specifying grammar rules with syntactic non-terminals in the source and target
languages. We refine the set of non-terminal symbols so that they not only include
syntactic categories, but also semantic categories.
Chiang (2005) re-popularized the use of SCFGs for machine translation, with the
introduction of his hierarchical phrase-based machine translation system, Hiero. Hiero
uses grammars with a single non-terminal symbol ?X? rather than using linguistically
informed non-terminal symbols. When moving to linguistic grammars, we use Syntax
Augmented Machine Translation (SAMT) developed by Venugopal, Zollmann, and
Vogel (2007). In SAMT the ?X? symbols in translation grammars are replaced with
nonterminal categories derived from parse trees that label the English side of the
427
Computational Linguistics Volume 38, Number 2
Figure 10
A sentence on the English side of the bilingual parallel training corpus is parsed with a
syntactic parser, and also tagged with our modality tagger. The tags are then grafted onto
the syntactic parse tree to form new categories like VP-TargNOTAble and VP-TargRequire.
Grafting happens prior to extracting translation rules, which happens normally except for
the use of the augmented trees.
Urdu?English parallel corpus.1 We refine the syntactic categories by combining them
with semantic categories. Recall that this progression was illustrated in Figure 3.
We extracted SCFG grammar rules containing modality, negation, and named enti-
ties using an extraction procedure that requires parse trees for one side of the parallel
corpus. Although it is assumed that these trees are labeled and bracketed in a syntac-
tically motivated fashion, the framework places no specific requirement on the label
inventory. We take advantage of this characteristic by providing the rule extraction
algorithm with augmented parse trees containing syntactic labels that have semantic
annotations grafted onto them so that they additionally express semantic information.
Our strategy for producing semantically grafted parse trees involves three steps:
1. The English sentences in the parallel training data are parsed with a
syntactic parser. In our work, we used the lexicalized probabilistic context
free grammar parser provided by Basis Technology Corporation.
2. The English sentences are MN-tagged by the system described herein and
named-entity-tagged by the Phoenix tagger (Richman and Schone 2008).
3. The modality/negation and entity markers are grafted onto the syntactic
parse trees using a tree-grafting procedure. The grafting procedure was
implemented as part of the SIMT effort. Details are further spelled out in
Section 7.2.
Figure 10 illustrates how modality tags are grafted onto a parse tree. Note that
although we focus the discussion here on the modality and negation, our framework
is general and we were able to incorporate other semantic elements (specifically, named
entities) into the SIMT effort.
Once the semantically grafted trees have been produced for the parallel corpus, the
trees are presented, along with word alignments (produced by the Berkeley aligner),
to the rule extraction software to extract synchronous grammar rules that are both
1 For non-constituent phrases, composite CCG-style categories are used (Steedman 1999).
428
Baker et al Modality and Negation in SIMT
syntactically and semantically informed. These grammar rules are used by the decoder
to produce translations. In our experiments, we used the Joshua decoder (Li et al 2009),
the SAMT grammar extraction software (Venugopal and Zollmann 2009), and special
purpose-built tree-grafting software.
Figure 11 shows example semantic rules that are used by the decoder. The verb
phrase rules are augmented with modality and negation, taken from the semantic
categories listed in Table 2. Because these get marked on the Urdu source as well as
the English translation, semantically enriched grammars also act as very simple named
entity or MN taggers for Urdu. Only entities, modality, and negation that occurred in
the parallel training corpus are marked in the output, however.
7.2 Tree-Grafting Algorithm
The overall scheme of our tree-grafting algorithm is to match semantic tags to syntactic
categories. There are two inputs to the process. Each is derived from a common text
file of sentences. The first input is a list of standoff annotations for the semantically
tagged word sequences in the input sentences, indexed by sentence number. The second
is a list of parse trees for the sentences in Penn Treebank format, indexed by sentence
number.
Table 2 lists the modality/negation types that were produced by the MN tagger. For
example, the sentence The students are able to swim is tagged as The students are ?TrigAble?
to ?TargAble swim?. The distinction between ?Negation? and ?NOT? corresponds to the
difference between negation that is inherently expressed in the triggering lexical item
and negation that is expressed explicitly as a separate lexical item. Thus, I achieved
my goal is tagged ?Succeed? and I did not achieve my goal is tagged as ?NOTSucceed,?
Figure 11
Example translation rules with tags for modality, negation, and entities combined with
syntactic categories.
429
Computational Linguistics Volume 38, Number 2
Table 2
Modality tags with their negated versions. Note that Require and Permit are in a dual relation,
and thus RequireNegation is represented as NOTPermit and PermitNegation is represented
as NOTRequire.
Require NOTRequire
Permit NOTPermit
Succeed NOTSucceed
SucceedNegation NOTSucceedNegation
Effort NOTEffort
EffortNegation NOTEffortNegation
Intend NOTIntend
IntendNegation NOTIntendNegation
Able NOTAble
AbleNegation NOTAbleNegation
Want NOTWant
WantNegation NOTWantNegation
Belief NOTBelief
BeliefNegation NOTBeliefNegation
Firm Belief NOTFirm Belief
Firm BeliefNegation NOTFirm BeliefNegation
Negation
but I failed to win is tagged as ?SucceedNegation,? and I did not fail to win is tagged as
?NOTSucceedNegation.?
The tree-grafting algorithm proceeds as follows. For each tagged sentence, we
iterate over the list of semantic tags. For each semantic tag, there is an associated word
or sequence of words. For example, the modality tag TargAble may tag the word swim.
For each semantically tagged word, we find the parent node in the correspond-
ing syntactic parse tree that dominates that word. For a word sequence, we find and
compare the parent nodes for all of the words. Each node in the syntax tree has a
category label. The following tests are then made and tree grafts applied:
 If there is a single node in the parse tree that dominates all and only the
words with the semantic tag, graft the name of the semantic tag onto
the highest corresponding syntactic constituent in the tree. For example,
in Figure 10, which shows the grafting process for modality tagging,
the semantic tag TargNOTAble that ?hand over? receives is grafted onto
the VB node that dominates all and only the words ?hand over.? Then the
semantic tag TargNOTAble is passed up the tree to the VP node, which is
the highest corresponding syntactic constituent.
 If the semantic tag corresponds to words that are adjacent daughters in
a syntactic constituent, but less than the full constituent, insert a node
dominating those words into the parse tree, as a daughter of the original
syntactic constituent. The name of the semantic tag is grafted onto the new
node and becomes its category label. This is a case of tree augmentation by
node insertion.
 If a syntactic constituent selected for grafting has already been labeled
with a semantic tag, overlay the previous tag with the current tag. We
chose to tag in this manner simply because our system was not set up to
handle the grafting of multiple tags onto a single constituent. An example
430
Baker et al Modality and Negation in SIMT
of this occurs in the sentence ?The Muslims had obtained Pakistan.? If the
NP node dominating Pakistan is grafted with a named entity tag such as
NP-GPE, we overlay this with the NP-TargSucceed tag in a modality
tagging scheme.
 In the case of a word sequence, if the words covered by the semantic tag
fall across two different syntactic constituents, do nothing. This is a case of
crossing brackets.
Our tree-grafting procedure was simplified to accept a single semantic tag per
syntactic tree node as the final result. The algorithm keeps the last tag seen as the tag of
precedence. In practice, we established a precedence ordering for modality/negation
tags over named entity tags by grafting named entity tags first and modality/negation
second. Our intuition was that, in case of a tie, finer-grained verbal categories would be
more helpful to parsing than finer-grained nominal categories.2 In cases where a word
was tagged both as a MN target and a MN trigger, we gave precedence to the target tag.
This is because, although MN targets vary, MN triggers are generally identifiable with
lexical items. Finally, we used the simplified specificity ordering of MN tags described
in Section 5.2 to ensure precedence of more specific tags over more general ones. Table 2
lists the modality/negation types from highest (Require modality) to lowest (Negation)
precedence.3
7.3 SIMT Results
We evaluated our tree grafting approach by performing a series of translation experi-
ments. Each version of our translation systemwas trained on the same bilingual training
data. The bilingual parallel corpus that we used was distributed as part of the 2008
NIST Open Machine Translation Evaluation Workshop.4 The training set contained
88,108 Urdu?English sentence pairs, and a bilingual dictionary with 113,911 entries.
For our development and test sets, we split the NIST MT-08 test set into two portions
(with each document going into either test or dev, and preserving the genre split).
Our test set contained 883 Urdu sentences, each with four translations into English,
and our dev set contained 981 Urdu sentences, each with four reference translations.
To extract a syntactically informed translation model, we parsed the English side of
the training corpus using a Penn Treebank?trained parser (Miller et al 1998). For the
experiments that involved grafting named entities onto the parse trees, we tagged
the English side of the training corpus with the Phoenix tagger (Richman and Schone
2008). We word-aligned the parallel corpus with the Berkeley aligner. All models used a
5-gram language model trained on the English Gigaword corpus (v5) using the SRILM
toolkit with modified KN smoothing. The Hiero translation grammar was extracted
using the Joshua toolkit (Li et al 2009). The other translation grammars were extracted
using the SAMT toolkit (Venugopal and Zollmann 2009).
2 In testing we found that grafting named entities first and MN last yielded a slightly higher BLEU score
than the reverse order.
3 Future work could include exploring additional methods of resolving tag conflicts or combining tag
types on single nodes, for example, by inserting multiple intermediate nodes (effectively using unary
rewrite rules) or by stringing tag names together.
4 http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/.
431
Computational Linguistics Volume 38, Number 2
Figure 12
Results for a range of experiments conducted during the SIMT effort show the score for our
top-performing baseline systems derived from a hierarchical phrase-based model (Hiero).
Substantial improvements obtained when syntax was introduced along with feature functions
(FFs) and further improvements resulted from the addition of semantic elements. The scores
are lowercased BLEU calculated on the held-out devtest set. NE = named entities.
Figure 12 gives the results for a number of experiments conducted during the SIMT
effort.5 The experiments are broken into three groups: baselines, syntax, and semantics.
To contextualize our results we experimented with a number of different baselines
that were composed from two different approaches to statistical machine translation?
phrase-based and hierarchical phrase-based SMT?along with different combinations
of language model sizes and word aligners. Our best-performing baseline was a Hiero
model. The Bleu score for this baseline on the development set was 22.9 Bleu points.
After experimenting with syntactically motivated grammar rules, we conducted
experiments on the effects of incorporating semantic elements (e.g., named entities and
modality/negation) into the translation grammars. In our devtest set our taggers tagged
on average 3.5 named entities per sentence and 0.35 MN markers per sentence. These
were included by grafting modality, negation, and named-entity markers onto the parse
trees. Individually, each of these made modest improvements over the syntactically
informed system alone. Grafting named entities onto the parse trees improved the Bleu
score by 0.2 points. Modality/negation improved it by 0.3 points. Doing both simulta-
neously had an additive effect and resulted in a 0.5 Bleu score improvement over syntax
alone. This improvement was the largest improvement that we got from anything other
than the move from linguistically naive models to syntactically informed models.
We used bootstrap resampling to test whether the differences in Bleu scores were
statistically significant (Koehn 2004). All of the results were a significant improvement
over Hiero (at p ? 0.01). The difference between the syntactic system and the syntactic
system with named entities is not significant (p = 0.38). The differences between the
5 These experiments were conducted on the devtest set, containing 883 Urdu sentences (21,623 Urdu
words) and four reference translations per sentence. The BLEU score for these experiments is measured
on uncased output.
432
Baker et al Modality and Negation in SIMT
syntactic system and the syntactic system with MN, and between the syntactic system
and the syntactic system with both MN and named entities were both significant at
(p ? 0.05).
Figure 13 shows example output from the final SIMT system in comparison to
the pre-SIMT results and the translation produced by a human (reference). An error
analysis of this example output illustrates that SIMT enhancements have resulted in the
elimination of misleading translation output in several cases:
1. pre-SIMT: China had the experience of Pakistan?s first nuclear bomb.
SIMT: China has the first nuclear bomb test.
reference: China has conducted the experiment of Pakistan?s first nuclear bomb.
2. pre-SIMT: the nuclear bomb in 1998 that Pakistan may experience
SIMT: the experience of the atom bomb Pakistan in May 1998
reference: the atom bomb, whose experiment was done in 1998
by Pakistan
3. pre-SIMT: He said that it is also present proof of that Dr. Abdul Qadeer
Khan after the Chinese design
SIMT: He said that there is evidence that Dr. Abdul Qadeer Khan has
also used the Chinese design
reference: He said that the proof to this also exists in that Dr. Abdul
Qadeer Khan used the Chinese design
The article in question pertains to claims by Thomas Reid that China allowed Pakistan
to detonate a nuclear weapon at its test site. In the first example, however, the reader is
potentially misled by the pre-SIMT output to believe that Pakistan launched a nuclear
bomb on China. The SIMT output leaves out the mention of Pakistan, but correctly con-
veys the firm belief that the bomb event is a test (closely resembling the term experiment
in the human reference), not a true bombing event. This is clearly an improvement over
the misleading pre-SIMT output.
In the second example, the pre-SIMT output misleads the reader to believe that
Pakistan is (or will be) attacked, through the use of the phrase may experience, where
may is poorly placed. (We note here that this is a date translation error, i.e., the month
of May should be next to the year 1998, further adding to the potential for confusion.)
Unfortunately, the SIMT output also uses the term experience (rather than experiment,
which is in the human reference), but in this case the month is correctly positioned in
the output, thus eliminating the potential for confusionwith respect to themodality. The
lack of a modal appropriately neutralizes the statement so that it refers to an abstract
event associated with the atom bomb, rather than an attack on the country.
In the third example, where the Chinese design used by Dr. Abdul Qandeer Khan is
argued to be proof of the nuclear testing relationship between Pakistan and China, the
first pre-SIMT output potentially leads the reader to believe that Dr. Abdul Qadeer is
after the Chinese design (not that he actually used it), whereas the SIMT output conveys
the firm belief that the Chinese design has been used by Dr. Abdul Qadeer. This output
very closely matches the human reference.
Note that even in the title of the article, the SIMT system produces much more
coherent English output than that of the linguistically naive system. The figure also
shows improvements due to transliteration, which are described in Irvine et al (2010).
The scores reported in Figure 12 do not include transliteration improvements.
433
Computational Linguistics Volume 38, Number 2
Figure 13
An example of the improvements to Urdu?English translation before and after the SIMT effort.
Output is from the baseline Hiero model, which does not use linguistic information, and from
the final model, which incorporates syntactic and semantic information.
434
Baker et al Modality and Negation in SIMT
8. Conclusions and Future Work
We developed a modality/negation lexicon and a set of automatic MN taggers, one of
which?the structure-based tagger?results in 86% precision for tagging of a standard
LDC data set. The MN tagger has been used to improve machine translation output
by imposing semantic constraints on possible translations in the face of sparse training
data. The tagger is also an important component of a language-understanding module
for a related project.
We have described a technique for translation that shows particular promise
for low-resource languages. We have integrated linguistic knowledge into statistical
machine translation in a unified and coherent framework. We demonstrated that
augmenting hierarchical phrase-based translation rules with semantic labels (through
?grafting?) resulted in a 0.5 Bleu score improvement over syntax alone.
Although our largest gains were from syntactic enrichments to the Hiero model,
demonstrating success on the integration of semantic aspects of language bodes well
for additional improvements based on the incorporation of other semantic aspects. For
example, we hypothesize that incorporating relations and temporal knowledge into
the translation rules would further improve translation quality. The syntactic grafting
framework is well-suited to support the exploration of the impact of many different
types of semantics on MT quality, though in this article we focused on exploring the
impact of modality and negation.
An important future study is one that focuses on demonstrating whether further
improvements in modality/negation identification are likely to lead to further gains in
translation performance. Such a study would benefit from the inclusion of a more de-
tailed manual evaluation to determine if modality and negation is adequately conveyed
in the downstream translations. This work would be additionally enhanced through
experimentation on other language pair(s) and larger corpora.
The work presented here represents the first small steps toward a full integration
of MT and semantics. Efforts underway in DARPA?s GALE program demonstrated the
potential for combining MT and semantics (termed distillation) to answer the informa-
tion needs of monolingual speakers using multilingual sources. Proper recognition of
modalities and negation is crucial for handling those information needs effectively.
In previous work, however, semantic processing proceeded largely independently of
the MT system, operating only on the translated output. Our approach is significantly
different in that it combines syntax, semantics, and MT into a single model, offering
the potential advantages of joint modeling and joint decision-making. It would be
interesting to explore whether the integration of MT with syntax and semantics can be
extended to provide a single-model solution for tasks such as cross-language informa-
tion extraction and question answering, and to evaluate our integrated approach (e.g.,
using GALE distillation metrics).
Acknowledgments
We thank Aaron Phillips for help with
conversion of the output of the entity tagger
for ingest by the tree-grafting program. We
thank Anni Irvine and David Zajic for their
help with experiments on an alternative
Urdu modality/negation tagger based on
projection and training an HMM-based
tagger derived from Identifinder (Bikel,
Schwartz, and Weischedel 1999). For their
helpful ideas and suggestions during the
development of the modality framework,
we are indebted to Mona Diab, Eduard
Hovy, Marge McShane, Teruko Mitamura,
Sergei Nirenburg, Boyan Onyshkevych,
and Owen Rambow. We also thank Basis
Technology Corporation for their generous
contribution of software components to this
work. This work was supported, in part,
by the Johns Hopkins Human Language
435
Computational Linguistics Volume 38, Number 2
Technology Center of Excellence (HLTCOE),
by the National Science Foundation under
grant IIS-0713448, and by BBN Technologies
under GALE DARPA/IPTO contract no.
HR0011-06-C-0022. Any opinions, findings,
and conclusions or recommendations
expressed in this material are those of the
authors and do not necessarily reflect the
views of the sponsor.
References
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley
FrameNet project. In Proceedings of the
36th Annual Meeting of the Association
for Computational Linguistics and
17th International Conference on
Computational Linguistics - Volume 1,
ACL ?98, pages 86?90, Stroudsburg, PA.
Baker, Kathryn, Steven Bethard, Michael
Bloodgood, Ralf Brown, Chris Callison-
Burch, Glen Coppersmith, Bonnie J. Dorr,
Nathaniel W. Filardo, Kendall Giles, Ann
Irvine, Michael Kayser, Lori Levin, Justin
Martineau, James Mayfield, Scott Miller,
Aaron Phillips, Andrew Philpot, Christine
Piatko, Lane Schwartz, and David Zajic.
2010a. Semantically informed machine
translation. Technical Report 002,
Human Language Technology Center of
Excellence, Johns Hopkins University,
Baltimore, MD.
Baker, Kathryn, Michael Bloodgood,
Chris Callison-Burch, Bonnie J. Dorr,
Nathaniel W. Filardo, Lori Levin, Scott
Miller, and Christine Piatko. 2010b.
Semantically-informed machine
translation: A tree-grafting approach.
In Proceedings of The Ninth Biennial
Conference of the Association for Machine
Translation in the Americas, Denver, CO.
Baker, Kathryn, Michael Bloodgood, Mona
Diab, Bonnie J. Dorr, Ed Hovy, Lori Levin,
Marjorie McShane, Teruko Mitamura,
Sergei Nirenburg, Christine Piatko, Owen
Rambow, and Gramm Richardson. 2010c.
SIMT SCALE 2009?Modality annotation
guidelines. Technical Report 004, Human
Language Technology Center of
Excellence, Johns Hopkins University,
Baltimore, MD.
Baker, Kathryn, Michael Bloodgood,
Bonnie J. Dorr, Nathanial W. Filardo,
Lori Levin, and Christine Piatko.
2010d. A modality lexicon and its use
in automatic tagging. In Proceedings of
the Seventh International Conference on
Language Resources and Evaluation
(LREC), pages 1402?1407, Mediterranean
Conference Center, Valletta.
Bar-Haim, Roy, Ido Dagan, Iddo Greental,
and Eyal Shnarch. 2007. Semantic
inference at the lexical-syntactic level.
In Proceedings of the 22nd National
Conference on Artificial intelligence -
Volume 1, pages 871?876, Vancouver,
British Columbia.
Bikel, Daniel M., Richard Schwartz, and
Ralph M. Weischedel. 1999. An algorithm
that learns what?s in a name.Machine
Learning, 34(1?3):211?231.
Bo?hmova?, Alena, Silvie Cinkova?, and
Eva Hajic?ova?. 2005. A manual for
tectogrammatical layer annotation of the
Prague Dependency Treebank [English
translation]. Technical Report #30, U?FAL
MFF UK, Prague, Czech Republic.
Chiang, David. 2005. A hierarchical
phrase-based model for statistical machine
translation. In Proceedings of the 43rd
Annual Meeting of the Association for
Computational Linguistics (ACL-2005),
pages 263?270, Ann Arbor, MI.
Diab, Mona T., Lori Levin, Teruko Mitamura,
Owen Rambow, Vinodkumar
Prabhakaran, and Weiwei Guo. 2009.
Committed belief annotation and tagging.
In Proceedings of the Third Linguistic
Annotation Workshop, ACL-IJCNLP ?09,
pages 68?73, Stroudsburg, PA.
Farkas, Richa?rd, Veronika Vincze, Gyo?rgy
Mo?ra, Ja?nos Csirik, and Gyo?rgy Szarvas.
2010. The CoNLL-2010 shared task:
Learning to detect hedges and their scope
in natural language text. In Proceedings of
the Fourteenth Conference on Computational
Natural Language Learning?Shared Task,
CoNLL ?10: Shared Task, pages 1?12,
Stroudsburg, PA.
Fellbaum, Christiane, editor. 1998.WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Hajic?, Jan, Eva Hajic?ova?, Petr Pajas, Jarmila
Panevova?, Petr Sgall, and Barbora Vidova?
Hladka?. 2001. Prague Dependency
Treebank 1.0 (Final Production Label),
UFAL MFF UK, Prague, Czech Republic.
Huang, Bryant and Kevin Knight. 2006.
Relabeling syntax trees to improve
syntax-based machine translation
quality. In HLT-NAACL, New York.
Irvine, Ann, Mike Kayser, Zhifei Li, Wren
Thornton, and Chris Callison-Burch.
2010. Integrating output from specialized
modules in machine translation:
Transliteration in Joshua. Proceedings
of the Human Language Technology
436
Baker et al Modality and Negation in SIMT
and North American Chapter of the
Association for Computational Linguistics,
pages 240?247. The Prague Bulletin of
Mathematical Linguistics, 93:107?116.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics,
pages 423?430, Sapporo, Japan.
Koehn, Philipp. 2004. Statistical significance
tests for machine translation evaluation. In
Proceedings of EMNLP 2004, pages 388?395,
Barcelona.
Koehn, Philipp, Hieu Hoang, Alexandra
Birch, Chris Callison-Burch, Marcello
Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine
translation. In Proceedings of the ACL-2007
Demo and Poster Sessions, Prague, Czech
Republic, pages 177?180.
Kratzer, Angelika. 1991. Modality. In
Arnim von Stechow and Dieter, editors,
Semantics: An International Handbook of
Contemporary Research. De Gruyter,
Berlin, pages 639?650.
Larreya, Paul. 2009. Towards a typology of
modality in language. In Raphael Salkie,
Pierre Busuttil, and Johan van der Auwera,
editors,Modality in English: Theory and
Description. Mouton de Gruyter, Paris,
pages 9?30.
Li, Zhifei, Chris Callison-Burch, Chris Dyer,
Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar
Zaidan. 2009. Joshua: An open source
toolkit for parsing-based machine
translation. In Proceedings of the Fourth
Workshop on Statistical Machine Translation,
pages 135?139, Athens.
Marcus, Mitchell P., Mary Ann
Marcinkiewicz, and Beatrice Santorini.
1993. Building a large annotated corpus of
English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
McShane, Marjorie, Sergei Nirenburg, and
Ron Zacharski. 2004. Mood and modality:
Out of the theory and into the fray. Natural
Language Engineering, 19(1):57?89.
Miller, Scott, Heidi Fox, Lance Ramshaw,
and Ralph Weischedel. 1998. SIFT:
Statistically-derived information from
text. In Seventh Message Understanding
Conference (MUC-7), Washington, DC,
Miller, Scott, Heidi J. Fox, Lance A.
Ramshaw, and Ralph M. Weischedel. 2000.
A novel use of statistical parsing to extract
information from text. In Proceedings of
Applied Natural Language Processing
and the North American Association for
Computational Linguistics, pages 226?233,
Seattle, Washington.
Murata, Masaki, Kiyotaka Uchimoto, Qing
Ma, Toshiyuki Kanamaru, and Hitoshi
Isahara. 2005. Analysis of machine
translation systems? errors in tense,
aspect, and modality. In Proceedings of the
19th Asia-Pacific Conference on Language,
Information and Computing (PACLIC 2005),
Taipei, Taiwan.
Nairn, Rowan, Cleo Condorovdi, and
Lauri Karttunen. 2006. Computing
relative polarity for textual inference.
In Proceedings of the International Workshop
on Inference in Computational Semantics
(ICoS-5), pages 66?76, Buxton, England.
Palmer, Martha, Daniel Gildea, and
Paul Kingsbury. 2005. The Proposition
Bank: An annotated corpus of semantic
roles. Computational Linguistics,
31:71?106.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. Bleu:
A method for automatic evaluation of
machine translation. In Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics (ACL-2002),
pages 311?318, Philadelphia, PA.
Petrov, Slav and Dan Klein. 2007. Learning
and inference for hierarchically split
PCFGs. In Proceedings of the 22nd American
Association for Artificial Intelligence,
pages 1663?1666, Vancouver, British
Columbia, Canada.
Prabhakaran, Vinodkumar, Owen Rambow,
and Mona Diab. 2010. Automatic
committed belief tagging. In Proceedings
of the 23rd International Conference on
Computational Linguistics: Posters, COLING
?10, pages 1014?1022, Beijing, China.
Prasad, Rashmi, Nikhil Dinesh, Alan Lee,
Eleni Miltsakaki, Livio Robaldo, Aravind
Joshi, and Bonnie Webber. 2008. The Penn
Discourse TreeBank 2.0. In Proceedings of
the Sixth International Language Resources
and Evaluation (LREC?08), pages 28?30,
Marrakech.
Pustejovsky, James, Marc Verhagen, Roser
Saur??, Jessica Littman, Robert Gaizauskas,
Graham Katz, Inderjeet Mani, Robert
Knippen, and Andrea Setzer. 2006.
TimeBank 1.2. Linguistic Data Consortium,
Philadelphia, PA.
Richman, Alexander and Patrick Schone.
2008. Mining wiki resources for
multilingual named entity recognition.
437
Computational Linguistics Volume 38, Number 2
In Proceedings of ACL-08: HLT, pages 1?9,
Columbus, OH.
Rubin, Victoria L. 2007. Stating with
certainty or stating with doubt:
Intercoder reliability results for manual
annotation of epistemically modalized
statements. In Proceedings of the Human
Language Technology and North American
Chapter of the Association for Computational
Linguistics (Short Papers), pages 141?144,
Rochester, NY.
Saur??, Roser and James Pustejovsky. 2009.
FactBank: A corpus annotated with
event factuality. Language Resources and
Evaluation, 43(3):227?268.
Saur??, Roser, Marc Verhagen, and James
Pustejovsky. 2006. Annotating and
recognizing event modality in text.
In Proceedings of the 19th International
Florida Artificial Intelligence Research
Society Conference, pages 333?339,
Melbourne Beach, FL.
Sigurd, Bengt and Barbara Gawro?nska.
1994. Modals as a problem for MT. In
Proceedings of the 15th International
Conference on Computational Linguistics
(COLING) - Volume 1, pages 120?124,
Kyoto, Japan.
Steedman, Mark. 1999. Alternating
quantifier scope in CCG. In Proceedings of
the 37th Annual Meeting of the Association
for Computational Linguistics (ACL),
College Park, MD.
Szarvas, Gyo?rgy, Veronika Vincze, Richa?rd
Farkas, and Ja?nos Csirik. 2008. The
BioScope corpus: Annotation for negation,
uncertainty and their scope in biomedical
texts. In Proceedings of the Workshop on
Current Trends in Biomedical Natural
Language Processing, pages 38?45,
Stroudsburg, PA.
van der Auwera, Johan and Andreas
Ammann. 2005. Overlap between
situational and epistemic modal marking.
In Martin Haspelmath, Matthew S. Dryer,
David Gil, and Bernard Comrie, editors,
World Atlas of Language Structures. Oxford
University Press, New York, chapter 76,
pages 310?313.
Venugopal, Ashish and Andreas Zollmann.
2009. Grammar based statistical MT on
Hadoop: An end-to-end toolkit for large
scale PSCFG based MT. Prague Bulletin of
Mathematical Linguistics, 91:67?78.
Venugopal, Ashish, Andreas Zollmann, and
Stephan Vogel. 2007. An efficient two-pass
approach to synchronous-CFG driven
statistical MT. In Proceedings of the
Human Language Technology Conference
of the North American Chapter of the
Association for Computational Linguistics
(HLT/NAACL-2007), pages 500?507,
Rochester, NY.
von Fintel, Kai and Sabine Iatridou.
2006. How to say ought in foreign: The
composition of weak necessity modals.
In Proceedings of the 6th Workshop on
Formal Linguistics, Florianopolis, Brazil,
August 2006.
Wang, Wei, Jonathan May, Kevin Knight,
and Daniel Marcu. 2010. Re-structuring,
re-labeling, and re-aligning for
syntax-based machine translation.
Computational Linguistics, 36(2):247?277.
Webber, Bonnie, Aravid Joshi, Matthew
Stone, and Alistair Knott. 2003. Anaphora
and discourse structure. Computational
Linguistics, 29:545?587.
Wiebe, Janyce, Theresa Wilson, and Claire
Cardie. 2005. Annotating expressions
of opinions and emotions in language.
Language Resources and Evaluation,
39(2?3):165?210.
Wilson, Theresa, Janyce Wiebe, and Paul
Hoffman. 2009. Recognizing contextual
polarity: An exploration of features
for phrase-level sentiment analysis.
Computational Linguistics, 35:399?433.
Zollmann, Andreas and Ashish Venugopal.
2006. Syntax augmented machine
translation via chart parsing. In
Proceedings on the Workshop on Statistical
Machine Translation, pages 138?141,
New York City.
438
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 854?864,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Bucking the Trend: Large-Scale Cost-Focused Active Learning for
Statistical Machine Translation
Michael Bloodgood
Human Language Technology
Center of Excellence
Johns Hopkins University
Baltimore, MD 21211
bloodgood@jhu.edu
Chris Callison-Burch
Center for Language and
Speech Processing
Johns Hopkins University
Baltimore, MD 21211
ccb@cs.jhu.edu
Abstract
We explore how to improve machine trans-
lation systems by adding more translation
data in situations where we already have
substantial resources. The main challenge
is how to buck the trend of diminishing re-
turns that is commonly encountered. We
present an active learning-style data solic-
itation algorithm to meet this challenge.
We test it, gathering annotations via Ama-
zon Mechanical Turk, and find that we get
an order of magnitude increase in perfor-
mance rates of improvement.
1 Introduction
Figure 1 shows the learning curves for two state of
the art statistical machine translation (SMT) sys-
tems for Urdu-English translation. Observe how
the learning curves rise rapidly at first but then a
trend of diminishing returns occurs: put simply,
the curves flatten.
This paper investigates whether we can buck the
trend of diminishing returns, and if so, how we can
do it effectively. Active learning (AL) has been ap-
plied to SMT recently (Haffari et al, 2009; Haffari
and Sarkar, 2009) but they were interested in start-
ing with a tiny seed set of data, and they stopped
their investigations after only adding a relatively
tiny amount of data as depicted in Figure 1.
In contrast, we are interested in applying AL
when a large amount of data already exists as is
the case for many important lanuage pairs. We de-
velop an AL algorithm that focuses on keeping an-
notation costs (measured by time in seconds) low.
It succeeds in doing this by only soliciting trans-
lations for parts of sentences. We show that this
gets a savings in human annotation time above and
beyond what the reduction in # words annotated
would have indicated by a factor of about three
and speculate as to why.
0 2 4 6 8 10x 1040
5
10
15
20
25
30
Number of Sentences in Training Data
BLEU 
Score
JSyntax and JHier Learning Curves on the LDC Urdu?English Language Pack (BLEU vs Sentences)
 
 
jHierjSyntax
as far as previous AL for SMT research studies were conducted
where we begin our main investigations into bucking the trend of diminishing returns
Figure 1: Syntax-based and Hierarchical Phrase-
Based MT systems? learning curves on the LDC
Urdu-English language pack. The x-axis measures
the number of sentence pairs in the training data.
The y-axis measures BLEU score. Note the di-
minishing returns as more data is added. Also
note how relatively early on in the process pre-
vious studies were terminated. In contrast, the
focus of our main experiments doesn?t even be-
gin until much higher performance has already
been achieved with a period of diminishing returns
firmly established.
We conduct experiments for Urdu-English
translation, gathering annotations via Amazon
Mechanical Turk (MTurk) and show that we can
indeed buck the trend of diminishing returns,
achieving an order of magnitude increase in the
rate of improvement in performance.
Section 2 discusses related work; Section 3
discusses preliminary experiments that show the
guiding principles behind the algorithm we use;
Section 4 explains our method for soliciting new
translation data; Section 5 presents our main re-
sults; and Section 6 concludes.
854
2 Related Work
Active learning has been shown to be effective
for improving NLP systems and reducing anno-
tation burdens for a number of NLP tasks (see,
e.g., (Hwa, 2000; Sassano, 2002; Bloodgood
and Vijay-Shanker, 2008; Bloodgood and Vijay-
Shanker, 2009b; Mairesse et al, 2010; Vickrey et
al., 2010)). The current paper is most highly re-
lated to previous work falling into three main ar-
eas: use of AL when large corpora already exist;
cost-focused AL; and AL for SMT.
In a sense, the work of Banko and Brill (2001)
is closely related to ours. Though their focus is
mainly on investigating the performance of learn-
ing methods on giant corpora many orders of mag-
nitude larger than previously used, they do lay out
how AL might be useful to apply to acquire data
to augment a large set cheaply because they rec-
ognize the problem of diminishing returns that we
discussed in Section 1.
The second area of work that is related to ours is
previous work on AL that is cost-conscious. The
vast majority of AL research has not focused on
accurate cost accounting and a typical assumption
is that each annotatable has equal annotation cost.
An early exception in the AL for NLP field was
the work of Hwa (2000), which makes a point of
using # of brackets to measure cost for a syntac-
tic analysis task instead of using # of sentences.
Another relatively early work in our field along
these lines was the work of Ngai and Yarowsky
(2000), which measured actual times of annota-
tion to compare the efficacy of rule writing ver-
sus annotation with AL for the task of BaseNP
chunking. Osborne and Baldridge (2004) argued
for the use of discriminant cost over unit cost for
the task of Head Phrase Structure Grammar parse
selection. King et al (2004) design a robot that
tests gene functions. The robot chooses which
experiments to conduct by using AL and takes
monetary costs (in pounds sterling) into account
during AL selection and evaluation. Unlike our
situation for SMT, their costs are all known be-
forehand because they are simply the cost of ma-
terials to conduct the experiments, which are al-
ready known to the robot. Hachey et al (2005)
showed that selectively sampled examples for an
NER task took longer to annotate and had lower
inter-annotator agreement. This work is related to
ours because it shows that how examples are se-
lected can impact the cost of annotation, an idea
we turn around to use for our advantage when de-
veloping our data selection algorithm. Haertel et
al. (2008) emphasize measuring costs carefully for
AL for POS tagging. They develop a model based
on a user study that can estimate the time required
for POS annotating. Kapoor et al (2007) assign
costs for AL based on message length for a voice-
mail classification task. In contrast, we show for
SMT that annotation times do not scale according
to length in words and we show our method can
achieve a speedup in annotation time above and
beyond what the reduction in words would indi-
cate. Tomanek and Hahn (2009) measure cost by #
of tokens for an NER task. Their AL method only
solicits labels for parts of sentences in the interest
of reducing annotation effort. Along these lines,
our method is similar in the respect that we also
will only solicit annotation for parts of sentences,
though we prefer to measure cost with time and
we show that time doesn?t track with token length
for SMT.
Haffari et al (2009), Haffari and Sarkar (2009),
and Ambati et al (2010) investigate AL for SMT.
There are two major differences between our work
and this previous work. One is that our intended
use cases are very different. They deal with the
more traditional AL setting of starting from an ex-
tremely small set of seed data. Also, by SMT stan-
dards, they only add a very tiny amount of data
during AL. All their simulations top out at 10,000
sentences of labeled data and the models learned
have relatively low translation quality compared to
the state of the art.
On the other hand, in the current paper, we
demonstrate how to apply AL in situations where
we already have large corpora. Our goal is to buck
the trend of diminishing returns and use AL to
add data to build some of the highest-performing
MT systems in the world while keeping annota-
tion costs low. See Figure 1 from Section 1, which
contrasts where (Haffari et al, 2009; Haffari and
Sarkar, 2009) stop their investigations with where
we begin our studies.
The other major difference is that (Haffari et al,
2009; Haffari and Sarkar, 2009) measure annota-
tion cost by # of sentences. In contrast, we bring
to light some potential drawbacks of this practice,
showing it can lead to different conclusions than
if other annotation cost metrics are used, such as
time and money, which are the metrics that we use.
855
3 Simulation Experiments
Here we report on results of simulation experi-
ments that help to illustrate and motivate the de-
sign decisions of the algorithm we present in Sec-
tion 4. We use the Urdu-English language pack1
from the Linguistic Data Consortium (LDC),
which contains ? 88000 Urdu-English sentence
translation pairs, amounting to? 1.7 million Urdu
words translated into English. All experiments in
this paper evaluate on a genre-balanced split of the
NIST2008 Urdu-English test set. In addition, the
language pack contains an Urdu-English dictio-
nary consisting of ? 114000 entries. In all the ex-
periments, we use the dictionary at every iteration
of training. This will make it harder for us to show
our methods providing substantial gains since the
dictionary will provide a higher base performance
to begin with. However, it would be artificial to
ignore dictionary resources when they exist.
We experiment with two translation models: hi-
erarchical phrase-based translation (Chiang, 2007)
and syntax augmented translation (Zollmann and
Venugopal, 2006), both of which are implemented
in the Joshua decoder (Li et al, 2009). We here-
after refer to these systems as jHier and jSyntax,
respectively.
We will now present results of experiments with
different methods for growing MT training data.
The results are organized into three areas of inves-
tigations:
1. annotation costs;
2. managing uncertainty; and
3. how to automatically detect when to stop so-
liciting annotations from a pool of data.
3.1 Annotation Costs
We begin our cost investigations with four sim-
ple methods for growing MT training data: ran-
dom, shortest, longest, and VocabGrowth sen-
tence selection. The first three methods are self-
explanatory. VocabGrowth (hereafter VG) selec-
tion is modeled after the best methods from previ-
ous work (Haffari et al, 2009; Haffari and Sarkar,
2009), which are based on preferring sentences
that contain phrases that occur frequently in un-
labeled data and infrequently in the so-far labeled
data. Our VG method selects sentences for transla-
tion that contain n-grams (for n in {1,2,3,4}) that
1LDC Catalog No.: LDC2006E110.
Init:
Go through all available training
data (labeled and unlabeled)
and obtain frequency counts for
every n-gram (n in {1, 2, 3, 4})
that occurs.
sortedNGrams? Sort n-grams by
frequency in descending order.
Loop
until stopping criterion (see Section 3.3) is met
1. trigger ? Go down sortedNGrams list
and find the first n-gram that isn?t covered in
the so far labeled training data.
2. selectedSentence? Find a sentence
that contains trigger.
3. Remove selectedSentence from unlabeled
data and add it to labeled training data.
End Loop
Figure 2: The VG sentence selection algorithm
do not occur at all in our so-far labeled data. We
call an n-gram ?covered? if it occurs at least once
in our so-far labeled data. VG has a preference
for covering frequent n-grams before covering in-
frequent n-grams. The VG method is depicted in
Figure 2.
Figure 3 shows the learning curves for both
jHier and jSyntax for VG selection and random
selection. The y-axis measures BLEU score (Pap-
ineni et al, 2002),which is a fast automatic way of
measuring translation quality that has been shown
to correlate with human judgments and is perhaps
the most widely used metric in the MT commu-
nity. The x-axis measures the number of sen-
tence translation pairs in the training data. The VG
curves are cut off at the point at which the stopping
criterion in Section 3.3 is met. From Figure 3 it
might appear that VG selection is better than ran-
dom selection, achieving higher-performing sys-
tems with fewer translations in the labeled data.
However, it is important to take care when mea-
suring annotation costs (especially for relatively
complicated tasks such as translation). Figure 4
shows the learning curves for the same systems
and selection methods as in Figure 3 but now the
x-axis measures the number of foreign words in
the training data. The difference between VG and
random selection now appears smaller.
For an extreme case, to illustrate the ramifica-
856
0 10,000 20,000 30,000 40,000 50,000 60,000 70,000 80,000 90,0000
5
10
15
20
25
30jHier and jSyntax: VG vs Random selection (BLEU vs Sents)
Number of Sentence Pairs in the Training Data
BLEU
 Score
 
 
jHier: random selectionjHier: VG selectionjSyntax: random selectionjSyntax: VG selection
where we will start our main experiments
where previous AL for SMT research stopped their experiments
Figure 3: Random vs VG selection. The x-axis
measures the number of sentence pairs in the train-
ing data. The y-axis measures BLEU score.
tions of measuring translation annotation cost by #
of sentences versus # of words, consider Figures 5
and 6. They both show the same three selection
methods but Figure 5 measures the x-axis by # of
sentences and Figure 6 measures by # of words. In
Figure 5, one would conclude that shortest is a far
inferior selection method to longest but in Figure 6
one would conclude the opposite.
Measuring annotation time and cost in dol-
lars are probably the most important measures
of annotation cost. We can?t measure these for
the simulated experiments but we will use time
(in seconds) and money (in US dollars) as cost
measures in Section 5, which discusses our non-
simulated AL experiments. If # sentences or #
words track these other more relevant costs in pre-
dictable known relationships, then it would suffice
to measure # sentences or # words instead. But it?s
clear that different sentences can have very differ-
ent annotation time requirements according to how
long and complicated they are so we will not use
# sentences as an annotation cost any more. It is
not as clear how # words tracks with annotation
time. In Section 5 we will present evidence show-
ing that time per word can vary considerably and
also show a method for soliciting annotations that
reduces time per word by nearly a factor of three.
As it is prudent to evaluate using accurate cost
accounting, so it is also prudent to develop new
AL algorithms that take costs carefully into ac-
count. Hence, reducing annotation time burdens
0 0.5 1 1.5 2x 1060
5
10
15
20
25
30jHier and jSyntax: VG vs Random selection (BLEU vs FWords)
Number of Foreign Words in Training Data
BLEU
 Score
 
 
jHier: random selectionjHier: VG selectionjSyntax: random selectionjSyntax: VG selection
Figure 4: Random vs VG selection. The x-axis
measures the number of foreign words in the train-
ing data. The y-axis measures BLEU score.
instead of the # of sentences translated (which
might be quite a different thing) will be a corner-
stone of the algorithm we describe in Section 4.
3.2 Managing Uncertainty
One of the most successful of all AL methods de-
veloped to date is uncertainty sampling and it has
been applied successfully many times (e.g.,(Lewis
and Gale, 1994; Tong and Koller, 2002)). The
intuition is clear: much can be learned (poten-
tially) if there is great uncertainty. However, with
MT being a relatively complicated task (compared
with binary classification, for example), it might
be the case that the uncertainty approach has to
be re-considered. If words have never occurred
in the training data, then uncertainty can be ex-
pected to be high. But we are concerned that if a
sentence is translated for which (almost) no words
have been seen in training yet, though uncertainty
will be high (which is usually considered good for
AL), the word alignments may be incorrect and
then subsequent learning from that translation pair
will be severely hampered.
We tested this hypothesis and Figure 7 shows
empirical evidence that it is true. Along with VG,
two other selection methods? learning curves are
charted in Figure 7: mostNew, which prefers to
select those sentences which have the largest # of
unseen words in them; and moderateNew, which
aims to prefer sentences that have a moderate #
of unseen words, preferring sentences with ? ten
857
0 2 4 6 8 10x 1040
5
10
15
20
25 jHiero: Random, Shortest, and Longest selection
BLEU
 Score
Number of Sentences in Training Data
 
 
randomshortestlongest
Figure 5: Random vs Shortest vs Longest selec-
tion. The x-axis measures the number of sentence
pairs in the training data. The y-axis measures
BLEU score.
unknown words in them. One can see that most-
New underperforms VG. This could have been due
to VG?s frequency component, which mostNew
doesn?t have. But moderateNew also doesn?t have
a frequency preference so it is likely that mostNew
winds up overwhelming the MT training system,
word alignments are incorrect, and less is learned
as a result. In light of this, the algorithm we de-
velop in Section 4 will be designed to avoid this
word alignment danger.
3.3 Automatic Stopping
The problem of automatically detecting when to
stop AL is a substantial one, discussed at length
in the literature (e.g., (Bloodgood and Vijay-
Shanker, 2009a; Schohn and Cohn, 2000; Vla-
chos, 2008)). In our simulation, we stop VG once
all n-grams (n in {1,2,3,4}) have been covered.
Though simple, this stopping criterion seems to
work well as can be seen by where the curve for
VG is cut off in Figures 3 and 4. It stops af-
ter 1,293,093 words have been translated, with
jHier?s BLEU=21.92 and jSyntax?s BLEU=26.10
at the stopping point. The ending BLEU scores
(with the full corpus annotated) are 21.87 and
26.01 for jHier and jSyntax, respectively. So
our stopping criterion saves 22.3% of the anno-
tation (in terms of words) and actually achieves
slightly higher BLEU scores than if all the data
were used. Note: this ?less is more? phenomenon
0 0.5 1 1.5 2x 1060
5
10
15
20
25
Number of Foreign Words in Training Data
BLEU
 Score
jHiero: Longest, Shortest, and Random Selection
 
 
randomshortestlongest
Figure 6: Random vs Shortest vs Longest selec-
tion. The x-axis measures the number of foreign
words in the training data. The y-axis measures
BLEU score.
has been commonly observed in AL settings (e.g.,
(Bloodgood and Vijay-Shanker, 2009a; Schohn
and Cohn, 2000)).
4 Highlighted N-Gram Method
In this section we describe a method for solicit-
ing human translations that we have applied suc-
cessfully to improving translation quality in real
(not simulated) conditions. We call the method the
Highlighted N-Gram method, or HNG, for short.
HNG solicits translations only for trigger n-grams
and not for entire sentences. We provide senten-
tial context, highlight the trigger n-gram that we
want translated, and ask for a translation of just the
highlighted trigger n-gram. HNG asks for transla-
tions for triggers in the same order that the triggers
are encountered by the algorithm in Figure 2. A
screenshot of our interface is depicted in Figure 8.
The same stopping criterion is used as was used in
the last section. When the stopping criterion be-
comes true, it is time to tap a new unlabeled pool
of foreign text, if available.
Our motivations for soliciting translations for
only parts of sentences are twofold, corresponding
to two possible cases. Case one is that a translation
model learned from the so-far labeled data will be
able to translate most of the non-trigger words in
the sentence correctly. Thus, by asking a human
to translate only the trigger words, we avoid wast-
ing human translation effort. (We will show in
858
0 0.5 1 1.5 2x 1060
5
10
15
20
25
Number of Foreign Words in Training Data
BLEU
 Score
jHiero: VG vs mostNew vs moderateNew
 
 
VGmostNewmoderateNew
Figure 7: VG vs MostNew vs ModerateNew se-
lection. The x-axis measures the number of sen-
tence pairs in the training data. The y-axis mea-
sures BLEU score.
!"#$% "& '() '* +,-./0)1 234 5678 9:-! !"#$ %$&'$ &( )*+ ;<= '$ >/?@3 /A  
>. +B!C D)C EF GH?I '3") D)+0) +&  .
"J& "J& "$K$!1 2L)M8 ':?3N !O#)P& GQ6- '& R7@* /& ST& ST& !9,8 UV)WX  
'8 ,"*)- !.( /0." 234 !.C 234 !D#8 EY).<3 '8 MH3 G:Z !"-[$% '8 R3\  
5#)T= 5#)] '3E& >'#)P8 ><&  .
^ : S_ <* '(* C+& +:Z '* /`$>a UH$GX "& 5,-.b '8 "$c 9* S_ /& <* dH#$!<)  
+& ? e(@)f e3<g : #1.2 #1.2 "(:Z  .
<* e@* ':) K) C+) E#) '* +$ /H0) +& <* G:I ' 3.45 ' '& ')C',$% "& 5#:68 +$  .
!1 ') '(,) 67$ !8 ' )9 GQ)PI '& UI ') .hX +& !.C !1 '$ "i3 !"-!f "(:Z ':) K)  
/H0)  ' .
Figure 8: Screenshot of the interface we used for
soliciting translations for triggers.
the next section that we even get a much larger
speedup above and beyond what the reduction in
number of translated words would give us.) Case
two is that a translation model learned from the so-
far labeled data will (in addition to not being able
to translate the trigger words correctly) also not be
able to translate most of the non-trigger words cor-
rectly. One might think then that this would be a
great sentence to have translated because the ma-
chine can potentially learn a lot from the transla-
tion. Indeed, one of the overarching themes of AL
research is to query examples where uncertainty is
greatest. But, as we showed evidence for in the
last section, for the case of SMT, too much un-
certainty could in a sense overwhelm the machine
and it might be better to provide new training data
in a more gradual manner. A sentence with large
#s of unseen words is likely to get word-aligned
incorrectly and then learning from that translation
could be hampered. By asking for a translation
of only the trigger words, we expect to be able to
circumvent this problem in large part.
The next section presents the results of experi-
ments that show that the HNG algorithm is indeed
practically effective. Also, the next section ana-
lyzes results regarding various aspects of HNG?s
behavior in more depth.
5 Experiments and Discussion
5.1 General Setup
We set out to see whether we could use the HNG
method to achieve translation quality improve-
ments by gathering additional translations to add
to the training data of the entire LDC language
pack, including its dictionary. In particular, we
wanted to see if we could achieve translation im-
provements on top of already state-of-the-art per-
forming systems trained already on the entire LDC
corpus. Note that at the outset this is an ambitious
endeavor (recall the flattening of the curves in Fig-
ure 1 from Section 1).
Snow et al (2008) explored the use of the Ama-
zon Mechanical Turk (MTurk) web service for
gathering annotations for a variety of natural lan-
guage processing tasks and recently MTurk has
been shown to be a quick, cost-effective way to
gather Urdu-English translations (Bloodgood and
Callison-Burch, 2010). We used the MTurk web
service to gather our annotations. Specifically, we
first crawled a large set of BBC articles on the in-
ternet in Urdu and used this as our unlabeled pool
from which to gather annotations. We applied the
HNG method from Section 4 to determine what to
post on MTurk for workers to translate.2 We gath-
ered 20,580 n-gram translations for which we paid
$0.01 USD per translation, giving us a total cost
of $205.80 USD. We also gathered 1632 randomly
chosen Urdu sentence translations as a control set,
for which we paid $0.10 USD per sentence trans-
lation.3
2For practical reasons we restricted ourselves to not con-
sidering sentences that were longer than 60 Urdu words, how-
ever.
3The prices we paid were not market-driven. We just
chose prices we thought were reasonable. In hindsight, given
how much quicker the phrase translations are for people we
could have had a greater disparity in price.
859
5.2 Accounting for Translation Time
MTurk returns with each assignment the ?Work-
TimeInSeconds.? This is the amount of time be-
tween when a worker accepts an assignment and
when the worker submits the completed assign-
ment. We use this value to estimate annotation
times.4
Figure 9 shows HNG collection versus random
collection from MTurk. The x-axis measures the
number of seconds of annotation time. Note that
HNG is more effective. A result that may be par-
ticularly interesting is that HNG results in a time
speedup by more than just the reduction in trans-
lated words would indicate. The average time to
translate a word of Urdu with the sentence post-
ings to MTurk was 32.92 seconds. The average
time to translate a word with the HNG postings to
MTurk was 11.98 seconds. This is nearly three
times faster. Figure 10 shows the distribution of
speeds (in seconds per word) for HNG postings
versus complete sentence postings. Note that the
HNG postings consistently result in faster transla-
tion speeds than the sentence postings5.
We hypothesize that this speedup comes about
because when translating a full sentence, there?s
the time required to examine each word and trans-
late them in some sense (even if not one-to-one)
and then there is an extra significant overhead time
to put it all together and synthesize into a larger
sentence translation. The factor of three speedup
is evidence that this overhead is significant effort
compared to just quickly translating short n-grams
from a sentence. This speedup is an additional
benefit of the HNG approach.
5.3 Bucking the Trend
We gathered translations for? 54,500 Urdu words
via the use of HNG on MTurk. This is a rela-
tively small amount, ? 3% of the LDC corpus.
Figure 11 shows the performance when we add
this training data to the LDC corpus. The rect-
4It?s imperfect because of network delays and if a person
is multitasking or pausing between their accept and submit
times. Nonetheless, the times ought to be better estimates as
they are taken over larger samples.
5The average speed for the HNG postings seems to be
slower than the histogram indicates. This is because there
were a few extremely slow outlier speeds for a handful of
HNG postings. These are almost certainly not cases when the
turker is working continuously on the task and so the average
speed we computed for the HNG postings might be slower
than the actual speed and hence the true speedup may even
be faster than indicated by the difference between the aver-
age speeds we reported.
0 1 2 3 4 5 6x 10521.6
21.8
22
22.2
22.4
22.6
22.8
Number of Seconds of Annotation Time
BLEU
 Score
jHier: HNG Collection vs Random Collection of Annotations from MTurk
 
 
randomHNG
Figure 9: HNG vs Random collection of new data
via MTurk. y-axis measures BLEU. x-axis mea-
sures annotation time in seconds.
angle around the last 700,000 words of the LDC
data is wide and short (it has a height of 0.9 BLEU
points and a width of 700,000 words) but the rect-
angle around the newly added translations is nar-
row and tall (a height of 1 BLEU point and a
width of 54,500 words). Visually, it appears we
are succeeding in bucking the trend of diminish-
ing returns. We further confirmed this by running
a least-squares linear regression on the points of
the last 700,000 words annotated in the LDC data
and also for the points in the new data that we ac-
quired via MTurk for $205.80 USD. We find that
the slope fit to our new data is 6.6245E-06 BLEU
points per Urdu word, or 6.6245 BLEU points for
a million Urdu words. The slope fit to the LDC
data is only 7.4957E-07 BLEU points per word,
or only 0.74957 BLEU points for a million words.
This is already an order of magnitude difference
that would make the difference between it being
worth adding more data and not being worth it;
and this is leaving aside the added time speedup
that our method enjoys.
Still, we wondered why we could not have
raised BLEU scores even faster. The main hur-
dle seems to be one of coverage. Of the 20,580 n-
grams we collected, only 571 (i.e., 2.77%) of them
ever even occur in the test set.
5.4 Beyond BLEU Scores
BLEU is an imperfect metric (Callison-Burch et
al., 2006). One reason is that it rates all ngram
860
0 20 40 60 80 100 1200
0.05
0.1
0.15
0.2
0.25
Time (in seconds) per foreign word translated
Relati
ve Fre
quenc
y
Histogram showing the distribution of translation speeds (in seconds per foreign word) when translations are collected via n?grams versus via complete sentences
 
 
n?gramssentencesaverage time perword for sentencesaverage time perword for n?grams
Figure 10: Distribution of translation speeds (in
seconds per word) for HNG postings versus com-
plete sentence postings. The y-axis measures rel-
ative frequency. The x-axis measures translation
speed in seconds per word (so farther to the left is
faster).
mismatches equally although some are much more
important than others. Another reason is it?s not
intuitive what a gain of x BLEU points means in
practice. Here we show some concrete example
translations to show the types of improvements
we?re achieving and also some examples which
suggest improvements we can make to our AL se-
lection algorithm in the future. Figure 12 shows a
prototypical example of our system working.
Figure 13 shows an example where the strategy
is working partially but not as well as it might. The
Urdu phrase was translated by turkers as ?gowned
veil?. However, since the word aligner just aligns
the word to ?gowned?, we only see ?gowned? in
our output. This prompts a number of discussion
points. First, the ?after system? has better transla-
tions but they?re not rewarded by BLEU scores be-
cause the references use the words ?burqah? or just
?veil? without ?gowned?. Second, we hypothesize
that we may be able to see improvements by over-
riding the automatic alignment software when-
ever we obtain a many-to-one or one-to-many (in
terms of words) translation for one of our trigger
phrases. In such cases, we?d like to make sure that
every word on the ?many? side is aligned to the
1 1.2 1.4 1.6 1.8x 10621
21.5
22
22.5
23
23.5 Bucking the Trend: JHiero Translation Quality versus Number of Foreign Words Annotated
BLEU 
Score
Number of Foreign Words Annotated
the approx. 54,500 foreign wordswe selectively sampled for annotation cost = $205.80last approx. 700,000 foreign words annotated in LDC data 
Figure 11: Bucking the trend: performance of
HNG-selected additional data from BBC web
crawl data annotated via Amazon Mechanical
Turk. y-axis measures BLEU. x-axis measures
number of words annotated.
Figure 12: Example of strategy working.
single word on the ?one? side. For example, we
would force both ?gowned? and ?veil? to be aligned
to the single Urdu word instead of allowing the au-
tomatic aligner to only align ?gowned?.
Figure 14 shows an example where our ?before?
system already got the translation correct without
the need for the additional phrase translation. This
is because though the ?before? system had never
seen the Urdu expression for ?12May?, it had seen
the Urdu words for ?12? and ?May? in isolation
and was able to successfully compose them. An
area of future work is to use the ?before? system to
determine such cases automatically and avoid ask-
ing humans to provide translations in such cases.
861
Figure 13: Example showing where we can im-
prove our selection strategy.
Figure 14: Example showing where we can im-
prove our selection strategy.
6 Conclusions and Future Work
We succeeded in bucking the trend of diminishing
returns and improving translation quality while
keeping annotation costs low. In future work we
would like to apply these ideas to domain adap-
tation (say, general-purpose MT system to work
for scientific domain such as chemistry). Also, we
would like to test with more languages, increase
the amount of data we can gather, and investigate
stopping criteria further. Also, we would like to
investigate increasing the efficiency of the selec-
tion algorithm by addressing issues such as the one
raised by the 12 May example presented earlier.
Acknowledgements
This work was supported by the Johns Hopkins
University Human Language Technology Center
of Excellence. Any opinions, findings, conclu-
sions, or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of the sponsor.
References
Vamshi Ambati, Stephan Vogel, and Jaime Carbonell.
2010. Active learning and crowd-sourcing for ma-
chine translation. In Proceedings of the Seventh con-
ference on International Language Resources and
Evaluation (LREC?10), Valletta, Malta, may. Euro-
pean Language Resources Association (ELRA).
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In Proceedings of 39th Annual Meeting of the
Association for Computational Linguistics, pages
26?33, Toulouse, France, July. Association for Com-
putational Linguistics.
Michael Bloodgood and Chris Callison-Burch. 2010.
Using mechanical turk to build machine translation
evaluation sets. In Proceedings of the Workshop on
Creating Speech and Language Data With Amazon?s
Mechanical Turk, Los Angeles, California, June.
Association for Computational Linguistics.
Michael Bloodgood and K Vijay-Shanker. 2008. An
approach to reducing annotation costs for bionlp.
In Proceedings of the Workshop on Current Trends
in Biomedical Natural Language Processing, pages
104?105, Columbus, Ohio, June. Association for
Computational Linguistics.
Michael Bloodgood and K Vijay-Shanker. 2009a. A
method for stopping active learning based on stabi-
lizing predictions and the need for user-adjustable
stopping. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 39?47, Boulder, Colorado,
June. Association for Computational Linguistics.
Michael Bloodgood and K Vijay-Shanker. 2009b. Tak-
ing into account the differences between actively
and passively acquired data: The case of active
learning with support vector machines for imbal-
anced datasets. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL), pages 137?
140, Boulder, Colorado, June. Association for Com-
putational Linguistics.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of Bleu in ma-
chine translation research. In 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL-2006), Trento, Italy.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning
(CoNLL-2005), pages 144?151, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Robbie Haertel, Eric Ringger, Kevin Seppi, James Car-
roll, and Peter McClanahan. 2008. Assessing the
862
costs of sampling methods in active learning for an-
notation. In Proceedings of ACL-08: HLT, Short Pa-
pers, pages 65?68, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Gholamreza Haffari and Anoop Sarkar. 2009. Active
learning for multilingual statistical machine trans-
lation. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 181?189, Suntec,
Singapore, August. Association for Computational
Linguistics.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based
machine translation. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 415?423,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Rebecca Hwa. 2000. Sample selection for statistical
grammar induction. In Hinrich Schu?tze and Keh-
Yih Su, editors, Proceedings of the 2000 Joint SIG-
DAT Conference on Empirical Methods in Natural
Language Processing, pages 45?53. Association for
Computational Linguistics, Somerset, New Jersey.
Ashish Kapoor, Eric Horvitz, and Sumit Basu. 2007.
Selective supervision: Guiding supervised learn-
ing with decision-theoretic active learning. In
Manuela M. Veloso, editor, IJCAI 2007, Proceed-
ings of the 20th International Joint Conference on
Artificial Intelligence, Hyderabad, India, January 6-
12, 2007, pages 877?882.
Ross D. King, Kenneth E. Whelan, Ffion M.
Jones, Philip G. K. Reiser, Christopher H. Bryant,
Stephen H. Muggleton, Douglas B. Kell, and
Stephen G. Oliver. 2004. Functional genomic hy-
pothesis generation and experimentation by a robot
scientist. Nature, 427:247?252, 15 January.
David D. Lewis and William A. Gale. 1994. A se-
quential algorithm for training text classifiers. In SI-
GIR ?94: Proceedings of the 17th annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 3?12, New
York, NY, USA. Springer-Verlag New York, Inc.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based
machine translation. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
135?139, Athens, Greece, March. Association for
Computational Linguistics.
Francois Mairesse, Milica Gasic, Filip Jurcicek, Simon
Keizer, Jorge Prombonas, Blaise Thomson, Kai Yu,
and Steve Young. 2010. Phrase-based statistical
language generation using graphical models and ac-
tive learning. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), Uppsala, Sweden, July. Association
for Computational Linguistics.
Grace Ngai and David Yarowsky. 2000. Rule writ-
ing or annotation: cost-efficient resource usage for
base noun phrase chunking. In Proceedings of the
38th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Miles Osborne and Jason Baldridge. 2004. Ensemble-
based active learning for parse selection. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Main Proceedings, pages
89?96, Boston, Massachusetts, USA, May 2 - May
7. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Manabu Sassano. 2002. An empirical study of active
learning with support vector machines for japanese
word segmentation. In ACL ?02: Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 505?512, Morristown, NJ,
USA. Association for Computational Linguistics.
Greg Schohn and David Cohn. 2000. Less is more:
Active learning with support vector machines. In
Proc. 17th International Conf. on Machine Learn-
ing, pages 839?846. Morgan Kaufmann, San Fran-
cisco, CA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast ? but is it
good? evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 254?263, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
Katrin Tomanek and Udo Hahn. 2009. Semi-
supervised active learning for sequence labeling. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 1039?1047, Suntec, Singapore,
August. Association for Computational Linguistics.
Simon Tong and Daphne Koller. 2002. Support vec-
tor machine active learning with applications to text
classification. Journal of Machine Learning Re-
search (JMLR), 2:45?66.
David Vickrey, Oscar Kipersztok, and Daphne Koller.
2010. An active learning approach to finding related
terms. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), Uppsala, Sweden, July. Association for
Computational Linguistics.
863
Andreas Vlachos. 2008. A stopping criterion for
active learning. Computer Speech and Language,
22(3):295?312.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the NAACL-2006 Workshop
on Statistical Machine Translation (WMT06), New
York, New York.
864
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 208?211,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using Mechanical Turk to Build Machine Translation Evaluation Sets
Michael Bloodgood
Human Language Technology
Center of Excellence
Johns Hopkins University
bloodgood@jhu.edu
Chris Callison-Burch
Center for Language and
Speech Processing
Johns Hopkins University
ccb@cs.jhu.edu
Abstract
Building machine translation (MT) test sets is
a relatively expensive task. As MT becomes
increasingly desired for more and more lan-
guage pairs and more and more domains, it
becomes necessary to build test sets for each
case. In this paper, we investigate using Ama-
zon?s Mechanical Turk (MTurk) to make MT
test sets cheaply. We find that MTurk can
be used to make test sets much cheaper than
professionally-produced test sets. More im-
portantly, in experiments with multiple MT
systems, we find that the MTurk-produced
test sets yield essentially the same conclu-
sions regarding system performance as the
professionally-produced test sets yield.
1 Introduction
Machine translation (MT) research is empirically
evaluated by comparing system output against refer-
ence human translations, typically using automatic
evaluation metrics. One method for establishing a
translation test set is to hold out part of the training
set to be used for testing. However, this practice typ-
ically overestimates system quality when compared
to evaluating on a test set drawn from a different do-
main. Therefore, it?s necessary to make new test sets
not only for new language pairs but also for new do-
mains.
Creating reasonable sized test sets for new do-
mains can be expensive. For example, the Workshop
on Statistical Machine Translation (WMT) uses a
mix of non-professional and professional translators
to create the test sets for its annual shared translation
tasks (Callison-Burch et al, 2008; Callison-Burch
et al, 2009). For WMT09, the total cost of creat-
ing the test sets consisting of roughly 80,000 words
across 3027 sentences in seven European languages
was approximately $39,800 USD, or slightly more
than $0.08 USD/word. For WMT08, creating test
sets consisting of 2,051 sentences in six languages
was approximately $26,500 USD or slightly more
than $0.10 USD/word.
In this paper we examine the use of Amazon?s
Mechanical Turk (MTurk) to create translation test
sets for statistical machine translation research.
Snow et al (2008) showed that MTurk can be useful
for creating data for a variety of NLP tasks, and that
a combination of judgments from non-experts can
attain expert-level quality in many cases. Callison-
Burch (2009) showed that MTurk could be used for
low-cost manual evaluation of machine translation
quality, and suggested that it might be possible to
use MTurk to create MT test sets after an initial pi-
lot study where turkers (the people who complete
the work assignments posted on MTurk) produced
translations of 50 sentences in five languages.
This paper explores this in more detail by ask-
ing turkers to translate the Urdu sentences of the
Urdu-English test set used in the 2009 NIST Ma-
chine Translation Evaluation Workshop. We evalu-
ate multiple MT systems on both the professionally-
produced NIST2009 test set and our MTurk-
produced test set and find that the MTurk-produced
test set yields essentially the same conclusions about
system performance as the NIST2009 set yields.
208
2 Gathering the Translations via
Mechanical Turk
The NIST2009 Urdu-English test set1 is a pro-
fessionally produced machine translation evalua-
tion set, containing four human-produced reference
translations for each of 1792 Urdu sentences. We
posted the 1792 Urdu sentences onMTurk and asked
for translations into English. We charged $0.10 USD
per translation, giving us a total translation cost of
$179.20 USD. A challenge we encountered during
this data collection was that many turkers would
cheat, giving us fake translations. We noticed that
many turkers were pasting the Urdu into an online
machine translation system and giving us the output
as their response even though our instructions said
not to do this. We manually monitored for this and
rejected these responses and blocked these workers
from computing any of our future work assignments.
In the future, we plan to combat this in a more prin-
cipled manner by converting our Urdu sentences into
an image and posting the images. This way, the
cheating turkers will not be able to cut and paste into
a machine translation system.
We also noticed that many of the translations had
simple mistakes such as misspellings and typos. We
wanted to investigate whether these would decrease
the value of our test set so we did a second phase
of data collection where we posted the translations
we gathered and asked turkers (likely to be com-
pletely different people than the ones who provided
the initial translations) to correct simple grammar
mistakes, misspellings, and typos. For this post-
editing phase, we paid $0.25 USD per ten sentences,
giving a total post-editing cost of $44.80 USD.
In summary, we built two sets of reference trans-
lations, one with no editing, and one with post-
editing. In the next section, we present the results
of experiments that test how effective these test sets
are for evaluating MT systems.
3 Experimental Results
A main purpose of an MT test set is to evaluate vari-
ousMT systems? performances relative to each other
and assist in drawing conclusions about the relative
1http://www.itl.nist.gov/iad/894.01/tests/mt/2009/
ResultsRelease/currentUrdu.html
quality of the translations produced by the systems.2
Therefore, if a given system, say System A, out-
performs another given system, say System B, on
a high-quality professionally-produced test set, then
we would want to see that System A also outper-
forms System B on our MTurk-produced test set. It
is also desirable that the magnitudes of the differ-
ences in performance between systems also be main-
tained.
In order to measure the differences in perfor-
mance, using the differences in the absolute mag-
nitudes of the BLEU scores will not work well be-
cause the magnitudes of the BLEU scores are af-
fected by many factors of the test set being used,
such as the number of reference translations per for-
eign sentence. For determining performance differ-
ences between systems and especially for compar-
ing them across different test sets, we use percentage
of baseline performance. To compute percentage of
baseline performance, we designate one system as
the baseline system and use percentage of that base-
line system?s performance. For example, Table 1
shows both absolute BLEU scores and percentage
performance for three MT systems when tested on
five different test sets. The first test set in the table
is the NIST-2009 set with all four reference trans-
lations per Urdu sentence. The next four test sets
use only a single reference translation per Urdu sen-
tence (ref 1 uses the first reference translation only,
ref 2 the second only, etc.). Note that the BLEU
scores for the single-reference translation test sets
are much lower than for the test set with all four ref-
erence translations and the difference in the absolute
magnitudes of the BLEU scores between the three
different systems are different for the different test
sets. However, the percentage performance of the
MT systems is maintained (both the ordering of the
systems and the amount of the difference between
them) across the different test sets.
We evaluated three different MT systems on the
NIST2009 test set and on our two MTurk-produced
test sets (MTurk-NoEditing and MTurk-Edited).
Two of the MT systems (ISI Syntax (Galley et al,
2Another useful purpose would be to get some absolute
sense of the quality of the translations but that seems out of
reach currently as the values of BLEU scores (the defacto stan-
dard evaluation metric) are difficult to map to precise levels of
translation quality.
209
Eval ISI JHU Joshua
Set (Syntax) (Syntax) (Hier.)
NIST-2009 33.10 32.77 26.65
(4 refs) 100% 99.00% 80.51%
NIST-2009 17.22 16.98 14.25
(ref 1) 100% 98.61% 82.75%
NIST-2009 17.76 17.14 14.69
(ref 2) 100% 96.51% 82.71%
NIST-2009 16.94 16.54 13.80
(ref 3) 100% 97.64% 81.46%
NIST-2009 13.63 13.67 11.05
(ref 4) 100% 100.29% 81.07%
Table 1: This table shows three MT systems evaluated
on five different test sets. For each system-test set pair,
two numbers are displayed. The top number is the BLEU
score for that system when using that test set. For ex-
ample, ISI-Syntax tested on the NIST-2009 test set has
a BLEU score of 33.10. The bottom number is the per-
centage of baseline system performance that is achieved.
ISI-Syntax (the highest-performing system on NIST2009
to our knowledge) is used as the baseline. Thus, it will
always have 100% as the percentage performance for all
of the test sets. To illustrate computing the percentage
performance for the other systems, consider for JHU-
Syntax tested on NIST2009, that its BLEU score of 32.77
divided by the BLEU score of the baseline system is
32.77/33.10 ? 99.00%
2004; Galley et al, 2006) and JHU Syntax (Li et al,
2009) augmented with (Zollmann and Venugopal,
2006)) were chosen because they represent state-
of-the-art performance, having achieved the highest
scores on NIST2009 to our knowledge. They also
have very similar performance on NIST2009 so we
want to see if that similar performance is maintained
as we evaluate on our MTurk-produced test sets.
The third MT system (Joshua-Hierarchical) (Li et
al., 2009), an open source implementation of (Chi-
ang, 2007), was chosen because though it is a com-
petitive system, it had clear, markedly lower perfor-
mance on NIST2009 than the other two systems and
we want to see if that difference in performance is
also maintained if we were to shift evaluation to our
MTurk-produced test sets.
Table 2 shows the results. There are a number
of observations to make. One is that the absolute
magnitude of the BLEU scores is much lower for
all systems on the MTurk-produced test sets than on
Eval ISI JHU Joshua
Set (Syntax) (Syntax) (Hier.)
NIST- 33.10 32.77 26.65
2009 100% 99.00% 80.51%
MTurk- 13.81 13.93 11.10
NoEditing 100% 100.87% 80.38%
MTurk- 14.16 14.23 11.68
Edited 100% 100.49% 82.49%
Table 2: This table shows three MT systems evaluated us-
ing the official NIST2009 test set and the two test sets we
constructed (MTurk-NoEditing and MTurk-Edited). For
each system-test set pair, two numbers are displayed. The
top number is the BLEU score for that system when using
that test set. For example, ISI-Syntax tested on the NIST-
2009 test set has a BLEU score of 33.10. The bottom
number is the percentage of baseline system performance
that is achieved. ISI-Syntax (the highest-performing sys-
tem on NIST2009 to our knowledge) is used as the base-
line.
the NIST2009 test set. This is primarily because the
NIST2009 set had four translations per foreign sen-
tence whereas the MTurk-produced sets only have
one translation per foreign sentence. Due to this
different scale of BLEU scores, we compare perfor-
mances using percentage of baseline performance.
We use the ISI Syntax system as the baseline since
it achieved the highest results on NIST2009. The
main observation of the results in Table 2 is that
both the relative performance of the various MT sys-
tems and the amount of the differences in perfor-
mance (in terms of percentage performance of the
baseline) are maintained when we use the MTurk-
produced test sets as when we use the NIST2009 test
set. In particular, we can see that whether using the
NIST2009 test set or the MTurk-produced test sets,
one would conclude that ISI Syntax and JHU Syn-
tax perform about the same and Joshua-Hierarchical
delivers about 80% of the performance of the two
syntax systems. The post-edited test set did not
yield different conclusions than the non-edited test
set yielded so the value of post-editing for test set
creation remains an open question.
4 Conclusions and Future Work
In conclusion, we have shown that it is feasible to
use MTurk to build MT evaluation sets at a sig-
210
nificantly reduced cost. But the large cost sav-
ings does not hamper the utility of the test set for
evaluating systems? translation quality. In exper-
iments, MTurk-produced test sets lead to essen-
tially the same conclusions about multiple MT sys-
tems? translation quality as much more expensive
professionally-produced MT test sets.
It?s important to be able to build MT test sets
quickly and cheaply because we need new ones for
new domains (as discussed in Section 1). Now that
we have shown the feasibility of using MTurk to
build MT test sets, in the future we plan to build
new MT test sets for specific domains (e.g., enter-
tainment, science, etc.) and release them to the com-
munity to spur work on domain-adaptation for MT.
We also envision using MTurk to collect addi-
tional training data to tune an MT system for a new
domain. It?s been shown that active learning can be
used to reduce training data annotation burdens for
a variety of NLP tasks (see, e.g., (Bloodgood and
Vijay-Shanker, 2009)). Therefore, in future work,
we plan to use MTurk combined with an active
learning approach to gather new data in the new do-
main to investigate improving MT performance for
specialized domains. But we?ll need new test sets in
the specialized domains to be able to evaluate the ef-
fectiveness of this line of research and therefore, we
will need to be able to build new test sets. In light of
the findings we presented in this paper, it seems we
can build those test sets using MTurk for relatively
low costs without sacrificing much in their utility for
evaluating MT systems.
Acknowledgements
This research was supported by the EuroMatrix-
Plus project funded by the European Commission,
by the DARPA GALE program under Contract No.
HR0011-06-2-0001, and the NSF under grant IIS-
0713448. Thanks to Amazon Mechanical Turk for
providing a $100 credit.
References
Michael Bloodgood and K Vijay-Shanker. 2009. Taking
into account the differences between actively and pas-
sively acquired data: The case of active learning with
support vector machines for imbalanced datasets. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 137?140, Boulder, Colorado, June. Association
for Computational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation (WMT08), Colmbus, Ohio.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation (WMT09), March.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing, pages 286?295, Singapore, August. Association
for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of the Human Language Technology Con-
ference of the North American chapter of the Asso-
ciation for Computational Linguistics (HLT/NAACL-
2004), Boston, Massachusetts.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics (ACL-
CoLing-2006), Sydney, Australia.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In Proceedings of the Fourth Work-
shop on Statistical Machine Translation, pages 135?
139, Athens, Greece, March. Association for Compu-
tational Linguistics.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but is it
good? Evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of EMNLP-2008,
Honolulu, Hawaii.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the NAACL-2006 Workshop on Statis-
tical Machine Translation (WMT-06), New York, New
York.
211
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 78?86,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
A random forest system combination approach for error detection in
digital dictionaries
Michael Bloodgood and Peng Ye and Paul Rodrigues
and David Zajic and David Doermann
University of Maryland
College Park, MD
meb@umd.edu, pengye@umiacs.umd.edu, prr@umd.edu,
dzajic@casl.umd.edu, doermann@umiacs.umd.edu
Abstract
When digitizing a print bilingual dictionary,
whether via optical character recognition or
manual entry, it is inevitable that errors are
introduced into the electronic version that is
created. We investigate automating the pro-
cess of detecting errors in an XML repre-
sentation of a digitized print dictionary us-
ing a hybrid approach that combines rule-
based, feature-based, and language model-
based methods. We investigate combin-
ing methods and show that using random
forests is a promising approach. We find
that in isolation, unsupervised methods ri-
val the performance of supervised methods.
Random forests typically require training
data so we investigate how we can apply
random forests to combine individual base
methods that are themselves unsupervised
without requiring large amounts of training
data. Experiments reveal empirically that
a relatively small amount of data is suffi-
cient and can potentially be further reduced
through specific selection criteria.
1 Introduction
Digital versions of bilingual dictionaries often
have errors that need to be fixed. For example,
Figures 1 through 5 show an example of an er-
ror that occurred in one of our development dic-
tionaries and how the error should be corrected.
Figure 1 shows the entry for the word ?turfah? as
it appeared in the original print copy of (Qureshi
and Haq, 1991). We see this word has three senses
with slightly different meanings. The third sense
is ?rare?. In the original digitized XML version
of (Qureshi and Haq, 1991) depicted in Figure 2,
this was misrepresented as not being the meaning
Figure 1: Example dictionary entry
Figure 2: Example of error in XML
of ?turfah? but instead being a usage note that fre-
quency of use of the third sense was rare. Figure 3
shows the tree corresponding to this XML repre-
sentation. The corrected digital XML representa-
tion is depicted in Figure 4 and the corresponding
corrected tree is shown in Figure 5.
Zajic et al (2011) presented a method for re-
pairing a digital dictionary in an XML format us-
ing a dictionary markup language called DML. It
remains time-consuming and error-prone however
to have a human read through and manually cor-
rect a digital version of a dictionary, even with
languages such as DML available. We therefore
investigate automating the detection of errors.
We investigate the use of three individual meth-
ods. The first is a supervised feature-based
method trained using SVMs (Support Vector Ma-
chines). The second is a language-modeling
78
.ENTRY
. .? ? ?
.
.SENSE
.USG
.rare
.? ? ?FORM
. .PRON
.t?r?fah
ORTH
.????
Figure 3: Tree structure of error
Figure 4: Example of error in XML, fixed
.ENTRY
. .? ? ?
.
.SENSE
.TRANS
.TR
.rare
.
.? ? ?FORM
. .PRON
.t?r?fah
ORTH
.????
Figure 5: Tree structure of error, fixed
method that replicates the method presented in
(Rodrigues et al, 2011). The third is a simple
rule inference method. The three individual meth-
ods have different performances. So we investi-
gate how we can combine the methods most effec-
tively. We experiment with majority vote, score
combination, and random forest methods and find
that random forest combinations work the best.
For many dictionaries, training data will not be
available in large quantities a priori and therefore
methods that require only small amounts of train-
ing data are desirable. Interestingly, for automati-
cally detecting errors in dictionaries, we find that
the unsupervised methods have performance that
rivals that of the supervised feature-based method
trained using SVMs. Moreover, when we com-
bine methods using the random forest method, the
combination of unsupervised methods works bet-
ter than the supervised method in isolation and al-
most as well as the combination of all available
methods. A potential drawback of using the ran-
dom forest combination method however is that it
requires training data. We investigated how much
training data is needed and find that the amount
of training data required is modest. Furthermore,
by selecting the training data to be labeled with
the use of specific selection methods reminiscent
of active learning, it may be possible to train the
random forest system combination method with
even less data without sacrificing performance.
In section 2 we discuss previous related work
and in section 3 we explain the three individual
methods we use for our application. In section 4
we explain the three methods we explored for
combining methods; in section 5 we present and
discuss experimental results and in section 6 we
conclude and discuss future work.
2 Related Work
Classifier combination techniques can be broadly
classified into two categories: mathematical and
behavioral (Tulyakov et al, 2008). In the first
category, functions or rules combine normalized
classifier scores from individual classifiers. Ex-
amples of techniques in this category include Ma-
jority Voting (Lam and Suen, 1997), as well as
simple score combination rules such as: sum rule,
min rule, max rule and product rule (Kittler et al,
1998; Ross and Jain, 2003; Jain et al, 2005). In
the second category, the output of individual clas-
sifiers are combined to form a feature vector as
79
the input to a generic classifier such as classifi-
cation trees (P. and Chollet, 1999; Ross and Jain,
2003) or the k-nearest neighbors classifier (P. and
Chollet, 1999). Our method falls into the second
category, where we use a random forest for sys-
tem combination.
The random forest method is described in
(Breiman, 2001). It is an ensemble classifier con-
sisting of a collection of decision trees (called a
random forest) and the output of the random for-
est is the mode of the classes output by the indi-
vidual trees. Each single tree is trained as follows:
1) a random set of samples from the initial train-
ing set is selected as a training set and 2) at each
node of the tree, a random subset of the features is
selected, and the locally optimal split is based on
only this feature subset. The tree is fully grown
without pruning. Ma et al (2005) used random
forests for combining scores of several biometric
devices for identity verification and have shown
encouraging results. They use all fully supervised
methods. In contrast, we explore minimizing the
amount of training data needed to train a random
forest of unsupervised methods.
The use of active learning in order to re-
duce training data requirements without sacri-
ficing model performance has been reported on
extensively in the literature (e.g., (Seung et al,
1992; Cohn et al, 1994; Lewis and Gale, 1994;
Cohn et al, 1996; Freund et al, 1997)). When
training our random forest combination of indi-
vidual methods that are themselves unsupervised,
we explore how to select the data so that only
small amounts of training data are needed because
for many dictionaries, gathering training data may
be expensive and labor-intensive.
3 Three Single Method Approaches for
Error Detection
Before we discuss our approaches for combining
systems, we briefly explain the three individual
systems that form the foundation of our combined
system.
First, we use a supervised approach where we
train a model using SVMlight (Joachims, 1999)
with a linear kernel and default regularization pa-
rameters. We use a depth first traversal of the
XML tree and use unigrams and bigrams of the
tags that occur as features for each subtree to
make a classification decision.
We also explore two unsupervised approaches.
The first unsupervised approach learns rules for
when to classify nodes as errors or not. The rule-
based method computes an anomaly score based
on the probability of subtree structures. Given
a structure A and its probability P(A), the event
that A occurs has anomaly score 1-P(A) and the
event that A does not occur has anomaly score
P(A). The basic idea is if a certain structure hap-
pens rarely, i.e. P(A) is very small, then the oc-
currence of A should have a high anomaly score.
On the other hand, if A occurs frequently, then
the absence of A indicates anomaly. To obtain
the anomaly score of a tree, we simply take the
maximal scores of all events induced by subtrees
within this tree.
The second unsupervised approach uses a reim-
plementation of the language modeling method
described in (Rodrigues et al, 2011). Briefly,
this methods works by calculating the probabil-
ity a flattened XML branch can occur, given a
probability model trained on the XML branches
from the original dictionary. We used (Stolcke,
2002) to generate bigram models using Good Tur-
ing smoothing and Katz back off, and evaluated
the log probability of the XML branches, ranking
the likelihood. The first 1000 branches were sub-
mitted to the hybrid system marked as an error,
and the remaining were submitted as a non-error.
Results for the individual classifiers are presented
in section 5.
4 Three Methods for Combining
Systems
We investigate three methods for combining the
three individual methods. As a baseline, we in-
vestigate simple majority vote. This method takes
the classification decisions of the three methods
and assigns the final classification as the classifi-
cation that the majority of the methods predicted.
A drawback of majority vote is that it does not
weight the votes at all. However, it might make
sense to weight the votes according to factors such
as the strength of the classification score. For ex-
ample, all of our classifiers make binary decisions
but output scores that are indicative of the confi-
dence of their classifications. Therefore we also
explore a score combination method that consid-
ers these scores. Since measures from the differ-
ent systems are in different ranges, we normal-
ize these measurements before combining them
(Jain et al, 2005). We use z-score which com-
80
putes the arithmetic mean and standard deviation
of the given data for score normalization. We then
take the summation of normalized measures as
the final measure. Classification is performed by
thresholding this final measure.1
Another approach would be to weight them by
the performance level of the various constituent
classifiers in the ensemble. Weighting based on
performance level of the individual classifiers is
difficult because it would require extra labeled
data to estimate the various performance lev-
els. It is not clear how to translate the differ-
ent performance estimates into weights, or how
to have those weights interact with weights based
on strengths of classification. Therefore, we did
not weigh based on performance level explicitly.
We believe that our third combination method,
the use of random forests, implicitly cap-
tures weighting based on performance level and
strengths of classifications. Our random forest ap-
proach uses three features, one for each of the in-
dividual systems we use. With random forests,
strengths of classification are taken into account
because they form the values of the three fea-
tures we use. In addition, the performance level
is taken into account because the training data
used to train the decision trees that form the for-
est help to guide binning of the feature values into
appropriate ranges where classification decisions
are made correctly. This will be discussed further
in section 5.
5 Experiments
This section explains the details of the experi-
ments we conducted testing the performance of
the various individual and combined systems.
Subsection 5.1 explains the details of the data we
experiment on; subsection 5.2 provides a sum-
mary of the main results of our experiments; and
subsection 5.3 discusses the results.
5.1 Experimental Setup
We obtained the data for our experiments using
a digitized version of (Qureshi and Haq, 1991),
the same Urdu-English dictionary that Zajic et
al. (2011) had used. Zajic et al (2011) pre-
sented DML, a programming language used to
fix errors in XML documents that contain lexico-
graphic data. A team of language experts used
1In our experiments we used 0 as the threshold.
Recall Precision F1-Measure Accuracy
LM 11.97 89.90 21.13 57.53
RULE 99.79 70.83 82.85 80.37
FV 35.34 93.68 51.32 68.14
Table 1: Performance of individual systems at
ENTRY tier.
DML to correct errors in a digital, XML repre-
sentation of the Kitabistan Urdu dictionary. The
current research compared the source XML doc-
ument and the DML commands to identify the el-
ements that the language experts decided to mod-
ify. We consider those elements to be errors. This
is the ground truth used for training and evalua-
tion. We evaluate at two tiers, corresponding to
two node types in the XML representation of the
dictionary: ENTRY and SENSE. The example de-
picted in Figures 1 through 5 shows an example of
SENSE. The intuition of the tier is that errors are
detectable (or learnable) from observing the ele-
ments within a tier, and do not cross tier bound-
aries. These tiers are specific to the Kitabistan
Urdu dictionary, and we selected them by observ-
ing the data. A limitation of our work is that we do
not know at this time whether they are generally
useful across dictionaries. Future work will be
to automatically discover the meaningful evalua-
tion tiers for a new dictionary. After this process,
we have a dataset with 15,808 Entries, of which
47.53% are marked as errors and 78,919 Senses,
of which 10.79% are marked as errors. We per-
form tenfold cross-validation in all experiments.
In our random forest experiments, we use 12 de-
cision trees, each with only 1 feature.
5.2 Results
This section presents experimental results, first
for individual systems and then for combined sys-
tems.
5.2.1 Performance of individual systems
Tables 1 and 2 show the performance of lan-
guage modeling-based method (LM), rule-based
method (RULE) and the supervised feature-based
method (FV) at different tiers. As can be seen,
at the ENTRY tier, RULE obtains the highest F1-
Measure and accuracy, while at the SENSE tier,
FV performs the best.
81
Recall Precision F1-Measure Accuracy
LM 9.85 94.00 17.83 90.20
RULE 84.59 58.86 69.42 91.96
FV 72.44 98.66 83.54 96.92
Table 2: Performance of individual systems at
SENSE tier.
5.2.2 Improving individual systems using
random forests
In this section, we show that by applying ran-
dom forests on top of the output of individual sys-
tems, we can have gains (absolute gains, not rel-
ative) in accuracy of 4.34% to 6.39% and gains
(again absolute, not relative) in F1-measure of
3.64% to 11.39%. Tables 3 and 4 show our ex-
perimental results at ENTRY and SENSE tiers
when applying random forests with the rule-based
method.2 These results are all obtained from 100
iterations of the experiments with different parti-
tions of the training data chosen at each iteration.
Mean values of different evaluation measures and
their standard deviations are shown in these ta-
bles. We change the percentage of training data
and repeat the experiments to see how the amount
of training data affects performance.
It might be surprising to see the gains in per-
formance that can be achieved by using a ran-
dom forest of decision trees created using only
the rule-based scores as features. To shed light
on why this is so, we show the distribution of
RULE-based output scores for anomaly nodes and
clean nodes in Figure 6. They are well separated
and this explains why RULE alone can have good
performance. Recall RULE classifies nodes with
anomaly scores larger than 0.9 as errors. How-
ever, in Figure 6, we can see that there are many
clean nodes with anomaly scores larger than 0.9.
Thus, the simple thresholding strategy will bring
in errors. Applying random forest will help us
identify these errorful regions to improve the per-
formance. Another method for helping to identify
these errorful regions and classify them correctly
is to apply random forest of RULE combined with
the other methods, which we will see will even
further boost the performance.
2We also applied random forests to our language mod-
eling and feature-based methods, and saw similar gains in
performance.
0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
0
500
1000
1500
output score of rule-based system
o
c
c
u
r
r
e
n
c
e
s
 
 
anomaly
clean
Figure 6: Output anomalies score from RULE
(ENTRY tier).
5.2.3 System combination
In this section, we explore different methods
for combining measures from the three systems.
Table 5 shows the results of majority voting and
score combination at the ENTRY tier. As can
be seen, majority voting performs poorly. This
may be due to the fact that the performances of
the three systems are very different. RULE sig-
nificantly outperforms the other two systems, and
as discussed in Section 4 neither majority voting
nor score combination weights this higher perfor-
mance appropriately.
Tables 6 and 7 show the results of combining
RULE and LM. This is of particular interest since
these two systems are unsupervised. Combin-
ing these two unsupervised systems works better
than the individual methods, including supervised
methods. Tables 8 and 9 show the results for com-
binations of all available systems. This yields the
highest performance, but only slightly higher than
the combination of only unsupervised base meth-
ods.
The random forest combination technique does
require labeled data even if the underlying base
methods are unsupervised. Based on the ob-
servation in Figure 6, we further study whether
choosing more training data from the most error-
ful regions will help to improve the performance.
Experimental results in Table 10 show how the
choice of training data affects performance. It
appears that there may be a weak trend toward
higher performance when we force the selection
of the majority of the training data to be from
ENTRY nodes whose RULE anomaly scores are
82
Training % Recall Precision F1-Measure Accuracy
0.1 78.17( 14.83) 75.87( 3.96) 76.18( 7.99) 77.68( 5.11)
1 82.46( 4.81) 81.34( 2.14) 81.79( 2.20) 82.61( 1.69)
10 87.30( 1.96) 84.11( 1.29) 85.64( 0.46) 86.10( 0.35)
50 89.19( 1.75) 83.99( 1.20) 86.49( 0.34) 86.76( 0.28)
Table 3: Mean and std of evaluation measures from 100 iterations of experiments using RULE+RF.
(ENTRY tier)
Training % Recall Precision F1-Measure Accuracy
0.1 60.22( 12.95) 69.66( 9.54) 63.29( 7.92) 92.61( 1.57)
1 70.28( 3.48) 86.26( 3.69) 77.31( 1.39) 95.55( 0.25)
10 71.52( 1.23) 91.26( 1.39) 80.18( 0.41) 96.18( 0.07)
50 72.11( 0.75) 91.90( 0.64) 80.81( 0.39) 96.30( 0.06)
Table 4: Mean and std of evaluation measures from 100 iterations of experiments using RULE+RF.
(SENSE tier)
larger than 0.9. However, the magnitudes of the
observed differences in performance are within a
single standard deviation so it remains for future
work to determine if there are ways to select the
training data for our random forest combination
in ways that substantially improve upon random
selection.
5.3 Discussion
Majority voting (at the entry level) performs
poorly, since the performance of the three individ-
ual systems are very different and majority voting
does not weight votes at all. Score combination
is a type of weighted voting. It takes into account
the confidence level of output from different sys-
tems, which enables it to perform better than ma-
jority voting. However, score combination does
not take into account the performance levels of
the different systems, and we believe this limits its
performance compared with random forest com-
binations.
Random forest combinations perform the best,
but the cost is that it is a supervised combination
method. We investigated how the amount of train-
ing data affects the performance, and found that a
small amount of labeled data is all that the random
forest needs in order to be successful. Moreover,
although this requires further exploration, there is
weak evidence that the size of the labeled data can
potentially be reduced by choosing it carefully
from the region that is expected to be most error-
ful. For our application with a rule-based system,
this is the high-anomaly scoring region because
although it is true that anomalies are often errors,
it is also the case that some structures occur rarely
but are not errorful.
RULE+LM with random forest is a little bet-
ter than RULE with random forest, with gain of
about 0.7% on F1-measure when evaluated at the
ENTRY level using 10% data for training.
An examination of examples that are marked as
being errors in our ground truth but that were not
detected to be errors by any of our systems sug-
gests that some examples are decided on the ba-
sis of features not yet considered by any system.
For example, in Figure 7 the second FORM is
well-formed structurally, but the Urdu text in the
first FORM is the beginning of the phrase translit-
erated in the second FORM. Automatic systems
detected that the first FORM was an error, how-
ever did not mark the second FORM as an error
whereas our ground truth marked both as errors.
Examination of false negatives also revealed
cases where the systems were correct that there
was no error but our ground truth wrongly indi-
cated that there was an error. These were due to
our semi-automated method for producing ground
truth that considers elements mentioned in DML
commands to be errors. We discovered instances
in which merely mentioning an element in a DML
command does not imply that the element is an er-
ror. These cases are useful for making refinements
to how ground truth is generated from DML com-
mands.
Examination of false positives revealed two
categories. One was where the element is indeed
an error but was not marked as an element in our
ground truth because it was part of a larger error
83
Method Recall Precision F1-Measure Accuracy
Majority voting 36.71 90.90 52.30 68.18
Score combination 76.48 75.82 76.15 77.23
Table 5: LM+RULE+FV (ENTRY tier)
Training % Recall Precision F1-Measure Accuracy
0.1 77.43( 15.14) 72.77( 6.03) 74.26( 8.68) 75.32( 6.71)
1 86.50( 3.59) 80.41( 1.95) 83.27( 1.33) 83.51( 1.11)
10 88.12( 1.12) 84.65( 0.57) 86.34( 0.46) 86.76( 0.39)
50 89.12( 0.62) 87.39( 0.56) 88.25( 0.30) 88.72( 0.29)
Table 6: System combination based on random forest (LM+RULE). (ENTRY tier, mean (std))
Training % Recall Precision F1-Measure Accuracy
0.1 65.85( 12.70) 71.96( 7.63) 67.68( 7.06) 93.38( 1.03)
1 80.29( 3.58) 84.97( 3.13) 82.45( 1.36) 96.31( 0.28)
10 82.68( 2.49) 90.91( 2.37) 86.53( 0.41) 97.22( 0.07)
50 83.22( 2.43) 92.21( 2.29) 87.42( 0.35) 97.42( 0.04)
Table 7: System combination based on random forest (LM+RULE). (SENSE tier, mean (std))
Training % Recall Precision F1-Measure Accuracy
20 91.57( 0.55) 87.77( 0.43) 89.63( 0.23) 89.93( 0.22)
50 92.04( 0.54) 88.85( 0.48) 90.41( 0.29) 90.72( 0.28)
Table 8: System combination based on random forest (LM+RULE+FV). (ENTRY tier, mean (std))
Training % Recall Precision F1-Measure Accuracy
20 86.47( 1.01) 90.67( 1.02) 88.51( 0.26) 97.58( 0.06)
50 86.50( 0.81) 92.04( 0.85) 89.18( 0.30) 97.73( 0.06)
Table 9: System combination based on random forest (LM+RULE+FV). (SENSE tier, mean (std))
Recall Precision F1-Measure Accuracy
50% 85.40( 4.65) 80.71( 3.49) 82.82( 1.57) 82.63( 1.54)
70% 86.13( 3.94) 80.97( 2.64) 83.36( 1.33) 83.30( 1.21)
90% 85.77( 3.61) 81.82( 2.72) 83.65( 1.45) 83.69( 1.35)
95% 85.93( 3.46) 82.14( 2.98) 83.89( 1.32) 83.94( 1.18)
random 86.50( 3.59) 80.41( 1.95) 83.27( 1.33) 83.51( 1.11)
Table 10: Effect of choice of training data based on rule based method (Mean evaluation measures
from 100 iterations of experiments using RULE+LM at ENTRY tier). We choose 1% of the data for
training and the first column in the table specifies the percentage of training data chosen from Entries
with anomalous score larger than 0.9.
84
Figure 7: Example of error in XML
that got deleted and therefore no DML command
ever mentioned the smaller element but lexicog-
raphers upon inspection agree that the smaller el-
ement is indeed errorful. The other category was
where there were actual errors that the dictionary
editors didn?t repair with DML but that should
have been repaired.
A major limitation of our work is testing how
well it generalizes to detecting errors in other dic-
tionaries besides the Urdu-English one (Qureshi
and Haq, 1991) that we conducted our experi-
ments on.
6 Conclusions
We explored hybrid approaches for the applica-
tion of automatically detecting errors in digitized
copies of dictionaries. The base methods we
explored consisted of a variety of unsupervised
and supervised methods. The combination meth-
ods we explored also consisted of some methods
which required labeled data and some which did
not.
We found that our base methods had differ-
ent levels of performance and with this scenario
majority voting and score combination methods,
though appealing since they require no labeled
data, did not perform well since they do not
weight votes well.
We found that random forests of decision trees
was the best combination method. We hypothe-
size that this is due to the nature of our task and
base systems. Random forests were able to help
tease apart the high-error region (where anoma-
lies take place). A drawback of random forests
as a combination method is that they require la-
beled data. However, experiments reveal empiri-
cally that a relatively small amount of data is suf-
ficient and the amount might be able to be further
reduced through specific selection criteria.
Acknowledgments
This material is based upon work supported, in
whole or in part, with funding from the United
States Government. Any opinions, findings and
conclusions, or recommendations expressed in
this material are those of the author(s) and do not
necessarily reflect the views of the University of
Maryland, College Park and/or any agency or en-
tity of the United States Government. Nothing
in this report is intended to be and shall not be
treated or construed as an endorsement or recom-
mendation by the University of Maryland, United
States Government, or the authors of the product,
process, or service that is the subject of this re-
port. No one may use any information contained
or based on this report in advertisements or pro-
motional materials related to any company prod-
uct, process, or service or in support of other com-
mercial purposes.
References
Leo Breiman. 2001. Random forests. Machine
Learning, 45:5?32. 10.1023/A:1010933404324.
David A. Cohn, Les Atlas, and Richard Ladner. 1994.
Improving generalization with active learning. Ma-
chine Learning, 15:201?221.
David A. Cohn, Zoubin Ghahramani, and Michael I.
Jordan. 1996. Active learning with statistical mod-
els. Journal of Artificial Intelligence Research,
4:129?145.
Yoav Freund, H. Sebastian Seung, Eli Shamir, and
Naftali Tishby. 1997. Selective sampling using the
query by committee algorithm. Machine Learning,
28:133?168.
Anil K. Jain, Karthik Nandakumar, and Arun Ross.
2005. Score normalization in multimodal biometric
systems. Pattern Recognition, pages 2270?2285.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scho?lkopf, Christo-
pher J. Burges, and Alexander J. Smola, editors, Ad-
vances in Kernel Methods ? Support Vector Learn-
ing, chapter 11, pages 169?184. The MIT Press,
Cambridge, US.
J. Kittler, M. Hatef, R.P.W. Duin, and J. Matas.
1998. On combining classifiers. Pattern Analysis
and Machine Intelligence, IEEE Transactions on,
20(3):226 ?239, mar.
L. Lam and S.Y. Suen. 1997. Application of majority
voting to pattern recognition: an analysis of its be-
havior and performance. Systems, Man and Cyber-
netics, Part A: Systems and Humans, IEEE Trans-
actions on, 27(5):553 ?568, sep.
David D. Lewis and William A. Gale. 1994. A se-
quential algorithm for training text classifiers. In
SIGIR ?94: Proceedings of the 17th annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, pages 3?12,
85
New York, NY, USA. Springer-Verlag New York,
Inc.
Yan Ma, Bojan Cukic, and Harshinder Singh. 2005.
A classification approach to multi-biometric score
fusion. In AVBPA?05, pages 484?493.
Verlinde P. and G. Chollet. 1999. Comparing deci-
sion fusion paradigms using k-nn based classifiers,
decision trees and logistic regression in a multi-
modal identity verification application. In Proceed-
ings of the 2nd International Conference on Audio
and Video-Based Biometric Person Authentication
(AVBPA), pages 189?193.
Bashir Ahmad Qureshi and Abdul Haq. 1991. Stan-
dard Twenty First Century Urdu-English Dictio-
nary. Educational Publishing House, Delhi.
Paul Rodrigues, David Zajic, David Doermann,
Michael Bloodgood, and Peng Ye. 2011. Detect-
ing structural irregularity in electronic dictionaries
using language modeling. In Proceedings of the
Conference on Electronic Lexicography in the 21st
Century, pages 227?232, Bled, Slovenia, Novem-
ber. Trojina, Institute for Applied Slovene Studies.
Arun Ross and Anil Jain. 2003. Information fusion in
biometrics. Pattern Recognition Letters, 24:2115?
2125.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In COLT ?92: Proceedings of
the fifth annual workshop on Computational learn-
ing theory, pages 287?294, New York, NY, USA.
ACM.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proceedings of the Interna-
tional Conference on Spoken Language Processing.
Sergey Tulyakov, Stefan Jaeger, Venu Govindaraju,
and David Doermann. 2008. Review of classi-
fier combination methods. In Machine Learning in
Document Analysis and Recognition, volume 90 of
Studies in Computational Intelligence, pages 361?
386. Springer Berlin / Heidelberg.
David Zajic, Michael Maxwell, David Doermann, Paul
Rodrigues, and Michael Bloodgood. 2011. Cor-
recting errors in digital lexicographic resources us-
ing a dictionary manipulation language. In Pro-
ceedings of the Conference on Electronic Lexicog-
raphy in the 21st Century, pages 297?301, Bled,
Slovenia, November. Trojina, Institute for Applied
Slovene Studies.
86
Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 57?64, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
Statistical Modality Tagging
from Rule-based Annotations and Crowdsourcing
Vinodkumar Prabhakaran Michael Bloodgood Mona Diab
CS CASL CCLS
Columbia University University of Maryland Columbia University
vinod@cs.columbia.edu meb@umd.edu mdiab@ccls.columbia.edu
Bonnie Dorr Lori Levin Christine D. Piatko
CS and UMIACS LTI APL
University of Maryland Carnegie Mellon University Johns Hopkins University
bonnie@umiacs.umd.edu lsl@cs.cmu.edu christine.piatko@jhuapl.edu
Owen Rambow Benjamin Van Durme
CCLS HLTCOE
Columbia University Johns Hopkins University
rambow@ccls.columbia.edu vandurme@cs.jhu.edu
Abstract
We explore training an automatic modality
tagger. Modality is the attitude that a speaker
might have toward an event or state. One of
the main hurdles for training a linguistic tag-
ger is gathering training data. This is par-
ticularly problematic for training a tagger for
modality because modality triggers are sparse
for the overwhelming majority of sentences.
We investigate an approach to automatically
training a modality tagger where we first gath-
ered sentences based on a high-recall simple
rule-based modality tagger and then provided
these sentences to Mechanical Turk annotators
for further annotation. We used the resulting
set of training data to train a precise modality
tagger using a multi-class SVM that delivers
good performance.
1 Introduction
Modality is an extra-propositional component of
meaning. In John may go to NY, the basic propo-
sition is John go to NY and the word may indi-
cates modality. Van Der Auwera and Ammann
(2005) define core cases of modality: John must
go to NY (epistemic necessity), John might go to
NY (epistemic possibility), John has to leave now
(deontic necessity) and John may leave now (de-
ontic possibility). Many semanticists (e.g. Kratzer
(1981), Kratzer (1991), Kaufmann et al (2006)) de-
fine modality as quantification over possible worlds.
John might go means that there exist some possi-
ble worlds in which John goes. Another view of
modality relates more to a speakers attitude toward
a proposition (e.g. McShane et al (2004)).
Modality might be construed broadly to include
several types of attitudes that a speaker wants to ex-
press towards an event, state or proposition. Modal-
ity might indicate factivity, evidentiality, or senti-
ment (McShane et al, 2004). Factivity is related to
whether the speaker wishes to convey his or her be-
lief that the propositional content is true or not, i.e.,
whether it actually obtains in this world or not. It
distinguishes things that (the speaker believes) hap-
pened from things that he or she desires, plans, or
considers merely probable. Evidentiality deals with
the source of information and may provide clues to
the reliability of the information. Did the speaker
57
have firsthand knowledge of what he or she is re-
porting, or was it hearsay or inferred from indirect
evidence? Sentiment deals with a speaker?s positive
or negative feelings toward an event, state, or propo-
sition.
In this paper, we focus on the following five
modalities; we have investigated the belief/factivity
modality previously (Diab et al, 2009b; Prab-
hakaran et al, 2010), and we leave other modalities
to future work.
? Ability: can H do P?
? Effort: does H try to do P?
? Intention: does H intend P?
? Success: does H succeed in P?
? Want: does H want P?
We investigate automatically training a modality
tagger by using multi-class Support Vector Ma-
chines (SVMs). One of the main hurdles for training
a linguistic tagger is gathering training data. This is
particularly problematic for training a modality tag-
ger because modality triggers are sparse for the over-
whelming majority of the sentences. Baker et al
(2010) created a modality tagger by using a semi-
automatic approach for creating rules for a rule-
based tagger. A pilot study revealed that it can boost
recall well above the naturally occurring proportion
of modality without annotated data but with only
60% precision. We investigated an approach where
we first gathered sentences based on a simple modal-
ity tagger and then provided these sentences to an-
notators for further annotation, The resulting anno-
tated data also preserved the level of inter-annotator
agreement for each example so that learning algo-
rithms could take that into account during training.
Finally, the resulting set of annotations was used for
training a modality tagger using SVMs, which gave
a high precision indicating the success of this ap-
proach.
Section 2 discusses related work. Section 3 dis-
cusses our procedure for gathering training data.
Section 4 discusses the machine learning setup
and features used to train our modality tagger and
presents experiments and results. Section 5 con-
cludes and discusses future work.
2 Related Work
Previous related work includes TimeML (Sauri et
al., 2006), which involves modality annotation on
events, and Factbank (Sauri and Pustejovsky, 2009),
where event mentions are marked with degree of fac-
tuality. Modality is also important in the detection of
uncertainty and hedging. The CoNLL shared task in
2010 (Farkas et al, 2010) deals with automatic de-
tection of uncertainty and hedging in Wikipedia and
biomedical sentences.
Baker et al (2010) and Baker et al (2012) ana-
lyze a set of eight modalities which include belief,
require and permit, in addition to the five modalities
we focus on in this paper. They built a rule-based
modality tagger using a semi-automatic approach to
create rules. This earlier work differs from the work
described in this paper in that the our emphasis is on
the creation of an automatic modality tagger using
machine learning techniques. Note that the anno-
tation and automatic tagging of the belief modality
(i.e., factivity) is described in more detail in (Diab et
al., 2009b; Prabhakaran et al, 2010).
There has been a considerable amount of inter-
est in modality in the biomedical domain. Negation,
uncertainty, and hedging are annotated in the Bio-
scope corpus (Vincze et al, 2008), along with infor-
mation about which words are in the scope of nega-
tion/uncertainty. The i2b2 NLP Shared Task in 2010
included a track for detecting assertion status (e.g.
present, absent, possible, conditional, hypothetical
etc.) of medical problems in clinical records.1 Apos-
tolova et al (2011) presents a rule-based system for
the detection of negation and speculation scopes us-
ing the Bioscope corpus. Other studies emphasize
the importance of detecting uncertainty in medical
text summarization (Morante and Daelemans, 2009;
Aramaki et al, 2009).
Modality has also received some attention in the
context of certain applications. Earlier work de-
scribing the difficulty of correctly translating modal-
ity using machine translation includes (Sigurd and
Gawro?nska, 1994) and (Murata et al, 2005). Sig-
urd et al (1994) write about rule based frameworks
and how using alternate grammatical constructions
such as the passive can improve the rendering of the
modal in the target language. Murata et al (2005)
1https://www.i2b2.org/NLP/Relations/
58
analyze the translation of Japanese into English
by several systems, showing they often render the
present incorrectly as the progressive. The authors
trained a support vector machine to specifically han-
dle modal constructions, while our modal annotation
approach is a part of a full translation system.
The textual entailment literature includes modal-
ity annotation schemes. Identifying modalities is
important to determine whether a text entails a hy-
pothesis. Bar-Haim et al (2007) include polarity
based rules and negation and modality annotation
rules. The polarity rules are based on an indepen-
dent polarity lexicon (Nairn et al, 2006). The an-
notation rules for negation and modality of predi-
cates are based on identifying modal verbs, as well
as conditional sentences and modal adverbials. The
authors read the modality off parse trees directly us-
ing simple structural rules for modifiers.
3 Constructing Modality Training Data
In this section, we will discuss the procedure we
followed to construct the training data for build-
ing the automatic modality tagger. In a pilot study,
we obtained and ran the modality tagger described
in (Baker et al, 2010) on the English side of the
Urdu-English LDC language pack.2 We randomly
selected 1997 sentences that the tagger had labeled
as not having the Want modality and posted them on
Amazon Mechanical Turk (MTurk). Three differ-
ent Turkers (MTurk annotators) marked, for each of
the sentences, whether it contained the Want modal-
ity. Using majority rules as the Turker judgment,
95 (i.e., 4.76%) of these sentences were marked as
having a Want modality. We also posted 1993 sen-
tences that the tagger had labeled as having a Want
modality and only 1238 of them were marked by the
Turkers as having a Want modality. Therefore, the
estimated precision of this type of approach is only
around 60%.
Hence, we will not be able to use the (Baker et
al., 2010) tagger to gather training data. Instead,
our approach was to apply a simple tagger as a first
pass, with positive examples subsequently hand-
annotated using MTurk. We made use of sentence
data from the Enron email corpus,3 derived from the
2LDC Catalog No.: LDC2006E110.
3http://www-2.cs.cmu.edu/?enron/
version owing to Fiore and Heer,4 further processed
as described by (Roark, 2009).5
To construct the simple tagger (the first pass), we
used a lexicon of modality trigger words (e.g., try,
plan, aim, wish, want) constructed by Baker et al
(2010). The tagger essentially tags each sentence
that has a word in the lexicon with the corresponding
modality. We wrote a few simple obvious filters for a
handful of exceptional cases that arise due to the fact
that our sentences are from e-mail. For example, we
filtered out best wishes expressions, which otherwise
would have been tagged as Want because of the word
wishes.
The words that trigger modality occur with very
different frequencies. If one is not careful, the
training data may be dominated by only the com-
monly occurring trigger words and the learned tag-
ger would then be biased towards these words. In
order to ensure that our training data had a diverse
set of examples containing many lexical triggers and
not just a lot of examples with the same lexical trig-
ger, for each modality we capped the number of sen-
tences from a single trigger to be at most 50. After
we had the set of sentences selected by the simple
tagger, we posted them on MTurk for annotation.
The Turkers were asked to check a box indicat-
ing that the modality was not present in the sentence
if the given modality was not expressed. If they did
not check that box, then they were asked to highlight
the target of the modality. Table 1 shows the number
of sentences we posted on MTurk for each modal-
ity.6 Three Turkers annotated each sentence. We
restricted the task to Turkers who were adults, had
greater than a 95% approval rating, and had com-
pleted at least 50 HITs (Human Intelligence Tasks)
on MTurk. We paid US$0.10 for each set of ten sen-
tences.
Since our data was annotated by three Turkers,
for training data we used only those examples for
which at least two Turkers agreed on the modality
and the target of the modality. This resulted in 1,008
examples. 674 examples had two Turkers agreeing
and 334 had unanimous agreement. We kept track
of the level of agreement for each example so that
4http://bailando.sims.berkeley.edu/enron/enron.sql.gz
5Data received through personal communication
6More detailed statistics on MTurk annotations are available
at http://hltcoe.jhu.edu/datasets/.
59
Modality Count
Ability 190
Effort 1350
Intention 1320
Success 1160
Want 1390
Table 1: For each modality, the number of sentences re-
turned by the simple tagger that we posted on MTurk.
our learner could weight the examples differently
depending on the level of inter-annotator agreement.
4 Multiclass SVM for Modality
In this section, we describe the automatic modal-
ity tagger we built using the MTurk annotations de-
scribed in Section 3 as the training data. Section 4.1
describes the training and evaluation data. In Sec-
tion 4.2, we present the machinery and Section 4.3
describes the features we used to train the tagger.
In Section 4.4, we present various experiments and
discuss results. Section 4.5, presents additional ex-
periments using annotator confidence.
4.1 Data
For training, we used the data presented in Section 3.
We refer to it as MTurk data in the rest of this paper.
For evaluation, we selected a part of the LU Corpus
(Diab et al, 2009a) (1228 sentences) and our expert
annotated it with modality tags. We first used the
high-recall simple modality tagger described in Sec-
tion 3 to select the sentences with modalities. Out
of the 235 sentences returned by the simple modal-
ity tagger, our expert removed the ones which did
not in fact have a modality. In the remaining sen-
tences (94 sentences), our expert annotated the tar-
get predicate. We refer to this as the Gold dataset
in this paper. The MTurk and Gold datasets differ in
terms of genres as well as annotators (Turker vs. Ex-
pert). The distribution of modalities in both MTurk
and Gold annotations are given in Table 2.
4.2 Approach
We applied a supervised learning framework us-
ing multi-class SVMs to automatically learn to tag
Modality MTurk Gold
Ability 6% 48%
Effort 25% 10%
Intention 30% 11%
Success 24% 9%
Want 15% 23%
Table 2: Frequency of Modalities
modalities in context. For tagging, we used the Yam-
cha (Kudo and Matsumoto, 2003) sequence labeling
system which uses the SVMlight (Joachims, 1999)
package for classification. We used One versus All
method for multi-class classification on a quadratic
kernel with a C value of 1. We report recall and pre-
cision on word tokens in our corpus for each modal-
ity. We also report F?=1 (F)-measure as the har-
monic mean between (P)recision and (R)ecall.
4.3 Features
We used lexical features at the token level which can
be extracted without any parsing with relatively high
accuracy. We use the term context width to denote
the window of tokens whose features are considered
for predicting the tag for a given token. For example,
a context width of 2 means that the feature vector
of any given token includes, in addition to its own
features, those of 2 tokens before and after it as well
as the tag prediction for 2 tokens before it. We did
experiments varying the context width from 1 to 5
and found that a context width of 2 gives the optimal
performance. All results reported in this paper are
obtained with a context width of 2. For each token,
we performed experiments using following lexical
features:
? wordStem - Word stem.
? wordLemma - Word lemma.
? POS - Word?s POS tag.
? isNumeric - Word is Numeric?
? verbType - Modal/Auxiliary/Regular/Nil
? whichModal - If the word is a modal verb,
which modal?
60
We used the Porter stemmer (Porter, 1997) to ob-
tain the stem of a word token. To determine the
word lemma, we used an in-house lemmatizer using
dictionary and morphological analysis to obtain the
dictionary form of a word. We obtained POS tags
from Stanford POS tagger and used those tags to
determine verbType and whichModal features. The
verbType feature is assigned a value ?Nil? if the word
is not a verb and whichModal feature is assigned a
value ?Nil? if the word is not a modal verb. The fea-
ture isNumeric is a binary feature denoting whether
the token contains only digits or not.
4.4 Experiments and Results
In this section, we present experiments performed
considering all the MTurk annotations where two
annotators agreed and all the MTurk annotations
where all three annotators agreed to be equally cor-
rect annotations. We present experiments applying
differential weights for these annotations in Section
4.5. We performed 4-fold cross validation (4FCV)
on MTurk data in order to select the best feature
set configuration ?. The best feature set obtained
waswordStem,POS,whichModal with a context
width of 2. For finding the best performing fea-
ture set - context width configuration, we did an ex-
haustive search on the feature space, pruning away
features which were proven not useful by results at
stages. Table 3 presents results obtained for each
modality on 4-fold cross validation.
Modality Precision Recall F Measure
Ability 82.4 55.5 65.5
Effort 95.1 82.8 88.5
Intention 84.3 61.3 70.7
Success 93.2 76.6 83.8
Want 88.4 64.3 74.3
Overall 90.1 70.6 79.1
Table 3: Per modality results for best feature set ? on
4-fold cross validation on MTurk data
We also trained a model on the entire MTurk data
using the best feature set ? and evaluated it against
the Gold data. The results obtained for each modal-
ity on gold evaluation are given in Table 4. We at-
tribute the lower performance on the Gold dataset to
its difference from MTurk data. MTurk data is en-
tirely from email threads, whereas Gold data con-
tained sentences from newswire, letters and blogs
in addition to emails. Furthermore, the annotation
is different (Turkers vs expert). Finally, the distri-
bution of modalities in both datasets is very differ-
ent. For example, Ability modality was merely 6%
of MTurk data compared to 48% in Gold data (see
Table 2).
Modality Precision Recall F Measure
Ability 78.6 22.0 34.4
Effort 85.7 60.0 70.6
Intention 66.7 16.7 26.7
Success NA 0.0 NA
Want 92.3 50.0 64.9
Overall 72.1 29.5 41.9
Table 4: Per modality results for best feature set ? evalu-
ated on Gold dataset
We obtained reasonable performances for Effort
and Want modalities while the performance for other
modalities was rather low. Also, the Gold dataset
contained only 8 instances of Success, none of which
was recognized by the tagger resulting in a recall
of 0%. Precision (and, accordingly, F Measure) for
Success was considered ?not applicable? (NA), as no
such tag was assigned.
4.5 Annotation Confidence Experiments
Our MTurk data contains sentence for which at least
two of the three Turkers agreed on the modality and
the target of the modality. In this section, we investi-
gate the role of annotation confidence in training an
automatic tagger. The annotation confidence is de-
noted by whether an annotation was agreed by only
two annotators or was unanimous. We denote the set
of sentences for which only two annotators agreed as
Agr2 and that for which all three annotators agreed
as Agr3.
We present four training setups. The first setup
is Tr23 where we train a model using both Agr2
and Agr3 with equal weights. This is the setup we
used for results presented in the Section 4.4. Then,
we have Tr2 and Tr3, where we train using only
Agr2 and Agr3 respectively. Then, for Tr23W , we
61
TrainingSetup
Tested on Agr2 and Agr3 Tested on Agr3 only
Precision Recall F Measure Precision Recall F Measure
Tr23 90.1 70.6 79.1 95.9 86.8 91.1
Tr2 91.0 66.1 76.5 95.6 81.8 88.2
Tr3 88.1 52.3 65.6 96.8 71.7 82.3
Tr23W 89.9 70.5 79.0 95.8 86.5 90.9
Table 5: Annotator Confidence Experiment Results; the best results per column are boldfaced
(4-fold cross validation on MTurk Data)
train a model giving different cost values for Agr2
and Agr3 examples. The SVMLight package al-
lows users to input cost values ci for each training
instance separately.7 We tuned this cost value for
Agr2 and Agr3 examples and found the best value
at 20 and 30 respectively.
For all four setups, we used feature set ?. We per-
formed 4-fold cross validation on MTurk data in two
ways ? we tested against a combination of Agr2
and Agr3, and we tested against only Agr3. Results
of these experiments are presented in Table 5. We
also present the results of evaluating a tagger trained
on the whole MTurk data for each setup against the
Gold annotation in Table 6. The Tr23 tested on both
Agr2 andAgr3 presented in Table 5 and Tr23 tested
on Gold data presented in Table 6 correspond to the
results presented in Table 3 and Table 4 respectively.
TrainingSetup Precision Recall F Measure
Tr23 72.1 29.5 41.9
Tr2 67.4 27.6 39.2
Tr3 74.1 19.1 30.3
Tr23W 73.3 31.4 44.0
Table 6: Annotator Confidence Experiment Results; the
best results per column are boldfaced
(Evaluation against Gold)
One main observation is that including annota-
tions of lower agreement, but still above a threshold
(in our case, 66.7%), is definitely helpful. Tr23 out-
performed both Tr2 and Tr3 in both recall and F-
7This can be done by specifying ?cost:<value>? after the
label in each training instance. This feature has not yet been
documented on the SVMlight website.
measure in all evaluations. Also, even when evaluat-
ing against only the high confident Agr3 cases, Tr2
gave a high gain in recall (10 .1 percentage points)
over Tr3, with only a 1.2 percentage point loss on
precision. We conjecture that this is because there
are far more training instances in Tr2 than in Tr3
(674 vs 334), and that quantity beats quality.
Another important observation is the increase in
performance by using varied costs for Agr2 and
Agr3 examples (the Tr23W condition). Although
it dropped the performance by 0.1 to 0.2 points
in cross-validation F measure on the Enron cor-
pora, it gained 2.1 points in Gold evaluation F mea-
sure. These results seem to indicate that differential
weighting based on annotator agreement might have
more beneficial impact when training a model that
will be applied to a wide range of genres than when
training a model with genre-specific data for appli-
cation to data from the same genre. Put differently,
using varied costs prevents genre over-fitting. We
don?t have a full explanation for this difference in
behavior yet. We plan to explore this in future work.
5 Conclusion
We have presented an innovative way of combining
a high-recall simple tagger with Mechanical Turk
annotations to produce training data for a modality
tagger. We show that we obtain good performance
on the same genre as this training corpus (annotated
in the same manner), and reasonable performance
across genres (annotated by an independent expert).
We also present experiments utilizing the number of
agreeing Turkers to choose cost values for training
examples for the SVM. As future work, we plan to
extend this approach to other modalities which are
62
not covered in this study.
6 Acknowledgments
This work is supported, in part, by the Johns Hop-
kins Human Language Technology Center of Ex-
cellence. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the sponsor. We thank several anony-
mous reviewers for their constructive feedback.
References
Emilia Apostolova, Noriko Tomuro, and Dina Demner-
Fushman. 2011. Automatic extraction of lexico-
syntactic patterns for detection of negation and spec-
ulation scopes. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies: short papers -
Volume 2, HLT ?11, pages 283?287, Portland, Oregon.
Eiji Aramaki, Yasuhide Miura, Masatsugu Tonoike,
Tomoko Ohkuma, Hiroshi Mashuichi, and Kazuhiko
Ohe. 2009. Text2table: Medical text summarization
system based on named entity recognition and modal-
ity identification. In Proceedings of the BioNLP 2009
Workshop, pages 185?192, Boulder, Colorado, June.
Association for Computational Linguistics.
Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr,
Nathaniel W. Filardo, Lori S. Levin, and Christine D.
Piatko. 2010. A modality lexicon and its use in auto-
matic tagging. In LREC.
Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr,
Chris Callison-Burch, Nathaniel W. Filardo, Christine
Piatko, Lori Levin, and Scott Miller. 2012. Use of
modality and negation in semantically-informed syn-
tactic mt. Computational Linguistics, 38(22).
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of the 22nd Na-
tional Conference on Artificial intelligence - Volume 1,
pages 871?876, Vancouver, British Columbia, Canada.
AAAI Press.
Mona Diab, Bonnie Dorr, Lori Levin, Teruko Mitamura,
Rebecca Passonneau, Owen Rambow, and Lance
Ramshaw. 2009a. Language Understanding Anno-
tation Corpus. Linguistic Data Consortium (LDC),
USA.
Mona Diab, Lori Levin, Teruko Mitamura, Owen Ram-
bow, Vinodkumar Prabhakaran, and Weiwei Guo.
2009b. Committed belief annotation and tagging. In
Proceedings of the Third Linguistic Annotation Work-
shop, pages 68?73, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Szarvas,
Gyo?rgy Mo?ra, and Ja?nos Csirik, editors. 2010. Pro-
ceedings of the Fourteenth Conference on Computa-
tional Natural Language Learning. Association for
Computational Linguistics, Uppsala, Sweden, July.
Thorsten Joachims, 1999. Making large-scale support
vector machine learning practical, pages 169?184.
MIT Press, Cambridge, MA, USA.
Stefan Kaufmann, Cleo Condoravdi, and Valentina
Harizanov, 2006. Formal Approaches to Modality,
pages 72?106. Mouton de Gruyter.
Angelika Kratzer. 1981. The Notional Category of
Modality. In H. J. Eikmeyer and H. Rieser, editors,
Words, Worlds, and Contexts, pages 38?74. de Gruyter,
Berlin.
Angelika Kratzer. 1991. Modality. In Arnim von Ste-
chow and Dieter Wunderlich, editors, Semantics: An
International Handbook of Contemporary Research.
de Gruyter.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods
for kernel-based text analysis. In 41st Meeting of the
Association for Computational Linguistics (ACL?03),
Sapporo, Japan.
Marjorie McShane, Sergei Nirenburg, and Ron
Zacharsky. 2004. Mood and modality: Out of
the theory and into the fray. Natural Language
Engineering, 19(1):57?89.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the BioNLP 2009 Workshop, pages 28?
36, Boulder, Colorado, June. Association for Compu-
tational Linguistics.
Masaki Murata, Kiyotaka Uchimoto, Qing Ma, Toshiyuki
Kanamaru, and Hitoshi Isahara. 2005. Analysis of
machine translation systems? errors in tense, aspect,
and modality. In Proceedings of the 19th Asia-Pacific
Conference on Language, Information and Computa-
tion (PACLIC), Tapei.
Rowan Nairn, Cleo Condorovdi, and Lauri Karttunen.
2006. Computing relative polarity for textual infer-
ence. In Proceedings of the International Workshop on
Inference in Computational Semantics, ICoS-5, pages
66?76, Buxton, England.
M. F. Porter, 1997. An algorithm for suffix stripping,
pages 313?316. Morgan Kaufmann Publishers Inc.,
San Francisco, CA, USA.
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2010. Automatic committed belief tagging.
In Coling 2010: Posters, pages 1014?1022, Beijing,
China, August. Coling 2010 Organizing Committee.
63
Brian Roark. 2009. Open vocabulary language model-
ing for binary response typing interfaces. Technical
report, Oregon Health and Science University.
Roser Sauri and James Pustejovsky. 2009. Factbank:
a corpus annotated with event factuality. Language
Resources and Evaluation, 43(3):227?268.
Roser Sauri, Marc Verhagen, and James Pustejovsky.
2006. Annotating and recognizing event modality in
text. In FLAIRS Conference, pages 333?339.
Bengt Sigurd and Barbara Gawro?nska. 1994. Modals
as a problem for MT. In Proceedings of the 15th In-
ternational Conference on Computational Linguistics
(COLING) Volume 1, COLING ?94, pages 120?124,
Kyoto, Japan.
Johan Van Der Auwera and Andreas Ammann, 2005.
Overlap between situational and epistemic modal
marking, chapter 76, pages 310?313. Oxford Univer-
sity Press.
Veronika Vincze, Gy orgy Szarvas, Richa?d Farkas,
Gy orgy Mora, and Ja?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9+.
64
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 10?19,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Analysis of Stopping Active Learning based on Stabilizing Predictions
Michael Bloodgood
Center for Advanced Study of Language
University of Maryland
College Park, MD 20740
meb@umd.edu
John Grothendieck
Raytheon BBN Technologies
9861 Broken Land Parkway, Suite 400
Columbia, MD 21046
jgrothen@bbn.com
Abstract
Within the natural language processing
(NLP) community, active learning has
been widely investigated and applied in or-
der to alleviate the annotation bottleneck
faced by developers of new NLP systems
and technologies. This paper presents the
first theoretical analysis of stopping active
learning based on stabilizing predictions
(SP). The analysis has revealed three ele-
ments that are central to the success of the
SP method: (1) bounds on Cohen?s Kappa
agreement between successively trained
models impose bounds on differences in
F-measure performance of the models; (2)
since the stop set does not have to be la-
beled, it can be made large in practice,
helping to guarantee that the results trans-
fer to previously unseen streams of ex-
amples at test/application time; and (3)
good (low variance) sample estimates of
Kappa between successive models can be
obtained. Proofs of relationships between
the level of Kappa agreement and the dif-
ference in performance between consecu-
tive models are presented. Specifically, if
the Kappa agreement between two mod-
els exceeds a threshold T (where T > 0),
then the difference in F-measure perfor-
mance between those models is bounded
above by 4(1?T )T in all cases. If precisionof the positive conjunction of the models
is assumed to be p, then the bound can be
tightened to 4(1?T )(p+1)T .
1 Introduction
Active learning (AL), also called query learning
and selective sampling, is an approach to reduce
the costs of creating training data that has received
considerable interest (e.g., (Argamon-Engelson
and Dagan, 1999; Baldridge and Osborne, 2008;
Bloodgood and Vijay-Shanker, 2009b; Bloodgood
and Callison-Burch, 2010; Hachey et al, 2005;
Haertel et al, 2008; Haffari and Sarkar, 2009;
Hwa, 2000; Lewis and Gale, 1994; Sassano,
2002; Settles and Craven, 2008; Shen et al, 2004;
Thompson et al, 1999; Tomanek et al, 2007; Zhu
and Hovy, 2007)).
Within the NLP community, active learning has
been widely investigated and applied in order to
alleviate the annotation bottleneck faced by devel-
opers of new NLP systems and technologies. The
main idea is that by judiciously selecting which
examples to have labeled, annotation effort will be
focused on the most helpful examples and less an-
notation effort will be required to achieve given
levels of performance than if a passive learning
policy had been used.
Historically, the problem of developing meth-
ods for detecting when to stop AL was tabled for
future work and the research literature was fo-
cused on how to select which examples to have la-
beled and analyzing the selection methods (Cohn
et al, 1996; Seung et al, 1992; Freund et al, 1997;
Roy and McCallum, 2001). However, to realize
the savings in annotation effort that AL enables,
we must have a method for knowing when to stop
the annotation process. The challenge is that if we
stop too early while useful generalizations are still
being made, then we can wind up with a model
that performs poorly, but if we stop too late after
all the useful generalizations are made, then hu-
man annotation effort is wasted and the benefits of
using active learning are lost.
Recently research has begun to develop meth-
ods for stopping AL (Schohn and Cohn, 2000;
Ertekin et al, 2007b; Ertekin et al, 2007a; Zhu
and Hovy, 2007; Laws and Schu?tze, 2008; Zhu
et al, 2008a; Zhu et al, 2008b; Vlachos, 2008;
Bloodgood, 2009; Bloodgood and Vijay-Shanker,
2009a; Ghayoomi, 2010). The methods are all
10
heuristics based on estimates of model confidence,
error, or stability. Although these heuristic meth-
ods have appealing intuitions and have had ex-
perimental success on a small handful of tasks
and datasets, the methods are not widely usable in
practice yet because our community?s understand-
ing of the stopping methods remains too coarse
and inexact. Pushing forward on understanding
the mechanics of stopping at a more exact level
is therefore crucial for achieving the design of
widely usable effective stopping criteria.
Bloodgood and Vijay-Shanker (2009a) intro-
duce the terminology aggressive and conserva-
tive to describe the behavior of stopping meth-
ods1 and conduct an empirical evaluation of the
different published stopping methods on several
datasets. While most stopping methods tend to
behave conservatively, stopping based on stabiliz-
ing predictions computed via inter-model Kappa
agreement has been shown to be consistently ag-
gressive without losing performance (in terms of
F-Measure2) in several published empirical tests.
This method stops when the Kappa agreement be-
tween consecutively learned models during AL
exceeds a threshold for three consecutive itera-
tions of AL. Although this is an intuitive heuristic
that has performed well in published experimental
results, there has not been any theoretical analysis
of the method.
The current paper presents the first theoretical
analysis of stopping based on stabilizing predic-
tions. The analysis helps to explain at a deeper
and more exact level why the method works as it
does. The results of the analysis help to character-
ize classes of problems where the method can be
expected to work well and where (unmodified) it
will not be expected to work as well. The theory
is suggestive of modifications to improve the ro-
bustness of the stopping method for certain classes
of problems. And perhaps most important, the
approach that we use in our analysis provides an
enabling framework for more precise analysis of
stopping criteria and possibly other parts of the ac-
tive learning decision space.
In addition, the information presented in this pa-
1Aggressive methods stop sooner, aggressively trying to
reduce unnecessary annotations while conservative methods
are careful not to risk losing model performance, even if it
means annotating many more examples than were necessary.
2For the rest of this paper, we will use F-measure to de-
note F1-measure, that is, the balanced harmonic mean of pre-
cision and recall, which is a standard metric used to evaluate
NLP systems.
per is useful for works that consider switching be-
tween different active learning strategies and oper-
ating regions such as (Baram et al, 2004; Do?nmez
et al, 2007; Roth and Small, 2008). Knowing
when to switch strategies, for example, is sim-
ilar to the stopping problem and is another set-
ting where detailed understanding of the variance
of stabilization estimates and their link to perfor-
mance ramifications is useful. More exact un-
derstanding of the mechanics of stopping is also
useful for applications of co-training (Blum and
Mitchell, 1998), and agreement-based co-training
(Clark et al, 2003) in particular. Finally, the
proofs of the Theorems regarding the relationships
between Cohen?s Kappa statistic and F-measure
may be of broader use in works that consider inter-
annotator agreement and its ramifications for per-
formance appraisals, a topic that has been of long-
standing interest in computational linguistics (Car-
letta, 1996; Artstein and Poesio, 2008).
In the next section we summarize the stabiliz-
ing predictions (SP) stopping method. Section 3
analyzes SP and Section 4 concludes.
2 Stopping Active Learning based on
Stabilizing Predictions
The intuition behind the SP method is that the
models learned during AL can be applied to a large
representative set of unlabeled data called a stop
set and when consecutively learned models have
high agreement on their predictions for classify-
ing the examples in the stop set, this indicates that
it is time to stop (Bloodgood and Vijay-Shanker,
2009a; Bloodgood, 2009). The active learning
stopping strategy explicitly examined in (Blood-
good and Vijay-Shanker, 2009a) (after the general
form is discussed) is to calculate Cohen?s Kappa
agreement statistic between consecutive rounds of
active learning and stop once it is above 0.99 for
three consecutive calculations.
Since the Kappa statistic is an important as-
pect of this method, we now discuss some back-
ground regarding measuring agreement in general,
and Cohen?s Kappa in particular. Measurement
of agreement between human annotators has re-
ceived significant attention and in that context,
the drawbacks of using percentage agreement have
been recognized (Artstein and Poesio, 2008). Al-
ternative metrics have been proposed that take
chance agreement into account. Artstein and Poe-
sio (2008) survey several agreement metrics. Most
11
of the agreement metrics they discuss are of the
form:
agreement = Ao ?Ae1?Ae
, (1)
whereAo = observed agreement, andAe = agree-
ment expected by chance. The different metrics
differ in how they compute Ae. All the instances
of usage of an agreement metric in this article will
have two categories and two coders. The two cat-
egories are ?+1? and ?-1? and the two coders are
the two consecutive models for which agreement
is being measured.
Cohen?s Kappa statistic3 (Cohen, 1960) mea-
sures agreement expected by chance by modeling
each coder (in our case model) with a separate dis-
tribution governing their likelihood of assigning a
particular category. Formally, Kappa is defined by
Equation 1 with Ae computed as follows:
Ae =
?
k?{+1,?1}
P (k|c1) ? P (k|c2), (2)
where each ci is one of the coders (in our case,
models), and P (k|ci) is the probability that coder
(model) ci labels an instance as being in category
k. Kappa estimates the P (k|ci) in Equation 2
based on the proportion of observed instances that
coder (model) ci labeled as being in category k.
3 Analysis
This section analyzes the SP stopping method.
Section 3.1 analyzes the variance of the estima-
tor of Kappa that SP uses and in particular the re-
lationship of this variance to specific aspects of
the operationalization of SP, such as the stop set
size. Section 3.2 analyzes relationships between
the Kappa agreement between two models and the
difference in F-measure between those two mod-
els.
3.1 Variance of Kappa Estimator
SP bases its decision to stop on the information
contained in the contingency tables between the
classifications of models learned at consecutive
iterations during AL. In determining whether to
stop at iteration t, the classifications of the current
model Mt are compared with the classifications of
the previous model Mt?1. Table 1 shows the pop-
ulation parameters for these two models, where:
3We note that there are other agreement measures (beyond
Cohen?s Kappa) which could also be applicable to stopping
based on stabilizing predictions, but an analysis of these is
outside the scope of the current paper.
Mt
Mt?1 + - Total
+ pi++ pi+? pi+.
- pi?+ pi?? pi?.
Total pi.+ pi.? 1
Table 1: Contingency table population probabili-
ties forMt (model learned at iteration t) andMt?1
(model learned at iteration t-1).
population probability piij for i, j ? {+,?} is the
probability of an example being placed in category
i by model Mt?1 and category j by model Mt;
population probability pi.j for j ? {+,?} is the
probability of an example being placed in category
j by model Mt; and population probability pii. for
i ? {+,?} is the probability of an example being
placed in category i by model Mt?1. The actual
probability of agreement is pio = pi++ + pi??. As
indicated in Equation 2, Kappa models the prob-
ability of agreement expected due to chance by
assuming that classifications are made indepen-
dently. Hence, the probability of agreement ex-
pected by chance in terms of the population prob-
abilities is pie = pi+.pi.++pi?.pi.?. From the defini-
tion of Kappa (see Equation 1), we then have that
the Kappa parameter K in terms of the population
probabilities is given by
K = pio ? pie1? pie
. (3)
For practical applications we will not know the
true population probabilities and we will have to
resort to using sample estimates. The SP method
uses a stop set of size n for deriving its estimates.
Table 2 shows the contingency table counts for
the classifications of models Mt and Mt?1 on a
sample of size n. The population probabilities piij
can be estimated by the relative frequencies pij for
i, j ? {+,?, .}, where: p++ = a/n; p+? = b/n;
p?+ = c/n; p?? = d/n; p+. = (a+ b)/n; p?. =
(c+d)/n; p.+ = (a+ c)/n; and p.? = (c+d)/n.
Let po = p++ + p??, the observed proportion of
agreement and let pe = p+.p.+ + p?.p.?, the pro-
portion of agreement expected by chance if we as-
sume that Mt and Mt?1 make their classifications
independently. Then the Kappa measure of agree-
ment K between Mt and Mt?1 (see Equation 3) is
estimated by
K? = po ? pe1? pe
. (4)
12
Mt
Mt?1 + - Total
+ a b a+ b
- c d c+ d
Total a+ c b+ d n
Table 2: Contingency table counts for Mt (model
learned at iteration t) and Mt?1 (model learned at
iteration t-1).
Using the delta method, as described in (Bishop
et al, 1975), Fleiss et al (1969) derived an estima-
tor of the large-sample variance of K?. According
to Hale and Fleiss (1993), the estimator simplifies
to
V ar(K?) = 1n(1? pe)2
?
{ ?
i?{+,?}
pii[1? 4p?i(1? K?)]
? (K? ? pe(1? K?))2 + (1? K?)2?
?
i,j?{+,?}
pij [2(p?i + p?j)? (pi. + p.j)]2
}
,
(5)
where p?i = (pi. + p.i)/2. From Equation 5, we
can see that the variance of our estimate of Kappa
is inversely proportional to the size of the stop set
we use.
Bloodgood and Vijay-Shanker (2009a) used a
stop set of size 2000 for each of their datasets.
Although this worked well in the results they re-
ported, we do not believe that 2000 is a fixed size
that will work well for all tasks and datasets where
the SP method could be used. Table 3 shows
the variances of K? computed using Equation 5
at the points at which SP stopped AL for each of
the datasets4 from (Bloodgood and Vijay-Shanker,
2009a).
These variances indicate that the size of 2000
was typically sufficient to get tight estimates of
Kappa, helping to illuminate the empirical success
of the SP method on these datasets. More gener-
ally, the SP method can be augmented with a vari-
ance check: if the variance of estimated Kappa at
a potential stopping point exceeds some desired
4We note that each of the datasets was set up as a binary
classification task (or multiple binary classification tasks).
Further details and descriptions of each of the datasets can
be found in (Bloodgood and Vijay-Shanker, 2009a).
threshold, then the stop set size can be increased
as needed to reduce the variance.
Looking at Equation 5 again, one can note that
when pe is relatively close to 1, the variance of K?
can be expected to get quite large. In these situ-
ations, users of SP should expect to have to use
larger stop set sizes and in extreme conditions, SP
may not be an advisable method to use.
3.2 Relationship between Kappa agreement
and change in performance between
models
Heretofore, the published literature contained only
informal explanations of why stabilizing predic-
tions is expected to work well as a stopping
method (along with empirical tests demonstrat-
ing successful operation on a handful of tasks and
datasets). In the remainder of this section we
describe the mathematical foundations for stop-
ping methods based on stabilizing predictions. In
particular, we will prove that even in the worst
possible case, if the Kappa agreement between
two subsequently learned models is greater than
a threshold T , then it must be the case that the
change in performance between these two models
is bounded above by 4(1?T )T . We then go on toprove additional Theorems that tighten this bound
when assumptions are made about model preci-
sion.
Lemma 3.1 Suppose F-measure F and Kappa K
are computed from the same contingency table of
counts, such as the one given in Table 2. Suppose
ad? bc ? 0. Then F ? K.
Proof By definition, in terms of the contingency
table counts,
K = 2ad? 2bc(a+ b)(b+ d) + (a+ c)(c+ d) (6)
and
F = 2a2a+ b+ c . (7)
Rewriting F so that it will have the same numera-
tor as K, we have:
F = F
(
d? bca
d? bca
)
(8)
=
( 2a
2a+ b+ c
)(d? bca
d? bca
)
(9)
= 2ad? 2bc
2ad+ bd+ cd? 2bc? b2c+bc2a
.(10)
13
Task-Dataset Variance of K?
NER-DNA (10-fold CV) 0.0000223
NER-cellType (10-fold CV) 0.0000211
NER-protein (10-fold CV) 0.0000074
Reuters (10 Categories) 0.0000298
20 Newsgroups (20 Categories) 0.0000739
WebKB Student (10-fold CV) 0.0000137
WebKB Project (10-fold CV) 0.0000190
WebKB Faculty (10-fold CV) 0.0000115
WebKB Course (10-fold CV) 0.0000179
TC-spamassassin (10-fold CV) 0.0000042
TC-TREC-SPAM (10-fold CV) 0.0000043
Average (macro-avg) 0.0000209
Table 3: Estimates of the variance of K?. For each dataset, the estimate of the variance of K? is computed
(using Equation 5) from the contingency table at the point at which SP stopped AL and the average of
all the variances (across all folds of CV) is displayed. The last row contains the macro-average of the
average variances for all the datasets.
We can see that the expression for F in Equa-
tion 10 has the same numerator as K in Equa-
tion 6 but the denominator ofK in Equation 6 is?
the denominator of F in Equation 10. Therefore,
F ? K.
Theorem 3.2 LetMt be the model learned at iter-
ation t of active learning and Mt?1 be the model
learned at iteration t ? 1. Let Kt be the estimate
of Kappa agreement between the classifications of
Mt and Mt?1 on the examples in the stop set. Let
F?t be the F-measure between the classifications of
Mt and truth on the stop set. Let F?t?1 be the F-
measure between the classifications of Mt?1 and
truth on the stop set. Let ?Ft be F?t ? F?t?1. Sup-
pose T > 0. Then Kt > T ? |?Ft| ? 4(1?T )T .
Proof Suppose Mt, Mt?1, Kt, F?t, F?t?1, ?Ft,
and T are defined as stated in the statement of
Theorem 3.2. Let Ft be the F-measure between
the classifications of Mt and Mt?1 on the exam-
ples in the stop set. Let Table 2 show the con-
tingency table counts for Mt versus Mt?1 on the
examples in the stop set. Then, from their defi-
nitions, we have Kt = 2(ad?bc)(a+b)(b+d)+(a+c)(c+d) and
Ft = 2a2a+b+c . There exist true labels for the ex-amples in the stop set, which we don?t know since
the stop set is unlabeled, but nonetheless must ex-
ist. We use the truth on the stop set to split Table 2
into two subtables of counts, one table for all the
examples that are truly positive and one table for
all the examples that are truly negative. Table 4
Mt
Mt?1 + - Total
+ a1 b1 a1 + b1
- c1 d1 c1 + d1
Total a1 + c1 b1 + d1 n1
Table 4: Contingency table counts for Mt (model
learned at iteration t) versus Mt?1 (model learned
at iteration t-1) for only the examples in the stop
set that have truth = +1.
Mt
Mt?1 + - Total
+ a?1 b?1 a?1 + b?1
- c?1 d?1 c?1 + d?1
Total a?1 + c?1 b?1 + d?1 n?1
Table 5: Contingency table counts for Mt (model
learned at iteration t) versus Mt?1 (model learned
at iteration t-1) for only the examples in the stop
set that have truth = -1.
shows the contingency table for Mt versus Mt?1
for all of the examples in the stop set that have true
labels of +1 and Table 5 shows the contingency ta-
ble for Mt versus Mt?1 for all of the examples in
the stop set that have true labels of -1.
From Tables 2, 4, and 5 one can see that a is
the number of examples in the stop set that both
Mt and Mt?1 classified as positive. Furthermore,
out of these a examples, a1 of them truly are pos-
14
Mt
Truth + - Total
+ a1 + c1 b1 + d1 n1
- a?1 + c?1 b?1 + d?1 n?1
Total a+ c b+ d n
Table 6: Contingency table counts for Mt (model
learned at iteration t) versus truth. (Derived from
Tables 4 and 5
Mt?1
Truth + - Total
+ a1 + b1 c1 + d1 n1
- a?1 + b?1 c?1 + d?1 n?1
Total a+ b c+ d n
Table 7: Contingency table counts for Mt?1
(model learned at iteration t-1) versus truth. (De-
rived from Tables 4 and 5
itive and a?1 of them truly are negative. Similar
explanations hold for the other counts. Also, from
Tables 2, 4, and 5, one can see that the equalities
a = a1 + a?1, b = b1 + b?1, c = c1 + c?1, and
d = d1 + d?1 all hold. The contingency tables
for Mt versus truth and Mt?1 versus truth can be
derived from Tables 4 and 5. For convenience, Ta-
ble 6 shows the contingency table for Mt versus
truth and Table 7 shows the contingency table for
Mt?1 versus truth. Suppose that Kt > T . This
implies, by Lemma 3.15, that Ft > T . This im-
plies that
2a
2a+b+c > T (11)
? 2a > (2a+ b+ c)T (12)
? 2a(1? T ) > (b+ c)T (13)
? b+ c < 2a(1?T )T . (14)
Note that Equations 12 and 14 are justified since
2a+ b+ c > 0 and T > 0, respectively.
From Table 6 we can see that
F?t = 2(a1+c1)2(a1+c1)+b1+d1+a?1+c?1 ; from Table 7
we can see that F?t?1 = 2(a1+b1)2(a1+b1)+c1+d1+a?1+b?1 .For notational convenience, let: g =
2(a1 + c1) + b1 + d1 + a?1 + c?1; and
h = 2(a1 + b1) + c1 + d1 + a?1 + b?1.
5Note that the condition ad ? bc ? 0 of Lemma 3.1 is
met since Kt > T and T > 0 imply Kt > 0, which in turn
implies ad? bc > 0.
It follows that
?Ft =
2(a1 + c1)
g ?
2(a1 + b1)
h (15)
= (2a1 + 2c1)h? (2a1 + 2b1)ggh (16)
For notational convenience, let: x = 2(a1c1 +
a1b?1 + c21 + c1d1 + c1a?1 + c1b?1); and y =
2(a1b1 + a1c?1 + b21 + b1d1 + b1a?1 + b1c?1).
Then picking up from Equation 16, it follows that
?Ft =
x? y
gh (17)
= 2[u1 + c1u2 ? b1u3]gh , (18)
where u1 = a1c1 ? a1b1 + a1b?1 ? a1c?1, u2 =
c1+d1+a?1+b?1, and u3 = b1+d1+a?1+c?1.
For notational convenience, let: dA = c1 ? b1
and dB = c?1 ? b?1. Then it follows that
?Ft =
2u4
gh , (19)
where: u4 = a1(dA ? dB) + dA(d1 + a?1 + b1 +
c1) + c1b?1 ? b1c?1.
Noting that g = h+ dA + dB , we have
?Ft =
2u4
h(h+ dA + dB)
. (20)
Noting that 2u4 = 2[dA(a1 + b1 + c1 + d1 +
a?1 + b?1)? dB(a1 + b1)] and letting u5 = a1 +
b1 + c1 + d1 + a?1 + b?1, we have
?Ft =
2[dAu5 ? dB(a1 + b1)]
h(h+ dA + dB)
. (21)
Therefore,
|?Ft| ? 2
(????
dAu5
h(h+ dA + dB)
????
+
????
dB(a1 + b1)
h(h+ dA + dB)
????
) (22)
Recall that b+ c = b1 + b?1 + c1 + c?1. Then
observe that the following three inequalities hold:
b+ c ? dA; b+ c ? dB; and h(h+dA +dB) > 0.
Therefore,
|?Ft| ? 2(b+c)[2a1+2b1+c1+d1+a?1+b?1]h(h+dA+dB) (23)
= 2(b+c)hh(h+dA+dB) (24)
= 2(b+c)h+dA+dB (25)
? 2(2a)(1?T )T (h+dA+dB) (26)
=
(4(1?T )
T
)( a
h+dA+dB
)
. (27)
15
Observe that h+dA+dB = 2a1+b1+2c1+d1+
a?1 + c?1. Therefore, ah+dA+dB ? 1. Therefore,we have
|?Ft| ?
4(1? T )
T . (28)
Note that in deriving Inequality 26, we used
the previously derived Inequality 14. Also, the
proof of Theorem 3.2 assumes a worst possible
case in the sense that all examples where the clas-
sifications of Mt and Mt?1 differ are assumed
to have truth values that all serve to maximize
one model?s F-measure and minimize the other
model?s F-measure so as to maximize |?Ft| as
much as possible. A resulting limitation is that the
bound is loose in many cases. It may be possible
to derive tighter bounds, perhaps by easing off to
an expected case instead of a worst case and/or by
making additional assumptions.6
Taking this possibility up, we now prove tighter
bounds when assumptions about the precision of
the models Mt and Mt?1 are made. Consider that
in the proof of Theorem 3.2 when transitioning
from Equality 27 to Inequality 28, we used the
fact that ah+dA+dB ? 1. Note that ah+dA+dB =a
2a1+b1+2c1+d1+a?1+c?1 , from which one sees thata
h+dA+dB = 1 only if all of a1, b1, c1, d1 and c?1are all zero. This is a pathological case. In many
practically important classes of cases to consider,
a
h+dA+dB will be strictly less than 1, and often sub-stantially less than 1. The following two Theorems
prove tighter bounds on |?Ft| than Theorem 3.2
by utilizing this insight.
Theorem 3.3 Suppose Mt, Mt?1, Kt, F?t, F?t?1,
?Ft, and T are defined as stated in the statement
of Theorem 3.2. Let the contingency tables be de-
fined as they were in the proof of Theorem 3.3. Let
MPositiveConjunction be a model that only clas-
sifies an example as positive if both models Mt
and Mt?1 classify the example as positive. Sup-
pose that MPositiveConjunction has perfect preci-
sion on the stop set, or in other words that every
single example from the stop set that both Mt and
Mt?1 classify as positive is truthfully positive (i.e.,
a?1 = 0). Then Kt > T ? |?Ft| ? 2(1?T )T .
Proof The proof of Theorem 3.2 holds exactly
as it is up until Equality 27. Now, using the
additional assumption that a?1 = 0, we have
6If one is planning to undertake this challenge, we would
suggest further consideration of Inequalities 22, 23, 26, and
28 as a possible starting point.
a
h+dA+dB ?
1
2 . Therefore, we have
|?Ft| ?
2(1? T )
T . (29)
Theorem 3.3 is a special case (in the limit) of
a more general Theorem. Before stating and prov-
ing the more general Theorem, we prove a Lemma
that will be helpful in making the proof of the gen-
eral Theorem clearer.
Lemma 3.4 Let f , dA, dB and contingency ta-
ble counts be defined as they were in the proof
of Theorem 3.2. Suppose a1 = xa?1. Then
a
h+dA+dB ?
x+1
2x+1 .
Proof a1 = xa?1 by hypothesis. a = a1 + a?1
by definition of contingency table counts. Hence,
a = (x+ 1)a?1. Therefore,
a
h+ dA + dB
? (x+1)a?12xa?1+a?1 (30)
= (x+1)a?1(2x+1)a?1
= x+12x+1 .
The following Theorem generalizes Theo-
rem 3.3 to cases when MPositiveConjunction has
precision p in (0, 1).7
Theorem 3.5 Suppose Mt, Mt?1, Kt, F?t, F?t?1,
?Ft, and T are defined as stated in the statement
of Theorem 3.2. Let the contingency tables be de-
fined as they were in the proof of Theorem 3.2. Let
MPositiveConjunction be a model that only classi-
fies an example as positive if both models Mt and
Mt?1 classify the example as positive. Suppose
that MPositiveConjunction has precision p on the
stop set. Then Kt > T ? |?Ft| ? 4(1?T )(p+1)T .
Proof The proof of Theorem 3.2 holds exactly as
it is up until Equality 27. MPositiveConjunction has
precision p on the stop set? p = a1a1+a?1 . Solv-
ing for a1 in terms of a?1 we have a1 = p1?pa?1.
Therefore, applying Lemma 3.4 with x = p1?p , we
have ah+dA+dB ?
p
1?p+1
2p
1?p+1
. Therefore we have
|?Ft| ? 4
(
p
1?p+1
2p
1?p+1
)
(1?T )
T (31)
= 4(1?T )(p+1)T . (32)
7The case when p = 0 is handled by Theorem 3.2 and the
case when p = 1 is handled by Theorem 3.3.
16
Precision 1p+1 (to 3 decimal places)
50% 0.667
80% 0.556
90% 0.526
95% 0.513
98% 0.505
99% 0.503
99.9% 0.500
Table 8: Values of the scaling factor from Theo-
rem 3.5 for different precision values.
The scaling factor 1p+1 in Theorem 3.5 showshow the precision of the conjunctive model affects
the bound. Theorem 3.2 had the scaling factor im-
plicitly set to 1 in order to handle the pathologi-
cal case where the positive conjunctive model has
precision = 0. In Theorem 3.3, where the positive
conjunctive model has precision = 1 on the exam-
ples in the stop set, the scaling factor is set to 1/2.
Theorem 3.5 generalizes the scaling factor so that
it is a function of the precision of the positive con-
junctive model. For convenience, Table 8 shows
the scaling factor values for a few different preci-
sion values.
The bounds in Theorems 3.2, 3.3, and 3.5 all
bound the difference in performance on the stop
set of two consecutively learned models Mt and
Mt?1. An issue to consider is how connected the
difference in performance on the stop set is to the
difference in performance on a stream of applica-
tion examples generated according to the popula-
tion probabilities. Taking up this issue, consider
that the proof of Theorems 3.2, 3.3, and 3.5 would
hold as it is if we had used sample proportions in-
stead of sample counts (this can be seen by simply
dividing every count by n, the size of the stop set).
Since the stop set is unbiased (selected at random
from the population), as n approaches infinity, the
sample proportions will approach the population
probabilities and the difference between the dif-
ference in performance between Mt and Mt?1 on
the stop set and on a stream of application exam-
ples generated according to the population proba-
bilities will approach zero.
4 Conclusions
To date, the work on stopping criteria has been
dominated by heuristics based on intuitions and
experimental success on a small handful of tasks
and datasets. But the methods are not widely
usable in practice yet because our community?s
understanding of the stopping methods remains
too inexact. Pushing forward on understanding
the mechanics of stopping at a more exact level
is therefore crucial for achieving the design of
widely usable effective stopping criteria.
This paper presented the first theoretical anal-
ysis of stopping based on stabilizing predictions.
The analysis revealed three elements that are cen-
tral to the SP method?s success: (1) the sample es-
timates of Kappa have low variance; (2) Kappa has
tight connections with differences in F-measure;
and (3) since the stop set doesn?t have to be la-
beled, it can be arbitrarily large, helping to guar-
antee that the results transfer to previously unseen
streams of examples at test/application time.
We presented proofs of relationships between
the level of Kappa agreement and the difference in
performance between consecutive models. Specif-
ically, if the Kappa agreement between two mod-
els is at least T, then the difference in F-measure
performance between those models is bounded
above by 4(1?T )T . If precision of the positive con-junction of the models is assumed to be p, then the
bound can be tightened to 4(1?T )(p+1)T .
The setup and methodology of the proofs can
serve as a launching pad for many further inves-
tigations, including: analyses of stopping; works
that consider switching between different active
learning strategies and operating regions; and
works that consider stopping co-training, and es-
pecially agreement-based co-training. Finally, the
relationships that have been exposed between the
Kappa statistic and F-measure may be of broader
use in works that consider inter-annotator agree-
ment and its interplay with system evaluation, a
topic that has been of long-standing interest.
References
Shlomo Argamon-Engelson and Ido Dagan. 1999.
Committee-based sample selection for probabilis-
tic classifiers. Journal of Artificial Intelligence Re-
search (JAIR), 11:335?360.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Jason Baldridge and Miles Osborne. 2008. Ac-
tive learning and logarithmic opinion pools for hpsg
parse selection. Nat. Lang. Eng., 14(2):191?222.
17
Yoram Baram, Ran El-Yaniv, and Kobi Luz. 2004. On-
line choice of active learning algorithms. Journal of
Machine Learning Research, 5:255?291, March.
Yvonne M. Bishop, Stephen E. Fienberg, and Paul W.
Holland. 1975. Discrete Multivariate Analysis:
Theory and Practice. MIT Press, Cambridge, MA.
Michael Bloodgood and Chris Callison-Burch. 2010.
Bucking the trend: Large-scale cost-focused active
learning for statistical machine translation. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 854?864,
Uppsala, Sweden, July. Association for Computa-
tional Linguistics.
Michael Bloodgood and K Vijay-Shanker. 2009a. A
method for stopping active learning based on stabi-
lizing predictions and the need for user-adjustable
stopping. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 39?47, Boulder, Colorado,
June. Association for Computational Linguistics.
Michael Bloodgood and K Vijay-Shanker. 2009b. Tak-
ing into account the differences between actively
and passively acquired data: The case of active
learning with support vector machines for imbal-
anced datasets. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 137?140,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Michael Bloodgood. 2009. Active learning with sup-
port vector machines for imbalanced datasets and a
method for stopping active learning based on sta-
bilizing predictions. Ph.D. thesis, University of
Delaware, Newark, DE, USA.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In COLT?
98: Proceedings of the eleventh annual conference
on Computational learning theory, pages 92?100,
New York, NY, USA. ACM.
J. Carletta. 1996. Assessing agreement on classifica-
tion tasks: The kappa statistic. Computational lin-
guistics, 22(2):249?254.
Stephen Clark, James Curran, and Miles Osborne.
2003. Bootstrapping pos-taggers using unlabelled
data. In Walter Daelemans and Miles Osborne,
editors, Proceedings of the Seventh Conference on
Natural Language Learning at HLT-NAACL 2003,
pages 49?55.
J. Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Mea-
surement, 20:37?46.
David A. Cohn, Zoubin Ghahramani, and Michael I.
Jordan. 1996. Active learning with statistical mod-
els. Journal of Artificial Intelligence Research,
4:129?145.
Meryem Pinar Do?nmez, Jaime G. Carbonell, and
Paul N. Bennett. 2007. Dual strategy active
learning. In Joost N. Kok, Jacek Koronacki,
Ramon Lo?pez de Ma?ntaras, Stan Matwin, Dunja
Mladenic, and Andrzej Skowron, editors, Machine
Learning: ECML 2007, 18th European Conference
on Machine Learning, Warsaw, Poland, September
17-21, 2007, Proceedings, volume 4701 of Lec-
ture Notes in Computer Science, pages 116?127.
Springer.
Seyda Ertekin, Jian Huang, Le?on Bottou, and C. Lee
Giles. 2007a. Learning on the border: active learn-
ing in imbalanced data classification. In Ma?rio J.
Silva, Alberto H. F. Laender, Ricardo A. Baeza-
Yates, Deborah L. McGuinness, Bj?rn Olstad, ?ys-
tein Haug Olsen, and Andre? O. Falca?o, editors, Pro-
ceedings of the Sixteenth ACM Conference on Infor-
mation and Knowledge Management, CIKM 2007,
Lisbon, Portugal, November 6-10, 2007, pages 127?
136. ACM.
Seyda Ertekin, Jian Huang, and C. Lee Giles. 2007b.
Active learning for class imbalance problem. In
Wessel Kraaij, Arjen P. de Vries, Charles L. A.
Clarke, Norbert Fuhr, and Noriko Kando, editors,
SIGIR 2007: Proceedings of the 30th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, Amsterdam,
The Netherlands, July 23-27, 2007, pages 823?824.
ACM.
Joseph L. Fleiss, Jacob Cohen, and B. S. Everitt. 1969.
Large sample standard errors of kappa and weighted
kappa. Psychological Bulletin, 72(5):323 ? 327.
Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naf-
tali Tishby. 1997. Selective sampling using the
query by committee algorithm. Machine Learning,
28:133?168.
Masood Ghayoomi. 2010. Using variance as a stop-
ping criterion for active learning of frame assign-
ment. In Proceedings of the NAACL HLT 2010
Workshop on Active Learning for Natural Language
Processing, pages 1?9, Los Angeles, California,
June. Association for Computational Linguistics.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning
(CoNLL-2005), pages 144?151, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Robbie Haertel, Eric Ringger, Kevin Seppi, James Car-
roll, and Peter McClanahan. 2008. Assessing the
costs of sampling methods in active learning for an-
notation. In Proceedings of ACL-08: HLT, Short Pa-
pers, pages 65?68, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Gholamreza Haffari and Anoop Sarkar. 2009. Active
learning for multilingual statistical machine trans-
lation. In Proceedings of the Joint Conference of
18
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 181?189, Suntec,
Singapore, August. Association for Computational
Linguistics.
Cecilia A. Hale and Joseph L. Fleiss. 1993. Interval es-
timation under two study designs for kappa with bi-
nary classifications. Biometrics, 49(2):pp. 523?534.
Rebecca Hwa. 2000. Sample selection for statistical
grammar induction. In Hinrich Schu?tze and Keh-
Yih Su, editors, Proceedings of the 2000 Joint SIG-
DAT Conference on Empirical Methods in Natural
Language Processing, pages 45?53. Association for
Computational Linguistics, Somerset, New Jersey.
Florian Laws and Hinrich Schu?tze. 2008. Stopping cri-
teria for active learning of named entity recognition.
In Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
465?472, Manchester, UK, August. Coling 2008 Or-
ganizing Committee.
David D. Lewis and William A. Gale. 1994. A se-
quential algorithm for training text classifiers. In SI-
GIR ?94: Proceedings of the 17th annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 3?12, New
York, NY, USA. Springer-Verlag New York, Inc.
D. Roth and K. Small. 2008. Active learning for
pipeline models. In Proceedings of the National
Conference on Artificial Intelligence (AAAI), pages
683?688.
Nicholas Roy and Andrew McCallum. 2001. Toward
optimal active learning through sampling estimation
of error reduction. In In Proceedings of the 18th In-
ternational Conference on Machine Learning, pages
441?448. Morgan Kaufmann.
Manabu Sassano. 2002. An empirical study of active
learning with support vector machines for japanese
word segmentation. In ACL ?02: Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 505?512, Morristown, NJ,
USA. Association for Computational Linguistics.
Greg Schohn and David Cohn. 2000. Less is more:
Active learning with support vector machines. In
Proc. 17th International Conf. on Machine Learn-
ing, pages 839?846. Morgan Kaufmann, San Fran-
cisco, CA.
Burr Settles and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1070?1079, Honolulu, Hawaii, October.
Association for Computational Linguistics.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In COLT ?92: Proceedings of
the fifth annual workshop on Computational learn-
ing theory, pages 287?294, New York, NY, USA.
ACM.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and
Chew-Lim Tan. 2004. Multi-criteria-based ac-
tive learning for named entity recognition. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Volume,
pages 589?596, Barcelona, Spain, July.
Cynthia A. Thompson, Mary Elaine Califf, and Ray-
mond J. Mooney. 1999. Active learning for natural
language parsing and information extraction. In In
Proceedings of the 16th International Conference on
Machine Learning, pages 406?414. Morgan Kauf-
mann, San Francisco, CA.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction
which cuts annotation costs and maintains reusabil-
ity of annotated data. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 486?
495.
Andreas Vlachos. 2008. A stopping criterion for
active learning. Computer Speech and Language,
22(3):295?312.
Jingbo Zhu and Eduard Hovy. 2007. Active learn-
ing for word sense disambiguation with methods for
addressing the class imbalance problem. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 783?790.
Jingbo Zhu, Huizhen Wang, and Eduard Hovy. 2008a.
Learning a stopping criterion for active learning for
word sense disambiguation and text classification.
In IJCNLP.
Jingbo Zhu, Huizhen Wang, and Eduard Hovy. 2008b.
Multi-criteria-based strategy to stop active learning
for data annotation. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (Coling 2008), pages 1129?1136, Manchester,
UK, August. Coling 2008 Organizing Committee.
19
