Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 28?36,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Fully Unsupervised Word Sense Disambiguation Method Using
Dependency Knowledge
Ping Chen
Dept. of Computer and Math. Sciences
University of Houston-Downtown
chenp@uhd.edu
Chris Bowes
Dept. of Computer and Math. Sciences
University of Houston-Downtown
bowesc@uhd.edu
Wei Ding
Department of Computer Science
University of Massachusetts-Boston
ding@cs.umb.edu
David Brown
Dept. of Computer and Math. Sciences
University of Houston-Downtown
brownd@uhd.edu
Abstract
Word sense disambiguation is the process of
determining which sense of a word is used
in a given context. Due to its importance in
understanding semantics of natural languages,
word sense disambiguation has been exten-
sively studied in Computational Linguistics.
However, existing methods either are brit-
tle and narrowly focus on specific topics or
words, or provide only mediocre performance
in real-world settings. Broad coverage and
disambiguation quality are critical for a word
sense disambiguation system. In this paper we
present a fully unsupervised word sense dis-
ambiguation method that requires only a dic-
tionary and unannotated text as input. Such an
automatic approach overcomes the problem of
brittleness suffered in many existing methods
and makes broad-coverage word sense dis-
ambiguation feasible in practice. We evalu-
ated our approach using SemEval 2007 Task
7 (Coarse-grained English All-words Task),
and our system significantly outperformed the
best unsupervised system participating in Se-
mEval 2007 and achieved the performance ap-
proaching top-performing supervised systems.
Although our method was only tested with
coarse-grained sense disambiguation, it can be
directly applied to fine-grained sense disam-
biguation.
1 Introduction
In many natural languages, a word can represent
multiple meanings/senses, and such a word is called
a homograph. Word sense disambiguation(WSD)
is the process of determining which sense of a ho-
mograph is used in a given context. WSD is a
long-standing problem in Computational Linguis-
tics, and has significant impact in many real-world
applications including machine translation, informa-
tion extraction, and information retrieval. Gener-
ally, WSD methods use the context of a word for
its sense disambiguation, and the context informa-
tion can come from either annotated/unannotated
text or other knowledge resources, such as Word-
Net (Fellbaum, 1998), SemCor (SemCor, 2008),
Open Mind Word Expert (Chklovski and Mihal-
cea, 2002), eXtended WordNet (Moldovan and Rus,
2001), Wikipedia (Mihalcea, 2007), parallel corpora
(Ng, Wang, and Chan, 2003). In (Ide and Ve?ronis,
1998) many different WSD approaches were de-
scribed. Usually, WSD techniques can be divided
into four categories (Agirre and Edmonds, 2006),
? Dictionary and knowledge based methods.
These methods use lexical knowledge bases
such as dictionaries and thesauri, and hypoth-
esize that context knowledge can be extracted
from definitions of words. For example, Lesk
disambiguated two words by finding the pair of
senses with the greatest word overlap in their
dictionary definitions (Lesk, 1986).
? Supervised methods. Supervised methods
mainly adopt context to disambiguate words.
A supervised method includes a training phase
and a testing phase. In the training phase,
a sense-annotated training corpus is required,
from which syntactic and semantic features are
extracted to create a classifier using machine
28
learning techniques, such as Support Vector
Machine (Novischi et al, 2007). In the fol-
lowing testing phase, a word is classified into
senses (Mihalcea, 2002) (Ng and Lee, 1996).
Currently supervised methods achieve the best
disambiguation quality (about 80% precision
and recall for coarse-grained WSD in the most
recent WSD evaluation conference SemEval
2007 (Navigli et al, 2007)). Nevertheless,
since training corpora are manually annotated
and expensive, supervised methods are often
brittle due to data scarcity, and it is hard to an-
notate and acquire sufficient contextual infor-
mation for every sense of a large number of
words existing in natural languages.
? Semi-supervised methods. To overcome the
knowledge acquisition bottleneck problem suf-
fered by supervised methods, these methods
make use of a small annotated corpus as seed
data in a bootstrapping process (Hearst, 1991)
(Yarowsky, 1995). A word-aligned bilingual
corpus can also serve as seed data (Ng, Wang,
and Chan, 2003).
? Unsupervised methods. These methods acquire
contextual information directly from unanno-
tated raw text, and senses can be induced from
text using some similarity measure (Lin, 1997).
However, automatically acquired information
is often noisy or even erroneous. In the most
recent SemEval 2007 (Navigli et al, 2007), the
best unsupervised systems only achieved about
70% precision and 50% recall.
Disambiguation of a limited number of words is
not hard, and necessary context information can be
carefully collected and hand-crafted to achieve high
disambiguation accuracy as shown in (Yarowsky,
1995). However, such approaches suffer a signifi-
cant performance drop in practice when domain or
vocabulary is not limited. Such a ?cliff-style? per-
formance collapse is called brittleness, which is due
to insufficient knowledge and shared by many tech-
niques in Artificial Intelligence. The main challenge
of a WSD system is how to overcome the knowl-
edge acquisition bottleneck and efficiently collect
the huge amount of context knowledge. More pre-
cisely, a practical WSD need figure out how to create
and maintain a comprehensive, dynamic, and up-to-
date context knowledge base in a highly automatic
manner. The context knowledge required in WSD
has the following properties:
1. The context knowledge need cover a large
number of words and their usage. Such a
requirement of broad coverage is not trivial
because a natural language usually contains
thousands of words, and some popular words
can have dozens of senses. For example, the
Oxford English Dictionary has approximately
301,100 main entries (Oxford, 2003), and the
average polysemy of the WordNet inventory is
6.18 (Fellbaum, 1998). Clearly acquisition of
such a huge amount of knowledge can only be
achieved with automatic techniques.
2. Natural language is not a static phenomenon.
New usage of existing words emerges, which
creates new senses. New words are created,
and some words may ?die? over time. It is esti-
mated that every year around 2,500 new words
appear in English (Kister, 1992). Such dynam-
ics requires a timely maintenance and updating
of context knowledge base, which makes man-
ual collection even more impractical.
Taking into consideration the large amount and
dynamic nature of context knowledge, we only have
limited options when choosing knowledge sources
for WSD. WSD is often an unconscious process to
human beings. With a dictionary and sample sen-
tences/phrases an average educated person can cor-
rectly disambiguate most polysemous words. In-
spired by human WSD process, we choose an elec-
tronic dictionary and unannotated text samples of
word instances as context knowledge sources for
our WSD system. Both sources can be automat-
ically accessed, provide an excellent coverage of
word meanings and usage, and are actively updated
to reflect the current state of languages. In this pa-
per we present a fully unsupervised WSD system,
which only requires WordNet sense inventory and
unannotated text. In the rest of this paper, section
2 describes how to acquire and represent the con-
text knowledge for WSD. We present our WSD al-
gorithm in section 3. Our WSD system is evaluated
with SemEval-2007 Task 7 (Coarse-grained English
29
Figure 1: Context Knowledge Acquisition and Represen-
tation Process
All-words Task) data set, and the experiment results
are discussed in section 4. We conclude in section 5.
2 Context Knowledge Acquisition and
Representation
Figure 1 shows an overview of our context knowl-
edge acquisition process, and collected knowledge
is saved in a local knowledge base. Here are some
details about each step.
2.1 Corpus building through Web search
The goal of this step is to collect as many as possi-
ble valid sample sentences containing the instances
of to-be-disambiguated words. Preferably these in-
stances are also diverse and cover many senses of a
word. We have considered two possible text sources,
1. Electronic text collection, e.g., Gutenberg
project (Gutenberg, 1971). Such collections of-
ten include thousands of books, which are often
written by professionals and can provide many
valid and accurate usage of a large number of
words. Nevertheless, books in these collections
are usually copyright-free and old, hence are
lack of new words or new senses of words used
in modern English.
2. Web documents. Billions of documents exist
in the World Wide Web, and millions of Web
pages are created and updated everyday. Such a
huge dynamic text collection is an ideal source
to provide broad and up-to-date context knowl-
edge for WSD. The major concern about Web
documents is inconsistency of their quality, and
many Web pages are spam or contain erroneous
information. However, factual errors in Web
pages will not hurt the performance of WSD.
Nevertheless, the quality of context knowledge
is affected by broken sentences of poor linguis-
tic quality and invalid word usage, e.g., sen-
tences like ?Colorless green ideas sleep furi-
ously? that violate commonsense knowledge.
Based on our experience these kind of errors
are negligible when using popular Web search
engines to retrieve relevant Web pages.
To start the acquisition process, words that need
to be disambiguated are compiled and saved in a
text file. Each single word is submitted to a Web
search engine as a query. Several search engines
provide API?s for research communities to auto-
matically retrieve large number of Web pages. In
our experiments we used both Google and Yahoo!
API?s to retrieve up to 1,000 Web pages for each to-
be-disambiguated word. Collected Web pages are
cleaned first, e.g., control characters and HTML tags
are removed. Then sentences are segmented simply
based on punctuation (e.g., ?, !, .). Sentences that
contain the instances of a specific word are extracted
and saved into a local repository.
2.2 Parsing
Sentences organized according to each word are
sent to a dependency parser, Minipar. Dependency
parsers have been widely used in Computational
Linguistics and natural language processing. An
evaluation with the SUSANNE corpus shows that
Minipar achieves 89% precision with respect to de-
pendency relations (Lin, 1998). After parsing sen-
tences are converted to parsing trees and saved in
files. Neither our simple sentence segmentation ap-
proach nor Minipar parsing is 100% accurate, so a
small number of invalid dependency relations may
exist in parsing trees. The impact of these erroneous
relations will be minimized in our WSD algorithm.
Comparing with tagging or chunking, parsing is rel-
atively expensive and time-consuming. However, in
our method parsing is not performed in real time
when we disambiguate words. Instead, sentences
30
Figure 2: Merging two parsing trees. The number beside
each edge is the number of occurrences of this depen-
dency relation existing in the context knowledge base.
are parsed only once to extract dependency relations,
then these relations are merged and saved in a local
knowledge base for the following disambiguation.
Hence, parsing will not affect the speed of disam-
biguation at all.
2.3 Merging dependency relations
After parsing, dependency relations from different
sentences are merged and saved in a context knowl-
edge base. The merging process is straightforward.
A dependency relation includes one head word/node
and one dependent word/node. Nodes from different
dependency relations are merged into one as long as
they represent the same word. An example is shown
in Figure 2, which merges the following two sen-
tences:
?Computer programmers write software.?
?Many companies hire computer programmers.?
In a dependency relation ?word1 ? word2?,
word1 is the head word, and word2 is the depen-
dent word. After merging dependency relations, we
will obtain a weighted directed graph with a word
as a node, a dependency relation as an edge, and
the number of occurrences of dependency relation as
weight of an edge. This weight indicates the strength
of semantic relevancy of head word and dependent
word. This graph will be used in the following WSD
Figure 3: WSD Procedure
process as our context knowledge base. As a fully
automatic knowledge acquisition process, it is in-
evitable to include erroneous dependency relations
in the knowledge base. However, since in a large text
collection valid dependency relations tend to repeat
far more times than invalid ones, these erroneous
edges only have minimal impact on the disambigua-
tion quality as shown in our evaluation results.
3 WSD Algorithm
Our WSD approach is based on the following in-
sight:
If a word is semantically coherent with its context,
then at least one sense of this word is semantically
coherent with its context.
Assume that the text to be disambiguated is se-
mantically valid, if we replace a word with its
glosses one by one, the correct sense should be
the one that will maximize the semantic coherence
within this word?s context. Based on this idea we
set up our WSD procedure as shown in Figure 3.
First both the original sentence that contains the
to-be-disambiguated word and the glosses of to-be-
disambiguated word are parsed. Then the parsing
tree generated from each gloss is matched with the
parsing tree of original sentence one by one. The
gloss most semantically coherent with the original
sentence will be chosen as the correct sense. How
to measure the semantic coherence is critical. Our
idea is based on the following hypotheses (assume
word1 is the to-be-disambiguated word):
? In a sentence if word1 is dependent on word2,
and we denote the gloss of the correct sense of
word1 as g1i, then g1i contains the most se-
mantically coherent words that are dependent
31
on word2;
? In a sentence if a set of words DEP1 are de-
pendent on word1, and we denote the gloss of
the correct sense of word1 as g1i, then g1i con-
tains the most semantically coherent words that
DEP1 are dependent on.
For example, we try to disambiguate ?company?
in ?A large company hires many computer program-
mers?, after parsing we obtain the dependency rela-
tions ?hire ? company? and ?company ? large?.
The correct sense for the word ?company? should
be ?an institution created to conduct business?. If
in the context knowledge base there exist the depen-
dency relations ?hire ? institution? or ?institution
? large?, then we believe that the gloss ?an institu-
tion created to conduct business? is semantically co-
herent with its context - the original sentence. The
gloss with the highest semantic coherence will be
chosen as the correct sense. Obviously, the size of
context knowledge base has a positive impact on the
disambiguation quality, which is also verified in our
experiments (see Section 4.2). Figure 4 shows our
detailed WSD algorithm. Semantic coherence score
is generated by the function TreeMatching, and
we adopt a sentence as the context of a word.
We illustrate our WSD algorithm through an ex-
ample. Assume we try to disambiguate ?company?
in the sentence ?A large software company hires
many computer programmers?. ?company? has 9
senses as a noun in WordNet 2.1. Let?s pick the fol-
lowing two glosses to go through our WSD process.
? an institution created to conduct business
? small military unit
First we parse the original sentence and two
glosses, and get three weighted parsing trees as
shown in Figure 5. All weights are assigned to
nodes/words in these parsing trees. In the parsing
tree of the original sentence the weight of a node is
reciprocal of the distance between this node and to-
be-disambiguated node ?company? (line 12 in Fig-
ure 4). In the parsing tree of a gloss the weight
of a node is reciprocal of the level of this node in
the parsing tree (line 16 in Figure 4). Assume that
our context knowledge base contains relevant depen-
dency relations shown in Figure 6.
Input: Glosses from WordNet;
S: the sentence to be disambiguated;
G: the knowledge base generated in Section 2;
1. Input a sentence S, W = {w| w?s part of speech
is noun, verb, adjective, or adverb, w ? S};
2. Parse S with a dependency parser, generate
parsing tree TS ;
3. For each w ?W {
4. Input all w?s glosses from WordNet;
5. For each gloss wi {
6. Parse wi, get a parsing tree Twi;
7. score = TreeMatching(TS , Twi);
}
8. If the highest score is larger than a preset
threshold, choose the sense with the
highest score as the correct sense;
9. Otherwise, choose the first sense.
10. }
TreeMatching(TS , Twi)
11. For each node nSi ? TS {
12. Assign weight wSi = 1lSi , lSi is thelength between nSi and wi in TS ;
13. }
14. For each node nwi ? Twi {
15. Load its dependent words Dwi from G;
16. Assign weight wwi = 1lwi , lwi is thelevel number of nwi in Twi;
17. For each nSj {
18. If nSj ? Dwi
19. calculate connection strength sji
between nSj and nwi;
20. score = score + wSi ? wwi ? sji;
21. }
22. }
23. Return score;
Figure 4: WSD Algorithm
The weights in the context knowledge base are as-
signed to dependency relation edges. These weights
are normalized to [0, 1] based on the number of de-
pendency relation instances obtained in the acquisi-
tion and merging process. A large number of occur-
rences will be normalized to a high value (close to
1), and a small number of occurrences will be nor-
32
Figure 5: Weighted parsing trees of the original sentence
and two glosses of ?company?
Figure 6: A fragment of context knowledge base
malized to a low value (close to 0).
Now we load the dependent words of each word
in gloss 1 from the knowledge base (line 14, 15 in
Figure 4), and we get {small, large} for ?institu-
tion? and {large, software} for ?business?. In the
dependent words of ?company?, ?large? belongs to
the dependent word sets of ?institution? and ?busi-
ness?, and ?software? belongs to the dependent word
set of ?business?, so the coherence score of gloss 1
is calculated as (line 19, 20 in Figure 4):
1.0?1.0?0.7 + 1.0?0.25?0.8 + 1.0?0.25?0.9
= 1.125
We go through the same process with the second
gloss ?small military unit?. ?Large? is the only de-
pendent word of ?company? appearing in the depen-
dent word set of ?unit? in gloss 2, so the coherence
score of gloss 2 in the current context is:
1.0? 1.0? 0.8 = 0.8
After comparing the coherence scores of two
glosses, we choose sense 1 of ?company? as the cor-
rect sense (line 9 in Figure 4). This example illus-
trates that a strong dependency relation between a
head word and a dependent word has a powerful dis-
ambiguation capability, and disambiguation quality
is also significantly affected by the quality of dictio-
nary definitions.
In Figure 4 the TreeMatching function matches
the dependent words of to-be-disambiguated word
(line 15 in Figure 4), and we call this matching strat-
egy as dependency matching. This strategy will not
work if a to-be-disambiguated word has no depen-
dent words at all, for example, when the word ?com-
pany? in ?Companies hire computer programmers?
has no dependent words. In this case, we developed
the second matching strategy, which is to match the
head words that the to-be-disambiguated word is de-
pendent on, such as matching ?hire? (the head word
of ?company?) in Figure 5(a). Using the dependency
relation ?hire ? company?, we can correctly choose
sense 1 since there is no such relation as ?hire ?
unit? in the knowledge base. This strategy is also
helpful when disambiguating adjectives and adverbs
since they usually only depend on other words, and
rarely any other words are dependent on them. The
third matching strategy is to consider synonyms as a
match besides the exact matching words. Synonyms
can be obtained through the synsets in WordNet.
For example, when we disambiguate ?company? in
?Big companies hire many computer programmers?,
?big? can be considered as a match for ?large?. We
call this matching strategy as synonym matching.
The three matching strategies can be combined and
applied together, and in Section 4.1 we show the
experiment results of 5 different matching strategy
combinations.
33
4 Experiments
We have evaluated our method using SemEval-2007
Task 07 (Coarse-grained English All-words Task)
test set (Navigli et al, 2007). The task organiz-
ers provide a coarse-grained sense inventory cre-
ated with SSI algorithm (Navigli and Velardi, 2005),
training data, and test data. Since our method
does not need any training or special tuning, neither
coarse-grained sense inventory nor training data was
used. The test data includes: a news article about
?homeless? (including totally 951 words, 368 words
are annotated and need to be disambiguated), a re-
view of the book ?Feeding Frenzy? (including to-
tally 987 words, 379 words are annotated and need
to be disambiguated), an article about some trav-
eling experience in France (including totally 1311
words, 500 words are annotated and need to be dis-
ambiguated), computer programming(including to-
tally 1326 words, 677 words are annotated and need
to be disambiguated), and a biography of the painter
Masaccio (including totally 802 words, 345 words
are annotated and need to be disambiguated). Two
authors of (Navigli et al, 2007) independently and
manually annotated part of the test set (710 word
instances), and the pairwise agreement was 93.80%.
This inter-annotator agreement is usually considered
an upper-bound for WSD systems.
We followed the WSD process described in Sec-
tion 2 and 3 using the WordNet 2.1 sense repository
that is adopted by SemEval-2007 Task 07. All exper-
iments were performed on a Pentium 2.33GHz dual
core PC with 3GB memory. Among the 2269 to-
be-disambiguated words in the five test documents,
1112 words are unique and submitted to Google
API as queries. The retrieved Web pages were
cleaned, and 1945189 relevant sentences were ex-
tracted. On average 1749 sentences were obtained
for each word. The Web page retrieval step took 3
days, and the cleaning step took 2 days. Parsing was
very time-consuming and took 11 days. The merg-
ing step took 3 days. Disambiguation of 2269 words
in the 5 test articles took 4 hours. All these steps can
be parallelized and run on multiple computers, and
the whole process will be shortened accordingly.
The overall disambiguation results are shown in
Table 1. For comparison we also listed the re-
sults of the top three systems and three unsuper-
vised systems participating in SemEval-2007 Task
07. All of the top three systems (UoR-SSI, NUS-
PT, NUS-ML) are supervised systems, which used
annotated resources (e.g., SemCor, Defense Science
Organization Corpus) during the training phase. Our
fully unsupervised WSD system significantly out-
performs the three unsupervised systems (SUSSZ-
FR, SUSSX-C-WD, SUSSX-CR) and achieves per-
formance approaching the top-performing super-
vised WSD systems.
4.1 Impact of different matching strategies to
disambiguation quality
To test the effectiveness of different matching strate-
gies discussed in Section 3, we performed some ad-
ditional experiments. Table 2 shows the disambigua-
tion results by each individual document with the
following 5 matching strategies:
1. Dependency matching only.
2. Dependency and backward matching.
3. Dependency and synonym backward matching.
4. Dependency and synonym dependency match-
ing.
5. Dependency, backward, synonym backward,
and synonym dependency matching.
As expected combination of more matching
strategies results in higher disambiguation quality.
By analyzing the scoring details, we verified that
backward matching is especially useful to disam-
biguate adjectives and adverbs. Adjectives and ad-
verbs are often dependent words, so dependency
matching itself rarely finds any matched words.
Since synonyms are semantically equivalent, it is
reasonable that synonym matching can also improve
disambiguation performance.
4.2 Impact of knowledge base size to
disambiguation quality
To test the impact of knowledge base size to dis-
ambiguation quality we randomly selected 1339264
sentences (about two thirds of all sentences) from
our text collection and built a smaller knowledge
base. Table 3 shows the experiment results. Overall
disambiguation quality has dropped slightly, which
34
System Attempted Precision Recall F1
UoR-SSI 100.0 83.21 83.21 83.21
NUS-PT 100.0 82.50 82.50 82.50
NUS-ML 100.0 81.58 81.58 81.58
TreeMatch 100.0 73.65 73.65 73.65
SUSSZ-FR 72.8 71.73 52.23 60.44
SUSSX-C-WD 72.8 54.54 39.71 45.96
SUSSX-CR 72.8 54.30 39.53 45.75
Table 1: Overall disambiguation scores (Our system ?TreeMatch? is marked in bold)
Matching d001 d002 d003 d004 d005 Overall
strategy P R P R P R P R P R P R
1 72.28 72.28 66.23 66.23 63.20 63.20 66.47 66.47 56.52 56.52 65.14 65.14
2 70.65 70.65 70.98 70.98 65.20 65.20 72.23 72.23 58.84 58.84 68.18 68.18
3 79.89 79.89 75.20 75.20 69.00 69.00 71.94 71.94 64.64 64.64 72.01 72.01
4 80.71 80.71 78.10 78.10 72.80 72.80 71.05 71.05 67.54 67.54 73.65 73.65
5 80.16 80.16 78.10 78.10 69.40 69.40 72.82 72.82 66.09 66.09 73.12 73.12
Table 2: Disambiguation scores by article with 5 matching strategies
shows a positive correlation between the amount of
context knowledge and disambiguation quality. It is
reasonable to assume that our disambiguation per-
formance can be improved further by collecting and
incorporating more context knowledge.
Matching Overall
strategy P R
1 65.36 65.36
2 67.78 67.78
3 68.09 68.09
4 70.69 70.69
5 67.78 67.78
Table 3: Disambiguation scores by article with a smaller
knowledge base
5 Conclusion and Future Work
Broad coverage and disambiguation quality are crit-
ical for WSD techniques to be adopted in prac-
tice. This paper proposed a fully unsupervised
WSD method. We have evaluated our approach with
SemEval-2007 Task 7 (Coarse-grained English All-
words Task) data set, and we achieved F-scores ap-
proaching the top performing supervised WSD sys-
tems. By using widely available unannotated text
and a fully unsupervised disambiguation approach,
our method may provide a viable solution to the
problem of WSD. The future work includes:
1. Continue to build the knowledge base, enlarge
the coverage and improve the system perfor-
mance. The experiment results in Section 4.2
clearly show that more word instances can im-
prove the disambiguation accuracy and recall
scores;
2. WSD is often an unconscious process for hu-
man beings. It is unlikely that a reader exam-
ines all surrounding words when determining
the sense of a word, which calls for a smarter
and more selective matching strategy than what
we have tried in Section 4.1;
3. Test our WSD system on fine-grained SemEval
2007 WSD task 17. Although we only evalu-
ated our approach with coarse-grained senses,
our method can be directly applied to fine-
grained WSD without any modifications.
Acknowledgments
This work is partially funded by NSF grant 0737408
and Scholar Academy at the University of Houston
Downtown. This paper contains proprietary infor-
mation protected under a pending U.S. patent.
35
References
Agirre, Eneko, Philip Edmonds (eds.). 2006. Word
Sense Disambiguation: Algorithms and Applications,
Springer.
Chklovski, T. and Mihalcea, R. 2002. Building a sense
tagged corpus with open mind word expert. In Pro-
ceedings of the Acl-02 Workshop on Word Sense Dis-
ambiguation: Recent Successes and Future Directions,
Morristown, NJ, 116-122.
C. Fellbaum, WordNet: An Electronic Lexical Database,
MIT press, 1998
Project Gutenberg, available at www.gutenberg.org
Hearst, M. (1991) Noun Homograph Disambiguation Us-
ing Local Context in Large Text Corpora, Proc. 7th
Annual Conference of the University of Waterloo Cen-
ter for the New OED and Text Research, Oxford.
Nancy Ide and Jean Ve?ronis. 1998. Introduction to the
special issue on word sense disambiguation: the state
of the art. Comput. Linguist., 24(1):2?40.
Kister, Ken. ?Dictionaries defined?, Library Journal, Vol.
117 Issue 11, p43, 4p, 2bw
Lesk, M. 1986. Automatic sense disambiguation using
machine readable dictionaries: how to tell a pine cone
from an ice cream cone. In Proceedings of the 5th An-
nual international Conference on Systems Documenta-
tion (Toronto, Ontario, Canada). V. DeBuys, Ed. SIG-
DOC ?86.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the LREC Workshop on
the Evaluation of Parsing Systems, pages 234?241,
Granada, Spain.
Lin, D. 1997. Using syntactic dependency as local con-
text to resolve word sense ambiguity. In Proceedings of
the 35th Annual Meeting of the Association For Com-
putational Linguistics and Eighth Conference of the
European Chapter of the Association For Computa-
tional Linguistics (Madrid, Spain, July 07 - 12, 1997).
Rada Mihalcea, Using Wikipedia for Automatic Word
Sense Disambiguation, in Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL 2007), Rochester, April
2007.
Rada Mihalcea. 2002. Instance based learning with au-
tomatic feature selection applied to word sense disam-
biguation. In Proceedings of the 19th international
conference on Computational linguistics, pages 1?7,
Morristown, NJ.
Dan Moldovan and Vasile Rus, Explaining Answers with
Extended WordNet, ACL 2001.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-
grained english all-words task. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 30?35, Prague, Czech
Republic.
Roberto Navigli and Paola Velardi. 2005. Structural se-
mantic interconnections: a knowledge-based approach
to word sense disambiguation. IEEE Transactions on
Pattern Analysis and Machine Intelligence (PAMI),
27(7):10631074.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. Exploit-
ing Parallel Texts for Word Sense Disambiguation: An
Empirical Study. ACL, 2003.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrat-
ing multiple knowledge sources to disambiguate word
sense: an exemplar-based approach. In Proceedings of
the 34th annual meeting on Association for Computa-
tional Linguistics, pages 40?47, Morristown, NJ.
Adrian Novischi, Muirathnam Srikanth, and Andrew
Bennett. 2007. Lcc-wsd: System description for En-
glish coarse grained all words task at semeval 2007.
In Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), pages 223?
226, Prague, Czech Republic.
Catherine Soanes and Angus Stevenson, editors. 2003.
Oxford Dictionary of English. Oxford University
Press.
Rada Mihalcea, available at
http://www.cs.unt.edu/ rada/downloads.html
Yarowsky, D. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceedings
of the 33rd Annual Meeting on Association For Com-
putational Linguistics (Cambridge, Massachusetts,
June 26 - 30, 1995).
36
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 396?401,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
TreeMatch: A Fully Unsupervised WSD System Using Dependency 
Knowledge on a Specific Domain 
Andrew Tran          Chris Bowes         David Brown         Ping Chen 
University of Houston-Downtown 
Max Choly           Wei Ding 
University of Massachusetts-Boston 
Abstract 
Word sense disambiguation (WSD) is one of 
the main challenges in Computational 
Linguistics. TreeMatch is a WSD system 
originally developed using data from SemEval 
2007 Task 7 (Coarse-grained English All-
words Task) that has been adapted for use in 
SemEval 2010 Task 17 (All-words Word 
Sense Disambiguation on a Specific Domain). 
The system is based on a fully unsupervised 
method using dependency knowledge drawn 
from a domain specific knowledge base that 
was built for this task. When evaluated on the 
task, the system precision performs above the 
First Sense Baseline. 
1 Introduction 
There are many words within natural 
languages that can have multiple meanings or 
senses depending on its usage. These words are 
called homographs. Word sense disambiguation 
is the process of determining which sense of a 
homograph is correct in a given context. Most 
WSD systems use supervised methods to 
identify senses and tend to achieve the best 
results.  However, supervised systems rely on 
manually annotated training corpora. 
Availability of manually tagged corpora is 
limited and generating these corpora is costly 
and time consuming. With our TreeMatch 
system, we use a fully unsupervised domain-
independent method that only requires a 
dictionary (WordNet, Fallbaum, 1998.) and 
unannotated text as input (Chen et.al, 2009). 
WSD systems trained on general corpora tend 
to perform worse when disambiguating words 
from a document on a specific domain. The 
SemEval 2010 WSD-domain task (Agirre et. al., 
2010) addresses this issue by testing participant 
systems on documents from the environment 
domain. The environment domain specific 
corpus for this task  
was built from documents contributed by the 
European Centre for Nature Conservation  
 
 
(ECNC) and the World Wildlife Fund (WWF). 
We adapted our existing TreeMatch system from  
running on a general context knowledge base to 
one targeted at the environment domain. 
This paper is organized as follows. Section 2 
will detail the construction of the knowledge 
base. In Section 3 the WSD algorithm will be 
explained. The construction procedure and WSD 
algorithm described in these two sections are 
similar to the procedure presented in our 
NAACL 2009 paper (Chen et.al, 2009). In 
Section 4 we present our experiments and 
results, and Section 5 discusses related work on 
WSD. Section 6 finishes the paper with 
conclusions. 
2 Context Knowledge Acquisition and 
Representation 
Figure 1 shows an overview of our context 
knowledge acquisition process. The collected 
knowledge is saved in a local knowledge base. 
Here are some details about each step. 
 
Figure 1: Context Knowledge Acquisition and 
Representation Process 
2.1 Corpus Building Through Web Search 
The goal of this step is to collect as many 
valid sample sentences as possible that contain 
instances of the target word. Preferably these 
instances are also diverse enough to contain all 
the different glosses of a word. 
396
The World Wide Web is a boundless source 
of textual information that can be utilized for 
corpus building. This huge dynamic text 
collection represents a wide cross section of 
writing backgrounds that may not be represented 
in other corpora and may be able to better 
represent common human knowledge. 
However, because the content on the internet 
is not necessarily checked for grammatical or 
factual accuracy, concerns may arise about the 
use of a corpus built from it. The quality of 
context knowledge will be affected by sentences 
of poor linguistic and poor word usage but from 
our experience these kind of errors are negligible 
when weighted against the staggering volume of 
valid content also retrieved. 
To start the acquisition process, words that are 
candidates for disambiguation are compiled and 
saved in a text file as seeds for search queries. 
Each single word is submitted to a Web search 
engine as a query. Several search engines 
provide API?s for research communities to 
automatically retrieve large number of Web 
pages. In our experiments we used MSN Bing! 
API (Bing!, 2010) to retrieve up to 1,000 Web 
pages and PDF documents for each to-be-
disambiguated word. Collected Web pages are 
cleaned first, e.g., control characters and HTML 
tags are removed. Then sentences are segmented 
simply based on punctuation (e.g., ?, !, .). PDF 
files undergo a similar cleaning process, except 
that they are converted from PDF to HMTL 
beforehand. Sentences that contain the instances 
of a specific word are extracted and saved into a 
local repository. 
2.2 Parsing 
After the sentences have been cleaned and 
segmented they are sent to the dependency 
parser Minipar (Lin, 1998). After parsing, 
sentences are converted to parsing trees and 
saved into files. The files contain the weights of 
all connections between all words existing 
within the knowledge base. Parsing tends to take 
the most time in the entire WSD process. 
Depending on the initial size of the corpus, 
parsing can take weeks. The long parsing time 
can be attributed to Minipar?s execution through 
system calls and also to the lack of 
multithreading used. However, we only need to 
parse the corpus once to construct the knowledge 
base. Any further parsing is only done on the 
input sentences from the words to-be-
disambiguated, and the glosses of those words. 
2.3 Merging dependency relations 
After parsing, dependency relations from 
different sentences are merged and saved in a 
context knowledge base. The merging process is 
straightforward. A dependency relation includes 
one head word/node and one dependent 
word/node. Nodes from different dependency 
relations are merged into one as long as they 
represent the same word. An example is shown 
in Figure 2, which merges the following two 
sentences: 
?Computer programmers write software.? 
?Many companies hire computer 
programmers.? 
 
 
Figure 2: Merging two parsing trees. The number 
beside each edge is the number of occurrences of this 
dependency relation existing in the context 
knowledge base. 
 
In a dependency relation ?word1 -> word2?, 
word1 is the head word, and word2 is the 
dependent word. After merging dependency 
relations, we will obtain a weighted directed 
graph with a word as a node, a dependency 
relation as an edge, and the number of 
occurrences of dependency relation as weight of 
an edge. This weight indicates the strength of 
semantic relevancy of head word and dependent 
word. This graph will be used in the following 
WSD process as our context knowledge base. As 
a fully automatic knowledge acquisition process, 
it is inevitable to include erroneous dependency 
relations in the knowledge base. However, since 
in a large text collection valid dependency 
relations tend to repeat far more times than 
invalid ones, these erroneous edges only have 
minimal impact on the disambiguation quality as 
shown in our evaluation results. 
397
3 WSD Algorithm 
Our WSD approach is based on the following 
insight: 
If a word is semantically coherent with its 
context, then at least one sense of this word is 
semantically coherent with its context. 
Assuming that the documents given are 
semantically coherent, if we replace a targeted 
to-be-disambiguated word with its glosses one 
by one, eventually one of the glosses will have 
semantic coherence within the context of its 
sentence. From that idea we can show the 
overview of our WSD procedure in Figure 3. For 
a given to-be-disambiguated word, its glosses 
from WordNet are parsed one by one along with 
the original sentence of the target word. The 
semantic coherency between the parse tree of 
each individual gloss and the parse tree of the 
original sentence are compared one by one to 
determine which sense is the most relevant.  
 
Figure 3: WSD Procedure 
To measure the semantic coherence we use 
the following hypotheses (assume word1 is the 
to-be-disambiguated word): 
? If in a sentence word1 is dependent on word2, 
and we denote the gloss of the correct sense 
of word1 as g1i, then g1i contains the most 
semantically coherent words that are 
dependent on word2; 
? If in a sentence a set of words DEP1 are 
dependent on word1, and we denote the gloss 
of the correct sense of word1 as g1i, then g1i 
contains the most semantically coherent 
words that DEP1 are dependent on. 
These hypotheses are used for the functions in 
Figure 4. The TreeMatching function uses what 
we call dependency matching to ascertain the 
correct sense of the to-be-disambiguated word. 
NodeMatching function is an extension from 
Lesk algorithm (Lesk, 1986). 
 
Input: Glosses from WordNet; 
S: the to-be-disambiguated sentence; 
G: the knowledge base generated in Section 2; 
1. Input a sentence S, W = {w| w?s part of speech 
is noun, verb, adjective, or adverb, w ? S}; 
2. Parse S with a dependency parser, generate 
parsing tree TS; 
3. For each w ?W {  
4.  Input all w?s glosses from WordNet; 
5.  For each gloss wi {  
6.  Parse wi, get a parsing tree Twi; 
7.   scored = TreeMatching(TS, Twi); 
Scoren = NodeMatching(TS, Twi); 
} 
8. If the highest scored and Scoren indicate 
the sense, choose this sense; 
9. Otherwise, choose the first sense. 
10. } 
 
TreeMatching(TS, Twi) 
11. For each node nSi ?TS { 
12. Assign weight wSi = 
1
??????
 , lSi is the 
length between nSi and wi in TS; 
13. } 
14. For each node nwi ? Twi { 
15. Load its dependent words Dwi from G; 
16.  Assign weight wwi = 
1
??????
, lwi is the 
level number of nwi in Twi; 
17.  For each nSj { 
18.   If nSj ? Dwi 
19.   calculate connection strength sji 
between nSj and nwi; 
20.   score = score + wSi ? wwi ? sji; 
21.  } 
      }  
22. Return score; 
 
NodeMatching (TS, Twi) 
23. For each node nSi ?TS { 
24. Assign weight wwi = 
1
??????
, lwi is the 
level number of nwi in Twi; 
25. For each nSj { 
28.  If nSi == wwi 
29.  score = score + wSi ? wwi  
} 
      } 
Figure 4: WSD Algorithm 
4 Experiment 
The WSD-domain task for SemEval 2010 
focused on the environment domain. To prepare 
for the tests, we constructed a new domain 
specific knowledge base.  
 
398
Table 1: Fine-Grained SemEval 2010 Task 17 
Disambiguation Scores 
 
Since we knew the task?s domain specific 
corpus would be derived from ECNC and WWF 
materials, we produced our query list from the 
same source. A web crawl starting from both the 
ECNC and WWF main web pages was 
performed that retrieved 772 PDF documents. 
Any words that were in the PDFs and also had 
more than one gloss in WordNet were retained 
for Bing! search queries to start the acquisition 
process as described in section 2. 10779 unique 
words were obtained in this manner. 
Using the 10779 unique words for search 
queries, the web page and PDF retrieval step 
took 35 days, collecting over 3 TB of raw html 
and PDF files, and the cleaning and sentence 
extraction step took 2 days, reducing it down to 
3 GB of relevant sentences, while running on 5 
machines. Parsing took 26 days and merging 
took 6 days on 9 machines. From the parse trees 
we obtained 2202295 total nodes with an 
average of 87 connections and 13 dependents per 
node. 
Each machine was a 2.66 GHz dual core PC 
with 2 GB of memory with a total of 10 
machines used throughout the process. 
There were 3 test documents provided by the 
task organizers with about 6000 total words and 
1398 to-be-disambiguated words. 
Disambiguation of the target words took 1.5 
hours for each complete run. Each run used the 
same WSD procedure with different parameters. 
The overall disambiguation results are shown 
in Table 1. The precision of our best submission 
edged out the First Sense Baseline (1sense) 
baseline by .001 and is ahead of the Random 
selection baseline by .276. 
The recall of our submissions is lower than 
the precision because of our reliance on Minipar 
for the part of speech and lemma information of 
the target words. Sometimes Minipar would give 
an incorrect lemma which at times cannot be 
found in WordNet and thus our system would 
not attempt to disambiguate the words. Previous 
tasks provided the lemma and part of speech for 
target words so we were able to bypass that step. 
5 Related work 
Generally WSD techniques can be divided into 
four categories (Agirre, 2006), 
? Dictionary and knowledge based methods. 
These methods use lexical knowledge bases 
(LKB) such as dictionaries and thesauri, and 
extract knowledge from word definitions 
(Lesk, 1986) and relations among 
words/senses. Recently, several graph-based 
WSD methods were proposed. In these 
approaches, first a graph is built with senses 
as nodes and relations among words/senses 
(e.g., synonymy, antonymy) as edges, and 
the relations are usually acquired from a 
LKB (e.g., Wordnet). Then a ranking 
algorithm is conducted over the graph, and 
senses ranked the highest are assigned to the 
corresponding words. Different relations and 
ranking algorithms were experimented with 
these methods, such as TexRank algorithm 
(Mihalcea, 2005), personalized PageRank 
algorithm (Agirre, 2009), a two-stage 
searching algorithm (Navigli, 2007), 
Structural Semantic Interconnections 
algorithm (Navigli, 2005), centrality 
algorithms (Sinha, 2009). 
? Supervised methods. A supervised method 
includes a training phase and a testing phase. 
In the training phase, a sense-annotated 
training corpus is required, from which 
syntactic and semantic features are extracted 
to build a classifier using machine learning 
techniques, such as Support Vector Machine 
(Novisch, 2007). In the following testing 
phase, the classifier picks the best sense for a 
word based on its surrounding words 
(Mihalcea, 2002). Currently supervised 
methods achieved the best disambiguation 
quality (about 80% in precision and recall 
for coarse-grained WSD in the most recent 
WSD evaluation conference SemEval 2007 
(Novisch, 2007). Nevertheless, since training 
corpora are manually annotated and 
expensive, supervised methods are often 
brittle due to data scarcity, and it is 
impractical to manually annotate huge 
number of words existing in a natural 
language. 
? Semi-supervised methods. To overcome the 
knowledge acquisition bottleneck suffered in 
supervised methods, semi-supervised 
methods make use of a small annotated 
corpus as seed data in a bootstrapping 
process (Hearst, 1991) (Yarowsky, 1995). A 
System Precision Recall 
1sense 0.505 0.505 
TreeMatch-1 0.506 0.493 
TreeMatch-2 0.504 0.491 
TreeMatch-3 0.492 0.479 
Random 0.23 0.23 
399
word-aligned bilingual corpus can also serve 
as seed data (Zhong, 2009). 
? Unsupervised methods. These methods 
acquire knowledge from unannotated raw 
text, and induce senses using similarity 
measures (Lin, 1997). Unsupervised 
methods overcome the problem of 
knowledge acquisition bottleneck, but none 
of existing methods can outperform the most 
frequent sense baseline, which makes them 
not useful at all in practice. The best 
unsupervised systems only achieved about 
70% in precision and 50% in recall in the 
SemEval 2007 (Navigli, 2007). One recent 
study utilized automatically acquired 
dependency knowledge and achieved 73% in 
precision and recall (Chen, 2009), which is 
still below the most-frequent-sense baseline 
(78.89% in precision and recall in the 
SemEval 2007 Task 07). 
Additionally there exist some ?meta-
disambiguation? methods that ensemble multiple 
disambiguation algorithms following the ideas of 
bagging or boosting in supervised learning 
(Brody, 2006). 
6 Conclusion 
This paper has described a WSD system 
which has been adapted for use in a specific 
domain for SemEval 2010 Task 17: All-Words 
Word Sense Disambiguation on a Specific 
Domain. Our system has shown that domain 
adaptation can be handled by unsupervised 
systems without the brittleness of supervised 
methods by utilizing readily available 
unannotated text from internet sources and still 
achieve viable results.  
Acknowledgments 
This work is partially funded by National 
Science Foundation grants CNS 0851984 and 
DHS #2009-ST-061-C10001. 
References  
E. Agirre, Philip Edmonds, editors. Word Sense     
Disambiguation: Algorithms and Applications, 
Springer. 2006. 
E. Agirre, O. Lopez de Lacalle, C. Fellbaum, S. 
Hsieh, M. Tesconi, P. Vossen, and R. Segers. 
SemEval-2010 Task 17: All-words Word Sense 
Disambiguation on a Specific Domain. In 
Proceedings of the 5th International Workshop on 
Semantic Evaluations(SemEval-2010), 
Association for Computational Linguistics, 
Uppsala, Sweden. 2010. 
E. Agirre, A. Soroa. Personalizing pagerank for word 
sense disambiguation. In Proceedings of the 12th 
conference of the European chapter of the 
Association for Computational Linguistics 
(EACL-2009). 
Bing! API, available at msdn.microsoft.com 
A. Brody, R. Navigli, M. Lapata, Ensemble Methods 
For Unsupervised WSD, COLING-ACL, 2006 
P. Chen, W. Ding, C. Bowes, D. Brown. 2009. A 
Fully Unsupervised Word Sense Disambiguation 
Method and Its Evaluation on Coarse-grained All-
words Task, NAACL 2009. 
C. Fellbaum. 1998. WordNet: An Electronic Lexical 
Database, MIT press, 1998 
M. Hearst. Noun Homograph Disambiguation Using 
Local Context in Large Text Corpora. Proc. 7th 
Annual Conference of the Univ. of Waterloo 
Center for the New OED and Text Research, 
Oxford. 1991. 
M. Lesk. 1986. Automatic sense disambiguation 
using machine readable dictionaries: how to tell a 
pine cone from an ice cream cone. In Proceedings 
of the 5th Annual international Conference on 
Systems Documentation (Toronto, Ontario, 
Canada). V. DeBuys, Ed. SIGDOC ?86. 
D. Lin. Using syntactic dependency as local context 
to resolve word sense ambiguity. In Proceedings 
of the 35th Annual Meeting of the Association For 
Computational Linguistics and Eighth Conference 
of the European Chapter of the Association For 
Computational Linguistics.  1997. 
D. Lin. 1998. Dependency-based evaluation of 
minipar. In Proceedings of the LREC Workshop 
on the Evaluation of Parsing Systems, pages 234?
241, Granada, Spain. 
R. Mihalcea. Unsupervised Large-Vocabulary Word 
Sense Disambiguation with Graph-based 
Algorithms for Sequence Data Labeling, in 
Proceedings of the Joint Conference on Human 
Language Technology Empirical Methods in 
Natural Language Processing (HLT/EMNLP), 
Vancouver, October, 2005. 
R. Mihalcea. Instance based learning with automatic 
feature selection applied to word sense 
disambiguation. In Proceedings of the 19th 
International Conference on Computational 
linguistics. 2002. 
R. Navigli, Mirella Lapata. Graph Connectivity 
Measures for Unsupervised Word Sense 
Disambiguation. IJCAI 2007 
R. Navigli, Paola Velardi. Structural semantic 
interconnections: a knowledge-based approach to 
word sense disambiguation. IEEE Transactions on 
Pattern Analysis and Machine Intelligence 
(PAMI), 27(7):1063-1074. 2005. 
A. Novischi, Muirathnam Srikanth, and Andrew 
Bennett. Lcc-wsd: System description for English 
400
coarse grained all words task at semeval 2007. 
Proceedings of the Fourth International Workshop 
on Semantic Evaluations (SemEval-2007), pages 
223--226, Prague, Czech Republic. 2007. 
R. Sinha, Rada Mihalcea. Unsupervised Graph-based 
Word Sense Disambiguation, in ?Current Issues in 
Linguistic Theory: Recent Advances in Natural 
Language Processing?, Editors Nicolas Nicolov 
and Ruslan Mitkov, John Benjamins, 2009. 
D. Yarowsky. Unsupervised word sense 
disambiguation rivaling supervised methods. In 
Proceedings of the 33rd Annual Meeting on 
Association For Computational Linguistics, 
Cambridge, Massachusetts, 1995. 
Z. Zhong, Hwee Tou Ng. Word Sense 
Disambiguation for All Words without Hard 
Labor. In Proceeding of the Twenty-first 
International Joint Conference on Artificial 
Intelligence. 2009. 
401
