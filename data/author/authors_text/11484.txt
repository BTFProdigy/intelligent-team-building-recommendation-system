Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1086?1095,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Empirical Exploitation of Click Data for Task Specific Ranking
Anlei Dong Yi Chang Shihao Ji Ciya Liao Xin Li Zhaohui Zheng
Yahoo! Labs
701 First Avenue
Sunnyvale, CA 94089
{anlei,yichang,shihao,ciyaliao,xinli,zhaohui}@yahoo-inc.com
Abstract
There have been increasing needs for task
specific rankings in web search such as
rankings for specific query segments like
long queries, time-sensitive queries, navi-
gational queries, etc; or rankings for spe-
cific domains/contents like answers, blogs,
news, etc. In the spirit of ?divide-and-
conquer?, task specific ranking may have
potential advantages over generic ranking
since different tasks have task-specific fea-
tures, data distributions, as well as feature-
grade correlations. A critical problem for
the task-specific ranking is training data
insufficiency, which may be solved by us-
ing the data extracted from click log. This
paper empirically studies how to appro-
priately exploit click data to improve rank
function learning in task-specific ranking.
The main contributions are 1) the explo-
ration on the utilities of two promising ap-
proaches for click pair extraction; 2) the
analysis of the role played by the noise
information which inevitably appears in
click data extraction; 3) the appropriate
strategy for combining training data and
click data; 4) the comparison of click data
which are consistent and inconsistent with
baseline function.
1 Introduction
Learning-to-rank approaches (Liu, 2008) have
been widely applied in commercial search en-
gines, in which ranking models are learned using
labeled documents. Significant efforts have been
made in attempt to learn a generic ranking model
which can appropriately rank documents for all
queries . However, web users? query intentions are
extremely heterogeneous, which makes it difficult
for a generic ranking model to achieve best rank-
ing results for all queries. For this reason, there
have been increasing needs for task specific rank-
ings in web search such as rankings for specific
query segments like long queries, time-sensitive
queries, navigational queries, etc; or rankings
for specific domains/contents like answers, blogs,
news, etc. Therefore, a specific ranking task usu-
ally correspond to a category of queries; when
the search engine determines that a query is be-
longing to this category, it will call the ranking
function dedicated to this ranking task. The mo-
tivation of this divide-and-conquer strategy is that,
task specific ranking may have potential advan-
tages over generic ranking since different tasks
have task-specific features, data distributions, as
well as feature-grade correlations.
Such a dedicated ranking model can be trained
using the labeled data belonging to this query cat-
egory (which is called dedicated training data).
However, the amount of training data dedicated
to a specific ranking task is usually insufficient
because human labeling is expensive and time-
consuming, not to mention there are multiple rank-
ing tasks that need to be taken care of. To deal
with the training data insufficiency problem for
task-specific ranking, we propose to extract click-
through data and incorporate it with dedicated
training data to learn a dedicated model.
In order to incorporate click data to improve the
ranking for a dedicate query category, it is critical
to fully exploit click information. We empirically
explore the related approaches for the appropriate
click data exploitation in task-specific rank func-
tion learning. Figure 1 illustrates the procedures
and critical components to be studied.
1) Click data mining: the purpose is to extract
informative and reliable users? preference infor-
mation from click log. We employ two promis-
ing approaches: one is heuristic rule approach, the
other is sequential supervised learning approach.
2) Sample selection and combination: with la-
beled training data and unlabeled click data, how
1086
Generic training data
Dedicated training data
GBrank algorithm
Task-specific ranking model
Generic click data
Dedicated click data
Sample selection and combination
Click log Click data mining? Heuristic-rule-based approach
? Sequential supervised learning approach
Figure 1: Framework of incorporating click-
through data with training data to improve dedi-
cated model for task-specific ranking.
to select and combine them so that the samples
have the best utility for learning? As the data
distribution for a specific ranking task is differ-
ent from the generic data distribution, it is nat-
ural to select those labeled training samples and
unlabeled click preference pairs which belong to
this query category, so that the data distributions
of training set and testing set are consistent for
this category. On the other hand, we should keep
in mind that: a) non-dedicated data, i.e, the data
that does not belong the specific category, might
also have similar distribution as the dedicated data.
Such distribution similarity makes non-dedicated
data also useful for task-specific rank function
learning, especially for the scenario that dedicated
training samples is insufficient. b) The quality of
dedicated click data may be not as reliable as hu-
man labeled training data. In other words, there
are some extracted click preference pairs that are
inconsistent with human labeling while we regard
human labeling as correct labeling.
3) Rank function learning algorithm: we use
GBrank (Zheng et al, 2007) algorithm for rank
function learning, which has proved to be one
of the most effective up-to-date learning-to-rank
algorithms; furthermore, GBrank algorithm also
takes preference pairs as inputs, which will be il-
lustrated with more details in the paper.
2 Related work
Learning to rank has been a promising research
area which continuously improves web search rel-
evance (Burges et al, 2005) (Zha et al, 2006)
(Cao et al, 2007) (Freund et al, 1998) (Fried-
man, 2001) (Joachims, 2002) (Wang and Zhai,
2007) (Zheng et al, 2007). The ranking prob-
lem is usually formulated as learning a ranking
function from preference data. The basic idea
is to minimize the number of contradicted pairs
in the training data, and different algorithm cast
the preference learning problem from different
point of view, for example, RankSVM (Joachims,
2002) uses support vector machines; RankBoost
(Freund et al, 1998) applies the idea of boost-
ing from weak learners; GBrank (Zheng et al,
2007) uses gradient boosting with decision tree;
RankNet (Burges et al, 2005) uses gradient boost-
ing with neural net-work. In (Zha et al, 2006),
query difference is taken into consideration for
learning effective retrieval function, which leads
to a multi-task learning problem using risk mini-
mization framework.
There are a few related works to apply multi-
ple ranking models for different query categories.
However, none of them takes click-through infor-
mation into consideration. In (Kang and Kim,
2003), queries are categorized into 3 types, infor-
mational, navigational and transactional, and dif-
ferent models are applied on each query category.
a KNN method is proposed to employ different
ranking models to handle different types of queries
(Geng et al, 2008). The KNN method is unsuper-
vised, and it targets to improve the overall ranking
instead of the rank-ing for a certain query cate-
gory. In addition, the KNN method requires all
feature vector to be the same.
Quite a few research papers explore how to ob-
tain useful information from click-through data,
which could benefit search relevance (Carterette
et al, 2008) (Fox et al, 2005) (Radlinski and
Joachims, 2007) (Wang and Zhai, 2007). The in-
formation can be expressed as pair-wise prefer-
ences (Chapelle and Zhang, 2009) (Ji et al, 2009)
(Radlinski et al, 2008), or represented as rank fea-
tures (Agichtein et al, 2006). Task-specific rank-
ing relies on the accuracy of query classification.
Query classification or query intention identifica-
tion has been extensively studied in (Beitzel et al,
2007) (Lee et al, 2005) (Li et al, 2008) (Rose and
Levinson, 2004). How to combine editorial data
and click data is well discussed in (Chen et al,
2008) (Zheng et al, 2007). In addition, how to use
click data to improve ranking are also exploited
in personalized or preference-based search (Coyle
1087
Table 1: Statistics of click occurrences for heuris-
tic rule approach.
imp impression, number of occurrence of the tuple
cc number of occurrence of the tuple where two
documents both get clicked
ncc number of occurrence of the tuple where url
1
is not clicked but url
2
is clicked
cnc number of occurrence of the tuple where url
1
is clicked but url
2
is not clicked
ncnc number of occurrence of the tuple where url
1
and url
2
are not clicked
and Smyth, 2007) (Glance, 2001) (R. Jin, 2008).
3 Technical approach
This section presents the related approaches in
Figure 1. In Section 4, we will make deeper anal-
ysis based on experimental results.
3.1 Click data mining
We use two approaches for click data mining,
whose outputs are preference pairs. A preference
pair is defined as a tuple {< x
q
, y
q
> |x
q
? y
q
},
which means for the query q, the document x
q
is
more relevant than y
q
. We need to extract infor-
mative and reliable preference pairs which can be
used to improve rank function learning.
3.1.1 Heuristic rule approach
We use heuristic rules to extract skip-above pairs
and skip-next pairs, which are similar to Strategy
1 (click > skip above) and Strategy 5 (click > no-
click next) proposed in (Joachims et al, 2005). To
reduce the misleading effect of an individual click
behavior, click information from different query
sessions is aggregated before applying heuristic
rules. For a tuple (q, url
1
, url
2
, pos
1
, pos
2
) where
q is query, url
1
and url
2
are urls representing two
documents, pos
1
and pos
2
are ranking positions
for the two documents with pos
1
? pos
2
mean-
ing url
1
has higher rank than url
2
, the statistics for
this tuple are listed in Table 1.
Skip-above pair extraction: if ncc is much
larger than cnc, and
cc
imp
,
ncnc
imp
is much smaller
than 1, that means, when url
1
is ranked higher than
url
2
in query q, most users click url
2
but not click
url
1
. In this case, we extract a skip-above pair, i.e.,
url
2
is more relevant than url
1
. In order to have
highly accurate skip-above pairs, a set of thresh-
Table 2: Skip-above pairs count vs. human judge-
ments (e.g., the element in the third row and sec-
ond column means we have 40 skip-above pairs
with ?excellent? url
1
and ?perfect? url
2
). P: per-
fect; E: excellent; G: good; F: fair; B: bad.
P E G F B
P 13 13 12 4 0
E 40 44 16 2 2
G 27 53 103 29 8
F 10 15 43 27 5
B 4 4 11 20 14
Table 3: Skip-next pairs vs. human judgements
(e.g., the element in the third row and second col-
umn means we have 10 skip-next pairs with ?ex-
cellent? url
1
and ?perfect? url
2
). P: perfect; E:
excellent; G: good; F: fair; B: bad.
P E G F B
P 126 343 225 100 35
E 10 71 84 37 12
G 6 9 116 56 21
F 1 5 17 29 14
B 1 1 1 2 5
olds are applied to only extract the pairs that have
high impression and ncc is larger enough than cnc.
Skip-next pair extraction: if pos
1
= pos
2
? 1,
cnc is much larger than ncc, and
cc
imp
,
ncnc
imp
is much
smaller than 1, that means, in most of cases when
url
2
is ranked just below url
1
in query q, most
users click url
1
but not click url
2
. In this case, we
regard this tuple as a skip-next pair.
To test the accuracy of preference pairs, we
ask editors to judge some randomly selected pairs
from skip-above pairs and skip-next pairs. Edi-
tors label each query-url pair using five grades ac-
cording to relevance: perfect, excellent, good, fair,
bad. Table 2 shows skip-above pair distribution.
The diagonal elements have high values, which
are for tied pairs labeled by editors but determined
as skip-above pairs from heuristic rules. Higher
values appear in the left-bottom triangle than in
the right-top triangle, because there are more skip-
above preferences agreed with editors than dis-
agreed with editors. Summing up the tied pairs,
agreed and disagreed pairs, 44% skip-above pref-
erence judgments agree with editors, 18% skip-
above preference judgments disagree with editors,
1088
and there are 38% skip-above pairs judged as tie
pairs by editors.
Table 3 shows skip-next pair distribution. Sum-
ming up the tied pairs, agreed and disagreed pairs,
70% skip-next preference judgments agree with
editors, 4% skip-next preference judgments dis-
agree with editors, and 26% skip-next pairs judged
as tie pairs by editors.
Therefore, skip-next pairs have much higher
accuracy than skip-above. That is because in a
search engine that already has a good ranking
function, it is much easier to find a correct skip-
next pairs which are consistent with the search en-
gine than to find a correct skip-above pairs which
are contradictory to the search engine. Skip-above
and skip-next preferences provide us two kinds of
users?s feedbacks which are complementary: skip-
above preferences provide us the feedback that the
user?s vote is contradictory to the current ranking,
which implies the current relative ranking should
be reversed; skip-next preferences shows that the
user?s vote is consistent with the current ranking,
which implies the current relative ranking should
be maintained with high confidence provided by
users? vote.
3.1.2 Sequential supervised learning
The click modeling by sequential supervised
learning (SSL) was proposed in (Ji et al, 2009),
in which user?s sequential click information is
exploited to extract relevance information from
click-logs. This approach is reliable because 1)
the sequential click information embedded in an
aggregation of user clicks provides substantial rel-
evance information of the documents displayed in
the search results, and 2) the SSL is supervised
learning (i.e., human judgments are provided with
relevance labels for the training).
The SSL is formulated in the framework
of global ranking (Qin et al, 2008). Let
x
(q)
= {x
(q)
1
, x
(q)
2
, . . . , x
(q)
n
} represent the doc-
uments retrieved with a query q, and y
(q)
=
{y
(q)
1
, y
(q)
2
, . . . , y
(q)
n
} represent the relevance la-
bels assigned to the documents. Here n is the
number of documents retrieved with q. Without
loss of generality, we assume that n is fixed and
invariant with respect to different queries. The
SSL determines to find a function F in the form
of y
(q)
=F (x
(q)
) that takes all the documents as
its inputs, exploiting both local and global infor-
mation among the documents, and predict the rel-
evance labels of all the document jointly. This
is distinct to most of learning to rank methods
that optimize a ranking model defined on a sin-
gle document, i.e., in the form of y
(q)
i
=f(x
(q)
i
),
? i = 1, 2, . . . , n. This formulation of the SSL
is important in extracting relevance information
from user click data since users? click decisions
among different documents displayed in a search
session tend to rely not only on the relevance judg-
ment of a single document, but also on the relative
relevance comparison among the documents dis-
played; and the global ranking framework is well-
formulated to exploit both local and global infor-
mation from an aggregation of user clicks.
The SSL aggregates all the user sessions for
the same query into a tuple <query, n-document
list, and an aggregation of user clicks>. Fig-
ure 2 illustrates the process of feature extrac-
tion from an aggregated session, where x
(q)
=
{x
(q)
1
, x
(q)
2
, . . . , x
(q)
n
} denotes a sequence of fea-
ture vectors extracted from the aggregated ses-
sion, with x
(q)
i
representing the feature vector ex-
tracted for document i. Specifically, to form fea-
ture vector x
(q)
i
, first a feature vector x
(q)
i,j
is ex-
tracted from each user j?s click information, and
j ? {1, 2, . . . }, then x
(q)
i
is formed by averaging
over x
(q)
i,j
, ?j ? {1, 2, . . . }, i.e., x
(q)
i
is actually an
aggregated feature vector for document i. Table
4 lists all the features used in the SSL modeling.
Note that some features are statistics independent
of temporal information of the clicks, such as ?Po-
sition? and ?Frequency?, while other features re-
ply on their surrounding documents and the click
sequences. We use 90,000 query-url pairs to train
the SSL model, and 10,000 query-url pairs for best
model selection.
With the sequential click modeling discussed
above, several sequential supervised algorithms,
including the conditional random fields (CRF)
(Lafferty et al, 2001), the sliding window method
and the recurrent sliding window method (Diet-
terich, 2002), are explored to find a global ranking
function F . We omit the details but refer one to
(Ji et al, 2009). The emphasis here is on the im-
portance to adapt these algorithms to the ranking
problem.
After training, the SSL model can be used to
predict the relevance labels of all the documents in
a new aggregated session, and thus pair-wise pref-
erence data can be extracted, with the score dif-
ference representing the confidence of preference
1089
={
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 }
?
o
o
user2
doc10
odoci
oodoc2
odoc1 user1
q
?
?
Feature Extraction
x
(q)
x1
x2
xi
x10
?
?
={
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 }
y
(q)
y1
y2
yi
y10
?
?
Figure 2: An illustration of feature extraction for
an aggregated session for SSL approach. x
(q)
de-
notes an extracted sequence of feature vectors, and
y
(q)
denotes the corresponding label sequence that
is assigned by human judges for training.
Table 4: Click features used in SSL model.
Position Position of the document
in the result list
ClickRank Rank of 1st click of doc. in click seq.
Frequency Average number of clicks for this doc.
FrequencyRank Rank in the list sorted by num. of clicks
IsNextClicked 1 if next position is clicked, 0 otherwise
IsPreClicked 1 if previous position is clicked,
0 otherwise
IsAboveClicked 1 if there is a click above, 0 otherwise
IsBelowClicked 1 if there is a click below, 0 otherwise
ClickDuration Time spent on the document
prediction. For the reason of convenience, we also
call the preference pairs contradicting with pro-
duction ranking as skip-above pairs and those con-
sistent with production ranking as skip-next pairs,
so that we can analyze these two types of prefer-
ence pairs respectively.
3.2 Modeling algorithm
The basic idea of GBrank (Zheng et al, 2007)
is that if the ordering of a preference pair
by the ranking function is contradictory to this
preference, we need to modify the ranking
function along the direction by swapping this
prefence pair. Preferences pairs could be gen-
erated from labeled data, or could be extracted
from click data. For each preference pair <
x, y > in the available preference set S =
{< x
i
, y
i
> |x
i
? y
i
, i = 1, 2, ..., N}, x should
be ranked higher than y. In GBrank algorithm, the
problem of learning ranking functions is to com-
pute a ranking function h , so that h matches the
set of preference, i.e, h(x
i
) ? h(y
i
) , if x ? y,
i = 1, 2, ..., N as many as possible. The following
loss function is used to measure the risk of a given
ranking function h.
R(h) =
1
2
N
?
i=1
(max{0, h(y
i
)?h(x
i
)+?})
2
, (1)
where ? is the margin between the two documents
in the pair. To minimize the loss function, h(x) has
to be larger than h(y) with the margin ? , which can
be chosen as constant value, or as dynamic val-
ues varying with pairs. When pair-wise judgments
are extracted from editors? labels with different
grades, pair-wise judgments can include grade dif-
ference, which can further be used as margin ? .
The GBrank algorithm is illustrated in Algorithm
1, and two parameters need to be determined: the
shrinkage factor ? and the number of iteration.
Algorithm 1 GBrank algorithm.
Start with an initial guess h
0
, for m = 1, 2, ...
1. Construct a training set: for each < x
i
, y
i
>?
S, derive (x
i
,max{0, h
m?1
(y
i
) ? h
m
1
(x
i
) +
?}), and
(y
i
,?max{0, h
m?1
(y
i
)? h
m
1
(x
i
) + ?}).
2. Fit h
m
by using a base regressor with the
above training set.
3. h
m
= h
m?1
+?s
m
h
m
(x), where s
m
is found
by line search to minimize the object function,
? is shrinkage factor.
3.3 Sample selection and combination
We use a straightforward approach to learn rank-
ing model from the combined data, which is illus-
trated in Algorithm 2.
Algorithm 2 Learn ranking model by combining
editorial data and click preference pairs.
Input:
? Editorial absolute judgement data.
? Preference pairs from click data.
1. Extract preference pairs from labeled data
with absolute judgement.
2. Select and combine preference pairs from
click data and labeled data.
3. Learn GBrank model from the combined
preference pairs.
Absolute judgement on labeled data contains
(query, url) pairs with absolute grade values la-
beled by human. In Step 1, for each query with
1090
nq
query-url pairs with corresponding grades, {<
query, url
i
, grade
i
> |i = 1, 2, . . . , n
q
}, its prefer-
ence pairs are extracted as
{< query, url
i
, url
j
, grade
i
? grade
j
> |i, j =
1, 2, . . . , n
q
, i 6= j} .
When combining human-labeled pairs and click
preference pairs, we can give use different relative
weights for these two data sources. The loss func-
tion becomes
R(h) =
w
N
l
?
i?Labeled
(max{0, h(y
i
)? h(x
i
) + ?})
2
1? w
N
c
?
i?Click
(max{0, h(y
i
)? h(x
i
) + ?})
2
,(2)
where w is used to control the relative weights be-
tween labeled training data and click data, N
l
is
the number of training data pairs, and N
c
is the
number of click pairs. The margin ? can be deter-
mined as grade difference for editor pairs, and be
a constant parameter for click pairs.
Step 2 is critical for the efficacy of the approach.
A few factors need to be considered:
1) data distribution: for the application of task-
specific ranking, our purpose is to improve ranking
for the queries belonging to this category. An im-
portant observation is that the relevance patterns
for the ranking within a specific category may
have some unique characteristics, which are differ-
ent from generic relevance ranking. Thus, it is rea-
sonable to consider only using dedicated labeled
training data and dedicated click preference data
for training. The reality is that dedicated training
data is usually insufficient, while it is possible that
non-dedicated data can also help the learning.
2) click pair quality: it is inevitable there exist
some incorrect pairs in the click preference pairs.
Such incorrect pairs may mislead the learning. So
overall, can the click preference pairs still help the
learning for task-specific ranking? By our study,
skip-above pairs usually contain more incorrect
pairs compared with skip-above pairs. Does this
mean skip-next pairs are always more helpful in
improving learning than skip-above pairs?
3) click pair utility: use labeled training data as
baseline, how much complimentary information
can click pairs bring? This is determined by the
methodology of click data mining approach.
While it is possible to achieve some learning
improvement for task-specific ranking by using
click pairs by a plausible method, we attempt to
empirically explore the above interweaving fac-
tors for deeper understanding, in order to apply the
most appropriate strategy to exploit click data on
real-world applications of task-specific ranking.
4 Experiments
4.1 Data set
Query category: in the experiments, we use long
query ranking as an example of task-specific rank-
ing, because it is commonly known that long query
ranking has some unique relevance patterns com-
pared with generic ranking. We define the long
queries as the queries containing at least three to-
kens. The techniques and analysis proposed in this
paper can be applied to other ranking tasks, such
as rankings for specific query segments like time-
sensitive queries, navigational queries, or rankings
for specific domains/contents like answers, blogs,
news, as long as the tasks have their own charac-
teristics of data distributions and discriminant rank
features.
Labeled training data: we do experiments
based on a data set for a commercial search en-
gine, for which there are 16,797 query-url pairs
(with 1,123 different queries) that have been la-
beled by editors. The proportion of long queries
is about 35% of all queries. The data distribution
of such long queries may be different from gen-
eral data distribution, as it will be validated in the
experiments below.
The human labeled data is randomly split into
two sets: training set (8,831 query-url pairs, 589
queries), and testing set (7,966 query-url pairs,
534 queries). The training set will be combined
with click preference pairs for rank function learn-
ing, and the testing set will be used to evaluate the
efficacy of the ranking function. In the training set,
there are 3,842 long query-url pairs (229 queries).
At testing stage, the learned rank functions are ap-
plied only to the long queries in the testing data,
as our concern in this paper is how to improve
task-specific ranking, i.e., long query ranking in
the experiment. In the testing data, there are 3,210
query-url pairs (193 queries) are long query data,
which will be used to test rank functions.
Click preference pairs: using the two ap-
proaches of heuristic rule approach and sequen-
tial supervised approach, we extract click prefence
pairs from the click log of the search engine. Each
approach yields both skip-next and skip-above
pairs, which are sorted by confidence descending
order respectively.
1091
Table 5: Use click data by heuristic rule approach
(Data Selection: ?N?: not use; ?D?: use dedicated
data; ?G?: use generic data. Data Source: ?T?:
training data; ?C?: click data)
(a) skip-next pairs
NT DT GT
NC n/a 0.7736 0.7813
DC 0.7822 0.7906 (1.2%) 0.7997(2.4%)
GC 0.7834 0.7908 (1.2%) 0.7950 (1.7%)
(b) skip-above pairs
NT DT GT
NC n/a 0.7736 0.7813
DC 0.6649 0.7676 (-1.6%) 0.7748 (-0.8%)
GC 0.6792 0.7656 (-2.0%) 0.7989 (2.2%)
4.2 Setup and measurements
We try different sample selection and combination
strategies to train rank functions using GBrank al-
gorithm. For the labeled training data, we either
use generic data or dedicated data. For the click
preference pairs, we also try these two options.
Furthermore, as more click preference pairs may
bring more useful information to help the learn-
ing while on the other hand, the more incorrect
pairs may be given so that they mislead the learn-
ing, we try different amounts of these prefence
pairs: 5,000, 10,000, 30,000, 50,000, 70,000 and
100,000 pairs.
We use NDCG to evaluate ranking model,
which is defined as
NDCG
n
= Z
n
?
n
i=1
2
r(i)
?1
log(i+1)
where i is the position in the document list, r(i) is
the score of Document i, and Z
n
is a normalization
factor, which is used to make the NDCG of ideal
list be 1.
4.3 Results
Table 5 and 6 show the NDCG
5
results by using
heuristic rule approach and SSL approach respec-
tively. We do not present NDCG
1
results due to
space limitation, but NDCG
1
results have the sim-
ilar trends as NDCG
5
.
Baseline by training data: there are two base-
line functions by using training data sets 1) use
dedicated training data (DT), NDCG
5
on the test-
ing set by the rank function is 0.7736; 2) use
generic training data (GT), NDCG
5
is 0.7813. It
is reasonable that using generic training data is
Table 6: Use click data by SSL approach (Data
Selection: ?N?: not use; ?D?: use dedicated data;
?G?: use generic data. Data Source: ?T?: training
data; ?C?: click data)
(a) skip-next pairs
NT DT GT
NC n/a 0.7736 0.7813
DC 0.7752 0.7933 (1.5%) 0.7936 (1.5%)
GC 0.7624 0.7844 (0.4%) 0.7914 (1.2%)
(b) skip-above pairs
NT DT GT
NC n/a 0.7736 0.7813
DC 0.6756 0.7636 (-2.2%) 0.7784 (-0.3%)
GC 0.6860 0.7717 (-1.2%) 0.7774 (-0.5%)
better than only using dedicated training data, be-
cause the distributions of non-dedicated data and
dedicated data share some similarity. As the ded-
icated training data is insufficient, the adoption of
the extra non-dedicated data helps the learning.
We compare learning results with Baseline 2) (use
generic training data, the slot of NC + GT in the
tables), which is the higher baseline.
Baseline by click data: we then study the utili-
ties of click preference pairs by using them alone
for training without using labeled training data.
In Table 5 and 6, each of the NDCG
5
results us-
ing click preference pairs is the highest NDCG
5
value over the cases of using different amounts of
pairs (5000, 10,000, 30,000, 50,000, 70,000 and
100,000 pairs). The results regarding the pairs
amounts are illustrated in Figure 3, which will help
us to analyze the results more deeply.
If we only use click preference pairs for training
(the two table slots DC+NT and GC+NT, corre-
sponding to using dedicated click preference pairs
and generic click pairs respectively), the best case
is using skip-next pairs extracted by heuristic rule
approach (Table 5 (a) ). It is not surprising that
skip-next pairs outperform skip-above pairs be-
cause there are significantly lower percentage of
incorrect pairs in skip-next pairs compared with
skip-above pairs. It is a little bit surprising that
the case of DC+NT has no dominant advantage
over GC+NT as we expected. For example, in Ta-
ble 5 (a), the NDCG
5
values (0.7822 and 0.7834)
are very close to each other. However, in Figure
3, we find that with the same amount of pairs,
when we use 30,000 or fewer pairs, using dedi-
1092
1 2 3 4 5 6 7 8 9 10
x 104
0.755
0.76
0.765
0.77
0.775
0.78
0.785
0.79
0.795
0.8
click pairs
ND
CG
5
 
 
dedicate train + dedicate clickgeneric train + dedicate clickdedicate clickgeneric click
Figure 3: Incorporate different amounts of skip-
next pairs by heuristic rule approach with generic
training data.
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0.76
0.77
0.78
0.79
0.8
0.81
trainging sample weight
ND
CG
5
 
 
+ 5,000 dedicate click
+ 10,000 dedicate click
+ 30,000 dedicate click
Figure 4: The effects of using different combin-
ing weights. Skip-next pairs by heuristic rule ap-
proach are combined with generic training data.
cated click pairs alone is always better than using
generic click pairs alone. With more click pairs
being used (> 30, 000), the noise rates become
higher in the pairs, which makes the distribution
factor less important.
Combine training data and click data: we
compare the four table slots, DC+DT, GC+DT,
DC+GT, GC+GT, in Table 5 and 6, and there are
quite a few interesting observations:
1) Skip-next vs. skip-above: overall, incorporat-
ing skip-next pairs with training data is better than
incorporating skip-above pairs, due to the reason
that there are more incorrect pairs in skip-above
pairs, which may mislead the learning. The only
exception is the slot GC+GT in Table 5 (b), whose
NDCG
5
improvement is as high as 2.2%. We fur-
ther track this result, and find that this is the case
by using only 5,000 generic skip-above pairs. The
noise rate of these 5,000 pairs is low because they
have the highest pair extraction confidence values.
At the same time, these 5,000 pairs may provide
good complementary signals to the generic train-
ing data, so that the learning result is good. How-
ever, in general, skip-next pairs have better utilities
than skip-above pairs.
2) Dedicated training data vs. generic train-
ing data: using generic training data is gen-
erally better than only using dedicated training
data. If training data is insufficient, the extra
non-dedicated data provides useful information
for relevance pattern learning, and the distribu-
tion dissimilarity between dedicated data and non-
dedicated data is not the most important factor.
3) Dedicated click data vs. generic click data:
using dedicated click data is more effective than
using generic click data. From Figure 3, we ob-
serve that when 30,000 or fewer pairs are incorpo-
rated into training data, using dedicate click pairs
is always better than using generic click pairs.
0 0.5 1 1.5 2 2.50.784
0.786
0.788
0.79
0.792
0.794
0.796
0.798
0.8
0.802
?
ND
CG
5
 
 
+ 5,000 dedicate click
+ 10,000 dedicate click
+ 30,000 dedicate click
Figure 5: The effects of using different margin
values for click preference pairs. Skip-next pairs
by heuristic rule approach are incorporated with
generic training data.
4) Heuristic rule approach vs. SSL approach:
the preference pairs extracted by heuristic rule ap-
proach have better utilities than those extracted by
SSL approach.
5) GBrank parameters for combining training
data and click pairs: the relative weight w for
combining training data and click pairs in (2) may
also affect rank function learning. Figure 4 shows
the effects of using different combining weights,
1093
for which skip-next pairs by heuristic rule ap-
proach are combined with generic training data.
We observe that neither over-weighting training
data or over-weighting click pairs yields good re-
sults while the two data sources are best exploited
at certain weight values when there is good bal-
ance between them. Another concern is the ap-
propriate margin value ? for the click pairs in (2).
Figure 5 shows that ? = 1 consistently yields good
learning results, which suggests us that click pair
provides good information at ? = 1.
4.4 Discussions
we have defactorized the related approaches for
exploiting click data to improve task-specific rank
learning. The utility of click preference pairs de-
pends on the following factors:
1) Data distribution: if click pairs have good
quality, we should use dedicated click pairs in-
stead of generic click pairs, so that the samples
for training have similar distribution to the task of
task-specific ranking.
2) The amount of dedicated training data: the
more dedicated training data, the more reliable the
task-specific rank function is; thus, the less room
for learning improvement using click data. For the
case in the experiment that dedicated training is in-
sufficient, the non-dedicated training data can also
help the learning as non-dedicated training data
share relevance pattern similarity with the dedi-
cated data distribution.
3) The quality of click pairs: if we can extract
large amount of high-quality click pairs, the learn-
ing improvement will be significant. For example,
as shown in Figure 3, at the early stage with fewer
click pairs (5,000 and 10,000 pairs) being com-
bined with training data, the learning improvement
is best. With more click pairs are used, the noise
rate in the click pairs becomes higher so that the
learning misleading factor is more important than
information complementary factor. Thus, it is im-
portant to improve the reliability of the click pairs.
4) The utility of click pairs: by our study, the
quality of click pairs extracted by SSL approach
is comparable to those extracted by heuristic rule
approach. The possible reason that heuristic-rule-
based click pairs can bring more benefit is that
these pairs provide more complementary infor-
mation compared with SSL approach. As the
methodologies of these two click data extraction
approaches are totally different, in future we will
explore the concrete reason that causes such utility
difference.
5 Conclusions
By empirically exploring the related factors in
utilizing click-through data to improve dedicated
model learning for task-specific ranking, we have
better understood the principles of using click
preference pairs appropriately, which is impor-
tant for the real-world applications in commer-
cial search engines as using click data can sig-
nificantly save human labeling costs and makes
rank function learning more efficient. In the case
that dedicated training data is limited, while non-
dedicated training data is helpful, using dedicated
skip-next pairs is the most effective way to further
improve the learning. Heuristic rule approach pro-
vides more useful click pairs compared with se-
quential supervised learning approach. The qual-
ity of click pairs is critical for the efficacy of the
approach. Therefore, an interesting topic is how
to further reduce the inconsistency between skip-
above pairs and human labeling so that such data
may also be useful for task-specific ranking.
1094
References
E. Agichtein, E. Brill, and S. Dumais. 2006. Improv-
ing web search ranking by incorporating user behav-
ior information. Proc. of ACM SIGIR Conference.
S. M. Beitzel, E. C. Jensen, A. Chowdhury, and
O. Frieder. 2007. Varying approaches to topical
web query classification. Proceedings of ACM SI-
GIR conference.
C. Burges, T. Shaked, E. Renshaw, A. Lazier,
M. Deeds, N. Hamilton, and G. Hullender. 2005.
Learning to rank using gradient descent. Proc. of
Intl. Conf. on Machine Learning.
Z. Cao, T. Qin, T. Liu, M. Tsai, and H. Li. 2007.
Learning to rank: From pairwise approach to list-
wise. Proceedings of ICML conference.
B. Carterette, P. N. Bennett, D. M. Chickering, and S. T.
Dumais. 2008. Here or there: preference judgments
for relevance. Proc. of ECIR.
O. Chapelle and Y. Zhang. 2009. A dynamic bayesian
network click model for web search ranking. Pro-
ceedings of the 18th International World Wide Web
Conference.
K. Chen, Y. Zhang, Z. Zheng, H. Zha, and G. Sun.
2008. Adapting ranking functions to user prefer-
ence. ICDE Workshops, pages 580?587.
M. Coyle and B. Smyth. 2007. Supporting intelligent
web search. ACM Transaction Internet Tech., 7(4).
T. G. Dietterich. 2002. Machine learning for sequen-
tial data: a review. Lecture Notes in Computer Sci-
ence, (2396):15?30.
S. Fox, K. Karnawat, M. Mydland, S. Dumias, and
T. White. 2005. Evaluating implicit measures to
improve web search. ACM Trans. on Information
Systems, 23(2):147?168.
Y. Freund, R. D. Iyer, R. E. Schapire, and Y. Singer.
1998. An efficient boosting algorithm for combin-
ing preferences. Proceedings of International Con-
ference on Machine Learning.
J. Friedman. 2001. Greedy function approximation: a
gradient boosting machine. Ann. Statist., 29:1189?
1232.
X. Geng, T. Liu, T. Qin, A. Arnold, H. Li, and H. Shum.
2008. Query dependent ranking with k nearest
neighbor. Proceedings of ACM SIGIR Conference.
N. S. Glance. 2001. Community search assistant. In-
telligent User Interfaces, pages 91?96.
S. Ji, K. Zhou, C. Liao, Z. Zheng, G. Xue, O. Chapelle,
G. Sun, and H. Zha. 2009. Global ranking by ex-
ploiting user clicks. In SIGIR?09, Boston, USA, July
19-23.
T. Joachims, L. Granka, B. Pan, and G Gay. 2005.
Accurately interpreting clickthough data as implicit
feedback. Proc. of ACM SIGIR Conference.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proceedings of the ACM Con-
ference on Knowledge Discovery and Data Mining
(KDD).
I. Kang and G. Kim. 2003. Query type classification
for web document retrieval. Proceedings of ACM
SIGIR Conference.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML, pages
282?289.
U. Lee, Z. Liu, and J. Cho. 2005. Automatic identifi-
cation of user goals in web search. Proceedings of
International Conference on World Wide Web.
X. Li, Y.Y Wang, and A. Acero. 2008. Learning query
intent from regularized click graphs. Proceedings of
ACM SIGIR Conference.
T. Y Liu. 2008. Learning to rank for information re-
trieval. SIGIR tutorial.
T. Qin, T. Liu, X. Zhang, D. Wang, and H. Li. 2008.
Global ranking using continuous conditional ran-
dom fields. In NIPS.
H. Li R. Jin, H. Valizadegan. 2008. Ranking re-
finement and its application to information retrieval.
Proceedings of International Conference on World
Wide Web.
F. Radlinski and T. Joachims. 2007. Active exploration
for learning rankings from clickthrough data. Proc.
of ACM SIGKDD Conference.
F. Radlinski, M. Kurup, and T. Joachims. 2008. How
does clickthrough data reflect retrieval quality? Pro-
ceedings of ACM CIKM Conference.
D. E. Rose and D. Levinson. 2004. Understanding user
goals in web search. Proceedings of International
Conference on World Wide Web.
X. Wang and C. Zhai. 2007. Learn from web search
logs to organize search results. In Proceedings of
the 30th ACM SIGIR.
H. Zha, Z. Zheng, H. Fu, and G. Sun. 2006. Incor-
porating query difference for learning retrieval func-
tions in world wide web search. Proceedings of the
15th ACM Conference on Information and Knowl-
edge Management.
Z. Zheng, H. Zhang, T. Zhang, O. Chapelle, K. Chen,
and G. Sun. 2007. A general boosting method and
its application to learning ranking functions for web
search. NIPS.
1095
Proceedings of NAACL HLT 2009: Short Papers, pages 165?168,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Search Result Re-ranking by Feedback Control Adjustment for
Time-sensitive Query
Ruiqiang Zhang? and Yi Chang? and Zhaohui Zheng?
Donald Metzler? and Jian-yun Nie?
?Yahoo! Labs, 701 First Avenue, Sunnyvale, CA94089
?University of Montreal, Montreal, Quebec,H3C 3J7, Canada
?{ruiqiang,yichang,zhaohui,metzler}@yahoo-inc.com
?nie@iro.umontreal.ca
Abstract
We propose a new method to rank a special
category of time-sensitive queries that are year
qualified. The method adjusts the retrieval
scores of a base ranking function according
to time-stamps of web documents so that the
freshest documents are ranked higher. Our
method, which is based on feedback control
theory, uses ranking errors to adjust the search
engine behavior. For this purpose, we use
a simple but effective method to extract year
qualified queries by mining query logs and a
time-stamp recognition method that considers
titles and urls of web documents. Our method
was tested on a commercial search engine. The
experiments show that our approach can sig-
nificantly improve relevance ranking for year
qualified queries even if all the existing meth-
ods for comparison failed.
1 Introduction
Relevance ranking plays a crucial role in search
engines. There are many proposed machine learn-
ing based ranking algorithms such as language
modeling-based methods (Zhai and Lafferty, 2004),
RankSVM (Joachims, 2002), RankBoost (Freund et al,
1998) and GBrank (Zheng et al, 2007). The input to
these algorithms is a set of feature vectors extracted from
queries and documents. The goal is to find the parameter
setting that optimizes some relevance metric given
training data. While these machine learning algorithms
can improve average relevance, they may be ineffctive
for certain special cases. Time-sensitive queries are one
such special case that machine-learned ranking functions
may have a hard time learning, due to the small number
of such queries.
Consider the query ?sigir? (the name of a conference),
which is time sensitive. Table 1 shows two example
search result pages for the query, SERP1 and SERP2. The
query: sigir
SERP1 url1: http://www.sigir.org
url2: http://www.sigir2008.org
url3: http://www.sigir2004.org
url4: http://www.sigir2009.org
url5: http://www.sigir2009.org/schedule
SERP2 url1: http://www.sigir.org
url2: http://www.sigir2009.org
url3: http://www.sigir2009.org/schedule
url4: http://www.sigir2008.org
url5: http://www.sigir2004.org
Table 1: Two contrived search engine result pages
ranking of SERP2 is clearly better than that of SERP1 be-
cause the most recent event, ?sigir2009?, is ranked higher
than other years.
Time is an important dimension of relevance in web
search, since users tend to prefer recent documents to old
documents. At the time of this writing (February 2009),
none of the major commercial search engines ranked the
homepage for SIGIR 2009 higher than previous SIGIR
homepages for the query ?sigir?. One possible reason for
this is that ranking algorithms are typically based on an-
chor text features, hyperlink induced features, and click-
through rate features. However, these features tend to fa-
vor old pages more than recent ones. For example, ?si-
gir2008? has more links and clicks than ?sigir2009? be-
cause ?sigir2008? has existed longer time and therefore
has been visited more. It is less likely that newer web
pages from ?sigir2009? can be ranked higher using fea-
tures that implicitly favor old pages.
However, the fundamental problem is that current ap-
proaches have focused on improving general ranking al-
gorithms. Methods for improving ranking of specific
types of query like temporal queries are often overlooked.
Aiming to improve ranking results, some methods of
re-ranking search results are proposed, such as the work
by (Agichtein et al, 2006) and (Teevan et al, 2005).
165
Search Engine
Detector 
Controller
error R(q, yn)R(q, yo)
_
+
Figure 1: Feedback control for search engine
These work uses user search behavior information or per-
sonalization information as features that are integrated
into an enhanced ranking model. We propose a novel
method of re-ranking search results. This new method
is based on feedback control theory, as illustrated in 1.
We make a Detector to monitor search engine (SE) out-
put and compare it with the input, which is the desired
search engine ranking. If an error is found, we design
the controller that uses the error to adjust the search en-
gine output, such that the search engine output tracks the
input. We will detail the algorithm in Section 4.1.
Our method was applied to a special class of time-
sensitive query, year qualified queries (YQQs). For this
category, we found users either attached a year with the
query explicitly, like ?sigir 2009?, or used the query only
without a year attached,like ?sigir?. We call the former
explicit YQQs, and the latter implicit YQQs. Using query
log analysis, we found these types of queries made up
about 10% of the total query volume. We focus exclu-
sively on implicit YQQs by translating the user?s im-
plicit intention as the most recent year. Explicit YQQs
are less interesting, because the user?s temporal inten-
tion is clearly specified in the query. Therefore, rank-
ing for these types of queries is relatively straightfor-
ward. Throughout the remainder of this paper, we use
the ?YQQ? to refer to implicit YQQs, unless otherwise
stated.
2 Adaptive score adjustment
Our proposed re-ranking model is shown in Eq. 1, as be-
low.
F(q, d) =
{
R(q, d) if q < YQQ
R(q, d) + Q(q, d) otherwise
Q(q, d) =
{ (e(do, dn) + k)e??(q) if y(d) = yn
0 otherwise
e(do, dn) = R(q, do) ? R(q, dn)
(1)
This work assumes that a base ranking function is used
to rank documents with respect to an incoming query. We
denote this base ranking function as R(q, d). This ranking
function is conditioned on a query q and a document d. It
is assumed to model the relevance between q and d.
Our proposed method is flexible for all YQQ queries.
Suppose the current base ranking function gives the re-
sults as SERP1 of Table 1. To correct the ranking, we
propose making an adjustment to R(q, d).
In Eq. 1, F(q, d) is the final ranking function. If the
query is not an YQQ, the base ranking function is used.
Otherwise, we propose an adjustment function, Q(q, d) ,
to adjust the base ranking function. Q(q, d) is controlled
by the ranking error, e(do, dn), signifying the base func-
tion ranking error if the newest web page dn is ranked
lower than the oldest web page do. y(d) is the year that
the event described by d has occurred or will occur. If
yo and yn indicate the oldest year and the newest year,
then y(do) = yo, y(dn) = yn. R(q, do) and R(q, dn) are the
base ranking function scores for the oldest and the newest
documents.
k is a small shift value for direction control. When
k < 0, the newest document is adjusted slightly under the
old one. Otherwise, it is adjusted slightly over the old
one. Experiments show k > 0 gave better results. The
value of k is determined in training.
?(q) is the confidence score of a YQQ query, mean-
ing the likelihood of a query to be YQQ. The confidence
score is bigger if a query is more likely to be YQQ. More
details are given in next section. ? is a weighting param-
eter for adjusting ?(q).
The exp function e??(q) is a weighting to control boost-
ing value. A higher value, confidence ?, a larger boosting
value, Q(q, d).
Our method can be understood by feedback control
theory, as illustrated in Fig. 1. The ideal input is R(q, yo)
representing the desired ranking score for the newest
Web page, R(q, yn). But the search engine real output
is R(q, yn). Because search engine is a dynamic system,
its ranking is changing over time. This results in ranking
errors, e(do, dn) = R(q, do) ? R(q, dn). The function of
?Controller? is to design a function to adjust the search
engine ranking so that the error approximates to zero,
e(do, dn) = 0. For this work, ?Controller? is Q(q, d).
?Detector? is a document year-stamp recognizer, which
will be described more in the next section. ?Detector?
is used to detect the newest Web pages and their ranking
scores. Fig. 1 is an ideal implementation of our methods.
We cannot carry out real-time experiments in this work.
Therefore, the calculation of ranking errors was made in
offline training.
3 YQQ detection and year-stamp
recognition
To implement Eq. 1, we need to find YQQ queries and to
identify the year-stamp of web documents.
Our YQQ detection method is simple, efficient, and
relies only on having access to a query log with frequency
information. First, we extracted all explicit YQQs from
166
query log. Then, we removed all the years from explicit
YQQs. Thus, implicit YQQs are obtained from explicit
YQQs. The implicit YQQs are saved in a dictionary. In
online test, we match input queries with each of implicit
YQQs in the dictionary. If an exact match is found, we
regard the input query as YQQ, and apply Eq. 1 to re-rank
search results.
After analyzing samples of the extracted YQQs, we
group them into three classes. One is recurring-event
query, like ?sigir?, ?us open tennis?; the second is news-
worthy query, like ?steve ballmer?, ?china foreign re-
serves?; And the class not belong to any of the above
two, like ?christmas?, ?youtube?. We found our proposed
methods were the most effective for the first category. In
Eq. 1, we can use confidence ?(q) to distinguish the three
categories and their change of ranking as shown in Eq.1,
that is defined as below.
?(q) =
?
y w(q, y)
#(q) +?y w(q, y)
(2)
where w(q, y) = #(q.y)+#(y.q). #(q.y) denotes the num-
ber of times that the base query q is post-qualified with
the year y in the query log. Similarly, #(y.q) is the num-
ber of times that q is pre-qualified with the year y. This
weight measures how likely q is to be qualified with y,
which forms the basis of our mining and analysis. #(q) is
the counts of independent query, without associating with
any other terms.
We also need to know the year-stamp y(d) for each
web document so that the ranking score of a document
is updated if y(d) = yn is satisfied. We can do this
from a few sources such as title, url, anchar text, and
extract date from documents that is possible for many
news pages. For example, from url of the web page,
?www.sigir2009.org?, we detect its year-stamp is 2009.
We have also tried to use some machine generated
dates. However, in the end we found such dates are in-
accurate and cannot be trusted. For example, discovery
time is the time when the document was found by the
crawler. But a web document may exist several years be-
fore a crawler found it. We show the worse effect of using
discovery time in the experiments.
4 Experiments
We will describe the implementation methods and experi-
mental results in this section. Our methods include offline
dictionary building and online test. In offline training, our
first step is to mine YQQs. A commercial search engine
company provided us with six months of query logs. We
extracted a list of YQQs using Section 3?s method. For
each of the YQQs, we run the search engine and output
the top N results. For each document, we used the method
described in Section 3 to recognize the year-stamp and
find the oldest and the newest page. If there are multiple
urls with the same yearstamp, we choose the first oldest
and the first most recent. Next,we calculated the boost-
ing value according to Eq. 1. Each query has a boosting
value. For online test, a user?s query is matched with each
of the YQQs in the dictionary. If an exact match is found,
the boosting value will be added to the base ranking score
iff the document has the newest yearstamp.
For evaluating our methods, we randomly extracted
600 YQQs from the dictionary. We extracted the top-5
search results for each of queries using the base ranking
function and the proposed ranking function. We asked
human editors to judge all the scraped results. We used
five judgment grades: Perfect, Excellent, Good, Fair,
and Bad. Editors were instructed to consider temporal
issues when judging. For example, sigir2004 is given
a worse grade than sigir2009. To avoid bias, we ad-
vised editors to retain relevance as their primary judg-
ment criteria. Our evaluation metric is relative change
in DCG, %?dcg = DCGproposed?DCGbaselineDCGbaseline , where DCG is
the traditional Discounted Cumulative Gain (Jarvelin and
Kekalainen, 2002).
4.1 Effect of the proposed boosting method
Our experimental results are shown in Table 2, where we
compared our work with the existing methods. While we
cannot apply (Li and Croft, 2003)?s approach directly be-
cause first, our search engine is not based on language
modeling; second, it is impossible to obtain exact times-
tamp for web pages as (Li and Croft, 2003) did in the
track evaluation. However, we tried to simulate (Li and
Croft, 2003)?s approach in web search by using the linear
integration method exactly as the same as(Li and Croft,
2003) by adding a time-based function with our base
ranking function. For the timestamp, we used discovery
time in the time-based function. The parameters (?, ?)
have the exact same meaning as in (Li and Croft, 2003)
but were tuned according to our base ranking function.
With regards to the approach by (Diaz and Jones, 2004),
we ranked the web pages in decreasing order of discov-
ery time. Our own approaches were tested under options
with and without using adaptation. For no adaption, we
let the e of Eq.1 equal to 0, meaning no score difference
between the oldest document and the newest document
was captured, but a constant value was used. It is equiv-
alent to an open loop in Fig.1. For adaption, we used the
ranking errors to adjust the base ranking. In the Table we
used multiple ks to show the effect of changing k. Using
different k can have a big impact on the performance. The
best value we found was k = 0.3. In this experiment, we
let ?(q) = 0 so that the result responds to k only.
Our approach is significantly better than the existing
methods. Both of the two existing methods produced
worse results than the baseline, which shows the ap-
167
Li & Croft (?, ?)=(0.2,2.0) -0.5
(?, ?)=(0.2,4.0) -1.2
Diaz & Jones -4.5?
No adaptation (e = 0, k=0.3 1.2
open loop) k=0.4 0.8
Adaptation (closed loop) k=0.3 6.6?
k=0.4 6.2?
Table 2: %?dcg of proposed method comparing with
existing methods.A sign ??? indicates statistical signifi-
cance (p-value<0.05)
? 0 0.2 0.4 0.6 0.8 1.0
%?dcg 6.6? 7.8? 8.4? 4.5 2.1 -0.2?
Table 3: Effect of confidence as changing ?.
proaches may be inappropriate for Web search. Not sur-
prisingly, using adaption achieved much better results
than without using adaption. Thus, these experiments
prove the effectiveness of our proposed methods.
Another important parameter in the Eq.1 is the confi-
dence score ?(q), which indicates the confidence of query
to be YQQ. In Eq. 1, ? is used to adjusting ?(q). We
observed dcg gain for each different ?. The results are
shown in Table 3. The value of ? needs to be tuned for
different base ranking functions. A higher ? can hurt per-
formance. In our experiments, the best value of 0.4 gave a
8.4% statistically significant gain in DCG. The ? = 0 set-
ting means we turn off confidence, which results in lower
performance. Thus, using YQQ confidence is effective.
5 Discussions and conclusions
In this paper, we proposed a novel approach to solve
YQQ ranking problem, which is a problem that seems
to plague most major commercial search engines. Our
approach for handling YQQs does not involve any query
expansion that adds a year to the query. Instead, keeping
the user?s query intact, we re-rank search results by ad-
justing the base ranking function. Our work assumes the
intent of YQQs is to find documents about the most recent
year. For this reason, we use YQQ confidence to measure
the probability of this intent. As our results showed, our
proposed method is highly effective. A real example is
given in Fig. 2 to show the significant improvement by
our method.
Our adaptive methods are not limited to YQQs only.
We believe this framework can be applied to any category
of queries once a query classification and a score detector
have been implemented.
Figure 2: Ranking improvement for query ICML by our
method: before re-rank(left) and after(right)
References
Eugene Agichtein, Eric Brill, and Susan Dumais. 2006.
Improving web search ranking by incorporating user
behavior information. In SIGIR ?06, pages 19?26.
Fernando Diaz and Rosie Jones. 2004. Using temporal
profiles of queries for precision prediction. In Proc.
27th Ann. Intl. ACM SIGIR Conf. on Research and De-
velopment in Information Retrieval, pages 18?24, New
York, NY, USA. ACM.
Yoav Freund, Raj D. Iyer, Robert E. Schapire, and Yoram
Singer. 1998. An efficient boosting algorithm for
combining preferences. In ICML ?98: Proceedings
of the Fifteenth International Conference on Machine
Learning, pages 170?178.
Kalervo Jarvelin and Jaana Kekalainen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Transactions on Information Systems, 20:2002.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In KDD ?02: Proceedings
of the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 133?
142.
Xiaoyan Li and W. Bruce Croft. 2003. Time-based
language models. In Proc. 12th Intl. Conf. on Infor-
mation and Knowledge Management, pages 469?475,
New York, NY, USA. ACM.
Jaime Teevan, Susan T. Dumais, and Eric Horvitz. 2005.
Personalizing search via automated analysis of inter-
ests and activities. In SIGIR ?05, pages 449?456.
Chengxiang Zhai and John Lafferty. 2004. A study of
smoothing methods for language models applied to in-
formation retrieval. ACM Trans. Inf. Syst., 22(2):179?
214.
Zhaohui Zheng, Keke Chen, Gordon Sun, and Hongyuan
Zha. 2007. A regression framework for learning rank-
ing functions using relative relevance judgments. In
SIGIR ?07, pages 287?294.
168
Coling 2010: Poster Volume, pages 18?26,
Beijing, August 2010
Cross-Market Model Adaptation with Pairwise Preference Data for
Web Search Ranking
Jing Bai
Microsoft Bing
1065 La Avenida
Mountain View, CA 94043
jbai@microsoft.com
Fernando Diaz, Yi Chang, Zhaohui Zheng
Yahoo! Labs
701 First Avenue
Sunnyvale, CA 94089
{diazf,yichang,zhaohui}@yahoo-inc.com
Keke Chen
Computer Science
Wright State
Dayton, Ohio 45435
keke.chen@wright.edu
Abstract
Machine-learned ranking techniques au-
tomatically learn a complex document
ranking function given training data.
These techniques have demonstrated the
effectiveness and flexibility required of a
commercial web search. However, man-
ually labeled training data (with multiple
absolute grades) has become the bottle-
neck for training a quality ranking func-
tion, particularly for a new domain. In
this paper, we explore the adaptation of
machine-learned ranking models across
a set of geographically diverse markets
with the market-specific pairwise prefer-
ence data, which can be easily obtained
from clickthrough logs. We propose
a novel adaptation algorithm, Pairwise-
Trada, which is able to adapt ranking
models that are trained with multi-grade
labeled training data to the target mar-
ket using the target-market-specific pair-
wise preference data. We present results
demonstrating the efficacy of our tech-
nique on a set of commercial search en-
gine data.
1 Introduction
Web search algorithms provide methods for
ranking web scale collection of documents given
a short query. The success of these algorithms
often relies on the rich set of document prop-
erties or features and the complex relationships
between them. Increasingly, machine learn-
ing techniques are being used to learn these
relationships for an effective ranking function
(Liu, 2009). These techniques use a set of la-
beled training data labeled with multiple rele-
vance grades to automatically estimate parame-
ters of a model which directly optimizes a per-
formance metric. Although training data often
is derived from editorial labels of document rel-
evance, it can also be inferred from a careful
analysis of users? interactions with a working
system (Joachims, 2002). For example, in web
search, given a query, document preference in-
formation can be derived from user clicks. This
data can then be used with an algorithm which
learns from pairwise preference data (Joachims,
2002; Zheng et al, 2007). However, automati-
cally extracted pairwise preference data is sub-
ject to noise due to the specific sampling meth-
ods used (Joachims et al, 2005; Radlinski and
Joachim, 2006; Radlinski and Joachim, 2007).
One of the fundamental problems for a web
search engine with global reach is the develop-
ment of ranking models for different regional
markets. While the approach of training a single
model for all markets is attractive, it fails to fully
exploit of specific properties of the markets. On
the other hand, the approach of training market-
specific models requires the huge overhead of
acquiring a large training set for each market.
As a result, techniques have been developed to
create a model for a small market, say a South-
east Asian country, by combining a strong model
in another market, say the United States, with a
18
small amount of manually labeled training data
in the small market (Chen et al, 2008b). How-
ever, the existing Trada method takes only multi-
grade labeled training data for adaptation, mak-
ing it impossible to take advantage of the easily
harvested pairwise preference data. In fact, to
our knowledge, there is no adaptation algorithm
that is specifically developed for pairwise data.
In this paper, we address the development
market-specific ranking models by leveraging
pairwise preference data. The pairwise prefer-
ence data contains most market-specific train-
ing examples, while a model from a large mar-
ket may capture the common characteristics of
a ranking function. By combining them algo-
rithmically, our approach has two unique advan-
tages. (1) The biases and noises of the pairwise
preference data can be depressed by using the
base model from the large market. (2) The base
model can be tailored to the characteristics of the
new market by incorporating the market specific
pairwise training data. As the pairwise data has
the particular form, the challenge is how to ef-
fectively use pairwise data in adaptation. This
appeals to the following objective of many web
search engines: design algorithms which mini-
mize manually labeled data requirements while
maintaining strong performance.
2 Related Work
In recent years, the ranking problem is fre-
quently formulated as a supervised machine
learning problem, which combines different
kinds of features to train a ranking function.
The ranking problem can be formulated as learn-
ing a function with pair-wise preference data,
which is to minimize the number of contra-
dicting pairs in training data. For example,
RankSVM (Joachims, 2002) uses support vector
machines to learn a ranking function from pref-
erence data; RankNet (Burges et al, 2005a) ap-
plies neural network and gradient descent to ob-
tain a ranking function; RankBoost (Freund et
al., 1998) applies the idea of boosting to con-
struct an efficient ranking function from a set of
weak ranking functions; GBRank (Zheng et al,
2007; Xia et al, 2008) using gradient descent in
function spaces, which is able to learn relative
ranking information in the context of web search.
In addition, Several studies have been focused
on learning ranking functions in semi-supervised
learning framework (Amini et al, 2008; Duh and
Kirchhoff, 2008), where unlabeled data are ex-
ploited to enhance ranking function. Another ap-
proach to learning a ranking function addresses
the problem of optimizing the list-wise perfor-
mance measures of information retrieval, such
as mean average precision or Discount Cumu-
lative Gain (Cao et al, 2007; Xu et al, 2008;
Wu et al, 2009; Chen et al, 2008c). The idea
of these methods is to obtain a ranking function
that is optimal with respect to some information
retrieval performance measure.
Model adaptation has previously been applied
in the area of natural language processing and
speech recognition. This approach has been suc-
cessfully applied to parsing (Hwa, 1999), tag-
ging (Blitzer et al, 2006), and language model-
ing for speech recognition (Bacchiani and Roark,
2003). Until very recently, several works have
been presented on the topic of model adaptation
for ranking (Gao et al, 2009; Chen et al, 2008b;
Chen et al, 2009), however, none of them target
the model adaptation with the pair-wise learn-
ing framework. Finally, multitask learning for
ranking has also been proposed as a means of
addressing problems similar to those we have
encountered in model adaptation (Chen et al,
2008a; Bai et al, 2009; Geng et al, 2009).
3 Background
3.1 Gradient Boosted Decision Trees for
Ranking
Assume we have a training data set, D =
{?(q, d), y?1, . . . , ?(q, d), y?n}, where ?(q, d), t?i
encodes the labeled relevance, y, of a docu-
ment, d, given query, q. Each query-document
pair, (q, d), is represented by a set of features,
(q, d) = {xi1, xi2, xi3, ..., xim}. These features
include, for example, query-document match
features, query-specific features, and document-
specific features. Each relevance judgment, y,
is a relevance grade mapped (e.g. ?relevant?,
?somewhat relevant?, ?non-relevant?) to a real
19
x1 > a1?
x2 > a2? x3 > a3?
YES NO
Figure 1: An example of base tree, where x1, x2
and x3 are features and a1, a2 and a3 are their
splitting values.
number. Given this representation, we can learn
a gradient boosted decision tree (GBDT) which
models the relationship between document fea-
tures, (q, d), and the relevance score, y, as a de-
cision tree (Friedman, 2001). Figure 1 shows a
portion of such a tree. Given a new query docu-
ment pair, the GBDT can be used to predict the
relevance grade of the document. A ranking is
then inferred from these predictions. We refer to
this method as GBDTreg.
In the training phase, GBDTreg iteratively
constructs regression trees. The initial regres-
sion tree minimizes the L2 loss with respect to
the targets, y,
L2(f, y) =
?
?(q,d),y?
(f(q, d)? y)2 (1)
As with other boosting algorithms, the subse-
quent trees minimize the L2 loss with respect to
the residuals of the predicted values and the tar-
gets. The final prediction, then, is the sum of the
predictions of the trees estimated at each step,
f(x) = f1(x) + . . .+ fk(x) (2)
where f i(x) is the prediction of the ith tree.
3.2 Pairwise Training
As alternative to the absolute grades in D,
we can also imagine assembling a data set
of relative judgments. In this case, as-
sume we have a training data set D =
{?(q, d), (q, d?), ??1, . . . , ?(q, d), (q, d?), ??n},
where ?(q, d), (q, d?), ??i encodes the prefer-
ence, of a document, d, to a second document,
d?, given query, q. Again, each query-document
pair is represented by a set of features. Each
preference judgment, ? ? {,?}, indicates
whether document d is preferred to document d?
(d  d?) or not (d ? d?).
Preference data is attractive for several rea-
sons. First, editors can often more easily deter-
mine preference between documents than the ab-
solute grade of single documents. Second, rel-
evance grades can often vary between editors.
Some editors may tend to overestimate relevance
compared to another editor. As a result, judg-
ments need to be rescaled for editor biases. Al-
though preference data is not immune to inter-
editor inconsistency, absolute judgments intro-
duce two potential sources of noise: determin-
ing a relevance ordering and determining a rele-
vance grade. Third, even if grades can be accu-
rately labeled, mapping those grades to real val-
ues is often done in a heuristic or ad hoc manner.
Fourth, GBDTreg potentially wastes modeling
effort on predicting the grade of a document as
opposed to focusing on optimizing the rank order
of documents, the real goal a search engine. Fi-
nally, preference data can often be mined from a
production system using assumptions about user
clicks.
In order to support preference-based
training data, (Zheng et al, 2007) pro-
posed GBRANK based on GBDTreg. The
GBRANK training algorithm begins by con-
structing an initial tree which predicts a constant
score, c, for all instances. A pair is contra-
dicting if the ?(q, d), (q, d?),? and prediction
f(q, d) < f(q, d?). At each boosting stage,
the algorithm constructs a set of contradicting
pairs, Dcontra. The GBRANK algorithm then
adjusts the response variables, f(q, d) and
f(q, d?), so that f(q, d) > f(q, d?). Assume
that (q, d)  (q, d?) and f(q, d) < f(q, d?). To
correct the order, we modify the target values,
f?(q, d) = f(q, d) + ? (3)
f?(q, d?) = f(q, d?)? ? (4)
where ? > 0 is a margin parameter that we
20
need to assign. In our experiments, we set ? to
1. Note that if preferences are inferred from ab-
solute grades, D, minimizing the L2 to 0 also
minimizes the contradictions.
3.3 Tree Adaptation
Recall that we are also interested in using the
information learned from one market, which we
will call the source market, on a second market,
which we will call the target market. To this end,
the Trada algorithm adapts a GBDTreg model
from the source market for the target market by
using a small amount of target market absolute
relevance judgments (Chen et al, 2008b). Let
the Ds be the data in the source domain and
Dt be the data in target domain. Assume we
have trained a model using GBDTreg. Our ap-
proach will be to use the decision tree structure
learned from Ds but to adapt the thresholds in
each node?s feature. We will use Figure 1 to il-
lustrate Trada. The splitting thresholds are a1, a2
and a3 for rank features x1, x2 and x3. Assume
that the data set Dt is being evaluated at the root
node v in Figure 1. We will split the using the
feature vx = x1 but will compute a new thresh-
old v?a using Dt and the GBDTreg algorithm.
Because we are discussing the root node, when
we select a threshold b, Dt will be partitioned
into two sets, D>bt and D<bt representing those
instances whose feature x1 has a value greater
and lower than b. The response value for each
partition will be the uniform average of instances
in that partition,
f =
?
?
?
1
|D>bt |
?
di?D>bt yi if di ? D
>b
t
1
|D<bt |
?
di?D<bt yi if di ? D
<b
t
(5)
We would like to select a value for b which min-
imizes the L2 loss between y and f in Equation
5; equivalently, b can be selected to minimize the
variance of y in each partition. In our imple-
mentation, we compute the L2 loss for all pos-
sible values of the feature v?x and select the value
which minimizes the loss.
Once b is determined, the adaptation consists
of performing a linear interpolation between the
original splitting threshold va and the new split-
ting threshold b as follows:
v?a = pva + (1? p)b (6)
where p is an adaptation parameter which deter-
mines the scale of how we want to adapt the tree
to the new task. If there is no additional informa-
tion, we can select p according to the size of the
data set,
p = |D
<a
s |
|D<as |+ |D<bt |
(7)
In practice, we often want to enhance the adapta-
tion scale since the training data of the extended
task is small. Therefore, we add a parameter ?
to boost the extended task as follows:
p = |D
<a
s |
|D<as |+ ?|D<bt |
(8)
The value of ? can be determined by cross-
validation. In our experiments, we set ? to 1.
The above process can also be applied to ad-
just the response value of nodes as follows:
v?f = pvf + (1? p)f (9)
where v?f is the adapted response at a node, vf is
its original response value of source model, and
f is the response value (Equation 5).
The complete Trada algorithm used in our ex-
periments is presented in Algorithm 1.
Algorithm 1 Tree Adaptation Algorithm
TRADA(v,Dt, p)
1 b? COMPUTE-THRESHOLD(vx,Dt)
2 v?a ? pva + (1? p)b
3 v?f ? pvf + (1? p)MEAN-RESPONSE(Dt)
4 D?t ? {x ? Dt : xi < v?a}
5 v?< ? TRADA(v<,D?t, p)
6 D??t ? {x ? Dt : xi > v?a}
7 v?> ? TRADA(v>,D??t , p)
8 return v?
21
The Trada algorithm can be augmented with a
second phase which directly incorporates the tar-
get training data. Assume that our source model,
Ms, was trained using source data, Ds. Re-
call that Ms can be decomposed as a sum of
regression tree output, fMs(x) = f1Ms(x) +
. . . + fkMs(x). Additive tree adaptation refersaugmenting this summation with a set of regres-
sion trees trained on the residuals between the
model, Ms, and the target training data, Dt.
That is, fMt(x) = f1Ms(x) + . . . + fkMs(x) +
fMt(x)k+1+. . .+fMt(x)k+k
? . In order for us to
perform additive tree adaptation, the source and
target data must use the same absolute relevance
grades.
4 Pairwise Adaptation
Both GBRANK and Trada can be used
to reduce the requirement on editorial data.
GBRANK achieves the goal by leveraging pref-
erence data, while Trada does so by leveraging
data from a different search market. A natural
extension to these methods is to leverage both
sources of data simultaneously. However, no al-
gorithm has been proposed to do this so far in
the literature. We propose an adaptation method
using pairwise preference data.
Our approach shares the same intuition as
Trada: maintain the tree structure but adjust
decision threshold values against some target
value. However, an important difference is
that our adjustment of threshold values does not
regress against some target grade values; rather
its objective is to improve the ordering of doc-
uments. To make use of preference data in
the tree adaptation, we follow the method used
in GBRANK to adjust the target values when-
ever necessary to preserve correct document or-
der. Given a base model, Ms, and preference
data, Dt , we can use Equations 3 and 4 to in-
fer target values. Specifically, we construct a set
Dcontra from Dt and Ms. For each item (q, d)
in Dcontra, we use the value of f?(q, d) as the tar-
get. These tuples, ?(q, d), f?(q, d)? along with
Ms are then are provided as input to Trada. Our
approach is described in Algorithm 2.
Compared to Trada, Pairwise-Trada has two
Algorithm 2 Pairwise Tree Adaptation Algo-
rithm
PAIRWISE-TRADA(Ms,Dt , p)
1 Dcontra ? FIND-CONTRADICTIONS(Ms,Dt )
2 D?t ? {?(q, d), f?(q, d)? : (q, d) ? Dcontra}
3 return TRADA(ROOT(Ms), D?t, p)
important differences. First, Pairwise-Trada can
use a source GBDT model trained either against
absolute or pairwise judgments. When an orga-
nization maintains a set of ranking models for
different markets, although the underlying mod-
eling method may be shared (e.g. GBDT), the
learning algorithm used may be market-specific
(e.g. GBRANK or GBDTreg). Unfortunately,
classic Trada relies on the source model being
trained using GBDTreg. Second, Pairwise-Trada
can be adapted using pairwise judgments. This
means that we can expand our adaptation data to
include click feedback, which is easily obtain-
able in practice.
5 Methods and Materials
The proposed algorithm is a straightforward
modification of previous ones. The question we
want to examine in this section is whether this
simple modification is effective in practice. In
particular, we want to examine whether pairwise
adaptation is better than the original adaptation
Trada using grade data, and whether the pairwise
data from a market can help improve the ranking
function on a different market.
Our experiments evaluate the performance of
Pairwise-Trada for web ranking in ten target
markets. These markets, listed in Table 1, cover
a variety of languages and cultures. Further-
more, resources, in terms of documents, judg-
ments, and click-through data, also varies across
markets. In particular, editorial query-document
judgments range from hundreds of thousands
(e.g. SEA1) to tens of thousands (e.g. SEA5).
Editors graded query-document pairs on a five-
point relevance scale, resulting in our data setD.
Preference labels, D, are inferred from these
judgments.
22
We also include a second set of experiments
which incorporate click data.1 In these experi-
ments, we infer a preference from click data by
assuming the following model. The user is pre-
sented with ten results. An item i  j if i the fol-
lowing conditions hold: i is positioned below j,
i receives a click, and j does not receive a click.
In our experiments, we tested the following
runs,
? GBDTreg trained using only Ds or Dt
? GBRANK trained using only Ds or Dt
? GBRANK trained using only Ds , Dt , and
Ct
? Trada with both GBDTs and GBRANKs,
adapted with Dt.
? Pairwise-Trada with both GBDTs and
GBRANKs, adapted with Dt and Ct at dif-
ferent ratios.
In the all experiments, we use 400 additive trees
when additive adaptation is used.
All models are evaluated using discounted cu-
mulative gain (DCG) at rank cutoff 5 (Ja?rvelin
and Keka?la?inen, 2002).
6 Results
6.1 Adaptation with Manually Labeled
Data
In Table 1, we show the results for all of our ex-
perimental conditions.
We can make a few observations about the
non-adaptation baselines. First, models trained
on the (limited) target editorial data, GBDTt
and GBRANKt, tend to outperform those trained
only on the source editorial data, GBDTs and
GBRANKs. The critical exception is SEA5, the
market with the fewest judgments. We believe
that this behavior is a result of similarity between
the United States source data and the SEA5 tar-
get market; both the source and target query pop-
ulations share the same language, a property not
1For technical reasons, this data set is slightly differ-
ent from the results we show with the purely editorial data.
Therefore the size of the training and testing sets are differ-
ent, but not to a significant degree.
exhibited in other markets. Notice that other
small markets such as LA2 and LA3 see modest
improvements when using target-only runs com-
pared to source-only runs. Second, GBRANK
tends to outperform GBDT when only trained on
the source data. This implies that we should pre-
fer a base model which is based on GBRANK,
something that is difficult to combine with clas-
sic Trada. Third, by comparing GBRANK and
GBDT when only trained on the target data, we
notice that the effectiveness of GBRANK de-
pends on the amount of training data. For mar-
kets where there training data is plentiful (e.g.
SEA1), GBRANK outperforms GBDT. On the
other hand, for smaller markets (e.g. LA3),
GBDT outperforms GBRANK.
In general, the results confirm the hypothe-
sis that adaptation runs outperform all of non-
adaptation baselines. This is the case for both
Trada and Pairwise-Trada. As with the baseline
runs, the Australian market sees different perfor-
mance as a result of the combination of a small
target editorial set and a representative source
domain. This effect has been observed in pre-
vious results (Chen et al, 2009).
We can also make a few observations by com-
paring the adaptation runs. Trada works better
with a GBDT base model than with a GBRANK
base model. We We believe this is the case be-
cause the absolute regression targets are diffi-
cult to compare with the unbounded output of
GBRANK. Pairwise-Trada on the other hand
tends to perform better with a GBRANK base
model than with a GBDT base model. There
are a few exceptions, SEA3 and LA2, where
Pairwise-Trada works better with a GBDT base
model. Comparing Trada to Pairwise-Trada, we
find that using preference targets tends to im-
prove performance for some markets but not all.
The underperformance of Pairwise-Trada tends
to occur in smaller markets such as LA1, LA2,
and LA3. This is similar to the behavior we ob-
served in the non-adaptation runs and suggests
that, in operation, a modeler may have to decide
on the training algorithm based on the amount of
data available.
23
SEA1 SEA2 EU1 SEA3 EU2 SEA4 LA1 LA2 LA3 SEA5
training size 243,790 174,435 137,540 135,066 101,076 100,846 91,638 75,989 66,151 37,445
testing size 18,652 26,752 11,431 13,839 12,118 12,214 11,038 16,339 10,379 21,034
GBDTs 9.4483 8.1271 9.0018 10.0630 8.5339 5.9176 6.1699 11.4167 8.1416 10.5356
GBDTt 9.6011 8.6225 9.3310 10.7591 9.0323 6.4185 6.8441 11.8553 8.5702 10.4561
GBRANKs 9.6059 8.1784 9.0775 10.2486 8.6248 6.1298 6.2614 11.5186 8.2851 10.5915
GBRANKt 9.6952 8.6225 9.3575 10.8595 9.0384 6.4620 6.8543 11.7086 8.4825 10.3469
Trada
GBDTs,Dt 9.6718 8.6120 9.3086 10.8001 9.1024 6.3440 6.9444 11.9513 8.6519 10.6279
GBRANKs,Dt 9.6116 8.5681 9.2125 10.7597 8.9675 6.4110 6.8286 11.7326 8.5498 10.6508
Pairwise-Trada
GBDTs,Dt 9.7364 8.6261 9.3824 10.8549 9.0842 6.4705 6.9438 11.8255 8.5323 10.4655
GBRANKs,Dt 9.7539 8.6538 9.4269 10.8362 9.1044 6.4716 6.9438 11.8034 8.6187 10.6564
Table 1: Adaptation using manually labeled training data Southeast Asia (SEA), Europe (EU), and
Latin America (LA) markets. Markets are sorted by target training set size. Significance tests use
a t-test. Bolded numbers indicate statistically significant improvements over the respective source
model.
SEA1 SEA2 EU1 SEA3 EU2 SEA4 LA1 LA2 LA3 SEA5
training size 194,114 166,396 136,829 161,663 94,875 96,642 73,977 108,350 64,481 71,549
testing size 15,655 11,844 11,028 11,839 11,118 5,092 10,038 12,246 10,201 7,477
GBRANKs 9.0159 8.5763 8.7119 11.4512 9.7641 6.5941 6.894 7.9366 8.058 10.7935
Pairwise-Trada
GBRANKs,Dt, Ct
editorial 9.3577 8.9205 8.901 12.2247 9.9531 6.7421 7.1455 8.2811 8.2503 10.7973
click 9.1149 8.7622 8.8187 11.9361 9.8818 6.7703 7.1812 8.264 8.2485 10.9042
editorial+click 9.4898 9.0177 8.945 12.3172 10.1156 6.8459 7.2414 8.4111 8.292 11.1407
Table 2: Adaptation incorporating click data. Bolded numbers indicate statistically significant im-
provements over the baseline. Markets ordered as in Table 1.
6.2 Incorporating Click Data
One of the advantages of Pairwise-Trada is the
ability to incorporate multiple sources of pair-
wise preference data. In this paper, we use the
heuristic rule approach which is introduced by
(Dong et al, 2009) to extract pairwise preference
data from the click log of the search engine. This
approach yields both skip-next and skip-above
pairs (Joachims et al, 2005), which are sorted
by confidence descending order respectively. In
these experiments, we combine manually gener-
ated preferences with those gathered from click
data. We present these results in Table 2.
We notice that no matter the source of prefer-
ence data, Pairwise-Trada outperforms the base-
line GBRANK model. The magnitude of the
improvement depends on the source data used.
Comparing the editorial-only to the click-only
models, we notice that click-only models outper-
form editorial-only models for smaller markets
(SEA4, LA1, and SEA5). This is likely the case
because the relative quantity of click data with
respect to editorial data is higher in these mar-
kets. This is despite the fact that the click data
may be noisier than the editorial data. The best
performance, though, comes when we combine
both editorial and click data.
6.3 Additive tree adaptation
Recall that Pairwise-Trada consists of two parts:
parameter adaptation and additive tree adapta-
tion. In this section, we examine the contri-
bution to performance each part is responsible
for. Figure 2 illustrates the adaptation results for
the LA1 market. In this experiment, we use a
United States base model and 100K LA1 edito-
rial judgments for adaptation. Pairwise-Trada is
performed on top of differently sized base mod-
els with 600, 900 and 1200 trees. The original
base model has 1200 trees; we selected the first
600, 900 or full 1200 trees for experiments. The
number of trees used in the additive tree adap-
tation step ranges up to 600 trees. From Fig-
ure 2 we can see that the additive adaptation can
24
0 500 1000 1500 2000
6.0
6.2
6.4
6.6
6.8
7.0
number of trees
DC
G5
adaptation
additive (600)
additive (900)
additive (1200)
source model
Figure 2: Illustration of additive tree adaptation
for LA1. The curves are average performance
over a range of parameter settings.
significantly increase DCG over simple parame-
ter adaptation and is therefore a critical step of
Pairwise-Trada. When the number of trees in
the additive tree adaptation step reaches roughly
400, the DCG plateaus.
7 Conclusion
We have proposed a model for adapting retrieval
models using preference data instead of abso-
lute relevance grades. Our experiments demon-
strate that, when much editorial data is present,
our method, Pairwise-Trada, may be preferable
to competing methods based on absolute rele-
vance grades. However, in real world systems,
we often have access to sources of preference
data beyond those resulting from editorial judg-
ments. We demonstrated that Pairwise-Trada can
exploit such data and boost performance signif-
icantly. In fact, if we omit editorial data alto-
gether we see performance improvements over
the baseline model. This suggests that, in prin-
ciple, we can train a single, strong source model
and improve it using target click data alone. De-
spite the fact that the modification we made is
quite simple, we showed that modification is ef-
fective in practice. This tends to validate the
general principle of using pairwise data from a
different market. This principle can be easily
used in other frameworks such as neural net-
works (Burges et al, 2005b). Therefore, the pro-
posed method also points to a new direction for
future improvements of search engines.
There are several areas of future work. First,
we believe that detecting other sources of pref-
erence data from user behavior can further im-
prove the performance of our model. Second,
we only used a single source model in our ex-
periments. We would also like to explore the
effect of learning from an ensemble of source
models. The importance of each may depend on
the similarity to the target domain. Finally, we
would also like to more accurately understand
the queries where click data improves adaptation
and those where editorial judgments is required.
This sort of knowledge will allow us to train sys-
tems which maximally exploit our editorial re-
sources.
References
Amini, M.-R., T.-V. Truong, and C. Goutte. 2008.
A boosting algorithm for learning bipartite rank-
ing functions with partially labeled data. In SIGIR
?08: Proceedings of the 31st annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval.
Bacchiani, M. and B. Roark. 2003. Unsuper-
vised language model adaptation. In ICASSP ?03:
Proceedings of the International Conference on
Acoustics, Speech and Signal Processing.
Bai, J., K. Zhou, H. Zha, B. Tseng, Z. Zheng, and
Y. Chang. 2009. Multi-task learning for learning
to rank in web search. In CIKM ?09: Proceeding
of the 18th ACM conference on Information and
knowledge management.
Blitzer, J., R. McDonald, and F. Pereira. 2006.
Domain adaptation with structural correspondence
learning. In EMNLP ?06: Proceedings of the
2006 Conference on Empirical Methods on Nat-
ural Language Processing.
Burges, C., T. Shaked, E. Renshaw, A. Lazier,
M. Deeds, N. Hamilton, and G. Hullender. 2005a.
Learning to rank using gradient descent. In ICML
?05: Proceedings of the 22nd International Con-
ference on Machine learning.
Burges, Chris, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hul-
lender. 2005b. Learning to rank using gradi-
ent descent. In ICML ?05: Proceedings of the
25
22nd international conference on Machine learn-
ing, pages 89?96. ACM.
Cao, Z., T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li.
2007. from pairwise approach to listwise ap-
proach. In ICML ?07: Proceedings of the 24th
international conference on Machine learning.
Chen, D., J. Yan, G. Wang, Y. Xiong, W. Fan, and
Z. Chen. 2008a. Transrank: A novel algorithm for
transfer of rank learning. In ICDM workshop ?08:
Proceeding of IEEE Conference on Data Mining.
Chen, K., R. Lu, C. K. Wong, G. Sun, L. Heck, and
B. Tseng. 2008b. Trada: tree based ranking func-
tion adaptation. In CIKM ?08: Proceeding of the
17th ACM conference on Information and knowl-
edge management, pages 1143?1152, New York,
NY, USA. ACM.
Chen, W., T.-Y. Liu, Y. Lan, Z. Ma, and H. Li. 2008c.
Measures and loss functions in learning to rank. In
NIPS ?08: Proceedings of the Twenty-Second An-
nual Conference on Neural Information Process-
ing Systems.
Chen, K., J. Bai, S. Reddy, and B. Tseng. 2009. On
domain similarity and effectiveness of adapting-
to-rank. In CIKM ?09: Proceeding of the 18th
ACM conference on Information and knowledge
management, pages 1601?1604, New York, NY,
USA. ACM.
Dong, A., Y. Chang, S. Ji, C. Liao, X. Li, and
Z. Zheng. 2009. Empirical exploitation of click
data for query-type-based ranking. In EMNLP
?09: Proceedings of the 2009 Conference on Em-
pirical Methods on Natural Language Processing.
Duh, K. and K. Kirchhoff. 2008. Learning to rank
with partially-labeled data. In SIGIR ?08: Pro-
ceedings of the 31st annual international ACM SI-
GIR conference on Research and development in
information retrieval.
Freund, Y., R. D. Iyer, R. E. Schapire, and Y. Singer.
1998. An efficient boosting algorithm for com-
bining preferences. In ICML ?98: Proceedings of
the Fifteenth International Conference onMachine
Learning.
Friedman, J. H. 2001. Greedy function approxima-
tion: A gradient boosting machine. The Annals of
Statistics, 29(5):1189?1232.
Gao, J., Q. Wu, C. Burges, K. Svore, Y. Su, N. Khan,
Shah S., and H. Zhou. 2009. Model adapta-
tion via model interpolation and boosting for web
search ranking. In EMNLP ?09: Proceedings of
the 2009 Conference on Empirical Methods on
Natural Language Processing.
Geng, B., L. Yang, C. Xu, and X.-S. Hua. 2009.
Ranking model adaptation for domain-specific
search. In CIKM ?09: Proceeding of the 18th ACM
conference on Information and knowledge man-
agement, pages 197?206, New York, NY, USA.
ACM.
Hwa, R. 1999. Supervised grammar induction using
training data with limited constituent information.
In ACL ?99: Proceedings of the Conference of the
Association for Computational Linguistics.
Ja?rvelin, Kalervo and Jaana Keka?la?inen. 2002. Cu-
mulated gain-based evaluation of ir techniques.
TOIS, 20(4):422?446.
Joachims, T., L. Granka, B. Pan, and G. Gay. 2005.
Accurately interpreting clickthrough data as im-
plicit feedback.
Joachims, T. 2002. Optimizing search engines using
clickthrough data. In KDD ?02: Proceedings of
the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining, pages
133?142. ACM Press.
Liu, T.-Y. 2009. Learning to Rank for Information
Retrieval. Now Publishers.
Radlinski, F. and T. Joachim. 2006. Minimally inva-
sive randomization for collecting unbiased prefer-
ences from clickthrough logs.
Radlinski, F. and T. Joachim. 2007. Active ex-
ploration for learning rankings from clickthrough
data.
Wu, M., Y. Chang, Z. Zheng, and H. Zha. 2009.
Smoothing dcg for learning to rank: A novel ap-
proach using smoothed hinge functions. In CIKM
?09: Proceeding of the 18th ACM conference on
Information and knowledge management.
Xia, F., T.-Y. Liu, J. Wang, W. Zhang, and H. Li.
2008. Listwise approach to learning to rank: The-
orem and algorithm. In ICML ?08: Proceedings
of the 25th international conference on Machine
learning.
Xu, J., T.Y. Liu, M. Lu, H. Li, and W.Y. Ma. 2008.
Directly optimizing evaluation measures in learn-
ing to rank. In SIGIR ?08: Proceedings of the
31st annual international ACM SIGIR conference
on Research and development in information re-
trieval.
Zheng, Z., K. Chen, G. Sun, and H. Zha. 2007. A re-
gression framework for learning ranking functions
using relative relevance judgments. In SIGIR ?07:
Proceedings of the 30th annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 287?294. ACM.
26
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1129?1139,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Learning Recurrent Event Queries for Web Search
Ruiqiang Zhang and Yuki Konda and Anlei Dong
Pranam Kolari and Yi Chang and ZhaohuiZheng
Yahoo! Inc
701 First Avenue, Sunnyvale, CA94089
Abstract
Recurrent event queries (REQ) constitute a
special class of search queries occurring at
regular, predictable time intervals. The fresh-
ness of documents ranked for such queries is
generally of critical importance. REQ forms a
significant volume, as much as 6% of query
traffic received by search engines. In this
work, we develop an improved REQ classi-
fier that could provide significant improve-
ments in addressing this problem. We ana-
lyze REQ queries, and develop novel features
from multiple sources, and evaluate them us-
ing machine learning techniques. From histor-
ical query logs, we develop features utilizing
query frequency, click information, and user
intent dynamics within a search session. We
also develop temporal features by time series
analysis from query frequency. Other gener-
ated features include word matching with re-
current event seed words and time sensitiv-
ity of search result set. We use Naive Bayes,
SVM and decision tree based logistic regres-
sion model to train REQ classifier. The re-
sults on test data show that our models outper-
formed baseline approach significantly. Ex-
periments on a commercial Web search en-
gine also show significant gains in overall rel-
evance, and thus overall user experience.
1 Introduction
REQ pertains to queries about events which oc-
cur at regular, predictable time intervals, most often
weekly, monthly, annually, bi-annually, etc. Natu-
rally, users issue REQ periodically. REQ usually re-
fer to:
Organized public events such as festivals, confer-
ences, expos, sports competitions, elections: winter
olympics, boston marathon, the International Ocean
Research Conference, oscar night.
Public holidays and other noteworthy dates: labor day,
date of Good Friday, Thanksgiving, black friday.
Products with annual model releases, such as car models:
ford explorer, prius.
Lottery drawings: California lotto results.
TV shows and programs which are currently running:
American idol, Inside Edition.
Cultural related activities: presidential election, tax re-
turn, 1040 form.
Our interest in studying REQ arises from the chal-
lenge imposed on Web search ranking. To illustrate
this, we show an example in Fig. 1 that snapshots
the real ranking results of the query, EMNLP, is-
sued in 2010 when the authors composed this pa-
per, on Google search engine. It is obvious the
ranking is not satisfactory because the page about
EMNLP2008 is on the first position in 2010. Ide-
ally, the page about EMNLP2010 on the 6th position
should be on the first position even if users don?t
explicitly issue the query, EMNLP 2010, because
EMNLP is a REQ. The query, ?EMNLP?, implic-
itly, without a year qualifier, needs to be served the
most recent pages about ?EMNLP?.
A better search ranking result cannot be achieved
if we do not categorize ?EMNLP? as a REQ, and
provide special ranking treatment to such queries.
Existing search engines adopt a fairly involved rank-
ing algorithm to order Web search results by con-
sidering many factors. Time is an important fac-
tor but not the most critical. The page?s rank-
ing score mostly depends on other features such
as tf-idf (Salton and McGill, 1983), BM25 (Jones
1129
Figure 1: A real problematic ranking result by Google for
a REQ query, ?EMNLP?. The EMNLP2010 page should
be on the 1st position.
et al, 2000), anchor text, historical clicks, pager-
ank (Brin and Page, 1998), and overall page qual-
ity. New pages about EMNLP2010 obtain less fa-
vorable feature values than the pages of 2009 earlier
in terms of anchor text, click or pagerank because
they have existed for a shorter time and haven?t ac-
cumulated sufficient popularity to make them stand
out. Without special treatment, the new pages about
?EMNLP2010? will typically not be ranked appro-
priately for the users.
Typically, a recurrent event is associated with a
root, and spawns a large set of queries. Oscar,
for instance, is a recurrent event about the annual
Academy Award. Based on this, queries like ?oscar
best actress?, ?oscar best dress?, ?oscar best movie
award?, are all recurrent event queries. As such,
REQ is a highly frequent category of query in Web
search. By Web search query log analysis, we ob-
serve that there about 5-6% queries of total query
volume belongs to this category.
In this work, we learn if a query is in the REQ
class, by effectively combining multiple features.
Our features are developed through analysis of his-
torical query logs. We discuss our approaches in de-
tail in Section 3. We then develop a REQ classi-
fier where all the features are integrated by machine
learning models. We use Naive Bayes, SVM and de-
cision tree based logistic regression models. These
models are described in Section 4. Our experiments
for REQ classifier and Web search ranking are de-
tailed in Section 5 and 6.
2 Related Work
We found our work were related to two other prob-
lems: general query classification and time-sensitive
query classification. For general query classifica-
tion, the task is to assign a Web search query to
one or more predefined categories based on its top-
ics. In the query classification contest in KDD-
CUP 2005 (Li et al, 2005), seven categories and
67 sub-categories were defined. The winning so-
lution (Shen et al, 2005) used multiple classifiers
integrated by ensemble method. The difficulties for
query classification are from short queries, lack of
labeled data, and query sense ambiguity. Most pop-
ular studies use query log, web search results, unla-
beled data to enrich query classification (Shen et al,
2006; Beitzel et al, 2005), or use document classifi-
cation to predict query classification (Broder et al, ).
General query classification is also studied for query
intent detection by (Li et al, 2008).
There are many prior works to study the time sen-
sitivity issue in web search. For example, Baeza-
Yates et al (Baeza-Yates et al, 2002) studied the re-
lation between the web dynamics, structure and page
quality, and demonstrated that PageRank is biased
against new pages. In T-Rank Light and T-Rank al-
gorithms (Berberich et al, 2005), both activity (i.e.,
update rates) and freshness (i.e., timestamps of most
recent updates) of pages and links are taken into ac-
count for link analysis. Cho et al (Cho et al, 2005)
proposed a page quality ranking function in order to
alleviate the problem of popularity-based ranking,
and they used the derivatives of PageRank to fore-
cast future PageRank values for new pages. Pandey
et al (Pandey et al, 2005) studied the tradeoff be-
tween new page exploration and high-quality page
exploitation, which is based on a ranking method to
randomly promote some new pages so that they can
accumulate links quickly.
More recently, Dong et al (Dong et al, 2010a)
1130
proposed a machine-learned framework to improve
ranking result freshness, in which novel features,
modeling algorithms and editorial guideline are used
to deal with time sensitivities of queries and doc-
uments. In another work (Dong et al, 2010b), they
use micro-blogging data (e.g., Twitter data) to detect
fresh URLs. Novel and effective features are also
extracted for fresh URLs so that ranking recency in
web search is improved.
Perhaps the most related work to this paper is
the query classification approach used in (Zhang
et al, 2009) and (Metzler et al, 2009), in which
year qualified queries (YQQs) are detected based
on heuristic rules. For example, a query contain-
ing a year stamp is an explicit YQQ; if the year
stamp is removed from this YQQ, the remaining part
of this query is also a YQQ, which is called im-
plicit YQQ. Different ranking approaches were used
in (Zhang et al, 2009) and (Metzler et al, 2009)
where (Zhang et al, 2009) boosted pages of the most
latest year while (Metzler et al, 2009) promoted
pages of the most influential years. Similarly, Nunes
et al (Nunes, 2007) applied information extraction
techniques to identify temporal expression in web
search queries, and found 1.5% of queries contain-
ing temporal expression.
Dong et al (Dong et al, 2010a) proposed a
breaking-news query classifier with high accuracy
and reasonable coverage, which works not by mod-
eling each individual topic and tracking it over time,
but by modeling each discrete time slot, and compar-
ing the models representing different time slots. The
buzziness of a query is computed as the language
model likelihood difference between different time
slots. In this approach, both query log and news
contents are exploited to compute language model
likelihood.
Diaz (Diaz, 2009) determined the newsworthiness
of a query by predicting the probability of a user
clicks on the news display of a query. In this frame-
work, the data sources of both query log and news
corpus are leveraged to compute contextual features.
Furthermore, the online click feedback also plays a
critical role for future click prediction.
Konig et al (Knig et al, 2009) estimated the
click-through rate for dedicated news search result
with a supervised model, which is to satisfy the
requirement of adapting quickly to emerging news
event. Some additional corpora such as blog crawl
and Wikipedia is used for buzziness inference. Com-
pared with (Diaz, 2009), different feature and learn-
ing algorithms are used.
Elsas et al (Elsas and Dumais, 2010) studied
improving relevance ranking by detecting document
content change to leverage temporal information.
3 Feature Generation
To better understand our work, we first introduce
three terms. We subdivide all raw queries in query
log into three categories: Explicit Timestamp, Im-
plicit Timestamp, and No Timestamp. An Explicit
Timestamp query contains at least one token being a
time indicator. For example, emnlp 2010, 2007 De-
cember holiday calendar, amsterdam weather sum-
mer 2009, Google Q1 reports 2010. These queries
are considered to conatin time indicators, because
we can regard {2010, 2007, 2009} as year indica-
tor, december as month indicator, {summer, Q1(first
quarter)} as seasonal indicator. To simplify our
work, we only consider the year indicators, 2010,
2007, 2009. Such year indicators are also the most
important and most popular indicators, as noted in
(Zhang et al, 2009). Any query containing at least
one year indicator is an Explicit Timestamp query.
Due to word sense ambiguity, some queries labeled
as Explicit Timestamp by this method may have no
connection with time such as Windows Office 2007,
2010 Sunset Boulevard, or call number 2008. In this
work, we tolerate this type of error because word
sense disambiguation is a peripheral problem for this
task.
Implicit Timestamp queries are resulted by re-
moving all year indicators from the corresponding
Explicit Timestamp queries. For example, the Im-
plicit Timestamp query of emnlp 2010 is emnlp.
All other queries are No Timestamp queries because
they have never been found together with a year in-
dicator.
Classifying queries into the above three cate-
gories depends on the used query log. A search
engine company partner provided us a query log
from 08/01/2009 to 02/29/2010 for this research.
We found the proportions of the three categories
in this query log are 13.8% (Explicit), 17.1% (Im-
plicit) and 69.1% (No Timestamp). These numbers
1131
could be slightly different depending on the source
of query logs. Note that 17.1% of Implicit Times-
tamp queries in the query log is a significant num-
ber. However, not all Implicit Timestamp queries
are REQ. Many Implicit Timestamp queries have no
time sense. They belong to Implicit Timestamp just
because users issued the query with a year indica-
tor through varied intents. For example, ?google? is
found to be an Implicit Timestamp query since there
were many ?google 2008? or ?google 2009? in the
query log.
The next few sections introduce our work in rec-
ognizing recurrent event time sense for Implicit
Timestamp queries. We first focus on features.
There are many features that were exploited in REQ
classifier. We extract these features from query log,
query session log, click log, search results, time se-
ries and NLP morphological analysis.
3.1 Query log analysis
The following features are extracted from query log
analysis:
QueryDailyFrequency: the total counts of the
query divided by the number of the days in the pe-
riod.
ExplicitQueryRatio: Ratio of number of counts
query was issued with year and number of counts
query was issued with or without year. This feature
is the method used by (Zhang et al, 2009).
UniqExplicitQueryCount: Number of uniq Ex-
plicit Timestamp queries associated with query. For
example, if a query was issued with query+2009 and
query+2008, this feature?s value is two.
ChiSquareYearDist: this feature is the distance be-
tween two distributions: one is frequency distribu-
tion over years for all REQ queries. The other is
that for single REQ query. It is calculated through
following steps: (a) Aggregate the frequencies for
all queries for all years. Suppose we observe all
years from 2001 to 2010. So we can get vector,
E = ( a f10
sum1 ,
a f09
sum1 , ...,
a f01
sum1 ) where a fi is the frequency
sum of year 20i for all REQ queries. sum1 =
a f10 + a f09 + ... + a f01, the sum of all year fre-
quency. (b) Given a query, suppose we observe
this query?s yearly frequency distribution is , Oq =
(q f10, q f09, , ..., q f01). q fi is this query?s frequency
for the year 20i. Pad the slot with zeros if no fre-
quency found. The expected distribution for this
query is, Eq = ( sum2?a f10
sum1 ,
sum2?a f09
sum1 , ...,
sum2?a f01
sum1 ),
where sum2 = q f10 + q f09 + ... + q f01 is sum of
all year frequency for the query. (d) Calculate CHI-
squared value to represent the different yearly fre-
quency distribution between Eq and Oq according to
?2 =
?N
i=1
(Oqi ?E
q
i )2
Eqi
. Using CHI square distance as a
method is widely used for statistical hypothesis test.
We found it to be a useful feature for REQ classifier.
3.2 Query reformulation
If users cannot find the newest page by issuing Im-
plicit Timestamp query, they may re-issue the query
using an Explicit Timestamp query. We can detect
this change in a search session (a 30 minutes period
for each query). By finding this kind of behavior
from users, we next extract three features.
UserSwitch: Number of unique users that switched
from Implicit Timestamp queries to Explicit Times-
tamp queries.
YearSwitch: Number of unique year-like tokens
switched by users in a query session.
NormalizedUserSwitch: Feature UserSwitch di-
vided by QueryDailyFrequency.
3.3 Click log analysis
If a query is time sensitive, users may click a
page that displays the year indicator on title or
url. An example that shows year indicator on
url is www.lsi.upc.edu/events/emnlp2010/call.html.
Search engine click log saves all users? click infor-
mation. We used click log to derive the following
features.
YearUrlTop5CTR: Aggregated click through rate
(CTR) of all top five URLs containing a year in-
dicator. CTR of an URL is defined as the number
of clicks of an URL divided by the number of page
views.
YearUrlFPCTR: Aggregated click through rate
(CTR) of all first page URLs containing a year in-
dicator.
3.4 Search engine result set
For each Implicited Timestamp query, we can scrape
the search engine to get search results. We count the
number of titles and urls that contain year indicator.
We use this number as a feature, and generate 6 fea-
tures.
1132
TitleYearTop5: the number of titles containing a
year indication on the top 5 results. This value is
4 in Fig. 1.
TitleYearTop10: the number of titles containing a
year indication on the top 10 results. This value is 6
in Fig. 1.
TitleYearTop30: the number of titles containing a
year indication on the top 30 results.
UrlYearTop5: the number of urls containing a year
indication on the top 5 results. This value is 1 in
Fig. 1.
UrlYearTop10: the number of urls containing a year
indication on the top 10 results.
UrlYearTop30: the number of titles containing a
year indication on the top 30 results.
3.5 Time series analysis
Recurrent event query has periodic occurrence pat-
tern in time series. Top graph of Figure 2 shows the
frequency change of the query, ?Oscar?. The annual
event usually starts from Oscar nomination as ear-
lier as last year December to award announcement
of February this year. So a small spike and a big
spike are observed in the graph to indicate nomina-
tion period and ceremony period. There are a period
of silence between the two periods. The frequency
pattern keeps unchanged each year. We show three
years (2007,8,9) in the graph. By making use of re-
current event queries? periodic properties, we calcu-
lated the query period as a new feature.
We use autocorrelation to calculate the period.
R(?) =
?N??
t=1 (xt ? ?)(xt+? ? ?)
{
?N??
t=1 (xt ? ?)2(xt+? ? ?)2}1/2
where x(t) is query daily frequency. N is the num-
ber of days used for this query. We can get maxi-
mum of 3 years data for some queries but only a few
months for others. R(?) is autocorrelation function.
Peaks (the local biggest R(?) given a time window)
can be detected from R(?) plot. The period T is cal-
culated as the duration between two neighbor peaks.
T = 365 for the query, ?Oscar?. The bottom graph
of Fig. 2 shows the autocorrelation function plot for
the query Oscar.
3.6 Recurrent event seed word list
Many recurrent event queries share some common
words that have recurrent time sense. We list most
new results top schedule
football festival movie world
show day best tax
result calendar honda ford
download exam nfl miss
awards toyota tour sale
american fair list pictures
election game basketball cup
Table 1: Top recurrent event seed words
frequently used recurrent seeds in Table 1. Those
seeds are likely combined with other words to form
new recurrent event queries. For example, the seed,
?new?, can be used by queries ?new bmw cars?,
?whitney houston new songs?, ?apple new iphone?,
or ?hairstyle new?.
To generate the seed list, we tokenized all the
queries from Implicit Timestamp queries and split
all the tokens. We then sort and unique all the to-
kens, and submit top tokens to professional editors
who are asked to pick 8,000 seeds from the top fre-
quent tokens. Some top tokens were removed if they
are not qualified to form recurrent event queries. The
editors took about four days to do the judgment ac-
cording to the token?s time sense and examples of
recurrent event queries. However, this is a one-time
effort. A token will be in the seed if there are many
recurrent event examples formed by this token, by
editors? judgment.
Table 1 shows 32 top seeds. Some seeds connect
with time such as, ?new, schedule, day, best, calen-
dar?; some relate to sports, ?football, game, nfl, tour,
basketball, cup?; some about cars, ?honda, ford, toy-
ota?. The reason why ?miss? is in the seeds is that
there are many annual events about beauty contest
such as ?miss america, miss california, miss korea?.
We use the seed list to generate the following
three features:
AveNumberTokensSeeds: number of tokens that is
in the seed list divided by number of tokens in the
query.
AveNumberTokensNotSeeds: number of tokens
that is not in the seed list divided by number of to-
kens in the query.
DiffNumberTokensSeeds: The difference of the
above two values.
1133
-0 .2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1 15 29 43 57 71 85 99 113 127 141 155 169 183 197 211 225 239 253 267 281 295 309 323 337 351 365 379 393 407 421 435 449 463 477 491 505 519 533 547 561 575 589 603 617 631 645 659 673 687 701 715 729 743 757 771 785 799 813 827 841 855 869 883 897
Figure 2: Frequency waveform(top) and corresponding autocorrelation curve (bottom) for query Oscar.
4 Learning Approach for REQ
The REQ classification is a typical machine learn-
ing task. Given M observed samples used for train-
ing data, {(x0, y0), (x1, y1), ? ? ? , (xM, yM)} where xi is
a feature vector we developed in last section for a
given query. yi is the observation value, {+1,?1},
indicating the class of REQ and non-REQ. The task
is to find the class probability given an unknown fea-
ture vector, x?, that is,
p(y = c|x?), c = +1,?1. (1)
There are a lot of machine learning methods ap-
plicable to implement Eq. 1. In this work, we
adopted three representative methods.
The first method is Naive Bayes method. This
method treats features independent. If x is enx-
tended into feature vector, x = {x0, x1, ? ? ? , xN} then,
p(y = c|x) = 1
Z
p(c)
i=N
?
i=0
p(xi|c)
The second method is SVM. In this work we used
the tool for our experiments, LIBSVM (Chang and
Lin, 2001). Because SVM is a well known approach
and widely used in many classification task, we skip
to describe how to use this tool. Readers can turn to
the reference for more details.
The third method is based on decision tree based
logistic regression model. The probability is given
by the formula below,
p(y = c|x) = 1
1 + e? f (x)
(2)
We employ Gradient Boosted Decision Tree algo-
rithm (Friedman, 2001) to learn the function f (X).
Gradient Boosted Decision Tree is an additive re-
gression algorithm consisting of an ensemble of
trees, fitted to current residuals, gradients of the loss
function, in a forward step-wise manner. It itera-
tively fits an additive model as
ft(x) = Tt(x;?) + ?
T
?
t=1
?tTt(x;?t)
such that certain loss function L(yi, fT (x + i)) is
minimized, where Tt(x;?t) is a tree at iteration t,
weighted by parameter ?t, with a finite number of
parameters, ?t and ? is the learning rate. At iteration
t, tree Tt(x; ?) is induced to fit the negative gradient
by least squares.
The optimal weights of trees ?t are determined by
?t = argmin?
N
?
i
L(yi, ft?1(xi) + ?T (xi, ?))
Each node in the trees represents a split on a fea-
ture. The tuneable parameters in such a machine-
learnt model include the number of leaf nodes in
each tree, the relative contribution of score from
each tree called the shrinkage, and total number of
shallow decision trees.
The relative importance of a feature S i, in such
forests of decision trees, is aggregated over all the
1134
m shallow decision trees (Breiman et al, 1984) as
follows:
S 2i =
1
M
M
?
m=1
L?1
?
n=1
wl ? wr
wl + wr
(ylyr)2I(vt = i) (3)
where vt is the feature on which a split occurs, yl
and yr are the mean regression responses from the
right, and left sub-tree, and wl and wr are the corre-
sponding weights to the means, as measured by the
number of training examples traversing the left and
right sub-trees.
5 REQ Learner Evaluation
We collected 6,000 queries labeled as either Recur-
rent or Non-recurrent by professional human edi-
tors. The 6,000 queries were sampled from Implicit
Timestamp queries according to frequency distribu-
tion to be representative. We split the queries into
5,000 for training and 1,000 for test. For each query,
we calculated features? values as described in Sec-
tion 3.
The Naive Bayes method used single Gaussian
function for each independent feature. Mean and
variance were calculated from the training data.
As for LIBSVM, we used C-SVC, linear function
as kernel and 1.0 of shrinkage.
The parameters used in the regression model were
20 of trees, 20 of nodes and 0.8 of learning rate
(shrinkage).
The test results are shown in Fig. 3, recall-
precision curve. We set a series of threshold to the
probability of c = +1 calculated by Eq. 1 so that
we can get the point values of recall and precision in
Fig. 3. For example, if we set a threshold of 0.6, a
query with a probability larger than 0.6 is classified
as REQ. Otherwise, it is non-REQ. The precision
is a measure of correctly classified REQ queries di-
vided by all classified REQ queries. The recall is a
measure of correctly classified REQ queries divided
by all REQ queries in test data.
In addition to the three plots, we also show the
results using only one feature, ExplicitQueryRatio,
for comparison with the classification method used
by (Zhang et al, 2009).All the three models us-
ing all features performed better than the existing
method using ExplicitQueryRatio. The highest im-
provement was achieved by GBDT regression tree
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 0.4  0.5  0.6  0.7  0.8  0.9  1
Pr
ec
is
io
n
Recall
ExplicitQueryRatio
GBDTree
Naive Bayes
SVM
Figure 3: Comparison of precision and recall rate be-
tween our method and the existing method.
model. The results of Naive Bayes were lower than
SVM and GBDTree. This model is weaker because
it treats features independently. Typically SVMs and
GBDT gives comparable results on a large class of
problems. Since for this task we use features from
different sources, the feature values are designed to
have larger dynamic range, which is better handled
by GBDT.
The features? importance ranked by Equation 3
is shown in Table 2. We list the top 10 features.
The No.1 important feature is ExplicitQueryRatio.
The second and seventh features are from search ses-
sion analysis by counting users who changed queries
from Implicit Timestamp to Explicit Timestamp.
This is a strong source of features. The time se-
ries analysis feature is ranked No.3. Calculation of
this feature needs two years query log to be much
more effective, but we didn?t get so large data for
many queries. One of the features from recurrent
event seed list is ranked No.4. This is also an impor-
tant feature source. The ChiSquareYearDist feature
is ranked 5th, that proves the recurrent event query
frequency has a statistical distribution pattern over
years. TitleYearTop30 and TitleYearTop10 that are
derived from scraping results are ranked the 9th and
10th important.
Fig. 4 shows the distribution of feature values for
1135
Feature Rank Score
ExplicitQueryRatio 1 100
NormalizedUserSwitch 2 71.7
AutoCorrelation 3 54.0
AveNumberTokenSeeds 4 48.7
ChiSquareYearDist 5 36.3
YearUrlFPCTR 6 19.1
UserSwitch 7 11.7
QueryDailyFreq 8 10.7
TitleYearTop30 9 10.6
TitleYearTop10 10 5.8
Table 2: Top 10 most important features: rank and im-
portance score (100 is maximum)
1
2
3
4
5
6
7
8
9
10
Figure 4: Feature value distribution of all data
(blue=REQ, red=non-REQ)
each sample of the 6,000 data, where each point rep-
resents a query and each line represents a feature?s
value for all queries. One point is a query. The fea-
tures are ordered according to feature importance of
Table 2. The ?blue? points indicate REQ queries and
the ?red? points, non-REQ queries. Some features
are continuous like the 1st and 2nd. Some feature
values are discrete like the last two indicating Ti-
tleYearTop30 and TitleYearTop10. There are ?red?
samples in the 4th feature but overlapped with and
covered by ?blue? samples visually.
In the Table 3, we show F-Measure values as we
gradually added features from the feature, Explicit-
QueryRatio, according to feature importance in Ta-
ble 2. We listed the F-Measure values under three
threshold, 0.6, 0.7 and 0.8. Higher threshold will in-
crease classifier precision rate but reduce recall rate.
F-Measure is a metric combining precision rate and
recall rate. It is clearly observed that the classifier
performance is improved as more features are used.
Threshold
Feature 0.6 0.7 0.8
ExplicitQueryRatio 0.833 0.833 0.752
+NormalizedUserSwitch 0.840 0.837 0.791
+AutoCorrelation 0.850 0.839 0.823
+AveNumberTokenSeeds 0.857 0.854 0.834
+ChiSquareYearDist 0.857 0.864 0.839
+YearUrlFPCTR 0.869 0.867 0.837
+UserSwitch 0.862 0.862 0.846
+QueryDailyFreq 0.860 0.852 0.847
+TitleYearTop30 0.854 0.853 0.843
+TitleYearTop10 0.858 0.861 0.852
+All 0.876 0.867 0.862
Table 3: F-Measures as varying thresholds by adding top
features.
Query Probability
ncaa men?s basketball tournament 0.999
bmw 328i sedan reviews 0.999
new apple iphone release 0.932
sigir 0.920
new york weather in april 0.717
academy awards reviews 0.404
google ipo 0.120
adidas jp 0.082
Table 4: Probabilities of example queries by GBDT tree
classifier
Some query examples, and their scores from our
model are listed in Table 4. The last two exam-
ples, google ipo and adidas jp, have very low values,
and are not REQs. The first four queries are typical
REQs. They have higher values of features Explicit-
QueryRatio,Normalized UserSwitch and YearUrlF-
PCTR. Although both new apple iphone release re-
views and academy awards reviews are about re-
views, academy awards reviews has lower value
of NormalizedUserSwitch and ChiSquareYearDist
could be the reason for a lower score.
6 Web Search Ranking
In this section, we use the approach proposed
by (Zhang et al, 2009) to test the REQ classifier
for Web search ranking. In their approach, search
ranking is altered by boosting pages with most re-
cent year if the query is a REQ. The year indicator
1136
DCG@5 DCG@1
bucket #(query) Organic Our?s % over Organic Organic Ours % over Organic
[0.0,0.1] 59 6.87 6.96 1.48(-2.3) 4.08 4.19 2.69(-1.07)
[0.1,0.2] 76 5.86 6.01 2.52(0.98) 2.88 2.91 1.14(1.69)
[0.2,0.3] 85 6.33 6.41 1.24(2.12) 3.7 3.7 0.0(0.8)
[0.3,0.4] 75 5.18 5.24 1.18(-0.7) 2.92 2.95 1.14(1.37)
[0.4,0.5] 78 4.96 4.82 -2.84(-1.35) 2.5 2.42 -3.06(0)
[0.5,0.6] 84 5.4 5.37 -0.45(-0.3) 2.82 2.85 1.05(-1.5)
[0.6,0.7] 78 4.78) 5.19) 8.42(3.64) 2.56 2.83 10.75(4.1)
[0.7,0.8] 80 4.45 4.60 3.41(3.19) 2.21 2.26 1.98(2.8)
[0.8,0.9] 78 4.81 4.96 3.15(4.79) 2.32 2.33 0.55(0.65)
[0.9,1.0] 107 5.08 5.50 8.41*(4.41) 2.64 3.09 16.78*(1.36)
[0.0,1.0] 800 5.33 5.47 2.74*(2.17) 2.83 2.93 3.6*(1.26)
Table 5: REQ learner improves search engine organic results. The numbers in the brackets are by Zhang?s methods.
Direct comparison with Zhang?s method is valid only in the last line, using all queries. A sign ??? indicates statistical
significance (p-value<0.05)
can be detected either from title or URL of the re-
sult. For clarity, we re-write their ranking function
as below,
F(q, d) = R(q, d) + [e(do, dn) + k]e??(q)
where the ranking function, F(q, d), consists of
two parts: the base function R(q, d) plus boosting.
If the query q is not a REQ, boosting is set to zero.
Otherwise, boosting is decided by e(do, dn), k, ? and
?(q). e(do, dn) is the difference of base ranking score
between the oldest page and the newest page. If the
newest page has a lower ranking score than the old-
est page, then the difference is added to the newest
page to promote the ranking of the newest page.
?(q) is the confidence score of a REQ query. It is
the value of Eq. 1. ? and k are two empirical param-
eters. (Zhang et al, 2009)?s work has experimented
the effects of using different value of ? and k (? = 0
equals to no discounts for ranking adjustment). We
used ? = 0.4 and k = 0.3 which were the best con-
figuration in (Zhang et al, 2009).
For evaluating our methods, we randomly ex-
tracted 800 queries from the Implicit Timestamp
queries. We scraped a commercial search engine us-
ing the 800 queries. We extracted the top five search
results for each query under three configures: or-
ganic search engine results, (Zhang et al, 2009)?s
method and ours using REQ classifier. We asked
human editors to judge all the scraped (query, url)
pairs. Editors assign five grades according to rel-
evance between query and articles: Perfect, Excel-
lent, Good, Fair, and Bad. For example, a ?Perfect?
grade means the content of the url match exactly the
query intent.
We use Discounted Cumulative Gain
(DCG) (Jarvelin and Kekalainen, 2002) at rank k as
our primary evaluation metrics to measure retrieval
performance. DCG is defined as,
DCG@k =
k
?
i=1
2r(i) ? 1
log2(1 + i)
where r(i) ? {0 . . . 4} is the relevance grade of the ith
ranked document.
The Web search ranking results are shown in Ta-
ble 5. We used GBDT tree learning methods be-
cause it achieved the best results. We divided 800
test queries into 10 buckets according to the classi-
fier probability. The bucket, [0.0,0.1], contains the
query with a classifier probability greater than 0 but
less than 0.1. Our results are compared with organic
search results, but we also show the improvements
over search organic by (Zhang et al, 2009) in the
brackets. Because Zhang?s approach output differ-
ent classifier values from Ours for the same query,
buckets of the same range in the Table contain dif-
ferent queries. Hence, it is inappropriate to compare
1137
Zhang?s with Ours for the same buckets except the
last row where we used all the queries.
Our classifier?s overall performance is much bet-
ter than the organic search results. We achieved
2.74% DCG@5 gain and 3.6% DCG@1 gain over
organic search for all queries. The gains are higher
than (Zhang et al, 2009)?s results with regards to
improvement over organic results. By direct com-
parison, Ours was 2.7% better than Zhangs signif-
icantly in terms of DCG@1 by Wilcoxon signifi-
cant test. DCG@5 is 1.1% better, but not signifi-
cant. The table also show that the higher buckets
with higher probability achieved higher DCG gain
than the lower buckets overall. Our approach ob-
served 16.78% DCG@1 gain for bucket [0.9,1.0].
This shows that our methods are very effective.
7 Conclusions
We found most of REQ are long tail queries that
pose a major challenge to Web search. We have
demonstrated learning REQ is important for Web
search. this type of queries can?t be solved in tra-
ditional ranking method. We found building a REQ
classifier was a good solution. Our work described
using machine learning method to build REQ clas-
sifier. Our proposed methods are novel compar-
ing with traditional query classification methods.
We identified and developed features from query
log, search session, click and time series analysis.
We applied several ML approaches including Naive
Bayes, SVM and GBDT tree to implement REQ
learner. Finally, we show through ranking experi-
ments that the methods we proposed are very effec-
tive and beneficial for search engine ranking.
Acknowledgements
We express our thanks to who have assisted us
to complete this work, especially, to Fumiaki Ya-
maoka, Toru Shimizu, Yoshinori Kobayashi, Mit-
suharu Makita, Garrett Kaminaga, Zhuoran Chen.
References
R. Baeza-Yates, F. Saint-Jean, and C. Castillo. 2002.
Web dynamics, age and page qualit. String Process-
ing and Information Retrieval, pages 453?461.
Steven M. Beitzel, Eric C. Jensen, Ophir Frieder, David
Grossman, David D. Lewis, Abdur Chowdhury, and
Aleksandr Kolcz. 2005. Automatic web query classi-
fication using labeled and unlabeled training data. In
SIGIR ?05, pages 581?582.
K. Berberich, M. Vazirgiannis, and G. Weikum. 2005.
Time-aware authority rankings. Internet Math,
2(3):301?332.
L. Breiman, J. Friedman, R. Olshen, and C. Stone. 1984.
Classification and Regression Trees. Wadsworth and
Brooks, Monterey, CA.
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual web search engine. Proceedings of
International Conference on World Wide Web.
Andrei Z. Broder, Marcus Fontoura, Evgeniy
Gabrilovich, Amruta Joshi, Vanja Josifovski, and
Tong Zhang. Robust classification of rare queries
using web knowledge. In SIGIR ?07, pages 231?238.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines. Software avail-
able at http://www.csie.ntu.edu.tw/ cjlin/libsvm.
J. Cho, S. Roy, and R. Adams. 2005. Page quality: In
search of an unbiased web ranking. Proc. of ACM SIG-
MOD Conference.
F. Diaz. 2009. Integration of news content into web re-
sults. Proceedings of the Second ACM International
Conference on Web Search and Data Mining (WSDM),
pages 182?191.
Anlei Dong, Yi Chang, Zhaohui Zheng, Gilad Mishne,
Jing Bai, Ruiqiang Zhang, Karolina Buchner, Ciya
Liao, and Fernando Diaz. 2010a. Towards recency
ranking in web search. Proceedings of the Third ACM
International Conference on Web Search and Data
Mining (WSDM), pages 11?20.
Anlei Dong, Ruiqiang Zhang, Pranam Kolari, Jing
Bai, Fernando Diaz, Yi Chang, Zhaohui Zheng, and
Hongyuan Zha. 2010b. Time is of the essence: im-
proving recency ranking using twitter data. 19th Inter-
national World Wide Web Conference (WWW), pages
331?340.
Jonathan L. Elsas and Susan T. Dumais. 2010. Lever-
aging temporal dynamics of document content in rele-
vance ranking. In WSDM, pages 1?10.
J. H. Friedman. 2001. Greedy function approximation:
A gradient boosting machine. Annals of Statistics,
29(5):1189?1232.
Kalervo Jarvelin and Jaana Kekalainen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Transactions on Information Systems, 20:2002.
K. Sparck Jones, S. Walker, and S. E. Robertson. 2000.
A probabilistic model of information retrieval: devel-
opment and comparative experiments. Inf. Process.
Manage., 36(6):779?808.
A. C. Knig, M. Gamon, and Q. Wu. 2009. Click-through
prediction for news queries. Proc. of SIGIR, pages
347?354.
1138
Ying Li, Zijian Zheng, and Honghua (Kathy) Dai.
2005. Kdd cup-2005 report: facing a great challenge.
SIGKDD Explor. Newsl., 7(2):91?99.
Xiao Li, Ye yi Wang, and Alex Acero. 2008. Learning
query intent from regularized click graphs. In In SI-
GIR 2008, pages 339?346. ACM.
Donald Metzler, Rosie Jones, Fuchun Peng, and Ruiqiang
Zhang. 2009. Improving search relevance for im-
plicitly temporal queries. In SIGIR ?09: Proceed-
ings of the 32nd international ACM SIGIR conference
on Research and development in information retrieval,
pages 700?701.
S. Nunes. 2007. Exploring temporal evidence in web
information retrieval. BCS IRSG Symposium: Future
Directions in Information Access.
S. Pandey, S. Roy, C. Olston, J. Cho, and S. Chakrabarti.
2005. Shuffling a stacked deck: The case for partially
randomized ranking of search engine results. VLDB.
G. Salton and M. J. McGill. 1983. Introduction to mod-
ern information retrieval. McGraw-Hill, NY.
Dou Shen, Rong Pan, Jian-Tao Sun, Jeffrey Junfeng
Pan, Kangheng Wu, Jie Yin, and Qiang Yang. 2005.
Q2c@ust: our winning solution to query classification
in kddcup 2005. SIGKDD Explor. Newsl., 7(2):100?
110.
Dou Shen, Rong Pan, Jian-Tao Sun, Jeffrey Junfeng Pan,
Kangheng Wu, Jie Yin, and Qiang Yang. 2006. Query
enrichment for web-query classification. ACM Trans.
Inf. Syst., 24(3):320?352.
Ruiqiang Zhang, Yi Chang, Zhaohui Zheng, Donald
Metzler, and Jian-yun Nie. 2009. Search result
re-ranking by feedback control adjustment for time-
sensitive query. In HLT-NAACL ?09, pages 165?168.
1139
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 611?619,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Iterative Viterbi A* Algorithm forK-Best Sequential Decoding
Zhiheng Huang?, Yi Chang, Bo Long, Jean-Francois Crespo?,
Anlei Dong, Sathiya Keerthi and Su-Lin Wu
Yahoo! Labs
701 First Avenue, Sunnyvale
CA 94089, USA
{zhiheng huang,jfcrespo}@yahoo.com?
{yichang,bolong,anlei,selvarak,sulin}@yahoo-inc.com
Abstract
Sequential modeling has been widely used in
a variety of important applications including
named entity recognition and shallow pars-
ing. However, as more and more real time
large-scale tagging applications arise, decod-
ing speed has become a bottleneck for exist-
ing sequential tagging algorithms. In this pa-
per we propose 1-best A*, 1-best iterative A*,
k-best A* and k-best iterative Viterbi A* al-
gorithms for sequential decoding. We show
the efficiency of these proposed algorithms for
five NLP tagging tasks. In particular, we show
that iterative Viterbi A* decoding can be sev-
eral times or orders of magnitude faster than
the state-of-the-art algorithm for tagging tasks
with a large number of labels. This algorithm
makes real-time large-scale tagging applica-
tions with thousands of labels feasible.
1 Introduction
Sequence tagging algorithms including HMMs (Ra-
biner, 1989), CRFs (Lafferty et al, 2001), and
Collins?s perceptron (Collins, 2002) have been
widely employed in NLP applications. Sequential
decoding, which finds the best tag sequences for
given inputs, is an important part of the sequential
tagging framework. Traditionally, the Viterbi al-
gorithm (Viterbi, 1967) is used. This algorithm is
quite efficient when the label size of problem mod-
eled is low. Unfortunately, due to its O(TL2) time
complexity, where T is the input token size and L
is the label size, the Viterbi decoding can become
prohibitively slow when the label size is large (say,
larger than 200).
It is not uncommon that the problem modeled
consists of more than 200 labels. The Viterbi al-
gorithm cannot find the best sequences in tolerable
response time. To resolve this, Esposito and Radi-
cioni (2009) have proposed a Carpediem algorithm
which opens only necessary nodes in searching the
best sequence. More recently, Kaji et al (2010) pro-
posed a staggered decoding algorithm, which proves
to be very efficient on datasets with a large number
of labels.
What the aforementioned literature does not cover
is the k-best sequential decoding problem, which is
indeed frequently required in practice. For example
to pursue a high recall ratio, a named entity recogni-
tion system may have to adopt k-best sequences in
case the true entities are not recognized at the best
one. The k-best parses have been extensively stud-
ied in syntactic parsing context (Huang, 2005; Pauls
and Klein, 2009), but it is not well accommodated
in sequential decoding context. To our best knowl-
edge, the state-of-the-art k-best sequential decoding
algorithm is Viterbi A* 1. In this paper, we general-
ize the iterative process from the work of (Kaji et al,
2010) and propose a k-best sequential decoding al-
gorithm, namely iterative Viterbi A*. We show that
the proposed algorithm is several times or orders of
magnitude faster than the state-of-the-art in all tag-
ging tasks which consist of more than 200 labels.
Our contributions can be summarized as follows.
(1) We apply the A* search framework to sequential
decoding problem. We show that A* with a proper
heuristic can outperform the classic Viterbi decod-
ing. (2) We propose 1-best A*, 1-best iterative A*
decoding algorithms which are the second and third
fastest decoding algorithms among the five decod-
ing algorithms for comparison, although there is a
significant gap to the fastest 1-best decoding algo-
rithm. (3) We propose k-best A* and k-best iterative
Viterbi A* algorithms. The latter is several times or
orders of magnitude faster than the state-of-the-art
1Implemented in both CRFPP (http://crfpp.sourceforge.net/)
and LingPipe (http://alias-i.com/lingpipe/) packages.
611
k-best decoding algorithm. This algorithm makes
real-time large-scale tagging applications with thou-
sands of labels feasible.
2 Problem formulation
In this section, we formulate the sequential decod-
ing problem in the context of perceptron algorithm
(Collins, 2002) and CRFs (Lafferty et al, 2001). All
the discussions apply to HMMs as well. Formally, a
perceptron model is
f(y,x) =
T?
t=1
K?
k=1
?kfk(yt, yt?1,xt), (1)
and a CRFs model is
p(y|x) =
1
Z(x)
exp{
T?
t=1
K?
k=1
?kfk(yt, yt?1,xt)}, (2)
where x and y is an observation sequence and a la-
bel sequence respectively, t is the sequence position,
T is the sequence size, fk are feature functions and
K is the number of feature functions. ?k are the pa-
rameters that need to be estimated. They represent
the importance of feature functions fk in prediction.
For CRFs, Z(x) is an instance-specific normaliza-
tion function
Z(x) =
?
y
exp{
T?
t=1
K?
k=1
?kfk(yt, yt?1,xt)}. (3)
If x is given, the decoding is to find the best y which
maximizes the score of f(y,x) for perceptron or the
probability of p(y|x) for CRFs. As Z(x) is a con-
stant for any given input sequence x, the decoding
for perceptron or CRFs is identical, that is,
argmax
y
f(y,x). (4)
To simplify the discussion, we divide the features
into two groups: unigram label features and bi-
gram label features. Unigram features are of form
fk(yt,xt) which are concerned with the current la-
bel and arbitrary feature patterns from input se-
quence. Bigram features are of form fk(yt, yt?1,xt)
which are concerned with both the previous and the
current labels. We thus rewrite the decoding prob-
lem as
argmax
y
T?
t=1
(
K1?
k=1
?1kf
1
k (yt,xt)+
K2?
k=1
?2kf
2
k (yt, yt?1,xt)).
(5)
For a better understanding, one can inter-
pret the term
?K1
k=1 ?
1
kf
1
k (yt,xt) as node yt?s
score at position t, and interpret the term
?K2
k=1 ?
2
kf
2
k (yt, yt?1,xt) as edge (yt?1, yt)?s
score. So the sequential decoding problem is cast as
a max score pathfinding problem2. In the discussion
hereafter, we assume scores of nodes and edges are
pre-computed (denoted as n(yt) and e(yt?1, yt)),
and we can thus focus on the analysis of different
decoding algorithms.
3 Background
We present the existing algorithms for both 1-best
and k-best sequential decoding in this section. These
algorithms serve as basis for the proposed algo-
rithms in Section 4.
3.1 1-Best Viterbi
The Viterbi algorithm is a classic dynamic program-
ming based decoding algorithm. It has the computa-
tional complexity of O(TL2), where T is the input
sequence size and L is the label size3. Formally, the
Viterbi computes ?(yt), the best score from starting
position to label yt, as follows.
max
yt?1
(?yt?1 + e(yt?1, yt)) + n(yt), (6)
where e(yt?1, yt) is the edge score between nodes
yt?1 and yt, n(yt) is the node score for yt. Note
that the terms ?yt?1 and e(yt?1, yt) take value 0 for
t = 0 at initialization. Using the recursion defined
above, we can compute the highest score at end po-
sition T ? 1 and its corresponding sequence. The
recursive computation of ?yt is denoted as forward
pass since the computing traverses the lattice from
left to right. Conversely, the backward pass com-
putes ?yt as the follows.
max
yt+1
(?yt+1 + e(yt, yt+1) + n(yt+1)). (7)
Note that ?yT?1 = 0 at initialization. The max
score can be computed using maxy0(?0 + n(y0)).
We can use either forward or backward pass to
compute the best sequence. Table 1 summarizes
the computational complexity of all decoding algo-
rithms including Viterbi, which has the complexity
of TL2 for both best and worst cases. Note that
N/A means the decoding algorithms are not applica-
ble (for example, iterative Viterbi is not applicable
to k-best decoding). The proposed algorithms (see
Section 4) are highlighted in bold.
3.2 1-Best iterative Viterbi
Kaji et al (Kaji et al, 2010) presented an efficient
sequential decoding algorithm named staggered de-
coding. We use the name iterative Viterbi to describe
2With the constraint that the path consists of one and only
one node at each position.
3We ignore the feature size terms for simplicity.
612
this algorithm for the reason that the iterative pro-
cess plays a central role in this algorithm. Indeed,
this iterative process is generalized in this paper to
handle k-best sequential decoding (see Section 4.4).
The main idea is to start with a coarse lattice
which consists of both active labels and degenerate
labels. A label is referred to as an active label if it
is not grouped (e.g., all labels in Fig. 1 (a) and la-
bel A at each position in Fig. 1 (b)), and otherwise
as an inactive label (i.e., dotted nodes). The new la-
bel, which is made by grouping the inactive labels,
is referred to as a degenerate label (i.e., large nodes
covering the dotted ones). Fig. 1 (a) shows a lattice
which consists of active labels only and (b) shows
a lattice which consists of both active and degener-
ate ones. The score of a degenerate label is the max
score of inactive labels which are included in the de-
generate label. Similarly, the edge score between a
degenerate label z and an active label y? is the max
edge score between any inactive label y ? z and y?,
and the score of two degenerate labels z and z? is the
max edge score between any inactive label y ? z
and y? ? z?. Using the above definitions, the best
sequence derived from a degenerate lattice would be
the upper bound of the sequence derived from the
original lattice. If the best sequence does not include
any degenerate labels, it is indeed the best sequence
for the original lattice.
F
A
B
C
D
E
F
A
B
C
D
E
F
A
B
C
D
E
F
A
B
C
D
E
F
A A
B
C
D
E
F
A
B
C
D
E
F
A
B
C
D
E
F
B
C
D
E
Figure 1: (a) A lattice consisting of active labels only.
(b) A lattice consisting of both active labels and degener-
ate ones. Each position has one active label (A) and one
degenerate label (consisting of B, C. D, E, and F).
The pseudo code for this algorithm is shown in
Algorithm 1. The lattice is initialized to include one
active label and one degenerate label at each position
(see Figure 1 (b)). Note that the labels are ranked
by the probabilities estimated from the training data.
The Viterbi algorithm is applied to the lattice to find
the best sequence. If the sequence consists of ac-
tive labels only, the algorithm terminates and returns
such a sequence. Otherwise, the lower bound lb4 of
the active sequence in the lattice is updated and the
lattice is expanded. The lower bound can be initial-
ized to the best sequence score using a beam search
(with beam size being 1). After either a forward or
a backward pass, the lower bound is assigned with
4The maximum score of the active sequences found so far.
the best active sequence score best(lattice)5 if the
former is less than the latter. The expansion of lat-
tice ensures that the lattice has twice active labels
as before at a given position. Figure 2 shows the
column-wise expansion step. The number of active
labels in the column is doubled only if the best se-
quence of the degenerate lattice passes through the
degenerate label of that column.
Algorithm 1 Iterative Viterbi Algorithm
1: lb = best score from beam search
2: init lattice
3: for i=0;;i++ do
4: if i %2 == 0 then
5: y = forward()
6: else
7: y = backward()
8: end if
9: if y consists of active labels only then
10: return y
11: end if
12: if lb < best(lattice) then
13: lb = best(lattice)
14: end if
15: expand lattice
16: end for
Algorithm 2 Forward
1: for i=0; i < T; i++ do
2: Compute ?(yi) and ?(yi) according to Equations (6) and (7)
3: if ?(yi) + ?(yi) < lb then
4: prune yi from the current lattice
5: end if
6: end for
7: Node b = argmaxyT?1 ?(yT?1)
8: return sequence back tracked by b
(c)
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
A A A A
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
A A A A
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
A A A A
(a) (b)
Figure 2: Column-wise lattice expansion: (a) The best
sequence of the initial degenerate lattice, which does not
pass through the degenerate label in the first column. (b)
Column-wise expansion is performed and the best se-
quence is searched again. Notice that the active label in
the first column is not expanded. (c) The final result.
Algorithm 2 shows the forward pass in which the
node pruning is performed. That is, for any node,
if the best score of sequence which passes such a
node is less than the lower bound lb, such a node
is removed from the lattice. This removal is safe
as such a node does not have a chance to form an
optimal sequence. It is worth noting that, if a node
is removed, it can no longer be added into the lattice.
5We do not update the lower bound lb if we cannot find an
active sequence.
613
This property ensures the efficiency of the iterative
Viterbi algorithm. The backward pass is similar to
the forward one and it is thus omitted.
The alternative calls of forward and backward
passes (in Algorithm 1) ensure the alternative updat-
ing/lowering of node forward and backward scores,
which makes the node pruning in either forward pass
(see Algorithm 2) or backward pass more efficient.
The lower bound lb is updated once in each iteration
of the main loop in Algorithm 1. While the forward
and backwards scores of nodes gradually decrease
and the lower bound lb increases, more and more
nodes are pruned.
The iterative Viterbi algorithm has computational
complexity of T and TL2 for best and worst cases
respectively. This can be proved as follows (Kaji et
al., 2010). At the m-th iteration in Algorithm 1, it-
erative Viterbi decoding requires order of T4m time
because there are 2m active labels (plus one degen-
erate label). Therefore, it has
?m
i=0 T4
i time com-
plexity if it terminates at the m-th iteration. In the
best case in which m = 0, the time complexity is T .
In the worst case in which m = dlog2 Le ? 1 (d.e is
the ceiling function which maps a real number to the
smallest following integer), the time complexity is
order of TL2 because
?dlog2 Le?1
i=0 T4
i < 4/3TL2.
3.3 1-Best Carpediem
Esposito and Radicioni (2009) have proposed a
novel 1-best6 sequential decoding algorithm, Car-
pediem, which attempts to open only necessary
nodes in searching the best sequence in a given lat-
tice. Carpediem has the complexity of TL logL and
TL2 for the best and worst cases respectively. We
skip the description of this algorithm due to space
limitations. Carpediem is used as a baseline in our
experiments for decoding speed comparison.
3.4 K-Best Viterbi
In order to produce k-best sequences, it is not
enough to store 1-best label per node, as the k-
best sequences may include suboptimal labels. The
k-best sequential decoding gives up this 1-best
label memorization in the dynamic programming
paradigm. It stores up to k-best labels which are nec-
essary to form k-best sequences. The k-best Viterbi
algorithm thus has the computational complexity of
KTL2 for both best and worst cases.
Once we store the k-best labels per node in a lat-
tice, the k-best Viterbi algorithm calls either the for-
ward or the backward passes just in the same way as
the 1-best Viterbi decoding does. We can compute
6They did not provide k-best solutions.
the k highest score at the end position T ? 1 and the
corresponding k-best sequences.
3.5 K-Best Viterbi A*
To our best knowledge the most efficient k-best se-
quence algorithm is the Viterbi A* algorithm as
shown in Algorithm 3. The algorithm consists of one
forward pass and an A* backward pass. The forward
pass computes and stores the Viterbi forward scores,
which are the best scores from the start to the cur-
rent nodes. In addition, each node stores a backlink
which points to its predecessor.
The major part of Algorithm 3 describes the back-
ward A* pass. Before describing the algorithm, we
note that each node in the agenda represents a se-
quence. So the operations on nodes (push or pop)
correspond to the operations on sequences. Initially,
the L nodes at position T ? 1 are pushed to an
agenda. Each of the L nodes ni, i = 0, . . . , L ? 1,
represents a sequence. That is, node ni represents
the best sequence from the start to itself. The best of
the L sequences is the globally best sequence. How-
ever, the i-th best, i = 2, . . . , k, of the L sequence
may not be the globally i-th best sequence. The pri-
ority of each node is set as the score of the sequence
which is derived by such a node. The algorithm then
goes to a loop of k. In each loop, the best node is
popped off from the agenda and is stored in a set r.
The algorithm adds alternative candidate nodes (or
sequences) to the agenda via a double nested loop.
The idea is that, when an optimal node (or sequence)
is popped off, we have to push to the agenda all
nodes (sequences) which are slightly worse than the
just popped one. The interpretation of slightly worse
is to replace one edge from the popped node (se-
quence). The slightly worse sequences can be found
by the exact heuristic derived from the first Viterbi
forward pass.
Figure 3 shows an example of the push operations
for a lattice of T = 4, Y = 4. Suppose an optimal
node 2:B (in red, standing for node B at position 2,
representing the sequence of 0:A 1:D 2:B 3:C) is
popped off, new nodes of 1:A, 1:B, 1:C and 0:B,
0:C and 0:D are pushed to the agenda according to
the double nested for loop in Algorithm 3. Each
of the pushed nodes represents a sequence, for ex-
ample, node 1:B represents a sequence which con-
sists of three parts: Viterb sequence from start to
1:B (0:C 1:B), 2:B and forward link of 2:B (3:C
in this case). All of these pushed nodes (sequences)
are served as candidates for the next agenda pop op-
eration.
The algorithm terminates the loop once it has op-
timal k nodes. The k-best sequences can be de-
rived by the k optimal nodes. This algorithm has
614
TB
C
D
B
C
D
B
C
D
B
C
D
A A A A
31 20
Figure 3: Alternative nodes push after popping an opti-
mal node.
computation complexity of TL2 + TL for both best
and worst cases, with the first term accounting for
Viterbi forward pass and the second term account-
ing for A* backward process. The bottleneck is thus
at the Viterbi forward pass.
Algorithm 3K-Best Viterbi A* algorithm
1: forward()
2: push L best nodes to agenda q
3: c = 0
4: r = {}
5: while c < K do
6: Node n = q.pop()
7: r = r ? n
8: for i = n.t? 1; i ? 0; i?? do
9: for j = 0; j < L; j + + do
10: if j! = n.backlink.y then
11: create new node s at position i and label j
12: s.forwardlink = n
13: q.push(s)
14: end if
15: end for
16: n = n.backlink
17: end for
18: c+ +
19: end while
20: return K best sequences derived by r
4 Proposed Algorithms
In this section, we propose A* based sequen-
tial decoding algorithms that can efficiently handle
datasets with a large number of labels. In particular,
we first propose the A* and the iterative A* decod-
ing algorithm for 1-best sequential decoding. We
then extend the 1-best A* algorithm to a k-best A*
decoding algorithm. We finally apply the iterative
process to the Viterbi A* algorithm, resulting in the
iterative Viterbi A* decoding algorithm.
4.1 1-Best A*
A*(Hart et al, 1968; Russell and Norvig, 1995), as
a classic search algorithm, has been successfully ap-
plied in syntactic parsing (Klein and Manning, 2003;
Pauls and Klein, 2009). The general idea of A* is to
consider labels yt which are likely to result in the
best sequence using a score f as follows.
f(y) = g(y) + h(y), (8)
where g(y) is the score from start to the current node
and h(y) is a heuristic which estimates the score
from the current node to the target. A* uses an
agenda (based on the f score) to decide which nodes
are to be processed next. If the heuristic satisfies the
condition h(yt?1) ? e(yt?1, yt) + h(yt), then h is
called monotone or admissible. In such a case, A* is
guaranteed to find the best sequence. We start with
the naive (but admissible) heuristic as follows
h(yt) =
T?1?
i=t+1
(maxn(yi) + max e(yi?1, yi)). (9)
That is, the heuristic of node yt to the end is the sum
of max edge scores between any two positions and
max node scores per position. Similar to (Pauls and
Klein, 2009) we explore the heuristic in different
coarse levels. We apply the Viterbi backward pass
to different degenerate lattices and use the Viterbi
backward scores as different heuristics. Different
degenerate lattices are generated from different it-
erations of Algorithm 1: The m-th iteration corre-
sponds to a lattice of (2m+1)?T nodes. A largerm
indicates a more accurate heuristic, which results in
a more efficient A* search (fewer nodes being pro-
cessed). However, this efficiency comes with the
price that such an accurate heuristic requires more
computation time in the Viterbi backward pass. In
our experiments, we try the naive heuristic and the
following values of m: 0, 3, 6 and 9.
In the best case, A* expands one node per posi-
tion, and each expansion results in the push of all
nodes at next position to the agenda. The search is
similar to the beam search with beam size being 1.
The complexity is thus TL. In the worst case, A*
expands every node per position, and each expan-
sion results in the push of all nodes at next position
to the agenda. The complexity thus becomes TL2.
4.2 1-Best Iterative A*
The iterative process as described in the iterative
Viterbi decoding can be used to boost A* algorithm,
resulting in the iterative A* algorithm. For simplic-
ity, we only make use of the naive heuristic in Equa-
tion (9) in the iterative A* algorithm. We initialize
the lattice with one active label and one degenerate
label at each position (see Figure 1 (b)). We then run
A* algorithm on the degenerate lattice and get the
best sequence. If the sequence is active we return
it. Otherwise we expand the lattice in each iteration
until we find the best active sequence. Similar to
iterative Viterbi algorithm, iterative A* has the com-
plexity of T and TL2 for the best and worst cases
respectively.
4.3 K-Best A*
The extension from 1-best A* to k-best A* is again
due to the memorization of k-best labels per node.
615
Table 1: Best case and worst case computational complexity of various decoding algorithms.
1-best decoding K-best decoding
best case worst case best case worst case
beam TL TL KTL KTL
Viterbi TL2 TL2 KTL2 KTL2
iterative Viterbi T TL2 N/A N/A
Carpediem TL logL TL2 N/A N/A
A* TL TL2 KTL KTL2
iterative A* T TL2 N/A N/A
Viterbi A* N/A N/A TL2 +KTL TL2 +KTL
iterative Viterbi A* N/A N/A T +KT TL2 +KTL
We use either the naive heuristic (Equation (9)) or
different coarse level heuristics by setting m to be 0,
3, 6 or 9 (see Section 4.1). The first k nodes which
are popped off the agenda can be used to back track
the k-best sequences. The k-best A* algorithm has
the computational complexity of KTL and KTL2
for best and worst cases respectively.
4.4 K-Best Iterative Viterbi A*
We now present the k-best iterative Viterbi A* algo-
rithm (see Algorithm 4) which applies the iterative
process to k-best Viterbi A* algorithm. The major
difference between 1-best iterative Viterbi A* algo-
rithm (Algorithm 1) and this algorithm is that the
latter calls the k-best Vitebi A* (Algorithm 3) after
the best sequence is found. If the k-best sequences
are all active, we terminate the algorithm and return
the k-best sequences. If we cannot find either the
best active sequence or the k-best active sequences,
we expand the lattice to continue the search in the
next iteration.
As in the iterative Viterbi algorithm (see Section
3.2), nodes are pruned at each position in forward
or backward passes. Efficient pruning contributes
significantly to speeding up decoding. Therefore, to
have a tighter (higher) lower bound lb is important.
We initialize the lower bound lb with the k-th best
score from beam search (with beam size being k) at
line 1. Note that the beam search is performed on the
original lattice which consists of L active labels per
position. The beam search time is negligible com-
pared to the total decoding time. At line 16, we up-
date lb as follows. We enumerate the best active se-
quences backtracked by the nodes at position T ? 1.
If the current lb is less than the k-th active sequence
score, we update the lbwith the k-th active sequence
score (we do not update lb if there are less than k ac-
tive sequences). At line 19, we use the sequences
returned from Viterbi A* algorithm to update the lb
in the same manner. To enable this update, we re-
quest the Viterbi A* algorithm to return k?, k? > k,
sequences (line 10). A larger number of k? results
in a higher chance to find the k-th active sequence,
which in turn offers a tighter (higher) lb, but it comes
with the expense of additional time (the backward
A* process takes O(TL) time to return one more
sequence). In experiments, we found the lb updates
on line 1 and line 16 are essential for fast decoding.
The updating of lb using Viterbi A* sequences (line
19) can boost the decoding speed further. We exper-
imented with different k? values (k? = nk, where n
is an integer) and selected k? = 2k which results in
the largest decoding speed boost.
Algorithm 4K-Best iterative Viterbi A* algorithm
1: lb = k-th best (original lattice)
2: init lattice
3: for i = 0; ; i+ + do
4: if i%2 == 0 then
5: y = forward()
6: else
7: y = backward()
8: end if
9: if y consists of active labels only then
10: ys= k-best Viterbi A* (Algorithm 3)
11: if ys consists of active sequences only then
12: return ys
13: end if
14: end if
15: if lb < k-th best(lattice) then
16: lb = k-th best(lattice)
17: end if
18: if lb < k-th best(ys) then
19: lb = k-th best(ys)
20: end if
21: expand lattice
22: end for
5 Experiments
We compare aforementioned 1-best and k-best se-
quential decoding algorithms using five datasets in
this section.
5.1 Experimental setting
We apply 1-best and k-best sequential decoding al-
gorithms to five NLP tagging tasks: Penn TreeBank
(PTB) POS tagging, CoNLL2000 joint POS tag-
ging and chunking, CoNLL 2003 joint POS tagging,
chunking and named entity tagging, HPSG supertag-
ging (Matsuzaki et al, 2007) and a search query
named entity recognition (NER) dataset. We used
616
sections 02-21 of PTB for training and section 23
for testing in POS task. As in (Kaji et al, 2010),
we combine the POS tags and chunk tags to form
joint tags for CoNLL 2000 dataset, e.g., NN|B-NP.
Similarly we combine the POS tags, chunk tags, and
named entity tags to form joint tags for CoNLL 2003
dataset, e.g., PRP$|I-NP|O. Note that by such tag
joining, we are able to offer different tag decodings
(for example, chunking and named entity tagging)
simultaneously. This indeed is one of the effective
approaches for joint tag decoding problems. The
search query NER dataset is an in-house annotated
dataset which assigns semantic labels, such as prod-
uct, business tags to web search queries.
Table 2 shows the training and test sets size (sen-
tence #), the average token length of test dataset and
the label size for the five datasets. POS and su-
pertag datasets assign tags to tokens while CoNLL
2000 , CoNLL 2003 and search query datasets as-
sign tags to phrases. We use the standard BIO en-
coding for CoNLL 2000, CoNLL 2003 and search
query datasets.
Table 2: Training and test datasets size, average token
length of test set and label size for five datasets.
training # test # token length label size
POS 39831 2415 23 45
CoNLL2000 8936 2012 23 319
CoNLL2003 14987 3684 12 443
Supertag 37806 2291 22 2602
search query 79569 6867 3 323
Due to the long CRF training time (days to weeks
even for stochastic gradient descent training) for
these large label size datasets, we choose the percep-
tron algorithm for training. The models are averaged
over 10 iterations (Collins, 2002). The training time
takes minutes to hours for all datasets. We note that
the selection of training algorithm does not affect
the decoding process: the decoding is identical for
both CRF and perceptron training algorithms. We
use the common features which are adopted in previ-
ous studies, for example (Sha and Periera, 2003). In
particular, we use the unigrams of the current and its
neighboring words, word bigrams, prefixes and suf-
fixes of the current word, capitalization, all-number,
punctuation, and tag bigrams for POS, CoNLL2000
and CoNLL 2003 datasets. For supertag dataset,
we use the same features for the word inputs, and
the unigrams and bigrams for gold POS inputs. For
search query dataset, we use the same features plus
gazetteer based features.
5.2 Results
We report the token accuracy for all datasets to facil-
itate comparison to previous work. They are 97.00,
94.70, 95.80, 90.60 and 88.60 for POS, CoNLL
2000, CoNLL 2003, supertag, and search query re-
spectively. We note that all decoding algorithms as
listed in Section 3 and Section 4 are exact. That is,
they produce exactly the same accuracy. The accu-
racy we get for the first four tasks is comparable to
the state-of-the-art. We do not have a baseline to
compare with for the last dataset as it is not pub-
licly available7. Higher accuracy may be achieved if
more task specific features are introduced on top of
the standard features. As this paper is more con-
cerned with the decoding speed, the feature engi-
neering is beyond the scope of this paper.
Table 3 shows how many iterations in average
are required for iterative Viterbi and iterative Viterbi
A* algorithms. Although the max iteration size is
bounded to dlog2 Le for each position (for exam-
ple, 9 for CoNLL 2003 dataset), the total iteration
number for the whole lattice may be greater than
dlog2 Le as different positions may not expand at
the same time. Despite the large number of itera-
tions used in iterative based algorithms (especially
iterative Viterbi A* algorithm), the algorithms are
still very efficient (see below).
Table 3: Iteration numbers of iterative Viterbi and itera-
tive Viterbi A* algorithms for five datasets.
POS CoNLL2000 CoNLL2003 Supertag search query
iter Viter 6.32 8.76 9.18 10.63 6.71
iter Viter A* 14.42 16.40 15.41 18.62 9.48
Table 4 and 5 show the decoding speed (sen-
tences per second) of 1-best and 5-best decoding al-
gorithms respectively. The proposed decoding algo-
rithms and the largest decoding speeds across differ-
ent decoding algorithms (other than beam) are high-
lighted in bold. We exclude the time for feature ex-
traction in computing the speed. The beam search
decoding is also shown as a baseline. We note that
beam decoding is the only approximate decoding al-
gorithm in this table. All other decoding algorithms
produce exactly the same accuracy, which is usually
much better than the accuracy of beam decoding.
For 1-best decoding, iterative Viterbi always out-
performs other ones. A* with a proper heuristic de-
noted as A* (best), that is, the best A* using naive
heuristic or the values of m being 0, 3, 6 or 9 (see
Section 4.1), can be the second best choice (ex-
cept for the POS task), although the gap between
iterative Viterbi and A* is significant. For exam-
ple, for CoNLL 2003 dataset, the former can de-
code 2239 sentences per second while the latter only
decodes 225 sentences per second. The iterative
process successfully boosts the decoding speed of
iterative Viterbi compared to Viterbi, but it slows
down the decoding speed of iterative A* compared
7The lower accuracy is due to the dynamic nature of queries:
many of test query tokens are unseen in the training set.
617
to A*(best). This is because in the Viterbi case,
the iterative process has a node pruning procedure,
while it does not have such pruning in A*(best)
algorithm. Take CoNLL 2003 data as an exam-
ple, the removal of the pruning slows down the 1-
best iterative Viterbi decoding from 2239 to 604
sentences/second. Carpediem algorithm performs
poorly in four out of five tasks. This can be ex-
plained as follows. The Carpediem implicitly as-
sumes that the node scores are the dominant factors
to determine the best sequence. However, this as-
sumption does not hold as the edge scores play an
important role.
For 5-best decoding, k-best Viterbi decoding is
very slow. A* with a proper heuristic is still slow.
For example, it only reaches 11 sentences per second
for CoNLL 2003 dataset. The classic Viterbi A* can
usually obtain a decent decoding speed, for example,
40 sentences per second for CoNLL 2003 dataset.
The only exception is supertag dataset, on which the
Viterbi A* decodes 0.1 sentence per second while
the A* decodes 3. This indicates the scalability is-
sue of Viterbi A* algorithm for datasets with more
than one thousand labels. The proposed iterative
Viterbi A* is clearly the winner. It speeds up the
Viterbi A* to factors of 4, 7, 360, and 3 for CoNLL
2000, CoNLL 2003, supertag and query search data
respectively. The decoding speed of iterative Viterbi
A* can even be comparable to that of beam search.
Figure 4 shows k-best decoding algorithms de-
coding speed with respect to different k values for
CoNLL 2003 data . The Viterbi A* and iterative
Viterbi A* algorithms are significantly faster than
the Viterbi and A*(best) algorithms. Although the
iterative Viterbi A* significantly outperforms the
Viterbi A* for k < 30, the speed of the former con-
verges to the latter when k becomes 90 or larger.
This is expected as the k-best sequences span over
the whole lattice: the earlier iteration in iterative
Viterbi A* algorithm cannot provide the k-best se-
quences using the degenerate lattice. The over-
head of multiple iterations slows down the decoding
speed compared to the Viterbi A* algorithm.
l l l l l l l l l l10 20 30 40 50 60 70 80 90 100020
406080
100120140
160180200
k
sentenc
es/secon
d l ViterbiA*(best)Viterbi A*iterative Viterbi A*
Figure 4: Decoding speed of k-best decoding algorithms
for various k for CoNLL 2003 dataset.
6 Related work
The Viterbi algorithm is the only exact algorithm
widely adopted in the NLP applications. Esposito
and Radicioni (2009) proposed an algorithm which
opens necessary nodes in a lattice in searching the
best sequence. The staggered decoding (Kaji et al,
2010) forms the basis for our work on iterative based
decoding algorithms. Apart from the exact decod-
ing, approximate decoding algorithms such as beam
search are also related to our work. Tsuruoka and
Tsujii (2005) proposed easiest-first deterministic de-
coding. Siddiqi and Moore (2005) presented the pa-
rameter tying approach for fast inference in HMMs.
A similar idea was applied to CRFs as well (Cohn,
2006; Jeong, 2009). We note that the exact algo-
rithm always guarantees the optimality which can-
not be attained in approximate algorithms.
In terms of k-best parsing, Huang and Chiang
(2005) proposed an efficient algorithm which is sim-
ilar to the k-best Viterbi A* algorithm presented in
this paper. Pauls and Klein (2009) proposed an algo-
rithm which replaces the Viterbi forward pass with
an A* search. Their algorithm optimizes the Viterbi
pass, while the proposed iterative Viterbi A* algo-
rithm optimizes both Viterbi and A* passes.
This paper is also related to the coarse to fine
PCFG parsing (Charniak et al, 2006) as the degen-
erate labels can be treated as coarse levels. How-
ever, the difference is that the coarse-to-fine parsing
is an approximate decoding while ours is exact one.
In terms of different coarse levels of heuristic used
in A* decoding, this paper is related to the work of
hierarchical A* framework (Raphael, 2001; Felzen-
szwalb et al, 2007). In terms of iterative process,
this paper is close to (Burkett et al, 2011) as both
exploit the search-and-expand approach.
7 Conclusions
We have presented and evaluated the A* and itera-
tive A* algorithms for 1-best sequential decoding in
this paper. In addition, we proposed A* and iterative
Viterbi A* algorithm for k-best sequential decoding.
K-best Iterative A* algorithm can be several times
or orders of magnitude faster than the state-of-the-
art k-best decoding algorithm. It makes real-time
large-scale tagging applications with thousands of
labels feasible.
Acknowledgments
We wish to thank Yusuke Miyao and Nobuhiro Kaji
for providing us the HPSG Treebank data. We are
grateful for the invaluable comments offered by the
anonymous reviewers.
618
Table 4: Decoding speed (sentences per second) of 1-best decoding algorithms for five datasets.
POS CoNLL2000 CoNLL2003 supertag query search
beam 7252 1381 1650 395 7571
Viterbi 2779 51 41 0.19 443
iterative Viterbi 5833 972 2239 213 6805
Carpediem 2638 14 20 0.15 243
A* (best) 802 131 225 8 880
iterative A* 1112 84 109 3 501
Table 5: Decoding speed (sentences per second) of 5-best decoding algorithms for five datasets.
POS CoNLL2000 CoNLL2003 supertag query search
beam 2760 461 592 75 4354
Viterbi 19 0.41 0.25 0.12 3.83
A* (best) 205 4 11 3 92
Viterbi A* 1266 47 40 0.1 357
iterative Viterbi A* 788 200 295 36 1025
References
D. Burkett, D. Hall, and D. Klein. 2011. Optimal graph
search with iterated graph cuts. Proceedings of AAAI.
E. Charniak, M. Johnson, M. Elsner, J. Austerweil, D.
Ellis, I. Haxton, C. Hill, R. Shrivaths, J. Moore, M.
Pozar, and T. Vu. 2006. Multi-level coarse-to-fine
PCFG parsing. Proceedings of NAACL.
T. Cohn. 2006. Efficient inference in large conditional
random fields. Proceedings of ECML.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. Proceedings of EMNLP.
R. Esposito and D. P. Radicioni. 2009. Carpediem:
Optimizing the Viterbi Algorithm and Applications to
Supervised Sequential Learning. Journal of Machine
Learning Research.
P. Felzenszwalb and D. McAllester. 2007. The general-
ized A* architecture. Journal of Artificial Intelligence
Research.
P. E. Hart, N. J. Nilsson, and B. Raphael. 1968. A For-
mal Basis for the Heuristic Determination of Minimum
Cost Paths. IEEE Transactions on Systems Science
and Cybernetics.
L. Huang and D. Chiang. 2005. Better k-best parsing.
Proceedings of the International Workshops on Parsing
Technologies (IWPT).
M. Jeong, C. Y. Lin, and G. G. Lee. 2009. Efficient infer-
ence of CRFs for large-scale natural language data.
Proceedings of ACL-IJCNLP Short Papers.
N. Kaji, Y. Fujiwara, N. Yoshinaga, and M. Kitsuregawa.
2010. Efficient Staggered Decoding for Sequence La-
beling. Proceedings of ACL.
D. Klein and C. Manning. 2003. A* parsing: Fast exact
Viterbi parse selection. Proceedings of ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. Proceedings of
ICML.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2007. Efficient
HPSG parsing with supertagging and CFG-filtering.
Proceedings of IJCAI.
A. Pauls and D. Klein. 2009. K-Best A* Parsing. Pro-
ceedings of ACL.
L. R. Rabiner. 1989. A tutorial on hidden Markov models
and selected applications in speech recognition. Pro-
ceedings of The IEEE.
C. Raphael. 2001. Coarse-to-fine dynamic program-
ming. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence.
S. Russell and P. Norvig. 1995. Artificial Intelligence: A
Modern Approach.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. Proceedings of HLT-NAACL.
S. M. Siddiqi and A. Moore. 2005. Fast inference and
learning in large-state-space HMMs. Proceedings of
ICML.
Y. Tsuruoka and J. Tsujii. 2005. Bidirectional in-
ference with the easiest-first strategy for tagging se-
quence data. Proceedings of HLT/EMNLP.
A. J. Viterbi. 1967. Error bounds for convolutional
codes and an asymptotically optimum decoding algo-
rithm. IEEE Transactions on Information Theory.
619
