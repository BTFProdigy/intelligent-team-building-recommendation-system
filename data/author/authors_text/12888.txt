Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 297?304
Manchester, August 2008
Dependency-Based N-Gram Models for
General Purpose Sentence Realisation
Yuqing Guo
NCLT, School of Computing
Dublin City University
Dublin 9, Ireland
yguo@computing.dcu.ie
Josef van Genabith
NCLT, School of Computing
Dublin City University
IBM CAS, Dublin, Ireland
josef@computing.dcu.ie
Haifeng Wang
Toshiba (China)
Research & Development Center
Beijing, 100738, China
wanghaifeng@rdc.toshiba.com.cn
Abstract
We present dependency-based n-gram
models for general-purpose, wide-
coverage, probabilistic sentence realisa-
tion. Our method linearises unordered
dependencies in input representations
directly rather than via the application
of grammar rules, as in traditional chart-
based generators. The method is simple,
efficient, and achieves competitive accu-
racy and complete coverage on standard
English (Penn-II, 0.7440 BLEU, 0.05
sec/sent) and Chinese (CTB6, 0.7123
BLEU, 0.14 sec/sent) test data.
1 Introduction
Sentence generation,1 or surface realisation can be
described as the problem of producing syntacti-
cally, morphologically, and orthographically cor-
rect sentences from a given semantic or syntactic
representation.
Most general-purpose realisation systems de-
veloped to date transform the input into sur-
face form via the application of a set of gram-
mar rules based on particular linguistic theories,
e.g. Lexical Functional Grammar (LFG), Head-
Driven Phrase Structure Grammar (HPSG), Com-
binatory Categorial Grammar (CCG), Tree Ad-
joining Grammar (TAG) etc. These grammar rules
are either carefully handcrafted, as those used in
FUF/SURGE (Elhadad, 1991), LKB (Carroll et al,
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1In this paper, the term ?generation? is used generally for
what is more strictly referred to by the term ?tactical genera-
tion? or ?surface realisation?.
1999), OpenCCG (White, 2004) and XLE (Crouch
et al, 2007), or created semi-automatically (Belz,
2007), or fully automatically extracted from an-
notated corpora, like the HPSG (Nakanishi et
al., 2005), LFG (Cahill and van Genabith, 2006;
Hogan et al, 2007) and CCG (White et al,
2007) resources derived from the Penn-II Treebank
(PTB) (Marcus et al, 1993).
Over the last decade, probabilistic models have
become widely used in the field of natural lan-
guage generation (NLG), often in the form of a re-
alisation ranker in a two-stage generation architec-
ture. The two-stage methodology is characterised
by a separation between generation and selection,
in which rule-based methods are used to generate a
space of possible paraphrases, and statistical meth-
ods are used to select the most likely realisation
from the space. By and large, two statistical mod-
els are used in the rankers to choose output strings:
? N-gram language models over different units,
such as word-level bigram/trigram mod-
els (Bangalore and Rambow, 2000; Langk-
ilde, 2000), or factored language models inte-
grated with syntactic tags (White et al, 2007).
? Log-linear models with different syntactic
and semantic features (Velldal and Oepen,
2005; Nakanishi et al, 2005; Cahill et al,
2007).
To date, however, probabilistic models learn-
ing direct mappings from generation input to sur-
face strings, without the effort to construct a gram-
mar, have rarely been explored. An exception is
Ratnaparkhi (2000), who presents maximum en-
tropy models to learn attribute ordering and lexi-
cal choice for sentence generation from a semantic
representation of attribute-value pairs, restricted to
an air travel domain.
297
SNP
PRP
We
VP
VBP
believe
PP
IN
in
NP
NP
DT
the
NN
law
PP
IN
of
NP
NNS
averages
f
1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
PRED ?believe?
TENSE pres
SUBJ f
2
?
?
?
PRED ?pro?
PERS 1
NUM pl
?
?
?
OBL f
3
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
PFORM ?in?
OBJ f
4
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
PRED ?law?
PERS 3
NUM sg
SPEC f
5
[
DET f
6
[
PRED ?the?
]
]
ADJ
?
?
?
?
?
?
?
?
f
7
?
?
?
?
?
?
PFORM ?of?
OBJ f
8
?
?
?
PRED ?average?
PERS 3
NUM pl
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(a.) c-structure (b.) f-structure
string We believe in the law of averages
position 1 2 3 4 5 6 7
f
1
SUBJ PRED OBL
f
3
PFORM OBJ
f
4
SPEC PRED ADJ
f
7
PFORM OBJ
(c.) linearised grammatical functions / bilexical dependencies
Figure 1: C- and f-structures for the sentence We believe in the law of averages.
In this paper, we develop an efficient, wide-
coverage generator based on simple n-gram mod-
els to directly linearise dependency relations from
the input representations. Our work is aimed at
general-purpose sentence generation but couched
in the framework of Lexical Functional Grammar.
We give an overview of LFG and the dependency
representations we use in Section 2. We describe
the general idea of our dependency-based gener-
ation in Section 3 and give details of the n-gram
generation models in Section 4. Section 5 explains
the experiments and provides results for both En-
glish and Chinese data. Section 6 compares the re-
sults with previous work and between languages.
Finally we conclude with a summary and outline
future work.
2 LFG-Based Generation
2.1 Lexical Functional Grammar
Lexical Functional Grammar (Kaplan and Bres-
nan, 1982) is a constraint-based grammar for-
malism which postulates (minimally) two lev-
els of representation: c(onstituent)-structure and
f(unctional)-structure. As illustrated in Figure 1,
a c-structure is a conventional phrase structure
tree and captures surface grammatical configu-
rations. The f-structure encodes more abstract
functional relations like SUBJ(ect), OBJ(ect) and
ADJ(unct). F-structures are hierarchical attribute-
value matrix representations of bilexical labelled
dependencies, approximating to basic predicate-
argument/adjunct structures.2 Attributes in f-
structure come in two different types:
? Grammatical Functions (GFs) indicate the re-
lationship between the predicate and depen-
dents. GFs can be divided into:
? arguments are subcategorised for by the
predicate, such as SUBJ(ect), OBJ(ect),
and thus can only occur once in each lo-
cal f-structure.
? modifiers like ADJ(unct), COORD(inate)
are not subcategorised for by the predi-
cate, and can occur any number of times
in a local f-structure.
? Atomic-valued features describe linguistic
properties of the predicate, such as TENSE,
ASPECT, MOOD, PERS, NUM, CASE etc.
2.2 Generation from F-Structures
Work on generation in LFG generally assumes that
the generation task is to determine the set of strings
of the language that corresponds to a specified f-
structure, given a particular grammar (Kaplan and
Wedekind, 2000). Previous work on generation
2F-structures can be also interpreted as quasi-logical
forms (van Genabith and Crouch, 1996), which more closely
resemble inputs used by some other generators.
298
within LFG includes the XLE,3 Cahill and van
Genabith (2006), Hogan et al (2007) and Cahill et
al. (2007). The XLE generates sentences from f-
structures according to parallel handcrafted gram-
mars for English, French, German, Norwegian,
Japanese, and Urdu. Based on the German XLE
resources, Cahill et al (2007) describe a two-stage,
log-linear generation model. Cahill and van Gen-
abith (2006) and Hogan et al (2007) present a
chart generator using wide-coverage PCFG-based
LFG approximations automatically acquired from
treebanks (Cahill et al, 2004).
3 Dependency-Based Generation: the
Basic Idea
Traditional LFG generation models can be re-
garded as the reverse process of parsing, and
use bi-directional f-structure-annotated CFG rules.
In a sense, the generation process is driven by
an input dependency (or f-structure) representa-
tion, but proceeds through the ?detour? of us-
ing dependency-annotated CFG (or PCFG) gram-
mars and chart-based generators. In this paper,
we develop a simple n-gram and dependency-
based, wide-coverage, robust, probabilistic gener-
ation model, which cuts out the middle-man from
previous approaches: the CFG-component.
Our approach is data-driven: following the
methodology in (Cahill et al, 2004; Guo et al,
2007), we automatically convert the English Penn-
II treebank and the Chinese Penn Treebank (Xue
et al, 2005) into f-structure banks. F-structures
such as Figure 1(b.) are unordered, i.e. they do
not carry information on to the relative surface or-
der of local GFs. In order to generate a string
from an f-structure, we need to linearise the GFs
(at each level of embedding) in the f-structure (and
map lemmas and features to surface forms). We
do this in terms of n-gram models over GFs. In or-
der to build the n-gram models, we linearise the f-
structures automatically produced from treebanks
by associating the numerical string position (word
offset from start of the sentence) with the predicate
in each local f-structure, producing GF sequences
as in Figure 1(c.).
Even though the n-gram models are exemplified
using LFG f-structures, they are general-purpose
models and thus suitable for any bilexical labelled
dependency (Nivre, 2006) or predicate-argument
type representations, such as the labelled feature-
3http://www2.parc.com/isl/groups/nltt/xle/
value structures used in HALogen and the func-
tional descriptions in the FUF/SURGE system.
4 N-Gram Models for Dependency-Based
Generation
4.1 Basic N-Gram Model
The primary task of a sentence generator is to de-
termine the linear order of constituents and words,
represented as lemmas in predicates in f-structures.
At a particular local f-structure, the task of gen-
erating a string covered by the local f-structure
is equivalent to linearising all the GFs present at
that local f-structure. E.g. in f
4
in Figure 1, the
unordered set of local GFs {SPEC, PRED, ADJ}
generates the surface sequence ?the law of aver-
ages?. We linearise the GFs in the set by com-
puting n-gram models, similar to traditional word-
based language models, except using the names of
GFs (including PRED) instead of words. Given
a (sub-) f-structure F containing m GFs, the n-
gram model searches for the best surface sequence
S
m
1
=s
1
...s
m
generated by the GF linearisation
GF
m
1
= GF
1
...GF
m
, which maximises the prob-
ability P (GFm
1
). Using n-gram models, P (GFm
1
)
is calculated according to Eq.(1).
P (GF
m
1
) = P (GF
1
...GF
m
)
=
m
?
k=1
P (GF
k
|GF
k?1
k?n+1
) (1)
4.2 Factored N-Gram Models
In addition to the basic n-gram model over bare
GFs, we integrate contextual and fine-grained
lexical information into several factored models.
Eq.(2) additionally conditions the probability of
the n-gram on the parent GF label of the cur-
rent local f-structure f
i
, Eq.(3) on the instantiated
PRED of the local f-structure f
i
, and Eq.(4) lexi-
calises the model, where each GF is augmented
with its own predicate lemma.
P
g
(GF
m
1
) =
m
?
k=1
P (GF
k
|GF
k?1
k?n+1
, GF
i
) (2)
P
p
(GF
m
1
) =
m
?
k=1
P (GF
k
|GF
k?1
k?n+1
, P red
i
) (3)
P
l
(GF
m
1
) =
m
?
k=1
P (Lex
k
|Lex
k?1
k?n+1
) (4)
299
To avoid data sparseness, the factored n-gram
models P f are smoothed by linearly interpolating
the basic n-gram model P , as in Eq.(5).
?
P
f
(GF
m
1
) = ?P
f
(GF
m
1
) + (1? ?)P (GF
m
1
) (5)
Additionally, the lexicalised n-gram models P l
are combined with the other two models con-
ditioned on the additional parent GF P g and
PRED P p, as shown in Eqs. (6) & (7), respectively.
?
P
lg
(GF
m
1
) = ?
1
P
l
(GF
m
1
) + ?
2
P
g
(GF
m
1
)
+?
3
P (GF
m
1
) (6)
?
P
lp
(GF
m
1
) = ?
1
P
l
(GF
m
1
) + ?
2
P
p
(GF
m
1
)
+?
3
P (GF
m
1
) (7)
where
?
?
i
= 1
Table 1 exemplifies the different n-gram models
for the local f-structure f
4
in Figure 1.
Model N-grams Cond.
basic (P ) SPEC PRED ADJ
gf (P g ) SPEC PRED ADJ OBL
pred (Pp) SPEC PRED ADJ ?law?
lex (P l) SPEC PRED[?law?] ADJ[?of?]
Table 1: Examples of n-grams for f
4
in Figure 1
Besides grammatical functions, we also make
use of atomic-valued features like TENSE, PERS,
NUM (etc.) to aid linearisation. The attributes and
values of these features are integrated into the GF
n-grams for disambiguation (see Section 5.2).
4.3 Generation Algorithm
Our basic n-gram based generation model im-
plements the simplifying assumption that lineari-
sation at one sub-f-structure is independent of
linearisation at any other sub-f-structures. This
assumption is feasible for projective dependen-
cies. In most cases (at least in English and
Chinese), non-projective dependencies are only
used to account for Long-Distance Dependen-
cies (LDDs). Consider sentence (1) discussed
in Carroll et al (1999) and its corresponding f-
structure in Figure 2. In LFG f-structures, LDDs
are represented via reentrancies between ?dislo-
cated? TOPIC, TOPIC REL, FOCUS (etc.) GFs and
?source? GFs subcategorised for by local predi-
cates, but only the dislocated GFs are instantiated
in generation. Therefore traces of the source GFs
in input f-structures are removed before genera-
tion, and non-projective dependencies are trans-
formed into simple projective dependencies.
(1) How quickly did the newspapers say the ath-
lete ran?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
FOCUS
?
?
?
PRED ?quickly?
ADJ
{
[
PRED ?how?
]
}
?
?
?
1
PRED ?say?
SUBJ
?
?
PRED ?newspaper?
SPEC
[
PRED ?the?
]
?
?
COMP
?
?
?
?
?
?
?
?
PRED ?run?
SUBJ
?
?
PRED ?athlete?
SPEC
[
PRED ?the?
]
?
?
ADJ 1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 2: schematic f-structure for How quickly
did the newspapers say the athlete ran?
In summary, given an input f-structure f , the
core algorithm of the generator recursively tra-
verses f and at each sub-f-structure f
i
:
1. instantiates the local predicate at f
i
and per-
forms inflections/declensions if necessary
2. calculates the GF linearisations present at f
i
by n-gram models
3. finds the most probable GF sequence among
all possibilities by Viterbi search
4. generates the string covered by f
i
according
to the linearised GFs
5 Experiments and Evaluation
To test the performance and coverage of our n-
gram-based generation models, experiments are
carried out for both English and Chinese, two lan-
guages with distinct properties.
5.1 Experiment Design
Experiments on English data are carried out on
the WSJ portion of the PTB, using standard train-
ing/test/development splits, viz 39,832 sentences
from sections 02-21 are used for training, 2,416
sentences from section 23 for testing, while 1,700
sentences from section 22 are held out for develop-
ment. The latest version of the Penn Chinese Tree-
bank 6.0 (CTB6), excluding the portion of ACE
broadcast news, is used for experiments on Chi-
nese data.4 We follow the recommended splits (in
the list-of-file of CTB6) to divide the data into test
set, development set and training set. The training
set includes 756 files with a total of 15,663 sen-
tences. The test set includes 84 files with 1,708
4Sentences labelled as fragment are not included in our
development and test set.
300
sentences. The development set includes 50 files
with 1,116 sentences. Table 2 shows some of the
characteristics of the English and Chinese data ob-
tained from the development sets.
Development Set English Chinese
num of sent 1,700 1,116
max length of sent (#words) 110 145
ave length of sent (#words) 23 31
num of local fstr 23,289 15,847
num of local fstr per sent 13.70 14.20
max length of local fstr (#gfs) 12 16
ave length of local fstr (#gfs) 2.56 2.90
Table 2: Comparison English and Chinese data
The n-gram models are created using the
SRILM toolkit (Stolcke, 2002) with Good-Turning
smoothing for both the Chinese and English data.
For morphological realisation of English, a set of
lexical macros is automatically extracted from the
training data. This is not required for Chinese sur-
face realisation as Chinese has very little morphol-
ogy. Lexical macro examples are listed in Table 3.
lexical macro surface word
pred=law, num=sg, pers=3 law
pred=average, num=pl, pers=3 averages
pred=believe, num=pl, tense=pres believe
Table 3: Examples of lexical macros
The input to our generator are unordered f-
structures automatically derived from the develop-
ment and test set trees of our treebanks, which do
not contain any string position information. But,
due to the particulars of the automatic f-structure
annotation algorithm, the order of sub-f-structures
in set-valued GFs, such as ADJ, COORD, happens
to correspond to their surface order. To avoid un-
fairly inflating evaluation results, we lexically re-
order the GFs in each sub-f-structure of the devel-
opment and test input before the generation pro-
cess. This resembles the ?permute, no dir? type
experiment in (Langkilde, 2002).
5.2 Experimental Results
Following (Langkilde, 2002) and other work
on general-purpose generators, BLEU score (Pa-
pineni et al, 2002), average NIST simple
string accuracy (SSA) and percentage of exactly
matched sentences are adopted as evaluation met-
rics. As our system guarantees that all input f-
structures can generate a complete sentence, spe-
cial coverage-dependent evaluation (as has been
adopted in most grammar-based generation sys-
tems) is not necessary in our experiments.
Experiments are carried out on an Intel Pentium
4 server, with a 3.80GHz CPU and 3GB mem-
ory. It takes less than 2 minutes to generate all
2,416 sentences (with average sentence length of
21 words) of WSJ section 23 (average 0.05 sec per
sentence), and approximately 4 minutes to gener-
ate 1,708 sentences (with average sentence length
of 30 words) of CTB test data (average 0.14 sec
per sentence), using 4-gram models in all experi-
ments. Our evaluation results for English and Chi-
nese data are shown in Tables 4 and 5, respectively.
Different n-gram models perform nearly consis-
tently in all the experiments on both English and
Chinese data. The results show that factored n-
gram models outperform the basic n-gram models,
and in turn the combined n-gram models outper-
form single n-gram models. The combined model
interpolating n-grams over lexicalised GFs with n-
grams conditioned on PRED achieves the best re-
sults in both experiments on English (with feature
names) and Chinese (with feature names & val-
ues), with BLEU scores of 0.7440 and 0.7123 re-
spectively, and full coverage.
Lexicalisation plays an important role in both
English and Chinese, boosting the BLEU score
without features from 0.5074 to 0.6741 for En-
glish, and from 0.5752 to 0.6639 for Chinese.
Atomic-valued features play an important role
in English, and boost the BLEU score from 0.5074
in the baseline model to 0.6842 when feature
names are integrated into the n-gram models.
However, feature names in Chinese only increase
the BLEU score from 0.5752 to 0.6160. This
is likely to be the case as English has a richer
morphology than Chinese, and important func-
tion words such as ?if?, ?to?, ?that? are encoded
in atomic-valued features in English f-structures,
which helps to determine string order. However,
combined feature names and values work better on
Chinese data, but turn out to hurt the n-gram model
performance for English data. This may suggest
that the feature names in English already include
enough information, while the value of morpho-
logical features, such as TENSE, NUM does not pro-
vide any new information to help determine word
order, but aggravate data sparseness instead.
301
WSJ Sec23 Without Features Feature Names Feature Names & Values
Model ExMatch BLEU SSA ExMatch BLEU SSA ExMatch BLEU SSA
baseline 5.30% 0.5074 57.29% 15.27% 0.6842 69.48% 15.15% 0.6829 69.15%
gf 6.62% 0.5318 60.06% 16.76% 0.6969 71.51% 16.68% 0.6977 71.55%
pred 8.03% 0.5697 60.73% 16.72% 0.7035 70.12% 16.76% 0.7042 71.08%
lex 12.87% 0.6741 69.43% 19.41% 0.7384 74.76% 18.96% 0.7375 74.12%
lex+gf 12.62% 0.6611 69.41% 19.70% 0.7388 74.98% 19.74% 0.7405 75.08%
lex+pred 12.25% 0.6569 68.04% 19.83% 0.7440 75.34% 19.58% 0.7422 75.04%
Table 4: Results for English Penn-II WSJ section 23
Test Without Features Feature Names Feature Names & Values
Model ExMatch BLEU SSA ExMatch BLEU SSA ExMatch BLEU SSA
baseline 8.96% 0.5752 51.92% 11.77% 0.6160 54.64% 12.30% 0.6239 55.20%
gf 9.54% 0.6009 53.02% 12.53% 0.6391 55.78% 13.47% 0.6486 56.60%
pred 10.07% 0.6180 53.80% 13.35% 0.6608 56.72% 14.46% 0.6720 57.67%
lex 13.93% 0.6639 59.61% 15.16% 0.6770 60.44% 15.98% 0.6804 60.20%
lex+gf 14.81% 0.6773 59.92% 15.52% 0.6911 60.97% 16.80% 0.6957 61.07%
lex+pred 16.04% 0.6952 60.82% 16.22% 0.7060 61.45% 17.51% 0.7123 61.54%
Table 5: Results for Chinese CTB6 test data
WSJ Sec23 Sentence length ? 20 words All sentences
Coverage ExMatch BLEU SSA Coverage ExMatch BLEU SSA
Langkilde(2002) 82.7% 28.2% 0.757 69.6%
Callaway(2003) 98.7% 49.0% 88.84%
Nakanishi(2005) 90.75% 0.7733 83.6% 0.705
Cahill(2006) 98.65% 0.7077 73.73% 98.05% 0.6651 68.08%
Hogan(2007) 100% 0.7139 99.96% 0.6882 70.92%
White(2007) 94.3% 6.9% 0.5768
this paper 100% 35.40% 0.7625 81.09% 100% 19.83% 0.7440 75.34%
Table 6: Cross system comparison of results for English WSJ section 23
6 Discussion
6.1 Comparison to Previous Work
It is very difficult to compare sentence generators
since the information contained in the input rep-
resentation varies greatly between systems. The
most direct comparison is between our system and
those presented in Cahill and van Genabith (2006)
and Hogan et al (2007), as they also use treebank-
based automatically generated f-structures as the
generator inputs. The labelled feature-value struc-
tures used in HALogen (Langkilde, 2002) and
functional descriptions in FUF/SURGE (Callaway,
2003) also bear some broad similarities to our f-
structures. A number of systems using different
input but adopting the same evaluation metrics and
testing on the same data are listed in Table 6.
Surprisingly (or not), the best results are
achieved by a purely symbolic generation
system?FUF/SURGE (Callaway, 2003). How-
ever the approach uses handcrafted grammars
which are very time-consuming to produce and
adapt to different languages and domains. Langk-
ilde (2002) reports results for experiments with
varying levels of linguistic detail in the input
given to the generator. The type ?permute, no dir?
is most comparable to the level of information
contained in our f-structure in that the modifiers
(adjuncts, coordinates etc.) in the input are not
ordered. However her labelled feature-value
structure is more specific than our f-structure
as it also includes syntactic properties such as
part-of-speech, which might contribute to the
higher BLEU score of HALogen. And moreover,
in HALogen nearly 20% of the sentences are only
partially generated (or not at all). Nakanishi et
al. (2005) carry out experiments on sentences up
to 20 words, with BLEU scores slightly higher
than ours. However their results without sentence
length limitation (listed in the right column), for
500 sentences randomly selected from WSJ Sec22
are lower than ours, even at a lower coverage.
Overall our system is competitive, with best results
for coverage (100%), second best for BLEU and
SSA scores, and third best overall on exact match.
However, we admit that automatic metrics such as
BLEU are not fully reliable to compare different
systems, and results vary widely depending on the
coverage of the systems and the specificity of the
generation input.
302
6.2 Error Analysis and Differences Between
the Languages
Though our dependency-based n-gram models per-
form well in both the English and Chinese exper-
iments, we are surprised that experiments on En-
glish data produce better results than those for Chi-
nese. It is widely accepted that English generation
is more difficult than Chinese, due to morpholog-
ical inflections and the somewhat less predictable
word order of English compared to Chinese. This
is reflected by the results of the baseline models.
Chinese has a BLEU score of 0.5752 and 8.96%
exact match, both are higher than those of English.
However with feature augmentation and lexicali-
sation, the results for English data exceed Chinese.
This is probably because of the following reasons:
Data size of the English training set is more than
twice that of Chinese.
Grammatical functions are more fine-grained
in English f-structures than those in Chinese.
There are 32 GFs defined for English compared to
20 for Chinese in our input f-structures.
Properties of the languages and data sets are
different. For example, due to lack of inflection
and case markers, many sequences of VPs in Chi-
nese have to be treated as coordinates, whereas
their counterparts in English act as different gram-
matical functions, e.g. (2).
(2) ?? z ,?????
invest million build this construction
?invest million yuan to build the construction?
This results in a total of 7,377 coordinates (4.32
per sentence) in the Chinese development data,
compared to 2,699 (1.12 per sentence) in the En-
glish data. The most extreme case in the Chinese
data features 14 coordinates of country names in
a local f-structure. This may account for the low
SSA score for the Chinese experiments, as many
coordinates are tied in the n-gram scoring method
and can not be ordered correctly. Examining the
development data shows different types of coordi-
nation errors:
? syntactic coordinates, but not semantic coor-
dinates, as in sentence (2).
? syntactic and semantic coordinates, but usu-
ally expressed in a fixed order, e.g. (3).
(3) U? m?
reform opening-up
?reform and opening up?
? syntactic and semantic coordinates, which
can freely swap positions, e.g. (4).
(4) ? ?? ? ?$g?
plentiful energy and quick thinking
?energetic and agile?
At the current stage, our n-gram generation
model only keeps the most likely realisation for
each local f-structure. We believe that packing all
equivalent elements, like coordinates in a local f-
structure into equivalent classes, and outputing n-
best candidate realisations will greatly increase the
SSA score and may also further benefit the effi-
ciency of the algorithm.
7 Conclusions and Further Work
We have described a number of increasingly so-
phisticated n-gram models for sentence genera-
tion from labelled bilexical dependencies, in the
form of LFG f-structures. The models include
additional conditioning on parent GFs and differ-
ent degrees of lexicalisation. Our method is sim-
ple, highly efficient, broad coverage and accurate
in practice. We present experiments on English
and Chinese, showing that the method generalises
well to different languages and data sets. We are
currently exploring further combinations of con-
ditioning context and lexicalisation, application to
different languages and to dependency represen-
tations used to train state-of-the-art dependency
parsers (Nivre, 2006).
Acknowledgments
This research is funded by Science Foundation Ire-
land grant 04/IN/I527. We thank Aoife Cahill for
providing the treebank-based LFG resources for
the English data. We gratefully acknowledge the
feedback provided by our anonymous reviewers.
References
Bangalore, Srinivas and Rambow, Owen. 2000. Ex-
ploiting a Probabilistic Hierarchical Model for Gen-
eration. Proceedings of the 18th International
Conference on Computational Linguistics, 42?48.
Saarbru?cken, Germany.
Belz, Anja. 2007. Probabilistic Generation of Weather
Forecast Texts. Proceedings of the Conference of the
North American Chapter of the Association for Com-
putational Linguistics, 164?171. New York.
Cahill, Aoife, Burke, Michael, O?Donovan, Ruth, van
Genabith, Josef and Way, Andy. 2004. Long-
Distance Dependency Resolution in Automatically
303
Acquired Wide-Coverage PCFG-Based LFG Ap-
proximations. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics, 320-327. Barcelona, Spain.
Cahill, Aoife and van Genabith, Josef. 2006. Ro-
bust PCFG-Based Generation Using Automatically
Acquired LFG Approximations. Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, 1033?1040.
Sydney, Australia.
Cahill, Aoife, Forst, Martin and Rohrer, Christian.
2007. Stochastic Realisation Ranking for a Free
Word Order Language. Proceedings of the 11th Eu-
ropean Workshop on Natural Language Generation,
17?24. Schloss Dagstuhl, Germany.
Callaway, Charles B.. 2003. Evaluating Coverage for
Large Symbolic NLG Grammars. Proceedings of the
Eighteenth International Joint Conference on Artifi-
cial Intelligence, 811?817. Acapulco, Mexico.
Carroll, John, Copestake, Ann, Flickinger, Dan and
Poznanski, Victor. 1999. An efficient chart gen-
erator for (semi-)lexicalist grammars. Proceedings
of the 7th European Workshop on Natural Language
Generation, 86?95. Toulouse, France.
Crouch, Dick, Dalrymple, Mary, Kaplan, Ron, King,
Tracy, Maxwell, John and Newman, Paula. 2007.
XLE Documentation. Palo Alto Research Center,
CA.
Elhadad, Michael. 1991. FUF: The universal unifier
user manual version 5.0. Technical Report CUCS-
038-91. Dept. of Computer Science, Columbia Uni-
versity.
Guo, Yuqing and van Genabith, Josef and Wang,
Haifeng. 2007. Treebank-based Acquisition of LFG
Resources for Chinese. Proceedings of LFG07 Con-
ference, 214?232. Stanford, CA, USA.
Hogan, Deirdre Cafferkey, Conor Cahill, Aoife and van
Genabith, Josef. 2007. Exploiting Multi-Word Units
in History-Based Probabilistic Generation. Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
CoNLL, 267?276. Prague, Czech Republic.
Kaplan, Ronald and Bresnan, Joan. 1982. Lexical
Functional Grammar: a Formal System for Gram-
matical Representation. The Mental Representation
of Grammatical Relations, 173?282. MIT Press,
Cambridge.
Kaplan, Ronald and Wedekind, Jurgen. 2000. LFG
Generation Produces Context-free Languages. Pro-
ceedings of the 18th International Conference on
Computational Linguistics, 425?431. Saarbru?cken,
Germany.
Langkilde, Irene. 2000. Forest-Based Statistical Sen-
tence Generation. Proceedings of 1st Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, 170?177. Seattle, WA.
Langkilde, Irene. 2002. An Empirical Verification
of Coverage and Correctness for a General-Purpose
Sentence Generator. Proceedings of the Second In-
ternational Conference on Natural Language Gener-
ation, 17?24. New York, USA.
Marcus, Mitchell P., Santorini, Beatrice and
Marcinkiewicz, Mary Ann. 1993. Building a
large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2).
Nakanishi, Hiroko and Nakanishi, Yusuke and Tsu-
jii, Jun?ichi. 2005. Probabilistic Models for Dis-
ambiguation of an HPSG-Based Chart Generator.
Proceedings of the 9th International Workshop on
Parsing Technology, 93?102. Vancouver, British
Columbia.
Nivre, Joakim. 2006. Inductive Dependency Parsing.
Springer.
Papineni, Kishore, Roukos, Salim, Ward, Todd and
Zhu, Wei-Jing. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, 311-318. Philadelphia, USA.
Ratnaparkhi, Adwait. 2000. Trainable methods for nat-
ural language generation. Proceedings of NAACL
2000, 194?201. Seattle, WA.
Stolcke, Andreas. 2002. SRILM-An Extensible Lan-
guage Modeling Toolkit. Proceedings of Interna-
tional Conference of Spoken Language Processing.
Denver, Colorado.
van Genabith, Josef and Crouch, Dick. 1996. Di-
rect and underspecified interpretations of LFG f-
structures. Proceedings of the 16th conference on
Computational linguistics, 262?267. Copenhagen,
Denmark
Velldal, Erik and Oepen, Stephan. 2005. Maximum
entropy models for realization ranking. Proceedings
of the MTSummit ?05.
White, Michael. 2004. Reining in CCG Chart Realiza-
tion. Proceedings of the third International Natural
Language Generation Conference. Hampshire, UK.
White, Michael, Rajkumar, Rajakrishnan and Martin,
Scott. 2007. Towards Broad Coverage Surface Re-
alization with CCG. Proceedings of the MT Summit
XI Workshop, 22?30. Copenhagen, Danmark.
Xue, Nianwen, Xia, Fei, Chiou, Fu dong and Palmer,
Martha. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural
Language Engineering, 11(2): 207?238.
304
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 112?121, Prague, June 2007. c?2007 Association for Computational Linguistics
A Comparative Evaluation of Deep and Shallow Approaches to the
Automatic Detection of Common Grammatical Errors
Joachim Wagner, Jennifer Foster, and Josef van Genabith?
National Centre for Language Technology
School of Computing, Dublin City University, Dublin 9, Ireland
{jwagner, jfoster, josef}@computing.dcu.ie
Abstract
This paper compares a deep and a shallow
processing approach to the problem of clas-
sifying a sentence as grammatically well-
formed or ill-formed. The deep processing
approach uses the XLE LFG parser and En-
glish grammar: two versions are presented,
one which uses the XLE directly to perform
the classification, and another one which
uses a decision tree trained on features con-
sisting of the XLE?s output statistics. The
shallow processing approach predicts gram-
maticality based on n-gram frequency statis-
tics: we present two versions, one which
uses frequency thresholds and one which
uses a decision tree trained on the frequen-
cies of the rarest n-grams in the input sen-
tence. We find that the use of a decision tree
improves on the basic approach only for the
deep parser-based approach. We also show
that combining both the shallow and deep
decision tree features is effective. Our eval-
uation is carried out using a large test set of
grammatical and ungrammatical sentences.
The ungrammatical test set is generated au-
tomatically by inserting grammatical errors
into well-formed BNC sentences.
1 Introduction
This paper is concerned with the task of predict-
ing whether a sentence contains a grammatical er-
ror. An accurate method for carrying out automatic
?Also affiliated to IBM CAS, Dublin.
grammaticality judgements has uses in the areas of
computer-assisted language learning and grammar
checking. Comparative evaluation of existing error
detection approaches has been hampered by a lack
of large and commonly used evaluation error cor-
pora. We attempt to overcome this by automatically
creating a large error corpus, containing four dif-
ferent types of frequently occurring grammatical er-
rors. We use this corpus to evaluate the performance
of two approaches to the task of automatic error de-
tection. One approach uses low-level detection tech-
niques based on POS n-grams. The other approach
is a novel parser-based method which employs deep
linguistic processing to discriminate grammatical in-
put from ungrammatical. For both approaches, we
implement a basic solution, and then attempt to im-
prove upon this solution using a decision tree clas-
sifier. We show that combining both methods im-
proves upon the individual methods.
N-gram-based approaches to the problem of error
detection have been proposed and implemented in
various forms by Atwell(1987), Bigert and Knutsson
(2002), and Chodorow and Leacock (2000) amongst
others. Existing approaches are hard to compare
since they are evaluated on different test sets which
vary in size and error density. Furthermore, most of
these approaches concentrate on one type of gram-
matical error only, namely, context-sensitive or real-
word spelling errors. We implement a vanilla n-
gram-based approach which is tested on a very large
test set containing four different types of error.
The idea behind the parser-based approach to er-
ror detection is to use a broad-coverage hand-crafted
precision grammar to detect ungrammatical sen-
112
tences. This approach exploits the fact that a pre-
cision grammar is designed, in the traditional gen-
erative grammar sense (Chomsky, 1957), to dis-
tinguish grammatical sentences from ungrammati-
cal sentences. This is in contrast to treebank-based
grammars which tend to massively overgenerate and
do not generally aim to discriminate between the
two. In order for our approach to work, the coverage
of the precision grammars must be broad enough to
parse a large corpus of grammatical sentences, and
for this reason, we choose the XLE (Maxwell and
Kaplan, 1996), an efficient and robust parsing sys-
tem for Lexical Functional Grammar (LFG) (Kaplan
and Bresnan, 1982) and the ParGram English gram-
mar (Butt et al, 2002) for our experiments. This sys-
tem employs robustness techniques, some borrowed
from Optimality Theory (OT) (Prince and Smolen-
sky, 1993), to parse extra-grammatical input (Frank
et al, 1998), but crucially still distinguishes between
optimal and suboptimal solutions.
The evaluation corpus is a subset of an un-
grammatical version of the British National Cor-
pus (BNC), a 100 million word balanced corpus of
British English (Burnard, 2000). This corpus is ob-
tained by automatically inserting grammatical errors
into the original BNC sentences based on an analysis
of a manually compiled ?real? error corpus.
This paper makes the following contributions to
the task of automatic error detection:
1. A novel deep processing XLE-based approach
2. An effective and novel application of decision
tree machine learning to both shallow and deep
approaches
3. A novel combination of deep and shallow pro-
cessing
4. An evaluation of an n-gram-based approach on
a wider variety of errors than has previously
been carried out
5. A large evaluation error corpus
The paper is organised as follows: in Section 2,
we describe previous approaches to the problem of
error detection; in Section 3, a description of the
error corpus used in our evaluation experiments is
presented, and in Section 4, the two approaches to
error detection are presented, evaluated, combined
and compared. Section 5 provides a summary and
suggestions for future work.
2 Background
2.1 Precision Grammars
A precision grammar is a formal grammar designed
to distinguish ungrammatical from grammatical sen-
tences. This is in contrast to large treebank-induced
grammars which often accept ungrammatical input
(Charniak, 1996). While high coverage is required,
it is difficult to increase coverage without also in-
creasing the amount of ungrammatical sentences
that are accepted as grammatical by the grammar.
Most publications in grammar-based automatic error
detection focus on locating and categorising errors
and giving feedback. Existing grammars are re-used
(Vandeventer Faltin, 2003), or grammars of limited
size are developed from scratch (Reuer, 2003).
The ParGram English LFG is a hand-crafted
broad-coverage grammar developed over several
years with the XLE platform (Butt et al, 2002). The
XLE parser uses OT to resolve ambiguities (Prince
and Smolensky, 1993). Grammar constraints re-
sulting in rare constructions can be marked as ?dis-
preferred? and constraints resulting in common un-
grammatical constructions can be marked as ?un-
grammatical?. The use of constraint ordering and
marking increases the robustness of the grammar,
while maintaining the grammatical / ungrammati-
cal distinction (Frank et al, 1998). The English
Resource Grammar (ERG) is a precision Head-
Driven Phrase Structure Grammar (HPSG) of En-
glish (Copestake and Flickinger, 2000; Pollard and
Sag, 1994). Its coverage is not as broad as the XLE
English grammar. Baldwin et al (2004) propose a
method to identify gaps in the grammar. Blunsom
and Baldwin (2006) report ongoing development.
There has been previous work using the ERG and
the XLE grammars in the area of computer-assisted
language learning. Bender et al (2004) use a ver-
sion of the ERG containing mal-rules to parse ill-
formed sentences from the SST corpus of Japanese
learner English (Emi et al, 2004). They then use
the semantic representations of the ill-formed input
to generate well-formed corrections. Khader et al
(2004) study whether the ParGram English LFG can
be used for computer-assisted language learning by
113
adding additional OT marks for ungrammatical con-
structions observed in a learner corpus. However,
the evaluation is preliminary, on only 50 test items.
2.2 N-gram Methods
Most shallow approaches to grammar error detection
originate from the area of real-word spelling error
correction. A real-word spelling error is a spelling
or typing error which results in a token which is an-
other valid word of the language in question.
The (to our knowledge) oldest work in this area
is that of Atwell (1987) who uses a POS tagger to
flag POS bigrams that are unlikely according to a
reference corpus. While he speculates that the bi-
gram frequency should be compared to how often
the same POS bigram is involved in errors in an error
corpus, the proposed system uses the raw frequency
with an empirically established threshold to decide
whether a bigram indicates an error. In the same
paper, a completely different approach is presented
that uses the same POS tagger to consider spelling
variants that have a different POS. In the example
sentence I am very hit the POS of the spelling vari-
ant hot/JJ is added to the list NN-VB-VBD-VBN of
possible POS tags of hit. If the POS tagger chooses
hit/JJ, the word is flagged and the correction hot is
proposed to the user. Unlike most n-gram-based ap-
proaches, Atwell?s work aims to detect grammar er-
rors in general and not just real-word spelling errors.
However, a complete evaluation is missing.
The idea of disambiguating between the elements
of confusion sets is related to word sense disam-
biguation. Golding (1995) builds a classifier based
on a rich set of context features. Mays et al (1991)
apply the noisy channel model to the disambiguation
problem. For each candidate correction S? of the
input S the probability P (S?)P (S|S?) is calculated
and the most likely correction selected. This method
is re-evaluated by Wilcox-O?Hearn et al (2006) on
WSJ data with artificial real-word spelling errors.
Bigert and Knutsson (2002) extend upon a basic
n-gram approach by attempting to match n-grams of
low frequency with similar n-grams in order to re-
duce overflagging. Furthermore, n-grams crossing
clause boundaries are not flagged and the similarity
measure is adapted in the case of phrase boundaries
that usually result in low frequency n-grams.
Chodorow and Leacock (2000) use a mutual in-
formation measure in addition to raw frequency of n-
grams. Apart from this, their ALEK system employs
other extensions to the basic approach, for exam-
ple frequency counts from both generic and word-
specific corpora are used in the measures. It is not
reported how much each of these contribute to the
overall performance.
Rather than trying to implement all of the pre-
vious n-gram approaches, we implement the basic
approach which uses rare n-grams to predict gram-
maticality. This property is shared by all previous
shallow approaches. We also test our approach on a
wider class of grammatical errors.
3 Ungrammatical Data
In this section, we discuss the notion of an artifi-
cial error corpus (Section 3.1), define the type of
ungrammatical language we are dealing with (Sec-
tion 3.2), and describe our procedure for creating a
large artificial error corpus derived from the BNC
(Section 3.3).
3.1 An Artificial Error Corpus
In order to meaningfully evaluate a shallow ver-
sus deep approach to automatic error detection, a
large test set of ungrammatical sentences is needed.
A corpus of ungrammatical sentences can take the
form of a learner corpus (Granger, 1993; Emi et al,
2004), i. e. a corpus of sentences produced by lan-
guage learners, or it can take the form of a more gen-
eral error corpus comprising sentences which are not
necessarily produced in a language-learning context
and which contain competence and performance er-
rors produced by native and non-native speakers of
the language (Becker et al, 1999; Foster and Vogel,
2004; Foster, 2005). For both types of error corpus,
it is not enough to collect a large set of sentences
which are likely to contain an error - it is also neces-
sary to examine each sentence in order to determine
whether an error has actually occurred, and, if it has,
to note the nature of the error. Thus, like the cre-
ation of a treebank, the creation of a corpus of un-
grammatical sentences requires time and linguistic
knowledge, and is by no means a trivial task.
A corpus of ungrammatical sentences which is
large enough to be useful can be created auto-
matically by inserting, deleting or replacing words
114
in grammatical sentences. These transformations
should be linguistically realistic and should, there-
fore, be based on an analysis of naturally produced
grammatical errors. Automatically generated error
corpora have been used before in natural language
processing. Bigert (2004) and Wilcox-O?Hearn et
al. (2006), for example, automatically introduce
spelling errors into texts. Here, we generate a large
error corpus by automatically inserting four different
kinds of grammatical errors into BNC sentences.
3.2 Commonly Produced Grammatical Errors
Following Foster (2005), we define a sentence to be
ungrammatical if all the words in the sentence are
well-formed words of the language in question, but
the sentence contains one or more error. This er-
ror can take the form of a performance slip which
can occur due to carelessness or tiredness, or a com-
petence error which occurs due to a lack of knowl-
edge of a particular construction. This definition in-
cludes real-word spelling errors and excludes non-
word spelling errors. It also excludes the abbrevi-
ated informal language used in electronic communi-
cation. Using the above definition as a guideline, a
20,000 word corpus of ungrammatical English sen-
tences was collected from a variety of written texts
including newspapers, academic papers, emails and
website forums (Foster and Vogel, 2004; Foster,
2005). The errors in the corpus were carefully anal-
ysed and classified in terms of how they might be
corrected using the three word-level correction op-
erators: insert, delete and substitute. The following
frequency ordering of the three word-level correc-
tion operators was found:
substitute (48%) > insert (24%) > delete (17%) >
combination (11%)
Stemberger (1982) reports the same ordering of the
substitution, deletion and insertion correction oper-
ators in a study of native speaker spoken language
slips. Among the grammatical errors which can be
corrected by substituting one word for another, the
most common errors are real-word spelling errors
and agreement errors. In fact, 72% of all errors fall
into one of the following four classes:
1. missing word errors:
What are the subjects? > What the subjects?
2. extra word errors:
Was that in the summer? > Was that in the sum-
mer in?
3. real-word spelling errors:
She could not comprehend. > She could no
comprehend.
4. agreement errors:
She steered Melissa round a corner. > She
steered Melissa round a corners.
A similar classification was adopted by Nicholls
(1999), having analysed the errors in a learner cor-
pus. Our research is currently limited to the four er-
ror types given above, i. e. missing word errors, ex-
tra word errors, real-word spelling errors and agree-
ments errors. However, it is possible for it to be ex-
tended to handle a wider class of errors.
3.3 Automatic Error Creation
The error creation procedure takes as input a part-
of-speech-tagged corpus of sentences which are as-
sumed to be well-formed, and outputs a corpus of
ungrammatical sentences. The automatically intro-
duced errors take the form of the four most com-
mon error types found in the manually created cor-
pus, i. e. missing word errors, extra word errors, real-
word spelling errors and agreement errors. For each
sentence in the original tagged corpus, an attempt is
made to automatically produce four ungrammatical
sentences, one for each of the four error types. Thus,
the output of the error creation procedure is, in fact,
four error corpora.
3.3.1 Missing Word Errors
In the manually created error corpus of Foster
(2005), missing word errors are classified based on
the part-of-speech (POS) of the missing word. 98%
of the missing parts-of-speech come from the fol-
lowing list (the frequency distribution in the error
corpus is given in brackets):
det (28%) > verb (23%) > prep (21%) > pro (10%)
> noun (7%) > ?to? (7%) > conj (2%)
We use this information when introducing missing
word errors into the BNC sentences. For each sen-
tence, all words with the above POS tags are noted.
One of these is selected and deleted. The above
frequency ordering is respected so that, for exam-
ple, missing determiner errors are produced more of-
ten than missing pronoun errors. No ungrammatical
115
sentence is produced if the original sentence con-
tains just one word or if the sentence contains no
words with parts-of-speech in the above list.
3.3.2 Extra Word Errors
We introduce extra word errors in the following
three ways:
1. Random duplication of any token within a sen-
tence: That?s the way we we learn here.
2. Random duplication of any POS within a sen-
tence: There it he was.
3. Random insertion of an arbitrary token into the
sentence: Joanna drew as a long breadth.
Apart from the case of duplicate tokens, the extra
words are selected from a list of tagged words com-
piled from a random subset of the BNC. Again, our
procedure for inserting extra words is based on the
analysis of extra word errors in the 20,000 word er-
ror corpus of Foster (2005).
3.3.3 Real-Word Spelling Errors
We classify an error as a real-word spelling er-
ror if it can be corrected by replacing the erroneous
word with another word with a Levenshtein distance
of one from the erroneous word, e.g. the and they.
Based on the analysis of the manually created er-
ror corpus (Foster, 2005), we compile a list of com-
mon English real-word spelling error word pairs.
For each BNC sentence, the error creation proce-
dure records all tokens in the sentence which appear
as one half of one of these word pairs. One token
is selected at random and replaced by the other half
of the pair. The list of common real-word spelling
error pairs contains such frequently occurring words
as is and a, and the procedure therefore produces an
ill-formed sentence for most input sentences.
3.3.4 Agreement Errors
We introduce subject-verb and determiner-noun
number agreement errors into the BNC sentences.
We consider both types of agreement error equally
likely and introduce the error by replacing a singular
determiner, noun or verb with its plural counterpart,
or vice versa. For English, subject-verb agreement
errors can only be introduced for present tense verbs,
and determiner-noun agreement errors can only be
introduced for determiners which are marked for
number, e.g. demonstratives and the indefinite ar-
ticle. The procedure would be more productive if
applied to a morphologically richer language.
3.3.5 Covert Errors
James (1998) uses the term covert error to de-
scribe a genuine language error which results in a
sentence which is syntactically well-formed under
some interpretation different from the intended one.
The prominence of covert errors in our automati-
cally created error corpus is estimated by manually
inspecting 100 sentences of each error type. The per-
centage of grammatical structures that are inadver-
tently produced for each error type and an example
of each one are shown below:
? Agreement Errors, 7%
Mary?s staff include Jones,Smith and Murphy
> Mary?s staff includes Jones,Smith and Mur-
phy
? Real-Word Spelling Errors, 10%
And then? > And them?
? Extra Word Errors, 5%
in defiance of the free rider prediction > in de-
fiance of the free rider near prediction
? Missing Word Errors, 13%
She steered Melissa round a corner > She
steered round a corner
The occurrence of these covert errors can be re-
duced by fine-tuning the error creation procedure but
they can never be completely eliminated. Indeed,
they should not be eliminated from the test data,
because, ideally, an optimal error detection system
should be sophisticated enough to flag syntactically
well-formed sentences containing covert errors as
potentially ill-formed.1
4 Error Detection Evaluation
In this section we present the error detection eval-
uation experiments. The experimental setup is ex-
plained in Section 4.1, the results are presented in
Section 4.2 and they are analysed in Section 4.3.
1An example of this is given in the XLE User Documen-
tation (http://www2.parc.com/isl/groups/nltt/
xle/doc/). The authors remark that an ungrammatical read-
ing of the sentence Lets go to the store in which Lets is missing
an apostrophe, is preferable to the grammatical yet implausible
analysis in which Lets is a plural noun.
116
4.1 Experimental Setup
4.1.1 Test Data and Evaluation Procedure
The following steps are carried out to produce
training and test data for this experiment:
1. Speech material, poems, captions and list items
are removed from the BNC. 4.2 million sen-
tences remain. The order of sentences is ran-
domised.
2. For the purpose of cross-validation, the corpus
is split into 10 parts.
3. Each part is passed to the 4 automatic error in-
sertion modules described in Section 3.3, re-
sulting in 40 additional sets of varying size.
4. The first 60,000 sentences of each of the 50
sets, i. e. 3 million sentences, are parsed with
XLE.2
5. N-gram frequency information is extracted for
the first 60,000 sentences of each set. An addi-
tional 20,000 is extracted as held-out data.
6. 10 sets with mixed error types are produced by
joining a quarter of each respective error set.
7. For each error type (including mixed errors)
and cross-validation set, the 60,000 grammat-
ical and 60,000 ungrammatical sentences are
joined.
8. Each cross-validation run uses one set out of
the 10 as test data (120,000 sentences) and the
remaining 9 sets for training (1,080,000 sen-
tences).
The experiment is a standard binary classification
task. The methods classify the sentences of the test
sets as grammatical or ungrammatical. We use the
standard measures of precision, recall, f-score and
accuracy (Figure 1). True positives are understood
to be ungrammatical sentences that are identified as
such. The baseline precision and accuracy is 50%
as half of the test data is ungrammatical. If 100%
of the test data is classified as ungrammatical, re-
call will be 100% and f-score 2/3. Recall shows
the accuracy we would get if the grammatical half
of the test data was removed. Parametrised methods
2We use the XLE command parse-testfile with parse-
literally set to 1, max xle scratch storage set to 1,000 MB, time-
out to 60 seconds, and the XLE English LFG. Skimming is not
switched on and fragments are.
Measure Formula
precision tp/(tp + fp)
recall tp/(tp + fn)
f-score 2pr ? re/(pr + re)
accuracy (tp + tn)/(tp + tn + fp + fn)
Figure 1: Evaluation measures: tp = true positives,
fp = false positives, tn = true negatives, fn = false
negatives, pr = precision, re = recall
are first optimised for accuracy and then the other
measures are taken. Therefore, f-scores below the
artificial 2/3 baseline are meaningful.
4.1.2 Method 1: Precision Grammar
According to the XLE documentation, a sentence
is marked with a star (*) if its optimal solution uses
a constraint marked as ungrammatical. We use this
star feature, parser exceptions and zero number of
parses to classify a sentence as ungrammatical.
4.1.3 Method 2: POS N-grams
In each cross-validation run, the full data of the
remaining 9 sets of step 2 of the data generation
(see Section 4.1.1) is used as a reference corpus of
0.9?4, 200, 000 = 3, 800, 000 assumedly grammat-
ical sentences. The reference corpora and data sets
are POS tagged with the IMS TreeTagger (Schmidt,
1994). Frequencies of POS n-grams (n = 2, . . . , 7)
are counted in the reference corpora. A test sentence
is flagged as ungrammatical if it contains an n-gram
below a fixed frequency threshold. Method 2 has
two parameters: n and the frequency threshold.
4.1.4 Method 3: Decision Trees on XLE Output
The XLE parser outputs additional statistics for
each sentence that we encode in six features:
? An integer indicating starredness (0 or 1) and
various parser exceptions (-1 for time out, -2
for exceeded memory, etc.)
? The number of optimal parses3
? The number of unoptimal parses
? The duration of parsing
? The number of subtrees
? The number of words
3The use of preferred versus dispreferred constraints are
used to distinguish optimal parses from unoptimal ones.
117
Training data for the decision tree learner is com-
posed of 9?60, 000 = 540, 000 feature vectors from
grammatical sentences and 9 ? 15, 000 = 135, 000
feature vectors from ungrammatical sentences of
each error type, resulting in equal amounts of gram-
matical and ungrammatical training data.
We choose the weka implementation of machine
learning algorithms for the experiments (Witten and
Frank, 2000). We use a J48 decision tree learner
with the default model.
4.1.5 Method 4: Decision Trees on N-grams
Method 4 follows the setup of Method 3. How-
ever, the features are the frequencies of the rarest
n-grams (n = 2, . . . , 7) in the sentence. Therefore,
the feature vector of one sentence contains 6 num-
bers.
4.1.6 Method 5: Decision Trees on Combined
Feature Sets
This method combines the features of Methods 3
and 4 for training a decision tree.
4.2 Results
Table 1 shows the results for Method 1, which uses
XLE starredness, parser exceptions4 and zero parses
to classify grammaticality. Table 2 shows the re-
sults for Method 2, the basic n-gram approach. Ta-
ble 3 shows the results for Method 3, which classi-
fies based on a decision tree of XLE features. The
results for Method 4, the n-gram-based decision tree
approach, are shown in Table 4. Finally, Table 5
shows the results for Method 5 which combines n-
gram and XLE features in decision trees.
In the case of Method 2, we first have to find opti-
mal parameters. As only very limited integer values
for n and the threshold are reasonable, an exhaustive
search is feasible. We considered n = 2, . . . , 7 and
frequency thresholds below 20,000. Separate held-
out data (400,000 sentences) is used in order to avoid
overfitting. Best accuracy is achieved with 5-grams
and a threshold of 4. Table 2 reports results with
these parameters.
4XLE parsing (see footnote 2 for configuration) runs out
of time for 0.7 % and out of memory for 2.5 % of sentences,
measured on training data of the first cross-validation run, i. e.
540,000 grammatical sentence and 135,000 of each error type.
14 sentences of 3 million caused the parser to terminate abnor-
mally.
Error type Pr. Re. F-Sc. Acc.
Agreement 66.2 64.6 65.4 65.8
Real-word 63.5 57.3 60.3 62.2
Extra word 64.4 59.7 62.0 63.4
Missing word 59.2 47.8 52.9 57.4
Mixed errors 63.5 57.3 60.3 62.2
Table 1: Classification results with XLE starredness,
parser exceptions and zero parses (Method 1)
Error type Pr. Re. F-Sc. Acc.
Agreement 58.6 51.7 55.0 57.6
Real-word 64.0 64.9 64.5 64.2
Extra word 64.8 67.3 66.0 65.4
Missing word 57.2 48.8 52.7 56.1
Mixed errors 61.5 58.2 59.8 60.8
Table 2: Classification results with 5-gram and fre-
quency threshold 4 (Method 2)
The standard deviation of results across cross-
validation runs is below 0.006 on all measures, ex-
cept for Method 4. Therefore we only report average
percentages. The highest observed standard devia-
tion is 0.0257 for recall of Method 4 on agreement
errors.
For Methods 3, 4 and 5, the decision tree learner
optimises accuracy and, in doing so, chooses a trade-
off between precision and recall.
4.3 Analysis
Both Method 1 (Table 1) and Method 2 (Table 2)
achieve above baseline accuracy for all error types.
However, Method 1, which uses the XLE starred
feature, parser exceptions and zero parses to de-
termine whether or not a sentence is grammatical,
slightly outperforms Method 2, which uses the fre-
Error type Pr. Re. F-Sc. Acc.
Agreement 67.0 79.3 72.6 70.1
Real-word 63.4 67.6 65.4 64.3
Extra word 63.0 66.4 64.7 63.7
Missing word 59.7 57.8 58.7 59.4
Mixed errors 63.4 67.8 65.6 64.4
Table 3: Classification results with decision tree on
XLE output (Method 3)
118
Error type Pr. Re. F-Sc. Acc.
Agreement 61.2 53.8 57.3 59.9
Real-word 65.3 64.3 64.8 65.1
Extra word 66.4 67.4 66.9 66.7
Missing word 59.1 49.2 53.7 57.5
Mixed errors 63.3 58.7 60.9 62.3
Table 4: Classification results with decision tree on
vectors of frequency of rarest n-grams (Method 4)
Error type Pr. Re. F-Sc. Acc.
Agreement 67.1 75.2 70.9 69.2
Real-word 65.8 70.7 68.1 67.0
Extra word 65.9 71.2 68.5 67.2
Missing word 61.2 58.0 59.5 60.6
Mixed errors 65.2 68.8 66.9 66.0
Table 5: Classification results with decision tree on
joined feature set (Method 5)
quency of POS 5-grams to detect an error. The
XLE deep-processing approach is better than the n-
gram-based approach for agreement errors (f-score
+10.4). Examining the various types of agree-
ment errors, we can see that this is especially the
case for singular subjects followed by plural cop-
ula verbs (recall +37.7) and determiner-noun num-
ber mismatches (recall +23.6 for singular nouns and
+18.0 for plural nouns), but not for plural subjects
followed by singular verbs (recall -24.0). The rela-
tively poor performance of Method 2 on agreement
errors involving determiners could be due to the lack
of agreement marking on the Penn Treebank deter-
miner tag used by TreeTagger.
Method 1 is outperformed by Method 2 for real-
word spelling and extra word errors (f-score -4.2,
-4.0). Unsurprisingly, Method 2 has an advantage
on those real-word spelling errors that change the
POS (recall -8.8 for Method 1). Both methods per-
form poorly on missing word errors. For both meth-
ods there are only very small differences in perfor-
mance between the various missing word error sub-
types (identified by the POS of the deleted word).
Method 3, which uses machine learning to exploit
all the information returned by the XLE parser, im-
proves performance from Method 1, the basic XLE
method, for all error types.5 The general improve-
ment comes from an improvement in recall, mean-
ing that more ungrammatical sentences are actu-
ally flagged as such without compromising preci-
sion. The improvement is highest for agreement
errors (f-score +7.2). Singular subject with plural
copula errors (e. g. The man are) peak at a recall of
91.0. The Method 3 results indicate that information
on the number of solutions (optimal and unoptimal),
the number of subtrees, the time taken to parse the
sentence and the number of words can be used to
predict grammaticality. It would be interesting to
investigate this approach with other parsers.
Method 4, which uses a decision tree with n-
gram-based features, confirms the results of Method
2. The decision trees? root nodes are similar or even
identical (depending on cross-validation run) to the
decision rule of Method 2 (5-gram frequency below
4). However, the 10 decision trees have between
1,111 and 1,905 nodes and draw from all features,
even bigrams and 7-grams that perform poorly on
their own. The improvements are very small though
and they are not significant according the criterion of
non-overlapping cross-validation results. The main
reason for the evaluation of Method 4 is to provide
another reference point for comparison of the final
method.
The overall best results are those for Method 5,
the combined XLE, n-gram and machine-learning-
based method, which outperforms the next best
method, Method 3, on all error types apart from
agreement errors (f-score -1.7, +2.7, +3.8, +0.8).
For agreement errors, it seems that the relatively
poor results for n-grams have a negative effect on the
relatively good results for the XLE. Figure 2 shows
that the performance is almost constant on ungram-
matical data in the important sentence length range
from 5 to 40. However, there is a negative correla-
tion of accuracy and sentence length for grammati-
cal sentences. Very long sentences of any kind tend
to be classified as ungrammatical, except for missing
word errors which remain close to the 50% baseline
of coin-flipping.
For all methods, missing word errors are the
worst-performing, particularly in recall (i. e. the ac-
5The +0.3 increase in average accuracy for extra word errors
is not clearly significant as the results of cross-validation runs
overlap.
119
Figure 2: Accuracy by sentence length for Method 5
measured on separate grammatical and ungrammat-
ical data: Gr = Grammatical, AG = Agreement, RW
= Real-Word, EW = Extra Word, MW = Missing
Word
curacy on ungrammatical data alone). This means
that the omission of a word is less likely to result in
the sentence being flagged as erroneous. In contrast,
extra word errors perform consistently and relatively
well for all methods.
5 Conclusion and Future Work
We evaluated a deep processing approach and a POS
n-gram-based approach to the automatic detection of
common grammatical errors in a BNC-derived arti-
ficial error corpus. The results are broken down by
error type. Together with the deep approach, a deci-
sion tree machine learning algorithm can be used ef-
fectively. However, extending the shallow approach
with the same learning algorithm gives only small
improvements. Combining the deep and shallow ap-
proaches gives an additional improvement on all but
one error type.
Our plan is to investigate why all methods per-
form poorly on missing word errors, to extend the
error creation procedure so that it includes a wider
range of errors, to try the deep approach with other
parsers, to integrate additional features from state-
of-the-art shallow techniques and to repeat the ex-
periments for languages other than English.
Acknowledgements
This work is supported by the IRCSET Embark Ini-
tiative (basic research grant SC/02/298 and postdoc-
toral fellowship P/04/232). The training and test
data used in this reseach is based on the British Na-
tional Corpus (BNC), distributed by Oxford Univer-
sity Computing Services on behalf of the BNC Con-
sortium. We thank Djame? Seddah for helping us to
run the XLE parsing on the SFI/HEA Irish Centre
for High-End Computing (ICHEC) and the authors
wish to acknowledge ICHEC for the provision of
computational facilities and support.
References
Eric Atwell. 1987. How to detect grammatical errors in
a text without parsing it. In Proceedings of the 3rd
EACL, pages 38?45, Morristown, NJ.
Timothy Baldwin, John Beavers, Emily M. Bender, Dan
Flickinger, Ara Kim, and Stephan Oepen. 2004.
Beauty and the beast: What running a broad-coverage
precision grammar over the BNC taught us about the
grammar - and the corpus. In Pre-Proceedings of the
International Conference on Linguistic Evidence: Em-
pirical, Theoretical and Computational Perspectives,
pages 21?26.
Markus Becker, Andrew Bredenkamp, Berthold Crys-
mann, and Judith Klein. 1999. Annotation of error
types for German news corpus. In Proceedings of the
ATALA Workshop on Treebanks, Paris, France.
Emily M. Bender, Dan Flickinger, Stephan Oepen, and
Timothy Baldwin. 2004. Arboretum: Using a preci-
sion grammar for grammar checking in CALL. In Pro-
ceedings of the InSTIL/ICALL Symposium: NLP and
Speech Technologies in Advanced Language Learning
Systems, Venice, Italy.
Johnny Bigert and Ola Knutsson. 2002. Robust error
detection: a hybrid approach combining unsupervised
error detection and linguistic knowledge. In Proceed-
ings RO-MAND-02, Frascati, Italy.
Johnny Bigert. 2004. Probabilistic detection of context-
sensitive spelling errors. In Proceedings of LREC-04,
volume Five, pages 1633?1636, Lisbon, Portugal.
Phil Blunsom and Timothy Baldwin. 2006. Multilingual
deep lexical acquisition for HPSGs via supertagging.
In Proceedings of EMNLP-06, pages 164?171, Syd-
ney.
Lou Burnard. 2000. User reference guide for the British
national corpus. Technical report, Oxford University
Computing Services.
120
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The par-
allel grammar project. In Proceedings of COLING-
2002 Workshop on Grammar Engineering and Evalu-
ation, pages 1?7, Morristown, NJ, USA.
Eugene Charniak. 1996. Tree-bank grammars. Tech-
nical Report CS-96-02, Department of Computer Sci-
ence, Brown University.
Martin Chodorow and Claudia Leacock. 2000. An unsu-
pervised method for detecting grammatical errors. In
Proceedings of NAACL-00, pages 140?147, San Fran-
cisco, CA.
Noam Chomsky. 1957. Syntactic Structures. Mouton.
Ann Copestake and Dan Flickinger. 2000. An open-
source grammar development environment and broad-
coverage English grammar using HPSG. In Proceed-
ings of LREC-02, Athens, Greece.
Izumi Emi, Kiyotaka Uchimoto, and Hitoshi Isahara.
2004. The overview of the SST speech corpus of
Japanese learner English and evaluation through the
experiment on automatic detection of learners? er-
rors. In Proceedings of LREC-04, volume Four, pages
1435?1439, Lisbon, Portugal.
Jennifer Foster and Carl Vogel. 2004. Good reasons
for noting bad grammar: Constructing a corpus of un-
grammatical language. In Stephan Kepser and Marga
Reis, editors, Pre-Proceedings of the International
Conference on Linguistic Evidence: Empirical, The-
oretical and Computational Perspectives, pages 151?
152, Tu?bingen, Germany.
Jennifer Foster. 2005. Good Reasons for Noting Bad
Grammar: Empirical Investigations into the Parsing
of Ungrammatical Written English. Ph.D. thesis, Uni-
versity of Dublin, Trinity College, Dublin, Ireland.
Anette Frank, Tracy Holloway King, Jonas Kuhn, and
John Maxwell. 1998. Optimality theory style con-
straint ranking in large-scale LFG grammars. In Pro-
ceedings of LFG-98, Brisbane, Australia.
Andrew R. Golding. 1995. A Bayesian hybrid method
for context-sensitive spelling correction. In Proceed-
ings of the Third Workshop on Very Large Corpora,
pages 39?53, Boston, MA.
Sylviane Granger. 1993. International corpus of learner
English. In J. Aarts, P. de Haan, and N.Oostdijk, ed-
itors, English Language Corpora: Design, Analysis
and Exploitation, pages 57?71. Rodopi, Amsterdam.
Carl James. 1998. Errors in Language Learning and
Use: Exploring Error Analysis. Addison Wesley
Longman.
Ron Kaplan and Joan Bresnan. 1982. Lexical Functional
Grammar: a formal system for grammatical represen-
tation. In Joan Bresnan, editor, The Mental Represen-
tation of Grammatical Relations, pages 173?281. MIT
Press.
Rafiq Abdul Khader, Tracy Holloway King, and Miriam
Butt. 2004. Deep CALL grammars: The LFG-
OT experiment. http://ling.uni-konstanz.de/pages/
home/butt/dgfs04call.pdf.
John Maxwell and Ron Kaplan. 1996. An Efficient
Parser for LFG. In Proceedings of LFG-96, Grenoble.
Eric Mays, Fred J. Damerau, and Robert L. Mercer.
1991. Context based spelling correction. Information
Processing and Management, 23(5):517?522.
D. Nicholls. 1999. The Cambridge learner corpus ? error
coding and analysis. In Summer Workshop on Learner
Corpora, Tokyo, Japan.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press and
CSLI Publications.
Alan Prince and Paul Smolensky. 1993. Optimality The-
ory. MIT Press, Cambridge, Massachusetts.
Veit Reuer. 2003. PromisD - Ein Analyseverfahren
zur antizipationsfreien Erkennung und Erkla?rung von
grammatischen Fehlern in Sprachlehrsystemen. Ph.D.
thesis, Humboldt-Universita?t zu Berlin, Berlin, Ger-
many.
Helmut Schmidt. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49, Manchester, England.
J.P. Stemberger. 1982. Syntactic errors in speech. Jour-
nal of Psycholinguistic Research, 11(4):313?45.
Anne Vandeventer Faltin. 2003. Syntactic Error Diag-
nosis in the context of Computer Assisted Language
Learning. Ph.D. thesis, Universite? de Gene`ve.
L. Amber Wilcox-O?Hearn, Graeme Hirst, and Alexan-
der Budanitsky. 2006. Real-word spelling correc-
tion with trigrams: A reconsideration of the Mays,
Damerau, and Mercer model. http://ftp.cs.toronto.edu/
pub/gh/WilcoxOHearn-etal-2006.pdf.
Ian H. Witten and Eibe Frank. 2000. Data Mining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann Publishers.
121
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 257?266, Prague, June 2007. c?2007 Association for Computational Linguistics
Recovering Non-Local Dependencies for Chinese
Yuqing Guo
NCLT, School of Computing
Dublin City University
Dublin 9, Ireland
yguo@computing.dcu.ie
Haifeng Wang
Toshiba (China)
Research and Development Center
Beijing, 100738, China
wanghaifeng@rdc.toshiba.com.cn
Josef van Genabith
NCLT, School of Computing
Dublin City University
IBM CAS, Dublin, Ireland
josef@computing.dcu.ie
Abstract
To date, work on Non-Local Dependencies
(NLDs) has focused almost exclusively on
English and it is an open research question
how well these approaches migrate to other
languages. This paper surveys non-local de-
pendency constructions in Chinese as repre-
sented in the Penn Chinese Treebank (CTB)
and provides an approach for generating
proper predicate-argument-modifier struc-
tures including NLDs from surface context-
free phrase structure trees. Our approach re-
covers non-local dependencies at the level
of Lexical-Functional Grammar f-structures,
using automatically acquired subcategorisa-
tion frames and f-structure paths linking an-
tecedents and traces in NLDs. Currently our
algorithm achieves 92.2% f-score for trace
insertion and 84.3% for antecedent recovery
evaluating on gold-standard CTB trees, and
64.7% and 54.7%, respectively, on CTB-
trained state-of-the-art parser output trees.
1 Introduction
A substantial number of linguistic phenomena such
as topicalisation, relativisation, coordination and
raising & control constructions, permit a constituent
in one position to bear the grammatical role asso-
ciated with another position. These relationships
are referred to Non-Local Dependencies (NLDs),
where the surface location of the constituent is
called /antecedent0, and the site where the an-
tecedent should be interpreted semantically is called
/trace0. Capturing non-local dependencies is cru-
cial to the accurate and complete determination of
semantic interpretation in the form of predicate-
argument-modifier structures or deep dependencies.
However, with few exceptions (Model 3 of
Collins, 1999; Schmid, 2006), output trees pro-
duced by state-of-the-art broad coverage statistical
parsers (Charniak, 2000; Bikel, 2004) are only sur-
face context-free phrase structure trees (CFG-trees)
without empty categories and coindexation to repre-
sent displaced constituents. Because of the impor-
tance of non-local dependencies in the proper de-
termination of predicate-argument structures, recent
years have witnessed a considerable amount of re-
search on reconstructing such hidden relationships
in CFG-trees. Three strategies have been proposed:
(i) post-processing parser output with pattern match-
ers (Johnson, 2002), linguistic principles (Campbell,
2004) or machine learning methods (Higgins, 2003;
Levy and Manning, 2004; Gabbard et al, 2006) to
recover empty nodes and identify their antecedents;1
(ii) integrating non-local dependency recovery into
the parser by enriching a simple PCFG model with
GPSG-style gap features (Collins, 1999; Schmid,
2006); (iii) pre-processing the input sentence with
a finite-state trace tagger which detects empty nodes
before parsing, and identify the antecedents on the
parser output with the gap information (Dienes and
Dubey, 2003a; Dienes and Dubey, 2003b).
In addition to CFG-oriented approaches, a num-
ber of richer treebank-based grammar acquisition
and parsing methods based on HPSG (Miyao et
al., 2003), CCG (Clark and Hockenmaier, 2002),
LFG (Riezler et al, 2002; Cahill et al, 2004) and
Dependency Grammar (Nivre and Nilsson, 2005)
incorporate non-local dependencies into their deep
syntactic or semantic representations.
A common characteristic of all these approaches
1(Jijkoun, 2003; Jijkoun and Rijke, 2004) also describe post-
processing methods to recover NLDs, which are applied to syn-
tactic dependency structures converted from CFG-trees.
257
is that, to date, the research has focused almost
entirely on English,2 despite the disparity in type
and frequency of non-local dependencies for vari-
ous languages. In this paper, we address recover-
ing non-local dependencies for Chinese, a language
drastically different from English and whose spe-
cial features such as lack of morphological inflection
make NLD recovery more challenging. Inspired by
(Cahill et al, 2004)?s methodology which was origi-
nally designed for English and Penn-II treebank, our
approach to Chinese non-local dependency recovery
is based on Lexical-Functional Grammar (LFG), a
formalism that involves both phrase structure trees
and predicate-argument structures. NLDs are re-
covered in LFG f-structures using automatically ac-
quired subcategorisation frames and finite approxi-
mations of functional uncertainty equations describ-
ing NLD paths at the level of f-structures.
The paper is structured as follows: in Section 2 we
outline the distinguishing features of Chinese non-
local dependencies compared to English. In Section
3 we review (Cahill et al, 2004)?s method for recov-
ering English NLDs in treebank-based LFG approx-
imations. In Section 4, we describe how we mod-
ify and substantially extend the previous method
to recover all types of NLDs for Chinese data.
We present experiments and provide a dependency-
based evaluation in Section 5. Finally we conclude
and summarise future work.
2 Non-Local Dependencies in Chinese
In the Penn Chinese Treebank (CTB) (Xue et al,
2002) non-local dependencies are represented in
terms of empty categories (ECs) and (for some of
them) coindexation with antecedents, as exemplified
in Figure 1. Following previous work for English
and the CTB annotation scheme, we use /non-
local dependencies0as a cover term for all miss-
ing or dislocated elements represented in the CTB
as an empty category (with or without coindexa-
tion/antecedent), and our use of the term remains ag-
nostic about fine-grained distinctions between non-
local dependencies drawn in the theoretical linguis-
tics literature.
In order to give an overview on the character-
2 (Levy and Manning, 2004) is the only approach we are
aware of that has been applied to both English and German.
(1) ? ?u? ?k d?  # ?[
not want look-for train have potential DE new writer
?(People) don?t want to look for and train new writers who
have potential.?
IP
NP-SBJ
-NONE-
*pro*
VP
ADVP
AD
?
not
VP
VV
?
want
IP-OBJ
NP-SBJ
-NONE-
*PRO*
VP
VP
VV
u?
look for
NP-OBJ
-NONE-
*RNR*-2
PU
!
VP
VV
?
train
NP-OBJ-2
CP
WHNP-1
-NONE-
*OP*
CP
IP
NP-SBJ
-NONE-
*T*-1
VP
VE
k
have
NP
NN
d?
potential
DEC

DE
ADJP
JJ
#
new
NP
NN
?[
writer
Figure 1: Example of non-local annotations in CTB,
including dropped subject (*pro*), control subject
(*PRO*), relative clause (*T*), and coordination
(*RNR*).
istics of Chinese non-local dependencies, we ex-
tracted all empty categories together with coindexed
antecedents from the Penn Chinese Treebank ver-
sion 5.1 (CTB5.1). Table 1 gives a breakdown of the
most frequent types of empty categories and their
antecedents, which account for 43,791 of the total
43,954 (99.6%) ECs in CTB5.1.3
According to their different linguistics properties,
we divide the empty nodes listed in Table 1 into
three major types: null relative pronouns, locally
mediated dependencies, and long-distance depen-
dencies.
Null Relative Pronouns (lines 2, 7) themselves
are local dependencies, and thus are not coindexed
with an antecedent. But they mediate non-local de-
pendencies by functioning as antecedents for the dis-
3An extensive description of the types of empty categories
and the use of coindexation in CTB can be found in Section VI
of the bracketing guidelines.
258
Antecedent POS Label Count Description
1 WHNP NP *T* 11670 WH trace (e.g. *OP*?I/Chinau/launch*T*/DE?(/satellite)
2 WHNP *OP* 11621 Empty relative pronouns (e.g. *OP*?I/Chinau/launch/DE?(/satellite)
3 NP *PRO* 10946 Control constructions (e.g. ?p/here?/notN/allow*PRO*??/smoke)
4 NP *pro* 7481 Pro-drop situations (e.g. *pro*?/notQ/ever?/encounter/DE?K/problem)
5 IP IP *T* 575 Topicalisation (e.g. ??/weU/canI/win?/he`/say*T*)
6 WHPP PP *T* 337 WH trace (e.g. *OP*<?/population*T*?8/dense/?/area)
7 WHPP *OP* 337 Empty relative pronouns (e.g. *OP*<?/population?8/dense/?/area)
8 NP NP * 291 Raising & passive constructions (e.g. ??/weProceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 267?276, Prague, June 2007. c?2007 Association for Computational Linguistics
Exploiting Multi-Word Units in History-Based Probabilistic Generation
Deirdre Hogan, Conor Cafferkey, Aoife Cahill? and Josef van Genabith
National Centre for Language Technology
School of Computing, Dublin City University
Dublin 9, Ireland
dhogan,ccafferkey,josef@computing.dcu.ie
Abstract
We present a simple history-based model for
sentence generation from LFG f-structures,
which improves on the accuracy of previous
models by breaking down PCFG indepen-
dence assumptions so that more f-structure
conditioning context is used in the predic-
tion of grammar rule expansions. In addi-
tion, we present work on experiments with
named entities and other multi-word units,
showing a statistically significant improve-
ment of generation accuracy. Tested on sec-
tion 23 of the Penn Wall Street Journal Tree-
bank, the techniques described in this paper
improve BLEU scores from 66.52 to 68.82,
and coverage from 98.18% to 99.96%.
1 Introduction
Sentence generation, or surface realisation, is the
task of generating meaningful, grammatically cor-
rect and fluent text from some abstract semantic or
syntactic representation of the sentence. It is an im-
portant and growing field of natural language pro-
cessing with applications in areas such as transfer-
based machine translation (Riezler and Maxwell,
2006) and sentence condensation (Riezler et al,
2003). While recent work on generation in restricted
domains, such as (Belz, 2007), has shown promising
results there remains much room for improvement
particularly for broad coverage and robust genera-
tors, like those of Nakanishi et al (2005) and Cahill
? Now at the Institut fu?r Maschinelle Sprachverarbeitung,
Universita?t Stuttgart, Azenbergstrae 12, D-70174 Stuttgart,
Germany. aoife.cahill@ims.uni-stuttgart.de
and van Genabith (2006), which do not rely on hand-
crafted grammars and thus can easily be ported to
new languages.
This paper is concerned with sentence genera-
tion from Lexical-Functional Grammar (LFG) f-
structures (Kaplan, 1995). We present improve-
ments in previous LFG-based generation models
firstly by breaking down PCFG independence as-
sumptions so that more f-structure conditioning con-
text is included when predicting grammar rule ex-
pansions. This history-based approach has worked
well in parsing (Collins, 1999; Charniak, 2000) and
we show that it also improves PCFG-based genera-
tion.
We also present work on utilising named entities
and other multi-word units to improve generation
results for both accuracy and coverage. There has
been a limited amount of exploration into the use
of multi-word units in probabilistic parsing, for ex-
ample in (Kaplan and King, 2003) (LFG parsing)
and (Nivre and Nilsson, 2004) (dependency pars-
ing). We are not aware of any similar work on gen-
eration. In the LFG-based generation algorithm pre-
sented by Cahill and van Genabith (2006) complex
named entities (i.e. those consisting of more than
one word token) and other multi-word units can be
fragmented in the surface realization. We show that
the identification of such units may be used as a sim-
ple measure to constrain the generation model?s out-
put.
We take the generator of (Cahill and van Gen-
abith, 2006) as our baseline generator. When tested
on f-structures for all sentences from Section 23 of
the Penn Wall Street Journal (WSJ) treebank (Mar-
267
cus et al, 1993), the techniques described in this pa-
per improve BLEU score from 66.52 to 68.82. In
addition, coverage is increased from 98.18% to al-
most 100% (99.96%).
The remainder of the paper is structured as fol-
lows: in Section 2 we review related work on sta-
tistical sentence generation. Section 3 describes the
baseline generation model and in Section 4 we show
how the new history-based model improves over the
baseline. In Section 5 we describe the source of the
multi-word units (MWU) used in our experiments
and the various techniques we employ to make use
of these MWUs in the generation process. Section 6
gives experimental details and results.
2 Related Work on Statistical Generation
In (statistical) generators, sentences are generated
from an abstract linguistic encoding via the appli-
cation of grammar rules. These rules can be hand-
crafted grammar rules, such as those of (Langkilde-
Geary, 2002; Carroll and Oepen, 2005), created
semi-automatically (Belz, 2007) or, alternatively,
extracted fully automatically from treebanks (Ban-
galore and Rambow, 2000; Nakanishi et al, 2005;
Cahill and van Genabith, 2006).
Insofar as it is a broad coverage generator, which
has been trained and tested on sections of the WSJ
corpus, our generator is closer to the generators
of (Bangalore and Rambow, 2000; Langkilde-Geary,
2002; Nakanishi et al, 2005) than to those designed
for more restricted domains such as weather fore-
cast (Belz, 2007) and air travel domains (Ratna-
parkhi, 2000).
Another feature which characterises statistical
generators is the probability model used to select the
most probable sentence from among the space of all
possible sentences licensed by the grammar. One
generation technique is to first generate all possible
sentences, storing them in a word lattice (Langkilde
and Knight, 1998) or, alternatively, a generation for-
est, a packed represention of alternate trees proposed
by the generator (Langkilde, 2000), and then select
the most probable sequence of words via an n-gram
language model.
Increasingly syntax-based information is being
incorporated directly into the generation model. For
example, Carroll and Oepen (2005) describe a sen-
tence realisation process which uses a hand-crafted
HPSG grammar to generate a generation forest. A
selective unpacking algorithm allows the extraction
of an n-best list of realisations where realisation
ranking is based on a maximum entropy model. This
unpacking algorithm is used in (Velldal and Oepen,
2005) to rank realisations with features defined over
HPSG derivation trees. They achieved the best re-
sults when combining the tree-based model with an
n-gram language model.
Nakanishi et al (2005) describe a treebank-
extracted HPSG-based chart generator. Importing
techniques developed for HPSG parsing, they apply
a log linear model to a packed representation of all
alternative derivation trees for a given input. They
found that a model which included syntactic infor-
mation outperformed a bigram model as well as a
combination of bigram and syntax model.
The probability model described in this paper also
incorporates syntactic information, however, unlike
the discriminative HPSG models just described, it
is a generative history- and PCFG-based model.
While Belz (2007) and Humphreys et al (2001)
mention the use of contextual features for the rules
in their generation models, they do not provide de-
tails nor do they provide a formal probability model.
To the best of our knowledge this is the first paper
providing a probabilistic generative, history-based
generation model.
3 Surface Realisation from f-Structures
Cahill and van Genabith (2006) present a prob-
abilistic surface generation model for LFG (Ka-
plan, 1995). LFG is a constraint-based theory
of grammar, which analyses strings in terms of
c(onstituency)-structure and f(unctional)-structure
(Figure 1). C-structure is defined in terms of CFGs,
and f-structures are recursive attribute-value ma-
trices which represent abstract syntactic functions
(such as SUBJect, OBJect, OBLique, COMPlement
(sentential), ADJ(N)unct), agreement, control, long-
distance dependencies and some semantic informa-
tion (e.g. tense, aspect).
C-structures and f-structures are related in a pro-
jection architecture in terms of a piecewise corre-
spondence ?.1 The correspondence is indicated in
1Our formalisation follows (Kaplan, 1995).
268
S
?=?
NP VP
(? SUBJ)= ? ?=?
NNP V NP
?=? ?=? (? OBJ)= ?
Susan contacted PRP
(? PRED) = ?Susan? (? PRED) = ?contact? ?=?
(? NUM) = SG (? TENSE) = past
(? PERS) = 3 her
(? PRED) = ?pro?
(? NUM) = SG
(? PERS) = 3
f1:
?
?
?
?
?
?
PRED ?CONTACT?(?SUBJ)(?OBJ)??
SUBJ f2:
[
PRED ?SUSAN?
NUM SG
PERS 3
]
OBJ f2:
[
PRED ?PRO?
NUM SG
PERS 3
]
TENSE PAST
?
?
?
?
?
?
Figure 1: C- and f-structures with ? links for the sentence Susan contacted her.
terms of the curvy arrows pointing from c-structure
nodes to f-structure components in Figure 1. Given
a c-structure node ni, the corresponding f-structure
component fj is ?(ni). F-structures and the c-
structure/f-structure correspondence are described
in terms of functional annotations on c-structure
nodes (CFG grammar rules). An equation of the
form (?F) = ? states that the f-structure associated
with the mother of the current c-structure node (?)
has an attribute (grammatical function) (F), whose
value is the f-structure of the current node (?).
The up-arrows and down-arrows are shorthand for
?(M(ni)) = ?(ni) where ni is the c-structure node
annotated with the equation.2
Treebest := argmaxTreeP (Tree|F-Str) (1)
P (Tree|F-Str) :=
?
X ? Y in Tree
Feats = {ai|?vj(?(X))ai = vj}
P (X ? Y |X, Feats) (2)
The generation model of (Cahill and van Gen-
abith, 2006) maximises the probability of a tree
given an f-structure (Eqn. 1), and the string gener-
ated is the yield of the highest probability tree. The
generation process is guided by purely local infor-
mation in the input f-structure: f-structure annotated
CFG rules (LHS ? RHS) are conditioned on their
LHSs and on the set of features/attributes Feats =
{ai|?vj?(LHS)ai = vj}3 ?-linked to the LHS (Eqn.
2M is the mother function on CFG tree nodes.
3In words, Feats is the set of top level features/attributes
(those attributes ai for which there is a value vi) of the f-
structure ? linked to the LHS.
2). Table 1 shows a generation grammar rule and
conditioning features extracted from the example in
Figure 1. The probability of a tree is decomposed
into the product of the probabilities of the f-structure
annotated rules (conditioned on the LHS and local
Feats) contributing to the tree. Conditional proba-
bilities are estimated using maximum likelihood es-
timation.
grammar rule local conditioning features
S(?=?)? NP(?SUBJ=?) VP(?=?) S(?=?), {SUBJ,OBJ,PRED,TENSE}
Table 1: Example grammar rule (from Figure 1).
Cahill and van Genabith (2006) note that condi-
tioning f-structure annotated generation rules on lo-
cal features (Eqn. 2) can sometimes cause the model
to make inappropriate choices. Consider the follow-
ing scenario where in addition to the c-/f-structure in
Figure 1, the training set contains the c-/f-structure
displayed in Figure 2.
From Figures 1 and 2, the model learns (among
others) the generation rules and conditional proba-
bilities displayed in Tables 2 and 3.
F-Struct Feats Grammar Rules Prob
{SUBJ, OBJ, PRED} S(?=?) ? NP(?SUBJ=?) VP(?=?) 1
{SUBJ, OBJ, PRED} VP(?=?) ? V(?=?) NP(?OBJ=?) 1
{NUM, PER, GEN} NP(?SUBJ=?) ? NNP(?=?) 0.5
{NUM, PER, GEN} NP(?SUBJ=?) ? PRP(?=?) 0.5
{NUM, PER, GEN} NP(?OBJ=?) ? PRP(?=?) 1
Table 2: A sample of internal grammar rules ex-
tracted from Figures 1 and 2.
Given the input f-structure (for She
accepted) in Figure 3, (and assuming suit-
able generation rules for intransitive VPs and
accepted) the model would produce the inappro-
priate highest probability tree of Figure 4 with an
incorrect case for the pronoun in subject position.
269
S
?=?
NP VP
(? SUBJ)= ? ?=?
PRP V NP
?=? ?=? (? OBJ)= ?
She hired PRP
(? PRED) = ?pro? (? PRED) = ?hire? ?=?
(? NUM) = SG (? TENSE) = past
(? PERS) = 3 her
(? PRED) = ?pro?
(? NUM) = SG
(? PERS) = 3
f1 :
?
?
?
?
?
?
PRED ?HIRE?(?SUBJ)(?OBJ)??
SUBJ f2 :
[
PRED ?PRO?
NUM SG
PERS 3
]
OBJ f2 :
[
PRED ?PRO?
NUM SG
PERS 3
]
TENSE PAST
?
?
?
?
?
?
Figure 2: C- and f-structures with ? links for the sentence She hired her.
F-Struct Feats Grammar Rules Prob
{PRED=PRO,NUM=SG PER=3, GEN=FEM} PRP(?=?) ? she 0.33
{PRED=PRO,NUM=SG PER=3, GEN=FEM} PRP(?=?) ? her 0.66
Table 3: A sample of lexical item rules extracted
from Figures 1 and 2.
?
?
?
?
?
?
SUBJ
?
?
PRED pro
NUM sg
PERS 3
GEND fem
?
?
PRED accept
TENSE past
?
?
?
?
?
?
Figure 3: Input f-structure for She accepted.
To solve the problem, Cahill and van Gen-
abith (2006) apply an automatic generation gram-
mar transformation to their training data: they au-
tomatically label CFG nodes with additional case
information and the model now learns the new im-
proved generation rules of Tables 4 and 5. Note
how the additional case labelling subverts the prob-
lematic independence assumptions of the probabil-
ity model and communicates the fact that a subject
NP has to be realised as nominative case from the
S ? NP-nom VP production, via the intermediate
NP-nom ? PRP-nom, down to the lexical produc-
tion PRP-nom ? she. The labelling guarantees that,
given the example f-structure in Figure 3, the model
generates the correct string she accepted.
F-Struct Feats Grammar Rules
{SUBJ, OBJ, PRED} S(?=?) ? NP-nom(?SUBJ=?) VP(?=?)
{SUBJ, OBJ, PRED} VP(?=?) ? V(?=?) NP-acc(?OBJ=?)
{NUM, PER, GEN} NP-nom(?SUBJ=?) ? PRP-nom(?=?)
{NUM, PER, GEN} NP-nom(?SUBJ=?) ? NNP-nom(?=?)
{NUM, PER, GEN} NP-acc(?OBJ=?) ? PRP-acc(?=?)
Table 4: Internal grammar rules with case markings.
S
?=?
NP VP
(? SUBJ)= ? ?=?
PRP V
?=? ?=?
her accepted
(? PRED) = ?pro? (? PRED) = ?hire?
(? NUM) = SG (? TENSE) = past
(? PERS) = 3
Figure 4: Inappropriate output: her accepted.
F-Struct Feats Grammar Rules
{PRED=PRO,NUM=SG PER=3, GEN=FEM} PRP-nom(?=?) ? she
{PRED=PRO,NUM=SG PER=3, GEN=FEM} PRP-acc(?=?) ? her
Table 5: Lexical item rules with case markings
4 A History-Based Generation Model
The automatic generation grammar transform pre-
sented in (Cahill and van Genabith, 2006) provides
a solution to coarse-grained and (in fact) inappropri-
ate independence assumptions in the basic genera-
tion model. However, there is a sense in which the
proposed cure improves on the symptoms, but not
the cause of the problem: it weakens independence
assumptions by multiplying and hence increasing
the specificity of conditioning CFG category labels.
There is another option available to us, and that is
the option we will explore in this paper: instead of
applying a generation grammar transform, we will
improve the f-structure-based conditioning of the
generation rule probabilities. In the original model,
rules are conditioned on purely local f-structure con-
text: the set of features/attributes ?-linked to the
LHS of a grammar rule. As a direct consequence
of this, the conditioning (and hence the model) can-
not not distinguish between NP, PRP and NNP rules
270
appropriate to e.g. subject (SUBJ) or object con-
texts (OBJ) in a given input f-structure. However,
the required information can easily be incorporated
into the generation model by uniformly conditioning
generation rules on their parent (mother) grammati-
cal function, in addition to the local ?-linked feature
set. This additional conditioning has the effect of
making the choice of generation rules sensitive to
the history of the generation process, and, we argue,
provides a simpler, more uniform, general, intuitive
and natural probabilistic generation model obviating
the need for CFG-grammar transforms in the origi-
nal proposal of (Cahill and van Genabith, 2006).
In the new model, each generation rule is now
conditioned on the LHS rule CFG category, the set
of features ?-linked to LHS and the parent grammat-
ical function of the f-structure ?-linked to LHS. In a
given c-/f-structure pair, for a CFG node n, the par-
ent grammatical function of the f-structure ?-linked
to n is that grammatical function GF, which, if we
take the f-structure ?-linked to the mother M(n), and
apply it to GF, returns the f-structure ?-linked to n:
(?(M(n))GF) = ?(n).
The basic idea is best explained by way of an
example. Consider again Figure 1. The mother
grammatical function of the f-structure f2 asso-
ciated with node NP(?SUBJ=?) and its daughter
NNP(?=?) (via the ?=? functional annotation) is
SUBJ, as (?(M(n2))SUBJ) = ?(n2), or equivalently
(f1SUBJ) = f2.
Given Figures 1 and 2 as training set, the im-
proved model learns the generation rules (the mother
grammatical function of the outermost f-structure is
assumed to be a dummy TOP grammatical function)
of Tables 6 and 7.
F-Struct Feats Grammar Rules
{SUBJ, OBJ, PRED, TOP} S(?=?) ? NP(?SUBJ=?) VP(?=?)
{SUBJ, OBJ, PRED, TOP} VP(?=?) ? V(?=?) NP(?OBJ=?)
{NUM, PER, GEN, SUBJ} NP(?SUBJ=?) ? PRP(?=?)
{NUM, PER, GEN, OBJ} NP(?OBJ=?) ? PRP(?=?)
{NUM, PER, GEN, SUBJ} NP(?SUBJ=?) ? NNP(?=?)
Table 6: Grammar rules with extra feature extracted
from F-Structures.
Note, that for our example the effect of the uni-
form additional conditioning on mother grammat-
ical function has the same effect as the genera-
tion grammar transform of (Cahill and van Gen-
abith, 2006), but without the need for the gram-
F-Struct Feats Grammar Rules
{PRED=PRO,NUM=SG PER=3, GEN=FEM, SUBJ} PRP(?=?) ? she
{PRED=PRO,NUM=SG PER=3, GEN=FEM, OBJ} PRP(?=?) ? her
Table 7: Lexical item rules.
mar transform. Given the input f-structure in Fig-
ure 3, the model will generate the correct string
she accepted. In addition, uniform condition-
ing on mother grammatical function is more general
than the case-phenomena specific generation gram-
mar transform of (Cahill and van Genabith, 2006),
in that it applies to each and every sub-part of a
recursive input f-structure driving generation, mak-
ing available relevant generation history (context) to
guide local generation decisions.
The new history-based probabilistic generation
model is defined as:
P (Tree|F-Str) :=
?
X ? Y in Tree
Feats = {ai|?vj(?(X))ai = vj}
(?(M(X)))GF = ?(X)
P (X ? Y |X, Feats,GF) (3)
Note that the new conditioning feature, the f-
structure mother grammatical function, GF, is avail-
able from structure previously generated in the c-
structure tree. As such, it is part of the history of
the tree, i.e. it has already been generated in the top-
down derivation of the tree. In this way, the gen-
eration model resembles history-based models for
parsing (Black et al, 1992; Collins, 1999; Charniak,
2000). Unlike, say, the parent annotation for parsing
of (Johnson, 1998) the parent GF feature for a par-
ticular node expansion is not merely extracted from
the parent node in the c-structure tree, but is some-
times extracted from an ancestor node further up the
c-structure tree via intervening ?=? functional an-
notations.
Section 6 provides evaluation results for the new
model on section 23 of the Penn treebank.
5 Multi-Word Units
In another effort to improve generator accuracy over
the baseline model we explored the use of multi-
word units in generation. We expect that the identi-
fication of MWUs may be useful in imposing word-
order constraints and reducing the complexity of the
generation task. Take, for example, the following
271
??
?
?
?
?
?
?
APP
?
?
?
?
?
?
ADJUNCT
[
PRED ?New?
NUM sg
PERS 3
]
PRED ?York?
NUM sg
PERS 3
?
?
?
?
?
?
?
?
?
?
?
?
?
?
[
APP
[
PRED ?New York?
NUM sg
PERS 3
]]
?
?
?
?
?
?
?
?
APP
?
?
?
?
?
?
ADJUNCT
[
PRED ?New?/NE1 1
NUM sg
PERS 3
]
PRED ?York?/NE1 2
NUM sg
PERS 3
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 5: Three different f-structure formats. From left to right: the original f-structure format; the MWU
chunk format; the MWU mark-up format.
two sentences which show the gold version of a sen-
tence followed by the version of the sentence pro-
duced by the generator:
Gold By this time , it was 4:30 a.m. in New York ,
and Mr. Smith fielded a call from a New York
customer wanting an opinion on the British
stock market , which had been having trou-
bles of its own even before Friday ?s New York
market break .
Test By this time , in New York , it was 4:30 a.m.
, and Mr. Smith fielded a call from New a
customer York , wanting an opinion on the
market British stock which had been having
troubles of its own even before Friday ?s New
York market break .
The gold version of the sentence contains a multi-
word unit, New York, which appears fragmented in
the generator output. If multi-word units were either
treated as one token throughout the generation pro-
cess, or, alternatively, if a constraint were imposed
on the generator such that multi-word units were al-
ways generated in the correct order, then this should
help improve generation accuracy. In Section 5.1
we describe the various techniques that were used
to incorporate multi-word units into the generation
process and in 5.2 we detail the different types and
sources of multi-word unit used in the experiments.
Section 6 provides evaluation results on test and de-
velopment sets from the WSJ treebank.
5.1 Incorporating MWUs into the Generation
Process
We carried out three types of experiment which, in
different ways, enabled the generation process to
respect the restrictions on word-order provided by
multi-word units. For the first experiments (type
1), the WSJ treebank training and test data were
altered so that multi-word units are concatenated
into single words (for example, New York becomes
New York). As in (Cahill and van Genabith, 2006) f-
structures are generated from the (now altered) tree-
bank and from this data, along with the treebank
trees, the PCFG-based grammar, which is used for
training the generation model, is extracted. Simi-
larly, the f-structures for the test and development
sets are created from Penn Treebank trees which
have been modified so that multi-word units form
single units. The leftmost and middle f-structures in
Figure 5 show an example of an original f-structure
format and a named-entity chunked format, respec-
tively. Strings output by the generator are then post-
processed so that the concatenated word sequences
are converted back into single words.
In the second experiment (type 2) only the test
data was altered with no concatenation of MWUs
carried out on the training data.
In the final experiments (type 3), instead of con-
catenating named entities, a constraint is introduced
to the generation algorithm which penalises the gen-
eration of sequences of words which violate the in-
ternal word order of named entities. The input is
marked-up in such a way that, although named en-
tities are no longer chunked together to form single
words, the algorithm can read which items are part
of named entities. See the rightmost f-structure in
Figure 5 for an example of an f-structure marked-
up in this way. The tag NE1 1, for example, indi-
cates that the sub-f-structure is part of a named iden-
tity with id number 1 and that the item corresponds
to the first word of the named entity. The baseline
generation algorithm, following Kay (1996)?s work
on chart generation, already contains the hard con-
straint that when combining two chart edges they
must cover disjoint sets of words. We added an ad-
ditional constraint which prevents edges from being
combined if this would result in the generation of
a string which contained a named entity which was
272
either incomplete or where the words in the named
entity were generated in the wrong order.
5.2 Types of MWUs used in Experiments
We carry out experiments with multi-word units
from three different sources. First, we use the output
of the maximum entropy-based named entity recog-
nition system of (Chieu and Ng, 2003). This sys-
tem identifies four types of named entity: person,
organisation, location, and miscellaneous. Addition-
ally we use a dictionary of candidate multi-word ex-
pressions based on a list from the Stanford Multi-
word Expression Project4. Finally, we also carry out
experiments with multi-word units extracted from
the BBN Pronoun Coreference and Entity Type Cor-
pus (Weischedel and Brunstein, 2005). This supple-
ments the Penn WSJ treebank?s one million words of
syntax-annotated Wall Street Journal text with addi-
tional annotations of 23 named entity types, includ-
ing nominal-type named entities such as person, or-
ganisation, location, etc. as well as numeric types
such as date, time, quantity and money. Since the
BBN corpus data is very comprehensive and is hand-
annotated we take this be be a gold standard, repre-
senting an upper bound for any gains that might be
made by identifying complex named entities in our
experiments.5 Table 8 gives examples of the various
types of MWUs identified by the three sources.
For our purposes we are not concerned with the
distinctions between different types of named enti-
ties; we are merely exploiting the fact that they may
be treated as atomic units in the generation model. In
all cases we disregard multi-word units that cross the
original syntactic bracketing of the WSJ treebank.
An overview of the various types of multi-word units
used in our experiments is presented in Table 9.
6 Experimental Evaluation
All experiments were carried out on the WSJ tree-
bank with sections 02-21 for training, section 24 for
development and section 23 for final test results. The
LFG annotation algorithm of (Cahill et al, 2004)
was used to produce the f-structures for develop-
ment, test and training sets.
4mwe.stanford.edu
5Although it is possible there are other types of MWUs that
may be more suitable to the task than the named entities identi-
fied by BBN, so further gains might be possible.
MWU type Examples
Names Martha Matthews
Yoshio Hatakeyama
Organisations Rolls-Royce Motor Cars Inc.
Washington State University
Locations New York City
New Zealand
Time expressions October 19th
two years ago
the 21st century
Quantities $2.7 million to $3 million
about 25 %
60 mph
Prepositional expressions in fact
at the time
on average
Table 8: Examples of some of the types of MWU
from the three different sources.
average number average length
(Chieu and Ng, 2003) 0.61 2.40
Stanford MWE Project 0.10 2.48
BBN Corpus 1.15 2.66
Table 9: Average number of MWUs per sentence
and average MWU length in the WSJ treebank
grouped by MWU source.
Table 10 shows the final results for section 23. For
each test we present BLEU score results as well as
String Edit Distance and coverage. We measure sta-
tistical significance using two different tests. First
we use a bootstrap resampling method, popular for
machine translation evaluations, to measure the sig-
nificance of improvements in BLEU scores, with a
resampling rate of 1000.6 We also calculated the
significance of an increase in String Edit Distance
by carrying out a paired t-test on the mean differ-
ence of the String Edit Distance scores. In Table 10,
? means significant at level 0.005. > means signif-
icant at level 0.05.
In Table 10, Baseline gives the results of the
generation algorithm of (Cahill and van Genabith,
2006). HB Model refers to the improved model
with the increased history context, as described in
Section 4. The results, where for example the
BLEU score rises from 66.52 to 67.24, show that
even increasing the conditioning context by a limited
6Scripts for running the bootstrapping method carried
out in our evaluation are available for download at projec-
tile.is.cs.cmu.edu/research/public/tools/bootStrap/tutorial.htm
273
Section 23 (2416 sentences)
Model BLEU StringEd Coverage BLEU Bootstrap Signif StringEd Paired T-Test
1. Baseline 66.52 68.69 98.18
2. HB Model 67.24 69.89 99.88 ? 1 ? 1
3. +MWU Best Automatic 67.81 70.36 99.92 ? 2 ? 2
4. MWU BBN 68.82 70.92 99.96 ? 3 > 3
Table 10: Results on Section 23 for all sentence lengths.
amount increases the accuracy of the system signif-
icantly for both BLEU and String Edit Distance. In
addition, coverage goes up from 98.18% to 99.88%.
+MWU Best Automatic displays our best results
using automatically identified named entities. These
were achieved using experiment type 2, described
in Section 5, with the MWUs produced by (Chieu
and Ng, 2003). Results displayed in Table 10 up
to this point are cumulative. The final row in Ta-
ble 10, MWU BBN, shows the best results with BBN
MWUs: the history-based model with BBN multi-
word units incorporated in a type 1 experiment.
We now discuss the various MWU experiments
in more detail. See Table 11 for a breakdown of
the MWU experiment results on the development
set, WSJ section 24. Our baseline for these exper-
iments is the history-based generator presented in
Section 4. For each experiment type described in
Section 5.1 we ran three experiments, varying the
source of MWUs. First, MWUs came from the auto-
matic NE recogniser of (Chieu and Ng, 2003), then
we added the MWUs from the Stanford list and fi-
nally we ran tests with MWUs extracted from the
BBN corpus.
Our first set of experiments (type 1), where both
training data and development set data were MWU-
chunked, produced the worst results for the automat-
ically chunked MWUs. BLEU score accuracy actu-
ally decreased for the automatically chunked MWU
experiments. In an error analysis of type 1 ex-
periments with (Chieu and Ng, 2003) concatenated
MWUs, we inspected those sentences where accu-
racy had decreased from the baseline. We found
that for over half (51.5%) of these sentences, the in-
put f-structures contained no multi-word units at all.
The problem for these sentences therefore lay with
the probabilistic grammar extracted from the MWU-
chunked training data. When the source of MWU
for the type 1 experiments was the BBN, however,
accuracy improved significantly over the baseline
and the result is the highest accuracy achieved over
all experiment types. One possible reason for the
low accuracy scores in the type 1 experiments with
the (Chieu and Ng, 2003) MWU chunked data could
be noisy MWUs which negatively affect the gram-
mar. For example, the named entity recogniser
of (Chieu and Ng, 2003) achieves an accuracy of
88.3% on section 23 of the Penn Treebank.
In order to avoid changing the grammar through
concatenation of MWU components (as in exper-
iment type 1) and thus risking side-effects which
cause some heretofore likely constructions become
less likely and vice versa, we ran the next set of ex-
periments (type 2) which leave the original grammar
intact and alter the input f-structures only. These
experiments were more successful overall and we
achieved an improvement over the baseline for both
BLEU and String Edit Distance scores with all
MWU types. As can be seen from Table 11 the
best score for automatically chunked MWUs are
with the (Chieu and Ng, 2003) MWUs. Accuracy
decreases marginally when we added the Stanford
MWUs. In our final set of experiments (type 3) al-
though the accuracy for all three types of MWUs
improves over the baseline, accuracy is a little be-
low the type 2 experiments.
It is difficult to compare sentence generators since
the information contained in the input varies greatly
between systems, systems are evaluated on different
test sets and coverage also varies considerably. In
order to compare our system with those of (Nakan-
ishi et al, 2005) and (Langkilde-Geary, 2002) we
report our best results with automatically acquired
MWUs for sentences of ? 20 words in length on
section 23: our system gets coverage of 100% and a
BLEU score of 71.39. For the same test set Nakan-
ishi et al (2005) achieved coverage of 90.75 and a
BLEU score of 77.33. Langkilde-Geary (2002) re-
274
Section 24 (1346 sentences)
Model MWUs BLEU StringEd Coverage
HB Model 65.85 69.93 99.93
type 1 (Chieu and Ng, 2003) 65.81 70.34 99.93
(training and test data chunked) +Stanford MWEs 64.81 69.67 99.93
BBN 67.24 71.46 99.93
type 2 (Chieu and Ng, 2003) 66.37 70.26 99.93
(test data chunked) +Stanford MWEs 66.28 70.21 99.93
BBN 66.84 70.74 99.93
type 3 (Chieu and Ng, 2003) 66.30 70.12 100
(internal generation constraint) +Stanford MWEs 66.07 70.02 99.93
BBN 66.45 70.14 99.93
Table 11: Results on Section 24, all sentence lengths.
ports 82.7% coverage and a BLEU score of 75.7%
on the same test set with the ?permute,no dir? type
input. Langkilde-Geary (2002) report results for ex-
periments with varying levels of linguistic detail in
the input given to the generator. As with Nakanishi
et al (2005) we find the ?permute,no dir? type of in-
put is most comparable to the level of information
contained in our input f-structures. Finally, the sym-
bolic generator of Callaway (2003) reports a Sim-
ple String Accuracy score of 88.84 and coverage of
98.7% on section 23 for all sentence lengths.
7 Conclusion and Future Work
We have presented techniques which improve the ac-
curacy of an already state-of-art surface generation
model. We found that a history-based model that
increases conditioning context in PCFG style rules
by simply including the grammatical function of the
f-structure parent, improves generator accuracy. In
the future we will experiment with increasing condi-
tioning context further and using more sophisticated
smoothing techniques to avoid sparse data problems
when conditioning is increased.
We have also demonstrated that automatically ac-
quired multi-word units can bring about moderate,
but significant, improvements in generator accuracy.
For automatically acquired MWUs, we found that
this could best be achieved by concatenating input
items when generating the f-structure input to the
generator, while training the input generation gram-
mar on the original (i.e. non-MWU concatenated)
sections of the treebank. Relying on the BBN cor-
pus as a source of multi-word units, we gave an up-
per bound to the potential usefulness of multi-word
units in generation and showed that automatically
acquired multi-word units, encouragingly, give re-
sults not far below the upper bound.
References
Srinivas Bangalore and Owen Rambow. 2000. Exploit-
ing a probabilistic hierarchical model for generation.
In Proceedings of the 18th COLING.
Anja Belz. 2007. Probabilistic generation of wether fore-
cast texts. In Proceedings of NAACL-HLT.
Ezra Black, Fred Jelinek, John Lafferty, David M. Mager-
man, Robert Mercer, and Salim Roukos. 1992. To-
wards history-based grammars: Using richer models
for probabilistic parsing. In Proceeding of the 5th
DARPA Speech and Language Workshop.
Aoife Cahill and Josef van Genabith. 2006. Robust
PCFG-based generation using automatically acquired
LFG approximations. In Proceedings of the 44th ACL.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef van
Genabith, and Andy Way. 2004. Long-distance de-
pendency resolution in automatically acquired wide-
coverage PCFG-based LFG approximations. In Pro-
ceedings of the 42nd ACL.
Charles B. Callaway. 2003. Evaluating coverage for
large symbolic NLG grammars. In In Proceedings of
the 18th IJCAI.
John A. Carroll and Stephan Oepen. 2005. High ef-
ficiency realization for a wide-coverage unification
grammar. In Proceedings of IJCNLP.
Eugene Charniak. 2000. A maximum entropy-inspired
parser. In Proceedings of the 1st NAACL.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach. In
Proceedings of the CoNLL.
275
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Kevin Humphreys, Mike Calcagno, and David Weise.
2001. Reusing a statistical language model for gen-
eration. In Proceedings of the 8th European Workshop
on Natural Language Generation (EWNLG).
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24.
Ronald M. Kaplan and Tracy Holloway King. 2003.
Low-level mark-up and large-scale LFG grammar pro-
cessing. In Proceedings of the Lexical Functional
Grammar Conference.
Ron Kaplan. 1995. The formal architecture of
lexical-functional grammar. In Dalrymple, Kaplan,
Maxwell, and Zaenen, editors, Formal Issues in
Lexical-Functional Grammar, pages 7?27. CSLI Pub-
lications.
Martin Kay. 1996. Chart generation. In Proceedings of
the 34th ACL.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of the 36th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 17th In-
ternational Conference on Computational Linguistics
(ACL-COLING).
Irene Langkilde-Geary. 2002. An empirical verification
of coverage and correctness for a general-purpose sen-
tence generator. In Proceedings of the 2nd INLG.
Irene Langkilde. 2000. Forest-based statistical sentence
generation. In Proceedings of the 1st NAACL.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic models for disambiguation of an
HPSG-based chart generator. In Proceedings of the
9th IWPT.
Joakim Nivre and Jens Nilsson. 2004. Multiword units
in syntactic parsing. In Workshop on Methodologies
and Evaluation of Multiword Units in Real-World Ap-
plications.
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proceedings of
the 1st NAACL.
Stefan Riezler and John T. Maxwell. 2006. Grammat-
ical machine translation. In Proceedings of the 6th
NAACL.
Stefan Riezler, Tracy H. King, Richard Crouch, and An-
nie Zaenen. 2003. Statistical sentence condensation
using ambiguity packing and stochastic disambigua-
tion methods for lexical-functional grammar. In Pro-
ceedings of the 3rd NAACL.
Erik Velldal and Stephan Oepen. 2005. Maximum en-
tropy models for realization ranking. In Proceedings
of the MT-Summit.
Ralph Weischedel and Ada Brunstein, 2005. BBN pro-
noun coreference and entity type corpus. Technical
Report.
276
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 630?639, Prague, June 2007. c?2007 Association for Computational Linguistics
Treebank Annotation Schemes and Parser Evaluation for German
Ines Rehbein
NCLT
School of Computing, DCU,
Dublin, Ireland
irehbein@computing.dcu.ie
Josef van Genabith
NCLT,
School of Computing, DCU,
Dublin, Ireland
IBM Dublin Center for Advanced Studies
josef@computing.dcu.ie
Abstract
Recent studies focussed on the question
whether less-configurational languages like
German are harder to parse than English,
or whether the lower parsing scores are an
artefact of treebank encoding schemes and
data structures, as claimed by Ku?bler et al
(2006). This claim is based on the as-
sumption that PARSEVAL metrics fully re-
flect parse quality across treebank encoding
schemes. In this paper we present new ex-
periments to test this claim. We use the
PARSEVAL metric, the Leaf-Ancestor met-
ric as well as a dependency-based evalua-
tion, and present novel approaches measur-
ing the effect of controlled error insertion
on treebank trees and parser output. We
also provide extensive past-parsing cross-
treebank conversion. The results of the ex-
periments show that, contrary to Ku?bler et
al. (2006), the question whether or not Ger-
man is harder to parse than English remains
undecided.
1 Introduction
A long-standing and unresolved issue in the pars-
ing literature is whether parsing less-configurational
languages is harder than e.g. parsing English. Ger-
man is a case in point. Results from Dubey and
Keller (2003) suggest that state-of-the-art parsing
scores for German are generally lower than those ob-
tained for English, while recent results from Ku?bler
et al (2006) raise the possibility that this might
be an artefact of particular encoding schemes and
data structures of treebanks, which serve as training
resources for probabilistic parsers. Ku?bler (2005)
and Maier (2006) show that treebank annotation
schemes have considerable influence on parsing re-
sults. A comparison of unlexicalised PCFG pars-
ing (Ku?bler, 2005) trained and evaluated on the Ger-
man NEGRA (Skut et al, 1997) and the Tu?Ba-
D/Z (Telljohann et al, 2004) treebanks using LoPar
(Schmid, 2000) shows a difference in parsing results
of about 16%, using the PARSEVAL metric (Black
et al, 1991). Ku?bler et al (2006) conclude that,
contrary to what had been assumed, German is not
actually harder to parse than English, but that the
NEGRA annotation scheme does not support opti-
mal PCFG parsing performance.
Despite being the standard metric for measuring
PCFG parser performance, PARSEVAL has been
criticised for not representing ?real? parser quality
(Carroll et al, 1998; Brisco et al, 2002; Sampson
and Babarbczy, 2003). PARSEVAL checks label and
wordspan identity in parser output compared to the
original treebank trees. It neither weights results,
differentiating between linguistically more or less
severe errors, nor does it give credit to constituents
where the syntactic categories have been recognised
correctly but the phrase boundary is slightly wrong.
With this in mind, we question the assumption
that the PARSEVAL results for NEGRA and Tu?Ba-
D/Z reflect a real difference in quality between the
parser output for parsers trained on the two different
treebanks. As a consequence we also question the
conclusion that PARSEVAL results for German in
the same range as the parsing results for the English
630
Penn-II Treebank prove that German is not harder
to parse than the more configurational English. To
investigate this issue we present experiments on the
German TIGER treebank (Dipper et al, 2001) and
the Tu?Ba-D/Z treebank. TIGER is based on and ex-
tends the NEGRA data and annotation scheme. Our
error insertion and past-parsing treebank-encoding
experiments experiments show that the differences
in parsing results for the two treebanks are not
caused by a higher number of errors in the output
of the parser trained on the TIGER treebank, but are
due to the bias of the PARSEVAL metric towards an-
notation schemes (such as that of Tu?Ba-D/Z) with a
higher ratio of non-terminal/terminal nodes. The ex-
periments also show that compared to PARSEVAL
the Leaf-Ancestor metric is somewhat less suscep-
tible to non-terminal/terminal ratios and that con-
trary to the PARSEVAL results, dependency-based
evaluations score TIGER trained parsers higher than
Tu?Ba-D/Z trained parsers.
This paper is structured as follows: Section 2
gives an overview of the main features of the two
treebanks. Section 3 describes our first experiment,
where we systematically insert controlled errors into
the original treebank trees and compare the influence
of these modifications on the evaluation results in
the PARSEVAL metric and the Leaf-Ancestor met-
ric against the original, unmodified trees for both
treebanks. In Section 4 we present the second ex-
periment, where we extract an unlexicalised PCFG
from each of the treebanks. Then we convert the out-
put of the PCFG parser trained on the Tu?Ba-D/Z into
a TIGER-style format and evaluate the converted
trees. In Section 5 we present a dependency-based
evaluation and compare the results to the results of
the two other measures. The last section concludes.
2 The TIGER Treebank and the Tu?Ba-D/Z
The two German treebanks used in our experiments
are the TIGER Treebank (Release 2) and the Tu?ba-
D/Z (Release 2). The Tu?Ba-D/Z consists of approx-
imately 22 000 sentences, while the TIGER Tree-
bank is much larger with more than 50 000 sen-
tences. Both treebanks contain German newspaper
text and are annotated with phrase structure and de-
pendency (functional) information. Both treebanks
use the Stuttgart Tu?bingen POS Tag Set (Schiller
et al, 95). TIGER uses 49 different grammatical
function labels, while the Tu?Ba-D/Z utilises only
36 function labels. For the encoding of phrasal
node categories the Tu?Ba-D/Z uses 30 different cat-
egories, the TIGER Treebank uses a set of 27 cate-
gory labels.
Other major differences between the two tree-
banks are: in the Tiger Treebank long distance de-
pendencies are expressed through crossing branches
(Figure 1), while in the Tu?Ba-D/Z the same phe-
nomenon is expressed with the help of grammati-
cal function labels (Figure 2), where the node label
V-MOD encodes the information that the PP mod-
ifies the verb. The annotation in the Tiger Tree-
bank is rather flat and allows no unary branching,
whereas the nodes in the Tu?Ba-D/Z do contain unary
branches and a more hierarchical structure, resulting
in a much deeper tree structure than the trees in the
Tiger Treebank. This results in an average higher
number of nodes per sentence for the Tu?Ba-D/Z. Ta-
ble 1 shows the differences in the ratio of nodes for
the Tiger treebank and the Tu?Ba-D/Z.
phrasal phrasal words
nodes/sent nodes/word /sent
TIGER 8.29 0.47 17.60
Tu?Ba-D/Z 20.69 1.20 17.27
Table 1: Average number of phrasal nodes/words in
TIGER and Tu?Ba-D/Z
Figures 1 and 2 also illustrate the different annota-
tion of PPs in both annotation schemes. In the Tiger
treebank the internal structure of the PP is flat and
the adjective and noun inside the PP are directly at-
tached to the PP, while the Tu?Ba-D/Z is more hier-
archical and inserts an additional NP node.
Another major difference is the annotation of
topological fields in the style of Drach (1937) and
Ho?hle (1986) in the Tu?Ba-D/Z. The model captures
German word order, which accepts three possible
sentence configurations (verb first, verb second and
verb last), by providing fields like the initial field
(VF), the middle field (MF) and the final field (NF).
The fields are positioned relative to the verb, which
can fill in the left (LK) or the right sentence bracket
(VC). The ordering of topological fields is deter-
mined by syntactic constraints.
631
Auch mit staatlichen Auftr?agen sieht es schlecht aus.
?It also looks bad for public contracts.?
Figure 1: TIGER treebank tree
In Wales sieht es besser aus.
?Things seem better in Wales.?
Figure 2: Tu?Ba-D/Z treebank tree
2.1 Differences between TIGER and NEGRA
To date, most PCFG parsing for German has
been done using the NEGRA corpus as a train-
ing resource. The flat annotation scheme of the
TIGER treebank is based on the NEGRA anno-
tation scheme, but it also employs some impor-
tant extensions, which include the annotation of
verb-subcategorisation, appositions and parenthe-
ses, coordinations and the encoding of proper nouns
(Brants et al, 2002).
3 Treebank Preprocessing: Converting
TIGER Graphs into CFG Trees
The sentences in the TIGER treebank are repre-
sented as graphs with LDDs expressed through
crossing branches. Before being able to insert er-
rors or extract a PCFG we had to resolve these cross-
ing branches in the TIGER treebank. This was done
by attaching the non-head child nodes higher up in
the tree, following Ku?bler (2006). For the graph
in Figure 1 this would mean that the modifying PP
?Auch mit staatlichen Auftra?gen? (also for public
contracts) was attached directly to the S node, while
the head of the adjectival phrase (AP) remained in
it?s original position. As a side effect this leads to the
creation of some unary nodes in the TIGER trees.
We also inserted a virtual root node and removed
all functional labels from the TIGER and Tu?Ba-D/Z
trees.
4 Experiment I
Experiment I is designed to assess the impact
of identical errors on the two treebank encoding
schemes and the PARSEVAL1 and Leaf-Ancestor
evaluation metrics.
4.1 Experimental Setup
The TIGER treebank and the Tu?Ba-D/Z both con-
tain newspaper text, but from different German
newspapers. To support a meaningful comparison
we have to compare similar sentences from both
treebanks. In order to control for similarity we se-
lected all sentences of length 10 ? n ? 40 from
both treebanks. For all sentences with equal length
we computed the average number of prepositions,
determiners, nouns (and related POS such as proper
names and personal pronouns), interrogative pro-
nouns, finite verbs, infinite verbs, past participles
and imperative verb forms. For each sentence length
we selected all sentences from both treebanks which
showed an average for each of the POS listed above
which did not deviate more than 0.8 from the av-
erage for all sentences for this particular sentence
length. From this set we randomly selected 1024
sentences for each of the treebanks. This results in
two test sets, comparable in word length, syntactic
structure and complexity. Table 2 shows the ratio of
phrasal versus terminal nodes in the test sets.
We then inserted different types of controlled er-
rors automatically into the original treebank trees in
our test sets and evaluated the modified trees against
1In all our experiments we use the evalb metric (Sekine
and Collins, 1997), the most commonly used implementation
of the PARSEVAL metric.
632
phrasal phrasal nodes words
nodes/sent nodes/word /sent
TIGER 6.97 0.48 14.49
Tu?Ba-D/Z 19.18 1.30 14.75
Table 2: Average number of phrasal nodes/words in
the TIGER and Tu?Ba-D/Z test set
the original treebank trees, in order to assess the im-
pact of similar (controlled for type and number) er-
rors on the two encoding schemes.
4.2 Error Insertion
The errors fall into three types: attachment, span and
labeling (Table 3). We carried out the same number
of error insertions in both test sets.
Error description
ATTACH I Attach PPs inside an NP one level
higher up in the tree
ATTACH II Change verb attachment to noun
attachment for PPs on sentence level,
inside a VP or in the MF (middle field)
LABEL I Change labels of PPs to NP
LABEL II Change labels of VPs to PP
SPAN I Include adverb to the left of a PP
into the PP
SPAN II Include NN to the left of a PP
into the PP
SPAN III Combination of SPANI and SPANII
Table 3: Description of inserted error types
4.3 Results for Error Insertion for the Original
Treebank Trees
Table 4 shows the impact of the error insertion into
the original treebank trees on PARSEVAL results,
evaluated against the gold trees. PARSEVAL results
in all experiments report labelled precision and re-
call. The first error (PP attachment I, 85 insertions
in each test set) leads to a decrease in f-score of 1.16
for the TIGER test set, while for the Tu?Ba-D/Z test
set the same error only caused a decrease of 0.43.
The effect remains the same for all error types and
is most pronounced for the category label errors, be-
cause the frequency of the labels resulted in a large
number of substitutions. The last row lists the total
weighted average for all error types, weighted with
respect to their frequency of occurrence in the test
sets.
Table 4 clearly shows that the PARSEVAL
measure punishes the TIGER treebank annotation
TIGER Tu?Ba # errors
PP attachment I 98.84 99.57 85
PP attachment II 98.75 99.55 89
Label I 80.02 92.73 1427
Label II 93.00 97.45 500
SPAN I 99.01 99.64 71
SPAN II 97.47 99.08 181
SPAN III 96.51 98.73 252
total weighted ave. 87.09 95.30
Table 4: f-score for PARSEVAL results for error in-
sertion in the original treebank trees
scheme to a greater extent, while the same num-
ber and type of errors in the Tu?Ba-D/Z annotation
scheme does not have an equally strong effect on
PARSEVAL results for similar sentences.
4.4 Discussion: PARSEVAL and LA
Experiment I shows that the gap between the PAR-
SEVAL results for the two annotation schemes does
not reflect a difference in quality between the trees.
Both test sets contain the same number of sentences
with the same sentence length and are equivalent in
complexity and structure. They contain the same
number and type of errors. This suggests that the
difference between the results for the TIGER and
the Tu?Ba-D/Z test set are due to the higher ratio of
non-terminal/terminal nodes in the Tu?Ba-D/Z trees
(Table 1).
In order to obtain an alternative view on the
quality of our annotation schemes we used the
leaf-ancestor (LA) metric (Sampson and Babarbczy,
2003), a parser evaluation metric which measures
the similarity of the path from each terminal node
in the parse tree to the root node. The path con-
sists of the sequence of node labels between the ter-
minal node and the root node, and the similarity of
two paths is calculated by using the Levenshtein dis-
tance (Levenshtein, 1966). Table 5 shows the results
for the leaf-ancestor evaluation metric for our error
insertion test sets. Here the weighted average re-
sults for the two test sets are much closer to each
other (94.98 vs. 97.18 as against 87.09 vs. 95.30).
Only the label errors, due to the large numbers, show
a significant difference between the two annotation
schemes. Tables 4 and 5 show that compared to
PARSEVAL the LA metric is somewhat less sensi-
tive to the nonterminal/terminal ratio.
Figure 3 illustrates the different behaviour of the
633
TIGER Tu?Ba # errors
PP attachment I 99.62 99.70 85
PP attachment II 99.66 99.78 89
Label I 92.45 95.24 1427
Label II 96.05 99.28 500
SPAN I 99.82 99.84 71
SPAN II 99.51 99.77 181
SPAN III 99.34 99.62 252
total weighted ave. 94.98 97.18
Table 5: LA results for error insertion in the original
treebank trees
two evaluation metrics with respect to an example
sentence.
Sentence 9:
Die Stadtverwaltung von Venedig hat erstmals streunende
Katzen gez?ahlt.
?For the first time the city council of Venice has counted stray-
ing cats.?
(TOP
(S
(NP
(ART Die [the] )
(NN Stadtverwaltung [city counsil] )
(PP
(APPR von [of] )
(NE Venedig [Venice] )
)
)
(VAFIN hat [has] )
(VP
(ADV erstmals [for the first time] )
(NP
(ADJA streunende [straying] )
(NN Katzen [cats] )
)
(VVPP geza?hlt [counted] )
)
)
($. .)
)
Figure 3: Sentence 9 from the TIGER Test Set
Table 6 shows that all error types inserted into
Sentence 9 in our test set result in the same eval-
uation score for the PARSEVAL metric, while the
LA metric provides a more discriminative treatment
of PP attachment errors, label errors and span errors
for the same sentence (Table 6). However, the dif-
ferences in the LA results are only indirectly caused
by the different error types. They actually reflect
the number of terminal nodes affected by the error
insertion. For Label I and II the LA results vary
considerably, because the substitution of the PP for
an NP (Label I) in Figure 3 affects two terminal
nodes only (PP von [of] Venedig [Venice]), while
the change of the VP into a PP (Label II) alters
the paths of four terminal nodes (VP erstmals [for
the first time] streunende [straying] Katzen [cats]
geza?hlt [counted]) and therefore has a much greater
impact on the overall result for the sentence.
ERROR PARSEVAL LA
PP attachment I 83.33 96.30
Label I 83.33 96.00
Label II 83.33 91.00
SPAN II 83.33 96.40
Table 6: Evaluation results for Sentence 9
The Tu?Ba-D/Z benefits from its overall higher ra-
tio of nodes per sentence, resulting in a higher ratio
of non-terminal/terminal nodes per phrase and the
effect, that the inserted label error affects a smaller
number of terminal nodes than in the TIGER test set
for LA testing.
5 Experiment II
Ku?bler (2005) and Maier (2006) assess the impact of
the different treebank annotation schemes on PCFG
parsing by conducting a number of modifications
converting the Tu?Ba-D/Z into a format more sim-
ilar to the NEGRA (and hence TIGER) treebank.
After each modification they extract a PCFG from
the modified treebank and measure the effect of the
changes on parsing results. They show that with
each modification transforming the Tu?Ba-D/Z into
a more NEGRA-like format the parsing results also
become more similar to the results of the NEGRA
treebank, i.e. the results get worse. Maier takes this
as evidence that the Tu?Ba-D/Z is more adequate for
PCFG parsing. This assumption is based on the be-
lief that PARSEVAL results fully reflect parse qual-
ity across different treebank encoding schemes. This
is not always true, as shown in Experiment I.
In our second experiment we crucially change the
order of events in the Ku?bler (2005), Maier (2006)
and Ku?bler et al (2006) experiments: We first ex-
tract an unlexicalised PCFG from each of the orig-
inal treebanks. We then transform the output of
the parser trained on the Tu?Ba-D/Z into a format
more similar to the TIGER Treebank. In contrast to
Ku?bler (2005) and Maier (2006), who converted the
634
treebank before extracting the grammars in order to
measure the impact of single features like topologi-
cal fields or unary nodes on PCFG parsing, we con-
vert the trees in the parser output of a parser trained
on the original unconverted treebank resources. This
allows us to preserve the basic syntactic structure
and also the errors present in the output trees re-
sulting from a potential bias in the original tree-
bank training resources. The results for the original
parser output evaluated against the unmodified gold
trees should not be crucially different from the re-
sults for the modified parser output evaluated against
the modified gold trees.
5.1 Experimental Setup
For Experiment II we trained BitPar (Schmid, 2004),
a parser for highly ambiguous PCFG grammars, on
the two treebanks. The Tu?Ba-D/Z training data con-
sists of the 21067 treebank trees not included in the
Tu?Ba-D/Z test set. Because of the different size of
the two treebanks we selected 21067 sentences from
the TIGER treebank, starting from sentence 10000
(and excluding the sentences in the TIGER test set).
Before extracting the grammars we resolved the
crossing branches in the TIGER treebank as de-
scribed in Section 3. After this preprocessing step
we extracted an unlexicalised PCFG from each of
our training sets. Our TIGER grammar has a total of
21163 rule types, while the grammar extracted from
the Tu?Ba-D/Z treebank consists of 5021 rules only.
We parsed the TIGER and Tu?Ba-D/Z test set with
the extracted grammars, using the gold POS tags for
parser input. We then automatically converted the
Tu?Ba-D/Z output to a TIGER-like format and com-
pare the evaluation results for the unmodified trees
against the gold trees with the results for the con-
verted parser output against the converted gold trees.
5.2 Converting the Tu?Ba-D/Z Trees
The automatic conversion of the Tu?Ba-D/Z-style
trees includes the removal of topological fields and
unary nodes as well as the deletion of NPs inside
of PPs, because the NP child nodes are directly at-
tached to the PP in the TIGER annotation scheme.
As a last step in the conversion process we adapted
the Tu?Ba-D/Z node labels to the TIGER categories.
5.2.1 The Conversion Process: An Example
We demonstrate the conversion process using an
example sentence from the Tu?Ba-D/Z test set (Fig-
ure 4). The converted tree is given in Figure 5:
topological fields, here VF (initial field), MF (mid-
dle field) and LK (left sentence bracket), as well as
unary nodes have been removed. The category la-
bels have been changed to TIGER-style annotation.
Erziehungsurlaub nehmen bisher nur zwei Prozent der M?anner.
?Until now only two percent of the men take parental leave.?
Figure 4: Original Tu?Ba-D/Z-style gold tree
Figure 5: Converted TIGER-style gold tree
Figure 6 shows the unmodified parser output from
the Tu?Ba-D/Z trained grammar for the same string.
The parser incorrectly included all adverbs inside an
NP governed by the PP, while in the gold tree (Figure
4) both adverbs are attached to the PP. The modified
parser output is shown in Figure 7.
5.3 Results for Converted Parser Output
We applied the conversion method described above
to the original trees and the parser output for the sen-
tences in the TIGER and the Tu?Ba-D/Z test sets. Ta-
ble 7 shows PARSEVAL and LA results for the mod-
ified trees, evaluating the converted parser output
635
Figure 6: Parser output (Tu?Ba-D/Z grammar)
Figure 7: Converted parser output (Tu?Ba-D/Z)
for each treebank against the converted gold trees
of the same treebank. Due to the resolved crossing
branches in the TIGER treebank we also have some
unary nodes in the TIGER test set. Their removal
surprisingly improves both PARSEVAL and LA re-
sults. For the Tu?Ba-D/Z all conversions lead to a
decrease in precision and recall for the PARSEVAL
metric. Converting the trees parsed by the Tu?Ba-
D/Z grammar to a TIGER-like format produces an f-
score which is slightly lower than that for the TIGER
trees. The same is true for the LA metric, but not to
the same extent as for PARSEVAL. The LA met-
ric also gives slightly better results for the original
TIGER trees compared to the result for the unmodi-
fied Tu?Ba-D/Z trees.
The constant decrease in PARSEVAL results for
the modified trees is consistent with the results in
Ku?bler et al (2005), but our conclusions are slightly
different. Our experiment shows that the Tu?Ba-
D/Z annotation scheme does not generally produce
higher quality parser output, but that the PARSE-
VAL results are highly sensitive to the ratio of non-
terminal/terminal nodes. However, the parser output
for the grammar trained on the Tu?Ba-D/Z yields a
EVALB LA
prec. recall f-sco. avg.
TIGER 83.54 83.65 83.59 94.69
no Unary 84.33 84.48 84.41 94.83
Tu?Ba-D/Z 92.59 89.79 91.17 94.23
Tu?Ba-D/Z? TIGER
no Top 92.38 88.76 90.53 93.93
no Unary 89.96 85.67 87.76 93.59
no Top + no U. 88.44 82.24 85.23 92.91
no Top + no U. 87.15 79.52 83.16 92.47
+ no NP in PP
Table 7: The impact of the conversion process on
PARSEVAL and LA
higher precision in the PARSEVAL metric against
the Tu?Ba-D/Z gold trees than the parser output of
the TIGER grammar against the TIGER gold trees.
For PARSEVAL recall, the TIGER grammar gives
better results.
6 Experiment III
In Experiment I and II we showed that the tree-
based PARSEVAL metric is not a reliable measure
for comparing the impact of different treebank an-
notation schemes on the quality of parser output and
that the issue, whether German is harder to parse
than English, remains undecided. In Experiment III
we report a dependency-based evaluation and com-
pare the results to the results of the other metrics.
6.1 Dependency-Based (DB) Evaluation
The dependency-based evaluation used in the exper-
iments follows the method of Lin (1998) and Ku?bler
and Telljohann (2002), converting the original tree-
bank trees and the parser output into dependency re-
lations of the form WORD POS HEAD. Functional
labels have been omitted for parsing, therefore the
dependencies do not comprise functional informa-
tion. Figure 8 shows the original TIGER Treebank
representation for the CFG tree in Figure 3. Square
boxes denote grammatical functions. Figure 9 shows
the dependency relations for the same tree, indicated
by labelled arrows. Converted into a WORD POS
HEAD triple format the dependency tree looks as
follows (Table 8).
Following Lin (1998), our DB evaluation algo-
rithm computes precision and recall:
? Precision: the percentage of dependency re-
lationships in the parser output that are also
636
Figure 8: TIGER treebank representation for Figure 3
SB
NKPGNK NK OA
MO
OC
the  city counsil   of   Venice  has  for the    straying   cats  counted
                                      first time
Die    Stadtverwaltung    von    Venedig    hat     erstmals       streunende    Katzen    gez?hlt    
?For the first time the city counsil of Venice has counted straying cats.?
Figure 9: Dependency relations for Figure 8
found in the gold triples
? Recall: the percentage of dependency relation-
ships in the gold triples that are also found in
the parser output triples.
WORD POS HEAD
Die [the] ART Stadtverwaltung
Stadtverwaltung NN hat
[city counsil]
von [of] APPR Stadtverwaltung
Venedig [Venice] NE von
hat [has] VAFIN -
erstmals ADV geza?hlt
[for the first time]
streunende [straying] ADJA Katzen
Katzen [cats] NN geza?hlt
geza?hlt [counted] VVPP hat
Table 8: Dependency triples for Figure 9
We assessed the quality of the automatic conver-
sion methodology by converting the 1024 original
trees from each of our test sets into dependency rela-
tions, using the functional labels in the original trees
to determine the dependencies. Topological fields
in the Tu?Ba-D/Z test set have been removed before
extracting the dependency relationships.
We then removed all functional information from
the trees and converted the stripped trees into depen-
dencies, using heuristics to find the head. We eval-
uated the dependencies for the stripped gold trees
against the dependencies for the original gold trees
including functional labels and obtained an f-score
of 99.64% for TIGER and 99.13% for the Tu?Ba-D/Z
dependencies. This shows that the conversion is re-
liable and not unduly biased to either the TIGER or
Tu?Ba-D/Z annotation schemes.
6.2 Experimental Setup
For Experiment III we used the same PCFG gram-
mars and test sets as in Experiment II. Before ex-
tracting the dependency relationships we removed
the topological fields in the Tu?Ba-D/Z parser output.
As shown in Section 6.1, this does not penalise the
dependency-based evaluation results for the Tu?Ba-
D/Z. In contrast to Experiment II we used raw text
as parser input instead of the gold POS tags, allow-
637
ing a comparison with the gold tag results in Table 7.
6.3 Results
Table 9 shows the evaluation results for the three
different evaluation metrics. For the DB evalua-
tion the parser trained on the TIGER training set
achieves about 7% higher results for precision and
recall than the parser trained on the Tu?Ba-D/Z. This
result is clearly in contrast to the PARSEVAL scores,
which show higher results for precision and recall
for the Tu?Ba-D/Z. But contrary to the PARSEVAL
results on gold POS tags as parser input (Table 7),
the gap between the results for TIGER and Tu?Ba-
D/Z is not as wide as before. PARSEVAL gives
a labelled bracketing f-score of 81.12% (TIGER)
and 85.47% (Tu?Ba-D/Z) on raw text as parser in-
put, while the results on gold POS tags are more dis-
tinctive with an f-score of 83.59% for TIGER and
91.17% for Tu?Ba-D/Z. The LA results again give
better scores to the TIGER parser output, this time
the difference is more pronounced than for Experi-
ment II (Table 7).
Dependencies PARSEVAL LA
Prec Rec Prec Rec Avg
TIGER 85.71 85.72 81.21 81.04 93.88
Tu?Ba 76.64 76.63 87.24 83.77 92.58
Table 9: Parsing results for three evaluation metrics
The considerable difference between the results
for the metrics raises the question which of the met-
rics is the most adequate for judging parser output
quality across treebank encoding schemes.
7 Conclusions
In this paper we presented novel experiments assess-
ing the validity of parsing results measured along
different dimensions: the tree-based PARSEVAL
metric, the string-based Leaf-Ancestor metric and
a dependency-based evaluation. By inserting con-
trolled errors into gold treebank trees and measuring
the effects on parser evaluation results we gave new
evidence for the downsides of PARSEVAL which,
despite severe criticism, is still the standard mea-
sure for parser evaluation. We showed that PAR-
SEVAL cannot be used to compare the output of
PCFG parsers trained on different treebank anno-
tation schemes, because the results correlate with
the ratio of non-terminal/terminal nodes. Compar-
ing two different annotation schemes, PARSEVAL
consistently favours the one with the higher node ra-
tio.
We examined the influence of treebank annotation
schemes on unlexicalised PCFG parsing, and re-
jected the claim that the German Tu?Ba-D/Z treebank
is more appropriate for PCFG parsing than the Ger-
man TIGER treebank and showed that converting
the Tu?Ba-D/Z trained parser output to a TIGER-like
format leads to PARSEVAL results slightly worse
than the ones for the TIGER treebank trained parser.
Additional evidence comes from a dependency-
based evaluation, showing that, for the output of the
parser trained on the TIGER treebank, the mapping
from the CFG trees to dependency relations yields
better results than for the grammar trained on the
Tu?Ba-D/Z annotation scheme, even though PARSE-
VAL scores suggest that the TIGER-based parser
output trees are substantial worse than Tu?Ba-D/Z-
based parser output trees.
We have shown that different treebank annotation
schemes have a strong impact on parsing results for
similar input data with similar (simulated) parser er-
rors. Therefore the question whether a particular
language is harder to parse than another language
or not, can not be answered by comparing parsing
results for parsers trained on treebanks with differ-
ent annotation schemes. Comparing PARSEVAL-
based parsing results for a parser trained on the
Tu?Ba-D/Z or TIGER to results achieved by a parser
trained on the English Penn-II treebank (Marcus
et al, 1994) does not provide conclusive evidence
about the parsability of a particular language, be-
cause the results show a bias introduced by the
combined effect of annotation scheme and evalua-
tion metric. This means that the question whether
German is harder to parse than English, is still
undecided. A possible way forward is perhaps a
dependency-based evaluation of TIGER/Tu?Ba-D/Z
with Penn-II trained grammars for ?similar? test and
training sets and cross-treebank and -language con-
trolled error insertion experiments. Even this is not
entirely straightforward as it is not completely clear
what constitutes ?similar? test/training sets across
languages. We will attempt to pursue this in further
research.
638
Acknowledgements
We would like to thank the anomymous reviewers
for many helpful comments. This research has been
supported by a Science Foundation Ireland grant
04|IN|I527.
References
Black, E., S. P. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. P. Marcus, S. Roukos, B.
Santorini, and T. Strzalkowski. 1991. A procedure for
quantitatively comparing the syntactic coverage of En-
glish grammars. In Proceedings DARPA Speech and
Natural Language Workshop, Pacific Grove, CA, pp.
306-311.
Brants, Sabine, and Silvia Hansen. 2002. Developments
in the TIGER Annotation Scheme and their Realiza-
tion in the Corpus. In Proceedings of the Third Confer-
ence on Language Resources and Evaluation (LREC
2002), pp. 1643-1649 Las Palmas.
Briscoe, E. J., J. A. Carroll, and A. Copestake. 2002.
Relational evaluation schemes. In Proceedings Work-
shop ?Beyond Parseval - towards improved evaluation
measures for parsing systems?, 3rd International Con-
ference on Language Resources and Evaluation, pp.
4-38. Las Palmas, Canary Islands.
Carroll, J., E. Briscoe and A. Sanfilippo. 1998. Parser
evaluation: a survey and a new proposal. In Proceed-
ings of the 1st International Conference on Language
Resources and Evaluation, Granada, Spain. 447-454.
Dipper, S., T. Brants, W. Lezius, O. Plaehn, and G. Smith.
2001. The TIGER Treebank. In Third Workshop on
Linguistically Interpreted Corpora LINC-2001, Leu-
ven, Belgium.
Drach, Erich. 1937. Grundgedanken der Deutschen Sat-
zlehre. Frankfurt/M.
Dubey, A., and F. Keller. 2003. Probabilistic parsing for
German using sisterhead dependencies. In Proceed-
ings of the 41st Annual Meeting of the Association for
Computational Linguistics, Sapporo, Japan.
Ho?hle, Tilman. 1998. Der Begriff ?Mittelfeld?, An-
merkungen u?ber die Theorie der topologischen Felder.
In Akten des Siebten Internationalen Germansitenkon-
gresses 1985, pages 329-340, Go?ttingen, Germany.
Ku?bler, Sandra, and Heike Telljohann. 2002. Towards
a Dependency-Oriented Evaluation for Partial Pars-
ing. In Proceedings of Beyond PARSEVAL ? Towards
Improved Evaluation Measures for Parsing Systems
(LREC 2002 Workshop), Las Palmas, Gran Canaria,
June 2002.
Lin, Dekang. 1998. A dependency-based method for
evaluating broad-coverage parsers. Natural Language
Engineering, 1998.
Ku?bler, Sandra. 2005. How Do Treebank Annotation
Schemes Influence Parsing Results? Or How Not to
Compare Apples And Oranges. In Proceedings of
FANLP 2005), Borovets, Bulgaria, September 2005.
Ku?bler, Sandra, Erhard Hinrichs, and Wolfgang Maier.
2006. Is it Really that Difficult to Parse German?
In Proceedings of the 2006 Conference on Empiri-
cal Methods in Natural Language Processing, EMNLP
2006), Sydney, Australia, July 2006.
Levenshtein, V. I. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet Physics
- Doklady, 10.707-10 (translation of Russian original
published in 1965).
Maier, Wolfgang. 2006. Annotation Schemes and
their Influence on Parsing Results. In Proceedings of
the COLING/ACL 2006 Student Research Workshop),
Sydney, Australia, July 2006.
Marcus, M., G. Kim, M. A. Marcinkiewicz, R. MacIn-
tyre, M. Ferguson, K. Katz and B. Schasberger. 1994.
The Penn Treebank: Annotating Predicate Argument
Structure. In Proceedings of the ARPA Human Lan-
guage Technology Workshop, Princeton, NJ.
Sampson, Geoffrey, and Anna Babarczy. 2003. A test
of the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9 (4):365-380.
Schmid, Helmut. 2000. LoPar: Design and Implemen-
tation. Arbeitspapiere des Sonderforschungsbereiches
340, No. 149, IMS Stuttgart, July 2000.
Schmid, Helmut. 2004. Efficient Parsing of Highly Am-
biguous Context-Free Grammars with Bit Vectors. In
Proceedings of the 20th International Conference on
Computational Linguistics (COLING 2004), Geneva,
Switzerland.
Sekine, S. and M. J. Collins. 1997. The evalb software.
http://nlp.cs.nyu.edu/evalb/
Skut, Wojciech, Brigitte Krann, Thorsten Brants, and
Hans Uszkoreit. 1997. An annotation scheme for free
word order languages. In Proceedings of ANLP 1997,
Washington, D.C.
Telljohann, Heike, Erhard W. Hinrichs, Sandra Ku?bler,
and Heike Zinsmeister. 2005. Stylebook for
the Tu?bingen Treebank of Written German (Tu?Ba-
D/Z). Seminar fu?r Sprachwissenschaft, Universita?t
Tu?bingen, Germany.
Schiller, Anne, Simone Teufel, and Christine Thielen.
1995. Guidelines fr das Tagging deutscher Textcor-
pora mit STTS. Technical Report, IMS-CL, University
Stuttgart, 1995.
639
Large-Scale Induction and Evaluation of
Lexical Resources from the Penn-II and
Penn-III Treebanks
Ruth O?Donovan?
Dublin City University
Michael Burke??
Dublin City University
Aoife Cahill?
Dublin City University
Josef van Genabith??
Dublin City University
Andy Way??
Dublin City University
We present a methodology for extracting subcategorization frames based on an automatic
lexical-functional grammar (LFG) f-structure annotation algorithm for the Penn-II and
Penn-III Treebanks. We extract syntactic-function-based subcategorization frames (LFG
semantic forms) and traditional CFG category-based subcategorization frames as well as
mixed function/category-based frames, with or without preposition information for obliques
and particle information for particle verbs. Our approach associates probabilities with frames
conditional on the lemma, distinguishes between active and passive frames, and fully
reflects the effects of long-distance dependencies in the source data structures. In contrast
to many other approaches, ours does not predefine the subcategorization frame types extracted,
learning them instead from the source data. Including particles and prepositions, we extract
21,005 lemma frame types for 4,362 verb lemmas, with a total of 577 frame types and an
average of 4.8 frame types per verb. We present a large-scale evaluation of the complete
set of forms extracted against the full COMLEX resource. To our knowledge, this is
the largest and most complete evaluation of subcategorization frames acquired automatically
for English.
1. Introduction
In modern syntactic theories (e.g., lexical-functional grammar [LFG] [Kaplan and
Bresnan 1982; Bresnan 2001; Dalrymple 2001], head-driven phrase structure gram-
mar [HPSG] [Pollard and Sag 1994], tree-adjoining grammar [TAG] [Joshi 1988], and
combinatory categorial grammar [CCG] [Ades and Steedman 1982]), the lexicon is
the central repository for much morphological, syntactic, and semantic information.
? National Centre for Language Technology, School of Computing, Dublin City University, Glasnevin,
Dublin 9, Ireland. E-mail: {rodonovan,mburke,acahill,josef,away}@computing.dcu.ie.
? Centre for Advanced Studies, IBM, Dublin, Ireland.
Submission received: 19 March 2004; revised submission received: 18 December 2004; accepted for
publication: 2 March 2005.
? 2005 Association for Computational Linguistics
Computational Linguistics Volume 31, Number 3
Extensive lexical resources, therefore, are crucial in the construction of wide-coverage
computational systems based on such theories.
One important type of lexical information is the subcategorization requirements
of an entry (i.e., the arguments a predicate must take in order to form a grammatical
construction). Lexicons, including subcategorization details, were traditionally pro-
duced by hand. However, as the manual construction of lexical resources is time con-
suming, error prone, expensive, and rarely ever complete, it is often the case that the
limitations of NLP systems based on lexicalized approaches are due to bottlenecks in
the lexicon component. In addition, subcategorization requirements may vary across
linguistic domain or genre (Carroll and Rooth 1998). Manning (1993) argues that, aside
from missing domain-specific complementation trends, dictionaries produced by hand
will tend to lag behind real language use because of their static nature. Given these
facts, research on automating acquisition of dictionaries for lexically based NLP sys-
tems is a particularly important issue.
Aside from the extraction of theory-neutral subcategorization lexicons, there has
also been work in the automatic construction of lexical resources which comply
with the principles of particular linguistic theories such as LTAG, CCG, and HPSG
(Chen and Vijay-Shanker 2000; Xia 1999; Hockenmaier, Bierner, and Baldridge 2004;
Nakanishi, Miyao, and Tsujii 2004). In this article we present an approach to auto-
mating the process of lexical acquisition for LFG (i.e., grammatical-function-based sys-
tems). However, our approach also generalizes to CFG category-based approaches. In
LFG, subcategorization requirements are enforced through semantic forms specifying
which grammatical functions are required by a particular predicate. Our approach is
based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and
Way 1999) and recent progress in automatically annotating the Penn-II and Penn-III
Treebanks with LFG f-structures (Cahill et al 2002; Cahill, McCarthy, et al 2004). Our
technique requires a treebank annotated with LFG functional schemata. In the early
approach of van Genabith, Sadler, and Way (1999), this was provided by manually
annotating the rules extracted from the publicly available subset of the AP Treebank to
automatically produce corresponding f-structures. If the f-structures are of high qual-
ity, reliable LFG semantic forms can be generated quite simply by recursively reading
off the subcategorizable grammatical functions for each local PRED value at each level of
embedding in the f-structures. The work reported in van Genabith, Sadler, and Way
(1999) was small scale (100 trees) and proof of concept and required considerable
manual annotation work. It did not associate frames with probabilities, discriminate
between frames for active and passive constructions, properly reflect the effects of
long-distance dependencies (LDDs), or include CFG category information. In this
article we show how the extraction process can be scaled to the complete Wall
Street Journal (WSJ) section of the Penn-II Treebank, with about one million words
in 50,000 sentences, based on the automatic LFG f-structure annotation algorithm
described in Cahill et al (2002) and Cahill, McCarthy, et al (2004). More recently
we have extended the extraction approach to the larger, domain-diverse Penn-III
Treebank. Aside from the parsed WSJ section, this version of the treebank contains
parses for a subsection of the Brown corpus (almost 385,000 words in 24,000 trees)
taken from a variety of text genres.1 In addition to extracting grammatical-function-
1 For the remainder of this work, when we refer to the Penn-II Treebank, we mean the parse-annotated WSJ,
and when we refer to the Penn-III Treebank, we mean the parse-annotated WSJ and Brown corpus
combined.
330
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
based subcategorization frames, we also include the syntactic categories of the predicate
and its subcategorized arguments, as well as additional details such as the prepositions
required by obliques and particles accompanying particle verbs. Our method discrim-
inates between active and passive frames, properly reflects LDDs in the source data
structures, assigns conditional probabilities to the semantic forms associated with each
predicate, and does not predefine the subcategorization frames extracted.
In Section 2 of this article, we briefly outline LFG, presenting typical lexical entries
and the encoding of subcategorization information. Section 3 reviews related work in
the area of automatic subcategorization frame extraction. Our methodology and its
implementation are presented in Section 4. In Section 5 we present results from the
extraction process. We evaluate the complete induced lexicon against the COMLEX
resource (Grishman, MacLeod, and Meyers 1994) and present the results in Section 6.
To our knowledge, this is by far the largest and most complete evaluation of subcat-
egorization frames automatically acquired for English. In Section 7, we examine the
coverage of our lexicon in regard to unseen data and the rate at which new lexical
entries are learned. Finally, in Section 8 we conclude and give suggestions for future
work.
2. Subcategorization in LFG
Lexical functional grammar (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple
2001) is a member of the family of constraint-based grammars. It posits minimally
two levels of syntactic representation:2 c(onstituent)-structure encodes details of sur-
face syntactic constituency, whereas f(unctional)-structure expresses abstract syntactic
information about predicate?argument?modifier relations and certain morphosyntactic
properties such as tense, aspect, and case. C-structure takes the form of phrase structure
trees and is defined in terms of CFG rules and lexical entries. F-structure is pro-
duced from functional annotations on the nodes of the c-structure and implemented
in terms of recursive feature structures (attribute?value matrices). This is exemplified
by the analysis of the string The inquiry soon focused on the judge (wsj 0267 72) using
the grammar in Figure 1, which results in the annotated c-structure and f-structure in
Figure 2.
The value of the PRED attribute in an f-structure is a semantic form ??gf1, gf2, . . . ,
gfn?, where ? is a lemma and gf a grammatical function. The semantic form provides
an argument list ?gf1,gf2, . . . ,gfn? specifying the governable grammatical functions (or
arguments) required by the predicate to form a grammatical construction. In Figure 1
the verb FOCUS requires a subject and an oblique object introduced by the preposition
on: FOCUS?(? SUBJ)(? OBLon)?. The argument list can be empty, as in the PRED value
for judge in Figure 1. According to Dalrymple (2001), LFG assumes the following uni-
versally available inventory of grammatical functions: SUBJ(ect), OBJ(ect), OBJ?, COMP,
XCOMP, OBL(ique)?, ADJ(unct), XADJ. OBJ? and OBL? represent families of grammatical
functions indexed by their semantic role, represented by the theta subscript. This list
of grammatical functions is divided into governable (subcategorizable) grammatical
functions (arguments) and nongovernable (nonsubcategorizable) grammatical func-
tions (modifiers/adjuncts), as summarized in Table 1.
2 LFGs may also involve morphological and semantic levels of representation.
331
Computational Linguistics Volume 31, Number 3
Figure 1
Sample LFG rules and lexical entries.
A number of languages allow the possibility of object functions in addition to the
primary OBJ, such as the second or indirect object in English. Oblique arguments are
realized as prepositional phrases in English. COMP, XCOMP, and XADJ are all clausal
functions which differ in the way in which they are controlled. A COMP is a closed
function which contains its own internal SUBJ:
The judge thinks [COMP that it will resume].
XCOMP and XADJ are open functions not requiring an internal SUBJ. The subject is
instead specified externally in the matrix phrase:
The judge wants [XCOMP to open an inquiry].
While many linguistic theories state subcategorization requirements in terms
of phrase structure (CFG categories), Dalrymple (2001) questions the viability and
universality of such an approach because of the variety of ways in which grammatical
functions may be realized at the language-specific constituent structure level. LFG
argues that subcategorization requirements are best stated at the f-structure level,
in functional rather than phrasal terms. This is because of the assumption that
abstract grammatical functions are primitive concepts as opposed to derivatives
332
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Figure 2
C- and f-structures for Penn Treebank sentence wsj 0267 72, The inquiry soon focused on the judge.
of phrase structural position. In LFG, the subcategorization requirements of a
particular predicate are expressed by its semantic form: FOCUS?(? SUBJ)(? OBLon)? in
Figure 1.
The subcategorization requirements expressed by semantic forms are enforced at
f-structure level through completeness and coherence well-formedness conditions on
f-structure (Kaplan and Bresnan 1982):
An f-structure is locally complete iff it contains all the governable grammatical
functions that its predicate governs. An f-structure is complete iff it and all its
subsidiary f-structures are locally complete. An f-structure is locally coherent iff
all the governable grammatical functions that it contains are governed by a
local predicate. An f-structure is coherent iff it and all its subsidiary f-structures
are locally coherent. (page 211)
Consider again the f-structure in Figure 2. The semantic form associated with
the verb focus is FOCUS?(? SUBJ)(? OBLon)?. The f-structure is locally complete, as it
contains the SUBJ and an OBL with the preposition on specified by the semantic
form. The f-structure also satisfies the coherence condition, as it does not contain
any governable grammatical functions other than the SUBJ and OBL required by the
local PRED.
333
Computational Linguistics Volume 31, Number 3
Table 1
Governable and nongovernable grammatical functions in LFG.
Governable GFs Nongovernable GFs
SUBJ ADJ
OBJ XADJ
XCOMP
COMP
OBJ?
OBL?
Because of the specific form of the LFG lexicon, our extraction approach differs in
interesting ways from that of previous lexical extraction experiments. This contrast is
made evident in Sections 3 and 4.
3. Related Work
The encoding of verb subcategorization properties is an essential step in the
construction of computational lexicons for tasks such as parsing, generation, and
machine translation. Creating such a resource by hand is time consuming and error
prone, requires considerable linguistic expertise, and is rarely if ever complete. In
addition, a hand-crafted lexicon cannot be easily adapted to specific domains or
account for linguistic change. Accordingly, many researchers have attempted to
construct lexicons automatically, especially for English. In this section, we discuss
approaches to CFG-based subcategorization frame extraction as well as attempts to
induce lexical resources which comply with specific linguistic theories or express
information in terms of more abstract predicate-argument relations. The evaluation of
these approaches is discussed in greater detail in Section 6, in which we compare our
results with those reported elsewhere in the literature.
We will divide more-general approaches to subcategorization frame acquisition
into two groups: those which extract information from raw text and those which
use preparsed and hand-corrected treebank data as their input. Typically in the
approaches based on raw text, a number of subcategorization patterns are predefined,
a set of verb subcategorization frame associations are hypothesized from the data,
and statistical methods are applied to reliably select hypotheses for the final lexicon.
Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators
of six predefined subcategorization frames. The frames do not include details of specific
prepositions. Brent used hypothesis testing on binomial frequency data to statistically
filter the induced frames. Ushioda et al (1993) run a finite-state NP parser on a
POS-tagged corpus to calculate the relative frequency of the same six subcategoriza-
tion verb classes. The experiment is limited by the fact that all prepositional phrases
are treated as adjuncts. Ushioda et al (1993) employ an additional statistical method
based on log-linear models and Bayes? theorem to filter the extra noise introduced by
the parser and were the first to induce relative frequencies for the extracted frames.
Manning (1993) attempts to improve on the approach of Brent (1993) by passing raw
text through a stochastic tagger and a finite-state parser (which includes a set of
simple rules for subcategorization frame recognition) in order to extract verbs and
the constituents with which they co-occur. He assumes 19 different subcategorization
334
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
frame definitions, and the extracted frames include details of specific prepositions.
The extracted frames are noisy as a result of parser errors and so are filtered using
the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique
to approximately four million words of New York Times newswire, Manning acquired
4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames
per verb. Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames,
obtained by manually merging the classes exemplified in the COMLEX (MacLeod,
Grishman, and Meyers 1994) and ANLT (Boguraev et al 1987) dictionaries and adding
around 30 frames found by manual inspection. The frames incorporate control informa-
tion and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a
priori information about the probabilities of subcategorization frame membership and
use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering
phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining
more accurate back-off estimates for hypothesis selection. Carroll and Rooth (1998)
use a handwritten head-lexicalized, context-free grammar and a text corpus to
compute the probability of particular subcategorization patterns. The approach is
iterative with the aim of estimating the distribution of subcategorization frames
associated with a particular predicate. They perform a mapping between their frames
and those of the OALD, resulting in 15 frame types. These do not contain details of
specific prepositions.
More recently, a number of researchers have applied similar techniques to auto-
matically derive lexical resources for languages other than English. Schulte im Walde
(2002a, 2002b) uses a head-lexicalized probabilistic context-free grammar similar to
that of Caroll and Rooth (1998) to extract subcategorization frames from a large
German newspaper corpus from the 1990s. She predefines 38 distinct frame types,
which contain maximally three arguments each and are made up of a combination
of the following: nominative, dative, and accusative noun phrases; reflexive pro-
nouns; prepositional phrases; expletive es; subordinated nonfinite clauses; subordinated
finite clauses; and copula constructions. The frames may optionally contain details of
particular prepositional use. Unsupervised training is performed on a large German
newspaper corpus, and the resulting probabilistic grammar establishes the relevance of
different frame types to a specific lexical head. Because of computing time constraints,
Schulte im Walde limits sentence length for grammar training and parsing. Sentences
of length between 5 and 10 words were used to bootstrap the lexicalized grammar
model. For lexicalized training, sentences of length between 5 and 13 words were
used. The result is a subcategorization lexicon for over 14,000 German verbs. The
extensive evaluation carried out by Schulte im Walde will be discussed in greater detail
in Section 6.
Approaches using treebank-based data as a source for subcategorization infor-
mation, such as ours, do not predefine the frames to be extracted but rather learn them
from the data. Kinyon and Prolo (2002) describe a simple tool which uses fine-grained
rules to identify the arguments of verb occurrences in the Penn-II Treebank. This is
made possible by manual examination of more than 150 different sequences of syntactic
and functional tags in the treebank. Each of these sequences was categorized as a
modifier or argument. Arguments were then mapped to traditional syntactic functions.
For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic
function is subject. In general, argumenthood was preferred over adjuncthoood. As
Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to
say how effective their technique is. Sarkar and Zeman (2000) present an approach to
learn previously unknown frames for Czech from the Prague Dependency Bank (Hajic
335
Computational Linguistics Volume 31, Number 3
1998). Czech is a language with a freer word order than English and so configurational
information cannot be relied upon. In a dependency tree, the set of all dependents
of the verb make up a so-called observed frame, whereas a subcategorization frame
contains a subset of the dependents in the observed frame. Finding subcategorization
frames involves filtering adjuncts from the observed frame. This is achieved using three
different hypothesis tests: BHT, log-likelihood ratio, and t-score. The system learns 137
subcategorization frames from 19,126 sentences for 914 verbs (those which occurred
five times or more). Marinov and Hemming (2004) present preliminary work on the
automatic extraction of subcategorization frames for Bulgarian from the BulTreeBank
(Simov, Popova, and Osenova 2002). In a similar way to that of Sarkar and Zeman
(2000), Marinov and Hemming?s system collects both arguments and adjuncts. It then
uses the binomial log-likelihood ratio to filter incorrect frames. The BulTreebank trees
are annotated with HPSG-typed feature structure information and thus contain more
detail than the dependency trees. The work done for Bulgarian is small-scale, however,
as Marinov and Hemming are working with a preliminary version of the treebank with
580 sentences.
Work has been carried out on the extraction of formalism-specific lexical resources
from the Penn-II Treebank, in particular TAG, CCG, and HPSG. As these formalisms are
fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component,
the extraction of a lexicon essentially amounts to the creation of a grammar. Chen and
Vijay-Shanker (2000) explore a number of related approaches to the extraction of a
lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical
model for parsing. The extraction procedure utilizes a head percolation table as intro-
duced by Magerman (1995) in combination with a variation of Collins?s (1997) approach
to the differentiation between complement and adjunct. This results in the construction
of a set of lexically anchored elementary trees which make up the TAG in question.
The number of frame types extracted (i.e., an elementary tree without a specific lexical
anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for
the extraction of a TAG from the Penn Treebank. The extraction procedure consists
of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and
extended based on the approaches of Magerman (1994) and Collins (1997). Then the
elementary trees are read off in a quite straightforward manner. Finally any invalid
elementary trees produced as a result of annotation errors in the treebank are filtered out
using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged
from 3,014 to 6,099.
Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic
extraction of a large syntactic CCG lexicon from the Penn-II Treebank. For each tree, the
algorithm annotates the nodes with CCG categories in a top-down recursive manner.
The first step is to label each node as either a head, complement, or adjunct based
on the approaches of Magerman (1994) and Collins (1997). Each node is subsequently
assigned the relevant category based on its constituent type and surface configuration.
The algorithm handles ?like? coordination and exploits the traces used in the treebank
in order to interpret LDDs. Unlike our approach, those of Xia (1999) and Hockenmaier,
Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the
Penn-II trees.
Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii (2004)
describe a methodology for acquiring an English HPSG from the Penn-II Treebank.
Manually defined heuristics are used to automatically annotate each tree in the treebank
with partially specified HPSG derivation trees: Head/argument/modifier distinctions
are made for each node in the tree based on Magerman (1994) and Collins (1997);
336
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
the whole tree is then converted to a binary tree; heuristics are applied to deal with
phenomena such as LDDs and coordination and to correct some errors in the tree-
bank, and finally an HPSG category is assigned to each node in the tree in accordance
with its CFG category. In the next phase of the process (externalization), HPSG lexical
entries are automatically extracted from the annotated trees through the application of
?inverse schemata.?
4. Methodology
The first step in the application of our methodology is the production of a tree-
bank annotated with LFG f-structure information. F-structures are attribute?value
structures which represent abstract syntactic information, approximating to ba-
sic predicate?argument?modifier structures. Most of the early work on automatic
f-structure annotation (e.g., van Genabith, Way, and Sadler 1999; Frank 2000; Sadler,
van Genabith, and Way 2000) was applied only to small data sets (fewer than 200
sentences) and was largely proof of concept. However, more recent work (Cahill et al
2002; Cahill, McCarthy, et al 2004) has presented efforts in evolving and scaling up
annotation techniques to the Penn-II Treebank (Marcus et al 1994), containing more
than 1,000,000 words and 49,000 sentences.
We utilize the automatic annotation algorithm of Cahill et al (2002) and Cahill,
McCarthy, et al (2004) to derive a version of Penn-II in which each node in each
tree is annotated with LFG functional annotations in the form of attribute-value struc-
ture equations. The algorithm uses categorial, configurational, local head, and Penn-II
functional and trace information. The annotation procedure is dependent on locating
the head daughter, for which an amended version of Magerman (1994) is used. The
head is annotated with the LFG equation ?=?. Linguistic generalizations are provided
over the left (the prefix) and the right (suffix) context of the head for each syntactic
category occurring as the mother nodes of such heads. To give a simple example, the
rightmost NP to the left of a VP head under an S is likely to be the subject of the sen-
tence (? SUBJ =?), while the leftmost NP to the right of the V head of a VP is most
probably the verb?s object (? OBJ =?). Cahill, McCarthy, et al (2004) provide four
classes of annotation principles: one for noncoordinate configurations, one for coor-
dinate configurations, one for traces (long-distance dependencies), and a final ?catch
all and clean up? phase.
The satisfactory treatment of long-distance dependencies by the annotation algo-
rithm is imperative for the extraction of accurate semantic forms. The Penn Treebank
employs a rich arsenal of traces and empty productions (nodes which do not realize
any lexical material) to coindex displaced material with the position where it should
be interpreted semantically. The algorithm of Cahill, McCarthy, et al (2004) translates
the traces into corresponding reentrancies in the f-structure representation by treating
null constituents as full nodes and recording the traces in terms of index=i f-structure
annotations (Figure 3). Passive movement is captured and expressed at f-structure level
using a passive:+ annotation. Once a treebank tree is annotated with feature structure
equations by the annotation algorithm, the equations are collected, and a constraint
solver produces an f-structure.
In order to ensure the quality of the semantic forms extracted by our method, we
must first ensure the quality of the f-structure annotations. The results of two different
evaluations of the automatically generated f-structures are presented in Table 2. Both
use the evaluation software and triple encoding presented in Crouch et al (2002). The
first of these is against the DCU 105, a gold-standard set of 105 hand-coded f-structures
337
Computational Linguistics Volume 31, Number 3
Figure 3
Use of reentrancy between TOPIC and COMP to capture long-distance dependency in Penn
Treebank sentence wsj 0008 2, Until Congress acts, the government hasn?t any authority to issue new
debt obligations of any kind, the Treasury said.
from Section 23 of the Penn Treebank as described in Cahill, McCarthy, et al (2004). For
the full set of annotations they achieve precision of over 96.5% and recall of over 96.6%.
There is, however, a risk of overfitting when evaluation is limited to a gold standard
of this size. More recently, Burke, Cahill, et al (2004a) carried out an evaluation of the
automatic annotation algorithm against the publicly available PARC 700 Dependency
Bank (King et al 2003), a set of 700 randomly selected sentences from Section 23
which have been parsed, converted to dependency format, and manually corrected
and extended by human validators. They report precision of over 88.5% and recall of
over 86% (Table 2). The PARC 700 Dependency Bank differs substantially from both
the DCU 105 f-structure bank and the automatically generated f-structures in regard to
Table 2
Results of f-structure evaluation.
DCU 105 PARC 700
Precision 96.52% 88.57%
Recall 96.62% 86.10%
F-score 96.57% 87.32%
338
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
the style of linguistic analysis, feature nomenclature, and feature geometry. Some, but
not all, of these differences are captured by automatic conversion software. A detailed
discussion of the issues inherent in this process and a full analysis of results is presented
in Burke, Cahill, et al (2004a). Results broken down by grammatical function for the
DCU 105 evaluation are presented in Table 3. OBL (prepositional phrase) arguments are
traditionally difficult to annotate reliably. The results show, however, that with respect
to obliques, the annotation algorithm, while slightly conservative (recall of 82%), is very
accurate: 96% of the time it annotates an oblique, the annotation is correct.
A high-quality set of f-structures having been produced, the semantic form ex-
traction methodology is applied. This is based on and substantially extends both the
granularity and coverage of an idea in van Genabith, Sadler, and Way (1999):
For each f-structure generated, for each level of embedding we determine the local
PRED value and collect the subcategorisable grammatical functions present at that level
of embedding. (page 72)
Consider the automatically generated f-structure in Figure 4 for tree wsj 0003 22
in the Penn-II and Penn-III Treebanks. It is crucial to note that in the automatically
generated f-structures the value of the PRED feature is a lemma and not a semantic
form. Exploiting the information contained in the f-structure and applying the
method described above, we recursively extract the following nonempty semantic
forms: impose([subj, obj, obl:on]), in([obj]), of([obj]), and on([obj]). In effect,
in both the approach of van Genabith, Sadler, and Way (1999) and our approach,
semantic forms are reverse-engineered from automatically generated f-structures
for treebank trees. The automatically induced semantic forms contain the following
subcategorizable syntactic functions:
SUBJ OBJ OBJ2 OBLprep OBL2 COMP XCOMP PART
PART is not a syntactic function in the strict sense, but we decided to capture the
relevant co-occurrence patterns of verbs and particles in the semantic forms. Just as
Table 3
Precision and recall on automatically generated f-structures by feature against the DCU 105.
Feature Precision Recall F-score
ADJUNCT 892/968 = 92 892/950 = 94 93
COMP 88/92 = 96 88/102 = 86 91
COORD 153/184 = 83 153/167 = 92 87
DET 265/267 = 99 265/269 = 99 99
OBJ 442/459 = 96 442/461 = 96 96
OBL 50/52 = 96 50/61 = 82 88
OBLAG 12/12 = 100 12/12 = 100 100
PASSIVE 76/79 = 96 76/80 = 95 96
RELMOD 46/48 = 96 46/50 = 92 94
SUBJ 396/412 = 96 396/414 = 96 96
TOPIC 13/13 = 100 13/13 = 100 100
TOPICREL 46/49 = 94 46/52 = 88 91
XCOMP 145/153 = 95 145/146 = 99 97
339
Computational Linguistics Volume 31, Number 3
Figure 4
Automatically generated f-structure and extracted semantic forms for the Penn-II Treebank
string wsj 0003 22, In July, the Environmental Protection Agency imposed a gradual ban on virtually
all uses of asbestos.
OBLprep includes the prepositional head of the PP, PART includes the actual particle
which occurs, for example, add([subj, obj, part:up]).
In the work presented here, we substantially extend and scale the approach of
van Genabith, Sadler, and Way (1999) in regard to coverage, granularity, and eval-
uation. First, we scale the approach to the full WSJ section of the Penn-II Treebank
and the parsed Brown corpus section of Penn-III, with a combined total of approx-
imately 75,000 trees. Van Genabith, Sadler, and Way (1999) was proof of concept on
100 trees. Second, in contrast to the approach of van Genabith, Sadler, and Way (1999)
(and many other approaches), our approach fully reflects long-distance dependencies,
indicated in terms of traces in the Penn-II and Penn-III Treebanks and correspond-
ing reentrancies at f-structure. Third, in addition to abstract syntactic-function-
based subcategorization frames, we also compute frames for syntactic function?CFG
category pairs, for both the verbal heads and their arguments, and also generate
340
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 4
Conflation of Penn Treebank tags.
Conflated Category Penn Treebank Category
JJ JJ
JJR
JJS
N NN
NNS
NNP
NNPS
PRP
RB RB
RBR
RBS
V VB
VBD
VBG
VBN
VBP
VBZ
MD
pure CFG-based subcategorization frames. Fourth, in contrast to the approach of
van Genabith, Sadler, and Way (1999) (and many other approaches), our method differ-
entiates between frames for active and passive constructions. Fifth, in contrast to that of
van Genabith, Sadler, and Way (1999), our method associates conditional probabilities
with frames. Sixth, we evaluate the complete set of semantic forms extracted (not
just a selection) against the manually constructed COMLEX (MacLeod, Grishman, and
Meyers 1994) resource.
In order to capture CFG-based categorial information, we add a CAT feature to
the f-structures automatically generated from the Penn-II and Penn-III Treebanks. Its
value is the syntactic category of the lexical item whose lemma gives rise to the PRED
value at that particular level of embedding. This makes it possible to classify words
and their semantic forms based on their syntactic category and reduces the risk of
inaccurate assignment of subcategorization frame frequencies due to POS ambiguity,
distinguishing, for example, between the nominal and verbal occurrences of the lemma
fight. With this, the output for the verb impose in Figure 4 is impose(v,[subj, obj,
obl:on]). For some of our experiments, we conflate the different verbal (and other)
tags used in the Penn Treebanks to a single verbal marker (Table 4). As a further
extension, the extraction procedure reads off the syntactic category of the head of
each of the subcategorized syntactic functions: impose(v,[subj(n),obj(n),obl:on]).3
In this way, our methodology is able to produce surface syntactic as well as abstract
functional subcategorization details. Dalrymple (2001) argues that there are cases,
albeit exceptional ones, in which constraints on syntactic category are an issue in
subcategorization. In contrast to much of the work reviewed in Section 3, which limits
itself to the extraction of surface syntactic subcategorization details, our system can
provide this information as well as details of grammatical function.
3 We do not associate syntactic categories with OBLs as they are always PPs.
341
Computational Linguistics Volume 31, Number 3
Another way in which we develop and extend the basic extraction algorithm
is to deal with passive voice and its effect on subcategorization behavior. Consider
Figure 5: Not taking into account that the example sentence is a passive construction,
the extraction algorithm extracts outlaw([subj]). This is incorrect, as outlaw is a tran-
sitive verb and therefore requires both a subject and an object to form a gram-
matical sentence in the active voice. To cope with this problem, the extraction al-
gorithm uses the feature-value pair passive:+, which appears in the f-structure at
the level of embedding of the verb in question, to mark that predicate as occurring
in the passive: outlaw([subj],p). The annotation algorithm?s accuracy in recognizing
passive constructions is reflected by the f-score of 96% reported in Table 3 for the
PASSIVE feature.
The syntactic functions COMP and XCOMP refer to clausal complements with
different predicate control patterns as described in Section 2. However, as it stands,
neither of these functions betrays anything about the syntactic nature of the constructs
in question. Many lexicons, both automatically acquired and manually created, are
more fine grained in their approaches to subcategorized clausal arguments, differ-
entiating, for example, between a that-clause and a to + infinitive clause (Ushioda
et al 1993). With only a slight modification, our system, along with the details
provided by the automatically generated f-structures, allows us to extract frames
with an equivalent level of detail. For example, to identify a that-clause, we use
Figure 5
Automatically generated f-structure for the Penn-II Treebank string wsj 0003 23. By 1997, almost
all remaining uses of cancer-causing asbestos will be outlawed.
342
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 5
Semantic forms for the verb accept.
Semantic form Occurrences Conditional probability
accept([subj, obj]) 122 0.813
accept ([subj]) 11 0.073
accept([subj, comp]) 5 0.033
accept([subj, obl:as]) 3 0.020
accept([subj, obj, obl:as]) 3 0.020
accept([subj, obj, obl:from]) 3 0.020
accept([subj, obj, obl:at]) 1 0.007
accept([subj, obj, obl:for]) 1 0.007
accept([subj, obj, xcomp]) 1 0.007
the feature-value pair that:+ at f-structure level to read off the following subcate-
gorization frame for the verb add: add([subj,comp(that)]). Using the feature-value pair
to inf:+, we can identify to + infinitive clauses, resulting in the following frame for
the verb want: want([subj,xcomp(to inf)]). We can also derive control information about
open complements. In Figure 5, the reentrant XCOMP subject is identical to the subject
of will in the matrix clause, which allows us to induce information about the nature
of the external control of the XCOMP (i.e., whether it is subject or object control).
In order to estimate the likelihood of the co-occurrence of a predicate with a partic-
ular argument list, we compute conditional probabilities for subcategorization frames
based on the number of token occurrences in the corpus:
P (ArgList|?) = count(??ArgList?)?n
i=1 count(??ArgListi?)
where ArgList1... ArgListn are the possible argument lists which can occur for ?. Be-
cause of variations in verbal subcategorization across domains, probabilities are also
useful for predicting the way in which verbs behave in certain contexts. In Section 6,
we use the conditional probabilities to filter possible error judgments by our system.
Tables 5?7 show, with varying levels of analysis, the attested semantic forms for the
verb accept with their associated conditional probabilities. The effect of differentiating
between the active and passive occurrences of verbs can be seen in the different con-
ditional probabilities associated with the intransitive frame ([subj]) of the verb accept
(shown in boldface type) in Tables 5 and 6.4 Table 7 shows the joint grammatical-
function/syntactic-category-based subcategorization frames.
5. Results
We extract semantic forms for 4,362 verb lemmas from Penn-III. Table 8 shows the
number of distinct semantic form types (i.e., lemma and argument list combination)
4 Given these, it is possible to condition frames on both lemma (?) and voice (v: active/passive):
P (ArgList|?, v) = count(??ArgList, v?)?n
i=1 count(??ArgListi, v?)
343
Computational Linguistics Volume 31, Number 3
Table 6
Semantic forms for the verb accept marked with p for passive use.
Semantic form Occurrences Conditional probability
accept([subj, obj]) 122 0.813
accept ([subj],p) 9 0.060
accept([subj, comp]) 5 0.033
accept([subj, obl:as],p) 3 0.020
accept([subj, obj, obl:as]) 3 0.020
accept([subj, obj, obl:from]) 3 0.020
accept ([subj]) 2 0.013
accept([subj, obj, obl:at]) 1 0.007
accept([subj, obj, obl:for]) 1 0.007
accept([subj, obj, xcomp]) 1 0.007
Table 7
Semantic forms for the verb accept including syntactic category for each grammatical function.
Semantic form Occurrences Conditional probability
accept([subj(n), obj(n)]) 116 0.773
accept([subj(n)]) 11 0.073
accept([subj(n), comp(that)]) 4 0.027
accept([subj(n), obj(n), obl:from]) 3 0.020
accept([subj(n), obl:as]) 3 0.020
Other 13 0.087
extracted. Discriminating obliques by associated preposition and recording particle
information, the algorithm finds a total of 21,005 semantic form types, 16,000 occurring
in active voice and 5,005 in passive voice. When the obliques are parameterized for
prepositions and particles are included for particle verbs, we find an average of 4.82
semantic form types per verb. Without the inclusion of details for individual preposi-
tions or particles, there was an average of 3.45 semantic form types per verb. Unlike
many of the researchers whose work is reviewed in Section 3, we do not predefine the
frames extracted by our system. Table 9 shows the numbers of distinct frame types
extracted from Penn-II, ignoring PRED values.5 We provide two columns of statistics,
one in which all oblique (PP) arguments are condensed into one OBL function and
all particle arguments are condensed into part, and the other in which we differen-
tiate among obl:to (e.g., give), obl:on (e.g., rely), obl:for (e.g., compensate), etc., and
likewise for particles. Collapsing obliques and particles into simple functions, we extract
38 frame types. Discriminating particles and obliques by preposition, we extract 577
frame types. Table 10 shows the same results for Penn-III, with 50 simple frame types
and 1,084 types when parameterized for prepositions and particles. We also show the
result of applying absolute thresholding techniques to the semantic forms induced.
Applying an absolute threshold of five occurrences, we still generate 162 frame types
5 To recap, if two verbs have the same subcategorization requirements (e.g., give([subj, obj, obj2]),
send([subj, obj, obj2])), then that frame [subj, obj, obj2] is counted only once.
344
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 8
Number of semantic form types for Penn-III.
Without prepositions and particles With prepositions and particles
Semantic form types 15,166 21,005
Active 11,038 16,000
Passive 4,128 5,005
Table 9
Number of frame types for verbs for Penn-II.
Without prepositions With prepositions
and particles and particles
Number of frame types 38 577
Number of singletons 1 243
Number occurring twice 1 84
Number occurring five or fewer times 7 415
Number occurring more than five times 31 162
from Penn-II and 221 from Penn-III. Briscoe and Carroll (1997), by comparison, employ
163 distinct predefined frames.
6. Evaluation
Most of the previous approaches discussed in Section 3 have been evaluated to
different degrees. In general, a small number of frequently occurring verbs is selected,
and the subcategorization frames extracted for these verbs (from some quantity of
unseen test data) are compared to a gold standard. The gold standard is either manually
custom-made based on the test data or adapted from an existing external resource
such as the OALD (Hornby 1980) or COMLEX (MacLeod, Grishman, and Meyers
1994). There are advantages and disadvantages to both types of gold standard. While
it is time-consuming to manually construct a custom-made standard, the resulting
standard has the advantage of containing only the subcategorization frames exhibited
in the test data. Using an existing externally produced resource is quicker, but the gold
Table 10
Number of frame types for verbs for Penn-III.
Without prepositions With prepositions
and particles and particles
Number of frame types 50 1,084
Number of singletons 6 544
Number occurring twice 2 147
Number occurring five or fewer times 12 863
Number occurring more than five times 38 221
345
Computational Linguistics Volume 31, Number 3
standard may contain many more frames than those which occur in the data from which
the test lexicon is induced or, indeed, may omit relevant correct frames contained in
the data. As a result, systems generally score better against custom-made, manually
established gold standards.
Carroll and Rooth (1998) achieve an F-score of 77% against the OALD when they
evaluate a selection of 100 verbs with absolute frequency of greater than 500 each.
Their system recognizes 15 frames, and these do not contain details of subcategorized-
for prepositions. Still, to date this is the largest number of verbs used in any of the
evaluations of the systems for English described in Section 3. Sarkar and Zeman (2000)
evaluate 914 Czech verbs against a custom-made gold standard and record a token
recall of 88%. However, their evaluation does not examine the extracted subcatego-
rization frames but rather the argument?adjunct distinctions posited by their sys-
tem. The largest lexical evaluation we know of is that of Schulte im Walde (2002b)
for German. She evaluates 3,000 German verbs with a token frequency between
10 and 2,000 against the Duden (Dudenredaktion 2001). We will refer to this work
and the methods and results presented by Schulte im Walde again in Sections 6.2
and 6.3.
We carried out a large-scale evaluation of our automatically induced lexicon (2,993
active verb lemmas for Penn-II and 3,529 for Penn-III, as well as 1,422 passive verb
lemmas from Penn-II) against the COMLEX resource. To our knowledge this is the most
extensive evaluation ever carried out for English lexical extraction. We conducted a
number of experiments on the subcategorization frames extracted from Penn-II and
Penn-III which are described and discussed in Sections 6.2, 6.3, and 6.4. Finding a
common format for the gold standard and induced lexical entries is a nontrivial task.
To ensure that we did not bias the evaluation in favor of either resource, we carried
out two different mappings for the frames from Penn-II and Penn-III: COMLEX-LFG
Mapping I and COMLEX-LFG Mapping II. For each mapping we carried out six basic
experiments (and two additional ones for COMLEX-LFG Mapping II) for the active
subcategorization frames extracted. Within each experiment, the following factors were
varied: level of prepositional phrase detail, level of particle detail, relative threshold
(1% or 5%), and incorporation of an expanded set of directional prepositions. Using
the second mapping we also evaluated the automatically extracted passive frames and
experimented with absolute thresholds. Direct comparison of subcategorization frame
acquisition systems is difficult because of variations in the number of frames extracted,
the number of test verbs, the gold standards used, the size of the test data, and the
level of detail in the subcategorization frames (e.g., whether they are parameterized
for specific prepositions). Therefore, in order to establish a baseline against which to
compare our results, following Schulte in Walde (2002b), we assigned the two most
frequent frame types (transitive and intransitive) by default to each verb and compared
this ?artificial? lexicon to the gold standard. The section concludes with a full discussion
of the reported results.
6.1 COMLEX
We evaluate our induced semantic forms against COMLEX (MacLeod, Grishman, and
Meyers 1994), a computational machine-readable lexicon containing syntactic infor-
mation for approximately 38,000 English headwords. Its creators paid particular
attention to the encoding of more detailed subcategorization information than is avail-
able in either the OALD or the LDOCE (Proctor 1978), both for verbs and for nouns
346
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Figure 6
Intersection between active-verb lemma types in COMLEX and the Penn-II-induced lexicon.
and adjectives which take complements (Grishman, MacLeod, and Meyers 1994). By
choosing to evaluate against COMLEX, we set our sights high: Our extracted semantic
forms are fine-grained, and COMLEX is considerably more detailed than the OALD
or LDOCE used for earlier evaluations. While our system can generate semantic forms
for any lemma (regardless of part of speech) which induces a PRED value, we have
thus far evaluated the automatic generation of subcategorization frames for verbs
only. COMLEX defines 138 distinct verb frame types without the inclusion of specific
prepositions or particles.
As COMLEX contains information other than subcategorization details, it was
necessary for us to extract the subcategorization frames associated with each verbal
lexicon entry. The following is a sample entry for the verb reimburse:
(VERB :ORTH ?reimburse? :SUBC ((NP-NP)
(NP-PP :PVAL (?for?))
(NP)))
Each entry is organized as a nested set of typed feature-value lists. The first symbol
(i.e., VERB) gives the part of speech. The value of the :ORTH feature is the base form
of the verb. Any entry with irregular morphological behavior will also include the
features :PLURAL, :PAST, and so on, with the relevant values. All verbs have a :SUBC
feature, and for our purposes, this is the most interesting feature. In the case of the
example above, the subcategorization values specify that reimburse can occur with two
object noun phrases (NP-NP), an object noun phrase followed by a prepositional phrase
headed by for (NP-PP :PVAL (?for?)) or just an object noun phrase (NP). (Note that the
details of the subject are not included in COMLEX frames.) What makes the COMLEX
resource particularly suitable for our evaluation is that each of the complement types
(NP-NP, NP-PP, and NP) which make up the value of the :SUBC feature is associated with
a formal frame definition which looks like the following:
(vp-frame np-np :cs ((np 2)(np 3))
:gs (:subject 1 :obj 2 :obj2 3)
:ex ?she asked him his name?)
The value of the :cs feature is the constituent structure of the subcategorization
frame, which lists the syntactic CF-PSG constituents in sequence (omitting the sub-
ject, again). The value of the :gs feature is the grammatical structure which indicates
the functional role played by each of the CF-PSG constituents. The elements of the
347
Computational Linguistics Volume 31, Number 3
Figure 7
Intersection between active-verb lemma types in COMLEX and the Penn-III-induced lexicon.
constituent structure are indexed, and these indices are referenced in the :gs field.
The index 1 always refers to the surface subject of the verb. This mapping between
constituent structure and functional structure makes the information contained in
COMLEX particularly suitable as an evaluation standard for the LFG semantic forms
which we induce.
We present the evaluation for the verbs which occur in an active context in the
treebank. COMLEX does not provide passive frames. For Penn-II, there are 2,993
verb lemmas (used actively) that both resources have in common. 2,669 verb lemmas
appear in COMLEX but not in the induced lexicon, and 416 verb lemmas (used actively)
appear in the induced lexicon but not in COMLEX (Figure 6). For Penn-III, COMLEX
and the induced lexicon share 3,529 verb lemmas (used actively). This is shown in
Figure 7. 6
6.2 COMLEX-LFG Mapping I and Penn-II
In order to carry out the evaluation, we have to find a common format for the expression
of subcategorization information between our induced LFG-style subcategorization
frames and those contained in COMLEX. The following are the common syntactic
functions: SUBJ, OBJ, OBJi, COMP, and PART. Unlike our system, COMLEX does not
distinguish an OBL from an OBJi, so we converted all the obliques in the induced frames
to OBJi. As in COMLEX, the value of i depends on the number of objects/obliques
already present in the semantic form. COMLEX does not differentiate between COMPs
and XCOMPs as our system does (control information is expressed in a different way:
see Section 6.3), so we conflate our two LFG categories to that of COMP. The process is
summarized in Table 11.
The manually constructed COMLEX entries provide a gold standard against which
we evaluate the automatically induced frames. We calculate the number of true pos-
itives (tps) (where our semantic forms and those from COMLEX are the same), the
number of false negatives ( fns) (those frames which appeared in COMLEX but were not
produced by our system), and the number of false positives ( fps) (those frames
6 Given these figures, one might begin to wonder about the value of automatic induction. First, COMLEX
does not rank frames by probabilities, which are essential in disambiguation. Second, the coverage of
COMLEX is not complete: 518 lemmas ?discovered? by the induction experiment are not listed in
COMLEX; see the error analysis in Section 6.5.
348
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 11
Mapping I: Merging of COMLEX and LFG syntactic functions.
Our syntactic functions COMLEX syntactic functions Merged function
SUBJ Subject SUBJ
OBJ Object OBJ
OBJ2 Obj2 OBJi
OBL Obj3
OBL2 Obj4
COMP Comp COMP
XCOMP
PART Part PART
produced by our system which do not appear in COMLEX). We calculate precision,
recall, and F-score using the following standard equations:
recall =
tp
tp + fn
precision =
tp
tp + fp
f-score =
2 ? recall ? precision
recall + precision
We use the frequencies associated with each of our semantic forms in order to set
a relative threshold to filter the selection of semantic forms. For a threshold of 1% we
disregard any semantic forms with a conditional probability (i.e., given a lemma) of
less than or equal to 0.01. As some verbs occur less frequently than others, we think it
is important to use a relative rather than absolute threshold (as in Carroll and Rooth
[1998], for instance) in this way. We carried out the evaluation in a similar way to
Schulte im Walde?s (2002b) for German, the only experiment comparable in scale to
ours. Despite the obvious differences in approach and language, this allows us to make
some tentative comparisons between our respective results. The statistics shown in
Table 12 give the results of three different experiments with the relative threshold set
to 1%. As for all the results tables, the baseline statistics (simply assigning the most
frequent frames, in this case transitive and intransitive, to each lemma by default) are
in each case shown in the left column, and the results achieved by our induced lexicon
are presented in the right column. Distinguishing between complement and adjunct
prepositional phrases is a notoriously difficult aspect of automatic subcategorization
frame acquisition. For this reason, following the evaluation setup in Schulte im Walde
(2002b), the three experiments vary with respect to the amount of prepositional infor-
mation contained in the subcategorization frames.
Experiment 1. Here we excluded subcategorized prepositional-phrase arguments en-
tirely from the comparison. In a manner similar to that of Schulte im Walde (2002b), any
349
Computational Linguistics Volume 31, Number 3
Table 12
Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 1%).
Precision Recall F-score
Mapping I Baseline Induced Baseline Induced Baseline Induced
Experiment 1 66.1% 75.2% 65.8% 69.1% 66.0% 72.0%
Experiment 2 71.5% 65.5% 64.3% 63.1% 67.7% 64.3%
Experiment 3 64.7% 71.8% 11.9% 16.8% 20.1% 27.3%
frames containing an OBL were mapped to the same frame type minus that argument.
For example, the frame [subj,obl:for] becomes [subj]. Using a relative threshold of
1% (Table 12), our results (precision of 75.2%, recall of 69.1%, and F-score of 72.0%)
are remarkably similar to those of Schulte im Walde (2002b), who reports precision of
74.53%, recall of 69.74%, and an f-score of 72.05%.
Experiment 2. Here we include subcategorized prepositional phrase arguments but
only in their simplest form; that is, they were not parameterized for particular prepo-
sitions. For example, the frame [subj,obl:for] is rewritten as [subj,obl]. Using a
relative threshold of 1% (Table 12), our results (precision of 65.5%, recall of 63.1%, and
F-score of 64.3%) compare favorably to those of Schulte im Walde (2002b), who recorded
precision of 60.76%, recall of 63.91%, and an F-score of 62.30%.
Experiment 3. Here we used semantic forms which contain details of specific prepo-
sitions for any subcategorized prepositional phrase (e.g., [subj,obl:for]). Using a rela-
tive threshold of 1% (Table 12), our precision figure (71.8%) is quite high (in comparison
to 65.52% as recorded by Schulte im Walde [2002b]). However our recall (16.8%) is very
low (compared to the 50.83% that Schulte im Walde [2002b] reports). Consequently our
F-score (27.3%) is also low (Schulte im Walde [2002b] records an F-score of 57.24%). The
reason for this is discussed in Section 6.2.1.
The statistics in Table 13 are the result of the second experiment, in which the
relative threshold was increased to 5%. The effect of such an increase is obvious in
that precision goes up (by as much as 5%) for each of the three evaluations while
recall goes down (by as much as 5.5%). This is to be expected, as a greater threshold
means that there are fewer semantic forms associated with each verb in the induced
lexicon, but they are more likely to be correct because of their greater frequency of
occurrence. The conditional probabilities we associate with each semantic form together
with thresholding can be used to customize the induced lexicon to the task for which
it is required, that is, whether a very precise lexicon is preferred to one with broader
Table 13
Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 5%).
Precision Recall F-score
Mapping I Baseline Induced Baseline Induced Baseline Induced
Experiment 1 66.1% 80.2% 65.8% 63.6% 66.0% 70.9%
Experiment 2 71.5% 69.6% 64.3% 56.9% 67.7% 62.7%
Experiment 3 64.7% 76.7% 11.9% 13.9% 20.1% 23.5%
350
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
coverage. In Tables 12 and 13, the baseline is exceeded in all experiments with the
exception of Experiment 2. This can be attributed to Mapping I, in which OBLi becomes
OBJi (Table 11). Experiment 2 includes obliques without the specific preposition, mean-
ing that in this mapping, the frame [subj,obj:with] becomes [subj,obj]. Therefore,
the transitive baseline frame scores better than it should against the gold standard. A
more fine-grained LFG-COMLEX mapping in which this effect disappears is presented
in Section 6.3.
6.2.1 Directional Prepositions. Our recall statistic was particularly low in the case of
evaluation using details of prepositions (Experiment 3, Tables 12 and 13). This can be
accounted for by the fact that the creators of COMLEX have chosen to err on the side
of overgeneration in regard to the list of prepositions which may occur with a verb and
a subcategorization frame containing a prepositional phrase. This is particularly true
of directional prepositions. For COMLEX, a list of 31 directional prepositions (Table 14)
was prepared and assigned in its entirety by default to any verb which can potentially
appear with any directional preposition in order to save time and avoid the risk of
missing prepositions. Grishman, MacLeod, and Meyers (1994) acknowledge that this
can lead to a preposition list which is ?a little rich? for a particular verb, but this is
the approach they have chosen to take. In a subsequent experiment, we incorporated
this list of directional prepositions by default into our semantic form induction process
in the same way as the creators of COMLEX have done. Table 15 shows that doing
so results in a significant improvement in the recall statistic (45.1%), as would be
expected, with the new statistic being almost three times as good as the result re-
ported in Table 12 for Experiment 3 (16.8%). There is also an improvement in the
precision figure (from 71.8% to 86.9%). This is due to a substantial increase in the
number of true positives (from 5,612 to 14,675) compared with a stationary false pos-
itive figure (2,205 in both cases). The f-score increases from 27.3% to 59.4%.
6.3 COMLEX-LFG Mapping II and Penn-II
The COMLEX-LFG Mapping I presented above establishes a ?least common denomi-
nator? for the COMLEX and our LFG-inspired resources. More-fine-grained mappings
are possible: in order to ensure that the mapping from our semantic forms to the
COMLEX frames did not oversimplify the information in the automatically extracted
subcategorization frames, we conducted a further set of experiments in which we
converted the information in the COMLEX entries to the format of our extracted
semantic forms. We explicitly differentiated between OBLs and OBJs by automatically
Table 14
COMLEX directional prepositions.
about across along around
behind below beneath between
beyond by down from
in inside into off
on onto out out of
outside over past through
throughout to toward toward
up up to via
351
Computational Linguistics Volume 31, Number 3
Table 15
Penn-II evaluation of active frames against COMLEX using p-dir list (relative threshold of 1%).
Mapping I Precision Recall F-score
Experiment 3 86.9% 45.1% 59.4%
deducing whether a COMLEX OBJi was coindexed with an NP or a PP. Furthermore, as
can be seen in the following example, COMLEX frame definitions contain details of the
control patterns of sentential complements, encoded using the :features attribute. This
allows for automatic discrimination between COMPs and XCOMPs.
(vp-frame to-inf-sc :cs (vp 2 :mood to-infinitive :subject 1)
:features (:control subject)
:gs (:subject 1 :comp 2)
:ex ?I wanted to come?)
The mapping is summarized in Table 16. The results of the subsequent evaluation are
presented in Tables 17 and 18. We have added Experiments 2a and 3a. These are the
same as Experiments 2 and 3, except that they additionally include the specific particle
with each PART function. While the recall figures in Tables 17 and 18 are slightly lower
than those in Tables 12 and 13, changing the mapping in this way results in an increase
in precision in each case (by as much as 11.6%). The results of the lexical evaluation
are consistently better than the baseline, in some cases by almost 16% (Experiment 2,
threshold 5%). Notice that in contrast to Tables 12 and 13, in the more-fine-grained
COMLEX-LFG Mapping II presented here, all experiments exceed the baseline.
6.3.1 Directional Prepositions. The recall figures for Experiments 3 and 3a in Table 17
(24.0% and 21.5%) and Table 18 (19.7% and 17.4%) drop in a similar fashion to the results
seen in Tables 12 and 13. For this reason, we again incorporated the list of 31 directional
prepositions (Table 14) by default and reran Experiments 3 and 3a for a threshold of
1%. The results are presented in Table 19. The effect was as expected: The recall scores
for the two experiments increased to 40.8% and 35.4% (from 24.0% and 22.5%), and the
F-scores increased to 54.4% and 49.7% (from 35.9% and 33.0%).
6.3.2 Passive Evaluation. Table 20 presents the results of evaluating the extracted pas-
sive semantic forms for 1,422 verb lemmas shared by the induced lexicon and COMLEX.
Table 16
Mapping II: Merging of COMLEX and LFG syntactic functions.
Our syntactic functions COMLEX syntactic functions Merged function
SUBJ Subject SUBJ
OBJ Object OBJ
OBJ2 Obj2 OBJ2
OBL Obj3 OBL
OBL2 Obj4 OBL2
COMP Comp COMP
XCOMP Comp XCOMP
PART Part PART
352
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 17
Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 1%).
Precision Recall F-score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 72.1% 79.0% 58.5% 59.6% 64.6% 68.0%
Experiment 2 65.2% 77.1% 37.4% 50.4% 47.5% 61.0%
Experiment 2a 65.2% 76.4% 32.7% 44.5% 43.6% 56.3%
Experiment 3 65.2% 75.9% 15.2% 24.0% 24.7% 35.9%
Experiment 3a 65.2% 71.0% 13.6% 21.5% 22.5% 33.0%
Table 18
Results of Penn-II evaluation of active frames against COMLEX (relative threshold of 5%).
Precision Recall F-score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 72.1% 83.5% 58.5% 54.7% 64.6% 66.1%
Experiment 2 65.2% 81.4% 37.4% 44.8% 47.5% 57.8%
Experiment 2a 65.2% 80.9% 32.7% 39.0% 43.6% 52.6%
Experiment 3 65.2% 75.9% 15.2% 19.7% 24.7% 31.3%
Experiment 3a 65.2% 75.5% 13.6% 17.4% 22.5% 28.3%
We applied lexical-redundancy rules (Kaplan and Bresnan 1982) to automatically con-
vert the active COMLEX frames to their passive counterparts: For example, subjects are
demoted to optional by oblique agents, and direct objects become subjects. The resulting
precision was very high (from 72.3% to 80.2%), and there was the expected drop in recall
when prepositional details were included (from 54.7% to 29.3%).
Table 19
Penn-II evaluation of active frames against COMLEX using p-dir list (relative threshold of 1%).
Mapping II Precision Recall F-score
Experiment 3 81.7% 40.8% 54.4%
Experiment 3a 83.1% 35.4% 49.7%
Table 20
Results of Penn-II evaluation of passive frames (relative threshold of 1%).
Passive Precision Recall F-score
Experiment 2 80.2% 54.7% 65.1%
Experiment 2a 79.7% 46.2% 58.5%
Experiment 3 72.6% 33.4% 45.8%
Experiment 3a 72.3% 29.3% 41.7%
353
Computational Linguistics Volume 31, Number 3
6.3.3 Absolute Thresholds. Many of the previous approaches discussed in Section 3 use
a limited number of verbs for evaluation, based on the verbs? absolute frequency in the
corpus. We carried out a similar experiment. Table 21 shows the results of Experiment
2 for all verbs, for the verb lemmas with an absolute frequency greater than 100, and
for verbs with a frequency greater than 200. The use of an absolute threshold results
in an increase in precision (from 77.1% to 82.3% and 81.7%), an increase in recall (from
50.4% to 60.8% to 58.7%), and an overall increase in F-score (from 61.0% to 69.9%
and 68.4%).
6.4 Penn-III (Mapping-II)
Recently we have applied our methodology to the Penn-III Treebank, a more balanced
corpus resource with a number of text genres. Penn-III consists of the WSJ section from
Penn-II as well as a parse-annotated subset of the Brown corpus. The Brown corpus
comprises 24,242 trees compiled from a variety of text genres including popular lore,
general fiction, science fiction, mystery and detective fiction, and humor. It has been
shown (Roland and Jurafsky 1998) that the subcategorization tendencies of verbs vary
across linguistic domains. Our aim, therefore, is to increase the scope of the induced
lexicon not only in terms of the verb lemmas for which there are entries, but also in
terms of the frames with which they co-occur. The f-structure annotation algorithm was
extended with only minor amendments to cover the parsed Brown corpus. The most
important of these was the way in which we distinguish between oblique and adjunct.
We noted in Section 4 that our method of assigning an oblique annotation in Penn-II
was precise, albeit conservative. Because of a change of annotation policy in Penn-III,
the -CLR tag (indicating a close relationship between a PP and the local syntactic head),
information which we had previously exploited, is no longer used. For Penn-III the
algorithm annotates all PPs which do not carry a Penn adverbial functional tag (such
as -TMP or -LOC) and occur as the sisters of the verbal head of a VP as obliques.
In addition, the algorithm annotates as obliques PPs associated with -PUT (locative
complements of the verb put) or -DTV (second object in ditransitives) tags.
When evaluating the application of the lexical extraction system on Penn-III, we
carried out two sets of experiments, identical in each case to those described for Penn-II
in Section 6.3, including the use of relative (1% and 5%) rather than absolute thresholds.
For the first set of experiments we evaluated the lexicon induced from the parse-
annotated Brown corpus only. This evaluation was performed for 2,713 active-verb
lemmas using the more fine-grained Mapping-II. Tables 22 and 23 show that the results
generally exceed the baseline, in some cases by almost 10%, similar to those recorded
for Penn-II (Tables 17 and 18). While the precision is slightly lower than that re-
ported for the experiments in Tables 17 and 18, in particular for Experiments 2, 2a, 3,
Table 21
Penn-II evaluation of active frames against COMLEX using absolute thresholds (Experiment 2).
Threshold Precision Recall F-score
All 77.1% 50.4% 61.0%
Threshold 100 82.3% 60.8% 69.9%
Threshold 200 81.7% 58.7% 68.4%
354
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Table 22
Results of Penn-III active frames (Brown Corpus only) COMLEX comparison (relative threshold
of 1%).
Precision Recall F-Score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 73.2% 79.2% 60.1% 60.0% 66.0% 68.2%
Experiment 2 66.0% 70.5% 37.5% 50.5% 47.8% 58.9%
Experiment 2a 66.0% 71.3% 32.7% 44.5% 43.7% 54.8%
Experiment 3 66.0% 64.3% 15.2% 23.1% 24.8% 34.0%
Experiment 3a 66.0% 64.1% 13.5% 20.7% 22.4% 31.3%
and 3a, in which details of obliques are included, the recall in each of these experi-
ments is slightly higher than that recorded for Penn-II. We conjecture that the main
reason for this is that the amended approach to the annotation of obliques is slightly
less precise and conservative than the largely -CLR-tag-driven approach taken for
Penn-II. Consequently we record an increase in recall and a drop in precision. This
trend is repeated in the second set of experiments. In this instance, we combined the
lexicon extracted from the WSJ with that extracted from the parse-annotated Brown
corpus, and evaluated the resulting resource for 3,529 active-verb lemmas. The results
are shown in Tables 24 and 25. The results compare very positively against the baseline.
The precision scores are lower (by between 1.5% and 9.7%) than those reported for
Penn-II (Tables 17 and 18). There has however been a significant increase in recall (up to
8.7%) and an overall increase in F-score (by up to 4.4%).
6.5 Error Analysis and Discussion
The work presented in this section highlights a number of issues associated with the
evaluation of automatically induced subcategorization frames against an existing exter-
nal gold standard, in this case COMLEX. While this evaluation approach is arguably
less labor-intensive than the manual construction of a custom-made gold standard,
it does introduce a number of difficulties into the evaluation procedure. It is a
nontrivial task to convert both the gold standard and the induced resource to a common
Table 23
Results of Penn-III active frames (Brown corpus only) COMLEX comparison (relative threshold
of 5%).
Precision Recall F-score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 73.2% 82.7% 60.1% 56.4% 66.0% 67.0%
Experiment 2 66.0% 74.6% 37.5% 46.1% 47.8% 57.0%
Experiment 2a 66.0% 76.0% 32.7% 40.0% 43.7% 52.4%
Experiment 3 66.0% 69.2% 15.2% 18.7% 24.8% 29.5%
Experiment 3a 66.0% 69.0% 13.5% 16.6% 22.4% 26.7%
355
Computational Linguistics Volume 31, Number 3
Table 24
Results of Penn-III active frames (Brown and WSJ) COMLEX comparison (relative threshold of
1%).
Precision Recall F-score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 71.2% 77.4% 62.9% 66.2% 66.8% 71.4%
Experiment 2 64.5% 70.4% 40.0% 58.0% 49.3% 63.6%
Experiment 2a 64.5% 71.5% 35.1% 51.9% 45.5% 60.2%
Experiment 3 64.5% 66.2% 17.0% 27.4% 26.8% 38.8%
Experiment 3a 64.5% 66.0% 15.1% 24.8% 24.5% 36.0%
format in order to facilitate evaluation. In addition, as our results show, the choice
of common format and mapping to it can affect the results. In COMLEX-LFG Map-
ping I (Section 6.2), we found that mapping from the induced lexicon to COMLEX
resulted in higher recall scores than those achieved when we (effectively) reversed the
mapping (COMLEX-LFG Mapping II [Section 6.3]). The first mapping is essentially a
conflation of our more fine-grained LFG grammatical functions with the more generic
COMLEX functions, while the second mapping tries to maintain as many distinctions
as possible.
Another drawback to using an existing external gold standard such as COMLEX
to evaluate an automatically induced subcategorization lexicon is that the resources
are not necessarily constructed from the same source data. As noted above, it is well doc-
umented (Roland and Jurafsky 1998) that subcategorization frames (and their frequen-
cies) vary across domains. We have extracted frames from two sources (the WSJ and the
Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury
News, the Brown corpus, several literary works from the Library of America, scientific
abstracts from the U.S. Department of Energy, and the WSJ. For this reason, it is likely
to contain a greater variety of subcategorization frames than our induced lexicon. It is
also possible that because of human error, COMLEX contains subcategorization frames
the validity of which are in doubt, for example, the overgeneration of subcategorized-for
directional prepositional phrases. This is because the aim of the COMLEX project was to
construct as complete a set of subcategorization frames as possible, even for infrequent
verbs. Lexicographers were allowed to extrapolate from the citations found, a procedure
Table 25
Results of Penn-III active frames (Brown and WSJ) COMLEX comparison (relative threshold of
5%).
Precision Recall F-score
Mapping II Baseline Induced Baseline Induced Baseline Induced
Experiment 1 71.2% 82.0% 62.9% 61.0% 66.8% 69.9%
Experiment 2 64.5% 74.3% 40.0% 53.5% 49.3% 62.2%
Experiment 2a 64.5% 76.4% 35.1% 45.1% 45.5% 56.7%
Experiment 3 64.5% 71.1% 17.0% 21.5% 26.8% 33.0%
Experiment 3a 64.5% 70.8% 15.1% 19.2% 24.5% 30.2%
356
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
which is bound to be less certain than the assignment of frames based entirely on exist-
ing examples. As a generalization, Briscoe (2001) notes that lexicons such as COMLEX
tend to demonstrate high precision but low recall. Briscoe and Carroll (1997) report
on manually analyzing an open-class vocabulary of 35,000 head words for predicate
subcategorization information and comparing the results against the subcategorization
details in COMLEX. Precision was quite high (95%), but recall was low (84%). This has
an effect on both the precision and recall scores of our system against COMLEX. In order
to ascertain the effect of using COMLEX as a gold standard for our induced lexicon,
we carried out some more-detailed error analysis, the results of which are summarized
in Table 26. We randomly selected 80 false negatives (fn) and 80 false positives (fp)
across a range of active frame types containing prepositional and particle detail taken
from Penn-III and manually examined them in order to classify them as ?correct? or
?incorrect.? Of the 80 fps, 33 were manually judged to be legitimate subcategorization
frames. For example, as Table 26 shows, there are a number of correct transitive verbs
([subj,obj]) in our automatically induced lexicon which are not included in COMLEX.
This examination was also useful in highlighting to us the frame types on which
the lexical extraction procedure was performing poorly, in our case, those containing
XCOMPs and those containing OBJ2S. Out of 80 fns, 14 were judged to be incorrect when
manually examined. These can be broken down as follows: one intransitive frame, three
ditransitive frames, three frames containing a COMP, and seven frames containing an
oblique were found to be invalid.
7. Lexical Accession Rates
In addition to evaluating the quality of our extracted semantic forms, we also examined
the rate at which they are induced. This can be expressed as a measure of the coverage
of the induced lexicon on new data. Following Hockenmaier, Bierner, and Baldridge
(2002), Xia (1999), and Miyao, Ninomiya, and Tsujii (2004), we extract a reference
lexicon from Sections 02?21 of the WSJ. We then compare this to a test lexicon from
Section 23. Table 27 shows the results of the evaluation of the coverage of an induced
lexicon for verbs only. There is a corresponding semantic form in the reference lexicon
for 89.89% of the verbs in Section 23. 10.11% of the entries in the test lexicon did not
appear in the reference lexicon. Within this group, we can distinguish between known
words, which have an entry in the reference lexicon, and unknown words, which do
not exist at all in the reference lexicon. In the same way we make the distinction
Table 26
Error analysis.
Frame type COMLEX: False negatives Induced: False positives
Correct Incorrect Correct Incorrect
[subj] 9 1 4 6
[subj, obj] 10 0 9 1
[subj, obj, obj2] 7 3 1 9
[.., xcomp, ..] 10 0 1 10
[.., comp, ..] 7 3 4 5
[.., obl, ..] 23 7 14 16
357
Computational Linguistics Volume 31, Number 3
Table 27
Coverage of induced lexicon (WSJ 02?21) on unseen data (WSJ 23) (verbs only).
Entries also in reference lexicon 89.89%
Entries not in reference lexicon 10.11%
Known words 7.85%
Known words, known frames 7.85%
Known words, unknown frames 0
Unknown words 2.32%
Unknown words, known frames 2.32%
Unknown words, unknown frames 0
between known frames and unknown frames. There are, therefore, four different cases
in which an entry may not appear in the reference lexicon. Table 27 shows that the
most common case is that of known verbs occurring with a different, although known,
subcategorization frame (7.85%).
The rate of accession may also be represented graphically. In Charniak (1996) and
Krotov et al (1998), it was observed that treebank grammars (CFGs extracted from
treebanks) are very large and grow with the size of the treebank. We were interested in
discovering whether the acquisition of lexical material from the same data displayed a
similar propensity. Figure 8 graphs the rate of induction of semantic form and CFG rule
types from Penn-III (the WSJ and parse-annotated Brown corpus combined). Because
of the variation in the size of sections between the Brown and the WSJ, we plotted
accession against word count. The first part of the graph (up to 1,004,414 words)
Figure 8
Comparison of accession rates for semantic form and CFG rule types for Penn-III (nonempty
frames) (WSJ followed by Brown).
358
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
represents the rate of accession from the WSJ, and the final 384,646 words are those
of the Brown corpus. The seven curves represent the following: The acquisition of
semantic form types (nonempty) for all syntactic categories with and without specific
preposition and particle information, the acquisition of semantic form types (non-
empty) for all verbs with and without specific preposition and particle information,
the number of lemmas associated with the extract semantic forms, and the acqui-
sition of CFG rule types. The curve representing the growth in the overall size of
the lexicon is similar in shape to that of the PCFG, while the rate of increase in
the number of verbal semantic forms (particularly when obliques and particles are
excluded) appears to slow more quickly. Figure 8 shows the effect of domain di-
versity from the Brown section in terms of increased growth rates for 1e+06 words
upward. Figure 9 depicts the same information, this time extracted from the Brown
section first followed by the WSJ. The curves are different, but similar trends are
represented. This time the effects of domain diversity for the Brown section are
discernible by comparing the absolute accession rate for the 0.4e+06 mark between
Figures 8 and 9.
Figure 10 shows the result when we abstract away from semantic forms (verb
frame combinations) to subcategorization frames and plot their rate of acces-
sion. The graph represents the growth rate of frame types for Penn-III (WSJ fol-
lowed by Brown and Brown followed by WSJ). The curve rises sharply initially
but gradually levels, practically flattening out, despite the increase in the number
of words. This reflects the information about Section 23 in Table 27, where we demon-
strate that although new verb frame combinations occur, all of the frame types in
Section 23 have been seen by the lexical extraction program in previous sections.
Figure 9
Comparison of accession rates for semantic form and CFG rule types for Penn-III (nonempty
frames) (Brown followed by WSJ).
359
Computational Linguistics Volume 31, Number 3
Figure 10
Accession rates for frame types (without prepositions and particles) for Penn-III.
Figure 11 shows that including information about prepositions and particles in the
frames results in an accession rate which continues to grow, albeit ever more slowly,
with the increase in size of the extraction data. This emphasizes the advantage of our
approach, which extracts frames containing such information without the limitation
of predefinition.
Figure 11
Accession rates for frame types for Penn-III.
360
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
8. Conclusions and Further Work
We have presented an algorithm for the extraction of semantic forms (or subcatego-
rization frames) from the Penn-II and Penn-III Treebanks, automatically annotated with
LFG f-structures. In contrast to many other approaches, ours does not predefine the sub-
categorization frames we extract. We have applied the algorithm to the WSJ sections of
Penn-II (50,000 trees) (O?Donovan et al 2004) and to the parse-annotated Brown corpus
of Penn-III (almost 25,000 additional trees). We extract syntactic-function-based subcat-
egorization frames (LFG semantic forms) and traditional CFG category-based frames, as
well as mixed-function-category-based frames. Unlike many other approaches to sub-
categorization frame extraction, our system properly reflects the effects of long-distance
dependencies. Also unlike many approaches, our method distinguishes between active
and passive frames. Finally, our system associates conditional probabilities with the
frames we extract. Making the distinction between the behavior of verbs in active and
passive contexts is particularly important for the accurate assignment of probabilities
to semantic forms. We carried out an extensive evaluation of the complete induced
lexicon against the full COMLEX resource. To our knowledge, this is the most extensive
qualitative evaluation of subcategorization extraction in English. The only evaluation of
a similar scale is that carried out by Schulte im Walde (2002b) for German. The results
reported here for Penn-II compare favorably against the baseline and, in fact, are an
improvement on those reported in O?Donovan et al (2004). The results for the larger,
more domain-diverse Penn-III lexicon are very encouraging, in some cases almost 15%
above the baseline. We believe our semantic forms are fine-grained, and by choosing
to evaluate against COMLEX, we set our sights high: COMLEX is considerably more
detailed than the OALD or LDOCE used for other earlier evaluations. Our error analysis
also revealed some interesting issues associated with using an external standard such as
COMLEX. In the future, we hope to evaluate the automatic annotations and extracted
lexicon against Propbank (Kingsbury and Palmer 2002).
Apart from the related approach of Miyao, Ninomiya, and Tsujii (2004), which
does not distinguish between argument and adjunct prepositional phrases, our
treebank and automatic f-structure annotation-based architecture for the automatic
acquisition of detailed subcategorization frames is quite unlike any of the architec-
tures presented in the literature. Subcategorization frames are reverse-engineered and
almost a byproduct of the automatic f-structure annotation algorithm. It is important
to realize that the induction of lexical resources is part of a larger project on the
acquisition of wide-coverage, robust, probabilistic, deep unification grammar resources
from treebanks Burke, Cahill, et al (2004b). We are already using the extracted seman-
tic forms in parsing new text with robust, wide-coverage probabilistic LFG grammar
approximations automatically acquired from the f-structure-annotated Penn-II tree-
bank, specifically in the resolution of LDDs, as described in Cahill, Burke, et al (2004).
We hope to be able to apply our lexical acquisition methodology beyond existing
parse-annotated corpora (Penn-II and Penn-III): New text is parsed by our probabilistic
LFG approximations into f-structures from which we can then extract further seman-
tic forms. The work reported here is part of the core components for bootstrapping
this approach.
In the shorter term, we intend to make the extracted subcategorization lexicons from
Penn-II and Penn-III available as a downloadable public-domain research resource.
We have also applied our more general unification grammar acquisition meth-
odology to the TIGER Treebank (Brants et al 2002) and Penn Chinese Treebank
(Xue, Chiou, and Palmer 2002), extracting wide-coverage, probabilistic LFG grammar
361
Computational Linguistics Volume 31, Number 3
approximations and lexical resources for German (Cahill et al 2003) and Chinese
(Burke, Lam, et al 2004). The lexical resources, however, have not yet been evaluated.
This, and much else, has to await further research.
Acknowledgments
The research reported here is partially
supported by Enterprise Ireland Basic
Research Grant SC/2001/186, an IRCSET
PhD fellowship award, and an IBM PhD
fellowship award. We are particularly
grateful to our anonymous reviewers, whose
insightful comments have helped to improve
this article considerably.
References
Ades, Anthony and Mark Steedman. 1982.
On the order of words. Linguistics and
Philosophy, 4(4):517? 558.
Boguraev, Branimir, Edward Briscoe,
John Carroll, David Carter, and
Claire Grover. 1987. The derivation of
a grammatically indexed lexicon from
the Longman Dictionary of Contemporary
English. In Proceedings of the 25th
Annual Meeting of the Association of
Computational Linguistics, pages 193?200,
Stanford, CA.
Brants, Sabine, Stefanie Dipper, Silvia
Hansen, Wolfgang Lezius, and George
Smith. 2002. The TIGER Treebank. In
Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol, Bulgaria.
Brent, Michael. 1993. From grammar to
lexicon: Unsupervised learning of lexical
syntax. Computational Linguistics,
19(2):203?222.
Bresnan, Joan. 2001. Lexical-Functional Syntax.
Blackwell, Oxford.
Briscoe, Edward. 2001. From dictionary to
corpus to self-organizing dictionary:
Learning valency associations in the face
of variation and change. In Proceedings of
Corpus Linguistics 2001, Lancaster, UK.
Briscoe, Edward and John Carroll. 1997.
Automatic extraction of subcategorization
from corpora. In Proceedings of the Fifth
ACL Conference on Applied Natural
Language Processing, pages 356?363,
Washington, DC.
Burke, Michael, Aoife Cahill, Ruth
O?Donovan, Josef van Genabith, and
Andy Way. 2004a. Evaluation of an
automatic annotation algorithm against
the PARC 700 Dependency Bank. In
Proceedings of the Ninth International
Conference on LFG, pages 101?121,
Christchurch, New Zealand.
Burke, Michael, Aoife Cahill, Ruth
O?Donovan, Josef van Genabith, and
Andy Way. 2004b. Treebank-based
acquisition of wide-coverage, probabilistic
LFG resources: Project overview, results
and evaluation. In Proceedings of the
Workshop ?Beyond Shallow Analyses?
Formalisms and Statistical Modelling
for Deep Analyses? at the First International
Joint Conference on Natural Language
Processing (IJCNLP-04), Hainan
Island, China.
Burke, Michael, Olivia Lam, Rowena
Chan, Aoife Cahill, Ruth O?Donovan,
Adams Bodomo, Josef van Genabith,
and Andy Way. 2004. Treebank-based
acquisition of a Chinese lexical-functional
grammar. In Proceedings of the 18th
Pacific Asia Conference on Language,
Information and Computation,
pages 161?172, Tokyo.
Cahill, Aoife, Michael Burke, Ruth
O?Donovan, Josef van Genabith, and Andy
Way. 2004. Long-distance dependency
resolution in automatically acquired
wide-coverage PCFG-based LFG
approximations. In Proceedings
of the 42nd Annual Meeting of the
Association of Computational Linguistics,
pages 320?327, Barcelona.
Cahill, Aoife, Martin Forst, Mairead
McCarthy, Ruth O?Donovan, Christian
Rohrer, Josef van Genabith, and Andy
Way. 2003. Treebank-based multilingual
unification-grammar development. In
Proceedings of the Workshop on Ideas and
Strategies for Multilingual Grammar
Development at the 15th ESS-LLI,
pages 17?24, Vienna.
Cahill, Aoife, Mairead McCarthy,
Michael Burke, Ruth O?Donovan,
Josef van Genabith, and Andy Way.
2004. Evaluating automatic F-structure
annotation for the Penn-II Treebank.
Journal of Research on Language and
Computation, 2(4):523?547.
Cahill, Aoife, Mairead McCarthy, Josef van
Genabith, and Andy Way. 2002. Parsing
text with a PCFG derived from Penn-II
with an automatic F-structure annotation
procedure. In Proceedings of the Seventh
International Conference on LFG, edited by
Miriam Butt and Tracy Holloway King.
CSLI Publications, Stanford, CA,
pages 76?95.
362
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
Carroll, Glenn and Mats Rooth. 1998. Valence
induction with a head-lexicalised PCFG. In
Proceedings of the Third Conference on
Empirical Methods in Natural Language
Processing, pages 36?45,
Granada, Spain.
Charniak, Eugene. 1996. Tree-bank
grammars. In AAAI-96: Proceedings of the
Thirteenth National Conference on Artificial
Intelligence. MIT Press, Cambridge, MA,
pages 1031?1036.
Chen, John and K. Vijay-Shanker. 2000.
Automated extraction of TAGs from the
Penn Treebank. In Proceedings of the 38th
Annual Meeting of the Association of
Computational Linguistics, pages 65?76,
Hong Kong.
Collins, Michael. 1997. Three generative
lexicalised models for statistical parsing. In
Proceedings of the 35th Annual Meeting of the
Association for Computational Linguistics,
pages 16?23, Madrid.
Crouch, Richard, Ron Kaplan, Tracy King,
and Stefan Riezler. 2002. A comparison
of evaluation metrics for a broad coverage
parser. In Proceedings of Workshop
?Beyond PARSEVAL? at Third International
Conference on Language Resources and
Evaluation, Las Palmas, Spain.
Dalrymple, Mary. 2001. Lexical Functional
Grammar. Volume 34 of Syntax and
Semantics. Academic Press, New York.
Dowty, David. 1982. Grammatical relations
and Montague grammar. In Pauline
Jacobson and Geoffrey Pullum, editors,
The Nature of Syntactic Representation.
Reidel, Dordrecht, The Netherlands,
pages 79?130.
Dudenredaktion, editor. 2001. DUDEN?Das
Stilworterbuch. [DUDEN?The Style
Dictionary]. Number 2 in Duden in zwo?lf
Banden [Duden in Twelve Volumes].
Dudenverlag, Mannheim, Germany.
Eckle, Judith. 1999. Linguistic Knowledge for
Automatic Lexicon Acquisition from German
Text Corpora. Ph.D. thesis, University of
Stuttgart, Germany.
Frank, Anette. 2000. Automatic F-structure
annotation of treebank trees. In Proceedings
of the Fifth International Conference on LFG,
Berkeley, CA, edited by Miriam Butt
and Tracy Holloway King. CSLI,
pages 139?160.
Grishman, Ralph, Catherine MacLeod, and
Adam Meyers. 1994. COMLEX syntax:
Building a computational lexicon. In
Proceedings of the 15th International
Conference on Computational Linguistics,
pages 268?272, Kyoto.
Hajic, Jan. 1998. Building a syntactically
annotated corpus: The Prague
Dependency Treebank. In Issues in Valency
and Meaning, edited by Eva Hajicova.
Karolinum, Prague, Czech Republic,
pages 106?132.
Hindle, Donald and Mats Rooth. 1993.
Ambiguity and lexical relations.
Computational Linguistics, 19(1):103?120.
Hockenmaier, Julia, Gann Bierner, and Jason
Baldridge. 2004. Extending the coverage of
a CCG system. Journal of Language and
Computation, 2(2):165?208.
Hornby, Albert, editor. 1980. Oxford Advanced
Learner?s Dictionary of Current English.
Oxford University Press, Oxford, UK.
Joshi, Aravind. 1988. Tree adjoining
grammars. In David Dowty, Lauri
Karttunen, and Arnold Zwicky, editors,
Natural Language Parsing. Cambridge
University Press, Cambridge,
pages 206?250.
Kaplan, Ronald and Joan Bresnan. 1982.
Lexical functional grammar: A formal
system for grammatical representation. In
Joan Bresnan, editor, The Mental
Representation of Grammatical Relations. MIT
Press, Cambridge, MA, pages 173?281.
King, Tracy Holloway, Richard Crouch,
Stefan Riezler, Mary Dalrymple, and
Ronald Kaplan. 2003. The PARC 700
Dependency Bank. In Proceedings of the
Fourth International Workshop on
Linguistically Interpreted Corpora, Budapest.
Kingsbury, Paul and Martha Palmer. 2002.
From Treebank to PropBank. In Proceedings
of the Third International Conference on
Language Resources and Evaluation
(LREC-2002), Las Palmas, Spain.
Kinyon, Alexandra and Carlos Prolo. 2002.
Identifying verb arguments and their
syntactic function in the Penn Treebank. In
Proceedings of the Third LREC Conference,
pages 1982?1987, Las Palmas, Spain.
Korhonen, Anna. 2002. Subcategorization
acquisition. As Technical Report
UCAM-CL-TR-530, Computer Laboratory,
University of Cambridge, UK.
Krotov, Alexander, Mark Hepple, Robert
Gaizauskas, and Yorick Wilks. 1998.
Compacting the Penn Treebank grammar.
In Proceedings of the 36th Annual Meeting of
the Association for Computational Linguistics
and 17th International Conference on
Computational Linguistics, pages 669?703,
Montreal.
Levin, Beth. 1993. English Verb Classes and
Alternations. University of Chicago Press,
Chicago.
363
Computational Linguistics Volume 31, Number 3
MacLeod, Catherine, Ralph Grishman, and
Adam Meyers. 1994. The Comlex Syntax
Project: The first year. In Proceedings of the
ARPA Workshop on Human Language
Technology, pages 669?703, Princeton.
Magerman, David. 1994. Natural Language
Parsing as Statistical Pattern Recognition.
Ph.D. thesis, Stanford University,
Stanford, CA.
Magerman, David. 1995. Statistical decision
tree models for parsing. In Proceedings of
the 33rd Annual Meeting for the Association
of Computational Linguistics, pages 276?283,
Cambridge, MA.
Manning, Christopher. 1993. Automatic
acquisition of a large subcategorisation
dictionary from corpora. In Proceedings of
the 31st Annual Meeting of the Association for
Computational Linguistics, pages 235?242,
Columbus, OH.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Mark
Ferguson, Karen Katz, and Britta
Schasberger. 1994. The Penn Treebank:
Annotating predicate argument structure.
In Proceedings of the ARPA Human Language
Technology Workshop, Princeton.
Marinov, Svetoslav and Cecilia Hemming.
2004. Automatic Extraction of
Subcategorization Frames from the
Bulgarian Tree Bank. Unpublished
manuscript, Graduate School of Language
Technology, Go?teborg, Sweden.
Meyers, Adam, Catherine MacLeod, and
Ralph Grishman. 1996. Standardization of
the complement/adjunct distinction.
In Proceedings of the Seventh
EURALEX International Conference,
Go?teborg, Sweden.
Miyao, Yusuke, Takashi Ninomiya, and
Jun?ichi Tsujii. 2004. Corpus-oriented
grammar development for acquiring a
head-driven phrase structure grammar
from the Penn Treebank. In Proceedings of
the First International Joint Conference on
Natural Language Processing (IJCNLP-04),
pages 390?398, Hainan Island, China.
Nakanishi, Hiroko, Yusuke Miyao, and
Jun?ichi Tsujii. 2004. Using inverse
lexical rules to acquire a wide-coverage
lexicalized grammar. In Proceedings
of the Workshop ?Beyond Shallow
Analyses?Formalisms and Statistical
Modelling for Deep Analyses? at the First
International Joint Conference on Natural
Language Processing (IJCNLP-04), Hainan
Island, China.
O?Donovan, Ruth, Michael Burke,
Aoife Cahill, Josef van Genabith, and
Andy Way. 2004. Large-scale induction
and evaluation of lexical resources from
the Penn-II Treebank. In Proceedings
of the 42nd Annual Meeting of the
Association of Computational Linguistics,
pages 368?375, Barcelona.
Pollard, Carl and Ivan Sag. 1994.
Head-Driven Phrase Structure Grammar.
University of Chicago Press, Chicago.
Proctor, Paul, editor. 1978. Longman
Dictionary of Contemporary English.
Longman, London.
Roland, Douglas and Daniel Jurafsky.
1998. How verb subcategorization
frequencies are affected by corpus
choice. In Proceedings of the 36th
Annual Meeting of the Association
for Computational Linguistics and
17th International Conference on
Computational Linguistics,
pages 1117?1121, Montreal.
Sadler, Louisa, Josef van Genabith,
and Andy Way. 2000. Automatic
F-structure annotation from the
AP Treebank. In Proceedings of the
Fifth International Conference on LFG,
Berkeley, CA, edited by Miriam
Butt and Tracy Holloway King. CSLI,
pages 226?243.
Sarkar, Anoop and Daniel Zeman. 2000.
Automatic extraction of subcategorization
frames for Czech. In Proceedings of the 19th
International Conference on Computational
Linguistics, pages 691?697, Saarbru?cken,
Germany.
Schulte im Walde, Sabine. 2002a. A
subcategorisation lexicon for German
verbs induced from a lexicalised PCFG. In
Proceedings of the Third LREC Conference,
pages 1351?1357, Las Palmas, Spain.
Schulte im Walde, Sabine. 2002b. Evaluating
verb subcategorisation frames learned by a
German statistical grammar against
manual definitions in the Duden
Dictionary. In Proceedings of the 10th
EURALEX International Congress,
pages 187?197, Copenhagen.
Simov, Kiril, Gergana Popova, and Petya
Osenova. 2002. HPSG-based syntactic
treebank of Bulgarian (BulTreeBank). In
Andrew Wilson, Paul Rayson, and Tony
McEnery, editors, A Rainbow of Corpora:
Corpus Linguistics and the Languages of the
World. Lincon-Europa, Munich,
pages 135?142.
Ushioda, Akira, David Evans, Ted Gibson,
and Alex Waibel. 1993. The Automatic
acquisition of frequencies of verb
subcategorization frames from tagged
364
O?Donovan et al Large-Scale Induction and Evaluation of Lexical Resources
corpora. In SIGLEX ACL Workshop on the
Acquisition of Lexical Knowledge from Text,
pages 95?106, Columbus, OH.
van Genabith, Josef, Louisa Sadler, and
Andy Way. 1999. Data-driven compilation
of LFG semantic forms. In EACL-99
Workshop on Linguistically Interpreted
Corpora (LINC-99), pages 69?76, Bergen,
Norway.
van Genabith, Josef, Andy Way, and Louisa
Sadler. 1999. Semi-automatic generation of
F-structures from Treebanks. In
Proceedings of the Fourth International
Conference on Lexical-Functional Grammar,
Manchester, UK. Available at
http://cslipublications.stanford.edu/.
Wauschkuhn, Oliver. 1999. Automatische
Extraktion von Verbvalenzen aus deutschen
Textkorpora [Automatic Extraction of Verb
Valence from German Text Corpora]. PhD
thesis, University of Stuttgart, Germany.
Xia, Fei. 1999. Extracting tree adjoining
grammars from bracketed corpora.
In Fifth Natural Language Processing
Pacific Rim Symposium (NLPRS-99),
Beijing, China.
Xue, Nianwen, Fu-Dong Chiou, and Martha
Palmer. 2002. Building a large-scale
annotated Chinese corpus. In Proceedings
of the 19th International Conference on
Computational Linguistics (COLING 2002),
Taipei, Taiwan.
365

Long-Distance Dependency Resolution in Automatically Acquired
Wide-Coverage PCFG-Based LFG Approximations
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef van Genabith, Andy Way
National Centre for Language Technology and School of Computing,
Dublin City University, Dublin, Ireland
{acahill,mburke,rodonovan,josef,away}@computing.dcu.ie
Abstract
This paper shows how finite approximations of
long distance dependency (LDD) resolution can be
obtained automatically for wide-coverage, robust,
probabilistic Lexical-Functional Grammar (LFG)
resources acquired from treebanks. We extract LFG
subcategorisation frames and paths linking LDD
reentrancies from f-structures generated automati-
cally for the Penn-II treebank trees and use them
in an LDD resolution algorithm to parse new text.
Unlike (Collins, 1999; Johnson, 2002), in our ap-
proach resolution of LDDs is done at f-structure
(attribute-value structure representations of basic
predicate-argument or dependency structure) with-
out empty productions, traces and coindexation in
CFG parse trees. Currently our best automatically
induced grammars achieve 80.97% f-score for f-
structures parsing section 23 of the WSJ part of the
Penn-II treebank and evaluating against the DCU
1051 and 80.24% against the PARC 700 Depen-
dency Bank (King et al, 2003), performing at the
same or a slightly better level than state-of-the-art
hand-crafted grammars (Kaplan et al, 2004).
1 Introduction
The determination of syntactic structure is an im-
portant step in natural language processing as syn-
tactic structure strongly determines semantic inter-
pretation in the form of predicate-argument struc-
ture, dependency relations or logical form. For a
substantial number of linguistic phenomena such
as topicalisation, wh-movement in relative clauses
and interrogative sentences, however, there is an im-
portant difference between the location of the (sur-
face) realisation of linguistic material and the loca-
tion where this material should be interpreted se-
mantically. Resolution of such long-distance de-
pendencies (LDDs) is therefore crucial in the de-
termination of accurate predicate-argument struc-
1Manually constructed f-structures for 105 randomly se-
lected trees from Section 23 of the WSJ section of the Penn-II
Treebank
ture, deep dependency relations and the construc-
tion of proper meaning representations such as log-
ical forms (Johnson, 2002).
Modern unification/constraint-based grammars
such as LFG or HPSG capture deep linguistic infor-
mation including LDDs, predicate-argument struc-
ture, or logical form. Manually scaling rich uni-
fication grammars to naturally occurring free text,
however, is extremely time-consuming, expensive
and requires considerable linguistic and computa-
tional expertise. Few hand-crafted, deep unification
grammars have in fact achieved the coverage and
robustness required to parse a corpus of say the size
and complexity of the Penn treebank: (Riezler et
al., 2002) show how a deep, carefully hand-crafted
LFG is successfully scaled to parse the Penn-II tree-
bank (Marcus et al, 1994) with discriminative (log-
linear) parameter estimation techniques.
The last 20 years have seen continuously increas-
ing efforts in the construction of parse-annotated
corpora. Substantial treebanks2 are now available
for many languages (including English, Japanese,
Chinese, German, French, Czech, Turkish), others
are currently under construction (Arabic, Bulgarian)
or near completion (Spanish, Catalan). Treebanks
have been enormously influential in the develop-
ment of robust, state-of-the-art parsing technology:
grammars (or grammatical information) automat-
ically extracted from treebank resources provide
the backbone of many state-of-the-art probabilis-
tic parsing approaches (Charniak, 1996; Collins,
1999; Charniak, 1999; Hockenmaier, 2003; Klein
and Manning, 2003). Such approaches are attrac-
tive as they achieve robustness, coverage and per-
formance while incurring very low grammar devel-
opment cost. However, with few notable exceptions
(e.g. Collins? Model 3, (Johnson, 2002), (Hocken-
maier, 2003) ), treebank-based probabilistic parsers
return fairly simple ?surfacey? CFG trees, with-
out deep syntactic or semantic information. The
grammars used by such systems are sometimes re-
2Or dependency banks.
ferred to as ?half? (or ?shallow?) grammars (John-
son, 2002), i.e. they do not resolve LDDs but inter-
pret linguistic material purely locally where it oc-
curs in the tree.
Recently (Cahill et al, 2002) showed how
wide-coverage, probabilistic unification grammar
resources can be acquired automatically from f-
structure-annotated treebanks. Many second gen-
eration treebanks provide a certain amount of
deep syntactic or dependency information (e.g. in
the form of Penn-II functional tags and traces)
supporting the computation of representations of
deep linguistic information. Exploiting this in-
formation (Cahill et al, 2002) implement an
automatic LFG f-structure annotation algorithm
that associates nodes in treebank trees with f-
structure annotations in the form of attribute-value
structure equations representing abstract predicate-
argument structure/dependency relations. From the
f-structure annotated treebank they automatically
extract wide-coverage, robust, PCFG-based LFG
approximations that parse new text into trees and
f-structure representations.
The LFG approximations of (Cahill et al, 2002),
however, are only ?half? grammars, i.e. like most
of their probabilistic CFG cousins (Charniak, 1996;
Johnson, 1999; Klein and Manning, 2003) they do
not resolve LDDs but interpret linguistic material
purely locally where it occurs in the tree.
In this paper we show how finite approxima-
tions of long distance dependency resolution can be
obtained automatically for wide-coverage, robust,
probabilistic LFG resources automatically acquired
from treebanks. We extract LFG subcategorisation
frames and paths linking LDD reentrancies from
f-structures generated automatically for the Penn-
II treebank trees and use them in an LDD resolu-
tion algorithm to parse new text. Unlike (Collins,
1999; Johnson, 2002), in our approach LDDs are
resolved on the level of f-structure representation,
rather than in terms of empty productions and co-
indexation on parse trees. Currently we achieve f-
structure/dependency f-scores of 80.24 and 80.97
for parsing section 23 of the WSJ part of the Penn-
II treebank, evaluating against the PARC 700 and
DCU 105 respectively.
The paper is structured as follows: we give a
brief introduction to LFG. We outline the automatic
f-structure annotation algorithm, PCFG-based LFG
grammar approximations and parsing architectures
of (Cahill et al, 2002). We present our subcategori-
sation frame extraction and introduce the treebank-
based acquisition of finite approximations of LFG
functional uncertainty equations in terms of LDD
paths. We present the f-structure LDD resolution
algorithm, provide results and extensive evaluation.
We compare our method with previous work. Fi-
nally, we conclude.
2 Lexical Functional Grammar (LFG)
Lexical-Functional Grammar (Kaplan and Bres-
nan, 1982; Dalrymple, 2001) minimally involves
two levels of syntactic representation:3 c-structure
and f-structure. C(onstituent)-structure represents
the grouping of words and phrases into larger
constituents and is realised in terms of a CF-
PSG grammar. F(unctional)-structure represents
abstract syntactic functions such as SUBJ(ect),
OBJ(ect), OBL(ique), closed and open clausal
COMP/XCOMP(lement), ADJ(unct), APP(osition)
etc. and is implemented in terms of recursive feature
structures (attribute-value matrices). C-structure
captures surface grammatical configurations, f-
structure encodes abstract syntactic information
approximating to predicate-argument/dependency
structure or simple logical form (van Genabith
and Crouch, 1996). C- and f-structures are re-
lated in terms of functional annotations (constraints,
attribute-value equations) on c-structure rules (cf.
Figure 1).
S
NP VP
U.N. V NP
signs treaty
[
SUBJ
[
PRED U.N.
]
PRED sign
OBJ
[
PRED treaty
]
]
S ? NP VP
?SUBJ=? ?=?
VP ? V NP
?=? ?OBJ=?
NP ? U.N V ? signs
?PRED=U.N. ?PRED=sign
Figure 1: Simple LFG C- and F-Structure
Uparrows point to the f-structure associated with the
mother node, downarrows to that of the local node.
The equations are collected with arrows instanti-
ated to unique tree node identifiers, and a constraint
solver generates an f-structure.
3 Automatic F-Structure Annotation
The Penn-II treebank employs CFG trees with addi-
tional ?functional? node annotations (such as -LOC,
-TMP, -SBJ, -LGS, . . . ) as well as traces and coin-
dexation (to indicate LDDs) as basic data structures.
The f-structure annotation algorithm of (Cahill et
3LFGs may also involve morphological and semantic levels
of representation.
al., 2002) exploits configurational, categorial, Penn-
II ?functional?, local head and trace information
to annotate nodes with LFG feature-structure equa-
tions. A slightly adapted version of (Magerman,
1994)?s scheme automatically head-lexicalises the
Penn-II trees. This partitions local subtrees of depth
one (corresponding to CFG rules) into left and right
contexts (relative to head). The annotation algo-
rithm is modular with four components (Figure 2):
left-right (L-R) annotation principles (e.g. leftmost
NP to right of V head of VP type rule is likely to be
an object etc.); coordination annotation principles
(separating these out simplifies other components
of the algorithm); traces (translates traces and coin-
dexation in trees into corresponding reentrancies in
f-structure ( 1 in Figure 3)); catch all and clean-up.
Lexical information is provided via macros for POS
tag classes.
L/R Context ? Coordination ? Traces ? Catch-All
Figure 2: Annotation Algorithm
The f-structure annotations are passed to a con-
straint solver to produce f-structures. Annotation
is evaluated in terms of coverage and quality, sum-
marised in Table 1. Coverage is near complete with
99.82% of the 48K Penn-II sentences receiving a
single, connected f-structure. Annotation quality is
measured in terms of precision and recall (P&R)
against the DCU 105. The algorithm achieves an
F-score of 96.57% for full f-structures and 94.3%
for preds-only f-structures.4
S
S-TPC- 1
NP
U.N.
VP
V
signs
NP
treaty
NP
Det
the
N
headline
VP
V
said
S
T- 1
?
?
?
?
?
?
?
TOPIC
[
SUBJ
[
PRED U.N.
]
PRED sign
OBJ
[
PRED treaty
]
]
1
SUBJ
[
SPEC the
PRED headline
]
PRED say
COMP 1
?
?
?
?
?
?
?
Figure 3: Penn-II style tree with LDD trace and cor-
responding reentrancy in f-structure
4Full f-structures measure all attribute-value pairs includ-
ing?minor? features such as person, number etc. The stricter
preds-only captures only paths ending in PRED:VALUE.
# frags # sent percent
0 85 0.176
1 48337 99.820
2 2 0.004
all preds
P 96.52 94.45
R 96.63 94.16
Table 1: F-structure annotation results for DCU 105
4 PCFG-Based LFG Approximations
Based on these resources (Cahill et al, 2002) de-
veloped two parsing architectures. Both generate
PCFG-based approximations of LFG grammars.
In the pipeline architecture a standard PCFG is
extracted from the ?raw? treebank to parse unseen
text. The resulting parse-trees are then annotated by
the automatic f-structure annotation algorithm and
resolved into f-structures.
In the integrated architecture the treebank
is first annotated with f-structure equations.
An annotated PCFG is then extracted where
each non-terminal symbol in the grammar
has been augmented with LFG f-equations:
NP[?OBJ=?] ? DT[?SPEC=?] NN[?=?] . Nodes
followed by annotations are treated as a monadic
category for grammar extraction and parsing.
Post-parsing, equations are collected from parse
trees and resolved into f-structures.
Both architectures parse raw text into ?proto? f-
structures with LDDs unresolved resulting in in-
complete argument structures as in Figure 4.
S
S
NP
U.N.
VP
V
signs
NP
treaty
NP
Det
the
N
headline
VP
V
said
?
?
?
?
?
TOPIC
[
SUBJ
[
PRED U.N.
]
PRED sign
OBJ
[
PRED treaty
]
]
SUBJ
[
SPEC the
PRED headline
]
PRED say
?
?
?
?
?
Figure 4: Shallow-Parser Output with Unresolved
LDD and Incomplete Argument Structure (cf. Fig-
ure 3)
5 LDDs and LFG FU-Equations
Theoretically, LDDs can span unbounded amounts
of intervening linguistic material as in
[U.N. signs treaty]1 the paper claimed . . . a source said []1.
In LFG, LDDs are resolved at the f-structure level,
obviating the need for empty productions and traces
in trees (Dalrymple, 2001), using functional uncer-
tainty (FU) equations. FUs are regular expressions
specifying paths in f-structure between a source
(where linguistic material is encountered) and a tar-
get (where linguistic material is interpreted seman-
tically). To account for the fronted sentential con-
stituents in Figures 3 and 4, an FU equation of the
form ? TOPIC = ? COMP* COMP would be required.
The equation states that the value of the TOPIC at-
tribute is token identical with the value of the final
COMP argument along a path through the immedi-
ately enclosing f-structure along zero or more COMP
attributes. This FU equation is annotated to the top-
icalised sentential constituent in the relevant CFG
rules as follows
S ? S NP VP
?TOPIC=? ?SUBJ=? ?=?
?TOPIC=?COMP*COMP
and generates the LDD-resolved proper f-structure
in Figure 3 for the traceless tree in Figure 4, as re-
quired.
In addition to FU equations, subcategorisation in-
formation is a crucial ingredient in LFG?s account
of LDDs. As an example, for a topicalised con-
stituent to be resolved as the argument of a local
predicate as specified by the FU equation, the local
predicate must (i) subcategorise for the argument in
question and (ii) the argument in question must not
be already filled. Subcategorisation requirements
are provided lexically in terms of semantic forms
(subcat lists) and coherence and completeness con-
ditions (all GFs specified must be present, and no
others may be present) on f-structure representa-
tions. Semantic forms specify which grammatical
functions (GFs) a predicate requires locally. For our
example in Figures 3 and 4, the relevant lexical en-
tries are:
V ? said ?PRED=say?? SUBJ, ? COMP?
V ? signs ?PRED=sign?? SUBJ, ? OBJ?
FU equations and subcategorisation requirements
together ensure that LDDs can only be resolved at
suitable f-structure locations.
6 Acquiring Lexical and LDD Resources
In order to model the LFG account of LDD resolu-
tion we require subcat frames (i.e. semantic forms)
and LDD resolution paths through f-structure. Tra-
ditionally, such resources were handcoded. Here we
show how they can be acquired from f-structure an-
notated treebank resources.
LFG distinguishes between governable (argu-
ments) and nongovernable (adjuncts) grammati-
cal functions (GFs). If the automatic f-structure
annotation algorithm outlined in Section 3 gen-
erates high quality f-structures, reliable seman-
tic forms can be extracted (reverse-engineered):
for each f-structure generated, for each level of
embedding we determine the local PRED value
and collect the governable, i.e. subcategoris-
able grammatical functions present at that level
of embedding. For the proper f-structure in
Figure 3 we obtain sign([subj,obj]) and
say([subj,comp]). We extract frames from
the full WSJ section of the Penn-II Treebank with
48K trees. Unlike many other approaches, our ex-
traction process does not predefine frames, fully
reflects LDDs in the source data-structures (cf.
Figure 3), discriminates between active and pas-
sive frames, computes GF, GF:CFG category pair-
as well as CFG category-based subcategorisation
frames and associates conditional probabilities with
frames. Given a lemma l and an argument list s, the
probability of s given l is estimated as:
P(s|l) := count(l, s)?n
i=1 count(l, si)
Table 2 summarises the results. We extract 3586
verb lemmas and 10969 unique verbal semantic
form types (lemma followed by non-empty argu-
ment list). Including prepositions associated with
the subcategorised OBLs and particles, this number
goes up to 14348. The number of unique frame
types (without lemma) is 38 without specific prepo-
sitions and particles, 577 with. F-structure anno-
tations allow us to distinguish passive and active
frames. Table 3 shows the most frequent seman-
tic forms for accept. Passive frames are marked
p. We carried out a comprehensive evaluation of
the automatically acquired verbal semantic forms
against the COMLEX Resource (Macleod et al,
1994) for the 2992 active verb lemmas that both re-
sources have in common. We report on the evalu-
ation of GF-based frames for the full frames with
complete prepositional and particle infomation. We
use relative conditional probability thresholds (1%
and 5%) to filter the selection of semantic forms
(Table 4). (O?Donovan et al, 2004) provide a more
detailed description of the extraction and evaluation
of semantic forms.
Without Prep/Part With Prep/Part
Lemmas 3586 3586
Sem. Forms 10969 14348
Frame Types 38 577
Active Frame Types 38 548
Passive Frame Types 21 177
Table 2: Verb Results
Semantic Form Occurrences Prob.
accept([obj,subj]) 122 0.813
accept([subj],p) 9 0.060
accept([comp,subj]) 5 0.033
accept([subj,obl:as],p) 3 0.020
accept([obj,subj,obl:as]) 3 0.020
accept([obj,subj,obl:from]) 3 0.020
accept([subj]) 2 0.013
accept([obj,subj,obl:at]) 1 0.007
accept([obj,subj,obl:for]) 1 0.007
accept([obj,subj,xcomp]) 1 0.007
Table 3: Semantic forms for the verb accept.
Threshold 1% Threshold 5%
P R F-Score P R F-Score
Exp. 73.7% 22.1% 34.0% 78.0% 18.3% 29.6%
Table 4: COMLEX Comparison
We further acquire finite approximations of FU-
equations. by extracting paths between co-indexed
material occurring in the automatically generated f-
structures from sections 02-21 of the Penn-II tree-
bank. We extract 26 unique TOPIC, 60 TOPIC-REL
and 13 FOCUS path types (with a total of 14,911 to-
ken occurrences), each with an associated probabil-
ity. We distinguish between two types of TOPIC-
REL paths, those that occur in wh-less constructions,
and all other types (c.f Table 5). Given a path p and
an LDD type t (either TOPIC, TOPIC-REL or FO-
CUS), the probability of p given t is estimated as:
P(p|t) := count(t, p)?n
i=1 count(t, pi)
In order to get a first measure of how well the ap-
proximation models the data, we compute the path
types in section 23 not covered by those extracted
from 02-21: 23/(02-21). There are 3 such path types
(Table 6), each occuring exactly once. Given that
the total number of path tokens in section 23 is 949,
the finite approximation extracted from 02-23 cov-
ers 99.69% of all LDD paths in section 23.
7 Resolving LDDs in F-Structure
Given a set of semantic forms s with probabilities
P(s|l) (where l is a lemma), a set of paths p with
P(p|t) (where t is either TOPIC, TOPIC-REL or FO-
CUS) and an f-structure f , the core of the algorithm
to resolve LDDs recursively traverses f to:
find TOPIC|TOPIC-REL|FOCUS:g pair; retrieve
TOPIC|TOPIC-REL|FOCUS paths; for each path p
with GF1 : . . . : GFn : GF, traverse f along GF1 : . . . :
GFn to sub-f-structure h; retrieve local PRED:l;
add GF:g to h iff
? GF is not present at h
wh-less TOPIC-REL # wh-less TOPIC-REL #
subj 5692 adjunct 1314
xcomp:adjunct 610 obj 364
xcomp:obj 291 xcomp:xcomp:adjunct 96
comp:subj 76 xcomp:subj 67
Table 5: Most frequent wh-less TOPIC-REL paths
02?21 23 23 /(02?21)
TOPIC 26 7 2
FOCUS 13 4 0
TOPIC-REL 60 22 1
Table 6: Number of path types extracted
? h together with GF is locally complete and co-
herent with respect to a semantic form s for l
rank resolution by P(s|l) ? P(p|t)
The algorithm supports multiple, interacting TOPIC,
TOPIC-REL and FOCUS LDDs. We use P(s|l) ?
P(p|t) to rank a solution, depending on how likely
the PRED takes semantic frame s, and how likely
the TOPIC, FOCUS or TOPIC-REL is resolved using
path p. The algorithm also supports resolution of
LDDs where no overt linguistic material introduces
a source TOPIC-REL function (e.g. in reduced rela-
tive clause constructions). We distinguish between
passive and active constructions, using the relevant
semantic frame type when resolving LDDs.
8 Experiments and Evaluation
We ran experiments with grammars in both the
pipeline and the integrated parsing architectures.
The first grammar is a basic PCFG, while A-PCFG
includes the f-structure annotations. We apply a
parent transformation to each grammar (Johnson,
1999) to give P-PCFG and PA-PCFG. We train
on sections 02-21 (grammar, lexical extraction and
LDD paths) of the Penn-II Treebank and test on sec-
tion 23. The only pre-processing of the trees that we
do is to remove empty nodes, and remove all Penn-
II functional tags in the integrated model. We evalu-
ate the parse trees using evalb. Following (Riezler et
al., 2002), we convert f-structures into dependency
triple format. Using their software we evaluate the
f-structure parser output against:
1. The DCU 105 (Cahill et al, 2002)
2. The full 2,416 f-structures automatically gen-
erated by the f-structure annotation algorithm
for the original Penn-II trees, in a CCG-style
(Hockenmaier, 2003) evaluation experiment
Pipeline Integrated
PCFG P-PCFG A-PCFG PA-PCFG
2416 Section 23 trees
# Parses 2416 2416 2416 2414
Lab. F-Score 75.83 80.80 79.17 81.32
Unlab. F-Score 78.28 82.70 81.49 83.28
DCU 105 F-Strs
All GFs F-Score (before LDD resolution) 79.82 79.24 81.12 81.20
All GFs F-Score (after LDD resolution) 83.79 84.59 86.30 87.04
Preds only F-Score (before LDD resolution) 70.00 71.57 73.45 74.61
Preds only F-Score (after LDD resolution) 73.78 77.43 78.76 80.97
2416 F-Strs
All GFs F-Score (before LDD resolution) 81.98 81.49 83.32 82.78
All GFs F-Score (after LDD resolution) 84.16 84.37 86.45 86.00
Preds only F-Score (before LDD resolution) 72.00 73.23 75.22 75.10
Preds only F-Score (after LDD resolution) 74.07 76.12 78.36 78.40
PARC 700 Dependency Bank
Subset of GFs following (Kaplan et al, 2004) 77.86 80.24 77.68 78.60
Table 7: Parser Evaluation
3. A subset of 560 dependency structures of the
PARC 700 Dependency Bank following (Ka-
plan et al, 2004)
The results are given in Table 7. The parent-
transformed grammars perform best in both archi-
tectures. In all cases, there is a marked improve-
ment (2.07-6.36%) in the f-structures after LDD res-
olution. We achieve between 73.78% and 80.97%
preds-only and 83.79% to 87.04% all GFs f-score,
depending on gold-standard. We achieve between
77.68% and 80.24% against the PARC 700 follow-
ing the experiments in (Kaplan et al, 2004). For
details on how we map the f-structures produced
by our parsers to a format similar to that of the
PARC 700 Dependency Bank, see (Burke et al,
2004). Table 8 shows the evaluation result broken
down by individual GF (preds-only) for the inte-
grated model PA-PCFG against the DCU 105. In
order to measure how many of the LDD reentran-
cies in the gold-standard f-structures are captured
correctly by our parsers, we developed evaluation
software for f-structure LDD reentrancies (similar
to Johnson?s (2002) evaluation to capture traces and
their antecedents in trees). Table 9 shows the results
with the integrated model achieving more than 76%
correct LDD reentrancies.
9 Related Work
(Collins, 1999)?s Model 3 is limited to wh-traces
in relative clauses (it doesn?t treat topicalisation,
focus etc.). Johnson?s (2002) work is closest to
ours in spirit. Like our approach he provides a fi-
nite approximation of LDDs. Unlike our approach,
however, he works with tree fragments in a post-
processing approach to add empty nodes and their
DEP. PRECISION RECALL F-SCORE
adjunct 717/903 = 79 717/947 = 76 78
app 14/15 = 93 14/19 = 74 82
comp 35/43 = 81 35/65 = 54 65
coord 109/143 = 76 109/161 = 68 72
det 253/264 = 96 253/269 = 94 95
focus 1/1 = 100 1/1 = 100 100
obj 387/445 = 87 387/461 = 84 85
obj2 0/1 = 0 0/2 = 0 0
obl 27/56 = 48 27/61 = 44 46
obl2 1/3 = 33 1/2 = 50 40
obl ag 5/11 = 45 5/12 = 42 43
poss 69/73 = 95 69/81 = 85 90
quant 40/55 = 73 40/52 = 77 75
relmod 26/38 = 68 26/50 = 52 59
subj 330/361 = 91 330/414 = 80 85
topic 12/12 = 100 12/13 = 92 96
topicrel 35/42 = 83 35/52 = 67 74
xcomp 139/160 = 87 139/146 = 95 91
OVERALL 83.78 78.35 80.97
Table 8: Preds-only results of PA-PCFG against the
DCU 105
antecedents to parse trees, while we present an ap-
proach to LDD resolution on the level of f-structure.
It seems that the f-structure-based approach is more
abstract (99 LDD path types against approximately
9,000 tree-fragment types in (Johnson, 2002)) and
fine-grained in its use of lexical information (sub-
cat frames). In contrast to Johnson?s approach, our
LDD resolution algorithm is not biased. It com-
putes all possible complete resolutions and order-
ranks them using LDD path and subcat frame prob-
abilities. It is difficult to provide a satisfactory com-
parison between the two methods, but we have car-
ried out an experiment that compares them at the
f-structure level. We take the output of Charniak?s
Pipeline Integrated
PCFG P-PCFG A-PCFG PA-PCFG
TOPIC
Precision (11/14) (12/13) (12/13) (12/12)
Recall (11/13) (12/13) (12/13) (12/13)
F-Score 0.81 0.92 0.92 0.96
FOCUS
Precision (0/1) (0/1) (0/1) (0/1)
Recall (0/1) (0/1) (0/1) (0/1)
F-Score 0 0 0 0
TOPIC-REL
Precision (20/34) (27/36) (34/42) (34/42)
Recall (20/52) (27/52) (34/52) (34/52)
F-Score 0.47 0.613 0.72 0.72
OVERALL 0.54 0.67 0.75 0.76
Table 9: LDD Evaluation on the DCU 105
Charniak -LDD res. +LDD res. (Johnson, 2002)
All GFs 80.86 86.65 85.16
Preds Only 74.63 80.97 79.75
Table 10: Comparison at f-structure level of LDD
resolution to (Johnson, 2002) on the DCU 105
parser (Charniak, 1999) and, using the pipeline
f-structure annotation model, evaluate against the
DCU 105, both before and after LDD resolution.
Using the software described in (Johnson, 2002) we
add empty nodes to the output of Charniak?s parser,
pass these trees to our automatic annotation algo-
rithm and evaluate against the DCU 105. The re-
sults are given in Table 10. Our method of resolv-
ing LDDs at f-structure level results in a preds-only
f-score of 80.97%. Using (Johnson, 2002)?s method
of adding empty nodes to the parse-trees results in
an f-score of 79.75%.
(Hockenmaier, 2003) provides CCG-based mod-
els of LDDs. Some of these involve extensive clean-
up of the underlying Penn-II treebank resource prior
to grammar extraction. In contrast, in our approach
we leave the treebank as is and only add (but never
correct) annotations. Earlier HPSG work (Tateisi
et al, 1998) is based on independently constructed
hand-crafted XTAG resources. In contrast, we ac-
quire our resources from treebanks and achieve sub-
stantially wider coverage.
Our approach provides wide-coverage, robust,
and ? with the addition of LDD resolution ? ?deep?
or ?full?, PCFG-based LFG approximations. Cru-
cially, we do not claim to provide fully adequate sta-
tistical models. It is well known (Abney, 1997) that
PCFG-type approximations to unification grammars
can yield inconsistent probability models due to
loss of probability mass: the parser successfully re-
turns the highest ranked parse tree but the constraint
solver cannot resolve the f-equations (generated in
the pipeline or ?hidden? in the integrated model)
and the probability mass associated with that tree is
lost. This case, however, is surprisingly rare for our
grammars: only 0.0018% (85 out of 48424) of the
original Penn-II trees (without FRAGs) fail to pro-
duce an f-structure due to inconsistent annotations
(Table 1), and for parsing section 23 with the in-
tegrated model (A-PCFG), only 9 sentences do not
receive a parse because no f-structure can be gen-
erated for the highest ranked tree (0.4%). Parsing
with the pipeline model, all sentences receive one
complete f-structure. Research on adequate prob-
ability models for unification grammars is impor-
tant. (Miyao et al, 2003) present a Penn-II tree-
bank based HPSG with log-linear probability mod-
els. They achieve coverage of 50.2% on section
23, as against 99% in our approach. (Riezler et
al., 2002; Kaplan et al, 2004) describe how a care-
fully hand-crafted LFG is scaled to the full Penn-II
treebank with log-linear based probability models.
They achieve 79% coverage (full parse) and 21%
fragement/skimmed parses. By the same measure,
full parse coverage is around 99% for our automat-
ically acquired PCFG-based LFG approximations.
Against the PARC 700, the hand-crafted LFG gram-
mar reported in (Kaplan et al, 2004) achieves an f-
score of 79.6%. For the same experiment, our best
automatically-induced grammar achieves an f-score
of 80.24%.
10 Conclusions
We presented and extensively evaluated a finite
approximation of LDD resolution in automati-
cally constructed, wide-coverage, robust, PCFG-
based LFG approximations, effectively turning the
?half?(or ?shallow?)-grammars presented in (Cahill
et al, 2002) into ?full? or ?deep? grammars. In
our approach, LDDs are resolved in f-structure, not
trees. The method achieves a preds-only f-score
of 80.97% for f-structures with the PA-PCFG in
the integrated architecture against the DCU 105
and 78.4% against the 2,416 automatically gener-
ated f-structures for the original Penn-II treebank
trees. Evaluating against the PARC 700 Depen-
dency Bank, the P-PCFG achieves an f-score of
80.24%, an overall improvement of approximately
0.6% on the result reported for the best hand-crafted
grammars in (Kaplan et al, 2004).
Acknowledgements
This research was funded by Enterprise Ireland Ba-
sic Research Grant SC/2001/186 and IRCSET.
References
S. Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4):597?
618.
M. Burke, A. Cahill, R. O?Donovan, J. van Gen-
abith, and A. Way 2004. The Evaluation of
an Automatic Annotation Algorithm against the
PARC 700 Dependency Bank. In Proceedings
of the Ninth International Conference on LFG,
Christchurch, New Zealand (to appear).
A. Cahill, M. McCarthy, J. van Genabith, and A.
Way. 2002. Parsing with PCFGs and Auto-
matic F-Structure Annotation. In Miriam Butt
and Tracy Holloway King, editors, Proceedings
of the Seventh International Conference on LFG,
pages 76?95. CSLI Publications, Stanford, CA.
E. Charniak. 1996. Tree-Bank Grammars. In
AAAI/IAAI, Vol. 2, pages 1031?1036.
E. Charniak. 1999. A Maximum-Entropy-Inspired
Parser. Technical Report CS-99-12, Brown Uni-
versity, Providence, RI.
M. Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Uni-
versity of Pennsylvania, Philadelphia, PA.
M. Dalrymple. 2001. Lexical-Functional Gram-
mar. San Diego, CA; London Academic Press.
J. Hockenmaier. 2003. Parsing with Generative
models of Predicate-Argument Structure. In Pro-
ceedings of the 41st Annual Conference of the
Association for Computational Linguistics, pages
359?366, Sapporo, Japan.
M. Johnson. 1999. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613?632.
M. Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their
antecedents. In Proceedings of the 40th Annual
Meeting of the Association for Computational
Linguistics, pages 136?143, Philadelphia, PA.
R. Kaplan and J. Bresnan. 1982. Lexical Func-
tional Grammar, a Formal System for Grammat-
ical Representation. In The Mental Representa-
tion of Grammatical Relations, pages 173?281.
MIT Press, Cambridge, MA.
R. Kaplan, S. Riezler, T. H. King, J. T. Maxwell,
A. Vasserman, and R. Crouch. 2004. Speed and
accuracy in shallow and deep stochastic parsing.
In Proceedings of the Human Language Tech-
nology Conference and the 4th Annual Meeting
of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 97?
104, Boston, MA.
T.H. King, R. Crouch, S. Riezler, M. Dalrymple,
and R. Kaplan. 2003. The PARC700 dependency
bank. In Proceedings of the EACL03: 4th Inter-
national Workshop on Linguistically Interpreted
Corpora (LINC-03), pages 1?8, Budapest.
D. Klein and C. Manning. 2003. Accurate Unlexi-
calized Parsing. In Proceedings of the 41st An-
nual Meeting of the Association for Computa-
tional Linguistics (ACL?02), pages 423?430, Sap-
poro, Japan.
C. Macleod, A. Meyers, and R. Grishman. 1994.
The COMLEX Syntax Project: The First Year.
In Proceedings of the ARPA Workshop on Human
Language Technology, pages 669-703, Princeton,
NJ.
D. Magerman. 1994. Natural Language Parsing as
Statistical Pattern Recognition. PhD thesis, Stan-
ford University, CA.
M. Marcus, G. Kim, M.A. Marcinkiewicz, R. Mac-
Intyre, A. Bies, M. Ferguson, K. Katz, and B.
Schasberger. 1994. The Penn Treebank: Anno-
tating Predicate Argument Structure. In Proceed-
ings of the ARPA Workshop on Human Language
Technology, pages 110?115, Princeton, NJ.
Y. Miyao, T. Ninomiya, and J. Tsujii. 2003. Proba-
bilistic modeling of argument structures includ-
ing non-local dependencies. In Proceedings of
the Conference on Recent Advances in Natural
Language Processing (RANLP), pages 285?291,
Borovets, Bulgaria.
R. O?Donovan, M. Burke, A. Cahill, J. van Gen-
abith, and A. Way. 2004. Large-Scale Induc-
tion and Evaluation of Lexical Resources from
the Penn-II Treebank. In Proceedings of the 42nd
Annual Conference of the Association for Com-
putational Linguistics (ACL-04), Barcelona.
S. Riezler, T.H. King, R. Kaplan, R. Crouch,
J. T. Maxwell III, and M. Johnson. 2002. Pars-
ing the Wall Street Journal using a Lexical-
Functional Grammar and Discriminative Estima-
tion Techniques. In Proceedings of the 40th An-
nual Conference of the Association for Compu-
tational Linguistics (ACL-02), pages 271?278,
Philadelphia, PA.
Y. Tateisi, K. Torisawa, Y. Miyao, and J. Tsujii.
1998. Translating the XTAG English Grammar
to HPSG. In 4th International Workshop on Tree
Adjoining Grammars and Related Frameworks,
Philadelphia, PA, pages 172?175.
J. van Genabith and R. Crouch. 1996. Direct
and Underspecified Interpretations of LFG f-
Structures. In Proceedings of the 16th Interna-
tional Conference on Computational Linguistics
(COLING), pages 262?267, Copenhagen.
Large-Scale Induction and Evaluation of Lexical Resources from the
Penn-II Treebank
Ruth O?Donovan, Michael Burke, Aoife Cahill, Josef van Genabith, Andy Way
National Centre for Language Technology and School of Computing
Dublin City University
Glasnevin
Dublin 9
Ireland
{rodonovan,mburke,acahill,josef,away}@computing.dcu.ie
Abstract
In this paper we present a methodology for ex-
tracting subcategorisation frames based on an
automatic LFG f-structure annotation algorithm
for the Penn-II Treebank. We extract abstract
syntactic function-based subcategorisation frames
(LFG semantic forms), traditional CFG category-
based subcategorisation frames as well as mixed
function/category-based frames, with or without
preposition information for obliques and particle in-
formation for particle verbs. Our approach does
not predefine frames, associates probabilities with
frames conditional on the lemma, distinguishes be-
tween active and passive frames, and fully reflects
the effects of long-distance dependencies in the
source data structures. We extract 3586 verb lem-
mas, 14348 semantic form types (an average of 4
per lemma) with 577 frame types. We present a
large-scale evaluation of the complete set of forms
extracted against the full COMLEX resource.
1 Introduction
Lexical resources are crucial in the construction
of wide-coverage computational systems based on
modern syntactic theories (e.g. LFG, HPSG, CCG,
LTAG etc.). However, as manual construction of
such lexical resources is time-consuming, error-
prone, expensive and rarely ever complete, it is of-
ten the case that limitations of NLP systems based
on lexicalised approaches are due to bottlenecks in
the lexicon component.
Given this, research on automating lexical acqui-
sition for lexically-based NLP systems is a partic-
ularly important issue. In this paper we present an
approach to automating subcategorisation frame ac-
quisition for LFG (Kaplan and Bresnan, 1982) i.e.
grammatical function-based systems. LFG has two
levels of structural representation: c(onstituent)-
structure, and f(unctional)-structure. LFG differ-
entiates between governable (argument) and non-
governable (adjunct) grammatical functions. Sub-
categorisation requirements are enforced through
semantic forms specifying the governable grammat-
ical functions required by a particular predicate (e.g.
FOCUS?(? SUBJ)(? OBLon)?). Our approach is
based on earlier work on LFG semantic form extrac-
tion (van Genabith et al, 1999) and recent progress
in automatically annotating the Penn-II treebank
with LFG f-structures (Cahill et al, 2004b). De-
pending on the quality of the f-structures, reliable
LFG semantic forms can then be generated quite
simply by recursively reading off the subcategoris-
able grammatical functions for each local pred
value at each level of embedding in the f-structures.
The work reported in (van Genabith et al, 1999)
was small scale (100 trees), proof of concept and
required considerable manual annotation work. In
this paper we show how the extraction process can
be scaled to the complete Wall Street Journal (WSJ)
section of the Penn-II treebank, with about 1 mil-
lion words in 50,000 sentences, based on the au-
tomatic LFG f-structure annotation algorithm de-
scribed in (Cahill et al, 2004b). In addition to ex-
tracting grammatical function-based subcategorisa-
tion frames, we also include the syntactic categories
of the predicate and its subcategorised arguments,
as well as additional details such as the prepositions
required by obliques, and particles accompanying
particle verbs. Our method does not predefine the
frames to be extracted. In contrast to many other
approaches, it discriminates between active and pas-
sive frames, properly reflects long distance depen-
dencies and assigns conditional probabilities to the
semantic forms associated with each predicate.
Section 2 reviews related work in the area of
automatic subcategorisation frame extraction. Our
methodology and its implementation are presented
in Section 3. Section 4 presents the results of our
lexical extraction. In Section 5 we evaluate the
complete extracted lexicon against the COMLEX
resource (MacLeod et al, 1994). To our knowl-
edge, this is the largest evaluation of subcategorisa-
tion frames for English. In Section 6, we conclude
and give suggestions for future work.
2 Related Work
Creating a (subcategorisation) lexicon by hand is
time-consuming, error-prone, requires considerable
linguistic expertise and is rarely, if ever, complete.
In addition, a system incorporating a manually con-
structed lexicon cannot easily be adapted to specific
domains. Accordingly, many researchers have at-
tempted to construct lexicons automatically, espe-
cially for English.
(Brent, 1993) relies on local morphosyntactic
cues (such as the -ing suffix, except where such a
word follows a determiner or a preposition other
than to) in the untagged Brown Corpus as proba-
bilistic indicators of six different predefined subcat-
egorisation frames. The frames do not include de-
tails of specific prepositions. (Manning, 1993) ob-
serves that Brent?s recognition technique is a ?rather
simplistic and inadequate approach to verb detec-
tion, with a very high error rate?. Manning feeds
the output from a stochastic tagger into a finite state
parser, and applies statistical filtering to the parsing
results. He predefines 19 different subcategorisation
frames, including details of prepositions. Applying
this technique to approx. 4 million words of New
York Times newswire, Manning acquires 4900 sub-
categorisation frames for 3104 verbs, an average of
1.6 per verb. (Ushioda et al, 1993) run a finite state
NP parser on a POS-tagged corpus to calculate the
relative frequency of just six subcategorisation verb
classes. In addition, all prepositional phrases are
treated as adjuncts. For 1565 tokens of 33 selected
verbs, they report an accuracy rate of 83%.
(Briscoe and Carroll, 1997) observe that in the
work of (Brent, 1993), (Manning, 1993) and (Ush-
ioda et al, 1993), ?the maximum number of distinct
subcategorization classes recognized is sixteen, and
only Ushioda et al attempt to derive relative subcat-
egorization frequency for individual predicates?. In
contrast, the system of (Briscoe and Carroll, 1997)
distinguishes 163 verbal subcategorisation classes
by means of a statistical shallow parser, a classifier
of subcategorisation classes, and a priori estimates
of the probability that any verb will be a member
of those classes. More recent work by Korhonen
(2002) on the filtering phase of this approach has
improved results. Korhonen experiments with the
use of linguistic verb classes for obtaining more ac-
curate back-off estimates for use in hypothesis se-
lection. Using this extended approach, the average
results for 45 semantically classified test verbs eval-
uated against hand judgements are precision 87.1%
and recall 71.2%. By comparison, the average re-
sults for 30 verbs not classified semantically are pre-
cision 78.2% and recall 58.7%.
Carroll and Rooth (1998) use a hand-written
head-lexicalised context-free grammar and a text
corpus to compute the probability of particular sub-
categorisation scenarios. The extracted frames do
not contain details of prepositions.
More recently, a number of researchers have
applied similar techniques to derive resources for
other languages, especially German. One of these,
(Schulte im Walde, 2002), induces a computational
subcategorisation lexicon for over 14,000 German
verbs. Using sentences of limited length, she ex-
tracts 38 distinct frame types, which contain max-
imally three arguments each. The frames may op-
tionally contain details of particular prepositional
use. Her evaluation on over 3000 frequently occur-
ring verbs against the German dictionary Duden -
Das Stilwo?rterbuch is similar in scale to ours and is
discussed further in Section 5.
There has also been some work on extracting
subcategorisation details from the Penn Treebank.
(Kinyon and Prolo, 2002) introduce a tool which
uses fine-grained rules to identify the arguments,
including optional arguments, of each verb occur-
rence in the Penn Treebank, along with their syn-
tactic functions. They manually examined the 150+
possible sequences of tags, both functional and cat-
egorial, in Penn-II and determined whether the se-
quence in question denoted a modifier, argument or
optional argument. Arguments were then mapped
to traditional syntactic functions. As they do not in-
clude an evaluation, currently it is impossible to say
how effective this technique is.
(Xia et al, 2000) and (Chen and Vijay-Shanker,
2000) extract lexicalised TAGs from the Penn Tree-
bank. Both techniques implement variations on
the approaches of (Magerman, 1994) and (Collins,
1997) for the purpose of differentiating between
complement and adjunct. In the case of (Xia et al,
2000), invalid elementary trees produced as a result
of annotation errors in the treebank are filtered out
using linguistic heuristics.
(Hockenmaier et al, 2002) outline a method for
the automatic extraction of a large syntactic CCG
lexicon from Penn-II. For each tree, the algorithm
annotates the nodes with CCG categories in a top-
down recursive manner. In order to examine the
coverage of the extracted lexicon in a manner simi-
lar to (Xia et al, 2000), (Hockenmaier et al, 2002)
compared the reference lexicon acquired from Sec-
tions 02-21 with a test lexicon extracted from Sec-
tion 23 of the WSJ. It was found that the reference
CCG lexicon contained 95.09% of the entries in the
test lexicon, while 94.03% of the entries in the test
TAG lexicon also occurred in the reference lexicon.
Both approaches involve extensive correction and
clean-up of the treebank prior to lexical extraction.
3 Our Methodology
The first step in the application of our methodology
is the production of a treebank annotated with LFG
f-structure information. F-structures are feature
structures which represent abstract syntactic infor-
mation, approximating to basic predicate-argument-
modifier structures. We utilise the automatic anno-
tation algorithm of (Cahill et al, 2004b) to derive
a version of Penn-II where each node in each tree
is annotated with an LFG functional annotation (i.e.
an attribute value structure equation). Trees are tra-
versed top-down, and annotation is driven by cate-
gorial, basic configurational, trace and Penn-II func-
tional tag information in local subtrees of mostly
depth one (i.e. CFG rules). The annotation proce-
dure is dependent on locating the head daughter, for
which the scheme of (Magerman, 1994) with some
changes and amendments is used. The head is anno-
tated with the LFG equation ?=?. Linguistic gen-
eralisations are provided over the left (the prefix)
and the right (suffix) context of the head for each
syntactic category occurring as the mother node of
such heads. To give a simple example, the rightmost
NP to the left of a VP head under an S is likely to
be its subject (? SUBJ =?), while the leftmost NP
to the right of the V head of a VP is most proba-
bly its object (? OBJ =?). (Cahill et al, 2004b)
provide four sets of annotation principles, one for
non-coordinate configurations, one for coordinate
configurations, one for traces (long distance depen-
dencies) and a final ?catch all and clean up? phase.
Distinguishing between argument and adjunct is an
inherent step in the automatic assignment of func-
tional annotations.
The satisfactory treatment of long distance de-
pendencies by the annotation algorithm is impera-
tive for the extraction of accurate semantic forms.
The Penn Treebank employs a rich arsenal of traces
and empty productions (nodes which do not re-
alise any lexical material) to co-index displaced ma-
terial with the position where it should be inter-
preted semantically. The algorithm of (Cahill et
al., 2004b) translates the traces into corresponding
re-entrancies in the f-structure representation (Fig-
ure 1). Passive movement is also captured and ex-
pressed at f-structure level using a passive:+ an-
notation. Once a treebank tree is annotated with
feature structure equations by the annotation algo-
rithm, the equations are collected and passed to a
constraint solver which produces the f-structures.
In order to ensure the quality of the seman-
S
S-TPC- 1
NP
U.N.
VP
V
signs
NP
treaty
NP
Det
the
N
headline
VP
V
said
S
T- 1
?
?
?
?
?
?
?
TOPIC
[
SUBJ
[
PRED U.N.
]
PRED sign
OBJ
[
PRED treaty
]
]
1
SUBJ
[
SPEC the
PRED headline
]
PRED say
COMP 1
?
?
?
?
?
?
?
Figure 1: Penn-II style tree with long distance depen-
dency trace and corresponding reentrancy in f-structure
tic forms extracted by our method, we must first
ensure the quality of the f-structure annotations.
(Cahill et al, 2004b) measure annotation quality
in terms of precision and recall against manually
constructed, gold-standard f-structures for 105 ran-
domly selected trees from section 23 of the WSJ
section of Penn-II. The algorithm currently achieves
an F-score of 96.3% for complete f-structures and
93.6% for preds-only f-structures.1
Our semantic form extraction methodology is
based on the procedure of (van Genabith et al,
1999): For each f-structure generated, for each
level of embedding we determine the local PRED
value and collect the subcategorisable grammat-
ical functions present at that level of embed-
ding. Consider the f-structure in Figure 1. From
this we recursively extract the following non-
empty semantic forms: say([subj,comp]),
sign([subj,obj]). In effect, in both (van
Genabith et al, 1999) and our approach seman-
tic forms are reverse engineered from automatically
generated f-structures for treebank trees. We ex-
tract the following subcategorisable syntactic func-
tions: SUBJ, OBJ, OBJ2, OBLprep, OBL2prep, COMP,
XCOMP and PART. Adjuncts (e.g. ADJ, APP etc)
are not included in the semantic forms. PART
is not a syntactic function in the strict sense but
we capture the relevant co-occurrence patterns of
verbs and particles in the semantic forms. Just
as OBL includes the prepositional head of the PP,
PART includes the actual particle which occurs e.g.
add([subj,obj,part:up]).
In the work presented here we substantially ex-
tend the approach of (van Genabith et al, 1999) as
1Preds-only measures only paths ending in PRED:VALUE so
features such as number, person etc are not included.
regards coverage, granularity and evaluation: First,
we scale the approach of (van Genabith et al, 1999)
which was proof of concept on 100 trees to the full
WSJ section of the Penn-II Treebank. Second, our
approach fully reflects long distance dependencies,
indicated in terms of traces in the Penn-II Tree-
bank and corresponding re-entrancies at f-structure.
Third, in addition to abstract syntactic function-
based subcategorisation frames we compute frames
for syntactic function-CFG category pairs, both for
the verbal heads and their arguments and also gen-
erate pure CFG-based subcat frames. Fourth, our
method differentiates between frames captured for
active or passive constructions. Fifth, our method
associates conditional probabilities with frames.
In contrast to much of the work reviewed in the
previous section, our system is able to produce sur-
face syntactic as well as abstract functional subcat-
egorisation details. To incorporate CFG details into
the extracted semantic forms, we add an extra fea-
ture to the generated f-structures, the value of which
is the syntactic category of the pred at each level
of embedding. Exploiting this information, the ex-
tracted semantic form for the verb sign looks as fol-
lows: sign(v,[subj(np),obj(np)]).
We have also extended the algorithm to deal with
passive voice and its effect on subcategorisation be-
haviour. Consider Figure 2: not taking voice into
account, the algorithm extracts an intransitive frame
outlaw([subj]) for the transitive outlaw. To
correct this, the extraction algorithm uses the fea-
ture value pair passive:+, which appears in the
f-structure at the level of embedding of the verb in
question, to mark that predicate as occurring in the
passive: outlaw([subj],p).
In order to estimate the likelihood of the cooc-
currence of a predicate with a particular argument
list, we compute conditional probabilities for sub-
categorisation frames based on the number of token
occurrences in the corpus. Given a lemma l and an
argument list s, the probability of s given l is esti-
mated as:
P(s|l) := count(l, s)?n
i=1 count(l, si)
We use thresholding to filter possible error judge-
ments by our system. Table 1 shows the attested
semantic forms for the verb accept with their as-
sociated conditional probabilities. Note that were
the distinction between active and passive not taken
into account, the intransitive occurrence of accept
would have been assigned an unmerited probability.
subj : spec : quant : pred : all
adjunct : 2 : pred : almost
adjunct : 3 : pred : remain
participle : pres
4 : obj : adjunct : 5 : pred : cancer-causing
pers : 3
pred : asbestos
num : sg
pform : of
pers : 3
pred : use
num : pl
passive : +
adjunct : 1 : obj : pred : 1997
pform : by
xcomp : subj : spec: quant : pred : all
adjunct : 2 : pred : almost
...
...
passive : +
xcomp : subj : spec: quant : pred : all
adjunct : 2 : pred : almost
...
...
passive : +
pred : outlaw
tense : past
pred : be
pred : will
modal : +
Figure 2: Automatically generated f-structure
for the string wsj 0003 23?By 1997, almost
all remaining uses of cancer-causing
asbestos will be outlawed.?
Semantic Form Frequency Probability
accept([subj,obj]) 122 0.813
- accept([subj],p) 9 0.060
accept([subj,comp]) 5 0.033
- accept([subj,obl:as],p) 3 0.020
accept([subj,obj,obl:as]) 3 0.020
accept([subj,obj,obl:from]) 3 0.020
- accept([subj]) 2 0.013
accept([subj,obj,obl:at]) 1 0.007
accept([subj,obj,obl:for]) 1 0.007
accept([subj,obj,xcomp]) 1 0.007
Table 1: Semantic Forms for the verb accept marked
with p for passive use.
4 Results
We extract non-empty semantic forms2 for 3586
verb lemmas and 10969 unique verbal semantic
form types (lemma followed by non-empty argu-
ment list). Including prepositions associated with
the OBLs and particles, this number rises to 14348,
an average of 4.0 per lemma (Table 2). The num-
ber of unique frame types (without lemma) is 38
without specific prepositions and particles, 577 with
(Table 3). F-structure annotations allow us to distin-
guish passive and active frames.
5 COMLEX Evaluation
We evaluated our induced (verbal) semantic forms
against COMLEX (MacLeod et al, 1994). COM-
2Frames with at least one subcategorised grammatical func-
tion.
Without Prep/Part With Prep/Part
Sem. Form Types 10969 14348
Active 8516 11367
Passive 2453 2981
Table 2: Number of Semantic Form Types
Without Prep/Part With Prep/Part
# Frame Types 38 577
# Singletons 1 243
# Twice Occurring 1 84
# Occurring max. 5 7 415
# Occurring > 5 31 162
Table 3: Number of Distinct Frames for Verbs (not in-
cluding syntactic category for grammatical function)
LEX defines 138 distinct verb frame types without
the inclusion of specific prepositions or particles.
The following is a sample entry for the verb
reimburse:
(VERB :ORTH ?reimburse? :SUBC ((NP-NP)
(NP-PP :PVAL (?for?))
(NP)))
Each verb has a :SUBC feature, specifying
its subcategorisation behaviour. For example,
reimburse can occur with two noun phrases
(NP-NP), a noun phrase and a prepositional phrase
headed by ?for? (NP-PP :PVAL (?for?)) or a single
noun phrase (NP). Note that the details of the subject
noun phrase are not included in COMLEX frames.
Each of the complement types which make up the
value of the :SUBC feature is associated with a for-
mal frame definition which looks as follows:
(vp-frame np-np :cs ((np 2)(np 3))
:gs (:subject 1 :obj 2 :obj2 3)
:ex ?she asked him his name?)
The value of the :cs feature is the constituent struc-
ture of the subcategorisation frame, which lists the
syntactic CF-PSG constituents in sequence. The
value of the :gs feature is the grammatical struc-
ture which indicates the functional role played by
each of the CF-PSG constituents. The elements of
the constituent structure are indexed, and referenced
in the :gs field. This mapping between constituent
structure and functional structure makes the infor-
mation contained in COMLEX suitable as an eval-
uation standard for the LFG semantic forms which
we induce.
5.1 COMLEX-LFG Mapping
We devised a common format for our induced se-
mantic forms and those contained in COMLEX.
This is summarised in Table 4. COMLEX does
not distinguish between obliques and objects so we
converted Obji to OBLi as required. In addition,
COMLEX does not explicitly differentiate between
COMPs and XCOMPs, but does encode control in-
formation for any Comps which occur, thus allow-
ing us to deduce the distinction automatically. The
manually constructed COMLEX entries provided us
with a gold standard against which we evaluated the
automatically induced frames for the 2992 (active)
verbs that both resources have in common.
LFG COMLEX Merged
SUBJ Subject SUBJ
OBJ Object OBJ
OBJ2 Obj2 OBJ2
OBL Obj3 OBL
OBL2 Obj4 OBL2
COMP Comp COMP
XCOMP Comp XCOMP
PART Part PART
Table 4: COMLEX and LFG Syntactic Functions
We use the computed conditional probabilities to set
a threshold to filter the selection of semantic forms.
As some verbs occur less frequently than others we
felt it was important to use a relative rather than ab-
solute threshold. For a threshold of 1%, we disre-
gard any frames with a conditional probability of
less than or equal to 0.01. We carried out the evalu-
ation in a similar way to (Schulte im Walde, 2002).
The scale of our evaluation is comparable to hers.
This allows us to make tentative comparisons be-
tween our respective results. The figures shown in
Table 5 are the results of three different kinds of
evaluation with the threshold set to 1% and 5%. The
effect of the threshold increase is obvious in that
Precision goes up for each of the experiments while
Recall goes down.
For Exp 1, we excluded prepositional phrases en-
tirely from the comparison, i.e. assumed that PPs
were adjunct material (e.g. [subj,obl:for] becomes
[subj]). Our results are better for Precision than for
Recall compared to Schulte im Walde (op cit.), who
reports Precision of 74.53%, Recall of 69.74% and
an F-score of 72.05%.
Exp 2 includes prepositional phrases but not
parameterised for particular prepositions (e.g.
[subj,obl:for] becomes [subj,obl]). While our fig-
ures for Recall are again lower, our results for
Precision are considerably higher than those of
Schulte im Walde (op cit.) who recorded Preci-
sion of 60.76%, Recall of 63.91% and an F-score
of 62.30%.
For Exp. 3, we used semantic forms which con-
tained details of specific prepositions for any sub-
categorised prepositional phrase. Our Precision fig-
ures are again high (in comparison to 65.52% as
recorded by (Schulte im Walde, 2002)). However,
Threshold 1% Threshold 5%
P R F-Score P R F-Score
Exp. 1 79.0% 59.6% 68.0% 83.5% 54.7% 66.1%
Exp. 2 77.1% 50.4% 61.0% 81.4% 44.8% 57.8%
Exp. 2a 76.4% 44.5% 56.3% 80.9% 39.0% 52.6%
Exp. 3 73.7% 22.1% 34.0% 78.0% 18.3% 29.6%
Exp. 3a 73.3% 19.9% 31.3% 77.6% 16.2% 26.8%
Table 5: COMLEX Comparison
our Recall is very low (compared to the 50.83% that
Schulte im Walde (op cit.) reports). Consequently
our F-score is also low (Schulte im Walde (op cit.)
records an F-score of 57.24%). Experiments 2a and
3a are similar to Experiments 2 and 3 respectively
except they include the specific particle associated
with each PART.
5.1.1 Directional Prepositions
There are a number of possible reasons for our
low recall scores for Experiment 3 in Table 5. It
is a well-documented fact (Briscoe and Carroll,
1997) that subcategorisation frames (and their fre-
quencies) vary across domains. We have extracted
frames from one domain (the WSJ) whereas COM-
LEX was built using examples from the San Jose
Mercury News, the Brown Corpus, several literary
works from the Library of America, scientific ab-
stracts from the U.S. Department of Energy, and
the WSJ. For this reason, it is likely to contain
a greater variety of subcategorisation frames than
our induced lexicon. It is also possible that due
to human error COMLEX contains subcategorisa-
tion frames, the validity of which may be in doubt.
This is due to the fact that the aim of the COMLEX
project was to construct as complete a set of subcat-
egorisation frames as possible, even for infrequent
verbs. Lexicographers were allowed to extrapo-
late from the citations found, a procedure which
is bound to be less certain than the assignment of
frames based entirely on existing examples. Our re-
call figure was particularly low in the case of eval-
uation using details of prepositions (Experiment 3).
This can be accounted for by the fact that COMLEX
errs on the side of overgeneration when it comes to
preposition assignment. This is particularly true of
directional prepositions, a list of 31 of which has
been prepared and is assigned in its entirety by de-
fault to any verb which can potentially appear with
any directional preposition. In a subsequent exper-
iment, we incorporate this list of directional prepo-
sitions by default into our semantic form induction
process in the same way as the creators of COM-
LEX have done. Table 6 shows the results of this
experiment. As expected there is a significant im-
Precision Recall F-Score
Experiment 3 81.7% 40.8% 54.4%
Experiment 3a 83.1% 35.4% 49.7%
Table 6: COMLEX Comparison using p-dir(Threshold
of 1%)
Passive Precision Recall F-Score
Experiment 2 80.2% 54.7% 65.1%
Experiment 2a 79.7% 46.2% 58.5%
Experiment 3 72.6% 33.4% 45.8%
Experiment 3a 72.3% 29.3% 41.7%
Table 7: Passive evaluation (Threshold of 1%)
provement in the recall figure, being almost double
the figures reported in Table 5 for Experiments 3
and 3a.
5.1.2 Passive Evaluation
Table 7 presents the results of our evaluation of
the passive semantic forms we extract. It was
carried out for 1422 verbs which occur with pas-
sive frames and are shared by the induced lexicon
and COMLEX. As COMLEX does not provide ex-
plicit passive entries, we applied Lexical Redun-
dancy Rules (Kaplan and Bresnan, 1982) to auto-
matically convert the active COMLEX frames to
their passive counterparts. For example, the COM-
LEX entry see([subj,obj]) is converted to
see([subj]). The resulting precision is very
high, a slight increase on that for the active frames.
The recall score drops for passive frames (from
54.7% to 29.3%) in a similar way to that for active
frames when prepositional details are included.
5.2 Lexical Accession Rates
As well as evaluating the quality of our extracted
semantic forms, we also examine the rate at which
they are induced. (Charniak, 1996) and (Krotov et
al., 1998) observed that treebank grammars (CFGs
extracted from treebanks) are very large and grow
with the size of the treebank. We were interested in
discovering whether the acquisition of lexical mate-
rial on the same data displays a similar propensity.
Figure 3 displays the accession rates for the seman-
tic forms induced by our method for sections 0?24
of the WSJ section of the Penn-II treebank. When
we do not distinguish semantic forms by category,
all semantic forms together with those for verbs dis-
play smaller accession rates than for the PCFG.
We also examined the coverage of our system in
a similar way to (Hockenmaier et al, 2002). We ex-
tracted a verb-only reference lexicon from Sections
02-21 of the WSJ and subsequently compared this
to a test lexicon constructed in the same way from
 0
 5000
 10000
 15000
 20000
 25000
 0  5  10  15  20  25
N
o.
 o
f S
Fs
/R
ul
es
WSJ Section
All SF Frames
All Verbs
All SF Frames, no category
All Verbs, no category
PCFG
Figure 3: Accession Rates for Semantic Forms and CFG
Rules
Entries also in reference lexicon: 89.89%
Entries not in reference lexicon: 10.11%
Known words: 7.85%
- Known words, known frames: 7.85%
- Known words, unknown frames: -
Unknown words: 2.32%
- Unknown words, known frames: 2.32%
- Unknown words, unknown frames: -
Table 8: Coverage of induced lexicon on unseen
data (Verbs Only)
Section 23. Table 8 shows the results of this ex-
periment. 89.89% of the entries in the test lexicon
appeared in the reference lexicon.
6 Conclusions
We have presented an algorithm and its implementa-
tion for the extraction of semantic forms or subcate-
gorisation frames from the Penn-II Treebank, auto-
matically annotated with LFG f-structures. We have
substantially extended an earlier approach by (van
Genabith et al, 1999). The original approach was
small-scale and ?proof of concept?. We have scaled
our approach to the entire WSJ Sections of Penn-
II (50,000 trees). Our approach does not predefine
the subcategorisation frames we extract as many
other approaches do. We extract abstract syntac-
tic function-based subcategorisation frames (LFG
semantic forms), traditional CFG category-based
frames as well as mixed function-category based
frames. Unlike many other approaches to subcate-
gorisation frame extraction, our system properly re-
flects the effects of long distance dependencies and
distinguishes between active and passive frames.
Finally our system associates conditional probabil-
ities with the frames we extract. We carried out an
extensive evaluation of the complete induced lexi-
con (not just a sample) against the full COMLEX
resource. To our knowledge, this is the most exten-
sive qualitative evaluation of subcategorisation ex-
traction in English. The only evaluation of a similar
scale is that carried out by (Schulte im Walde, 2002)
for German. Our results compare well with hers.
We believe our semantic forms are fine-grained and
by choosing to evaluate against COMLEX we set
our sights high: COMLEX is considerably more
detailed than the OALD or LDOCE used for other
evaluations.
Currently work is under way to extend the cov-
erage of our acquired lexicons by applying our
methodology to the Penn-III treebank, a more bal-
anced corpus resource with a number of text gen-
res (in addition to the WSJ sections). It is impor-
tant to realise that the induction of lexical resources
is part of a larger project on the acquisition of
wide-coverage, robust, probabilistic, deep unifica-
tion grammar resources from treebanks. We are al-
ready using the extracted semantic forms in parsing
new text with robust, wide-coverage PCFG-based
LFG grammar approximations automatically ac-
quired from the f-structure annotated Penn-II tree-
bank (Cahill et al, 2004a). We hope to be able to
apply our lexical acquisition methodology beyond
existing parse-annotated corpora (Penn-II and Penn-
III): new text is parsed by our PCFG-based LFG ap-
proximations into f-structures from which we can
then extract further semantic forms. The work re-
ported here is part of the core component for boot-
strapping this approach.
As the extraction algorithm we presented derives
semantic forms at f-structure level, it is easily ap-
plied to other, even typologically different, lan-
guages. We have successfully ported our automatic
annotation algorithm to the TIGER Treebank, de-
spite German being a less configurational language
than English, and extracted wide-coverage, proba-
bilistic LFG grammar approximations and lexical
resources for German (Cahill et al, 2003). Cur-
rently, we are migrating the technique to Spanish,
which has freer word order than English and less
morphological marking than German. Preliminary
results have been very encouraging.
7 Acknowledgements
The research reported here is supported by Enter-
prise Ireland Basic Research Grant SC/2001/186
and an IRCSET PhD fellowship award.
References
M. Brent. 1993. From Grammar to Lexicon: Unsu-
pervised Learning of Lexical Syntax. Computa-
tional Linguistics, 19(2):203?222.
E. Briscoe and J. Carroll. 1997. Automatic Extrac-
tion of Subcategorization from Corpora. In Pro-
ceedings of the 5th ACL Conference on Applied
Natural Language Processing, pages 356?363,
Washington, DC.
A. Cahill, M. Forst, M. McCarthy, R. O?Donovan,
C. Rohrer, J. van Genabith, and A. Way.
2003. Treebank-Based Multilingual Unification-
Grammar Development. In Proceedings of the
Workshop on Ideas and Strategies for Multilin-
gual Grammar Development at the 15th ESSLLI,
pages 17?24, Vienna, Austria.
A. Cahill, M. Burke, R. O?Donovan, J. van Gen-
abith, and A. Way. 2004a. Long-Distance De-
pendency Resolution in Automatically Acquired
Wide-Coverage PCFG-Based LFG Approxima-
tions. In Proceedings of the 42nd Annual Con-
ference of the Association for Computational Lin-
guistics (ACL-04), Barcelona, Spain.
A. Cahill, M. McCarthy, M. Burke, R. O?Donovan,
J. van Genabith, and A. Way. 2004b. Evaluating
Automatic F-Structure Annotation for the Penn-
II Treebank. Journal of Research on Language
and Computation.
G. Carroll and M. Rooth. 1998. Valence Induc-
tion with a Head-Lexicalised PCFG. In Proceed-
ings of the 3rd Conference on Empirical Meth-
ods in Natural Language Processing, pages 36?
45, Granada, Spain.
E. Charniak. 1996. Tree-bank Grammars. In AAAI-
96: Proceedings of the Thirteenth National Con-
ference on Artificial Intelligence, MIT Press,
pages 1031?1036, Cambridge, MA.
J. Chen and K. Vijay-Shanker. 2000. Automated
Extraction of TAGs from the Penn Treebank. In
Proceedings of the 38th Annual Meeting of the
Association of Computational Linguistics, pages
65?76, Hong Kong.
M. Collins. 1997. Three generative lexicalised
models for statistical parsing. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics, pages 16?23.
J. Hockenmaier, G. Bierner, and J. Baldridge. 2002.
Extending the Coverage of a CCG System. Jour-
nal of Language and Computation, (2).
R. Kaplan and J. Bresnan. 1982. Lexical Func-
tional Grammar: A Formal System for Gram-
matical Representation. In Joan Bresnan, editor,
The Mental Representation of Grammatical Re-
lations, pages 206?250. MIT Press, Cambridge,
MA, Mannheim, 8th Edition.
A. Kinyon and C. Prolo. 2002. Identifying Verb Ar-
guments and their Syntactic Function in the Penn
Treebank. In Proceedings of the 3rd LREC Con-
ference, pages 1982?1987, Las Palmas, Spain.
A. Korhonen. 2002. Subcategorization Acquisition.
PhD thesis published as Techical Report UCAM-
CL-TR-530, Computer Laboratory, University of
Cambridge, UK.
A. Krotov, M. Hepple, R. Gaizauskas, and Y. Wilks.
1998. Compacting the Penn Treebank Grammar.
In Proceedings of COLING-ACL?98, pages 669?
703, Montreal, Canada.
C. MacLeod, R. Grishman, and A. Meyers. 1994.
The Comlex Syntax Project: The First Year. In
Proceedings of the ARPA Workshop on Human
Language Technology, pages 669?703, Prince-
ton, NJ.
D. Magerman. 1994. Natural Language Parsing
as Statistical Pattern Recognition. PhD Thesis,
Stanford University, CA.
C. Manning. 1993. Automatic Acquisition of a
Large Subcategorisation Dictionary from Cor-
pora. In Proceedings of the 31st Annual Meeting
of the Association for Computational Linguistics,
pages 235?242, Columbus, OH.
S. Schulte im Walde. 2002. Evaluating Verb Sub-
categorisation Frames learned by a German Sta-
tistical Grammar against Manual Definitions in
the Duden Dictionary. In Proceedings of the 10th
EURALEX International Congress, pages 187?
197, Copenhagen, Denmark.
A. Ushioda, D. Evans, T. Gibson, and A. Waibel.
1993. The Automatic Acquisition of Frequencies
of Verb Subcategorization Frames from Tagged
Corpora. In SIGLEX ACL Workshop on the Ac-
quisition of Lexical Knowledge from Text, pages
95?106, Columbus, OH.
J. van Genabith, A. Way, and L. Sadler. 1999. Data-
driven Compilation of LFG Semantic Forms. In
EACL-99 Workshop on Linguistically Interpreted
Corpora, pages 69?76, Bergen, Norway.
F. Xia, M. Palmer, and A. Joshi. 2000. A Uniform
Method of Grammar Extraction and its Applica-
tions. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP-2000), pages 53?62, Hong Kong.
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 221?224,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Adapting a WSJ-Trained Parser to Grammatically Noisy Text
Jennifer Foster, Joachim Wagner and Josef van Genabith
National Centre for Language Technology
Dublin City University
Ireland
jfoster, jwagner, josef@computing.dcu.ie
Abstract
We present a robust parser which is trained on
a treebank of ungrammatical sentences. The
treebank is created automatically by modify-
ing Penn treebank sentences so that they con-
tain one or more syntactic errors. We eval-
uate an existing Penn-treebank-trained parser
on the ungrammatical treebank to see how it
reacts to noise in the form of grammatical er-
rors. We re-train this parser on the training
section of the ungrammatical treebank, lead-
ing to an significantly improved performance
on the ungrammatical test sets. We show how
a classifier can be used to prevent performance
degradation on the original grammatical data.
1 Introduction
The focus in English parsing research in recent years
has moved from Wall Street Journal parsing to im-
proving performance on other domains. Our re-
search aim is to improve parsing performance on
text which is mildly ungrammatical, i.e. text which
is well-formed enough to be understood by people
yet which contains the kind of grammatical errors
that are routinely produced by both native and non-
native speakers of a language. The intention is not
to detect and correct the error, but rather to ignore
it. Our approach is to introduce grammatical noise
into WSJ sentences while retaining as much of the
structure of the original trees as possible. These
sentences and their associated trees are then used
as training material for a statistical parser. It is im-
portant that parsing on grammatical sentences is not
harmed and we introduce a parse-probability-based
classifier which allows both grammatical and un-
grammatical sentences to be accurately parsed.
2 Background
Various strategies exist to build robustness into the
parsing process: grammar constraints can be relaxed
(Fouvry, 2003), partial parses can be concatenated to
form a full parse (Penstein Rose? and Lavie, 1997),
the input sentence can itself be transformed until a
parse can be found (Lee et al, 1995), and mal-rules
describing particular error patterns can be included
in the grammar (Schneider and McCoy, 1998). For a
parser which tends to fail when faced with ungram-
matical input, such techniques are needed. The over-
generation associated with a statistical data-driven
parser means that it does not typically fail on un-
grammatical sentences. However, it is not enough
to return some analysis for an ungrammatical sen-
tence. If the syntactic analysis is to guide semantic
analysis, it must reflect as closely as possible what
the person who produced the sentence was trying to
express. Thus, while statistical, data-driven parsing
has solved the robustness problem, it is not clear that
it is has solved the accurate robustness problem.
The problem of adapting parsers to accurately
handle ungrammatical text is an instance of the do-
main adaptation problem where the target domain is
grammatically noisy data. A parser can be adapted
to a target domain by training it on data from the new
domain ? the problem is to quickly produce high-
quality training material. Our solution is to simply
modify the existing training material so that it re-
sembles material from the noisy target domain.
In order to tune a parser to syntactically ill-formed
text, a treebank is automatically transformed into an
ungrammatical treebank. This transformation pro-
cess has two parts: 1. the yield of each tree is trans-
formed into an ungrammatical sentence by introduc-
ing a syntax error; 2. each tree is minimally trans-
formed, but left intact as much as possible to reflect
the syntactic structure of the original ?intended? sen-
221
tence prior to error insertion. Artificial ungrammati-
calities have been used in various NLP tasks (Smith
and Eisner, 2005; Okanohara and Tsujii, 2007)
The idea of an automatically generated ungram-
matical treebank was proposed by Foster (2007).
Foster generates an ungrammatical version of the
WSJ treebank and uses this to train two statistical
parsers. The performance of both parsers signifi-
cantly improves on the artificially created ungram-
matical test data, but significantly degrades on the
original grammatical test data. We show that it
is possible to obtain significantly improved perfor-
mance on ungrammatical data without a concomi-
tant performance decline on grammatical data.
3 Generating Noisy Treebanks
Generating Noisy Sentences We apply the error
introduction procedure described in detail in Foster
(2007). Errors are introduced into sentences by ap-
plying the operations of word substitution, deletion
and insertion. These operations can be iteratively
applied to generate increasingly noisy sentences.
We restrict our attention to ungrammatical sentences
with a edit-distance of one or two words from the
original sentence, because it is reasonable to expect
a parser?s performance to degrade as the input be-
comes more ill-formed. The operations of substitu-
tion, deletion and insertion are not carried out en-
tirely at random, but are subject to some constraints
derived from an empirical study of ill-formed En-
glish sentences (Foster, 2005). Three types of word
substitution errors are produced: agreement errors,
real word spelling errors and verb form errors. Any
word that is not an adjective or adverb can be deleted
from any position within the input sentence, but
some part-of-speech tags are favoured over others,
e.g. it is more likely that a determiner will be deleted
than a noun. The error creation procedure can insert
an arbitrary word at any position within a sentence
but it has a bias towards inserting a word directly af-
ter the same word or directly after a word with the
same part of speech. The empirical study also in-
fluences the frequency at which particular errors are
introduced, with missing word errors being the most
frequent, followed by extra word errors, real word
spelling errors, agreement errors, and finally, verb
form errors. Table 1 shows examples of the kind of
ill-formed sentences that are produced when we ap-
ply the procedure to Wall Street Journal sentences.
Generating Trees for Noisy Sentences The tree
structures associated with the modified sentences are
also modified, but crucially, this modification is min-
imal, since a truly robust parser should return an
analysis for a mildly ungrammatical sentence that
remains as similar as possible to the analysis it re-
turns for the original grammatical sentence.
Assume that (1) is an original treebank tree for the
sentence A storm is brewing. Example (2) is then the
tree for the ungrammatical sentence containing an
is/it confusion. No part of the original tree structure
is changed apart from the yield.
(1) (S (NP A storm) (VP (VBZ is) (VP (VBG brewing))))
(2) (S (NP A storm) (VP (VBZ it) (VP (VBG brewing))))
An example of a missing word error is shown in
(3) and (4). A pre-terminal dominating an empty
node is introduced into the tree at the point where
the word has been omitted.
(3) (S (NP Annotators) (VP (VBP parse) (NP the sentences)))
(4) (S (NP Annotators) (VP (-NONE- 0) (NP the sentences)))
An example of an extra word error is shown in (5),
(6) and (7). For this example, two ungrammatical
trees, (6) and (7), are generated because there are
two possible positions in the original tree where the
extra word can be inserted which will result in a tree
with the yield He likes of the cake and which will not
result in the creation of any additional structure.
(5) (S (NP He) (VP (VBZ likes) (NP (DT the) (NN cake))))
(6) (S (NP He) (VP (VBZ likes) (IN of) (NP (DT the) (NN
cake))))
(7) (S (NP He) (VP (VBZ likes) (NP (IN of) (DT the) (NN
cake))))
4 Parser Adaptation Experiments
In order to obtain training data for our parsing ex-
periments, we introduce syntactic noise into the
usual WSJ training material, Sections 2-21, using
the procedures outlined in Section 3, i.e. for every
sentence-tree pair in WSJ2-21, we introduce an er-
ror into the sentence and then transform the tree so
that it covers the newly created ungrammatical sen-
tence. For 4 of the 20 sections in WSJ2-21, we apply
the noise introduction procedure to its own output to
222
Error Type WSJ00
Missing Word likely to bring new attention to the problem ? likely to new attention to the problem
Extra Word the $ 5.9 million it posted ? the $ 5.9 million I it posted
Real Word Spell Mr Vinken is chairman of Elsevier? Mr. Vinken if chairman of Elsevier
Agreement this event took place 35 years ago? these event took place 35 years ago
Verb Form But the Soviets might still face legal obstacles? But the Soviets might still faces legal obstacles
Table 1: Automatically Generated Ungrammatical Sentences
create even noisier data. Our first development set is
a noisy version of WSJ00, Noisy00, produced by ap-
plying the noise introduction procedure to the 1,921
sentences in WSJ00. Our second development set is
an even noisier version of WSJ00, Noisiest00, which
is created by applying our noise introduction proce-
dure to the output of Noisy00. We apply the same
process to WSJ23 to obtain our two test sets.
For all our parsing experiments, we use the June
2006 version of the two-stage parser reported in
Charniak and Johnson (2005). Evaluation is carried
out using Parseval labelled precision/recall. For ex-
tra word errors, there may be more than one gold
standard tree (see (6) and (7)). When this happens
the parser output tree is evaluated against all gold
standard trees and the maximum f-score is chosen.
We carry out five experiments. In the first ex-
periment, E0, we apply the parser, trained on well-
formed data, to noisy input. The purpose of E0 is to
ascertain how well a parser trained on grammatical
sentences, can ignore grammatical noise. E0 pro-
vides a baseline against which the subsequent ex-
perimental results can be judged. In the E1 experi-
ments, the parser is retrained using the ungrammati-
cal version of WSJ2-21. In experiment E1error, the
parser is trained on ungrammatical material only,
i.e. the noisy version of WSJ2-21. In experiment
E1mixed, the parser is trained on grammatical and
ungrammatical material, i.e. the original WSJ2-21 is
merged with the noisy WSJ2-21. In the E2 experi-
ments, a classifier is applied to the input sentence.
If the sentence is classified as ungrammatical, a ver-
sion of the parser that has been trained on ungram-
matical data is employed. In the E2ngram experi-
ment, we train a J48 decision tree classifier. Follow-
ing Wagner et al (2007), the decision tree features
are part-of-speech n-gram frequency counts, with n
ranging from 2 to 7 and with a subset of the BNC
as the frequency reference corpus. The decision tree
is trained on the original WSJ2-21 and the ungram-
matical WSJ2-21. In the E2prob experiment, the in-
put sentence is parsed with two parsers, the origi-
nal parser (the E0 parser) and the parser trained on
ungrammatical material (either the E1error or the
E1mixed parser). A very simple classifier is used
to decide which parser output to choose: if the E1
parser returns a higher parse probability for the most
likely tree than the E0 parser, the E1 parser output is
returned. Otherwise the E0 parser output is returned.
The baseline E0 results are in the first column of
Table 2. As expected, the performance of a parser
trained on well-formed input degrades when faced
with ungrammatical input. It is also not surprising
that its performance is worse on Noisiest00 (-8.8%
f-score) than it is on Noisy00 (-4.3%) since the Nois-
iest00 sentences contain two errors rather than one.
The E1 results occupy the second and third
columns of Table 2. An up arrow indicates a sta-
tistically significant improvement over the baseline
results, a down arrow a statistically significant de-
cline and a dash a change which is not statistically
significant (p < 0.01). Training the parser on un-
grammatical data has a positive effect on its perfor-
mance on Noisy00 and Noisiest00 but has a negative
effect on its performance on WSJ00. Training on a
combination of grammatical and ungrammatical ma-
terial gives the best results for all three development
sets. Therefore, for the E2 experiments we use the
E1mixed parser rather than the E1error parser.
The E2 results are shown in the last two columns
of Table 2 and the accuracy of the two classifiers in
Table 3. Over the three test sets, the E2prob classi-
fier outperforms the E2ngram classifier. Both classi-
fiers misclassify approximately 45% of the Noisy00
sentences. However, the sentences misclassified by
the E2prob classifier are those that are handled well
by the E0 parser, and this is reflected in the pars-
ing results for Noisy00. An important feature of the
223
Dev Set P R F P R F P R F P R F P R F
E0 E1-error E1-mixed E2prob E2ngram
WSJ00 91.5 90.3 90.9 91.0? 89.4 ? 90.2 91.3? 89.8 ? 90.5 91.5? 90.2? 90.9 91.3? 89.9? 90.6
Noisy00 87.5 85.6 86.6 89.4 ? 86.6 ? 88.0 89.4 ? 86.8 ? 88.1 89.1 ? 86.8 ? 87.9 88.7? 86.2? 87.5
Noisiest00 83.5 80.8 82.1 87.6 ? 83.6 ? 85.6 87.6 ? 83.8 ? 85.7 87.2 ? 83.7 ? 85.4 86.6? 83.0? 84.8
Table 2: Results of Parsing Experiments
Development Set E2prob E2ngram
WSJ00 76.7% 63.3%
Noisy00 55.1% 55.6%
Noisiest00 70.2% 66.0%
Table 3: E2 Classifier Accuracy
Test Set P R F P R F
E0 E2prob
WSJ23 91.7 90.8 91.3 91.7? 90.7? 91.2
Noisy23 87.4 85.6 86.5 89.2 ? 87.0 ? 88.1
Noisiest23 83.2 80.8 82.0 87.4 ? 84.1 ? 85.7
Table 4: Final Results for Section 23 Test Sets
E2prob classifier is that its use results in a constant
performance on the grammatical data - with no sig-
nificant degradation from the baseline.
Taking the E2prob results as our optimum, we
carry out the same experiment again on our WSJ23
test sets. The results are shown in Table 4. The same
effect can be seen for the test sets as for the devel-
opment sets - a significantly improved performance
on the ungrammatical data without an accompany-
ing performance decrease for the grammatical data.
The Noisy23 breakdown by error type is shown in
Table 5. The error type which the original parser is
most able to ignore is an agreement error. For this er-
ror type alone, the ungrammatical training material
seems to hinder the parser. The biggest improve-
ment occurs for real word spelling errors.
5 Conclusion
We have shown that it is possible to tune a WSJ-
trained statistical parser to ungrammatical text with-
Error Type P R F P R F
E0 E2-prob
Missing Word 88.5 83.7 86.0 88.9 84.3 86.5
Extra Word 87.2 89.4 88.3 89.2 89.7 89.4
Real Word Spell 84.3 83.0 83.7 89.5 88.2 88.9
Agreement 90.4 88.8 89.6 90.3 88.6 89.4
Verb Form 88.6 87.0 87.8 89.1 87.9 88.5
Table 5: Noisy23: Breakdown by Error Type
out affecting its performance on grammatical text.
This has been achieved using an automatically gen-
erated ungrammatical version of the WSJ treebank
and a simple binary classifier which compares parse
probabilities. The next step in this research is to see
how the method copes on ?real? errors - this will re-
quire manual parsing of a suitably large test set.
Acknowledgments We thank the IRCSET Em-
bark Initiative (postdoctoral fellowship P/04/232)
for supporting this research.
References
Eugene Charniak and Mark Johnson. 2005. Course-to-fine n-
best-parsing and maxent discriminative reranking. In Pro-
ceedings of ACL-2005.
Jennifer Foster. 2005. Good Reasons for Noting Bad Gram-
mar: Empirical Investigations into the Parsing of Ungram-
matical Written English. Ph.D. thesis, University of Dublin,
Trinity College.
Jennifer Foster. 2007. Treebanks gone bad: Parser evaluation
and retraining using a treebank of ungrammatical sentences.
IJDAR, 10(3-4), December.
Frederik Fouvry. 2003. Robust Processing for Constraint-
based Grammar Formalisms. Ph.D. thesis, University of Es-
sex.
Kong Joo Lee, Cheol Jung Kweon, Jungyun Seo, and Gil Chang
Kim. 1995. A robust parser based on syntactic information.
In Proceedings of EACL-1995.
Daisuke Okanohara and Jun?ichi Tsujii. 2007. A discrimi-
native language model with pseudo-negative examples. In
Proceedings of ACL-2007.
Carolyn Penstein Rose? and Alon Lavie. 1997. An efficient dis-
tribution of labor in a two stage robust interpretation process.
In Proceedings of EMNLP-1997.
David Schneider and Kathleen McCoy. 1998. Recognizing
syntactic errors in the writing of second language learners.
In Proceedings of ACL/COLING-1998.
Noah A. Smith and Jason Eisner. 2005. Contrastive Estima-
tion: Training Log-Linear Models on Unlabeled Data. In
Proceedings of ACL-2005.
Joachim Wagner, Jennifer Foster, and Josef van Genabith.
2007. A comparative evaluation of deep and shallow ap-
proaches to the automatic detection of common grammatical
errors. In Proceedings of EMNLP-CoNLL-2007.
224
CL for CALL in the Primary School 
Katrina Keogh, Thomas Koller, Monica Ward,  
Elaine U? Dhonnchadha , Josef  van Genabith 
School of Computing 
Dublin City University 
Dublin 9, Ireland 
{kkeogh, tkoller, mward}@computing.dcu.ie,  
Elaine.UiDhonnchadha@dcu.ie, josef@computing.dcu.ie 
 
Abstract 
This paper looks at how Computational 
Linguistics (CL) and Natural Language Processing 
(NLP) resources can be deployed in Computer-
Assisted Language Learning (CALL) materials for 
primary school learners.  We draw a broad 
distinction between CL and NLP technology and 
briefly review the use of CL/NLP in e-Learning in 
general, how it has been deployed in CALL to date 
and specifically in the primary school context.  We 
outline how CL/NLP resources can be used in a 
project to teach Irish and German to primary 
school children in Ireland. This paper focuses on 
the use of Finite State morphological analysis 
(FST) resources for Irish and Part of Speech (POS) 
taggers for German. 
1 Introduction 
CL/NLP has a lot to offer many disciplines. One 
particular area of interest is e-Learning for 
languages or more specifically Computer-Assisted 
Language Learning (CALL). CALL aims to 
develop useful learning tools with the focus on the 
learner. The following sections outline the use of 
CL/NLP in CALL (also known as Intelligent 
Computer-Assisted Language Learning - ICALL) 
for a particular target audience ? primary school 
students in Ireland.  
First we review CL/NLP in e-Learning and the 
case for using CL/NLP in CALL. Next we describe 
ICALL and the case for its use in primary school. 
Section 4 goes into detail on the CL/NLP 
technologies we use for primary school students 
learning Irish and German. 
2 CL/NLP in e-Learning 
2.1 CL/NLP ? A Broad Distinction 
To a first approximation CL/NLP technologies 
split into two broad categories ? A and B. Category 
A (sometimes referred to as CL proper) typically 
includes small coverage, proof of concept, often 
hand-crafted, knowledge- or rule-based systems. 
They are usually used to test a particular linguistic 
theory, tend to be of limited coverage and are often 
quite brittle. Example technologies include DCGs 
and many (but not all) formal grammar-based 
parsing and generation systems. 
Category B (sometimes referred to as NLP) 
typically includes broad coverage systems where 
the lingware is often (but not always ? see e.g. 
FST) automatically induced and processed using 
statistical approaches. They are usually large scale 
engineering applications and very robust. Example 
technologies include speech processing, HMM 
taggers, probabilistic parsing and FST. 
This distinction is, of course, nothing more than 
a useful over-generalisation with an entire and 
interesting grey area existing between the two 
extremes. Khader et al (2004), for example, show 
how a wide-coverage, robust rule-based system is 
used in CALL. In this paper we look at the 
suitability of type A and B CL/NLP technologies 
for primary school education, in the context of 
Ireland in particular. 
2.2 e-Learning  
CL is generally not to the fore in e-Learning, 
although it does have a potentially powerful role to 
play.  It can help to enhance the accessibility of 
online teaching material (particularly when the 
material is not in the learner?s L1), in analysing 
learner input and the automatic generation of 
simple feedback. It can also be used with 
Computer-Mediated Communication (CMC) 
environments.  However, to date, the use of 
CL/NLP in e-Learning in general has not been a 
main stream focus of either the Computational 
Linguistics or the e-Learning community nor has 
there been much CL/NLP technology transfer into 
commercially available and deployed systems.  
2.3 CALL 
Within the domain of e-Learning, the area with 
the greatest fit and potential deployment of 
CL/NLP resources is that of Computer-Assisted 
Language Learning (CALL). This paper focuses on 
asynchronous e-Learning for natural languages in 
the primary school context. CL/NLP resources 
lend themselves naturally to the domain of 
language learning, given that the ?raw material? in 
both fields is language. However, attempts to 
successfully marry the two fields have been 
limited.  Schulze (2003) outlines several reasons 
for this.  Computational Linguists are specifically 
interested in the use of the computer in analysing, 
generating and processing language.  They are 
interested in testing out linguistic theories and 
using the computer to confirm their hypotheses. 
Researchers in NLP tend to be interested in wide-
coverage, robust engineering approaches. For the 
most part, use of their tools for language 
learning/teaching applications is not high on their 
research agenda.  A review of COLING papers in 
the last twenty years reveals that there are very few 
papers that specifically deal with the use of 
CL/NLP in language learning.  Furthermore, as 
Schulze (2003) points out, within the unspoken 
hierarchy that exists in Computer Science 
departments throughout the world, working with 
CALL is considered less prestigious than say, 
working on cryptography. Thus, socio-cultural 
factors may have played a part in limiting the 
number of CL/NLP researchers interested in 
CALL. 
From a CALL researcher?s or practitioner?s 
point of view, attempts to integrate CL/NLP 
resources into CALL have not been very 
successful. Many remain unconvinced about the 
benefits of using CL/NLP techniques in CALL and 
whether they can be integrated successfully or not.  
They sometimes expect an ?all-singing, all-
dancing? machine and are disappointed 
/disillusioned with the results of ICALL research, 
especially when they incorporate category A CL 
technologies. CALL practitioners generally come 
from a language teaching background and are often 
more interested in pedagogy than technology.  
Some feel that the technical knowledge required to 
integrate CL/NLP tools is beyond their scope. 
They may be wary of claims from CL/NLP 
developers that a certain CL/NLP resource will be 
?ideal? for CALL, especially if they have heard 
such claims before.  Even if they are favourably 
disposed to the use of CL/NLP resources in CALL, 
it is often very difficult to reuse existing resources, 
as they demand that a certain (often non-standard) 
format be used for data (see Sections 4.2 and 5.2 
below).  Also, the interfaces to the resources may 
have assumed a techno-savvy or CL/NLP-savvy 
user, which mitigates against their (re)use.   
In summary, apart from notable exceptions (e.g. 
Glosser (Dokter & Nerbonne, 1998) and FreeText 
(2001), for various technical and non-technical 
reasons, CL/NLP resources have not been 
extensively deployed in main-stream CALL 
applications.  
 
One of the problems in using CL/NLP resources 
in CALL materials is that the coverage achieved by 
the CL/NLP tools has to be broad to be able to 
handle a general range of learner language. 
Furthermore, the resources must be robust as 
learner language will contain input that is not well-
formed and this can cause problems for some CL 
resources. Observations such as these point to type 
B NLP technologies as being the better type of 
technologies to employ in the context of language 
learning. However, below we argue that this is not 
necessarily the case. 
2.4 ICALL in the Primary School 
It may be natural to assume that CL/NLP 
resources customarily lend themselves to 
intermediate or advanced learners of a language, as 
they are more likely to have the linguistic 
competence to understand output generated by 
CL/NLP resources. Considering the other end of 
the language-learning spectrum, that of primary 
school learners, it may be perceived that CL/NLP 
resources could not be so easily deployed with 
linguistically less advanced learners - these 
students will not be interested in viewing 
concordances, morphological annotations or parse 
trees.  
However, it can be argued that there are certain 
natural circumstances supporting the use of even 
type A CL technology in CALL in this 
environment.  Firstly, in comparison to adults, 
young learners have limited first language (L1) 
performance (Brown, 1994). The target primary 
school students are aged between 7 and 13 years 
(second to sixth class in the Irish primary school 
system). They tend to produce simpler sentences 
and have a smaller range of vocabulary than an 
adult. These L1 features have a number of 
implications ? the students? L1 knowledge further 
constrains their emerging L2 production. Complex 
linguistic constructs are less likely to transfer into 
the target language. Effectively, the target 
language amounts to a controlled language. 
Controlled languages are easier suited to type A 
CL systems and produce better results (Arnold et 
al., 1994).  
Secondly, the students? target language(s) (Irish 
and German in this context) represent a limited 
domain or sublanguage. The Irish curriculum is 
followed in primary schools from the age of 4/5. 
Students can take German (where it?s available) 
during their senior years of primary school (aged 
10-13) and the language domain is limited to a 2 
year beginners? curriculum. It is possible to 
anticipate students? L2 knowledge, especially since 
they have been following set curricula. Machine 
Translation (MT) can be used to highlight an 
example of the success of sublanguages with 
CL/NLP. The M?t?o translation system is used 
successfully in Canada to translate weather 
forecasts bi-directionally between French and 
English (Hutchins and Somers, 1992). The 
?weather? sublanguage has a small vocabulary and 
uses a telegraphic style of writing and omits tense.  
Primary school students? L1 and L2 performance 
characteristics ? controlled language and limited 
domain ? imply that some scalability problems that 
are sometimes encountered in certain type A CL 
resources can be avoided. 
While primary school learners will not be 
interested in viewing concordances or parse trees ? 
technology can be used but hidden from the 
learner, to generate exercises and learner feedback 
and to present students with an animation based on 
information computed by the underlying CL/NLP 
engines embedded (but not visible) in the CALL 
application. In this way the learner will benefit 
from the technologies but not be confused by 
linguistic elements that are beyond their capacity 
as young learners.  
3 CL/NLP Resources for CALL 
In this paper we look at how CL/NLP resources 
can be integrated into CALL materials in general, 
as well as specifically for Primary Schools in 
Ireland, with a focus on CALL materials for Irish 
and German. This section will briefly outline how 
a range of CL/NLP resources can be used in this 
environment, while later sections will focus on the 
use of specific CL/NLP resources in more detail.  
We return to our dichotomy of A- and B-type 
CL/NLP systems outlined in Section 2.1. ICALL 
systems have used a range of technologies, 
including both type A and type B systems. 
Examples of type A-like systems include small-
scale Lexical Functional Grammar (LFG) ?based 
robust parsers to provide error recognition and 
feedback (Reuer, 2003) and parsing for viewing 
sentence structures and error diagnosis 
(Vandeventer Faltin, 2003). Examples of type B-
like systems include a broad-coverage English 
LFG-based grammar for grammar checking 
(Khader et al 2004), the Systran MT system to 
improve translation skills (La Torre, 1999) and 
using speech recognition for pronunciation training 
(Menzel et al 2001).  
It is relatively straightforward to integrate type B 
(NLP) technology into CALL applications for 
primary school learners. In Section 4 of this paper 
we show how broad-coverage FST technology can 
be used to morphologically analyse word forms or 
to generate all inflected forms given a root form. 
Output from a FST morphology engine is fed into 
an interface engine which sends the information in 
the appropriate format to an XML/Flash 
environment for animation (Koller, 2004). The 
learner input can be collated over time into a 
learner corpus and later analysed by the teacher to 
detect common errors amongst students.  Part-Of-
Speech (POS) taggers can be used to identify the 
parts of speech in electronic versions of learners? 
textbooks or a corpus collated around their 
curriculum (Section 5).  The output can then be 
used for a variety of uses, including the automatic 
generation of online exercises (e.g. hangman) and 
together with the FST morphological engine - 
automatic dictionary extraction. 
Mainly due to scalability problems, type A CL 
technologies can be difficult to deploy in general 
ICALL systems. However, they can be used in the 
primary school context quite effectively. As 
outlined in Section 2.4, the limited linguistic 
performance knowledge of the learners? L1 and 
especially their L2 amounts to a ?controlled? 
language scenario and type A CL technologies can 
be deployed successfully. Curricula used in 
primary schools (in Ireland and elsewhere) 
represent a limited domain in which type A 
technologies can be highly appropriate. Small 
coverage DCGs, for example, can be written for 
the anticipated L2 learner input and can be used to 
provide immediate feedback to the learner. 
Problems associated with difficulties in building 
wider-coverage grammars do not present 
themselves in this context, as the curriculum is 
limited. 
The are many other potential uses of CL/NLP in 
this context, but this paper will focus on the FST 
and POS tagging examples mentioned above.  
4 CL/NLP Resources for Irish Primary 
School CALL  
4.1 Background 
Irish is a compulsory subject in schools in 
Ireland.  Students generally tend to have a negative 
attitude towards the language, which hinders 
learning (Harris & Murtagh, 1999).  Until recently, 
Irish has been taught using the Audio-Lingual 
method (structural patterns are taught using 
repetitive drills) and it is only since 1999 that a 
new communicative curriculum (language teaching 
is structured around topics in terms of 
communicative situations) has been developed and 
integrated. Currently, there are very few CALL 
resources available for Irish (Hetherington, 2000) 
and those that do exist may not be as error-free as 
one would like, are not specifically aimed at 
primary school learners and are therefore not tied 
to the Primary School curriculum which hinders 
their integration into the classroom. 
4.2 A FST-Based Morphological Engine for 
Irish 
U? Dhonnchadha (2002) has developed an 
analyser and generator for Irish inflectional 
morphology using Finite-State Transducers 
(Beesley and Karttunen, 2003). The FST engine 
contains approximately 5,000 lexical stems, 
generates/recognises over 50,000 unique inflected 
surface forms with a total of almost 400,000 
morphological descriptions (due to ambiguous 
surface forms). The final FST is the result of 
composing intermediate transducers, each 
encoding a different morphological process. It is 
useful to have a record of the morphological 
processes involved in mapping between lexical 
(i.e. lemmas and morphological features) and 
surface forms. By including a marker in the surface 
form each time a process is applied, a record of the 
morphological processes involved can be 
maintained and used in other applications. 
The morphological processes covered include: 
(i) internal mutations such as lenition, ellipsis, 
stem internal modification and vocal prefixing; (ii) 
final mutation, such as vowel harmony with 
suffixes (broadening, slenderising and 
syncopation); as well as concatenative morphology 
(prefixing, suffixing). 
4.3 Technology - FST, Perl, XML and Flash 
Primary school learners are not interested in 
viewing output generated by a FST Morphology 
engine. The challenge in CALL applications 
(particularly in the primary school scenario) is to 
exploit underlying technology to present 
information in a manner appropriate to the learner. 
To this end we developed animation software 
interfaced with the output generated by the FST 
engine.  
Animation can enhance the learning process and 
is especially interesting for younger learners.  
Flash (2004) is a useful software environment to 
develop animations but it is difficult for non-
programmers to use and it is often difficult to use 
the same animation templates for different inputs.  
One solution is to use XML (Extensible Markup 
Language, XML (2004)) files as input into Flash, 
so that the information displayed is customisable 
according to the information in the input data file. 
We outline how animated CALL materials were 
developed for teaching the conjugation of verbs in 
the present tense in Irish. 
Output from the FST engine is fed to a Perl 
script which converts the information into a 
specified XML format.  The XML files are then 
used by Flash to generate the required animation.  
Figure 1 outlines the software architecture. Figure 
2 shows the conjugation of the verb cuir (to put) in 
the present tense in Irish. Figure 3 shows modified 
output from the FST engine to enable automatic 
animations to be generated (^INF indicates 
inflectional infix, ^PP indicates inflectional 
postposition and ^SUF indicates inflectional suffix  
for Flash).  
 
 FST 
Output
XML
Files
Perl  
 
 
Flash Animation  
Figure 1: Software architecture 
 1S Chuir m? 
2S Chuir t? 
3S Chuir s?/s? 
1P Chuireamar 
2P Chuir sibh 
3P Chuir siad 
 
 
 
 
 
 
Figure 2: Conjugation of "cuir" 
 
 
 
A
Flas
 
 
 
 
 
 
 
 
 
 
 
T
"cu
"h"
pos
A
any
FSTPastInd  c^INFuir^PP  
PastInd+1P+Pl  c^INFuir^SUF 
Figure 3: Sample output from FST engine 
 section of the XML file that feeds into the 
h program is shown in Figure 4. 
<verb>cuir</verb> 
<stem1>c</stem1> 
<stem2>uir</stem2> 
<infix>h</infix> 
<fir_sg><postpos>m?</postpos></fir_sg> 
<sec_s ostpos>t?</ sec_sg> 
<thi_sg><postpos>s?/s?</postpos></thi_sg>
<fir_ ffix>eamar</suffix pl> 
<sec ostpos </postp ec_pl>
<thi_ ostpos> </postp i_pl> 
F
he an
ir" is 
 is ins
tpositi
nimat
 verb 
 engpl><su
_pl><p
pl><pigure 4: XML file for Flas
imation movie demonstrat
split up into "c" and "uir"
erted between "c" and "u
on "m?" is added (Figure 
ions can be developed a
and morphological proce
ine, as all morphological></fir_
os></s
os></th
>sibh
siadg><p postpos></h program 
es that the stem 
. Then the infix 
ir". Finally the 
5).   
utomatically for 
ss known to the 
 operations are 
coded for Flash.  This removes the necessity of 
hand-coding animations and reduces the risk of 
human error. 
 
 
 
 
 
 
 
 
 
Figure 5: Snapshot sequence from 
animation movie for past tense 1st person 
singular for the verb ?cuir? in Irish  
(Inn? means yesterday) 
 
The Flash-based interface dynamically displays 
XML data. It reads in XML data at runtime and 
generates an animation. Learners have full control 
over the animation. They can play, stop, rewind 
and skip through the animation. Further interaction 
is provided via menus to choose specific 
conjugations (e.g. number, person and tense.) 
The FST-Flash interface is language-
independent. The XML files contain detailed 
information about the different string operations 
and the corresponding targets. The only operations 
known to the Flash interface are insert, delete and 
replace. In this way, the animation of language 
data is abstracted from linguistic terms like 
prefixation, suffixation or lenition, thus avoiding 
the problem of varying definitions of these terms in 
different languages. The transformation of the 
(linguistically tagged) output from the morphology 
engine to the XML data necessary for animated 
presentation is done by Perl scripts which can be 
tailored specifically to each combination of 
language and output of a NLP tool. 
5 CL/NLP Resources for German Primary 
School CALL  
5.1 Background 
German is gradually being integrated into Irish 
primary schools through the Modern Languages in 
Primary School Initiative (MLPSI), which has 
been running since 1998. At present, over 300 
schools in Ireland are involved in the MLPSI.  
German is taught during the senior two years of 
the primary school cycle (children aged 10-13). 
Irish students do not receive any instruction in 
Modern Foreign Languages (MFL) up until this 
point (Irish is not considered a MFL). The 
communicative curriculum we developed is based 
on a draft curriculum which was developed by the 
National Council for Curriculum and Assessment 
(NCCA) (NCCA, 2004) for teachers participating 
in the MLPSI. 
The integration of type A CL technology into 
CALL in this environment is ideal. The target 
language is restricted to a beginner?s curriculum. 
This represents a restricted domain. Sentence 
constructions are simple with few structures that 
could present coverage or ambiguity difficulties to 
CL systems. Given that the target language is 
German, many CL tools are available for almost 
every aspect of language processing. 
In this section we will focus on the use of type B 
NLP technology in this environment to meet the 
needs of students learning German. These needs 
have been researched qualitatively through 
observation during German language lessons in a 
primary school in Ireland during the school year 
2003/4. Irish students are native English speakers 
(some are also native Irish speakers) and as such 
are unfamiliar with nouns being associated with 
genders as in German. These students also require 
extra practise with inflecting verbs correctly. 
Having being asked ?Wie hei?t du??, students will 
often respond with ?Ich hei?t ??, for example. We 
present the use of a POS tagger in the development 
of a tailored corpus which subsequently feeds into 
the automatic generation of exercises.    
5.2 Technology ? POS tagging, Perl and XML 
CALL courseware generally presents users with 
exercises to complete after they have studied a 
particular topic. These are usually static in content 
and are very time consuming to develop over the 
full set of language topics. Students are usually 
presented with a small number of exercises, which 
they will have completed in their entirety and 
become familiar with in a limited space of time. 
Larger sets of exercises prove beneficial in 
providing variety for the student ? they will not be 
presented with the same set of exercises each time 
they visit a topic. In addition, some students will 
complete exercises faster than others. This puts 
pressure on slower students to keep up and on 
teachers to provide alternative work to keep faster 
students occupied. Larger sets of exercises mean 
tha election can be randomised so that 
stu esented with new material each time 
the
less
stu
sam
req
C
gen
 
A
the
Sch
The
con
div
top
file
file
 
 
 
 
 
 
 
 
 
O
XM
suc
ext
gen
on 
ver
ide
can be practised when a student chooses the correct 
verb ending or article from a selection or types in 
the correct answer. A version of hangman (a game 
where students try to guess an unknown word by 
guessing letters in the word - they only get a 
certain number of chances for incorrect answers 
after which the game ends) can also be played with 
article-noun combinations. By simply specifying 
the topic section in the curriculum and the type of 
game, exercises are automatically generated. Each 
particular exercise is randomised so that the user is 
presented with a new variant of the problem each 
time they attempt an exercise or game. 
 
Multiple-choice
Exercises
 
 
 
 
 
 
Gap-fill 
Exercises
Annotated 
Corpus in XML
Ct exercise s
dents are pr
y visit the courseware; slower students will feel 
 pressure to work at a faster pace when faster 
dents complete additional exercises within the 
e language topic and teachers will not be 
uired to provide alternative material.  
L can significantly reduce the time needed to 
erate sets of exercises around language topics. 
 complete curriculum was developed around 
 NCCA guidelines and tagged using Helmut 
mid?s TreeTagger (see TreeTagger homepage). 
 annotated text file was then automatically 
verted to XML using Perl. The corpus is 
ided into separate XML files for each language 
ic. Additional information - audio and graphic 
 references were added manually to each topic 
 at this stage.  
 
 
 
Figure 6: Generating annotated corpus in XML 
nce the annotated corpus has been converted to 
L it can feed into a number of applications 
h as lesson generation, automatic dictionary 
raction, a concordancer and automatic 
eration of various exercise types. In focusing 
the latter, we are particularly interested in the 
bs, articles and nouns that the POS tagging 
ntifies. Inflection and article-noun combinations 
 
 
 
Figure 7: Automatic Exercise Generation 
 
Previous work in automatic exercise generation 
from corpora highlighted a number of potential 
pitfalls (Wilson, 1997). Most importantly, the 
language in the corpus used is best when the 
linguistic quality of the texts is appropriate for 
learning a language. Long and complex sentences 
are best avoided. Our design employs a corpus 
collated and tailored around the learner?s 
curriculum, thus avoiding this pitfall.  
The benefit of using CL resources here is similar 
to the situation in the Irish context. Exercises can 
be developed automatically for any verb or noun 
phrase within the curriculum and provide variety 
for the user. This removes the nec  hand-
coding each exercise and reduces th  human 
error. 
POS 
Tagger 
6 Conclusion 
It is difficult to integrate CL
CALL, especially as these 
generally designed with a CAL
However, there are environme
be successfully integrated, esp
imaginative and useful way.  T
not have to be particularly
complex - what is import
appropriately deployed.   
This paper outlined how two
be used in the development of 
primary schools.  It is novel in 
employ CL/NLP technologies 
especially when they are begi
Complete 
urriculum 
Perl 
Perl 
Annotated 
Corpus in XML
(individual 
language topics)
Hangman Game 
Additional info. ? 
graphics, audio files /NL
reso
L au
nts 
ecia
he 
 re
ant 
 NL
CAL
the 
for 
nneessity of
e risk ofP resources into 
urces are not 
dience in mind.  
where they can 
lly if used in an 
technology does 
volutionary or 
is that it is 
P resources can 
L resources for 
ICALL world to 
young learners, 
rs in learning a 
language. We outlined how the output of a FST 
engine can feed into the generation of Flash 
animations for Irish verb conjugations. We showed 
how a POS tagger can be used to annotate a 
curriculum to produce a corpus which can in turn 
be used to automatically generate exercises. Both 
of these initial modules will be comprehensively 
deployed and evaluated in the classroom during the 
coming school year (Sept. 2004-June 2005). Future 
modules will include type A CL technology like 
DCGs and will take advantage of the controlled 
languages and limited domains which exist in the 
primary school environment. Each module of the 
overall system is being developed so that 
concurrent evaluation can be carried out.  
This paper highlighted the point that even 
though neither of these NLP resources was 
developed with CALL applications in mind, when 
combined with relatively straightforward 
programming and interface techniques, they can be 
used fruitfully in a CALL environment. 
7 Acknowledgements 
This research has been funded by SFI Basic 
Research Grant SC/02/298 and IRCSET Embark 
Initiative Grant RS/2002/441-2. 
References  
D. Arnold, L. Balkan, S. Meijer, R. L. Humphreys 
and L. Sadler. 1994. Machine Translation - An 
Introductory Guide NCC Blackwell Ltd., 
London, USA. 
K. R. Beesley and L. Karttunen. 2003. Finite-State 
Morphology. Series: (CSLI-SCL) Center for the 
Study of Language and Information. 
H. D. Brown. 1994. Principles of Language 
Learning and Teaching. Prentice-Hall Inc, 
London, Sydney, Toronto, Mexico, New Delhi. 
D. Dokter and J. Nerbonne. 1998. A Session with 
Glosser-Rug. In ?Language Teaching and 
Language Technology? S. Jager, J. Nerbonne, 
and A. van Essen, ed., pages 88-94, Swets & 
Zeitlinger, Lisse. 
Flash. 2004.  Available at: 
http://www.macromedia.com/software/flash/ 
 [Accessed 10 April 2004] 
FreeText. 2001.  FreeText Homepage.  Available 
at: http://www.latl.unige.ch/freetext/ [Accessed 
10 April 2004] 
D. Hetherington. 2000.  Computer Resources for 
the Teaching of Irish at Primary and Secondary 
Levels. Language Centre NUI Maynooth, 
Ireland. 
J. Harris and L. Murtagh. 1999. Teaching and 
Learning Irish in Primary School. ITE, Dublin. 
W. J. Hutchins and H. L. Somers. 1992. An 
Introduction to Machine Translation. Academic 
Press, London. 
R. Khader, T. Holloway King and M. Butt. 2004. 
Deep CALL grammars: The LFG-OT 
experiment. DGfS 26.Jahrestagung, Mainz, 
Germany. 
T. Koller. 2004. Creating user-friendly, highly 
adaptable and flexible language learning 
environments via Flash, XML, Perl and PHP. 
Presentation at the EuroCALL SIG-LP workshop 
"Innovative Technologies and Their Didactic 
Application", Vienna, September 2004. 
M. D. La Torre. 1999. A web-based resource to 
improve translation skills. ReCALL, 11(3): 41-
49. 
W. Menzel, D. Herron, R. Morton, D. Pezzotta, P. 
Bonaventura, and P. Howarth. 2001. Interactive 
pronunciation training. ReCALL, 13(1): 67-78. 
NCCA. 2004. National Council for Curriculum   
   Assessment (NCCA) Homepage. Available at:    
   http://www.ncca.ie/j/index2.php?name=currinfo   
   [Accessed: 10 April 2004] 
V. Reuer. 2003. Error Recognition and Feedback 
with Lexical Functional Grammar. CALICO, 
20(3): 497-512 
M. Schulze. 2003. AI in CALL: Artifically Inated 
or Almost Imminent? WorldCALL 2003, Banff, 
Canada. 
TreeTagger Homepage. Available at: 
http://www.ims.uni-stuttgart.de/projekte/corplex/ 
TreeTagger/DecisionTreeTagger.html 
[Accessed: 20 April 2004] 
E. U? Dhonnchadha. 2002. An Analyser and 
Generator for Irish Inflectional  
Morphology Using Finite-State Transducers. 
Msc Thesis. 
A. Vandeventer Faltin. 2003. Natural language 
processing tools for computer assisted language 
learning. Linguistik Online 17, 5/03 
E. Wilson. 1997. The Automatic Generation of 
CALL Exercises from General Corpora. In 
?Teaching and Language Corpora? A. 
Wichmann, S. Fligelstone, T. McEnery, and G. 
Knowles, ed., pages 116-130, Addison Wesley 
Longman, London. 
XML. 2004.  Extensible Markup Language.  
Available at: http://www.w3.org/XML 
[Accessed 10 April 2004] 
Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 57?64,
Trento, Italy, April 2006. c?2006 Association for Computational Linguistics
German Particle Verbs and Pleonastic Prepositions
Ines Rehbein
NCLT
School of Computing, DCU,
Dublin, Ireland
irehbein@computing.dcu.ie
Josef van Genabith
NCLT,
School of Computing, DCU,
Dublin, Ireland
josef@computing.dcu.ie
Abstract
This paper discusses the behaviour of Ger-
man particle verbs formed by two-way
prepositions in combination with pleonas-
tic PPs including the verb particle as a
preposition. These particle verbs have a
characteristic feature: some of them li-
cense directional prepositional phrases in
the accusative, some only allow for loca-
tive PPs in the dative, and some parti-
cle verbs can occur with PPs in the ac-
cusative and in the dative. Directional par-
ticle verbs together with directional PPs
present an additional problem: the par-
ticle and the preposition in the PP seem
to provide redundant information. The
paper gives an overview of the semantic
verb classes influencing this phenomenon,
based on corpus data, and explains the un-
derlying reasons for the behaviour of the
particle verbs. We also show how the re-
strictions on particle verbs and pleonastic
PPs can be expressed in a grammar theory
like Lexical Functional Grammar (LFG).
1 Introduction
The subject of this paper are German particle verbs
with pleonastic prepositions (5). In German there
are nine two-way prepositions which can either
govern the accusative or the dative: an, auf, hinter,
in, neben, ?uber, unter, vor and zwischen. The dif-
ference in case assignment also causes a different
interpretation of the semantics of the prepositional
phrase: if the preposition governs the dative it ex-
presses a locative relation (1), while the accusative
goes together with a directional interpretation (2).
(1) Das Bild h?angt [PP an der Wand].
Det Picture hang-3Sg [PP on?dir Detdat wall].
?The picture hangs on the wall.?
(2) Sie h?angt das Bild [PP an die Wand].
She hang-3Sg Det picture [PP onto+dir Detacc wall].
?She hangs the picture on the wall.?
The two-way prepositions combined as prefixes
with a verb form the so-called particle verbs (also
called separable prefix verbs). The particles im-
plicitly include directional information and can
change the aspectual mode and argument struc-
ture of their base verbs. Particle verbs can be dif-
ferentiated according to whether they allow for a
pleonastic combination with the particle in ques-
tion and the resulting syntactic and semantic ef-
fects.
Olsen (1998) refers to this phenomenon as the
Pleonastic Directional, where the verb particle al-
ready saturates the directional requirement of the
verb and therefore there should be no need for a
further preposition offering the same directional
information. However, example (5) shows that
pleonastic directionals can in fact occur with di-
rectional PPs, while in (3) the main verb (without
particle) combines with a directional PP and in (4)
only the particle verb is used.
(3) Sie steigt [PP in das Auto].
She climb-3SG [PP into+DIR Det car].
?She gets into the car.?
(4) Sie steigt ein.
She climb-3SG Part+DIR.
?She gets in.?
(5) Sie steigt [PP in das Auto] ein.
She gets [PP into+DIR Det car] Part+DIR.
?She gets into the car.?
The problem is that it is not clear what licenses
57
the directional preposition in cases such as (5) and
why it is not supressed by the verb particle.
The base verb in (3) licenses a directional PP,
which is part of the argument structure of the verb.
If there is a verb particle which saturates this di-
rectional requirement (4), then the realisation of
the PP is optional. Wunderlich (1983) argues that
particle verbs require a stereotype or contextually
given object equal to the internal argument of the
prepositional relation, which can be reconstructed
from the context and therefore can be omitted.
If the directional information is already repre-
sented by the particle, then the question arises
what licenses the directional PP. It could be argued
that the particle should suppress a directional PP
or, conversely that the directional PP should sup-
press the verb particle. The question which of the
two is selected first, the particle verb or the prepo-
sition, is discussed controversially. In a speaker-
oriented view the particle verb will be selected
first, while the theory of linear sentence processing
claims that the particle, which is only encountered
at the end of the sentence, should be omitted.
Particle verbs with pleonastic PPs exhibit an-
other interesting property: some of them only
allow for pleonastic prepositions governing da-
tive PPs while others trigger the accusative, and
some particle verbs can even go together with both
cases. The underlying reasons for those case pref-
erences are not completely clear.
It is obvious that there are certain verb classes
whose semantics seem to influence the case as-
signed by the preposition. This is strongly con-
nected with the influence of directional informa-
tion concerning the case preference of the particle
verb. Particle verbs which express directional in-
formation trigger PPs in the accusative, while par-
ticle verbs whose semantics contain no directional
component never combine with an accusative PP.
But why are there also particle verbs which are
able to combine with both cases?
The aim of this paper is to give an explana-
tion for this phenomenon, based on data gained
through corpus research. Section 2 describes char-
acteristic features of spatial prepositions and par-
ticle verbs. Section 3 presents a novel corpus-
based typology of verb classes triggering different
case for pleonastic prepositions, accounting for
regularities in their observed behaviour. Section
4 provides a novel account of particle verbs with
their pleonastic prepositions using the framework
of Lexical Functional Grammar (Bresnan, 2000).
The last section summarizes the main results es-
tablished in this paper.
2 Characteristic Features of Particle
Verbs and Spatial Prepositions
Spatial prepositions are binary relations between
two entities, where one of the entities is located
with respect to a region defined by the second en-
tity, specified through the preposition. The mean-
ing of a two-way preposition depends on the case
of the PP: if it is in the dative, its reading will be
interpreted as a static, non-directional localisation,
while the accusative triggers a directional interpre-
tation. In the latter case the preposition implies a
change of location of the theme referent from an
unspecified region into the neighbouring region of
the relatum (Witt, 1998).
In this paper we only deal with spatial prepo-
sitions, ignoring lexicalised prepositions without
semantic content, as in (6):
(6) Sie wartet auf den Bus.
She wait-3Sg for Det bus.
?She is waiting for the bus.?
Dalrymple (2001) refers to (6) as idiosyncratic
case, because the lexical form of the preposition
is not related to the semantic role of the argu-
ment, while oblique arguments which are marked
according to the semantic role of the argument are
assigned semantic case. Particle verbs formed by
two-way prepositions always have a semantic con-
tent.
The semantics of verb particles basing on spa-
tial prepositions is equivalent to the semantics of
the prepositions. They are also binary, but the in-
ternal argument of the relation is not explicitly ex-
pressed in the argument structure of the complex
verb, but can be omitted (see examples (3) and
(4)). The semantics of the particle is integrated
into the semantics of the base verb which requires
a directional complement.
In example (5) both particle verb and pleonastic
PP occur together. Here the PP specifies the im-
plicit reference object of the particle verb, and its
relation of localisation is congruent with the direc-
tional semantics of the particle.
These characteristic features of particle verbs
and spatial prepositions are constitutive for the
classification into semantic verb classes given in
Section 3.
58
3 Corpus-Based Classification of Particle
Verbs with Pleonastic Prepositions
The classification of particle verbs with pleonas-
tic prepositions into semantic verb classes is based
on the proposals by Witt (1998) extented by the
results of our own corpus research.1 Witt?s clas-
sification only considers particle verbs with the
particle ein-. He divides them into three ma-
jor groups: compositional formations, regular for-
mations and non-compositional formations, which
can be further subclassified into more fine-grained
subclasses (Figure 1).
1. Compositional Formations
(a) Verb bases are causative Verbs of Localisation
(b) Verb bases are (static) Verbs of Localisation
(c) Verb bases are intransitive Verbs of Motion
(d) Verb bases are transitive Verbs of Motion
(Transport Verbs)
2. Regular Formations
(a) Verb Bases are Activity-Verbs
(b) Verb Bases are ?eingravieren (to engrave)?-Verbs
3. Non-Compositional Formations: Extensions
of Meaning
(a) Verb Bases are ein-Verbs with the meaning:
?downward, inward, into itself?
(b) Verb Bases are ein-Verbs with the meaning:
?to enclose something?
Figure 1: Witt?s (1998) classification of particle
verbs with ein-
In contrast to Witt, our classification includes all
two-way prepositions as verb particles. As we are
trying to explain the behaviour of particle verbs in
regard to their ability to combine with pleonastic
PPs, we divide the corpus data into the following
groups: particle verbs licensing pleonastic PPs in
the accusative only (Group A), particle verbs li-
censing pleonastic PPs in the dative only (Group
B) and particle verbs which are able to govern ei-
ther accusative or dative PPs (Group C).
Each of these groups can be divided into a num-
ber of subgroups, formed by different semantic
verb types. Figure 2 gives an overview of our clas-
sification scheme.
1The corpora used for the research are the text basis
of the Digital Dictionary of German Language (DWDS)
(http://www.dwds.de/textbasis) and the corpora
of the Institute of German Language (IDS) in Mannheim
(http://www.ids-mannheim.de/cosmas2).
1. Group A (combine only with accusative PPs)
(a) Verb bases are (static) Verbs of Localisation
(b) Verb bases are intransitive Verbs of Motion
(c) Verb bases are transitive Verbs of Motion
(Transport Verbs)
(d) Verb bases are Verbs of Perception
(e) Verb bases express a Change of State
2. Group B (combine only with dative PPs)
(a) Verb bases are (static) Verbs of Localisation
(b) Verb bases are intransitive Verbs of Motion
(c) Verb bases are (causative) Verbs of Position
3. Group C (combine with accusative and dative PPs)
(a) Verb bases are intransitive Verbs of Motion
(b) Verb bases are transitive Verbs of Motion
(Transport Verbs)
(c) Verb bases express an Inclusion into an
Environment, Institution or Abstract Area
(d) Verb bases express Effects of Action
(eingravieren-Verbs)
Figure 2: Classification of particle verbs with two-
way prepositions
3.1 Group A
The verbs in Group A licence PPs in the accusative
and have a directional reading. Group A includes
Verbs of Motion, Verbs of Localisation, Transport
Verbs, and two further subgroups: verbs whose
meaning can be interpreted as a Direction of Per-
ception and verbs which express the Localisation
of a Change of State.
Verbs of Motion include einfahren ?to drive
into? or aufspringen ?to jump on? and can be de-
fined as follows: there is an X which undergoes
a change of location, whereby X is in a particu-
lar manner of motion and moves in the specified
direction into a not further specified neighbour re-
gion which is defined through the relatum.
Verbs of Localisation licencing PPs in the ac-
cusative are rather rare. Only one example is
attested in the corpus: einm?unden ?to discharge
into?. Here an X is described, which can be lo-
calised relativ to a Y in a particular direction. The
rarity of those verbs is probably due to the more
static character of localisation, which contradicts
the implicit directional reading of the accusative
case marking.
Transport Verbs such as eingie?en ?to pour
in?, einf?uhren ?to insert? and also verbs with more
metaphorical readings like einbinden (in die Kon-
59
ventionen einbinden, ?to weave sth into social con-
ventions?), can be defined in the following way:
there is an X which causes a change of location
for a Y, whereby Y is set into a particular manner
of motion and is moved in a specified direction.
Direction of Perception verbs include
einf?uhlen ?to empathise?, einsehen ?to see? or
einh?oren ?to listen?.
Localisation of a Change of State verbs in-
clude aufbl?ahen ?to bloat?, aufheizen ?to heat up?,
angleichen ?to conform to something? or aufrun-
den ?to round up?. Here the particle expresses the
direction to the changed, new state.
All particle verbs in Group A can be interpreted
as having a directional reading.
3.2 Group B
Particle Verbs in Group B licence pleonastic PPs
in the dative. They can be divided into the fol-
lowing subgroups: Verbs of Localisation, Verbs of
Movement and Position Verbs.
Verbs of Localisation also occur in Group A,
but here they have a static, non-directional inter-
pretation of localisation. Examples for this are
verbs like einquartieren ?to quarter?, anstehen ?to
queue?, auiegen ?to bear on? or zwischenlagern
?to store temporarily?.
(7) anPART stehen
(PART + to stand? to queue).
More formally they can be described as follows:
There is an X which is in a particular state (e.g.
in the state of standing) and can be localised in a
specific relation to a reference object.
Verbs of Motion include vorfahren ?to drive
up? or hinterherhecheln ?to pand after someone?.
They can be defined as follows: there is an X
which undergoes a change of location, whereby X
is in a particular manner of motion, moving into
the specified direction relative to the position of
the relatum. These verbs clearly include an im-
plicit direction, but in comparison to the Verbs of
Motion in Group A their reading allows for the
possibility that X is already in the same region as
the relatum, while the verbs in Group A describe
the intrusion of an X from the outside into a not
further specified neighbour region.
Verbs of Position include aufstellen ?to array?,
aufbahren ?to lay out? or hinterlegen ?to deposit?.
The definition states that there is an X which
causes a Y to change its position, whereby Y is
in a particular manner of motion, moving into a
specified direction. The focus hereby is not on the
movement but on the result of the event.
The verbs in Group B normaly have a nondi-
rectional, static interpretation, but they may also
allow for a directional interpretation, if theme ref-
erent and relatum are both positioned in the same
specified region (8).
(8) Sie stellt die Leiter [PP auf dem Podest] auf.
She put-3-Sg Det ladder [PP on Det platform] Part.
?She puts the ladder up on the platform.?
Here it is not the direction of a motion which is
described by the particle (the ladder may already
have been lying on the platform), but a change of
the orientation of the referent in relation to the re-
latum (the ladder has changed its orientation and
is in a more or less vertical position now).
3.3 Group C
Group C consists of particle verbs which can be
followed by a pleonastic PP in the accusative or
dative. The subgroups of Group C include Verbs
of Motion like einsickern ?to soak into?, ein-
marschieren ?to march in?, ansp?ulen ?to be washed
up? or vorladen ?to subpoena?, and Transport
Verbs such as aufh?angen ?to hang?, einschieben
?to insert?, einr?aumen ?to place in? or andocken ?to
dock?. Group C also consists of verbs which ex-
press an Inclusion into an Environment, Institu-
tion or Abstract Area like eingliedern ?to incor-
porate?, zwischenschalten ?to interpose?, aufrei-
hen ?to string? or auff?adeln ?to bead?. Another
verb group which belongs to Group C are verbs
which express the Localisation of Effects of Ac-
tion like einpr?agen ?to impress?, einbrennen ?to
burn-in?, eint?atowieren ?to tattoo? or aufdrucken
?to imprint?.
The following example illustrates the semantic
effect of the choice of case for the PP for the verbs
in Group C:
(9) sickert in die Erde ein
soak.3.Sg in Det.Acc soil PART
?soaks into the soil?
(10) sickert in der Erde ein
soak.3.Sg in Det.Dat soil PART
?soaks the soil?
Example (9) describes an event where an X
60
(rainwater) undergoes a directed motion during
which it enters into the region of the reference ob-
ject Y (the soil). In (10) the situation is different:
X is already located in the region of Y and now
is in the process of soaking through that region.
Figure 3 gives an illustration of the two examples.
Figure 3: Illustration of examples (9) and (10)
Characteristic for the verbs in Group C is their
directionality reading when going together with a
pleonastic PP in the accusative. When they are
combined with the dative, the particle still has its
directional character, but in contrast to the parti-
cle verbs in Group A the directionality does not
include an intrusion into another region but can
be interpreted as a movement inside of the region
given by the reference object.
Summarizing the results we can say that for
Group C the particle can have different functions
which influence the choice of case marking for the
PPs governed by the verb. If the particle has a
nondirectional reading, then only PPs in the dative
are allowed. If the particle expresses directional
information, then a further analysis is needed: it
has to be examined whether the semantics of the
particle verb includes the intrusion into a new re-
gion specified by the preposition. In this case the
PP has to be in the accusative. If the semantics of
the verb does not express an intrusion into a new
region, then the dative is chosen. Only particle
verbs whose semantics allow for a directional and
a locative interpretation belong to group C.
In Section 2 we noted that the semantics of the
verb particle is equivalent to the semantics of the
preposition, and that the PP specifies the implicit
reference object of the particle verb. However, this
is only true for PPs with accusative case marking.
The prepositions in PPs which are in the dative ex-
press a locative relation rather than a direction, so
their reference object can not be the same as the
one implicitly included in the verb particle. On the
syntactic level this results in them having a differ-
ent grammatical function: the accusative PP can
be considered as a verb complement, while the da-
tive PP is a free adjunct, modifying the informa-
tion of the verb particle. Therefore only accusative
PPs are ?pleonastic?.
4 Description of Particle Verbs with
Pleonastic Prepositions in LFG
This section will show how the framework of Lex-
ical Functional Grammar (LFG) can be used to
describe the particular behaviour of particle verbs
and pleonastic prepositions.
4.1 Short Introduction to LFG
LFG has a layer of representation for constituent
structure (c-structure), where surface information
is expressed through CFG trees, and a func-
tional layer (f-structure) for expressing grammat-
ical functions such as subject, object and adjunct.
In the f-structure each argument of a predicate is
assigned a particular grammatical function. This
two-level representation is based on the idea that
while surface representations may differ consider-
ably between various languages, f-structures tend
to be more abstract and invariant representations.
The correspondence between the two layers is
many-to-one: different nodes in the c-structure
may be associated with the same f-structure com-
ponent. The c-structure is determined by phrase
structure rules as in (11), while the annotation in
(12) links the c-structure categories to the corre-
sponding grammatical functions in the f-structure.
(11) S ? NP VP
(12) (? SUBJ)= ? ?=?
LFG is a non-transformational theory, syntactic
phenomena are treated locally through the specifi-
cation of rules and constraints in the lexicon.
4.2 Using LFG to Describe Particle Verbs
with Pleonastic Prepositions
The LFG formalisation developed here follows
and substantially extends the treatment of particle
verbs and prepositional phrases in the LFG gram-
mar for German in (Berman and Frank, 1996) and
(Butt, King, Nin?o and Segond, 1999).
4.2.1 Berman & Frank (1996)
Figure 4 shows the lexical entry for the German
particle verb einfahren ?to drive into? as described
in (Berman and Frank, 1996).
61
fahren V
(? PRED)=?EINFAHREN<?SUBJ), (? OBL DIR)>?
(? FORM)=c EIN
(? VERBTYPE)=PARTICLE VERB
...
ein PART
(? FORM)=EIN
Figure 4: Lexical entry for einfahren ?to drive in?
(Berman and Frank, 1996)
The predicate (PRED) shows the argument
structure of the verb, while the attribute VERB-
TYPE explicitly describes the verb as a particle
verb. The FORM attribute contains the lexical
form of the particle and is formulated as a con-
straint (=c) to check that the particle is lexically
filled. The particle itself has no PRED value of its
own but is analysed as part of the complex verb.
German prepositional phrases can either occur
as prepositional objects or as adjuncts. Accord-
ing to Berman and Frank (1996) the second group
is further subdivided into adjuncts which are sub-
categorized by the verb and free adjuncts. Accord-
ingly, in the analysis of (Berman and Frank, 1996),
each two-way preposition has three lexical entries.
In their analysis, prepositional objects are gov-
erned by the verb and have no PRED attribute of
their own. The lexical form of the preposition and
also its case are determined by the verb. The value
of the PCASE attribute is assigned the lexical form
of the preposition, while the preposition is not able
to subcategorize an object.2
As for adjuncts subcategorized by the verb no
particular preposition is selected in (Berman and
Frank, 1996), but the verb determines the seman-
tic content of the preposition (eg: LOC, DIR). The
preposition has its own PRED attribute and sub-
categorizes an object (Figure 5).
auf P
(? PRED)=?LOC<?OBJ)>?
(? PCASE)=LOC
(? PDET)=-.
Figure 5: Lexical entry (Berman & Frank, 1996)
for preposition auf ?on? (adjunct subcategorized
by the verb)
Free adjuncts on the other hand must have the
2Prepositional objects are of no concern here, because the
paper deals with spatial prepositions which always have a se-
mantic content.
semantic content LOC. Like the first type of ad-
juncts they have their own PRED attribute and
subcategorize an object, but their semantic content
is defined by the ROLE attribute (Figure 6).
auf P
(? PRED)=?OBL LOCAL<?OBJ)>?
(? ROLE)=LOCAL
(? OBJ AGR CAS GOV)=+
(? OBJ AGR CAS OBL)=+
(? PDET)=-.
Figure 6: Lexical entry (Berman & Frank, 1996)
for auf ?on? (free adjunct)
4.2.2 Formalisation of Group C Verbs
We concentrate on the formalisation of the par-
ticle verbs in Group C which can either licence a
pleonastic PP in the accusative or a PP in the da-
tive. Extending the analysis in (Berman and Frank,
1996) we provide two f-structure configurations,
depending on the case of the governed PP.
Figure 7 shows the f-structure for example (9).
Here the pleonastic PP in the accusative saturates
the argument OBL DIR subcategorized by the par-
ticle verb. Figure 8 gives the f-structure for exam-
ple (10), where the particle verb combines with a
dative PP.
?
??????????
PRED einsickern < SUBJ,OBL DIR >
OBL DIR
?
??????
PRED in < OBJ >
PART? FORM ein
PCASE DIR
PSEM +
OBJ
[ PRED Erde
SPEC die
CASE acc
]
?
??????
?
??????????
Figure 7: sickert [ PP in die Erde ]ACC ein
?soaks into the soil?
In contrast to Figure 7 the dative PP in Figure
8 does not contribute any information to the argu-
ment OBL DIR subcategorized by the verb but is
represented in the adjunct set. The verb particle
saturizes the OBL DIR argument, and the PRED
attribute of the object of OBL DIR is assigned the
value PRO. This enables the PRED value to be-
have like a variable which can be unified with any
other value as in Figure 8, where both the particle
and the pleonastic prepositional phrases add infor-
mation to OBL DIR:OBJ:PRED.
62
?
????????????????????
PRED einsickern < SUBJ,OBL DIR >
OBL DIR
?
?????
PRED in < OBJ >
PART? FORM ein
PCASE DIR
PSEM +
OBJ
[
PRED PRO
CASE acc
]
?
?????
ADJ
?
?????
?????
?
?????
PRED in < OBJ >
PCASE LOC
PSEM +
OBJ
[ PRED Erde
SPEC der
CASE dat
]
?
?????
?
?????
?????
?
????????????????????
Figure 8: sickert [ PP in der Erde ]DAT ein
?soaks (through) the soil?
4.2.3 Lexical Entries and Grammar Rules
In the f-structure in Figure 7 the pleonastic PP
is subcategorized by the particle verb. Figure 9
shows the corresponding lexical entry for the verb.
To prevent a locative PP in the dative from fill-
ing in the object position of the verb argument the
lexical entry specifies that the object has to be as-
signed accusative case.
einsickern V
(? PRED) = ?einsickern<(? SUBJ, ? OBL DIR)>?
(? OBL DIR:PART-FORM) = ein
(? OBL DIR:OBJ:CASE) = acc
Figure 9: Lexical entry for einsickern ?to soak?
However, as shown in example (4) the pleonas-
tic PP can be omitted. In this case the argument
OBL DIR subcategorized by the particle verb is
provided by the particle ein- whose lexical entry is
given in Figure 10.
ein PART
(? PRED) = ?in<(? OBJ)>?
(? PART-FORM) = ein
(? PCASE) = DIR
(? PSEM) = +
(? OBJ PRED ) = PRO
Figure 10: Lexical entry for the particle ein
In contrast to (Berman and Frank, 1996), in our
representation the particle is assigned the PRED
value ?in? in the lexicon. The cause for the diver-
gence between the lexical form of the particle and
its PRED value is due to the fact that the particle
ein- historically is derived from the preposition in
and regarding its semantic features is comparable
to the other two-way prepositions where particle
and preposition have the same lexical form.
The attributes PSEM and PCASE are added to
the representation of the verb particles in Berman
and Frank (1996). They are derived from the at-
tribute set for prepositions, indicating the anal-
ogy in the semantics of particle and preposition.
PSEM always has the value ?+? for particle verbs
formed by spatial prepositions, because they al-
ways have a semantic content. The attribute
PCASE expresses the directionality in the seman-
tics of the verb particle ( (? PCASE) = DIR).
The predicate of the particle licences an object
and behaves like a directional preposition. How-
ever, the object position is not lexically filled and
therefore is assigned the predicate value ?PRO?.
We also want to model the behaviour of the par-
ticle verb governing a locative PP in the dative
(Figure 8). The lexical entry of the particle verb
(Figure 9) explicitly requires accusative case as-
signment and prevents the locative dative PP from
filling in the object position of the verb argument.
The locative dative PP is attached to the adjunct
set in the grammar rule shown in Figure 11.3
VP? V ?=?
PP * ? ? ( ? ADJ)
(? OBJ CASE) 6= acc
(PP (? OBL DIR) = ?)
PART (? OBL DIR) = ?.
Figure 11: Grammar Rule specifying restrictions
on particle verbs with pleonastic PPs
The first PP in the grammar rule models the be-
haviour of a particle verb combining with one or
more locative PPs in the dative. The constraint
(? OBJ CASE) 6= acc ensures that this part of the
rule will not be applied to a pleonastic PP with ac-
cusative case assignment.4
The second PP in the grammar rule captures a
pleonastic PP in the accusative. The restriction
that this PP has to be in the accusative is specified
in the lexical entry for the particle verb (Figure
10). The last part of the rule expresses that the verb
particle PART is also mapped to the OBL DIR ar-
3For expository purposes we use a simple VP rather than
a topological analysis.
4The Kleene * notation indicates zero or more occurences
of PP.
63
gument of the complex verb and so is able to satu-
rate the argument structure of the verb.
The formalisation in Figure 8 and 9 is consistent
with the analysis that the particle has an implicit
reference object which is identical to the object of
a pleonastic PP in the accusative, but not to the
object of a dative PP. The formalisation gives an
adequate description of the behaviour of particle
verbs in Group C, but it does not suppress the li-
cencing of a pleonastic accusative PP for verbs in
Group B which combine with locative PPs in the
dative only. This problem is solved through the
specification of a constraint (=c) in the lexical en-
tries for all particle verbs in Group B (Figure 12).
vorfahren V
(? PRED) = ?vorfahren<(? SUBJ, ? OBL DIR)>?
(? OBL DIR:PART-FORM) = vor
(? OBL DIR:OBJ:CASE) = acc
(? OBL DIR:OBJ:PRED) =c PRO
Figure 12: Lexical entry for vorfahren ?to drive
up? (Group B)
The constraint checks that the predicate of the
object in the OBL DIR f-structure is instantiated
with the value ?PRO?. For all cases where the pred-
icate is lexically realised, the constraint fails and
thus the interpretation of pleonastic accusative PPs
in the OBL DIR position for Group B verbs is sup-
pressed.
5 Conclusions
The aim of this paper is to explain the behaviour of
German particle verbs formed by two-way prepo-
sitions and their ability to combine with pleonastic
PPs. A classification of particle verbs based on se-
mantic criteria was given, illustrating the restric-
tions imposed on their behaviour. It was shown
that particle verbs occurring only with accusative
PPs (Group A) always have a directional read-
ing including the intrusion of the theme referent
into a region specified by the relatum. Particle
verbs which can not combine with an accusative
PP (Group B) either have a static, nondirectional
reading or describe a directed movement where
the referent already may be present in the region
specified by the relatum.
Syntactically this results in the fact that the
accusative PP is able to saturate the argument
OBL DIR subcategorized by the particle verbs in
Group A. The dative PP functions as an adjunct
(Group B). Here the verb particle saturates the di-
rectional OBL DIR argument required by the verb.
Group C verbs allow both accusative and dative
PPs. Only particle verbs governing PPs in the ac-
cusative are pleonastic, but the PP either modifies
or adds new information to the inherent argument
structure of the particle verb and therefore is not
suppressed by the verb particle.
Our formalisation describes the behaviour of
particle verbs concerning their ability to licence
pleonastic PPs. The semantic criteria restricting
the behaviour of the particle verbs are embed-
ded into the LFG representation and enable us
to model the semantic differences on a syntactic
level.
References
Judith Berman and Anette Frank. 1996. Deutsche und
franzo?sische Syntax im Formalismus der LFG. Max
Niemeyer Verlag, Tu?bingen.
Joan Bresnan. 2000. Lexical-Functional Syntax.
Blackwell.
Miriam Butt, Tracy Holloway King, Mar??a-Eugenia
Nino and Fre?de?rique Segond. 1999. A Grammar
Writer?s Cookbook. CSLI Publications, Stanford,
California.
Mary Dalrymple. 2001. Syntax and Semantics. Lexical
Functional Grammar, volume 34. Academic Press,
San Diego, California.
Junji Okamoto. 2002. Particle-Bound Directions in
German Particle Verb Constructions. Projektbericht
V: Typological Investigation of Languages and Cul-
tures of the East and West. (Part II).
Susan Olsen. 1998. Semantische und konzeptuelle
Aspekte der Partikelverbbildung mit ein-. Stauffen-
burg, Tu?bingen.
James Witt. 1998. Kompositionalita?t und Regularita?t,
In: Olsen, Susan (ed). Semantische und konzeptuelle
Aspekte der Partikelverbbildung mit ein-. Stauffen-
burg, Tu?bingen.
Dieter Wunderlich. 1983. On the Compositionality of
German Prefix Verbs. In: R. Ba?uerle, Ch. Schwarze
and A. von Stechow (eds.) Meaning, Use and Inter-
pretation of Language. de Gruyter, Berlin.
64
Proceedings of the Workshop on Statistical Machine Translation, pages 86?93,
New York City, June 2006. c?2006 Association for Computational Linguistics
Contextual Bitext-Derived Paraphrases in Automatic MT Evaluation 
 
Karolina Owczarzak Declan Groves Josef Van Genabith Andy Way 
National Centre for Language Technology 
School of Computing 
Dublin City University 
Dublin 9, Ireland 
{owczarzak,dgroves,josef,away}@computing.dcu.ie 
 
 
Abstract 
In this paper we present a novel method 
for deriving paraphrases during automatic 
MT evaluation using only the source and 
reference texts, which are necessary for 
the evaluation, and word and phrase 
alignment software. Using target language 
paraphrases produced through word and 
phrase alignment a number of alternative 
reference sentences are constructed auto-
matically for each candidate translation. 
The method produces lexical and low-
level syntactic paraphrases that are rele-
vant to the domain in hand, does not use 
external knowledge resources, and can be 
combined with a variety of automatic MT 
evaluation system. 
1 Introduction 
Since their appearance, BLEU (Papineni et al, 
2002) and NIST (Doddington, 2002) have been the 
standard tools used for evaluating the quality of 
machine translation. They both score candidate 
translations on the basis of the number of n-grams 
it shares with one or more reference translations 
provided. Such automatic measures are indispen-
sable in the development of machine translation 
systems, because they allow the developers to con-
duct frequent, cost-effective, and fast evaluations 
of their evolving models.  
These advantages come at a price, though: an 
automatic comparison of n-grams measures only 
the string similarity of the candidate translation to 
one or more reference strings, and will penalize 
any divergence from them. In effect, a candidate 
translation expressing the source meaning accu-
rately and fluently will be given a low score if the 
lexical choices and syntactic structure it contains, 
even though perfectly legitimate, are not present in 
at least one of the references. Necessarily, this 
score would not reflect a much more favourable 
human judgment that such a translation would re-
ceive. 
The limitations of string comparison are the 
reason why it is advisable to provide multiple ref-
erences for a candidate translation in the BLEU- or 
NIST-based evaluation in the first place. While 
(Zhang and Vogel, 2004) argue that increasing the 
size of the test set gives even more reliable system 
scores than multiple references, this still does not 
solve the inadequacy of BLEU and NIST for sen-
tence-level or small set evaluation. On the other 
hand, in practice even a number of references do 
not capture the whole potential variability of the 
translation. Moreover, often it is the case that mul-
tiple references are not available or are too difficult 
and expensive to produce: when designing a statis-
tical machine translation system, the need for large 
amounts of training data limits the researcher to 
collections of parallel corpora like Europarl 
(Koehn, 2005), which provides only one reference, 
namely the target text; and the cost of creating ad-
ditional reference translations of the test set, usu-
ally a few thousand sentences long, often exceeds 
the resources available. Therefore, it would be de-
sirable to find a way to automatically generate le-
gitimate translation alternatives not present in the 
reference(s) already available. 
86
In this paper, we present a novel method that 
automatically derives paraphrases using only the 
source and reference texts involved in for the 
evaluation of French-to-English Europarl transla-
tions produced by two MT systems: statistical 
phrase-based Pharaoh (Koehn, 2004) and rule-
based Logomedia.1 In using what is in fact a minia-
ture bilingual corpus our approach differs from the 
mainstream paraphrase generation based on mono-
lingual resources. We show that paraphrases pro-
duced in this way are more relevant to the task of 
evaluating machine translation than the use of ex-
ternal lexical knowledge resources like thesauri or 
WordNet2, in that our paraphrases contain both 
lexical equivalents and low-level syntactic vari-
ants, and in that, as a side-effect, evaluation bitext-
derived paraphrasing naturally yields domain-
specific paraphrases. The paraphrases generated 
from the evaluation bitext are added to the existing 
reference sentences, in effect creating multiple ref-
erences and resulting in a higher score for the can-
didate translation. Our hypothesis, confirmed by 
the experiments in this paper, is that the scores 
raised by additional references produced in this 
way will correlate better with human judgment 
than the original scores. 
The remainder of this paper is organized as fol-
lows: Section 2 describes related work; Section 3 
describes our method and presents examples of 
derived paraphrases; Section 4 presents the results 
of the comparison between the BLUE and NIST 
scores for a single-reference translation and the 
same translation using the paraphrases automati-
cally generated from the bitext, as well as the cor-
relations between the scores and human judgment; 
Section 5 discusses ongoing work; Section 6 con-
cludes. 
2 
2.1 
                                                          
Related work 
Word and phrase alignment 
Several researchers noted that the word and 
phrase alignment used in training translation mod-
els in Statistical MT can be used for other purposes 
as well. (Diab and Resnik, 2002) use second lan-
guage alignments to tag word senses. Working on 
an assumption that separate senses of a L1 word 
2.2 
1 http://www.lec.com/ 
2 http://wordnet.princeton.edu/ 
can be distinguished by its different translations in 
L2, they also note that a set of possible L2 transla-
tions for a L1 word may contain many synonyms. 
(Bannard and Callison-Burch, 2005), on the other 
hand, conduct an experiment to show that para-
phrases derived from such alignments can be se-
mantically correct in more than 70% of the cases. 
Automatic MT evaluation 
The insensitivity of BLEU and NIST to per-
fectly legitimate variation has been raised, among 
others, in (Callison-Burch et al, 2006), but the 
criticism is widespread. Even the creators of BLEU 
point out that it may not correlate particularly well 
with human judgment at the sentence level (Pap-
ineni et al, 2002), a problem also noted by (Och et 
al., 2003) and (Russo-Lassner et al, 2005). A side 
effect of this phenomenon is that BLEU is less re-
liable for smaller data sets, so the advantage it pro-
vides in the speed of evaluation is to some extent 
counterbalanced by the time spent by developers 
on producing a sufficiently large test data set in 
order to obtain a reliable score for their system.  
Recently a number of attempts to remedy these 
shortcomings have led to the development of other 
automatic machine translation metrics. Some of 
them concentrate mainly on the word reordering 
aspect, like Maximum Matching String (Turian et 
al., 2003) or Translation Error Rate (Snover et al, 
2005). Others try to accommodate both syntactic 
and lexical differences between the candidate 
translation and the reference, like CDER (Leusch 
et al, 2006), which employs a version of edit dis-
tance for word substitution and reordering; 
METEOR (Banerjee and Lavie, 2005), which uses 
stemming and WordNet synonymy; and a linear 
regression model developed by (Russo-Lassner et 
al., 2005), which makes use of stemming, Word-
Net synonymy, verb class synonymy, matching 
noun phrase heads, and proper name matching. 
A closer examination of these metrics suggests 
that the accommodation of lexical equivalence is 
as difficult as the appropriate treatment of syntactic 
variation, in that it requires considerable external 
knowledge resources like WordNet, verb class da-
tabases, and extensive text preparation: stemming, 
tagging, etc. The advantage of our method is that it 
produces relevant paraphrases with nothing more 
than the evaluation bitext and a widely available 
word and phrase alignment software, and therefore 
can be used with any existing evaluation metric. 
87
3 Contextual bitext-derived paraphrases 
The method presented in this paper rests on a 
combination of two simple ideas. First, the compo-
nents necessary for automatic MT evaluation like 
BLEU or NIST, a source text and a reference text, 
constitute a miniature parallel corpus, from which 
word and phrase alignments can be extracted 
automatically, much like during the training for a 
statistical machine translation system. Second, tar-
get language words ei1, ?,  ein aligned as the likely 
translations to a source language word fi are often 
synonyms or near-synonyms of each other. This 
also holds for phrases: target language phrases epi1, 
?, epin aligned with a source language phrase fpi 
are often paraphrases of each other. For example, 
in our experiment, for the French word question 
the most probable automatically aligned English 
translations are question, matter, and issue, which 
in English are practically synonyms. Section 3.2 
presents more examples of such equivalent expres-
sions.  
3.1 
3.2 
                                                          
Experimental design 
For our experiment, we used two test sets, 
each consisting of 2000 sentences, drawn ran-
domly from the test section of the Europarl parallel 
corpus. The source language was French and the 
target language was English. One of the test sets 
was translated by Pharaoh trained on 156,000 
French-English sentence pairs. The other test set 
was translated by Logomedia, a commercially 
available rule-based MT system. Each test set con-
sisted therefore of three files: the French source 
file, the English translation file, and the English 
reference file. 
Each translation was evaluated by the BLEU 
and NIST metrics first with the single reference, 
then with the multiple references for each sentence 
using the paraphrases automatically generated 
from the source-reference mini corpus. A subset of 
a 100 sentences was randomly extracted from each 
test set and evaluated by two independent human 
judges with respect to accuracy and fluency; the 
human scores were then compared to the BLEU 
and NIST scores for the single-reference and the 
automatically generated multiple-reference files. 
Word alignment and phrase extraction 
We used the GIZA++ word alignment soft-
ware3 to produce initial word alignments for our 
miniature bilingual corpus consisting of the source 
French file and the English reference file, and the 
refined word alignment strategy of (Och and Ney, 
2003; Koehn et al, 2003; Tiedemann, 2004) to 
obtain improved word and phrase alignments. 
For each source word or phrase fi that is 
aligned with more than one target words or 
phrases, its possible translations ei1, ..., ein were 
placed in a list as equivalent expressions (i.e. 
synonyms, near-synonyms, or paraphrases of each 
other). A few examples are given in (1). 
 
(1) agreement - accordance 
adopted - implemented 
matter - lot - case 
funds - money 
arms - weapons 
area - aspect  
question ? issue ? matter 
we would expect - we cer-
tainly expect 
bear on - are centred 
around 
 
Alignment divides target words and 
phrases into equivalence sets; each set corresponds 
to one source word/phrase that was originally 
aligned with the target elements. For example, for 
the French word citoyens three English words were 
deemed to be the most appropriate translations: 
people, public, and citizens; therefore these three 
words constitute an equivalence set. Another 
French word population was aligned with two 
English translations: population and people; so the 
word people appears in two equivalence set (this 
gives rise to the question of equivalence transitiv-
ity, which will be discussed in Section 3.3). From 
the 2000-sentence evaluation bitext we derived 769 
equivalence sets, containing in total 1658 words or 
phrases. Each set contained on average two or 
three elements. In effect, we produced at least one 
equivalent expression for 1658 English words or 
phrases. 
An advantage of our method is that the tar-
get paraphrases and words come ordered with re-
3 http://www.fjoch.com/GIZA++ 
88
spect to their likelihood of being the translation of 
the source word or phrase ? each of them is as-
signed a probability expressing this likelihood, so 
we are able to choose only the most likely transla-
tions, according to some experimentally estab-
lished threshold. The experiment reported here was 
conducted without such a threshold, since the word 
and phrase alignment was of a very high quality. 
3.3 
3.4 
3.5 
Domain-specific lexical and syntactic 
paraphrases 
It is important to notice here how the para-
phrases produced are more appropriate to the task 
at hand than synonyms extracted from a general-
purpose thesaurus or WordNet. First, our para-
phrases are contextual - they are restricted to only 
those relevant to the domain of the text, since they 
are derived from the text itself. Given the context 
provided by our evaluation bitext, the word area in 
(1) turns out to be only synonymous with aspect, 
and not with land, territory, neighbourhood, divi-
sion, or other synonyms a general-purpose thesau-
rus or WordNet would give for this entry. This 
allows us to limit our multiple references only to 
those that are likely to be useful in the context pro-
vided by the source text. Second, the phrase align-
ment captures something neither a thesaurus nor 
WordNet will be able to provide: a certain amount 
of syntactic variation of paraphrases. Therefore, we 
know that a string such as we would expect in (1), 
with the sequence noun-aux-verb, might be para-
phrased by we certainly expect, a sequence of 
noun-adv-verb. 
Open and closed class items 
One important conclusion we draw from 
analysing the synonyms obtained through word 
alignment is that equivalence is limited mainly to 
words that belong to open word classes, i.e. nouns, 
verbs, adjectives, adverbs, but is unlikely to extend 
to closed word classes like prepositions or pro-
nouns. For instance, while the French preposition ? 
can be translated in English as to, in, or at, depend-
ing on the context, it is not the case that these three 
prepositions are synonymous in English. The divi-
sion is not that clear-cut, however: within the class 
of pronouns, he, she, and you are definitely not 
synonymous, but the demonstrative pronouns this 
and that might be considered equivalent for some 
purposes. Therefore, in our experiment we exclude 
prepositions and in future work we plan to examine 
the word alignments more closely to decide 
whether to exclude any other words. 
Creating multiple references 
After the list of synonyms and paraphrases is 
extracted from the evaluation bitext, for each 
reference sentence a string search replaces every 
eligible word or phrase with its equivalent(s) from 
the paraphrase list, one at a time, and the resulting 
string is added to the array of references. The 
original string is added to the array as well. This 
process results in a different number of reference 
sentences for every test sentence, depending on 
whether there was anything to replace in the refer-
ence and how many paraphrases we have available 
for the original substring. One example of this 
process is shown in (2). 
 
(2) Original reference: 
i admire the answer mrs parly 
gave this morning but we have 
turned a blind eye to that 
Paraphrase 1: 
i admire the reply mrs parly 
gave this morning but we have 
turned a blind eye to that 
Paraphrase 2: 
i admire the answer mrs parly 
gave this morning however we 
have turned a blind eye to 
that  
Paraphrase 3: 
i admire the answer mrs parly 
gave this morning but we have 
turned a blind eye to it 
 
Transitivity 
As mentioned before, an interesting question 
that arises here is the potential transitivity of our 
automatically derived synonyms/paraphrases. It 
could be argued that if the word people is equiva-
lent to public according to one set from our list, 
and to the word population according to another 
set, then public can be thought of as equivalent to 
population. In this case, the equivalence is not con-
troversial. However, consider the following rela-
tion: if sure in one of the equivalence sets is 
synonymous to certain, and certain in a different 
89
set is listed as equivalent to some, then treating 
sure and some as synonyms is a mistake. In our 
experiment we do not allow synonym transitivity; 
we only use the paraphrases from equivalence sets 
containing the word/phrase we want to replace.  
Multiple simultaneous substitution 
Note that at the moment the references we are 
producing do not contain multiple simultaneous 
substitutions of equivalent expressions; for exam-
ple, in (2) we currently do not produce the follow-
ing versions: 
 
(3) Paraphrase 4:  
i admire the reply mrs parly 
gave this morning however we 
have turned a blind eye to 
that 
Paraphrase 5: 
i admire the answer mrs parly 
gave this morning however we 
have turned a blind eye to it 
Paraphrase 6: 
i admire the reply mrs parly 
gave this morning but we have 
turned a blind eye to it 
 
This can potentially prevent higher n-grams being 
successfully matched if two or more equivalent 
expressions find themselves within the range of n-
grams being tested by BLEU and NIST. To avoid 
combinatorial problems, implementing multiple 
simultaneous substitutions could be done using a 
lattice, much like in (Pang et al, 2003). 
4 Results 
As expected, the use of multiple references 
produced by our method raises both the BLEU and 
NIST scores for translations produced by Pharaoh 
(test set PH) and Logomedia (test set LM). The 
results are presented in Table 1. 
 
 BLEU NIST 
PH single ref 0.2131 6.1625 
PH multi ref 0.2407 7.0068 
LM single ref 0.1782 5.5406 
LM multi ref 0.2043 6.3834 
 
Table 1. Comparison of single-reference and multi-
reference scores for test set PH and test set LM 
 
The hypothesis that the multiple-reference 
scores reflect better human judgment is also con-
firmed. For 100-sentence subsets (Subset PH and 
Subset LM) randomly extracted from our test sets 
PH and LM, we calculated Pearson?s correlation 
between the average accuracy and fluency scores 
that the translations in this subset received from 
two human judges (for each subset) and the single-
reference and multiple-reference sentence-level 
BLEU and NIST scores.  
There are two issues that need to be noted at 
this point. First, BLEU scored many of the sen-
tences as zero, artificially leveling many of the 
weaker translations.4 This explains the low, al-
though still statistically significant (p value < 
0.015) correlation with BLEU for both single and 
multiple reference translations. Using a version of 
BLEU with add-one smoothing we obtain consid-
erably higher correlations. Table 2 shows Pear-
son?s correlation coefficient for BLEU, BLEU 
with add-one smoothing, NIST, and human judg-
ments for Subsets PH. Multiple paraphrase refer-
ences produced by our method consistently lead to 
a higher correlation with human judgment for 
every metric.6 
 
                           Subset PH 
Metric  
single 
ref 
multi 
ref 
H & BLEU 0.297 0.307 
H & BLEU smoothed 0.396 0.404 
H & NIST  0.323 0.355 
 
Table 2. Pearson?s correlation between human 
judgment and single-reference and multiple-
reference BLEU, smoothed BLEU, and NIST for 
subset PH (of test set PH)  
 
The second issue that requires explanation is 
the lower general scores Logomedia?s translation 
received on the full set of 2000 sentences, and the 
extremely low correlation of its automatic evalua-
tion with human judgment, irrespective of the 
number of references. It has been noticed (Calli-
                                                          
4 BLEU uses a geometric average while calculating the sen-
tence-level score and will score a sentence as 0 if it does not 
have at least one 4-gram.  
5 A critical value for Pearson?s correlation coefficient for the 
sample size between 90 and 100 is 0.267, with p < 0.01. 
6 The significance of the rise in scores was confirmed in a 
resampling/bootstrapping test, with p < 0.0001. 
90
son-Burch et al, 2006) that BLEU and NIST fa-
vour n-gram based MT models such as Pharaoh, so 
the translation produced by Logomedia scored 
lower on the automatic evaluation, even though the 
human judges rated Logomedia output higher than 
Pharaoh?s translation. Both human judges consis-
tently gave very high scores to most sentences in 
subset LM (Logomedia), and as a consequence 
there was not enough variation in the scores as-
signed by them to create a good correlation with 
the BLEU and NIST scores. The average human 
scores for the subsets PH and LM and the coeffi-
cients of variation are presented in Table 3. It is 
easy to see that Logomedia?s translation received a 
higher mean score (on a scale 0 to 5) from the hu-
man judges and with less variance than Pharaoh. 
 
 Mean score  Variation 
Subset PH 3.815 19.1% 
Subset LM 4.005 16.25% 
 
Table 3. Human judgment mean scores and coeffi-
cients of variation for Subset PH and Subset LM 
 
As a result of the consistently high human scores 
for Logomedia, none of the Pearson?s correlations 
computed for Subset LM is high enough to be sig-
nificant. The values are lower than the critical 
value 0.164 corresponding to p < 0.10. 
 
                          Subset LM 
Metric  
single 
ref 
multi 
ref 
H & BLEU 0.046* 0.067* 
H & BLEU smoothed 0.163* 0.151* 
H & NIST  0.078* 0.116* 
 
Table 4. Pearson?s correlation between human 
judgment and single-reference and multiple-
reference BLEU, smoothed BLEU, and NIST for 
subset LM (of test set LM). * denotes values with p >  
0.10. 
5 Current and future work 
We would like to experiment with the way in 
which the list of equivalent expressions is pro-
duced. One possible development would be to de-
rive the expressions from a very large training 
corpus used by a statistical machine translation 
system, following (Bannard and Callison-Burch, 
2005), for instance, and use it as an external wider-
purpose knowledge resource (rather than a current 
domain-tailored resource as in our experiment), 
which would be nevertheless improve on a thesau-
rus in that it would also include phrase equivalents 
with some syntactic variation. According to (Ban-
nard and Callison-Burch, 2005), who derived their 
paraphrases automatically from a corpus of over a 
million German-English Europarl sentences, the 
baseline syntactic and semantic accuracy of the 
best paraphrases (those with the highest probabil-
ity) reaches 48.9% and 64.5%, respectively. That 
is, by replacing a phrase with its one most likely 
paraphrase the sentence remained syntactically 
well-formed in 48.9% of the cases and retained its 
meaning in 65% of the cases. 
In a similar experiment we generated para-
phrases from a French-English Europarl corpus of 
700,000 sentences. The data contained a consid-
erably higher level of noise than our previous ex-
periment on the 2000-sentence test set, even 
though we excluded any non-word entities from 
the results. Like (Bannard and Callison-Burch, 
2005), we used the product of probabilities p(fi|ei1) 
and p(ei2|fi) to determine the best paraphrase for a 
given English word ei1. We then compared the ac-
curacy across four samples of data. Each sample 
contained 50 randomly drawn words/phrases and 
their paraphrases. For the first two samples, the 
paraphrases were derived from the initial 2000-
sentence corpus; for the second two, the para-
phrases were derived from the 700,000-sentence 
corpus. For each corpus, one of the two samples 
contained only one best paraphrase for each entry, 
while the other listed all possible paraphrases. We 
then evaluated the quality of each paraphrase with 
respect to its syntactic and semantic accuracy. In 
terms of syntax, we considered the paraphrase ac-
curate either if it had the same category as the 
original word/phrase; in terms of semantics, we 
relied on human judgment of similarity. Tables 5 
and 6 summarize the syntactic and semantic accu-
racy levels in the samples. 
 
                       Paraphrases 
Derived from 
Best All 
2000-sent. corpus 59% 60% 
700,000-sent. corpus 70% 48% 
 
Table 5. Syntactic accuracy of paraphrases 
 
 
91
                       Paraphrases 
Derived from 
Best All 
2000-sent. corpus 83% 74% 
700,000-sent. corpus 76% 68% 
 
Table 6. Semantic accuracy of paraphrases 
 
Although it has to be kept in mind that these 
percentages were taken from relatively small sam-
ples, an interesting pattern emerges from compar-
ing the results. It seems that the average syntactic 
accuracy of all paraphrases decreases with in-
creased corpus size, but the syntactic accuracy of 
the one best paraphrase improves. This reflects the 
idea behind word alignment: the bigger the corpus, 
the more potential alignments there are for a given 
word, but at the same time the better their order in 
terms of probability and the likelihood to obtain 
the correct translation. Interestingly, the same pat-
tern is not repeated for semantic accuracy, but 
again, these samples are quite small. In order to 
address this issue, we plan to repeat the experiment 
with more data. 
Additionally, it should be noted that certain 
expressions, although not completely correct syn-
tactically, could be retained in the paraphrase lists 
for the purposes of machine translation evaluation. 
Consider the case where our equivalence set looks 
like this: 
 
(4) abandon ? abandoning ? 
abandoned 
 
The words in (4) are all inflected forms of the verb 
abandon, and although they would produce rather 
ungrammatical paraphrases, those ungrammatical 
paraphrases still allow us to score our translation 
higher in terms of BLEU or NIST if it contains one 
of the forms of abandon than when it contains 
some unrelated word like piano instead. This is 
exactly what other scoring metrics mentioned in 
Section 2 attempt to obtain with the use of stem-
ming or prefix matching. 
6 Conclusions 
In this paper we present a novel combination 
of existing ideas from statistical machine transla-
tion and paraphrase generation that leads to the 
creation of multiple references for automatic MT 
evaluation, using only the source and reference 
files that are required for the evaluation. The 
method uses simple word and phrase alignment 
software to find possible synonyms and para-
phrases for words and phrases of the target text, 
and uses them to produce multiple reference sen-
tences for each test sentence, raising the BLEU and 
NIST evaluation scores and reflecting human 
judgment better. The advantage of this method 
over other ways to generate paraphrases is that (1) 
unlike other methods, it does not require extensive 
parallel monolingual paraphrase corpora, but it 
extracts equivalent expressions from the miniature 
bilingual corpus of the source and reference 
evaluation files; (2) unlike other ways to accom-
modate synonymy in automatic evaluation, it does 
not require external lexical knowledge sources like 
thesauri or WordNet; (3) it extracts only synonyms 
that are relevant to the domain in hand; and (4) the 
equivalent expressions it produces include a certain 
amount of syntactic paraphrases.  
The method is general and it can be used with 
any automatic evaluation metric that supports mul-
tiple references. In our future work, we plan to ap-
ply it to newly developed evaluation metrics like 
CDER and TER that aim to allow for syntactic 
variation between the candidate and the reference, 
therefore bringing together solutions for the two 
shortcomings of automatic evaluation systems: 
insensitivity to allowable lexical differences and 
syntactic variation. 
References 
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. Proceed-
ings of the ACL 2005 Workshop on Intrinsic and 
Extrinsic Evaluation Measures for MT and/or Sum-
marization: 65-73. 
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. Proceed-
ings of the 43rd Annual Meeting of the Association for 
Computational Linguistics (ACL 2005): 597-604. 
Chris Callison-Burch, Miles Osborne and Philipp 
Koehn. 2006. Re-evaluating the role of BLEU in 
Machine Translation Research. To appear in Pro-
ceedings of EACL-2006. 
Mona Diab and Philip Resnik. 2002. An unsupervised 
Method for Word Sense Tagging using Parallel Cor-
pora. Proceedings of the 40th Annual Meeting of the 
92
Association for Computational Linguistics, Philadel-
phia, PA. 
George Doddington. 2002. Automatic Evaluation of MT 
Quality using N-gram Co-occurrence Statistics. Pro-
ceedings of Human Language Technology Confer-
ence 2002: 138?145. 
Philipp Koehn, Franz Och and Daniel Marcu. 2003. 
Statistical Phrase-Based Translation. Proceedings of 
the  Human Language Technology Conference (HLT-
NAACL 2003): 48-54. 
Philipp Koehn. 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation mod-
els. Machine translation: From real users to re-
search. 6th Conference of the Association for 
Machine Translation in the Americas (AMTA 2004): 
115-124. 
Philipp Koehn. 2005. Europarl: A Parallel Corpus for 
Statistical Machine Translation. Proceedings of MT 
Summit 2005: 79-86. 
Gregor Leusch, Nicola Ueffing and Hermann Ney. 
2006. CDER: Efficient MT Evaluation Using Block 
Movements. To appear in Proceedings of the 11th 
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL 2006). 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Modes. 
Computational Linguistics, 29:19?51. 
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, 
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar 
Kumar, Libin Shen, David Smith, Katherine Eng, 
Viren Jain, Zhen Jin, and Dragomir Radev. 2003. 
Syntax for statistical machine translation. Technical 
report, Center for Language and Speech Processing, 
John Hopkins University, Baltimore, MD.  
Bo Pang, Kevin Knight and Daniel Marcu. 2003. Syn-
tax-based Alignment of Multiple Translations: Ex-
tracting Paraphrases and Generating New Sentences. 
Proceedings of Human Language Technology-North 
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL) 2003: 181?188. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
ACL: 311-318. 
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik. 
2005. A Paraphrase-based Approach to Machine 
Translation Evaluation. Technical Report LAMP-
TR-125/CS-TR-4754/UMIACS-TR-2005-57, Uni-
versity of Maryland, College Park, MD. 
Mathew Snover, Bonnie Dorr, Richard Schwartz, John 
Makhoul, Linnea Micciula and Ralph Weischedel. 
2005. A Study of Translation Error Rate with Tar-
geted Human Annotation. Technical Report LAMP-
TR-126, CS-TR-4755, UMIACS-TR-2005-58, Uni-
versity of Maryland, College Park. MD. 
J?rg Tiedemann. 2004. Word to word alignment strate-
gies. Proceedings of the 20th International Confer-
ence on Computational Linguistics (COLING 2004): 
212-218. 
Joseph P. Turian, Luke Shen, and I. Dan Melamed. 
2003. Evaluation of Machine translation and Its 
Evaluation. Proceedings of MT Summit 2003: 386-
393. 
Ying Zhang and Stephan Vogel. 2004. Measuring con-
fidence intervals for the machine translation evalua-
tion metrics. TMI-2004: Proceedings of the 10th 
Conference on Theoretical and Methodological Is-
sues in Machine Translation: 85-94. 
93
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 80?87,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Dependency-Based Automatic Evaluation for Machine Translation 
Karolina Owczarzak Josef van Genabith Andy Way  
National Centre for Language Technology  
School of Computing, Dublin City University  
Dublin 9, Ireland  
{owczarzak,josef,away}@computing.dcu.ie  
    
 
Abstract 
We present a novel method for evaluating 
the output of Machine Translation (MT), 
based on comparing the dependency 
structures of the translation and reference 
rather than their surface string forms. Our 
method uses a treebank-based, wide-
coverage, probabilistic Lexical-Functional 
Grammar (LFG) parser to produce a set of 
structural dependencies for each 
translation-reference sentence pair, and 
then calculates the precision and recall for 
these dependencies. Our dependency-
based evaluation, in contrast to most 
popular string-based evaluation metrics, 
will not unfairly penalize perfectly valid 
syntactic variations in the translation. In 
addition to allowing for legitimate 
syntactic differences, we use paraphrases 
in the evaluation process to account for 
lexical variation. In comparison with 
other metrics on 16,800 sentences of 
Chinese-English newswire text, our 
method reaches high correlation with 
human scores. An experiment with two 
translations of 4,000 sentences from 
Spanish-English Europarl shows that, in 
contrast to most other metrics, our method 
does not display a high bias towards 
statistical models of translation. 
1 Introduction 
Since their appearance, string-based evaluation 
metrics such as BLEU (Papineni et al, 2002) and 
NIST (Doddington, 2002) have been the standard 
tools used for evaluating MT quality. Both score a 
candidate translation on the basis of the number of 
n-grams shared with one or more reference 
translations. Automatic measures are indispensable 
in the development of MT systems, because they 
allow MT developers to conduct frequent, cost-
effective, and fast evaluations of their evolving 
models.  
These advantages come at a price, though: an 
automatic comparison of n-grams measures only 
the string similarity of the candidate translation to 
one or more reference strings, and will penalize 
any divergence from them. In effect, a candidate 
translation expressing the source meaning 
accurately and fluently will be given a low score if 
the lexical and syntactic choices it contains, even 
though perfectly legitimate, are not present in at 
least one of the references. Necessarily, this score 
would differ from a much more favourable human 
judgement that such a translation would receive. 
The limitations of string comparison are the 
reason why it is advisable to provide multiple 
references for a candidate translation in BLEU- or 
NIST-based evaluations. While Zhang and Vogel 
(2004) argue that increasing the size of the test set 
gives even more reliable system scores than 
multiple references, this still does not solve the 
inadequacy of BLEU and NIST for sentence-level 
or small set evaluation. In addition, in practice 
even a number of references do not capture the 
whole potential variability of the translation. 
Moreover, when designing a statistical MT system, 
the need for large amounts of training data limits 
the researcher to collections of parallel corpora 
such as Europarl (Koehn, 2005), which provides 
only one reference, namely the target text; and the 
cost of creating additional reference translations of 
the test set, usually a few thousand sentences long, 
is often prohibitive. Therefore, it would be 
desirable to find an evaluation method that accepts 
legitimate syntactic and lexical differences 
80
between the translation and the reference, thus 
better mirroring human assessment. 
In this paper, we present a novel method that 
automatically evaluates the quality of translation 
based on the dependency structure of the sentence, 
rather than its surface form. Dependencies abstract 
away from the particulars of the surface string (and 
CFG tree) realization and provide a ?normalized? 
representation of (some) syntactic variants of a 
given sentence. The translation and reference files 
are analyzed by a treebank-based, probabilistic 
Lexical-Functional Grammar (LFG) parser (Cahill 
et al, 2004), which produces a set of dependency 
triples for each input. The translation set is 
compared to the reference set, and the number of 
matches is calculated, giving the precision, recall, 
and f-score for that particular translation.   
In addition, to allow for the possibility of valid 
lexical differences between the translation and the 
references, we follow Kauchak and Barzilay 
(2006) and Owczarzak et al (2006) in adding a 
number of paraphrases in the process of evaluation 
to raise the number of matches between the 
translation and the reference, leading to a higher 
score. 
Comparing the LFG-based evaluation method 
with other popular metrics: BLEU, NIST, General 
Text Matcher (GTM) (Turian et al, 2003), 
Translation Error Rate (TER) (Snover et al, 
2006)1, and METEOR (Banerjee and Lavie, 2005), 
we show that combining dependency 
representations with paraphrases leads to a more 
accurate evaluation that correlates better with 
human judgment. 
The remainder of this paper is organized as 
follows: Section 2 gives a basic introduction to 
LFG; Section 3 describes related work; Section 4 
describes our method and gives results of two 
experiments on different sets of data: 4,000 
sentences from Spanish-English Europarl and 
16,800 sentences of Chinese-English newswire text 
from the Linguistic Data Consortium?s (LDC) 
Multiple Translation project; Section 5 discusses 
ongoing work; Section 6 concludes. 
                                                 
1 As we focus on purely automatic metrics, we omit 
HTER (Human-Targeted Translation Error Rate) here. 
2 Lexical-Functional Grammar 
In Lexical-Functional Grammar (Bresnan, 2001) 
sentence structure is represented in terms of 
c(onstituent)-structure and f(unctional)-structure. 
C-structure represents the surface string word order 
and the hierarchical organisation of phrases in 
terms of CFG trees. F-structures are recursive 
feature (or attribute-value) structures, representing 
abstract grammatical relations, such as subj(ect), 
obj(ect), obl(ique), adj(unct), approximating to 
predicate-argument structure or simple logical 
forms. C-structure and f-structure are related in 
terms of functional annotations (attribute-value 
structure equations) in c-structure trees, describing 
f-structures.  
While c-structure is sensitive to surface word 
order, f-structure is not. The sentences John 
resigned yesterday and Yesterday, John resigned 
will receive different tree representations, but 
identical f-structures, shown in (1). 
 
(1) C-structure:                         F-structure: 
 
              S 
                  
      
 NP                      VP 
   |                     
John       
              V               NP-TMP 
               |                      | 
       resigned       yesterday 
                         
SUBJ        PRED   john 
                 NUM    sg 
                 PERS   3 
PRED       resign 
TENSE     past 
ADJ      {[PRED   yesterday]} 
 
 
                     S 
                  
      
    NP       NP       VP 
      |                 |            | 
Yesterday  John        V              
                                    | 
                            resigned                             
SUBJ        PRED   john 
                 NUM    sg 
                 PERS   3 
PRED       resign 
TENSE     past 
ADJ      {[PRED   yesterday]} 
 
 
 
Notice that if these two sentences were a 
translation-reference pair, they would receive a 
less-than-perfect score from string-based metrics. 
For example, BLEU with add-one smoothing2 
gives this pair a score of barely 0.3781. 
The f-structure can also be described as a flat 
set of triples. In triples format, the f-structure in (1) 
could be represented as follows: {subj(resign, 
john), pers(john, 3), num(john, sg), tense(resign, 
                                                 
2 We use smoothing because the original BLEU gives 
zero points to sentences with fewer than one four-gram. 
81
past), adj(resign, yesterday), pers(yesterday, 3), 
num(yesterday, sg)}. 
Cahill et al (2004) presents Penn-II Treebank-
based LFG parsing resources. Her approach 
distinguishes 32 types of dependencies, including 
grammatical functions and morphological 
information. This set can be divided into two major 
groups: a group of predicate-only dependencies 
and non-predicate dependencies. Predicate-only 
dependencies are those whose path ends in a 
predicate-value pair, describing grammatical 
relations. For example, for the f-structure in (1), 
predicate-only dependencies would include: 
{subj(resign, john), adj(resign, yesterday)}.3  
In parser evaluation, the quality of the f-
structures produced automatically can be checked 
against a set of gold standard sentences annotated 
with f-structures by a linguist. The evaluation is 
conducted by calculating the precision and recall 
between the set of dependencies produced by the 
parser, and the set of dependencies derived from 
the human-created f-structure. Usually, two 
versions of f-score are calculated: one for all the 
dependencies for a given input, and a separate one 
for the subset of predicate-only dependencies. 
In this paper, we use the parser developed by 
Cahill et al (2004), which automatically annotates 
input text with c-structure trees and f-structure 
dependencies, reaching high precision and recall 
rates. 4  
3 Related work 
The insensitivity of BLEU and NIST to perfectly 
legitimate syntactic and lexical variation has been 
raised, among others, in Callison-Burch et al 
(2006), but the criticism is widespread. Even the 
creators of BLEU point out that it may not 
correlate particularly well with human judgment at 
the sentence level (Papineni et al, 2002). A side 
                                                 
3 Other predicate-only dependencies include: 
apposition,  complement, open complement, 
coordination, determiner, object, second object, 
oblique, second oblique, oblique agent, possessive, 
quantifier, relative clause, topic, relative clause 
pronoun. The remaining non-predicate dependencies 
are: adjectival degree, coordination surface form, focus, 
complementizer forms: if, whether, and that, modal, 
number, verbal particle, participle, passive, person, 
pronoun surface form, tense, infinitival clause. 
4 http://lfg-demo.computing.dcu.ie/lfgparser.html 
effect of this phenomenon is that BLEU is less 
reliable for smaller data sets, so the advantage it 
provides in the speed of evaluation is to some 
extent counterbalanced by the time spent by 
developers on producing a sufficiently large test 
set in order to obtain a reliable score for their 
system.  
Recently a number of attempts to remedy these 
shortcomings have led to the development of other 
automatic MT evaluation metrics. Some of them 
concentrate mainly on word order, like General 
Text Matcher (Turian et al, 2003), which 
calculates precision and recall for translation-
reference pairs, weighting contiguous matches 
more than non-sequential matches, or Translation 
Error Rate (Snover et al, 2005), which computes 
the number of substitutions, inserts, deletions, and 
shifts necessary to transform the translation text to 
match the reference. Others try to accommodate 
both syntactic and lexical differences between the 
candidate translation and the reference, like CDER 
(Leusch et al, 2006), which employs a version of 
edit distance for word substitution and reordering; 
or METEOR (Banerjee and Lavie, 2005), which 
uses stemming and WordNet synonymy. Kauchak 
and Barzilay (2006) and Owczarzak et al (2006) 
use paraphrases during BLEU and NIST evaluation 
to increase the number of matches between the 
translation and the reference; the paraphrases are 
either taken from WordNet5 in Kauchak and 
Barzilay (2006) or derived from the test set itself 
through automatic word and phrase alignment in 
Owczarzak et al (2006). Another metric making 
use of synonyms is the linear regression model 
developed by Russo-Lassner et al (2005), which 
makes use of stemming, WordNet synonymy, verb 
class synonymy, matching noun phrase heads, and 
proper name matching. Kulesza and Schieber 
(2004), on the other hand, train a Support Vector 
Machine using features like proportion of n-gram 
matches and word error rate to judge a given 
translation?s distance from human-level quality. 
Nevertheless, these metrics use only string-
based comparisons, even while taking into 
consideration reordering. By contrast, our 
dependency-based method concentrates on 
utilizing linguistic structure to establish a 
comparison between translated sentences and their 
reference.  
                                                 
5 http://wordnet.princeton.edu/ 
82
4 LFG f-structure in MT evaluation 
The process underlying the evaluation of f-
structure quality against a gold standard can be 
used in automatic MT evaluation as well: we parse 
the translation and the reference, and then, for each 
sentence, we check the set of translation 
dependencies against the set of reference 
dependencies, counting the number of matches. As 
a result, we obtain the precision and recall scores 
for the translation, and we calculate the f-score for 
the given pair. Because we are comparing two 
outputs that were produced automatically, there is 
a possibility that the result will not be noise-free. 
To assess the amount of noise that the parser 
may introduce we conducted an experiment where 
100 English Europarl sentences were modified by 
hand in such a way that the position of adjuncts 
was changed, but the sentence remained 
grammatical and the meaning was not changed. 
This way, an ideal parser should give both the 
source and the modified sentence the same f-
structure, similarly to the case presented in (1). The 
modified sentences were treated like a translation 
file, and the original sentences played the part of 
the reference. Each set was run through the parser. 
We evaluated the dependency triples obtained from 
the ?translation? against the dependency triples for 
the ?reference?, calculating the f-score, and applied 
other metrics (TER, METEOR, BLEU, NIST, and 
GTM) to the set in order to compare scores. The 
results, inluding the distinction between f-scores 
for all dependencies and predicate-only 
dependencies, appear in Table 1. 
 
 baseline modified 
TER 0.0 6.417 
METEOR   1.0 0.9970 
BLEU 1.0000 0.8725 
NIST 11.5232 11.1704 (96.94%) 
GTM 100 99.18 
dep f-score  100 96.56 
dep_preds f-score 100 94.13 
Table 1. Scores for sentences with reordered adjuncts 
 
The baseline column shows the upper bound for a 
given metric: the score which a perfect translation, 
word-for-word identical to the reference, would 
obtain.6 In the other column we list the scores that 
the metrics gave to the ?translation? containing 
reordered adjunct. As can be seen, the dependency 
and predicate-only dependency scores are lower 
than the perfect 100, reflecting the noise 
introduced by the parser.  
To show the difference between the scoring 
based on LFG dependencies and other metrics in 
an ideal situation, we created another set of a 
hundred sentences with reordered adjuncts, but this 
time selecting only those reordered sentences that 
were given the same set of dependencies by the 
parser (in other words, we simulated having the 
ideal parser). As can be seen in Table 2, other 
metrics are still unable to tolerate legitimate 
variation in the position of adjuncts, because the 
sentence surface form differs from the reference; 
however, it is not treated as an error by the parser. 
 
 baseline modified 
TER 0.0 7.841 
METEOR   1.0 0.9956 
BLEU 1.0000 0.8485 
NIST 11.1690 10.7422 (96.18%) 
GTM 100 99.35 
dep f-score  100 100 
dep_preds f-score 100 100 
Table 2. Scores for sentences with reordered adjuncts in 
an ideal situation 
4.1 Initial experiment ? Europarl 
In the first experiment, we attempted to determine 
whether the dependency-based measure is biased 
towards statistical MT output, a problem that has 
been observed for n-gram-based metrics like 
BLEU and NIST. Callison-Burch et al (2006) 
report that BLEU and NIST favour n-gram-based 
MT models such as Pharaoh (Koehn, 2004), so the 
translations produced by rule-based systems score 
lower on the automatic evaluation, even though 
human judges consistently rate their output higher 
than Pharaoh?s translation. Others repeatedly 
                                                 
6 Two things have to be noted here: (1) in case of NIST 
the perfect score differs from text to text, which is why 
we provide the percentage points as well, and (2) in case 
of TER the lower the score, the better the translation, so 
the perfect translation will receive 0, and there is no 
upper bound on the score, which makes this particular 
metric extremely difficult to directly compare with 
others. 
83
observed this tendency in previous research as 
well; in one experiment, reported in Owczarzak et 
al. (2006), where the rule-based system 
Logomedia7 was compared with Pharaoh, BLEU 
scored Pharaoh 0.0349 points higher, NIST scored 
Pharaoh 0.6219 points higher, but human judges 
scored Logomedia output 0.19 points higher (on a 
5-point scale).  
4.1.1 Experimental design 
In order to check for the existence of a bias in the 
dependency-based metric, we created a set of 
4,000 sentences drawn randomly from the Spanish-
English subset of Europarl (Koehn, 2005), and we 
produced two translations: one by a rule-based 
system Logomedia, and the other by the standard 
phrase-based statistical decoder Pharaoh, using 
alignments produced by GIZA++8 and the refined 
word alignment strategy of Och and Ney (2003). 
The translations were scored with a range of 
metrics: BLEU, NIST, GTM, TER, METEOR, and 
the dependency-based method. 
4.1.2 Adding synonyms 
Besides the ability to allow syntactic variants as 
valid translations, a good metric should also be 
able to accept legitimate lexical variation. We 
introduced synonyms and paraphrases into the 
process of evaluation, creating new best-matching 
references for the translations using either 
paraphrases derived from the test set itself 
(following Owczarzak et al (2006)) or WordNet 
synonyms (as in Kauchak and Barzilay (2006)). 
 
Bitext-derived paraphrases 
Owczarzak et al (2006) describe a simple way to 
produce a list of paraphrases, which can be useful 
in MT evaluation, by running word alignment 
software on the test set that is being evaluated. 
Paraphrases derived in this way are specific to the 
domain at hand and contain low-level syntactic 
variants in addition to word-level synonymy. 
Using the standard GIZA++ software and the 
refined word alignment strategy of Och and Ney 
(2003) on our test set of 4,000 Spanish-English 
sentences, the method generated paraphrases for 
just over 1100 items. These paraphrases served to 
                                                 
7 http://www.lec.com/ 
8 http://www.fjoch.com/GIZA++ 
create new individual best-matching references for 
the Logomedia and Pharaoh translations. Due to 
the small size of the paraphrase set, only about 
20% of reference sentences were actually modified 
to better reflect the translation. This, in turn, led to 
little difference in scores. 
WordNet synonyms 
To maximize the number of matches between a 
translation and a reference, Kauchak and Barzilay 
(2006) use WordNet synonyms during evaluation. 
In addition, METEOR also has an option of 
including WordNet in the evaluation process. As in 
the case of bitext-derived paraphrases, we used 
WordNet synonyms to create new best-matching 
references for each of the two translations. This 
time, given the extensive database containing 
synonyms for over 150,000 items, around 70% of 
reference sentences were modified: 67% for 
Pharaoh, and 75% for Logomedia. Note that the 
number of substitutions is higher for Logomedia; 
this confirms the intuition that the translation 
produced by Pharaoh, trained on the domain which 
is also the source of the reference text, will need 
fewer lexical replacements than Logomedia, which 
is based on a general non-domain-specific model. 
4.1.3 Results 
Table 3 shows the difference between the scores 
which Pharaoh?s and Logomedia?s translations 
obtained from each metric: a positive number 
shows by how much Pharaoh?s score was higher 
than Logomedia?s, and a negative number reflects 
Logomedia?s higher score (the percentages are 
absolute values). As can be seen, all the metrics 
scored Pharaoh higher, inlcuding METEOR and 
the dependency-based method that were boosted 
with WordNet. The values in the table are sorted in 
descending order, from the largest to the lowest 
advantage of Pharaoh over Logomedia. 
Interestingly, next to METEOR boosted with 
WordNet, it is the dependency-based method, and 
especially the predicates-only version, that shows 
the least bias towards the phrase-based translation. 
In the next step, we selected from this set smaller 
subsets of sentences that were more and more 
similar in terms of translation quality (as 
determined by a sentence?s BLEU score). As the 
similarity of the translation quality increased, most 
metrics lowered their bias, as is shown in Table 4. 
The first column shows the case where the 
sentences chosen differed at the most by 0.05 
84
points BLEU score; in the second column the 
difference was lowered to 0.01; and in the third 
column to 0.005. The numbers following the hash 
signs in the header row indicate the number of 
sentences in a given set.  
 
metric PH score ? LM score 
TER 1.997 
BLEU 7.16% 
NIST 6.58% 
dep 4.93% 
dep+paraphr 4.80% 
GTM 3.89% 
METEOR 3.80% 
dep_preds 3.79% 
dep+paraphr_preds 3.70% 
dep+WordNet 3.55% 
dep+WordNet_preds 2.60% 
METEOR+WordNet 1.56% 
Table 3. Difference between scores assigned to Pharaoh 
and Logomedia. Positive numbers show by how much 
Pharaoh?s score was higher than Logomedia?s. Legend: 
dep = dependency f-score, paraph = paraphrases, _preds = 
predicate-only f-score.  
 
~ 0.05 #1692 ~ 0.01 #567 ~ 0.005 #335 
NIST 2.29% NIST 1.76% NIST 1.48% 
BLEU 0.95% BLEU 0.42% BLEU 0.59% 
GTM 0.94% GTM 0.29% GTM -0.09% 
d+p 0.67% d 0.04% d+p -0.15% 
d 0.61% d+p 0.02% d -0.24% 
d+WN -0.29% d+WN -0.78% d+WN -0.99% 
d+p_pr -0.70% M -0.99% d+p_pr -1.30% 
d_pr -0.75% d_pr -1.37% d_pr -1.43% 
M -1.03% d+p_pr -1.38% M -1.57% 
d+WN_pr -1.43% d+WN_pr -1.97% d+WN_pr -1.94% 
M+WN -2.51% M+WN -2.21% M+WN -2.74% 
TER -1.579 TER -1.228 TER -1.739 
Table 4. Difference between scores assigned to Pharaoh 
and Logomedia for sets of increasing similarity. Positive 
numbers show Pharaoh?s advantage, negative numbers 
show Logomedia?s advantage. Legend: d = dependency f-
score, p = paraphrases, _pr = predicate-only f-score, M = 
METEOR, WN = WordNet.  
 
These results confirm earlier suggestions that 
the predicate-only version of the dependency-
based evaluation is less biased in favour of the 
statistical MT system than the version that includes 
all dependency types. Adding a sufficient number 
of lexical choices reduces the bias even further; 
although again, paraphrases generated from the test 
set only are too few to make a significant 
difference. Similarly to METEOR, the 
dependency-based method shows on the whole 
lower bias than other metrics. However, we cannot 
be certain that the underlying scores vary linearly 
with each other and with human judgements, as we 
have no framework of reference such as human 
segment-level assessment of translation quality in 
this case. Therefore, the correlation with human 
judgement is analysed in our next experiment.   
4.2 Correlation with human judgement ? 
MultiTrans 
To calculate how well the dependency-based 
method correlates with human judgement, and how 
it compares to the correlation shown by other 
metrics, we conducted an experiment on Chinese-
English newswire text.  
4.2.1 Experimental design 
We used the data from the Linguistic Data 
Consortium Multiple Translation Chinese (MTC) 
Parts 2 and 4. The data consists of multiple 
translations of Chinese newswire text, four human-
produced references, and segment-level human 
scores for a subset of the translation-reference 
pairs. Although a single translated segment was 
always evaluated by more than one judge, the 
judges used a different reference every time, which 
is why we treated each translation-reference-
human score triple as a separate segment. In effect, 
the test set created from this data contained 16,800 
segments. As in the previous experiment, the 
translation was scored using BLEU, NIST, GTM, 
TER, METEOR, and the dependency-based 
method. 
4.2.2 Results 
We calculated Pearson?s correlation coefficient for 
segment-level scores that were given by each 
metric and by human judges. The results of the 
correlation are shown in Table 5. Note that the 
correlation for TER is negative, because in TER 
zero is the perfect score, in contrast to other 
metrics where zero is the worst possible score; 
however, this time the absolute values can be 
easily compared to each other. Rows are ordered 
85
by the highest value of the (absolute) correlation 
with the human score. 
First, it seems like none of the metrics is very 
good at reflecting human fluency judgments; the 
correlation values in the first column are 
significantly lower than the correlation with 
accuracy. However, the dependency-based method 
in almost all its versions has decidedly the highest 
correlation in this area. This can be explained by 
the method?s sensitivity to the grammatical 
structure of the sentence: a more grammatical 
translation is also a translation that is more fluent. 
 
H_FL  H_AC  H_AVE  
d+WN 0.168 M+WN 0.294 M+WN 0.255 
d   0.162 M   0.278 d+WN 0.244 
d+WN_pr 0.162 NIST 0.273 M   0.242 
BLEU 0.155 d+WN 0.266 NIST 0.238 
d_pr 0.154 GTM 0.260 d   0.236 
M+WN 0.153 d  0.257 GTM 0.230 
M   0.149 d+WN_pr 0.232 d+WN_pr 0.220 
NIST 0.146 d_pr 0.224 d_pr 0.212 
GTM 0.146 BLEU 0.199 BLEU 0.197 
TER -0.133 TER -0.192 TER -0.182 
Table 5. Pearson?s correlation between human scores and 
evaluation metrics. Legend: d = dependency f-score, _pr = 
predicate-only f-score, M = METEOR, WN = WordNet, 
H_FL = human fluency score, H_AC = human accuracy 
score, H_AVE = human average score.9 
 
Second, and somewhat surprisingly, in this 
detailed examination the relative order of the 
metrics changed. The predicate-only version of the 
dependency-based method appears to be less 
adequate for correlation with human scores than its 
non-restricted versions. As to the correlation with 
human evaluation of translation accuracy, our 
method currently falls short of METEOR and even 
NIST. This is caused by the fact that both 
METEOR and NIST assign relatively little 
importance to the position of a specific word in a 
sentence, therefore rewarding the translation for 
content rather than linguistic form. For our 
dependency-based method, the noise introduced by 
the parser might be the reason for low correlation: 
if even one side of the translation-reference pair 
contains parsing errors, this may lead to a less 
reliable score. An obvious solution to this problem, 
                                                 
9 In general terms, an increase of 0.015 between any two 
scores is significant with a 95% confidence interval. 
which we are examining at the moment, is to 
include a number of best parses for each side of the 
evaluation. 
High correlation with human judgements of 
fluency and lower correlation with accuracy results 
in a high second place for our dependency-based 
method when it comes to the average correlation 
coefficient. The WordNet-boosted dependency-
based method scores only slightly lower than 
METEOR with WordNet. These results are very 
encouraging, especially as we see a number of 
ways the dependency-based method could be 
further developed.  
5 Current and future work 
While the idea of a dependency-based method is a 
natural step in the direction of a deeper linguistic 
analysis for MT evaluation, it does require an LFG 
grammar and parser for the target language. There 
are several obvious areas for improvement with 
respect to the method itself. First, we would also 
like to adapt the process of translation-reference 
dependency comparison to include n-best parsers 
for the input sentences, as well as some basic 
transformations which would allow an even deeper 
logical analysis of input (e.g. passive to active 
voice transformation). 
 Second, we want to repeat both 
experiments using a paraphrase set derived from a 
large parallel corpus, rather than the test set, as 
described in Owczarzak et al (2006). While 
retaining the advantage of having a similar size to 
a corresponding set of WordNet synonyms, this set 
will also capture low-level syntactic variations, 
which can increase the number of matches and the 
correlation with human scores. 
 Finally, we want to take advantage of the 
fact that the score produced by the dependency-
based method is the proportional average of f-
scores for a group of up to 32 (but usually far 
fewer) different dependency types. We plan to 
implement a set of weights, one for each 
dependency type, trained in such a way as to 
maximize the correlation of the final dependency f-
score with human evaluation.  
6 Conclusions 
In this paper we present a novel way of 
evaluating MT output. So far, all metrics relied on 
86
comparing translation and reference on a string 
level. Even given reordering, stemming, and 
synonyms for individual words, current methods 
are still far from reaching human ability to assess 
the quality of translation. Our method compares 
the sentences on the level of their grammatical 
structure, as exemplified by their f-structure 
dependency triples produced by an LFG parser. 
The dependency-based method can be further 
augmented by using paraphrases or WordNet 
synonyms, and is available in full version and 
predicate-only version. In our experiments we 
showed that the dependency-based method 
correlates higher than any other metric with human 
evaluation of translation fluency, and shows high 
correlation with the average human score. The use 
of dependencies in MT evaluation is a rather new 
idea and requires more research to improve it, but 
the method shows potential to become an accurate 
evaluation metric.  
 
Acknowledgements 
This work was partly funded by Microsoft Ireland 
PhD studentship  2006-8  for the first author of the 
paper. We would also like to thank our reviewers 
for their insightful comments. All remaining errors 
are our own. 
References 
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with 
Improved Correlation with Human Judgments. 
Proceedings of the ACL 2005 Workshop on Intrinsic 
and Extrinsic Evaluation Measures for MT and/or 
Summarization: 65-73. 
Joan Bresnan. 2001. Lexical-Functional Syntax, 
Blackwell, Oxford. 
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef 
van Genabith, and Andy Way. 2004. Long-Distance 
Dependency Resolution in Automatically Acquired 
Wide-Coverage PCFG-Based LFG Approximations, 
In Proceedings of ACL-04: 320-327 
Chris Callison-Burch, Miles Osborne and Philipp 
Koehn. 2006. Re-evaluating the role of BLEU in 
Machine Translation Research. Proceedings of  
EACL 2006: 249-256 
George Doddington. 2002. Automatic Evaluation of MT 
Quality using N-gram Co-occurrence Statistics. 
Proceedings of HLT 2002: 138-145. 
David Kauchak and Regina Barzilay. 2006. 
Paraphrasing for Automatic Evaluation. Proceedings 
of HLT-NAACL 2006: 45-462. 
Philipp Koehn. 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation 
models. Proceedings of the AMTA 2004 Workshop 
on Machine Translation: From real users to 
research: 115-124. 
Philipp Koehn. 2005. Europarl: A Parallel Corpus for 
Statistical Machine Translation. Proceedings of MT 
Summit 2005: 79-86. 
Alex Kulesza and Stuart M. Shieber. 2004. A learning 
approach to improving sentence-level MT evaluation. 
In Proceedings of the TMI 2004: 75-84. 
Gregor Leusch, Nicola Ueffing and Hermann Ney. 
2006. CDER: Efficient MT Evaluation Using Block 
Movements. Proceedings of EACL 2006: 241-248. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Modes. 
Computational Linguistics, 29:19-51. 
Karolina Owczarzak, Declan Groves, Josef van 
Genabith, and Andy Way. 2006. Contextual Bitext-
Derived Paraphrases in Automatic MT Evaluation. 
Proceedings of the HLT-NAACL 2006 Workshop on 
Statistical Machine Translation: 86-93. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
WeiJing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
ACL 2002: 311-318. 
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik. 
2005. A Paraphrase-based Approach to Machine 
Translation Evaluation. Technical Report LAMP-TR-
125/CS-TR-4754/UMIACS-TR-2005-57, University 
of Maryland, College Park, MD. 
Mathew Snover, Bonnie Dorr, Richard Schwartz, John 
Makhoul, Linnea Micciula. 2006. A Study of 
Translation Error Rate with Targeted Human 
Annotation. Proceedings of AMTA 2006: 223-231. 
Joseph P. Turian, Luke Shen, and I. Dan Melamed. 
2003. Evaluation of Machine Translation and Its 
Evaluation. Proceedings of MT Summit 2003: 386-
393. 
Ying Zhang and Stephan Vogel. 2004. Measuring 
confidence intervals for the machine translation 
evaluation metrics. Proceedings of TMI 2004: 85-94. 
87
Proceedings of the Second Workshop on Statistical Machine Translation, pages 104?111,
Prague, June 2007. c?2007 Association for Computational Linguistics
Labelled Dependencies in Machine Translation Evaluation 
Karolina Owczarzak Josef van Genabith Andy Way 
National Centre for Language Technology 
School of Computing, Dublin City University 
Dublin 9, Ireland 
{owczarzak,josef,away}@computing.dcu.ie 
    
 
Abstract 
We present a method for evaluating the 
quality of Machine Translation (MT) 
output, using labelled dependencies 
produced by a Lexical-Functional 
Grammar (LFG) parser. Our dependency-
based method, in contrast to most popular 
string-based evaluation metrics, does not 
unfairly penalize perfectly valid syntactic 
variations in the translation, and the 
addition of WordNet provides a way to 
accommodate lexical variation. In 
comparison with other metrics on 16,800 
sentences of Chinese-English newswire 
text, our method reaches high correlation 
with human scores.  
1 Introduction 
Since the creation of BLEU (Papineni et al, 2002) 
and NIST (Doddington, 2002), the subject of 
automatic evaluation metrics for MT has been 
given quite a lot of attention. Although widely 
popular thanks to their speed and efficiency, both 
BLEU and NIST have been criticized for 
inadequate accuracy of evaluation at the segment 
level (Callison-Burch et al, 2006). As string 
based-metrics, they are limited to superficial 
comparison of word sequences between a 
translated sentence and one or more reference 
sentences, and are unable to accommodate any 
legitimate grammatical variation when it comes to 
lexical choices or syntactic structure of the 
translation, beyond what can be found in the 
multiple references. A natural next step in the field 
of evaluation was to introduce metrics that would 
better reflect our human judgement by accepting 
synonyms in the translated sentence or evaluating 
the translation on the basis of what syntactic 
features it shares with the reference. 
Our method follows and substantially extends 
the earlier work of Liu and Gildea (2005), who use 
syntactic features and unlabelled dependencies to 
evaluate MT quality, outperforming BLEU on 
segment-level correlation with human judgement. 
Dependencies abstract away from the particulars of 
the surface string (and syntactic tree) realization 
and provide a ?normalized? representation of 
(some) syntactic variants of a given sentence.  
While Liu and Gildea (2005) calculate n-gram 
matches on non-labelled head-modifier sequences 
derived by head-extraction rules from syntactic 
trees, we automatically evaluate the quality of 
translation by calculating an f-score on labelled 
dependency structures produced by a Lexical-
Functional Grammar (LFG) parser. These 
dependencies differ from those used by Liu and 
Gildea (2005), in that they are extracted according 
to the rules of the LFG grammar and they are 
labelled with a type of grammatical relation that 
connects the head and the modifier, such as 
subject, determiner, etc. The presence of 
grammatical relation labels adds another layer of 
important linguistic information into the 
comparison and allows us to account for partial 
matches, for example when a lexical item finds 
itself in a correct relation but with an incorrect 
partner. Moreover, we use a number of best parses 
for the translation and the reference, which serves 
to decrease the amount of noise that can be 
introduced by the process of parsing and extracting 
dependency information. 
The translation and reference files are 
analyzed by a treebank-based, probabilistic LFG 
parser (Cahill et al, 2004), which produces a set of 
dependency triples for each input. The translation 
set is compared to the reference set, and the 
number of matches is calculated, giving the 
104
precision, recall, and f-score for each particular 
translation.   
In addition, to allow for the possibility of valid 
lexical differences between the translation and the 
references, we follow Kauchak and Barzilay 
(2006) in adding a number of synonyms in the 
process of evaluation to raise the number of 
matches between the translation and the reference, 
leading to a higher score. 
In an experiment on 16,800 sentences of 
Chinese-English newswire text with segment-level 
human evaluation from the Linguistic Data 
Consortium?s (LDC) Multiple Translation project, 
we compare the LFG-based evaluation method 
with other popular metrics like BLEU, NIST, 
General Text Matcher (GTM) (Turian et al, 2003), 
Translation Error Rate (TER) (Snover et al, 
2006)1, and METEOR (Banerjee and Lavie, 2005), 
and we show that combining dependency 
representations with synonyms leads to a more 
accurate evaluation that correlates better with 
human judgment. Although evaluated on a 
different test set, our method also outperforms the 
correlation with human scores reported in Liu and 
Gildea (2005). 
The remainder of this paper is organized as 
follows: Section 2 gives a basic introduction to 
LFG; Section 3 describes related work; Section 4 
describes our method and gives results of the 
experiment on the Multiple Translation data; 
Section 5 discusses ongoing work; Section 6 
concludes. 
2 Lexical-Functional Grammar 
In Lexical-Functional Grammar (Kaplan and 
Bresnan, 1982; Bresnan, 2001) sentence structure 
is represented in terms of c(onstituent)-structure 
and f(unctional)-structure. C-structure represents 
the word order of the surface string and the 
hierarchical organisation of phrases in terms of 
CFG trees. F-structures are recursive feature (or 
attribute-value) structures, representing abstract 
grammatical relations, such as subj(ect), obj(ect), 
obl(ique), adj(unct), etc., approximating to 
predicate-argument structure or simple logical 
forms. C-structure and f-structure are related in 
                                                 
1 We omit HTER (Human-Targeted Translation Error 
Rate), as it is not fully automatic and requires human 
input. 
terms of functional annotations (attribute-value 
structure equations) in c-structure trees, describing 
f-structures.  
While c-structure is sensitive to surface 
rearrangement of constituents, f-structure abstracts 
away from the particulars of the surface 
realization. The sentences John resigned yesterday 
and Yesterday, John resigned will receive different 
tree representations, but identical f-structures, 
shown in (1). 
 
(1) C-structure:                         F-structure: 
 
              S 
                  
      
 NP                      VP 
   |                     
John       
              V               NP-TMP 
               |                      | 
       resigned       yesterday 
                         
SUBJ        PRED   john 
                 NUM    sg 
                 PERS   3 
PRED       resign 
TENSE     past 
ADJ      {[PRED   yesterday]} 
 
 
                     S 
                  
      
    NP       NP       VP 
      |                 |            | 
Yesterday  John        V              
                                    | 
                            resigned                             
SUBJ        PRED   john 
                 NUM    sg 
                 PERS   3 
PRED       resign 
TENSE     past 
ADJ      {[PRED   yesterday]} 
 
 
 
Note that if these sentences were a translation-
reference pair, they would receive a less-than-
perfect score from string-based metrics. For 
example, BLEU with add-one smoothing2 gives 
this pair a score of barely 0.3781. This is because, 
although all three unigrams from the ?translation? 
(John; resigned; yesterday) are present in the 
reference, which contains four items including the 
comma (Yesterday; ,; John; resigned), the 
?translation? contains only one bigram (John 
resigned) that matches the ?reference? (Yesterday 
,; , John; John resigned), and no matching 
trigrams. 
The f-structure can also be described in terms 
of a flat set of triples. In triples format, the f-
structure in (1) is represented as follows: 
{subj(resign, john), pers(john, 3), num(john, sg), 
tense(resign, past), adj(resign, yesterday), 
pers(yesterday, 3), num(yesterday, sg)}. 
                                                 
2 We use smoothing because the original BLEU metric 
gives zero points to sentences with fewer than one four-
gram. 
105
Cahill et al (2004) presents a set of Penn-II 
Treebank-based LFG parsing resources. Their 
approach distinguishes 32 types of dependencies, 
including grammatical functions and 
morphological information. This set can be divided 
into two major groups: a group of predicate-only 
dependencies and non-predicate dependencies. 
Predicate-only dependencies are those whose path 
ends in a predicate-value pair, describing 
grammatical relations. For example, for the f-
structure in (1), predicate-only dependencies would 
include: {subj(resign, john), adj(resign, 
yesterday)}.  
Other predicate-only dependencies include: 
apposition, complement, open complement, 
coordination, determiner, object, second object, 
oblique, second oblique, oblique agent, possessive, 
quantifier, relative clause, topic, and relative 
clause pronoun. The remaining non-predicate 
dependencies are: adjectival degree, coordination 
surface form, focus, complementizer forms: if, 
whether, and that, modal, number, verbal particle, 
participle, passive, person, pronoun surface form, 
tense, and infinitival clause. 
In parser evaluation, the quality of the f-
structures produced automatically can be checked 
against a set of gold standard sentences annotated 
with f-structures by a linguist. The evaluation is 
conducted by calculating the precision and recall 
between the set of dependencies produced by the 
parser, and the set of dependencies derived from 
the human-created f-structure. Usually, two 
versions of f-score are calculated: one for all the 
dependencies for a given input, and a separate one 
for the subset of predicate-only dependencies. 
In this paper, we use the parser developed by 
Cahill et al (2004), which automatically annotates 
input text with c-structure trees and f-structure 
dependencies, obtaining high precision and recall 
rates. 3  
3 Related work 
3.1 String-based metrics 
The insensitivity of BLEU and NIST to perfectly 
legitimate syntactic and lexical variation has been 
raised, among others, in Callison-Burch et al 
(2006), but the criticism is widespread. Even the 
                                                 
3 A demo of the parser can be found at http://lfg-
demo.computing.dcu.ie/lfgparser.html 
creators of BLEU point out that it may not 
correlate particularly well with human judgment at 
the sentence level (Papineni et al, 2002).  
Recently a number of attempts to remedy these 
shortcomings have led to the development of other 
automatic MT evaluation metrics. Some of them 
concentrate mainly on word order, like General 
Text Matcher (Turian et al, 2003), which 
calculates precision and recall for translation-
reference pairs, weighting contiguous matches 
more than non-sequential matches, or Translation 
Error Rate (Snover et al, 2006), which computes 
the number of substitutions, insertions, deletions, 
and shifts necessary to transform the translation 
text to match the reference. Others try to 
accommodate both syntactic and lexical 
differences between the candidate translation and 
the reference, like CDER (Leusch et al, 2006), 
which employs a version of edit distance for word 
substitution and reordering; or METEOR 
(Banerjee and Lavie, 2005), which uses stemming 
and WordNet synonymy. Kauchak and Barzilay 
(2006) and Owczarzak et al (2006) use 
paraphrases during BLEU and NIST evaluation to 
increase the number of matches between the 
translation and the reference; the paraphrases are 
either taken from WordNet4 in Kauchak and 
Barzilay (2006) or derived from the test set itself 
through automatic word and phrase alignment in 
Owczarzak et al (2006). Another metric making 
use of synonyms is the linear regression model 
developed by Russo-Lassner et al (2005), which 
makes use of stemming, WordNet synonymy, verb 
class synonymy, matching noun phrase heads, and 
proper name matching. Kulesza and Shieber 
(2004), on the other hand, train a Support Vector 
Machine using features such as proportion of n-
gram matches and word error rate to judge a given 
translation?s distance from human-level quality.  
3.2 Dependency-based metric 
The metrics described above use only string-based 
comparisons, even while taking into consideration 
reordering. By contrast, Liu and Gildea (2005) 
present three metrics that use syntactic and 
unlabelled dependency information. Two of these 
metrics are based on matching syntactic subtrees 
between the translation and the reference, and one 
                                                 
4 http://wordnet.princeton.edu/ 
106
is based on matching headword chains, i.e. 
sequences of words that correspond to a path in the 
unlabelled dependency tree of the sentence. 
Dependency trees are created by extracting a 
headword for each node of the syntactic tree, 
according to the rules used by the parser of Collins 
(1999), where every subtree represents the 
modifier information for its root headword. The 
dependency trees for the translation and the 
reference are converted into flat headword chains, 
and the number of overlapping n-grams between 
the translation and the reference chains is 
calculated. Our method, extending this line of 
research with the use of labelled LFG 
dependencies, partial matching, and n-best parses, 
allows us to considerably outperform Liu and 
Gildea?s (2005) highest correlations with human 
judgement (they report 0.144 for the correlation 
with human fluency judgement, 0.202 for the 
correlation with human overall judgement), 
although it has to be kept in mind that such 
comparison is only tentative, as their correlation is 
calculated on a different test set. 
4 LFG f-structure in MT evaluation 
LFG-based automatic MT evaluation reflects the 
same process that underlies the evaluation of 
parser-produced f-structure quality against a gold 
standard: we parse the translation and the 
reference, and then, for each sentence, we check 
the set of labelled translation dependencies against 
the set of labelled reference dependencies, 
counting the number of matches. As a result, we 
obtain the precision and recall scores for the 
translation, and we calculate the f-score for the 
given pair.  
4.1 Determining parser noise 
Because we are comparing two outputs that were 
produced automatically, there is a possibility that 
the result will not be noise-free, even if the parser 
fails to provide a parse only in 0.1% of cases. 
To assess the amount of noise that the parser 
introduces, Owczarzak et al (2006) conducted an 
experiment where 100 English sentences were 
hand-modified so that the position of adjuncts was 
changed, but the sentence remained grammatical 
and the meaning was not influenced. This way, an 
ideal parser should give both the source and the 
modified sentence the same f-structure, similarly to 
the example presented in (1). The modified 
sentences were treated like a translation file, and 
the original sentences played the part of the 
reference. Each set was run through the parser, and 
the dependency triples obtained from the 
?translation? were compared against the 
dependency triples for the ?reference?, calculating 
the f-score. Additionally, the same ?translation-
reference? set was scored with other metrics (TER, 
METEOR, BLEU, NIST, and GTM). The results, 
including the distinction between f-scores for all 
dependencies and predicate-only dependencies, 
appear in Table 1. 
 
 baseline modified 
TER 0.0 6.417 
METEOR   1.0 0.9970 
BLEU 1.0000 0.8725 
NIST 11.5232 11.1704 (96.94%) 
GTM 100 99.18 
dep f-score  100 96.56 
dep_preds f-score 100 94.13 
Table 1. Scores for sentences with reordered adjuncts 
 
The baseline column shows the upper bound for a 
given metric: the score which a perfect translation, 
word-for-word identical to the reference, would 
obtain.5 The other column lists the scores that the 
metrics gave to the ?translation? containing 
reordered adjunct. As can be seen, the dependency 
and predicate-only dependency scores are lower 
than the perfect 100, reflecting the noise 
introduced by the parser. 
 We propose that the problem of parser 
noise can be alleviated by introducing a number of 
best parses into the comparison between the 
translation and the reference. Table 2 shows how 
increasing the number of parses available for 
comparison brings our method closer to an ideal 
noise-free parser.  
 
                                                 
5 Two things have to be noted here: (1) in the case of 
NIST the perfect score differs from text to text, which is 
why the percentage points are provided along the 
numerical score, and (2) in the case of TER the lower 
the score, the better the translation, so the perfect 
translation will receive 0, and there is no upper bound 
on the score, which makes this particular metric 
extremely difficult to directly compare with others. 
107
 dependency f-score 
1 best 96.56 
2 best 97.31 
5 best 97.90 
10 best 98.31 
20 best 98.59 
30 best 98.74 
50 best 98.79 
baseline 100 
Table 2.  Dependency f-scores for sentences with reordered 
adjuncts with n-best parses available 
 
It has to be noted, however, that increasing the 
number of parses beyond a certain threshold does 
little to further improve results, and at the same 
time it considerably decreases the efficiency of the 
method, so it is important to find the right balance 
between these two factors. In our opinion, the 
optimal value would be 10-best parses. 
4.2 Correlation with human judgement ? 
MultiTrans 
4.2.1 Experimental design 
To evaluate the correlation with human 
assessment, we used the data from the Linguistic 
Data Consortium Multiple Translation Chinese 
(MTC) Parts 2 and 4, which consists of multiple 
translations of Chinese newswire text, four human-
produced references, and segment-level human 
scores for a subset of the translation-reference 
pairs. Although a single translated segment was 
always evaluated by more than one judge, the 
judges used a different reference every time, which 
is why we treated each translation-reference-
human score triple as a separate segment. In effect, 
the test set created from this data contained 16,800 
segments. As in the previous experiment, the 
translation was scored using BLEU, NIST, GTM, 
TER, METEOR, and our labelled dependency-
based method. 
4.2.2 Labelled dependency-based method 
We examined a number of modifications of the 
dependency-based method in order to find out 
which one gives the highest correlation with 
human scores. The correlation differences between 
immediate neighbours in the ranking were often 
too small to be statistically significant; however, 
there is a clear overall trend towards improvement.  
Besides the plain version of the dependency f-
score, we also looked at the f-score calculated on 
predicate dependencies only (ignoring ?atomic? 
features such as person, number, tense, etc.), which 
turned out not to correlate well with human 
judgements. 
Another addition was the use of 2-, 10-, or 50-
best parses of the translation and reference 
sentences, which partially neutralized parser noise 
and resulted in increased correlations.  
We also created a version where predicate 
dependencies of the type subj(resign,John) are split 
into two parts, each time replacing one of the 
elements participating in the relation with a 
variable, giving in effect subj(resign,x) and 
subj(y,John). This lets us score partial matches, 
where one correct lexical object happens to find 
itself in the correct relation, but with an incorrect 
?partner?.  
Lastly, we added WordNet synonyms into the 
matching process to accommodate lexical 
variation, and to compare our WordNet-enhanced 
method with the WordNet-enhanced version of 
METEOR.  
4.2.3 Results 
We calculated Pearson?s correlation coefficient for 
segment-level scores that were given by each 
metric and by human judges. The results of the 
correlation are shown in Table 3. Note that the 
correlation for TER is negative, because in TER 
zero is the perfect score, in contrast to other 
metrics where zero is the worst possible score; 
however, this time the absolute values can be 
easily compared to each other. Rows are ordered 
by the highest value of the (absolute) correlation 
with the human score. 
First, it seems like none of the metrics is very 
good at reflecting human fluency judgments; the 
correlation values in the first column are 
significantly lower than the correlation with 
accuracy. This finding has been previously 
reported, among others, in Liu and Gildea (2005). 
However, the dependency-based method in almost 
all its versions has decidedly the highest 
correlation in this area. This can be explained by 
the method?s sensitivity to the grammatical 
structure of the sentence: a more grammatical 
translation is also a translation that is more fluent. 
As to the correlation with human evaluation of 
translation accuracy, our method currently falls 
108
short of METEOR. This is caused by the fact that 
METEOR assign relatively little importance to the 
position of a specific word in a sentence, therefore 
rewarding the translation for content rather than 
linguistic form. Interestingly, while METEOR, 
with or without WordNet, considerably 
outperforms all other metrics when it comes to the 
correlation with human judgements of translation 
accuracy, it falls well behind most versions of our 
dependency-based method in correlation with 
human scores of translation fluency. 
Surprisingly, adding partial matching to the 
dependency-based method resulted in the greatest 
increase in correlation levels, to the extent that the 
partial-match versions consistently outperformed 
versions with a larger number of parses available 
but without the partial match. The most interesting 
effect was that the partial-match versions (even 
those with just a single parse) offered results 
comparable to or higher than the addition of 
WordNet to the matching process when it comes to 
accuracy and overall judgement. 
5 Current and future work 
Fluency and accuracy are two very different 
aspects of translation quality, each with its own set 
of conditions along which the input is evaluated. 
Therefore, it seems unfair to expect a single 
automatic metric to correlate highly with human 
judgements of both at the same time. This pattern 
is very noticeable in Table 3: if a metric is 
(relatively) good at correlating with fluency, its 
accuracy correlation suffers (GTM might serve as 
an example here), and the opposite holds as well 
(see METEOR?s scores). It does not mean that any 
improvement that increases the method?s 
correlation with one aspect will result in a decrease 
in the correlation with the other aspect; but it does 
suggest that a possible way of development would 
be to target these correlations separately, if we 
want our automated metrics to reflect human 
scores better. At the same time, string-based 
metrics might have already exhausted their 
potential when it comes to increasing their 
correlation with human evaluation; as has been 
pointed out before, these metrics can only tell us 
that two strings differ, but they cannot distinguish 
legitimate grammatical variance from 
ungrammatical variance. As the quality of MT  
 
 
Table 3. Pearson?s correlation between human scores and 
evaluation metrics. Legend: d = dependency f-score, _pr = 
predicate-only f-score, 2, 10, 50 = n-best parses; var = 
partial-match version; M = METEOR, WN = WordNet6 
 
improves, the community will need metrics that are 
more sensitive in this respect. After all, the true 
quality of MT depends on producing grammatical 
output which describes the same concept as the 
source utterance, and the string identity with a 
reference is only a very selective approximation of 
this goal.  
                                                 
6 In general terms, an increase of 0.022 or more between 
any two scores in the same column is significant with a 
95% confidence interval. The statistical significance of 
correlation differences was calculated using Fisher?s z? 
transformation and the general formula for confidence 
interval. 
 
fluency  accuracy  average  
d_50+WN 0.177 M+WN 0.294 M+WN 0.255 
d+WN 0.175 M   0.278 d_50_var 0.252 
d_50_var 0.174 d_50_var 0.273 d_50+WN 0.250 
GTM 0.172 NIST 0.273 d_10_var 0.250 
d_10_var 0.172 d_10_var 0.273 d_2_var 0.247 
d_50 0.171 d_2_var 0.270 d+WN 0.244 
d_2_var 0.168 d_50+WN 0.269 d_50 0.243 
d_10 0.168 d_var 0.266 d_var 0.243 
d_var 0.165 d_50 0.262 M   0.242 
d_2 0.164 d_10 0.262 d_10 0.242 
d   0.161 d+WN 0.260 NIST 0.238 
BLEU 0.155 d_2 0.257 d_2 0.237 
M+WN 0.153 d  0.256 d   0.235 
M   0.149 d_pr 0.240 d_pr 0.216 
NIST 0.146 GTM 0.203 GTM 0.208 
d_pr 0.143 BLEU 0.199 BLEU 0.197 
TER -0.133 TER -0.192 TER -0.182 
109
 In order to maximize the correlation with 
human scores of fluency, we plan to look more 
closely at the parser output, and implement some 
basic transformations which would allow an even 
deeper logical analysis of input (e.g. passive to 
active voice transformation). 
  Additionally, we want to take advantage of 
the fact that the score produced by the dependency-
based method is the proportional average of 
matches for a group of up to 32 (but usually far 
fewer) different dependency types. We plan to 
implement a set of weights, one for each 
dependency type, trained in such a way as to 
maximize the correlation of the final dependency f-
score with human evaluation. In a preliminary 
experiment, for example, assigning a low weight to 
the topic dependency increases our correlations 
slightly (this particular case can also be seen as a 
transformation into a more basic logical form by 
removing non-elementary dependency types). 
 In a similar direction, we want to 
experiment more with the f-score calculations. 
Initial check shows that assigning a higher weight 
to recall than to precision improves results. 
 To improve the correlation with accuracy 
judgements, we would like to experiment using a 
paraphrase set derived from a large parallel corpus, 
as described in Owczarzak et al (2006). While 
retaining the advantage of having a similar size to 
a corresponding set of WordNet synonyms, this set 
will also capture low-level syntactic variations, 
which can increase the number of matches.  
6 Conclusions 
In this paper we present a linguistically-
motivated method for automatically evaluating the 
output of Machine Translation. Most currently 
used popular metrics rely on comparing translation 
and reference on a string level. Even given 
reordering, stemming, and synonyms for individual 
words, current methods are still far from reaching 
human ability to assess the quality of translation, 
and there exists a need in the community to 
develop more dependable metrics. Our method 
explores one such direction of development, 
comparing the sentences on the level of their 
grammatical structure, as exemplified by their f-
structure labelled dependency triples produced by 
an LFG parser. In our experiments we showed that 
the dependency-based method correlates higher 
than any other metric with human evaluation of 
translation fluency, and shows high correlation 
with the average human score. The use of 
dependencies in MT evaluation has not been 
extensively researched before (one exception here 
would be Liu and Gildea (2005)), and requires 
more research to improve it, but the method shows 
potential to become an accurate evaluation metric.  
 
Acknowledgements 
This work was partly funded by Microsoft Ireland 
PhD studentship  2006-8  for the first author of the 
paper. We would also like to thank our reviewers 
and Dan Melamed for their insightful comments. 
All remaining errors are our own. 
 
References 
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with 
Improved Correlation with Human Judgments. 
Proceedings of the Workshop on Intrinsic and 
Extrinsic Evaluation Measures for MT and/or 
Summarization at the Association for Computational 
Linguistics Conference 2005: 65-73. Ann Arbor, 
Michigan. 
Joan Bresnan. 2001. Lexical-Functional Syntax, 
Blackwell, Oxford. 
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef 
van Genabith, and Andy Way. 2004. Long-Distance 
Dependency Resolution in Automatically Acquired 
Wide-Coverage PCFG-Based LFG Approximations, 
In Proceedings of Association for Computational 
Linguistics 2004: 320-327. Barcelona, Spain. 
Chris Callison-Burch, Miles Osborne and Philipp 
Koehn. 2006. Re-evaluating the role of BLEU in 
Machine Translation Research. Proceedings of the 
European Chapter of the Association for 
Computational Linguistics 2006: 249-256. Oslo, 
Norway. 
Michael J. Collins. 1999. Head-driven Statistical 
Models for Natural Language Parsing. Ph.D. thesis, 
University of Pennsylvania, Philadelphia. 
George Doddington. 2002. Automatic Evaluation of MT 
Quality using N-gram Co-occurrence Statistics. 
Proceedings of Human Language Technology 
Conference 2002: 138-145. San Diego, California. 
Kaplan, R. M., and J. Bresnan. 1982. Lexical-functional 
Grammar: A Formal System for Grammatical 
110
Representation.  In J. Bresnan (ed.), The Mental 
Representation of Grammatical Relations.  MIT 
Press, Cambridge. 
David Kauchak and Regina Barzilay. 2006. 
Paraphrasing for Automatic Evaluation. Proceedings 
of Human Language Technology ? North American 
Chapter of the Association for Computational 
Linguistics Conference 2006: 45-462. New York, 
New York. 
Philipp Koehn. 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation 
models. Proceedings of the Workshop on Machine 
Translation: From real users to research at the 
Association for Machine Translation in the Americas 
Conference 2004: 115-124. Washington, DC. 
Philipp Koehn. 2005. Europarl: A Parallel Corpus for 
Statistical Machine Translation. Proceedings of MT 
Summit 2005: 79-86. Phuket, Thailand. 
Alex Kulesza and Stuart M. Shieber. 2004. A learning 
approach to improving sentence-level MT evaluation. 
In Proceedings of the Conference on Theoretical and 
Methodological Issues in Machine Translation 2004: 
75-84. Baltimore, Maryland. 
Gregor Leusch, Nicola Ueffing and Hermann Ney. 
2006. CDER: Efficient MT Evaluation Using Block 
Movements. Proceedings of European Chapter of the 
Association for Computational Linguistics 
Conference 2006: 241-248. Trento, Italy. 
Ding Liu and Daniel Gildea. 2005. Syntactic Features 
for Evaluation of Machine Translation. In 
Proceedings of the Workshop on Intrinsic and 
Extrinsic Evaluation Measures for Machine 
Translation and/or Summarization at the Association 
for Computational Linguistics Conference 2005. Ann 
Arbor, Michigan. 
Franz Josef Och and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Modes. 
Computational Linguistics, 29:19-51. 
Karolina Owczarzak, Declan Groves, Josef van 
Genabith, and Andy Way. 2006. Contextual Bitext-
Derived Paraphrases in Automatic MT Evaluation. 
Proceedings of the Workshop on Statistical Machine 
Translation at the Human Language Technology ? 
North American Chapter of the Association for 
Computational Linguistics Conference 2006: 86-93. 
New York, New York. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
WeiJing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
Association for Computational Linguistics 
Conference 2002: 311-318. Philadelphia, 
Pennsylvania. 
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik. 
2005. A Paraphrase-based Approach to Machine 
Translation Evaluation. Technical Report LAMP-TR-
125/CS-TR-4754/UMIACS-TR-2005-57, University 
of Maryland, College Park, Maryland. 
Mathew Snover, Bonnie Dorr, Richard Schwartz, John 
Makhoul, Linnea Micciula. 2006. A Study of 
Translation Error Rate with Targeted Human 
Annotation. Proceedings of the Association for 
Machine Translation in the Americas Conference 
2006: 223-231. Boston, Massachusetts. 
Joseph P. Turian, Luke Shen, and I. Dan Melamed. 
2003. Evaluation of Machine Translation and Its 
Evaluation. Proceedings of MT Summit 2003: 386-
393. New Orleans, Luisiana. 
Ying Zhang and Stephan Vogel. 2004. Measuring 
confidence intervals for the machine translation 
evaluation metrics. Proceedings of Conference on 
Theoretical and Methodological Issues in Machine 
Translation 2004: 85-94. Baltimore, Maryland. 
111
Proceedings of the 10th Conference on Parsing Technologies, pages 33?35,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Adapting WSJ-Trained Parsers to the British National Corpus Using
In-Domain Self-Training
Jennifer Foster, Joachim Wagner, Djame? Seddah and Josef van Genabith
National Centre for Language Technology
School of Computing, Dublin City University, Dublin 9, Ireland
{jfoster, jwagner, josef}@computing.dcu.ie, dseddah@paris4.sorbonne.fr?
Abstract
We introduce a set of 1,000 gold standard
parse trees for the British National Corpus
(BNC) and perform a series of self-training
experiments with Charniak and Johnson?s
reranking parser and BNC sentences. We
show that retraining this parser with a com-
bination of one million BNC parse trees
(produced by the same parser) and the orig-
inal WSJ training data yields improvements
of 0.4% on WSJ Section 23 and 1.7% on the
new BNC gold standard set.
1 Introduction
Given the success of statistical parsing models on
the Wall Street Journal (WSJ) section of the Penn
Treebank (PTB) (Charniak, 2000; Collins, 2003, for
example), there has been a change in focus in recent
years towards the problem of replicating this success
on genres other than American financial news sto-
ries. The main challenge in solving the parser adap-
tation problem are the resources required to con-
struct reliable annotated training examples.
A breakthrough has come in the form of research
by McClosky et al (2006a; 2006b) who show that
self-training can be used to improve parser perfor-
mance when combined with a two-stage reranking
parser model (Charniak and Johnson, 2005). Self-
training is the process of training a parser on its own
output, and earlier self-training experiments using
generative statistical parsers did not yield encour-
aging results (Steedman et al, 2003). McClosky et
al. (2006a; 2006b) proceed as follows: sentences
?Now affiliated to Lalic, Universite? Paris 4 La Sorbonne.
from the LA Times newspaper are parsed by a first-
stage generative statistical parser trained on some
seed training data (WSJ Sections 2-21) and the n-
best parse trees produced by this parser are reranked
by a discriminative reranker. The highest ranked
parse trees are added to the training set of the parser
and the parser is retrained. This self-training method
gives improved performance, not only on Section
23 of the WSJ (an absolute f-score improvement of
0.8%), but also on test sentences from the Brown
corpus (Francis and Kuc?era, 1979) (an absolute f-
score improvement of 2.6%).
In the experiments of McClosky et al (2006a;
2006b), the parse trees used for self-training come
from the same domain (American newspaper text)
as the parser?s original seed training material. Bac-
chiani et al (2006) find that self-training is ef-
fective when the parse trees used for self-training
(WSJ parse trees) come from a different domain to
the seed training data and from the same domain as
the test data (WSJ sentences). They report a per-
formance boost of 4.2% on WSJ Section 23 for a
generative statistical parser trained on Brown seed
data when it is self-trained using 200,000 WSJ parse
trees. However, McCloskey et al (2006b) report a
drop in performance for their reranking parser when
the experiment is repeated in the opposite direction,
i.e. with Brown data for self-training and testing,
and WSJ data for seed training. In contrast, we re-
port successful in-domain1 self-training experiments
with the BNC data as self-training and test material,
and with the WSJ-trained reranking parser used by
McCloskey et al (2006a; 2006b).
We parse the BNC (Burnard, 2000) in its entirety
1We refer to data as being in-domain if it comes from the
same domain as the test data and out-of-domain if it does not.
33
using the reranking parser of Charniak and Johnson
(2005). 1,000 BNC sentences are manually anno-
tated for constituent structure, resulting in the first
gold standard set for this corpus. The gold standard
set is split into a development set of 500 parse trees
and a test set of 500 parse trees and used in a series
of self-training experiments: Charniak and John-
son?s parser is retrained on combinations of WSJ
treebank data and its own parses of BNC sentences.
These combinations are tested on the BNC devel-
opment set and Section 00 of the WSJ. An optimal
combination is chosen which achieves a Parseval la-
belled bracketing f-score of 91.7% on Section 23
and 85.6% on the BNC gold standard test set. For
Section 23 this is an absolute improvement of 0.4%
on the baseline results of this parser, and for the
BNC data this is a statistically significant improve-
ment of 1.7%.
2 The BNC Data
The BNC is a 100-million-word balanced part-of-
speech-tagged corpus of written and transcribed
spoken English. Written text comprises 90% of the
BNC: 75% non-fictional and 25% fictional. To fa-
cilitate parsing with a WSJ-trained parser, some re-
versible transformations were applied to the BNC
data, e.g. British English spellings were converted
to American English and neutral quotes disam-
biguated. The reranking parser of Charniak and
Johnson (2005) was used to parse the BNC. 99.8%
of the 6 million BNC sentences obtained a parse,
with an average parsing speed of 1.4s per sentence.
A gold standard set of 1,000 BNC sentences was
constructed by one annotator by correcting the out-
put of the first stage of Charniak and Johnson?s
reranking parser. The sentences included in the gold
standard were chosen at random from the BNC, sub-
ject to the condition that they contain a verb which
does not occur in the training sections of the WSJ
section of the PTB (Marcus et al, 1993). A deci-
sion was made to select sentences for the gold stan-
dard set which differ from the sentences in the WSJ
training sections, and one way of finding different
sentences is to focus on verbs which are not attested
in the WSJ Sections 2-21. It is expected that these
gold standard parse trees can be used as training
data although they are used only as test and develop-
ment data in this work. Because they contain verbs
which do not occur in the parser?s training set, they
are likely to represent a hard test for WSJ-trained
parsers. The PTB bracketing guidelines (Bies et al,
1995) and the PTB itself were used as references by
the BNC annotator. Functional tags and traces were
not annotated. The annotator noticed that the PTB
parse trees sometimes violate the PTB bracketing
guidelines, and in these cases, the annotator chose
the analysis set out in the guidelines. It took approx-
imately 60 hours to build the gold standard set.
3 Self-Training Experiments
Charniak and Johnson?s reranking parser (June 2006
version) is evaluated against the BNC gold stan-
dard development set. Labelled precision (LP), re-
call (LR) and f-score measures2 for this parser are
shown in the first row of Table 1. The f-score of
83.7% is lower than the f-score of 85.2% reported
by McClosky et al (2006b) for the same parser on
Brown corpus data. This difference is reasonable
since there is greater domain variation between the
WSJ and the BNC than between the WSJ and the
Brown corpus, and all BNC gold standard sentences
contain verbs not attested in WSJ Sections 2-21.
We retrain the first-stage generative statistical
parser of Charniak and Johnson using combinations
of BNC trees (parsed using the reranking parser)
and WSJ treebank trees. We test the combinations
on the BNC gold standard development set and on
WSJ Section 00. Table 1 shows that parser accu-
racy increases with the size of the in-domain self-
training material.3 The figures confirm the claim of
McClosky et al (2006a) that self-training with a
reranking parsing model is effective for improving
parser accuracy in general, and the claim of Gildea
(2001) that training on in-domain data is effective
for parser adaption. They confirm that self-training
on in-domain data is effective for parser adaptation.
The WSJ Section 00 results suggest that, in order
to maintain performance on the seed training do-
main, it is necessary to combine BNC parse trees
2All scores are for the second stage of the parsing process,
i.e. the evaluation takes place after the reranking. All evalua-
tion is carried out using the Parseval labelled bracketing metrics,
with evalb and parameter file new.prm.
3The notation bnc500K+5wsj refers to a set of 500,000
parser output parse trees of sentences taken randomly from the
BNC concatenated with five copies of WSJ Sections 2-21.
34
BNC Development WSJ Section 00
Self-Training LP LR LF LP LR LF
- 83.6 83.7 83.7 91.6 90.5 91.0
bnc50k 83.7 83.7 83.7 90.0 88.0 89.0
bnc50k+1wsj 84.4 84.4 84.4 91.6 90.3 91.0
bnc250k 84.7 84.5 84.6 91.1 89.3 90.2
bnc250k+5wsj 85.0 84.9 85.0 91.8 90.5 91.2
bnc500k+5wsj 85.2 85.1 85.2 91.9 90.4 91.2
bnc500k+10wsj 85.1 85.1 85.1 91.9 90.6 91.2
bnc1000k+5wsj 86.5 86.2 86.3 91.7 90.3 91.0
bnc1000k+10wsj 86.1 85.9 86.0 92.0 90.5 91.3
bnc1000k+40wsj 85.5 85.5 85.5 91.9 90.6 91.3
BNC Test WSJ Section 23
- 84.0 83.7 83.9 91.8 90.9 91.3
bnc1000k+10wsj 85.7 85.4 85.6 92.3 91.1 91.7
Table 1: In-domain Self-Training Results
with the original seed training material during the
self-training phase.
Of the self-training combinations with above-
baseline improvements for both development sets,
the combination of 1,000K BNC parse trees and
Section 2-21 of the WSJ (multiplied by ten) yields
the highest improvement for the BNC data, and we
present final results with this combination for the
BNC gold standard test set and WSJ Section 23.
There is an absolute improvement on the original
reranking parser of 1.7% on the BNC gold standard
test set and 0.4% on WSJ Section 23. The improve-
ment on BNC data is statistically significant for both
precision and recall (p < 0.0002, p < 0.0002). The
improvement on WSJ Section 23 is statistically sig-
nificant for precision only (p < 0.003).
4 Conclusion and Future Work
We have introduced a set of 1,000 gold standard
parse trees for the BNC. We have performed self-
training experiments with Charniak and Johnson?s
reranking parser and sentences from the BNC. We
have shown that retraining this parser with a com-
bination of one million BNC parse trees (produced
by the same parser) and the original WSJ train-
ing data yields improvements of 0.4% on WSJ Sec-
tion 23 and 1.7% on the BNC gold standard sen-
tences. These results indicate that self-training on
in-domain data can be used for parser adaptation.
Our BNC gold standard set consists of sentences
containing verbs which are not in the WSJ train-
ing sections. We suspect that this makes the gold
standard set a hard test for WSJ-trained parsers, and
our results are likely to represent a lower bound for
WSJ-trained parsers on BNC data. When used as
training data, we predict that the novel verbs in the
BNC gold standard set add to the variety of train-
ing material, and will further help parser adaptation
from the WSJ domain ? a matter for further research.
Acknowledgments We thank the IRCSET Em-
bark Initiative (basic research grant SC/02/298
and postdoctoral fellowship P/04/232), Science
Foundation Ireland (Principal Investigator grant
04/IN.3/I527) and the Irish Centre for High End
Computing for supporting this research.
References
Michiel Bacchiani, Michael Riley, Brian Roark, and Richard
Sproat. 2006. Map adaptation of stochastic grammars.
Computer Speech and Language, 20(1):41?68.
Ann Bies, Mark Ferguson, Karen Katz, and Robert MacIntyre.
1995. Bracketing guidelines for treebank II style, Penn Tree-
bank project. Technical Report MS-CIS-95-06, University
of Pennsylvania.
Lou Burnard. 2000. User reference guide for the British Na-
tional Corpus. Technical report, Oxford University.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best-parsing and maxent discriminative reranking. In Pro-
ceedings of ACL-05, pages 173?180, Barcelona.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of NAACL-00, pages 132?139, Seattle.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguistics,
29(4):499?637.
W. Nelson Francis and Henry Kuc?era. 1979. Brown Corpus
Manual. Technical report, Brown University, Providence.
Daniel Gildea. 2001. Corpus variation and parser performance.
In Proceedings of EMNLP-01, pages 167?202, Barcelona.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Linguistics,
19(2):313?330.
David McClosky, Eugene Charniak, and Mark Johnson. 2006a.
Effective self-training for parsing. In Proceedings of HLT-
NAACL-06, pages 152?159, New York.
David McClosky, Eugene Charniak, and Mark Johnson. 2006b.
Reranking and self-training for parser adaptation. In Pro-
ceedings of COLING-ACL-06, pages 337?344, Sydney.
Mark Steedman, Miles Osbourne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Boot-strapping
statistical parsers from small datasets. In Proceedings of
EACL-03, Budapest.
35
Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 45?52,
Athens, Greece, 31 March, 2009. c?2009 Association for Computational Linguistics
Automatic Treebank-Based Acquisition of Arabic LFG Dependency
Structures
Lamia Tounsi Mohammed Attia
NCLT, School of Computing, Dublin City University, Ireland
{lamia.tounsi, mattia, josef}@computing.dcu.ie
Josef van Genabith
Abstract
A number of papers have reported on meth-
ods for the automatic acquisition of large-scale,
probabilistic LFG-based grammatical resources
from treebanks for English (Cahill and al., 2002),
(Cahill and al., 2004), German (Cahill and al.,
2003), Chinese (Burke, 2004), (Guo and al.,
2007), Spanish (O?Donovan, 2004), (Chrupala
and van Genabith, 2006) and French (Schluter
and van Genabith, 2008). Here, we extend the
LFG grammar acquisition approach to Arabic and
the Penn Arabic Treebank (ATB) (Maamouri and
Bies, 2004), adapting and extending the methodol-
ogy of (Cahill and al., 2004) originally developed
for English. Arabic is challenging because of its
morphological richness and syntactic complexity.
Currently 98% of ATB trees (without FRAG and
X) produce a covering and connected f-structure.
We conduct a qualitative evaluation of our annota-
tion against a gold standard and achieve an f-score
of 95%.
1 Introduction
Treebank-based statistical parsers tend to achieve
greater coverage and robustness compared to ap-
proaches using handcrafted grammars. However,
they are criticised for being too shallow to mark
important syntactic and semantic dependencies
needed for meaning-sensitive applications (Ka-
plan, 2004). To treat this deficiency, a number
of researchers have concentrated on enriching
shallow parsers with deep dependency informa-
tion. (Cahill and al., 2002), (Cahill and al., 2004)
outlined an approach which exploits information
encoded in the Penn-II Treebank (PTB) trees to
automatically annotate each node in each tree
with LFG f-structure equations representing deep
predicate-argument structure relations. From this
LFG annotated treebank, large-scale unification
grammar resources were automatically extracted
and used in parsing (Cahill and al., 2008) and
generation (Cahill and van Genabith, 2006).
This approach was subsequently extended to
other languages including German (Cahill and
al., 2003), Chinese (Burke, 2004), (Guo and al.,
2007), Spanish (O?Donovan, 2004), (Chrupala
and van Genabith, 2006) and French (Schluter
and van Genabith, 2008).
Arabic is a semitic language and is well-known
for its morphological richness and syntactic
complexity. In this paper we describe the porting
of the LFG annotation methodology to Arabic in
order to induce LFG f-structures from the Penn
Arabic Treebank (ATB) (Bies, 2003), (Maamouri
and Bies, 2004). We evaluate both the coverage
and quality of the automatic f-structure annotation
of the ATB. Ultimately, our goal is to use the f-
structure annotated ATB to derive wide-coverage
resources for parsing and generating unrestricted
Arabic text. In this paper we concentrate on the
annotation algorithm.
The paper first provides a brief overview of
Lexical Functional Grammar, and the Penn
Arabic Treebank (ATB). The next section presents
the architecture of the f-structure annotation
algorithm for the acquisition of f-structures from
the Arabic treebank. The last section provides
an evaluation of the quality and coverage of the
annotation algorithm.
1.1 Lexical Functional Grammar
Lexical-Functional Grammar (LFG) (Kaplan and
Bresnan, 1982); (Bresnan, 2001), (Falk, 2001)
2001, (Sells, 1985) is a constraint-based theory
of grammar. LFG rejects concepts of configura-
tionality and movement familiar from generative
grammar, and provides a non-derivational alterna-
tive of parallel structures of which phrase structure
trees are only one component.
LFG involves two basic, parallel forms of
45
knowledge representation: c(onstituent)-structure,
which is represented by (f-structure annotated)
phrase structure trees; and f(unctional)-structure,
represented by a matrix of attribute-value pairs.
While c-structure accounts for language-specific
lexical idiosyncrasies, syntactic surface config-
urations and word order variations, f-structure
provides a more abstract level of representation
(grammatical functions/ labeled dependencies),
abstracting from some cross-linguistic syntactic
differences. Languages may differ typologically
as regards surface structural representations, but
may still encode similar syntactic functions (such
as, subject, object, adjunct, etc.). For a recent
overview on LFG-based analyses of Arabic see
(Attia, 2008) who presents a hand-crafted Arabic
LFG parser using the XLE (Xerox Linguistics En-
vironment).
1.2 The Penn Arabic Treebank (ATB)
The Penn Arabic Treebank project started in
2001 with the aim of describing written Modern
Standard Arabic newswire. The Treebank consists
of 23611 sentences (Bies, 2003), (Maamouri and
Bies, 2004) .
Arabic is a subject pro-drop language: a null
category (pro) is allowed in the subject position
of a finite clause if the agreement features on
the verb are rich enough to enable content to be
recovered (Baptista, 1995), (Chomsky, 1981).
This is represented in the ATB annotation by an
empty node after the verb marked with a -SBJ
functional tag. The ATB annotation, following
the Penn-II Treebank, utilises the concept of
empty nodes and traces to mark long distance
dependencies, as in relative clauses and questions.
The default word order in Arabic is VSO. When
the subject precedes the verb (SVO), the con-
struction is considered as topicalized. Modern
Standard Arabic also allows VOS word order
under certain conditions, e.g. when the object is
a pronoun. The ATB annotation scheme involves
24 basic POS-tags (497 different tags with mor-
phological information ), 22 phrasal tags, and 20
individual functional tags (52 different combined
tags).
The relatively free word order of Arabic means
that phrase structural position is not an indicator
of grammatical function, a feature of English
which was heavily exploited in the automatic LFG
annotation of the Penn-II Treebank (Cahill and
al., 2002). Instead, in the ATB functional tags are
used to mark the subject as well as the object.
The syntactic annotation style of the ATB follows,
as much as possible, the methodologies and
bracketing guidelines already used for the English
Penn-II Treebank. For example, in the Penn
English Treebank (PTB) (Marcus, 1994), small
clauses are considered sentences composed of
a subject and a predicate, without traces for an
omitted verb or any sort of control relationship, as
in example (1) for the sentence ?I consider Kris a
fool?.
(1) (S (NP-SBJ I)
(VP consider
(S (NP-SBJ Kris)
(NP-PRD a fool))))
The team working on the ATB found this
approach very convenient for copula construc-
tions in Arabic, which are mainly verbless
(Maamouri and Bies, 2004). Therefore they used
a similar analysis without assuming a deleted
copula verb or control relationship, as in (2).
(2) (S (NP-SBJ Al-mas>alatu

??

A??
?
@)
(ADJ-PRD basiyTatuN

??J


?fl
.
))

??J


?fl
.

??

A??
?
@
Al-mas>alatu basiyTatuN
the-question simple
?The question is simple.?
2 Architecture of the Arabic Automatic
Annotation Algorithm
The annotation algorithm for Arabic is based on
and substantially revises the methodology used for
English.
For English, f-structure annotation is very much
driven by configurational information: e.g. the
leftmost NP sister of a VP is likely to be a direct
object and hence annotated ? OBJ =?. This infor-
mation is captured in the format of left-right anno-
tation matrices, which specify annotations for left
or right sisters relative to a local head.
By contrast, Arabic is a lot less configurational and
has much richer morphology. In addition, com-
pared to the Penn-II treebank, the ATB features a
larger functional tag set. This is reflected in the de-
sign of the Arabic f-structure annotation algorithm
46
(Figure 1), where left-right annotation matrices
play a much smaller role than for English. The
annotation algorithm recursively traverses trees in
the ATB. It exploits ATB morpho-syntactic fea-
tures, ATB functional tags, and (some) configura-
tional information in the local subtrees.
We first mask (conflate) some of the complex
morphological information available in the pre-
terminal nodes to be able to state generalisations
for some of the annotation components. We then
head-lexicalise ATB trees identifying local heads.
Lexical macros exploit the full morphological an-
notations available in the ATB and map them to
corresponding f-structure equations. We then ex-
ploit ATB functional tags mapping them to SUBJ,
OBJ, OBL, OBJ2, TOPIC and ADJUNCT etc.
grammatical functions. The remaining functions
(COMP, XCOMP, SPEC etc.) as well as some
cases of SUBJ, OBJ, OBL, OBJ2, TOPIC and AD-
JUNCT, which could not be identified by ATB
tags, are treated in terms of left-right context anno-
tation matrices. Coordination is treated in a sepa-
rate component to keep the other components sim-
ple. Catch-all & Clean-Up corrects overgenerali-
sations in the previous modules and uses defaults
for remaining unannotated nodes. Finally, non-
local dependencies are handled by a Traces com-
ponent.
The next sub-sections describe the main modules
of the annotation algorithm.
2.1 Conflation
ATB preterminals are very fine-grained, encod-
ing extensive morpho-syntactic details in addi-
tion to POS information. For example, the word
	
?

?
	
J? sanaqifu ?[we will] stand? is tagged as
(FUT+IV1P+IV+IVSUFF MOOD:I) denoting an
imperfective (I) verb (V) in the future tense (FUT),
and is first person (1) plural (P) with indicative
mood (IVSUFF MOOD:I). In total there are over
460 preterminal types in the treebank. This level
of fine-grainedness is an important issue for the
annotation as we cannot state grammatical func-
tion (dependency) generalizations about heads and
left and right contexts for such a large tag set. To
deal with this problem, for some of the annotation
algorithm components we masked the morpho-
syntactic details in preterminals, thereby conflat-
ing them into more generic POS tags. For exam-
ple, the above-mentioned tag will be conflated as
VERB.
Figure 1: Architecture of the Arabic annotation al-
gorithm
2.2 Lexical Macros
Lexical macros, by contrast, utilise the de-
tailed morpho-syntactic information encoded in
the preterminal nodes of the Penn Arabic Tree-
bank trees and provide the required functional an-
notations accordingly. These tags usually include
information related to person, number, gender,
definiteness, case, tense, aspect, mood, etc.
Table 1 lists common tags for nouns and verbs and
shows the LFG functional annotation assigned to
each tag.
2.3 Functional Tags
In addition to monadic POS categories, the ATB
treebank contains a set of labels (called functional
tags or functional labels) associated with func-
tional information, such as -SBJ for ?subject? and
-OBJ for ?object?. The functional tags module
translates these functional labels into LFG func-
tional equations, e.g. -OBJ is assigned the anno-
tation ?OBJ=?. An f-structure equation look-up
table assigns default f-structure equations to each
functional label in the ATB (Table 2).
A particular treatment is applied for the tag -PRD
(predicate). This functional tag is used with cop-
ula complements, as in (3) and the correspond-
ing c-structure in Figure 2. Copula complements
47
Tag Annotation
Nouns
MASC ? GEND = masc (masculine)
FEM ? GEND = fem (feminine)
SG ? NUM = sg (singular)
DU ? NUM = dual
PL ? NUM = pl (plural)
ACC ? CASE = acc (accusative)
NOM ? CASE = nom (nominative)
GEN ? CASE = gen (genitive)
Verbs
1 ? PERS = 1
2 ? PERS = 2
3 ? PERS = 3
S ? NUM = sg
D ? NUM = dual
P ? NUM = pl
F ? GEND = masc
M ? GEND = fem
Table 1: Morpho-syntactic tags and their functional anno-
tations
Functional Label Annotation
-SBJ (subject) ? SUBJ = ?
-OBJ (object) ? OBJ = ?
-DTV (dative), ? OBJ2 =?
-BNF (Benefactive)
-TPC (topicalized) ? TOPIC=?
-CLR (clearly related) ? OBL =?
-LOC (locative),
-MNR (manner),
-DIR (direction), ??? ADJUNCT
-TMP (temporal),
-ADV (adverbial)
-PRP (purpose),
Table 2: Functional tags used in the ATP Treebank and their
default annotations
correspond to the open complement grammatical
function XCOMP in LFG and the ATB tag -PRD
is associated with the annotation in (4) in order to
produce the f-structure in Figure 3. The resulting
analysis includes a main predicator ?null be? and
specifies the control relationship through a func-
tional equation stating that the main subject is co-
indexed with the subject of the XCOMP.
(3)

?

K


P?Q?
	
?

?
	
KY?? @
Al-hudonapu Daruwriy?apN
the-truce necessary
?The truce is necessary.?
(4) ? PRED = ?null be?
? XCOMP = ?
? SUBJ= ? SUBJ
S
NP-SBJ
N
Alhudonapu
NP-PRD
N
DaruwriyapN
Figure 2: C-structure for example (3)
2
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
PRED ?null be
D
SUBJ , XCOMP
E
?
SUBJ
2
6
6
6
6
6
4
PRED ?Al-hudonapu?
NUM sg
GEND fem
DEF +
CASE nom
3
7
7
7
7
7
5
1
XCOMP
2
6
6
6
6
6
6
6
6
4
PRED ?Daruwriy?apN?
NUM sg
GEND fem
DEF -
CASE nom
SUBJ
h i
1
3
7
7
7
7
7
7
7
7
5
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
Figure 3: F-structure for example (3)
2.4 Left-Right Context Rules
The left-right context annotation module is based
on a tripartite division of local subtrees into a left-
hand-side context (LHS) followed by a head (H)
followed by a right-hand-side context (RHS). We
developed our own head finding, or head lexical-
ization, rules based on a variety of heuristics and
manual inspection of the PS rules.
Initially, we extracted 45785 Phrase Structure (PS)
rules from the treebank. The reason for the rela-
tively large number of PS rules is the fine-grained
nature of the tags encoding morphological infor-
mation for pre-terminal nodes. When we conflate
pre-terminals containing morphological informa-
tion to basic POS tags, the set of PS rules is re-
duced to 9731.
Treebanks grammars follow the Zipfian law: for
each category, there is a small number of highly
frequent rules expanding that category, followed
by a large number of rules with a very low fre-
quency. Therefore, for each LHS category we se-
lect the most frequent rules which together give
85% coverage. This results is a reduced set of 339
(most frequent) PS rules. These rules are manu-
ally examined and used to construct left-right LFG
f-structure annotation matrices for the treebank.
The annotation matrices encode information about
48
the left and right context of a rule?s head and state
generalisations about the functional annotation of
constituents to the left and right of the local head.
Consider sentence (5), where an NP is expanded
as NP NP ADJP. The first NP is considered the
head and is given the annotation ?=?. The second
NP and the ADJP are located to the left (Arabic
reading) of the head (LHS). The left-right context
matrix for NP constituents analyses these phrases
as adjuncts and assigns them the annotation ? ? ?
ADJUNCT.
(5)

?

J


??m
.
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 146?149,
Paris, October 2009. c?2009 Association for Computational Linguistics
Guessing the Grammatical Function of a Non-Root F-Structure in LFG
Anton Bryl
CNGL,
Dublin City University,
Dublin 9, Ireland
Josef van Genabith
CNGL,
Dublin City University,
Dublin 9, Ireland
{abryl,josef,ygraham}@computing.dcu.ie
Yvette Graham
NCLT,
Dublin City University,
Dublin 9, Ireland
Abstract
Lexical-Functional Grammar (Kaplan and
Bresnan, 1982) f-structures are bilexical
labelled dependency representations. We
show that the Naive Bayes classifier is able
to guess missing grammatical function la-
bels (i.e. bilexical dependency labels) with
reasonably high accuracy (82?91%). In
the experiments we use f-structure parser
output for English and German Europarl
data, automatically ?broken? by replacing
grammatical function labels with a generic
UNKNOWN label and asking the classifier
to restore the label.
1 Introduction
The task of labeling unlabelled dependencies, a
sub-task of dependency parsing task, can occur
in transfer-based machine translation (when only
an inexact match can be found in the training
data for the given SL fragment) or in parsing
where the system produces fragmented output. In
such cases it is often reasonably straightforward
to guess which fragments are dependent on which
other fragments (e.g. in transfer-based MT). What
is harder to guess are the labels of the dependen-
cies connecting the fragments.
In this paper we systematically investigate the
labelling task by automatically deleting function
labels from Lexical-Functional Grammar-based
parser output for German and English Europarl
data, and then restoring them using a Naive Bayes
classifier trained on attribute names and attribute
values of the f-structure fragments. We achieve
82% (German) to 91% (English) accuracy for both
single and multiple missing function labels.
The paper is organized as follows: in Section 2
we define the problem and the proposed solution
more formally. Section 3 details the experimental
evaluations, and in Section 4 we present our con-
clusions.
?
????????
PRED ?adopt?
UNKNOWN f1
?
????
PERS 3
NUM sg
PRED ?resolution?
SPEC f2
[ DET f3 [PRED ?the?]
]
?
????
SUBJ f4
?
?
PERS 3
NUM sg
PRED ?Parliament?
?
?
?
????????
Figure 1: Example of a ?broken? f-structure (sim-
plified). The sentence is ?Parliament adopted the
resolution.? The missing function of f1 is OBJ.
2 Guessing Unknown Grammatical
Functions
Let us introduce some useful definitions. By de-
pendent f-structure of the parent f-structure fP we
mean an f-structure fd which bears a grammati-
cal function within fP , or belongs to a set which
bears a grammatical function within fP . E.g., in
Figure 1 f2 is a dependent f-structure of f1. In this
paper we will not distinguish between these two
situations, but simply refer to multiple f-structures
bearing the same function within the same parent
for set-valued grammatical functions. C(?, fP )
denotes the number of dependent f-structures of
fP which bear the grammatical function ? in fP
(either directly or as members of a set).
Let us formalize the simple case when the gram-
matical function of only one dependent f-structure
is missing. Let FP be the set of f-structures which
have a dependent f-structure with an UNKNOWN la-
bel instead of the grammatical function. Let ? be
the set of all grammatical functions of the given
grammar. We need a guessing function G : FP ?
?, such that G(fP ) is a meaningful replacement
for the UNKNOWN label in fP . As the set ? is fi-
nite, the problem is evidently a classification task.
F-structures are characterized by attributes
some of which potentially carry information about
the f-structure?s grammatical function, even if
146
Language N-GF N-DEP AVG-DEP MIN-DEP MAX-DEP
English 24 9724 1.57 1 5
German 39 10910 1.55 1 5
Table 1: Data used in the evaluation. N-GF is the number of different grammatical functions occurring
in the dataset. N-DEP is the number of dependent f-structures in the test set. AVG-DEP, MIN-DEP,
MAX-DEP is the average, min. and max. number of dependant structures per parent in the test set.
we observe these attributes completely separately
from each other. For example, it seems likely
that an f-structure with an ATYPE attribute is an
ADJUNCT, while an f-structure which has CASE
is probably a SUBJ or an OBJ. Given this, Naive
Bayes appears to be a promising solution here. Be-
low we describe a way to adapt this classifier to the
problem of grammatical function guessing.
Let ?P ? ? be the set of grammatical functions
which are already present in fP . Let ? = {?1..?n}
be the set of features, and let X = {x1..xn} be
the values of these features for the f-structure fd
for which the function should be guessed. Then
the answer ?d is chosen as follows:
?d = arg max
???
(
p(?)MP (?)
n?
i=1
p(?i = xi|?)
)
(1)
MP (?) =
{
p(C(?, fP ) > 1), if ? ? ?P
1, otherwise (2)
where the probabilities are estimated from the
training data. Equation (2) states that if ? is al-
ready present in the parent f-structure, the proba-
bility of ? being set-valued is considered.
We propose two ways of building the feature
set ?. First, it is possible to consider the pres-
ence/absence of each particular attribute in fd as
a binary feature. Second, it is possible to con-
sider atomic attribute values as features as well.
To give a motivating example, in many languages
the value of CASE is extremely informative when
distinguishing objects from subjects. We use only
those atomic attribute values which do not rep-
resent words. E.g., NUM, PRED or NUM=sg are
features, while PRED=?resolution? is not a
feature. This distinction prevents the feature set
from growing too large and thus the probability
estimates from being too inaccurate.
If grammatical functions are missing for sev-
eral dependent f-structures, it is possible to use
the same approach, guessing the missing func-
tions one by one. In general, however, these de-
cisions will not be independent. To illustrate this,
let us consider a situation when the functions are
to be guessed for two dependent f-structures of
the same parent f-structure, OBJ being the correct
answer for the first and SUBJ for the second. If
the guesser returns SUBJ for the first of the two,
this answer will not only be incorrect, but also de-
crease the probability of the correct answer for the
second by decreasing MP (SUBJ) in Equation (1).
This suggests that in such cases maximization of
the joint probability of the values of all the miss-
ing functions may be a better choice.
3 Experimental Evaluation
We present two experiments which assess the ac-
curacy of the proposed approach and compare dif-
ferent variants of it in order to select the best, and
an additional one which assesses the usefulness of
the approach for practical machine translation.
3.1 Data Used in the Evaluation
For our experiments we used sentences from
the German-English part of the Europarl cor-
pus (Koehn, 2005) parsed into f-structures with
the XLE parser (Kaplan et al, 2002) using En-
glish (Riezler et al, 2002) and German (Butt et
al., 2002) LFGs. We parsed only sentences of
length 5?15 words. For the first two experiments,
we picked 2000 sentences for training and 1000
for testing for both languages. We ignored robust-
ness features (FIRST, REST), functions related to
c-structure constraints (MOTHER, LEFT SISTER,
etc.), and TOPIC. Of the remaining functions, we
considered only those occurring in the PREDs-
only part of f-structure. If a dependent f-structure
has multiple functions within the same parent f-
structure, only the first function occurring in the
description is considered. This does not unduely
influence the results, as the grammatical function
of an f-structure, after exclusion of TOPIC, carries
multiple labels in only about 2% of the cases in the
English data and about 1% in the German data. In
Table 1 we provide some useful statistics to help
the reader interpret the results of the experiments.
147
Language MF NB-CASE NB-N NB-N&V
English 36.3% 56.7% 85.6% 91.6%
German 23.4% 51.0% 74.8% 82.5%
Table 2: Experiment 1: Guessing a Single Miss-
ing Grammatical Function. MF is the pick-most-
frequent classifier. NB-CASE is Naive Bayes
(NB) with only CASE values used as features. NB-
N is NB with only attribute names used as fea-
tures. NB-N&V is NB with both attribute names
and atomic attribute values used as features.
3.2 Experiment 1: Guessing a Single Missing
Grammatical Function
The goal of this experiment is to evaluate the ac-
curacy of the Bayesian guesser in the case when
the grammatical function is unknown only for one
dependent f-structure, and to assess whether the
inclusion of attribute values into the feature set
improves the results, and whether attributes other
than CASE are useful.
Procedure. As a baseline, we used a pick-most-
frequent algorithm MF which considers only the
function?s prior probability and the presence of
this function in the parent (returning to Equations
(1) and (2), MF is in fact Naive Bayes with an
empty feature set ?). The guesser was evaluated
in three variants: NB-CASE with the feature set
formed only from the values of CASE attributes
(if the f-structure has no CASE feature, the classi-
fier degenerates to MF), NB-N with the feature set
formed only from attribute names, and NB-N&V
with the feature set formed from both attribute
names and values. All grammatical functions in
the test set were used as test cases. At each step in
the evaluation, one function was removed and then
guessed by each algorithm. For both languages the
test set was split into 10 non-intersecting subsets
with approximately equal numbers of grammati-
cal functions in each, and the values obtained for
the 10 subsets were further used to assess the sta-
tistical significance of the differences in the results
with the paired Student?s t-test.
Results. Table 2 presents the results. For both
English and German all the three versions of the
classifier clearly outperform the baseline, and even
the advantage of NB-CASE over the baseline is
statistically significant at the 0.5% level for both
languages. However, NB-CASE performs much
worse than NB-N and NB-N&V (their advantage
over NB-CASE is statistically significant at the
0.5% level for both languages), confirming that
Language MF NB-S NB-J
English 22.0% 90.4% 91.2%
German 17.1% 81.4% 82.1%
Table 3: Experiment 2: Guessing Multiple Miss-
ing Functions. MF is the pick-most-frequent clas-
sifier. NB-S and NB-J are one-by-one and join-
probability-based Naive Bayesian guessers.
CASE is not the only feature which is useful in
our task. The increase in accuracy brought about
by including the atomic attribute values into the
feature space is visible and significant at the same
level. The increase is somewhat more pronounced
for German than for English. For English the in-
clusion of attribute values into the feature space
affects primarily the accuracy of SUBJ vs. OBJ
decisions. For German, the accuracy notably in-
creases for telling SUBJ, OBJ and ADJ-GEN from
one another.
3.3 Experiment 2: Guessing Multiple
Missing Grammatical Functions
The goal of this experiment is to assess the accu-
racy of the Bayesian guesser for multiple miss-
ing grammatical functions within one parent f-
structure, and to compare the accuracy of one-
by-one vs. joint-probability-based guessing. Our
evaluation procedure models the extreme case
when the functions are unknown for all the depen-
dent f-structures of a particular parent.
Procedure. As a baseline, we use the same al-
gorithm MF as in Experiment 1, applied to the
missing grammatical functions one by one. Two
Bayesian guessers are evaluated, NB-S guessing
the missing grammatical functions one by one, and
NB-J guessing them all at once by maximizing
the joint probability of the values. Both Bayesian
guessers use attribute names and values as fea-
tures. All grammatical functions in the test set
were used as test cases. At each step of the ex-
periment, the grammatical functions of all the de-
pendent f-structures of a particular parent were
removed simultaneously, and then guessed with
each of the algorithms considered in this experi-
ment. Statistical significance was assessed in the
same way as in Experiment 1.
Results. Table 3 presents the accuracy scores.
The one-by-one guesser and the joint-probability-
based guesser perform nearly equally well, result-
ing in accuracy levels very close to those obtained
in Experiment 1 for f-structures with a single
148
missing function. Joint-probability-based guess-
ing achieves an advantage which is statistically
significant at the 0.5% level for both languages
but is not exceeding 1% absolute improvement.
For both languages errors typically occur in distin-
guishing OBJ vs. SUBJ and ADJUNCT vs. MOD,
and additionally in XCOMP vs. OBJ for English.
3.3.1 Experiment 3: Postprocessing the
Output of an MT Decoder
The goal of this experiment is to see how the
method influences the results of an SMT system.
Procedure. For this experiment we use the Sulis
SMT system (Graham et al, 2009), and a decoder,
which selects the transfer rules by maximizing the
source-to-target probability of the complete trans-
lation. Such a decoder, though simple, allows us
to create a realistic environment for evaluation.
From the f-structures produced by the decoder,
candidate sentences are generated with XLE, and
then the one best translation is selected for each
sentence using a language model. The function
guesser is used to postprocess the output of the
decoder before sentence generation. In the ex-
periment, the function guesser uses both attribute
names and values to make a guess. Guessing of
multiple missing functions is performed one-by-
one, as joint guessing complicates the algorithm
and leads to a very small improvement in accuracy.
The function guesser is trained on 3000 sentences,
which are a subset of the set used for inducing the
transfer rules. The overall MT system is evaluated
both with and without function guessing on 500
held-out sentences, and the quality of the transla-
tion is measured using the BLEU metric (Papineni
et al, 2002). We also calculate the number of sen-
tences for which the generator output is unempty.
Results. The system without function guesser
produced results for 364 sentences out of 500,
with BLEU score equal to 5.69%; with function
guesser the number of successfully generated sen-
tences increases to 433, with BLEU improving to
6.95%. Thus, the absolute increase of BLEU score
brought about by the guesser is 1.24%. This sug-
gests that the algorithm succeeds on real data and
is useful in grammar-based machine translation.
4 Conclusion
In this paper we addressed the problem of restor-
ing unknown grammatical functions in automati-
cally generated f-structures. We proposed to view
this problem as a classification task and to solve
it with the Naive Bayes classifier, using the names
and the values of the attributes of the dependent
f-structure to construct the feature set.
The approach was evaluated on English and
German data, and showed reasonable accuracy,
restoring the missing functions correctly in about
91% of the cases for English and about 82% for
German. It is tempting to interpret the differences
in accuracy for English and German as reflecting
the complexity of grammatical function assign-
ment for the two languages. It is not clear, how-
ever, whether the differences are due to differences
in the grammars or in the underlying data.
The experiments reported here use LFG-type
representations. However, nothing much in the
method is specific to LFG, and therefore we are
confident that our method also applies to other
dependency-based representations.
Acknowledgments
The research presented here was supported by Sci-
ence Foundation Ireland grant 07/CE2/I1142 un-
der the CNGL CSET programme.
References
M. Butt, H. Dyvik, T. H. King, H. Masuichi, and
C. Rohrer. 2002. The parallel grammar project.
In COLING?02, Workshop on Grammar Engineer-
ing and Evaluation.
Y. Graham, A. Bryl, and J. van Genabith. 2009. F-
structure transfer-based statistical machine transla-
tion. In LFG?09 (To Appear).
R. Kaplan and J. Bresnan. 1982. Lexical functional
grammar, a formal system for grammatical represe-
nation. The Mental Representation of Grammatical
Relations, pages 173?281.
R. M. Kaplan, T. H. King, and J. T. Maxwell III. 2002.
Adapting existing grammars: the XLE experience.
In COLING?02, Workshop on Grammar Engineer-
ing and Evaluation.
P. Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In MT Summit X, pages
79?86.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL?02, pages 311?318.
S. Riezler, T. H. King, R. M. Kaplan, R. Crouch,
J. T. Maxwell III, and M. Johnson. 2002. Pars-
ing the wall street journal using a lexical-functional
grammar and discriminative estimation techniques.
In ACL?02, pages 271?278.
149
Wide-Coverage Deep Statistical Parsing
Using Automatic Dependency
Structure Annotation
Aoife Cahill?
Dublin City University
Michael Burke??,?
Dublin City University
IBM Center for Advanced Studies
Ruth O?Donovan??
Dublin City University
Stefan Riezler?
Palo Alto Research Center
Josef van Genabith??,?
Dublin City University
IBM Center for Advanced Studies
Andy Way??,?
Dublin City University
IBM Center for Advanced Studies
A number of researchers have recently conducted experiments comparing ?deep? hand-crafted
wide-coverage with ?shallow? treebank- and machine-learning-based parsers at the level of
dependencies, using simple and automatic methods to convert tree output generated by the
shallow parsers into dependencies. In this article, we revisit such experiments, this time using
sophisticated automatic LFG f-structure annotation methodologies with surprising results. We
compare various PCFG and history-based parsers to find a baseline parsing system that fits
best into our automatic dependency structure annotation technique. This combined system of
syntactic parser and dependency structure annotation is compared to two hand-crafted, deep
constraint-based parsers, RASP and XLE. We evaluate using dependency-based gold standards
? Now at the Institut fu?r Maschinelle Sprachverarbeitung, Universita?t Stuttgart, Germany. E-mail: aoife.
cahill@ims.uni-stuttgart.de.
?? National Centre for Language Technology, Dublin City University, Dublin 9, Ireland.
? IBM Dublin Center for Advanced Studies (CAS), Dublin 15, Ireland.
? Now at Google Inc., Mountain View, CA.
Submission received: 24 August 2005; revised submission received: 20 March 2007; accepted for publication:
2 June 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 1
and use the Approximate Randomization Test to test the statistical significance of the results.
Our experiments show that machine-learning-based shallow grammars augmented with so-
phisticated automatic dependency annotation technology outperform hand-crafted, deep, wide-
coverage constraint grammars. Currently our best system achieves an f-score of 82.73% against
the PARC 700 Dependency Bank, a statistically significant improvement of 2.18% over the
most recent results of 80.55% for the hand-crafted LFG grammar and XLE parsing system
and an f-score of 80.23% against the CBS 500 Dependency Bank, a statistically significant
3.66% improvement over the 76.57% achieved by the hand-crafted RASP grammar and parsing
system.
1. Introduction
Wide-coverage parsers are often evaluated against gold-standard CFG trees (e.g.,
Penn-II WSJ Section 23 trees) reporting traditional PARSEVALmetrics (Black et al 1991)
of labeled and unlabeled bracketing precision, recall and f-score measures, number of
crossing brackets, complete matches, and so forth. Although tree-based parser evalua-
tion provides valuable insights into the performance of grammars and parsing systems,
it is subject to a number of (related) drawbacks:
1. Bracketed trees do not always provide NLP applications with enough
information to carry out the required tasks: Many applications involve
a deeper analysis of the input in the form of semantically motivated
information such as deep dependency relations, predicate?argument
structures, or simple logical forms.
2. A number of alternative, but equally valid tree representations can
potentially be given for the same input. To give just a few examples: In
English, VPs containing modals and auxiliaries can be analyzed using
(predominantly) binary branching rules (Penn-II [Marcus et al 1994]), or
employ flatter analyses where modals and auxiliaries are sisters of the
main verb (AP treebank [Leech and Garside 1991]), or indeed do without
a designated VP constituent at all (SUSANNE [Sampson 1995]). Treebank
bracketing guidelines can use ?traditional? CFG categories such as S, NP,
and so on (Penn-II) or a maximal projection-inspired analysis with IPs
and DPs (Chinese Penn Treebank [Xue et al 2004]).
3. Because a tree-based gold standard for parser evaluation must adopt a
particular style of linguistic analysis (reflected in the geometry and
nomenclature of the nodes in the trees), evaluation of statistical parsers
and grammars that are derived from particular treebank resources (as
well as hand-crafted grammars/parsers) can suffer unduly if the gold
standard deviates systematically from the (possibly) equally valid style
of linguistic analysis provided by the parser.
Problems such as these have motivated research on more abstract, dependency-
based parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanfilippo 1998; Carroll
et al 2002; Clark and Hockenmaier 2002; King et al 2003; Preiss 2003; Kaplan et al
2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are ap-
proximations of abstract predicate-argument-adjunct (or more basic head-dependent)
82
Cahill et al Statistical Parsing Using Automatic Dependency Structures
structures, providing a more normalized representation abstracting away from the
particulars of surface realization or CFG-tree representation, which enables meaningful
cross-parser evaluation.
A related contrast holds between shallow and deep grammars and parsers.1 In
addition to defining a language (as a set of strings), deep grammars relate strings to in-
formation/meaning, often in the form of predicate?argument structure, dependency re-
lations,2 or logical forms. By contrast, a shallow grammar simply defines a language and
may associate syntactic (e.g., CFG tree) representations with strings. Natural languages
do not always interpret linguistic material locally where the material is encountered
in the string (or tree). In order to obtain accurate and complete predicate?argument,
dependency, or logical form representations, a hallmark of deep grammars is that they
usually involve a long-distance dependency (LDD) resolution mechanism.
Traditionally, deep grammars are hand-crafted (cf. the ALVEY Natural Language
Tools [Briscoe et al 1987], the Core Language Engine [Alshawi and Pulman 1992], the
Alpino Dutch dependency parser [Bouma, van Noord, andMalouf 2000], the Xerox Lin-
guistic Environment [Butt et al 2002], the RASP dependency parser [Carroll and Briscoe
2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al 2004]).
Wide-coverage, deep-grammar development, particularly in rich formalisms such as
LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard
and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting
an instance of the (in-)famous ?knowledge acquisition bottleneck? familiar from other
areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars
(Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al 2002)
have, in fact, been successfully scaled to unrestricted input.
The last 15 years have seen extensive efforts on treebank-based automatic gram-
mar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995;
Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein
and Manning 2003). These grammars are wide-coverage and robust and in contrast
to manual grammar development, machine-learning-based grammar acquisition in-
curs relatively low development cost. With few notable exceptions,3 however, these
treebank-induced wide-coverage grammars are shallow: They usually do not attempt
to resolve LDDs nor do they associate strings with meaning representations.
Over the last few years, addressing the knowledge acquisition bottleneck in deep
constraint-based grammar development, a growing body of research has emerged to au-
tomatically acquire wide-coverage deep grammars from treebank resources (TAG [Xia
1999], CCG [Hockenmaier and Steedman 2002], HPSG [Miyao, Ninomiya, and Tsujii
2003], LFG [Cahill et al 2002b, 2004]). To a first approximation, these approaches can
be classified as ?conversion?- or ?annotation?-based. TAG-based approaches convert
1 Our use of the terms ?shallow? and ?deep? parsers/grammars follows Kaplan et al (2004) where
a ?shallow parser? does not relate strings to meaning representations. This deviates from a more
common use of the terms where, for example, a ?shallow parser? refers to (often finite-state-based)
parsers (or chunkers) that may produce partial bracketings of input strings.
2 By dependency relations we mean deep, fine-grained, labeled dependencies that encode long-distance
dependencies and passive information, for example. These differ from the types of unlabeled
dependency relations in other work such as (McDonald and Pereira 2006).
3 Both Collins Model 3 (1999) and Johnson (2002) output CFG tree representations with traces. Collins
Model 3 performs LDD resolution for wh-relative clause constructions, Johnson (2002) for a wide range
of LDD phenomena in a post-processing approach based on Penn-II tree fragments linking displaced
material with where it is to be interpreted semantically. The work of Dienes and Dubey (2003) and
Levy and Manning (2004) is similar to that of Johnson (2002), recovering empty categories on top of
CFG-based parsers. None of them map strings into dependencies.
83
Computational Linguistics Volume 34, Number 1
treebank trees into (lexicalized) elementary or adjunct trees. CCG-based approaches
convert trees into CCG derivations fromwhich CCG categories can be extracted. HPSG-
and LFG-based grammar induction methods automatically annotate treebank trees
with (typed) attribute-value structure information for the extraction of constraint-based
grammars and lexical resources.
Two recent papers (Preiss 2003; Kaplan et al 2004) have started tying together
the research strands just sketched: They use dependency-based parser evaluation to
compare wide-coverage parsing systems using hand-crafted, deep, constraint-based
grammars with systems based on a simple version of treebank-based deep grammar
acquisition technology in the conversion paradigm. In the experiments, tree output
generated by Collins?s Model 1, 2, and 3 (1999) and Charniak?s (2000) parsers, for
example, are automatically translated into dependency structures and evaluated against
gold-standard dependency banks.
Preiss (2003) uses the grammatical relations and the CBS 500 Dependency Bank
described in Carroll, Briscoe, and Sanfilippo (1998) to compare a number of parsing
systems (Briscoe and Carroll 1993; Collins?s 1997 models 1 and 2; and Charniak 2000)
using a simple version of the conversion-based deep grammar acquisition process (i.e.,
reading off grammatical relations fromCFG parse trees produced by the treebank-based
shallow parsers). The article also reports on a task-based evaluation experiment to rank
the parsers using the grammatical relations as input to an anaphora resolution system.
Preiss concluded that parser ranking using grammatical relations reflected the absolute
ranking (between treebank-induced parsers) using traditional tree-based metrics, but
that the difference between the performance of the parsing algorithms narrowed when
they carried out the anaphora resolution task. Her results show that the hand-crafted
deep unification parser (Briscoe and Carroll 1993) outperforms the machine-learned
parsers (Collins 1997; Charniak 2000) on the f-score derived from weighted precision
and recall on grammatical relations.4 Kaplan et al (2004) compare their deep, hand-
crafted, LFG-based XLE parsing system (Riezler et al 2002) with Collins?s (1999) model
3 using a simple conversion-based approach, capturing dependencies from the tree
output of the machine-learned parser, and evaluating both parsers against the PARC
700 Dependency Bank (King et al 2003). They conclude that the hand-crafted, deep
grammar outperforms the state-of-the-art treebank-based shallow parser on the level of
dependency representation, at the price of a small decrease in parsing speed.
Both Preiss (2003) and Kaplan et al (2004) emphasize that they use rather basic
versions of the conversion-based deep grammar acquisition technology outlined herein.
In this article we revisit the experiments carried out by Preiss and Kaplan et al, this time
using the sophisticated and fine-grained treebank- and annotation-based, deep, probabilis-
tic LFG grammar acquisitionmethodology developed in Cahill et al (2002b), Cahill et al
(2004), O?Donovan et al (2004), and Burke (2006) with a number of surprising results:
1. Evaluating against the PARC 700 Dependency Bank (King et al 2003)
using a retrained version of Bikel?s (2002) parser, the best automatically
induced, deep LFG resources achieve an f-score of 82.73%. This is an
improvement of 3.13 percentage points over the previously best published
results established by Kaplan et al (2004) who use a hand-crafted,
wide-coverage, deep LFG and the XLE parsing system. This is also a
4 The numbers given are difficult to compare as the results for the Briscoe and Carroll (1993) parser were
captured for a richer set of grammatical relations than those for Collins (1997) and Charniak (2000).
84
Cahill et al Statistical Parsing Using Automatic Dependency Structures
statistically significant improvement of 2.18 percentage points over the
most recent improved results presented in this article for the XLE system.
2. Evaluating against the Carroll, Briscoe, and Sanfilippo (1998) CBS 500
gold-standard dependency bank using a retrained version of Bikel?s (2002)
parser, the best Penn-II treebank-based, automatically acquired, deep LFG
resources achieve an f-score of 80.23%. This is a statistically significant
improvement of 3.66 percentage points over Carroll and Briscoe (2002),
who use a hand-crafted, wide-coverage, deep, unification grammar and
the RASP parsing system.
Evaluation results on a reannotated version (Briscoe and Carroll 2006) of the PARC
700 Dependency Bank were recently published in Clark and Curran (2007), reporting
f-scores of 81.9% for the CCG parser, and 76.3% for RASP. As Briscoe and Carroll
(2006) point out, these evaluations are not directly comparable with the Kaplan et al
(2004) style evaluation against the original PARC 700 Dependency Bank, because the
annotation schemes are different.
The article is structured as follows: In Section 2, we outline the automatic LFG
f-structure annotation algorithm and the pipeline parsing architecture of Cahill et al
(2002b), Cahill et al (2004), and Burke (2006). In Section 3, we present our experiment
design. In Section 4, using the DCU 105 Dependency Bank as our development set, we
evaluate a number of treebank-induced LFG parsing systems against the automatically
generated Penn-II WSJ Section 22 Dependency Bank test set. We use the Approximate
Randomization Test (Noreen 1989) to test for statistical significance and choose the best
parsing system for the evaluations against the wide-coverage, hand-crafted RASP and
LFG grammars of Carroll and Briscoe (2002) and Kaplan et al (2004) using the CBS
500 and PARC 700 Dependency Banks in Section 5. In Section 6, we discuss results and
issues raised by our methodology, outline related and future research and conclude in
Section 7.
2. Methodology
In this section, we briefly outline LFG and present our automatic f-structure annotation
algorithm and parsing architecture. The parsing architecture enables us to integrate
PCFG- and history-based parsers, which allows us to compare these parsers at the level
of dependency structures, rather than just trees.
2.1 Lexical Functional Grammar
Lexical Functional Grammar (LFG) (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple
2001) is a constraint-based theory of grammar. It (minimally) posits two levels of
representation, c(onstituent)-structure and f(unctional)-structure. C-structure is rep-
resented by context-free phrase-structure trees, and captures surface grammatical
configurations such as word order. The nodes in the trees are annotated with functional
equations (attribute-value structure constraints, for example (?OBJ)=?) which are
resolved (in the case of well-formed strings) to produce an f-structure. F-structures
are recursive attribute-value matrices, representing abstract syntactic functions, which
85
Computational Linguistics Volume 34, Number 1
Figure 1
C- and f-structures for the sentence U.N. signs treaty.
approximate to basic predicate-argument-adjunct structures or dependency relations.5
Figure 1 shows the c- and f-structures for the string U.N. signs treaty. Each node in the
c-structure is annotated with f-structure equations, for example (? SUBJ)= ?. The
uparrows (?) point to the f-structure associated with the mother node, downarrows
(?) to that of the local node. In a complete parse tree, these ? and ? meta variables are
instantiated to unique tree node identifiers and a set of constraints (a set of terms in an
equality logic) is generated which (if satisfiable) generates an f-structure.
2.2 Automatic F-Structure Annotation Algorithm
Deep grammars can be induced from treebank resources if the treebank encodes
enough information to support the derivation of deep grammatical information, such
as predicate?argument structures, deep dependency relations, or logical forms. Many
second generation treebanks such as Penn-II provide information to support the compi-
lation of meaning representations, for example in the form of traces relating displaced
linguistic material to where it should be interpreted semantically. The f-structure anno-
tation algorithm exploits configurational and categorial information, as well as traces
and the Penn-II functional tag annotations (Table 1) to automatically associate Penn-II
CFG trees with LFG f-structure information.
Given a tree, such as the Penn-II-style tree in Figure 2, the algorithm will traverse
the tree and deterministically add f-structure equations to the phrasal and leaf nodes
of the tree, resulting in an f-structure annotated version of the tree. The annotations are
then collected and passed on to a constraint solver which generates an f-structure (if the
constraints are satisfiable). We use a simple graph-unification-based constraint solver
(Eisele and Do?rre 1986), extended to handle path, set-valued, disjunctive, and existential
constraints. Given parser output without Penn-II style annotations and traces, the same
algorithm is used to assign annotations to each node in the tree, whereas a separate
module is applied at the level of f-structure to resolve any long-distance dependencies
(see Section 2.3).
5 van Genabith and Crouch (1996, 1997) provide translations between f-structures, Quasi-Logical Forms
(QLFs), and Underspecified Discourse Representation Structures (UDRSs).
86
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 1
A complete list of the Penn-II functional labels.
Tag Description
Form/function discrepancies
-ADV clausal and NP adverbials
-NOM non NPs that function as NPs
Grammatical role
-DTV dative
-LGS logical subjects in passives
-PRD non VP predicates
-PUT locative complement of put
-SBJ surface subject
-TPC topicalized and fronted constituents
-VOC vocatives
Adverbials
-BNF benefactive
-DIR direction and trajectory
-EXT extent
-LOC location
-MNR manner
-PRP purpose and reason
-TMP temporal phrases
Miscellaneous
-CLR closely related to verb
-CLF true clefts
-HLN headlines and datelines
-TTL titles
The f-structure annotation algorithm is described in detail in Cahill et al (2002a),
McCarthy (2003), Cahill et al (2004), and Burke (2006). In brief, the algorithm is modular
with four components (Figure 3), taking Penn-II trees as input and automatically adding
LFG f-structure equations to each node in the tree.
Lexical Information. Lexical information is generated automatically by macros for each
of the POS classes in Penn-II. To give a simple example, third-person plural noun
Penn-II POS-word sequences of the form NNS word are automatically associated with
the equations (?PRED) = word?, (?NUM) = pl and (?PERS) = 3rd, where word? is the
lemmatized word.
Left?Right Context Annotation. The Left?Right context annotation component identifies
the heads of Penn-II trees using a modified version of the head finding rules of
Magerman (1994). This partitions each local subtree (of depth one) into a local head, a
left context (left sisters), and a right context (right sisters). The contexts together with
information about the local mother and daughter categories and (if present) Penn-II
87
Computational Linguistics Volume 34, Number 1
Figure 2
Trees for the sentence U.N. signs treaty, the headline said before and after automatic f-structure
annotation, with the f-structure automatically produced.
Figure 3
F-structure annotation algorithm modules.
88
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 2
Sample from an NP Annotation matrix.
Left context Head Right context
DT: (?SPEC DET)=? NN, NNS, NNP, NNPS, NP: RRC, SBAR: (?RELMOD)=?
CD: (?SPEC QUANT)=? ?=? PP: ??(?ADJUNCT)
ADJP, JJ, NN, NNP: ??(?ADJUNCT) NP: ??(?APP)
functional tag labels (Table 1) are used by the f-structure annotation algorithm. For each
Penn-II mother (i.e., phrasal) category an Annotation matrix expresses generalizations
about how to annotate immediate daughters dominated by the mother category relative
to their location in relation to the local head. To give a (much simplified) example,
the head finding rules for NPs state that the rightmost nominal (NN, NNS, NNP, . . . )
not preceded by a comma or ?-?6 is likely to be the local head. The Annotation ma-
trix for NPs states (inter alia) that heads are annotated ?=?, that DTs (determiners)
to the left of the head are annotated (? SPEC DET) = ?, NPs to the right of the head as
??(? APP) (appositions). Table 2 provides a sample extract from the NP Annotation
matrix. Figure 4 provides an example of the application of the NP and PP Annotation
matrices to a simple tree.
For each phrasal category, Annotation matrices are constructed by inspecting the
most frequent Penn-II rule types expanding the category such that the token occurrences
of these rule types cover more than 85% of all occurrences of expansions of that category
in Penn-II. For NP rules, for example, this means that we analyze the most frequent
102 rule types expanding NP, rather than the complete set of more than 6,500 Penn-II
NP rule types, in order to populate the NP Annotation matrix. Annotation matrices
generalize to unseen rule types as, in the case of NPs, these may also feature DTs to
the left of the local head and NPs to the right and similarly for rule types expanding
other categories.
Coordination. In order to support the modularity, maintainability, and extendability of
the annotation algorithm, the Left?Right Annotation matrices apply only to local trees
of depth one, which do not feature coordination. This keeps the statement of Annotation
matrices perspicuous and compact. The Penn-II treatment of coordination is (inten-
tionally) flat. The annotation algorithm has modules for like- and unlike-constituent
coordination. Coordinated constituents are elements of a COORD set and annotated ??
(? COORD). The Coordination module reuses the Left?Right context Annotation ma-
trices to annotate any remaining nodes in a local subtree containing a coordinating
conjunction. Figure 5 provides a VP-coordination example (with right-node-raising).
Catch-All and Clean-Up. The Catch-All and Clean-Up module provides defaults to cap-
ture remaining unannotated nodes (Catch-All) and corrects (Clean-Up) overgeneraliza-
tions resulting from the application of the Left?Right context Annotation matrices. The
Left?Right Annotation matrices are allowed a certain amount of overgeneralization as
this facilitates the perspicuous statement of generalizations and a separate statement of
exceptions, supporting the modularity and maintainability of the annotation algorithm.
PPs under VPs are a case in point. The VP Annotation matrix analyses PPs to the right
of the local VP head as adjuncts: ? ? (?ADJUNCT). The Catch-All and Clean-Up module
6 If the rightmost nominal is preceded by a comma or ?-?, it is likely to be an apposition to the head.
89
Computational Linguistics Volume 34, Number 1
Figure 4
Automatically annotated Penn-II tree (fragment) and f-structure (simplified) for Gerry Purdy,
director of marketing.
uses Penn-II functional tag (Table 1) information (if present), for example -CLR (closely
related to local head), to replace the original adjunct analysis by an oblique argument
analysis: (?OBL)=?. An example of this is provided by the PP-CLR in the left VP-conjunct
in Figure 5. In other cases, argument?adjunct distinctions are encoded configurationally
in Penn-II (without the use of -CLR tags). To give a simple example, the NP Anno-
tation matrix indiscriminately associates SBARs to the right of the local head with
(? RELMOD) = ?. However, some of these SBARs are actually arguments of the local
NP head and, unlike SBAR relative clauses which are Chomsky-adjoined to NP (i.e.,
relative clauses are daughters of an NP mother and sisters of a phrasal NP head), SBAR
arguments are sisters of non-phrasal NP heads.7 In such cases, the Catch-All and Clean-
Up module rewrites the original relative clause analysis into the correct complement
argument analysis (?COMP)=?. Figure 6 shows the COMP f-structure analyses for an
example NP containing an internal SBAR argument (rather than relative clause) node.
Traces. The Traces module translates traces and coindexed material in Penn-II trees
representing long-distance dependencies into corresponding reentrancies at f-structure.
Penn-II provides a rich arsenal of trace types to relate ?displaced? material to where it
7 Structural information of this kind is not encoded in the Annotation matrices; compare Table 2.
90
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 5
Automatically annotated Penn-II tree (fragment) and resulting f-structure for asked for and
received refunds.
should be interpreted semantically. The f-structure annotation algorithm coverswh- and
wh-less relative clause constructions, interrogatives, control and raising constructions,
right-node-raising, and general ICH (interpret constituent here) traces. Figure 5 gives an
example that shows the interplay between coordination, right-node-raising traces and
the corresponding automatically generated reentrancies at f-structure.
2.3 Parsing Architecture
The pipeline parsing architecture of Cahill et al (2004) and Cahill (2004) for parsing raw
text into LFG f-structures is shown in Figure 7. In this model, PCFGs or history-based
lexicalized parsers are extracted from the unannotated treebank and used to parse raw
text into trees. The resulting parse trees are then passed to the automatic f-structure
annotation algorithm to generate f-structures.8
Compared to full Penn-II treebank trees, the output of standard probabilistic
parsers is impoverished: Parsers do not normally output Penn-II functional tag an-
notations (Table 1) nor do they indicate/resolve long-distance dependencies, recorded
8 In the integratedmodel (Cahill et al 2004; Cahill 2004), we extract f-structure annotated PCFGs
(A-PCFGs) from the f-structure annotated treebank, where each non-terminal symbol in the grammar
has been augmented with LFG functional equations, such as NP[?OBJ=?] ? DT[?SPEC=?] NN[?=?].
We treat a non-terminal symbol followed by annotations as a monadic category for grammar extraction
and parsing. Parsing with A-PCFGs results in annotated parse trees, from which an f-structure can be
generated. In this article we only use the pipeline parsing architecture.
91
Computational Linguistics Volume 34, Number 1
Figure 6
Automatically annotated Penn-II tree (fragment) and f-structure for signs that managers
expect declines.
in terms of a fine-grained system of empty productions (traces) and coindexation in
the full Penn-II treebank trees. The f-structure annotation algorithm, as described in
Section 2.2, makes use of Penn-II functional tag information (if present) and relies on
traces and coindexation to capture LDDs in terms of corresponding reentrancies at
f-structure.
Penn-II functional labels are used by the annotation algorithm to discriminate
between adjuncts and (oblique) arguments. PP-sisters to a head verb are analyzed as
arguments iff they are labeled -CLR, -PUT, -DTV or -BNF, for example. Conversely,
functional labels (e.g., -TMP) are also used to analyze certain NPs as adjuncts, and
-LGS labels help to identify logical subjects in passive constructions. In the absence of
functional labels, the annotation algorithm will default to decisions based on simple
structural, configurational, and CFG-category information (and, for example, conserva-
tively analyze a PP sister to a head verb as an adjunct, rather than as an argument).
In Sections 3 and 4 we present a number of treebank-based parsers (in particular the
PCFGs and a version of Bikel?s history-based, lexicalized generative parser) trained to
output CFG categories with Penn-II functional tags. We achieve this through a simple
92
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 7
Treebank-based LFG parsing architecture.
masking and un-masking operation where functional tags are joined with their local
CFG category label to form a new (larger) set of (monadic) CFG category labels (e.g.,
PP-CLR goes to PP CLR) for training and parsing (for Bikel, the parser head-finding rules
are also adjusted to the expanded set of categories). After parsing, the Penn-II functional
tags are unmasked and available to the f-structure annotation algorithm.
The Traces component in the f-structure annotation algorithm (Figure 3) translates
LDDs represented in terms of traces and coindexation in the original Penn-II treebank
trees into corresponding reentrancies at f-structure. Most probabilistic treebank-based
parsers, however, do not indicate/resolve LDDs, and the Traces component of the an-
notation algorithm does not apply. Initially, the f-structures produced for parser output
trees in the architecture in Figure 7 are therefore LDD-unresolved: They are incomplete
(or proto) f-structures, where displaced material (e.g., the values of FOCUS, TOPIC, and
TOPICREL attributes [wh- and wh-less relative clauses, topicalization, and interrogative
constructions] at f-structure) is not yet linked to the appropriate argument grammati-
cal functions (or elements of adjunct sets) for the governing local PRED. A dedicated
LDD Resolution component in the architecture in Figure 7 turns parser output proto-
f-structures into fully LDD-resolved proper f-structures, without traces and coindexa-
tion in parse trees.
Consider the following fragment of a proper Penn-II treebank tree (Figure 8), where
the LDD between the WHNP in the relative clause and the embedded direct object
position of the verb reward is indicated in terms of the trace *T*-3 and its coindexation
with the antecedent WHNP-3. Note further that the control relation between the subject
of the verbs wanted and reward is similarly expressed in terms of traces (*T*-2) and
coindexation (NP-SBJ-2). From the treebank tree, the f-structure annotation algorithm
is able to derive a fully resolved f-structure where the LDD and the control relation are
captured in terms of corresponding reentrancies (Figure 9).
93
Computational Linguistics Volume 34, Number 1
Figure 8
Penn-II treebank tree with LDD indicated in terms of traces (empty productions) and
coindexation and f-structure annotations generated by the annotation algorithm.
Now consider the corresponding ?impoverished? (but otherwise correct) parser
output tree (Figure 10) for the same string: The parser output does not explicitly record
the control relation nor the LDD.
Given this parser output tree, prior to the LDD resolution component in the parsing
architecture (Figure 7), the f-structure annotation algorithmwould initially construct the
partial (proto-) f-structure in Figure 11, where the LDD indicated by the TOPICREL func-
tion is unresolved (i.e., the value of TOPICREL is not coindexedwith the OBJ grammatical
function of the embedded verb reward). The control relation (shared subject between
the two verbs in the relative clause) is in fact captured by the annotation algorithm in
terms of a default annotation (? SUBJ) = (? SUBJ) on sole argument VPs to the right of
head verbs (as often, even in the full Penn-II treebank trees, control relations are not
consistently captured through explicit argument traces).
In LFG, LDD resolution operates at the level of f-structure, using functional un-
certainty equations (regular expressions over paths in f-structure [Kaplan and Zaenen
1989] relating f-structure components in different parts of an f-structure), obviating
traces and coindexation in c-structure trees. For the example in Figure 10, a functional
uncertainty equation of the form (?TOPICREL) = (?[COMP|XCOMP]? [SUBJ|OBJ]) would
be associated with the WHNP daughter node of the SBAR relative clause. The equation
states that the value of the TOPICREL attribute is token-identical (re-entrant) with the
value of a SUBJ or OBJ function, reached through a path along any number (including
94
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 9
Fully LDD-resolved f-structure.
Figure 10
Impoverished parser output tree: LDDs not captured.
zero) of COMP or XCOMP attributes. This equation, together with subcategorization
frames (LFG semantic forms) for the local PREDs and the usual LFG completeness and
coherence conditions, resolve the partial proto-f-structure in Figure 11 into the fully
LDD-resolved proper f-structure in Figure 9.
95
Computational Linguistics Volume 34, Number 1
Figure 11
Proto-f-structure: LDDs not captured.
Following Cahill et al (2004), in our parsing architecture (Figure 7) we model
LFG LDD resolution using automatically induced finite approximations of functional-
uncertainty equations and subcategorization frames from the f-structure-annotated
Penn-II treebank (O?Donovan et al 2004) in an LDD resolution component. From the
fully LDD-resolved f-structures from the Penn-II training section treebank trees we
learn probabilistic LDD resolution paths (reentrancies in f-structure), conditional on
LDD type (Table 3), and subcategorization frames, conditional on lemma (and voice)
(Table 4). Table 3 lists the eight most probable TOPICREL paths (out of a total of 37
TOPICREL paths acquired). The totality of these paths constitutes a finite subset of the
reference language definde by the full functional uncertainty equation (?TOPICREL) =
(?[COMP|XCOMP]? [SUBJ|OBJ]). Given an unresolved LDD type (such as TOPICREL in
the parser output for the relative clause example in Figure 11), admissible LDD res-
olutions assert a reentrancy between the value of the LDD trigger (here, TOPICREL)
and a grammatical function (or adjunct set element) of an embedded local predicate,
subject to the conditions that (i) the local predicate can be reached from the LDD trigger
using the LDD path; (ii) the grammatical function terminates the LDD path; (iii) the
grammatical function is not already present (at the relevant level of embedding in
the local f-structure); and (vi) the local predicate subcategorizes for the grammatical
function in question.9 Solutions satisfying (i)?(iv) are ranked using the product of
LDD path and subcategorization frame probabilities and the highest ranked solution
(possibly involving multiple interacting LDDs for a single f-structure) is returned by
the algorithm (for details and comparison against alternative LDD resolution methods,
see Cahill et al 2004).10
For our example (Figure 11), the highest ranked LDD resolution is for LDD path
(?TOPICREL) = (? XCOMP OBJ) and the local subcat frame REWARD?? SUBJ, ? OBJ?. This
9 Conditions (i)?(iv) are suitably adapted for LDD resolutions terminating in adjunct sets.
10 In our experiments we do not use the limited LDD resolution for wh-phrases provided by Collins?s Model
3 parser as better results are achieved using the purely f-structure-based LDD resolution as shown in
Cahill et al (2004).
96
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 3
Most frequent wh-TOPICREL paths.
wh-TOPICREL Probability wh-TOPICREL Probability
subj .7583 xcomp .0830
obj .0458 xcomp:obj .0338
xcomp:xcomp .0168 xcomp:subj .0109
comp .0097 comp:subj .0073
Table 4
Most frequent semantic forms for active and passive (p) occurrences of the verb want and
reward.
Semantic form Probability
want([subj,xcomp]) .6208
want([subj,obj]) .2496
want([subj,obj,xcomp]) .1008
want([subj]) .0096
want([subj,obj,obl]) .0048
want([subj,obj,part]),p) .5000
want([subj,obl]),p) .1667
want([subj,part]),p) .1667
want([subj]),p) .1667
reward([subj,obj]) .8000
reward([subj,obj,obl]) .2000
reward([subj]),p) 1.0000
(together with the subject control equation described previously) turns the parser-
output proto-f-structure (in Figure 11) into the fully LDD resolved f-structure in
(Figure 9).
The full pipeline parsing architecture with the LDD resolution (rather than the
Traces component for LDD resolved Penn-II treebank trees) component (and the LDD
path and subcategorization frame extraction) is given in Figure 7.
The pipeline architecture supports flexible integration of treebank-based PCFGs
or state-of-the-art, history-based, and lexicalized parsers (Collins 1999; Charniak 2000;
Bikel 2002) and enables dependency-based evaluation of such parsers.
3. Experiment Design
In our experiments we compare four history-based parsers for integration into the
pipeline parsing architecture described in Section 2.3:
 Collins?s 1999 Models 311
 Charniak?s 2000 maximum-entropy inspired parser12
11 Downloaded from ftp://ftp.cis.upenn.edu/pub/mcollins/PARSER.tar.gz.
12 Downloaded from ftp://ftp.cs.brown.edu/pub/nlparser/.
97
Computational Linguistics Volume 34, Number 1
 Bikel?s 2002 emulation of Collins Model 213
 a retrained version of Bikel?s (2002) parser which retains Penn-II functional
tags
Input for Collins?s and Bikel?s parsers was pre-tagged using the MXPOST POS tag-
ger (Ratnaparkhi 1996). Charniak?s parser provides its own POS tagger. The combined
system of best history-based parser and automatic f-structure annotation is compared
to two probabilistic parsing systems based on hand-crafted, wide-coverage, constraint-
based, deep grammars:
 the RASP parsing system (Carroll and Briscoe 2002)
 the XLE parsing system (Riezler et al 2002; Kaplan et al 2004)
Both hand-crafted grammars perform their own POS tagging, resolve LDDs, and
associate strings with dependency relations (in the form of grammatical relations or
LFG f-structures).
We evaluate the parsers against a number of gold-standard dependency banks.
We use the DCU 105 Dependency Bank (Cahill et al 2002a) as our development set
for the treebank-based LFG parsers. We use the f-structure annotation algorithm to
automatically generate a gold-standard test set from the original Section 22 treebank
trees (the f-structure annotated WSJ Section 22 Dependency Bank) to choose the best
treebank-based LFG parsing systems for the PARC 700 and CBS 500 experiments.
Following the experimental setup in Kaplan et al (2004), we use the Penn-II Section 23-
based PARC 700 Dependency Bank (King et al 2003) to evaluate the treebank-induced
LFG resources against the hand-crafted XLE grammar and parsing system of Riezler
et al (2002) and Kaplan et al Following Preiss (2003), we use the SUSANNE Based CBS
500 Dependency Bank (Carroll, Briscoe, and Sanfilippo 1998) to evaluate the treebank-
induced LFG resources against the hand-crafted RASP grammar and parsing system
(Carroll and Briscoe 2002) as well as against the XLE system (Riezler et al 2002).
For each gold standard, our experiment design is as follows: We parse automati-
cally tagged input14 sentences with the treebank- and machine-learning-based parsers
trained on WSJ Sections 02?21 in the pipeline architecture, pass the resulting parse
trees to our automatic f-structure annotation algorithm, collect the f-structure equations,
pass them to a constraint-solver which generates an f-structure, resolve long-distance
dependencies at f-structure following Cahill et al (2004) and convert the resulting LDD-
resolved f-structures into dependency representations using the formats and software
of Crouch et al (2002) (for the DCU 105, PARC 700, and WSJ Section 22 evaluations)
and the formats and software of Carroll, Briscoe, and Sanfilippo (1998) (for the CBS
500 evaluation). In the experiments we did not use any additional annotations such as
-A (for argument) that can be generated by some of the history-based parsers (Collins
1999) as the f-structure annotation algorithm is designed for Penn-II trees (which do
not contain such annotations). We also did not use the limited LDD resolution for wh-
relative clauses provided by Collins?s Model 3 as better results are achieved by LDD
13 This was developed at the University of Pennsylvania by Dan Bikel and is freely available to download
from http://www.cis.upenn.edu/?dbikel/software.html.
14 Tags were automatically assigned either by the parsers themselves or by the MXPOST tagger
(Ratnaparkhi 1996).
98
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 5
Results of tree-based evaluation on all sentences WSJ section 23, Penn-II.
Parser Labeled
f-score (%)
PCFG 73.03
Parent-PCFG 78.05
Collins M3 88.33
Charniak 89.73
Bikel 88.32
Bikel+Tags 87.53
resolution on f-structure (Cahill et al 2004). A complete set of parameter settings for the
parsers is provided in the Appendix.
In order to evaluate the treebank-induced LFG resources against the PARC 700
and the CBS 500 dependency banks, a certain amount of automatic mapping is re-
quired to account for systematic differences in linguistic analysis, feature geometry,
and nomenclature at the level of dependencies. This is discussed in Sections 5.1 and
5.2. Throughout, we use the Approximate Randomization Test (Noreen 1989) to test the
statistical significance of the results.
4. Choosing a Treebank-Based LFG Parsing System
In this section, we choose the best treebank-based LFG parsing system for the compar-
isons with the hand-crafted XLE and RASP resources in Section 5. We use the DCU
105 Dependency Bank as our development set and carry out comparative evaluation
and statistical significance testing on the larger, automatically generated WSJ Section 22
Dependency Bank as a test set. The system based on Bikel?s (2002) parser retrained to
retain Penn-II functional tags (Table 1) achieves overall best results.
4.1 Tree-Based Evaluation against WSJ Section 23
For reference, we include the traditional CFG-tree-based comparison for treebank-
induced parsers. The parsers are trained on Sections 02 to 21 of the Penn-II Treebank and
tested on Section 23. The published results15 on these experiments for the history-based
parsers are given in Table 5. We also include figures for a PCFG and a Parent-PCFG (a
PCFG which has undergone the parent transformation [Johnson 1999]). These PCFGs
are induced following standard treebank preprocessing steps, including elimination of
empty nodes, but following Cahill et al (2004), they do include Penn-II functional tags
(Table 1), as these tags contain valuable information for the automatic f-structure anno-
tation algorithm (Section 2.2). These tags are removed for the tree-based evaluation.
The results show that the history-based parsers produce considerably better trees
than the more basic PCFGs (with and without parent transformations). Charniak?s
(2000) parser scores best with an f-score of 89.73% on all sentences in Section 23. The
15 Where there were no published results available for Section 23, we calculated them using the
downloadable versions of the parsers.
99
Computational Linguistics Volume 34, Number 1
vanilla PCFG achieves the lowest f-score of 73.03%, a difference of 16.7 percentage
points. The hand-crafted XLE and RASP grammars achieve around 80% coverage
(measured in terms of complete spanning parse) on Section 23 and use a variety of
(longest) fragments combining techniques to generate dependency representations for
the remaining 20% of Section 23 strings. By contrast, the treebank-induced PCFGs and
history-based parsers all achieve coverage of over 99.9%. Given that the history-based
parsers score considerably better than PCFGs on trees, we would also expect them to
produce dependency structures of substantially higher quality.
4.2 Using DCU 105 as a Development Set
The DCU 105 (Cahill et al 2002a) is a hand-crafted gold-standard dependency bank
for 105 sentences, randomly chosen from Section 23 of the Penn-II Treebank.16 This is a
relatively small gold standard, initially developed to evaluate the automatic f-structure
annotation algorithm. We parse the 105 tagged sentences into LFG f-structures with
each of the treebank-induced parsers in the pipeline parsing and f-structure annotation
architecture. The f-structures of the gold standard and the f-structures returned by the
parsing systems are converted into dependency triples following Crouch et al (2002)
and Riezler et al (2002) and we also use their software for evaluation. The following
dependency triples are produced by the f-structure in Figure 1:
subj(sign?0,U.N.?1)
obj(sign?0,treaty?2)
num(U.N.?1,sg)
pers(U.N.?1,3)
num(treaty?2,sg)
pers(treaty?3,3)
tense(sign?0,present)
We evaluate preds-only f-structures (i.e., where paths in f-structures end in a PRED
value: the predicate-argument-adjunct structure skeleton) and all grammatical func-
tions (GFs) including number, tense, person, and so on. The results are given in Table 6.
With one main exception, Tables 5 and 6 confirm the general expectation that
the better the trees produced by the parsers, the better the f-structures automatically
generated for those trees. The exception is Bikel+Tags. The automatic f-structure an-
notation algorithm will exploit Penn-II functional tag information if present to generate
appropriate f-structure equations (see Section 2.2). It will default to possibly less reliable
configurational and categorial information if Penn-II tags are not present in the trees.
In order to test whether the retention of Penn-II functional labels in the history-
based parser output will improve LFG f-structure-based dependency results, we use
Bikel?s (2002) training software,17 and retrain the parser on a version of the Penn-II
treebank (Sections 02 to 21) with the Penn-II functional tag labels (Table 1) annotated
in such a way that the resulting history-based parser will retain them (Section 2.3). The
retrained parser (Bikel+Tags) then produces CFG-trees with Penn-II functional labels
and these are used by the f-structure annotation algorithm. We evaluate the f-structure
dependencies against the DCU 105 (Table 6) and achieve an f-score of 82.92% preds-only
16 It is publicly available for download from: http://nclt.computing.dcu.ie/gold105.txt.
17 We use Bikel?s software rather than Charniak?s for this experiment as the former proved more stable
during the retraining phase.
100
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 6
Treebank-induced parsers: results of dependency-based evaluation against DCU 105.
Parser Preds only All GFs
f-score (%) f-score (%)
PCFG 70.24 79.90
Parent-PCFG 75.84 83.58
Collins M3 77.84 85.08
Charniak 79.61 85.66
Bikel 79.39 86.56
Bikel+Tags 82.92 88.30
Table 7
Treebank induced parsers: breakdown by dependency relation of preds-only evaluation against
DCU 105.
Dep. Percent of total F-score (%)
Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags
ADJUNCT 33.73 71 72 76 73 79
APP 0.68 61 0 55 70 65
COMP 2.31 60 61 66 61 73
COORD 5.73 64 73 77 67 76
DET 9.58 91 93 96 96 96
FOCUS 0.04 100 100 0 100 100
OBJ 16.42 82 84 86 85 90
OBJ2 0.07 80 57 50 57 50
OBL 2.17 58 24 27 23 63
OBL2 0.07 50 0 0 0 67
OBL AG 0.43 40 96 96 92 92
POSS 2.88 80 83 82 82 79
QUANT 1.85 70 67 69 70 70
RELMOD 1.78 50 78 67 78 73
SUBJ 14.74 80 81 83 85 85
TOPIC 0.46 85 87 96 96 89
TOPICREL 1.85 61 80 80 79 74
XCOMP 5.20 90 92 79 93 93
and 88.3% all GFs. A detailed breakdown by dependency is given in Table 7. The system
based on the retrained parser is nowmuch better able to identify oblique arguments and
overall preds-only accuracy has improved by 3.53% over the original Bikel experiment
and 3.31% over Charniak?s parser, even though Charniak?s parser performs more than
2% better on the tree-based scores in Table 5 and even though the retrained parser drops
0.79% against the original Bikel parser on the tree-based scores.18
Inspection of the results broken down by grammatical function (Table 7) for the
preds-only evaluation against the DCU 105 shows that just over one third of all depen-
dency triples in the gold standard are adjuncts. SUBJ(ects) and OBJ(ects) together make
up a further 30%.
18 The figures suggest that retraining Charniak?s parser to retain Penn-II functional tags is likely to produce
even better dependency scores than those achieved by Bikel?s retrained parser.
101
Computational Linguistics Volume 34, Number 1
Table 7 shows that the treebank-based LFG system using Collins?s Models 3 is
unable to identify APP(osition). This is due to Collins?s treatment of punctuation and
the fact that punctuation is often required to reliably identify apposition.19 None of
the original history-based parsers produced trees which enabled the annotation algo-
rithm to identify second oblique dependencies (OBL2), and they generally performed
considerably worse than Parent-PCFG when identifying OBL(ique) dependencies. This
is because the automatic f-structure annotation algorithm is cautious to the point of
undergeneralization when identifying oblique arguments. In many cases, the algorithm
relies on the presence of, for example, a -CLR Penn-II functional label (indicating that the
phrase is closely related to the verb), and the history-based (Collins M3, Charniak, and
Bikel) parsers do not produce these labels, whereas Parent-PCFG (as well as PCFG) are
trained to retain Penn-II functional labels. Parent-PCFG, by contrast, performs poorly
for oblique agents (OBL AG, agentive by-phrases in passive constructions), whereas the
history-based parsers are able to identify these with considerable accuracy. This is be-
cause Parent-PCFG often erroneously finds oblique agents, even when the preposition
is not by, as it never has enough context in which to distinguish by prepositional phrases
from other PPs. The history-based parsers produce trees from which the automatic
f-structure annotation algorithm can better identify RELMOD and TOPICREL dependen-
cies than Parent-PCFG. This, in turn, leads to improved long distance dependency
resolution which improves overall accuracy.
The DCU 105 development set is too small to support reliable statistical significance
testing of the performance ranking of the six treebank-based LFG parsing systems. In
order to carry out significance testing to select the best treebank-based LFG parsing
system for comparative evaluation against the hand-crafted deep XLE and RASP re-
sources, we move to a larger dependency-based evaluation data set: the gold-standard
dependency bank automatically generated fromWSJ Section 22.
4.3 Evaluation against WSJ Section 22 Dependencies
In an experimental setup similar to that of Hockenmaier and Steedman (2002),20 we
evaluate each parser against a large automatically generated gold standard. The gold-
standard dependency bank is automatically generated by annotating the original 1,700
treebank trees from WSJ Section 22 of the Penn-II Treebank with our f-structure an-
notation algorithm. We then evaluate the f-structures generated from the tree output
of the six parsers trained on Sections 02 to 21 resulting from parsing the Section 22
strings against the automatically produced f-structures for the original Section 22 Penn-II
treebank trees. The results are given in Table 8.
Compared to Table 6 for the DCU 105 gold standard, most scores are up, particularly
so for the history-based parsers. This trend is possibly due to the fact that the WSJ
19 The annotation algorithm relies on Penn-II-style punctuation patterns where an NP apposition follows a
nominal head separated by a comma ([NP [NP Bush ] , [NP the president ] ]), all three sisters of the same
mother node, while the trees produced by Collins?s parser attach the comma low in the tree ([NP [NP
Bush,] [NP the president ] ]). Although it would be trivial to carry out a tree transformation on the Collins
output to raise the punctuation to the expected level, we have not done this here.
20 This corresponds to experiments where the original Penn-II Section 23 treebank trees are automatically
converted into CCG derivations, which are then used as a gold standard to evaluate the CCG parser
trained on Sections 02?21. A similar methodology is used for the evaluation of treebank-based HPSG
resources (Miyao, Ninomiya, and Tsujii 2003) where Penn-II treebank trees are automatically annotated
with HPSG typed-feature structure information.
102
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 8
Results of dependency-based evaluation against the automatically generated gold standard for
WSJ Section 22.
Parser Preds only All GFs
f-score (%) f-score (%)
PCFG 70.76 80.44
Parent-PCFG 74.92 83.04
Collins M3 79.30 86.00
Charniak 81.35 86.96
Bikel 81.40 87.00
Bikel+Tags 83.06 87.63
Section 22 gold standard is generated automatically from the original ?perfect? Penn-II
treebank trees using the automatic f-structure annotation algorithm, whereas the DCU
105 has been created manually without regard as to whether or not the f-structure
annotation algorithm could ever generate the f-structures, even given the ?perfect?
trees.
The LFG system based on Bikel?s retrained parser achieves the highest f-score of
83.06% preds-only and 87.63% all GFs. Parent-PCFG achieves an f-score of 74.92%
preds-only and 83.04% all GFs. Table 9 provides a breakdown by feature of the preds-
only evaluation.
Table 9 shows that, once again, the automatic f-structure annotation algorithm is
not able to identify any cases of apposition from the output of Collins?s Model 3 parser.
Apart from Bikel?s retrained parser, none of the history-based parsers are able to identify
Table 9
Breakdown by dependency of results of preds-only evaluation against the automatically
generated Section 22 gold standard.
Dep. Percent of total F-score (%)
Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags
ADJUNCT 33.77 70 75 78 78 80
APP 0.74 61 0 77 77 71
COMP 1.35 60 72 70 70 80
COORD 5.11 74 78 82 82 81
DET 10.72 88 91 92 92 91
FOCUS 0.02 27 67 88 88 71
OBJ 16.17 80 85 87 87 88
OBJ2 0.07 15 30 32 32 71
OBL 1.92 50 19 21 21 73
OBL2 0.07 47 3 3 3 69
OBL AG 0.31 50 90 89 89 85
POSS 2.47 86 91 91 91 91
QUANT 2.12 89 89 93 93 92
RELMOD 1.84 51 71 72 72 69
SUBJ 15.45 75 80 81 81 82
TOPIC 0.44 81 85 84 84 76
TOPICREL 1.82 61 72 74 75 69
XCOMP 5.62 81 87 81 81 88
103
Computational Linguistics Volume 34, Number 1
Figure 12
Approximate Randomization Test for statistical significance testing.
OBJ2, OBL or OBL2 dependencies very well, although Parent-PCFG is able to produce
trees from which it is easier to identify obliques (OBL), because of the Penn-II functional
-CLR label. The automatic annotation algorithm is unable to identify RELMOD depen-
dencies satisfactorily from the trees produced by parsing with Parent-PCFG, although
the history-based parsers score reasonably well for this function. Whereas Charniak?s
parser is able to identify some dependencies better than Bikel?s retrained parser, overall
the system based on Bikel?s retrained parser performs better when evaluating against
the dependencies in WSJ Section 22.
In order to determine whether the results are statistically significant, we use the Ap-
proximate Randomization Test (Noreen 1989).21 This test is an example of a computer-
intensive statistical hypothesis test. Such tests are designed to assess result differences
with respect to a test statistic in cases where the sampling distribution of the test statistic
is unknown. Comparative evaluations of outputs of parsing systems according to test
statistics, such as differences in f-score, are examples of this situation. The test statistics
are computed by accumulating certain count variables over the sentences in the test
set. In the case of f-score, variable tuples consisting of the number of dependency-
relations in the parse for the system translation, the number of dependency-relations
in the parse for the reference translation, and the number of matching dependency-
relations between system and reference parse, are accumulated over the test set.
Under the null hypothesis, the compared systems are not different, thus any vari-
able tuple produced by one of the systems could just as likely have been produced by
the other system. So shuffling the variable tuples between the two systems with equal
probability, and recomputing the test statistic, creates an approximate distribution of
the test statistic under the null hypothesis. For a test set of S sentences there are 2S
different ways to shuffle the variable tuples between the two systems. Approximate
randomization produces shuffles by random assignments instead of evaluating all 2S
possible assignments. Significance levels are computed as the percentage of trials where
the pseudo statistic, that is the test statistic computed on the shuffled data, is greater
than or equal to the actual statistic, that is the test statistic computed on the test data. A
sketch of an algorithm for approximate randomization testing is given in Figure 12.
21 Applications of this test to natural language processing problems can be found in Chinchor et al (1993)
and Yeh (2000).
104
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 10
Comparing parsers evaluated against Section 22 dependencies (preds-only): p-values for
approximate randomization test for 10,000,000 randomizations.
PCFG Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags
PCFG - - - - - -
Parent-PCFG <.0001 - - - - -
Collins M3 <.0001 <.0001 - - - -
Charniak <.0001 <.0001 <.0001 - - -
Bikel <.0001 <.0001 <.0001 .0003 - -
Bikel+Tags <.0001 <.0001 <.0001 <.0001 <.0001 -
Table 10 gives the p-values (the smallest fixed level at which the null hypothesis can
be rejected) for comparing each parser against all of the other parsers. We test for sig-
nificance at the 95% level. Because we are doing a pairwise comparison of six systems,
giving 15 comparisons, the p-value needs to be below .0034 for there to be a significant
difference at the 95% level.22 For each parser, the values in the row corresponding to
that parser represent the p-values for those parsers that achieve a lower f-score than
that parser. This shows that the system based on Bikel?s retrained parser is significantly
better than those based on the other parsers with a statistical significance of >95%. For
the XLE and RASP comparisons, we will use the f-structure-annotation algorithm and
Bikel retrained-based LFG system.
5. Cross-Formalism Comparison of Treebank-Induced and Hand-Crafted Grammars
From the experiments in Section 4, we choose the treebank-based LFG system using
the retrained version of Bikel?s parser (which retains Penn-II functional tag labels) to
compare against parsing systems using deep, hand-crafted, constraint-based grammars
at the level of dependencies. We report on two experiments. In the first experiment
(Section 5.1), we evaluate the f-structure annotation algorithm and Bikel retrained
parser-based LFG system against the hand-crafted, wide-coverage LFG and XLE pars-
ing system (Riezler et al 2002; Kaplan et al 2004) on the PARC 700 Dependency Bank
(King et al 2003). In the second experiment (Section 5.2), we evaluate against the hand-
crafted, wide-coverage unification grammar and RASP parsing system of Carroll and
Briscoe (2002) on the CBS 500 Dependency Bank (Carroll, Briscoe, and Sanfilippo 1998).
5.1 Evaluation against PARC 700
The PARC 700 Dependency Bank (King et al 2003) provides dependency relations
(including LDD relations) for 700 sentences randomly selected from WSJ Section 23 of
the Penn-II Treebank. In order to evaluate the parsers, we follow the experimental setup
of Kaplan et al (2004) with a split of 560 dependency structures for the test set and 140
for the development set. The set of features (Table 12, later in this article) evaluated
in the experiment form a proper superset of preds-only, but a proper subset of all
22 Based on Cohen (1995, p. 190): ?e ? 1 ? (1 ? ?c )m, where m is the number of pairwise comparisons, ?e is
the experiment-wise error, and ?c is the per-comparison error.
105
Computational Linguistics Volume 34, Number 1
Figure 13
PARC 700 conversion software.
grammatical functions (preds-only ? PARC ? all GFs). This feature set was selected in
Kaplan et al because the features carry important semantic information. There are sys-
tematic differences between the PARC 700 dependencies and the f-structures generated
in our approach as regards feature geometry, feature nomenclature, and the treatment
of named entities. In order to evaluate against the PARC 700 test set, we automatically
map the f-structures produced by our parsers to a format similar to that of the PARC
700 Dependency Bank. This is done with conversion software in a post-processing stage
on the f-structure annotated trees (Figure 13).
The conversion software is developed on the 140-sentence development set of the
PARC 700, except for the Multi-Word Expressions section. Following the experimental
setup of Kaplan et al (2004), we mark up multi-word expression predicates based on
the gold-standard PARC 700 Dependency Bank.
Multi-Word Expressions The f-structure annotation algorithm analyzes the internal
structure of all noun phrases fully. In Figure 14, for example, BT is analyzed as
an ADJUNCT modifier of the head securities, whereas PARC 700 analyzes this and
other (more complex) named entities as multi-word expression predicates. The
conversion software transforms the output of the f-structure annotation algorithm
into the multi-word expression predicate format.
Feature Geometry In constructions such as Figure 2, the f-structure annotation algo-
rithm analyzes say as the main PRED with what is said as the value of a COMP
argument. In the PARC 700, these constructions are analyzed in such a way that
what is said/reported provides the top level f-structure whereas other material
(who reported, etc.) is analyzed in terms of ADJUNCTs modifying the top level
f-structure. A further systematic structural divergence is provided by the analysis
Figure 14
Named entity and OBL AG feature geometry mapping.
106
Cahill et al Statistical Parsing Using Automatic Dependency Structures
of passive oblique agent constructions (Figure 14): The f-structure annotation
algorithm generates a complex internal analysis of the oblique agent PP, whereas
the PARC analysis encodes a flat representation. The conversion software adjusts
the output of the f-structure annotation algorithm to the PARC-style encoding of
linguistic information.
Feature Nomenclature There are a number of systematic differences between feature
names used by the automatic annotation algorithm and PARC 700: For example,
DET is DET FORM in the PARC 700, COORD is CONJ, FOCUS is FOCUS INT. Nomen-
clature differences are treated in terms of a simple relabeling by the conversion
software.
Additional Features A number of features in the PARC 700 are not produced by the au-
tomatic annotation algorithm. These include: AQUANT for adjectival quantifiers,
MOD for NP-internal modifiers, and STMT TYPE for statement type (declarative,
interrogative, etc.). Additional features (and their values) are automatically gen-
erated by the mapping software, using categorial, configurational, and already
produced f-structure annotation information, extending the original annotation
algorithm.
XCOMP Flattening The automatic annotation algorithm treats both auxiliary and
modal verb constructions in terms of hierarchically cascading XCOMPs, whereas
in PARC 700 the temporal and aspectual information expressed by auxiliary verbs
is represented in terms of a flat analysis and features (Figure 15). The conversion
software automatically flattens the f-structures produced by the automatic anno-
tation algorithm into the PARC-style encoding.
For full details of the mapping, see Burke et al (2004).
In our parsing experiments, we used the most up-to-date version of the hand-
crafted, wide-coverage, deep LFG resources and XLE parsing system with improved
results over those reported in Kaplan et al (2004): This latest version achieves 80.55%
f-score, a 0.95 percentage point improvement on the previous 79.6%. The XLE parsing
system combines a large-scale, hand-crafted LFG for English and a statistical disam-
biguation component to choose the most likely analysis among those returned by
the symbolic parser. The statistical component is a log-linear model trained on 10,000
partially labeled structures from the WSJ. The results of the parsing experiments are
presented in Table 11. We also include a figure for the upper bound of each system.23
Using Bikel?s retrained parser, the treebank-based LFG system achieves an f-score of
82.73%, and the hand-crafted grammar and XLE-based system achieves an f-score
of 80.55%. The approximate randomization test produced a p-value of .0054 for this
pairwise comparison, showing that this result difference is statistically significant at
the 95% level. Evaluation results on a reannotated version (Briscoe and Carroll 2006) of
the PARC 700 Dependency Bank were recently published in Clark and Curran (2007),
reporting f-scores of 81.9% for the CCG parser, and 76.3% for RASP. As Briscoe and
23 The upper bound for the treebank-based LFG system is determined by taking the original Penn-II WSJ
Section 23 trees corresponding to the PARC 700 strings, automatically annotating them with the
f-structure annotation algorithm, and evaluating the f-structures against the PARC 700 dependencies. The
upper bound for the XLE system is determined by selecting the XLE parse that scores best against the
PARC 700 dependencies for each of the PARC 700 strings. It is interesting to note that the upper bound
for the treebank-based system is only 1.18 percentage points higher than that for the XLE system. Apart
from the two different methods for establishing the upper bounds, this is most likely due to the fact that
the mapping required for evaluating the treebank-based LFG system against PARC 700 is lossy (cf. the
discussion in Section 6).
107
Computational Linguistics Volume 34, Number 1
Figure 15
DCU 105 and PARC 700 analyses for the sentence Unlike 1987, interest rates have been falling
this year.
Carroll point out, these evaluations are not directly comparable with the Kaplan et al
(2004) style evaluation against the original PARC 700 Dependency Bank, because the
annotation schemes are different. Kaplan et al and our experiments use a fine-grained
feature set of 34 features (Table 12), while the Briscoe and Carroll scheme uses 17
features.
A breakdown by dependency relation for each system is given in Table 12. The
treebank-induced grammar system can better identify DET FORM, SUBORD FORM, and
Table 11
Results of evaluation against the PARC 700 Dependency Bank following the experimental setup
of Kaplan et al (2004).
Bikel+Tags XLE p-Value
F-score 82.73 80.55 .0054
Upper bound 86.83 85.65 -
108
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 12
Breakdown by dependency relation of results of evaluation against PARC 700.
Dep. Percent of deps. F-score (%)
Bikel+Tags XLE
ADEGREE 6.17 80 82
ADJUNCT 14.32 68 66
AQUANT 0.06 78 61
COMP 1.23 80 74
CONJ 2.64 73 69
COORD FORM 1.20 83 90
DET FORM 4.61 97 91
FOCUS 0.02 0 36
MOD 2.74 74 67
NUM 19.82 91 89
NUMBER 1.42 89 83
NUMBER TYPE 2.10 94 86
OBJ 8.92 87 78
OBJ THETA 0.05 43 31
OBL 0.83 55 69
OBL AG 0.22 82 76
OBL COMPAR 0.07 38 56
PASSIVE 1.14 80 88
PCASE 0.25 79 68
PERF 0.41 89 90
POSS 0.98 88 80
PRECOORD FORM 0.03 0 91
PROG 0.97 89 81
PRON FORM 2.54 92 94
PRON INT 0.03 0 33
PRON REL 0.57 74 72
PROPER 3.56 83 93
PRT FORM 0.22 80 41
QUANT 0.34 77 80
STMT TYPE 5.23 87 80
SUBJ 8.51 78 78
SUBORD FORM 0.93 47 42
TENSE 5.02 95 90
TOPIC REL 0.57 56 73
XCOMP 2.29 80 78
PRT FORM dependencies24 and achieves higher f-scores for OBJ and POSS. However, the
hand-crafted parsing system can better identify FOCUS, OBL(ique) arguments, PRECO-
ORD FORM and TOPICREL relations.
5.2 Evaluation against CBS 500
We also compare the hand-crafted, deep, probabilistic unification grammar-based RASP
parsing system of Carroll and Briscoe (2002) to our treebank- and retrained Bikel
24 DET FORM, SUBORD FORM, and PRT FORM (and in general X FORM) dependencies record (semantically
relevant) surface forms in f-structure for X-type closed class categories.
109
Computational Linguistics Volume 34, Number 1
Figure 16
CBS 500 conversion software.
parser-based LFG system. The RASP parsing system is a domain-independent, robust
statistical parsing system for English, based on a hand-written, feature-based unification
grammar. A probabilistic parse selection model conditioned on the structural parse
context, degree of support for a subanalysis in the parse forest, and lexical informa-
tion (when available) chooses the most likely parses. For this experiment, we evaluate
against the CBS 500,25 developed by Carroll, Briscoe, and Sanfilippo (1998) in order to
evaluate a precursor of the RASP parsing resources. The CBS 500 contains dependency
structures (including some long distance dependencies26) for 500 sentences chosen at
random from the SUSANNE corpus (Sampson 1995), but subject to the constraint that
they are parsable by the parser in Carroll, Briscoe, and Sanfilippo. Aswith the PARC 700,
there are systematic differences between the f-structures produced by our methodology
and the dependency structures of the CBS 500. In order to be able to evaluate against
the CBS 500, we automatically map our f-structures into a format similar to theirs. We
did not split the data into a heldout and a test set when developing the mapping, so
that a comparison could be made with other systems that report evaluations against
the CBS 500. The following CBS 500-style grammatical relations are produced from the
f-structure in Figure 1:
(ncsubj sign U.N.)
(dobj sign treaty)
Some mapping is carried out (as in the evaluation against the PARC 700) on the f-
structure annotated trees, and the remaining mapping is carried out on the f-structures
(Figure 16). As with the PARC 700 mapping, all mappings are carried out automatically.
The following phenomena were dealt with on the f-structure annotated trees:
Auxiliary verbs (xcomp flattening) XCOMPS were flattened to promote the main verb
to the top level, while maintaining a list of auxiliary and modal verbs and their
relation to one another.
Treatment of topicalized sentences The predicate of the topicalized sentence became
the main predicate and any other top level material became an adjunct.
Multi-word expressions Multi-word expressions (such as according to) were not
marked up in the parser input, but captured in the annotated trees and the
annotations adjusted accordingly.
Treatment of the verbs be and become Our automatic annotation algorithm does not
treat the verbs be and become differently from any other verbs when they are used
transitively. This analysis conflicted with the CBS 500 analysis, so was changed to
match theirs.
25 This was downloaded from http://www.informatics.susx.ac.uk/research/nlp/carroll/greval.html.
26 The long distance dependencies include passive, wh-less relative clauses, control verbs, and so forth.
110
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Table 13
Results of dependency evaluation against the CBS 500 (Carroll, Briscoe, and Sanfillipo 1998).
Bikel+Tags RASP p-Value
F-score 80.23 76.57 <.0001
The following are the main mappings carried out on the f-structures:
Encoding of Passive We treat passive as a feature in our automatic f-structure annota-
tion algorithm, whereas the CBS 500 triples encode this information indirectly.
Objects of Prepositional Phrases No dependency was generated for these objects, as
there was no corresponding dependency in the CBS 500 analyses.
Nomenclature Differences There were some trivial mappings to account for differ-
ences in nomenclature, for example OBL in our analyses became IOBJ in the
mapped dependencies.
Encoding of wh-less relative clauses These are encoded by means of reentrancies in
f-structure, but were encoded in a more indirect way in the mapped dependencies
to match the CBS 500 annotation format.
To carry out the experiments, we POS-tagged the tokenized CBS 500 sentences with
the MXPOST tagger (Ratnaparkhi 1996) and parsed the tag sequences with our Penn-II
and Bikel retrained-based LFG system. We use the evaluation software of Carroll,
Briscoe, and Sanfilippo (1998)27 to evaluate the grammatical relations produced by each
parser. The results are given in Table 13.
Our LFG system based on Bikel?s retrained parser achieves an f-score of 80.23%,
whereas the hand-crafted RASP grammar and parser achieves an f-score of 76.57%.
Crouch et al (2002) report that their XLE system achieves an f-score of 76.1% for the
same experiment. A detailed breakdown by dependency is given in Table 14. The
LFG system based on Bikel?s retrained parser is able to better identify MOD(ifier) de-
pendency relations, ARG MOD (the relation between a head and a semantic argument
which is syntactically realized as a modifier, for example by-phrases), IOBJ (indirect
object) and AUXiliary relations. RASP is able to better identify XSUBJ (clausal subjects
controlled from without), CSUBJ (clausal subjects), and COMP (clausal complement)
relations. Again we use the Approximate Randomization Test to test the parsing results
for statistical significance. The p-value for the test comparing our system using Bikel?s
retrained parser against RASP is <.0001. The treebank-based LFG system using Bikel?s
retrained parser is significantly better than the hand-crafted, deep, unification grammar-
based RASP parsing system with a statistical significance of >95%.
6. Discussion and Related Work
At the moment, we can only speculate as to why our treebank-based LFG resources
outperform the hand-crafted XLE and RASP grammars.
In Section 4, we observed that the treebank-induced LFG resources have consid-
erably wider coverage (>99.9% measured in terms of complete spanning parse) than
27 This was downloaded from http://www.informatics.susx.ac.uk/research/nlp/carroll/greval.html.
111
Computational Linguistics Volume 34, Number 1
Table 14
Breakdown by grammatical relation for results of evaluation against CBS 500.
Dep. Percent of deps. F-score (%)
Bikel+Tags RASP
DEPENDANT 100.00 80.23 76.57
MOD 48.96 80.30 75.29
NCMOD 30.42 85.37 72.98
XMOD 1.60 70.05 55.88
CMOD 2.61 75.60 53.08
DETMOD 14.06 95.85 91.97
ARG MOD 0.51 80.00 64.52
ARG 43.70 78.28 77.57
SUBJ 13.10 79.84 83.57
NCSUBJ 12.99 87.84 84.32
XSUBJ 0.06 0.00 88.89
CSUBJ 0.04 0.00 22.22
SUBJ OR DOBJ 18.22 81.21 83.84
COMP 12.38 76.73 71.87
OBJ 7.34 76.05 69.53
DOBJ 5.11 84.55 84.57
OBJ2 0.25 48.00 43.84
IOBJ 1.98 59.04 47.60
CLAUSAL 5.04 77.74 75.37
XCOMP 4.03 80.00 84.11
CCOMP 1.01 69.61 75.14
AUX 4.76 94.94 88.27
CONJ 2.06 68.84 69.09
the hand-crafted grammars (?80% for XLE and RASP grammars on unseen treebank
text). Both XLE and RASP use a number of (largest) fragment-combining techniques
to achieve full coverage. If coverage is a significant component in the performance
difference observed between the hand-crafted and treebank-induced resources, then
it is reasonable to expect that the performance difference is more pronounced with
increasing sentence length (with shorter sentences being simpler and more likely to
be within the coverage of the hand-crafted grammars). In other words, we expect the
hand-crafted, deep, precision grammars to do better on shorter sentences (more likely to
be within their coverage), whereas the treebank-induced grammars should show better
performance on longer strings (less likely to be within the coverage of the hand-crafted
grammars).
In order to test this hypothesis, we carried out a number of experiments:
First, we plotted the sentence length distribution for the 560 PARC 700 test sen-
tences and the 500 CBS 500 sentences (Figures 17 and 18). Both gold standards are
approximately normally distributed, with the CBS 500 distribution possibly showing
the effects of being chosen subject to the constraint that the strings are parsable by the
parser in Carroll, Briscoe, and Sanfilippo (1998). For each case we use the mean, ?, and
two standard deviations, 2?, to the left and right of themean to exclude sentence lengths
not supported by sufficient observations: For PARC 700, ? = 23.27, ?? 2? = 2.17, and
?+ 2? = 44.36; for CBS 500, ? = 17.27, ?? 2? = 1.59, and ?+ 2? = 32.96. Both the
PARC 700 and the CB 500 distributions are positively skewed. For the PARC 700, ?? 2?
is actually outside the observed data range, whereas for CB 500, ?? 2? almost coincides
112
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 17
Distribution of sentence frequency by sentence length in the PARC 700 test set with Bezier
interpolation. Vertical lines mark two standard deviations from the mean.
with the left border. It is therefore useful to further constrain the ?2? range by a
sentence count threshold of ? 5.28 This results in a sentence length range of 4?41 for
PARC 700 and 4?32 for CBS 500.
Second, in order to test whether fragment parses increase with sentence length, we
plotted the percentage of fragment parses over sentence length for the XLE parses of the
560-sentence test set of the PARC 700 (we did not do this for the CBS 500 as its strings
are selected to be fully parsable by RASP). Figure 19 shows that the number of fragment
parses tends to increase with sentence length.
Third, we plotted the average dependency f-score for the hand-crafted and the
treebank-induced resources against sentence lengths. Figure 20 shows the results for
PARC 700, Figure 21 for CBS 500.
In both cases, contrary to our (perhaps somewhat naive) assumption, the graphs
show that the treebank-induced resources outperform the hand-crafted resources
within (most of) the 4?41 and 4?32 sentence length bounds, with the results for the very
short and the very long strings outside those bounds not being supported by sufficient
data points.
In the parsing literature, results are often also provided for strings with lengths
?40. Below we give those results and statistical significance testing for the PARC 700
and CBS 500 (Tables 15 and 16). The results show that the Bikel retrained?based LFG
system achieves a higher dependency f-score on sentences of length ?40 than on all
sentences, whereas the XLE system achieves a slightly lower score on sentences of
length ?40. The Bikel-retrained system achieves an f-score of 83.18%, a statistically
28 Note that because the distributions in Figures 17 and 18 are Bezier interpolated, the constraint does not
guarantee that every sentence length within the range occurs five or more times.
113
Computational Linguistics Volume 34, Number 1
Figure 18
Distribution of sentence frequency by sentence length in the CB 500 test set with Bezier
interpolation. Vertical lines mark two standard deviations from the mean.
Figure 19
Percentage of fragment sentences for XLE parsing system per sentence length with Bezier
interpolation.
significant improvement of 2.67 percentage points over the XLE system on sentences of
length?40. Against the CBS 500, Bikel?s retrained system achieves a weighted f-score of
82.58%, a statistically significant improvement of 3.87 percentage points over the RASP
system which achieves a weighted f-score of 78.81% on sentences of length ?40.
114
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Figure 20
Average f-score by sentence length for PARC 700 test set with Bezier interpolation.
Figure 21
Average f-score by sentence length for CB 500 test set with Bezier interpolation.
Finally, we test whether the strict preds-only dependency evaluation has an ef-
fect on the results for the PARC 700 experiments. Recall that following Kaplan et al
(2004) for the PARC 700 evaluation we used a set of semantically relevant grammatical
functions that is a superset of preds-only and a subset of all-GFs. A preds-only based
evaluation is ?stricter? and tends to produce lower scores as it directly reflects the effects
115
Computational Linguistics Volume 34, Number 1
Table 15
Evaluation and significance testing of sentences length ?40 against the PARC 700.
All sentence lengths Length ?40
f-score f-score
Bikel + Tags 82.73 83.18
XLE 80.55 80.51
p-value .0054 .0010
Table 16
Evaluation and significance testing of sentences length ?40 against the CBS 500.
All sentence lengths Length ?40
f-score f-score
Bikel + Tags 80.23 82.58
RASP 76.57 78.81
p-value <.0001 <.0001
Table 17
Preds-only evaluation against the PARC 700 Dependency Bank.
All GFs Preds only
f-score f-score
Bikel + Tags 82.73 77.40
XLE 80.55 74.31
of predicate?argument/adjunct misattachments in the resulting dependency represen-
tations (while local functions such as NUM(ber), for example, can score properly even
if the local predicate is misattached). Table 1729 below gives the results for preds-only
evaluation30 on the PARC 700 for all sentence lengths. The results show that the Bikel-
retrained treebank-based LFG resource achieves an f-score of 77.40%, 5.33 percentage
points lower than the score for all the PARC dependencies. The XLE system achieves
an f-score of 74.31%, 6.24 percentage points lower than the score for all the PARC
dependencies and a 3.09 percentage point drop against the results obtained by the
Bikel-retrained treebank-based LFG resources. The performance of the Bikel retrained?
based LFG system suffers less than the XLE system when preds-only dependencies are
evaluated.
It is important to note that in our f-structure annotation algorithm and treebank-
based LFG parsing architectures, we do not claim to provide fully adequate statistical
models. It is well known (Abney 1997) that PCFG- or history-based parser approxima-
tions to general constraint-based grammars can yield inconsistent probability models
29 We do not include a p-value here as the breakdown by function per sentence was not available to us for
the XLE data.
30 The dependency relations we include in preds-only evaluation are: ADJUNCT, AQUANT, COMP, CONJ,
COORD FORM, DET FORM, FOCUS INT, MOD, NUMBER, OBJ, OBJ THETA, OBL, OBL AG, OBL COMPAR,
POSS, PRON INT, PRON REL, PRT FORM, QUANT, SUBJ, SUBORD FORM, TOPIC REL, XCOMP.
116
Cahill et al Statistical Parsing Using Automatic Dependency Structures
due to loss of probability mass: The parser successfully returns the highest ranked
parse tree but the constraint solver cannot resolve the f-structure equations and the
probability mass associated with that tree is lost. Research on adequate probability
models for constraint-based grammars is important (Bouma, van Noord, and Malouf
2000; Miyao and Tsujii 2002; Riezler et al 2002; Clark and Curran 2004). In this context,
it is interesting to compare parser performance against upper bounds. For the PARC
700 evaluation, the upper bound for the XLE-based resources is 85.65%, against 86.83%
for the treebank-based LFG resources. XLE-based parsing currently achieves 94.05%
(f-score 80.55%) of its upper bound using a discriminative disambiguation method,
whereas the treebank-based LFG resource achieves 95.28% (f-score 82.73%) of its upper
bound.
Although this seems to indicate that the two disambiguationmodels achieve similar
results, the figures are actually very difficult to compare. In the case of the XLE, the
upper bound is established by unpacking all parses for a string and scoring the best
match against the gold standard (rather than letting the probability model select a
parse). By contrast, in the case of the treebank-based LFG resources, we use the original
?perfect? Penn-II treebank trees (rather than the trees produced by the parser), auto-
matically annotate those trees with the f-structure annotation algorithm, and score the
results against the PARC 700 (it is not feasible to generate all parses for a string, there
are simply too many for treebank-induced resources). The upper bound computed in
this fashion for the treebank-based LFG resource (86.83%) is relatively low. The main
reason is that the automatic mapping required to relate the f-structures generated by the
treebank-based LFG resources to the PARC 700 dependencies is lossy. This is indicated
by comparing the upper bound for the treebank-based LFG resources for the PARC 700
against the upper bound for the DCU 105 gold standard, where little or no mapping
(apart from the feature-structure to dependency-triple conversion) is required: Scoring
the f-structure annotations for the original treebank trees results in 86.83% against PARC
700 versus 96.80% against DCU 105.
Our discussion shows how delicate it can be to compare parsing systems and
their disambiguation models. Ultimately what is required is an evaluation strategy that
separates out and clearly distinguishes between the grammar, parsing algorithm, and
disambiguation model and is capable of assessing different combinations of these core
components. Of course, this will not always be possible andmoving towards it is part of
a much more extended research agenda, well beyond the scope of the research reported
in the present article. Our approach, and previous approaches, evaluate systems at
the highest level of granularity, that of the complete package: the combined grammar-
parser-disambiguation model. The results show that machine-learning-based resources
can outperform deep, state-of-the-art hand-crafted resources with respect to the quality
of dependencies generated.
Treebank-based, deep and wide-coverage constraint-based grammar acquisition
has become an important research topic: Starting with the seminal TAG-based work
of Xia (1999), there are now also HPSG-, CCG- and LFG-based approaches. Miyao,
Ninomiya, and Tsujii (2003) and Tsuruoka, Miyao, and Tsujii (2004) present re-
search on inducing Penn-II treebank-based HPSGs with log-linear probability models.
Hockenmaier and Steedman (2002) and Hockenmaier (2003) provide treebank-induced
CCG-basedmodels including LDD resolution. It would be interesting to conduct a com-
parative evaluation involving treebank-based HPSG, CCG, and LFG resources against
the PARC 700 and CBS 500 Dependency Banks to enable more comprehensive compar-
ison of machine-learning-based with hand-crafted, deep, wide-coverage resources such
as those used in the XLE or RASP parsing systems.
117
Computational Linguistics Volume 34, Number 1
Gildea and Hockenmaier (2003) and Miyao and Tsujii (2004) present PropBank
(Kingsbury, Palmer, andMarcus 2002)-based evaluations of their automatically induced
CCG and HPSG resources. PropBank-based evaluations provide valuable information
to rank parsing systems. Currently, however, PropBank-based evaluations are some-
what partial: They only represent (and hence score) verbal arguments and disregard a
raft of other semantically important dependencies (e.g., temporal and aspectual infor-
mation, dependencies within nominal clusters, etc.) as captured in the PARC 700 or CBS
500 Dependency Banks.31
7. Conclusions
Parser comparison is a non-trivial and time-consuming exercise. We extensively eval-
uated four machine-learning-based shallow parsers and two hand-crafted, wide-
coverage deep probabilistic parsers involving four gold-standard dependency banks,
using the Approximate Randomization Test (Noreen 1989) to test for statistical signifi-
cance. We used a sophisticated method for automatically producing deep dependency
relations from Penn-II-style CFG-trees (Cahill et al 2002b, 2004) to compare shallow
parser output at the level of dependency relation and revisit experiments carried out by
Preiss (2003) and Kaplan et al (2004).
Our main findings are twofold:
1. Using our CFG-tree-to-dependency annotation technology, together with treebank- and
machine-learning-based parsers, we can outperform hand-crafted, wide-coverage, deep,
probabilistic, constraint-based grammars and parsers.
This result is surprising for two reasons. First, it is established against two externally-
provided dependency banks (the PARC 700 and the CBS 500 gold standards), originally
designed using the hand-crafted, wide-coverage, probabilistic grammars for the XLE
(Riezler et al 2002) and RASP (Carroll and Briscoe 2002) parsing systems to evaluate
those systems. What is more, the SUSANNE corpus-based CBS 500 constitutes an
instance of domain variation for the Penn-II-trained LFG resources, likely to adversely
affect scores. Second, the treebank- and machine-learning-based LFG resources require
automatic mapping to relate f-structure output of the treebank-based parsing systems
to the representation format in the PARC 700 and CBS 500 Dependency Banks. These
mappings are partial and lossy: That is, they do not cover all of the systematic dif-
ferences between f-structure and dependency bank representations and introduce a
certain amount of error in what they are designed to capture, that is they both over-
and undergeneralize, again adversely affecting scores. Improvements of the mappings
should lead to a further improvement in the dependency scores.
2. Parser evaluation at the level of dependency representation still requires non-trivial
mappings between different dependency representation formats.
31 In a sense, PropBank does not yet provide a single agreed upon gold standard: Role information is
provided indirectly and an evaluation gold-standard has to be computed from this. In doing so, choices
have to be made as regards the representation of shared arguments, the analysis of coordinate structures,
and so forth, and it is not clear that the same choices are currently made for evaluations carried out by
different research groups.
118
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Earlier we criticized tree-based parser evaluation on the grounds that equally valid
different tree-typologies can be associated with strings, and identified this as a major
obstacle to fair and unbiased parser evaluation. Dependency-based parser evaluation,
as it turns out, is not entirely free of this criticism either: There are significant systematic
differences between the PARC 700 dependency and the CBS 500 dependency represen-
tations; there are significant systematic differences between the LFG f-structures gener-
ated by the hand-crafted, wide-coverage grammars of Riezler et al (2002) and Kaplan
et al (2004) and those of the treebank-induced and f-structure annotation algorithm
based resources of Cahill et al (2004). These differences require careful implementation
of mappings if parsers are not to be unduly penalized for systematic and motivated
differences at the level of dependency representation. By and large, these differences
are, however, less pronounced than differences on CFG tree representations, making
dependency-based parser evaluation a worthwhile and rewarding exercise.
Summarizing our results, we find that against the DCU 105 development set, the
treebank- and f-structure annotation algorithm-based LFG system using Bikel?s parser
retrained to retain Penn-II functional tag labels performs best, achieving f-scores of
82.92% preds-only and 88.3% all grammatical functions. Against the automatically
generated WSJ Section 22 Dependency Bank, the system using Bikel?s retrained parser
achieves the highest results, achieving f-scores of 83.06% preds-only and 87.861% all
GFs. This is statistically significantly better than all other parsers. In order to evaluate
against the PARC 700 and CBS 500 gold standards, we automaticallymap the dependen-
cies produced by our treebank-based LFG system into a format compatible with the gold
standards. Against the PARC 700 Dependency Bank, the treebank-based LFG system
using Bikel?s retrained parser achieves an f-score of 82.73%, a statistically significant
improvement of 2.18% against the most up-to-date results of the hand-crafted XLE-
based parsing resources. Against the CBS 500, the treebank-based LFG system using
Bikel?s retrained parser achieved the highest f-score of 80.23%, a statistically significant
improvement of 3.66 percentage points on the highest previously published results
for the same experiment with the hand-crafted RASP resources in Carroll and Briscoe
(2002).
Appendix A. Parser Parameter Settings
This section provides a complete list of the parameter settings used for each of the
parsers described in this article.
Parser Parameters
Collins Model 3 We used the Collins parser with its default settings of a
beam size of 10,000 and where the values of the following
flags are set to 1: punctuation-flag, distaflag, distvflag,
and npbflag. Input was pre-tagged using the MXPOST
POS tagger (Ratnaparkhi 1996). We parse the input file
with all three models and use the scripts provided to
merge the outputs into the final parser output file. Note
that this file has been cleaned of all -A functional tags and
trace nodes.
Charniak We used the parser dated August 2005 and ran the
parser using the data provided in the download on pre-
tokenized sentences of length ?200. Input was automati-
cally tagged by the parser.
119
Computational Linguistics Volume 34, Number 1
Bikel Emulation of We used version 0.9.9b of the parser trained on the file
Collins Model 2 of observed events made available on the downloads
page. We used the collins.properties file and a maximum
heap size of 1,500 MB. Input was pre-tagged using the
MXPOST POS tagger (Ratnaparkhi 1996).
Bikel + Functional Tags We used version 0.9.9b of the parser trained on a version
of Sections 02?21 of the Penn-II treebank where functional
tags were not ignored by the parser. We updated the de-
fault head-finding rules to deal with the new categories.
We also trained on all sentences from the training set
(the default collins.properties file is set to ignore trees of
more than 500 tokens). Input was pre-tagged using the
MXPOST POS tagger (Ratnaparkhi 1996) and the maxi-
mum heap size was 1,500 MB.
RASP Weused a file of parser output provided through personal
communication with John Carroll. (Tagging is carried out
automatically by the parser.)
XLE We used a file of parser output provided by the Natural
Language Theory and Technology group at the Palo Alto
Research Center. (Tagging is carried out automatically by
the parser.)
Acknowledgments
We are grateful to our anonymous reviewers
whose insightful comments have improved
the article significantly. We would like to
thank John Carroll for discussion and help
with reproducing the RASP parsing results;
Michael Collins, Eugene Charniak, Dan
Bikel, and Helmut Schmid for making their
parsing engines available; and Ron Kaplan
and the team at PARC for discussion,
feedback, and support. Part of the research
presented here has been supported by
Science Foundation Ireland grants
04/BR/CS0370 and 04/IN/I527, Enterprise
Ireland Basic Research Grant SC/2001/0186,
an Irish Research Council for Science,
Engineering and Technology Ph.D.
studentship, an IBM Ph.D. studentship and
support from IBM?s Centre for Advanced
Studies (CAS) in Dublin.
References
Abney, Stephen. 1997. Stochastic
attribute-value grammars. Computational
Linguistics, 23(4):597?618.
Alshawi, Hiyan and Stephen Pulman, 1992.
Ellipsis, Comparatives, and Generation,
chapter 13. The MIT Press, Cambridge,
MA.
Baldwin, Timothy, Emily Bender, Dan
Flickinger, Ara Kim, and Stephan Oepen.
2004. Road-testing the English Resource
Grammar over the British National
Corpus. In Proceedings of the Fourth
International Conference on Language
Resources and Evaluation (LREC 2004),
pages 2047?2050, Lisbon, Portugal.
Bikel, Dan. 2002. Design of a multi-lingual,
parallel-processing statistical parsing
engine. In Proceedings of HLT 2002,
pages 24?27, San Diego, CA.
Black, Ezra, Steven Abney, Dan Flickenger,
Claudia Gdaniec, Ralph Grishman, Philip
Harrison, Donald Hindle, Robert Ingria,
Fred Jelineck, Judith Klavans, Mark
Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek
Strzalkowski. 1991. A procedure for
quantitatively comparing the syntactic
coverage of english grammars. In
Proceedings of the Speech and Natural
Language Workshop, pages 306?311, Pacific
Grove, CA.
Bod, Rens. 2003. An efficient implementation
of a new DOP model. In Proceedings of the
Tenth Conference of the European Chapter of
the Association for Computational Linguistics
(EACL?03), pages 19?26, Budapest,
Hungary.
Bouma, Gosse, Gertjan van Noord, and
Robert Malouf. 2000. Alpino:
Wide-coverage computational analysis of
dutch. In Walter Daelemans, Khalil
Sima?an, Jorn Veenstra, and Jakub Zavrel,
editors, Computational Linguistics in The
Netherlands 2000. Rodopi, Amsterdam,
pages 45?59.
120
Cahill et al Statistical Parsing Using Automatic Dependency Structures
Bresnan, Joan. 2001. Lexical-Functional Syntax.
Blackwell, Oxford, England.
Briscoe, Edward and John Carroll. 1993.
Generalized probabilistic LR parsing of
natural language (corpora) with
unification-based grammars. Computational
Linguistics, 19(1):25?59.
Briscoe, Ted and John Carroll. 2006.
Evaluating the accuracy of an
unlexicalized statistical parser on the
PARC DepBank. In Proceedings of the
COLING/ACL 2006 Main Conference Poster
Sessions, pages 41?48, Sydney, Australia.
Briscoe, Edward, Claire Grover, Bran
Boguraev, and John Carroll. 1987. A
formalism and environment for the
development of a large grammar of
English. In Proceedings of the 10th
International Joint Conference on AI,
pages 703?708, Milan, Italy.
Burke, Michael. 2006. Automatic Treebank
Annotation for the Acquisition of LFG
Resources. Ph.D. thesis, School of
Computing, Dublin City University,
Dublin, Ireland.
Burke, Michael, Aoife Cahill, Ruth
O?Donovan, Josef van Genabith, and Andy
Way. 2004. Evaluation of an automatic
annotation algorithm against the PARC
700 Dependency Bank. In Proceedings of the
Ninth International Conference on LFG,
pages 101?121, Christchurch, New Zealand.
Butt, Miriam, Helge Dyvik, Tracy Holloway
King, Hiroshi Masuichi, and Christian
Rohrer. 2002. The Parallel Grammar
Project. In Proceedings of COLING 2002,
Workshop on Grammar Engineering and
Evaluation, pages 1?7, Taipei, Taiwan.
Cahill, Aoife. 2004. Parsing with Automatically
Acquired, Wide-Coverage, Robust,
Probabilistic LFG Approximations. Ph.D.
thesis, School of Computing, Dublin City
University, Dublin, Ireland.
Cahill, Aoife, Michael Burke, Ruth
O?Donovan, Josef van Genabith, and Andy
Way. 2004. Long-distance dependency
resolution in automatically acquired
wide-coverage PCFG-based LFG
approximations. In Proceedings of the 42nd
Annual Meeting of the Association for
Computational Linguistics, pages 320?327,
Barcelona, Spain.
Cahill, Aoife, Maire?ad McCarthy, Josef van
Genabith, and Andy Way. 2002a.
Automatic annotation of the Penn
Treebank with LFG f-structure
information. In Proceedings of the LREC
Workshop on Linguistic Knowledge
Acquisition and Representation: Bootstrapping
Annotated Language Data, pages 8?15, Las
Palmas, Canary Islands, Spain.
Cahill, Aoife, Maire?ad McCarthy, Josef van
Genabith, and Andy Way. 2002b. Parsing
with PCFGs and automatic f-structure
annotation. In Proceedings of the Seventh
International Conference on LFG,
pages 76?95, Palo Alto, CA.
Carroll, John and Edward Briscoe. 2002.
High precision extraction of grammatical
relations. In Proceedings of the 19th
International Conference on Computational
Linguistics (COLING), pages 134?140,
Taipei, Taiwan.
Carroll, John, Edward Briscoe, and Antonio
Sanfilippo. 1998. Parser evaluation: A
survey and new proposal. In Proceedings of
the International Conference on Language
Resources and Evaluation, pages 447?454,
Granada, Spain.
Carroll, John, Anette Frank, Dekang Lin,
Detlef Prescher, and Hans Uszkoreit,
editors. 2002. HLT Workhop: ?Beyond
PARSEVAL ? Towards improved evaluation
measures for parsing systems?, Las Palmas,
Canary Islands, Spain.
Charniak, Eugene. 1996. Tree-bank
grammars. In Proceedings of the
Thirteenth National Conference on
Artificial Intelligence, pages 1031?1036,
Menlo Park, CA.
Charniak, Eugene. 2000. A maximum
entropy inspired parser. In Proceedings
of the First Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics (NAACL 2000),
pages 132?139, Seattle, WA.
Chinchor, Nancy, Lynette Hirschman, and
David D. Lewis. 1993. Evaluating message
understanding systems: An analysis of the
Third Message Understanding Conference
(MUC-3). Computational Linguistics,
19(3):409?449.
Clark, Stephen and James Curran. 2004.
Parsing the WSJ using CCG and log-linear
models. In Proceedings of the 42nd Annual
Meeting of the Association for Computational
Linguistics (ACL-04), pages 104?111,
Barcelona, Spain.
Clark, Stephen and James Curran. 2007.
Formalism-independent parser evaluation
with CCG and DepBank. In Proceedings of
the 45th Annual Meeting of the Association
for Computational Linguistics (ACL 2007),
pages 248?255, Prague, Czech Republic
http://www.aclweb-org/anthology/P/
P07/P07-1032.
Clark, Stephen and Julia Hockenmaier. 2002.
Evaluating a wide-coverage CCG parser.
121
Computational Linguistics Volume 34, Number 1
In Proceedings of the LREC 2002 Beyond
Parseval Workshop, pages 60?66, Las
Palmas, Canary Islands, Spain.
Cohen, Paul R. 1995. Empirical Methods for
Artificial Intelligence. The MIT Press,
Cambridge, MA.
Collins, Michael. 1997. Three generative,
lexicalized models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics,
pages 16?23, Madrid, Spain.
Collins, Michael. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania,
Philadelphia, PA.
Crouch, Richard, Ron Kaplan, Tracy Holloway
King, and Stefan Riezler. 2002. A
comparison of evaluation metrics for a
broad coverage parser. In Proceedings of
the LREC Workshop: Beyond PARSEVAL?
Towards Improved Evaluation Measures for
Parsing Systems, pages 67?74, Las Palmas,
Canary Islands, Spain.
Dalrymple, Mary. 2001. Lexical-Functional
Grammar. London, Academic Press.
Dienes, Pe?ter and Amit Dubey. 2003.
Antecedent recovery: Experiments with a
trace tagger. In Proceedings of the 2003
Conference on Empirical Methods in Natural
Language Processing, pages 33?40, Sapporo,
Japan.
Eisele, Andreas and Jochen Do?rre. 1986. A
lexical functional grammar system in
Prolog. In Proceedings of the 11th International
Conference on Computational Linguistics
(COLING 1986), pages 551?553, Bonn.
Flickinger, Dan. 2000. On building a more
efficient grammar by exploiting types.
Natural Language Engineering, 6(1):15?28.
Gaizauskas, Rob. 1995. Investigations into
the grammar underlying the Penn
Treebank II. Research Memorandum
CS-95-25, Department of Computer
Science, Univeristy of Sheffield, UK.
Gildea, Daniel and Julia Hockenmaier.
2003. Identifying semantic roles using
combinatory categorial grammar.
In Proceedings of the 2003 Conference
on Empirical Methods in Natural
Language Processing, pages 57?64,
Sapporo, Japan.
Hockenmaier, Julia. 2003. Parsing with
generative models of predicate?argument
structure. In Proceedings of the 41st Annual
Conference of the Association for
Computational Linguistics, pages 359?366,
Sapporo, Japan.
Hockenmaier, Julia and Mark Steedman.
2002. Generative models for statistical
parsing with combinatory categorial
grammar. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics, pages 335?342,
Philadelphia, PA.
Johnson, Mark. 1999. PCFG models of
linguistic tree representations.
Computational Linguistics, 24(4):613?632.
Johnson, Mark. 2002. A simple
pattern-matching algorithm for
recovering empty nodes and their
antecedents. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics, pages 136?143,
Philadelphia, PA.
Kaplan, Ron and Joan Bresnan. 1982.
Lexical functional grammar, a formal
system for grammatical representation.
In Joan Bresnan, editor, The Mental
Representation of Grammatical Relations.
MIT Press, Cambridge, MA,
pages 173?281.
Kaplan, Ron, Stefan Riezler, Tracy Holloway
King, John T. Maxwell, Alexander
Vasserman, and Richard Crouch. 2004.
Speed and accuracy in shallow and deep
stochastic parsing. In Proceedings of the
Human Language Technology Conference and
the 4th Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics (HLT-NAACL?04),
pages 97?104, Boston, MA.
Kaplan, Ronald and Annie Zaenen. 1989.
Long-distance dependencies, constituent
structure and functional uncertainty. In
Mark Baltin and Anthony Kroch, editors,
Alternative Conceptions of Phrase Structure,
pages 17?42, University of Chicago Press,
Chicago.
King, Tracy Holloway, Richard Crouch,
Stefan Riezler, Mary Dalrymple, and Ron
Kaplan. 2003. The PARC 700 dependency
bank. In Proceedings of the EACL03: 4th
International Workshop on Linguistically
Interpreted Corpora (LINC-03), pages 1?8,
Budapest, Hungary.
Kingsbury, Paul, Martha Palmer, and
Mitch Marcus. 2002. Adding semantic
annotation to the Penn TreeBank. In
Proceedings of the Human Language
Technology Conference, pages 252?256,
San Diego, CA.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics,
pages 423?430, Sapporo, Japan.
Leech, Geoffrey and Roger Garside. 1991.
Running a grammar factory: On the
122
Cahill et al Statistical Parsing Using Automatic Dependency Structures
compilation of parsed corpora, or
?treebanks?. In Stig Johansson and
Anna-Brita Stenstro?m, editors, English
Computer Corpora: Selected Papers. Mouton
de Gruyter, Berlin, pages 15?32.
Levy, Roger and Christopher D. Manning.
2004. Deep dependencies from context-free
statistical parsers: Correcting the surface
dependency approximation. In Proceedings
of the 42nd Annual Meeting of the Association
for Computational Linguistics (ACL 2004),
pages 328?335, Barcelona, Spain.
Lin, Dekang. 1995. A dependency-based
method for evaluating broad-coverage
parsers. In Proceedings of the International
Joint Conference on AI, pages 1420?1427,
Montre?al, Canada.
Magerman, David. 1994. Natural Language
Parsing as Statistical Pattern Recognition.
Ph.D. thesis, Department of Computer
Science, Stanford University, CA.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre,
Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The
Penn Treebank: Annotating predicate
argument structure. In Proceedings
of the ARPA Workshop on Human
Language Technology, pages 110?115,
Princeton, NJ.
McCarthy, Maire?ad. 2003. Design and
Evaluation of the Linguistic Basis of an
Automatic F-Structure Annotation Algorithm
for the Penn-II Treebank. Master?s thesis,
School of Computing, Dublin City
University, Dublin, Ireland.
McDonald, Ryan and Fernando Pereira. 2006.
Online learning of approximate
dependency parsing algorithms. In
Proceedings of the 11th Conference of the
European Chapter of the Association for
Computational Linguistics, pages 81?88,
Trento, Italy.
Miyao, Yusuke, Takashi Ninomiya, and
Jun?ichi Tsujii. 2003. Probabilistic modeling
of argument structures including non-local
dependencies. In Proceedings of the
Conference on Recent Advances in Natural
Language Processing (RANLP),
pages 285?291, Borovets, Bulgaria.
Miyao, Yusuke and Jun?ichi Tsujii. 2002.
Maximum entropy estimation for feature
forests. In Proceedings of Human Language
Technology Conference (HLT 2002),
pages 292?297, San Diego, CA.
Miyao, Yusuke and Jun?ichi Tsujii.
2004. Deep linguistic analysis
for the accurate identification of
predicate?argument relations. In
Proceedings of the 18th International
Conference on Computational Linguistics
(COLING 2004), pages 1392?1397,
Geneva, Switzerland.
Noreen, Eric W. 1989. Computer Intensive
Methods for Testing Hypotheses: An
Introduction. Wiley, New York.
O?Donovan, Ruth, Michael Burke, Aoife
Cahill, Josef van Genabith, and Andy
Way. 2004. Large-scale induction and
evaluation of lexical resources from the
Penn-II treebank. In Proceedings of the 42nd
Annual Meeting of the Association for
Computational Linguistics, pages 368?375,
Barcelona, Spain.
Pollard, Carl and Ivan Sag. 1994. Head-driven
Phrase Structure Grammar. CSLI
Publications, Stanford, CA.
Preiss, Judita. 2003. Using grammatical
relations to compare parsers. In Proceedings
of the Tenth Conference of the European
Chapter of the Association for Computational
Linguistics (EACL?03), pages 291?298,
Budapest, Hungary.
Ratnaparkhi, Adwait. 1996. A maximum
entropy part-of-speech tagger. In
Proceedings of the Empirical Methods in
Natural Language Processing Conference,
pages 133?142, Philadelphia, PA.
Riezler, Stefan, Tracy King, Ronald Kaplan,
Richard Crouch, John T. Maxwell,
and Mark Johnson. 2002. Parsing
theWall Street Journal using a
lexical-functional grammar and
discriminative estimation techniques.
In Proceedings of the 40th Annual
Conference of the Association for
Computational Linguistics (ACL-02),
pages 271?278, Philadelphia, PA.
Sampson, Geoffrey. 1995. English for the
Computer: The SUSANNE Corpus and
Analytic Scheme. Clarendon Press,
Oxford, England.
Tsuruoka, Yoshimasa, Yusuke Miyao,
and Jun?ichi Tsujii. 2004. Towards
efficient probabilistic HPSG parsing:
Integrating semantic and syntactic
preference to guide the parsing.
In Proceedings of IJCNLP-04 Workshop:
Beyond shallow analyses?Formalisms
and statistical modeling for deep
analyses, Hainan Island, China. [No
page numbers].
van Genabith, Josef and Richard Crouch.
1996. Direct and underspecified
interpretations of LFG f-structures. In 16th
International Conference on Computational
Linguistics (COLING 96), pages 262?267,
Copenhagen, Denmark.
123
Computational Linguistics Volume 34, Number 1
van Genabith, Josef and Richard
Crouch. 1997. On interpreting
f-structures as UDRSs. In Proceedings
of ACL-EACL-97, pages 402?409,
Madrid, Spain.
Xia, Fei. 1999. Extracting tree adjoining
grammars from bracketed corpora. In
Proceedings of the 5th Natural Language
Processing Pacific Rim Symposium
(NLPRS-99), pages 398?403, Beijing,
China.
Xue, Nianwen, Fei Xia, Fu-Dong Chiou, and
Martha Palmer. 2004. The Penn Chinese
treebank: Phrase structure annotation of a
large corpus. Natural Language Engineering,
10(4):1?30.
Yeh, Alexander. 2000. More accurate tests
for the statistical significance of result
differences. In Proceedings of the 18th
International Conference on Computational
Linguistics (COLING, 2000), pages 947?953,
Saarbru?cken, Germany.
124
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 497?504,
Sydney, July 2006. c?2006 Association for Computational Linguistics
QuestionBank: Creating a Corpus of Parse-Annotated Questions
John Judge1, Aoife Cahill1, and Josef van Genabith1,2
1National Centre for Language Technology and School of Computing,
Dublin City University, Dublin, Ireland
2IBM Dublin Center for Advanced Studies,
IBM Dublin, Ireland
{jjudge,acahill,josef}@computing.dcu.ie
Abstract
This paper describes the development of
QuestionBank, a corpus of 4000 parse-
annotated questions for (i) use in training
parsers employed in QA, and (ii) evalua-
tion of question parsing. We present a se-
ries of experiments to investigate the ef-
fectiveness of QuestionBank as both an
exclusive and supplementary training re-
source for a state-of-the-art parser in pars-
ing both question and non-question test
sets. We introduce a new method for
recovering empty nodes and their an-
tecedents (capturing long distance depen-
dencies) from parser output in CFG trees
using LFG f-structure reentrancies. Our
main findings are (i) using QuestionBank
training data improves parser performance
to 89.75% labelled bracketing f-score, an
increase of almost 11% over the base-
line; (ii) back-testing experiments on non-
question data (Penn-II WSJ Section 23)
shows that the retrained parser does not
suffer a performance drop on non-question
material; (iii) ablation experiments show
that the size of training material provided
by QuestionBank is sufficient to achieve
optimal results; (iv) our method for recov-
ering empty nodes captures long distance
dependencies in questions from the ATIS
corpus with high precision (96.82%) and
low recall (39.38%). In summary, Ques-
tionBank provides a useful new resource
in parser-based QA research.
1 Introduction
Parse-annotated corpora (treebanks) are crucial for
developing machine learning and statistics-based
parsing resources for a given language or task.
Large treebanks are available for major languages,
however these are often based on a specific text
type or genre, e.g. financial newspaper text (the
Penn-II Treebank (Marcus et al, 1993)). This can
limit the applicability of grammatical resources in-
duced from treebanks in that such resources un-
derperform when used on a different type of text
or for a specific task.
In this paper we present work on creating Ques-
tionBank, a treebank of parse-annotated questions,
which can be used as a supplementary training re-
source to allow parsers to accurately parse ques-
tions (as well as other text). Alternatively, the re-
source can be used as a stand-alone training corpus
to train a parser specifically for questions. Either
scenario will be useful in training parsers for use
in question answering (QA) tasks, and it also pro-
vides a suitable resource to evaluate the accuracy
of these parsers on questions.
We use a semi-automatic ?bootstrapping?
method to create the question treebank from raw
text. We show that a parser trained on the ques-
tion treebank alone can accurately parse ques-
tions. Training on a combined corpus consisting of
the question treebank and an established training
set (Sections 02-21 of the Penn-II Treebank), the
parser gives state-of-the-art performance on both
questions and a non-question test set (Section 23
of the Penn-II Treebank).
Section 2 describes background work and mo-
tivation for the research presented in this paper.
Section 3 describes the data we used to create
the corpus. In Section 4 we describe the semi-
automatic method to ?bootstrap? the question cor-
pus, discuss some interesting and problematic
phenomena, and show how the manual vs. auto-
matic workload distribution changed as work pro-
gressed. Two sets of experiments using our new
question corpus are presented in Section 5. In
Section 6 we introduce a new method for recover-
ing empty nodes and their antecedents using Lex-
ical Functional Grammar (LFG) f-structure reen-
497
trancies. Section 7 concludes and outlines future
work.
2 Background and Motivation
High quality probabilistic, treebank-based parsing
resources can be rapidly induced from appropri-
ate treebank material. However, treebank- and
machine learning-based grammatical resources re-
flect the characteristics of the training data. They
generally underperform on test data substantially
different from the training data.
Previous work on parser performance and do-
main variation by Gildea (2001) showed that by
training a parser on the Penn-II Treebank and test-
ing on the Brown corpus, parser accuracy drops by
5.7% compared to parsing the Wall Street Journal
(WSJ) based Penn-II Treebank Section 23. This
shows a negative effect on parser performance
even when the test data is not radically different
from the training data (both the Penn II and Brown
corpora consist primarily of written texts of Amer-
ican English, the main difference is the consider-
ably more varied nature of the text in the Brown
corpus). Gildea also shows how to resolve this
problem by adding appropriate data to the training
corpus, but notes that a large amount of additional
data has little impact if it is not matched to the test
material.
Work on more radical domain variance and on
adapting treebank-induced LFG resources to anal-
yse ATIS (Hemphill et al, 1990) question mate-
rial is described in Judge et al (2005). The re-
search established that even a small amount of ad-
ditional training data can give a substantial im-
provement in question analysis in terms of both
CFG parse accuracy and LFG grammatical func-
tional analysis, with no significant negative effects
on non-question analysis. Judge et al (2005) sug-
gest, however, that further improvements are pos-
sible given a larger question training corpus.
Clark et al (2004) worked specifically with
question parsing to generate dependencies for QA
with Penn-II treebank-based Combinatory Cate-
gorial Grammars (CCG?s). They use ?what? ques-
tions taken from the TREC QA datasets as the ba-
sis for a What-Question corpus with CCG annota-
tion.
3 Data Sources
The raw question data for QuestionBank comes
from two sources, the TREC 8-11 QA track
test sets1, and a question classifier training set
produced by the Cognitive Computation Group
(CCG2) at the University of Illinois at Urbana-
Champaign.3 We use equal amounts of data from
each source so as not to bias the corpus to either
data source.
3.1 TREC Questions
The TREC evaluations have become the standard
evaluation for QA systems. Their test sets con-
sist primarily of fact seeking questions with some
imperative statements which request information,
e.g. ?List the names of cell phone manufactur-
ers.? We included 2000 TREC questions in the
raw data from which we created the question tree-
bank. These 2000 questions consist of the test
questions for the first three years of the TREC QA
track (1893 questions) and 107 questions from the
2003 TREC test set.
3.2 CCG Group Questions
The CCG provide a number of resources for de-
veloping QA systems. One of these resources is
a set of 5500 questions and their answer types for
use in training question classifiers. The 5500 ques-
tions were stripped of answer type annotation, du-
plicated TREC questions were removed and 2000
questions were used for the question treebank.
The CCG 5500 questions come from a number
of sources (Li and Roth, 2002) and some of these
questions contain minor grammatical mistakes so
that, in essence, this corpus is more representa-
tive of genuine questions that would be put to a
working QA system. A number of changes in to-
kenisation were corrected (eg. separating contrac-
tions), but the minor grammatical errors were left
unchanged because we believe that it is necessary
for a parser for question analysis to be able to cope
with this sort of data if it is to be used in a working
QA system.
4 Creating the Treebank
4.1 Bootstrapping a Question Treebank
The algorithm used to generate the question tree-
bank is an iterative process of parsing, manual cor-
rection, retraining, and parsing.
1http://trec.nist.gov/data/qa.html
2Note that the acronym CCG here refers to Cognitive
Computation Group, rather than Combinatory Categorial
Grammar mentioned in Section 2.
3http://l2r.cs.uiuc.edu/ cogcomp/tools.php
498
Algorithm 1 Induce a parse-annotated treebank
from raw data
repeat
Parse a new section of raw data
Manually correct errors in the parser output
Add the corrected data to the training set
Extract a new grammar for the parser
until All the data has been processed
Algorithm 1 summarises the bootstrapping al-
gorithm. A section of raw data is parsed. The
parser output is then manually corrected, and
added to the parser?s training corpus. A new gram-
mar is then extracted, and the next section of raw
data is parsed. This process continues until all the
data has been parsed and hand corrected.
4.2 Parser
The parser used to process the raw questions prior
to manual correction was that of Bikel (2002)4 ,
a retrainable emulation of Collins (1999) model
2 parser. Bikel?s parser is a history-based parser
which uses a lexicalised generative model to parse
sentences. We used WSJ Sections 02-21 of the
Penn-II Treebank to train the parser for the first it-
eration of the algorithm. The training corpus for
subsequent iterations consisted of the WSJ ma-
terial and increasing amounts of processed ques-
tions.
4.3 Basic Corpus Development Statistics
Our question treebank was created over a period
of three months at an average annotation speed of
about 60 questions per day. This is quite rapid
for treebank development. The speed of the pro-
cess was helped by two main factors: the questions
are generally quite short (typically about 10 words
long), and, due to retraining on the continually in-
creasing training set, the quality of the parses out-
put by the parser improved dramatically during the
development of the treebank, with the effect that
corrections during the later stages were generally
quite small and not as time consuming as during
the initial phases of the bootstrapping process.
For example, in the first week of the project the
trees from the parser were of relatively poor qual-
ity and over 78% of the trees needed to be cor-
rected manually. This slowed the annotation pro-
cess considerably and parse-annotated questions
4Downloaded from http://www.cis.upenn.edu/?dbikel
/software.html#stat-parser
were being produced at an average rate of 40 trees
per day. During the later stages of the project this
had changed dramatically. The quality of trees
from the parser was much improved with less than
20% of the trees requiring manual correction. At
this stage parse-annotated questions were being
produced at an average rate of 90 trees per day.
4.4 Corpus Development Error Analysis
Some of the more frequent errors in the parser
output pertain to the syntactic analysis of WH-
phrases (WHNP, WHPP, etc). In Sections 02-21
of the Penn-II Treebank, these are used more often
in relative clause constructions than in questions.
As a result many of the corpus questions were
given syntactic analyses corresponding to relative
clauses (SBAR with an embedded S) instead of as
questions (SBARQ with an embedded SQ). Figure
1 provides an example.
SBAR
WHNP
WP
Who
S
VP
VBD
created
NP
DT
the
NN
Muppets
(a)
SBARQ
WHNP
WP
Who
SQ
VP
VBD
created
NP
DT
the
NNPS
Muppets
(b)
Figure 1: Example tree before (a) and after correc-
tion (b)
Because the questions are typically short, an er-
ror like this has quite a large effect on the accu-
racy for the overall tree; in this case the f-score
for the parser output (Figure 1(a)) would be only
60%. Errors of this nature were quite frequent
in the first section of questions analysed by the
parser, but with increased training material becom-
ing available during successive iterations, this er-
ror became less frequent and towards the end of
499
the project it was only seen in rare cases.
WH-XP marking was the source of a number of
consistent (though infrequent) errors during anno-
tation. This occurred mostly in PP constructions
containing WHNPs. The parser would output a
structure like Figure 2(a), where the PP mother of
the WHNP is not correctly labelled as a WHPP as
in Figure 2(b).
PP
IN
by
WHNP
WP$
whose
NN
authority
WHPP
IN
by
WHNP
WP$
whose
NN
authority
(a) (b)
Figure 2: WH-XP assignment
The parser output often had to be rearranged
structurally to varying degrees. This was common
in the longer questions. A recurring error in the
parser output was failing to identify VPs in SQs
with a single object NP. In these cases the verb
and the object NP were left as daughters of the
SQ node. Figure 3(a) illustrates this, and Figure
3(b) shows the corrected tree with the VP node in-
serted.
SBARQ
WHNP
WP
Who
SQ
VBD
killed
NP
Ghandi
SBARQ
WHNP
WP
Who
SQ
VP
VBD
killed
NP
Ghandi
(a) (b)
Figure 3: VP missing inside SQ with a single NP
On inspection, we found that the problem was
caused by copular constructions, which, accord-
ing to the Penn-II annotation guidelines, do not
feature VP constituents. Since almost half of the
question data contain copular constructions, the
parser trained on this data would sometimes mis-
analyse non-copular constructions or, conversely,
incorrectly bracket copular constructions using a
VP constituent (Figure 4(a)).
The predictable nature of these errors meant that
they were simple to correct. This is due to the par-
ticular context in which they occur and the finite
number of forms of the copular verb.
SBARQ
WHNP
WP
What
SQ
VP
VBZ
is
NP
a fear of shadows
SBARQ
WHNP
WP
What
SQ
VBZ
is
NP
a fear of shadows
(a) (b)
Figure 4: Erroneous VP in copular constructions
5 Experiments with QuestionBank
In order to test the effect training on the question
corpus has on parser performance, we carried out
a number of experiments. In cross-validation ex-
periments with 90%/10% splits we use all 4000
trees in the completed QuestionBank as the test
set. We performed ablation experiments to inves-
tigate the effect of varying the amount of question
and non-question training data on the parser?s per-
formance. For these experiments we divided the
4000 questions into two sets. We randomly se-
lected 400 trees to be held out as a gold standard
test set against which to evaluate, the remaining
3600 trees were then used as a training corpus.
5.1 Establishing the Baseline
The baseline we use for our experiments is pro-
vided by Bikel?s parser trained on WSJ Sections
02-21 of the Penn-II Treebank. We test on all 4000
questions in our question treebank, and also Sec-
tion 23 of the Penn-II Treebank.
QuestionBank
Coverage 100
F-Score 78.77
WSJ Section 23
Coverage 100
F-Score 82.97
Table 1: Baseline parsing results
Table 1 shows the results for our baseline eval-
uations on question and non-question test sets.
While the coverage for both tests is high, the
parser underperforms significantly on the question
test set with a labelled bracketing f-score of 78.77
compared to 82.97 on Section 23 of the Penn-II
Treebank. Note that unlike the published results
for Bikel?s parser in our evaluations we test on
Section 23 and include punctuation.
5.2 Cross-Validation Experiments
We carried out two cross-validation experiments.
In the first experiment we perform a 10-fold cross-
validation experiment using our 4000 question
500
treebank. In each case a randomly selected set of
10% of the questions in QuestionBank was held
out during training and used as a test set. In this
way parses from unseen data were generated for
all 4000 questions and evaluated against the Ques-
tionBank trees.
The second cross-validation experiment was
similar to the first, but in each of the 10 folds we
train on 90% of the 4000 questions in Question-
Bank and on all of Sections 02-21 of the Penn-II
Treebank.
In both experiments we also backtest each of the
ten grammars on Section 23 of the Penn-II Tree-
bank and report the average scores.
QuestionBank
Coverage 100
F-Score 88.82
Backtest on Sect 23
Coverage 98.79
F-Score 59.79
Table 2: Cross-validation experiment using the
4000 question treebank
Table 2 shows the results for the first cross-
validation experiment, using only the 4000 sen-
tence QuestionBank. Compared to Table 1, the re-
sults show a significant improvement of over 10%
on the baseline f-score for questions. However, the
tests on the non-question Section 23 data show not
only a significant drop in accuracy but also a drop
in coverage.
Questions
Coverage 100
F-Score 89.75
Backtest on Sect 23
Coverage 100
F-Score 82.39
Table 3: Cross-validation experiment using Penn-
II Treebank Sections 02-21 and 4000 questions
Table 3 shows the results for the second cross-
validation experiment using Sections 02-21 of the
Penn-II Treebank and the 4000 questions in Ques-
tionBank. The results show an even greater in-
crease on the baseline f-score than the experiments
using only the question training set (Table 2). The
non-question results are also better and are com-
parable to the baseline (Table 1).
5.3 Ablation Runs
In a further set of experiments we investigated the
effect of varying the amount of data in the parser?s
training corpus. We experiment with varying both
the amount of QuestionBank and Penn-II Tree-
bank data that the parser is trained on. In each
experiment we use the 400 question test set and
Section 23 of the Penn-II Treebank to evaluate
against, and the 3600 question training set de-
scribed above and Sections 02-21 of the Penn-II
Treebank as the basis for the parser?s training cor-
pus. We report on three experiments:
In the first experiment we train the parser using
only the 3600 question training set. We performed
ten training and parsing runs in this experiment,
incrementally reducing the size of the Question-
Bank training corpus by 10% of the whole on each
run.
The second experiment is similar to the first but
in each run we add Sections 02-21 of the Penn-II
Treebank to the (shrinking) training set of ques-
tions.
The third experiment is the converse of the sec-
ond, the amount of questions in the training set
remains fixed (all 3600) and the amount of Penn-
II Treebank material is incrementally reduced by
10% on each run.
 50
 60
 70
 80
 90
 100
 10 20 30 40 50 60 70 80 90 100
Co
ve
ra
ge
/F
-S
co
re
Percentage of 3600 questions in the training corpus
FScore Questions
FScore Section 23
Coverage Questions
Coverage Section 23
Figure 5: Results for ablation experiment reducing
3600 training questions in steps of 10%
Figure 5 graphs the coverage and f-score for
the parser in tests on the 400 question test set,
and Section 23 of the Penn-II Treebank in ten
parsing runs with the amount of data in the 3600
question training corpus reducing incrementally
on each run. The results show that training on only
a small amount of questions, the parser can parse
questions with high accuracy. For example when
trained on only 10% of the 3600 questions used
in this experiment, the parser successfully parses
all of the 400 question test set and achieves an f-
score of 85.59. However the results for the tests
on WSJ Section 23 are considerably worse. The
parser never manages to parse the full test set, and
the best score at 59.61 is very low.
Figure 6 graphs the results for the second abla-
501
 50
 60
 70
 80
 90
 100
 10 20 30 40 50 60 70 80 90 100
Co
ve
ra
ge
/F
-S
co
re
Percentage of 3600 questions in the training corpus
FScore Questions
FScore Section 23
Coverage Questions
Coverage Section 23
Figure 6: Results for ablation experiment using
PTB Sections 02-21 (fixed) and reducing 3600
questions in steps of 10%
 50
 60
 70
 80
 90
 100
 10 20 30 40 50 60 70 80 90 100
Co
ve
ra
ge
/F
-S
co
re
Percentage of PTB Stetcions 2-21 in the training corpus
FScore Questions
FScore Section 23
Coverage Questions
Coverage Section 23
Figure 7: Results for ablation experiment using
3600 questions (fixed) and reducing PTB Sections
02-21 in steps of 10%
tion experiment. The training set for the parser
consists of a fixed amount of Penn-II Treebank
data (Sections 02-21) and a reducing amount of
question data from the 3600 question training set.
Each grammar is tested on both the 400 question
test set, and WSJ Section 23. The results here
are significantly better than in the previous exper-
iment. In all of the runs the coverage for both test
sets is 100%, f-scores for the question test set de-
crease as the amount of question data in the train-
ing set is reduced (though they are still quite high.)
There is little change in the f-scores for the tests on
Section 23, the results all fall in the range 82.36 to
82.46, which is comparable to the baseline score.
Figure 7 graphs the results for the third abla-
tion experiment. In this case the training set is a
fixed amount of the question training set described
above (all 3600 questions) and a reducing amount
of data from Sections 02-21 of the Penn Treebank.
The graph shows that the parser performs consis-
tently well on the question test set in terms of both
coverage and accuracy. The tests on Section 23,
however, show that as the amount of Penn-II Tree-
bank material in the training set decreases, the f-
score also decreases.
6 Long Distance Dependencies
Long distance dependencies are crucial in the
proper analysis of question material. In English
wh-questions, the fronted wh-constituent refers to
an argument position of a verb inside the interrog-
ative construction. Compare the superficially sim-
ilar
1. Who1 [t1] killed Harvey Oswald?
2. Who1 did Harvey Oswald kill [t1]?
(1) queries the agent (syntactic subject) of the de-
scribed eventuality, while (2) queries the patient
(syntactic object). In the Penn-II and ATIS tree-
banks, dependencies such as these are represented
in terms of empty productions, traces and coindex-
ation in CFG tree representations (Figure 8).
SBARQ
WHNP-1
WP
Who
SQ
NP
*T*-1
VP
VBD
killed
NP
Harvey Oswald
(a)
SBARQ
WHNP-1
WP
Who
SQ
AUX
did
NP
Harvey Oswald
VP
VB
kill
NP
*T*-1
(b)
Figure 8: LDD resolved treebank style trees
With few exceptions5 the trees produced by cur-
rent treebank-based probabilistic parsers do not
represent long distance dependencies (Figure 9).
Johnson (2002) presents a tree-based method
for reconstructing LDD dependencies in Penn-
II trained parser output trees. Cahill et al
(2004) present a method for resolving LDDs
5Collins? Model 3 computes a limited number of wh-
dependencies in relative clause constructions.
502
SBARQ
WHNP
WP
Who
SQ
VP
VBD
killed
NP
Harvey Oswald
(a)
SBARQ
WHNP
WP
Who
SQ
AUX
did
NP
Harvey Oswald
VP
VB
kill
(b)
Figure 9: Parser output trees
at the level of Lexical-Functional Grammar f-
structure (attribute-value structure encodings of
basic predicate-argument structure or dependency
relations) without the need for empty productions
and coindexation in parse trees. Their method is
based on learning finite approximations of func-
tional uncertainty equations (regular expressions
over paths in f-structure) from an automatically f-
structure annotated version of the Penn-II treebank
and resolves LDDs at f-structure. In our work we
use the f-structure-based method of Cahill et al
(2004) to ?reverse engineer? empty productions,
traces and coindexation in parser output trees. We
explain the process by way of a worked example.
We use the parser output tree in Figure 9(a)
(without empty productions and coindexation) and
automatically annotate the tree with f-structure
information and compute LDD-resolution at the
level of f-structure using the resources of Cahill
et al (2004). This generates the f-structure an-
notated tree6 and the LDD resolved f-structure in
Figure 10.
Note that the LDD is indicated in terms of a
reentrancy 1 between the question FOCUS and the
SUBJ function in the resolved f-structure. Given
the correspondence between the f-structure and f-
structure annotated nodes in the parse tree, we
compute that the SUBJ function newly introduced
and reentrant with the FOCUS function is an argu-
ment of the PRED ?kill? and the verb form ?killed?
in the tree. In order to reconstruct the correspond-
ing empty subject NP node in the parser output
tree, we need to determine candidate anchor sites
6Lexical annotations are suppressed to aid readability.
SBARQ
WHNP
? FOCUS =?
WP
?=?
Who
SQ
?=?
VP
?=?
VBD
?=?
killed
NP
? OBJ =?
Harvey Oswald
(a)
?
?
?
FOCUS
[
PRED who
]
1
PRED ?kill?SUBJ OBJ??
OBJ
[
PRED ?Harvey Oswald?
]
SUBJ
[
PRED ?who?
]
1
?
?
?
(b)
Figure 10: Annotated tree and f-structure
for the empty node. These anchor sites can only be
realised along the path up to the maximal projec-
tion of the governing verb indicated by ?=? anno-
tations in LFG. This establishes three anchor sites:
VP, SQ and the top level SBARQ. From the auto-
matically f-structure annotated Penn-II treebank,
we extract f-structure annotated PCFG rules for
each of the three anchor sites whose RHSs contain
exactly the information (daughter categories plus
LFG annotations) in the tree in Figure 10 (in the
same order) plus an additional node (of whatever
CFG category) annotated ?SUBJ=?, located any-
where within the RHSs. This will retrieve rules of
the form
VP ? NP [? SUBJ =?] V BD[?=?] NP [? OBJ =?]
V P ? . . .
. . .
SQ ? NP [? SUBJ =?] V P [?=?]
SQ ? . . .
. . .
SBARQ ? . . .
. . .
each with their associated probabilities. We select
the rule with the highest probability and cut the
rule into the tree in Figure 10 at the appropriate
anchor site (as determined by the rule LHS). In our
case this selects SQ ? NP [? SUBJ=?]V P [?=?]
and the resulting tree is given in Figure 11. From
this tree, it is now easy to compute the tree with
the coindexed trace in Figure 8 (a).
In order to evaluate our empty node and coin-
dexation recovery method, we conducted two ex-
periments, one using 146 gold-standard ATIS
question trees and one using parser output on the
corresponding strings for the 146 ATIS question
trees.
503
SBARQ
WHNP-1
? FOCUS =?
WP
?=?
Who
SQ
?=?
NP
? SUBJ =?
-NONE-
*T*-1
VP
?=?
VBD
?=?
killed
NP
? OBJ =?
Harvey Oswald
Figure 11: Resolved tree
In the first experiment, we delete empty nodes
and coindexation from the ATIS gold standard
trees and and reconstruct them using our method
and the preprocessed ATIS trees. In the second
experiment, we parse the strings corresponding to
the ATIS trees with Bikel?s parser and reconstruct
the empty productions and coindexation. In both
cases we evaluate against the original (unreduced)
ATIS trees and score if and only if all of inser-
tion site, inserted CFG category and coindexation
match.
Parser Output Gold Standard Trees
Precision 96.77 96.82
Recall 38.75 39.38
Table 4: Scores for LDD recovery (empty nodes
and antecedents)
Table 4 shows that currently the recall of our
method is quite low at 39.38% while the accu-
racy is very high with precision at 96.82% on the
ATIS trees. Encouragingly, evaluating parser out-
put for the same sentences shows little change in
the scores with recall at 38.75% and precision at
96.77%.
7 Conclusions
The data represented in Figure 5 show that train-
ing a parser on 50% of QuestionBank achieves an
f-score of 88.56% as against 89.24% for training
on all of QuestionBank. This implies that while
we have not reached an absolute upper bound, the
question corpus is sufficiently large that the gain
in accuracy from adding more data is so small that
it does not justify the effort.
We will evaluate grammars learned from
QuestionBank as part of a working QA sys-
tem. A beta-release of the non-LDD-resolved
QuestionBank is available for download at
http://www.computing.dcu.ie/?
jjudge/qtreebank/4000qs.txt. The fi-
nal, hand-corrected, LDD-resolved version will be
available in October 2006.
Acknowledgments
We are grateful to the anonymous reviewers for
their comments and suggestions. This research
was supported by Science Foundation Ireland
(SFI) grant 04/BR/CS0370 and an Irish Research
Council for Science Engineering and Technology
(IRCSET) PhD scholarship 2002-05.
References
Daniel M. Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceedings of
HLT 2002, pages 24?27, San Diego, CA.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef van
Genabith, and Andy Way. 2004. Long-Distance De-
pendency Resolution in Automatically Acquired Wide-
Coverage PCFG-Based LFG Approximations. In Pro-
ceedings of ACL-04, pages 320?327, Barcelona, Spain.
Stephen Clark, Mark Steedman, and James R. Curran.
2004. Object-extraction and question-parsing using ccg.
In Dekang Lin and Dekai Wu, editors, Proceedings of
EMNLP-04, pages 111?118, Barcelona, Spain.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania, Philadelphia, PA.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Lillian Lee and Donna Harman, editors, Pro-
ceedings of EMNLP, pages 167?202, Pittsburgh, PA.
Charles T. Hemphill, John J. Godfrey, and George R. Dod-
dington. 1990. The ATIS Spoken Language Systems pi-
lot corpus. In Proceedings of DARPA Speech and Natural
Language Workshop, pages 96?101, Hidden Valley, PA.
Mark Johnson. 2002. A simple pattern-matching algorithm
for recovering empty nodes and their antecedents. In Pro-
ceedings ACL-02, University of Pennsylvania, Philadel-
phia, PA.
John Judge, Aoife Cahill, Michael Burke, Ruth O?Donovan,
Josef van Genabith, and Andy Way. 2005. Strong Domain
Variation and Treebank-Induced LFG Resources. In Pro-
ceedings LFG-05, pages 186?204, Bergen, Norway, July.
Xin Li and Dan Roth. 2002. Learning question classifiers. In
Proceedings of COLING-02, pages 556?562, Taipei, Tai-
wan.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated Cor-
pus of English: The Penn Treebank. Computational Lin-
guistics, 19(2):313?330.
504
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1033?1040,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Robust PCFG-Based Generation using Automatically Acquired LFG
Approximations
Aoife Cahill1 and Josef van Genabith1,2
1 National Centre for Language Technology (NCLT)
School of Computing, Dublin City University, Dublin 9, Ireland
2 Center for Advanced Studies, IBM Dublin, Ireland
{acahill,josef}@computing.dcu.ie
Abstract
We present a novel PCFG-based archi-
tecture for robust probabilistic generation
based on wide-coverage LFG approxima-
tions (Cahill et al, 2004) automatically
extracted from treebanks, maximising the
probability of a tree given an f-structure.
We evaluate our approach using string-
based evaluation. We currently achieve
coverage of 95.26%, a BLEU score of
0.7227 and string accuracy of 0.7476 on
the Penn-II WSJ Section 23 sentences of
length ?20.
1 Introduction
Wide coverage grammars automatically extracted
from treebanks are a corner-stone technology
in state-of-the-art probabilistic parsing. They
achieve robustness and coverage at a fraction of
the development cost of hand-crafted grammars. It
is surprising to note that to date, such grammars do
not usually figure in the complementary operation
to parsing ? natural language surface realisation.
Research on statistical natural language surface
realisation has taken three broad forms, differ-
ing in where statistical information is applied in
the generation process. Langkilde (2000), for ex-
ample, uses n-gram word statistics to rank alter-
native output strings from symbolic hand-crafted
generators to select paths in parse forest repre-
sentations. Bangalore and Rambow (2000) use
n-gram word sequence statistics in a TAG-based
generation model to rank output strings and ad-
ditional statistical and symbolic resources at in-
termediate generation stages. Ratnaparkhi (2000)
uses maximum entropy models to drive generation
with word bigram or dependency representations
taking into account (unrealised) semantic features.
Valldal and Oepen (2005) present a discriminative
disambiguation model using a hand-crafted HPSG
grammar for generation. Belz (2005) describes
a method for building statistical generation mod-
els using an automatically created generation tree-
bank for weather forecasts. None of these prob-
abilistic approaches to NLG uses a full treebank
grammar to drive generation.
Bangalore et al (2001) investigate the ef-
fect of training size on performance while using
grammars automatically extracted from the Penn-
II Treebank (Marcus et al, 1994) for generation.
Using an automatically extracted XTAG grammar,
they achieve a string accuracy of 0.749 on their
test set. Nakanishi et al (2005) present proba-
bilistic models for a chart generator using a HPSG
grammar acquired from the Penn-II Treebank (the
Enju HPSG). They investigate discriminative dis-
ambiguation models following Valldal and Oepen
(2005) and their best model achieves coverage of
90.56% and a BLEU score of 0.7723 on Penn-II
WSJ Section 23 sentences of length ?20.
In this paper we present a novel PCFG-based
architecture for probabilistic generation based on
wide-coverage, robust Lexical Functional Gram-
mar (LFG) approximations automatically ex-
tracted from treebanks (Cahill et al, 2004). In
Section 2 we briefly describe LFG (Kaplan and
Bresnan, 1982). Section 3 presents our genera-
tion architecture. Section 4 presents evaluation re-
sults on the Penn-II WSJ Section 23 test set us-
ing string-based metrics. Section 5 compares our
approach with alternative approaches in the litera-
ture. Section 6 concludes and outlines further re-
search.
2 Lexical Functional Grammar
Lexical Functional Grammar (LFG) (Kaplan and
Bresnan, 1982) is a constraint-based theory of
grammar. It (minimally) posits two levels of repre-
sentation, c(onstituent)-structure and f(unctional)-
structure. C-structure is represented by context-
free phrase-structure trees, and captures surface
1033
S
?=?
NP VP
(? SUBJ)= ? ?=?
NNP V SBAR
?=? ?=? (? COMP)= ?
They believe S
(? PRED) = ?pro? (? PRED) = ?believe? ?=?
(? NUM) = PL (? TENSE) = present
(? PERS) = 3 NP VP
(? SUBJ)= ? ?=?
NNP V
?=? ?=?
John resigned
(? PRED) = ?John? (? PRED) = ?resign?
(? NUM) = SG (? TENSE) = PAST
(? PERS) = 3
f1:
?
?
?
?
?
?
?
?
?
PRED ?BELIEVE?(?SUBJ)(?COMP)??
SUBJ f2:
[
PRED ?PRO?
NUM PL
PERS 3
]
COMP f3:
?
?
?
SUBJ f4:
[
PRED ?JOHN?
NUM SG
PERS 3
]
PRED RESIGN?(?SUBJ)??
TENSE PAST
?
?
?
TENSE PRESENT
?
?
?
?
?
?
?
?
?
Figure 1: C- and f-structures for the sentence They believe John resigned.
grammatical configurations such as word order.
The nodes in the trees are annotated with func-
tional equations (attribute-value structure con-
straints) which are resolved to produce an f-
structure. F-structures are recursive attribute-
value matrices, representing abstract syntactic
functions. F-structures approximate to basic
predicate-argument-adjunct structures or depen-
dency relations. Figure 1 shows the c- and f-
structures for the sentence ?They believe John re-
signed?.
3 PCFG-Based Generation for
Treebank-Based LFG Resources
Cahill et al (2004) present a method to au-
tomatically acquire wide-coverage robust proba-
bilistic LFG approximations1 from treebanks. The
method is based on an automatic f-structure an-
notation algorithm that associates nodes in tree-
bank trees with f-structure equations. For each
tree, the equations are collected and passed on to
a constraint solver which produces an f-structure
for the tree. Cahill et al (2004) present two
parsing architectures: the pipeline and the inte-
grated parsing architecture. In the pipeline ar-
chitecture, a PCFG (or a history-based lexicalised
generative parser) is extracted from the treebank
and used to parse unseen text into trees, the result-
ing trees are annotated with f-structure equations
by the f-structure annotation algorithm and a con-
straint solver produces an f-structure. In the in-
1The resources are approximations in that (i) they do not
enforce LFG completeness and coherence constraints and (ii)
PCFG-based models can only approximate LFG and similar
constraint-based formalisms (Abney, 1997).
tegrated architecture, first the treebank trees are
automatically annotated with f-structure informa-
tion, f-structure annotated PCFGs with rules of
the form NP(?OBJ=?)?DT(?=?) NN(?=?) are
extracted, syntactic categories followed by equa-
tions are treated as monadic CFG categories dur-
ing grammar extraction and parsing, unseen text is
parsed into trees with f-structure annotations, the
annotations are collected and a constraint solver
produces an f-structure.
The generation architecture presented here
builds on the integrated parsing architecture re-
sources of Cahill et al (2004). The generation
process takes an f-structure (such as the f-structure
on the right in Figure 1) as input and outputs the
most likely f-structure annotated tree (such as the
tree on the left in Figure 1) given the input f-
structure
argmaxTreeP (Tree|F-Str)
where the probability of a tree given an f-
structure is decomposed as the product of the
probabilities of all f-structure annotated produc-
tions contributing to the tree but where in addi-
tion to conditioning on the LHS of the produc-
tion (as in the integrated parsing architecture of
Cahill et al (2004)) each production X ? Y is
now also conditioned on the set of f-structure fea-
tures Feats ?-linked2 to the LHS of the rule. For
an f-structure annotated tree Tree and f-structure
F-Str with ?(Tree)=F-Str:3
2? links LFG?s c-structure to f-structure in terms of many-
to-one functions from tree nodes into f-structure.
3? resolves the equations in Tree into F-Str (if satisfiable)
in terms of the piece-wise function ?.
1034
Conditioning F-Structure Features Grammar Rules Probability
{PRED, SUBJ, COMP, TENSE} VP(?=?) ? VBD(?=?) SBAR(?COMP=?) 0.4998
{PRED, SUBJ, COMP, TENSE} VP(?=?) ? VBP(?=?) SBAR(?COMP=?) 0.0366
{PRED, SUBJ, COMP, TENSE} VP(?=?) ? VBD(?=?) , S(?COMP=?) 6.48e-6
{PRED, SUBJ, COMP, TENSE} VP(?=?) ? VBD(?=?) S(?COMP=?) 3.88e-6
{PRED, SUBJ, COMP, TENSE} VP(?=?) ? VBP(?=?) , SBARQ(?COMP=?) 7.86e-7
{PRED, SUBJ, COMP, TENSE} VP(?=?) ? VBD(?=?) SBARQ(?COMP=?) 1.59e-7
Table 1: Example VP Generation rules automatically extracted from Sections 02?21 of the Penn-II
Treebank
P (Tree|F-Str) :=
?
X ? Y in Tree
?(X) = Feats
P (X ? Y |X,Feats) (1)
P (X ? Y |X,Feats) = P (X ? Y,X, Feats)P (X,Feats) = (2)
P (X ? Y, Feats)
P (X,Feats) ?
#(X ? Y, Feats)
#(X ? . . . , F eats) (3)
and where probabilities are estimated using a
simple MLE and rule counts (#) from the auto-
matically f-structure annotated treebank resource
of Cahill et al (2004). Lexical rules (rules ex-
panding preterminals) are conditioned on the full
set of (atomic) feature-value pairs ?-linked to the
RHS. The intuition for conditioning rules in this
way is that local f-structure components of the in-
put f-structure drive the generation process. This
conditioning effectively turns the f-structure an-
notated PCFGs of Cahill et al (2004) into prob-
abilistic generation grammars. For example, in
Figure 1 (where ?-links are represented as ar-
rows), we automatically extract the rule S(?=?) ?
NP(?SUBJ=?) VP(?=?) conditioned on the feature
set {PRED,SUBJ,COMP,TENSE}. The probability
of the rule is then calculated by counting the num-
ber of occurrences of that rule (and the associated
set of features), divided by the number of occur-
rences of rules with the same LHS and set of fea-
tures. Table 1 gives example VP rule expansions
with their probabilities when we train a grammar
from Sections 02?21 of the Penn Treebank.
3.1 Chart Generation Algorithm
The generation algorithm is based on chart gen-
eration as first introduced by Kay (1996) with
Viterbi-pruning. The generation grammar is first
converted into Chomsky Normal Form (CNF). We
recursively build a chart-like data structure in a
bottom-up fashion. In contrast to packing of lo-
cally equivalent edges (Carroll and Oepen, 2005),
in our approach if two chart items have equiva-
lent rule left-hand sides and lexical coverage, only
the most probable one is kept. Each grammatical
function-labelled (sub-)f-structure in the overall f-
structure indexes a (sub-)chart. The chart for each
f-structure generates the most probable tree for
that f-structure, given the internal set of condition-
ing f-structure features and its grammatical func-
tion label. At each level, grammatical function in-
dexed charts are initially unordered. Charts are
linearised by generation grammar rules once the
charts themselves have produced the most prob-
able tree for the chart. Our example in Figure 1
generates the following grammatical function in-
dexed, embedded and (at each level of embedding)
unordered (sub-)chart configuration:
SUBJ f :2
COMP f :3
SUBJ f :4TOP f :1
For each local subchart, the following algorithm
is applied:
Add lexical rules
While subchart is Changing
Apply unary productions
Apply binary productions
Propagate compatible rules
3.2 A Worked Example
As an example, we step through the construc-
tion of the COMP-indexed chart at level f3 of
the f-structure in Figure 1. For lexical rules,
we check the feature set at the sub-f-structure
level and the values of the features. Only fea-
tures associated with lexical material are consid-
ered. The SUBJ-indexed sub-chart f4 is con-
structed by first adding the rule NNP(?=?) ?
John(?PRED=?John?,?NUM=pl,?PERS=3). If more
than one lexical rule corresponds to a particular set
of features and values in the f-structure, we add all
rules with different LHS categories. If two or more
1035
rules with equal LHS categories match the feature
set, we only add the most probable one.
Unary productions are applied if the RHS of the
unary production matches the LHS of an item al-
ready in the chart and the feature set of the unary
production matches the conditioning feature set of
the local sub-f-structure. In our example, this re-
sults in the rule NP(?SUBJ=?) ? NNP(?=?), con-
ditioned on {NUM, PERS, PRED}, being added to
the sub-chart at level f4 (the probability associated
with this item is the probability of the rule multi-
plied by the probability of the previous chart item
which combines with the new rule). When a rule
is added to the chart, it is automatically associated
with the yield of the rule, allowing us to propa-
gate chunks of generated material upwards in the
chart. If two items in the chart have the same LHS
(and the same yield independent of word order),
only the item with the highest probability is kept.
This Viterbi-style pruning ensures that processing
is efficient.
At sub-chart f4 there are no binary rules that
can be applied. At this stage, it is not possible
to add any more items to the sub-chart, therefore
we propagate items in the chart that are compat-
ible with the sub-chart index SUBJ. In our ex-
ample, only the rule NP(?SUBJ=?) ? NNP(?=?)
(which yields the string John) is propagated to the
next level up in the overall chart for consideration
in the next iteration. If the yield of an item be-
ing propagated upwards in the chart is subsumed
by an element already at that level, the subsumed
item is removed. This results in efficiently treat-
ing the well known problem originally described
in Kay (1996), where one unnecessarily retains
sub-optimal strings. For example, generating the
string ?The very tall strong athletic man?, one
does not want to keep variations such as ?The very
tall man?, or ?The athletic man?, if one can gener-
ate the entire string. Our method ensures that only
the most probable tree with the longest yield will
be propagated upwards.
The COMP-indexed chart at level f3 of the f-
structure is constructed in a similar fashion. First
the lexical rule V(?=?) ? resigned is added.
Next, conditioning on {PRED, SUBJ, TENSE}, the
unary rule VP(?=?) ? V(?=?) (with yield re-
signed) is added. We combine the new VP(?=?)
rule with the NP(?SUBJ=?) already present from
the previous iteration to enable us to add the rule
S(?=?) ? NP(?SUBJ=?) VP(?=?), conditioned
on {PRED, SUBJ, TENSE}. The yield of this rule
is John resigned. Next, conditioning on the same
feature set, we add the rule SBAR(?comp=?) ?
S(?=?) with yield John resigned to the chart. It is
not possible to add any more new rules, so at this
stage, only the SBAR(?COMP=?) rule with yield
John resigned is propagated up to the next level.
The process continues until at the outermost
level of the f-structure, there are no more rules to
be added to the chart. At this stage, we search for
the most probable rule with TOP as its LHS cate-
gory and return the yield of this rule as the output
of the generation process. Generation fails if there
is no rule with LHS TOP at this level in the chart.
3.3 Lexical Smoothing
Currently, the only smoothing in the system ap-
plies at the lexical level. Our backoff uses
the built-in lexical macros4 of the automatic f-
structure annotation algorithm of Cahill et al
(2004) to identify potential part-of-speech cate-
gories corresponding to a particular set of features.
Following Baayen and Sproat (1996) we assume
that unknown words have a probability distribu-
tion similar to hapax legomena. We add a lexical
rule for each POS tag that corresponds to the f-
structure features at that level to the chart with a
probability computed from the original POS tag
probability distribution multiplied by a very small
constant. This means that lexical rules seen during
training have a much higher probability than lexi-
cal rules added during the smoothing phase. Lexi-
cal smoothing has the advantage of boosting cov-
erage (as shown in Tables 3, 4, 5 and 6 below) but
slightly degrades the quality of the strings gener-
ated. We believe that the tradeoff in terms of qual-
ity is worth the increase in coverage.
Smoothing is not carried out when there is no
suitable phrasal grammar rule that applies during
the process of generation. This can lead to the gen-
eration of partial strings, since some f-structure
components may fail to generate a corresponding
string. In such cases, generation outputs the con-
catenation of the strings generated by the remain-
ing components.
4 Experiments
We train our system on WSJ Sections 02?21 of
the Penn-II Treebank and evaluate against the raw
4The lexical macros associate POS tags with sets of fea-
tures, for example the tag NNS (plural noun) is associated
with the features ?PRED=$LEMMA and ?NUM=pl.
1036
S. length ? 20 ? 25 ? 30 ? 40 all
Training 16667 23597 29647 36765 39832
Test 1034 1464 1812 2245 2416
Table 2: Number of training and test sentences per
sentence length
strings from Section 23. We use Section 22 as our
development set. As part of our evaluation, we ex-
periment with sentences of varying length (20, 25,
30, 40, all), both in training and testing. Table 2
gives the number of training and test sentences for
each sentence length. In each case, we use the au-
tomatically generated f-structures from Cahill et
al. (2004) from the original Section 23 treebank
trees as f-structure input to our generation experi-
ments. We automatically mark adjunct and coor-
dination scope in the input f-structure. Notice that
these automatically generated f-structures are not
?perfect?, i.e. they are not guaranteed to be com-
plete and coherent (Kaplan and Bresnan, 1982): a
local f-structure may contain material that is not
supposed to be there (incoherence) and/or may be
missing material that is supposed to be there (in-
completeness). The results presented below show
that our method is robust with respect to the qual-
ity of the f-structure input and will always attempt
to generate partial output rather than fail. We con-
sider this an important property as pristine gen-
eration input cannot always be guaranteed in re-
alistic application scenarios, such as probabilistic
transfer-based machine translation where genera-
tion input may contain a certain amount of noise.
4.1 Pre-Training Treebank Transformations
During the development of the generation system,
we carried out error analysis on our development
set WSJ Section 22 of the Penn-II Treebank. We
identified some initial pre-training transformations
to the treebank that help generation.
Punctuation: Punctuation is not usually en-
coded in f-structure representations. Because our
architecture is completely driven by rules con-
ditioned by f-structure information automatically
extracted from an f-structure annotated treebank,
its placement of punctuation is not principled.
This led to anomalies such as full stops appear-
ing mid sentence and quotation marks appearing
in undesired locations. One partial solution to this
was to reduce the amount of punctuation that the
system trained on. We removed all punctuation
apart from commas and full stops from the train-
ing data. We did not remove any punctuation from
the evaluation test set (Section 23), but our system
will ever only produce commas and full stops. In
the evaluation (Tables 3, 4, 5 and 6) we are pe-
nalised for the missing punctuation. To solve the
problem of full stops appearing mid sentence, we
carry out a punctuation post-processing step on all
generated strings. This removes mid-sentence full
stops and adds missing full stops at the end of gen-
erated sentences prior to evaluation. We are work-
ing on a more appropriate solution allowing the
system to generate all punctuation.
Case: English does not have much case mark-
ing, and for parsing no special treatment was en-
coded. However, when generating, it is very
important that the first person singular pronoun
is I in the nominative case and me in the ac-
cusative. Given the original grammar used in pars-
ing, our generation system was not able to distin-
guish nominative from accusative contexts. The
solution we implemented was to carry out a gram-
mar transformation in a pre-processing step, to au-
tomatically annotate personal pronouns with their
case information. This resulted in phrasal and lex-
ical rules such as NP(?SUBJ) ? PRP?nom(?=?)
and PRP?nom(?=?) ? I and greatly improved the
accuracy of the pronouns generated.
4.2 String-Based Evaluation
We evaluate the output of our generation system
against the raw strings of Section 23 using the
Simple String Accuracy and BLEU (Papineni et
al., 2002) evaluation metrics. Simple String Accu-
racy is based on the string edit distance between
the output of the generation system and the gold
standard sentence. BLEU is the weighted average
of n-gram precision against the gold standard sen-
tences. We also measure coverage as the percent-
age of input f-structures that generate a string. For
evaluation, we automatically expand all contracted
words. We only evaluate strings produced by the
system (similar to Nakanishi et al (2005)).
We conduct a total of four experiments. The
parameters we investigate are lexical smoothing
(Section 3.3) and partial output. Partial output
is a robustness feature for cases where a sub-f-
structure component fails to generate a string and
the system outputs a concatenation of the strings
generated by the remaining components, rather
than fail completely.
1037
Sentence length of Evaluation Section 23 Sentences of length:
Training Data Metric ? 20 ? 25 ? 30 ? 40 all
? 20 BLEU 0.6812 0.6601 0.6373 0.6013 0.5793
String Accuracy 0.7274 0.7052 0.6875 0.6572 0.6431
Coverage 96.52 95.83 94.59 93.76 93.92
? 25 BLEU 0.6915 0.6800 0.6696 0.6396 0.6233
String Accuracy 0.7262 0.7095 0.6983 0.6731 0.6618
Coverage 96.52 95.83 94.59 93.76 93.92
? 30 BLEU 0.6979 0.6881 0.6792 0.6576 0.6445
String Accuracy 0.7317 0.7169 0.7075 0.6853 0.6749
Coverage 97.97 97.95 97.41 97.15 97.31
? 40 BLEU 0.7045 0.6951 0.6852 0.6715 0.6605
String Accuracy 0.7349 0.7212 0.7074 0.6881 0.6788
Coverage 98.45 98.36 98.01 97.82 97.93
all BLEU 0.7077 0.6974 0.6859 0.6734 0.6651
String Accuracy 0.7373 0.7221 0.7087 0.6894 0.6808
Coverage 98.65 98.5 98.12 97.95 98.05
Table 3: Generation +partial output +lexical smoothing
Sentence length of Evaluation Section 23 Sentences of length:
Training Data Metric ? 20 ? 25 ? 30 ? 40 all
all BLEU 0.6253 0.6097 0.5887 0.5730 0.5590
String Accuracy 0.6886 0.6688 0.6513 0.6317 0.6207
Coverage 91.20 91.19 90.84 90.33 90.11
Table 4: Generation +partial output -lexical smoothing
Varying the length of the sentences included in
the training data (Tables 3 and 5) shows that re-
sults improve (both in terms of coverage and string
quality) as the length of sentence included in the
training data increases.
Tables 3 and 5 give the results for the exper-
iments including lexical smoothing and varying
partial output. Table 3 (+partial, +smoothing)
shows that training on sentences of all lengths and
evaluating all strings (including partial outputs),
our system achieves coverage of 98.05%, a BLEU
score of 0.6651 and string accuracy of 0.6808. Ta-
ble 5 (-partial, +smoothing) shows that coverage
drops to 89.49%, BLEU score increases to 0.6979
and string accuracy to 0.7012, when the system
is trained on sentences of all lengths. Similarly,
for strings ?20, coverage drops from 98.65% to
95.26%, BLEU increases from 0.7077 to 0.7227
and String Accuracy from 0.7373 to 0.7476. In-
cluding partial output increases coverage (by more
than 8.5 percentage points for all sentences) and
hence robustness while slightly decreasing quality.
Tables 3 (+partial, +smoothing) and 4 (+partial,
-smoothing) give results for the experiments in-
cluding partial output but varying lexical smooth-
ing. With no lexical smoothing (Table 4), the
system (trained on all sentence lengths) produces
strings for 90.11% of the input f-structures and
achieves a BLEU score of 0.5590 and string ac-
curacy of 0.6207. Switching off lexical smooth-
ing has a negative effect on all evaluation met-
rics (coverage and quality), because many more
strings produced are now partial (since for PRED
values unseen during training, no lexical entries
are added to the chart).
Comparing Tables 5 (-partial, +smoothing)
and 6 (-partial, -smoothing), where the system
does not produce any partial outputs and lexi-
cal smoothing is varied, shows that training on
all sentence lengths, BLEU score increases from
0.6979 to 0.7147 and string accuracy increases
from 0.7012 to 0.7192. At the same time, cover-
age drops dramatically from 89.49% (Table 5) to
47.60% (Table 6).
Comparing Tables 4 and 6 shows that while par-
tial output almost doubles coverage, this comes
at a price of a severe drop in quality (BLEU
score drops from 0.7147 to 0.5590). On the other
hand, comparing Tables 5 and 6 shows that lexical
smoothing achieves a similar increase in coverage
with only a very slight drop in quality.
5 Discussion
Nakanishi et al (2005) achieve 90.56% cover-
age and a BLEU score of 0.7723 on Section 23
1038
Sentence length of Evaluation Section 23 Sentences of length:
Training Data Metric ? 20 ? 25 ? 30 ? 40 all
? 20 BLEU 0.7326 0.7185 0.7165 0.7082 0.7052
String Accuracy 0.76 0.7428 0.7363 0.722 0.7175
Coverage 85.49 81.56 77.26 71.94 69.08
? 25 BLEU 0.7300 0.7235 0.7218 0.7118 0.7077
String Accuracy 0.7517 0.7382 0.7315 0.7172 0.7116
Coverage 89.65 87.77 84.38 80.31 78.56
? 30 BLEU 0.7207 0.7125 0.7107 0.6991 0.6946
String Accuracy 0.747 0.7336 0.7275 0.711 0.7045
Coverage 93.23 92.14 89.74 86.59 85.18
? 40 BLEU 0.7221 0.7140 0.7106 0.7016 0.6976
String Accuracy 0.746 0.7331 0.7236 0.7072 0.7001
Coverage 94.58 93.85 91.89 89.62 88.33
all BLEU 0.7227 0.7145 0.7095 0.7011 0.6979
String Accuracy 0.7476 0.7331 0.7239 0.7077 0.7012
Coverage 95.26 94.40 92.55 90.69 89.49
Table 5: Generation -partial output +lexical smoothing
Sentence length of Evaluation Section 23 Sentences of length:
Training Data Metric ? 20 ? 25 ? 30 ? 40 all
all BLEU 0.7272 0.7237 0.7201 0.7160 0.7147
String Accuracy 0.7547 0.7436 0.7361 0.7237 0.7192
Coverage 61.99 57.38 53.64 47.60 47.60
Table 6: Generation -partial output -lexical smoothing
sentences, restricted to length ?20 for efficiency
reasons. Langkilde-Geary?s (2002) best system
achieves 82.8% coverage, a BLEU score of 0.924
and string accuracy of 0.945 against Section 23
sentences of all lengths. Callaway (2003) achieves
98.7% coverage and a string accuracy of 0.6607
on sentences of all lengths. Our best results for
sentences of length ? 20 are coverage of 95.26%,
BLEU score of 0.7227 and string accuracy of
0.7476. For all sentence lengths, our best results
are coverage of 89.49%, a BLEU score of 0.6979
and string accuracy of 0.7012.
Using hand-crafted grammar-based genera-
tion systems (Langkilde-Geary, 2002; Callaway,
2003), it is possible to achieve very high results.
However, hand-crafted systems are expensive to
construct and not easily ported to new domains or
other languages. Our methodology, on the other
hand, is based on resources automatically acquired
from treebanks and easily ported to new domains
and languages, simply by retraining on suitable
data. Recent work on the automatic acquisition
of multilingual LFG resources from treebanks for
Chinese, German and Spanish (Burke et al, 2004;
Cahill et al, 2005; O?Donovan et al, 2005) has
shown that given a suitable treebank, it is possi-
ble to automatically acquire high quality LFG re-
sources in a very short space of time. The genera-
tion architecture presented here is easily ported to
those different languages and treebanks.
6 Conclusion and Further Work
We present a new architecture for stochastic LFG
surface realisation using the automatically anno-
tated treebanks and extracted PCFG-based LFG
approximations of Cahill et al (2004). Our model
maximises the probability of a tree given an f-
structure, supporting a simple and efficient imple-
mentation that scales to wide-coverage treebank-
based resources. An improved model would
maximise the probability of a string given an f-
structure by summing over trees with the same
yield. More research is required to implement
such a model efficiently using packed representa-
tions (Carroll and Oepen, 2005). Simple PCFG-
based models, while effective and computationally
efficient, can only provide approximations to LFG
and similar constraint-based formalisms (Abney,
1997). Research on discriminative disambigua-
tion methods (Valldal and Oepen, 2005; Nakanishi
et al, 2005) is important. Kaplan and Wedekind
(2000) show that for certain linguistically interest-
ing classes of LFG (and PATR etc.) grammars,
generation from f-structures yields a context free
language. Their proof involves the notion of a
1039
?refinement? grammar where f-structure informa-
tion is compiled into CFG rules. Our probabilis-
tic generation grammars bear a conceptual similar-
ity to Kaplan and Wedekind?s ?refinement? gram-
mars. It would be interesting to explore possible
connections between the treebank-based empirical
work presented here and the theoretical constructs
in Kaplan and Wedekind?s proofs.
We presented a full set of generation experi-
ments on varying sentence lengths training on Sec-
tions 02?21 of the Penn Treebank and evaluat-
ing on Section 23 strings. Sentences of length
?20 achieve coverage of 95.26%, BLEU score
of 0.7227 and string accuracy of 0.7476 against
the raw Section 23 text. Sentences of all lengths
achieve coverage of 89.49%, BLEU score of
0.6979 and string accuracy of 0.7012. Our method
is robust and can cope with noise in the f-structure
input to generation and will attempt to produce
partial output rather than fail.
Acknowledgements
We gratefully acknowledge support from Science
Foundation Ireland grant 04/BR/CS0370 for the
research reported in this paper.
References
Stephen Abney. 1997. Stochastic Attribute-Value Gram-
mars. Computational Linguistics, 23(4):597?618.
Harald Baayen and Richard Sproat. 1996. Estimating lexi-
cal priors for low-frequency morphologically ambiguous
forms. Computational Linguistics, 22(2):155?166.
Srinivas Bangalore and Owen Rambow. 2000. Exploit-
ing a probabilistic hierarchical model for generation. In
Proceedings of COLING 2000, pages 42?48, Saarbrcken,
Germany.
Srinivas Bangalore, John Chen, and Owen Rambow. 2001.
Impact of quality and quantity of corpora on stochastic
generation. In Proceedings of EMNLP 2001, pages 159?
166.
Anja Belz. 2005. Statistical generation: Three methods com-
pared and evaluated. In Proceedings of the 10th European
Workshop on Natural Language Generation (ENLG? 05),
pages 15?23, Aberdeen, Scotland.
Michael Burke, Olivia Lam, Rowena Chan, Aoife Cahill,
Ruth O?Donovan, Adams Bodomo, Josef van Genabith,
and Andy Way. 2004. Treebank-Based Acquisition of a
Chinese Lexical-Functional Grammar. In Proceedings of
the 18th Pacific Asia Conference on Language, Informa-
tion and Computation, pages 161?172, Tokyo, Japan.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef van
Genabith, and Andy Way. 2004. Long-Distance De-
pendency Resolution in Automatically Acquired Wide-
Coverage PCFG-Based LFG Approximations. In Pro-
ceedings of ACL-04, pages 320?327, Barcelona, Spain.
Aoife Cahill, Martin Forst, Michael Burke, Mairead Mc-
Carthy, Ruth O?Donovan, Christian Rohrer, Josef van
Genabith, and Andy Way. 2005. Treebank-based acquisi-
tion of multilingual unification grammar resources. Jour-
nal of Research on Language and Computation; Special
Issue on ?Shared Representations in Multilingual Gram-
mar Engineering?, pages 247?279.
Charles B. Callaway. 2003. Evaluating coverage for large
symbolic NLG grammars. In Proceedings of the Eigh-
teenth International Joint Conference on Artificial Intelli-
gence, pages 811?817, Acapulco, Mexico.
John Carroll and Stephan Oepen. 2005. High efficiency real-
ization for a wide-coverage unification grammar. In Pro-
ceedings of IJCNLP05, pages 165?176, Jeju Island, Ko-
rea.
Ron Kaplan and Joan Bresnan. 1982. Lexical Functional
Grammar, a Formal System for Grammatical Representa-
tion. In Joan Bresnan, editor, The Mental Representation
of Grammatical Relations, pages 173?281. MIT Press,
Cambridge, MA.
Ron Kaplan and Juergen Wedekind. 2000. LFG Generation
produces Context-free languages. In Proceedings of COL-
ING 2000, pages 141?148, Saarbruecken, Germany.
Martin Kay. 1996. Chart Generation. In Proceedings of the
34th Annual Meeting of the Association for Computational
Linguistics, pages 200?204, Santa Cruz, CA.
Irene Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sentence
generator. In Second International Natural Language
Generation Conference, pages 17?24, Harriman, NY.
Irene Langkilde. 2000. Forest-based statistical sentence gen-
eration. In Proceedings of NAACL 2000, pages 170?177,
Seattle, WA.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The Penn Treebank: An-
notating Predicate Argument Structure. In Proceedings
of the ARPA Workshop on Human Language Technology,
pages 110?115, Princton, NJ.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii. 2005.
Probabilistic models for disambiguation of an HPSG-
based chart generator. In Proceedings of the International
Workshop on Parsing Technology, Vancouver, Canada.
Ruth O?Donovan, Aoife Cahill, Josef van Genabith, and
Andy Way. 2005. Automatic Acquisition of Spanish LFG
Resources from the CAST3LB Treebank. In Proceedings
of LFG 05, pages 334?352, Bergen, Norway.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing
Zhu. 2002. BLEU: a Method for Automatic Evaluation of
Machine Translation. In Proceedings of ACL 2002, pages
311?318, Philadelphia, PA.
Adwait Ratnaparkhi. 2000. Trainable methods for natu-
ral language generation. In Proceedings of NAACL 2000,
pages 194?201, Seattle, WA.
Erik Valldal and Stephan Oepen. 2005. Maximum En-
tropy Models for Realization Reranking. In Proceedings
of the 10th Machine Translation Summit, pages 109?116,
Phuket, Thailand.
1040
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 136?143,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using Machine-Learning to Assign Function Labels to Parser
Output for Spanish
Grzegorz Chrupa?a1 and Josef van Genabith1,2
1National Center for Language Technology
Dublin City University
Glasnevin, Dublin 9, Ireland
2IBM Dublin Center for Advanced Studies
grzegorz.chrupala@computing.dcu.ie
josef@computing.dcu.ie
Abstract
Data-driven grammatical function tag as-
signment has been studied for English us-
ing the Penn-II Treebank data. In this pa-
per we address the question of whether
such methods can be applied success-
fully to other languages and treebank re-
sources. In addition to tag assignment ac-
curacy and f-scores we also present re-
sults of a task-based evaluation. We use
three machine-learning methods to assign
Cast3LB function tags to sentences parsed
with Bikel?s parser trained on the Cast3LB
treebank. The best performing method,
SVM, achieves an f-score of 86.87% on
gold-standard trees and 66.67% on parser
output - a statistically significant improve-
ment of 6.74% over the baseline. In a
task-based evaluation we generate LFG
functional-structures from the function-
tag-enriched trees. On this task we achive
an f-score of 75.67%, a statistically signif-
icant 3.4% improvement over the baseline.
1 Introduction
The research presented in this paper forms
part of an ongoing effort to develop methods
to induce wide-coverage multilingual Lexical-
Functional Grammar (LFG) (Bresnan, 2001) re-
sources from treebanks by means of automatically
associating LFG f-structure information with con-
stituency trees produced by probabilistic parsers
(Cahill et al, 2004). Inducing deep syntactic anal-
yses from treebank data avoids the cost and time
involved in manually creating wide-coverage re-
sources.
Lexical Functional Grammar f-structures pro-
vide a level of syntactic representation based on
the notion of grammatical functions (e.g. Sub-
ject, Object, Oblique, Adjunct etc.). This level
is more abstract and cross-linguistically more uni-
form than constituency trees. F-structures also in-
clude explicit encodings of phenomena such as
control and raising, pro-drop or long distance de-
pendencies. Those characteristics make this level
a suitable representation for many NLP applica-
tions such as transfer-based Machine Translation
or Question Answering.
The f-structure annotation algorithm used for
inducing LFG resources from the Penn-II treebank
for English (Cahill et al, 2004) uses configura-
tional, categorial, function tag and trace informa-
tion. In contrast to English, in many other lan-
guages configurational information is not a good
predictor for LFG grammatical function assign-
ment. For such languages the function tags in-
cluded in many treebanks are a much more impor-
tant source of information for the LFG annotation
algorithm than Penn-II tags are for English.
Cast3LB (Civit and Mart??, 2004), the Spanish
treebank used in the current research, contains
comprehensive grammatical function annotation.
In the present paper we use a machine-learning ap-
proach in order to add Cast3LB function tags to
nodes of basic constituent trees output by a prob-
abilistic parser trained on Cast3LB. To our knowl-
edge, this paper is the first to describe applying
a data-driven approach to function-tag assignment
to a language other than English.
Our method statistically significantly outper-
forms the previously used approach which relied
exclusively on the parser to produce trees with
Cast3LB tags (O?Donovan et al, 2005). Addi-
tionally, we perform a task-driven evaluation of
our Cast3LB tag assignment method by using the
tag-enriched trees as input to the Spanish LFG f-
structure annotation algorithm and evaluating the
quality of the resulting f-structures.
Section 2 describes the Spanish Cast3LB tree-
bank. In Section 3 we describe previous research
in LFG induction for English and Spanish as well
136
as research on data-driven function tag assign-
ment to parsed text in English. Section 4 provides
the details of our approach to the Cast3LB func-
tion tag assignment task. In Sections 5 and 6 we
present evaluation results for our method. In Sec-
tion 7 we present the error analysis of the results.
Finally, in Section 8 we conclude and discuss ideas
for further research.
2 The Spanish Treebank
As input to our LFG annotation algorithm we use
the output of Bikel?s parser (Bikel, 2002) trained
on the Cast3LB treebank (Civit and Mart??, 2004).
Cast3LB contains around 3,500 constituency trees
(100,000 words) taken from different genres of
European and Latin American Spanish. The POS
tags used in Cast3LB encode morphological infor-
mation in addition to Part-of-Speech information.
Due to the relatively flexible order of main sen-
tence constituents in Spanish, Cast3LB uses a flat,
multiply-branching structure for the S node. There
is no VP node, but rather all complements and ad-
juncts depending on a verb are sisters to the gv
(Verb Group) node containing this verb. An exam-
ple sentence (with the corresponding f-structure)
is shown in Figure 1.
Tree nodes are additionally labelled with gram-
matical function tags. Table 1 provides a list of
function tags with short explanations. Civit (2004)
provides Cast3LB function tag guidelines.
Functional tags carry some of the information
that would be encoded in terms of tree configura-
tions in languages with stricter constituent order
constraints than Spanish.
3 Previous Work
3.1 LFG Annotation
A methodology for automatically obtaining LFG
f-structures from trees output by probabilistic
parsers trained on the Penn-II treebank has been
described by Cahill et al (2004). It has been
shown that the methods can be ported to other lan-
guages and treebanks (Burke et al, 2004; Cahill et
al., 2003), including Cast3LB (O?Donovan et al,
2005).
Some properties of Spanish and the encoding
of syntactic information in the Cast3LB treebank
make it non-trivial to apply the method of auto-
matically mapping c-structures to f-structures used
by Cahill et al (2004), which assigns grammatical
Tag Meaning
ATR Attribute of copular verb
CAG Agent of passive verb
CC Compl. of circumstance
CD Direct object
CD.Q Direct object of quantity
CI Indirect object
CPRED Predicative complement
CPRED.CD Predicative of Direct Object
CPRED.SUJ Predicative of Subject
CREG Prepositional object
ET Textual element
IMPERS Impersonal marker
MOD Verbal modifier
NEG Negation
PASS Passive marker
SUJ Subject
VOC Vocative
Table 1: List of function tags in Cast3LB.
functions to tree nodes based on their phrasal cat-
egory, the category of the mother node and their
position relative to the local head.
In Spanish, the order of sentence constituents
is flexible and their position relative to the head
is an imperfect predictor of grammatical function.
Also, much of the information that the Penn-II
Treebank encodes in terms of tree configurations
is encoded in Cast3LB in the form of function
tags. As Cast3LB trees lack a VP node, the con-
figurational information normally used in English
to distinguish Subjects (NP which is left sister to
VP) from Direct Objects (NP which is right sister
to V) is not available in Cast3LB-style trees. This
means that assigning correct LFG functional an-
notations to nodes in Cast3LB trees is rather dif-
ficult without use of Cast3LB function tags, and
those tags are typically absent in output generated
by probabilistic parsers.
In order to solve this difficulty, O?Donovan et
al. (2005) train Bikel?s parser to output complex
category-function labels. A complex label such as
sn-SUJ (an NP node tagged with the Subject gram-
matical function) is treated as an atomic category
in the training data, and is output in the trees pro-
duced by the parser. This baseline process is rep-
resented in Figure 2.
This approach can be problematic for two main
reasons. Firstly, by treating complex labels as
atomic categories the number of unique labels in-
creases and parse quality can deteriorate due to
sparse data problems. Secondly, this approach, by
relying on the parser to assign function tags, offers
137
Sneg-NEG
no
not
gv
espere
expect
sn-SUJ
el lector
the reader
sn-CD
una definicio?n
a definition
?
?????????????
PRED ?esperar?SUBJ,OBJ??
NEG +
TENSE PRES
MOOD SUBJUNCTIVE
SUBJ
[
SPEC
[
SPEC-FORM EL]
PRED ?lector?
]
OBJ
[
SPEC
[
SPEC-FORM UNO]
PRED ?definicio?n?
]
?
?????????????
Figure 1: On the left flat structure of S. Cast3LB function tags are shown in bold. On the right the
corresponding (simplified) LFG f-structure. Translation: Let the reader not expect a definition.
Figure 2: Processing architecture for the baseline.
limited control over, or room for improvement in,
this task.
3.2 Adding Function Tags to Parser Output
The solution we adopt instead is to add Cast3LB
functional tags to simple constituent trees output
by the parser, as a postprocessing step. For En-
glish, such approaches have been shown to give
good results for the output of parsers trained on
the Penn-II Treebank.
Blaheta and Charniak (2000) use a probabilis-
tic model with feature dependencies encoded by
means of feature trees to add Penn-II Treebank
function tags to Charniak?s parser output. They re-
port an f-score 88.472% on original treebank trees
and 87.277% on the correctly parsed subset of tree
nodes.
Jijkoun and de Rijke (2004) describe a method
of enriching output of a parser with information
that is included in the original Penn-II trees, such
as function tags, empty nodes and coindexations.
They first transform Penn trees to a dependency
format and then use memory-based learning to
perform various graph transformations. One of the
transformations is node relabelling, which adds
function tags to parser output. They report an f-
score of 88.5% for the task of function tagging on
correctly parsed constituents.
4 Assigning Cast3LB Function Tags to
Parsed Spanish Text
The complete processing architecture of our ap-
proach is depicted in Figure 3. We describe it in
detail in this and the following sections.
We divided the Spanish treebank into a training
set of 80%, a development set of 10%, and a test
set of 10% of all trees. We randomly assigned tree-
bank files to these sets to ensure that different tex-
tual genres are about equally represented among
the training, development and test trees.
4.1 Constituency Parsing
For constituency parsing we use Bikel?s (2002)
parser for which we developed a Spanish language
package adapted to the Cast3LB data. Prior to
parsing, we perform one of the tree transforma-
tions described by Cowan and Collins (2005), i.e.
we add a CP and SBAR nodes to subordinate and
relative clauses. This is undone in parser output.
The category labels in the Spanish treebank are
rather fine grained and often contain redundant in-
formation.1 We preprocess the treebank and re-
1For example there are several labels for Nominal Group,
138
Figure 3: Processing architecture for the machine-
learning-based method.
duce the number of category labels, only retaining
distinctions that we deem useful for our purposes.2
For constituency parsing we also reduce the
number of POS tags by including only selected
morphological features. Table 2 provides the
list of features included for the different parts of
speech. In our experiments we use gold standard
POS tagged development and test-set sentences as
input rather than tagging text automatically.
The results of the evaluation of parsing perfor-
mance on the test set are shown in Table 3. La-
belled bracketing f-score for all sentences is just
below 84% for all sentences, and 84.58% for sen-
tences of length ? 70. In comparison, Cowan
and Collins (2005) report an f-score of 85.1%
(? 70) using a version of Collins? parser adapted
for Cast3LB, and using reranking to boost perfor-
such as grup.nom.ms (masculine singular), grup.nom.fs (fem-
inine singular), grup.nom.mp (masculine plural) etc. This
number and gender information is already encoded in the
POS tags of nouns heading these constituents.
2The labels we retain are the following: INC, S, S.NF,
S.NF.R, S.NF, S.R, conj.subord, coord, data, espec, gerundi,
grup.nom, gv, infinitiu, interjeccio, morf, neg, numero, prep,
relatiu, s.a, sa, sadv, sn, sp, and versions of those suffixed
with .co to indicate coordination).
Part of Speech Features included
Determiner type, number
Noun type, number
Adjective type, number
Pronoun type, number, person
Verb type, number, mood
Adverb type
Conjunction type
Table 2: Features included in POS tags. Type
refers to subcategories of parts of speech such as
e.g. common and proper for nouns, or main, aux-
iliary and semiauxiliary for verbs. For details see
(Civit, 2000).
LB Precision LB Recall F-score
All 84.18 83.74 83.96
? 70 84.82 84.35 84.58
Table 3: Parser performance.
mance. They use a different, more reduced cat-
egory label set as well as a different training-test
split. Both Cowan and Collins and the present pa-
per report scores which ignore punctuation.
4.2 Cast3LB Function Tagging
For the task of Cast3LB function tag assign-
ment we experimented with three generic machine
learning algorithms: a memory-based learner
(Daelemans and van den Bosch, 2005), a maxi-
mum entropy classifier (Berger et al, 1996) and a
Support Vector Machine classifier (Vapnik, 1998).
For each algorithm we use the same set of features
to represent nodes that are to be assigned one of
the Cast3LB function tags. We use a special null
tag for nodes where no Cast3LB tag is present.
In Cast3LB only nodes in certain contexts are
eligible for function tags. For this reason we only
consider a subset of all nodes as candidates for
function tag assignment, namely those which are
sisters of nodes with the category labels gv (Verb
Group), infinitiu (Infinitive) and gerundi (Gerund).
For these candidates we extract the following three
types of features encoding configurational, mor-
phological and lexical information for the target
node and neighboring context nodes:
? Node features: position relative to head, head
lemma, alternative head lemma (i.e. the head
of NP in PP), head POS, category, definite-
ness, agreement with head verb, yield, hu-
man/nonhuman
139
? Local features: head verb, verb person, verb
number, parent category
? Context features: node features (except posi-
tion) of the two previous and two following
sister nodes (if present).
We used cross-validation for refining the set
of features and for tuning the parameters of the
machine-learning algorithms. We did not use any
additional automated feature-selection procedure.
We made use of the following implementations:
TiMBL (Daelemans et al, 2004) for Memory-
Based Learning, the MaxEnt Toolkit (Le, 2004)
for Maximum Entropy and LIBSVM (Chang and
Lin, 2001) for Support Vector Machines. For
TiMBL we used k nearest neighbors = 7 and the
gain ratio metric for feature weighting. For Max-
Ent, we used the L-BFGS parameter estimation
and 110 iterations, and we regularize the model
using a Gaussian prior with ?2 = 1. For SVM we
used the RBF kernel with ? = 2?7 and the cost
parameter C = 32.
5 Cast3LB Tag Assignment Evaluation
We present evaluation results on the original gold-
standard trees of the test set as well as on the
test-set sentences parsed by Bikel?s parser. For
the evaluation of Cast3LB function tagging per-
formance on gold trees the most straightforward
metric is the accuracy, or the proportion of all can-
didate nodes that were assigned the correct label.
However we cannot use this metric for evalu-
ating results on the parser output. The trees out-
put by the parser are not identical to gold standard
trees due to parsing errors, and the set of candi-
date nodes extracted from parsed trees will not be
the same as for gold trees. For this reason we use
an alternative metric which is independent of tree
configuration and uses only the Cast3LB function
labels and positional indices of tokens in a sen-
tence. For each function-tagged tree we first re-
move the punctuation tokens. Then we extract a
set of tuples of the form ?GF, i, j?, where GF is
the Cast3LB function tag and i ? j is the range
of tokens spanned by the node annotated with this
function. We use the standard measures of preci-
sion, recall and f-score to evaluate the results.
Results for the three algorithms are shown in
Table 4. MBL and MaxEnt show a very sim-
ilar performance, while SVM outperforms both,
t
t
t
t
t
7.0 7.5 8.0 8.5 9.0 9.5
0.7
6
0.8
0
0.8
4
0.8
8
log(n)
Ac
cu
rac
y
s
s
s
s
s
m
m
m
m
m
Figure 4: Learning curves for TiMBL (t), MaxEnt
(m) and SVM (s).
Acc. Prec. Recall F-score
MBL 87.55 87.00 82.98 84.94
MaxEnt 88.06 87.66 86.87 85.52
SVM 89.34 88.93 84.90 86.87
Table 4: Cast3LB function tagging performance
for gold-standard trees
scoring 89.34% on accuracy and 86.87% on f-
score. The learning curves for the three algo-
rithms, shown in Figure 4, are also informative,
with SVM outperforming the other two methods
for all training set sizes. In particular, the last sec-
tion of the plot shows SVM performing almost as
well as MBL with half as much learning material.
Neither of the three curves shows signs of hav-
ing reached a maximum, which indicates that in-
Precision Recall F-score
all corr. all corr. all corr.
Baseline 59.26 72.63 60.61 75.35 59.93 73.96
MBL 64.74 78.09 64.18 78.75 64.46 78.42
MaxEnt 65.48 78.90 64.55 79.44 65.01 79.17
SVM 66.96 80.58 66.38 81.27 66.67 80.92
Table 5: Cast3LB function tagging performance
for parser output, for all constituents, and for cor-
rectly parsed constituents only
140
Methods p-value
Baseline vs SVM 1.169? 10?9
Baseline vs MBL 2.117? 10?6
MBL vs MaxEnt 0.0799
MaxEnt vs SVM 0.0005
Table 6: Statistical significance testing results on
for the Cast3LB tag assignment on parser output.
Precision Recall F-score
Baseline 73.95 70.67 72.27
SVM 76.90 74.48 75.67
Table 7: LFG F-structure evaluation results for
parser output
creasing the size of the training data should result
in further improvements in performance.
Table 5 shows the performance of the three
methods on parser output. The baseline con-
tains the results achieved by treating compound
category-function labels as atomic during parser
training so that they are included in parser output.
For this task we present two sets of results: (i) for
all constituents, and (ii) for correctly parsed con-
stituents only. Again the best algorithm turns out
to be SVM. It outperforms the baseline by a large
margin (6.74% for all constituents).
The difference in performance for gold stan-
dard trees, and the correctly parsed constituents
in parser output is rather larger than what Blaheta
and Charniak report. Further analysis is needed
to identify the source of this difference but we
suspect that one contributing factor is the use of
greater number of context features combined with
a higher parse error rate in comparison to their ex-
periments on the Penn II Treebank. Since any mis-
analysis of constituency structure in the vicinity of
target node can have negative impact, greater re-
liance on context means greater susceptibility to
parse errors. Another factor to consider is the fact
that we trained and adjusted parameters on gold-
standard trees, and the model learned may rely on
features of those trees that the parser is unable to
reproduce.
For the experiments on parser output (all con-
stituents) we performed a series of sign tests in
order to determine to what extent the differences
in performance between the different methods are
statistically significant. For each pair of methods
we calculate the f-score for each sentence in the
test set. For those sentences on which the scores
differ (i.e. the number of trials) we calculate in
how many cases the second method is better than
the first (i.e. the number of successes). We then
perform the test with the null hypothesis that the
probability of success is chance (= 0.5) and the
alternative hypothesis that the probability of suc-
cess is greater than chance (> 0.5). The results
are summarized in Table 6. Given that we perform
4 pairwise comparisons, we apply the Bonferroni
correction and adjust our target ?? = ?4 . For the
confidence level 95% (?? = 0.0125) all pairs give
statistically significant results, except for MBL vs
MaxEnt.
6 Task-Based LFG Annotation
Evaluation
Finally, we also evaluated the actual f-structures
obtained by running the LFG-annotation algo-
rithm on trees produced by the parser and enriched
with Cast3LB function tags assigned using SVM.
For this task-based evaluation we produced a gold
standard consisting of f-structures corresponding
to all sentences in the test set. The LFG-annotation
algorithm was run on the test set trees (which con-
tained original Cast3LB treebank function tags),
and the resulting f-structures were manually cor-
rected.
Following Crouch et al (2002), we convert
the f-structures to triples of the form ?GF,Pi, Pj?,
where Pi is the value of the PRED attribute of the
f-structure, GF is an LFG grammatical function
attribute, and Pj is the value of the PRED attribute
of the f-structure which is the value of the GF
attribute. This is done recursively for each level
of embedding in the f-structure. Attributes with
atomic values are ignored for the purposes of this
evaluation. The results obtained are shown in Ta-
ble 7. We also performed a statistical significance
test for these results, using the same method as for
the Cast3LB tag assigment task. The p-value given
by the sign test was 2.118?10?5, comfortably be-
low ? = 1%.
The higher scores achieved in the LFG f-
structure evaluation in comparison with the pre-
ceding Cast3LB tag assignment evaluation (Table
5) can be attributed to two main factors. Firstly,
the mapping from Cast3LB tags to LFG grammat-
ical functions is not one-to-one. For example three
Cast3LB tags (CC, MOD and ET) are all mapped
to LFG ADJUNCT. Thus mistagging a MOD as
141
ATR CC CD CI CREG MOD SUJ
ATR 136 2 0 0 0 0 5
CC 6 552 12 4 25 18 6
CD 1 19 418 5 3 0 26
CI 0 6 1 50 1 0 0
CREG 0 6 0 2 43 0 0
MOD 0 0 0 0 0 19 0
SUJ 0 8 24 2 0 0 465
Table 8: Simplified confusion matrix for SVM
on test-set gold-standard trees. The gold-standard
Cast3LB function tags are shown in the first row,
the predicted tags in the first column. So e.g. SUJ
was mistagged as CD in 26 cases. Low frequency
function tags as well as those rarely mispredicted
have been omitted for clarity.
CC does not affect the f-structure score. On the
other hand the Cast3LB CD tag can be mapped
to OBJ, COMP, or XCOMP, and it can be easily
decided which one is appropriate depending on
the category label of the target node. Addition-
ally many nodes which receive no function tag in
Cast3LB, such as noun modifiers, are straightfor-
wardly mapped to LFG ADJUNCT. Similarly, ob-
jects of prepositions receive the LFG OBJ function.
Secondly, the f-structure evaluation metric is
less sensitive to small constituency misconfigura-
tions: it is not necessary to correctly identify the
token range spanned by a target node as long as the
head (which provides the PRED attribute) is cor-
rect.
7 Error Analysis
In order to understand sources of error and de-
termine how much room for further improvement
there is, we examined the most common cases of
Cast3LB function mistagging. A simplified confu-
sion matrix with the most common Cast3LB tags
is shown in Table 8. The most common mistakes
occur between SUJ and CD, in both directions, and
many also CREGs are erroneously tagged as CC.
7.1 Subject vs Direct Object
We noticed that in over 50% of cases when a
Direct Object (CD) was misidentified as Subject
(SUJ), the target node?s mother was a relative
clause. It turns out that in Spanish relative clauses
genuine syntactic ambiguity is not uncommon.
Consider the following Spanish phrase:
(1) Sistemas
Systems
que
which
usan
use
el
DET
95%
95%
de
of
los
DET
ordenadores.
computers
Its translation into English is either Systems that
use 95% of computers or alternatively Systems that
95% of computers use. In Spanish, unlike in En-
glish, preverbal / postverbal position of a con-
stituent is not a good guide to its grammatical
function in this and similar contexts. Human an-
notators can use their world knowledge to decide
on the correct semantic role of a target constituent
and use it in assigning a correct grammatical func-
tion, but such information is obviously not used
in our machine learning methods. Thus such mis-
takes seem likely to remain unresolvable in our
current approach.
7.2 Prepositional Object vs Adjunct
The frequent misidentification of Prepositional
Objects (CREG) as Adjuncts (CC) seen in Table 8
can be accounted for by several factors. Firstly,
Prepositional Objects are strongly dependent on
specific verbs and the comparatively small size of
our training data means that there is limited oppor-
tunity for a machine-learning algorithm to learn
low-frequency lexical dependencies. Here the ob-
vious solution is to use a more adequate amount of
training material when it becomes available.
A further problem with the Prepositional Object
- Adjunct distinction is its inherent fuzziness. Be-
cause of this, treebank designers may fail to pro-
vide easy-to-follow, clearcut guidelines and hu-
man annotators necessarily exercise a certain de-
gree of arbitrariness in assigning one or the other
function.
8 Conclusions and Future Research
Our research has shown that machine-learning-
based Cast3LB tag assignment as a post-
processing step to raw tree parser output statisti-
cally significantly outperforms a baseline where
the parser itself is trained to learn category
/ Cast3LB-function pairs. In contrast to the
parser-based method, the machine-learning-based
method avoids some sparse data problems and al-
lows for more control over Cast3LB tag assign-
ment. We have found that the SVM algorithm out-
performs the other two machine learning methods
used.
142
In addition, we evaluated Cast3LB tag assign-
ment in a task-based setting in the context of au-
tomatically acquiring LFG resources for Spanish
from Cast3LB. Machine-learning-based Cast3LB
tag assignment yields statistically-significantly
improved LFG f-structures compared to parser-
based assignment.
One limitation of our method is the fact that it
treats the classification task separately for each tar-
get node. It thus fails to observe constraints on the
possible sequences of grammatical function tags
in the same local context. Some functions are
unique, such as the Subject, whereas others (Di-
rect and Indirect Object) can only be realized by a
full NP once, although they can be doubled by a
clitic pronoun. Capturing such global constraints
will need further work.
Acknowledgements
We gratefully acknowledge support from Science
Foundation Ireland grant 04/IN/I527 for the re-
search reported in this paper.
References
A. L. Berger, V. J. Della Pietra, and S. A. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39?71, March.
D. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In
Human Language Technology Conference (HLT),
San Diego, CA, USA. Software available
at http://www.cis.upenn.edu/?dbikel/
software.html#stat-parser.
D. Blaheta and E. Charniak. 2000. Assigning function
tags to parsed text. In Proceedings of the 1st Con-
ference of the North American Chapter of the ACL,
pages 234?240, Rochester, NY, USA.
J. Bresnan. 2001. Lexical-Functional Syntax. Black-
well Publishers, Oxford.
M. Burke, O. Lam, A. Cahill, R. Chan, R. O?Donovan,
A. Bodomo, J. van Genabith, and A. Way. 2004.
Treebank-based acquisition of a Chinese Lexical-
Functional Grammar. In Proceedings of the 18th
Pacific Asia Conference on Language, Information
and Computation (PACLIC-18).
A. Cahill, M. Forst, M. McCarthy, R. O?Donovan,
and C. Roher. 2003. Treebank-based multilingual
unification-grammar development. In Proceedings
of the 15th Workshop on Ideas and Strategies for
Multilingual Grammar Development, ESSLLI 15,
Vienna, Austria.
A. Cahill, M. Burke, R. O?Donovan, J. van Genabith,
and A. Way. 2004. Long-distance dependency
resolution in automatically acquired wide-coverage
PCFG-based LFG approximations. In Proceed-
ings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 319?326,
Barcelona, Spain.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
M. Civit and M. A. Mart??. 2004. Building Cast3LB: A
Spanish treebank. Research on Language and Com-
putation, 2(4):549?574, December.
M. Civit. 2000. Gu??a para la anotacio?n mor-
fosinta?ctica del corpus CLiC-TALP, X-TRACT
Working Paper. Technical report. Avail-
able at http://clic.fil.ub.es/personal/
civit/PUBLICA/guia morfol.ps.
M. Civit. 2004. Gu??a para la anotacio?n de las funciones
sinta?cticas de Cast3LB. Technical report. Avail-
able at http://clic.fil.ub.es/personal/
civit/PUBLICA/funcions.pdf.
B. Cowan and M. Collins. 2005. Morphology and
reranking for the statistical parsing of Spanish. In
Conference on Empirical Methods in Natural Lan-
guage Processing, Vancouver, B.C., Canada.
R. Crouch, R. M. Kaplan, T. H. King, and S. Riezler.
2002. A comparison of evaluation metrics for a
broad-coverage stochastic parser. In Conference on
Language Resources and Evaluation (LREC 02).
W. Daelemans and A. van den Bosch. 2005. Memory-
Based Language Processing. Cambridge University
Press, September.
W. Daelemans, J. Zavrel, K. van der Sloot, and
A. van den Bosch. 2004. TiMBL: Tilburg Memory
Based Learner, version 5.1, Reference Guide. Tech-
nical report. Available from http://ilk.uvt.
nl/downloads/pub/papers/ilk0402.pdf.
V. Jijkoun and M. de Rijke. 2004. Enriching the output
of a parser using memory-based learning. In Pro-
ceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?318,
Barcelona, Spain.
Zh. Le, 2004. Maximum Entropy Modeling
Toolkit for Python and C++. Available
at http://homepages.inf.ed.ac.uk/
s0450736/software/maxent/manual.pdf.
R. O?Donovan, A. Cahill, J. van Genabith, and A. Way.
2005. Automatic acquisition of Spanish LFG re-
sources from the CAST3LB treebank. In Proceed-
ings of the Tenth International Conference on LFG,
Bergen, Norway.
V. N. Vapnik. 1998. Statistical Learning Theory.
Wiley-Interscience, September.
143
Coling 2010: Poster Volume, pages 374?382,
Beijing, August 2010
Integrating N-best SMT Outputs into a TM System
Yifan He Yanjun Ma Andy Way Josef van Genabith
Centre for Next Generation Localisation
School of Computing
Dublin City University
{yhe,yma,away,josef}@computing.dcu.ie
Abstract
In this paper, we propose a novel frame-
work to enrich Translation Memory (TM)
systems with Statistical Machine Trans-
lation (SMT) outputs using ranking. In
order to offer the human translators mul-
tiple choices, instead of only using the
top SMT output and top TM hit, we
merge the N-best output from the SMT
system and the k-best hits with highest
fuzzy match scores from the TM sys-
tem. The merged list is then ranked ac-
cording to the prospective post-editing ef-
fort and provided to the translators to aid
their work. Experiments show that our
ranked output achieve 0.8747 precision at
top 1 and 0.8134 precision at top 5. Our
framework facilitates a tight integration
between SMT and TM, where full advan-
tage is taken of TM while high quality
SMT output is availed of to improve the
productivity of human translators.
1 Introduction
Translation Memories (TM) are databases that
store translated segments. They are often used to
assist translators and post-editors in a Computer
Assisted Translation (CAT) environment by re-
turning the most similar translated segments. Pro-
fessional post-editors and translators have long
been relying on TMs to avoid duplication of work
in translation.
With the rapid development in statistical ma-
chine translation (SMT), MT systems are begin-
ning to generate acceptable translations, espe-
cially in domains where abundant parallel corpora
exist. It is thus natural to ask if these translations
can be utilized in some way to enhance TMs.
However advances in MT are being adopted
only slowly and sometimes somewhat reluctantly
in professional localization and post-editing envi-
ronments because of 1) the usefulness of the TM,
2) the investment and effort the company has put
into TMs, and 3) the lack of robust SMT confi-
dence estimation measures which are as reliable
as fuzzy match scores (cf. Section 4.1.2) used in
TMs. Currently the localization industry relies on
TM fuzzy match scores to obtain both a good ap-
proximation of post-editing effort and an estima-
tion of the overall translation cost.
In a forthcoming paper, we propose a trans-
lation recommendation model to better integrate
MT outputs into a TM system. Using a binary
classifier, we only recommend an MT output to
the TM-user when the classifier is highly confi-
dent that it is better than the TM output. In this
framework, post-editors continue to work with the
TM while benefiting from (better) SMT outputs;
the assets in TMs are not wasted and TM fuzzy
match scores can still be used to estimate (the up-
per bound of) post-editing labor.
In the previous work, the binary predictor
works on the 1-best output of the MT and TM sys-
tems, presenting either the one or the other to the
post-editor. In this paper, we develop the idea fur-
ther by moving from binary prediction to ranking.
We use a ranking model to merge the k-best lists
of the two systems, and produce a ranked merged
374
list for post-editing. As the list is an enriched ver-
sion of the TM?s k-best list, the TM related assets
are better preserved and the cost estimation is still
valid as an upper bound.
More specifically, we recast SMT-TM integra-
tion as a ranking problem, where we apply the
Ranking SVM technique to produce a ranked list
of translations combining the k-best lists of both
the MT and the TM systems. We use features in-
dependent of the MT and TM systems for rank-
ing, so that outputs from MT and TM can have
the same set of features. Ideally the transla-
tions should be ranked by their associated post-
editing efforts, but given the very limited amounts
of human annotated data, we use an automatic
MT evaluation metric, TER (Snover et al, 2006),
which is specifically designed to simulate post-
editing effort to train and test our ranking model.
The rest of the paper is organized as follows:
we first briefly introduce related research in Sec-
tion 2, and review Ranking SVMs in Section 3.
The formulation of the problem and experiments
with the ranking models are presented in Sections
4 and 5. We analyze the post-editing effort ap-
proximated by the TER metric in Section 6. Sec-
tion 7 concludes and points out avenues for future
research.
2 Related Work
There has been some work to help TM users to
apply MT outputs more smoothly. One strand is
to improve the MT confidence measures to bet-
ter predict post-editing effort in order to obtain a
quality estimation that has the potential to replace
the fuzzy match score in the TM. To the best of
our knowledge, the first paper in this area is (Spe-
cia et al, 2009a), which uses regression on both
the automatic scores and scores assigned by post-
editors. The method is improved in (Specia et
al., 2009b), which applies Inductive Confidence
Machines and a larger set of features to model
post-editors? judgment of the translation quality
between ?good? and ?bad?, or among three levels
of post-editing effort.
Another strand is to integrate high confidence
MT outputs into the TM, so that the ?good? TM
entries will remain untouched. In our forthcoming
paper, we recommend SMT outputs to a TM user
when a binary classifier predicts that SMT outputs
are more suitable for post-editing for a particular
sentence.
The research presented here continues the line
of research in the second strand. The difference
is that we do not limit ourselves to the 1-best out-
put but try to produce a k-best output in a rank-
ing model. The ranking scheme also enables us
to show all TM hits to the user, and thus further
protects the TM assets.
There has also been work to improve SMT us-
ing the knowledge from the TM. In (Simard and
Isabelle, 2009), the SMT system can produce a
better translation when there is an exact or close
match in the corresponding TM. They use regres-
sion Support Vector Machines to model the qual-
ity of the TM segments. This is also related to
our work in spirit, but our work is in the opposite
direction, i.e. using SMT to enrich TM.
Moreover, our ranking model is related to
reranking (Shen et al, 2004) in SMT as well.
However, our method does not focus on produc-
ing better 1-best translation output for an SMT
system, but on improving the overall quality of the
k-best list that TM systems present to post-editors.
Some features in our work are also different in na-
ture to those used in MT reranking. For instance
we cannot use N-best posterior scores as they do
not make sense for the TM outputs.
3 The Support Vector Machines
3.1 The SVM Classifier
Classical SVMs (Cortes and Vapnik, 1995) are
binary classifiers that classify an input instance
based on decision rules which minimize the reg-
ularized error function in (Eq. 1):
min
w,b,?
1
2w
Tw + C
l?
i=1
?i
subject to: yi(wT xi + b) > 1 ? ?i
?i > 0
(1)
where (xi, yi) ? Rn ? {1,?1} are l training in-
stances. w is the weight vector, ? is the relaxation
variable and C > 0 is the penalty parameter.
3.2 Ranking SVM for SMT-TM Integration
The SVM classification algorithm is extended to
the ranking case in (Joachims, 2002). For a cer-
375
tain group of instances, the Ranking SVM aims
at producing a ranking r that has the maximum
Kendall?s ? coefficient with the the gold standard
ranking r?.
Kendall?s ? measures the relevance of two rank-
ings: ?(ra, rb) = P?QP+Q , where P and Q arethe amount of concordant and discordant pairs in
ra and rb. In practice, this is done by building
constraints to minimize the discordant pairs Q.
Following the basic idea, we show how Ranking
SVM can be applied to MT-TM integration as fol-
lows.
Assume that for each source sentence s, we
have a set of outputs from MT, M and a set of
outputs from TM, T. If we have a ranking r(s)
over translation outputs M?T where for each
translation output d ? M?T, (di, dj) ? r(s) iff
di <r(s) dj , we can rewrite the ranking constraints
as optimization constraints in an SVM, as in Eq.
(2).
min
w,b,?
1
2w
Tw + C? ?
subject to:
?(di, dj) ? r(s1) : w(?(s1, di)? ?(s1, dj)) > 1 ? ?i,j,1
...
?(di, dj) ? r(sn) : w(?(sn, di)? ?(sn, dj)) > 1? ?i,j,n
?i,j,k > 0
(2)
where ?(sn, di) is a feature vector of translation
output di given source sentence sn. The Ranking
SVM minimizes the discordant number of rank-
ings with the gold standard according to Kendall?s
? .
When the instances are not linearly separable,
we use a mapping function ? to map the features
xi (?(sn, di) in the case of ranking) to high di-
mensional space, and solve the SVMwith a kernel
function K in where K(xi, xj) = ?(xi)T?(xj).
We perform our experiments with the Radial
Basis Function (RBF) kernel, as in Eq. (3).
K(xi, xj) = exp(??||xi ? xj ||2), ? > 0 (3)
4 The Ranking-based Integration Model
In this section we present the Ranking-based
SMT-TM integration model in detail. We first in-
troduce the k-best lists in MT (called N-best list)
and TM systems (called m-best list in this section)
and then move on to the problem formulation and
the feature set.
4.1 K-Best Lists in SMT and TM
4.1.1 The SMT N-best List
The N-best list of the SMT system is generated
during decoding according to the internal feature
scores. The features include language and transla-
tion model probabilities, reordering model scores
and a word penalty.
4.1.2 The TM M-Best List and the Fuzzy
Match Score
The m-best list of the TM system is gener-
ated in descending fuzzy match score. The fuzzy
match score (Sikes, 2007) uses the similarity of
the source sentences to predict a level to which a
translation is reusable or editable.
The calculation of fuzzy match scores is one of
the core technologies in TM systems and varies
among different vendors. We compute fuzzy
match cost as the minimum Edit Distance (Lev-
enshtein, 1966) between the source and TM en-
try, normalized by the length of the source as in
Eq. (4), as most of the current implementations
are based on edit distance while allowing some
additional flexible matching.
FuzzyMatch(t) = min
e
EditDistance(s, e)
Len(s) (4)
where s is the source side of the TM hit t, and e
is the source side of an entry in the TM.
4.2 Problem Formulation
Ranking lists is a well-researched problem in
the information retrieval community, and Ranking
SVMs (Joachims, 2002), which optimizes on the
ranking correlation ? have already been applied
successfully in machine translation evaluation (Ye
et al, 2007). We apply the same method here to
rerank a merged list of MT and TM outputs.
Formally given an MT-produced N-best list
M = {m1,m2, ...,mn}, a TM-produced m-best
list T = {t1, t2, ..., tm} for a input sentence s,
we define the gold standard using the TER met-
ric (Snover et al, 2006): for each d ? M?T,
(di, dj) ? r(s) iff TER(di) < TER(dj). We
train and test a Ranking SVM using cross vali-
dation on a data set created according to this cri-
terion. Ideally the gold standard would be cre-
ated by human annotators. We choose to use TER
376
as large-scale annotation is not yet available for
this task. Furthermore, TER has a high correla-
tion with the HTER score (Snover et al, 2006),
which is the TER score using the post-edited MT
output as a reference, and is used as an estimation
of post-editing effort.
4.3 The Feature Set
When building features for the Ranking SVM, we
are limited to features that are independent of the
MT and TM system. We experiment with system-
independent fluency and fidelity features below,
which capture translation fluency and adequacy,
respectively.
4.3.1 Fluency Features
Source-side Language Model Scores. We
compute the LM probability and perplexity of the
input source sentence on a language model trained
on the source-side training data of the SMT sys-
tem, which is also the TM database. The inputs
that have lower perplexity on this language model
are more similar to the data set on which the SMT
system is built.
Target-side LanguageModel Scores. We com-
pute the LM probability and perplexity as a mea-
sure of the fluency of the translation.
4.3.2 Fidelity Features
The Pseudo-Source Fuzzy Match Score. We
translate the output back to obtain a pseudo source
sentence. We compute the fuzzy match score
between the original source sentence and this
pseudo-source. If the MT/TM performs well
enough, these two sentences should be the same
or very similar. Therefore the fuzzy match score
here gives an estimation of the confidence level of
the output.
The IBMModel 1 Score. We compute the IBM
Model 1 score in both directions to measure the
correspondence between the source and target, as
it serves as a rough estimation of how good a
translation it is on the word level.
5 Experiments
5.1 Experimental Settings
5.1.1 Data
Our raw data set is an English?French trans-
lation memory with technical translation from a
multi-national IT security company, consisting of
51K sentence pairs. We randomly select 43K to
train an SMT system and translate the English side
of the remaining 8K sentence pairs, which is used
to run cross validation. Note that the 8K sentence
pairs are from the same TM, so that we are able to
create a gold standard by ranking the TER scores
of the MT and TM outputs.
Duplicated sentences are removed from the
data set, as those will lead to an exact match in
the TM system and will not be translated by trans-
lators. The average sentence length of the training
set is 13.5 words and the size of the training set
is comparable to the (larger) translation memories
used in the industry.
5.1.2 SMT and TM systems
We use a standard log-linear PB-SMT
model (Och and Ney, 2002): GIZA++ imple-
mentation of IBM word alignment model 4, the
phrase-extraction heuristics described in (Koehn
et al, 2003), minimum-error-rate training (Och,
2003), a 5-gram language model with Kneser-Ney
smoothing trained with SRILM (Stolcke, 2002)
on the English side of the training data, and
Moses (Koehn et al, 2007) to decode. We train a
system in the opposite direction using the same
data to produce the pseudo-source sentences.
We merge distinct 5-best lists from MT and TM
systems to produce a new ranking. To create the
distinct list for the SMT system, we search over
a 100-best list and keep the top-5 distinct out-
puts. Our data set consists of mainly short sen-
tences, leading to many duplications in the N-best
output of the SMT decoder. In such cases, top-
5 distinct outputs are good representations of the
SMT?s output.
5.2 Training, Tuning and Testing the
Ranking SVM
We run training and prediction of the Ranking
SVM in 4-fold cross validation. We use the
377
SVMlight1 toolkit to perform training and testing.
When using the Ranking SVM with the RBF
kernel, we have two free parameters to tune on:
the cost parameter C in Eq. (1) and the radius
parameter ? in Eq. (3). We optimize C and
? using a brute-force grid search before running
cross-validation and maximize precision at top-5,
with an inner 3-fold cross validation on the (outer)
Fold-1 training set. We search within the range
[2?6, 29], the step size is 2 on the exponent.
5.3 The Gold Standard
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     



























     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     



























     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     



























  
  
  0%
  20%
  40%
  60%
  80%
  100%
Top1 Top3 Top5
Go
ld S
tan
dar
d %
TM
MT
Figure 1: MT and TM?s percentage in gold stan-
dard
Figure 1 shows the composition of translations
in the gold standard. Each source sentence is asso-
ciated with a list of translations from two sources,
i.e. MT output and TM matches. This list of
translations is ranked from best to worst accord-
ing TER scores. The figure shows that over 80%
of the translations are from the MT system if we
only consider the top-1 translation. As the num-
ber of top translations we consider increases, more
TM matches can be seen. On the one hand, this
does show a large gap in quality between MT out-
put and TM matches; on the other hand, however,
it also reveals that we will have to ensure two ob-
jectives in ranking: the first is to rank the 80%
MT translations higher and the second is to keep
the 20% ?good? TM hits in the Top-5. We design
our evaluation metrics accordingly.
5.4 Evaluation Metrics
The aim of this research is to provide post-editors
with translations that in many cases are easier to
1http://svmlight.joachims.org/
edit than the original TM output. As we formulate
this as a ranking problem, it is natural to measure
the quality of the ranking output by the number
of better translations that are ranked high. Some-
times the top TM output is the easiest to edit; in
such a case we need to ensure that this translation
has a high rank, otherwise the system performance
will degrade.
Based on this observation, we introduce the
idea of relevant translations, and our evaluation
metrics: PREC@k and HIT@k.
Relevant Translations. We borrow the idea
of relevence from the IR community to define
the idea of translations worth ranking high. For
a source sentence s which has a top TM hit t,
we define an MT/TM output m as relevant, if
TER(m) ? TER(t). According to the defini-
tion, relevant translations should need no more
post-edits than the original top hit from the TM
system. Clearly the top TM hit is always relevant.
PREC@k. We calculate the precision
(PREC@k) of the ranking for evaluation. As-
suming that there are n relevant translations in
the top k list for a source sentence s, we have
PREC@k= n/k for s. We test PREC@k, for
k = 1...10, in order to evaluate the overall quality
of the ranking.
HIT@k. We also estimate the probability of
having one of the relevant translations in the top
k, denoted as HIT@k. For a source sentence s,
HIT@k equals to 1 if there is at least one relevant
translation in top k, and 0 otherwise. This mea-
sures the quality of the best translation in top k,
which is the translation the post-editor will find
and work on if she reads till the kth place in the
list. HIT@k equals to 1.0 at the end of the list.
We report the mean PREC@k and HIT@k for
all s with the 0.95 confidence interval.
5.5 Experimental Results
In Table 1 we report PREC@k and HIT@k
for k = 1..10. The ranking receives 0.8747
PREC@1, which means that most of the top
ranked translations have at least the same quality
as the top TM output. We notice that precision re-
mains above 0.8 till k = 5, leading us to conclude
that most of the relevant translations are ranked in
the top-5 positions in the list.
378
Table 1: PREC@k and HIT@k of Ranking
PREC % HIT %
k=1 87.47?1.60 87.47?1.60
k=2 85.42?1.07 93.36?0.53
k=3 84.13?0.94 95.74?0.61
k=4 82.79?0.57 97.08?0.26
k=5 81.34?0.51 98.04?0.23
k=6 79.26?0.59 99.41?0.25
k=7 74.99?0.53 99.66?0.29
k=8 70.87?0.59 99.84?0.10
k=9 67.23?0.48 99.94?0.08
k=10 64.00?0.46 100.0?0.00
Using the HIT@k scores we can further con-
firm this argument. The HIT@k score grows
steadily from 0.8747 to 0.9941 for k = 1...6, so
most often there will be at least one relevant trans-
lation in top-6 for the post-editor to work with.
After that room for improvement becomes very
small.
In sum, both of the PREC@k scores and the
HIT@k scores show that the ranking model effec-
tively integrates the two translation sources (MT
and TM) into one merged k-best list, and ranks
the relevant translations higher.
Table 2: PREC@k - MT and TM Systems
MT % TM %
k=1 85.87?1.32 100.0?0.00
k=2 82.52?1.60 73.58?1.04
k=3 80.05?1.11 62.45?1.14
k=4 77.92?0.95 56.11?1.11
k=5 76.22?0.87 51.78?0.78
To measure whether the ranking model is ef-
fective compared to pure MT or TM outputs, we
report the PREC@k of those outputs in Table 2.
The k-best output used in this table is ranked by
the MT or TM system, without being ranked by
our model. We see the ranked outputs consistently
outperform the MT outputs for all k = 1...5 w.r.t.
precision at a significant level, indicating that our
system preserves some high quality hits from the
TM.
The TM outputs alone are generally of much
lower quality than the MT and Ranked outputs, as
is shown by the precision scores for k = 2...5. But
TM translations obtain 1.0 PREC@1 according to
the definition of the PREC calculation. Note that
it does not mean that those outputs will need less
post-editing (cf. Section 6.1), but rather indicates
that each one of these outputs meet the lowest ac-
ceptable criterion to be relevant.
6 Analysis of Post-Editing Effort
A natural question follows the PREC and HIT
numbers: after reading the ranked k-best list, will
the post-editors edit less than they would have to if
they did not have access to the list? This question
would be best answered by human post-editors in
a large-scale experimental setting. As we have not
yet conducted a manual post-editing experiment,
we try to measure the post-editing effort implied
by our model with the edit statistics captured by
the TER metric, sorted into four types: Insertion,
Substitution, Deletion and Shift. We report the av-
erage number of edits incurred along with the 0.95
confidence interval.
6.1 Top-1 Edit Statistics
We report the results on the 1-best output of TM,
MT and our ranking system in Table 3.
In the single best results, it is easy to see that
the 1-best output from the MT system requires
the least post-editing effort. This is not surpris-
ing given the distribution of the gold standard in
Section 5.3, where most MT outputs are of better
quality than the TM hits.
Moreover, since TM translations are generally
of much lower quality as is indicated by the num-
bers in Table 3 (e.g. 2x as many substitutions
and 3x as many deletions compared to MT), un-
justly including very few of them in the ranking
output will increase loss in the edit statistics. This
explains why the ranking model has better rank-
ing precision in Tables 1 and 2, but seems to in-
cur more edit efforts. However, in practice post-
editors can neglect an obvious ?bad? translation
very quickly.
6.2 Top-k Edit Statistics
We report edit statistics of the Top-3 and Top-5
outputs in Tables 4 and 5, respectively. For each
system we report two sets of statistics: the Best-
statistics calculated on the best output (according
379
Table 3: Edit Statistics on Ranked MT and TM Outputs - Single Best
Insertion Substitution Deletion Shift
TM-Top1 0.7554 ? 0.0376 4.2461 ? 0.0960 2.9173 ? 0.1027 1.1275 ? 0.0509
MT-Top1 0.9959 ? 0.0385 2.2793 ? 0.0628 0.8940 ? 0.0353 1.2821 ? 0.0575
Rank-Top1 1.0674 ? 0.0414 2.6990 ? 0.0699 1.1246 ? 0.0412 1.2800 ? 0.0570
to TER score) in the list, and the Mean- statistics
calculated on the whole Top-k list.
The Mean- numbers allow us to have a general
overview of the ranking quality, but it is strongly
influenced by the poor TM hits that can easily be
neglected in practice. To control the impact of
those TM hits, we rely on the Best- numbers to es-
timate the edits performed on the translations that
are more likely to be used by post-editors.
In Table 4, the ranking output?s edit statistics
is closer to the MT output than the Top-1 case
in Table 3. Table 5 continues this tendency, in
which the Best-in-Top5 Ranking output requires
marginally less Substitution and Deletion opera-
tions and significantly less Insertion and Shift op-
erations (starred) than its MT counterpart. This
shows that when more of the list is explored, the
advantage of the ranking model ? utilizing mul-
tiple translation sources ? begins to compensate
for the possible large number of edits required by
poor TM hits and finally leads to reduced post-
editing effort.
There are several explanations to why the rel-
ative performance of the ranking model improves
when k increases, as compared to other models.
The most obvious explanation is that a single poor
translation is less likely to hurt edit statistics on
a k-best list with large k, if most of the transla-
tions in the k-best list are of good quality. We see
from Tables 1 and 2 that the ranking output is of
better quality than the MT and TM outputs w.r.t.
precision. For a larger k, the small number of in-
correctly ranked translations are less likely to be
chosen as the Best- translation and hold back the
Best- numbers.
A further reason is related to our ranking model
which optimizes on Kendall?s ? score. Accord-
ingly the output might not be optimal when we
evaluate the Top-1 output, but will behave better
when we evaluate on the list. This is also in ac-
cordance with our aim, which is to enrich the TM
with MT outputs and help the post-editor, instead
of choosing the translation for the post-editor.
6.3 Comparing the MT, TM and Ranking
Outputs
One of the interesting findings from Tables 3 and
4 is that according to the TER edit statistics, the
MT outputs generally need a smaller number of
edits than the TM and Ranking outputs. This cer-
tainly confirms the necessity to integrate MT into
today?s TM systems.
However, this fact should not lead to the con-
clusion that TMs should be replaced by MT com-
pletely. First of all, all of our experiments exclude
exact TM matches, as those translations will sim-
ply be reused and not translated. While this is a
realistic setting in the translation industry, it re-
moves all sentences for which the TM works best
from our evaluations.
Furthermore, Table 5 shows that the Best-in-
Top5 Ranking output performs better than the MT
outputs, hence there are TM outputs that lead to
smaller number of edits. As k increases, the rank-
ing model is able to better utilize these outputs.
Finally, in this task we concentrate on rank-
ing useful translations higher, but we are not in-
terested in how useless translations are ranked.
Ranking SVM optimizes on the ranking of the
whole list, which is slightly different from what
we actually require. One option is to use other
optimization techniques that can make use of this
property to get better Top-k edit statistics for a
smaller k. Another option is obviously to perform
regression directly on the number of edits instead
of modeling on the ranking. We plan to explore
these ideas in future work.
7 Conclusions and Future Work
In this paper we present a novel ranking-based
model to integrate SMT into a TM system, in or-
der to facilitate the work of post-editors. In such
380
Table 4: Edit Statistics on Ranked MT and TM Outputs - Top 3
Insertion Substitution Deletion Shift
TM-Best-in-Top3 0.4241 ? 0.0250 3.7395 ? 0.0887 2.9561 ? 0.0966 0.9738 ? 0.0505
TM-Mean-Top3 0.6718 ? 0.0200 5.1428 ? 0.0559 3.6192 ? 0.0649 1.3233 ? 0.0310
MT-Best?in-Top3 0.7696 ? 0.0351 1.9210 ? 0.0610 0.7706 ? 0.0332 1.0842 ? 0.0545
MT-Mean-Top3 1.1296 ? 0.0229 2.4405 ? 0.0368 0.9341 ? 0.0209 1.3797 ? 0.0344
Rank-Best-in-Top3 0.8170 ? 0.0355 2.0744 ? 0.0608 0.8410 ? 0.0338 1.0399 ? 0.0529
Rank-Mean-Top3 1.0942 ? 0.0234 2.7437 ? 0.0392 1.0786 ? 0.0231 1.3309 ? 0.0334
Table 5: Edit Statistics on Ranked MT and TM Outputs
Insertion Substitution Deletion Shift
TM-Best-in-Top5 0.4239 ? 0.0250 3.7319 ? 0.0885 2.9552 ? 0.0967 0.9673 ? 0.0504
TM-Mean-Top5 0.6143 ? 0.0147 5.5092 ? 0.0473 3.9451 ? 0.0521 1.3737 ? 0.0240
MT-Best-in-Top5 0.7690 ? 0.0351 1.9163 ? 0.0610 0.7685 ? 0.0332 1.0811 ? 0.0544
MT-Mean-Top5 1.1912 ? 0.0182 2.5326 ? 0.0291 0.9487 ? 0.0165 1.4305 ? 0.0272
Rank-Best-in-Top5 0.7246 ? 0.0338* 1.8887 ? 0.0598 0.7562 ? 0.0327 0.9705 ? 0.0515*
Rank-Mean-Top5 1.1173 ? 0.0181 2.8777 ? 0.0312 1.1585 ? 0.0200 1.3675 ? 0.0260
a model, the user of the TM will be presented
with an augmented k-best list, consisting of trans-
lations from both the TM and theMT systems, and
ranked according to ascending prospective post-
editing effort.
From the post-editors? point of view, the TM
remains intact. And unlike in the binary transla-
tion recommendation, where only one translation
recommendation is provided, the ranking model
offers k-best post-editing candidates, enabling the
user to use more resources when translating. As
we do not actually throw away any translation pro-
duced from the TM, the assets represented by the
TM are preserved and the related estimation of the
upper bound cost is still valid.
We extract system independent features from
theMT and TM outputs and use Ranking SVMs to
train the ranking model, which outperforms both
the TM?s and MT?s k-best list w.r.t. precision at k,
for all ks.
We also analyze the edit statistics of the inte-
grated k-best output using the TER edit statistics.
Our ranking model results in slightly increased
number of edits compared to the MT output (ap-
parently held back by a small number of poor TM
outputs that are ranked high) for a smaller k, but
requires less edits than both the MT and the TM
output for a larger k.
This work can be extended in a number of ways.
Most importantly, We plan to conduct a user study
to validate the effectiveness of the method and
to gather HTER scores to train a better ranking
model. Furthermore, we will try to experiment
with learning models that can further reduce the
number of edit operations on the top ranked trans-
lations. We also plan to improve the adaptability
of this method and apply it beyond a specific do-
main and language pair.
Acknowledgements
This research is supported by the Science Foun-
dation Ireland (Grant 07/CE/I1142) as part of
the Centre for Next Generation Localisation
(www.cngl.ie) at Dublin City University. We
thank Symantec for providing the TM database
and the anonymous reviewers for their insightful
comments.
References
Cortes, Corinna and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273?297.
Joachims, Thorsten. 2002. Optimizing search engines
using clickthrough data. In KDD ?02: Proceed-
ings of the eighth ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 133?142, New York, NY, USA.
381
Koehn, Philipp., Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
(NAACL/HLT-2003), pages 48 ? 54, Edmonton, Al-
berta, Canada.
Koehn, Philipp, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the
Demo and Poster Sessions (ACL-2007), pages 177?
180, Prague, Czech Republic.
Levenshtein, Vladimir Iosifovich. 1966. Binary codes
capable of correcting deletions, insertions, and re-
versals. Soviet Physics Doklady, 10(8):707?710.
Och, Franz Josef and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics (ACL-2002), pages 295?302,
Philadelphia, PA, USA.
Och, Franz Josef. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Com-
putational Linguistics (ACL-2003), pages 160?167,
Morristown, NJ, USA.
Shen, Libin, Anoop Sarkar, and Franz Josef Och.
2004. Discriminative reranking for machine trans-
lation. In HLT-NAACL 2004: Main Proceedings,
pages 177?184, Boston, Massachusetts, USA. As-
sociation for Computational Linguistics.
Sikes, Richard. 2007. Fuzzy matching in theory and
practice. Multilingual, 18(6):39 ? 43.
Simard, Michel and Pierre Isabelle. 2009. Phrase-
based machine translation in a computer-assisted
translation environment. In Proceedings of the
Twelfth Machine Translation Summit (MT Summit
XII), pages 120 ? 127, Ottawa, Ontario, Canada.
Snover, Matthew, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas (AMTA-2006), pages 223?231,
Cambridge, MA, USA.
Specia, Lucia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009a. Esti-
mating the sentence-level quality of machine trans-
lation systems. In Proceedings of the 13th An-
nual Conference of the European Association for
Machine Translation (EAMT-2009), pages 28 ? 35,
Barcelona, Spain.
Specia, Lucia, Craig Saunders, Marco Turchi, Zhuo-
ran Wang, and John Shawe-Taylor. 2009b. Improv-
ing the confidence of machine translation quality
estimates. In Proceedings of the Twelfth Machine
Translation Summit (MT Summit XII), pages 136 ?
143, Ottawa, Ontario, Canada.
Stolcke, Andreas. 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of the Sev-
enth International Conference on Spoken Language
Processing, volume 2, pages 901?904, Denver, CO,
USA.
Ye, Yang, Ming Zhou, and Chin-Yew Lin. 2007.
Sentence level machine translation evaluation as a
ranking. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 240?247,
Prague, Czech Republic.
382
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 185?189,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Active Learning for Post-Editing Based Incrementally Retrained MT
Aswarth Dara Josef van Genabith Qun Liu John Judge Antonio Toral
School of Computing
Dublin City University
Dublin, Ireland
{adara,josef,qliu,jjudge,atoral}@computing.dcu.ie
Abstract
Machine translation, in particular statis-
tical machine translation (SMT), is mak-
ing big inroads into the localisation and
translation industry. In typical work-
flows (S)MT output is checked and (where
required) manually post-edited by hu-
man translators. Recently, a significant
amount of research has concentrated on
capturing human post-editing outputs as
early as possible to incrementally up-
date/modify SMT models to avoid repeat
mistakes. Typically in these approaches,
MT and post-edits happen sequentially
and chronologically, following the way
unseen data (the translation job) is pre-
sented. In this paper, we add to the ex-
isting literature addressing the question
whether and if so, to what extent, this
process can be improved upon by Active
Learning, where input is not presented
chronologically but dynamically selected
according to criteria that maximise perfor-
mance with respect to (whatever is) the re-
maining data. We explore novel (source
side-only) selection criteria and show per-
formance increases of 0.67-2.65 points
TER absolute on average on typical indus-
try data sets compared to sequential PE-
based incrementally retrained SMT.
1 Introduction and Related Research
Machine Translation (MT) has evolved dramati-
cally over the last two decades, especially since
the appearance of statistical approaches (Brown et
al., 1993). In fact, MT is nowadays succesfully
used in the localisation and translation industry,
as for many relevant domains such as technical
documentation, post-editing (PE) of MT output by
human translators (compared to human translation
from scratch) results in notable productivity gains,
as a number of industry studies have shown con-
vincingly, e.g. (Plitt and Masselot, 2010). Fur-
thermore, incremental retraining and update tech-
niques (Bertoldi et al., 2013; Levenberg et al.,
2010; Mathur et al., 2013; Simard and Foster,
2013) allow these PEs to be fed back into the MT
model, resulting in an MT system that is contin-
uously updated to perform better on forthcoming
sentences, which should lead to a further increase
in productivity.
Typically, post-editors are presented with MT
output units (sentences) in the order in which input
sentences appear one after the other in the trans-
lation job. Because of this, incremental MT re-
training and update models based on PE outputs
also proceed in the same chronological order de-
termined by the input data. This may be sub-
optimal. In this paper we study the application of
Active Learning (AL) to the scenario of PE MT
and subsequent PE-based incremental retraining.
AL selects data (here translation inputs and their
MT outputs for PE) according to criteria that max-
imise performance with respect to the remaining
data and may diverge from processing data items
in chronological order. This may allow incremen-
tally PE-based retrained MT to (i) improve more
rapidly than chronologically PE-based retrained
MT and (ii) result in overall productivity gains.
The main contributions of this paper include:
? Previous work (Haffari et al., 2009; Blood-
good and Callison-Burch, 2010) shows that,
given a (static) training set, AL can im-
prove the quality of MT. By contrast, here
we show that AL-based data selection for hu-
man PE improves incrementally and dynami-
cally retrained MT, reducing overall PE time
of translation jobs in the localisation industry
application scenarios.
? We propose novel selection criteria for AL-
based PE: we adapt cross-entropy difference
(Moore and Lewis, 2010; Axelrod et al.,
2011), originally used for domain adaptation,
and propose an extension to cross entropy
difference with a vocabulary saturation filter
(Lewis and Eetemadi, 2013).
? While much of previous work concentrates
on research datasets (e.g. Europarl, News
Commentary), we use industry data (techni-
185
cal manuals). (Bertoldi et al., 2013) shows
that the repetition rate of news is consider-
ably lower than that of technical documenta-
tion, which impacts on the results obtained
with incremental retraining.
? Unlike in previous research, our AL-based
selection criteria take into account only the
source side of the data. This supports se-
lection before translation, keeping costs to a
minimum, a priority in commercial PE MT
applications.
? Our experiments show that AL-based selec-
tion works for PE-based incrementally re-
trained MT with overall performance gains
around 0.67 to 2.65 TER absolute on average.
AL has been successfully applied to many tasks
in natural language processing, including pars-
ing (Tang et al., 2002), named entity recogni-
tion (Miller et al., 2004), to mention just a few. See
(Olsson, 2009) for a comprehensie overview of
the application of AL to natural language process-
ing. (Haffari et al., 2009; Bloodgood and Callison-
Burch, 2010) apply AL to MT where the aim is to
build an optimal MT model from a given, static
dataset. To the best of our knowledge, the most
relevant previous research is (Gonz?alez-Rubio et
al., 2012), which applies AL to interactive MT. In
addition to differences in the AL selection criteria
and data sets, our goals are fundamentally differ-
ent: while the previous work aimed at reducing
human effort in interactive MT, we aim at reduc-
ing the overall PE time in PE-based incremental
MT update applications in the localisation indus-
try.
In our experiments reported in Section 3 below
we want to explore a space consisting of a con-
siderable number of selection strategies and incre-
mental retraining batch sizes. In order to be able to
do this, we use the target side of our industry trans-
lation memory data to approximate human PE out-
put and automatic TER (Snover et al., 2006) scores
as a proxy for human PE times (O?Brien, 2011).
2 Methodology
Given a translation job, our goal is to reduce the
overall PE time. At each stage, we select sen-
tences that are given to the post editor in such a
way that uncertain sentences (with respect to the
MT system at hand)
1
are post-edited first. We then
translate the n top-ranked sentences using the MT
system and use the human PEs of the MT outputs
to retrain the system. Algorithm 1 describes our
1
The uncertainty of a sentence with respect to the model
can be measured according to different criteria, e.g. percent-
age of unknown n-grams, perplexity etc.
method, where s and t stand for source and target,
respectively.
Algorithm 1 Sentence Selection Algorithm
Input:
L?? Initial training data
M?? Initial MT model
for C ? (Random,Sequential,Ngram,CED,CEDN) do
U?? Translation job
while size(U) > 0 do
U1.s?? SelectTopSentences(C, U.s)
U1
1
.t?? Translate(M, U1.s)
U1.t?? PostEdit(U1
1
.t)
U?? U - U1
L?? L ? U1
M?? TrainModel (L)
end while
end for
We use two baselines, i.e. random and sequen-
tial. In the random baseline, the batch of sentences
at each iteration are selected randomly. In the se-
quential baseline, the batches of sentences follow
the same order as the data.
Aside from the Random and Sequential base-
lines we use the following selection criteria:
? N-gram Overlap. An SMT system will en-
counter problems translating sentences con-
taining n-grams not seen in the training data.
Thus, PEs of sentences with high number of
unseen n-grams are considered to be more in-
formative for updating the current MT sys-
tem. However, for the MT system to trans-
late unseen n-grams accurately, they need to
be seen a minimum number V times.
2
We
use an n-gram overlap function similar to
the one described in (Gonz?alez-Rubio et al.,
2012) given in Equation 1 where N(T
(i)
) and
N(S
(i)
) return i-grams in training data and
the sentence S, respectively.
unseen(S) =
n
?
i=1
{|N(T
(i)
) ?N(S
(i)
)|>V }
n
?
i=1
N(S
(i)
)
(1)
? Cross Entropy Difference (CED). This met-
ric is originally used in data selection (Moore
and Lewis, 2010; Axelrod et al., 2011).
Given an in-domain corpus I and a general
corpus O, language models are built from
both,
3
and each sentence in O is scored ac-
cording to the entropy H difference (Equation
2
Following (Gonz?alez-Rubio et al., 2012) we use V =
10.
3
In order to make the LMs comparable they have the same
size. As commonly the size of O is considerable bigger than
I, this means that the LM for O is built from a subset of the
same size as I.
186
2). The lower the score given to a sentence,
the more useful it is to train a system for the
specific domain I .
score(s) = H
I
(s)?H
O
(s) (2)
In our AL scenario, we have the current train-
ing corpus L and an untranslated corpus U.
CED is applied to select sentences from U
that are (i) different from L (as we would like
to add sentences that add new information to
the model) and (ii) similar to the overall cor-
pus U (as we would like to add sentences that
are common in the untranslated data). Hence
we apply CED and select sentences from U
that have high entropy with respect to L and
low entropy with respect to U (Equation 3).
score(s) = H
U
(s)?H
L
(s) (3)
? CED + n-gram (CEDN). This is an exten-
sion of the CED criterion inspired by the con-
cept of the vocabulary saturation filter (Lewis
and Eetemadi, 2013). CED may select many
very similar sentences, and thus it may be the
case that some of them are redundant. By
post-processing the selection made by CED
with vocabulary saturation we aim to spot
and remove redudant sentences. This works
in two steps. In the first step, all the sentences
from U are scored using the CED metric. In
the second step, we down-rank sentences that
are considered redundant. The top sentence is
selected, and its n-grams are stored in local-
ngrams. For the remaining sentences, one by
one, their n-grams are matched against local-
ngrams. If the intersection between them is
lower than a predefined threshold, the current
sentence is added and localngrams is updated
with the n-grams from the current sentence.
Otherwise the sentence is down-ranked to the
bottom. In our experiments, the value n = 1
produces best results.
3 Experiments and Results
We use technical documentation data taken from
Symantec translation memories for the English?
French (EN?FR) and English?German (EN?DE)
language pairs (both directions) for our experi-
ments. The statistics of the data (training and in-
cremental splits) are shown in Table 1.
All the systems are trained using the
Moses (Koehn et al., 2007) phrase-based sta-
tistical MT system, with IRSTLM (Federico et
al., 2008) for language modelling (n-grams up
to order five) and with the alignment heuristic
grow-diag-final-and.
For the experiments, we considered two settings
for each language pair in each direction. In the
first setting, the initial MT system is trained using
the training set (39,679 and 54,907 sentence pairs
for EN?FR and EN?DE, respectively). Then, a
batch of 500 source sentences is selected from the
incremental dataset according to each of the se-
lection criteria, and translations are obtained with
the initial MT system. These translations are post-
edited and the corrected translations are added to
the training data.
4
We then train a new MT sys-
tem using the updated training data (initial training
data plus PEs of the first batch of sentences). The
updated model will be used to translate the next
batch. The same process is repeated until the in-
cremental dataset is finished (16 and 20 iterations
for English?French and English?German, respec-
tively). For each batch we compute the TER score
between the MT output and the refererence trans-
lations for the sentences of that batch. We then
compute the average TER score for all the batches.
These average scores, for each selection criterion,
are reported in Table 2.
In the second setting, instead of using the whole
training data, we used a subset of (randomly se-
lected) 5,000 sentence pairs for training the initial
MT system and a subset of 20,000 sentences from
the remaining data as the incremental dataset.
Here we take batches of 1,000 sentences (thus 20
batches). The results are shown in Table 3.
The first setting aims to reflect the situation
where a translation job is to be completed for a do-
main for which we have a considerable amount of
data available. Conversely, the second setting re-
flects the situation where a translation job is to be
carried out for a domain with little (if any) avail-
able data.
Dir Random Seq. Ngram CED CEDN
EN?FR 29.64 29.81 28.97 29.25 29.05
FR?EN 27.08 27.04 26.15 26.63 26.39
EN?DE 24.00 24.08 22.34 22.60 22.32
DE?EN 19.36 19.34 17.70 17.97 17.48
Table 2: TER average scores for Setting 1
Dir Random Seq. Ngram CED CEDN
EN?FR 36.23 36.26 35.20 35.48 35.17
FR?EN 33.26 33.34 32.26 32.69 32.17
EN?DE 32.23 32.19 30.58 31.96 29.98
DE?EN 27.24 27.29 26.10 26.73 24.94
Table 3: TER average scores for Setting 2
For Setting 1 (Table 2), the best result is ob-
tained by the CEDN criterion for two out of the
four directions. For EN?FR, n-gram overlap
4
As this study simulates the post-editing, we use the ref-
erences of the translated segments instead of the PEs.
187
Type
EN?FR EN?DE
Sentences Avg. EN SL Avg. FR SL Sentences Avg. EN SL Avg. DE SL
Training 39,679 13.55 15.28 54,907 12.66 12.90
Incremental 8,000 13.74 15.50 10,000 12.38 12.61
Table 1: Data Statistics for English?French and English?German Symantec Translation Memory Data.
SL stands for sentence length, EN stands for English, FR stands for French and DE stands for German
performs slightly better than CEDN (0.08 points
lower) with a decrease of 0.67 and 0.84 points
when compared to the baselines (random and se-
quential, respectively). For FR?EN, n-gram
overlap results in a decrease of 0.93 and 0.89
points compared to the baselines. The decrease in
average TER score is higher for the EN?DE and
for DE?EN directions, i.e. 1.68 and 1.88 points
respectively for CEDN compared to the random
baseline.
In the scenario with limited data available be-
forehand (Table 3), CEDN is the best performing
criterion for all the language directions. For the
EN?FR and FR?EN language pairs, CEDN results
in a decrease of 1.06 and 1.09 points compared to
the random baseline. Again, the decrease is higher
for the EN?DE and DE?EN language pairs, i.e.
2.25 and 2.30 absolute points on average.
Figure 1 shows the TER scores per iteration for
each of the criteria, for the scenario DE?EN Set-
ting 2 (the trends are similar for the other scenar-
ios). The two baselines exhibit slight improve-
ment over the iterations, both starting at around
.35 TER points and finishing at around .25 points.
Conversely, all the three criteria start at very high
scores (in the range [.5,.6]) and then improve con-
siderably to arrive at scores below .1 for the last
iterations. Compared to Ngram and CED, CEDN
reaches better scores earlier on, being the criterion
with the lowest score up to iteration 13.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 200.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7 RandomSeqNgramCEDCEDN
Iteration
TER
 sco
re
Figure 1: Results per iteration, DE?EN Setting 2
Figure 1 together with Tables 2 and 3 show
that AL for PE-based incremental MT retrain-
ing really works: all AL based methods (Ngram,
CED, CEDN) show strong improvements over
both baselines after the initial 8-9 iterations (Fig-
ure 1) and best performance on the complete incre-
mental data sets, resulting in a noticeable decrease
of the overall TER score (Tables 2 and 3). In six
out of eight scenarios, our novel metric CEDN ob-
tains the best result.
4 Conclusions and Future Work
This paper has presented an application of AL to
MT for dynamically selecting automatic transla-
tions of sentences for human PE, with the aim of
reducing overall PE time in a PE-based incremen-
tal MT retraining scenario in a typical industrial
localisation workflow that aims to capitalise on
human PE as early as possible to avoid repeat mis-
takes.
Our approach makes use of source side informa-
tion only, uses two novel selection criteria based
on cross entropy difference and is tested on indus-
trial data for two language pairs. Our best per-
forming criteria allow the incrementally retrained
MT systems to improve their performance earlier
and reduce the overall TER score by around one
and two absolute points for English?French and
English?German, respectively.
In order to be able to explore a space of selec-
tion criteria and batch sizes, our experiments sim-
ulate PE, in the sense that we use the target ref-
erence (instead of PEs) and approximate PE time
with TER. Given that TER correlates well with PE
time (O?Brien, 2011), we expect AL-based selec-
tion of sentences for human PE to lead to overall
reduction of PE time. In the future work, we plan
to do the experiments using PEs to retrain the sys-
tem and measuring PE time.
In this work, we have taken batches of sentences
(size 500 to 1,000) and do full retraining. As fu-
ture work, we plan to use fully incremental retrain-
ing and perform the selection on a sentence-by-
sentence basis (instead of taking batches).
Finally and importantly, a potential drawback of
our approach is that by dynamically selecting in-
dividual sentences for PE, the human post-editor
looses context, which they may use if processing
sentences sequentially. We will explore the trade
off between the context lost and the productivity
gain achieved, and ways of supplying context (e.g.
previous and following sentence) for real PE.
188
Acknowledgements
This work is supported by Science Foundation
Ireland (Grants 12/TIDA/I2438, 07/CE/I1142 and
12/CE/I2267) as part of the Centre for Next Gen-
eration Localisation (www.cngl.ie) at Dublin City
University. We would like to thank Symantec for
the provision of data sets used in our experiments.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 355?362, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Nicola Bertoldi, Mauro Cettolo, and Marcello Fed-
erico. 2013. Cache-based online adaptation for ma-
chine translation enhanced computer assisted trans-
lation. In Proceedings of the XIV Machine Transla-
tion Summit, pages 35?42, Nice, France.
Michael Bloodgood and Chris Callison-Burch. 2010.
Bucking the trend: Large-scale cost-focused active
learning for statistical machine translation. In Jan
Hajic, Sandra Carberry, and Stephen Clark, editors,
ACL, pages 854?864. The Association for Computer
Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Comput. Linguist., 19(2):263?
311, June.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In INTER-
SPEECH, pages 1618?1621. ISCA.
Jes?us Gonz?alez-Rubio, Daniel Ortiz-Mart??nez, and
Francisco Casacuberta. 2012. Active learning for
interactive machine translation. In Proceedings of
the 13th Conference of the European Chapter of the
Association for Computational Linguistics, EACL
?12, pages 245?254, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based
machine translation. In HLT-NAACL, pages 415?
423. The Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL 2007, pages 177?180, Prague, Czech Repub-
lic. Association for Computational Linguistics.
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models
for statistical machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, HLT ?10, pages 394?
402, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
William Lewis and Sauleh Eetemadi. 2013. Dramati-
cally reducing training data size through vocabulary
saturation. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 281?291,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Prashant Mathur, Mauro Cettolo, and Marcello Fed-
erico. 2013. Online learning approaches in com-
puter assisted translation. In Proceedings of the
Eighth Workshop on Statistical Machine Transla-
tion, ACL, pages 301?308, Sofia, Bulgaria.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Proceedings of HLT, pages
337?342.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ?10, pages 220?224, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Sharon O?Brien. 2011. Towards predicting
post-editing productivity. Machine Translation,
25(3):197?215, September.
Fredrik Olsson. 2009. A literature survey of active
machine learning in the context of natural language
processing. Technical Report T2009:06.
Mirko Plitt and Franc?ois Masselot. 2010. A productiv-
ity test of statistical machine translation post-editing
in a typical localisation context. Prague Bull. Math.
Linguistics, 93:7?16.
Michel Simard and George Foster. 2013. Pepr: Post-
edit propagation using phrase-based statistical ma-
chine translation. In Proceedings of the XIV Ma-
chine Translation Summit, pages 191?198, Nice,
France.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Trans-
lation in the Americas, pages 223?231, Cambridge,
MA.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002.
Active learning for statistical natural language pars-
ing. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 120?127, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
189
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 10?13,
Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics
TMTprime: A Recommender System for MT and TM Integration
Aswarth Dara?, Sandipan Dandapat??, Declan Groves? and Josef van Genabith?
? Centre for Next Generation Localisation, School of Computing
Dublin City University, Dublin, Ireland
? Department of Computer Science and Engineering
IIT-Guwahati, Assam, India
{adara, dgroves, josef}@computing.dcu.ie, sdandapat@iitg.ernet.in
Abstract
TMTprime is a recommender system that fa-
cilitates the effective use of both transla-
tion memory (TM) and machine translation
(MT) technology within industrial language
service providers (LSPs) localization work-
flows. LSPs have long used Translation Mem-
ory (TM) technology to assist the translation
process. Recent research shows how MT sys-
tems can be combined with TMs in Computer
Aided Translation (CAT) systems, selecting
either TM or MT output based on sophis-
ticated translation quality estimation without
access to a reference. However, to date there
are no commercially available frameworks for
this. TMTprime takes confidence estimation
out of the lab and provides a commercially vi-
able platform that allows for the seamless inte-
gration of MT with legacy TM systems to pro-
vide the most effective (least effort/cost) trans-
lation options to human translators, based on
the TMTprime confidence score.
1 Introduction
Within the LSP community there is growing interest
in the use of MT as a means to increase automation
and reduce overall localisation project cost. When
high-quality MT output is available, translators see
significant productivity gains over translation from
scratch, but poor MT quality leads to frustration
and wasted time as suggested translations are dis-
carded in favour of providing a translation from
scratch. We present a commercially-relevant soft-
ware platform providing a translation confidence es-
timation metric and, based on this, a mechanism for
effectively integrating MT with TMs in localisation
workflows. The confidence metric ensures that only
?Author did this work during his post doctoral research at
CNGL.
those MT outputs that are guaranteed to require less
post-editing effort than the best corresponding TM
match are presented to the post-editor (He et al,
2010a). The MT is integrated seamlessly, and es-
tablished localisation cost estimation models based
on TM technologies still apply as upper bounds.
2 Related Work
MT confidence estimation and its relation to existing
TM scoring methods, together with how to make the
most effective use of both technologies, is an active
area of research.
(Specia, 2011) and (Specia et al, 2009, 2010) pro-
pose a confidence estimator that relates specifically
to the post-editing effort of translators. This re-
search uses regression on both the automatic scores
assigned to the MT and scores assigned by post-
editors and aims to model post-editors? judgements
of the translation quality between good and bad, or
among three levels of post-editing effort.
Our work is an extension of (He et al, 2010a,b,c),
and uses outputs and features relevant to the TM
and MT systems. We focus on using system exter-
nal features. This is important for cases where the
internals of the MT system are not available, as in
the use of MT as a service in a localisation work-
flow.1 Furthermore, instead of having to solve a
regression problem, our approach is based on solv-
ing an easier binary prediction problem (using Sup-
port Vector Machines) and can be easily integrated
into TMs. (He et al, 2010b) present a MT/TM seg-
ment recommender, (He et al, 2010c) a MT/TM n-
best list segment re-ranker and (He et al, 2010a) a
MT/TM integration method that can use matching
sub-segments in MT/TM combination. Importantly,
1(Specia et al, 2009) note that using glass-box features
when available, in addition to black-box features, offer only
small gains and also incur significant computational effort.
10
translators can tune the models for precision without
retraining the models.
Related research by (Simard and Isabelle., 2009)
focuses on combining TM information into an SMT
system for improving the performance of the MT
when a close match already exists within the TM.
(Koehn and Haddow, 2009) presents a post-editing
environment using information from the phrase-
based SMT system Moses.2 (Guerberof, 2009) com-
pares the post-editing effort required for TM and
MT output, respectively. (Tatsumi, 2009) studies the
correlation between automatic evaluation scores and
post-editing effort.
3 Translation Recommender
Figure 1: TMTprime Workflow
The workflow of the translation recommender is
shown in Figure 1. We train MT systems using a
significant portion of the training data and use these
models as well as TM outputs to obtain a recommen-
dation development data set. MT systems can be
either in-house, e.g. a Moses-based system, or ex-
ternally available systems, such as Microsoft Bing3
or Google Translate.4 For each sentence in the de-
velopment data set, we have access to the reference
as well as to the outputs for each of the MT and TM
systems. We then select the best MT (or TM) output
as the translation with the lowest TER score with
respect to the reference and label the data accord-
ingly. System-independent features for each trans-
lation output are fed as input to the SVM classi-
fier (Cortes and Vapnik, 1995). The SVM classi-
fier outputs class labels and the class labels are con-
verted into confidence scores using the techniques
given in (Lin et al, 2007). Relying on system inde-
pendent black-box features has allowed us to build
2http://www.statmt.org/moses/
3http://www.bing.com/translator
4http://translate.google.com/
a fully extendable platform that will allow any num-
ber of MT systems (or indeed TM systems) to be
plugged into the recommender with little effort.
4 Demo Description
Using the Amazon EC25 deployment as a back-end,
we have developed a front-end GUI for the system
(Figure 2). The interface allows the user to select
which of the available translation systems (whether
they be MT or TM) they wish to use within the rec-
ommender system. The user can input their own
pre-established estimated cost of post-editing, based
on error ranges. Typically the costs for post-editing
those translations which have a lower-error rate (i.e.
fewer errors) is less than the cost for post-editing
translations which have a greater number of errors,
as they are of lower quality. The user is requested to
upload a file for translation to the system.
Figure 2: TMTprime GUI
Once the user has selected their desired options,
the TMTprime platform provides various analysis
measures based on its recommendation engine, such
as how many segments from the input file are recom-
mended for translation by the various selected trans-
lation engines or TMs available. Based on the input
costs, it provides a visualisation of overall estimated
cost of either using an individual translation system
on its own, or using the recommender selecting the
best performing system on a segment-by-segment
basis. The TMTprime system is an implementa-
tion of a segment-based system selector selecting
the most appropriate available translation/TM sys-
tem for a given input. A snapshot of the results pro-
duced by TMTprime is given in Figure 3: the pie-
chart shows what percentage of segments are rec-
ommended from each of the translation systems; the
5http://aws.amazon.com/ec2/
11
bar-graph gives an estimated cost of using a single
translation system alone and the estimated cost when
using TMTprime?s combined recommendation. The
estimated cost using TMTprime is lower when com-
pared to using a single MT or TM system alone
(in the worst case, it will be the same as the best-
performing single translation engine or TM system).
This estimated cost includes both the cost for trans-
lation (currently uniform cost for each translation
system) and the cost required for post-editing. For
example, if the MT is an in-house system the cost
of translation will be (close to) zero whereas there is
potentially an additional base cost for using an exter-
nal MT engine. Finally, the interface provides statis-
tics related to various confidence levels for different
translation outputs across the various translation and
TM systems.
Figure 3: Results shown by TMTprime system
5 Experiments and Results
Evaluation targets two objectives and is described
below.
5.1 Correlation with Automatic Metrics
TER and METEOR are widely-used automatic met-
rics (Snover et al, 2006; Denkowski and Lavie,
2011) that calculate the quality of translation out-
put by comparing it against a human translation,
known as the reference translation. Our data sets
for the experiment consist of English-French trans-
lation memories from the IT domain. In all instances
MT was carried out for English-French translations.
As we have access to the reference target language
translations for our test set, we are able to calculate
the TER and METEOR scores for the three trans-
lation outputs (here TM, MaTrEx (Dandapat et al,
2010) and Microsoft Bing). For each sentence in the
test set, TMTprime recommends a particular transla-
tion output with a certain estimated confidence level
without access to a reference. We measure Pearson?s
correlation coefficient (Hollander and Wolfe, 1999)
between the recommendation scores, TER scores
and METEOR scores (for all system outputs) in or-
der to determine how well the TMTprime prediction
score correlates with the widely used automatic eval-
uation metrics. Results of these experiments are pro-
vided in Table 1 which shows there is a negative cor-
relation between TMTprime scores and TER scores.
This shows that both TMTprime scores and TER
scores are moving in opposite directions, supporting
the claim that the higher the recommendation scores,
the lower the TER scores. As TER is an error score,
the lower the TER score, the higher the quality of
the machine translation output compared to its refer-
ence. On the other hand, TMTprime scores are pos-
itively correlated with METEOR scores which sup-
ports the claim that the higher the recommendation
scores, the higher the METEOR scores.
Pearson?s r TER METEOR
TMTprime -0.402 0.447
Table 1: Correlation with automatic metrics
The evaluation has been performed on a test data
set of 2,500 sentences. Both the correlations are sig-
nificant at the (p<0.01) level.
5.2 Correlation with Post-Editing time
This is the most important and crucial metric for the
evaluation. For this experiment we made use of post-
editing data captured during a real-world translation
task, for English-French in the IT domain.
Pearson?s r TER METEOR PE Time
TMTprime -0.122 0.129 -0.132
Table 2: Correlation with Post-Editing times
For testing, we collect the post-editing times for
MT outputs from two different translators using a
commercial computer-aided translation (CAT tool)
in a real-world production scenario. The data set
consists of 1113 samples and is different from the
one used in the correlation with automatic metrics.
12
Post-editing times provide a real measure of the
amount of post-editing effort required to perfect the
output of the MT system. For this experiment, we
took the output of the MT system used in the task to-
gether with the post-editing times and measured the
Pearsons correlation coefficient between the TMT-
prime recommendation scores and the post-editing
(PE) times (only for MT output from a single sys-
tem since this data set does contain PE times for
other translation outputs). In addition, we also re-
peated the previous experiment setup for finding the
correlation between the TMTprime scores and the
automatically-produced TER, METEOR scores for
this data set. The results are given in Table 2.
The results show that the confidence scores do
correlate with automatic evaluation metrics and
post-editing times. Although the correlations do not
seem as strong as before, the results are statistically
significant (p<0.01).
6 Conclusions and Future Work
We present a commercially viable translation recom-
mender system which selects the best output from
multiple TM/MT outputs. We have shown that our
confidence score correlates with automatic metrics
and post-editing times. For future work, we are
looking into extending and evaluating the system for
different language pairs and data sets.
Acknowledgments
This work is supported by Science Foundation Ire-
land (Grants SFI11-TIDA-B2040 and 07/CE/I1142) as
part of the Centre for Next Generation Localisation
(www.cngl.ie) at Dublin City University. We would also
like to thank Symantec, Autodesk and Welocalize for
their support and provision of data sets used in our ex-
periments.
References
Cortes, Corinna and Vladimir Vapnik. 1995. Support-vector
networks. In Machine Learning. pages 273?297.
Dandapat, Sandipan, Mikel L. Forcada, Declan Groves, Ser-
gio Penkale, John Tinsley, and Andy Way. 2010. OpenMa-
TrEx: A free/open-source marker-driven example-based ma-
chine translation system. In Proceedings of the 7th interna-
tional conference on Advances in natural language process-
ing. Springer-Verlag, Berlin, Heidelberg, IceTAL?10, pages
121?126.
Denkowski, Michael and Alon Lavie. 2011. Meteor 1.3: Auto-
matic metric for reliable optimization and evaluation of ma-
chine translation systems. In Proceedings of the EMNLP
2011 Workshop on Statistical Machine Translation. Edin-
burgh, UK.
Guerberof, Ana. 2009. Productivity and quality in mt post-
editing. In Proceedings of Machine Translation Summit XII
- Workshop: Beyond Translation Memories: New Tools for
Translators. Ottawa, Canada.
He, Yifan, Yanjun Ma, J Roturier, Andy Way, and Josef van
Genabith. 2010a. Improving the post-editing experience us-
ing translation recommendation: A user study. In Proceed-
ings of the Ninth Conference of the Association for Ma-
chine Translation in the Americas. Denver, Colorado, AMTA
2010, pages 247?256.
He, Yifan, Yanjun Ma, Josef van Genabith, and Andy Way.
2010b. Bridging smt and tm with translation recommenda-
tion. In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics. Association for Com-
putational Linguistics, Uppsala, Sweden, ACL 2010, pages
622?630.
He, Yifan, Yanjun Ma, Andy Way, and Josef van Genabith.
2010c. Integrating n-best smt outputs into a tm system. In
Proceedings of the 23rd International Conference on Com-
putational Linguistics: Posters. Association for Computa-
tional Linguistics, Beijing, China, COLING 2010, pages
374?382.
Hollander, Myles and Douglas A. Wolfe. 1999. Nonparametric
Statistical Methods. John Wiley and Sons.
Koehn, Philip and Barry Haddow. 2009. Interactive assis-
tance to human translators using statistical machine trans-
lation methods. In Proceedings of MT Summit XII. Ottawa,
Canada, pages 73?80.
Lin, Hsuan-Tien, Chih-Jen Lin, and Ruby C. Weng. 2007. A
note on platt?s probabilistic outputs for support vector ma-
chines. Machine Learning 68(3):267?276.
Simard, Michael and Pierre Isabelle. 2009. Phrase-based ma-
chine translation in a computer-assisted translation environ-
ment. In Proceedings of Machine Translation Summit XII.
Ottawa, Canada, pages 120?127.
Snover, Matthew, Bonnie Dorr, Richard Schwartz, Linnea Mic-
ciulla, and John Makhoul. 2006. A study of translation edit
rate with targeted human annotation. In Proceedings of Asso-
ciation for Machine Translation in the Americas. Cambridge,
MA, pages 223?231.
Specia, Lucia. 2011. Exploiting objective annotations for mea-
suring translation post-editing effort. In Proceedings of the
15th Annual Conference of the European Association for
Machine Translation. Leuven, Belgium, EAMT 2011, pages
73?80.
Specia, Lucia, Nicola Cancedda, and Marc Dymetman. 2010. A
dataset for assessing machine translation evaluation metrics.
In Proceedings of LREC 2010. Valletta, Malta.
Specia, Lucia, Marco Turqui, Zhuoran Wang, John Shawe-
Taylor, and Craig Saunders. 2009. Improving the confidence
of machine translation quality estimates. In Proceedings
of Machine Translation Summit XII. Ottawa, Canada, pages
136?143.
Tatsumi, Midori. 2009. Correlation between automatic evalua-
tion scores, post-editing speed and some other factors. In
Proceedings of Machine Translation Summit XII. Ottawa,
Canada, pages 332?339.
13
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 622?630,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Bridging SMT and TM with Translation Recommendation
Yifan He Yanjun Ma Josef van Genabith Andy Way
Centre for Next Generation Localisation
School of Computing
Dublin City University
{yhe,yma,josef,away}@computing.dcu.ie
Abstract
We propose a translation recommendation
framework to integrate Statistical Machine
Translation (SMT) output with Transla-
tion Memory (TM) systems. The frame-
work recommends SMT outputs to a TM
user when it predicts that SMT outputs are
more suitable for post-editing than the hits
provided by the TM. We describe an im-
plementation of this framework using an
SVM binary classifier. We exploit meth-
ods to fine-tune the classifier and inves-
tigate a variety of features of different
types. We rely on automatic MT evalua-
tion metrics to approximate human judge-
ments in our experiments. Experimental
results show that our system can achieve
0.85 precision at 0.89 recall, excluding ex-
act matches. Furthermore, it is possible for
the end-user to achieve a desired balance
between precision and recall by adjusting
confidence levels.
1 Introduction
Recent years have witnessed rapid developments
in statistical machine translation (SMT), with con-
siderable improvements in translation quality. For
certain language pairs and applications, automated
translations are now beginning to be considered
acceptable, especially in domains where abundant
parallel corpora exist.
However, these advances are being adopted
only slowly and somewhat reluctantly in profes-
sional localization and post-editing environments.
Post-editors have long relied on translation memo-
ries (TMs) as the main technology assisting trans-
lation, and are understandably reluctant to give
them up. There are several simple reasons for
this: 1) TMs are useful; 2) TMs represent con-
siderable effort and investment by a company or
(even more so) an individual translator; 3) the
fuzzy match score used in TMs offers a good ap-
proximation of post-editing effort, which is useful
both for translators and translation cost estimation
and, 4) current SMT translation confidence esti-
mation measures are not as robust as TM fuzzy
match scores and professional translators are thus
not ready to replace fuzzy match scores with SMT
internal quality measures.
There has been some research to address this is-
sue, see e.g. (Specia et al, 2009a) and (Specia et
al., 2009b). However, to date most of the research
has focused on better confidence measures for MT,
e.g. based on training regression models to per-
form confidence estimation on scores assigned by
post-editors (cf. Section 2).
In this paper, we try to address the problem
from a different perspective. Given that most post-
editing work is (still) based on TM output, we pro-
pose to recommend MT outputs which are better
than TM hits to post-editors. In this framework,
post-editors still work with the TM while benefit-
ing from (better) SMT outputs; the assets in TMs
are not wasted and TM fuzzy match scores can
still be used to estimate (the upper bound of) post-
editing labor.
There are three specific goals we need to
achieve within this framework. Firstly, the rec-
ommendation should have high precision, other-
wise it would be confusing for post-editors and
may negatively affect the lower bound of the post-
editing effort. Secondly, although we have full
access to the SMT system used in this paper,
our method should be able to generalize to cases
where SMT is treated as a black-box, which is of-
622
ten the case in the translation industry. Finally,
post-editors should be able to easily adjust the rec-
ommendation threshold to particular requirements
without having to retrain the model.
In our framework, we recast translation recom-
mendation as a binary classification (rather than
regression) problem using SVMs, perform RBF
kernel parameter optimization, employ posterior
probability-based confidence estimation to sup-
port user-based tuning for precision and recall, ex-
periment with feature sets involvingMT-, TM- and
system-independent features, and use automatic
MT evaluation metrics to simulate post-editing ef-
fort.
The rest of the paper is organized as follows: we
first briefly introduce related research in Section 2,
and review the classification SVMs in Section 3.
We formulate the classification model in Section 4
and present experiments in Section 5. In Section
6, we analyze the post-editing effort approximated
by the TER metric (Snover et al, 2006). Section
7 concludes the paper and points out avenues for
future research.
2 Related Work
Previous research relating to this work mainly fo-
cuses on predicting the MT quality.
The first strand is confidence estimation for MT,
initiated by (Ueffing et al, 2003), in which pos-
terior probabilities on the word graph or N-best
list are used to estimate the quality of MT out-
puts. The idea is explored more comprehensively
in (Blatz et al, 2004). These estimations are often
used to rerank the MT output and to optimize it
directly. Extensions of this strand are presented
in (Quirk, 2004) and (Ueffing and Ney, 2005).
The former experimented with confidence esti-
mation with several different learning algorithms;
the latter uses word-level confidence measures to
determine whether a particular translation choice
should be accepted or rejected in an interactive
translation system.
The second strand of research focuses on com-
bining TM information with an SMT system, so
that the SMT system can produce better target lan-
guage output when there is an exact or close match
in the TM (Simard and Isabelle, 2009). This line
of research is shown to help the performance of
MT, but is less relevant to our task in this paper.
A third strand of research tries to incorporate
confidence measures into a post-editing environ-
ment. To the best of our knowledge, the first paper
in this area is (Specia et al, 2009a). Instead of
modeling on translation quality (often measured
by automatic evaluation scores), this research uses
regression on both the automatic scores and scores
assigned by post-editors. The method is improved
in (Specia et al, 2009b), which applies Inductive
Confidence Machines and a larger set of features
to model post-editors? judgement of the translation
quality between ?good? and ?bad?, or among three
levels of post-editing effort.
Our research is more similar in spirit to the third
strand. However, we use outputs and features from
the TM explicitly; therefore instead of having to
solve a regression problem, we only have to solve
a much easier binary prediction problem which
can be integrated into TMs in a straightforward
manner. Because of this, the precision and recall
scores reported in this paper are not directly com-
parable to those in (Specia et al, 2009b) as the lat-
ter are computed on a pure SMT system without a
TM in the background.
3 Support Vector Machines for
Translation Quality Estimation
SVMs (Cortes and Vapnik, 1995) are binary clas-
sifiers that classify an input instance based on de-
cision rules which minimize the regularized error
function in (1):
min
w,b,?
1
2w
Tw + C
l
?
i=1
?i
s. t. yi(wT?(xi) + b) > 1? ?i
?i > 0
(1)
where (xi, yi) ? Rn ? {+1,?1} are l training
instances that are mapped by the function ? to a
higher dimensional space. w is the weight vec-
tor, ? is the relaxation variable and C > 0 is the
penalty parameter.
Solving SVMs is viable using the ?kernel
trick?: finding a kernel function K in (1) with
K(xi, xj) = ?(xi)T?(xj). We perform our ex-
periments with the Radial Basis Function (RBF)
kernel, as in (2):
K(xi, xj) = exp(??||xi ? xj ||2), ? > 0 (2)
When using SVMs with the RBF kernel, we
have two free parameters to tune on: the cost pa-
rameter C in (1) and the radius parameter ? in (2).
In each of our experimental settings, the param-
eters C and ? are optimized by a brute-force grid
623
search. The classification result of each set of pa-
rameters is evaluated by cross validation on the
training set.
4 Translation Recommendation as
Binary Classification
We use an SVM binary classifier to predict the rel-
ative quality of the SMT output to make a recom-
mendation. The SVM classifier uses features from
the SMT system, the TM and additional linguis-
tic features to estimate whether the SMT output is
better than the hit from the TM.
4.1 Problem Formulation
As we treat translation recommendation as a bi-
nary classification problem, we have a pair of out-
puts from TM and MT for each sentence. Ideally
the classifier will recommend the output that needs
less post-editing effort. As large-scale annotated
data is not yet available for this task, we use auto-
matic TER scores (Snover et al, 2006) as the mea-
sure for the required post-editing effort. In the fu-
ture, we hope to train our system on HTER (TER
with human targeted references) scores (Snover et
al., 2006) once the necessary human annotations
are in place. In the meantime we use TER, as TER
is shown to have high correlation with HTER.
We label the training examples as in (3):
y =
{
+1 if TER(MT) < TER(TM)
?1 if TER(MT) ? TER(TM) (3)
Each instance is associated with a set of features
from both the MT and TM outputs, which are dis-
cussed in more detail in Section 4.3.
4.2 Recommendation Confidence Estimation
In classical settings involving SVMs, confidence
levels are represented as margins of binary predic-
tions. However, these margins provide little in-
sight for our application because the numbers are
only meaningful when compared to each other.
What is more preferable is a probabilistic confi-
dence score (e.g. 90% confidence) which is better
understood by post-editors and translators.
We use the techniques proposed by (Platt, 1999)
and improved by (Lin et al, 2007) to obtain the
posterior probability of a classification, which is
used as the confidence score in our system.
Platt?s method estimates the posterior probabil-
ity with a sigmod function, as in (4):
Pr(y = 1|x) ? PA,B(f) ?
1
1 + exp(Af + B) (4)
where f = f(x) is the decision function of the
estimated SVM. A and B are parameters that min-
imize the cross-entropy error function F on the
training data, as in Eq. (5):
min
z=(A,B)
F (z) = ?
l
?
i=1
(tilog(pi) + (1? ti)log(1? pi)),
where pi = PA,B(fi), and ti =
{N++1
N++2
if yi = +1
1
N?+2
if yi = ?1
(5)
where z = (A,B) is a parameter setting, and
N+ and N? are the numbers of observed positive
and negative examples, respectively, for the label
yi. These numbers are obtained using an internal
cross-validation on the training set.
4.3 The Feature Set
We use three types of features in classification: the
MT system features, the TM feature and system-
independent features.
4.3.1 The MT System Features
These features include those typically used in
SMT, namely the phrase-translation model scores,
the language model probability, the distance-based
reordering score, the lexicalized reordering model
scores, and the word penalty.
4.3.2 The TM Feature
The TM feature is the fuzzy match (Sikes, 2007)
cost of the TM hit. The calculation of fuzzy match
score itself is one of the core technologies in TM
systems and varies among different vendors. We
compute fuzzy match cost as the minimum Edit
Distance (Levenshtein, 1966) between the source
and TM entry, normalized by the length of the
source as in (6), as most of the current implemen-
tations are based on edit distance while allowing
some additional flexible matching.
hfm(t) = min
e
EditDistance(s, e)
Len(s) (6)
where s is the source side of t, the sentence to
translate, and e is the source side of an entry in the
TM. For fuzzy match scores F , this fuzzy match
cost hfm roughly corresponds to 1?F . The differ-
ence in calculation does not influence classifica-
tion, and allows direct comparison between a pure
TM system and a translation recommendation sys-
tem in Section 5.4.2.
624
4.3.3 System-Independent Features
We use several features that are independent of
the translation system, which are useful when a
third-party translation service is used or the MT
system is simply treated as a black-box. These
features are source and target side LM scores,
pseudo source fuzzy match scores and IBM model
1 scores.
Source-Side Language Model Score and Per-
plexity. We compute the language model (LM)
score and perplexity of the input source sentence
on a LM trained on the source-side training data of
the SMT system. The inputs that have lower per-
plexity or higher LM score are more similar to the
dataset on which the SMT system is built.
Target-Side Language Model Perplexity. We
compute the LM probability and perplexity of the
target side as a measure of fluency. Language
model perplexity of the MT outputs are calculated,
and LM probability is already part of the MT sys-
tems scores. LM scores on TM outputs are also
computed, though they are not as informative as
scores on the MT side, since TM outputs should
be grammatically perfect.
The Pseudo-Source Fuzzy Match Score. We
translate the output back to obtain a pseudo source
sentence. We compute the fuzzy match score
between the original source sentence and this
pseudo-source. If the MT/TM system performs
well enough, these two sentences should be the
same or very similar. Therefore, the fuzzy match
score here gives an estimation of the confidence
level of the output. We compute this score for both
the MT output and the TM hit.
The IBM Model 1 Score. The fuzzy match
score does not measure whether the hit could be
a correct translation, i.e. it does not take into ac-
count the correspondence between the source and
target, but rather only the source-side information.
For the TM hit, the IBM Model 1 score (Brown
et al, 1993) serves as a rough estimation of how
good a translation it is on the word level; for the
MT output, on the other hand, it is a black-box
feature to estimate translation quality when the in-
formation from the translation model is not avail-
able. We compute bidirectional (source-to-target
and target-to-source) model 1 scores on both TM
and MT outputs.
5 Experiments
5.1 Experimental Settings
Our raw data set is an English?French translation
memory with technical translation from Syman-
tec, consisting of 51K sentence pairs. We ran-
domly selected 43K to train an SMT system and
translated the English side of the remaining 8K
sentence pairs. The average sentence length of
the training set is 13.5 words and the size of the
training set is comparable to the (larger) TMs used
in the industry. Note that we remove the exact
matches in the TM from our dataset, because ex-
act matches will be reused and not presented to the
post-editor in a typical TM setting.
As for the SMT system, we use a stan-
dard log-linear PB-SMT model (Och and Ney,
2002): GIZA++ implementation of IBM word
alignment model 4,1 the refinement and phrase-
extraction heuristics described in (Koehn et
al., 2003), minimum-error-rate training (Och,
2003), a 5-gram language model with Kneser-Ney
smoothing (Kneser and Ney, 1995) trained with
SRILM (Stolcke, 2002) on the English side of the
training data, and Moses (Koehn et al, 2007) to
decode. We train a system in the opposite direc-
tion using the same data to produce the pseudo-
source sentences.
We train the SVM classifier using the lib-
SVM (Chang and Lin, 2001) toolkit. The SVM-
training and testing is performed on the remaining
8K sentences with 4-fold cross validation. We also
report 95% confidence intervals.
The SVM hyper-parameters are tuned using the
training data of the first fold in the 4-fold cross val-
idation via a brute force grid search. More specifi-
cally, for parameterC in (1) we search in the range
[2?5, 215], and for parameter ? (2) we search in the
range [2?15, 23]. The step size is 2 on the expo-
nent.
5.2 The Evaluation Metrics
We measure the quality of the classification by
precision and recall. Let A be the set of recom-
mended MT outputs, and B be the set of MT out-
puts that have lower TER than TM hits. We stan-
dardly define precision P , recall R and F-value as
in (7):
1More specifically, we performed 5 iterations of Model 1,
5 iterations of HMM, 3 iterations of Model 3, and 3 iterations
of Model 4.
625
P = |A
?
B|
|A| , R =
|A
?
B|
|B| and F =
2PR
P + R (7)
5.3 Recommendation Results
In Table 1, we report recommendation perfor-
mance using MT and TM system features (SYS),
system features plus system-independent features
(ALL:SYS+SI), and system-independent features
only (SI).
Table 1: Recommendation Results
Precision Recall F-Score
SYS 82.53?1.17 96.44?0.68 88.95?.56
SI 82.56?1.46 95.83?0.52 88.70?.65
ALL 83.45?1.33 95.56?1.33 89.09?.24
From Table 1, we observe that MT and TM
system-internal features are very useful for pro-
ducing a stable (as indicated by the smaller con-
fidence interval) recommendation system (SYS).
Interestingly, only using some simple system-
external features as described in Section 4.3.3 can
also yield a system with reasonably good per-
formance (SI). We expect that the performance
can be further boosted by adding more syntactic
and semantic features. Combining all the system-
internal and -external features leads to limited
gains in Precision and F-score compared to using
only system-internal features (SYS) only. This in-
dicates that at the default confidence level, current
system-external (resp. system-internal) features
can only play a limited role in informing the sys-
tem when current system-internal (resp. system-
external) features are available. We show in Sec-
tion 5.4.2 that combing both system-internal and -
external features can yield higher, more stable pre-
cision when adjusting the confidence levels of the
classifier. Additionally, the performance of system
SI is promising given the fact that we are using
only a limited number of simple features, which
demonstrates a good prospect of applying our rec-
ommendation system to MT systems where we do
not have access to their internal features.
5.4 Further Improving Recommendation
Precision
Table 1 shows that classification recall is very
high, which suggests that precision can still be im-
proved, even though the F-score is not low. Con-
sidering that TM is the dominant technology used
by post-editors, a recommendation to replace the
hit from the TM would require more confidence,
i.e. higher precision. Ideally our aim is to obtain
a level of 0.9 precision at the cost of some recall,
if necessary. We propose two methods to achieve
this goal.
5.4.1 Classifier Margins
We experiment with different margins on the train-
ing data to tune precision and recall in order to
obtain a desired balance. In the basic case, the
training example would be marked as in (3). If we
label both the training and test sets with this rule,
the accuracy of the prediction will be maximized.
We try to achieve higher precision by enforc-
ing a larger bias towards negative examples in the
training set so that some borderline positive in-
stances would actually be labeled as negative, and
the classifier would have higher precision in the
prediction stage as in (8).
y =
{
+1 if TER(SMT) + b < TER(TM)
?1 if TER(SMT) + b > TER(TM)
(8)
We experiment with b in [0, 0.25] usingMT sys-
tem features and TM features. Results are reported
in Table 2.
Table 2: Classifier margins
Precision Recall
TER+0 83.45?1.33 95.56?1.33
TER+0.05 82.41?1.23 94.41?1.01
TER+0.10 84.53?0.98 88.81?0.89
TER+0.15 85.24?0.91 87.08?2.38
TER+0.20 87.59?0.57 75.86?2.70
TER+0.25 89.29?0.93 66.67?2.53
The highest accuracy and F-value is achieved
by TER + 0, as all other settings are trained
on biased margins. Except for a small drop in
TER+0.05, other configurations all obtain higher
precision than TER+ 0. We note that we can ob-
tain 0.85 precision without a big sacrifice in recall
with b=0.15, but for larger improvements on pre-
cision, recall will drop more rapidly.
When we use b beyond 0.25, the margin be-
comes less reliable, as the number of positive
examples becomes too small. In particular, this
causes the SVM parameters we tune on in the first
fold to become less applicable to the other folds.
This is one limitation of using biased margins to
626
obtain high precision. The method presented in
Section 5.4.2 is less influenced by this limitation.
5.4.2 Adjusting Confidence Levels
An alternative to using a biased margin is to output
a confidence score during prediction and to thresh-
old on the confidence score. It is also possible to
add this method to the SVM model trained with a
biased margin.
We use the SVM confidence estimation tech-
niques in Section 4.2 to obtain the confidence
level of the recommendation, and change the con-
fidence threshold for recommendation when nec-
essary. This also allows us to compare directly
against a simple baseline inspired by TM users. In
a TM environment, some users simply ignore TM
hits below a certain fuzzy match score F (usually
from 0.7 to 0.8). This fuzzy match score reflects
the confidence of recommending the TM hits. To
obtain the confidence of recommending an SMT
output, our baseline (FM) uses fuzzy match costs
hFM ? 1?F (cf. Section 4.3.2) for the TM hits as
the level of confidence. In other words, the higher
the fuzzy match cost of the TM hit is (lower fuzzy
match score), the higher the confidence of recom-
mending the SMT output. We compare this base-
line with the three settings in Section 5.
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
Pre
cis
ion
Confidence
SISysAllFM
Figure 1: Precision Changes with Confidence
Level
Figure 1 shows that the precision curve of FM
is low and flat when the fuzzy match costs are
low (from 0 to 0.6), indicating that it is unwise to
recommend an SMT output when the TM hit has
a low fuzzy match cost (corresponding to higher
fuzzy match score, from 0.4 to 1). We also observe
that the precision of the recommendation receives
a boost when the fuzzy match costs for the TM
hits are above 0.7 (fuzzy match score lower than
0.3), indicating that SMT output should be recom-
mended when the TM hit has a high fuzzy match
cost (low fuzzy match score). With this boost, the
precision of the baseline system can reach 0.85,
demonstrating that a proper thresholding of fuzzy
match scores can be used effectively to discrimi-
nate the recommendation of the TM hit from the
recommendation of the SMT output.
However, using the TM information only does
not always find the easiest-to-edit translation. For
example, an excellent SMT output should be rec-
ommended even if there exists a good TM hit (e.g.
fuzzy match score is 0.7 or more). On the other
hand, a misleading SMT output should not be rec-
ommended if there exists a poor but useful TM
match (e.g. fuzzy match score is 0.2).
Our system is able to tackle these complica-
tions as it incorporates features from the MT and
the TM systems simultaneously. Figure 1 shows
that both the SYS and the ALL setting consistently
outperform FM, indicating that our classification
scheme can better integrate the MT output into the
TM system than this naive baseline.
The SI feature set does not perform well when
the confidence level is set above 0.85 (cf. the de-
scending tail of the SI curve in Figure 1). This
might indicate that this feature set is not reliable
enough to extract the best translations. How-
ever, when the requirement on precision is not that
high, and the MT-internal features are not avail-
able, it would still be desirable to obtain transla-
tion recommendations with these black-box fea-
tures. The difference between SYS and ALL is
generally small, but ALL performs steadily better
in [0.5, 0,8].
Table 3: Recall at Fixed Precision
Recall
SYS @85PREC 88.12?1.32
SYS @90PREC 52.73?2.31
SI @85PREC 87.33?1.53
ALL @85PREC 88.57?1.95
ALL @90PREC 51.92?4.28
5.5 Precision Constraints
In Table 3 we also present the recall scores at 0.85
and 0.9 precision for SYS, SI and ALL models to
demonstrate our system?s performance when there
is a hard constraint on precision. Note that our
system will return the TM entry when there is an
exact match, so the overall precision of the system
627
is above the precision score we set here in a ma-
ture TM environment, as a significant portion of
the material to be translated will have a complete
match in the TM system.
In Table 3 for MODEL@K, the recall scores are
achieved when the prediction precision is better
than K with 0.95 confidence. For each model, pre-
cision at 0.85 can be obtained without a very big
loss on recall. However, if we want to demand
further recommendation precision (more conser-
vative in recommending SMT output), the recall
level will begin to drop more quickly. If we use
only system-independent features (SI), we cannot
achieve as high precision as with other models
even if we sacrifice more recall.
Based on these results, the users of the TM sys-
tem can choose between precision and recall ac-
cording to their own needs. As the threshold does
not involve training of the SMT system or the
SVM classifier, the user is able to determine this
trade-off at runtime.
Table 4: Contribution of Features
Precision Recall F Score
SYS 82.53?1.17 96.44?0.68 88.95?.56
+M1 82.87?1.26 96.23?0.53 89.05?.52
+LM 82.82?1.16 96.20?1.14 89.01?.23
+PS 83.21?1.33 96.61?0.44 89.41?.84
5.6 Contribution of Features
In Section 4.3.3 we suggested three sets of
system-independent features: features based on
the source- and target-side language model (LM),
the IBMModel 1 (M1) and the fuzzy match scores
on pseudo-source (PS). We compare the contribu-
tion of these features in Table 4.
In sum, all the three sets of system-independent
features improve the precision and F-scores of the
MT and TM system features. The improvement
is not significant, but improvement on every set of
system-independent features gives some credit to
the capability of SI features, as does the fact that
SI features perform close to SYS features in Table
1.
6 Analysis of Post-Editing Effort
A natural question on the integration models is
whether the classification reduces the effort of the
translators and post-editors: after reading these
recommendations, will they translate/edit less than
they would otherwise have to? Ideally this ques-
tion would be answered by human post-editors in
a large-scale experimental setting. As we have
not yet conducted a manual post-editing experi-
ment, we conduct two sets of analyses, trying to
show which type of edits will be required for dif-
ferent recommendation confidence levels. We also
present possible methods for human evaluation at
the end of this section.
6.1 Edit Statistics
We provide the statistics of the number of edits
for each sentence with 0.95 confidence intervals,
sorted by TER edit types. Statistics of positive in-
stances in classification (i.e. the instances in which
MT output is recommended over the TM hit) are
given in Table 5.
When an MT output is recommended, its TM
counterpart will require a larger average number
of total edits than the MT output, as we expect. If
we drill down, however, we also observe that many
of the saved edits come from the Substitution cat-
egory, which is the most costly operation from the
post-editing perspective. In this case, the recom-
mended MT output actually saves more effort for
the editors than what is shown by the TER score.
It reflects the fact that TM outputs are not actual
translations, and might need heavier editing.
Table 6 shows the statistics of negative instances
in classification (i.e. the instances in which MT
output is not recommended over the TM hit). In
this case, the MT output requires considerably
more edits than the TM hits in terms of all four
TER edit types, i.e. insertion, substitution, dele-
tion and shift. This reflects the fact that some high
quality TM matches can be very useful as a trans-
lation.
6.2 Edit Statistics on Recommendations of
Higher Confidence
We present the edit statistics of recommendations
with higher confidence in Table 7. Comparing Ta-
bles 5 and 7, we see that if recommended with
higher confidence, the MT output will need sub-
stantially less edits than the TM output: e.g. 3.28
fewer substitutions on average.
From the characteristics of the high confidence
recommendations, we suspect that these mainly
comprise harder to translate (i.e. different from
the SMT training set/TM database) sentences, as
indicated by the slightly increased edit operations
628
Table 5: Edit Statistics when Recommending MT Outputs in Classification, confidence=0.5
Insertion Substitution Deletion Shift
MT 0.9849 ? 0.0408 2.2881 ? 0.0672 0.8686 ? 0.0370 1.2500 ? 0.0598
TM 0.7762 ? 0.0408 4.5841 ? 0.1036 3.1567 ? 0.1120 1.2096 ? 0.0554
Table 6: Edit Statistics when NOT Recommending MT Outputs in Classification, confidence=0.5
Insertion Substitution Deletion Shift
MT 1.0830 ? 0.1167 2.2885 ? 0.1376 1.0964 ? 0.1137 1.5381 ? 0.1962
TM 0.7554 ? 0.0376 1.5527 ? 0.1584 1.0090 ? 0.1850 0.4731 ? 0.1083
Table 7: Edit Statistics when Recommending MT Outputs in Classification, confidence=0.85
Insertion Substitution Deletion Shift
MT 1.1665 ? 0.0615 2.7334 ? 0.0969 1.0277 ? 0.0544 1.5549 ? 0.0899
TM 0.8894 ? 0.0594 6.0085 ? 0.1501 4.1770 ? 0.1719 1.6727 ? 0.0846
on the MT side. TM produces much worse edit-
candidates for such sentences, as indicated by
the numbers in Table 7, since TM does not have
the ability to automatically reconstruct an output
through the combination of several segments.
6.3 Plan for Human Evaluation
Evaluation with human post-editors is crucial to
validate and improve translation recommendation.
There are two possible avenues to pursue:
? Test our system on professional post-editors.
By providing them with the TM output, the
MT output and the one recommended to edit,
we can measure the true accuracy of our
recommendation, as well as the post-editing
time we save for the post-editors;
? Apply the presented method on open do-
main data and evaluate it using crowd-
sourcing. It has been shown that crowd-
sourcing tools, such as the Amazon Me-
chanical Turk (Callison-Burch, 2009), can
help developers to obtain good human judge-
ments on MT output quality both cheaply and
quickly. Given that our problem is related to
MT quality estimation in nature, it can poten-
tially benefit from such tools as well.
7 Conclusions and Future Work
In this paper we present a classification model to
integrate SMT into a TM system, in order to facili-
tate the work of post-editors. Insodoing we handle
the problem of MT quality estimation as binary
prediction instead of regression. From the post-
editors? perspective, they can continue to work in
their familiar TM environment, use the same cost-
estimation methods, and at the same time bene-
fit from the power of state-of-the-art MT. We use
SVMs to make these predictions, and use grid
search to find better RBF kernel parameters.
We explore features from inside the MT sys-
tem, from the TM, as well as features that make
no assumption on the translation model for the bi-
nary classification. With these features we make
glass-box and black-box predictions. Experiments
show that the models can achieve 0.85 precision at
a level of 0.89 recall, and even higher precision if
we sacrifice more recall. With this guarantee on
precision, our method can be used in a TM envi-
ronment without changing the upper-bound of the
related cost estimation.
Finally, we analyze the characteristics of the in-
tegrated outputs. We present results to show that,
if measured by number, type and content of ed-
its in TER, the recommended sentences produced
by the classification model would bring about less
post-editing effort than the TM outputs.
This work can be extended in the following
ways. Most importantly, it is useful to test the
model in user studies, as proposed in Section 6.3.
A user study can serve two purposes: 1) it can
validate the effectiveness of the method by mea-
suring the amount of edit effort it saves; and 2)
the byproduct of the user study ? post-edited sen-
tences ? can be used to generate HTER scores
to train a better recommendation model. Further-
more, we want to experiment and improve on the
adaptability of this method, as the current experi-
ment is on a specific domain and language pair.
629
Acknowledgements
This research is supported by the Science Foundation Ireland
(Grant 07/CE/I1142) as part of the Centre for Next Gener-
ation Localisation (www.cngl.ie) at Dublin City University.
We thank Symantec for providing the TM database and the
anonymous reviewers for their insightful comments.
References
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. 2004. Confidence estimation for ma-
chine translation. In The 20th International Conference
on Computational Linguistics (Coling-2004), pages 315 ?
321, Geneva, Switzerland.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: parameter estimation.
Computational Linguistics, 19(2):263 ? 311.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Mechani-
cal Turk. In The 2009 Conference on Empirical Methods
in Natural Language Processing (EMNLP-2009), pages
286 ? 295, Singapore.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.tw/
?cjlin/libsvm.
Corinna Cortes and Vladimir Vapnik. 1995. Support-vector
networks. Machine learning, 20(3):273 ? 297.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In The 1995 International
Conference on Acoustics, Speech, and Signal Processing
(ICASSP-95), pages 181 ? 184, Detroit, MI.
Philipp. Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In The 2003 Confer-
ence of the North American Chapter of the Association for
Computational Linguistics on Human Language Technol-
ogy (NAACL/HLT-2003), pages 48 ? 54, Edmonton, Al-
berta, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In The 45th Annual Meet-
ing of the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster Ses-
sions (ACL-2007), pages 177 ? 180, Prague, Czech Re-
public.
Vladimir Iosifovich Levenshtein. 1966. Binary codes capa-
ble of correcting deletions, insertions, and reversals. So-
viet Physics Doklady, 10(8):707 ? 710.
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng. 2007.
A note on platt?s probabilistic outputs for support vector
machines. Machine Learning, 68(3):267 ? 276.
Franz Josef Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical ma-
chine translation. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguistics (ACL-
2002), pages 295 ? 302, Philadelphia, PA.
Franz Josef Och. 2003. Minimum error rate training in sta-
tistical machine translation. In The 41st Annual Meet-
ing on Association for Computational Linguistics (ACL-
2003), pages 160 ? 167.
John C. Platt. 1999. Probabilistic outputs for support vector
machines and comparisons to regularized likelihood meth-
ods. Advances in Large Margin Classifiers, pages 61 ? 74.
Christopher B. Quirk. 2004. Training a sentence-level ma-
chine translation confidence measure. In The Fourth In-
ternational Conference on Language Resources and Eval-
uation (LREC-2004), pages 825 ? 828, Lisbon, Portugal.
Richard Sikes. 2007. Fuzzy matching in theory and practice.
Multilingual, 18(6):39 ? 43.
Michel Simard and Pierre Isabelle. 2009. Phrase-based
machine translation in a computer-assisted translation en-
vironment. In The Twelfth Machine Translation Sum-
mit (MT Summit XII), pages 120 ? 127, Ottawa, Ontario,
Canada.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of transla-
tion edit rate with targeted human annotation. In The 2006
conference of the Association for Machine Translation in
the Americas (AMTA-2006), pages 223 ? 231, Cambridge,
MA.
Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco
Turchi, and Nello Cristianini. 2009a. Estimating the
sentence-level quality of machine translation systems. In
The 13th Annual Conference of the European Association
for Machine Translation (EAMT-2009), pages 28 ? 35,
Barcelona, Spain.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran Wang,
and John Shawe-Taylor. 2009b. Improving the confidence
of machine translation quality estimates. In The Twelfth
Machine Translation Summit (MT Summit XII), pages 136
? 143, Ottawa, Ontario, Canada.
Andreas Stolcke. 2002. SRILM-an extensible language
modeling toolkit. In The Seventh International Confer-
ence on Spoken Language Processing, volume 2, pages
901 ? 904, Denver, CO.
Nicola Ueffing and Hermann Ney. 2005. Application
of word-level confidence measures in interactive statisti-
cal machine translation. In The Ninth Annual Confer-
ence of the European Association for Machine Translation
(EAMT-2005), pages 262 ? 270, Budapest, Hungary.
Nicola Ueffing, Klaus Macherey, and Hermann Ney. 2003.
Confidence measures for statistical machine translation.
In The Ninth Machine Translation Summit (MT Summit
IX), pages 394 ? 401, New Orleans, LA.
630
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1087?1097,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Hard Constraints for Grammatical Function Labelling
Wolfgang Seeker
University of Stuttgart
Institut fu?r Maschinelle Sprachverarbeitung
seeker@ims.uni-stuttgart.de
Ines Rehbein
University of Saarland
Dep. for Comp. Linguistics & Phonetics
rehbein@coli.uni-sb.de
Jonas Kuhn
University of Stuttgart
Institut fu?r Maschinelle Sprachverarbeitung
jonas@ims.uni-stuttgart.de
Josef van Genabith
Dublin City University
CNGL and School of Computing
josef@computing.dcu.ie
Abstract
For languages with (semi-) free word or-
der (such as German), labelling gramma-
tical functions on top of phrase-structural
constituent analyses is crucial for making
them interpretable. Unfortunately, most
statistical classifiers consider only local
information for function labelling and fail
to capture important restrictions on the
distribution of core argument functions
such as subject, object etc., namely that
there is at most one subject (etc.) per
clause. We augment a statistical classifier
with an integer linear program imposing
hard linguistic constraints on the solution
space output by the classifier, capturing
global distributional restrictions. We show
that this improves labelling quality, in par-
ticular for argument grammatical func-
tions, in an intrinsic evaluation, and, im-
portantly, grammar coverage for treebank-
based (Lexical-Functional) grammar ac-
quisition and parsing, in an extrinsic eval-
uation.
1 Introduction
Phrase or constituent structure is often regarded as
an analysis step guiding semantic interpretation,
while grammatical functions (i. e. subject, object,
modifier etc.) provide important information rele-
vant to determining predicate-argument structure.
In languages with restricted word order (e. g.
English), core grammatical functions can often
be recovered from configurational information in
constituent structure analyses. By contrast, sim-
ple constituent structures are not sufficient for less
configurational languages, which tend to encode
grammatical functions by morphological means
(Bresnan, 2001). Case features, for instance, can
be important indicators of grammatical functions.
Unfortunately, many of these languages (including
German) exhibit strong syncretism where morpho-
logical cues can be highly ambiguous with respect
to functional information.
Statistical classifiers have been successfully
used to label constituent structure parser output
with grammatical function information (Blaheta
and Charniak, 2000; Chrupa?a and Van Genabith,
2006). However, as these approaches tend to
use only limited and local context information
for learning and prediction, they often fail to en-
force simple yet important global linguistic con-
straints that exist for most languages, e. g. that
there will be at most one subject (object) per sen-
tence/clause.1
?Hard? linguistic constraints, such as these,
tend to affect mostly the ?core grammatical func-
tions?, i. e. the argument functions (rather than
e. g. adjuncts) of a particular predicate. As these
functions constitute the core meaning of a sen-
tence (as in: who did what to whom), it is impor-
tant to get them right. We present a system that
adds grammatical function labels to constituent
parser output for German in a postprocessing step.
We combine a statistical classifier with an inte-
ger linear program (ILP) to model non-violable
global linguistic constraints, restricting the solu-
tion space of the classifier to those labellings that
comply with our set of global constraints. There
are, of course, many other ways of including func-
tional information into the output of a syntactic
parser. Klein and Manning (2003) show that merg-
ing some linguistically motivated function labels
with specific syntactic categories can improve the
performance of a PCFG model on Penn-II En-
1Coordinate subjects/objects form a constituent that func-
tions as a joint subject/object.
1087
glish data.2 Tsarfaty and Sim?aan (2008) present
a statistical model (Relational-Realizational Pars-
ing) that alternates between functional and config-
urational information for constituency tree pars-
ing and Hebrew data. Dependency parsers like
the MST parser (McDonald and Pereira, 2006) and
Malt parser (Nivre et al, 2007) use function labels
as core part of their underlying formalism. In this
paper, we focus on phrase structure parsing with
function labelling as a post-processing step.
Integer linear programs have already been suc-
cessfully used in related fields including semantic
role labelling (Punyakanok et al, 2004), relation
and entity classification (Roth and Yih, 2004), sen-
tence compression (Clarke and Lapata, 2008) and
dependency parsing (Martins et al, 2009). Early
work on function labelling for German (Brants et
al., 1997) reports 94.2% accuracy on gold data (a
very early version of the TiGer Treebank (Brants
et al, 2002)) using Markov models. Klenner
(2007) uses a system similar to ? but more re-
stricted than ? ours to label syntactic chunks de-
rived from the TiGer Treebank. His research fo-
cusses on the correct selection of predefined sub-
categorisation frames for a verb (see also Klenner
(2005)). By contrast, our research does not involve
subcategorisation frames as an external resource,
instead opting for a less knowledge-intensive ap-
proach. Klenner?s system was evaluated on gold
treebank data and used a small set of 7 dependency
labels. We show that an ILP-based approach can
be scaled to a large and comprehensive set of 42
labels, achieving 97.99% label accuracy on gold
standard trees. Furthermore, we apply the sys-
tem to automatically parsed data using a state-of-
the-art statistical phrase-structure parser with a la-
bel accuracy of 94.10%. In both cases, the ILP-
based approach improves the quality of argument
function labelling when compared with a non-ILP-
approach. Finally, we show that the approach
substantially improves the quality and coverage
(from 93.6% to 98.4%) of treebank-based Lexical-
Functional Grammars for German over previous
work in Rehbein and van Genabith (2009).
The paper is structured as follows: Section 2
presents basic data demonstrating the challenges
presented by German word order and case syn-
cretism for the function labeller. Section 3 de-
2Table 6 shows that for our data a model with merged
category and function labels (but without hard constraints!)
performs slightly worse than the ILP approach developed in
this paper.
scribes the labeller including the feature model of
the classifier and the integer linear program used
to pick the correct labelling. The evaluation part
(Section 4) is split into an intrinsic evaluation mea-
suring the quality of the labelling directly using
the German TiGer Treebank (Brants et al, 2002),
and an extrinsic evaluation where we test the im-
pact of the constraint-based labelling on treebank-
based automatic LFG grammar acquisition.
2 Data
Unlike English, German exhibits a relatively free
word order, i. e. in main clauses, the verb occu-
pies second position (the last position in subor-
dinated clauses) and arguments and adjuncts can
be placed (fairly) freely. The grammatical func-
tion of a noun phrase is marked morphologically
on its constituting parts. Determiners, pronouns,
adjectives and nouns carry case markings and in
order to be well-formed, all parts of a noun phrase
have to agree on their case features. German uses
a nominative?accusative system to mark predicate
arguments. Subjects are marked with nominative
case, direct objects carry accusative case. Further-
more, indirect objects are mostly marked with da-
tive case and sometimes genitive case.
(1) Der Lo?we
NOM
the lion
gibt
gives
dem Wolf
DAT
the wolf
einen Besen.
ACC
a broom
The lion gives a broom to the wolf.
(1) shows a sentence containing the ditransi-
tive verb geben (to give) with its three arguments.
Here, the subject is unambiguously marked with
nominative case (NOM), the indirect object with
dative case (DAT) and the direct object with ac-
cusative case (ACC). (2) shows possible word or-
ders for the arguments in this sentence.3
(2) Der Lo?we gibt einen Besen dem Wolf.
Dem Wolf gibt der Lo?we einen Besen.
Dem Wolf gibt einen Besen der Lo?we.
Einen Besen gibt der Lo?we dem Wolf.
Einen Besen gibt dem Wolf der Lo?we.
Since all permutations of arguments are possi-
ble, there is no chance for a statistical classifier to
decide on the correct function of a noun phrase by
its position alone. Introducing adjuncts to this ex-
ample makes matters even worse.
3Note that although (apart from the position of the finite
verb) there are no syntactic restrictions on the word order,
there are restrictions pertaining to phonological or informa-
tion structure.
1088
Case information for a given noun phrase can
give a classifier some clue about the correct ar-
gument function, since functions are strongly re-
lated to case values. Unfortunately, the German
case system is complex (see Eisenberg (2006) for
a thorough description) and exhibits a high degree
of case syncretism. (3) shows a sentence where
both argument NPs are ambiguous between nom-
inative or accusative case. In such cases, addi-
tional semantic or contextual information is re-
quired for disambiguation. A statistical classifier
(with access to local information only) runs a high
risk of incorrectly classifying both NPs as sub-
jects, or both as direct objects or even as nominal
predicates (which are also required to carry nom-
inative case). This would leave us with uninter-
pretable results. Uninterpretability of this kind can
be avoided if we are able to constrain the number
of subjects and objects globally to one per clause.4
(3) Das Schaf
NOM/ACC
the sheep
sieht
sees
das Ma?dchen.
NOM/ACC
the girl
EITHER The sheep sees the girl
OR The girl sees the sheep.
3 Grammatical Function Labelling
Our function labeller was developed and tested on
the TiGer Treebank (Brants et al, 2002). The
TiGer Treebank is a phrase-structure and gram-
matical function annotated treebank with 50,000
newspaper sentences from the Frankfurter Rund-
schau (Release 2, July 2006). Its overall anno-
tation scheme is quite flat to account for the rel-
atively free word order of German and does not
allow for unary branching. The annotations use
non-projective trees modelling long distance de-
pendencies directly by crossing branches. Words
are lemmatised and part-of-speech tagged with the
Stuttgart-Tu?bingen Tag Set (STTS) (Schiller et al,
1999) and contain morphological annotations (Re-
lease 2). TiGer uses 25 syntactic categories and a
set of 42 function labels to annotate the grammat-
ical function of a phrase.
The function labeller consists of two main com-
ponents, a maximum entropy classifier and an in-
teger linear program. This basic architecture was
introduced by Punyakanok et al (2004) for the
task of semantic role labelling and since then has
been applied to different NLP tasks without signif-
icant changes. In our case, its input is a bare tree
4Although the classifier may, of course, still identify the
wrong phrase as subject or object.
structure (as obtained by a standard phrase struc-
ture parser) and it outputs a tree structure where
every node is labelled with the grammatical rela-
tion it bears to its mother node. For each possi-
ble label and for each node, the classifier assigns
a probability that this node is labelled by this la-
bel. This results in a complete probability distri-
bution over all labels for each node. An integer
linear program then tries to find the optimal over-
all tree labelling by picking for each node the label
with the highest probability without violating any
of its constraints. These constraints implement lin-
guistic rules like the one-subject-per-sentence rule
mentioned above. They can also be used to cap-
ture treebank particulars, such as for example that
punctuation marks never receive a label.
3.1 The Feature Model
Maximum entropy classifiers have been used in a
wide range of applications in NLP for a long time
(Berger et al, 1996; Ratnaparkhi, 1998). They
usually give good results while at the same time
allowing for the inclusion of arbitrarily complex
features. They also have the advantage that they
directly output probability distributions over their
set of labels (unlike e. g. SVMs).
The classifier uses the following features:
? the lemma (if terminal node)
? the category (the POS for terminal nodes)
? the number of left/right sisters
? the category of the two left/right sisters
? the number of daughters
? the number of terminals covered
? the lemma of the left/right corner terminal
? the category of the left/right corner terminal
? the category of the mother node
? the category of the mother?s head node
? the lemma of the mother?s head node
? the category of the grandmother node
? the category of the grandmother?s head node
? the lemma of the grandmother?s head node
? the case features for noun phrases
? the category for PP objects
? the lemma for PP objects (if terminal node)
These features are also computed for the head
of the phrase, determined using a set of head-
finding rules in the style of Magerman (1995)
adapted to TiGer. For lemmatisation, we use Tree-
Tagger (Schmid, 1994) and case features of noun
1089
phrases are obtained from a full German morpho-
logical analyser based on (Schiller, 1994). If a
noun phrase consists of a single word (e. g. pro-
nouns, but also bare common nouns and proper
nouns), all case values output by the analyser are
used to reflect the case syncretism. For multi-word
noun phrases, the case feature is computed by tak-
ing the intersection of all case-bearing words in-
side the noun phrase, i. e. determiners, pronouns,
adjectives, common nouns and proper nouns. If,
for some reason (e.g., due to a bracketing error in
phrase structure parsing), the intersection turns out
to be empty, all four case values are assigned to the
phrase.5
3.2 Constrained Optimisation
In the second step, a binary integer linear pro-
gram is used to select those labels that optimise the
whole tree labelling. A linear program consists of
a linear objective function that is to be maximised
(or minimised) and a set of constraints which im-
pose conditions on the variables of the objective
function (see (Clarke and Lapata, 2008) for a short
but readable introduction). Although solving a lin-
ear program has polynomial complexity, requiring
the variables to be integral or binary makes find-
ing a solution exponentially hard in the worst case.
Fortunately, there are efficient algorithms which
are capable of handling a large number of vari-
ables and constraints in practical applications.6
For the function labeller, we define the set of
binary variables V = N ? L to be the crossprod-
uct of the set of nodes N and the set of labels L.
Setting a variable xn,l to 1 means that node n is
labelled by label l. Every variable is weighted by
the probability wn,l = P (l|f(n)) which the clas-
sifier has assigned to this node-label combination.
The objective function that we seek to optimise is
defined as the sum over all weighted variables:
max
?
n?N
?
l?L
wn,lxn,l (4)
Since we want every node to receive exactly one
5We decided to train the classifier on automatically
assigned and possibly ambiguous morphological informa-
tion instead of on the hand-annotated and manually disam-
biguated morphological information provided by TiGer be-
cause we want the classifier to learn the German case syn-
cretism. This way, the classifier will perform better when pre-
sented with unseen data (e.g. from parser output) for which
no hand-annotated morphological information is available.
6See lpsolve (http://lpsolve.sourceforge.net/) or GLPK
(http://www.gnu.org/software/glpk/glpk.html) for open-
source implementations
label, we add a constraint that for every node n,
exactly one of its variables is set to 1.
?
l?L
xn,l = 1 (5)
Up to now, the whole system is doing exactly
the same as an ordinary classifier that always takes
the most probable label for each node. We will
now add additional global and local linguistic con-
straints.7
The first and most important constraint restricts
the number of each argument function (as opposed
to modifier functions) to at most one per clause.
Let D ? N ? N be the direct dominance rela-
tion between the nodes of the current tree. For ev-
ery node n with category S (sentence) or VP (verb
phrase), at most one of its daughters is allowed
to be labelled SB (subject). The single-subject-
function condition is defined as:
cat(n) ? {S, V P} ??
?
?n,m??D
xm,SB ? 1 (6)
Identical constraints are added for labels OA,
OA2, DA, OG, OP, PD, OC, EP.8
We add further constraints to capture the follow-
ing linguistic restrictions:
? Of all daughters of a phrase, only one is allowed
to be labelled HD (head).
?
?n,m??D
xm,HD ? 1 (7)
? If a noun phrase carries no case feature for nom-
inative case, it cannot be labelled SB, PD or EP.
case(n) 6= nom ??
?
l?{SB,PD,EP}
xn,l = 0
(8)
? If a noun phrase carries no case feature for ac-
cusative case, it cannot be labelled OA or OA2.
? If a noun phrase carries no case feature for da-
tive case, it cannot be labelled DA.
? If a noun phrase carries no case feature for gen-
itive case, it cannot be labelled OG or AG9.
7Note that some of these constraints are language specific
in that they represent linguistic facts about German and do
not necessarily hold for other languages. Furthermore, the
constraints are treebank specific to a certain degree in that
they use a TiGer-specific set of labels and are conditioned on
TiGer-specific configurations and categories.
8SB = subject, OA = accusative object, OA2 = sec-
ond accusative object, DA = dative, OG = genitive object,
OP = prepositional object, PD = predicate, OC = clausal ob-
ject, EP = expletive es
9AG = genitive adjunct
1090
Unlike Klenner (2007), we do not use prede-
fined subcategorization frames, instead letting the
statistical model choose arguments.
In TiGer, sentences whose main verbs are
formed from auxiliary-participle combinations,
are annotated by embedding the participle under
an extra VP node and non-subject arguments are
sisters to the participle. Therefore we add an ex-
tension of the constraint in (6) to the constraint set
in order to also include the daughters of an embed-
ded VP node in such a case.
Because of the particulars of the annotation
scheme of TiGer, we can decide some labels in
advance. As mentioned before, punctuation does
not get a label in TiGer. We set the label for those
nodes to ?? (no label). Other examples are:
? If a node?s category is PTKVZ (separated verb
particle), it is labeled SVP (separable verb par-
ticle).
cat(n) = PTKV Z ?? xn,SV P = 1 (9)
? If a node?s category is APPR, APPRART,
APPO or APZR (prepositions), it is labeled AC
(adpositional case marker).
? All daughters of an MTA node (multi-token
adjective) are labeled ADC (adjective compo-
nent).
These constraints are conditioned on part-of-
speech tags and require high POS-tagging accu-
racy (when dealing with raw text).
Due to the constraints imposed on the classifi-
cation, the function labeller can no longer assign
two subjects to the same S node. Faced with two
nodes whose most probable label is SB, it has to
decide on one of them taking the next best label for
the other. This way, it outputs the optimal solution
with respect to the set of constraints. Note that this
requires the feature model not only to rank the cor-
rect label highest but also to provide a reasonable
ranking of the other labels as well.
4 Evaluation
We conducted a number of experiments using
1,866 sentences of the TiGer Dependency Bank
(Forst et al, 2004) as our test set. The TiGerDB is
a part of the TiGer Treebank semi-automatically
converted into a dependency representation. We
use the manually labelled TiGer trees correspond-
ing to the sentences in the TiGerDB for assessing
the labelling quality in the intrinsic evaluation, and
the dependencies from TiGerDB for assessing the
quality and coverage of the automatically acquired
LFG resources in the extrinsic evaluation.
In order to test on real parser output, the test
set was parsed with the Berkeley Parser (Petrov et
al., 2006) trained on 48k sentences of the TiGer
corpus (Table 1), excluding the test set. Since the
Berkeley Parser assumes projective structures, the
training data and test data were made projective by
raising non-projective nodes in the tree (Ku?bler,
2005).
precision 83.60 recall 82.81
f-score 83.20 tagging acc. 97.97
Table 1: evalb unlabelled parsing scores on test set for Berke-
ley Parser trained on 48,000 sentences (sentence length? 40)
The maximum entropy classifier of the func-
tion labeller was trained on 46,473 sentences of
the TiGer Treebank (excluding the test set) which
yields about 1.2 million nodes as training samples.
For training the Maximum Entropy Model, we
used the BLMVM algorithm (Benson and More,
2001) with a width factor of 1.0 (Kazama and Tsu-
jii, 2005) implemented in an open-source C++ li-
brary from Tsujii Laboratory.10 The integer linear
program was solved with the simplex algorithm in
combination with a branch-and-bound method us-
ing the freely available GLPK.11
4.1 Intrinsic Evaluation
In the intrinsic evaluation, we measured the qual-
ity of the labelling itself. We used the node
span evaluation method of (Blaheta and Char-
niak, 2000) which takes only those nodes into ac-
count which have been recognised correctly by the
parser, i.e. if there are two nodes in the parse and
the reference treebank tree which cover the same
word span. Unlike Blaheta and Charniak (2000)
however, we do not require the two nodes to carry
the same syntactic category label.12
Table 2 shows the results of the node span eval-
uation. The labeller achieves close to 98% label
accuracy on gold treebank trees which shows that
the feature model captures the differences between
the individual labels well. Results on parser output
are about 4 percentage points (absolute) lower as
parsing errors can distort local context features for
the classifier even if the node itself has been parsed
10http://www-tsujii.is.s.u-tokyo.ac.jp/?tsuruoka/maxent/
11http://www.gnu.org/software/glpk/glpk.html
12We also excluded the root node, all punctuation marks
and both nodes in unary branching sub-trees from evaluation.
1091
correctly. The addition of the ILP constraints im-
proves results only slightly since the constraints
affect only (a small number of) argument labels
while the evaluation considers all 40 labels occur-
ring in the test set. Since the constraints restrict the
selection of certain labels, a less probable label has
to be picked by the labeller if the most probable
is not available. If the classifier is ranking labels
sensibly, the correct label should emerge. How-
ever, with an incorrect ranking, the ILP constraints
might also introduce new errors.
label accuracy error red.
without constraints
gold 44689/45691 = 97.81% ?
parser 40578/43140 = 94.06% ?
with constraints
gold 44773/45691 = 97.99%* 8.21%
parser 40593/43140 = 94.10% 0.68%
Table 2: label accuracy and error reduction (all labels) for
node span evaluation, * statistically significant, sign test, ? =
0.01 (Koo and Collins, 2005)
As the main target of the constraint set are argu-
ment functions, we also tested the quality of argu-
ment labels. Table 3 shows the node span evalua-
tion in terms of precision, recall and f-score for ar-
gument functions only, with clear statistically sig-
nificant improvements.
prec. rec. f-score
without constraints
gold standard 92.41 91.86 92.13
parser output 88.14 86.43 87.28
with constraints
gold standard 94.31 92.76 93.53*
parser output 89.51 86.73 88.09*
Table 3: node span results for the test set, argument functions
only (SB, EP, PD, OA, OA2, DA, OG, OP, OC), * statistically
significant, sign test, ? = 0.01 (Koo and Collins, 2005)
For comparison and to establish a highly com-
petitive baseline, we use the best-scoring system
in (Chrupa?a and Van Genabith, 2006), trained and
tested on exactly the same data sets. This purely
statistical labeller achieves accuracy of 96.44%
(gold) and 92.81% (parser) for all labels, and f-
scores of 89.88% (gold) and 84.98% (parser) for
argument labels. Tables 2 and 3 show that our sys-
tem (with and even without ILP constraints) com-
prehensively outperforms all corresponding base-
line scores.
The node span evaluation defines a correct la-
belling by taking only those nodes (in parser out-
put) into account that have a corresponding node
in the reference tree. However, as this restricts at-
tention to correctly parsed nodes, the results are
somewhat over-optimistic. Table 4 provides the
results obtained from an evalb evaluation of the
same data sets.13 The gold standard scores are
high confirming our previous findings about the
performance of the function labeller. However,
the results on parser output are much worse. The
evaluation scores are now taking the parsing qual-
ity into account (Table 1). The considerable drop
in quality between gold trees and parser output
clearly shows that a good parse tree is an impor-
tant prerequisite for reasonable function labelling.
This is in accordance with previous findings by
Punyakanok et al (2008) who emphasise the im-
portance of syntactic parsing for the closely re-
lated task of semantic role labelling.
prec. rec. f-score
without constraints
gold standard 95.94 95.94 95.94
parser output 76.27 75.55 75.91
with constraints
gold standard 96.21 96.21 96.21
parser output 76.36 75.64 76.00
Table 4: evalb results for the test set
4.1.1 Subcategorisation Frames
Early on in the paper we mention that, unlike e. g.
Klenner (2007), we did not include predefined
subcategorisation frames into the constraint set,
but rather let the joint statistical and ILP models
decide on the correct type of arguments assigned
to a verb. The assumption is that if one uses prede-
fined subcategorisation frames which fix the num-
ber and type of arguments for a verb, one runs the
risk of excluding correct labellings due to missing
subcat frames, unless a very comprehensive and
high quality subcat lexicon resource is available.
In order to test this assumption, we run an addi-
tional experiment with about 10,000 verb frames
for 4,508 verbs, which were automatically ex-
tracted from our training section. Following Klen-
ner (2007), for each verb and for each subcat frame
for this verb attested at least once in the training
data, we introduce a new binary variable fn to
the ILP model representing the n-th frame (for the
verb) weighted by its frequency.
We add an ILP constraint requiring exactly one
of the frames to be set to one (each verb has to have
a subcat frame) and replace the ILP constraint in
(6) by:
13Function labels were merged with the category symbols.
1092
??n,m??D
xm,SB ?
?
SB?fi
fi = 0 (10)
This constraint requires the number of subjects
in a phrase to be equal to the number of selected14
verb frames that require a subject. As each verb
is constrained to ?select? exactly one subcat frame
(see additional ILP constraint above), there is at
most one subject per phrase, if the frame in ques-
tion requires a subject. If the selected frame does
not require a subject, then the constraint blocks the
assignment of subjects for the entire phrase. The
same was done for the other argument functions
and as before we included an extension of this con-
straint to cover embedded VPs. For unseen verbs
(i.e. verbs not attested in the training set) we keep
the original constraints as a back-off.
prec. rec. f-score
all labels (cmp. Table 2)
gold standard 97.24 97.24 97.24
parser output 93.43 93.43 93.43
argument functions only (cmp. Table 3)
gold standard 91.36 90.12 90.74
parser output 86.64 84.38 85.49
Table 5: node span results for the test set using constraints
with automatically extracted subcat frames
Table 5 shows the results of the test set node
span evaluation when using the ILP system en-
hanced with subcat frames. Compared to Tables 2
and 3, the results are clearly inferior, and particu-
larly so for argument grammatical functions. This
seems to confirm our assumption that, given our
data, letting the joint statistical and ILP model de-
cide argument functions is superior to an approach
that involves subcat frames. However, and impor-
tantly, our results do not rule out that a more com-
prehensive subcat frame resource may in fact re-
sult in improvements.
4.2 Extrinsic Evaluation
Over the last number of years, treebank-based
deep grammar acquisition has emerged as an
attractive alternative to hand-crafting resources
within the HPSG, CCG and LFG paradigms
(Miyao et al, 2003; Clark and Hockenmaier,
2002; Cahill et al, 2004). While most of the ini-
tial development work focussed on English, more
recently efforts have branched to other languages.
Below we concentrate on LFG.
14The variable representing this frame has been set to 1.
Lexical-Functional Grammar (Bresnan, 2001)
is a constraint-based theory of grammar with min-
imally two levels of representation: c(onstituent)-
structure and f(unctional)-structure. C-structure
(CFG trees) captures language specific surface
configurations such as word order and the hier-
archical grouping of words into phrases, while
f-structure represents more abstract (and some-
what more language independent) grammatical re-
lations (essentially bilexical labelled dependencies
with some morphological and semantic informa-
tion, approximating to basic predicate-argument
structures) in the form of attribute-value struc-
tures. F-structures are defined in terms of equa-
tions annotated to nodes in c-structure trees (gram-
mar rules). Treebank-based LFG acquisition was
originally developed for English (Cahill, 2004;
Cahill et al, 2008) and is based on an f-structure
annotation algorithm that annotates c-structure
trees (from a treebank or parser output) with
f-structure equations, which are read off of the tree
and passed on to a constraint solver producing an
f-structure for the given sentence. The English
annotation algorithm (for Penn-II treebank-style
trees) relies heavily on configurational and catego-
rial information, translating this into grammatical
functional information (subject, object etc.) rep-
resented at f-structure. LFG is ?functional? in the
mathematical sense, in that argument grammatical
functions have to be single valued (there cannot be
two or more subjects etc. in the same clause). In
fact, if two or more values are assigned to a single
argument grammatical function in a local tree, the
LFG constraint solver will produce a clash (i. e.
it will fail to produce an f-structure) and the sen-
tence will be considered ungrammatical (in other
words, the corresponding c-structure tree will be
uninterpretable).
Rehbein (2009) and Rehbein and van Genabith
(2009) develop an f-structure annotation algorithm
for German based on the TiGer treebank resource.
Unlike the English annotation algorithm and be-
cause of the language-particular properties of Ger-
man (see Section 2), the German annotation al-
gorithm cannot rely on c-structure configurational
information, but instead heavily uses TiGer func-
tion labels in the treebank. Learning function la-
bels is therefore crucial to the German LFG an-
notation algorithm, in particular when parsing raw
text. Because of the strong case syncretism in Ger-
man, traditional classification models using local
1093
information only run the risk of predicting mul-
tiple occurences of the same function (subject,
object etc.) at the same level, causing feature
clashes in the constraint solver with no f-structure
being produced. Rehbein (2009) and Rehbein
and van Genabith (2009) identify this as a major
problem resulting in a considerable loss in cov-
erage of the German annotation algorithm com-
pared to English, in particular for parsing raw text,
where TiGer function labels have to be supplied by
a machine-learning-based method and where the
coverage of the LFG annotation algorithm drops
to 93.62% with corresponding drops in recall and
f-scores for the f-structure evaluations (Table 6).
Below we test whether the coverage problems
caused by incorrect multiple assignments of gram-
matical functions can be addressed using the com-
bination of classifier with ILP constraints devel-
oped in this paper. We report experiments where
automatically parsed and labelled data are handed
over to an LFG f-structure computation algorithm.
The f-structures produced are converted into a
dependency triple representation (Crouch et al,
2002) and evaluated against TiGerDB.
cov. prec. rec. f-score
upper bound 99.14 85.63 82.58 84.07
without constraints
gold 95.82 84.71 76.68 80.49
parser 93.41 79.70 70.38 74.75
with constraints
gold 99.30 84.62 82.15 83.37
parser 98.39 79.43 75.60 77.47
Rehbein 2009
parser 93.62 79.20 68.86 73.67
Table 6: f-structure evaluation results for the test set against
TigerDB
Table 6 shows the results of the f-structure
evaluation against TiGerDB, with 84.07% f-score
upper-bound results for the f-structure annotation
algorithm on the original TiGer treebank trees
with hand-annotated function labels. Using the
function labeller without ILP constraints results in
drastic drops in coverage (between 4.5% and 6.5%
points absolute) and hence recall (6% and 12%)
and f-score (3.5% and 9.5%) for both gold trees
and parser output (compared to upper bounds).
By contrast, with ILP constraints, the loss in cov-
erage observed above almost completely disap-
pears and recall and f-scores improve by between
4.4% and 5.5% (recall) and 3% (f-score) abso-
lute (over without ILP constraints). For compar-
ison, we repeated the experiment using the best-
scoring method of Rehbein (2009). Rehbein trains
the Berkeley Parser to learn an extended category
set, merging TiGer function labels with syntactic
categories, where the parser outputs fully-labelled
trees. The results show that this approach suf-
fers from the same drop in coverage as the classi-
fier without ILP constraints, with recall about 7%
and f-score about 4% (absolute) lower than for the
classifier with ILP constraints.
Table 7 shows the dramatic effect of the ILP
constraints on the number of sentences in the test
set that have multiple argument functions of the
same type within the same clause. With ILP con-
straints, the problem disappears and therefore, less
feature-clashes occur during f-structure computa-
tion.
no constraints constraints
gold 185 0
parser 212 0
Table 7: Number of sentences in the test set with doubly an-
notated argument functions
In order to assess whether ILP constraints help
with coverage only or whether they affect the qual-
ity of the f-structures as well, we repeat the experi-
ment in Table 6, however this time evaluating only
on those sentences that receive an f-structure, ig-
noring the rest. Table 8 shows that the impact of
ILP constraints on quality is much less dramatic
than on coverage, with only very small variations
in precison, recall and f-scores across the board,
and small increases over Rehbein (2009).
cov. prec. rec. f-score
no constr. 93.41 79.70 77.89 78.79
constraints 98.39 79.43 77.85 78.64
Rehbein 93.62 79.20 76.43 77.79
Table 8: f-structure evaluation results for parser output ex-
cluding sentences without f-structures
Early work on automatic LFG acquisition and
parsing for German is presented in Cahill et al
(2003) and Cahill (2004), adapting the English
Annotation Algorithm to an earlier and smaller
version of the TiGer treebank (without morpho-
logical information) and training a parser to learn
merged Tiger function-category labels, and report-
ing 95.75% coverage and an f-score of 74.56%
f-structure quality against 2,000 gold treebank
trees automatically converted into f-structures.
Rehbein (2009) uses the larger Release 2 of the
treebank (with morphological information) report-
ing 77.79% f-score and coverage of 93.62% (Ta-
1094
ble 8) against the dependencies in the TiGerDB
test set. The only rule-based approach to German
LFG-parsing we are aware of is the hand-crafted
German grammar in the ParGram Project (Butt
et al, 2002). Forst (2007) reports 83.01% de-
pendency f-score evaluated against a set of 1,497
sentences of the TiGerDB. It is very difficult to
compare results across the board, as individual pa-
pers use (i) different versions of the treebank, (ii)
different (sections of) gold-standards to evaluate
against (gold TiGer trees in TigerDB, the depen-
dency representations provided by TigerDB, auto-
matically generated gold-standards etc.) and (iii)
different label/grammatical function sets. Further-
more, (iv) coverage differs drastically (with the
hand-crafted LFG resources achieving about 80%
full f-structures) and finally, (v) some of the gram-
mars evaluated having been used in the generation
of the gold standards, possibly introducing a bias
towards these resources: the German hand-crafted
LFG was used to produce TiGerDB (Forst et al,
2004). In order to put the results into some per-
spective, Table 9 shows an evaluation of our re-
sources against a set of automatically generated
gold standard f-structures produced by using the
f-structure annotation algorithm on the original
hand-labelled TiGer gold trees in the section cor-
responding to TiGerDB: without ILP constraints
we achieve a dependency f-score of 84.35%, with
ILP constraints 87.23% and 98.89% coverage.
cov. prec. rec. f-score
without constraints
gold 95.24 97.76 90.93 94.22
parser 93.35 88.71 80.40 84.35
with constraints
gold 99.30 97.66 97.33 97.50
parser 98.89 88.37 86.12 87.23
Table 9: f-structure evaluation results for the test set against
automatically generated goldstandard (1,850 sentences)
5 Conclusion
In this paper, we addressed the problem of assign-
ing grammatical functions to constituent struc-
tures. We have proposed an approach to grammat-
ical function labelling that combines the flexibil-
ity of a statistical classifier with linguistic expert
knowledge in the form of hard constraints imple-
mented by an integer linear program. These con-
straints restrict the solution space of the classifier
by blocking those solutions that cannot be correct.
One of the strengths of an integer linear program
is the unlimited context it can take into account
by optimising over the entire structure, providing
an elegant way of supporting classifiers with ex-
plicit linguistic knowledge while at the same time
keeping feature models small and comprehensi-
ble. Most of the constraints are direct formaliza-
tions of linguistic generalizations for German. Our
approach should generalise to other languages for
which linguistic expertise is available.
We evaluated our system on the TiGer corpus
and the TiGerDB and gave results on gold stan-
dard trees and parser output. We also applied
the German f-structure annotation algorithm to
the automatically labelled data and evaluated the
system by measuring the quality of the resulting
f-structures. We found that by using the con-
straint set, the function labeller ensures the inter-
pretability and thus the usefulness of the syntac-
tic structure for a subsequently applied processing
step. In our f-structure evaluation, that means, the
f-structure computation algorithm is able to pro-
duce an f-structure for almost all sentences.
Acknowledgements
The first author would like to thank Gerlof Bouma
for a lot of very helpful discussions. We would
like to thank our anonymous reviewers for de-
tailed and helpful comments. The research was
supported by the Science Foundation Ireland SFI
(Grant 07/CE/I1142) as part of the Centre for
Next Generation Localisation (www.cngl.ie) and
by DFG (German Research Foundation) through
SFB 632 Potsdam-Berlin and SFB 732 Stuttgart.
References
Steven J. Benson and Jorge J. More. 2001. A limited
memory variable metric method in subspaces and
bound constrained optimization problems. Techni-
cal report, Argonne National Laboratory.
Adam L. Berger, Vincent J.D. Pietra, and Stephen A.D.
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational linguis-
tics, 22(1):71.
Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In Proceedings of the
1st North American chapter of the Association for
Computational Linguistics conference, pages 234 ?
240, Seattle, Washington. Morgan Kaufmann Pub-
lishers Inc.
Thorsten Brants, Wojciech Skut, and Brigitte Krenn.
1997. Tagging grammatical functions. In Proceed-
ings of EMNLP, volume 97, pages 64?74.
1095
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories, page 2441.
Joan Bresnan. 2001. Lexical-Functional Syntax.
Blackwell Publishers.
Miriam Butt, Helge Dyvik, Tracy Halloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
parallel grammar project. In COLING-02 on Gram-
mar engineering and evaluation-Volume 15, volume
pages, page 7. Association for Computational Lin-
guistics.
Aoife Cahill, Martin Forst, Mairead McCarthy, Ruth
ODonovan, Christian Rohrer, Josef van Genabith,
and Andy Way. 2003. Treebank-based multilingual
unification-grammar development. In Proceedings
of the Workshop on Ideas and Strategies for Multi-
lingual Grammar Development at the 15th ESSLLI,
page 1724.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef
van Genabith, and Andy Way. 2004. Long-
distance dependency resolution in automatically ac-
quired wide-coverage PCFG-based LFG approxima-
tions. Proceedings of the 42nd Annual Meeting
on Association for Computational Linguistics - ACL
?04, pages 319?es.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan
Riezler, Josef van Genabith, and Andy Way. 2008.
Wide-Coverage Deep Statistical Parsing Using Au-
tomatic Dependency Structure Annotation. Compu-
tational Linguistics, 34(1):81?124, Ma?rz.
Aoife Cahill. 2004. Parsing with Automatically Ac-
quired, Wide-Coverage, Robust, Probabilistic LFG
Approximations. Ph.D. thesis, Dublin City Univer-
sity.
Grzegorz Chrupa?a and Josef Van Genabith. 2006.
Using machine-learning to assign function labels
to parser output for Spanish. In Proceedings of
the COLING/ACL main conference poster session,
page 136143, Sydney. Association for Computa-
tional Linguistics.
Stephen Clark and Judith Hockenmaier. 2002. Evalu-
ating a wide-coverage CCG parser. In Proceedings
of the LREC 2002, pages 60?66.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression an integer linear
programming approach. Journal of Artificial Intelli-
gence Research, 31:399?429.
Richard Crouch, Ronald M. Kaplan, Tracy Halloway
King, and Stefan Riezler. 2002. A comparison of
evaluation metrics for a broad-coverage stochastic
parser. In Proceedings of LREC 2002 Workshop,
pages 67?74, Las Palmas, Canary Islands, Spain.
Peter Eisenberg. 2006. Grundriss der deutschen
Grammatik: Das Wort. J.B. Metzler, Stuttgart, 3
edition.
Martin Forst, Nu?ria Bertomeu, Berthold Crysmann,
Frederik Fouvry, Silvia Hansen-Shirra, and Valia
Kordoni. 2004. Towards a dependency-based gold
standard for German parsers The TiGer Dependency
Bank. In Proceedings of the COLING Workshop
on Linguistically Interpreted Corpora (LINC ?04),
Geneva, Switzerland.
Martin Forst. 2007. Filling Statistics with Linguistics
Property Design for the Disambiguation of German
LFG Parses. In Proceedings of ACL 2007. Associa-
tion for Computational Linguistics.
Jun?Ichi Kazama and Jun?Ichi Tsujii. 2005. Maxi-
mum entropy models with inequality constraints: A
case study on text categorization. Machine Learn-
ing, 60(1):159194.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL
2003, pages 423?430, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Manfred Klenner. 2005. Extracting Predicate Struc-
tures from Parse Trees. In Proceedings of the
RANLP 2005.
Manfred Klenner. 2007. Shallow dependency label-
ing. In Proceedings of the ACL 2007 Demo and
Poster Sessions, page 201204, Prague. Association
for Computational Linguistics.
Terry Koo and Michael Collins. 2005. Hidden-
variable models for discriminative reranking. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing - HLT ?05, pages 507?514, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Sandra Ku?bler. 2005. How Do Treebank Annotation
Schemes Influence Parsing Results? Or How Not to
Compare Apples And Oranges. In Proceedings of
RANLP 2005, Borovets, Bulgaria.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33rd an-
nual meeting on Association for Computational Lin-
guistics, page 276283, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics Morristown,
NJ, USA.
Andre? F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proceedings of
ACL 2009.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL, volume 6.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.
2003. Probabilistic modeling of argument structures
including non-local dependencies. In Proceedings
of the Conference on Recent Advances in Natural
Language Processing RANLP 2003, volume 2.
1096
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135, Januar.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the ACL
- ACL ?06, pages 433?440, Morristown, NJ, USA.
Association for Computational Linguistics.
Vasin Punyakanok, Wen-Tau Yih, Dan Roth, and Dav
Zimak. 2004. Semantic role labeling via integer
linear programming inference. In Proceedings of
the 20th international conference on Computational
Linguistics - COLING ?04, Morristown, NJ, USA.
Association for Computational Linguistics.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The Importance of Syntactic Parsing and Inference
in Semantic Role Labeling. Computational Linguis-
tics, 34(2):257?287, Juni.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution. Ph.D.
thesis, University of Pennsylvania.
Ines Rehbein and Josef van Genabith. 2009. Auto-
matic Acquisition of LFG Resources for German-
As Good as it gets. In Miriam Butt and Tracy Hol-
loway King, editors, Proceedings of LFG Confer-
ence 2009. CSLI Publications.
Ines Rehbein. 2009. Treebank-based grammar acqui-
sition for German. Ph.D. thesis, Dublin City Uni-
versity.
Dan Roth and Wen-Tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of CoNNL 2004.
Anne Schiller, Simone Teufel, and Christine Sto?ckert.
1999. Guidelines fu?r das Tagging deutscher
Textcorpora mit STTS (Kleines und gro?es Tagset).
Technical Report August, Universita?t Stuttgart.
Anne Schiller. 1994. Dmor - user?s guide. Technical
report, University of Stuttgart.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, volume 12. Manchester, UK.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
realizational parsing. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics - COLING ?08, pages 889?896, Morristown, NJ,
USA. Association for Computational Linguistics.
1097
Tutorial Abstracts of ACL 2010, page 1,
Uppsala, Sweden, 11 July 2010. c?2010 Association for Computational Linguistics
Wide-coverage NLP with Linguistically Expressive Grammars
Julia Hockenmaier
Department of Computer Science,
University of Illinois
juliahmr@illinois.edu
Yusuke Miyao
National Institute of Informatics
yusuke@nii.ac.jp
Josef van Genabith
Centre for Next Generation Localisation,
School of Computing,
Dublin City University
josef@computing.dcu.ie
1 Introduction
In recent years, there has been a lot of research
on wide-coverage statistical natural language
processing with linguistically expressive gram-
mars such as Combinatory Categorial Grammars
(CCG), Head-driven Phrase-Structure Grammars
(HPSG), Lexical-Functional Grammars (LFG)
and Tree-Adjoining Grammars (TAG). But al-
though many young researchers in natural lan-
guage processing are very well trained in machine
learning and statistical methods, they often lack
the necessary background to understand the lin-
guistic motivation behind these formalisms. Fur-
thermore, in many linguistics departments, syntax
is still taught from a purely Chomskian perspec-
tive. Additionally, research on these formalisms
often takes place within tightly-knit, formalism-
specific subcommunities. It is therefore often dif-
ficult for outsiders as well as experts to grasp the
commonalities of and differences between these
formalisms.
2 Content Overview
This tutorial overviews basic ideas of TAG/
CCG/LFG/HPSG, and provides attendees with a
comparison of these formalisms from a linguis-
tic and computational point of view. We start
from stating the motivation behind using these ex-
pressive grammar formalisms for NLP, contrast-
ing them with shallow formalisms like context-
free grammars. We introduce a common set of
examples illustrating various linguistic construc-
tions that elude context-free grammars, and reuse
them when introducing each formalism: bounded
and unbounded non-local dependencies that arise
through extraction and coordination, scrambling,
mappings to meaning representations, etc. In the
second half of the tutorial, we explain two key
technologies for wide-coverage NLP with these
grammar formalisms: grammar acquisition and
parsing models. Finally, we show NLP applica-
tions where these expressive grammar formalisms
provide additional benefits.
3 Tutorial Outline
1. Introduction: Why expressive grammars
2. Introduction to TAG
3. Introduction to CCG
4. Introduction to LFG
5. Introduction to HPSG
6. Inducing expressive grammars from corpora
7. Wide-coverage parsing with expressive
grammars
8. Applications
9. Summary
References
Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan
Riezler, Josef van Genabith and Andy Way. 2008.
Wide-Coverage Deep Statistical Parsing using Au-
tomatic Dependency Structure Annotation. Compu-
tational Linguistics, 34(1). pp.81-124, MIT Press.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature For-
est Models for Probabilistic HPSG Parsing. Compu-
tational Linguistics, 34(1). pp.35-80, MIT Press.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33(3). pp.355-396, MIT
Press.
1
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1239?1248,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Consistent Translation using Discriminative Learning:
A Translation Memory-inspired Approach?
Yanjun Ma? Yifan He? Andy Way? Josef van Genabith?
? Baidu Inc., Beijing, China
yma@baidu.com
?Centre for Next Generation Localisation
School of Computing, Dublin City University
{yhe,away,josef}@computing.dcu.ie
Abstract
We present a discriminative learning method
to improve the consistency of translations in
phrase-based Statistical Machine Translation
(SMT) systems. Our method is inspired by
Translation Memory (TM) systems which are
widely used by human translators in industrial
settings. We constrain the translation of an in-
put sentence using the most similar ?transla-
tion example? retrieved from the TM. Differ-
ently from previous research which used sim-
ple fuzzy match thresholds, these constraints
are imposed using discriminative learning to
optimise the translation performance. We ob-
serve that using this method can benefit the
SMT system by not only producing consis-
tent translations, but also improved translation
outputs. We report a 0.9 point improvement
in terms of BLEU score on English?Chinese
technical documents.
1 Introduction
Translation consistency is an important factor
for large-scale translation, especially for domain-
specific translations in an industrial environment.
For example, in the translation of technical docu-
ments, lexical as well as structural consistency is es-
sential to produce a fluent target-language sentence.
Moreover, even in the case of translation errors, con-
sistency in the errors (e.g. repetitive error patterns)
are easier to diagnose and subsequently correct by
translators.
?This work was done while the first author was in the Cen-
tre for Next Generation Localisation at Dublin City University.
In phrase-based SMT, translation models and lan-
guage models are automatically learned and/or gen-
eralised from the training data, and a translation is
produced by maximising a weighted combination of
these models. Given that global contextual informa-
tion is not normally incorporated, and that training
data is usually noisy in nature, there is no guaran-
tee that an SMT system can produce translations in
a consistent manner.
On the other hand, TM systems ? widely used by
translators in industrial environments for enterprise
localisation by translators ? can shed some light on
mitigating this limitation. TM systems can assist
translators by retrieving and displaying previously
translated similar ?example? sentences (displayed as
source-target pairs, widely called ?fuzzy matches? in
the localisation industry (Sikes, 2007)). In TM sys-
tems, fuzzy matches are retrieved by calculating the
similarity or the so-called ?fuzzy match score? (rang-
ing from 0 to 1 with 0 indicating no matches and 1
indicating a full match) between the input sentence
and sentences in the source side of the translation
memory.
When presented with fuzzy matches, translators
can then avail of useful chunks in previous transla-
tions while composing the translation of a new sen-
tence. Most translators only consider a few sen-
tences that are most similar to the current input sen-
tence; this process can inherently improve the con-
sistency of translation, given that the new transla-
tions produced by translators are likely to be similar
to the target side of the fuzzy match they have con-
sulted.
Previous research as discussed in detail in Sec-
1239
tion 2 has focused on using fuzzy match score as
a threshold when using the target side of the fuzzy
matches to constrain the translation of the input
sentence. In our approach, we use a more fine-
grained discriminative learning method to determine
whether the target side of the fuzzy matches should
be used as a constraint in translating the input sen-
tence. We demonstrate that our method can consis-
tently improve translation quality.
The rest of the paper is organized as follows:
we begin by briefly introducing related research in
Section 2. We present our discriminative learning
method for consistent translation in Section 3 and
our feature design in Section 4. We report the exper-
imental results in Section 5 and conclude the paper
and point out avenues for future research in Section
6.
2 Related Research
Despite the fact that TM and MT integration has
long existed as a major challenge in the localisation
industry, it has only recently received attention in
main-stream MT research. One can loosely combine
TM and MT at sentence (called segments in TMs)
level by choosing one of them (or both) to recom-
mend to the translators using automatic classifiers
(He et al, 2010), or simply using fuzzy match score
or MT confidence measures (Specia et al, 2009).
One can also tightly integrate TM with MT at the
sub-sentence level. The basic idea is as follows:
given a source sentence to translate, we firstly use
a TM system to retrieve the most similar ?example?
source sentences together with their translations. If
matched chunks between input sentence and fuzzy
matches can be detected, we can directly re-use the
corresponding parts of the translation in the fuzzy
matches, and use an MT system to translate the re-
maining chunks.
As a matter of fact, implementing this idea is
pretty straightforward: a TM system can easily de-
tect the word alignment between the input sentence
and the source side of the fuzzy match by retracing
the paths used in calculating the fuzzy match score.
To obtain the translation for the matched chunks, we
just require the word alignment between source and
target TM matches, which can be addressed using
state-of-the-art word alignment techniques. More
importantly, albeit not explicitly spelled out in pre-
vious work, this method can potentially increase the
consistency of translation, as the translation of new
input sentences is closely informed and guided (or
constrained) by previously translated sentences.
There are several different ways of using the
translation information derived from fuzzy matches,
with the following two being the most widely
adopted: 1) to add these translations into a phrase
table as in (Bic?ici and Dymetman, 2008; Simard and
Isabelle, 2009), or 2) to mark up the input sentence
using the relevant chunk translations in the fuzzy
match, and to use an MT system to translate the parts
that are not marked up, as in (Smith and Clark, 2009;
Koehn and Senellart, 2010; Zhechev and van Gen-
abith, 2010). It is worth mentioning that translation
consistency was not explicitly regarded as their pri-
mary motivation in this previous work. Our research
follows the direction of the second strand given that
consistency can no longer be guaranteed by con-
structing another phrase table.
However, to categorically reuse the translations
of matched chunks without any differentiation could
generate inferior translations given the fact that the
context of these matched chunks in the input sen-
tence could be completely different from the source
side of the fuzzy match. To address this problem,
both (Koehn and Senellart, 2010) and (Zhechev and
van Genabith, 2010) used fuzzy match score as a
threshold to determine whether to reuse the transla-
tions of the matched chunks. For example, (Koehn
and Senellart, 2010) showed that reusing these trans-
lations as large rules in a hierarchical system (Chi-
ang, 2005) can be beneficial when the fuzzy match
score is above 70%, while (Zhechev and van Gen-
abith, 2010) reported that it is only beneficial to a
phrase-based system when the fuzzy match score is
above 90%.
Despite being an informative measure, using
fuzzy match score as a threshold has a number of
limitations. Given the fact that fuzzy match score
is normally calculated based on Edit Distance (Lev-
enshtein, 1966), a low score does not necessarily
imply that the fuzzy match is harmful when used
to constrain an input sentence. For example, in
longer sentences where fuzzy match scores tend to
be low, some chunks and the corresponding trans-
lations within the sentences can still be useful. On
1240
the other hand, a high score cannot fully guarantee
the usefulness of a particular translation. We address
this problem using discriminative learning.
3 Constrained Translation with
Discriminative Learning
3.1 Formulation of the Problem
Given a sentence e to translate, we retrieve the most
similar sentence e? from the translation memory as-
sociated with target translation f ?. The m com-
mon ?phrases? e?m1 between e and e? can be iden-
tified. Given the word alignment information be-
tween e? and f ?, one can easily obtain the corre-
sponding translations f? ?m1 for each of the phrases in
e?m1 . This process can derive a number of ?phrase
pairs? < e?m, f? ?m >, which can be used to specify
the translations of the matched phrases in the input
sentence. The remaining words without specified
translations will be translated by an MT system.
For example, given an input sentence e1e2 ? ? ?
eiei+1 ? ? ? eI , and a phrase pair < e?, f? ? >, e? =
eiei+1, f? ? = f ?jf
?
j+1 derived from the fuzzy match,
we can mark up the input sentence as:
e1e2 ? ? ? <tm=?f ?jf ?j+1?> eiei+1 < /tm> ? ? ? eI .
Our method to constrain the translations using
TM fuzzy matches is similar to (Koehn and Senel-
lart, 2010), except that the word alignment between
e? and f ? is the intersection of bidirectional GIZA++
(Och and Ney, 2003) posterior alignments. We use
the intersected word alignment to minimise the noise
introduced by word alignment of only one direction
in marking up the input sentence.
3.2 Discriminative Learning
Whether the translation information from the fuzzy
matches should be used or not (i.e. whether the input
sentence should be marked up) is determined using
a discriminative learning procedure. The translation
information refers to the ?phrase pairs? derived us-
ing the method described in Section 3.1. We cast
this problem as a binary classification problem.
3.2.1 Support Vector Machines
SVMs (Cortes and Vapnik, 1995) are binary classi-
fiers that classify an input instance based on decision
rules which minimise the regularised error function
in (1):
min
w,b,?
1
2
wT w + C
l
?
i=1
?i
s. t. yi(wT?(xi) + b) > 1? ?i
?i > 0
(1)
where (xi, yi) ? Rn ? {+1,?1} are l training in-
stances that are mapped by the function ? to a higher
dimensional space. w is the weight vector, ? is the
relaxation variable and C > 0 is the penalty param-
eter.
Solving SVMs is viable using a kernel function
K in (1) with K(xi, xj) = ?(xi)T?(xj). We per-
form our experiments with the Radial Basis Func-
tion (RBF) kernel, as in (2):
K(xi, xj) = exp(??||xi ? xj ||2), ? > 0 (2)
When using SVMs with the RBF kernel, we have
two free parameters to tune on: the cost parameter
C in (1) and the radius parameter ? in (2).
In each of our experimental settings, the param-
eters C and ? are optimised by a brute-force grid
search. The classification result of each set of pa-
rameters is evaluated by cross validation on the
training set.
The SVM classifier will thus be able to predict
the usefulness of the TM fuzzy match, and deter-
mine whether the input sentence should be marked
up using relevant phrase pairs derived from the fuzzy
match before sending it to the SMT system for trans-
lation. The classifier uses features such as the fuzzy
match score, the phrase and lexical translation prob-
abilities of these relevant phrase pairs, and addi-
tional syntactic dependency features. Ideally the
classifier will decide to mark up the input sentence
if the translations of the marked phrases are accurate
when taken contextual information into account. As
large-scale manually annotated data is not available
for this task, we use automatic TER scores (Snover
et al, 2006) as the measure for training data annota-
tion.
We label the training examples as in (3):
y =
{
+1 if TER(w. markup) < TER(w/o markup)
?1 if TER(w/o markup) ? TER(w. markup)
(3)
Each instance is associated with a set of features
which are discussed in more detail in Section 4.
1241
3.2.2 Classification Confidence Estimation
We use the techniques proposed by (Platt, 1999) and
improved by (Lin et al, 2007) to convert classifica-
tion margin to posterior probability, so that we can
easily threshold our classifier (cf. Section 5.4.2).
Platt?s method estimates the posterior probability
with a sigmoid function, as in (4):
Pr(y = 1|x) ? PA,B(f) ?
1
1 + exp(Af + B)
(4)
where f = f(x) is the decision function of the esti-
mated SVM. A and B are parameters that minimise
the cross-entropy error function F on the training
data, as in (5):
min
z=(A,B)
F (z) = ?
l
?
i=1
(tilog(pi) + (1 ? ti)log(1? pi)),
where pi = PA,B(fi), and ti =
{
N++1
N++2 if yi = +1
1
N?+2 if yi = ?1
(5)
where z = (A,B) is a parameter setting, and
N+ and N? are the numbers of observed positive
and negative examples, respectively, for the label yi.
These numbers are obtained using an internal cross-
validation on the training set.
4 Feature Set
The features used to train the discriminative classi-
fier, all on the sentence level, are described in the
following sections.
4.1 The TM Feature
The TM feature is the fuzzy match score, which in-
dicates the overall similarity between the input sen-
tence and the source side of the TM output. If the
input sentence is similar to the source side of the
matching segment, it is more likely that the match-
ing segment can be used to mark up the input sen-
tence.
The calculation of the fuzzy match score itself is
one of the core technologies in TM systems, and
varies among different vendors. We compute fuzzy
match cost as the minimum Edit Distance (Leven-
shtein, 1966) between the source and TM entry, nor-
malised by the length of the source as in (6), as
most of the current implementations are based on
edit distance while allowing some additional flexi-
ble matching.
hfm(e) = min
s
EditDistance(e, s)
Len(e)
(6)
where e is the sentence to translate, and s is the
source side of an entry in the TM. For fuzzy match
scores F , hfm roughly corresponds to 1? F .
4.2 Translation Features
We use four features related to translation probabil-
ities, i.e. the phrase translation and lexical probabil-
ities for the phrase pairs < e?m, f? ?m > derived us-
ing the method in Section 3.1. Specifically, we use
the phrase translation probabilities p(f? ?m|e?m) and
p(e?m|f? ?m), as well as the lexical translation prob-
abilities plex(f? ?m|e?m) and plex(e?m|f? ?m) as calcu-
lated in (Koehn et al, 2003). In cases where mul-
tiple phrase pairs are used to mark up one single
input sentence e, we use a unified score for each
of the four features, which is an average over the
corresponding feature in each phrase pair. The intu-
ition behind these features is as follows: phrase pairs
< e?m, f? ?m > derived from the fuzzy match should
also be reliable with respect to statistically produced
models.
We also have a count feature, i.e. the number of
phrases used to mark up the input sentence, and a
binary feature, i.e. whether the phrase table contains
at least one phrase pair < e?m, f? ?m > that is used to
mark up the input sentence.
4.3 Dependency Features
Given the phrase pairs < e?m, f? ?m > derived from
the fuzzy match, and used to translate the corre-
sponding chunks of the input sentence (cf. Sec-
tion 3.1), these translations are more likely to be co-
herent in the context of the particular input sentence
if the matched parts on the input side are syntacti-
cally and semantically related.
For matched phrases e?m between the input sen-
tence and the source side of the fuzzy match, we de-
fine the contextual information of the input side us-
ing dependency relations between words em in e?m
and the remaining words ej in the input sentence e.
We use the Stanford parser to obtain the depen-
dency structure of the input sentence. We add
a pseudo-label SYS PUNCT to punctuation marks,
whose governor and dependent are both the punc-
tuation mark. The dependency features designed to
capture the context of the matched input phrases e?m
are as follows:
1242
Coverage features measure the coverage of de-
pendency labels on the input sentence in order to
obtain a bigger picture of the matched parts in the
input. For each dependency label L, we consider its
head or modifier as covered if the corresponding in-
put word em is covered by a matched phrase e?m.
Our coverage features are the frequencies of gov-
ernor and dependent coverage calculated separately
for each dependency label.
Position features identify whether the head and
the tail of a sentence are matched, as these are the
cases in which the matched translation is not af-
fected by the preceding words (when it is the head)
or following words (when it is the tail), and is there-
fore more reliable. The feature is set to 1 if this hap-
pens, and to 0 otherwise. We distinguish among the
possible dependency labels, the head or the tail of
the sentence, and whether the aligned word is the
governor or the dependent. As a result, each per-
mutation of these possibilities constitutes a distinct
binary feature.
The consistency feature is a single feature which
determines whether matched phrases e?m belong to
a consistent dependency structure, instead of being
distributed discontinuously around in the input sen-
tence. We assume that a consistent structure is less
influenced by its surrounding context. We set this
feature to 1 if every word in e?m is dependent on an-
other word in e?m, and to 0 otherwise.
5 Experiments
5.1 Experimental Setup
Our data set is an English?Chinese translation mem-
ory with technical translation from Symantec, con-
sisting of 87K sentence pairs. The average sentence
length of the English training set is 13.3 words and
the size of the training set is comparable to the larger
TMs used in the industry. Detailed corpus statistics
about the training, development and test sets for the
SMT system are shown in Table 1.
The composition of test subsets based on fuzzy
match scores is shown in Table 2. We can see that
sentences in the test sets are longer than those in the
training data, implying a relatively difficult trans-
lation task. We train the SVM classifier using the
libSVM (Chang and Lin, 2001) toolkit. The SVM-
Train Develop Test
SENTENCES 86,602 762 943
ENG. TOKENS 1,148,126 13,955 20,786
ENG. VOC. 13,074 3,212 3,115
CHI. TOKENS 1,171,322 10,791 16,375
CHI. VOC. 12,823 3,212 1,431
Table 1: Corpus Statistics
Scores Sentences Words W/S
(0.9, 1.0) 80 1526 19.0750
(0.8, 0.9] 96 1430 14.8958
(0.7, 0.8] 110 1596 14.5091
(0.6, 0.7] 74 1031 13.9324
(0.5, 0.6] 104 1811 17.4135
(0, 0.5] 479 8972 18.7307
Table 2: Composition of test subsets based on fuzzy
match scores
training and validation is on the same training sen-
tences1 as the SMT system with 5-fold cross valida-
tion.
The SVM hyper-parameters are tuned using the
training data of the first fold in the 5-fold cross val-
idation via a brute force grid search. More specifi-
cally, for parameter C in (1), we search in the range
[2?5, 215], while for parameter ? (2) we search in the
range [2?15, 23]. The step size is 2 on the exponent.
We conducted experiments using a standard log-
linear PB-SMT model: GIZA++ implementation of
IBM word alignment model 4 (Och and Ney, 2003),
the refinement and phrase-extraction heuristics de-
scribed in (Koehn et al, 2003), minimum-error-
rate training (Och, 2003), a 5-gram language model
with Kneser-Ney smoothing (Kneser and Ney, 1995)
trained with SRILM (Stolcke, 2002) on the Chinese
side of the training data, and Moses (Koehn et al,
2007) which is capable of handling user-specified
translations for some portions of the input during de-
coding. The maximum phrase length is set to 7.
5.2 Evaluation
The performance of the phrase-based SMT system
is measured by BLEU score (Papineni et al, 2002)
and TER (Snover et al, 2006). Significance test-
1We have around 87K sentence pairs in our training data.
However, for 67.5% of the input sentences, our MT system pro-
duces the same translation irrespective of whether the input sen-
tence is marked up or not.
1243
ing is carried out using approximate randomisation
(Noreen, 1989) with a 95% confidence level.
We also measure the quality of the classification
by precision and recall. Let A be the set of pre-
dicted markup input sentences, and B be the set
of input sentences where the markup version has a
lower TER score than the plain version. We stan-
dardly define precision P and recall R as in (7):
P =
|A?B|
|A| , R =
|A?B|
|B| (7)
5.3 Cross-fold translation
In order to obtain training samples for the classifier,
we need to label each sentence in the SMT training
data as to whether marking up the sentence can pro-
duce better translations. To achieve this, we translate
both the marked-up versions and plain versions of
the sentence and compare the two translations using
the sentence-level evaluation metric TER.
We do not make use of additional training data to
translate the sentences for SMT training, but instead
use cross-fold translation. We create a new training
corpus T by keeping 95% of the sentences in the
original training corpus, and creating a new test cor-
pus H by using the remaining 5% of the sentences.
Using this scheme we make 20 different pairs of cor-
pora (Ti,Hi) in such a way that each sentence from
the original training corpus is in exactly one Hi for
some 1 ? i ? 20. We train 20 different systems
using each Ti, and use each system to translate the
corresponding Hi as well as the marked-up version
of Hi using the procedure described in Section 3.1.
The development set is kept the same for all systems.
5.4 Experimental Results
5.4.1 Translation Results
Table 3 contains the translation results of the SMT
system when we use discriminative learning to mark
up the input sentence (MARKUP-DL). The first row
(BASELINE) is the result of translating plain test
sets without any markup, while the second row is
the result when all the test sentences are marked
up. We also report the oracle scores, i.e. the up-
perbound of using our discriminative learning ap-
proach. As we can see from this table, we obtain sig-
nificantly inferior results compared to the the Base-
line system if we categorically mark up all the in-
TER BLEU
BASELINE 39.82 45.80
MARKUP 41.62 44.41
MARKUP-DL 39.61 46.46
ORACLE 37.27 48.32
Table 3: Performance of Discriminative Learning (%)
put sentences using phrase pairs derived from fuzzy
matches. This is reflected by an absolute 1.4 point
drop in BLEU score and a 1.8 point increase in TER.
On the other hand, both the oracle BLEU and TER
scores represent as much as a 2.5 point improve-
ment over the baseline. Our discriminative learning
method (MARKUP-DL), which automatically clas-
sifies whether an input sentence should be marked
up, leads to an increase of 0.7 absolute BLEU points
over the BASELINE, which is statistically signifi-
cant. We also observe a slight decrease in TER com-
pared to the BASELINE. Despite there being much
room for further improvement when compared to the
Oracle score, the discriminative learning method ap-
pears to be effective not only in maintaining transla-
tion consistency, but also a statistically significant
improvement in translation quality.
5.4.2 Classification Confidence Thresholding
To further analyse our discriminative learning ap-
proach, we report the classification results on the test
set using the SVM classifier. We also investigate the
use of classification confidence, as described in Sec-
tion 3.2.2, as a threshold to boost classification pre-
cision if required. Table 4 shows the classification
and translation results when we use different con-
fidence thresholds. The default classification con-
fidence is 0.50, and the corresponding translation
results were described in Section 5.4.1. We inves-
tigate the impact of increasing classification confi-
dence on the performance of the classifier and the
translation results. As can be seen from Table 4,
increasing the classification confidence up to 0.70
leads to a steady increase in classification precision
with a corresponding sacrifice in recall. The fluc-
tuation in classification performance has an impact
on the translation results as measured by BLEU and
TER. We can see that the best BLEU as well as TER
scores are achieved when we set the classification
confidence to 0.60, representing a modest improve-
1244
Classification Confidence
0.50 0.55 0.60 0.65 0.70 0.75 0.80
BLEU 46.46 46.65 46.69 46.59 46.34 46.06 46.00
TER 39.61 39.46 39.32 39.36 39.52 39.71 39.71
P 60.00 68.67 70.31 74.47 72.97 64.28 88.89
R 32.14 29.08 22.96 17.86 13.78 9.18 4.08
Table 4: The impact of classification confidence thresholding
ment over the default setting (0.50). Despite the
higher precision when the confidence is set to 0.7,
the dramatic decrease in recall cannot be compen-
sated for by the increase in precision.
We can also observe from Table 4 that the recall
is quite low across the board, and the classification
results become unstable when we further increase
the level of confidence to above 0.70. This indicates
the degree of difficulty of this classification task, and
suggests some directions for future research as dis-
cussed at the end of this paper.
5.4.3 Comparison with Previous Work
As discussed in Section 2, both (Koehn and Senel-
lart, 2010) and (Zhechev and van Genabith, 2010)
used fuzzy match score to determine whether the in-
put sentences should be marked up. The input sen-
tences are only marked up when the fuzzy match
score is above a certain threshold. We present the
results using this method in Table 5. From this ta-
Fuzzy Match Scores
0.50 0.60 0.70 0.80 0.90
BLEU 45.13 45.55 45.58 45.84 45.82
TER 40.99 40.62 40.56 40.29 40.07
Table 5: Performance using fuzzy match score for classi-
fication
ble, we can see an inferior performance compared to
the BASELINE results (cf. Table 3) when the fuzzy
match score is below 0.70. A modest gain can only
be achieved when the fuzzy match score is above
0.8. This is slightly different from the conclusions
drawn in (Koehn and Senellart, 2010), where gains
are observed when the fuzzy match score is above
0.7, and in (Zhechev and van Genabith, 2010) where
gains are only observed when the score is above 0.9.
Comparing Table 5 with Table 4, we can see that
our classification method is more effective. This
confirms our argument in the last paragraph of Sec-
tion 2, namely that fuzzy match score is not informa-
tive enough to determine the usefulness of the sub-
sentences in a fuzzy match, and that a more compre-
hensive set of features, as we have explored in this
paper, is essential for the discriminative learning-
based method to work.
FM Scores w. markup w/o markup
[0,0.5] 37.75 62.24
(0.5,0.6] 40.64 59.36
(0.6,0.7] 40.94 59.06
(0.7,0.8] 46.67 53.33
(0.8,0.9] 54.28 45.72
(0.9,1.0] 44.14 55.86
Table 6: Percentage of training sentences with markup
vs without markup grouped by fuzzy match (FM) score
ranges
To further validate our assumption, we analyse
the training sentences by grouping them accord-
ing to their fuzzy match score ranges. For each
group of sentences, we calculate the percentage of
sentences where markup (and respectively without
markup) can produce better translations. The statis-
tics are shown in Table 6. We can see that for sen-
tences with fuzzy match scores lower than 0.8, more
sentences can be better translated without markup.
For sentences where fuzzy match scores are within
the range (0.8, 0.9], more sentences can be better
translated with markup. However, within the range
(0.9, 1.0], surprisingly, actually more sentences re-
ceive better translation without markup. This indi-
cates that fuzzy match score is not a good measure to
predict whether fuzzy matches are beneficial when
used to constrain the translation of an input sentence.
5.5 Contribution of Features
We also investigated the contribution of our differ-
ent feature sets. We are especially interested in
the contribution of dependency features, as they re-
1245
Example 1
w/o markup after policy name , type the name of the policy ( it shows new host integrity
policy by default ) .
Translation ????????????????? (????? ???????
??????
w. markup after policy name <tm translation=????????????? ??
?? ?????????>, type the name of the policy ( it shows new host
integrity policy by default ) .< /tm>
Translation ????????????????????? ???? ????????
Reference ????????????????????? ???? ????????
Example 2
w/o markup changes apply only to the specific scan that you select .
Translation ??????????????
w. markup changes apply only to the specific scan that you select <tm translation=???>.< /tm>
Translation ???????????????
Reference ???????????????
flect whether translation consistency can be captured
using syntactic knowledge. The classification and
TER BLEU P R
TM+TRANS 40.57 45.51 52.48 27.04
+DEP 39.61 46.46 60.00 32.14
Table 7: Contribution of Features (%)
translation results using different features are re-
ported in Table 7. We observe a significant improve-
ment in both classification precision and recall by
adding dependency (DEP) features on top of TM
and translation features. As a result, the translation
quality also significantly improves. This indicates
that dependency features which can capture struc-
tural and semantic similarities are effective in gaug-
ing the usefulness of the phrase pairs derived from
the fuzzy matches. Note also that without including
the dependency features, our discriminative learning
method cannot outperform the BASELINE (cf. Ta-
ble 3) in terms of translation quality.
5.6 Improved Translations
In order to pinpoint the sources of improvements by
marking up the input sentence, we performed some
manual analysis of the output. We observe that the
improvements can broadly be attributed to two rea-
sons: 1) the use of long phrase pairs which are miss-
ing in the phrase table, and 2) deterministically using
highly reliable phrase pairs.
Phrase-based SMT systems normally impose a
limit on the length of phrase pairs for storage and
speed considerations. Our method can overcome
this limitation by retrieving and reusing long phrase
pairs on the fly. A similar idea, albeit from a dif-
ferent perspective, was explored by (Lopez, 2008),
where he proposed to construct a phrase table on the
fly for each sentence to be translated. Differently
from his approach, our method directly translates
part of the input sentence using fuzzy matches re-
trieved on the fly, with the rest of the sentence trans-
lated by the pre-trained MT system. We offer some
more insights into the advantages of our method by
means of a few examples.
Example 1 shows translation improvements by
using long phrase pairs. Compared to the refer-
ence translation, we can see that for the underlined
phrase, the translation without markup contains (i)
word ordering errors and (ii) a missing right quota-
tion mark. In Example 2, by specifying the transla-
tion of the final punctuation mark, the system cor-
rectly translates the relative clause ?that you select?.
The translation of this relative clause is missing
when translating the input without markup. This
improvement can be partly attributed to the reduc-
tion in search errors by specifying the highly reliable
translations for phrases in an input sentence.
6 Conclusions and Future Work
In this paper, we introduced a discriminative learn-
ing method to tightly integrate fuzzy matches re-
trieved using translation memory technologies with
phrase-based SMT systems to improve translation
consistency. We used an SVM classifier to predict
whether phrase pairs derived from fuzzy matches
could be used to constrain the translation of an in-
1246
put sentence. A number of feature functions includ-
ing a series of novel dependency features were used
to train the classifier. Experiments demonstrated
that discriminative learning is effective in improving
translation quality and is more informative than the
fuzzy match score used in previous research. We re-
port a statistically significant 0.9 absolute improve-
ment in BLEU score using a procedure to promote
translation consistency.
As mentioned in Section 2, the potential improve-
ment in sentence-level translation consistency us-
ing our method can be attributed to the fact that
the translation of new input sentences is closely in-
formed and guided (or constrained) by previously
translated sentences using global features such as
dependencies. However, it is worth noting that
the level of gains in translation consistency is also
dependent on the nature of the TM itself; a self-
contained coherent TM would facilitate consistent
translations. In the future, we plan to investigate
the impact of TM quality on translation consistency
when using our approach. Furthermore, we will ex-
plore methods to promote translation consistency at
document level.
Moreover, we also plan to experiment with
phrase-by-phrase classification instead of sentence-
by-sentence classification presented in this paper,
in order to obtain more stable classification results.
We also plan to label the training examples using
other sentence-level evaluation metrics such as Me-
teor (Banerjee and Lavie, 2005), and to incorporate
features that can measure syntactic similarities in
training the classifier, in the spirit of (Owczarzak et
al., 2007). Currently, only a standard phrase-based
SMT system is used, so we plan to test our method
on a hierarchical system (Chiang, 2005) to facilitate
direct comparison with (Koehn and Senellart, 2010).
We will also carry out experiments on other data sets
and for more language pairs.
Acknowledgments
This work is supported by Science Foundation Ire-
land (Grant No 07/CE/I1142) and part funded under
FP7 of the EC within the EuroMatrix+ project (grant
No 231720). The authors would like to thank the
reviewers for their insightful comments and sugges-
tions.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65?72, Ann Arbor, MI.
Ergun Bic?ici and Marc Dymetman. 2008. Dynamic
translation memory: Using statistical machine trans-
lation to improve translation memory. In Proceedings
of the 9th Internation Conference on Intelligent Text
Processing and Computational Linguistics (CICLing),
pages 454?465, Haifa, Israel.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/
?
cjlin/libsvm.
David Chiang. 2005. A hierarchical Phrase-Based model
for Statistical Machine Translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 263?270, Ann
Arbor, MI.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273?297.
Yifan He, Yanjun Ma, Josef van Genabith, and Andy
Way. 2010. Bridging SMT and TM with translation
recommendation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 622?630, Uppsala, Sweden.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, volume 1,
pages 181?184, Detroit, MI.
Philipp Koehn and Jean Senellart. 2010. Convergence of
translation memory and statistical machine translation.
In Proceedings of AMTA Workshop on MT Research
and the Translation Industry, pages 21?31, Denver,
CO.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of the 2003 Human Language Technology Conference
and the North American Chapter of the Association
for Computational Linguistics, pages 48?54, Edmon-
ton, AB, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics Companion Vol-
1247
ume Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic.
Vladimir Iosifovich Levenshtein. 1966. Binary codes ca-
pable of correcting deletions, insertions, and reversals.
Soviet Physics Doklady, 10(8):707?710.
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng. 2007.
A note on platt?s probabilistic outputs for support vec-
tor machines. Machine Learning, 68(3):267?276.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 505?512, Manchester, UK, August.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience, New York, NY.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51.
Franz Och. 2003. Minimum Error Rate Training in Sta-
tistical Machine Translation. In 41st Annual Meet-
ing of the Association for Computational Linguistics,
pages 160?167, Sapporo, Japan.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2007. Labelled dependencies in machine translation
evaluation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 104?111,
Prague, Czech Republic.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of Machine Translation. In 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318, Philadelphia, PA.
John C. Platt. 1999. Probabilistic outputs for support
vector machines and comparisons to regularized likeli-
hood methods. Advances in Large Margin Classifiers,
pages 61?74.
Richard Sikes. 2007. Fuzzy matching in theory and prac-
tice. Multilingual, 18(6):39?43.
Michel Simard and Pierre Isabelle. 2009. Phrase-based
machine translation in a computer-assisted translation
environment. In Proceedings of the Twelfth Machine
Translation Summit (MT Summit XII), pages 120 ?
127, Ottawa, Ontario, Canada.
James Smith and Stephen Clark. 2009. EBMT for SMT:
A new EBMT-SMT hybrid. In Proceedings of the 3rd
International Workshop on Example-Based Machine
Translation, pages 3?10, Dublin, Ireland.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas (AMTA-2006), pages 223?231, Cam-
bridge, MA, USA.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-Taylor. 2009. Improving the
confidence of machine translation quality estimates.
In Proceedings of the Twelfth Machine Translation
Summit (MT Summit XII), pages 136 ? 143, Ottawa,
Ontario, Canada.
Andreas Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, CO.
Ventsislav Zhechev and Josef van Genabith. 2010.
Seeding statistical machine translation with translation
memory output through tree-based structural align-
ment. In Proceedings of the Fourth Workshop on Syn-
tax and Structure in Statistical Translation, pages 43?
51, Beijing, China.
1248
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 33?37,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Head-Driven Hierarchical Phrase-based Translation
Junhui Li Zhaopeng Tu? Guodong Zhou? Josef van Genabith
Centre for Next Generation Localisation
School of Computing, Dublin City University
? Key Lab. of Intelligent Info. Processing
Institute of Computing Technology, Chinese Academy of Sciences
?School of Computer Science and Technology
Soochow University, China
{jli,josef}@computing.dcu.ie
tuzhaopeng@ict.ac.cn gdzhou@suda.edu.cn
Abstract
This paper presents an extension of Chi-
ang?s hierarchical phrase-based (HPB) model,
called Head-Driven HPB (HD-HPB), which
incorporates head information in translation
rules to better capture syntax-driven infor-
mation, as well as improved reordering be-
tween any two neighboring non-terminals at
any stage of a derivation to explore a larger
reordering search space. Experiments on
Chinese-English translation on four NIST MT
test sets show that the HD-HPB model signifi-
cantly outperforms Chiang?s model with aver-
age gains of 1.91 points absolute in BLEU.
1 Introduction
Chiang?s hierarchical phrase-based (HPB) transla-
tion model utilizes synchronous context free gram-
mar (SCFG) for translation derivation (Chiang,
2005; Chiang, 2007) and has been widely adopted
in statistical machine translation (SMT). Typically,
such models define two types of translation rules:
hierarchical (translation) rules which consist of both
terminals and non-terminals, and glue (grammar)
rules which combine translated phrases in a mono-
tone fashion. Due to lack of linguistic knowledge,
Chiang?s HPB model contains only one type of non-
terminal symbol X , often making it difficult to se-
lect the most appropriate translation rules.1 What
is more, Chiang?s HPB model suffers from limited
phrase reordering combining translated phrases in a
monotonic way with glue rules. In addition, once a
1Another non-terminal symbol S is used in glue rules.
glue rule is adopted, it requires all rules above it to
be glue rules.
One important research question is therefore how
to refine the non-terminal category X using linguis-
tically motivated information: Zollmann and Venu-
gopal (2006) (SAMT) e.g. use (partial) syntactic
categories derived from CFG trees while Zollmann
and Vogel (2011) use word tags, generated by ei-
ther POS analysis or unsupervised word class in-
duction. Almaghout et al (2011) employ CCG-
based supertags. Mylonakis and Sima?an (2011) use
linguistic information of various granularities such
as Phrase-Pair, Constituent, Concatenation of Con-
stituents, and Partial Constituents, where applica-
ble. Inspired by previous work in parsing (Char-
niak, 2000; Collins, 2003), our Head-Driven HPB
(HD-HPB) model is based on the intuition that lin-
guistic heads provide important information about a
constituent or distributionally defined fragment, as
in HPB. We identify heads using linguistically mo-
tivated dependency parsing, and use their POS to
refine X. In addition HD-HPB provides flexible re-
ordering rules freely mixing translation and reorder-
ing (including swap) at any stage in a derivation.
Different from the soft constraint modeling
adopted in (Chan et al, 2007; Marton and Resnik,
2008; Shen et al, 2009; He et al, 2010; Huang et
al., 2010; Gao et al, 2011), our approach encodes
syntactic information in translation rules. However,
the two approaches are not mutually exclusive, as
we could also include a set of syntax-driven features
into our translation model. Our approach maintains
the advantages of Chiang?s HPB model while at the
same time incorporating head information and flex-
33
 ??/NR 
Ouzhou 
??/NN 
baguo 
??/AD 
lianming 
??/VV 
zhichi 
??/NR 
meiguo 
??/NN 
lichang 
root 
Eight European countries jointly support America?s stand 
Figure 1: An example word alignment for a Chinese-
English sentence pair with the dependency parse tree for
the Chinese sentence. Here, each Chinese word is at-
tached with its POS tag and Pinyin.
ible reordering in a derivation in a natural way. Ex-
periments on Chinese-English translation using four
NIST MT test sets show that our HD-HPB model
significantly outperforms Chiang?s HPB as well as a
SAMT-style refined version of HPB.
2 Head-Driven HPB Translation Model
Like Chiang (2005) and Chiang (2007), our HD-
HPB translation model adopts a synchronous con-
text free grammar, a rewriting system which gen-
erates source and target side string pairs simulta-
neously using a context-free grammar. Instead of
collapsing all non-terminals in the source language
into a single symbol X as in Chiang (2007), given a
word sequence f ij from position i to position j, we
first find heads and then concatenate the POS tags
of these heads as f ij?s non-terminal symbol. Specif-
ically, we adopt unlabeled dependency structure to
derive heads, which are defined as:
Definition 1. For word sequence f ij , word
fk (i ? k ? j) is regarded as a head if it is domi-
nated by a word outside of this sequence.
Note that this definition (i) allows for a word se-
quence to have one or more heads (largely due to
the fact that a word sequence is not necessarily lin-
guistically constrained) and (ii) ensures that heads
are always the highest heads in the sequence from a
dependency structure perspective. For example, the
word sequence ouzhou baguo lianming in Figure 1
has two heads (i.e., baguo and lianming, ouzhou is
not a head of this sequence since its headword baguo
falls within this sequence) and the non-terminal cor-
responding to the sequence is thus labeled as NN-
AD. It is worth noting that in this paper we only
refine non-terminal X on the source side to head-
informed ones, while still usingX on the target side.
According to the occurrence of terminals in
translation rules, we group rules in the HD-HPB
model into two categories: head-driven hierarchical
rules (HD-HRs) and non-terminal reordering rules
(NRRs), where the former have at least one terminal
on both source and target sides and the later have no
terminals. For rule extraction, we first identify ini-
tial phrase pairs on word-aligned sentence pairs by
using the same criterion as most phrase-based trans-
lation models (Och and Ney, 2004) and Chiang?s
HPB model (Chiang, 2005; Chiang, 2007). We
extract HD-HRs and NRRs based on initial phrase
pairs, respectively.
2.1 HD-HRs: Head-Driven Hierarchical Rules
As mentioned, a HD-HR has at least one terminal
on both source and target sides. This is the same
as the hierarchical rules defined in Chiang?s HPB
model (Chiang, 2007), except that we use head POS-
informed non-terminal symbols in the source lan-
guage. We look for initial phrase pairs that contain
other phrases and then replace sub-phrases with POS
tags corresponding to their heads. Given the word
alignment in Figure 1, Table 1 demonstrates the dif-
ference between hierarchical rules in Chiang (2007)
and HD-HRs defined here.
Similar to Chiang?s HPB model, our HD-HPB
model will result in a large number of rules causing
problems in decoding. To alleviate these problems,
we filter our HD-HRs according to the same con-
straints as described in Chiang (2007). Moreover,
we discard rules that have non-terminals with more
than four heads.
2.2 NRRs: Non-terminal Reordering Rules
NRRs are translation rules without terminals. Given
an initial phrase pair on the source side, there are
four possible positional relationships for their target
side translations (we use Y as a variable for non-
terminals on the source side while all non-terminals
on the target side are labeled as X):
? Monotone ?Y ? Y1Y2, X ? X1X2?;
? Discontinuous monotone
?Y ? Y1Y2, X ? X1 . . . X2?;
? Swap ?Y ? Y1Y2, X ? X2X1?;
? Discontinuous swap
?Y ? Y1Y2, X ? X2 . . . X1?.
34
phrase pairs hierarchical rule head-driven hierarchical rule
lichang, stand X?lichang, stand
NN?lichang,
X?stand
meiguo lichang1, America?s stand1 X?meiguo X1, America?s X1
NN?meiguo NN1,
X?America?s X1
zhichi meiguo, support America?s X?zhichi meiguo, support America?s
VV-NR?zhichi meiguo,
X?support America?s
zhichi meiguo1 lichang,
support America?s1 stand
X?X1 lichang,
X1 stand
VV?VV-NR1 lichang,
X?X1 stand
Table 1: Comparison of hierarchical rules in Chiang (2007) and HD-HRs. Indexed underlines indicate sub-phrases
and corresponding non-terminal symbols. The non-terminals in HD-HRs (e.g., NN, VV, VV-NR) capture the head(s)
POS tags of the corresponding word sequence in the source language.
Merging two neighboring non-terminals into a
single non-terminal, NRRs enable the translation
model to explore a wider search space. During train-
ing, we extract four types of NRRs and calculate
probabilities for each type. To speed up decoding,
we currently (i) only use monotone and swap NRRs
and (ii) limit the number of non-terminals in a NRR
to 2.
2.3 Features and Decoding
Given e for the translation output in the target lan-
guage, s and t for strings of terminals and non-
terminals on the source and target side, respectively,
we use a feature set analogous to the default feature
set of Chiang (2007), including:
? Phd-hr (t|s) and Phd-hr (s|t), translation probabili-
ties for HD-HRs;
? Plex (t|s) and Plex (s|t), lexical translation proba-
bilities for HD-HRs;
? Ptyhd-hr = exp (?1), rule penalty for HD-HRs;
? Pnrr (t|s), translation probability for NRRs;
? Ptynrr = exp (?1), rule penalty for NRRs;
? Plm (e), language model;
? Ptyword (e) = exp (?|e|), word penalty.
Our decoder is based on CKY-style chart parsing
with beam search and searches for the best deriva-
tion bottom-up. For a source span [i, j], it applies
both types of HD-HRs and NRRs. However, HD-
HRs are only applied to generate derivations span-
ning no more than K words ? the initial phrase
length limit used in training to extract HD-HRs ?
while NRRs are applied to derivations spanning any
length. Unlike in Chiang?s HPB model, it is pos-
sible for a non-terminal generated by a NRR to be
included afterwards by a HD-HR or another NRR.
3 Experiments
We evaluate the performance of our HD-HPB model
and compare it with our implementation of Chiang?s
HPB model (Chiang, 2007), a source-side SAMT-
style refined version of HPB (SAMT-HPB), and the
Moses implementation of HPB. For fair compari-
son, we adopt the same parameter settings for our
HD-HPB and HPB systems, including initial phrase
length (as 10) in training, the maximum number of
non-terminals (as 2) in translation rules, maximum
number of non-terminals plus terminals (as 5) on
the source, beam threshold ? (as 10?5) (to discard
derivations with a score worse than ? times the best
score in the same chart cell), beam size b (as 200)
(i.e. each chart cell contains at most b derivations).
For Moses HPB, we use ?grow-diag-final-and? to
obtain symmetric word alignments, 10 for the max-
imum phrase length, and the recommended default
values for all other parameters.
We train our model on a dataset with ?1.5M sen-
tence pairs from the LDC dataset.2 We use the
2002 NIST MT evaluation test data (878 sentence
pairs) as the development data, and the 2003, 2004,
2005, 2006-news NIST MT evaluation test data
(919, 1788, 1082, and 616 sentence pairs, respec-
tively) as the test data. To find heads, we parse the
source sentences with the Berkeley Parser3 (Petrov
and Klein, 2007) trained on Chinese TreeBank 6.0
and use the Penn2Malt toolkit4 to obtain (unlabeled)
dependency structures.
We obtain the word alignments by running
2This dataset includes LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06
3http://code.google.com/p/berkeleyparser/
4http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html/
35
GIZA++ (Och and Ney, 2000) on the corpus in both
directions and applying ?grow-diag-final-and? re-
finement (Koehn et al, 2003). We use the SRI lan-
guage modeling toolkit to train a 5-gram language
model on the Xinhua portion of the Gigaword corpus
and standard MERT (Och, 2003) to tune the feature
weights on the development data.
For evaluation, the NIST BLEU script (version
12) with the default settings is used to calculate the
BLEU scores. To test whether a performance differ-
ence is statistically significant, we conduct signifi-
cance tests following the paired bootstrap approach
(Koehn, 2004). In this paper,?**? and?*? de-
note p-values less than 0.01 and in-between [0.01,
0.05), respectively.
Table 2 lists the rule table sizes. The full rule ta-
ble size (including HD-HRs and NRRs) of our HD-
HPB model is ?1.5 times that of Chiang?s, largely
due to refining the non-terminal symbol X in Chi-
ang?s model into head-informed ones in our model.
It is also unsurprising, that the test set-filtered rule
table size of our model is only ?0.7 times that of Chi-
ang?s: this is due to the fact that some of the refined
translation rule patterns required by the test set are
unattested in the training data. Furthermore, the rule
table size of NRRs is much smaller than that of HD-
HRs since a NRR contains only two non-terminals.
Table 3 lists the translation performance with
BLEU scores. Note that our re-implementation of
Chiang?s original HPB model performs on a par with
Moses HPB. Table 3 shows that our HD-HPB model
significantly outperforms Chiang?s HPB model with
an average improvement of 1.91 in BLEU (and sim-
ilar improvements over Moses HPB).
Table 3 shows that the head-driven scheme out-
performs a SAMT-style approach (for each test set
p < 0.01), indicating that head information is more
effective than (partial) CFG categories. Taking lian-
ming zhichi in Figure 1 as an example, HD-HPB
labels the span VV, as lianming is dominated by
zhichi, effecively ignoring lianming in the transla-
tion rule, while the SAMT label is ADVP:AD+VV5
which is more susceptible to data sparsity. In addi-
tion, SAMT resorts to X if a text span fails to satisify
pre-defined categories. Examining initial phrases
5the constituency structure for lianming zhichi is (VP (ADVP
(AD lianming)) (VP (VV zhichi) ...)).
System Total MT 03 MT 04 MT 05 MT 06 Avg.
HPB 39.6 2.8 4.7 3.3 3.0 3.4
HD-HPB 59.5/0.6 1.9/0.1 3.4/0.2 2.3/0.2 2.0/0.1 2.4/0.2
Table 2: Rule table sizes (in million) of different mod-
els. Note: 1) For HD-HPB, the rule sizes separated by /
indicate HD-HRs and NRRs, respectively; 2) Except for
?Total?, the figures correspond to rules filtered on the cor-
responding test set.
System MT 03 MT 04 MT 05 MT 06 Avg.
Moses HPB 32.94* 35.16 32.18 29.88* 32.54
HPB 33.59 35.39 32.20 30.60 32.95
HD-HPB 35.50** 37.61** 34.56** 31.78** 34.86
SAMT-HPB 34.07 36.52** 32.90* 30.66 33.54
HD-HR+Glue 34.58** 36.55** 33.84** 31.06 34.01
Table 3: BLEU (%) scores of different models. Note:
1) SAMT-HPB indicates our HD-HPB model with non-
terminal scheme of Zollmann and Venugopal (2006);
2) HD-HR+Glue indicates our HD-HPB model replac-
ing NRRs with glue rules; 3) Significance tests for
Moses HPB, HD-HPB, SAMT-HPB, and HD-HR+Glue
are done against HPB.
extracted from the SAMT training data shows that
28% of them are labeled as X.
In order to separate out the individual contribu-
tions of the novel HD-HRs and NRRs, we carry out
an additional experiment (HD-HR+Glue) using HD-
HRs with monotonic glue rules only (adjusted to re-
fined rule labels, but effectively switching off the ex-
tra reordering power of full NRRs). Table 3 shows
that on average more than half of the improvement
over HPB (Chiang and Moses) comes from the re-
fined HD-HRs, the rest from NRRs.
Examining translation rules extracted from the
training data shows that there are 72,366 types of
non-terminals with respect to 33 types of POS tags.
On average each sentence employs 16.6/5.2 HD-
HRs/NRRs in our HD-HPB model, compared to
15.9/3.6 hierarchical rules/glue rules in Chiang?s
model, providing further indication of the impor-
tance of NRRs in translation.
4 Conclusion
We present a head-driven hierarchical phrase-based
(HD-HPB) translation model, which adopts head in-
formation (derived through unlabeled dependency
analysis) in the definition of non-terminals to bet-
ter differentiate among translation rules. In ad-
36
dition, improved and better integrated reordering
rules allow better reordering between consecutive
non-terminals through exploration of a larger search
space in the derivation. Experimental results on
Chinese-English translation across four test sets
demonstrate significant improvements of the HD-
HPB model over both Chiang?s HPB and a source-
side SAMT-style refined version of HPB.
Acknowledgments
This work was supported by Science Foundation Ire-
land (Grant No. 07/CE/I1142) as part of the Cen-
tre for Next Generation Localisation (www.cngl.ie)
at Dublin City University. It was also partially
supported by Project 90920004 under the National
Natural Science Foundation of China and Project
2012AA011102 under the ?863? National High-
Tech Research and Development of China. We
thank the reviewers for their insightful comments.
References
Hala Almaghout, Jie Jiang, and Andy Way. 2011. CCG
contextual labels in hierarchical phrase-based SMT. In
Proceedings of EAMT 2011, pages 281?288.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of ACL 2007, pages
33?40.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL 2000, pages 132?
139.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL 2005, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589?637.
Yang Gao, Philipp Koehn, and Alexandra Birch. 2011.
Soft dependency constraints for reordering in hierar-
chical phrase-based translation. In Proceedings of
EMNLP 2011, pages 857?868.
Zhongjun He, Yao Meng, and Hao Yu. 2010. Maxi-
mum entropy based phrase reordering for hierarchical
phrase-based translation. In Proceedings of EMNLP
2010, pages 555?563.
Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.
2010. Soft syntactic constraints for hierarchical
phrase-based translation using latent syntactic distri-
butions. In Proceedings of EMNLP 2010, pages 138?
147.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL 2003, pages 48?54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-HLT 2008, pages 1003?1011.
Markos Mylonakis and Khalil Sima?an. 2011. Learning
hierarchical translation structure with linguistic anno-
tations. In Proceedings of ACL-HLT 2011, pages 642?
652.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000, pages 440?447.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
2003, pages 160?167.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
2007, pages 404?411.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proceedings of EMNLP 2009, pages
72?80.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of NAACL 2006 - Workshop on Statistical
Machine Translation, pages 138?141.
Andreas Zollmann and Stephan Vogel. 2011. A word-
class approach to labeling PSCFG rules for machine
translation. In Proceedings of ACL-HLT 2011, pages
1?11.
37
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 338?343,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Identifying High-Impact Sub-Structures for Convolution Kernels in
Document-level Sentiment Classification
Zhaopeng Tu? Yifan He?? Jennifer Foster? Josef van Genabith? Qun Liu? Shouxun Lin?
?Key Lab. of Intelligent Info. Processing ?Computer Science Department ?School of Computing
Institute of Computing Technology, CAS New York University Dublin City University
?{tuzhaopeng,liuqun,sxlin}@ict.ac.cn,
?yhe@cs.nyu.edu, ?{jfoster,josef}@computing.dcu.ie
Abstract
Convolution kernels support the modeling of
complex syntactic information in machine-
learning tasks. However, such models are
highly sensitive to the type and size of syntac-
tic structure used. It is therefore an importan-
t challenge to automatically identify high im-
pact sub-structures relevant to a given task. In
this paper we present a systematic study inves-
tigating (combinations of) sequence and con-
volution kernels using different types of sub-
structures in document-level sentiment classi-
fication. We show that minimal sub-structures
extracted from constituency and dependency
trees guided by a polarity lexicon show 1.45
point absolute improvement in accuracy over a
bag-of-words classifier on a widely used sen-
timent corpus.
1 Introduction
An important subtask in sentiment analysis is sen-
timent classification. Sentiment classification in-
volves the identification of positive and negative
opinions from a text segment at various levels of
granularity including document-level, paragraph-
level, sentence-level and phrase-level. This paper
focuses on document-level sentiment classification.
There has been a substantial amount of work
on document-level sentiment classification. In ear-
ly pioneering work, Pang and Lee (2004) use a
flat feature vector (e.g., a bag-of-words) to rep-
resent the documents. A bag-of-words approach,
however, cannot capture important information ob-
tained from structural linguistic analysis of the doc-
uments. More recently, there have been several ap-
proaches which employ features based on deep lin-
guistic analysis with encouraging results including
Joshi and Penstein-Rose (2009) and Liu and Senef-
f (2009). However, as they select features manually,
these methods would require additional labor when
ported to other languages and domains.
In this paper, we study and evaluate diverse lin-
guistic structures encoded as convolution kernels for
the document-level sentiment classification prob-
lem, in order to utilize syntactic structures without
defining explicit linguistic rules. While the applica-
tion of kernel methods could seem intuitive for many
tasks, it is non-trivial to apply convolution kernels
to document-level sentiment classification: previous
work has already shown that categorically using the
entire syntactic structure of a single sentence would
produce too many features for a convolution ker-
nel (Zhang et al, 2006; Moschitti et al, 2008). We
expect the situation to be worse for our task as we
work with documents that tend to comprise dozens
of sentences.
It is therefore necessary to choose appropriate
substructures of a sentence as opposed to using the
whole structure in order to effectively use convolu-
tion kernels in our task. It has been observed that
not every part of a document is equally informa-
tive for identifying the polarity of the whole doc-
ument (Yu and Hatzivassiloglou, 2003; Pang and
Lee, 2004; Koppel and Schler, 2005; Ferguson et
al., 2009): a film review often uses lengthy objective
paragraphs to simply describe the plot. Such objec-
tive portions do not contain the author?s opinion and
are irrelevant with respect to the sentiment classifi-
338
cation task. Indeed, separating objective sentences
from subjective sentences in a document produces
encouraging results (Yu and Hatzivassiloglou, 2003;
Pang and Lee, 2004; Koppel and Schler, 2005; Fer-
guson et al, 2009). Our research is inspired by these
observations. Unlike in the previous work, however,
we focus on syntactic substructures (rather than en-
tire paragraphs or sentences) that contain subjective
words.
More specifically, we use the terms in the lexi-
con constructed from (Wilson et al, 2005) as the
indicators to identify the substructures for the con-
volution kernels, and extract different sub-structures
according to these indicators for various types of
parse trees (Section 3). An empirical evaluation on
a widely used sentiment corpus shows an improve-
ment of 1.45 point in accuracy over the baseline
resulting from a combination of bag-of-words and
high-impact parse features (Section 4).
2 Related Work
Our research builds on previous work in the field
of sentiment classification and convolution kernel-
s. For sentiment classification, the design of lexi-
cal and syntactic features is an important first step.
Several approaches propose feature-based learning
algorithms for this problem. Pang and Lee (2004)
and Dave et al (2003) represent a document as a
bag-of-words; Matsumoto et al, (2005) extract fre-
quently occurring connected subtrees from depen-
dency parsing; Joshi and Penstein-Rose (2009) use
a transformation of dependency relation triples; Liu
and Seneff (2009) extract adverb-adjective-noun re-
lations from dependency parser output.
Previous research has convincingly demonstrat-
ed a kernel?s ability to generate large feature set-
s, which is useful to quickly model new and not
well understood linguistic phenomena in machine
learning, and has led to improvements in various
NLP tasks, including relation extraction (Bunescu
and Mooney, 2005a; Bunescu and Mooney, 2005b;
Zhang et al, 2006; Nguyen et al, 2009), question
answering (Moschitti and Quarteroni, 2008), seman-
tic role labeling (Moschitti et al, 2008).
Convolution kernels have been used before in sen-
timent analysis: Wiegand and Klakow (2010) use
convolution kernels for opinion holder extraction,
Johansson and Moschitti (2010) for opinion expres-
sion detection and Agarwal et al (2011) for sen-
timent analysis of Twitter data. Wiegand and K-
lakow (2010) use e.g. noun phrases as possible can-
didate opinion holders, in our work we extract any
minimal syntactic context containing a subjective
word. Johansson and Moschitti (2010) and Agarwal
et al (2011) process sentences and tweets respec-
tively. However, as these are considerably shorter
than documents, their feature space is less complex,
and pruning is not as pertinent.
3 Kernels for Sentiment Classification
3.1 Linguistic Representations
We explore both sequence and convolution kernels
to exploit information on surface and syntactic lev-
els. For sequence kernels, we make use of lexical
words with some syntactic information in the form
of part-of-speech (POS) tags. More specifically, we
define three types of sequences:
? SW, a sequence of lexical words, e.g.: A tragic
waste of talent and incredible visual effects.
? SP, a sequence of POS tags, e.g.: DT JJ NN IN
NN CC JJ JJ NNS.
? SWP, a sequence of words and POS tags,
e.g.: A/DT tragic/JJ waste/NN of/IN talent/NN
and/CC incredible/JJ visual/JJ effects/NNS.
In addition, we experiment with constituency tree
kernels (CON), and dependency tree kernels (D),
which capture hierarchical constituency structure
and labeled dependency relations between words,
respectively. For dependency kernels, we test with
word (DW), POS (DP), and combined word-and-
POS settings (DWP), and similarly for simple se-
quence kernels (SW, SP and SWP). We also use a
vector kernel (VK) in a bag-of-words baseline. Fig-
ure 1 shows the constituent and dependency struc-
ture for the above sentence.
3.2 Settings
As kernel-based algorithms inherently explore the
whole feature space to weight the features, it is im-
portant to choose appropriate substructures to re-
move unnecessary features as much as possible.
339
NP
PP
NP
DT JJ NN
A tragic waste
NP
IN
of
NP NP
NN
talent
CC
and
JJ JJ NNS
incredible visual effect
(a)
waste
det amod prep of
A tragic talent
conj and
effects
amod amod
incredible visual
(b)
waste
det amod prep of
DT JJ NN
conj and
NNS
amod amod
JJ JJ
(c)
waste
det amod prep of
DT
A
JJ
tragic
NN
talent
conj and
NNS
effects
amod amod
JJ
incredible
visual
visual
(d)
Figure 1: Illustration of the different tree structures employed for convolution kernels. (a) Constituent parse tree
(CON); (b) Dependency tree-based words integrated with grammatical relations (DW); (c) Dependency tree in (b)
with words substituted by POS tags (DP); (d) Dependency tree in (b) with POS tags inserted before words (DWP).
NP
DT JJ NN
A tragic waste
(a)
waste
amod
JJ
tragic
(b)
Figure 2: Illustration of the different settings on con-
stituency (CON) and dependency (DWP) parse trees with
tragic as the indicator word.
Unfortunately, in our task there exist several cues
indicating the polarity of the document, which are
distributed in different sentences. To solve this prob-
lem, we define the indicators in this task as subjec-
tive words in a polarity lexicon (Wilson et al, 2005).
For each polarity indicator, we define the ?scope?
(the minimal syntactic structure containing at least
one subjective word) of each indicator for different
representations as follows:
For a constituent tree, a node and its children
correspond to a grammatical production. There-
fore, considering the terminal node tragic in the con-
stituent structure tree in Figure 1(a), we extract the
subtree rooted at the grandparent of the terminal, see
Figure 2(a). We also use the corresponding sequence
Scopes Trees Size
Document 32 24
Subjective Sentences 22 27
Constituent Substructures 30 10
Dependency Substructures 40 3
Table 1: The detail of the corpus. Here Trees denotes the
average number of trees, and Size denotes the averaged
number of words in each tree.
of words in the subtree for the sequential kernel.
For a dependency tree, we only consider the sub-
tree containing the lexical items that are directly
connected to the subjective word. For instance, giv-
en the node tragic in Figure 1(d), we will extract its
direct parent waste integrated with dependency rela-
tions and (possibly) POS, as in Figure 2(b).
We further add two background scopes, one be-
ing subjective sentences (the sentences that contain
subjective words), and the entire document.
4 Experiments
4.1 Setup
We carried out experiments on the movie review
dataset (Pang and Lee, 2004), which consists of
340
1000 positive reviews and 1000 negative reviews.
To obtain constituency trees, we parsed the docu-
ment using the Stanford Parser (Klein and Man-
ning, 2003). To obtain dependency trees, we passed
the Stanford constituency trees through the Stanford
constituency-to-dependency converter (de Marneffe
and Manning, 2008).
We exploited Subset Tree (SST) (Collins and
Duffy, 2001) and Partial Tree (PT) kernels (Mos-
chitti, 2006) for constituent and dependency parse
trees1, respectively. A sequential kernel is applied
for lexical sequences. Kernels were combined using
plain (unweighted) summation. Corpus statistics are
provided in Table 1.
We use a manually constructed polarity lexicon
(Wilson et al, 2005), in which each entry is annotat-
ed with its degree of subjectivity (strong, weak), as
well as its sentiment polarity (positive, negative and
neutral). We only take into account the subjective
terms with the degree of strong subjectivity.
We consider two baselines:
? VK: bag-of-words features using a vector ker-
nel (Pang and Lee, 2004; Ng et al, 2006)
? Rand: a number of randomly selected sub-
structures similar to the number of extracted
substructures defined in Section 3.2
All experiments were carried out using the SVM-
Light-TK toolkit2 with default parameter settings.
All results reported are based on 10-fold cross vali-
dation.
4.2 Results and Discussions
Table 2 lists the results of the different kernel type
combinations. The best performance is obtained by
combining VK and DW kernels, gaining a signifi-
cant improvement of 1.45 point in accuracy. As far
as PT kernels are concerned, we find dependency
trees with simple words (DW) outperform both de-
pendency trees with POS (DP) and those with both
words and POS (DWP). We conjecture that in this
case, as syntactic information is already captured by
1A SubSet Tree is a structure that satisfies the constraint that
grammatical rules cannot be broken, while a Partial Tree is a
more general form of substructures obtained by the application
of partial production rules of the grammar.
2available at http://disi.unitn.it/moschitti/
Kernels Doc Sent Rand Sub
VK 87.05
VK + SW 87.25 86.95 87.25 87.40
VK + SP 87.35 86.95 87.45 87.35
VK + SWP 87.30 87.45 87.30 88.15*
VK + CON 87.45 87.65 87.45 88.30**
VK + DW 87.35 87.50 87.30 88.50**
VK + DP 87.75* 87.20 87.35 87.75
VK + DWP 87.70* 87.30 87.65 87.80*
Table 2: Results of kernels. Here Doc denotes the whole
document of the text, Sent denotes the sentences that con-
tains subjective terms in the lexicon, Rand denotes ran-
domly selected substructures, and Sub denotes the sub-
structures defined in Section 3.2. We use ?*? and ?**? to
denote a result is better than baseline VK significantly at
p < 0.05 and p < 0.01 (sign test), respectively.
the dependency representation, POS tags can intro-
duce little new information, and will add unneces-
sary complexity. For example, given the substruc-
ture (waste (amod (JJ (tragic)))), the PT kernel will
use both (waste (amod (JJ))) and (waste (amod (JJ
(tragic)))). We can see that the former is adding no
value to the model, as the JJ tag could indicate ei-
ther positive words (e.g. good) or negative words
(e.g. tragic). In contrast, words are good indicators
for sentiment polarity.
The results in Table 2 confirm two of our hy-
potheses. Firstly, it clearly demonstrates the val-
ue of incorporating syntactic information into the
document-level sentiment classifier, as the tree k-
ernels (CON and D*) generally outperforms vector
and sequence kernels (VK and S*). More impor-
tantly, it also shows the necessity of extracting ap-
propriate substructures when using convolution ker-
nels in our task: when using the dependency kernel
(VK+DW), the result on lexicon guided substruc-
tures (Sub) outperforms the results on document,
sentence, or randomly selected substructures, with
statistical significance (p<0.05).
5 Conclusion and Future Work
We studied the impact of syntactic information on
document-level sentiment classification using con-
volution kernels, and reduced the complexity of the
kernels by extracting minimal high-impact substruc-
tures, guided by a polarity lexicon. Experiments
341
show that our method outperformed a bag-of-words
baseline with a statistically significant gain of 1.45
absolute point in accuracy.
Our research focuses on identifying and using
high-impact substructures for convolution kernels in
document-level sentiment classification. We expect
our method to be complementary with sophisticated
methods used in state-of-the-art sentiment classifica-
tion systems, which is to be explored in future work.
Acknowledgement
The authors were supported by 863 State Key
Project No. 2006AA010108, the EuroMatrixPlus F-
P7 EU project (grant No 231720) and Science Foun-
dation Ireland (Grant No. 07/CE/I1142). Part of the
research was done while Zhaopeng Tu was visiting,
and Yifan He was at the Centre for Next Generation
Localisation (www.cngl.ie), School of Computing,
Dublin City University. We thank the anonymous
reviewers for their insightful comments. We are al-
so grateful to Junhui Li for his helpful feedback.
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Languages in Social Media, pages 30?38. Association
for Computational Linguistics.
Razvan Bunescu and Raymond Mooney. 2005a. A
Shortest Path Dependency Kernel for Relation Extrac-
tion. In Proceedings of Human Language Technolo-
gy Conference and Conference on Empirical Methods
in Natural Language Processing, pages 724?731, Van-
couver, British Columbia, Canada, oct. Association for
Computational Linguistics.
Razvan Bunescu and Raymond Mooney. 2005b. Sub-
sequence Kernels for Relation Extraction. In Y Weis-
s, B Sch o lkopf, and J Platt, editors, Proceedings of
the 19th Conference on Neural Information Processing
Systems, pages 171?178, Cambridge, MA. MIT Press.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of Neural
Information Processing Systems, pages 625?632.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the COLING Workshop
on Cross-Framework and Cross-Domain Parser Eval-
uation, Manchester, August.
Paul Ferguson, Neil O?Hare, Michael Davy, Adam
Bermingham, Paraic Sheridan, Cathal Gurrin, and
Alan F. Smeaton. 2009. Exploring the use of
paragraph-level annotations for sentiment analysis of
financial blogs. In Proceedings of the Workshop on
Opinion Mining and Sentiment Analysis.
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion expres-
sion detection. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing, pages 67?76, Uppsala, Sweden, July.
Mahesh Joshi and Carolyn Penstein-Rose. 2009. Gen-
eralizing Dependency Features for Opinion Mining.
In Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 313?316, Suntec, Singapore, jul.
Suntec, Singapore.
Dan Klein and Christopher D Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan, jul. As-
sociation for Computational Linguistics.
Moshe Koppel and Jonathan Schler. 2005. Using neutral
examples for learning polarity. In Proceedings of In-
ternational Joint Conferences on Artificial Intelligence
(IJCAI) 2005, pages 1616?1616.
Steve Lawrence Kushal Dave and David Pennock. 2003.
Mining the peanut gallery: Opinion extraction and se-
mantic classification of product reviews. In Proceed-
ings of the 12th International Conference on World
Wide Web, pages 519?528, ACM. ACM.
Jingjing Liu and Stephanie Seneff. 2009. Review Sen-
timent Scoring via a Parse-and-Paraphrase Paradigm.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 161?
169, Singapore, aug. Singapore.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using word
sub-sequences and dependency sub-trees. Proceed-
ings of PAKDD?05, the 9th Pacific-Asia Conference on
Advances in Knowledge Discovery and Data Mining,
3518/2005:21?32.
Alessandro Moschitti and Silvia Quarteroni. 2008. K-
ernels on Linguistic Structures for Answer Extraction.
In Proceedings of ACL-08: HLT, Short Papers, pages
113?116, Columbus, Ohio, jun. Association for Com-
putational Linguistics.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2006. Efficient Convolution Ker-
nels for Dependency and Constituent Syntactic Trees.
In Proceedings of the 17th European Conference on
Machine Learning, pages 318?329, Berlin, Germany,
342
sep. Machine Learning: ECML 2006, 17th European
Conference on Machine Learning, Proceedings.
Vincent Ng, Sajib Dasgupta, and S M Niaz Arifin. 2006.
Examining the Role of Linguistic Knowledge Sources
in the Automatic Identification and Classification of
Reviews. In Proceedings of the COLING/ACL 2006
Main Conference Poster Sessions, pages 611?618,
Sydney, Australia, jul. Sydney, Australia.
Truc-Vien T Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1378?1387.
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Summa-
rization Based on Minimum Cuts. In Proceedings of
the 42nd Annual Meeting of the Association for Com-
putational Linguistics, pages 271?278, Barcelona, S-
pain, jun. Barcelona, Spain.
Michael Wiegand and Dietrich Klakow. 2010. Convolu-
tion Kernels for Opinion Holder Extraction. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 795?803, Los An-
geles, California, jun. Los Angeles, California.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 347?354, Vancouver, British Columbia, Cana-
da, oct. Association for Computational Linguistics.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Toward-
s answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of the 2003 Conference on
Empirical Methods in Natural Language Processing,
pages 129?136, Association for Computational Lin-
guistics. Association for Computational Linguistics.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A Composite Kernel to Extract Relations be-
tween Entities with Both Flat and Structured Features.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 825?832, Sydney, Australia, jul. Association for
Computational Linguistics.
343
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 234?240, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
CNGL-CORE: Referential Translation Machines
for Measuring Semantic Similarity
Ergun Bic?ici
Centre for Next Generation Localisation,
Dublin City University, Dublin, Ireland.
ebicici@computing.dcu.ie
Josef van Genabith
Centre for Next Generation Localisation,
Dublin City University, Dublin, Ireland.
josef@computing.dcu.ie
Abstract
We invent referential translation machines
(RTMs), a computational model for identify-
ing the translation acts between any two data
sets with respect to a reference corpus selected
in the same domain, which can be used for
judging the semantic similarity between text.
RTMs make quality and semantic similarity
judgments possible by using retrieved rele-
vant training data as interpretants for reach-
ing shared semantics. An MTPP (machine
translation performance predictor) model de-
rives features measuring the closeness of the
test sentences to the training data, the diffi-
culty of translating them, and the presence of
acts of translation involved. We view seman-
tic similarity as paraphrasing between any two
given texts. Each view is modeled by an RTM
model, giving us a new perspective on the bi-
nary relationship between the two. Our pre-
diction model is the 15th on some tasks and
30th overall out of 89 submissions in total ac-
cording to the official results of the Semantic
Textual Similarity (STS 2013) challenge.
1 Semantic Textual Similarity Judgments
We introduce a fully automated judge for semantic
similarity that performs well in the semantic textual
similarity (STS) task (Agirre et al, 2013). STS is
a degree of semantic equivalence between two texts
based on the observations that ?vehicle? and ?car?
are more similar than ?wave? and ?car?. Accurate
prediction of STS has a wide application area in-
cluding: identifying whether two tweets are talk-
ing about the same thing, whether an answer is cor-
rect by comparing it with a reference answer, and
whether a given shorter text is a valid summary of
another text.
The translation quality estimation task (Callison-
Burch et al, 2012) aims to develop quality indicators
for translations at the sentence-level and predictors
without access to a reference translation. Bicici et
al. (2013) develop a top performing machine transla-
tion performance predictor (MTPP), which uses ma-
chine learning models over features measuring how
well the test set matches the training set relying on
extrinsic and language independent features.
The semantic textual similarity (STS) task (Agirre
et al, 2013) addresses the following problem. Given
two sentences S1 and S2 in the same language, quan-
tify the degree of similarity with a similarity score,
which is a number in the range [0, 5]. The semantic
textual similarity prediction problem involves find-
ing a function f approximating the semantic textual
similarity score given two sentences, S1 and S2:
f(S1, S2) ? q(S1, S2). (1)
We approach f as a supervised learning problem
with (S1, S2, q(S1, S2)) tuples being the training
data and q(S1, S2) being the target similarity score.
We model the problem as a translation task where
one possible interpretation is obtained by translat-
ing S1 (the source to translate, S) to S2 (the target
translation, T). Since linguistic processing can re-
veal deeper similarity relationships, we also look at
the translation task at different granularities of infor-
mation: plain text (R for regular) , after lemmatiza-
tion (L), after part-of-speech (POS) tagging (P), and
after removing 128 English stop-words (S) 1. Thus,
1http://anoncvs.postgresql.org/cvsweb.cgi/pgsql/
234
we obtain 4 different perspectives on the binary re-
lationship between S1 and S2.
2 Referential Translation Machine (RTM)
Referential translation machines (RTMs) we de-
velop provide a computational model for quality and
semantic similarity judgments using retrieval of rel-
evant training data (Bic?ici and Yuret, 2011a; Bic?ici,
2011) as interpretants for reaching shared seman-
tics (Bic?ici, 2008). We show that RTM achieves very
good performance in judging the semantic similarity
of sentences and we can also use RTM to automat-
ically assess the correctness of student answers to
obtain better results (Bic?ici and van Genabith, 2013)
than the state-of-the-art (Dzikovska et al, 2012).
RTM is a computational model for identifying
the acts of translation for translating between any
given two data sets with respect to a reference cor-
pus selected in the same domain. RTM can be used
for automatically judging the semantic similarity be-
tween texts. An RTM model is based on the selec-
tion of common training data relevant and close to
both the training set and the test set where the se-
lected relevant set of instances are called the inter-
pretants. Interpretants allow shared semantics to be
possible by behaving as a reference point for simi-
larity judgments and providing the context. In semi-
otics, an interpretant I interprets the signs used to
refer to the real objects (Bic?ici, 2008). RTMs pro-
vide a model for computational semantics using in-
terpretants as a reference according to which seman-
tic judgments with translation acts are made. Each
RTM model is a data translation model between the
instances in the training set and the test set. We use
the FDA (Feature Decay Algorithms) instance se-
lection model for selecting the interpretants (Bic?ici
and Yuret, 2011a) from a given corpus, which can
be monolingual when modeling paraphrasing acts,
in which case the MTPP model (Section 2.1) is built
using the interpretants themselves as both the source
and the target side of the parallel corpus. RTMs map
the training and test data to a space where translation
acts can be identified. We view that acts of transla-
tion are ubiquitously used during communication:
Every act of communication is an act of
translation (Bliss, 2012).
src/backend/snowball/stopwords/
Translation need not be between different languages
and paraphrasing or communication also contain
acts of translation. When creating sentences, we use
our background knowledge and translate informa-
tion content according to the current context.
Given a training set train, a test set test, and
some monolingual corpus C, preferably in the same
domain as the training and test sets, the RTM steps
are:
1. T = train ? test.
2. select(T, C)? I
3. MTPP(I,train)? Ftrain
4. MTPP(I,test)? Ftest
5. learn(M,Ftrain)?M
6. predict(M,Ftest)? q?
Step 2 selects the interpretants, I, relevant to the
instances in the combined training and test data.
Steps 3, 4 use I to map train and test to a new
space where similarities between translation acts can
be derived more easily. Step 5 trains a learning
model M over the training features, Ftrain, and
Step 6 obtains the predictions. RTM relies on the
representativeness of I as a medium for building
translation models for translating between train
and test.
Our encouraging results in the STS task provides
a greater understanding of the acts of translation we
ubiquitously use when communicating and how they
can be used to predict the performance of transla-
tion, judging the semantic similarity between text,
and evaluating the quality of student answers. RTM
and MTPP models are not data or language specific
and their modeling power and good performance are
applicable across different domains and tasks. RTM
expands the applicability of MTPP by making it fea-
sible when making monolingual quality and simi-
larity judgments and it enhances the computational
scalability by building models over smaller but more
relevant training data as interpretants.
2.1 The Machine Translation Performance
Predictor (MTPP)
In machine translation (MT), pairs of source and tar-
get sentences are used for training statistical MT
(SMT) models. SMT system performance is af-
fected by the amount of training data used as well
235
as the closeness of the test set to the training set.
MTPP (Bic?ici et al, 2013) is a top performing ma-
chine translation performance predictor, which uses
machine learning models over features measuring
how well the test set matches the training set to pre-
dict the quality of a translation without using a ref-
erence translation. MTPP measures the coverage of
individual test sentence features and syntactic struc-
tures found in the training set and derives feature
functions measuring the closeness of test sentences
to the available training data, the difficulty of trans-
lating the sentence, and the presence of acts of trans-
lation for data transformation.
2.2 MTPP Features for Translation Acts
MTPP uses n-gram features defined over text or
common cover link (CCL) (Seginer, 2007) struc-
tures as the basic units of information over which
similarity calculations are made. Unsupervised
parsing with CCL extracts links from base words
to head words, which allow us to obtain structures
representing the grammatical information instanti-
ated in the training and test data. Feature functions
use statistics involving the training set and the test
sentences to determine their closeness. Since they
are language independent, MTPP allows quality es-
timation to be performed extrinsically. Categories
for the 289 features used are listed below and their
detailed descriptions are presented in (Bic?ici et al,
2013) where the number of features are given in {#}.
? Coverage {110}: Measures the degree to
which the test features are found in the train-
ing set for both S ({56}) and T ({54}).
? Synthetic Translation Performance {6}: Calcu-
lates translation scores achievable according to
the n-gram coverage.
? Length {4}: Calculates the number of words
and characters for S and T and their ratios.
? Feature Vector Similarity {16}: Calculates the
similarities between vector representations.
? Perplexity {90}: Measures the fluency of the
sentences according to language models (LM).
We use both forward ({30}) and backward
({15}) LM based features for S and T.
? Entropy {4}: Calculates the distributional sim-
ilarity of test sentences to the training set.
? Retrieval Closeness {24}: Measures the de-
gree to which sentences close to the test set are
found in the training set.
? Diversity {6}: Measures the diversity of co-
occurring features in the training set.
? IBM1 Translation Probability {16}: Calculates
the translation probability of test sentences us-
ing the training set (Brown et al, 1993).
? Minimum Bayes Retrieval Risk {4}: Calculates
the translation probability for the translation
having the minimum Bayes risk among the re-
trieved training instances.
? Sentence Translation Performance {3}: Calcu-
lates translation scores obtained according to
q(T,R) using BLEU (Papineni et al, 2002),
NIST (Doddington, 2002), or F1 (Bic?ici and
Yuret, 2011b) for q.
? Character n-grams {4}: Calculates the cosine
between the character n-grams (for n=2,3,4,5)
obtained for S and T (Ba?r et al, 2012).
? LIX {2}: Calculates the LIX readability
score (Wikipedia, 2013; Bjo?rnsson, 1968) for
S and T. 2
3 Experiments
STS contains sentence pairs from news headlines
(headlines), sense definitions from semantic lexical
resources (OnWN is from OntoNotes (Pradhan et
al., 2007) and WordNet (Miller, 1995) and FNWN is
from FrameNet (Baker et al, 1998) and WordNet),
and statistical machine translation (SMT) (Agirre et
al., 2013). STS challenge results are evaluated with
the Pearson?s correlation score (r).
The test set contains 2250 (S1, S2) sentence pairs
with 750, 561, 189, and 750 sentences from each
type respectively. The training set contains 5342
sentence pairs with 1500 each from MSRpar and
MSRvid (Microsoft Research paraphrase and video
description corpus (Agirre et al, 2012)), 1592 from
SMT, and 750 from OnWN.
3.1 RTM Models
We obtain CNGL results for the STS task as fol-
lows. For each perspective described in Section 1,
we build an RTM model. Each RTM model views
the STS task from a different perspective using the
289 features extracted dependent on the interpre-
tants using MTPP. We extract the features both on
2LIX=AB + C
100
A , where A is the number of words, C is
words longer than 6 characters, B is words that start or end with
any of ?.?, ?:?, ?!?, ??? similar to (Hagstro?m, 2012).
236
r R P L S R
+
P
R
+
L
R
+
S
L
+
P
L
+
S
L
+
S
T
L
R
+
P+
L
R
+
P+
S
L
+
P+
S
L
+
P+
S
T
L
R
+
P+
L
+
S
R
+
P+
L
+
S
T
L
S1 ? S2
RR .7904 .7502 .8200 .7788 .8074 .8232 .8101 .8247 .8218 .8509 .8266 .8172 .8304 .8530 .8323 .8499
SVR .8311 .8060 .8443 .8330 .8404 .8517 .8498 .8501 .8593 .8556 .8496 .8422 .8586 .8579 .8527 .8564
S2 ? S1
RR .7922 .7651 .8169 .7891 .8064 .8196 .8136 .8219 .8257 .8257 .8226 .8164 .8284 .8284 .8313 .8324
SVR .8308 .8165 .8407 .8302 .8361 .8506 .8467 .8510 .8567 .8567 .8525 .8460 .8588 .8588 .8575 .8574
S1  S2
RR .8079 .787 .8279 .8101 .8216 .8333 .8275 .8346 .8375 .8409 .8361 .8312 .8412 .8434 .8432 .844
SVR .8397 .8237 .8554 .841 .8432 .857 .851 .8557 .8605 .8626 .8505 .8505 .8591 .8622 .8602 .8588
Table 1: CV performance on the training set with tuning. Underlined are the settings we use in our submissions. RTM
models in directions S1 ? S2, S2 ? S1, and the bi-directional models S1  S2 are displayed.
the training set and the test set. The training cor-
pus used is the English side of an out-of-domain
corpus on European parliamentary discussions, Eu-
roparl (Callison-Burch et al, 2012) 3. In-domain
corpora are likely to improve the performance. We
use the Stanford POS tagger (Toutanova et al, 2003)
to obtain the perspectives P and L. We use the train-
ing corpus to build a 5-gram target LM.
We use ridge regression (RR) and support vec-
tor regression (SVR) with RBF kernel (Smola and
Scho?lkopf, 2004). Both of these models learn a re-
gression function using the features to estimate a nu-
merical target value. The parameters that govern the
behavior of RR and SVR are the regularization ?
for RR and the C, , and ? parameters for SVR. At
testing time, the predictions are bounded to obtain
scores in the range [0, 5]. We perform tuning on a
subset of the training set separately for each RTM
model and optimize against the performance evalu-
ated with R2, the coefficient of determination.
We do not build a separate model for different
types of sentences and instead use all of the train-
ing set for building a large prediction model. We
also use transductive learning since using only the
relevant training data for training can improve the
performance (Bic?ici, 2011). Transductive learning
is performed at the sentence level where for each test
instance, we select 1250 relevant training instances
using the cosine similarity metric over the feature
vectors and build an individual model for the test in-
stance and predict the similarity score.
3We use WMT?13 corpora from www.statmt.org/wmt13/.
3.2 Training Results
Table 1 lists the 10-fold cross-validation (CV) re-
sults on the training set for RR and SVR for differ-
ent RTM systems using optimized parameters. As
we combine different perspectives, the performance
improves and we use the L+S with SVR for run 1
(LSSVR), L+P+S with SVR for run 2 (LPSSVR),
and L+P+S with SVR using transductive learning
for run 3 (LPSSVRTL) all in the translation direc-
tion S1 ? S2. Lemmatized RTM, L, performs the
best among the individual perspectives. We also
build RTM models in the direction S2 ? S1, which
gives similar results. The last main row combines
them to obtain the bi-directional results, S1  S2,
which improves the performance. Each additional
perspective adds another 289 features to the repre-
sentation and the bi-directional results double the
number of features. Thus, S1  S2 L+P+S is us-
ing 1734 features.
3.3 STS Challenge Results
Table 2 presents the STS challenge r and ranking
results containing our CNGL submissions, the best
system result, and the mean results over all submis-
sions. There were 89 submissions from 35 compet-
ing systems (Agirre et al, 2013). The results are
ranked according to the mean r obtained. We also
include the mean result over all of the submissions
and its corresponding rank.
According to the official results, CNGL-LSSVR
is the 30th system from the top based on the mean r
obtained and CNGL-LPSSVR is 15th according to
the results on OnWN out of 89 submissions in total.
237
System head OnWN FNWN SMT mean rank
CNGL-LSSVR .6552 .6943 .2016 .3005 .5086 30
CNGL-LPSSVRTL .6385 .6756 .1823 .3098 .4998 33
CNGL-LPSSVR .6510 .6971 .1180 .2861 .4961 36
UMBC-EB.-PW .7642 .7529 .5818 .3804 .6181 1
mean .6071 .5089 .2906 .3004 .4538 57
Table 2: STS challenge r and ranking results ranked ac-
cording to the mean r obtained. head is headlines and
mean is the mean of all submissions.
CNGL submissions perform unexpectedly low in the
FNWN task and only slightly better than the average
in the SMT task. The lower performance is likely to
be due to using an out-of-domain corpus for building
the RTM models and it may also be due to using and
optimizing a single model for all types of tasks.
3.4 Bi-directional RTM Models
The STS task similarity score is directional invari-
ant: q(S1, S2) = q(S2, S1). We develop RTM mod-
els in the reverse direction and obtain bi-directional
RTM models by combining both. Table 3 lists the
bi-directional results on the STS challenge test set
after tuning, which shows that slight improvement in
the scores are possible when compared with Table 2.
Transductive learning improves the performance in
general. We also compare with the performance ob-
tained when combining uni-directional models with
mean, min, or max functions. Taking the minimum
performs better than other combination approaches
and can achieve r = 0.5129 with TL. One can also
take the individual confidence scores obtained for
each score when combining scores.
4 Conclusion
Referential translation machines provide a clean
and intuitive computational model for automatically
measuring semantic similarity by measuring the acts
of translation involved and achieve to be the 15th on
some tasks and 30th overall in the STS challenge out
of 89 submissions in total. RTMs make quality and
semantic similarity judgments possible based on the
retrieval of relevant training data as interpretants for
reaching shared semantics.
System head OnWN FNWN SMT mean
LS
mean .6552 .6943 .2016 .3005 .5086
mean TL .6397 .6808 .1776 .3147 .5028
min .6512 .6947 .2003 .2984 .5066
min TL .6416 .6853 .1903 .3143 .5055
max .6669 .6680 .1867 .2737 .4958
max TL .6493 .6805 .1846 .3127 .5059
S1  S2 .6388 .6695 .1667 .2999 .4938
S1  S2 TL .6285 .6686 .0918 .2931 .4816
LPS
mean .6510 .6971 .1179 .2861 .4961
mean TL .6524 .6918 .1940 .3176 .5121
min .6608 .6953 .1704 .2922 .5053
min TL .6509 .6864 .1792 .3156 .5084
max .6588 .6800 .1355 .2868 .4961
max TL .6493 .6805 .1846 .3127 .5059
S1  S2 .6251 .6843 .0677 .2994 .4845
S1  S2 TL .6370 .6978 .0951 .2980 .4936
RLPS
mean .6517 .7136 .1002 .2880 .4996
mean TL .6383 .6841 .2434 .3063 .5059
min .6615 .7099 .1644 .2877 .5072
min TL .6606 .6987 .1972 .3059 .5129
max .6589 .7019 .0995 .2935 .5008
max TL .6362 .6896 .2044 .3153 .5063
S1  S2 .6300 .7011 .0817 .2798 .4850
S1  S2 TL .6321 .6956 .1995 .3128 .5052
Table 3: Bi-directional STS challenge r and ranking re-
sults ranked according to the mean r obtained. We com-
bine the two directions by taking the mean, min, or the
max or use the bi-directional RTM model S1  S2.
Acknowledgments
This work is supported in part by SFI (07/CE/I1142)
as part of the Centre for Next Generation Locali-
sation (www.cngl.ie) at Dublin City University and
in part by the European Commission through the
QTLaunchPad FP7 project (No: 296347). We also
thank the SFI/HEA Irish Centre for High-End Com-
puting (ICHEC) for the provision of computational
facilities and support.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385?393,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
238
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th International
Conference on Computational Linguistics - Volume 1,
ACL ?98, pages 86?90, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual simi-
larity by combining multiple content similarity mea-
sures. In *SEM 2012: The First Joint Conference
on Lexical and Computational Semantics ? Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
2012), pages 435?440, Montre?al, Canada, 7-8 June.
Association for Computational Linguistics.
Ergun Bic?ici and Josef van Genabith. 2013. CNGL:
Grading student answers by acts of translation. In
*SEM 2013: The First Joint Conference on Lexical
and Computational Semantics and Proceedings of the
Seventh International Workshop on Semantic Evalua-
tion (SemEval 2013), Atlanta, Georgia, USA, 14-15
June. Association for Computational Linguistics.
Ergun Bic?ici and Deniz Yuret. 2011a. Instance selec-
tion for machine translation using feature decay al-
gorithms. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 272?283, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Ergun Bic?ici and Deniz Yuret. 2011b. RegMT system for
machine translation, system combination, and evalua-
tion. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation, pages 323?329, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Ergun Bic?ici, Declan Groves, and Josef van Genabith.
2013. Predicting sentence translation quality using ex-
trinsic and language independent features. Machine
Translation.
Ergun Bic?ici. 2011. The Regression Model of Machine
Translation. Ph.D. thesis, Koc? University. Supervisor:
Deniz Yuret.
Ergun Bic?ici. 2008. Consensus ontologies in socially
interacting multiagent systems. Journal of Multiagent
and Grid Systems.
Carl Hugo Bjo?rnsson. 1968. La?sbarhet. Liber.
Chris Bliss. 2012. Comedy is transla-
tion, February. http://www.ted.com/talks/
chris bliss comedy is translation.html.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical machine
translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 10?
51, Montre?al, Canada, June. Association for Compu-
tational Linguistics.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, pages 138?145, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback
for explanation questions: A dataset and baselines.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 200?210, Montre?al, Canada, June. Association
for Computational Linguistics.
Kenth Hagstro?m. 2012. Swedish readability calcula-
tor. https://github.com/keha76/Swedish-Readability-
Calculator.
George A. Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39?41,
November.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Sameer S. Pradhan, Eduard H. Hovy, Mitchell P. Mar-
cus, Martha Palmer, Lance A. Ramshaw, and Ralph M.
Weischedel. 2007. Ontonotes: a unified relational
semantic representation. Int. J. Semantic Computing,
1(4):405?419.
Yoav Seginer. 2007. Learning Syntactic Structure. Ph.D.
thesis, Universiteit van Amsterdam.
Alex J. Smola and Bernhard Scho?lkopf. 2004. A tutorial
on support vector regression. Statistics and Comput-
ing, 14(3):199?222, August.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
239
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 173?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Wikipedia. 2013. Lix. http://en.wikipedia.org/wiki/LIX.
240
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 585?591, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
CNGL: Grading Student Answers by Acts of Translation
Ergun Bic?ici
Centre for Next Generation Localisation,
Dublin City University, Dublin, Ireland.
ebicici@computing.dcu.ie
Josef van Genabith
Centre for Next Generation Localisation,
Dublin City University, Dublin, Ireland.
josef@computing.dcu.ie
Abstract
We invent referential translation machines
(RTMs), a computational model for identify-
ing the translation acts between any two data
sets with respect to a reference corpus se-
lected in the same domain, which can be used
for automatically grading student answers.
RTMs make quality and semantic similarity
judgments possible by using retrieved rele-
vant training data as interpretants for reach-
ing shared semantics. An MTPP (machine
translation performance predictor) model de-
rives features measuring the closeness of the
test sentences to the training data, the diffi-
culty of translating them, and the presence of
acts of translation involved. We view question
answering as translation from the question to
the answer, from the question to the reference
answer, from the answer to the reference an-
swer, or from the question and the answer to
the reference answer. Each view is modeled
by an RTM model, giving us a new perspective
on the ternary relationship between the ques-
tion, the answer, and the reference answer. We
show that all RTM models contribute and a
prediction model based on all four perspec-
tives performs the best. Our prediction model
is the 2nd best system on some tasks according
to the official results of the Student Response
Analysis (SRA 2013) challenge.
1 Automatically Grading Student Answers
We introduce a fully automated student answer
grader that performs well in the student response
analysis (SRA) task (Dzikovska et al, 2013) and es-
pecially well in tasks with unseen answers. Auto-
matic grading can be used for assessing the level of
competency for students and estimating the required
tutoring effort in e-learning platforms. It can also
be used to adapt questions according to the average
student performance. Low scored topics can be dis-
cussed further in classrooms, enhancing the overall
coverage of the course material.
The quality estimation task (QET) (Callison-
Burch et al, 2012) aims to develop quality indica-
tors for translations at the sentence-level and pre-
dictors without access to the reference. Bicici et
al. (2013) develop a top performing machine transla-
tion performance predictor (MTPP), which uses ma-
chine learning models over features measuring how
well the test set matches the training set relying on
extrinsic and language independent features.
The student response analysis (SRA)
task (Dzikovska et al, 2013) addresses the fol-
lowing problem. Given a question, a known correct
reference answer, and a student answer, assess the
correctness of the student?s answer. The student
answers are categorized as correct, partially correct
incomplete, contradictory, irrelevant, or non do-
main, in the 5-way task; as correct, contradictory,
or incorrect in the 3-way task; and as correct or
incorrect in the 2-way task.
The student answer correctness prediction prob-
lem involves finding a function f approximating the
student answer correctness given the question (Q),
the answer (A), and the reference answer (R):
f(Q,A,R) ? q(A,R). (1)
We approach f as a supervised learning problem
with (Q, A, R, q(A,R)) tuples being the training
585
data and q(A,R) being the target correctness score.
We model the problem as a translation task where
one possible interpretation is translating Q (source
to translate, S) to R (target translation, T) and evalu-
ating with A (as reference target, RT) (QRA). Since
the information appearing in the question may be re-
peated in the reference answer or may be omitted in
the student answer, it also makes sense to concate-
nate Q and A when translating to R (QARQA). We
obtain 4 different perspectives on the ternary rela-
tionship between Q, A, and R depending on how we
model their relationship as an instance of translation:
QAR : S = Q, T = A, RT = R.
QRA : S = Q, T = R, RT = A.
ARA : S = A, T = R, RT = A.
QARQA : S = Q+A, T = R, RT = Q+A.
2 The Machine Translation Performance
Predictor (MTPP)
In machine translation (MT), pairs of source and tar-
get sentences are used for training statistical MT
(SMT) models. SMT system performance is af-
fected by the amount of training data used as well
as the closeness of the test set to the training set.
MTPP (Bic?ici et al, 2013) is a top performing ma-
chine translation performance predictor, which uses
machine learning models over features measuring
how well the test set matches the training set to pre-
dict the quality of a translation without using a ref-
erence translation. MTPP measures the coverage of
individual test sentence features and syntactic struc-
tures found in the training set and derives feature
functions measuring the closeness of test sentences
to the available training data, the difficulty of trans-
lating the sentence, and the presence of acts of trans-
lation involved.
Features for Translation Acts
MTPP uses n-gram features defined over text or
common cover link (CCL) (Seginer, 2007) struc-
tures as the basic units of information over which
similarity calculations are made. Unsupervised
parsing with CCL extracts links from base words
to head words, which allow us to obtain structures
representing the grammatical information instanti-
ated in the training and test data. Feature functions
use statistics involving the training set and the test
sentences to determine their closeness. Since they
are language independent, MTPP allows quality es-
timation to be performed extrinsically. Categories
for the 283 features used are listed below and their
detailed descriptions are presented in (Bic?ici et al,
2013) where the number of features are given in {#}.
? Coverage {110}: Measures the degree to
which the test features are found in the train-
ing set for both S ({56}) and T ({54}).
? Synthetic Translation Performance {6}: Calcu-
lates translation scores achievable according to
the n-gram coverage.
? Length {4}: Calculates the number of words
and characters for S and T and their ratios.
? Feature Vector Similarity {16}: Calculates the
similarities between vector representations.
? Perplexity {90}: Measures the fluency of the
sentences according to language models (LM).
We use both forward ({30}) and backward
({15}) LM based features for S and T.
? Entropy {4}: Calculates the distributional sim-
ilarity of test sentences to the training set.
? Retrieval Closeness {24}: Measures the de-
gree to which sentences close to the test set are
found in the training set.
? Diversity {6}: Measures the diversity of co-
occurring features in the training set.
? IBM1 Translation Probability {16}: Calculates
the translation probability of test sentences us-
ing the training set (Brown et al, 1993).
? Minimum Bayes Retrieval Risk {4}: Calculates
the translation probability for the translation
having the minimum Bayes risk among the re-
trieved training instances.
? Sentence Translation Performance {3}: Calcu-
lates translation scores obtained according to
q(T,R) using BLEU (Papineni et al, 2002),
NIST (Doddington, 2002), or F1 (Bic?ici and
Yuret, 2011b) for q.
3 Referential Translation Machine (RTM)
Referential translation machines (RTMs) we de-
velop provide a computational model for quality and
semantic similarity judgments using retrieval of rel-
evant training data (Bic?ici and Yuret, 2011a; Bic?ici,
2011) as interpretants for reaching shared seman-
tics (Bic?ici, 2008). We show that RTM achieves
586
very good performance in judging the semantic sim-
ilarity of sentences (Bic?ici and van Genabith, 2013)
and we can also use RTM to automatically assess
the correctness of student answers to obtain better
results than the baselines proposed by (Dzikovska et
al., 2012), which achieve the best performance on
some tasks (Dzikovska et al, 2013).
RTM is a computational model for identifying the
acts of translation for translating between any given
two data sets with respect to a reference corpus se-
lected in the same domain. RTM can be used for
automatically grading student answers. An RTM
model is based on the selection of common train-
ing data relevant and close to both the training set
and the test set where the selected relevant set of
instances are called the interpretants. Interpretants
allow shared semantics to be possible by behaving
as a reference point for similarity judgments and
providing the context. In semiotics, an interpretant
I interprets the signs used to refer to the real ob-
jects (Bic?ici, 2008). RTMs provide a model for com-
putational semantics using interpretants as a refer-
ence according to which semantic judgments with
translation acts are made. Each RTM model is a data
translation model between the instances in the train-
ing set and the test set. We use the FDA (Feature De-
cay Algorithms) instance selection model for select-
ing the interpretants (Bic?ici and Yuret, 2011a) from a
given corpus, which can be monolingual when mod-
eling paraphrasing acts, in which case the MTPP
model is built using the interpretants themselves as
both the source and the target side of the parallel cor-
pus. RTMs map the training and test data to a space
where translation acts can be identified. We view
that acts of translation are ubiquitously used during
communication:
Every act of communication is an act of
translation (Bliss, 2012).
Translation need not be between different languages
and paraphrasing or communication also contain
acts of translation. When creating sentences, we use
our background knowledge and translate informa-
tion content according to the current context.
Given a training set train, a test set test, and
some monolingual corpus C, preferably in the same
domain as the training and test sets, the RTM steps
are:
1. T = train ? test.
2. select(T, C)? I
3. MTPP(I,train)? Ftrain
4. MTPP(I,test)? Ftest
Step 2 selects the interpretants, I, relevant to the
instances in the combined training and test data.
Steps 3 and 4 use I to map train and test to
a new space where similarities between the transla-
tion acts can be derived more easily. RTM relies on
the representativeness of I as a medium for building
translation models for translating between train
and test.
Our encouraging results in the SRA task provides
a greater understanding of the acts of translation we
ubiquitously use when communicating and how they
can be used to predict the performance of trans-
lation, judging the semantic similarity of text, and
evaluating the quality of student answers. RTM and
MTPP models are not data or language specific and
their modeling power and good performance are ap-
plicable across different domains and tasks. RTM
expands the applicability of MTPP by making it fea-
sible when making monolingual quality and simi-
larity judgments and it enhances the computational
scalability by building models over smaller but more
relevant training data as interpretants.
4 Experiments
SRA involves the prediction on Beetle (student
interactions when learning conceptual knowledge
in the basic electricity and electronics domain)
and SciEntsBank (science assessment questions)
datasets. SciEntsBank is harder due to contain-
ing questions from multiple domains (Dzikovska
et al, 2012). SRA challenge results are eval-
uated with the weighted average F1, Fw1 =
1
N
?
c?C NcF1(c) and the macro average F1, F
m
1 =
1
|C|
?
c?C F1(c) (Dzikovska et al, 2012).
The lexical baseline system is based on measures
of lexical overlap using 4 features: the number of
overlapping words, F1, Lesk (Lesk, 1986), and co-
sine scores over the words when comparing A and
R ({4}) and Q and R ({4}). Lesk score is calculated
as: L(A,R) =
?
p?M |p|
2/(|A||R|), where M con-
tains the maximal overlapping phrases that match in
587
A and R and |p| is the length of a phrase 1. This lex-
ical baseline is highly competitive: no submission
performed better in the 2-way Beetle unseen ques-
tions task.
4.1 RTM Models
We obtain CNGL results for the SRA task as fol-
lows. For each perspective described in Section 1,
we build an RTM model. Each RTM model views
the SRA task from a different perspective using the
283 features extracted dependent on the interpre-
tants using MTPP. We extract the features both on
the training set of 4155 and the test set of 1258 (Q,
A, R) sentence triples for the Beetle task and the
training set of 5251 and the test set of 5835 (Q, A,
R) sentence triples for the SciEntsBank task. The
addition of lexical overlap baseline features slightly
helps. We use the best reference answer if the refer-
ence answer is not identified in the training set.
The training corpus used is the English side of
an out-of-domain corpus on European parliamen-
tary discussions, Europarl (Callison-Burch et al,
2012) 2, to which we also add the unique sentences
from R. In-domain corpora are likely to improve the
performance. We do not perform any linguistic pro-
cessing or use other external resources. We use only
extrinsic features, or features that are ignorant of any
information intrinsic to, and dependent on, a given
language or domain. We use the training corpus to
build a 5-gram target LM. We use ridge regression
(RR) and support vector regression (SVR) with RBF
kernel (Smola and Scho?lkopf, 2004). Both of these
models learn a regression function using the features
to estimate a numerical target value. The parameters
that govern the behavior of RR and SVR are the reg-
ularization ? for RR and the C, , and ? parameters
for SVR. At testing time, the predictions are bound
so as to have scores in the range [0, 1], [0, 2], or [0, 4]
and rounded for finding the predicted category.
4.2 Training Results
Table 1 lists the 10-fold cross-validation (CV) re-
sults on the training set for RR and SVR for dif-
ferent RTM systems without the parameter op-
timization. As we combine different perspec-
tives, the performance improves and we use the
1http://search.cpan.org/dist/Text-Similarity/
2We use WMT?13 corpora from www.statmt.org/wmt13/.
QAR+QRA+ARA+QARQA system for our submis-
sions using RR for run 1, SVR for run 2. ARA per-
forms the best among individual perspectives. Each
additional perspective adds another 283 features to
the representation.
Fm1 / F
w
1 Beetle SciEntsBank
Model RR SVR RR SVR
QAR .38/.49 .45/.57 .21/.30 .28/.36
QRA .33/.50 .33/.53 .22/.31 .29/.42
ARA .45/.54 .50/.60 .21/.30 .30/.38
QARQA .35/.50 .40/.58 .20/.27 .27/.40
QAR+ARA .47/.55 .49/.61 .26/.36 .32/.39
QAR+ARA+QARQA .48/.57 .49/.62 .31/.38 .29/.40
QAR+QRA+ARA+QARQA .48/.56 .48/.61 .31/.38 .29/.40
Table 1: Performance on the training set without tuning.
We perform tuning on a subset of the Beetle
and SciEntsBank datasets separately after including
the baseline lexical overlap features and optimize
against the performance evaluated withR2, the coef-
ficient of determination. SVR performance is given
in Table 2. The CNGL system significantly outper-
forms the lexical overlap baseline in all tasks for
Beetle and in the 2-way task for SciEntsBank. For
3-way and 5-way, CNGL performs slightly better.
Fm1 / F
w
1 Beetle SciEntsBank
System 2 3 5 2 3 5
Lexical .74/.75 .53/.56 .46/.53 .61/.64 .43/.55 .29/.41
CNGL .84/.84 .61/.63 .55/.63 .74/.75 .47/.56 .30/.41
Table 2: Optimized SVR results vs. lexical overlap base-
line on the training set for 2-way, 3-way, or 5-way tasks.
4.3 SRA Challenge Results
The SRA task test set alo contains instances that be-
long to unseen questions (uQ) and unseen domains
(uD), which make it harder to predict. The train-
ing data provided for the task correspond to learning
with unseen answers (uA). Table 3 presents the SRA
challenge results containing the lexical overlap, our
CNGL SVR submission (RR is slightly worse), and
the maximum and mean results 3.
According to the official results, CNGL SVR is
the 2nd best system based on 5-way evaluation (4th
3Max is not the performance of the best performing system
but the maximum result obtained for each metric and subtask.
588
Fm1 / F
w
1 Beetle SciEntsBank
System uA uQ uA uQ uD
2
Lexical .80/.79 .74/.72 .64/.62 .65/.63 .66/.65
CNGL .80/.81 .67/.68 .55/.57 .56/.58 .56/.57
Mean .71/.72 .61/.62 .64/.66 .60/.62 .61/.63
Max .84/.84 .72/.73 .77/.77 .74/.74 .70/.71
3
Lexical .55/.58 .48/.50 .40/.52 .39/.52 .42/.55
CNGL .57/.59 .45/.47 .33/.38 .31/.37 .31/.36
Mean .54/.55 .41/.42 .48/.56 .39/.51 .39/.51
Max .72/.73 .58/.60 .65/.71 .47/.63 .49/.62
5
Lexical .42/.48 .41/.46 .30/.44 .26/.40 .25/.40
CNGL .43/.55 .38/.47 .20/.27 .21/.30 .22/.29
Mean .44/.51 .34/.40 .34/.46 .24/.38 .26/.37
Max .62/.70 .55/.61 .48/.64 .31/.49 .38/.47
Table 3: SRA challenge results: CNGL SVR submission,
the lexical overlap baseline, and the maximum and mean
results for 2-way, 3-way, or 5-way tasks. uA, uQ, and uD
correspond to unseen answers, questions, and domains.
result overall) and the 3rd best system based on 2-
way and 3-way evaluation (5th result overall) on the
uQ Beetle task. The SVR model performs better
than the lexical baseline and the mean result in the
Beetle task but performs worse in the SciEntsBank.
The lower performance is likely to be due to using an
out-of-domain training corpus for building the RTM
models and on the uQ and uD tasks, it may also be
due to optimizing on the uA task only. The lower
performance in SciEntsBank is also due to multiple
question domains (Dzikovska et al, 2012).
SVR Beetle SciEntsBank
Fw1 2 3 5 2 3 5
(a) QAR+ARA .86 .66 .64 .77 .56 .42
(b) QAR+ARA+QARQA .86 .66 .65 .77 .57 .45
(c) QAR+QRA+ARA+QARQA .85 .64 .63 .77 .58 .45
Fm1 2 3 5 2 3 5
(a) QAR+ARA .86 .64 .55 .76 .47 .34
(b) QAR+ARA+QARQA .85 .64 .55 .76 .48 .36
(c) QAR+QRA+ARA+QARQA .85 .62 .54 .76 .49 .35
Table 4: Improved SVR performance on the training set
with tuning for 2-way, 3-way, or 5-way tasks.
4.4 Improved RTM Models
We improve the RTM model with the expansion of
our representation by adding the following features:
? Character n-grams {4}: Calculates the cosine
between the character n-grams (for n=2,3,4,5)
obtained for S and T (Ba?r et al, 2012).
? LIX {2}: Calculates the LIX readability
score (Wikipedia, 2013; Bjo?rnsson, 1968) for
S and T. 4
Table 4 lists the improved results on the training set
after tuning, which shows about 0.04 increase in all
scores when compared with Table 1 and Table 2.
Fm1 /F
w
1 Beetle SciEntsBank
Model uA uQ uA uQ uD
2
(a) .81/.82 .70/.71 .55/.57 .58/.58 .56/.57
(b) .80/.81 .71/.72 .69/.70 .54/.56 .56/.58
(c) .79/.79 .70/.71 .60/.59 .57/.58 .55/.57
3
(a) .59/.61 .48/.49 .26/.34 .34/.40 .26/.32
(b) .60/.62 .47/.48 .36/.43 .31/.38 .29/.34
(c) .58/.60 .46/.48 .41/.48 .30/.39 .29/.34
5
(a) .47/.56 .37/.45 .19/.22 .22/.33 .22/.29
(b) .43/.56 .36/.45 .26/.37 .23/.33 .21/.30
(c) .42/.52 .40/.48 .27/.39 .24/.33 .20/.30
Table 5: Improved SVR results on the SRA task test set.
Fm1 /F
w
1 SciEntsBank
Model uA uQ uD
2
(a) .56/.57 .54/.55 .53/.55
(b) .57/.58 .53/.54 .56/.57
(c) .57/.58 .55/.57 .57/.59
3
(a) .36/.45 .33/.44 .39/.49
(b) .35/.40 .36/.44 .39/.48
(c) .37/.46 .36/.48 .40/.50
5
(a) .24/.34 .23/.33 .26/.39
(b) .24/.36 .25/.38 .26/.38
(c) .24/.36 .21/.32 .28/.39
Table 6: Improved TREE results on the SRA task test set.
Table 5 presents the improved SVR results on the
SRA task test set, which shows about 0.03 increase
in all scores when compared with Table 3. SVR be-
comes the 2nd best system and 2nd best result in
2-way evaluation and the 3rd best system from the
top based on 2-way and 3-way evaluation (5th result
overall) on the uQ Beetle task.
4LIX=AB + C
100
A , where A is the number of words, C is
words longer than 6 characters, B is words that start or end with
any of ?.?, ?:?, ?!?, ??? similar to (Hagstro?m, 2012).
589
We observe that decision tree regression (Hastie
et al, 2009) (TREE) generalizes to uQ and uD do-
mains better than the RR or SVR models especially
in the SciEntsBank corpus. Table 6 presents TREE
results on the SRA SciEntsBank test set, which
shows significant increase in uQ and uD tasks when
compared with Table 5.
5 Conclusion
Referential translation machines provide a clean
and intuitive computational model for automatically
grading student answers by measuring the acts of
translation involved and achieve to be the 2nd best
system on some tasks in the SRA challenge. RTMs
make quality and semantic similarity judgments
possible based on the retrieval of relevant training
data as interpretants for reaching shared semantics.
Acknowledgments
This work is supported in part by SFI (07/CE/I1142)
as part of the Centre for Next Generation Locali-
sation (www.cngl.ie) at Dublin City University and
in part by the European Commission through the
QTLaunchPad FP7 project (No: 296347). We also
thank the SFI/HEA Irish Centre for High-End Com-
puting (ICHEC) for the provision of computational
facilities and support.
References
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual simi-
larity by combining multiple content similarity mea-
sures. In *SEM 2012: The First Joint Conference
on Lexical and Computational Semantics ? Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
2012), pages 435?440, Montre?al, Canada, 7-8 June.
Association for Computational Linguistics.
Ergun Bic?ici and Josef van Genabith. 2013. CNGL-
CORE: Referential translation machines for measur-
ing semantic similarity. In *SEM 2013: The First Joint
Conference on Lexical and Computational Semantics,
Atlanta, Georgia, USA, 13-14 June. Association for
Computational Linguistics.
Ergun Bic?ici and Deniz Yuret. 2011a. Instance selec-
tion for machine translation using feature decay al-
gorithms. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 272?283, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Ergun Bic?ici and Deniz Yuret. 2011b. RegMT system for
machine translation, system combination, and evalua-
tion. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation, pages 323?329, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Ergun Bic?ici, Declan Groves, and Josef van Genabith.
2013. Predicting sentence translation quality using ex-
trinsic and language independent features. Machine
Translation.
Ergun Bic?ici. 2011. The Regression Model of Machine
Translation. Ph.D. thesis, Koc? University. Supervisor:
Deniz Yuret.
Ergun Bic?ici. 2008. Consensus ontologies in socially
interacting multiagent systems. Journal of Multiagent
and Grid Systems.
Carl Hugo Bjo?rnsson. 1968. La?sbarhet. Liber.
Chris Bliss. 2012. Comedy is transla-
tion, February. http://www.ted.com/talks/
chris bliss comedy is translation.html.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical machine
translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 10?
51, Montre?al, Canada, June. Association for Compu-
tational Linguistics.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, pages 138?145, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback
for explanation questions: A dataset and baselines.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 200?210, Montre?al, Canada, June. Association
for Computational Linguistics.
Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,
Claudia Leacock, Danilo Giampiccolo, Luisa Ben-
tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.
2013. Semeval-2013 task 7: The joint student re-
sponse analysis and 8th recognizing textual entailment
590
challenge. In *SEM 2013: The First Joint Conference
on Lexical and Computational Semantics, Atlanta,
Georgia, USA, 13-14 June. Association for Compu-
tational Linguistics.
Kenth Hagstro?m. 2012. Swedish readability calcula-
tor. https://github.com/keha76/Swedish-Readability-
Calculator.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
2009. The Elements of Statistical Learning: Data
Mining, Inference and Prediction. Springer-Verlag,
2nd edition.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proceedings of the
5th annual international conference on Systems docu-
mentation, SIGDOC ?86, pages 24?26, New York, NY,
USA. ACM.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Yoav Seginer. 2007. Learning Syntactic Structure. Ph.D.
thesis, Universiteit van Amsterdam.
Alex J. Smola and Bernhard Scho?lkopf. 2004. A tutorial
on support vector regression. Statistics and Comput-
ing, 14(3):199?222, August.
Wikipedia. 2013. Lix. http://en.wikipedia.org/wiki/LIX.
591
Accurate and Robust LFG-Based Generation for Chinese
Yuqing Guo
NCLT, School of Computing
Dublin City University
Dublin 9, Ireland
yguo@computing.dcu.ie
Haifeng Wang
Toshiba (China)
Research and Development Center
Beijing, 100738, China
wanghaifeng@rdc.toshiba.com.cn
Josef van Genabith
NCLT, School of Computing
Dublin City University
IBM CAS, Dublin, Ireland
josef@computing.dcu.ie
Abstract
We describe three PCFG-based models for
Chinese sentence realisation from Lexical-
Functional Grammar (LFG) f-structures. Both
the lexicalised model and the history-based
model improve on the accuracy of a simple
wide-coverage PCFG model by adding lexical
and contextual information to weaken inap-
propriate independence assumptions implicit
in the PCFG models. In addition, we pro-
vide techniques for lexical smoothing and rule
smoothing to increase the generation cover-
age. Trained on 15,663 automatically LFG f-
structure annotated sentences of the Penn Chi-
nese treebank and tested on 500 sentences ran-
domly selected from the treebank test set, the
lexicalised model achieves a BLEU score of
0.7265 at 100% coverage, while the history-
based model achieves a BLEU score of 0.7245
also at 100% coverage.
1 Introduction
Sentence generation, or surface realisation can be
described as the problem of producing syntacti-
cally, morphologically, and orthographically cor-
rect sentences from a given abstract semantic /
logical representation according to some linguistic
theory, e.g. Lexical Functional Grammar (LFG),
Head-Driven Phrase Structure Grammar (HPSG),
Combinatory Categorial Grammar (CCG), Tree Ad-
joining Grammar (TAG) etc. Grammars, such as
these, are declarative formulations of the correspon-
dences between semantic and syntactic representa-
tions. Traditionally, grammar rules have been care-
fully handcrafted, such as those used in LinGo (Car-
roll et al, 1999), OpenCCG (White, 2004) and
XLE (Crouch et al, 2007). As handcrafting gram-
mar rules is time-consuming, language-dependent
and domain-specific, recent years have witnessed re-
search on extracting wide-coverage grammars auto-
matically from annotated corpora, for both parsing
and generation. FERGUS (Bangalore and Rambow,
2000) took dependency structures as inputs, and pro-
duced XTAG derivations by a stochastic tree model
automatically acquired from an annotated corpus.
Nakanishi et al (2005) presented log-linear models
for a chart generator using a HPSG grammar ac-
quired from the Penn-II Treebank. From the same
treebank, Cahill and van Genabith (2006) automati-
cally extracted wide-coverage LFG approximations
for a PCFG-based generation model.
In addition to applying statistical techniques to
automatically acquire generation grammars, over the
last decade, there has been a lot of interest in a
generate-and-select paradigm for surface realisation.
The paradigm is characterised by a separation be-
tween generation and selection, in which symbolic
or rule-based methods are used to generate a space
of possible paraphrases, and statistical methods are
used to select one or more outputs from the space.
Starting from Langkilde (2002) who used a n-gram
language model to rank generated output strings, a
substantial number of traditional handcrafted sur-
face realisers have been augmented with sophisti-
cated stochastic rankers (Velldal and Oepen, 2005;
White et al, 2007; Cahill et al, 2007).
It is interesting to note that, while the study of
how the granularity of context-free grammars (CFG)
affects the performance of a parser (e.g. in the form
86
n1:IP
[?=?]
n2:NP
[?SUBJ=?]
n4:NR
[?=?]
L?
JiangZemin
n3:VP
[?=?]
n5:VV
[?=?]
??
interview
n6:NP
[?OBJ=?]
n7:NR
[???ADJUNCT]
I
Thai
n8:NN
[?=?]
on
president
f1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
PRED ????
SUBJ f2
?
?
?
PRED ?L??
NTYPE proper
NUM sg
?
?
?
OBJ f3
?
?
?
?
?
?
?
?
?
?
?
PRED ?on?
NTYPE common
NUM sg
ADJUNCT
?
?
?
?
?
f4
?
?
?
PRED ?I?
NTYPE proper
NUM sg
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
? : N ? F
?(n1)=?(n3)=?(n5)=f1 ?(n2)=?(n4)=f2 ?(n6)=?(n8)=f3 ?(n7)=f4
Figure 1: C- and f-structures with ? links for the sentence ?L???Ion?
of grammar transforms (Johnson, 1998) and lexical-
isation (Collins, 1997)) has attracted substantial at-
tention, to our knowledge, there has been a lot less
research on this subject for surface realisation, a pro-
cess that is generally regarded as the reverse pro-
cess of parsing. Moreover, while most of the re-
search so far has concentrated on English or Euro-
pean languages, we are also interested in generation
for other languages with diverse properties, such as
Chinese which is currently a focus language in pars-
ing (Bikel, 2004; Cao et al, 2007).
In this paper, we investigate three generative
PCFG models for Chinese generation based on
wide-coverage LFG grammars automatically ex-
tracted from the Penn Chinese Treebank (CTB). Our
work is couched in the framework of Lexical Func-
tional Grammar and is implemented in a chart-style
generator. We briefly describe LFG and the basic
generation model in Section 2. We improve the
baseline PCFG model by weakening the indepen-
dence assumptions in two disambiguation models in
Section 3. Section 4 describes the smoothing algo-
rithms adopted for the chart generator and Section 5
gives the experimental details and results.
2 LFG-Based Generation
2.1 Lexical Functional Grammar
Lexical Functional Grammar (Kaplan and Bres-
nan, 1982) is a constraint-based grammar formal-
ism which postulates (minimally) two levels of rep-
resentation: c(onstituent)-structure and f(unctional)-
structure. C-structure takes the form of phrase struc-
ture trees and captures surface grammatical config-
urations. F-structure encodes more abstract gram-
matical functions (GFs) such as SUBJ(ect), OBJ(ect),
ADJUNCT and TOPIC etc., in the form of hierar-
chical attribute-value matrices. C-structures and
f-structures are related by a piecewise correspon-
dence function ? that goes from the nodes of a c-
structure tree into units of f-structure spaces (Ka-
plan, 1995). As illustrated in Figure 1, given a
c-structure node ni, the corresponding f-structure
component fj is ?(ni). Admissible c-structures
are specified by a context-free grammar. The cor-
responding f-structures are derived from functional
annotations attached to the CFG rewriting rules.
(1) a. IP ?? NP VP
[?SUBJ=?] [?=?]
b. VP ?? VV NP
[?=?] [?OBJ=?]
c. NP ?? NR NN
[?ADJ=?] [?=?]
d. NP ?? NR
[?=?]
(1) shows a miniature set of annotated CFG rules
(lexical entries omitted) which generates the c- and
f-structure in Figure 1. In the functional annotations,
(?) refers to the f-structure associated with the local
c-structure node ni, i.e. ?(ni), and (?) refers to the
87
Model Grammar Rule Conditions
PCFG VP[?=?] ? VV[?=?] NP[?OBJ=?] VP[?=?], {PRED, SUBJ, OBJ}
HB-PCFG VP[?=?] ? VV[?=?] NP[?OBJ=?] VP[?=?], {PRED, SUBJ, OBJ}, TOP
LEX-PCFG VP(??)[?=?] ? VV(??)[?=?] NP(on)[?OBJ=?] VP(??)[?=?], {PRED, SUBJ, OBJ}
Table 1: Examples of f-structure annotated CFG rules (from Figure 1) in different models
f-structure associated with the mother (M ) node of
ni, i.e. ?(M(ni)).
2.2 Generation from f-Structures
The generation task in LFG is to determine which
sentences correspond to a specified f-structure,
given a particular grammar, such as (1). Kaplan
and Wedekind (2000) proved that the set of strings
generated by an LFG grammar from fully speci-
fied f-structures is a context-free language. Based
on this theoretical cornerstone, Cahill and van Gen-
abith (2006) presented a PCFG-based chart genera-
tor using wide-coverage LFG approximations auto-
matically extracted from the Penn-II treebank. The
LFG-based statistical generation model defines the
conditional probability P (T |F ), for each candidate
functionally annotated c-structure tree T (which
fully specifies a surface realisation) given an f-
structure F . The generation model searches for the
Tbest that maximises P (T |F ) (Eq. 1). P (T |F ) is
then decomposed as the product of the probabilities
of all the functionally annotated CFG rewriting rules
X ? Y (conditioned on the left hand side (LHS) X
and local features of the corresponding f-structure
?(X)) contributing to the tree T (Eq. 2). The first
line (PCFG) of Table 1 shows the f-structure anno-
tated CFG rule to expand node n3 in Figure 1.
Tbest = argmax
T
P (T |F ) (1)
P (T |F ) =
?
X ? Y in T
Feats = {ai|ai ? ?(X)}
P (X ? Y |X,Feats) (2)
3 Disambiguation Models
The basic generation model presented in (Cahill
and van Genabith, 2006) used simple probabilis-
tic context-free grammars. However, the indepen-
dence assumptions implicit in PCFG models may
not be appropriate to best capture natural language
phenomena. Methodologies such as lexicalisa-
tion (Collins, 1997; Charniak, 2000) and tree trans-
formations (Johnson, 1998), weaken the indepen-
dence assumptions and have been applied success-
fully to parsing and shown significant improvements
over simple PCFGs. In this section we study the ef-
fect of such methods in LFG-based generation for
Chinese.
3.1 A History-Based Model
The history-based (HB) approach which incorpo-
rates more context information has worked well
in parsing (Collins, 1997; Charniak, 2000). Re-
sembling history-based models for parsing, Hogan
et al (2007) presented a history-based generation
model to overcome some of the inappropriate inde-
pendence assumptions in the basic generation model
of (Cahill and van Genabith, 2006). The history-
based model increases the context by simply includ-
ing the parent grammatical function GF of the f-
structure in addition to the local ?-linked feature set
in the conditioning context (Eq. 3). The f-structure
annotated CFG rule expanding n3 in the history-
based model is shown in the second line (HB-PCFG)
of Table 1.1
P (T |F ) =
?
X ? Y in T
Feats = {ai|ai ? ?(X)}
?f (f GF) = ?(X)
P (X ? Y |X,Feats,GF) (3)
The history-based model is motivated by English
data, for example, to generate the appropriate case
for pronouns in subject position and object position,
respectively. Though Chinese does not distinguish
cases, we expect the f-structure parent GF to help
predict grammar rule expansions more accurately in
the tree derivation than the simple PCFG model. We
will investigate how the HB model performs while
migrating it from English to Chinese data.
1The parent grammatical function of the outermost f-
structure is assumed to be a dummy GF TOP.
88
3.2 A Lexicalised Model
Compared to the HB model which includes the par-
ent grammatical function in the conditioning con-
text, lexicalised grammar rules contain more fine-
grained categorial information. To the best of our
knowledge, lexicalised parsers (Bikel, 2004) outper-
form unlexicalised parsers for Chinese. The expec-
tation is that a lexicalised PCFG model also works
better than a simple PCFG model in Chinese gen-
eration, considering e.g. prepositional phrase (PP)
modification in Chinese. Some prepositions indicat-
ing directions can occur either before or after the
main verbs, for instance both (2a) and (2b) are ac-
ceptable in Chinese. However, most PP modifiers
only act as adverbial adjuncts between the subjects
and verbal predicates. For instance ??/to? never
follows a verb as exemplified in the ungrammatical
sentence (3b).
(2) a. ? 4 ?m  ?
this CLS train run to Beijing
?The train is bound for Beijing.?
b. ? 4 ? ? m
this CLS train to Beijing run
(3) a. Ion ??I ?1 ??
Thai president to China make visit
?The Thai president paid a visit to China.?
b. *Ion ?1 ????I
Thai president make visit to China
In order to model phenomena such as these, we
head-lexicalise our grammar by associating each
non-terminal node with the head word2 in the c-
structure tree along the head-projection line. A non-
terminal node is written as X(x), where x is the lex-
ical head of X. The example generation grammar
rule in the lexicalised model is shown in the last line
(LEX-PCFG) of Table 1.
As in CKY chart parsing, generation grammars
are binarised in our chart generator. Thus all gram-
mar rules are either unary of the form X ? H or
binary X ? Y H (or X ? HY ), where H is the
head constituent and Y is the modifier. To handle the
problem of sparse data while estimating rule proba-
bilities, a back-off to baseline model is employed.
As, from a linguistic perspective, it is the modifier
2We use a mechanism similar to (Collins, 1997) but adapted
to Chinese data to find lexical heads in the treebank data.
rather than the head word which plays the main role
in determining word order, a back-off to partial lexi-
calisation on the modifier only is also used for bi-
nary rules. As a result, the probabilities of lexi-
calised unary and binary CFG rules are calculated
as in Eq. (4) and Eq. (5), respectively.
Pbk(H(h)|X(h)) = ?1P (H(h)|X(h))
+?2P (H |X) (4)
Pbk(Y (y)H(h)|X(h)) = ?1P (Y (y)H(h)|X(h))
+?2P (Y (y)H |X) + ?3P (Y H |X) (5)
where
?
i=1
?i = 1
In principle, grammars binarisation from left-to-
right (left-) or from right-to-left (right-) are equiva-
lent to represent the original grammar and the prob-
ability distributions. However the head word is the
final constituent for most phrasal categories in Chi-
nese.3 In lexicalised model, the head word imme-
diately projects to the top level in a left-binary tree,
and as a result, the intermediate NP nodes cannot
be lexicalised with the head word as illustrated in
Figure (2b). By contrast, right-binary rules are lex-
icalised and the head word is percolated from the
bottom of the tree (Figure (2c)). Therefore we adopt
the right binarisation method in our generation algo-
rithm.
4 Chart Generation and Smoothing
Algorithms
4.1 Chart Generation Algorithm
The PCFG-based generation algorithms are imple-
mented in terms of a chart generator (Kay, 1996).
In the generation algorithm, each (sub-)f-structure
indexes a (sub-)chart. Each local chart generates
the most probable trees for the local f-structure in
a bottom-up manner:
? generating lexical edges from the the local GF
PRED and some atomic features representing
function words, mood or aspect etc.
3Except for prepositional phrases, localiser and some verbal
phrases.
89
NP(m)
NR
[???ADJUNCT]
??
Shanghai
NN
[???ADJUNCT]
?
tennis
NN
[???ADJUNCT]
??
masters
NN
[?=?]
m
cup
(a.) the original tree
NP(m)
NP(null)
[?=?]
NP(null)
[?=?]
NR
[???ADJUNCT]
??
Shanghai
NN
[???ADJUNCT]
?
tennis
NN
[???ADJUNCT]
??
masters
NN
[?=?]
m
cup
NP(m)
NR
[???ADJUNCT]
??
Shanghai
NP(m)
[?=?]
NN
[???ADJUNCT]
?
tennis
NP(m)
[?=?]
NN
[???ADJUNCT]
??
masters
NN
[?=?]
m
cup
(b.) left-binarisation (c.) right-binarisation
Figure 2: Lexicalised binary trees
? applying unary rules and binary rules to gener-
ate new edges until no any new edges can be
generated in the current local chart.
? propagating compatible edges to the upper-
level chart.
For efficiency, the generation algorithm does
Viterbi-pruning for each local chart, viz. if two
edges have equivalent categories and lexical cover-
age, only the most probable one is kept.
The generation coverage is impacted on by un-
known words4 and unmatched grammar rules in
chart generation. We present a lexical smoothing
and a rule smoothing strategy in the following sub-
sections.
4.2 Lexical Smoothing
In LFG f-structure, the surface form of the lemma
is represented via lexical rules involving a particular
set of features, e.g. the lemma ?on /president? is
represented as {?PRED=?on ?, ?NTYPE=common,
?NUM=sg}. Particular lexical rules can be cap-
tured in general lexical macros abstracting away
4We use unknown words as a cover term to refer to all words
occurring in the test set but not in the training set.
from particular surface forms to lemmas, e.g. the
lexical macro encapsuling the above lexical rule is
{?PRED=$LEMMA, ?NTYPE=common, ?NUM=sg},
which generally associates to common nouns NN in
the CTB. According to the assumption that unknown
words have a probability distribution similar to ha-
pax legomenon (Baayen and Sproat, 1996), we pre-
dict the part-of-speech of unknown words from in-
frequent words in the training set by automatically
extracting lexical macros corresponding to the par-
ticular set of f-structure features. The probability of
the potential POS tag t associated to a feature set f
is estimated according to Eq. (6).
P (t|f) = count(t, f)?n
i=1 count(ti, f)
(6)
4.3 Rule Smoothing
The coverage of grammar rules increases with the
size of training data and in theory all the rules can
be fully covered by a training set, if it is big enough.
With limited training resources we have to resort to
fuzzy matching of grammar rules. Two smoothing
strategies are carried out at the level of grammar
rules.
90
Mathched Grammar Rule
Nonsmooth VP[?=?] ? VV[?=?] NP[?OBJ=?], {SUBJ, OBJ, PRED}
Feature smooth VP[?=?] ? VV[?=?] NP[?OBJ=?]
Partial match VP ? VV [?OBJ=?], {SUBJ, OBJ, PRED}
Table 2: Smoothing of CFG rules
? Reducing the conditioning f-structure features
during rule matching;
? Applying partial match during rule application.
A node in each unlexicalised grammar rule X ?
Y H includes two parts: constituent category c, such
as IP, NP, VP etc.; functional f-structure annotation
a, such as [?SUBJ=?], [?=?] etc. As a heuristic based
on linguistic experience, we define the order of im-
portance of these elements as follows:
X(c) > H(c) > Y (a) > Y (c) > X(a) > H(a)
(4) IP[?COMP=?] ? NP[?SUBJ=?] VP[?=?]
For the above example rule (4), the importance of
the elements is:
IP > VP > [?SUBJ=?] > NP > [?COMP=?] > [?=?]
The elements can be deleted from the rules in an im-
portance order from low to high.5 The partial rules
adopted in our system ignore the least important 3
elements, viz. the functional annotation of the head
node H(a), the functional annotation on LHS X(a)
and constituent category of the modifier node Y (c).
Examples of the two types of smoothed rules are
shown in Table 2.
5 Experimental Results
Our experiments are carried out on the newly
released Penn Chinese treebank version 6.0
(CTB6) (Xue et al, 2005), excluding the portion of
ACE broadcast news. We follow the recommended
splits (in the list-of-file of CTB6) to divide the
data into test set, development set and training set.
The training set includes 756 files with a total of
15,663 sentences. The CTB trees of the training set
were automatically annotated with LFG f-structure
equations following (Guo et al, 2007). Table 3
shows the number of different grammar rule types
extracted from the training set. From the test files,
5However c and a on the same node can?t be deleted at the
same time.
we randomly select 500 sentences as test data
with minimal sentence length 5 words, maximal
length 80 words, and average length 28.84 words.
The development set alo includes 500 sentences
randomly selected from the development files with
sentence length between 5 and 80 words. The
c-structure trees of the test and development data
were also automatically converted to f-structures as
input to the generator.
Type with features without features
PCFG 22,372 8,548
HB-PCFG 28,487 11,969
LEX-PCFG 325,094 286,468
Table 3: Number of rules in the training set
The generation system is evaluated against the
raw text of the test data in terms of accuracy and cov-
erage. Following (Langkilde, 2002) and other work
on general-purpose generators, we adopt BLEU
score (Papineni et al, 2002), average simple string
accuracy (SSA) and percentage of exactly matched
sentences for accuracy evaluation.6 For coverage
evaluation, we measure the percentage of input f-
structures that generate a sentence.
Table 4 reports the initial experiments on the sim-
ple PCFG, HB-based PCFG and lexicalised PCFG
models. The results in the left column evaluate all
input f-structures, the right column evaluate only
those f-structures which yield a complete sentence.
The results show that the lexicalised model outper-
forms the baseline PCFG model. The HB model is
the most accurate for complete sentences, but with
reduced coverage compared to the other two mod-
els. However the low coverage of sentences com-
pletely generated due to unknown words and un-
matched rules makes the results unusable in prac-
6We are aware of the limitations in fully automatic evalua-
tion metrics, and in an ideal scenario, we would complement the
BLEU and SSA scores by a human evaluation. Unfortunately,
this is beyond the scope of the current paper.
91
All Output Strings Complete Output Sentences
Coverage ExMatch BLEU SSA Coverage ExMatch BLEU SSA
PCFG 100% 7.2% 0.5401 0.6261 36.40% 19.78% 0.7101 0.7687
HB-PCFG 100% 8.60% 0.5474 0.6281 34.80% 24.71% 0.7513 0.8092
LEX-PCFG 100% 9.40% 0.5687 0.6537 37.00% 25.41% 0.7431 0.8024
Table 4: Results without smoothing
All Output Strings Complete Output Sentences
Coverage ExMatch BLEU SSA Coverage ExMatch BLEU SSA
PCFG 100% 11.00% 0.6894 0.7240 94.20% 11.68% 0.7047 0.7388
HB-PCFG 100% 11.80% 0.7108 0.7348 94.00% 12.55% 0.7284 0.7506
LEX-PCFG 100% 14.00% 0.7152 0.7595 94.40% 14.83% 0.7302 0.7754
Table 5: Results with lexical smoothing
Partial match Feature smooth
Complete Sentences Coverage ExMatch BLEU SSA Coverage ExMatch BLEU SSA
PCFG 97.20% 11.32% 0.7022 0.7356 100% 11.20% 0.7021 0.7330
HB-PCFG 96.20% 12.27% 0.7263 0.7458 100% 12.00% 0.7245 0.7413
LEX-PCFG 97.80% 14.31% 0.7265 0.7696 100% 14.20% 0.7265 0.7675
Table 6: Results with lexical and rule smoothing
tice.
Table 5 gives the results with lexical smoothing.
The coverage for complete sentences increases by
nearly 60% absolute for all models. The increased
coverage also improves the overall results evaluated
against all sentences. The HB model performs better
than the simple PCFG model in nearly all respects
and in turn the lexicalised model comprehensively
outperforms the HB model.
The final results with both lexical smoothing and
rule smoothing by two different strategies are tabu-
lated in Table 6. The left column provides the results
of smoothing by partial match and the right column
the results by reducing conditioning f-structure fea-
tures. All results are evaluated for completely gen-
erated sentences only. The feature smoothing re-
sults in a full coverage of 100%, while slightly de-
grading the quality of sentences generated compared
with partial match smoothing. We feel the tradeoff
at the cost of a small decrease in quality is still worth
the full coverage. Throughout the experiments, the
lexicalised model exhibits consistently better perfor-
mance than the unlexicalised models, which proves
our intuition that successful techniques in parsing
also work well in generation.
6 Conclusion and Further Work
We have presented an accurate, robust chart genera-
tor for Chinese based on treebank-based, automati-
cally acquired LFG resources. Our model improves
the baseline provided by (Cahill and van Genabith,
2006): (i) accuracy is increased by creating a lexi-
calised PCFG grammar and enriching conditioning
context with parent f-structure features; and (ii) cov-
erage is increased by providing lexical smoothing
and fuzzy matching techniques for rule smoothing.
The combinational explosion of grammar rules
encountered in the chart generator is similar to that
in parsing. In the current system, we only keep the
most probable realisation for each input f-structure.
An alternative model in line with the generate-and-
select paradigm, would pack all the locally equiva-
lent edges in a forest and re-rank all the realisations
by a separate language model. This might help us to
reduce some errors caused in our current model, for
instance, the generation of function words in fixed
phrases. As shown in ex. (5), the function word
??? is incorrectly generated as ??. This is be-
cause they share the same part-of-speech DEG in
CTB, however ?? has a much higher frequency
than ??? in Chinese text and thus has a higher prob-
ability to be generated.
92
(5) a. ? ? ?
all things DE in
?among all things?
b. *?  ?
all things DE in
Acknowledgments
The research reported in this paper is supported by
Science Foundation Ireland grant 04/IN/I527. Also,
we would like to thank Aoife Cahill for many help-
ful and insightful discussions on the work. And we
gratefully acknowledge the anonymous reviewers.
References
Harald Baayen and Richard Sproat. 1996. Estimat-
ing Lexical Priors for Low-Frequency Morphologi-
cally Ambiguous Forms. Computational Linguistics,
22(2): 155?166.
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a Probabilistic Hierarchical Model for Gen-
eration. Proceedings of the 18th International Con-
ference on Computational Linguistics, pages 42?48.
Saarbru?cken, Germany.
Daniel M. Bikel. 2004. On the Parameter Space of Gen-
erative Lexicalized Statistical Parsing Models. Ph.D.
Thesis of Department of Computer & Information Sci-
ence, University of Pennsylvania.
Aoife Cahill and Josef van Genabith. 2006. Robust
PCFG-Based Generation Using Automatically Ac-
quired LFG Approximations. Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 1033?1040. Syd-
ney, Australia.
Aoife Cahill and Martin Forst and Christian Rohrer.
2007. Stochastic Realisation Ranking for a Free Word
Order Language. Proceedings of the 11th European
Workshop on Natural Language Generation, pages
17?24. Schloss Dagstuhl, Germany.
John Carroll and Ann Copestake and Dan Flickinger and
Victor Poznanski. 1999. An efficient chart genera-
tor for (semi-)lexicalist grammars. Proceedings of the
7th European Workshop on Natural Language Gener-
ation, pages 86?95. Toulouse, France.
Hailong Cao and Yujie Zhang and Hitoshi Isahara. 2007.
Empirical study on Parsing Chinese Based on Collins?
Model. Proceedings of the 10th Conference of the Pa-
cific Association for Computational Linguistics, pages
113?119. Melbourne, Australia.
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. Proceedings of the 1st Annual Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, pages 132?139. Seattle, WA.
Michael Collins. 1997. Three Generative, Lexicalized
Models for Statistical Parsing. Proceedings of the 35th
Annual Meeting of the Association for Computational
Linguistics, pages 16?23. Madrid, Spain.
Dick Crouch and Mary Dalrymple and Ron Kaplan and
Tracy King and John Maxwell and Paula Newman.
2007. XLE Documentation. Palo Alto Research Cen-
ter, CA.
Yuqing Guo and Josef van Genabith and Haifeng Wang.
2007. Treebank-based Acquisition of LFG Resources
for Chinese. Proceedings of LFG07 Conference,
pages 214?232. Stanford, CA, USA.
Deirdre Hogan and Conor Cafferkey and Aoife Cahill
and Josef van Genabith. 2007. Exploiting Multi-Word
Units in History-Based Probabilistic Generation. Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 267?276.
Prague, Czech Republic.
Mark Johnson. 1998. PCFG Models of Linguistic Tree
Representations. Computational Linguistics, 24(4):
613?632. MIT Press, Cambridge, MA,
Ronald M. Kaplan. 1995. The formal architecture of
lexical-functional grammar. Formal Issues in Lexical-
Functional Grammar, pages 7?27. CSLI Publications,
Standford, USA.
Ronald M. Kaplan and Joan Bresnan. 1982. Lexical
Functional Grammar: a Formal System for Grammat-
ical Representation. The Mental Representation of
Grammatical Relations, pages 173-282. MIT Press,
Cambridge, MA.
Ronald M. Kaplan and Jurgen Wedekind. 2000.
LFG Generation Produces Context-free Languages.
Proceedings of the 18th International Conference
on Computational Linguistics, pages 425?431.
Saarbru?cken, Germany.
Martin Kay. 1996. Chart Generation. Proceedings of the
34th Annual Meeting of the Association for Computa-
tional Linguistics, pages 200?204. Santa Cruz, USA.
Irene Langkilde. 2000. Forest-Based Statistical Sen-
tence Generation. Proceedings of 1st Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, pages 170?177. Seattle, WA.
Langkilde, Irene. 2002. An Empirical Verification of
Coverage and Correctness for a General-Purpose Sen-
tence Generator. Proceedings of the Second Interna-
tional Conference on Natural Language Generation,
17?24. New York, USA.
93
Hiroko Nakanishi and Yusuke Nakanishi and Jun?ichi
Tsujii. 2005. Probabilistic Models for Disambigua-
tion of an HPSG-Based Chart Generator. Proceedings
of the 9th International Workshop on Parsing Technol-
ogy, pages 93?102. Vancouver, British Columbia.
Kishore Papineni and Salim Roukos and Todd Ward and
Wei-Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311-318. Philadelphia,
USA.
Erik Velldal and Stephan Oepen. 2005. Maximum en-
tropy models for realization ranking. Proceedings of
the MTSummit ?05.
Michael White. 2004. Reining in CCG Chart Realiza-
tion. Proceedings of the third International Natural
Language Generation Conference. Hampshire, UK.
Michael White and Rajakrishnan Rajkumar and Scott
Martin. 2007. Towards Broad Coverage Surface Re-
alization with CCG. Proceedings of the MT Summit
XI Workshop on Language Generation and Machine
Translation, pages 22?30. Copenhagen, Danmark.
Nianwen Xue and Fei Xia and Fu dong Chiou and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2): 207?238.
94
Parser-Based Retraining for Domain Adaptation of Probabilistic Generators
Deirdre Hogan, Jennifer Foster, Joachim Wagner and Josef van Genabith
National Centre for Language Technology
School of Computing
Dublin City University
Ireland
{dhogan, jfoster, jwagner, josef}@computing.dcu.ie
Abstract
While the effect of domain variation on Penn-
treebank-trained probabilistic parsers has been
investigated in previous work, we study its ef-
fect on a Penn-Treebank-trained probabilistic
generator. We show that applying the gener-
ator to data from the British National Corpus
results in a performance drop (from a BLEU
score of 0.66 on the standard WSJ test set to a
BLEU score of 0.54 on our BNC test set). We
develop a generator retraining method where
the domain-specific training data is automat-
ically produced using state-of-the-art parser
output. The retraining method recovers a sub-
stantial portion of the performance drop, re-
sulting in a generator which achieves a BLEU
score of 0.61 on our BNC test data.
1 Introduction
Grammars extracted from the Wall Street Journal
(WSJ) section of the Penn Treebank have been suc-
cessfully applied to natural language parsing, and
more recently, to natural language generation. It is
clear that high-quality grammars can be extracted
for the WSJ domain but it is not so clear how
these grammars scale to other text genres. Gildea
(2001), for example, has shown that WSJ-trained
parsers suffer a drop in performance when applied
to the more varied sentences of the Brown Cor-
pus. We investigate the effect of domain variation in
treebank-grammar-based generation by applying a
WSJ-trained generator to sentences from the British
National Corpus (BNC).
As with probabilistic parsing, probabilistic gener-
ation aims to produce the most likely output(s) given
the input. We can distinguish three types of prob-
abilistic generators, based on the type of probabil-
ity model used to select the most likely sentence.
The first type uses an n-gram language model, e.g.
(Langkilde, 2000), the second type uses a proba-
bility model defined over trees or feature-structure-
annotated trees, e.g. (Cahill and van Genabith,
2006), and the third type is a mixture of the first
and second type, employing n-gram and grammar-
based features, e.g. (Velldal and Oepen, 2005). The
generator used in our experiments is an instance of
the second type, using a probability model defined
over Lexical Functional Grammar c-structure and
f-structure annotations (Cahill and van Genabith,
2006; Hogan et al, 2007).
In an initial evaluation, we apply our probabilistic
WSJ-trained generator to BNC material, and show
that the generator suffers a substantial performance
degradation, with a drop in BLEU score from 0.66
to 0.54. We then turn our attention to the problem
of adapting the generator so that it can more accu-
rately generate the 1,000 sentences in our BNC test
set. The problem of adapting any NLP system to a
domain different from the domain upon which it has
been trained and for which no gold standard train-
ing material is available is a very real one, and one
which has been the focus of much recent research in
parsing. Some success has been achieved by training
a parser, not on gold standard hand-corrected trees,
but on parser output trees. These parser output trees
can by produced by a second parser in a co-training
scenario (Steedman et al, 2003), or by the same
parser with a reranking component in a type of self-
training scenario (McClosky et al, 2006). We tackle
165
the problem of domain adaptation in generation in
a similar way, by training the generator on domain
specific parser output trees instead of manually cor-
rected gold standard trees. This experiment achieves
promising results, with an increase in BLEU score
from 0.54 to 0.61. The method is generic and can be
applied to other probabilistic generators (for which
suitable training material can be automatically pro-
duced).
2 Background
The natural language generator used in our experi-
ments is the WSJ-trained system described in Cahill
and van Genabith (2006) and Hogan et al (2007).
Sentences are generated from Lexical Functional
Grammar (LFG) f-structures (Kaplan and Bresnan,
1982). The f-structures are created automatically
by annotating nodes in the gold standard WSJ trees
with LFG functional equations and then passing
these equations through a constraint solver (Cahill
et al, 2004). The generation algorithm is a chart-
based one which works by finding the most proba-
ble tree associated with the input f-structure. The
yield of the most probable tree is the output sen-
tence. An annotated PCFG, in which the non-
terminal symbols are decorated with functional in-
formation, is used to generate the most probable tree
from an f-structure. Cahill and van Genabith (2006)
attain 98.2% coverage and a BLEU score of 0.6652
on the standard WSJ test set (Section 23). Hogan
et al (2007) describe an extension to the system
which replaces the annotated PCFG selection model
with a more sophisticated history-based probabilis-
tic model. Instead of conditioning the righthand side
of a rule on the lefthand non-terminal and its asso-
ciated functional information alone, the new model
includes non-local conditioning information in the
form of functional information associated with an-
cestor nodes of the lefthand side category. This sys-
tem achieves a BLEU score of 0.6724 and 99.9%
coverage.
Other WSJ-trained generation systems include
Nakanishi et al (2005) and White et al (2007).
Nakanishi et al (2005) describe a generator trained
on a HPSG grammar derived from the WSJ Section
of the Penn Treebank. On sentences of ? 20 words
in length, their system attains coverage of 90.75%
and a BLEU score of 0.7733. White et al (2007)
describe a CCG-based realisation system which has
been trained on logical forms derived from CCG-
Bank (Hockenmaier and Steedman, 2005), achiev-
ing 94.3% coverage and a BLEU score of 0.5768 on
WSJ23 for all sentence lengths. The input structures
upon which these systems are trained vary in form
and specificity, but what the systems have in com-
mon is that their various input structures are derived
from Penn Treebank trees.
3 The BNC Test Data
The new English test set consists of 1,000 sentences
taken from the British National Corpus (Burnard,
2000). The BNC is a one hundred million word bal-
anced corpus of British English from the late twenti-
eth century. Ninety per cent of it is written text, and
the remaining 10% consists of transcribed sponta-
neous and scripted spoken language. The BNC sen-
tences in the test set are not chosen completely at
random. Each sentence in the test set has the prop-
erty of containing a word which appears as a verb
in the BNC but not in the usual training sections of
the Wall Street Journal section of the Penn Treebank
(WSJ02-21). Sentences were chosen in this way so
that the resulting test set would be a difficult one
for WSJ-trained systems. In order to produce in-
put f-structures for the generator, the test sentences
were manually parsed by one annotator, using as
references the Penn Treebank trees themselves and
the Penn Treebank bracketing guidelines (Bies et
al., 1995). When the two references did not agree,
the guidelines took precedence over the Penn Tree-
bank trees. Difficult parsing decisions were docu-
mented. Due to time constraints, the annotator did
not mark functional tags or traces. The context-free
gold standard parse trees were transformed into f-
structures using the automatic procedure of Cahill et
al. (2004).
4 Experiments
Experimental Setup In our first experiment, we
apply the original WSJ-trained generator to our
BNC test set. The gold standard trees for our BNC
test set differ from the gold standard Wall Street
Journal trees, in that they do not contain Penn-II
traces or functional tags. The process which pro-
166
duces f-structures from trees makes use of trace and
functional tag information, if available. Thus, to en-
sure that the training and test input f-structures are
created in the same way, we use a version of the
generator which is trained using gold standard WSJ
trees without functional tag or trace information.
When we test this system on the WSJ23 f-structures
(produced in the same way as the WSJ training ma-
terial), the BLEU score decreases slightly from 0.67
to 0.66. This is our baseline system.
In a further experiment, we attempt to adapt
the generator to BNC data by using BNC trees as
training material. Because we lack gold standard
BNC trees (apart from those in our test set), we
try instead to use parse trees produced by an accu-
rate parser. We choose the Charniak and Johnson
reranking parser because it is freely available and
achieves state-of-the-art accuracy (a Parseval f-score
of 91.3%) on the WSJ domain (Charniak and John-
son, 2005). It is, however, affected by domain vari-
ation ? Foster et al (2007) report that its f-score
drops by approximately 8 percentage points when
applied to the BNC domain. Our training size is
500,000 sentences. We conduct two experiments:
the first, in which 500,000 sentences are extracted
randomly from the BNC (minus the test set sen-
tences), and the second in which only shorter sen-
tences, of length ? 20 words, are chosen as training
material. The rationale behind the second experi-
ment is that shorter sentences are less likely to con-
tain parser errors.
We use the BLEU evaluation metric for our ex-
periments. We measure both coverage and full cov-
erage. Coverage measures the number of cases for
which the generator produced some kind of out-
put. Full coverage measures the number of cases for
which the generator produced a tree spanning all of
the words in the input.
Results The results of our experiments are shown
in Fig. 1. The first row shows the results we ob-
tain when the baseline system is applied to the f-
structures derived from the 1,000 BNC gold stan-
dard parse trees. The second row shows the results
on the same test set for a system trained on Charniak
and Johnson parser output trees for 500,000 BNC
sentences. The results in the final row are obtained
by training the generator on Charniak and Johnson
parser output trees for 500,000 BNC sentences of
length ? 20 words in length.
Discussion As expected, the performance of the
baseline system degrades when faced with out-of-
domain test data. The BLEU score drops from a
0.66 score for WSJ test data to a 0.54 score for
the BNC test data, and full coverage drops from
85.97% to 68.77%. There is a substantial improve-
ment, however, when the generator is trained on
BNC data. The BLEU score jumps from 0.5358
to 0.6135. There are at least two possible reasons
why a BLEU score of 0.66 is not obtained: The first
is that the quality of the f-structure-annotated trees
upon which the generator has been trained has de-
graded. For the baseline system, the generator is
trained on f-structure-annotated trees derived from
gold trees. The new system is trained on f-structure-
annotated parser output trees, and the performance
of Charniak and Johnson?s parser degrades when ap-
plied to BNC data (Foster et al, 2007). The second
reason has been suggested by Gildea (2001): WSJ
data is easier to learn than the more varied data in the
Brown Corpus or BNC. Perhaps even if gold stan-
dard BNC parse trees were available for training, the
system would not behave as well as it does for WSJ
material.
It is interesting to note that training on 500,000
shorter sentences does not appear to help. We hy-
pothesized that it would improve results because
shorter sentences are less likely to contain parser
errors. The drop in full coverage from 86.69% to
79.58% suggests that the number of short sentences
needs to be increased so that the size of the training
material stays constant.
5 Conclusion
We have investigated the effect of domain varia-
tion on a LFG-based WSJ-trained generation sys-
tem by testing the system?s performance on 1,000
sentences from the British National Corpus. Perfor-
mance drops from a BLEU score of 0.66 onWSJ test
data to 0.54 on the BNC test set. Encouragingly, we
have also shown that domain-specific training mate-
rial produced by a parser can be used to claw back
a significant portion of this performance degrada-
tion. Our method is general and could be applied
to other WSJ-trained generators (e.g. (Nakanishi et
167
Train BLEU Coverage Full Coverage
WSJ02-21 0.5358 99.1 68.77
BNC(500k) 0.6135 99.1 86.69
BNC(500k) ? 20 words 0.5834 99.1 79.58
Figure 1: Results for 1,000 BNC Sentences
al., 2005; White et al, 2007)). We intend to con-
tinue this research by training our generator on parse
trees produced by a BNC-self-trained version of the
Charniak and Johnson reranking parser (Foster et al,
2007). We also hope to extend the evaluation beyond
the BLEU metric by carrying out a human judge-
ment evaluation.
Acknowledgments
This research has been supported by the Enterprise
Ireland Commercialisation Fund (CFTD/2007/229),
Science Foundation Ireland (04/IN/I527) and the
IRCSET Embark Initative (P/04/232). We thank the
Irish Centre for High End Computing for providing
computing facilities.
References
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for treebank
II style, Penn Treebank project. Technical Report
Tech Report MS-CIS-95-06, University of Pennsylva-
nia, Philadelphia, PA.
Lou Burnard. 2000. User reference guide for the British
National Corpus. Technical report, Oxford University
Computing Services.
Aoife Cahill and Josef van Genabith. 2006. Robust
PCFG-based generation using automatically acquired
lfg approximations. In Proceedings of the 21st COL-
ING and the 44th Annual Meeting of the ACL, pages
1033?1040, Sydney.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef
van Genabith, and Andy Way. 2004. Long-Distance
Dependency Resolution in Automatically Acquired
Wide-Coverage PCFG-Based LFG Approximations.
In Proceedings of the 42nd Meeting of the ACL, pages
320?327, Barcelona.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
ACL, pages 173?180, Ann Arbor.
Jennifer Foster, Joachim Wagner, Djame? Seddah, and
Josef van Genabith. 2007. Adapting WSJ-trained
parsers to the British National Corpus using in-domain
self-training. In Proceedings of the Tenth IWPT, pages
33?35, Prague.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Proceedings of EMNLP, Pittsburgh.
Julia Hockenmaier and Mark Steedman. 2005. Ccgbank:
Users? manual. Technical report, Computer and Infor-
mation Science, University of Pennsylvania.
Deirdre Hogan, Conor Cafferkey, Aoife Cahill, and Josef
van Genabith. 2007. Exploiting multi-word units in
history-based probabilistic generation. In Proceedings
of the joint EMNLP/CoNLL, pages 267?276, Prague.
Ron Kaplan and Joan Bresnan. 1982. Lexical Functional
Grammar: a Formal System for Grammatical Repre-
sentation. In Joan Bresnan, editor, The Mental Repre-
sentation of Grammatical Relations, pages 173?281.
MIT Press.
Irene Langkilde. 2000. Forest-based statistical sentence
generation. In Proceedings of NAACL, Seattle.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, Main Conference, pages 152?159, New
York City.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic methods for disambiguation of an
HPSG-based chart generator. In Proceedings of the
Ninth IWPT, pages 93?102, Vancouver.
Mark Steedman, Miles Osbourne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Boot-
strapping statistical parsers from small datasets. In
Proceedings of EACL, pages 331?338, Budapest.
Erik Velldal and Stephan Oepen. 2005. Maximum en-
tropy models for realization ranking. In Proceedings
of the MT-Summit, Phuket.
Michael White, Rajakrishnan Rajkumar, and Scott Mar-
tin. 2007. Towards broad coverage surface realiza-
tion with CCG. In Proceedings of the Workshop on
Using Corpora for NLG: Language Generation and
Machine Translation (UCNLG+MT), pages 267?276,
Copenhagen.
168
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 67?75,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Handling Unknown Words in Statistical Latent-Variable Parsing Models for
Arabic, English and French
Mohammed Attia, Jennifer Foster, Deirdre Hogan, Joseph Le Roux, Lamia Tounsi,
Josef van Genabith?
National Centre for Language Technology
School of Computing, Dublin City University
{mattia,jfoster,dhogan,jleroux,ltounsi,josef}@computing.dcu.ie
Abstract
This paper presents a study of the impact
of using simple and complex morphological
clues to improve the classification of rare and
unknown words for parsing. We compare
this approach to a language-independent tech-
nique often used in parsers which is based
solely on word frequencies. This study is ap-
plied to three languages that exhibit different
levels of morphological expressiveness: Ara-
bic, French and English. We integrate infor-
mation about Arabic affixes and morphotac-
tics into a PCFG-LA parser and obtain state-
of-the-art accuracy. We also show that these
morphological clues can be learnt automati-
cally from an annotated corpus.
1 Introduction
For a parser to do a reasonable job of analysing free
text, it must have a strategy for assigning part-of-
speech tags to words which are not in its lexicon.
This problem, also known as the problem of un-
known words, has received relatively little attention
in the vast literature on Wall-Street-Journal (WSJ)
statistical parsing. This is likely due to the fact that
the proportion of unknown words in the standard
English test set, Section 23 of the WSJ section of
Penn Treebank, is quite small. The problem mani-
fests itself when the text to be analysed comes from
a different domain to the text upon which the parser
has been trained, when the treebank upon which the
parser has been trained is limited in size and when
?Author names are listed in alphabetical order. For further
correspondence, contact L. Tounsi, D. Hogan or J. Foster.
the language to be parsed is heavily inflected. We
concentrate on the latter case, and examine the prob-
lem of unknown words for two languages which lie
on opposite ends of the spectrum of morphologi-
cal expressiveness and for one language which lies
somewhere in between: Arabic, English and French.
In our experiments we use a Berkeley-style latent-
variable PCFG parser and we contrast two tech-
niques for handling unknown words within the gen-
erative parsing model: one in which no language-
specific information is employed and one in which
morphological clues (or signatures) are exploited.
We find that the improvement accrued from look-
ing at a word?s morphology is greater for Arabic
and French than for English. The morphological
clues we use for English are taken directly from the
Berkeley parser (Petrov et al, 2006) and those for
French from recent work on French statistical pars-
ing with the Berkeley parser (Crabbe? and Candito,
2008; Candito et al, 2009). For Arabic, we present
our own set of heuristics to extract these signatures
and demonstrate a statistically significant improve-
ment of 3.25% over the baseline model which does
not employ morphological information.
We next try to establish to what extent these clues
can be learnt automatically by extracting affixes
from the words in the training data and ranking these
using information gain. We show that this automatic
method performs quite well for all three languages.
The paper is organised as follows: In Section 2
we describe latent variable PCFG parsing models.
This is followed in Section 3 by a description of our
three datasets, including statistics on the extent of
the unknown word problem in each. In Section 4, we
67
present results on applying a version of the parser
which uses a simple, language-agnostic, unknown-
word handling technique to our three languages. In
Section 5, we show how this technique is extended
to include morphological information and present
parsing results for English and French. In Section 6,
we describe the Arabic morphological system and
explain how we used heuristic rules to cluster words
into word-classes or signatures. We present parsing
results for the version of the parser which uses this
information. In Section 7, we describe our attempts
to automatically determine the signatures for a lan-
guage and present parsing results for the three lan-
guages. Finally, in Section 8, we discuss how this
work might be fruitfully extended.
2 Latent Variable PCFG Parsing
Johnson (1998) showed that refining treebank cate-
gories with parent information leads to more accu-
rate grammars. This was followed by a collection of
linguistically motivated propositions for manual or
semi-automatic modifications of categories in tree-
banks (Klein and Manning, 2003). In PCFG-LAs,
first introduced by Matsuzaki et al (2005), the re-
fined categories are learnt from the treebank us-
ing unsupervised techniques. Each base category
? and this includes part-of-speech tags ? is aug-
mented with an annotation that refines its distribu-
tional properties.
Following Petrov et al (2006) latent annotations
and probabilities for the associated rules are learnt
incrementally following an iterative process consist-
ing of the repetition of three steps.
1. Split each annotation of each symbol into n
(usually 2) new annotations and create rules
with the new annotated symbols. Estimate1 the
probabilities of the newly created rules.
2. Evaluate the impact of the newly created anno-
tations and discard the least useful ones. Re-
estimate probabilities with the new set of anno-
tations.
3. Smooth the probabilities to prevent overfitting.
We use our own parser which trains a PCFG-LA us-
ing the above procedure and parses using the max-
1Estimation of the parameters is performed by running Ex-
pectation/Maximisation on the training corpus.
rule parsing algorithm (Petrov et al, 2006; Petrov
and Klein, 2007). PCFG-LA parsing is relatively
language-independent but has been shown to be very
effective on several languages (Petrov, 2009). For
our experiments, we set the number of iterations to
be 5 and we test on sentences less than or equal to
40 words in length. All our experiments, apart from
the final one, are carried out on the development sets
of our three languages.
3 The Datasets
Arabic We use the the Penn Arabic Treebank
(ATB) (Bies and Maamouri, 2003; Maamouri and
Bies., 2004). The ATB describes written Modern
Standard Arabic newswire and follows the style and
guidelines of the English Penn-II treebank. We use
the part-of-speech tagset defined by Bikel and Bies
(Bikel, 2004). We employ the usual treebank split
(80% training, 10% development and 10% test).
English We use the Wall Street Journal section of
the Penn-II Treebank (Marcus et al, 1994). We train
our parser on sections 2-21 and use section 22 con-
catenated with section 24 as our development set.
Final testing is carried out on Section 23.
French We use the French Treebank (Abeille? et
al., 2003) and divide it into 80% for training, 10%
for development and 10% for final results. We fol-
low the methodology defined by Crabbe? and Can-
dito (2008): compound words are merged and the
tagset consists of base categories augmented with
morphological information in some cases2.
Table 1 gives basic unknown word statistics for
our three datasets. We calculate the proportion of
words in our development sets which are unknown
or rare (specified by the cutoff value) in the corre-
sponding training set. To control for training set
size, we also provide statistics when the English
training set is reduced to the size of the Arabic and
French training sets and when the Arabic training set
is reduced to the size of the French training set. In an
ideal world where training set sizes are the same for
all languages, the problem of unknown words will
be greatest for Arabic and smallest for English. It is
2This is called the CC tagset: base categories with verbal
moods and extraction features
68
language cutoff #train #dev #unk %unk language #train #dev #unk %unk
Arabic 0 594,683 70,188 3794 5.40 Reduced English 597,999 72,970 2627 3.60
- 1 - - 6023 8.58 (Arabic Size) - - 3849 5.27
- 5 - - 11,347 16.17 - - - 6700 9.18
- 10 - - 15,035 21.42 - - - 9083 12.45
English 0 950,028 72,970 2062 2.83 Reduced Arabic 266,132 70,188 7027 10.01
- 1 - - 2983 4.09 (French Size) - - 10,208 14.54
- 5 - - 5306 7.27 - - - 16,977 24.19
- 10 - - 7230 9.91 - - - 21,434 30.54
French 0 268,842 35,374 2116 5.98 Reduced English 265,464 72,970 4188 5.74
- 1 - - 3136 8.89 (French Size) - - 5894 8.08
- 5 - - 5697 16.11 - - - 10,105 13.85
- 10 - - 7584 21.44 - - - 13,053 17.89
Table 1: Basic Unknown Word Statistics for Arabic, French and English
reasonable to assume that the levels of inflectional
richness have a role to play in these differences.
4 A Simple Lexical Probability Model
The simplest method for handling unknown words
within a generative probabilistic parsing/tagging
model is to reserve a proportion of the lexical rule
probability mass for such cases. This is done by
mapping rare words in the training data to a spe-
cial UNKNOWN terminal symbol and estimating rule
probabilities in the usual way. We illustrate the pro-
cess with the toy unannotated PCFG in Figures 1
and 2. The lexical rules in Fig. 1 are the original
rules and the ones in Fig. 2 are the result of apply-
ing the rare-word-to-unknown-symbol transforma-
tion. Given the input sentence The shares recovered,
the word recovered is mapped to the UNKNOWN to-
ken and the three edges corresponding to the rules
NNS ? UNKNOWN, V BD ? UNKNOWN and
JJ ? UNKNOWN are added to the chart at this posi-
tion. The disadvantage of this simple approach is ob-
vious: all unknown words are treated equally and the
tag whose probability distribution is most dominated
by rare words in the training will be deemed the
most likely (JJ for this example), regardless of the
characteristics of the individual word. Apart from
its ease of implementation, its main advantage is its
language-independence - it can be used off-the-shelf
for any language for which a PCFG is available.3
One parameter along which the simple lexical
3Our simple lexical model is equivalent to the Berkeley sim-
pleLexicon option.
probability model can vary is the threshold used to
decide whether a word in the training data is rare or
?unknown?. When the threshold is set to n, a word
in the training data is considered to be unknown if it
occurs n or fewer times. We experiment with three
thresholds: 1, 5 and 10. The result of this experi-
ment for our three languages is shown in Table 2.
The general trend we see in Table 2 is that the
number of training set words considered to be un-
known should be minimized. For all three lan-
guages, the worst performing grammar is the one
obtained when the threshold is increased to 10. This
result is not unexpected. With this simple lexical
probability model, there is a trade-off between ob-
taining good guesses for words which do not occur
in the training data and obtaining reliable statistics
for words which do. The greater the proportion of
the probability mass that we reserve for the unknown
word section of the grammar, the more performance
suffers on the known yet rare words since these are
the words which are mapped to the UNKNOWN sym-
bol. For example, assume the word restructuring oc-
curs 10 times in the training data, always tagged as
a VBG. If the unknown threshold is less than ten and
if the word occurs in the sentence to be parsed, a
VBG edge will be added to the chart at this word?s
position with the probability 10/#VBG. If, however,
the threshold is set to 10, the word (in the training set
and the input sentence) will be mapped to UNKNOWN
and more possibilities will be explored (an edge for
each TAG ? UNKNOWN rule in the grammar). We
can see from Table 1 that at threshold 10, one fifth
69
VBD -> fell 50/153
VBD -> reoriented 2/153
VBD -> went 100/153
VBD -> latched 1/153
NNS -> photofinishers 1/201
NNS -> shares 200/201
JJ -> financial 20/24
JJ -> centrist 4/24
DT -> the 170/170
Figure 1: The original toy PCFG
VBD -> fell 50/153
VBD -> UNKNOWN 3/153
VBD -> went 100/153
NNS -> UNKNOWN 1/201
NNS -> shares 200/201
JJ -> financial 20/24
JJ -> UNKNOWN 4/24
DT -> the 170/170
Figure 2: Rare ? UNKNOWN
VBD -> fell 50/153
VBD -> UNK-ed 3/153
VBD -> went 100/153
NNS -> UNK-s 1/201
NNS -> shares 200/201
JJ -> financial 20/24
JJ -> UNK-ist 4/24
DT -> the 170/170
Figure 3: Rare ? UN-
KNOWN+SIGNATURE
Unknown Threshold Recall Precision F-Score Tagging Accuracy
Arabic
1 78.60 80.49 79.53 94.03
5 77.17 79.81 78.47 91.16
10 75.32 78.69 76.97 89.06
English
1 89.20 89.73 89.47 95.60
5 88.91 89.74 89.33 94.66
10 88.00 88.97 88.48 93.61
French
1 83.60 84.17 83.88 94.90
5 82.31 83.10 82.70 92.99
10 80.87 82.05 81.45 91.56
Table 2: Varying the Unknown Threshold with the Simple Lexical Probability Model
of the words in the Arabic and French development
sets are unknown, and this is reflected in the drop in
parsing performance at these thresholds.
5 Making use of Morphology
Unknown words are not all the same. We exploit this
fact by examining the effect on parsing accuracy of
clustering rare training set words using cues from
the word?s morphological structure. Affixes have
been shown to be useful in part-of-speech tagging
(Schmid, 1994; Tseng et al, 2005) and have been
used in the Charniak (Charniak, 2000), Stanford
(Klein and Manning, 2003) and Berkeley (Petrov et
al., 2006) parsers. In this section, we contrast the
effect on parsing accuracy of making use of such in-
formation for our three languages of interest.
Returning to our toy English example in Figures 1
and 2, and given the input sentence The shares re-
covered, we would like to use the fact that the un-
known word recovered ends with the past tense
suffix -ed to boost the probability of the lexical
rule V BD ? UNKNOWN. If we specialise the
UNKNOWN terminal using information from English
morphology, we can do just that, resulting in the
grammar in Figure 3. Now the word recovered is
mapped to the symbol UNK-ed and the only edge
which is added to the chart at this position is the one
corresponding to the rule V BD ? UNK-ed.
For our English experiments we use the unknown
word classes (or signatures) which are used in the
Berkeley parser. A signature indicates whether a
words contains a digit or a hyphen, if a word starts
with a capital letter or ends with one of the following
English suffixes (both derivational and inflectional):
-s, -ed, -ing, -ion, -er, -est, -ly, -ity, -y and -al.
For our French experiments we employ the same
signature list as Crabbe? and Candito (2008), which
itself was adapted from Arun and Keller (2005).
This list consists of (a) conjugation suffixes of regu-
70
lar verbs for common tenses (eg. -ons, -ez, -ent. . . )
and (b) derivational suffixes for nouns, adverbs and
adjectives (eg. -tion, -ment, -able. . . ).
The result of employing signature information
for French and English is shown in Table 3. Be-
side each f-score the absolute improvement over the
UNKNOWN baseline (Table 2) is given. For both
languages there is an improvement at all unknown
thresholds. The improvement for English is statis-
tically significant at unknown thresholds 1 and 10.4
The improvement is more marked for French and is
statistically significant at all levels.
In the next section, we experiment with signature
lists for Arabic.5
6 Arabic Signatures
In order to use morphological clues for Arabic we
go further than just looking at suffixes. We exploit
all the richness of the morphology of this language
which can be expressed through morphotactics.
6.1 Handling Arabic Morphotactics
Morphotactics refers to the way morphemes com-
bine together to form words (Beesley, 1998; Beesley
and Karttunen, 2003). Generally speaking, morpho-
tactics can be concatenative, with morphemes either
prefixed or suffixed to stems, or non-concatenative,
with stems undergoing internal alternations to con-
vey morphosyntactic information. Arabic is consid-
ered a typical example of a language that employs
non-concatenative morphotactics.
Arabic words are traditionally classified into three
types: verbs, nouns and particles. Adjectives take
almost all the morphological forms of, and share the
same templatic structures with, nouns. Adjectives,
for example, can be definite, and are inflected for
case, number and gender.
There are a number of indicators that tell us
whether the word is a verb or a noun. Among
4Statistical significance was determined using the strati-
fied shuffling method. The software used to perform the test
was downloaded from http://www.cis.upenn.edu/
?
dbikel/software.html.
5An inspection of the Berkeley Arabic grammar (available
at http://code.google.com/p/berkeleyparser/
downloads/list) shows that no Arabic-specific signatures
were employed. The Stanford parser uses 9 signatures for Ara-
bic, designed for use with unvocalised text. An immediate fu-
ture goal is to test this signature list with our parser.
these indicators are prefixes, suffixes and word tem-
plates. A template (Beesley and Karttunen, 2003) is
a kind of vocalization mould in which a word fits. In
derivational morphology Arabic words are formed
through the amalgamation of two tiers, namely, root
and template. A root is a sequence of three (rarely
two or four) consonants which are called radicals,
and the template is a pattern of vowels, or a com-
bination of consonants and vowels, with slots into
which the radicals of the root are inserted.
For the purpose of detection we use the reverse
of this information. Given that we have a word, we
try to extract the stem, by removing prefixes and suf-
fixes, and match the word against a number of verbal
and nominal templates. We found that most Ara-
bic templatic structures are in complementary dis-
tribution, i.e. they are either restricted to nominal
or verbal usage, and with simple regular expression
matching we can decide whether a word form is a
noun or a verb.
6.2 Noun Indicators
In order to detect that a word form is a noun (or ad-
jective), we employ heuristic rules related to Arabic
prefixes/suffixes and if none of these rules apply we
attempt to match the word against templatic struc-
tures. Using this methodology, we are able to detect
95% of ATB nouns.6
We define a list of 42 noun templates which are
used to indicate active/passive participle nouns, ver-
bal nouns, nouns of instrument and broken plural
nouns (see Table 4 for some examples). Note that
templates ending with taa marboutah ?ap? or start-
ing with meem madmoumah ?mu? are not consid-
ered since they are covered by our suffix/prefix rules,
which are as follows:
1- The definite article prefix ?  or in Buckwalter
transliteration ?Al?.
2- The tanween suffix

, 

,

 or ?N?, ?F?, ?K?, ?AF?.
3- The feminine plural suffix HA, or ?+At?.
4- The taa marboutah ending ? or ?ap? whether as a
6The heuristics we developed are designed to work on dia-
critized texts. Although diacritics are generally ignored in mod-
ern writing, the issue of restoring diacritics has been satisfac-
torily addressed by different researchers. For example, Nelken
and Shieber (2005) presented an algorithm for restoring diacrit-
ics to undiacritized MSA texts with an accuracy of over 90%
and Habasah et al (2009) reported on a freely-available toolkit
(MADA-TOKAN) an accuracy of over 96%.
71
Unknown Threshold Recall Precision F-Score Tagging Accuracy
Arabic
1 80.67 82.19 *81.42 (+ 1.89) 96.32
5 80.66 82.81 *81.72 (+ 3.25) 95.15
10 79.86 82.49 *81.15 (+ 4.18) 94.38
English
1 ***89.64 89.95 89.79 (+ 0.32) 96.44
5 89.16 89.80 89.48 (+ 0.15) 96.32
10 89.14 89.78 **89.46 (+ 0.98) 96.21
French
1 85.15 85.77 *85.46 (+ 1.58) 96.13
5 84.08 84.80 *84.44 (+ 1.74) 95.54
10 84.21 84.78 *84.49 (+ 3.04) 94.68
Table 3: Baseline Signatures for Arabic, French and English
statistically significant with *:p < 10?4, **: p < 10?3, ***: p < 0.004,
Template Name Regular Specification
Arabic Buckwalter Expression
?A

?
	
?

	
K 

{inofiEAl {ino.i.A. verbal noun (masdar)
?A

?
	
??

mifoEAl mi.o.A. noun instrument
??

	
?


J?

? musotafoEil musota.o.i. noun participle
?J


?

A

	
?

? mafAEiyl ma.A.iy. noun plural
?

?
	
?


J?

{isotafoEal {isota.o.a. verb
??

?

	
? fuwEil .uw.i. verb passive
Table 4: Sample Arabic Templatic Structures for Nouns and Verbs
feminine marker suffix or part of the word.
5- The genitive case marking kasrah 

, or ?+i?.
6- Words of length of at least five characters ending
with doubled yaa ?


or ?y??.
7- Words of length of at least six characters ending
with alif mamdoudah and hamzah Z  or ?A??.
8- Words of length of at least seven characters start-
ing with meem madmoumah ? or ?mu?.
6.3 Verb Indicators
In the same way, we define a list of 16 templates and
we combine them with heuristic rules related to Ara-
bic prefixes/suffixes to detect whether a word form
is exclusively a verb. The prefix/suffix heuristics are
as follows:
9-The plural marker suffix  ? or ?uwA? indicates a
verb.
10- The prefixes H , ?


,
	
?
,



,

? or ?sa?, ?>a?,
?>u?, ?na?, ?nu?, ?ya?, ?yu?, ?ta?, ?tu? indicate im-
prefective verb.
The verbal templates are less in number than the
noun templates yet they are no less effective in de-
tecting the word class (see Table 4 for examples).
Using these heuristics we are able to detect 85% of
ATB verbs.
6.4 Arabic Signatures
We map the 72 noun/verb classes that are identi-
fied using our hand-crafted heuristics into sets of
signatures of varying sizes: 4, 6, 14, 21, 25, 28
and 72. The very coarse-grained set considers just
4 signatures UNK-noun, UNK-verb, UNK-num,
and UNK and the most fine-grained set of 72 signa-
tures associates one signature per heuristic. In ad-
dition, we have evaluated the effect of reordering
rules and templates and also the effect of collating
all signatures satisfying an unknown word. The re-
sults of using these various signatures sets in parsing
72
UNK
NUM NOUN VERB
digits (see section 6.2) (see section 6.3)
Al definiteness tashkil At suffix ap suffix imperfect
rule 1 rules 2 and 5 rule 3 rule 4 rule 10
y? suffix A? suffix mu prefix verbal noun templates suffixes
rule 6 rule 7 rule 8 3 groupings dual/plural suffixes
plural templates participle active templates participle passive templates instrument templates passive templates
4 groupings
other templates verbal templates
5 groupings
Table 6: Arabic signatures
Cutoff 1 5 10
4 80.78 80.71 80.09
6 81.14 81.16 81.06
14 80.88 81.45 81.19
14 reorder 81.39 81.01 80.81
21 81.38 81.55 81.35
21 reorder 81.20 81.13 80.58
21 collect 80.94 80.56 79.63
25 81.18 81.25 81.26
28 81.42 81.72 (+ 3.25) 81.15
72 79.64 78.87 77.58
Table 5: Baseline Signatures for Arabic
our Arabic development set are presented in Table 5.
We achieve our best labeled bracketing f-score using
28 signatures with an unknown threshold of five. In
fact we get an improvement of 3.25% over using no
signatures at all (see Table 2). Table 3 describes in
more detail the scores obtained using the 28 signa-
tures present in Table 6. Apart from the set contain-
ing 72 signatures, all of the baseline signature sets in
Table 5 yield a statistically significant improvement
over the generic UNKNOWN results (p < 10?4).
7 Using Information Gain to Determine
Signatures
It is clear that dividing the UNKNOWN terminal into
more fine-grained categories based on morpholog-
ical information helps parsing for our three lan-
guages. In this section we explore whether useful
morphological clues can be learnt automatically. If
they can, it means that a latent-variable PCFG parser
can be adapted to any language without knowledge
of the language in question since the only language-
specific component in such a parser is the unknown-
signature specification.
In a nutshell, we extract affix features from train-
ing set words7 and then use information gain to rank
these features in terms of their predictive power in a
POS-tagging task. The features deemed most dis-
criminative are then used as signatures, replacing
our baseline signatures described in Sections 5 and
6. We are not going as far as actual POS-tagging,
but rather seeing whether the affixes that make good
features for a part-of-speech tagger also make good
unknown word signatures.
We experiment with English and French suffixes
of length 1-3 and Arabic prefixes and suffixes of var-
ious lengths as well as stem prefixes and suffixes of
length 2, 4 and 6. For each of our languages we
experiment with several information gain thresholds
on our development sets and we fix on an English
signature list containing 24 suffixes, a French list
containing 48 suffixes and an Arabic list containing
38 prefixes and suffixes.
Our development set results are presented in Ta-
ble 7. For all three languages, the information gain
signatures perform at a comparable level to the base-
line hand-crafted signatures (Table 3). For each
of the three unknown-word handling techniques, no
signature (UNKNOWN), hand-crafted signatures and
information gain signatures, we select the best un-
known threshold for each language?s development
set and apply these grammars to our test sets. The
f-scores are presented in Table 8, along with the up-
per bounds obtained by parsing with these grammars
in gold-tag mode. For French, the effect of tagging
accuracy on overall parse accuracy is striking. The
improvements that we get from using morphological
signatures are greatest for Arabic8 and smallest for
7We omit all function words and high frequency words be-
cause we are interested in the behaviour of words which are
likely to be similar to rare words.
8Bikel?s parser trained on the same Arabic data and tested
on the same input achieves an f-score of 76.50%. We trained
a 5-split-merge-iteration Berkeley grammar and parsed with the
73
Unknown Threshold Recall Precision F-Score Tagging Accuracy
Arabic IG
1 80.10 82.15 *81.11 (+ 1.58) 96.53
5 80.03 82.49 *81.32 (+ 2.85) 95.30
10 80.17 82.40 *81.27 (+ 4.3) 94.66
English IG
1 89.38 89.87 89.63 (+ 0.16) 96.45
5 89.54 90.22 ***89.88 (+ 0.55) 96.41
10 89.22 90.05 *89.63 (+ 1.15) 96.19
French IG
1 84.78 85.36 *85.07 (+ 1.19) 96.17
5 84.63 85.24 **84.93 (+ 2.23) 95.30
10 84.18 84.80 *84.49 (+ 3.09) 94.68
Table 7: Information Gain Signature Results
statistically significant with *:p < 10?4, **: p < 2 ? 10?4, ***: p < 0.005
Language No Sig Baseline Sig IG Sig
Arabic 78.34 *81.59 *81.33
Arabic Gold Tag 81.46 82.43 81.90
English 89.48 89.65 89.77
English Gold Tag 89.94 90.10 90.23
French 83.74 *85.77 **85.55
French Gold Tag 88.82 88.41 88.86
statistically significant with *: p < 10?4, **: p < 10?3
Table 8: F-Scores on Test Sets
English. The results for the information gain signa-
tures are promising and warrant further exploration.
8 Conclusion
We experiment with two unknown-word-handling
techniques in a statistical generative parsing model,
applying them to Arabic, French and English. One
technique is language-agnostic and the other makes
use of some morphological information (signatures)
in assigning part-of-speech tags to unknown words.
The performance differences from the two tech-
niques are smallest for English, the language with
the sparsest morphology of the three and the small-
est proportion of unknown words in its development
set. As a result of carrying out these experiments,
we have developed a list of Arabic signatures which
can be used with any statistical parser which does
Berkeley parser, achieving an f-score of 75.28%. We trained the
Berkeley parser with the -treebank SINGLEFILE option so that
English signatures were not employed.
its own tagging. We also present results which show
that signatures can be learnt automatically.
Our experiments have been carried out using gold
tokens. Tokenisation is an issue particularly for Ara-
bic, but also for French (since the treebank contains
merged compounds) and to a much lesser extent for
English (unedited text with missing apostrophes). It
is important that the experiments in this paper are re-
peated on untokenised text using automatic tokeni-
sation methods (e.g. MADA-TOKAN).
The performance improvements that we demon-
strate for Arabic unknown-word handling are obvi-
ously just the tip of the iceberg in terms of what can
be done to improve performance on a morpholog-
ically rich language. The simple generative lexical
probability model we use can be improved by adopt-
ing a more sophisticated approach in which known
and unknown word counts are combined when esti-
mating lexical rule probabilities for rare words (see
Huang and Harper (2009) and the Berkeley sophis-
ticatedLexicon training option). Further work will
also include making use of a lexical resource exter-
nal to the treebank (Goldberg et al, 2009; Habash,
2008) and investigating clustering techniques to re-
duce data sparseness (Candito and Crabbe?, 2009).
Acknowledgements
This research is funded by Enterprise Ireland
(CFTD/07/229 and PC/09/037) and the Irish Re-
search Council for Science Engineering and Tech-
nology (IRCSET). We thank Marie Candito and our
three reviewers for their very helpful suggestions.
74
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel,
2003. Treebanks: Building and Using Parsed
Corpora, chapter Building a Treebank for French.
Kluwer, Dordrecht.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
French. In ACL. The Association for Computer Lin-
guistics.
Kenneth R. Beesley and Lauri Karttunen. 2003. Finite
State Morphology. CSLI studies in computational lin-
guistics.
Kenneth R. Beesley. 1998. Arabic morphology using
only finite-state operations. In The Workshop on Com-
putational Approaches to Semitic Languages.
Ann Bies and Mohammed Maamouri. 2003. Penn Ara-
bic Treebank guidelines. Technical Report TB-1-28-
03.
Dan Bikel. 2004. On the Parameter Space of Generative
Lexicalized Parsing Models. Ph.D. thesis, University
of Pennslyvania.
Marie Candito and Benoit Crabbe?. 2009. Improving gen-
erative statistical parsing with semi-supervised word
clustering. In Proceedings of IWPT?09.
Marie Candito, Beno??t Crabbe?, and Djame? Seddah. 2009.
On statistical parsing of French with supervised and
semi-supervised strategies. In Proceedings of the
EACL 2009 Workshop on Computational Linguis-
tic Aspects of Grammatical Inference, pages 49?57,
Athens, Greece, March.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the Annual Meeting of the
North American Association for Computational Lin-
guistics (NAACL-00), pages 132?139, Seattle, Wash-
ington.
Beno??t Crabbe? and Marie Candito. 2008. Expe?riences
d?analyse syntaxique statistique du franc?ais. In Actes
de TALN.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities.
In EACL, pages 327?335. The Association for Com-
puter Linguistics.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
Mada+tokan: A toolkit for Arabic tokenization, di-
acritization, morphological disambiguation, pos tag-
ging, stemming and lemmatization. In Proceedings of
the 2nd International Conference on Arabic Language
Resources and Tools (MEDAR).
Nizar Habash. 2008. Four techniques for online handling
of out-of-vocabulary words in arabic-english statistical
machine translation. In Proceedings of Association for
Computational Linguistics, pages 57?60.
Zhongqiang Huang and Mary Harper. 2009. Self-
training pcfg grammars with latent annotations across
languages. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
Singapore, August.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Dan Klein and Chris Manning. 2003. Accurate unlex-
icalised parsing. In Proceedings of the 41st Annual
Meeting of the ACL.
Mohammed Maamouri and Ann Bies. 2004. Developing
an Arabic Treebank: Methods, guidelines, procedures,
and tools. In Workshop on Computational Approaches
to Arabic Script-based Languages, COLING.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
Proceedings of the 1994 ARPA Speech and Natural
Language Workshop, pages 114?119, Princeton, New
Jersey.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting of the ACL,
pages 75?82, Ann Arbor, June.
Rani Nelken and Stuart M. Shieber. 2005. Arabic dia-
critization using weighted finite-state transducers. In
ACL-05 Workshop on Computational Approaches to
Semitic Languages.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL, Rochester, NY, April.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the ACL, Sydney,
Australia, July.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Berkeley, Berkeley, CA, USA.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing (NeMLaP-1), pages 44?49.
Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005. Morphological features help POS tagging
of unknown words across language varieties. In Pro-
ceedings of the Fourth SIGHAN Workshop on Chinese
Language Processing.
75
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 85?93,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Lemmatization and Lexicalized Statistical Parsing of Morphologically Rich
Languages: the Case of French
Djam? Seddah
Alpage Inria & Univ. Paris-Sorbonne
Paris, France
Grzegorz Chrupa?a
Spoken Language System, Saarland Univ.
Saarbr?cken, Germany
?zlem ?etinog?lu and Josef van Genabith
NCLT & CNGL, Dublin City Univ.
Dublin, Ireland
Marie Candito
Alpage Inria & Univ. Paris 7
Paris, France
Abstract
This paper shows that training a lexicalized
parser on a lemmatized morphologically-rich
treebank such as the French Treebank slightly
improves parsing results. We also show that
lemmatizing a similar in size subset of the En-
glish Penn Treebank has almost no effect on
parsing performance with gold lemmas and
leads to a small drop of performance when au-
tomatically assigned lemmas and POS tags are
used. This highlights two facts: (i) lemmati-
zation helps to reduce lexicon data-sparseness
issues for French, (ii) it also makes the pars-
ing process sensitive to correct assignment of
POS tags to unknown words.
1 Introduction
Large parse-annotated corpora have led to an explo-
sion of interest in statistical parsing methods, includ-
ing the development of highly successful models for
parsing English using the Wall Street Journal Penn
Treebank (PTB, (Marcus et al, 1994)). Over the
last 10 years, parsing performance on the PTB has
hit a performance plateau of 90-92% f-score using
the PARSEVAL evaluation metric. When adapted to
other language/treebank pairs (such as German, He-
brew, Arabic, Italian or French), to date these mod-
els have performed much worse.
A number of arguments have been advanced
to explain this performance gap, including limited
amounts of training data, differences in treebank an-
notation schemes, inadequacies of evaluation met-
rics, linguistic factors such as the degree of word or-
der variation, the amount of morphological informa-
tion available to the parser as well as the effects of
syncretism prevalent in many morphologically rich
languages.
Even though none of these arguments in isola-
tion can account for the systematic performance gap,
a pattern is beginning to emerge: morphologically
rich languages tend to be susceptible to parsing per-
formance degradation.
Except for a residual clitic case system, French
does not have explicit case marking, yet its mor-
phology is considerably richer than that of English,
and French is therefore a candidate to serve as an
instance of a morphologically rich language (MRL)
that requires specific treatment to achieve reasonable
parsing performance.
Interestingly, French also exhibits a limited
amount of word order variation occurring at dif-
ferent syntactic levels including (i) the word level
(e.g. pre or post nominal adjective, pre or post ver-
bal adverbs); (ii) phrase level (e.g. possible alterna-
tions between post verbal NPs and PPs). In order
to avoid discontinuous constituents as well as traces
and coindexations, treebanks for this language, such
as the French Treebank (FTB, (Abeill? et al, 2003))
or the Modified French Treebank (MFT, (Schluter
and van Genabith, 2007)), propose a flat annota-
tion scheme with a non-configurational distinction
between adjunct and arguments.
Finally, the extraction of treebank grammars from
the French treebanks, which contain less than a third
of the annotated data as compared to PTB, is subject
to many data sparseness issues that contribute to a
performance ceiling, preventing the statistical pars-
ing of French to reach the same level of performance
as for PTB-trained parsers (Candito et al, 2009).
This data sparseness bottleneck can be summa-
rized as a problem of optimizing a parsing model
along two axes: the grammar and the lexicon. In
both cases, the goal is either to get a more compact
grammar at the rule level or to obtain a consider-
85
ably less sparse lexicon. So far, both approaches
have been tested for French using different means
and with different degrees of success.
To obtain better grammars, Schluter and van Gen-
abith (2007) extracted a subset of an early release
of the FTB and carried out extensive restructuring,
extensions and corrections (referred to as the Modi-
fied French Treebank MFT) to support grammar ac-
quisition for PCFG-based LFG Parsing (Cahill et
al., 2004) while Crabb? and Candito (2008) slightly
modified the original FTB POS tagset to optimize
the grammar with latent annotations extracted by the
Berkeley parser (BKY, (Petrov et al, 2006)).
Moreover, research oriented towards adapting
more complex parsing models to French showed
that lexicalized models such as Collins? model 2
(Collins, 1999) can be tuned to cope effectively with
the flatness of the annotation scheme in the FTB,
with the Charniak model (Charniak, 2000) perform-
ing particularly well, but outperformed by the BKY
parser on French data (Seddah et al, 2009).
Focusing on the lexicon, experiments have been
carried out to study the impact of different forms of
word clustering on the BKY parser trained on the
FTB. Candito et al (2009) showed that using gold
lemmatization provides a significant increase in per-
formance. Obviously, less sparse lexical data which
retains critical pieces of information can only help a
model to perform better. This was shown in (Candito
and Crabb?, 2009) where distributional word clus-
ters were acquired from a 125 million words corpus
and combined with inflectional suffixes extracted
from the training data. Training the BKY parser
with 1000 clusters boosts its performance to the cur-
rent state-of-the-art with a PARSEVAL F1 score of
88.28% (baseline was 86.29 %).
We performed the same experiment using the
CHARNIAK parser and recorded only a small im-
provement (from 84.96% to 85.51%). Given the
fact that lexical information is crucial for lexicalized
parsers in the form of bilexical dependencies, this
result raises the question whether this kind of clus-
tering is in fact too drastic for lexicalized parsers as
it may give rise to head-to-head dependencies which
are too coarse. To answer this question, in this paper
we explore the impact of lemmatization, as a (rather
limited) constrained form of clustering, on a state-
of-the-art lexicalized parser (CHARNIAK). In order
to evaluate the influence of lemmatization on this
parser (which is known to be highly tuned for En-
glish) we carry out experiments on both the FTB and
on a lemmatized version of the PTB. We used gold
lemmatization when available and an automatic sta-
tistical morphological analyzer (Chrupa?a, 2010) to
provide more realistic parsing results.
The idea is to verify whether lemmatization will help
to reduce data sparseness issues due to the French
rich morphology and to see if this process, when
applied to English will harm the performance of a
parser optimized for the limited morphology of En-
glish.
Our results show that the key issue is the way un-
seen tokens (lemmas or words) are handled by the
CHARNIAK parser. Indeed, using pure lemma is
equally suboptimal for both languages. On the other
hand, feeding the parser with both lemma and part-
of-speech slightly enhances parsing performance for
French.
We first describe our data sets in Section 2, intro-
duce our data driven morphology process in Section
3, then present experiments in Section 4. We dis-
cuss our results in Section 5 and compare them with
related research in Section 6 before concluding and
outlining further research.
2 Corpus
THE FRENCH TREEBANK is the first annotated and
manually corrected treebank for French. The data is
annotated with labeled constituent trees augmented
with morphological annotations and functional an-
notations of verbal dependents. Its key properties,
compared with the PTB, are the following :
Size: The FTB consists of 350,931 tokens and
12,351 sentences, that is less than a third of the size
of PTB. The average length of a sentence is 28.41
tokens. By contrast, the average sentence length in
the Wall Street Journal section of the PTB is 25.4
tokens.
A Flat Annotation Scheme: Both the FTB and the
PTB are annotated with constituent trees. However,
the annotation scheme is flatter in the FTB. For in-
stance, there are no VPs for finite verbs and only one
sentential level for clauses or sentences whether or
not they are introduced by a complementizer. Only
the verbal nucleus (VN) is annotated and comprises
86
the verb, its clitics, auxiliaries, adverbs and nega-
tion.
Inflection: French morphology is richer than En-
glish and leads to increased data sparseness for sta-
tistical parsing. There are 24,098 lexical types in
the FTB, with an average of 16 tokens occurring for
each type.
Compounds: Compounds are explicitly annotated
and very frequent in the treebank: 14.52% of to-
kens are part of a compound. Following Candito
and Crabb? (2009), we use a variation of the tree-
bank where compounds with regular syntactic pat-
terns have been expanded. We refer to this instance
as FTB-UC.
Lemmatization: Lemmas are included in the tree-
bank?s morphological annotations and denote an ab-
straction over a group of inflected forms. As there
is no distinction between semantically ambiguous
lexemes at the word form level, polysemic homo-
graphs with common inflections are associated with
the same lemma (Abeill? et al, 2003). Thus, except
for some very rare cases, a pair consisting of a word
form and its part-of-speech unambiguously maps to
the same lemma.
2.1 Lemmatizing the Penn Treebank
Unlike the FTB, the PTB does not have gold lem-
mas provided within the treebank. We use the finite
state morphological analyzer which comes within
the English ParGram Grammar (Butt et al, 1999) for
lemmatization. For open class words (nouns, verbs,
adjectives, adverbs) the word form is sent to the mor-
phological analyzer. The English ParGram morpho-
logical analyzer outputs all possible analyses of the
word form. The associated gold POS from the PTB
is used to disambiguate the result. The same process
is applied to closed class words where the word form
is different from the lemma (e.g. ?ll for will). For the
remaining parts of speech the word form is assigned
to the lemma.
Since gold lemmas are not available for the PTB,
a large-scale automatic evaluation of the lemmatizer
is not possible. Instead, we conducted two manual
evaluations. First, we randomly extracted 5 sam-
ples of 200 <POS,word> pairs from Section 23 of
the PTB. Each data set is fed into the lemmatiza-
tion script, and the output is manually checked. For
the 5x200 <POS,word> sets the number of incorrect
lemmas is 1, 3, 2, 0, and 2. The variance is small
indicating that the results are fairly stable. For the
second evaluation, we extracted each unseen word
from Section 23 and manually checked the accuracy
of the lemmatization. Of the total of 1802 unseen
words, 394 words are associated with an incorrect
lemma (331 unique) and only 8 with an incorrect
<POS,lemma> pair (5 unique). For an overall un-
seen word percentage of 3.22%, the lemma accu-
racy is 77.70%. If we assume that all seen words
are correctly lemmatized, overall accuracy would be
99.28%.
2.2 Treebank properties
In order to evaluate the influence of lemmatization
on comparable corpora, we extracted a random sub-
set of the PTB with properties comparable to the
FTB-UC (mainly with respect to CFG size and num-
ber of tokens). We call this PTB subset S.PTB. Ta-
ble 1 presents a summary of some relevant features
of those treebanks.
FTBUC S.PTB PTB
# of tokens 350,931 350,992 1,152,305
# of sentences 12,351 13,811 45,293
average length 28,41 25.41 25.44
CFG size 607,162 638,955 2,097,757
# unique CFG rules 43,413 46,783 91,027
# unique word forms 27,130 26,536 47,678
# unique lemmas 17,570 20,226 36,316
ratio words/lemma 1.544 1.311 1.312
Table 1: French and Penn Treebanks properties
Table 1 shows that the average number of word
forms associated with a lemma (i.e. the lemma ratio)
is higher in the FTB-UC (1.54 words/lemma) than in
the PTB (1.31). Even though the PTB ratio is lower,
it is still large enough to suggest that even the limited
English morphology should be taken into account
when aiming at reducing lexicon sparseness.
Trying to learn French and English morphology
in a data driven fashion in order to predict lemma
from word forms is the subject of the next section.
3 Morphology learning
In order to assign morphological tags and lemmas
to words we use the MORFETTE model (Chrupa?a,
2010), which is a variation of the approach described
in (Chrupa?a et al, 2008).
87
MORFETTE is a sequence labeling model which
combines the predictions of two classification mod-
els (one for morphological tagging and one for
lemmatization) at decoding time, using beam search.
3.1 Overview of the Morfette model
The morphological classes correspond simply to the
(fine-grained) POS tags. Lemma classes are edit
scripts computed from training data: they specify
which string manipulations (such as character dele-
tions and insertions) need to be performed in order
to transform the input string (word form) into the
corresponding output string (lemma).
The best sequence of lemmas and morphological
tags for input sentence x is defined as:
(?l, m?) = arg max
(l,m)
P (l,m|x)
The joint probability is decomposed as follows:
P (l0...li,m0...mi|x) =PL(li|mi,x)PM (mi|x)
? P (m0...mi?1, l0...li?1|x)
where PL(li|mi,x) is the probability of lemma class
l at position i according to the lemma classifier,
PM (mi|x) is the probability of the tag m at posi-
tion i according to the morphological tag classifier,
and x is the sequence of words to label.
While Chrupa?a et al (2008) use Maximum En-
tropy training to learn PM and PL, here we learn
them using Averaged Perceptron algorithm due to
Freund and Schapire (1999). It is a much simpler
algorithm which in many scenarios (including ours)
performs as well as or better than MaxEnt.
We also use the general Edit Tree instantiation of
the edit script as developed in (Chrupa?a, 2008). We
find the longest common substring (LCS) between
the form w and the lemma w?. The portions of the
string in the word form before (prefix) and after (suf-
fix) the LCS need to be modified in some way, while
the LCS (stem) stays the same. If there is no LCS,
then we simply record that we need to replace w
with w? . As for the modifications to the prefix and
the suffix, we apply the same procedure recursively:
we try to find the LCS between the prefix of w and
the prefix of w?. If we find one, we recurse; if we do
not, we record the replacement; we do the same for
the suffix.
3.2 Data Set
We trained MORFETTE on the standard splits of the
FTB with the first 10% as test set, the next 10% for
the development set and the remaining for training
(i.e. 1235/1235/9881 sentences). Lemmas and part-
of-speech tags are given by the treebank annotation
scheme.
As pointed out in section 2.1, PTB?s lemmas have
been automatically generated by a deterministic pro-
cess, and only a random subset of them have been
manually checked. For the remainder of this paper,
we treat them as gold, regardless of the errors in-
duced by our PTB lemmatizer.
The S.PTB follows the same split as the FTB-UC,
first 10% for test, next 10% for dev and the last 80%
for training (i.e. 1380/1381/11050 sentences).
MORFETTE can optionally use a morphological
lexicon to extract features. For French, we used the
extended version of Lefff (Sagot et al, 2006) and for
English, the lexicon used in the Penn XTAG project
(Doran et al, 1994). We reduced the granularity of
the XTAG tag set, keeping only the bare categories.
Both lexicons contain around 225 thousands word
form entries.
3.3 Performance on French and English
Table 2 presents results of MORFETTE applied to the
development and test sets of our treebanks. Part-of-
speech tagging performance for French is state-of-
the-art on the FTB-UC, with an accuracy of 97.68%,
on the FTB-UC test set, only 0.02 points (absolute)
below the MaxEnt POS tagger of Denis and Sagot
(2009). Comparing MORFETTE?s tagging perfor-
mance for English is a bit more challenging as we
only trained on one third of the full PTB and evalu-
ated on approximately one section, whereas results
reported in the literature are usually based on train-
ing on sections 02-18 and evaluating on either sec-
tions 19-21 or 22-24. For this setting, state-of-the-
art POS accuracy for PTB tagging is around 97.33%.
On our PTB sample, MORFETTE achieves 96.36%
for all words and 89.64 for unseen words.
Comparing the lemmatization performance for both
languages on the same kind of data is even more dif-
ficult as we are not aware of any data driven lem-
matizer on the same data. However, with an overall
accuracy above 98% for the FTB-UC (91.5% for un-
88
seen words) and above 99% for the S.PTB (95% for
unseen words), lemmatization performs well enough
to properly evaluate parsing on lemmatized data.
FTBUC S.PTB
DEV All Unk. (4.8) All Unk. (4.67)
POS acc 97.38 91.95 96.36 88.90
Lemma acc 98.20 92.52 99.11 95.51
Joint acc 96.35 87.16 96.26 87.05
TEST All Unk. (4.62) All Unk. (5.04)
POS acc 97.68 90.52 96.53 89.64
Lemma acc 98.36 91.54 99.13 95.72
Joint acc 96.74 85.28 96.45 88.49
Table 2: POS tagging and lemmatization performance on
the FTB and on the S.PTB
4 Parsing Experiments
In this section, we present the results of two sets
of experiments to evaluate the impact of lemmatiza-
tion on the lexicalized statistical parsing of two lan-
guages, one morphologically rich (French), but with
none of its morphological features exploited by the
CHARNIAK parser, the other (English) being quite
the opposite, with the parser developed mainly for
this language and PTB annotated data. We show that
lemmatization results in increased performance for
French, while doing the same for English penalizes
parser performance.
4.1 Experimental Protocol
Data The data sets described in section 3.2 are used
throughout. The version of the CHARNIAK parser
(Charniak, 2000) was released in August 2005 and
recently adapted to French (Seddah et al, 2009).
Metrics We report results on sentences of length
less than 40 words, with three evaluation met-
rics: the classical PARSEVAL Labeled brackets F1
score, POS tagging accuracy (excluding punctua-
tion tags) and the Leaf Ancestor metric (Sampson
and Babarczy, 2003) which is believed to be some-
what more neutral with respect to the treebank an-
notation scheme than PARSEVAL (Rehbein and van
Genabith, 2007).
Treebank tag sets Our experiments involve the in-
clusion of POS tags directly in tokens. We briefly
describe our treebank tag sets below.
? FTB-UC TAG SET: ?CC? This is the tag set de-
veloped by (Crabb? and Candito, 2008) (Table
4), known to provide the best parsing perfor-
mance for French (Seddah et al, 2009). Like in
the FTB, preterminals are the main categories,
but they are also augmented with a WH flag
for A, ADV, PRO and with the mood for verbs
(there are 6 moods). No information is propa-
gated to non-terminal symbols.
ADJ ADJWH ADV ADVWH CC CLO CLR CLS CS DET
DETWH ET I NC NPP P P+D P+PRO PONCT PREF PRO
PROREL PROWH V VIMP VINF VPP VPR VS
Table 4: CC tag set
? THE PTB TAG SET This tag set is described
at length in (Marcus et al, 1994) and contains
supplementary morphological information (e.g.
number) over and above what is represented in
the CC tag set for French. Note that some infor-
mation is marked at the morphological level in
English (superlative, ?the greatest (JJS)?) and
not in French (? le plus (ADV) grand (ADJ)?).
CC CD DT EX FW IN JJ JJR JJS LS MD NN NNP NNPS
NNS PDT POS PRP PRP$ RB RBR RBS RP SYM TO UH
VB VBD VBG VBN VBP VBZ WDT WP WP$ WRB
Table 5: PTB tag set
4.2 Cross token variation and parsing impact
From the source treebanks, we produce 5 versions
of tokens: tokens are generated as either simple
POS tag, gold lemma, gold lemma+gold POS, word
form, and word form+gold POS. The token versions
successively add more morphological information.
Parsing results are presented in Table 3.
Varying the token form The results show that
having no lexical information at all (POS-only) re-
sults in a small drop of PARSEVAL performance for
French compared to parsing lemmas, while the cor-
responding Leaf Ancestor score is actually higher.
For English having no lexical information at all
leads to a drop of 2 points in PARSEVAL. The so-
called impoverished morphology of English appears
to bring enough morphological information to raise
tagging performance to 95.92% (from POS-only to
word-only).
For French the corresponding gain is only 2 points
of POS tagging accuracy. Moreover, between these
89
Tokens
POS-only
lemma-only
word-only
(1)lemma-POS
(1)word-POS
French Treebank UC
F1 score Pos acc. leaf-Anc.
84.48 100 93.97
84.77 94.23 93.76
84.96 96.26 94.08
86.83(1) 98.79 94.65
86.13(2) 98.4 94.46
Sampled Penn Treebank
F1 score Pos acc. leaf-Anc.
85.62 100 94.02
87.69 89.22 94.92
88.64 95.92 95.10
89.59(3) 99.97 95.41
89.53(4) 99.96 95.38
Table 3: Parsing performance on the FTB-UC and the S.PTB with tokens variations using gold lemmas and gold POS.
( p-value (1) & (2) = 0.007; p-value (3) & (4) = 0.146. All other configurations are statistically significant.)
two tokens variations, POS-only and word-only,
parsing results gain only half a point in PARSEVAL
and almost nothing in leaf Ancestor.
Thus, it seems that encoding more morphology
(i.e. including word forms) in the tokens does not
lead to much improvement for parsing French as op-
posed to English. The reduction in data sparseness
due to the use of lemmas alone is thus not sufficient
to counterbalance the lack of morphological infor-
mation.
However, the large gap between POS tagging
accuracy seen between lemma-only and word-only
for English indicates that the parser makes use of
this information to provide at least reasonable POS
guesses.
For French, only 0.2 points are gained for PAR-
SEVAL results between lemma-only to word-only,
while POS accuracy benefits a bit more from includ-
ing richer morphological information.
This raises the question whether the FTB-UC pro-
vides enough data to make its richer morphology in-
formative enough for a parsing model.
Suffixing tokens with POS tags It is only when
gold POS are added to the lemmas that one can see
the advantage of a reduced lexicon for French. In-
deed, performance peaks for this setting (lemma-
POS). The situation is not as clear for English, where
performance is almost identical when gold POS are
added to lemmas or words. POS Tagging is nearly
perfect, thus a performance ceiling is reached. The
very small differences between those two configura-
tions (most noticeable with the Leaf Ancestor score
of 95.41 vs. 95.38) indicates that the reduced lemma
lexicon is actually of some limited use but its impact
is negligible compared to perfect tagging.
While the lemma+POS setting clearly boosts per-
formance for parsing the FTB, the situation is less
clear for English. Indeed, the lemma+POS and the
word+POS gold variations give almost the same re-
sults. The fact that the POS tagging accuracy is close
to 100% in this mode shows that the key parameter
for optimum parsing performance in this experiment
is the ability to guess POS for unknown words well.
In fact, the CHARNIAK parser uses a two letter
suffix context for its tagging model, and when gold
POS are suffixed to any type of token (being lemma
or word form), the PTB POS tagset is used as a sub-
stitute for lack of morphology.
It should also be noted that the FTB-UC tag set
does include some discriminative features (such as
PART, INF and so on) but those are expressed by
more than two letters, and therefore a two letter
suffix tag cannot really be useful to discriminate
a richer morphology. For example, in the PTB,
the suffix BZ, as in VBZ, always refers to a verb,
whereas the FTB pos tag suffix PP, as in NPP
(Proper Noun) is also found in POS labels such as
VPP (past participle verb).
4.3 Realistic Setup: Using Morfette to help
parsing
Having shown that parsing French benefits from a
reduced lexicon is not enough as results imply that a
key factor is POS tag guessing. We therefore test our
hypothesis in a more realistic set up. We use MOR-
FETTE to lemmatize and tag raw words (instead of
the ?gold? lemma-based approach described above),
and the resulting corpus is then parsed using the cor-
responding training set.
In order to be consistent with PARSEVAL POS eval-
uation, which does not take punctuation POS into
account, we provide a summary of MORFETTE?s
performance for such a configuration in (Table 6).
Results shown in Table 7 confirm our initial hy-
90
POS acc Lemma acc Joint acc
FTB-UC 97.34 98.12 96.26
S.PTB 96.15 99.04 96.07
Table 6: PARSEVAL Pos tagging accuracy of treebanks
test set
pothesis for French. Indeed, parsing performance
peaks with a setup involving automatically gener-
ated lemma and POS pairs, even though the differ-
ence with raw words+auto POS is not statistically
significant for the PARSEVAL F1 metric1. Note that
parser POS accuracy does not follow this pattern. It
is unclear exactly why this is the case. We specu-
late that the parser is helped by the reduced lexicon
but that performance suffers when a <lemma,POS>
pair has been incorrectly assigned by MORFETTE,
leading to an increase in unseen tokens. This is con-
firmed by parsing the same lemma but with gold
POS. In that case, parsing performance does not suf-
fer too much from CHARNIAK?s POS guessing on
unseen data.
For the S.PTB, results clearly show that both the
automatic <lemma,POS> and <word,POS> config-
urations lead to very similar results (yet statistically
significant with a F1 p-value = 0.027); having the
same POS accuracy indicates that most of the work
is done at the level of POS guessing for unseen
tokens, and in this respect the CHARNIAK parser
clearly takes advantage of the information included
in the PTB tag set.
F1 score Pos acc. leaf-Anc.
S.PTB
auto lemma only 87.11 89.82 94.71
auto lemma+auto pos (a) 88.15 96.21 94.85
word +auto pos (b) 88.28 96.21 94.88
F1 p-value: (a) and (b) 0.027
auto lemma+gold pos 89.51 99.96 95,36
FTB-UC
auto lemma only 83.92 92.98 93.53
auto lemma+auto pos (c) 85.06 96.04 94.14
word +auto pos (d) 84.99 96.47 94.09
F1 p-value: (c) and (d) 0.247
auto lemma+gold pos 86.39 97.35 94.68
Table 7: Realistic evaluation of parsing performance
1Statistical significance is computed using Dan Bikel?s
stratified shuffling implementation: www.cis.upenn.edu/
~dbikel/software.html.
5 Discussion
When we started this work, we wanted to explore
the benefit of lemmatization as a means to reduce
data sparseness issues underlying statistical lexical-
ized parsing of small treebanks for morphologically
rich languages, such as the FTB. We showed that
the expected benefit of lemmatization, a less sparse
lexicon, was in fact hidden by the absence of inflec-
tional information, as required by e.g. the CHAR-
NIAK parser to provide good POS guesses for un-
seen words. Even the inclusion of POS tags gen-
erated by a state-of-the-art tagger (MORFETTE) did
not lead to much improvement compared to a parser
run in a regular bare word set up.
An unexpected effect is that the POS accuracy
of the parser trained on the French data does not
reach the same level of performance as our tag-
ger (96.47% for <word, auto POS> vs. 97.34% for
MORFETTE). Of course, extending the CHARNIAK
tagging model to cope with lemmatized input should
be enough, because its POS guessing model builds
on features such as capitalization, hyphenation and
a two-letter suffix (Charniak, 2000). Those features
are not present in our current lemmatized input and
thus cannot be properly estimated.
CHARNIAK also uses the probability that a given
POS is realized by a previously unobserved word.
If any part of a <lemma,POS> pair is incorrect, the
number of unseen words in the test set would be
higher than the one estimated from the training set,
which only contained correct lemmas and POS tags
in our setting. This would lead to unsatisfying POS
accuracy. This inadequate behavior of the unknown
word tagging model may be responsible for the POS
accuracy result for <auto lemma> (cf. Table 7, lines
<auto lemma only> for both treebanks).
We believe that this performance degradation (or
in this case the somewhat less than expected im-
provement in parsing results) calls for the inclusion
of all available lexical information in the parsing
model. For example, nothing prevents a parsing
model to condition the generation of a head upon
a lemma, while the probability to generate a POS
would depend on both morphological features and
(potentially) the supplied POS.
91
6 Related Work
A fair amount of recent research in parsing morpho-
logically rich languages has focused on coping with
unknowns words and more generally with the small
and limited lexicons acquired from treebanks. For
instance, Goldberg et al (2009) augment the lex-
icon for a generative parsing model by including
lexical probabilities coming from an external lexi-
con. These are estimated using an HMM tagger with
Baum-Welch training. This method leads to a sig-
nificant increase of parsing performance over pre-
viously reported results for Modern Hebrew. Our
method is more stratified: external lexical resources
are included as features for MORFETTE and there-
fore are not directly seen by the parser besides gen-
erated lemma and POS.
For parsing German, Versley and Rehbein (2009)
cluster words according to linear context features.
The clusters are then integrated as features to boost a
discriminative parsing model to cope with unknown
words. Interestingly, they also include all possible
information: valence information, extracted from a
lexicon, is added to verbs and preterminal nodes are
annotated with case/number. This leads their dis-
criminative model to state-of-the-art results for pars-
ing German.
Concerning French, Candito and Crabb? (2009)
present the results of different clustering methods
applied to the parsing of FTB with the BKY parser.
They applied an unsupervised clustering algorithm
on the 125 millions words ?Est Republicain? corpus
to get a reduced lexicon of 1000 clusters which they
then augmented with various features such as capi-
talization and suffixes. Their method is the best cur-
rent approach for the probabilistic parsing of French
with a F1 score (<=40) of 88.29% on the standard
test set. We run the CHARNIAK parser on their clus-
terized corpus. Table 8 summarizes the current state-
of-the-art for lexicalized parsing on the FTB-UC.2
Clearly, the approach consisting in extending clus-
ters with features and suffixes seems to improve
CHARNIAK?s performance more than our method.
2For this comparison, we also trained the CHARNIAK parser
on a disinflected variation of the FTB-UC. Disinflection is a de-
terministic, lexicon based process, standing between stemming
and lemmatization, which preserves POS assignment ambigui-
ties (Candito and Crabb?, 2009).
In that case, the lexicon is drastically reduced, as
well as the amount of out of vocabulary words
(OOVs). Nevertheless, the relatively low POS ac-
curacy, with only 36 OOVs, for this configuration
confirms that POS guessing is the current bottleneck
if a process of reducing the lexicon increases POS
assignment ambiguities.
tokens F1 Pos acc % of OOVs
raw word (a) 84.96 96.26 4.89
auto <lemma,pos> (b) 85.06 96.04 6.47
disinflected (c) 85.45 96.51 3.59
cluster+caps+suffixes (d) 85.51 96.89 0.10
Table 8: CHARNIAK parser performance summary on the
FTB-UC test set (36340 tokens). Compared to (a), all F1 re-
sults, but (b), are statistically significant (p-values < 0.05), dif-
ferences between (c) & (d), (b) & (c) and (b) & (d) are not
(p-values are resp. 0.12, 0.41 and 0.11). Note that the (b) &
(d) p-value for all sentences is of 0.034, correlating thus the
observed gap in parsing performance between these two con-
figuration.
7 Conclusion
We showed that while lemmatization can be of
some benefit to reduce lexicon size and remedy data
sparseness for a MRL such as French, the key factor
that drives parsing performance for the CHARNIAK
parser is the amount of unseen words resulting from
the generation of <lemma,POS> pairs for the FTB-
UC. For a sample of the English PTB, morphologi-
cal analysis did not produce any significant improve-
ment.
Finally, even if this architecture has the potential to
help out-of-domain parsing, adding morphological
analysis on top of an existing highly tuned statisti-
cal parsing system can result in suboptimal perfor-
mance. Thus, in future we will investigate tighter
integration of the morphological features with the
parsing model.
Acknowledgments
D. Seddah and M. Candito were supported by the ANR
Sequoia (ANR-08-EMER-013); ?. ?etinog?lu and J.
van Genabith by the Science Foundation Ireland (Grant
07/CE/I1142) as part of the Centre for Next Generation
Localisation at Dublin City University; G. Chrupa?a by
BMBF project NL-Search (contract 01IS08020B).
92
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel,
2003. Building a Treebank for French. Kluwer, Dor-
drecht.
Miriam Butt, Mar?a-Eugenia Ni?o, and Fr?d?rique
Segond. 1999. A Grammar Writer?s Cookbook. CSLI
Publications, Stanford, CA.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef
van Genabith, and Andy Way. 2004. Long-Distance
Dependency Resolution in Automatically Acquired
Wide-Coverage PCFG-Based LFG Approximations.
In Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 320?
327, Barcelona, Spain.
Marie Candito and Beno?t Crabb?. 2009. Im-
proving generative statistical parsing with semi-
supervised word clustering. In Proceedings of the
11th International Conference on Parsing Technolo-
gies (IWPT?09), pages 138?141, Paris, France, Octo-
ber. Association for Computational Linguistics.
Marie Candito, Benoit Crabb?, and Djam? Seddah. 2009.
On statistical parsing of french with supervised and
semi-supervised strategies. In EACL 2009 Workshop
Grammatical inference for Computational Linguistics,
Athens, Greece.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of the First Annual Meeting
of the North American Chapter of the Association for
Computational Linguistics (NAACL 2000), pages 132?
139, Seattle, WA.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with morfette. In
In Proceedings of LREC 2008, Marrakech, Morocco.
ELDA/ELRA.
Grzegorz Chrupa?a. 2008. Towards a machine-learning
architecture for lexical functional grammar parsing.
Ph.D. thesis, Dublin City University.
Grzegorz Chrupa?a. 2010. Morfette: A tool for su-
pervised learning of morphology. http://sites.
google.com/site/morfetteweb/. Version
0.3.1.
Michael Collins. 1999. Head Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In Actes
de la 15?me Conf?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
Pascal Denis and Beno?t Sagot. 2009. Coupling an anno-
tated corpus and a morphosyntactic lexicon for state-
of-the-art pos tagging with less human effort. In Proc.
of PACLIC, Hong Kong, China.
Christy Doran, Dania Egedi, Beth Ann Hockey, B. Srini-
vas, and Martin Zaidel. 1994. Xtag system: A wide
coverage grammar for english. In Proceedings of the
15th conference on Computational linguistics, pages
922?928, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine learning, 37(3):277?296.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities.
In Proc. of EACL-09, pages 327?335, Athens, Greece.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of English: The Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Ines Rehbein and Josef van Genabith. 2007. Treebank
annotation schemes and parser evaluation for german.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), Prague.
Benoit Sagot, Lionel Cl?ment, Eric V. de La Clergerie,
and Pierre Boullier. 2006. The lefff 2 syntactic lexi-
con for french: Architecture, acquisition, use. Proc. of
LREC 06, Genoa, Italy.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Djam? Seddah, Marie Candito, and Benoit Crabb?. 2009.
Cross parser evaluation and tagset variation: A French
Treebank study. In Proceedings of the 11th Interna-
tion Conference on Parsing Technologies (IWPT?09),
pages 150?161, Paris, France, October. Association
for Computational Linguistics.
Yannick Versley and Ines Rehbein. 2009. Scalable dis-
criminative parsing for german. In Proceedings of the
11th International Conference on Parsing Technolo-
gies (IWPT?09), pages 134?137, Paris, France, Octo-
ber. Association for Computational Linguistics.
93
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 349?353,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The DCU Dependency-Based Metric in WMT-MetricsMATR 2010
Yifan He Jinhua Du Andy Way Josef van Genabith
Centre for Next Generation Localisation
School of Computing
Dublin City University
Dublin 9, Ireland
{yhe,jdu,away,josef}@computing.dcu.ie
Abstract
We describe DCU?s LFG dependency-
based metric submitted to the shared eval-
uation task of WMT-MetricsMATR 2010.
The metric is built on the LFG F-structure-
based approach presented in (Owczarzak
et al, 2007). We explore the following
improvements on the original metric: 1)
we replace the in-house LFG parser with
an open source dependency parser that
directly parses strings into LFG depen-
dencies; 2) we add a stemming module
and unigram paraphrases to strengthen the
aligner; 3) we introduce a chunk penalty
following the practice of METEOR to re-
ward continuous matches; and 4) we intro-
duce and tune parameters to maximize the
correlation with human judgement. Exper-
iments show that these enhancements im-
prove the dependency-based metric?s cor-
relation with human judgement.
1 Introduction
String-based automatic evaluation metrics such as
BLEU (Papineni et al, 2002) have led directly
to quality improvements in machine translation
(MT). These metrics provide an alternative to ex-
pensive human evaluations, and enable tuning of
MT systems based on automatic evaluation results.
However, there is widespread recognition in
the MT community that string-based metrics are
not discriminative enough to reflect the translation
quality of today?s MT systems, many of which
have gone beyond pure string-based approaches
(cf. (Callison-Burch et al, 2006)).
With that in mind, a number of researchers have
come up with metrics which incorporate more so-
phisticated and linguistically motivated resources.
Examples include METEOR (Banerjee and Lavie,
2005; Lavie and Denkowski, 2009) and TERP
(Snover et al, 2010), both of which now uti-
lize stemming, WordNet and paraphrase informa-
tion. Experimental and evaluation campaign re-
sults have shown that these metrics can obtain bet-
ter correlation with human judgements than met-
rics that only use surface-level information.
Given that many of today?s MT systems incor-
porate some kind of syntactic information, it was
perhaps natural to use syntax in automatic MT
evaluation as well. This direction was first ex-
plored by (Liu and Gildea, 2005), who used syn-
tactic structure and dependency information to go
beyond the surface level matching.
Owczarzak et al (2007) extended this line of
research with the use of a term-based encoding of
Lexical Functional Grammar (LFG:(Kaplan and
Bresnan, 1982)) labelled dependency graphs into
unordered sets of dependency triples, and calculat-
ing precision, recall, and F-score on the triple sets
corresponding to the translation and reference sen-
tences. With the addition of partial matching and
n-best parses, Owczarzak et al (2007)?s method
considerably outperforms Liu and Gildea?s (2005)
w.r.t. correlation with human judgement.
The EDPM metric (Kahn et al, 2010) im-
proves this line of research by using arc labels
derived from a Probabilistic Context-Free Gram-
mar (PCFG) parse to replace the LFG labels,
showing that a PCFG parser is sufficient for pre-
processing, compared to a dependency parser in
(Liu and Gildea, 2005) and (Owczarzak et al,
2007). EDPM also incorporates more information
sources: e.g. the parser confidence, the Porter
stemmer, WordNet synonyms and paraphrases.
Besides the metrics that rely solely on the de-
pendency structures, information from the depen-
dency parser is a component of some other metrics
that use more diverse resources, such as the textual
entailment-based metric of (Pado et al, 2009).
In this paper we extend the work of (Owczarzak
et al, 2007) in a different manner: we use an
349
adapted version of the Malt parser (Nivre et al,
2006) to produce 1-best LFG dependencies and
allow triple matches where the dependency la-
bels are different. We incorporate stemming, syn-
onym and paraphrase information as in (Kahn et
al., 2010), and at the same time introduce a chunk
penalty in the spirit of METEOR to penalize dis-
continuous matches. We sort the matches accord-
ing to the match level and the dependency type,
and weight the matches to maximize correlation
with human judgement.
The remainder of the paper is organized as fol-
lows. Section 2 reviews the dependency-based
metric. Sections 3, 4, 5 and 6 introduce our im-
provements on this metric. We report experimen-
tal results in Section 7 and conclude in Section 8.
2 The Dependency-Based Metric
In this section, we briefly review the metric pre-
sented in (Owczarzak et al, 2007).
2.1 C-Structure and F-Structure in LFG
In Lexical Functional Grammar (Kaplan and Bres-
nan, 1982), a sentence is represented as both a hi-
erarchical c-(onstituent) structure which captures
the phrasal organization of a sentence, and a f-
(unctional) structure which captures the functional
relations between different parts of the sentence.
Our metric currently only relies on the f-structure,
which is encoded as labeled dependencies in our
metric.
2.2 MT Evaluation as Dependency Triple
Matching
The basic method of (Owczarzak et al, 2007) can
be illustrated by the example in Table 1.
The metric in (Owczarzak et al, 2007) performs
triple matching over the Hyp- and Ref-Triples and
calculates the metric score using the F-score of
matching precision and recall. Let m be the num-
ber of matches, h be the number of triples in the
hypothesis and e be the number of triples in the
reference. Then we have the matching precision
P = m/h and recall R = m/e. The score of the
hypothesis in (Owczarzak et al, 2007) is the F-
score based on the precision and recall of match-
ing as in (1):
Fscore = 2PRP +R (1)
Table 1: Sample Hypothesis and Reference
Hypothesis
rice will be held talks in egypt next week
Hyp-Triples
adjunct(will, rice)
xcomp(will, be)
adjunct(talks, held)
xcomp(be, talks)
adjunct(talks, in)
obj(in, egypt)
adjunct(week, next)
adjunct(talks, week)
Reference
rice to hold talks in egypt next week
Ref-Triples
obl(rice, to)
obj(hold, to)
adjunct(week, talks)
adjunct(talks, in)
obj(in, egypt)
adjunct(week, next)
obj(hold, week)
2.3 Details of the Matching Strategy
(Owczarzak et al, 2007) uses several techniques
to facilitate triple matching. First of all, consider-
ing that the MT-generated hypotheses have vari-
able quality and are sometimes ungrammatical,
the metric will search the 50-best parses of both
the hypothesis and reference and use the pair that
has the highest F-score to compensate for parser
noise.
Secondly, the metric performs complete or par-
tial matching according to the dependency labels,
so the metric will find more matches on depen-
dency structures that are presumably more infor-
mative.
More specifically, for all except the LFG
Predicate-Only labeled triples of the form
dep(head, modifier), the method does not
allow a match if the dependency labels (deps)
are different, thus enforcing a complete match.
For the Predicate-Only dependencies, par-
tial matching is allowed: i.e. two triples are con-
sidered identical even if only the head or the
modifier are the same.
Finally, the metric also uses linguistic resources
for better coverage. Besides using WordNet syn-
onyms, the method also uses the lemmatized out-
put of the LFG parser, which is equivalent to using
350
an English lemmatizer.
If we do not consider these additional lin-
guistic resources, the metric would find the fol-
lowing matches in the example in Table 1:
adjunct(talks, in), obj(in, egypt)
and adjunct(week, next), as these three
triples appear both in the reference and in the hy-
pothesis.
2.4 Points for Improvement
We see several points for improvement from Table
1 and the analysis above.
? More linguistic resources: we can use more
linguistic resources than WordNet in pursuit
of better coverage.
? Using the 1-best parse instead of 50-best
parses: the parsing model we currently use
does not produce k-best parses and using only
the 1-best parse significantly improves the
speed of triple matching. We allow ?soft?
triple matches to capture the triple matches
which we might otherwise miss using the 1-
best parse.
? Rewarding continuous matches: it
would be more desirable to reflect
the fact that the 3 matching triples
adjunct(talks, in), obj(in,
egypt) and adjunct(week, next)
are continuous in Table 1.
We introduce our improvements to the metric
in response to these observations in the following
sections.
3 Producing and Matching LFG
Dependency Triples
3.1 The LFG Parser
The metric described in (Owczarzak et al, 2007)
uses the DCU LFG parser (Cahill et al, 2004)
to produce LFG dependency triples. The parser
uses a Penn treebank-trained parser to produce
c-structures (constituency trees) and an LFG f-
structure annotation algorithm on the c-structure
to obtain f-structures. In (Owczarzak et al, 2007),
triple matching on f-structures produced by this
paradigm correlates well with human judgement,
but this paradigm is not adequate for the WMT-
MetricsMatr evaluation in two respects: 1) the in-
house LFG annotation algorithm is not publicly
available and 2) the speed of this paradigm is not
satisfactory.
We instead use the Malt Parser1 (Nivre et al,
2006) with a parsing model trained on LFG de-
pendencies to produce the f-structure triples. Our
collaborators2 first apply the LFG annotation algo-
rithm to the Penn Treebank training data to obtain
f-structures, and then the f-structures are converted
into dependency trees in CoNLL format to train
the parsing model. We use the liblinear (Fan et
al., 2008) classification module to for fast parsing
speed.
3.2 Hard and Soft Dependency Matching
Currently our parser produces only the 1-best
outputs. Compared to the 50-best parses in
(Owczarzak et al, 2007), the 1-best parse limits
the number of triple matches that can be found. To
compensate for this, we allow triple matches that
have the same Head and Modifier to consti-
tute a match, even if their dependency labels are
different. Therefore for triples Dep1(Head1,
Mod1) and Dep2(Head2, Mod2), we allow
three types of match: a complete match if
the two triples are identical, a partial match if
Dep1=Dep2 and Head1=Head2, and a soft
match if Head1=Head2 and Mod1=Mod2.
4 Capturing Variations in Language
In (Owczarzak et al, 2007), lexical variations at
the word-level are captured by WordNet. We
use a Porter stemmer and a unigram paraphrase
database to allow more lexical variations.
With these two resources combined, there are
four stages of word level matching in our sys-
tem: exact match, stem match, WordNet match and
unigram paraphrase match. The stemming mod-
ule uses Porter?s stemmer implementation3 and the
WordNet module uses the JAWS WordNet inter-
face.4 Our metric only considers unigram para-
phrases, which are extracted from the paraphrase
database in TERP5 using the script in the ME-
TEOR6 metric.
1http://maltparser.org/index.html
2O?zlem C?etinog?lu and Jennifer Foster at the National
Centre for Language Technology, Dublin City University
3http://tartarus.org/?martin/
PorterStemmer/
4http://lyle.smu.edu/?tspell/jaws/
index.html
5http://www.umiacs.umd.edu/?snover/
terp/
6http://www.cs.cmu.edu/?alavie/METEOR/
351
5 Adding Chunk Penalty to the
Dependency-Based Metric
The metric described in (Owczarzak et al, 2007)
does not explicitly consider word order and flu-
ency. METEOR, on the other hand, utilizes this in-
formation through a chunk penalty. We introduce
a chunk penalty to our dependency-based metric
following METEOR?s string-based approach.
Given a reference r = wr1...wrn, we denote
wri as ?covered? if it is the head or modifier of
a matched triple. We only consider the wris that
appear as head or modifier in the reference
triples. After this notation, we follow METEOR?s
approach by counting the number of chunks in
the reference string, where a chunk wrj ...wrk is
a sequence of adjacent covered words in the refer-
ence. Using the hypothesis and reference in Ta-
ble 1 as an example, the three matched triples
adjunct(talks, in), obj(in, egypt)
and adjunct(week, next) will cover a con-
tinuous word sequence in the reference (under-
lined), constituting one single chunk:
rice to hold talks (in) egypt next week
Based on this observation, we introduce a simi-
lar chunk penalty Pen as in METEOR in our met-
ric, as in 2:
Pen = ? ? ( #chunks#matches )
? (2)
where ? and ? are free parameters, which we tune
in Section 6.2. We add this penalty to the depen-
dency based metric (cf. Eq. (1)), as in Eq. (3).
score = (1? Pen) ? Fscore (3)
6 Parameter Tuning
6.1 Parameters of the Metric
In our metric, dependency triple matches can be
categorized according to many criteria. We as-
sume that some matches are more critical than
others and encode the importance of matches by
weighting them differently. The final match will
be the sum of weighted matches, as in (4):
m =
?
?tmt (4)
where ?t and mt are the weight and number of
match category t. We categorize a triple match ac-
cording to three perspectives: 1) the level of match
L={complete, partial}; 2) the linguistic resource
used in matching R={exact, stem, WordNet, para-
phrase}; and 3) the type of dependency D. To
avoid too large a number of parameters, we only
allow a set of frequent dependency types, along
with the type other, which represents all the other
types and the type soft for soft matches. We have
D={app, subj, obj, poss, adjunct, topicrel, other,
soft}.
Therefore for each triple match m, we can have
the type of the match t ? L?R?D.
6.2 Tuning
In sum, we have the following parameters to tune
in our metric: precision weight ?, chunk penalty
parameters ?, ?, and the match type weights
?1...?n. We perform Powell?s line search (Press et
al., 2007) on the sufficient statistics of our metric
to find the set of parameters that maximizes Pear-
son?s ? on the segment level. We perform the op-
timization on the MT06 portion of the NIST Met-
ricsMATR 2010 development set with 2-fold cross
validation.
7 Experiments
We experiment with four settings of the metric:
HARD, SOFT, SOFTALL and WEIGHTED in or-
der to validate our enhancements. The first two
settings compare the effect of allowing/not al-
lowing soft matches, but only uses WordNet as
in (Owczarzak et al, 2007). The third setting ap-
plies our additional linguistic features and the final
setting tunes parameter weights for higher correla-
tion with human judgement.
We report Pearson?s r, Spearman?s ? and
Kendall?s ? on segment and system levels on the
NIST MetricsMATR 2010 development set using
Snover?s scoring tool.7
Table 2: Correlation on the Segment Level
r ? ?
HARD 0.557 0.586 0.176
SOFT 0.600 0.634 0.213
SOFTALL 0.633 0.662 0.235
WEIGHTED 0.673 0.709 0.277
Table 2 shows that allowing soft triple matches
and using more linguistic features all lead
to higher correlation with human judgement.
Though the parameters might somehow overfit on
7http://www.umiacs.umd.edu/?snover/
terp/scoring/
352
the data set even if we apply cross validation, this
certainly confirms the necessity of weighing de-
pendency matches according to their types.
Table 3: Correlation on the System Level
r ? ?
HARD 0.948 0.905 0.786
SOFT 0.964 0.905 0.786
SOFTALL 0.975 0.976 0.929
WEIGHTED 0.989 1.000 1.000
When considering the system-level correlation
in Table 3, the trend is very similar to that of the
segment level. The improvements we introduce all
lead to improvements in correlation with human
judgement.
8 Conclusions and Future Work
In this paper we describe DCU?s dependency-
based MT evaluation metric submitted to WMT-
MetricsMATR 2010. Building upon the LFG-
based metric described in (Owczarzak et al,
2007), we use a publicly available parser instead
of an in-house parser to produce dependency la-
bels, so that the metric can run on a third party
machine. We improve the metric by allowing more
lexical variations and weighting dependency triple
matches depending on their importance according
to correlation with human judgement.
For future work, we hope to apply this method
to languages other than English, and performmore
refinement on dependency type labels and linguis-
tic resources.
Acknowledgements
This research is supported by the Science Foundation Ireland
(Grant 07/CE/I1142) as part of the Centre for Next Gener-
ation Localisation (www.cngl.ie) at Dublin City University.
We thank O?zlem C?etinog?lu and Jennifer Foster for providing
us with the LFG parsing model for the Malt Parser, as well as
the anonymous reviewers for their insightful comments.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An
automatic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of the ACL
Workshop on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization, pages
65?72, Ann Arbor, MI.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef van
Genabith, and Andy Way. 2004. Long-distance depen-
dency resolution in automatically acquired wide-coverage
PCFG-based LFG approximations. In Proceedings of the
42nd Meeting of the Association for Computational Lin-
guistics (ACL-2004), pages 319?326, Barcelona, Spain.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn.
2006. Re-evaluation the role of bleu in machine trans-
lation research. In Proceedings of 11th Conference of the
European Chapter of the Association for Computational
Linguistics, pages 249?256, Trento, Italy.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library for
large linear classification. Journal of Machine Learning
Research, 9:1871?1874.
Jeremy G. Kahn, Matthew Snover, and Mari Ostendorf.
2010. Expected dependency pair match: predicting trans-
lation quality with expected syntactic structure. Machine
Translation.
Ronald M. Kaplan and Joan Bresnan. 1982. Lexical-
functional grammar: A formal system for grammatical
representation. The mental representation of grammatical
relations, pages 173?281.
Alon Lavie andMichael J. Denkowski. 2009. he meteor met-
ric for automatic evaluation of machine translation. Ma-
chine Translation, 23(2-3).
Ding Liu and Daniel Gildea. 2005. Syntactic features
for evaluation of machine translation. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summarization,
pages 25?32, Ann Arbor, MI.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In In The fifth international conference on Lan-
guage Resources and Evaluation (LREC-2006), pages
2216?2219, Genoa, Italy.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2007. Labelled dependencies in machine translation eval-
uation. In Proceedings of the Second Workshop on Statis-
tical Machine Translation, pages 104?111, Prague, Czech
Republic.
Sebastian Pado, Michel Galley, Dan Jurafsky, and Christo-
pher D. Manning. 2009. Robust machine translation
evaluation with entailment features. In Proceedings of
the Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 297?305,
Suntec, Singapore.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation
of machine translation. In Proceedings of 40th Annual
Meeting of the Association for Computational Linguistics
(ACL-2002), pages 311?318, Philadelphia, PA.
William H. Press, Saul A. Teukolsky, William T. Vetterling,
and Brian P. Flannery. 2007. Numerical Recipes 3rd Edi-
tion: The Art of Scientific Computing. Cambridge Univer-
sity Press, New York, NY.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard
Schwartz. 2010. Ter-plus: paraphrase, semantic, and
alignment enhancements to translation edit rate. Machine
Translation.
353
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 19?27,
Beijing, August 2010
Automatic Extraction of Arabic Multiword Expressions
Mohammed Attia, Antonio Toral, Lamia Tounsi, Pavel Pecina and Josef van Genabith
School of Computing, Dublin City University
{mattia,atoral,ltounsi,ppecina,josef}@computing.dcu.ie
Abstract
In this paper we investigate the automatic
acquisition of Arabic Multiword Expres-
sions (MWE). We propose three com-
plementary approaches to extract MWEs
from available data resources. The first
approach relies on the correspondence
asymmetries between Arabic Wikipedia
titles and titles in 21 different languages.
The second approach collects English
MWEs from Princeton WordNet 3.0,
translates the collection into Arabic us-
ing Google Translate, and utilizes differ-
ent search engines to validate the output.
The third uses lexical association mea-
sures to extract MWEs from a large unan-
notated corpus. We experimentally ex-
plore the feasibility of each approach and
measure the quality and coverage of the
output against gold standards.
1 Introduction
A lexicon of multiword expressions (MWEs) has
a significant importance as a linguistic resource
because MWEs cannot usually be analyzed lit-
erally, or word-for-word. In this paper we ap-
ply three approaches to the extraction of Arabic
MWEs from multilingual, bilingual, and monolin-
gual data sources. We rely on linguistic informa-
tion, frequency counts, and statistical measures to
create a refined list of candidates. We validate the
results with manual and automatic testing.
The paper is organized as follows: in this intro-
duction we describe MWEs and provide a sum-
mary of previous related research. Section 2 gives
a brief description of the data sources used. Sec-
tion 3 presents the three approaches used in our
experiments, and each approach is tested and eval-
uated in its relevant sub-section. In Section 4 we
discuss the results of the experiments. Finally, we
conclude in Section 5.
1.1 What Are Multiword Expressions?
Multiword expressions (MWEs) are defined
as idiosyncratic interpretations that cross word
boundaries or spaces (Sag et al, 2002). The exact
meaning of an MWE is not directly obtained from
its component parts. Accommodating MWEs in
NLP applications has been reported to improve
tasks, such as text mining (SanJuan and Ibekwe-
SanJuan, 2006), syntactic parsing (Nivre and Nils-
son, 2004; Attia, 2006), and Machine Translation
(Deksne, 2008).
There are two basic criteria for identifying
MWEs: first, component words exhibit statisti-
cally significant co-occurrence, and second, they
show a certain level of semantic opaqueness or
non-compositionality. Statistically significant co-
occurrence can give a good indication of how
likely a sequence of words is to form an MWE.
This is particularly interesting for statistical tech-
niques which utilize the fact that a large number
of MWEs are composed of words that co-occur to-
gether more often than can be expected by chance.
The compositionality, or decomposabil-
ity (Villavicencio et al 2004), of MWEs is also
a core issue that presents a challenge for NLP ap-
plications because the meaning of the expression
is not directly predicted from the meaning of the
component words. In this respect, composition-
alily varies between phrases that are highly com-
19
positional, such as,       	  
 	   qfla-
?idatun ?askariyyatun, ?military base?, and those
that show a degree of idiomaticity, such as,       

   Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 43?51,
COLING 2010, Beijing, August 2010.
Seeding Statistical Machine Translation with Translation Memory 
Output through Tree-Based Structural Alignment
Ventsislav Zhechev Josef van Genabith
EuroMatrixPlus, CNGL
School of Computing, Dublin City University
EuroMatrixPlus, CNGL
School of Computing, Dublin City University
contact@VentsislavZhechev.eu josef@computing.dcu.ie
Abstract
With the steadily increasing demand for 
high-quality translation, the localisation 
industry is constantly searching for tech-
nologies that would increase translator 
throughput, with the current focus on the 
use of high-quality Statistical Machine 
Translation (SMT) as a supplement to the 
established Translation Memory (TM) 
technology. In this paper we present a 
novel modular approach that utilises 
state-of-the-art sub-tree alignment to pick 
out pre-translated segments from a TM 
match and seed with them an SMT sys-
tem to produce a final translation. We 
show that the presented system can out-
perform pure SMT when a good TM 
match is found. It can also be used in a 
Computer-Aided Translation (CAT) envi-
ronment to present almost perfect transla-
tions to the human user with markup 
highlighting the segments of the transla-
tion that need to be checked manually for 
correctness.
1. Introduction
As the world becomes increasingly intercon-
nected, the major trend is to try to deliver ideas 
and products to the widest audience possible. 
This requires the localisation of products for as 
many countries and cultures as possible, with 
translation being one of the main parts of the lo-
calisation process. Because of this, the amount of 
data that needs professional high-quality transla-
tion is continuing to increase well beyond the 
capacity of the world?s human translators.
Thus, current efforts in the localisation indus-
try are mostly directed at the reduction of the 
amount of data that needs to be translated from 
scratch by hand. Such efforts mainly include the 
use of Translation Memory (TM) systems, where 
earlier translations are stored in a database and 
offered as suggestions when new data needs to 
be translated. As TM systems were originally 
limited to providing translations only for (al-
most) exact matches of the new data, the integra-
tion of Machine Translation (MT) techniques is 
seen as the only feasible development that has 
the potential to significantly reduce the amount 
of manual translation required.
At the same time, the use of SMT is frowned 
upon by the users of CAT tools as they still do 
not trust the quality of the SMT output. There are 
two main reasons for that. First, currently there is 
no reliable way to automatically ascertain the 
quality of SMT-generated translations, so that the 
user could at a glance make a judgement as to the 
amount of effort that might be needed to post-
edit the suggested translation (Simard and Isa-
belle, 2009). Not having such automatic quality 
metrics also has the side effect of it being impos-
sible for a Translation-Services Provider (TSP) 
company to reliably determine in advance the 
increase in translator productivity due to the use 
of MT and to adjust their resources-allocation 
and cost models correspondingly.
The second major problem for users is that SMT-
generated translations are as a rule only obtained 
for cases where the TM system could not produce 
a good-enough translation (cf. Heyn, 1996). Given 
that the SMT system used is usually trained only 
on the data available in the TM, expectedly it also 
has few examples from which to construct the 
translation, thus producing low quality output.
43
In this paper, we combine a TM, SMT and an 
automatic Sub-Tree Alignment (STA) backends 
in a single integrated tool. When a new sentence 
that needs to be translated is supplied, first a 
Fuzzy-Match Score (FMS ? see Section 2.2) is 
obtained from the TM backend, together with the 
suggested matching sentence and its translation. 
For sentences that receive a reasonably high 
FMS, the STA backend is used to find the corre-
spondences between the input sentence and the 
TM-suggested translation, marking up the parts 
of the input that are correctly translated by the 
TM. The SMT backend is then employed to ob-
tain the final translation from the marked-up in-
put sentence. In this way we expect to achieve a 
better result compared to using pure SMT.
In Section 2, we present the technical details 
of the design of our system, together with moti-
vation for the particular design choices. Section 3 
details the experimental setup and the data set 
used for the evaluation results in Section 4. We 
present improvements that we plan to investigate 
in further work in Section 5, and provide con-
cluding remarks in Section 6.
2. System Framework
We present a system that uses a TM-match to 
pre-translate parts of the input sentence and 
guide an SMT system to the generation of a 
higher-quality translation.
2.1. Related Approaches
We are not aware of any published research 
where TM output is used to improve the per-
formance of an SMT system in a manner similar 
to the system presented in this paper.
Most closely related to our approach are the 
systems by Bi?ici and Dymetman (2008) and 
Simard and Isabelle (2009), where the authors 
use the TM output to extract new phrase pairs 
that supplement the SMT phrase table. Such an 
approach, however, does not guarantee that the 
SMT system will select the TM-motivated 
phrases even if a heavy bias is applied to them.
Another related system is presented in (Smith 
and Clark, 2009). Here the authors use a syntax-
based EBMT system to pre-translate and mark-
up parts of the input sentence and then supply 
this marked-up input to an SMT system. This 
differs to our system in two ways. First, Smith 
and Clark use EMBT techniques to obtain partial 
translations of the input from the complete ex-
ample base, while we are only looking at the best 
TM match for the given input. Second, the authors 
use dependency structures for EMBT matching, 
while we employ phrase-based structures.
2.2. Translation Memory Backend
Although the intention is to use a full-scale TM 
system as the translation memory backend, to 
have complete control over the process for this 
initial research we decided to build a simple pro-
totype TM backend ourselves.
We employ a database setup using the Post-
greSQL v.8.4.3
1
 relational database management 
(RDBM) system. The segment pairs from a given 
TM are stored in this database and assigned 
unique IDs for further reference. When a new 
sentence is supplied for translation, the database 
is searched for (near) matches, using an FMS 
based on normalised character-level Levenshtein 
edit distance (Levenshtein, 1965).
Thus for each input sentence, from the data-
base we obtain the matching segment with the 
highest FMS, its translation and the score itself.
2.3. Sub-Tree Alignment Backend
The system presented in this paper uses phrase-
based sub-tree structural alignment (Zhechev, 
2010) to discover parts of the input sentence that 
correspond to parts of the suggested translation 
extracted from the TM database. We chose this 
particular tool, because it can produce aligned 
phrase-based-tree pairs from unannotated (i.e. 
unparsed) data. It can also function fully auto-
matically without the need for any training data. 
The only auxiliary requirement it has is for a 
probabilistic dictionary for the languages that are 
being aligned. As described later in this section, 
in our case this is obtained automatically from the 
TM data during the training of the SMT backend.
The matching between the input sentence and 
the TM-suggested translation is done in a three-
step process. First, the plain TM match and its 
1
 http://www.postgresql.org/
44
translation are aligned, which produces a sub-
tree-aligned phrase-based tree pair with all non-
terminal nodes labelled ?X? (cf. Zhechev, 2010). 
As we are only interested in the relations be-
tween the lexical spans of the non-terminal 
nodes, we can safely ignore their labels. We call 
this first step of our algorithm bilingual alignment.
In the second step, called monolingual align-
ment, the phrase-based tree-annotated version of 
the TM match is aligned to the unannotated input 
sentence. The reuse of the tree structure for the 
TM match allows us to use it in the third step as 
an intermediary to establish the available sub-
tree alignments between the input sentence and 
the translation suggested from the TM.
During this final alignment, we identify 
matched and mismatched portions of the input 
sentence and their possible translations in the 
TM suggestion and, thus, this step is called 
matching. Additionally, the sub-tree alignments 
implicitly provide us with reordering informa-
tion, telling us where the portions of the input 
sentence that we translate should be positioned in 
the final translation.
The alignment process is exemplified in Figure 1. 
The tree marked ?I? corresponds to the input sen-
tence, the one marked ?M? to the TM match and 
the one marked ?T? to the TM translation. Due to 
space constraints, we only display the node ID 
numbers of the non-terminal nodes in the phrase-
structure trees???in reality all nodes carry the 
label ?X?. These IDs are used to identify the sub-
sentential alignment links. The lexical items cor-
responding to the leaves of the trees are pre-
sented in the table below the graph.
The alignment process can be visually repre-
sented as starting at a linked node in the I tree 
and following the link to the M tree. Then, if 
available, we follow the link to the T tree and 
this leads us to the T-tree node corresponding to 
the I-tree node we started from. In Figure 1, this 
results in the I?T alignments I1?T18, I2?T2, I3?
T1, I4?T32 and I6?T34. The first three links are 
matches, because the lexical items covered by 
the I nodes correspond exactly to the lexical 
items covered by their M node counterparts. 
Such alignments provide us with direct TM 
translations for our input. The last two links in 
the group are mismatched, because there is no 
lexical correspondence between the I and M 
nodes (node I4  corresponds to the phrase sender 
email, while the linked node M10  corresponds to 
sender ?s email). Such alignments can only be 
used to infer reordering information. In particular 
in this case, we can infer that the target word or-
der for the input sentence is address email 
sender, which produces the translation adresse 
?lectronique de l? exp?diteur.
15
13
10 4
6 3
1 2
5
36
34 8
1 32
2 24 7
18 6
3 4 5
6
4
1 2
3
T
I
M
I
input
M
match
T
trans-
lation
1 2 3
sender email address
1 2 3 4 5
sender ?s email address .
1 2 3 4 5 6 7 8
adresse
?lectro-
nique
de l?
exp?-
diteur
du
mes-
sage
.
Figure 1. Example of sub-tree alignment between 
an input sentence, TM match and TM translation
We decided to use sub-tree-based alignment, 
rather than plain word alignment (e.g. GIZA++ ? 
Och and Ney, 2003), due to a number of factors. 
First, sub-tree-based alignment provides much 
better handling of long-distance reorderings, 
while word? and phrase-based alignment models 
always have a fixed limit on reordering distance 
that tends to be relatively low to allow efficient 
computation.
The alignments produced by a sub-tree align-
ment model are also precision-oriented, rather 
than recall-oriented (cf. Tinsley, 2010). This is 
important in our case, where we want to only 
extract those parts of the translation suggested by 
the TM for which we are most certain that they 
are good translations.
45
As stated earlier, the only resource necessary 
for the operation of this system is a probabilistic 
bilingual dictionary covering the data that needs 
to be aligned. For the bilingual alignment step, 
such a bilingual dictionary is produced as a by-
product of the training of the SMT backend and 
therefore available. For the monolingual align-
ment step, the required probabilistic dictionary is 
generated by simply listing each unique token 
seen in the source-language data in the TM as 
translating only as itself with probability 1.
2.4. Statistical Machine Translation Backend
Once the matching  step is completed, we have 
identified and marked-up the parts of the input 
sentence for which translations will be extracted 
from the TM suggestions, as well as the parts 
that need to be translated from scratch. The 
lengths of the non-translated segments vary de-
pending on the FMS, but are in general relatively 
short (one to three tokens).
The further processing of the input relies on a 
specific feature of the SMT backend we use, 
namely the Moses system (Koehn et al, 2007). 
We decided to use this particular system as it is 
the most widely adopted open-source SMT sys-
tem, both for academic and commercial pur-
poses. In this approach, we annotate the seg-
ments of the input sentence for which transla-
tions have been found from the TM suggestion 
using XML tags with the translation correspond-
ing to each segment given as an attribute to the 
encapsulating XML tag, similarly to the system 
described in (Smith and Clark, 2009). The SMT 
backend is supplied with marked-up input in the 
form of a string consisting of the concatenation 
of the XML-enclosed translated segments and 
the plain non-translated segments in the target-
language word order, as established by the 
alignment process. The SMT backend is in-
structed to translate this input, while keeping the 
translations supplied via the XML annotation. 
This allows the SMT backend to produce transla-
tions informed by and conforming to actual ex-
amples from the TM, which should result in im-
provements in translation quality.
2.5. Auxilliary Tools
It must be noted that in general the SMT backend 
sees the data it needs to translate in the target-
language word order (e.g. it is asked to translate 
an English sentence that has French word order). 
This, however, does not correspond to the data 
found in the TM, which we use for deriving the 
SMT models. Because of this discrepancy, we 
developed a pre-processing tool that goes over 
the TM data performing bilingual alignment and 
outputting reordered versions of the sentences it 
processes by using the information implicitly 
encoded in the sub-tree alignments. In this way 
we obtain the necessary reordered data to train a 
translation model where the source language al-
ready has the target-language word order. In our 
system we than use this model???together with 
the proper-word-order model???for translation.
One specific aspect of real-world TM data that 
we need to deal with is that they often contain 
meta-tag annotations of various sorts. Namely, 
annotation tags specific to the file format used for 
storing the TM data, XML tags annotating parts 
of the text as appearing in Graphical User Inter-
face (GUI) elements, formatting tags specific to 
the file format the TM data was originally taken 
from, e.g. RTF, OpenDoc, etc. Letting any MT 
system try to deal with these tags in a probabilis-
tic manner can easily result in ill-formed, mis-
translated and/or out-of-order meta-tags in the 
translation.
This motivates the implementation of a rudi-
mentary handling of meta-tags in the system pre-
sented in this paper, in particular handling the 
XML tags found in the TM data we work with, 
as described in Section 3. The tool we developed 
for this purpose simply builds a map of all 
unique XML tags per language and replaces 
them in the data with short placeholders that are 
designed in such a way that they would not inter-
fere with the rest of the TM data.
2
 A special case 
that the tool has to take care of is when an XML 
tag contains an attribute whose value needs to be 
translated. In such situations, we decided to not 
perform any processing, but rather leave the 
XML tag as is, so that all text may be translated 
as needed. A complete treatment of meta-tags, 
however, is beyond the scope of the current paper.
2
 In the current implementation, the XML tags are replaced with the string ?<tag_id>?, where <tag_id> is a unique nu-
meric identifier for the XML tag that is being replaced.
46
We also had to build a dedicated tokeniser/de-
tokeniser pair to handle real world TM data con-
taining meta-tags, e-mail addresses, file paths, 
etc., as described in Section 3. Both tools are 
implemented as a cascade of regular expression 
substitutions in Perl.
Finally, we use a tool to extract the textual 
data from the TM. That is, we strip all tags spe-
cific to the format in which the TM is stored, as 
they can in general be recreated and thus do not 
need to be present during translation. In our par-
ticular case the TM is stored in the XML-based 
TMX format.
3
2.6. Complete Workflow
Besides the components described above, we 
also performed two further transformations on 
the data. First, we lowercase the TM data before 
using it to train the SMT backend models. This 
also means that the alignment steps from Section 
2.3 are performed on lowercased data, as the bi-
lingual dictionary used there is obtained during 
the SMT training process.
4
Additionally, the SMT and sub-tree alignment 
systems that we use cannot handle certain char-
acters, which we need to mask in the data. For 
the SMT backend, this includes ?|?, ?<? and ?>? 
and for the sub-tree aligner, ?(? and ?)?. The rea-
son these characters cannot be handled is that the 
SMT system uses ?|? internally to separate data 
fields in the trained models and ?<? and ?>? can-
not be handled whilst using XML tags to anno-
tate pre-translated portions of the input. The sub-
tree aligner uses ?(? and ?)? to represent the 
phrase-based tree structures it generates and the 
presence of these characters in the data may cre-
ate ambiguity when parsing the tree structures. 
All these characters are masked by substituting 
in high-Unicode counterparts, namely ???, ???, 
???, ??? and ???. Visually, there is a very slight 
distinction and this is intentionally so to simplify 
debugging. However, the fact that the character 
codes are different alleviates the problems dis-
cussed above. Of course, in the final output, the 
masking is reversed and the translation contains 
the regular versions of the characters.
Extract Textual Data 
from TMX Format
TMX Data
Meta-Tag Handling
Tokenisation and 
Masking of Special 
Characters
Start
Lowercasing
Generation of Probabilistic 
Dictionary for 
Monolingual Alignment
Language-Model 
Training and 
Binarisation
Automatic 
Word-Alignment
SMT Model 
Training and 
Binarisation
Language Models
Probabilistic Dictionary 
for Monolingual 
Alignment
Probabilistic Dictionary 
for Bilingual Alignment
SMT Model with
Normal Word Order
Generation of 
Bilingual Parallel 
Treebank
Bilingual Parallel 
Treebank
TM 
Database
Reorder Source-
Language Data
Normal 
Word Order
Reordered Source-
Language Data
Automatic 
Word-Alignment
SMT Model 
Training and 
Binarisation
SMT Model with
Target Word Order
Stop
T
a
r
g
e
t
 
W
o
r
d
 
O
r
d
e
r
Sub-Tree Alignment 
Input Data
Meta-Tag 
Substitution Maps
Figure 2. Pre-Processing Workflow
The complete pre-processing workflow is pre-
sented in Figure 2, where the rectangles with ver-
tical bars represent the use of open-source tools, 
while the plain rectangles represent tools devel-
oped by the authors of this paper.
First, the textual data is extracted from the 
original TM format, producing one plain-text file 
for each language side. These data can either be 
pre-loaded in a PostgreSQL database at this time, 
or during the first run of the translation system.
Next, the meta-tag-handling tool is used to 
generate the substitution tables for the source and 
target languages, as well as new files for each 
language with the tags substituted by the corre-
sponding identifiers (cf. Section 2.5). These files 
are then tokenised, lowercased and all conflicting 
characters are masked, as described above.
The pre-processed files are then used to pro-
duce a file containing pairs of sentences in the 
input format of the sub-tree aligner, as well as to 
generate the probabilistic dictionary required for 
3
 http://www.lisa.org/fileadmin/standards/tmx1.4/tmx.htm
4
 Currently, we do not use a recaser tool and the translations produced are always in lowercase. This component, however, 
will be added in a future version of the system.
47
the monolingual alignment and to train the SMT 
model on the data in the proper word order. The 
SMT training produces the necessary bilingual 
dictionary for use by the sub-tree aligner, which 
is run to obtain a parallel-treebank version of the 
TM data. The parallel treebank is then used to 
retrieve bilingual alignments for the TM data, 
rather than generate them on the fly during trans-
lation. This is an important design decision, as 
the complexity of the alignment algorithm is high 
for plain-text alignment (cf. Zhechev, 2010).
Once we have generated the bilingual parallel 
treebank, we run the reordering tool, which gen-
erates a new plain-text file for the source lan-
guage, where the sentences are modified to con-
form to the target-language word order, as im-
plied by the data in the parallel treebank. This is 
then matched with the proper-order target-
language file to train the SMT backend for the 
actual use in the translation process.
SMT Model with
Normal Word Order
SMT Backend
(normal word order)
SMT Backend
(both word orders)
TM 
Database
Find TM Match with 
Highest FMS
Meta-Tag Handling, 
Tokenisation and 
Masking of Special 
Characters for I, M, T
Bilingual Parallel 
Treebank
Extract Bilingual 
Alignment for M, T
Generate Mono-
lingual Alignment
for M, I
Output T
(tm)
Perform Alignment 
Matching
xml Approach
Output 
Translation
(xml)
SMT Model with
Target Word Order
Output 
Translation
(direct)
FMS >= 50
Probabilistic Dictionary 
for Monolingual 
Alignment
Read Input 
Sentence
Meta-Tag Handling, 
Detokenisation and 
Unmasking of Special 
Characters for Output
yes
no
Language Models
Meta-Tag 
Substitution Maps
 
Figure 3. Translation Workflow
Once all the necessary files have been gener-
ated and all pre-processing steps have been com-
pleted, the system is ready for use for translation. 
The translation workflow is shown in Figure 3, 
?I?, ?M? and ?T? having the same meanings as in 
Figure 1. Here, the first step after an input sen-
tence has been read in is to find the TM match 
with the highest FMS. This is done using the 
original plain non-pre-processed data to simulate 
real-life operation with a proper TM backend.
After the best TM match and its translation are 
extracted from the TM, they???together with the 
input sentence???are pre-processed by tokenisa-
tion, lowercasing, meta-tag and special-character 
substitution. Next, the corresponding tree pair is 
extracted from the bilingual parallel treebank to 
establish the tree structure for the TM source-
language match. This tree structure is then used 
to perform the monolingual alignment, which 
allows us to perform the matching step next. Af-
ter the matching is complete, we generate a final 
translation as described in Section 2.4. Finally, 
the translations are de-tokenised and the XML 
tags and special characters are unmasked.
3. Experimental Setup
We use real-life TM data from an industrial part-
ner. The TM was generated during the translation 
of RTF-formatted customer support documenta-
tion. The data is in TMX format and originally 
contains 108 ? 967 English?French translation 
segments, out of which 14 segments either have 
an empty language side or have an extreme dis-
crepancy in the number of tokens for each lan-
guage side and were therefore discarded.
A particular real-life trait of the data is the 
presence of a large number of XML tags. Run-
ning the tag-mapping tool described in Section 
2.6, we gathered 2?049 distinct tags for the Eng-
lish side of the data and 2 ?653 for the French 
side. Still, there were certain XML tags that in-
cluded a label argument whose value was trans-
lated from one language to the other. These XML 
tags were left intact so that our system could 
handle the translation correctly.
The TM data also contain a large number of 
file paths, e-mail addresses, URLs and others, 
which makes bespoke tokenisation of the data 
necessary. Our tokenisation tool ensures that 
none of these elements are tokenised, keeps RTF 
formatting sequences non-tokenised and properly 
handles non-masked XML tags, minimising their 
fragmentation.
As translation segments rarely occur more than 
once in a TM, we observe a high number of unique 
tokens (measured after pre-processing)???41?379 
for English and 49 ? 971 for French???out of 
48
108 ?953 segment pairs. The average sentence 
length is 13.2 for English and 15.0 for French.
For evaluation, we use a data set of 4977 Eng-
lish?French segments from the domain of the 
TM. The sentences in the test set are significantly 
shorter on average, compared to the TM???9.2 
tokens for English and 10.9 for French.
It must be noted that we used SMT models 
with maximum phrase length of 3 tokens, rather 
than the standard 5 tokens, and for decoding we 
used a 3-gram language model. This results in 
much smaller models than the ones usually used 
in mainstream SMT applications. (The standard 
for some tools goes as far as 7-token phase-
length limit and 7-gram language models)
4. Evaluation Results
For the evaluation of our system, we used a 
number of widely accepted automatic metrics, 
namely BLEU (Papineni et al, 2002), METEOR 
(Banerjee and Lavie, 2005), TER (Snover et al, 
2006) and inverse F-Score based on token-level 
precision and recall.
We setup our system to only fully process in-
put sentences for which a TM match with an 
FMS over 50% was found, although all sen-
tences were translated directly using the SMT 
backend to check the overall pure SMT perform-
ance. The TM-suggested translations were also 
output for all input sentences.
The results of the evaluation are given in Fig-
ure 4, where the tm and direct scores are also 
given for the FMS range [0%; 50%)?{100%}. 
Across all metrics we see a uniform drop in the 
quality of TM-suggested translations, which is 
what we expected, given that these translations 
contain one or more wrong words. We believe 
that the relatively high scores recorded for the 
TM-suggested translations at the high end of the 
FMS scale are a result of the otherwise perfect 
word order and lexical choice. For n-gram-
match-based metrics like the ones we used such a 
result is expected and predictable. Although the 
inverse F-score results show the potential of our 
setup to translate the outstanding tokens in a 
90%?100% TM match, it appears that the SMT 
system produces word order that does not corre-
spond to the reference translation and because of 
this receives lower scores on the other metrics.
The unexpected drop in scores for perfect TM 
matches is due to discrepancies between the ref-
erence translations in our test set and the transla-
tions stored in the TM. We believe that this issue 
Figure 4. Evaluation results for English-to-French translation, broken down by FMS range
      0?50/1963 50?60/779 60?70/621 70?80/537 80?90/537 90?100/375 100/165
   
0,1
0,2
0,3
0,4
0,5
0,6
0,7
0,8
FMS Range/Segments
BLEU
tm
direct
xml
      0?50/1963 50?60/779 60?70/621 70?80/537 80?90/537 90?100/375 100/165
   
   
0,3
0,4
0,5
0,6
0,7
0,8
0,9
FMS Range/Segments
ME
TE
OR
xml
direct
tm
      0?50/1963 50?60/779 60?70/621 70?80/537 80?90/537 90?100/375 100/165
   
0,1
0,2
0,3
0,4
0,5
0,6
0,7
0,8
0,9
FMS Range/Segments
TER
xml
direct
tm
      0?50/1963 50?60/779 60?70/621 70?80/537 80?90/537 90?100/375 100/165
   
   
0,2
0,3
0,4
0,5
0,6
0,7
FMS Range/Segments
Inv
ers
e F
-Sc
ore
tm
direct
xml
49
affects all FMS ranges, albeit to a lower extent 
for non-perfect matches. Unfortunately, the exact 
impact cannot be ascertained without human 
evaluation.
We observe a significant drop-off in translation 
quality for the direct output below FMS 50%. 
This suggests that sentences with such low FMS 
should be translated either by a human translator 
from scratch, or by an SMT system trained on 
different/more data.
Our system (i.e. the xml setup) clearly outper-
forms the direct SMT translation for FMS be-
tween 80 and 100 and has comparable perform-
ance between FMS 70 and 80. Below FMS 70, 
the SMT backend has the best performance. Al-
though these results are positive, we still need to 
investigate why our system has poor perform-
ance at lower FMS ranges. Theoretically, it 
should outperform the SMT backend across all 
ranges, as its output is generated by supplying 
the SMT backend with good pre-translated frag-
ments. The Inverse F-Score graph suggest that 
this is due to worse lexical choice, but only man-
ual evaluation can provide us with clues for solv-
ing the issue.
The discrepancy in the results in the Inverse F-
Score graph with the other metrics suggest that 
the biggest problem for our system is producing 
output in the expected word-order.
5. Future Work
There are a number of possible directions for 
improvement that can be explored.
As mentioned earlier, we plan to integrate our 
system with a full-featured open-source or com-
mercial TM product that will supply the TM 
matches and translations. We expect this to im-
prove our results, as the quality of the TM matches 
will better correspond to the reported FMS.
Such an integration will also be the first neces-
sary step to perform a user study evaluating the 
effect of the use of our system on post-editing 
speeds. We expect the findings of such a study to 
show a significant increase of throughput that 
will significantly reduce the costs of translation 
for large-scale projects.
It would be interesting to also conduct a user 
study where our system is used to additionally 
mark up the segments that need to be edited in 
the final SMT translation. We expect this to pro-
vide additional speedup to the post-editing proc-
ess. Such a study will require tight integration 
between our system and a CAT tool and the 
modular design we presented will facilitate this 
significantly.
The proposed treatment of meta-tags is cur-
rently very rudimentary and may be extended 
with additional features and to handle additional 
types of tags. The design of our system currently 
allows the meta-tag-handling tool to be devel-
oped independently, thus giving the user the 
choice of using a different meta-tag tool for each 
type of data they work with.
In addition, the reordering tool needs to be 
developed further, with emphasis on properly 
handling situations where the appropriate posi-
tion of an input-sentence segment cannot be re-
liably established. In general, further research is 
needed into the reordering errors introduced by 
the SMT system into otherwise good translations.
6. Conclusions
In this paper, we presented a novel modular ap-
proach to the utilisation of Translation Memory 
data to improve the quality of Statistical Machine 
Translation.
The system we developed uses precise sub-
tree-based alignments to reliably determine and 
mark up correspondences between an input sen-
tence and a TM-suggested translation, which en-
sures the utilisation of the high-quality transla-
tion data stored in the TM database. An SMT 
backend then translates the marked-up input sen-
tence to produce a final translation with im-
proved quality.
Our evaluation shows that the system pre-
sented in this paper significantly improves the 
quality of SMT output when using TM matches 
with FMS above 80 and produces results on par 
with the pure SMT output for SMT between 70 
and 80. TM matches with FMS under 70 seem to 
provide insufficient reordering information and 
too few matches to improve on the SMT output. 
Still, further investigation is needed to properly 
diagnose the drop in quality for FMS below 70.
We expect further improvements to the reor-
dering functionality of our system to result in 
higher-quality output even for lower FMS ranges.
50
Acknowledgements
This research is funded under the 7
th
 Framework 
Programme of the European Commission within 
the EuroMatrixPlus project (grant ? 231720). 
The data used for evaluation was generously pro-
vided by Symantec Ireland.
References
Banerjee, Satanjeev and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with 
Improved Correlation with Human Judgements.
In Proceedings of the Workshop on Intrinsic and 
Extrinsic Evaluation Measures for MT and/or 
Summarization at the 43rd Annual Meeting of the 
Association for Computational Linguistics
(ACL ?05), pp. 65?72. Ann Arbor, MI.
Bi?ici, Ergun and Marc Dymetman. 2008. Dynamic 
Translation Memory: Using Statistical Machine 
Translation to improve Translation Memory Fuzzy 
Matches. In Proceedings of the 9th International 
Conference on Intelligent Text Processing and 
Computational Linguistics (CICLing??08),
ed. Alexander F. Gelbukh, pp. 454?465. Vol. 4919 
of Lecture Notes in Computer Science. Haifa, 
Israel: Springer Verlag.
Heyn, Matthias. 1996. Integrating Machine 
Translation into Translation Memory Systems.
In Proceedings of the EAMT Machine Translation 
Workshop, TKE??96, pp. 113?126. Vienna, Austria.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola 
Bertoldi, Brooke Cowan, Wade Shen, Christine 
Moran, Richard Zens, Chris Dyer, Ond?ej Bojar, 
Alexandra Constantin and Evan Herbst. 2007. 
Moses: Open Source Toolkit for Statistical 
Machine Translation. In Proceedings of the Demo 
and Poster Sessions of the 45th Annual Meeting of 
the Association for Computational Linguistics 
(ACL ?07), pp. 177?180. Prague, Czech Republic.
Levenshtein, Vladimir I. 1965. ???????? ???? ? 
???????????? ?????????, ??????? ? ????????? 
???????? (Binary Codes Capable of Correcting 
Deletions, Insertions, and Reversals). ??????? 
???????? ???? ????, 163 (4): 845?848. 
[reprinted in: Soviet Physics Doklady, 10: 707?710.].
Och, Franz Josef and Hermann Ney. 2003.
A Systematic Comparison of Various Statistical 
Alignment Models. Computational Linguistics,
29 (1): 19?51.
Papineni, Kishore, Salim Roukos, Todd Ward and 
Wei-Jing Zhu. 2002. BLEU: A Method for 
Automatic Evaluation of Machine Translation.
In Proceedings of the 40th Annual Meeting of the 
Association of Computational Linguistics 
(ACL??02), pp. 311?318. Philadelphia, PA.
Simard, Michel and Pierre Isabelle. 2009. Phrase-
based Machine Translation in a Computer-assisted 
Translation Environment. In The Twelfth Machine 
Translation Summit (MT Summit XII), pp. 120?127. 
Ottawa, ON, Canada.
Smith, James and Stephen Clark. 2009. EBMT for 
SMT: A New EBMT?SMT Hybrid. In Proceedings 
of the 3rd International Workshop on Example-
Based Machine Translation (EBMT??09),
eds. Mikel L. Forcada and Andy Way, pp. 3?10.
Dublin, Ireland.
Snover, Matthew, Bonnie J. Dorr, Richard Schwartz, 
Linnea Micciulla and John Makhoul. 2006.
A Study of Translation Edit Rate with Targeted 
Human Annotation. In Proceedings of the
7th Conference of the Association for Machine 
Translation in the Americas (AMTA??06),
pp. 223?231. Cambridge, MA.
Tinsley, John. 2010. Resourcing Machine Translation 
with Parallel Treebanks. School of Computing, 
Dublin City Univercity: PhD Thesis. Dublin, Ireland.
Zhechev, Ventsislav. 2010. Automatic Generation of 
Parallel Treebanks. An Efficient Unsupervised 
System: Lambert Academic Publishing.
51
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 118?126,
COLING 2010, Beijing, August 2010.
Deep Syntax Language Models and Statistical Machine Translation
Yvette Graham
NCLT
Dublin City University
ygraham@computing.dcu.ie josef@computing.dcu.ie
Josef van Genabith
CNGL
Dublin City University
Abstract
Hierarchical Models increase the re-
ordering capabilities of MT systems
by introducing non-terminal symbols to
phrases that map source language (SL)
words/phrases to the correct position
in the target language (TL) translation.
Building translations via discontiguous
TL phrases increases the difficulty of lan-
guage modeling, however, introducing the
need for heuristic techniques such as cube
pruning (Chiang, 2005), for example.
An additional possibility to aid language
modeling in hierarchical systems is to use
a language model that models fluency of
words not using their local context in the
string, as in traditional language models,
but instead using the deeper context of
a word. In this paper, we explore the
potential of deep syntax language mod-
els providing an interesting comparison
with the traditional string-based language
model. We include an experimental evalu-
ation that compares the two kinds of mod-
els independently of any MT system to in-
vestigate the possible potential of integrat-
ing a deep syntax language model into Hi-
erarchical SMT systems.
1 Introduction
In Phrase-Based Models of Machine Translation
all phrases consistent with the word alignment
are extracted (Koehn et al, 2003), with shorter
phrases needed for high coverage of unseen data
and longer phrases providing improved fluency in
target language translations. Hierarchical Mod-
els (Chiang, 2007; Chiang, 2005) build on Phrase-
Based Models by relaxing the constraint that
phrases must be contiguous sequences of words
and allow a short phrase (or phrases) nested within
a longer phrase to be replaced by a non-terminal
symbol forming a new hierarchical phrase. Tra-
ditional language models use the local context of
words to estimate the probability of the sentence
and introducing hierarchical phrases that generate
discontiguous sequences of TL words increases
the difficulty of computing language model proba-
bilities during decoding and require sophisticated
heuristic language modeling techniques (Chiang,
2007; Chiang, 2005).
Leaving aside heuristic language modeling for
a moment, the difficulty of integrating a tradi-
tional string-based language model into the de-
coding process in a hierarchical system, highlights
a slight incongruity between the translation model
and language model in Hierarchical Models. Ac-
cording to the translation model, the best way to
build a fluent TL translation is via discontiguous
phrases, while the language model can only pro-
vide information about the fluency of contiguous
sequences of words. Intuitively, a language model
that models fluency between discontiguous words
may be well-suited to hierarchical models. Deep
syntax language models condition the probability
of a word on its deep context, i.e. words linked to
it via dependency relations, as opposed to preced-
ing words in the string. During decoding in Hi-
erarchical Models, words missing a context in the
string due to being preceded by a non-terminal,
might however be in a dependency relation with
a word that is already present in the string and
118
this context could add useful information about
the fluency of the hypothesis as its constructed.
In addition, using the deep context of a word
provides a deeper notion of fluency than the lo-
cal context provides on its own and this might be
useful to improve such things as lexical choice in
SMT systems. Good lexical choice is very im-
portant and the deeper context of a word, if avail-
able, may provide more meaningful information
and result in better lexical choice. Integrating
such a model into a Hierarchical SMT system is
not straightforward, however, and we believe be-
fore embarking on this its worthwhile to evalu-
ate the model independently of any MT system.
We therefore provide an experimental evaluation
of the model and in order to provide an interesting
comparison, we evaluate a traditional string-based
language model on the same data.
2 Related Work
The idea of using a language model based on deep
syntax is not new to SMT. Shen et al (2008) use
a dependency-based language model in a string
to dependency tree SMT system for Chinese-
English translation, using information from the
deeper structure about dependency relations be-
tween words, in addition to the position of the
words in the string, including information about
whether context words were positioned on the left
or right of a word. Bojar and Hajic? (2008) use a
deep syntax language model in an English-Czech
dependency tree-to-tree transfer system, and in-
clude three separate bigram language models: a
reverse, direct and joint model. The model in our
evaluation is similar to their direct bigram model,
but is not restricted to bigrams.
Riezler and Maxwell (2006) use a trigram deep
syntax language model in German-English depen-
dency tree-to-tree transfer to re-rank decoder out-
put. The language model of Riezler and Maxwell
(2006) is similar to the model in our evaluation,
but differs in that it is restricted to a trigram model
trained on LFG f-structures. In addition, as lan-
guage modeling is not the main focus of their
work, they provide little detail on the language
model they use, except to say that it is based on
?log-probability of strings of predicates from root
to frontier of target f-structure, estimated from
predicate trigrams in English f-structures? (Rie-
zler and Maxwell, 2006). An important prop-
erty of LFG f-structures (and deep syntactic struc-
tures in general) was possibly overlooked here.
F-structures can contain more than one path of
predicates from the root to a frontier that in-
clude the same ngram, and this occurs when the
underlying graph includes unary branching fol-
lowed by branching with arity greater than one.
In such cases, the language model probability as
described in Riezler and Maxwell (2006) is incor-
rect as the probability of these ngrams will be in-
cluded multiple times. In our definition of a deep
syntax language model, we ensure that such du-
plicate ngrams are omitted in training and testing.
In addition, Wu (1998) use a bigram deep syntax
language model in a stochastic inversion transduc-
tion grammar for English to Chinese. None of the
related research we discuss here has included an
evaluation of the deep syntax language model they
employ in isolation from the MT system, however.
3 Deep Syntax
The deep syntax language model we describe is
not restricted to any individual theory of deep
syntax. For clarity, however, we restrict our ex-
amples to LFG, which is also the deep syntax
theory we use for our evaluation. The Lexical
Functional Grammar (LFG) (Kaplan and Bres-
nan, 1982; Kaplan, 1995; Bresnan, 2001; Dalrym-
ple, 2001) functional structure (f-structure) is an
attribute-value encoding of bi-lexical labeled de-
pendencies, such as subject, object and adjunct
for example, with morpho-syntactic atomic at-
tributes encoding information such as mood and
tense of verbs, and person, number and case for
nouns. Figure 1 shows the LFG f-structure for En-
glish sentence ?Today congress passed Obama?s
health care bill.?1
Encoded within the f-structure is a directed
graph and our language model uses a simplified
acyclic unlabeled version of this graph. Figure
1(b) shows the graph structure encoded within the
f-structure of Figure 1(a). We discuss the simpli-
fication procedure later in Section 5.
1Morpho-syntactic information/ atomic features are omit-
ted from the diagram.
119
(a) ?
??????????
PRED pass
SUBJ
[
PRED congress
]
OBJ
?
????
PRED bill
SPEC
[
POSS
[
PRED Obama
]]
MOD
[
PRED care
MOD
[
PRED health
]
]
?
????
ADJ
[
PRED today
]
?
??????????
(b) <s>
pass
today congress bill
</s> </s> obama care
</s> health
</s>
Figure 1: ?Today congress passed Obama?s health care bill.?
4 Language Model
We use a simplified approximation of the deep
syntactic structure, de, that encodes the unlabeled
dependencies between the words of the sentence,
to estimate a deep syntax language model prob-
ability. Traditional string-based language mod-
els combine the probability of each word in the
sentence, wi, given its preceding context, the se-
quence of words from w1 to wi?1, as shown in
Equation 1.
p(w1, w2, ..., wl) =
l?
i=1
p(wi|w1, ..., wi?1) (1)
In a similar way, a deep syntax language model
probability combines the probability of each word
in the structure, wi, given its context within the
structure, the sequence of words from wr, the
head of the sentence, to wm(i), as shown in Equa-
tion 2, with function m used to map the index of a
word in the structure to the index of its head. 2
p(de) =
l?
i=1
p(wi|wr, ..., wm(m(i))wm(i)) (2)
In order to combat data sparseness, we apply
the Markov assumption, as is done in traditional
string-based language modeling, and simplify the
probability by only including a limited length of
history when estimating the probability of each
2We refer to the lexicalized nodes in the dependency
structure as words, alternatively the term predicate can be
used.
word in the structure. For example, a trigram deep
syntax language model conditions the probability
of each word on the sequence of words consisting
of the head of the head of the word followed by
the head of the word as follows:
p(de) =
l?
i=1
P (wi|wm(m(i)) , wm(i)) (3)
In addition, similar to string-based language
modeling, we add a start symbol, <s>, at the
root of the structure and end symbols, </s>, at
the leaves to include the probability of a word be-
ing the head of the sentence and the probability
of words occurring as leaf nodes in the structure.
Figure 2(a) shows an example of how a trigram
deep syntax language model probability is com-
puted for the example sentence in Figure 1(a).
5 Simplified Approximation of the Deep
Syntactic Representation
We describe the deep syntactic structure, de, as
an approximation since a parser is employed to
automatically produce it and there is therefore no
certainty that we use the actual/correct deep syn-
tactic representation for the sentence. In addi-
tion, the function m requires that each node in the
structure has exactly one head, however, structure-
sharing can occur within deep syntactic structures
resulting in a single word legitimately having two
heads. In such cases we use a simplification of
the graph in the deep syntactic structure. Fig-
ure 3 shows an f-structure in which the subject
120
(a) Deep Syntax LM (b) Traditional LM
p(e) ? p( pass | <s>)? p(e) ? p( passed | today congress )?
p( today | <s> pass )? p( today | <s>)?
p(</s> | pass today )?
p( congress | <s> pass )? p( congress | <s> today )?
p(</s> | pass congress )?
p( bill | <s> pass )? p( bill | health care )?
p( obama | pass bill )? p( obama | congress passed )?
p(</s> | bill obama )?
p( care | pass bill )? p( care | s health )?
p( health | bill care )? p( health | ? s )?
p(</s> | care health )
p( ? | passed Obama )?
p( s | obama ? )?
p( . | care bill )?
p(</s> | bill . )
Figure 2: Example Comparison of Deep Syntax and Traditional Language Models
of both like, be and president is hillary. In our
simplified structure, the dependency relations be-
tween be and hillary and president and hillary are
dropped. We discuss how we do this later in Sec-
tion 6. Similar to our simplification for structure
sharing, we also simplify structures that contain
cycles by discarding edges that cause loops in the
structure.
6 Implementation
SRILM (Stolcke, 2002) can be used to compute
a language model from ngram counts (the -read
option of the ngram-count command). Implemen-
tation to train the language model, therefore, sim-
ply requires accurately extracting counts from the
deep syntax parsed training corpus. To simplify
the structures to acyclic graphs, nodes are labeled
with an increasing index number via a depth first
traversal. This allows each arc causing a loop in
the graph or argument sharing to be identified by
a simple comparison of index numbers, as the in-
dex number of its start node will be greater than
that of its end node. The algorithm we use to
extract ngrams from the dependency structures is
straightforward: we simply carry out a depth-first
traversal of the graph to construct paths of words
that stretch from the root of the graph to words
?
????????????????
PRED like
SUBJ 1:
[
PRED Hillary
]
XCOMP
?
???????????
PRED be
SUBJ 1
XCOMP-PRED
[
PRED president
SUBJ 1
]
ADJ
?
??
PRED at
OBJ
[
PRED U.N.
SPEC
[
PRED the
]
]
?
??
?
???????????
?
????????????????
<s>
like
hillary be
</s> president at
</s> U.N.
the
</s>
Figure 3: ?Hillary liked being president at the
U.N.?
121
?
????????????
PRED agree
SUBJ
[
PRED nobody
]
XCOMP
?
???????
PRED with
OBJ
?
?????
PRED point
ADJ
?
??
??
COORD and{[
PRED two
]
,[
PRED three
]
}
?
??
??
?
?????
?
???????
?
????????????
<s>
agree
nobody with
</s> point
and
two three
</s> </s>
Figure 4: ?Nobody agreed with points two and
three.?
at the leaves and then extract the required order
ngrams from each path. As mentioned earlier,
some ngrams can belong to more than one path.
Figure 4 shows an example structure containing
unary branching followed by binary branching in
which the sequence of symbols and words ?<s>
agree with point and? belong to the path ending
in two </s> and three </s>. In order to ensure
that only distinct ngrams are extracted we assign
each word in the structure a unique id number
and include this in the extracted ngrams. Paths
are split into ngrams and duplicate ngrams result-
ing from their occurrence in more than one path
are discarded. Its also possible for ngrams to le-
gitimately be repeated in a deep structure, and in
such cases we do not discard these ngrams. Legit-
imately repeating ngrams are easily identified as
the id numbers attached to words will be differ-
ent.
7 Deep Syntax and Lexical Choice in
SMT
Correct lexical choice in machine translation is
extremely important and PB-SMT systems rely
on the language model to ensure, that when two
phrases are combined with each other, that the
model can rank combined phrases that are flu-
ent higher than less fluent combinations. Con-
ditioning the probability of each word on its
deep context has the potential to provide a
more meaningful context than the local context
within the string. A comparison of the proba-
bilities of individual words in the deep syntax
model and traditional language model in Figure
2 clearly shows this. For instance, let us con-
sider how the language model in a German to
English SMT system is used to help rank the
following two translations today congress passed
... and today convention passed ... (the word
Kongress in German can be translated into ei-
ther congress or convention in English). In
the deep syntax model, the important compet-
ing probabilities are (i) p(congress|<s>pass)
and (ii) p(convention|<s>pass), where (i)
can be interpreted as the probability of the
word congress modifying pass when pass is
the head of the entire sentence and, simi-
larly (ii) the probability of the word conven-
tion modifying pass when pass is the head of
the entire sentence. In the traditional string-
based language model, the equivalent compet-
ing probabilities are (i) p(congress|<s>today),
the probability of congress following today when
today is the start of the sentence and (ii)
p(convention|<s>today), probability of con-
vention following today when today is the start
of the sentence, showing that the deep syntax
language model is able to use more meaningful
context for good lexical choice when estimating
the probability of words congress and convention
compared to the traditional language model.
In addition, the deep syntax language model
will encounter less data sparseness problems for
some words than a string-based language model.
In many languages words occur that can legiti-
mately be moved to different positions within the
string without any change to dependencies be-
tween words. For example, sentential adverbs
in English, can legitimately change position in
a sentence, without affecting the underlying de-
pendencies between words. The word today in
?Today congress passed Obama?s health bill?
122
can appear as ?Congress passed Obama?s health
bill today? and ?Congress today passed Obama?s
health bill?. Any sentence in the training cor-
pus in which the word pass is modified by today
will result in a bigram being counted for the two
words, regardless of the position of today within
each sentence.
In addition, some surface form words such as
auxiliary verbs for example, are not represented
as predicates in the deep syntactic structure. For
lexical choice, its not really the choice of auxiliary
verbs that is most important, but rather the choice
of an appropriate lexical item for the main verb
(that belongs to the auxiliary verb). Omitting aux-
iliary verbs during language modeling could aid
good lexical choice, by focusing on the choice of
a main verb without the effect of what auxiliary
verb is used with it.
For some words, however, the probability in the
string-based language model provides as good if
not better context than the deep syntax model, but
only for the few words that happen to be preceded
by words that are important to its lexical choice,
and this reinforces the idea that SMT systems can
benefit from using both a deep syntax and string-
based language model. For example, the proba-
bility of bill in Figures 2(a) and 2(b) is computed
in the deep syntax model as p(bill| <s> pass)
and in the string-based model using p(bill|health
care), and for this word the local context seems to
provide more important information than the deep
context when it comes to lexical choice. The deep
model nevertheless adds some useful information,
as it includes the probability of bill being an argu-
ment of pass when pass is the head of a sentence.
In traditional language modeling, the special
start symbol is added at the beginning of a sen-
tence so that the probability of the first word ap-
pearing as the first word of a sentence can be
included when estimating the probability. With
similar motivation, we add a start symbol to the
deep syntactic representation so that the probabil-
ity of the head of the sentence occurring as the
head of a sentence can be included. For exam-
ple, p(be| <s>) will have a high probability as
the verb be is the head of many sentences of En-
glish, whereas p(colorless| <s>) will have a low
probability since it is unlikely to occur as the head.
We also add end symbols at the leaf nodes in the
structure to include the probability of these words
appearing at that position in a structure. For in-
stance, a noun followed by its determiner such as
p(</s> |attorney a) would have a high probabil-
ity compared to a conjunction followed by a verb
p(</s> |and be).
8 Evaluation
We carry out an experimental evaluation to inves-
tigate the potential of the deep syntax language
model we describe in this paper independently of
any machine translation system. We train a 5-
gram deep syntax language model on 7M English
f-structures, and evaluate it by computing the per-
plexity and ngram coverage statistics on a held-
out test set of parsed fluent English sentences. In
order to provide an interesting comparison, we
also train a traditional string-based 5-gram lan-
guage model on the same training data and test
it on the same held-out test set of English sen-
tences. A deep syntax language model comes with
the obvious disadvantage that any data it is trained
on must be in-coverage of the parser, whereas a
string-based language model can be trained on any
available data of the appropriate language. Since
parser coverage is not the focus of our work, we
eliminate its effects from the evaluation by select-
ing the training and test data for both the string-
based and deep syntax language models on the ba-
sis that they are in fact in-coverage of the parser.
8.1 Language Model Training
Our training data consists of English sentences
from the WMT09 monolingual training corpus
with sentence length range of 5-20 words that are
in coverage of the parsing resources (Kaplan et al,
2004; Riezler et al, 2002) resulting in approxi-
mately 7M sentences. Preparation of training and
test data for the traditional language model con-
sisted of tokenization and lower casing. Parsing
was carried out with XLE (Kaplan et al, 2002)
and an English LFG grammar (Kaplan et al,
2004; Riezler et al, 2002). The parser produces
a packed representation of all possible parses ac-
cording to the LFG grammar and we select only
the single best parse for language model training
by means of a disambiguation model (Kaplan et
123
Corpus Tokens Ave. Tokens Vocab
per Sent.
strings 138.6M 19 345K
LFG lemmas/predicates 118.4M 16 280K
Table 1: Language model statistics for string-based and deep syntax language models, statistics are for
string tokens and LFG lemmas for the same set of 7.29M English sentences
al., 2004; Riezler et al, 2002). Ngrams were auto-
matically extracted from the f-structures and low-
ercased. SRILM (Stolcke, 2002) was used to com-
pute both language models. Table 1 shows statis-
tics on the number of words and lemmas used to
train each model.
8.2 Testing
The test set consisted of 789 sentences selected
from WMT09 additional development sets3 con-
taining English Europarl text and again was se-
lected on the basis of sentences being in-coverage
of the parsing resources. SRILM (Stolcke, 2002)
was used to compute test set perplexity and ngram
coverage statistics for each order model.
Since the deep syntax language model adds end
of sentence markers to leaf nodes in the structures,
the number of (so-called) end of sentence markers
in the test set for the deep syntax model is much
higher than in the string-based model. We there-
fore also compute statistics for each model when
end of sentence markers are omitted from training
and testing. 4 In addition, since the vast majority
of punctuation is not represented as predicates in
LFG f-structures, we also test the string-based lan-
guage model when punctuation has been removed.
8.3 Results
Table 2 shows perplexity scores and ngram cover-
age statistics for each order and type of language
model. Note that perplexity scores for the string-
based and deep syntax language models are not
directly comparable because each model has a dif-
ferent vocabulary. Although both models train on
an identical set of sentences, the data is in a dif-
ferent format for each model, as the string-based
3test2006.en and test2007.en
4When we include end of sentence marker probabilities
we also include them for normalization, and omit them from
normalization when their probabilities are omitted.
model is trained on surface form tokens, whereas
the deep syntax model uses lemmas. Ngram cov-
erage statistics provide a better comparison.
Unigram coverage for all models is high with
all models achieving close to 100% coverage on
the held-out test set. Bigram coverage is high-
est for the deep syntax language model when eos
markers are included (94.71%) with next high-
est coverage achieved by the string-based model
that includes eos markers (93.09%). When eos
markers are omitted bigram coverage goes down
slightly to 92.44% for the deep syntax model and
to 92.83% for the string-based model, and when
punctuation is also omitted from the string-based
model, coverage goes down again to 91.57%.
Trigram coverage statistics for the test set main-
tain the same rank between models as in the bi-
gram coverage, from highest to lowest as follows:
DS+eos at 64.71%, SB+eos at 58.75%, SB-eos
at 56.89%, DS-eos at 53.67%, SB-eos-punc at
53.45%. For 4-gram and 5-gram coverage a sim-
ilar coverage ranking is seen, but with DS-eos
(4gram at 17.17%, 5gram at 3.59%) and SB-eos-
punc (4gram at 20.24%, 5gram at 5.76%) swap-
ping rank position.
8.4 Discussion
Ngram coverage statistics for the DS-eos and
SB-eos-punc models provide the fairest com-
parison, with the deep syntax model achiev-
ing higher coverage than the string-based model
for bigrams (+0.87%) and trigrams (+0.22%),
marginally lower coverage coverage of unigrams
(-0.02%) and lower coverage of 4-grams (-3.07%)
and 5-grams (2.17%) compared to the string-
based model.
Perplexity scores for the deep syntax model
when eos symbols are included are low (79 for the
5gram model) and this is caused by eos markers
124
1-gram 2-gram 3-gram 4-gram 5-gram
cov. ppl cov. ppl cov. ppl cov. ppl cov. ppl
SB-eos 99.61% 1045 92.83% 297 56.89% 251 23.32% 268 7.19% 279
SB-eos-punc 99.58% 1357 91.57% 382 53.45% 327 20.24% 348 5.76% 360
DS-eos 99.56% 1005 92.44% 422 53.67% 412 17.17% 446 3.59% 453
SB+eos 99.63% 900 93.09% 227 58.75% 194 25.48% 207 8.35% 215
DS+eos 99.70% 211 94.71% 77 64.71% 73 29.86% 78 8.75% 79
Table 2: Ngram coverage and perplexity (ppl) on held-out test set. Note: DS = deep syntax, SB string-
based, eos = end of sentence markers
in the test set in general being assigned relatively
high probabilities by the model, and since several
occur per sentence, the perplexity increases when
the are omitted (453 for the 5gram model).
Tables 3 and 4 show the most frequently en-
countered trigrams in the test data for each type
of model. A comparison shows how different the
two models are and highlights the potential of the
deep syntax language model to aid lexical choice
in SMT systems. Many of the most frequently oc-
curring trigram probabilities for the deep syntax
model are for arguments of the main verb of the
sentence, conditioned on the main verb, and in-
cluding such probabilities in a system could im-
prove fluency by using information about which
words are in a dependency relation together ex-
plicitely in the model. In addition, a frequent tri-
gram in the held-out data is <s> be also, where
the word also is a sentential adverb modifying
be. Trigrams for sentential adverbs are likely to
be less effected by data sparseness in the deep
syntax model compared to the string-based model
which could result in the deep syntax model im-
proving fluency with respect to combinations of
main verbs and their modifying adverbs. The most
frequent trigram in the deep syntax test set is <s>
and be, in which the head of the sentence is the
conjunction and with argument be. In this type of
syntactic construction in English, its often the case
that the conjunction and verb will be distant from
each other in the sentence, for example: Nobody
was there except the old lady and without thinking
we quickly left. (where was and and are in a de-
pendency relation). Using a deep syntax language
model could therefore improve lexical choice for
such words, since they are too distant for a string-
3-gram No. Occ. Prob.
<s> and be 42 0.1251
<s> be this 21 0.0110
<s> must we 19 0.0347
<s> would i 19 0.0414
<s> be in 17 0.0326
<s> be that 14 0.0122
be debate the 13 0.0947
<s> be debate 13 0.0003
<s> can not 12 0.0348
<s> and president 11 0.0002
<s> would like 11 0.0136
<s> would be 11 0.0835
<s> be also 10 0.0075
Table 3: Most frequent trigrams in test set for deep
syntax model
based model.
9 Conclusions
We presented a comparison of a deep syntax
language and traditional string-based language
model. Results showed that the deep syntax lan-
guage model achieves similar ngram coverage to
the string-based model on a held out test set.
We highlighted the potential of integrating such
a model into SMT systems for improving lexical
choice by using a deeper context for probabilities
of words compared to a string-based model.
References
Bojar, Ondr?ej, Jan Hajic?. 2008. Phrase-Based and
Deep Syntactic English-to-Czech Statistical Ma-
chine Translation. In Proceedings of the third Work-
125
3-gram No. Occ. Prob.
mr president , 40 0.5385
<s> this is 25 0.1877
by the european 20 0.0014
the european union 18 0.1096
<s> it is 16 0.1815
the european parliament 15 0.0252
would like to 15 0.4944
<s> i would 15 0.0250
<s> that is 14 0.1094
i would like 14 0.0335
and gentlemen , 13 0.1005
ladies and gentlemen 13 0.2834
<s> we must 12 0.0120
should like to 12 0.1304
i should like 11 0.0089
, ladies and 11 0.5944
, it is 10 0.1090
Table 4: Most frequent trigrams in test set for
string-based model
shop on Statistical Machine Translation, Columbus,
Ohio.
Bresnan, Joan. 2001. Lexical-Functional Syntax.,
Blackwell Oxford.
Chiang, David. 2007. Hierarchical Phrase-based
Models of Translation In Computational Linguis-
tics, No. 33:2.
Chiang, David. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 263-270,
Ann Arbor, Michigan.
Dalrymple, Mary. 2001. Lexical Functional Gram-
mar, Academic Press, San Diego, CA; London.
Kaplan, Ronald, Stefan Riezler, Tracy H. King, John
T. Maxwell, Alexander Vasserman. 2004. Speed
and Accuracy in Shallow and Deep Stochastic Pars-
ing. In Proceedings of Human Language Tech-
nology Conference/North American Chapter of the
Association for Computational Linguistics Meeting,
Boston, MA.
Kaplan, Ronald M., Tracy H. King, John T. Maxwell.
2002. Adapting Existing Grammars: the XLE Ex-
perience. In Proceedings of the 19th International
Conference on Computational Linguistics (COL-
ING) 2002, Taipei, Taiwan.
Kaplan, Ronald M. 1995. The Formal Architecture of
Lexical Functional Grammar. In Formal Issues in
Lexical Functional Grammar, ed. Mary Dalrymple,
pages 7-28, CSLI Publications, Stanford, CA.
Kaplan, Ronald M., Joan Bresnan. 1982. Lexical
Functional Grammar, a Formal System for Gram-
matical Represenation. In J. Bresnan, editor, The
Mental Representation of Grammatical Relations,
173-281, MIT Press, Cambridge, MA.
Koehn, Philipp, Hieu Hoang. 2007. Factored Trans-
lation Models. Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, 868-876.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicoli Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
Annual Meeting of the Association for Computa-
tional Linguistics, demonstration session
Koehn, Philipp 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the tenth Machine Translation Summit.
Koehn, Philipp, Franz Josef Och, Daniel Marcu. 2003.
Statistical Phrase-based Translation. In Proceed-
ings of Human Language Technology and North
American Chapter of the Association for Computa-
tional Linguistics Conference, 48-54.
Riezler, Stefan, John T. Maxwell III. 2006. Grammat-
ical Machine Translation. In Proceedings of HLT-
ACL, pages 248-255, New York.
Riezler, Stefan, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, Mark Johnson.
2002. Parsing the Wall Street Journal using Lexical
Functional Grammar and Discriminitive Estimation
Techniques . (grammar version 2005) In Proceed-
ings of the 40th ACL, Philadelphia.
Shen, Libin, Jinxi Xu, Ralph Weischedel. 2008.
A New String-to-Dependency Machine Translation
Algorithm with a Target Dependency Language
Model. Proceedings of ACL-08: HLT, pages 577-
585.
Stolcke, Andreas. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing, Denver, Colorado.
Dekai, Wu, Hongsing Wong. 1998. Machine Trans-
lation with a Stochastic Grammatical Channel. In
Proceedings of the 36th ACL and 17th COLING,
Montreal, Quebec.
126
Finding Common Ground: Towards a Surface Realisation Shared Task
Anja Belz
Natural Language Technology Group
Computing, Mathematical and Information Sciences
University of Brighton, Brighton BN2 4GJ, UK
a.s.belz@brighton.ac.uk
Mike White
Department of Linguistics
The Ohio State University
Columbus, OH, USA
mwhite@ling.osu.edu
Josef van Genabith and Deirdre Hogan
National Centre for Language Technology
School of Computing
Dublin City University
Dublin 9, Ireland
{dhogan,josef}@computing.dcu.ie
Amanda Stent
AT&T Labs Research, Inc.,
180 Park Avenue
Florham Park, NJ 07932, USA
stent@research.att.com
Abstract
In many areas of NLP reuse of utility tools
such as parsers and POS taggers is now
common, but this is still rare in NLG. The
subfield of surface realisation has perhaps
come closest, but at present we still lack
a basis on which different surface realis-
ers could be compared, chiefly because of
the wide variety of different input repre-
sentations used by different realisers. This
paper outlines an idea for a shared task in
surface realisation, where inputs are pro-
vided in a common-ground representation
formalism which participants map to the
types of input required by their system.
These inputs are derived from existing an-
notated corpora developed for language
analysis (parsing etc.). Outputs (realisa-
tions) are evaluated by automatic compari-
son against the human-authored text in the
corpora as well as by human assessors.
1 Background
When reading a paper reporting a new NLP sys-
tem, it is common these days to find that the
authors have taken an NLP utility tool off the
shelf and reused it. Researchers frequently reuse
parsers, POS-taggers, named entity recognisers,
coreference resolvers, and many other tools. Not
only is there a real choice between a range of dif-
ferent systems performing the same task, there are
also evaluation methodologies to help determine
what the state of the art is.
Natural Language Generation (NLG) has not
so far developed generic tools and methods for
comparing them to the same extent as Natural
Language Analysis (NLA) has. The subfield of
NLG that has perhaps come closest to developing
generic tools is surface realisation. Wide-coverage
surface realisers such as PENMAN/NIGEL (Mann
and Mathiesen, 1983), FUF/SURGE (Elhadad and
Robin, 1996) and REALPRO (Lavoie and Ram-
bow, 1997) were intended to be more or less off-
the-shelf plug-and-play modules. But they tended
to require a significant amount of work to adapt
and integrate, and required highly specific inputs
incorporating up to several hundred features that
needed to be set.
With the advent of statistical techniques in NLG
surface realisers appeared for which it was far sim-
pler to supply inputs, as information not provided
in the inputs could be added on the basis of like-
lihood. An early example, the Japan-Gloss sys-
tem (Knight et al, 1995) replaced PENMAN?s de-
fault settings with statistical decisions. The Halo-
gen/Nitrogen developers (Langkilde and Knight,
1998a) allowed inputs to be arbitrarily underspec-
ified, and any decision not made before the realiser
was decided simply by highest likelihood accord-
ing to a language model, automatically trainable
from raw corpora.
The Halogen/Nitrogen work sparked an interest
in statistical NLG which led to a range of surface
realisation methods that used corpus frequencies
in one way or another (Varges and Mellish, 2001;
White, 2004; Velldal et al, 2004; Paiva and Evans,
2005). Some surface realisation work looked at
directly applying statistical models during a lin-
guistically informed generation process to prune
the search space (White, 2004; Carroll and Oepen,
2005).
While statistical techniques have led to realisers
that are more (re)usable, we currently still have
no way of determining what the state of the art
is. A significant subset of statistical realisation
work (Langkilde, 2002; Callaway, 2003; Nakan-
ishi et al, 2005; Zhong and Stent, 2005; Cahill and
van Genabith, 2006; White and Rajkumar, 2009)
has recently produced results for regenerating the
Penn Treebank. The basic approach in all this
work is to remove information from the Penn Tree-
bank parses (the word strings themselves as well
as some of the parse information), and then con-
vert and use these underspecified representations
as inputs to the surface realiser whose task it is to
reproduce the original treebank sentence. Results
are typically evaluated using BLEU, and, roughly
speaking, BLEU scores go down as more informa-
tion is removed.
While publications of work along these lines do
refer to each other and (tentatively) compare BLEU
scores, the results are not in fact directly compara-
ble, because of the differences in the input repre-
sentations automatically derived from Penn Tree-
bank annotations. In particular, the extent to which
they are underspecified varies from one system to
the next.
The idea we would like to put forward with
this short paper is to develop a shared task in sur-
face realisation based on common inputs and an-
notated corpora of paired inputs and outputs de-
rived from various resources from NLA that build
on the Penn Treebank. Inputs are provided in a
common-ground representation formalism which
participants map to the types of input required by
their system. These inputs are automatically de-
rived from the Penn Treebank and the various lay-
ers of annotation (syntactic, semantic, discourse)
that have been developed for the documents in it.
Outputs (realisations) are evaluated by automatic
comparison against the human-authored text in the
corpora as well as by by human assessors.
In the short term, such a shared task would
make existing and new approaches directly com-
parable by evaluation on the benchmark data asso-
ciated with the shared task. In the long term, the
common-ground input representation may lead to
a standardised level of representation that can act
as a link between surface realisers and preceding
modules, and can make it possible to use alterna-
tive surface realisers as drop-in replacements for
each other.
2 Towards Common Inputs
One hugely challenging aspect in developing a
Surface Realisation task is developing a common
input representation that all, or at least a major-
ity of, surface realisation researchers are happy to
work with. While many different formalisms have
been used for input representations to surface re-
alisers, one cannot simply use e.g. van Genabith
et al?s automatically generated LFG f-structures,
White et als CCG logical forms, Nivre?s depen-
dencies, Miyao et al?s HPSG predicate-argument
structures or Copestake?s MRSs etc., as each of
them would introduce a bias in favour of one type
of system.
One possible solution is to develop a meta-
representation which contains, perhaps on multi-
ple layers of representation, all the information
needed to map to any of a given set of realiser in-
put representations, a common-ground representa-
tion that acts as a kind of interlingua for translating
between different input representations.
An important issue in deriving input repre-
sentations from semantically, syntactically and
discourse-annotated corpora is deciding what in-
formation not to include. A concern is that mak-
ing such decisions by committee may be difficult.
One way to make it easier might be to define sev-
eral versions of the task, where each version uses
inputs of different levels of specificity.
Basing a common input representation on what
can feasibly be obtained from non-NLG resources
would put everyone on reasonably common foot-
ing. If, moreover, the common input representa-
tions can be automatically derived from annota-
tions in existing resources, then data can be pro-
duced in sufficient quantities to make it feasible
for participants to automatically learn mappings
from the system-neutral input to their own input.
The above could be achieved by doing some-
thing along the lines of the CoNLL?08 shared task
on Joint Parsing of Syntactic and Semantic De-
pendencies, for which the organisers combined the
Penn Treebank, Propbank, Nombank and the BBN
Named Entity corpus into a dependency represen-
tation. Brief descriptions of these resources and
more details on this idea are provided in Section 4
below.
3 Evaluation
As many NLG researchers have argued, there is
usually not a single right answer in NLG, but var-
ious answers, some better than others, and NLG
tasks should take this into account. If a surface
realisation task is focused on single-best realiza-
tions, then it will not encourage research on pro-
ducing all possible good realizations, or multiple
acceptable realizations in a ranked list, etc. It
may not be the best approach to encourage sys-
tems that try to make a single, safe choice; in-
stead, perhaps one should encourage approaches
that can tell when multiple choices would be ok,
and if some would be better than others.
In the long term we need to develop task defi-
nitions, data resources and evaluation methodolo-
gies that properly take into account the one-to-
many nature of NLG, but in the short term it may be
more realistic to reuse existing non-NLG resources
(which do not provide alternative realisations) and
to adapt existing evaluation methodologies includ-
ing intrinsic assessment of Fluency, Clarity and
Appropriateness by trained evaluators, and auto-
matic intrinsic methods such as BLEU and NIST.
One simple way of adapting the latter, for exam-
ple, could be to calculate scores for the n best re-
alisations produced by a realiser and then to com-
pute a weighted average where scores for reali-
sations are weighted in inverse proportion to the
ranks given to the realisations by the realiser.
4 Data
There is a wide variety of different annotated re-
sources that could be of use in a shared task in sur-
face realisation. Many of these include documents
originally included in the Penn Treebank, and thus
make it possible in principle to combine the var-
ious levels of annotation into a single common-
ground representation. The following is a (non-
exhaustive) list of such resources:
1. Penn Treebank-3 (Marcus et al, 1999): one
million words of hand-parsed 1989 Wall
Street Journal material annotated in Treebank
II style. The Treebank bracketing style al-
lows extraction of simple predicate/argument
structure. In addition to Treebank-1 mate-
rial, Treebank-3 contains documents from the
Switchboard and Brown corpora.
2. Propbank (Palmer et al, 2005): This is a se-
mantic annotation of the Wall Street Journal
section of Penn Treebank-2. More specifi-
cally, each verb occurring in the Treebank has
been treated as a semantic predicate and the
surrounding text has been annotated for ar-
guments and adjuncts of the predicate. The
verbs have also been tagged with coarse
grained senses and with inflectional informa-
tion.
3. NomBank 1.0 (Meyers et al, 2004): Nom-
Bank is an annotation project at New York
University that provides argument structure
for common nouns in the Penn Treebank.
NomBank marks the sets of arguments that
occur with nouns in PropBank I, just as the
latter records such information for verbs.
4. BBN Pronoun Coreference and Entity Type
Corpus (Weischedel and Brunstein, 2005):
supplements the Wall Street Journal corpus,
adding annotation of pronoun coreference,
and a variety of entity and numeric types.
5. FrameNet (Johnson et al, 2002): 150,000
sentences annotated for semantic roles and
possible syntactic realisations. The annotated
sentences come from a variety of sources, in-
cluding some PropBank texts.
6. OntoNotes 2.0 (Weischedel et al, 2008):
OntoNotes 1.0 contains 674k words of Chi-
nese and 500k words of English newswire
and broadcast news data. OntoNotes follows
the Penn Treebank for syntax and PropBank
for predicate-argument structure. Its seman-
tic representation will include word sense
disambiguation for nouns and verbs, with
each word sense connected to an ontology,
and coreference. The current goal is to anno-
tate over a million words each of English and
Chinese, and half a million words of Arabic
over five years.
There are other resources which may be use-
ful. Zettelmoyer and Collins (2009) have man-
ually converted the original SQL meaning an-
notations of the ATIS corpus (et al, 1994)?
some 4,637 sentences?into lambda-calculus ex-
pressions which were used for training and testing
their semantic parser. This resource might make a
good out-of-domain test set for generation systems
trained on WSJ data.
FrameNet, used for semantic parsing, see for
example Gildea and Jurafsky (2002), identifies a
sentence?s frame elements and assigns semantic
roles to the frame elements. FrameNet data (Baker
and Sato, 2003) was used for training and test sets
in one of the SensEval-3 shared tasks in 2004 (Au-
tomatic Labeling of Semantic Roles). There has
been some work combining FrameNet with other
lexical resources. For example, Shi and Mihal-
cea (2005) integrated FrameNet with VerbNet and
WordNet for the purpose of enabling more robust
semantic parsing.
The Semlink project (http://verbs.colorado.
edu/semlink/) aims to integrate Propbank,
FrameNet, WordNet and VerbNet.
Other relevant work includes Moldovan and
Rus (Moldovan and Rus, 2001; Rus, 2002) who
developed a technique for parsing into logical
forms and used this to transform WordNet concept
definitions into logical forms. The same method
(with additional manual correction) was used to
produce the test set for another SensEval-3 shared
task (Identification of Logic Forms in English).
4.1 CoNLL 2008 Shared Task Data
Perhaps the most immediately promising resource
is is the CoNLL shared task data from 2008 (Sur-
deanu et al, 2008) which has syntactic depen-
dency annotations, named-entity boundaries and
the semantic dependencies model roles of both
verbal and nominal predicates. The data consist
of excerpts from Penn Treebank-3, BBN Pronoun
Coreference and Entity Type Corpus, PropBank I
and NomBank 1.0. In CoNLL ?08, the data was
used to train and test systems for the task of pro-
ducing a joint semantic and syntactic dependency
analysis of English sentences (the 2009 CoNLL
Shared Task extended this to multi-lingual data).
It seems feasible that we could reuse the CoNLL
data for a prototype Surface Realisation task,
adapting it and inversing the direction of the task,
i.e. mapping from syntactic-semantic dependency
representations to word strings.
5 Developing the Task
The first step in developing a Surface Realisa-
tion task could be to get together a working
group of surface realisation researchers to develop
a common-ground input representation automati-
cally derivable from a set of existing resources.
As part of this task a prototype corpus exempli-
fying inputs/outputs and annotations could be de-
veloped. At the end of this stage it would be use-
ful to write a white paper and circulate it and the
prototype corpus among the NLG (and wider NLP)
community for feedback and input.
After a further stage of development, it may be
feasible to run a prototype surface realisation task
at Generation Challenges 2011, combined with a
session for discussion and roadmapping. Depend-
ing on the outcome of all of this, a full-blown task
might be feasible by 2012. Some of this work will
need funding to be feasible, and the authors of this
paper are in the process of applying for financial
support for these plans.
6 Concluding Remarks
In this paper we have provided an overview of ex-
isting resources that could potentially be used for
a surface realisation task, and have outlined ideas
for how such a task might work. The core idea
is to develop a common-ground input representa-
tion which participants map to the types of input
required by their system. These inputs are derived
from existing annotated corpora developed for lan-
guage analysis. Outputs (realisations) are evalu-
ated by automatic comparison against the human-
authored text in the corpora as well as by by hu-
man assessors. Evaluation methods are adapted to
take account of the one-to-many nature of the re-
alisation mapping.
The ideas outlined in this paper began as a pro-
longed email exchange, interspersed with discus-
sions at conferences, among the authors. This pa-
per summarises our ideas as they have evolved so
far, to enable feedback and input from other re-
searchers interested in this type of task.
References
Colin F. Baker and Hiroaki Sato. 2003. The framenet
data and software. In Proceedings of ACL?03.
A. Cahill and J. van Genabith. 2006. Robust PCFG-
based generation using automatically acquired LFG
approximations. In Proc. ACL?06, pages 1033?44.
Charles Callaway. 2003. Evaluating coverage for large
symbolic NLG grammars. In Proceedings of the
18th International Joint Conference on Artificial In-
telligence (IJCAI 2003), pages 811?817.
J. Carroll and S. Oepen. 2005. High efficiency
realization for a wide-coverage unification gram-
mar. In Proceedings of the 2nd International Joint
Conference on Natural Language Processing (IJC-
NLP?05), volume 3651, pages 165?176. Springer
Lecture Notes in Artificial Intelligence.
M. Elhadad and J. Robin. 1996. An overview of
SURGE: A reusable comprehensive syntactic real-
ization component. Technical Report 96-03, Dept
of Mathematics and Computer Science, Ben Gurion
University, Beer Sheva, Israel.
Deborah Dahl et al 1994. Expanding the scope of the
ATIS task: the ATIS-3 corpus. In Proceedings of the
ARPA HLT Workshop.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
C. Johnson, C. Fillmore, M. Petruck, C. Baker,
M. Ellsworth, J. Ruppenhoper, and E.Wood. 2002.
Framenet theory and practice. Technical report.
K. Knight, I. Chander, M. Haines, V. Hatzivassiloglou,
E. Hovy, M. Iida, S. Luk, R. Whitney, and K. Ya-
mada. 1995. Filling knowledge gaps in a broad-
coverage MT system. In Proceedings of the Four-
teenth International Joint Conference on Artificial
Intelligence (IJCAI ?95), pages 1390?1397.
I. Langkilde and K. Knight. 1998a. Generation
that exploits corpus-based statistical knowledge. In
Proc. COLING-ACL. http://www.isi.edu/licensed-
sw/halogen/nitro98.ps.
I. Langkilde. 2002. An empirical verification of cover-
age and correctness for a general-purpose sentence
generator. In Proc. 2nd International Natural Lan-
guage Generation Conference (INLG ?02).
B. Lavoie and O. Rambow. 1997. A fast and portable
realizer for text generation systems. In Proceedings
of the 5th Conference on Applied Natural Language
Processing (ANLP?97), pages 265?268.
W. Mann and C. Mathiesen. 1983. NIGEL: A sys-
temic grammar for text generation. Technical Re-
port ISI/RR-85-105, Information Sciences Institute.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-
3. Technical report, Linguistic Data Consortium,
Philadelphia.
Adam Meyers, Ruth Reeves, and Catherine Macleod.
2004. Np-external arguments a study of argument
sharing in english. In MWE ?04: Proceedings of
the Workshop on Multiword Expressions, pages 96?
103, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Dan I. Moldovan and Vasile Rus. 2001. Logic form
transformation of wordnet and its applicability to
question answering. In Proceedings of ACL?01.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic models for disambiguation of an
hpsg-based chart generator. In Proceedings of the
9th International Workshop on Parsing Technology
(Parsing?05), pages 93?102. Association for Com-
putational Linguistics.
D. S. Paiva and R. Evans. 2005. Empirically-based
control of natural language generation. In Proceed-
ings ACL?05.
M. Palmer, P. Kingsbury, and D. Gildea. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Vasile Rus. 2002. Logic Form For WordNet Glosses
and Application to Question Answering. Ph.D. the-
sis.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining framenet, verbnet and wordnet
for robust semantic parsing. In Proceedings of CI-
CLing?05.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In CoNLL ?08:
Proceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, pages 159?177.
S. Varges and C. Mellish. 2001. Instance-based natu-
ral language generation. In Proceedings of the 2nd
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL ?01),
pages 1?8.
E. Velldal, S. Oepen, and D. Flickinger. 2004. Para-
phrasing treebanks for stochastic realization rank-
ing. In Proceedings of the 3rd Workshop on Tree-
banks and Linguistic Theories (TLT ?04), Tuebin-
gen, Germany.
Ralph Weischedel and Ada Brunstein. 2005. Bbn pro-
noun coreference and entity type corpus. Technical
report, Linguistic Data Consortium.
Ralph Weischedel et al 2008. Ontonotes release 2.0.
Technical report, Linguistic Data Consortium.
Michael White and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for ccg realisation. In Pro-
ceedings of the 2009 Conference on Empririal Meth-
ods in Natural Language Processing (EMNLP?09),
pages 410?419.
M. White. 2004. Reining in CCG chart realization. In
A. Belz, R. Evans, and P. Piwek, editors, Proceed-
ings INLG?04, volume 3123 of LNAI, pages 182?
191. Springer.
Luke Zettlemoyer and Michael Collins. 2009. Learn-
ing context-dependent mappings from sentences to
logical forms. In Proceedings of ACL-IJCNLP?09.
H. Zhong and A. Stent. 2005. Building surface
realizers automatically from corpora. In A. Belz
and S. Varges, editors, Proceedings of UCNLG?05,
pages 49?54.
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 14?19,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Decreasing lexical data sparsity in statistical syntactic parsing - experiments
with named entities
Deirdre Hogan, Jennifer Foster and Josef van Genabith
National Centre for Language Technology
School of Computing
Dublin City University
Dublin 9, Ireland
dhogan,jfoster,josef@computing.dcu.ie
Abstract
In this paper we present preliminary exper-
iments that aim to reduce lexical data spar-
sity in statistical parsing by exploiting infor-
mation about named entities. Words in the
WSJ corpus are mapped to named entity clus-
ters and a latent variable constituency parser
is trained and tested on the transformed cor-
pus. We explore two different methods for
mapping words to entities, and look at the ef-
fect of mapping various subsets of named en-
tity types. Thus far, results show no improve-
ment in parsing accuracy over the best base-
line score; we identify possible problems and
outline suggestions for future directions.
1 Introduction
Techniques for handling lexical data sparsity in
parsers have been important ever since the lexical-
isation of parsers led to significant improvements
in parser performance (Collins, 1999; Charniak,
2000). The original treebank set of non-terminal la-
bels is too general to give good parsing results. To
overcome this problem, in lexicalised constituency
parsers, non-terminals are enriched with lexical in-
formation. Lexicalisation of the grammar vastly
increases the number of parameters in the model,
spreading the data over more specific events. Statis-
tics based on low frequency events are not as reliable
as statistics on phenomena which occur regularly in
the data; frequency counts involving words are typi-
cally sparse.
Word statistics are also important in more re-
cent unlexicalised approaches to constituency pars-
ing such as latent variable parsing (Matsuzaki et al,
2005; Petrov et al, 2006). The basic idea of latent
variable parsing is that rather than enrich the non-
terminal labels by augmenting them with words, a
set of enriched labels which can encapsulate the syn-
tactic behaviour of words is automatically learned
via an EM training mechanism.
Parsers need to be able to handle both low fre-
quency words and words occurring in the test set
which were unseen in the training set (unknown
words). The problem of rare and unknown words is
particularly significant for languages where the size
of the treebank is small. Lexical sparseness is also
critical when running a parser on data that is in a dif-
ferent domain to the domain upon which the parser
was trained. As interest in parsing real world data
increases, a parsers ability to adequately handle out-
of-domain data is critical.
In this paper we examine whether clustering
words based on their named entity category can be
useful for reducing lexical sparsity in parsing. In-
tuitively word tokens in the corpus such as, say,
?Dublin? and ?New York? should play similar syn-
tactic roles in sentences. Likewise, it is difficult to
see how different people names could have differ-
ent discriminatory influences on the syntax of sen-
tences. This paper describes experiments at replac-
ing word tokens with special named entity tokens
(person names are mapped to PERSON tokens and
so on). Words in the original WSJ treebank are
mapped to entity types extracted from the BBN cor-
pus (Weischedel and Brunstein, 2005) and a latent
variable parser is trained and tested on the mapped
corpus. Ultimately, the motivation behind grouping
words together in this fashion is to make it easier for
14
the parser to recognise regularities in the data.1
The structure of paper is as follows: A brief sum-
mary of related work is given in Section 2. This
includes an outline of a common treatment of low
frequency and rare words in constituency parsing,
involving a mapping process that is similar to the
named entity mappings. Section 3 presents the ex-
periments carried out, starting with a short introduc-
tion of the named entity resource used in our exper-
iments and a description of the types of basic entity
mappings we examine. In ?3.1 and ?3.2 we describe
the two different types of mapping technique. Re-
sults are presented in Section 4, followed by a brief
discussion in Section 5 indicating possible problems
and avenues worth pursuing. Finally, we conclude.
2 Related Work
Much previous work on parsing and multiword units
(MWUs) adopts the words-with-spaces approach
which treats MWUs as one token (by concatenat-
ing the words together) (Nivre and Nilsson, 2004;
Cafferkey et al, 2007; Korkontzelos and Manand-
har, 2010). Alternative approaches are that of Finkel
and Manning (2009) on joint parsing and named en-
tity recognition and the work of (Wehrli et al, 2010)
which uses collocation information to rank compet-
ing hypotheses in a symbolic parser. Also related
is work on MWUs and grammar engineering, such
as (Zhang et al, 2006; Villavicencio et al, 2007)
where automatically detected MWUs are added to
the lexicon of a HPSG grammar to improve cover-
age.
Our work is most similar to the words-with-
spaces approach. Our many-to-one experiments
(see ?3.1) in particular are similar to previous
work on parsing words-with-spaces, except that we
map words to entity types rather than concatenated
words. Results are difficult to compare however, due
to different parsing methodologies, different types
of MWUs, as well as different evaluation methods.
Other relevant work is the integration of named
1It is true that latent variable parsers automatically induce
categories for similar words, and thus might be expected to
induce a category for say names of people if examples of
such words occurred in similar syntactic patterns in the data.
Nonetheless, the problem of data sparsity remains - it is diffi-
cult even for latent variable parsers to learn accurate patterns
based on words which only occur say once in the training set.
entity types in a surface realisation task by Rajku-
mar et al (2009) and the French parsing experiments
of (Candito and Crabbe?, 2009; Candito and Sed-
dah, 2010) which involve mapping words to clusters
based on morphology as well as clusters automati-
cally induced via unsupervised learning on a large
corpus.
2.1 Parsing unknown words
Most state-of-the-art constituency parsers (e.g.
(Petrov et al, 2006; Klein and Manning, 2003))
take a similar approach to rare and unknown words.
At the beginning of the training process very low
frequency words in the training set are mapped to
special UNKNOWN tokens. In this way, some
probability mass is reserved for occurrences of UN-
KNOWN tokens and the lexicon contains produc-
tions for such tokens (X ? UNKNOWN), with as-
sociated probabilities. When faced with a word in
the test set that the parser has not seen in its train-
ing set - the unknown word is mapped to the special
UNKNOWN token.
In syntactic parsing, rather than map all low fre-
quency words to one generic UNKNOWN type, it
is useful to have several different clusters of un-
known words, grouped according to morphologi-
cal and other ?surfacey? clues in the original word.
For example, certain suffixes in English are strong
predictors for the part-of-speech tag of the word
(e.g. ?ly?) and so all low frequency words end-
ing in ?ly? are mapped to ?UNKNOWN-ly?. As
well as suffix information, UNKNOWN words are
commonly grouped based on information on capi-
talisation and hyphenation. Similar techniques for
handling unknown words have been used for POS
tagging (e.g. (Weischedel et al, 1993; Tseng et
al., 2005)) and are used in the Charniak (Char-
niak, 2000), Berkeley (Petrov et al, 2006) and Stan-
ford (Klein and Manning, 2003) parsers, as well as
in the parser used for the experiments in this paper,
an in-house implementation of the Berkeley parser.
3 Experiments
The BBN Entity Type Corpus (Weischedel and
Brunstein, 2005) consists of sentences from the
Penn WSJ corpus, manually annotated with named
entities. The Entity Type corpus includes annota-
15
type count examples
PERSON 11254 Kim Cattrall
PER DESC 21451 president,chief executive officer,
FAC 383 office, Rockefeller Center
FAC DESC 2193 chateau ,stadiums, golf course
ORGANIZATION 24239 Securities and Exchange Commission
ORG DESC 15765 auto maker, college
GPE 10323 Los Angeles,South Africa
GPE DESC 1479 center, nation, country
LOCATION 907 North America,Europe, Hudson River
NORP 3269 Far Eastern
PRODUCT 667 Maxima, 300ZX
PRODUCT DESC 1156 cars
EVENT 296 Vietnam war,HUGO ,World War II
WORK OF ART 561 Revitalized Classics Take..
LAW 300 Catastrophic Care Act,Bill of Rights
LANGUAGE 62 Latin
CONTACT INFO 30 555 W. 57th St.
PLANT 172 crops, tree
ANIMAL 355 hawks
SUBSTANCE 2205 gold,drugs, oil
DISEASE 254 schizophrenia,alcoholism
GAME 74 football senior tennis and golf tours
Table 1: Name expression entity types (sections 02-21)
tion for three classes of named entity: name expres-
sions, time expressions and numeric expressions (in
this paper we focus on name expressions). These
are further broken down into types. Table 1 displays
name expression entity types, their frequency in the
training set (sections 02-21), as well as some illus-
trative examples from the training set data.
We carried out experiments with different subsets
of entity types. In one set of experiments, all name
expression entities were mapped, with no restriction
on the types (ALL NAMED). We also carried
out experiments on a reduced set of named entities
- where only entities marked as PERSON, ORGA-
NIZATION, or GPE and LOCATION were mapped
(REDUCED). Finally, we ran experiments where
only one type of named entity was mapped at a time.
In all cases the words in the named entities were re-
placed by their entity type.
3.1 Many-to-one Mapping
In the many-to-one mapping all words in a named
entity were replaced with one named entity type
token. This approach is distinct from the words-
with-spaces approach previously pursued in parsing
where, for example, ?New York? would be replaced
with ?New York?. Instead, in our experiments ?New
York? is replaced with ?GPE? (geo-political entity).
In both approaches, the parser is forced to respect
unk map NE map #unks f-score POS
generic
none (baseline 1) 2966 (4.08%) 88.69 95.57
ALL NAMED 1908 (2.73%) 89.21 95.49
REDUCED 2122 (3.02%) 89.43 96.08
Person 2671 (3.68%) 88.98 95.55
Organisation 2521 (3.55%) 89.38 95.92
Location 2945 (4.05%) 89.00 95.62
sigs
none (baseline 2) 2966 (4.08%) 89.72 96.51
ALL NAMED 1908 (2.73%) 89.67 95.99
REDUCED 2122 (3.02%) 89.53 96.65
Person 2671 (3.68%) 89.32 96.47
Organisation 2521 (3.55%) 89.53 96.64
Location 2945 (4.05%) 89.20 96.52
Table 2: Many-to-One Parsing Results.
the multiword unit boundary (and analyses which
contain constituents that cross the MWU boundary
will not be considered by the parser). Intuitively,
this should help parser accuracy and speed. The ad-
vantage of mapping the word tokens to their entity
type rather than to a words-with-spaces token is that
in addition we will be reducing data sparsity.
One issue with the many-to-one mapping is that
in evaluation exact comparison with a baseline re-
sult is difficult because the tokenisation of test and
gold sets is different. When named entities span
more than one word, we are reducing the number
of words in the sentences. As parsers tend to do bet-
ter on short sentences than on long sentences, this
could make parsing somewhat easier. However, we
found that the average number of words in a sen-
tence before and after this mapping does not change
by much. The average number of words in the devel-
opment set is 23.9. When we map words to named
entity tokens (ALL NAMED), the average drops
by just one word to 22.9.2
3.2 One-to-one Mapping
In the one-to-one experiments we replaced each
word in named entity with a named entity type to-
ken (e.g. Ada Lovelace ? pperson pperson).3 The
motivation was to measure the effect of reducing
word sparsity using named entities without altering
the original tokenisation of the data.4
2A related issue is that the resulting parse tree will lack an
analysis for the named entity.
3The entity type was given an extra letter where needed (e.g.
?pperson?) to avoid the conflation of a mapped entity token with
an original word (e.g. ?person?) in the corpus.
4Note, where there is punctuation as part of a named entity
we do not map the punctuation.
16
unk map NE map #unks f-score POS
generic
none (baseline 1) 2966 (4.08%) 88.69 95.57
ALL NAMED 1923 (2.64%) 89.28 94.99
REDUCED 2122 (2.90%) 88.76 95.76
Person 2654(3.65%) 88.95 95.57
Organisation 2521 (3.45%) 88.80 95.59
Location 2945 (4.04%) 88.88 95.66
sigs
none (baseline 2) 2966 (4.08%) 89.72 96.51
ALL NAMED 1923 (2.64%) 89.36 95.64
REDUCED 2122 (2.90%) 89.01 96.32
Person 2654(3.65%) 89.30 96.52
Organisation 2521 (3.45%) 89.29 96.30
Location 2945 (4.04%) 89.55 96.54
Table 3: One-to-One Parsing Results
In an initial experiment, where the mapping was
simply the word to the named entity type, many sen-
tences received no parse. This happened often when
a named entity consisted of three or more words and
resulted in a sentence such as ?But while the Oor-
ganization Oorganization Oorganization Oorganiza-
tion did n?t fall apart Friday?. We found that refining
the named entity by adding the number of the word
in the entity to the mapping resolved the coverage
problem. The example sentence is now: ?But while
the Oorganization1 Oorganization2 Oorganization3
Oorganization4 did n?t fall apart Friday?. See ?5 for
a possible explanation for the parser?s difficulty with
one-to-one mappings to coarse grained entity types.
4 Results
Table 2 and Table 3 give the results for the many-to-
one and one-to-one experiments respectively. Re-
sults are given against a baseline where unknowns
are given a ?generic? treatment (baseline 1) - i.e.
they are not clustered according to morphological
and surface information - and for the second baseline
(baseline 2), where morphological or surface feature
markers (sigs) are affixed to the unknowns.5
The results indicate that though lexical spar-
sity is decreasing, insofar as the number of un-
known words (#unks column) in the development
set decreases with all named entity mappings, the
named entity clusters are not informative enough
and parser accuracy falls short of the previous best
result. For all experiments, a pattern that emerges
5For all experiments, a split-merge cycle of 5 was used. Fol-
lowing convention, sections 02-21 were used for training. Sec-
tions 22 and 24 (sentences less than or equal to 100 words) were
used for the development set. As experiments are ongoing we
do not report results on a test set.
is that mapping words to named entities improves
results when low frequency words are mapped to
a generic UNKNOWN token. However, when low
frequency words are mapped to more fine-grained
UNKNOWN tokens, mapping words to named enti-
ties decreases accuracy marginally.
If a particular named entity occurs often in the text
then data sparsity is possibly not a problem for this
word. Rather than map all occurrences of a named
entity to its entity type, we experimented with map-
ping only low frequency entities. These named en-
tity mapping experiments now mirror more closely
the unknown words mappings - low frequency en-
tities are mapped to special entity types, then the
parser maps all remaining low frequency words to
UNKNOWN types. Table 4 shows the effect of map-
ping only entities that occur less than 10 times in the
training set, to the person type and the reduced set
of entity types. Results somewhat improve for all
but one of the one-to-one experiments, but nonethe-
less remain below the best baseline result. There is
still no advantage in mapping low frequency person
name words to, say, the person cluster, rather than to
an UNKNOWN-plus-signature cluster.
5 Discussion
Our results thus far suggest that clusters based on
morphology or surface clues are more informative
than the named entity clusters.
For the one-to-one mappings one obvious prob-
lem that emerged is that all words in entities (in-
cluding function words for example) get mapped to
a generic named entity token. A multi-word named
entity has its own internal syntactic structure, re-
flected for example in its sequence of part-of-speech
tags. By replacing each word in the entity with
the generic entity token we end up loosing informa-
tion about words, conflating words that take differ-
ent part-of-speech categories, and in fact make pars-
ing more difficult. The named entity clusters in this
case are too coarse-grained and words with different
syntactic properties are merged into the one cluster,
something we would like to avoid.
In future work, as well as avoiding mapping more
complex named entities, we will refine the named
entity clusters by attaching to the entity type signa-
tures similar to those attached to the UNKNOWN
17
unk map NE map one2one f-score many2one f-score
generic
Person 88.95 88.98
Person < 10 88.97 89.05
Reduced 88.76 89.43
Reduced < 10 89.51 88.85
sigs
Person 89.30 89.32
Person < 10 89.49 89.33
Reduced 89.01 89.53
Reduced < 10 89.42 89.15
Table 4: Measuring the effect of mapping only low fre-
quency named entities.
types. It would also be interesting to examine the ef-
fect of mapping other types of named entities, such
as dates and numeric expressions. Finally, we intend
trying similar experiments on out-of-domain data,
such as social media text where unknown words are
more problematic.
6 Conclusion
We have presented preliminary experiments which
test the novel technique of mapping word tokens to
named entity clusters, with the aim of improving
parser accuracy by reducing data sparsity. While our
results so far are disappointing, we have identified
possible problems and outlined future experiments,
including suggestions for refining the named entity
clusters so that they become more syntactically ho-
mogenous.
References
Conor Cafferkey, Deirdre Hogan, and Josef van Gen-
abith. 2007. Multi-word units in treebank-based prob-
abilistic parsing and generation. In Proceedings of the
10th International Conference on Recent Advances in
Natural Language Processing (RANLP-07), Borovets,
Bulgaria.
Marie Candito and Benoit Crabbe?. 2009. Improving gen-
erative statistical parsing with semi-supervised word
clustering. In Proceedings of the International Work-
shop on Parsing Technologies (IWPT-09).
Marie Candito and Djame? Seddah. 2010. Lemmatization
and statistical lexicalized parsing of morphologically-
rich languages. In Proceedings of the NAACL/HLT
Workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL).
Eugene Charniak. 2000. A maximum entropy-inspired
parser. In Proceedings of the 1st North American
Chapter of the Association for Computational Linguis-
tics (NAACL).
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL-2009).
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association of Computational
Linguistics (ACL).
Ioannis Korkontzelos and Suresh Manandhar. 2010. Can
recognising multiword expressions improve shallow
parsing? In Proceedings of the Conference of the
North American Chapter of the ACL (NAACL-10), Los
Angeles, California.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic cfg with latent annotations. In
Proceedings of the 43rd Annual Meeting of the ACL,
pages 75?82, Ann Arbor, June.
Joakim Nivre and Jens Nilsson. 2004. Multiword units
in syntactic parsing. In Workshop on Methodologies
and Evaluation of Multiword Units in Real-World Ap-
plications.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the ACL, Sydney,
Australia, July.
Rajakrishnan Rajkumar, Michael White, and Dominic
Espinosa. 2009. Exploiting named entity classes in
ccg surface realisation. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL-09).
Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005. Morpholgical features help pos tagging
of unknown words across language varieties. In Pro-
ceedings of the Fourth SIGHAN Workshop on Chinese
Language Processing.
Aline Villavicencio, Valia Kordoni, Yi Zhang, Marco
Idiart, and Carlos Ramisch. 2007. Validation and
evaluation of automatically acquired multiword ex-
pressions for grammar engineering. In Proceedings of
the Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
Eric Wehrli, Violeta Seretan, and Luke Nerima. 2010.
Sentence analysis and collocation identification. In
Proceedings of the Workshop on Multiword Expres-
sion: From Theory to Applications (MWE).
Ralph Weischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus. In Tehcnical
Report.
18
Ralph Weischedel, Richard Schwartz, Jeff Palmucci,
Marie Meteer, and Lance Ramshaw. 1993. Coping
with ambiguity and unknown words through proba-
bilistic models. Computational Linguistics, 19(2).
Yi Zhang, Valia Kordoni, Aline Villavicencio, and Marco
Idiart. 2006. Automated multiword expression pre-
diction for grammar engineering. In Proceedings of
the Workshop on Multiword Expressions: Identifying
and Exploiting Underlying Properties.
19
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 232?242,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Using Syntactic Head Information in Hierarchical Phrase-Based Translation
Junhui Li Zhaopeng Tu? Guodong Zhou? Josef van Genabith
Centre for Next Generation Localisation
School of Computing, Dublin City University
? Key Lab. of Intelligent Info. Processing
Institute of Computing Technology, Chinese Academy of Sciences
?School of Computer Science and Technology
Soochow University, China
{jli,josef}@computing.dcu.ie
tuzhaopeng@ict.ac.cn gdzhou@suda.edu.cn
Abstract
Chiang?s hierarchical phrase-based (HPB)
translation model advances the state-of-the-art
in statistical machine translation by expanding
conventional phrases to hierarchical phrases
? phrases that contain sub-phrases. How-
ever, the original HPB model is prone to over-
generation due to lack of linguistic knowl-
edge: the grammar may suggest more deriva-
tions than appropriate, many of which may
lead to ungrammatical translations. On the
other hand, limitations of glue grammar rules
in the original HPB model may actually pre-
vent systems from considering some reason-
able derivations. This paper presents a sim-
ple but effective translation model, called the
Head-Driven HPB (HD-HPB) model, which
incorporates head information in translation
rules to better capture syntax-driven informa-
tion in a derivation. In addition, unlike the
original glue rules, the HD-HPB model allows
improved reordering between any two neigh-
boring non-terminals to explore a larger re-
ordering search space. An extensive set of ex-
periments on Chinese-English translation on
four NIST MT test sets, using both a small
and a large training set, show that our HD-
HPB model consistently and statistically sig-
nificantly outperforms Chiang?s model as well
as a source side SAMT-style model.
1 Introduction
Chiang?s hierarchical phrase-based (HPB) transla-
tion model utilizes synchronous context free gram-
mar (SCFG) for translation derivation (Chiang,
2005; Chiang, 2007) and has been widely adopted
in statistical machine translation (SMT). Typically,
such models define two types of translation rules:
hierarchical (translation) rules which consist of both
terminals and non-terminals, and glue (grammar)
rules which combine translated phrases in a mono-
tone fashion. However, due to lack of linguistic
knowledge, Chiang?s HPB model contains only one
type of non-terminal symbol X , often making it
difficult to select the most appropriate translation
rules.1
One important research question is therefore how
to refine the non-terminal category X using linguis-
tically motivated information: Zollmann and Venu-
gopal (2006) (SAMT) e.g. use (partial) syntactic
categories derived from CFG trees while Zollmann
and Vogel (2011) use word tags, generated by ei-
ther POS analysis or unsupervised word class in-
duction. Almaghout et al (2011) employ CCG-
based supertags. Mylonakis and Sima?an (2011) use
linguistic information of various granularities such
as Phrase-Pair, Constituent, Concatenation of Con-
stituents, and Partial Constituents, where applica-
ble.
By contrast, and inspired by previous work in
parsing (Charniak, 2000; Collins, 2003), our Head-
Driven HPB (HD-HPB) model is based on the in-
tuition that linguistic heads provide important in-
formation about a constituent or distributionally de-
fined fragment, as in HPB. We identify heads using
linguistically motivated dependency parsing, and
use head information to refine X.
Furthermore, Chiang?s HPB model suffers from
limited phrase reordering by combining translated
1Another non-terminal symbol S is used in glue rules.
232
 (a) (b) 
zuotian chuxi huiyi 
attended a meeting yesterday 
X2 X1 
X1 X2 
S2 
S1 
S2 
S1 
zuotian chuxi huiyi 
attended a meeting yesterday 
X4 X3 
X3 X4 
X2 
X1 
X2 
S2 
X1 
S1 
S1 
X1 
Figure 1: Example of derivations disallowed in Chiang?s
HPB model. The rules with dotted lines are not covered
in Chiang?s model.
phrases in a monotonic way with glue rules. In
addition, once a glue rule is adopted, it requires
all rules above it to be glue rules. For exam-
ple, given a Chinese-English sentence pair (?
?/zuotian1 ??/chuxi2 ??/huiyi3, Attended2 a3
meeting3 yesterday1), a correct translation is impos-
sible via HPB derivations in Figure 1. For the deriva-
tion in Figure 1(a), swap reordering in the glue rule
(i.e., S1 ? ?S2X2, X2S2?) is disallowed and, even
if such a swap reordering is available, it lacks useful
information for rule selection. For the derivation in
Figure 1(b), the combination of two non-terminals
(i.e., X2 ? ?X3X4, X3X4?) is disallowed to form
a new non-terminal which in turn is a sub-phrase of
a hierarchical rule. These limitations prevent tra-
ditional HPB systems from even considering some
reasonable derivations.
To tackle the problem of glue rules, He (2010) ex-
tended the HPB model by using bracketing transduc-
tion grammar (Wu, 1996) instead of the monotone
glue rules, and trained an extra classifier for glue
rules to predict reorderings of neighboring phrases.
By contrast, our HD-HPB model refines the non-
terminal symbol X with syntactic head informa-
tion and provides flexible reordering rules, including
swap, which can mix freely with hierarchical trans-
lation rules for better interleaving of translation and
reordering in translation derivations.
Different from the soft constraint modeling
adopted in (Chan et al, 2007; Marton and Resnik,
2008; Shen et al, 2009; He et al, 2010; Huang et
al., 2010; Gao et al, 2011), our approach encodes
syntactic information in translation rules. However,
the two approaches are not mutually exclusive, as
we could also include a set of syntax-driven features
into our translation model. Our approach maintains
the advantages of Chiang?s HPB model while at the
same time incorporating head information and flex-
ible reordering in a derivation in a natural way. Ex-
periments on Chinese-English translation using four
NIST MT test sets show that our HD-HPB model
significantly outperforms Chiang?s HPB as well as a
SAMT-style refined version of HPB.
The paper is structured as follows: Section 2
describes the synchronous context-free grammar
(SCFG) in our HD-HPB translation model. Sec-
tion 3 presents our model and features, followed by
the decoding algorithm in Section 4. We report ex-
perimental results in Section 5. Finally we conclude
in Section 6.
2 Head-Driven HPB Translation Model
Like Chiang (2005) and Chiang (2007), our HD-
HPB translation model adopts a synchronous con-
text free grammar, a rewriting system which gen-
erates source and target side string pairs simultane-
ously using a context-free grammar. In particular,
each synchronous rule rewrites a non-terminal into
a pair of strings, s and t, where s (or t) contains ter-
minals and non-terminals from the source (or target)
language and there is a one-to-one correspondence
between the non-terminal symbols on both sides.
A good and informative inventory of non-terminal
symbols is always important, especially for a suc-
cessful SCFG-based translation model. Instead of
collapsing all non-terminals in the source language
into a single symbol X as in Chiang (2007), ideally
non-terminals should capture important information
of the word sequences they cover to be able to prop-
erly discriminate between similar and different word
sequences during translation. This motivates our
approach to provide syntax-enriched non-terminal
symbols. Given a word sequence f ij from position i
to position j, we refine the non-terminal symbol X
to reflect some of the internal syntactic structure of
233
?
?
/N
R
 
O
uz
ho
u 
?
?
/N
N 
ba
gu
o 
?
?
/A
D
li
an
mi
ng
?
?
/V
V
zh
ic
hi
 
?
?
/N
R 
me
ig
uo
 
?
/P du
i
?
?
/N
N
ce
li
e 
?
/N
R
yi
 
ro
ot
E
ig
ht
 
E
ur
op
ea
n 
co
un
tr
ie
s
jo
in
tly
su
pp
or
t
A
m
er
ic
a?
s
st
an
d
ag
ai
ns
t
Ir
aq
Figure 2: An example word alignment for a Chinese-English sentence pair with the dependency parse tree for the
Chinese sentence. Here, each Chinese word is attached with its POS tag and Pinyin.
the word sequence covered by X . A correct transla-
tion rule selection therefore not only maps terminals
into terminals, but is both constrained and guided
by syntactic information in the non-terminals. At
the same time, it is not clear whether an ?ideal? ap-
proach that captures a full syntactic analysis of the
string fragment covered by a non-terminal is feasi-
ble: the diversity of syntactic structures could make
training impossible and lead to serious data sparse-
ness issues. As a compromise, given a word se-
quence f ij , we first find heads and then concatenate
the POS tags of these heads as f ij?s non-terminal
symbol.2 Our approach is guided by the intuition
that linguistic heads provide important information
about a constituent or distributionally defined frag-
ment, as in HPB. Specifically, we adopt dependency
structure to derive heads, which are defined as:
Definition 1. For word sequence f ij , word
fk (i ? k ? j) is regarded as a head if it is domi-
nated by a word outside of this sequence.
Note that this definition (i) allows for a word se-
quence to have one or more heads (largely due to
the fact that a word sequence is not necessarily lin-
guistically constrained) and (ii) ensures that heads
are always the highest heads in the sequence from a
dependency structure perspective. For example, the
word sequence ouzhou baguo lianming in Figure 2
has two heads (i.e., baguo and lianming, ouzhou is
not a head of this sequence since its headword baguo
falls within this sequence) and the non-terminal cor-
responding to the sequence is thus labeled as NN-
AD. It is worth noting that in this paper we only
refine non-terminal X on the source side to head-
informed ones, while still usingX on the target side.
2Note that instead of POS tags, it is also possible to use other
types of syntactic information associated with heads to refine
non-terminal symbols (Section 5.5.2).
In our HD-HPB model, the SCFG is defined as
a tuple ??, N,?,?,<?, where ? is a set of source
language terminals,N is a set of non-terminals cate-
gorizing terminals in ?, ? is a set of target language
terminals, ? is a set of non-terminals categorizing
terminals in ?, and < is a set of translation rules.
A rule ? in < is in the form of ?Ps ? s, Pt ? t, ??,
where:
? Ps ? N and Pt ? ?;
? s ? (? ?N)+ and t ? (? ? ?)+
? ? is a bijection between non-terminals in s and t.
According to the occurrence of terminals in s and
t, we group the rules in the HD-HPB model into two
categories: head-driven hierarchical rules (HD-HRs)
and non-terminal reordering rules (NRRs), where
the former have at least one terminal on both source
and target sides and the later have no terminals. For
rule extraction, we first identify initial phrase pairs
on word-aligned sentence pairs by using the same
criterion as most phrase-based translation models
(Och and Ney, 2004) and Chiang?s HPB model (Chi-
ang, 2005; Chiang, 2007). We extract HD-HRs and
NRRs based on initial phrase pairs, respectively.
2.1 HD-HRs: Head-Driven Hierarchical Rules
As mentioned, a HD-HR has at least one terminal
on both source and target sides. This is the same
as the hierarchical rules defined in Chiang?s HPB
model (Chiang, 2007), except that we use head POS-
informed non-terminal symbols in the source lan-
guage. We look for initial phrase pairs that con-
tain other phrases and then replace sub-phrases with
their corresponding non-terminal symbols. Given
the word alignment as shown in Figure 2, Table 1
demonstrates the difference between hierarchical
rules in Chiang (2007) and HD-HRs defined here.
234
phrase pairs hierarchical rule head-driven hierarchical rule
celie, stand X?celie, stand
NN?celie,
X?stand
dui yi celie1, stand1 against Iraq X?dui yi X1, X1 against Iraq
NN?dui yi NN1,
X?X1 against Iraq
zhichi meiguo, support America?s X?zhichi meiguo, support America?s
VV-NR?zhichi meiguo,
X?support America?s
zhichi meiguo1 dui yi celie2,
support America?s1 stand2 against Iraq
X?X1 dui yi X2,
X1 X2 against Iraq
VV?VV-NR1 dui yi NN2,
X?X1 X2 against Iraq
Table 1: Comparison of hierarchical rules in Chiang (2007) and HD-HRs. Indexed underlines indicate sub-phrases
and corresponding non-terminal symbols. The non-terminals in HD-HRs (e.g., NN, VV, VV-NR) capture the head(s)
POS tags of the corresponding word sequence in the source language.
Similar to Chiang?s HPB model, our HD-HPB
model will result in a large number of rules causing
problems in decoding. To alleviate these problems,
we filter our HD-HRs according to the same con-
straints as described in Chiang (2007). Moreover,
we discard rules that have non-terminals with more
than four heads.
2.2 NRRs: Non-terminal Reordering Rules
NRRs are translation rules without terminals. Given
an initial phrase pair
?
f ij , e
i?
j?
?
, we check all other
initial phrase pairs
?
fkl , e
k?
l?
?
which satisfy k = j+1
(i.e., phrase fkl is located immediately to the right
of f ij in the source language). For their target
side translations, there are four possible positional
relationships: monotone, discontinuous monotone,
swap, and discontinuous swap. In order to differen-
tiate non-terminals from those in the target language
(i.e., X), we use Y as a variable for non-terminals in
the source language, and obtain four types of NRRs:
? Monotone ?Y ? Y1Y2, X ? X1X2?;
? Discontinuous monotone
?Y ? Y1Y2, X ? X1 . . . X2?;
? Swap ?Y ? Y1Y2, X ? X2X1?;
? Discontinuous swap
?Y ? Y1Y2, X ? X2 . . . X1?.
For example in Figure 2, the NRR for initial
phrase pairs ?zhichi meiguo, support America?s?
and ?dui yi celie, stand against Iraq? would be
?V V ? V V -NR1NN2, X ? X1X2?.
Merging two neighboring non-terminals into a
single non-terminal, NRRs enable the translation
model to explore a wider search space. During train-
ing, we extract four types of NRRs and calculate
probabilities for each type. To speed up decoding,
we currently (i) only use monotone and swap NRRs
and (ii) limit the number of non-terminals in a NRR
to 2.
3 Log-linear Model and Features
Following Och and Ney (2002), we depart from the
traditional noisy-channel approach and use a general
log-linear model. Let d be a derivation from sen-
tence f in the source language to sentence e in the
target language. The probability of d is defined as:
P (d) ?
?
i
?i (d)
?i (1)
where ?i are features defined on derivations and
?i are feature weights. In particular, we use a fea-
ture set analogous to the default feature set of Chi-
ang (2007), which includes:
? Phd-hr (t|s) and Phd-hr (s|t), translation probabili-
ties for HD-HRs;
? Plex (t|s) and Plex (s|t), lexical translation proba-
bilities for HD-HRs;
? Ptyhd-hr = exp (?1), rule penalty for HD-HRs;
? Pnrr (t|s), translation probability for NRRs;
? Ptynrr = exp (?1), rule penalty for NRRs;
? Plm (e), language model;
? Ptyword (e) = exp (?|e|), word penalty.
235
Algorithm 1: Decoding Algorithm
Input: Sentence f1n in the source language
Dependency structure of f1n
HD-HR rule set HDHR
NRR rule set NRR
Initial phrase length K
Output: Best derivation d?
1. set chart[i, j]=NIL (1 ? i ? j ? n);
2. for l from 1 to n do
3. for all i, j such that j ? i = l do
4. if l ? K do
5. for all derivations d derived from
HDHR spanning from i to j do
6. add d into chart[i, j]
7. for all derivations d derived from
NRR spanning from i to j do
8. add d into chart[i, j]
9. set d? as the top derivation of chart[1, n]
10.return d?
It is worth pointing out that we define translation
probabilities for NRRs only for the direction from
source language to target language, although trans-
lation probabilities for HD-HRs are defined for both
directions. This is mostly due to the fact that a NRR
excludes terminals and has only two options on the
target side (i.e., either X ? X1X2 or X ? X2X1).
4 Decoding
Our decoder is based on CKY-style chart parsing
with beam search. Given an input sentence f , it finds
a sentence e in the target language derived from the
best derivation d? among all possible derivations D:
d? = arg max
d?D
P (D) (2)
Algorithm 1 presents the decoding process. Given
a source sentence, it searches for the best deriva-
tion bottom-up. For a source span [i, j], it applies
both types of HD-HRs and NRRs. However, HD-
HRs are only applied to generate derivations span-
ning no more than K words ? the initial phrase
length limit used in training to extract HD-HRs ?
while NRRs are applied to derivations spanning any
length. Unlike in Chiang (2007), it is possible for
a non-terminal generated by a NRR to be included
afterwards by a HD-HR or another NRR. Similar to
Chiang (2007) in generating k-best derivations from
i to j, we make use of cube pruning (Huang and Chi-
ang, 2005) with an integrated language model for
each derivation.
5 Experiments
We evaluate the performance of our HD-HPB model
and compare it with our implementation of Chiang?s
HPB model (Chiang, 2007), a source-side SAMT-
style refined version of HPB (SAMT-HPB), and the
Moses implementation of HPB. For fair compari-
son, we adopt the same parameter settings for HD-
HPB, HPB and SAMT-HPB systems, including ini-
tial phrase length (as 10) in training, the maximum
number of non-terminals (as 2) in translation rules,
maximum number of non-terminals plus terminals
(as 5) on the source, prohibition of non-terminals
to be adjacent on the source, beam threshold ? (as
10?5) (to discard derivations with a score worse than
? times the best score in the same chart cell), beam
size b (as 200) (i.e. each chart cell contains at most
b derivations). For Moses HPB, we use ?grow-diag-
final-and? to obtain symmetric word alignments, 10
for the maximum phrase length, and the recom-
mended default values for all other parameters.
5.1 Experimental Settings
To examine the efficacy of our approach on training
datasets of different scales, we first train translation
models on a small-sized corpus, and then scale to a
larger one. We use the 2002 NIST MT evaluation
test data (878 sentence pairs) as the development
data, and the 2003, 2004, 2005, 2006-news NIST
MT evaluation test data (919, 1788, 1082, and 616
sentence pairs, respectively) as the test data. To find
heads, we parse the source sentences with the Berke-
ley Parser3 (Petrov and Klein, 2007) trained on Chi-
nese TreeBank 6.0 and use the Penn2Malt toolkit4
to obtain dependency structures.
We obtain the word alignments by running
GIZA++ (Och and Ney, 2000) on the corpus in
both directions, applying ?grow-diag-final-and? re-
finement (Koehn et al, 2003). We use the SRI lan-
guage modeling toolkit to train a 5-gram language
model on the Xinhua portion of the Gigaword corpus
and standard MERT (Och, 2003) to tune the feature
3http://code.google.com/p/berkeleyparser/
4http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html/
236
weights on the development data.
For evaluation, the NIST BLEU script (version
12) with the default settings is used to calculate the
NIST and the BLEU scores, which measures case-
insensitive matching of n-grams with n up to 4. To
test whether a performance difference is statistically
significant, we conduct significance tests following
the paired bootstrap approach (Koehn, 2004). In this
paper, ?**? and ?*? denote p-values less than
0.01 and in-between [0.01, 0.05), respectively.
5.2 Results on Small Data
To test the HD-HPB models, we firstly carried out
experiments using the FBIS corpus as training data,
which contains ?240K sentence pairs. Table 2 lists
the rule table sizes. The full rule table size (includ-
ing HD-HRs and NRRs) of our HD-HPB model is
about 1.5 times that of Chiang?s, largely due to re-
fining the non-terminal symbolX in Chiang?s model
into head-informed ones in our model. It is also
unsurprising, that the test set-filtered rule table size
of our model is only about 0.8 times that of Chi-
ang?s: this is due to the fact that some of the re-
fined translation rule patterns required by the test
set are unattested in the training data. Furthermore,
the rule table size of NRRs is much smaller than
that of HD-HRs since a NRR contains only two
non-terminals. Table 3 lists the translation perfor-
mance with NIST and BLEU scores. Note that our
re-implementation of Chiang?s original HPB model
performs on a par with Moses HPB. Table 3 shows
that our HD-HPB model significantly outperforms
Chiang?s HPB model with an average improvement
of 1.32 in BLEU and 0.16 in NIST (and similar im-
provements over Moses HPB).
Although HD-HPB has small size of phrase ta-
bles compared to HPB, it still consumes more time
in decoding (e.g., 15.1 vs. 11.0), mostly due to the
flexible reordering of NRRs.
5.3 Results on Large Data
We also conduct experiments on larger training
data with ?1.5M sentence pairs from the LDC
dataset.5 Table 4 lists the rule table sizes and Ta-
ble 5 presents translation performance with NIST
5This dataset includes LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06
and BLEU scores. It shows that our HD-HPB model
consistently outperforms Chiang?s HPB model with
an average improvement of 1.91 in BLEU and 0.35
in NIST (similar for Moses HPB). Compared to the
improvement achieved on the small data, it is en-
couraging to see that our HD-HPB model benefits
more from larger training data with little adverse ef-
fect on decoding time which increases only slightly
from 15.1 to 16.6 seconds per sentence.
5.4 Comparison with SAMT-HPB
Comparing the performance of SAMT-HPB with
regular HPB in Table 3 and Table 5, it is interest-
ing to see that in general the SAMT-style approach
leads to a deterioration of translation performance
for the small training set (e.g., 30.09 for SAMT-HPB
vs. 30.64 for HPB) while it comes into its own for
the large training set (e.g., 33.54 for SAMT-HPB vs.
32.95 for HPB), indicating that the SAMT-style ap-
proach is more prone to data sparseness than HPB
(or, indeed, HD-HPB).
Comparing the performance of SAMT-HPB with
HD-HPB, shows that our head-driven non-terminal
refining approach consistently outperforms the
SAMT-style approach on an extensive set of ex-
periments (for each test set p < 0.01), indicating
that head information is more effective than (par-
tial) CFG categories. To make the comparison fair,
it is important to note that our implementation of
source-side SAMT-HPB includes the same sophis-
ticated non-terminal re-ordering NRR rules as HD-
HPB (Section 2.2 ). Thus the performance differ-
ences reported here are not due to different reorder-
ing capabilities, but to the discriminative impact of
the head information in HD-HPB over SAMT-style
annotation. Taking lianming zhichi in Figure 2 as an
example, HD-HPB labels the span VV, as lianming
is dominated by zhichi, effecively ignoring lianming
in the translation rule, while the SAMT label is
ADVP:AD+VV6 which is more susceptible to data
sparsity (Table 2 and Table 4). In addition, SAMT
resorts to X if a text span fails to satisify pre-defined
categories. Examining initial phrases extracted from
the SAMT training data shows that 28% of them are
labeled as X. Finally, for Chinese syntactic analy-
6The constituency structure for lianming zhichi is (VP
(ADVP (AD lianming)) (VP (VV zhichi) ...)).
237
System Total Rules MT 03 MT 04 MT 05 MT 06 Avg.
HPB 39.6M 2.8M 4.7M 3.3M 3.0M 3.4M
HD-HPB 59.5/0.6M 1.9/0.1M 3.4/0.2M 2.3/0.2M 2.0/0.1M 2.4/0.2M
SAMT-HPB 70.1/0.4M 2.2/0.2M 4.0/0.2M 2.7/0.2M 2.3/0.2M 2.8/0.2M
Table 2: Rule table sizes of different models trained on small data. Note: 1) SAMT-HPB indicates our HD-HPB model
with the non-terminal scheme of Zollmann and Venugopal (2006); 2) For HD-HPB and SAMT-HPB, the rule sizes
separated by / indicate HD-HRs and NRRs, respectively; 2) Except for ?Total Rules?, the figures correspond to rules
filtered on the corresponding test set.
System
MT 03 MT 04 MT 05 MT 06 Avg.
Time
NIST BLEU NIST BLEU NIST BLEU NIST BLEU NIST BLEU
Moses HPB 7.377 29.67 8.209 33.60 7.571 29.49 6.773 28.90 7.483 30.42 NA
HPB 8.137 29.75 9.050 34.06 8.264 30.09 7.788 28.64 8.310 30.64 11.0
HD-HPB 8.308 31.01** 9.211 35.11** 8.426 31.57** 7.930 30.15** 8.469 31.96 15.1
SAMT-HPB 7.886 29.14* 8.703 33.32** 7.961 29.49* 7.307 28.41 7.964 30.09 17.3
HD-HR+Glue 7.966 29.51 8.826 33.68 8.116 29.84 7.474 28.51 8.095 30.39 5.4
Table 3: NIST and BLEU (%) scores of different models trained on small data. Note: 1) HD-HR+Glue indicates our
HD-HPB model replacing NRRs with glue rules; 2) Significance tests for Moses HPB, HD-HPB, SAMT-HPB and
HD-HR+Glue are done against HPB.
System Total Rules MT 03 MT 04 MT 05 MT 06 Avg.
HPB 206.8M 11.3M 17.6M 12.9M 10.4M 13.0M
HD-HPB 318.6/2.3M 7.3/0.3M 12.2/0.4M 8.5/0.3M 6.7/0.2M 8.7/0.3M
SAMT-HPB 371.0/1.1M 8.6/0.3M 14.3/0.4M 10.1/0.3M 7.9/0.3M 10.2/0.3M
Table 4: Rule table sizes of different models trained on large data.
System
MT 03 MT 04 MT 05 MT 06 Avg.
Time
NIST BLEU NIST BLEU NIST BLEU NIST BLEU NIST BLEU
Moses HPB 7.914 32.94* 8.429 35.16 7.962 32.18 6.483 29.88* 7.697 32.54 NA
HPB 8.583 33.59 9.114 35.39 8.465 32.20 7.532 30.60 8.423 32.95 13.7
HD-HPB 8.885 35.50** 9.494 37.61** 8.871 34.56** 7.839 31.78** 8.772 34.86 16.6
SAMT-HPB 8.644 34.07 9.245 36.52** 8.618 32.90* 7.543 30.66 8.493 33.54 19.1
HD-HR+Glue 8.831 34.58** 9.435 36.55** 8.821 33.84** 7.863 31.06 8.737 34.01 6.7
Table 5: NIST and BLEU (%) scores of different models trained on large data. Note: System labels and significance
testing as in Table 3.
238
sis, dependency structure is more reliable than con-
stituency structure. Moreover, SAMT-HPB takes
more time in decoding than HD-HPB due to larger
phrase tables.
5.5 Discussion
5.5.1 Individual Contribution of HD-HRs and
NRRs
Examining translation output shows that on aver-
age each sentence employs 16.6/5.2 HD-HRs/NRRs
in our HD-HPB model, compared to 15.9/3.6 hier-
archical rules/glue rules in Chiang?s model, provid-
ing further indication of the importance of NRRs in
translation. In order to separate out the individual
contributions of the novel HD-HRs and NRRs, we
carry out an additional experiment (HD-HR+Glue)
using HD-HRs with monotonic glue rules only (ad-
justed to refined rule labels, but effectively switching
off the extra reordering power of full NRRs) both
on the small and the large datasets, with interest-
ing results: Table 3 (HD-HR+Glue) shows that for
the small training set most of the improvement of
our full HD-HPB model comes from the NRRs, as
RR+Glue performs on the same level as Chiang?s
original and Moses HPB (the differences are not
statistically significant), perhaps indicating sparse-
ness for the refined HD-HRs given the small train-
ing set. Table 5 shows that for the large training
set, HD-HRs come into their own: on average more
than half of the improvement over HPB (Chiang and
Moses) comes from the refined HD-HRs, the rest
from NRRs.
It is not surprising that compared to the others
HD-HR+Glue takes much less time in decoding.
This is due to the fact that 1) compared to HPB, the
refined translation rule patterns on the source side
have fewer entries in phrase table; 2) compared to
HD-HPB, HD-HR+Glue switches off the extra re-
ordering of NRRs. The decoding time for HD-HPB
and HD-HR+Glue suggests that NRRs are more than
doubling the time required to decode.
5.5.2 Different Head Label Sets
Examining initial phrases extracted from the large
size training data shows that there are 63K types
of refined non-terminals with respect to 33 types of
POS tags. Considering the sparseness in translation
rules caused by this comparatively detained POS tag
set, we carry out an experiment with a reduced set
of non-terminal types by using a less granular POS
tag set (C-HPB). Moreover, due to the fact that con-
catenation of POS tags of heads mostly captures in-
ternal structure of a text span, it is interesting to ex-
amine the effect of other syntactic labels, in partic-
ular dependency labels, to try to better capture the
impact of the external context on the text span. To
this end, we replace the POS tag of head with its
incoming dependency label (DL-HPB), or the com-
bination of (the original fine-grained) POS tag and
its dependency label (POS-DL-HPB). For C-HPB
we use the coarse POS tag set obtained by group-
ing the 33 types of Chinese POS tags into 11 types
following Xia (2000). For example, we generalize
all verbal tags (e.g., VA, VC, VE, and VV ) and all
nominal tags (e.g., NR, NT, and NN) into Verb and
Noun, respectively. We use the dependency labels
in Penn2Malt which defines 9 types of dependency
labels for Chinese, including AMOD, DEP, NMOD,
P, PMOD, ROOT, SBAR, VC, and VMOD.7
Table 6 shows the results trained on large data.
Although the number of non-terminal types de-
creased sharply from 63K to 3K, using the coarse
POS tag set in C-HPB surprisingly lowers the per-
formance with 1.1 BLEU scores on average (e.g.,
33.75 vs. 34.86), indicating that grouping POS
tags using simple linguistic rules is inappropriate for
HD-HPB. We still believe that this initial negative
finding should be supplemented by future work on
groupping POS tags using machine learning tech-
niques considering contextual information.
Table 6 also shows that replacing POS tags
of heads with their dependency labels (DL-HPB)
substantially lowers the average performance from
34.86 on BLEU score to 32.54, probably due to
the very coarse granularity of the dependency la-
bels used. In addition, replacing non-terminal label
with more refined tags (e.g., combination of original
POS tag and dependency label) also lowers trans-
lation performance (POS-DL-HPB). Further experi-
ments with more fine-grained dependency labels are
required.
7Some other types of dependency labels (e.g., SUB, OBJ)
are generated from function tags which are not available in our
automatic parse trees.
239
V
V-
N
R
1 d
u
i 
yi
 
N
N
2 
V
V
?
 
,
 
X
?
 
X
1 
X
2 
ag
ai
n
st
 
Ir
aq
 
(b)
 
zh
ic
hi
 
m
ei
gu
o
1 
du
i y
i c
el
ie
2,
 
 
su
pp
o
rt
 
A
m
er
ic
a?
s 1
 
st
an
d 2
 
ag
ai
n
st
 
Ir
aq
 
 
V
V-
N
R
?
 
zh
ic
hi
 
m
ei
gu
o
 ,
 
X
?
 
su
pp
o
rt
 
A
m
er
ic
a?
s 
(a)
 
zh
ic
hi
 
m
ei
gu
o
,
 
su
pp
o
rt
 
A
m
er
ic
a?
s 
Figure 3: Examples of pharse pairs and their head-driven
translation rules with dependency relation, regarding Fig-
ure 2
System MT 03 MT 04 MT 05 MT 06 Avg.
HPB 33.59 35.39 32.20 30.60 32.95
HD-HPB 35.50 37.61 34.56 31.78 34.86
C-HPB 34.10 36.43 33.46 31.00 33.75
DL-HPB 32.81 35.19 32.27 29.89 32.54
POS-DL-HPB 34.08 36.78 33.14 30.43 33.61
HD-DEP-HPB 35.48 38.17 34.81 32.38 35.21
Table 6: BLEU (%) scores of models trained on large
data.
5.5.3 Encoding Full Dependency Relations in
Translation Rule
Xie et al (2011) present a dependency-to-string
translation model with a complete dependency struc-
ture on the source side and a moderate average im-
provement of 0.46 BLEU over the HPB baseline. By
contrast, in our HD-HPB approach, dependency in-
formation is used to identify heads in the strings cov-
ered by non-terminals in HD-HR rules, and to refine
non-terminal labels accordingly, with an average im-
provement of 1.91 in BLEU over the HPB baseline
(when trained on the large data). This raises the
question whether and to what extent complete (un-
labeled) dependency information between the string
and the heads in head-labeled non-terminal parts of
the source side of SCFGs in HD-HPB can further
improve results.
Given the source side of a translation rule (ei-
ther HD-HR or NRR), say Ps ? s1 . . . sm (where
each si is either a terminal or a head POS in a re-
fined non-terminal), in a further set of experiments
we keep the full unlabeled dependency relations be-
tween s1 . . . sm so as to capture contextual syntactic
information in translation rules. For example, on the
source side of Figure 3 (b) where VV-NR maps into
words zhichi and meiguo while NN maps into word
celie, we keep the full unlabeled dependency rela-
tions among words {zhichi, meiguo, dui, yi, celie}.
HD-DEP-HPB (Table 6) augments translation rules
in HD-HPB with full dependency relations on the
source side. This further boosts the performance
by 0.35 BLEU scores on average over HD-HPB and
outperforms the HPB baseline by 2.26 BLEU scores
on average.
5.5.4 Error Analysis
We carried out a manual error analysis compar-
ing the outputs of our HD-HPB system with those
of Chiang?s (both trained on the large data). We ob-
serve that improved BLEU score often correspond to
better topological ordering of phrases in the hierar-
chical structure of the source side, with a direct im-
pact on which words in a source sentence should be
translated first, and which later. As ungrammatical
translations are often due to inappropriate topologi-
cal orderings of phrases in the hierarchical structure,
guiding the translation through appropriate topolog-
ical ordering should improve translation quality. To
give an example, consider the following input sen-
tence from the 04 NIST MT test data and its two
translation results:
? Input: ??0 ??1 ?2 ?3 ??4 ????5 ?
?6 ?7 ??8 ??9
? HPB: chinese delegation to us dollar purchase of
more high technology equipment
? HD-HPB: chinese delegation went to the united
states to buy more us high - tech equipment
Figure 4 demonstrates the topological orderings
in the two hierarchical structures. In addition to dis-
fluency and some grammar errors (e.g., a main verb
is missing), the basic HPB system also makes mis-
takes in reordering (e.g., ??4 ????5 ??6
translated as dollar purchase of more). The poor
translation quality, unsurprisingly, is caused by in-
appropriate topological ordering (Figure 4(a)). By
comparison, the topological ordering reflected in the
hierarchical structure of our HD-HPB model bet-
ter respects syntactic structure (Figure 4(b)). Let
240
??
?
0?
??
?
1?
?? 2?
?? 3?
??
?
4?
??
??
?
5?
??
?
6?
?? 7?
??
?
8?
??
?
9?
X [4
?4]
?
X [6
?6]
?
X [4
?6]
?
X [3
?7]
?
X [3
?8]
?
X [2
?9]
?
X [1
?9]
?
X [0
?9]
?
S [0
?9]
?
(a)
.?T
op
olo
gic
al?
or
de
rin
gs
?of
?ph
ra
se
s?i
n?C
hia
ng
?s?
HP
B.?
(b
).?I
mp
ro
ve
d?t
op
olo
gic
al?
or
de
rin
gs
?of
?ph
ra
se
s?i
n?H
D?
HP
B.
1.?
S [0
?9]
??
?X [
0?9
],??
????
????
????
????
??X
[0?
9]
?
2.?
X [0
?9]
??
???
[0?
0]
?X [
1?9
],??
????
????
????
????
???c
hin
es
e?X
[1?
9]?
3.?
X [1
?9]
??
???
[1?
1]
?X [
2?9
],??
????
????
????
????
??d
ele
ga
tio
n?X
[2
?9
]?
4.?
X [2
?9]
??
?? [
2?2
]?X
[3
?8]
???
[9?
9],?
?
????
????
????
????
??to
?X [
3?8
]?e
qu
ipm
en
t?
5.?
X [3
?8]
??
?X [
3?7
]??
?,?
?
X [3
?7]
?te
ch
no
log
y?
6.?
X [3
?7]
??
?? [
3?3
]?X
[4
?6]
?? [
7?
7],?
?
us
?X [
4?6
]?h
igh
?
7.?
X [4
?6]
??
?X [
4?4
]??
??
? [5
?5]
?X [
6?6
],?
X [6
?6]
?X [
4?
4]?o
f?m
or
e?
8.?
X [4
?4]
??
???
[4?
4]
,??
pu
rch
as
e?
9.?
X [6
?6]
??
???
[6?
6]
,??
do
lla
r
1. ?
VV
[0
?9]
??
?N
N [
0?1
]?V
V [2
?9
],??
????
????
????
?X?
?
?X [
0?
1]?X
[2
?9]
?
2.?
NN
[0?
1]
??
???
[0?
0]?N
N [
1?
1]
,??
????
????
????
??X
??
?ch
ine
se
?X [
1?
1]??
3.?
NN
[1?
1]
??
???
[1
?1]
,??
????
????
????
??X
??
?de
leg
ati
on
?
4.?
VV
[2
?9]
??
?? [
2?2
]??
[3?
3]?
VV
[4?
9]
,???
????
?X?
?
?w
en
t?t
o?t
he
?un
ite
d?s
tat
es
?to
?X [
4?
9]?
5.?
VV
[4
?9]
??
?VV
?M
[4
?6
]??
[7?
7]
???
[8
?8]
?N
N [
9?9
],?
????
????
????
X??
?X [
4?6
]?h
igh
??t
ec
h?X
[9?
9]??
6.?
VV
?M
[4
?6]
??
???
[4
?4]
?M
[5?
6]
,?
????
????
????
????
??X
??
?bu
y?X
[5?
6]
??
7.?
M
[5
?6]
??
?CD
[5?
5]?M
[6
?6]
,??
????
????
????
X??
?X [
5?5
]?X
[6
?6]
?
8.?
CD
[5
?5]
??
???
??
[5
?5]
,??
????
????
????
?X?
?
?m
or
e?
9.?
M
[6
?6]
??
???
[6?
6]
,??
????
????
????
X??
?us
?
10
.?N
N [
9?9
]??
???
[9
?9]
,??
????
????
????
?X?
?
?eq
uip
me
nt
?
ro
ot
?? NR 0 
?? NN 1 
? VV 2
? NR 3
??
 
VV 4 
??
?? CD 5 
?? M 6 
? JJ 7
?? NN 8 
?? NN 9
CD
[5-5
]
M
[6-6
]
M
[5-6
]
VV-M
[4-6
]
VV
[4-9
]
VV
[2-9
] 
NN
 
VV
[0-9
]
[0-1
]
NN
NN
[1-1
]
[9-9
]
Figure 4: An example Chinese sentence and its two hierarchical structures. Note: subscript [i-j] represents spanning
from word i to word j on the source side.
us refer to the HD-HPB hierarchical structure on
the source side as translation parse tree and to the
treebank-based parser derived tree as syntactic parse
tree from which we obtain unlabeled dependency
structure. Examining the translation parse trees of
our HD-HPB model shows that phrases with 1/2/3/4
heads account for 64.9%/23.1%/8.8%/3.2%, respec-
tively. Compared to 37.9% of the phrases in the
translation parse trees of the HPB model, 43.2% of
the phrases of our HD-HPB model correspond to a
linguistically motivated constituent in the syntactic
parse tree with exactly the same text span. In sum,
therefore, instead of simply enforcing hard linguistic
constraints imposed by a full syntactic parse struc-
ture, our model opts for a successful mix of linguis-
tically motivated and combinatorial (matching sub-
phrases in HPB) constraints.
6 Conclusion
In this paper, we present a head-driven hierarchi-
cal phrase-based translation model, which adopts
head information (derived through unlabeled depen-
dency analysis) in the definition of non-terminals
to better differentiate among translation rules. In
addition, improved and better integrated reorder-
ing rules allow better reordering between consecu-
tive non-terminals through exploration of a larger
search space in the derivation. Our model main-
tains the strengths of Chiang?s HPB model while at
the same time it addresses the over-generation prob-
lem caused by using a uniform non-terminal symbol.
Experimental results on Chinese-English translation
across a wide range of training and test sets demon-
strate significant and consistent improvements of our
HD-HPB model over Chiang?s HPB model as well
as over a source side version of the SAMT-style
model.
Currently, we only consider head information in a
word sequence. In the future work, we will exploit
more syntactic and semantic information to system-
atically and automatically define the inventory of
non-terminals (in source and target). For example,
for a non-terminal symbol VV, we believe it will
benefit translation if we use fine-grained dependency
labels (subject, object etc.) used to link it to its gov-
erning head elsewhere in the translation rule.
Acknowledgments
This work was supported by Science Foundation Ire-
land (Grant No. 07/CE/I1142) as part of the Cen-
tre for Next Generation Localisation (www.cngl.ie)
at Dublin City University. It was also partially
supported by Project 90920004 under the National
Natural Science Foundation of China and Project
2012AA011102 under the ?863? National High-
Tech Research and Development of China. We
thank the reviewers for their insightful comments.
References
Hala Almaghout, Jie Jiang, and Andy Way. 2011. CCG
contextual labels in hierarchical phrase-based SMT. In
Proceedings of EAMT 2011, pages 281?288.
241
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of ACL 2007, pages
33?40.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL 2000, pages 132?
139.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL 2005, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589?637.
Yang Gao, Philipp Koehn, and Alexandra Birch. 2011.
Soft dependency constraints for reordering in hierar-
chical phrase-based translation. In Proceedings of
EMNLP 2011, pages 857?868.
Zhongjun He, Yao Meng, and Hao Yu. 2010. Maxi-
mum entropy based phrase reordering for hierarchical
phrase-based translation. In Proceedings of EMNLP
2010, pages 555?563.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT 2005, pages 53?64.
Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.
2010. Soft syntactic constraints for hierarchical
phrase-based translation using latent syntactic distri-
butions. In Proceedings of EMNLP 2010, pages 138?
147.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL 2003, pages 48?54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-HLT 2008, pages 1003?1011.
Markos Mylonakis and Khalil Sima?an. 2011. Learning
hierarchical translation structure with linguistic anno-
tations. In Proceedings of ACL-HLT 2011, pages 642?
652.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000, pages 440?447.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statisti-
cal machine translation. In Proceedings of ACL 2002,
pages 295?302.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
2003, pages 160?167.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
2007, pages 404?411.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proceedings of EMNLP 2009, pages
72?80.
Dekai Wu. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In Proceedings of ACL
1996, pages 152?158.
Fei Xia. 2000. The part-of-speech tagging guidelines for
the Penn Chinese Treebank (3.0). Technical Report
IRCS-00-07, University of Pennsylvania Institute for
Research in Cognitive Science Technical.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel
dependency-to-string model for statistical machine
translation. In Proceedings of EMNLP 2011, pages
216?226.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of NAACL 2006 - Workshop on Statistical
Machine Translation, pages 138?141.
Andreas Zollmann and Stephan Vogel. 2011. A word-
class approach to labeling PSCFG rules for machine
translation. In Proceedings of ACL-HLT 2011, pages
1?11.
242
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 177?184,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Shallow Semantically-Informed PBSMT and HPBSMT
Tsuyoshi Okita
Qun Liu
Josef van Genabith
Dublin City University
Glasnevin, Dublin 9, Ireland
{tokita,qliu,josef}@computing.dcu.ie
Abstract
This paper describes shallow
semantically-informed Hierarchical
Phrase-based SMT (HPBSMT) and
Phrase-Based SMT (PBSMT) systems
developed at Dublin City University
for participation in the translation task
between EN-ES and ES-EN at the Work-
shop on Statistical Machine Translation
(WMT 13). The system uses PBSMT
and HPBSMT decoders with multiple
LMs, but will run only one decoding
path decided before starting translation.
Therefore the paper does not present a
multi-engine system combination. We
investigate three types of shallow seman-
tics: (i) Quality Estimation (QE) score,
(ii) genre ID, and (iii) context ID derived
from context-dependent language models.
Our results show that the improvement is
0.8 points absolute (BLEU) for EN-ES
and 0.7 points for ES-EN compared to
the standard PBSMT system (single best
system). It is important to note that we
developed this method when the standard
(confusion network-based) system com-
bination is ineffective such as in the case
when the input is only two.
1 Introduction
This paper describes shallow semantically-
informed Hierarchical Phrase-based SMT
(HPBSMT) and Phrase-Based SMT (PBSMT)
systems developed at Dublin City University
for participation in the translation task between
EN-ES and ES-EN at WMT 13. Our objectives
are to incorporate several shallow semantics into
SMT systems. The first semantics is the QE score
for a given input sentence which can be used to
select the decoding path either of HPBSMT or
PBSMT. Although we call this a QE score, this
score is not quite a standard one which does not
have access to translation output information. The
second semantics is genre ID which is intended to
capture domain adaptation. The third semantics
is context ID: this context ID is used to adjust the
context for the local words. Context ID is used in
a continuous-space LM (Schwenk, 2007), but is
implicit since the context does not appear in the
construction of a continuous-space LM. Note that
our usage of the term semantics refers to meaning
constructed by a sentence or words. The QE
score works as a sentence level switch to select
HPBSMT or PBSMT, based on the semantics
of a sentence. The genre ID gives an indication
that the sentence is to be translated by genre ID-
sensitive MT systems, again based on semantics
on a sentence level. The context-dependent LM
can be interpreted as supplying the local context
to a word, capturing semantics on a word level.
The architecture presented in this paper is sub-
stantially different from multi-engine system com-
bination. Although the system has multiple paths,
only one path is chosen at decoding when process-
ing unseen data. Note that standard multi-engine
system combination using these three semantics
has been presented before (Okita et al, 2012b;
Okita et al, 2012a; Okita, 2012). This paper also
compares the two approaches.
The remainder of this paper is organized as fol-
lows. Section 2 describes the motivation for our
approach. In Section 3, we describe our proposed
systems, while in Section 4 we describe the exper-
imental results. We conclude in Section 5.
2 Motivation
Model Difference of PBSMT and HPBSMT
Our motivation is identical with a system combi-
nation strategy which would obtain a better trans-
lation if we can access more than two translations.
Even though we are limited in the type of MT sys-
177
tems, i.e. SMT systems, we can access at least
two systems, i.e. PBSMT and HPBSMT systems.
The merit that accrues from accessing these two
translation is shown in Figure 1. In this exam-
ple between EN-ES, the skirts of the distribution
shows that around 20% of the examples obtain the
same BLEU score, 37% are better under PBSMT,
and 42% under HPBSMT. Moreover, around 10%
of sentences show difference of 10 BLEU points.
Even a selection of outputs would improve the re-
sults. Unfortunately, some pitfall of system com-
bination (Rosti et al, 2007) impact on the process
when the number of available translation is only
two. If there are only two inputs, (1) the mismatch
of word order and word selection would yield a
bad combination since system combination relies
on monolingual word alignment (or TER-based
alignment) which seeks identical words, and (2)
Minimum Bayes Risk (MBR) decoding, which is
a first step, will not work effectively since it re-
lies on voting. (In fact, only selecting one of the
translation outputs is even effective: this method
is called system combination as well (Specia et al,
2010).) Hence, although the aim is similar, we do
not use a system combination strategy, but we de-
velop a semantically-informed SMT system.
Figure 1: Figure shows the difference of sentence-
based performance between PBSMT and HPB-
SMT systems.
Relation of Complexity of Source Sentence and
Performance of HPBSMT and PBSMT It is
interesting to note that PBSMT tends to be bet-
ter than HPBSMT for European language pairs
as the recent WMT workshop shows, while HPB-
SMT shows often better performance for distant
language pairs such as EN-JP (Okita et al, 2010b)
and EN-ZH in other workshops.
Under the assumption that we use the same
training corpus for training PBSMT and HPBSMT
systems, our hypothesis is that we may be able
to predict the quality of translation. Note that al-
though this is the analogy of quality estimation,
the setting is slightly different in that in test phase,
we will not be given a translation output, but only
a source sentence. Our aim is to predict whether
HPBSMT obtains better translation output than
PBSMT or not. Hence, our aim does not require
that the quality prediction here is very accurate
compared to the standard quality estimation task.
We use a feature set consisting of various charac-
teristics of input sentences.
3 Our Methods: Shallow Semantics
Our system accommodates PBSMT and HPBSMT
with multiple of LMs. A decoder which handles
shallow semantic information is shown in Table
3.1.
3.1 QE Score
Quality estimation aims to predict the quality of
translation outputs for unseen data (e.g. by build-
ing a regressor or a classifier) without access to
references: the inputs are translation outputs and
source sentences in a test phase, while in a training
phase the corresponding BLEU or HTER scores
are used. In this subsection, we try to build a re-
gressor with the similar settings but without sup-
plying the translation outputs. That is, we supply
only the input sentences. (Since our method is not
a quality estimation for a given translation output,
quality estimation may not be an entirely appro-
priate term. However, we borrow this term for this
paper.) If we can build such a regressor for PB-
SMT and HPBSMT systems, we would be able
to select a better translation output without actu-
ally translating them for a given input sentence.
Note that we translate the training set by PBSMT
and HPBSMT in a training phase only to supply
their BLEU scores to a regressor (since a regres-
sor is a supervised learning method). Then, we
use these regressors for a given unseen source sen-
tence (which has no translation output attached) to
predict their BLEU scores for PBSMT and HPB-
SMT.
Our motivation came from the comparison of
a sequential learning system and a parser-based
system. The typical decoder of the former is a
178
Viterbi decoder while that of the latter is a Cocke-
Younger-Kasami (CYK) decoder (Younger, 1967).
The capability of these two systems provides
an intuition about the difference of PBSMT and
HPBSMT: the CYK decoder-based system has
some capability to handle syntactic constructions
while the Viterbi decoder-based system has only
the capability of learning a sequence. For ex-
Input: Foreign sent f=f1,...,f1f , language model,
translation model, rule table.
Output: English translation e
ceScore = predictQEScore(fi)
if (ceScore == HPBSMTBetter)
for span length l=1 to 1f do
for start=0..1f -1 do
genreID = predictGenreID(fi)
end = start + 1
forall seq s of entries and words in span
[start,end] do
forall rules r do
if rule r applies to chart seq s then
create new chart entry c
with LM(genreID)
add chart entry c to chart
return e from best chart entry in span [0,1f ]
else:
genreID = predictGenreID(fi)
place empty hypothesis into stack 0
for all stacks 0...n-1 do
for all hypotheses in stack do
for all translation options do
if applicable then
create new hyp with LM(ID)
place in stack
recombine with existing hyp if
possible
prune stack if too big
return e
predictQEScore()
predictGenreID()
predictContextID(wordi, wordi?1)
Table 1: Decoding algorithm: the main algorithm
of PBSMT and HPBSMT are from (Koehn, 2010).
The modification is related to predictQEScore(),
predictGenreID(), and predictContextID().
ample, the (context-free) grammar-based system
has the capability of handling various difficul-
ties caused by inserted clauses, coordination, long
Multiword Expressions, and parentheses, while
the sequential learning system does not (This is
since this is what the aim of the context-free
grammar-based system is.) These difficulties are
manifest in input sentences.
0 50 100 150 200 250 300
sample ID
?1.0
?0.5
0.0
0.5
1.0
d
i
f
f
e
r
e
n
c
e
 
o
f
 
B
L
E
U
 
p
o
i
n
t
s
true BLEU difference of PBSMT and HPBSMT
predicted BLEU difference of PBSMT and HPBSMT
Figure 2: A blue line shows the true BLEU dif-
ference between PBSMT and HPBSMT (y-axis)
where x-axis is the sample IDs reordered in de-
scending order (blue), while green dots show the
BLEU absolute difference (y-axis) of the typical
samples where x-axis is shared with the above.
This example is sampled 300 points from new-
stest2013 (ES-EN). Even if the regressor does not
achieve a good performance, the bottom line of the
overall performance is already really high in this
tricky problem. Roughly, even if we plot randomly
we could achieve around 80 - 90% of correctness.
Around 50% of samples (middle of the curve) do
not care (since the true performance of PBSMT
and HPBSMT are even), there is a slope in the left
side of the curve where random plot around this
curve would achieve 15 - 20% among 25% of cor-
rectness (the performance of PBSMT is superior),
and there is another slope in the right side of the
curve where random plot would achieve again 15
- 20% among 25% (the performance of HPBSMT
is superior). In this case, accuracy is 86%.
If we assume that this is one major difference
between these two systems, the complexity of the
input sentence will correlate with the difference of
translation quality of these two systems. In this
subsection, we assume that this is one major dif-
ference of these two systems and that the complex-
ity of the input sentence will correlate with the dif-
ference of translation quality of these two systems.
Based on these assumptions, we build a regressor
179
for each system for a given input sentence where in
a training phase we supply the BLEU score mea-
sured using the training set. One remark is that the
BLEU score which we predict is only meaning-
ful in a relative manner since we actually generate
a translation output in preparation phase (there is
a dependency to the mean of BLEU score in the
training set). Nevertheless, this is still meaningful
as a relative value if we want to talk about their
difference, which is what we want in our settings
to predict which system, either PBSMT or HPB-
SMT, will generate a better output.
The main features used for training the regres-
sor are as follows: (1) number of / length of in-
serted clause / coordination / multiword expres-
sions, (2) number of long phrases (connection by
?of?; ordering of words), (3) number of OOV
words (which let it lower the prediction quality),
(4) number of / length of parenthesis, etc. We ob-
tained these features using parser (de Marneffe et
al., 2006) and multiword extractor (Okita et al,
2010a).
3.2 Genre ID
Genre IDs allow us to apply domain adaptation
technique according to the genre ID of the testset.
Among various methods of domain adaptation, we
investigate unsupervised clustering rather than al-
ready specified genres.
We used (unsupervised) classification via La-
tent Dirichlet Allocation (LDA) (Blei et al, 2003)
to obtain genre ID. LDA represents topics as
multinomial distributions over the W unique
word-types in the corpus and represents docu-
ments as a mixture of topics.
Let C be the number of unique labels in the
corpus. Each label c is represented by a W -
dimensional multinomial distribution ?c over the
vocabulary. For document d, we observe both the
words in the document w(d) as well as the docu-
ment labels c(d). Given the distribution over top-
ics ?d, the generation of words in the document is
captured by the following generative model.
1. For each label c ? {1, . . . C}, sample a distri-
bution over word-types ?c ? Dirichlet(?|?)
2. For each document d ? {1, . . . , D}
(a) Sample a distribution over its observed
labels ?d ? Dirichlet(?|?)
(b) For each word i ? {1, . . . , NWd }
i. Sample a label z(d)i ?
Multinomial(?d)
ii. Sample a word w(d)i ?
Multinomial(?c) from the la-
bel c = z(d)i
Using topic modeling (or LDA) as described
above, we perform the in-domain data partitioning
as follows, building LMs for each class, and run-
ning a decoding process for the development set,
which will obtain the best weights for cluster i.
1. Fix the number of clusters C, we explore val-
ues from small to big.1
2. Do unsupervised document classification (or
LDA) on the source side of the training, de-
velopment and test sets.
3. Separate each class of training sets and build
LM for each cluster i (1 ? i ? C).
4. Separate each class of development set (keep
the original index and new index in the allo-
cated separated dataset).
5. (Using the same class of development set):
Run the decoder on each class to obtain the
n-best lists, run a MERT process to obtain the
best weights based on the n-best lists, (Repeat
the decoding / MERT process several itera-
tions. Then, we obtain the best weights for a
particular class.)
For the test phase,
1. Separate each class of the test set (keep the
original index and new index in the allocated
separated dataset).
2. Suppose the test sentence belongs to cluster
i, run the decoder of cluster i.
3. Repeat the previous step until all the test sen-
tences are decoded.
3.3 Context ID
Context ID semantics is used through the re-
ranking of the n-best list in a MERT process
(Schwenk, 2007; Schwenk et al, 2012; Le et al,
2012). 2-layer ngram-HMM LM is a two layer
version of the 1-layer ngram-HMM LM (Blun-
som and Cohn, 2011) which is a nonparametric
1Currently, we do not have a definite recommendation on
this. It needs to be studied more deeply.
180
Bayesian method using hierarchical Pitman-Yor
prior. In the 2-layer LM, the hidden sequence of
the first layer becomes the input to the higher layer
of inputs. Note that such an architecture comes
from the Restricted Boltzmann Machine (Smolen-
sky, 1986) accumulating in multiple layers in or-
der to build deep belief networks (Taylor and Hin-
ton, 2009). Although a 2-layer ngram-HMM LM
is inferior in its performance compared with other
two LMs, the runtime cost is cheaper than these.
ht denotes the hidden word for the first layer, h?t
denotes the hidden word for the second layer, wi
denotes the word in output layer. The generative
model for this is shown below.
ht|h?t ? F (??st) (1)
wt|ht ? F (?st) (2)
wi|w1:i?1 ? PY(di, ?i, Gi) (3)
where ? is a concentration parameter, ? is a
strength parameter, and Gi is a base measure.
Note that these terms belong to the hierarchical
Pitman-Yor language model (Teh, 2006). We used
a blocked inference for inference. The perfor-
mance of 2-layer LM is shown in Table 3.
4 Experimental Settings
We used Moses (Koehn et al, 2007) for PBSMT
and HPBSMT systems in our experiments. The
GIZA++ implementation (Och and Ney, 2003) of
IBM Model 4 is used as the baseline for word
alignment: Model 4 is incrementally trained by
performing 5 iterations of Model 1, 5 iterations
of HMM, 3 iterations of Model 3, and 3 iter-
ations of Model 4. For phrase extraction the
grow-diag-final heuristics described in (Koehn et
al., 2003) is used to derive the refined alignment
from bidirectional alignments. We then perform
MERT process (Och, 2003) which optimizes the
BLEU metric, while a 5-gram language model is
derived with Kneser-Ney smoothing (Kneser and
Ney, 1995) trained with SRILM (Stolcke, 2002).
For the HPBSMT system, the chart-based decoder
of Moses (Koehn et al, 2007) is used. Most of the
procedures are identical with the PBSMT systems
except the rule extraction process (Chiang, 2005).
The procedures to handle three kinds of se-
mantics are implemented using the already men-
tioned algorithm. We use libSVM (Chang and Lin,
2011), and Mallet (McCallum, 2002) for Latent
Dirichlet Allocation (LDA) (Blei et al, 2003).
For the corpus, we used all the resources pro-
vided for the translation task at WMT13 for lan-
output layer
2?layer conditional RBM language model
ngram language model
1st RBM
2nd RBM
hidden layer
output layer
N
projection layer
discrete representation
N
P
neural network
probability estimation
continuous?space language
model [Schwenk, 2007]
1st hidden layer
2?layer ngram?HMM language model
2nd hidden layer
output layer
ngram language model
Figure 3: Figure shows the three kinds of context-
dependent LM. The upper-side shows continuous-
space language model (Schwenk, 2007). The
lower-left shows ours, i.e. the 2-layer ngram-
HMM LM. The lower-right shows the 2-layer con-
ditional Restricted Boltzmann Machine LM (Tay-
lor and Hinton, 2009).
guage model, that is parallel corpora (Europarl
V7 (Koehn, 2005), Common Crawl corpus, UN
corpus, and News Commentary) and monolingual
corpora (Europarl V7, News Commentary, and
News Crawl from 2007 to 2012).
Experimental results are shown in Table 2.
The left-most column (sem-inform) shows our re-
sults. The sem-inform made a improvement of 0.8
BLEU points absolute compared to the PBSMT
results in EN-ES, while the standard system com-
bination lost 0.1 BLEU points absolute compared
to the single worst. For ES-EN, the sem-inform
made an improvement of 0.7 BLEU points abso-
lute compared to the PBSMT results. These im-
provements over both of PBSMT and HPBSMT
are statistically significant by a paired bootstrap
test (Koehn, 2004).
5 Conclusion
This paper describes shallow semantically-
informed HPBSMT and PBSMT systems devel-
oped at Dublin City University for participation in
the translation task at the Workshop on Statistical
Machine Translation (WMT 13). Our system has
181
EN-ES sem-inform PBSMT HPBSMT syscomb aug-syscomb
BLEU 30.3 29.5 28.2 28.1 28.5
BLEU(11b) 30.3 29.5 28.2 28.1 28.5
BLEU-cased 29.0 28.4 27.1 27.0 27.5
BLEU-cased(11b) 29.0 28.4 27.1 27.0 27.5
NIST 7.91 7.74 7.35 7.35 7.36
Meteor 0.580 0.579 0.577 0.577 0.578
WER 53.7 55.4 59.3 59.2 58.9
PER 41.3 42.4 46.0 45.8 45.5
ES-EN sem-inform PBSMT HPBSMT syscomb aug-syscomb
BLEU 31.1 30.4 23.1? 28.8 29.9
BLEU(11b) 31.1 30.4 23.1? 28.8 29.9
BLEU-cased 29.7 29.1 22.3? 27.9 28.8
BLEU-cased(11b) 29.7 29.1 22.3? 27.9 28.8
NIST 7.87 7.79 6.67? 7.40 7.71
Meteor 0.615 0.612 0.533? 0.612 0.613
WER 54.8 55.4 62.5? 59.3 56.1
PER 41.3 41.8 48.3? 45.8 41.9
Table 2: Table shows the score where ?sem-inform? shows our system. Underlined figure shows the
official score. ?syscomb? denotes the confusion-network-based system combination using BLEU, while
?aug-syscomb? uses three shallow semantics described in QE score (Okita et al, 2012a), genre ID (Okita
et al, 2012b), and context ID (Okita, 2012). Note that the inputs for syscomb and aug-syscomb are the
output of HPBSMT and PBSMT. HPBSMT from ES to EN has marked with ?, which indicates that this
is trained only with Europarl V7.
2-layer ngram- SRI-
EN HMM LM LM
newstest12 130.4 140.3
newstest11 146.2 157.1
newstest10 156.4 166.8
newstest09 176.3 187.1
Table 3: Table shows the perplexity of context-
dependent language models, which is 2-layer
ngram HMM LM, and that of SRILM (Stolcke,
2002) in terms of newstest09 to 12.
PBSMT and HPBSMT decoders with multiple
LMs, but our system will execute only one path,
which is different from multi-engine system
combination. We consider investigate three types
of shallow semantic information: (i) a Quality
Estimate (QE) score, (ii) genre ID, and (iii) a
context ID through context-dependent language
models. Our experimental results show that the
improvement is 0.8 points absolute (BLEU) for
EN-ES and 0.7 points for ES-EN compared to
the standard PBSMT system (single best system).
We developed this method when the standard
(confusion network-based) system combination is
ineffective such as in the case when the input is
only two.
A further avenue would be the investigation of
other semantics such as linguistic semantics, in-
cluding co-reference resolution or anaphora reso-
lution, hyper-graph decoding, and text understand-
ing. Some of which are investigated in the context
of textual entailment task (Okita, 2013b) and we
would like to extend this to SMT task. Another
investigation would be the integration of genre ID
into the context-dependent LM. The preliminary
work shows that such integration would decrease
the overall perplexity (Okita, 2013a).
Acknowledgments
We thank Antonio Toral and Santiago Corte?s
Va??lo for providing parts of their processing
data. This research is supported by the Science
Foundation Ireland (Grant 07/CE/I1142) as part
of the Centre for Next Generation Localisation
(http://www.cngl.ie) at Dublin City Uni-
versity.
182
References
David Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research, 3:9931022.
Phil Blunsom and Trevor Cohn. 2011. A hierarchical
pitman-yor process hmm for unsupervised part of
speech induction. In Proceedings of Annual Meet-
ing of the Association for Computational Linguistics
(ACL11), pages 865?874.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1?27:27.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-05), pages
263?270.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC-
2006), pages 449?454.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for n-gram language modeling.
In Proceedings of the IEEE International Confer-
ence on Acoustics, Speech and Signal Processing,
pages 181?184.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computationa Linguistics (HLT / NAACL
2003), pages 115?124.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for Statistical Machine Translation. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions, pages 177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), pages 388?395.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
Machine Translation Summit, pages 79?86.
Philipp Koehn. 2010. Statistical machine translation.
Cambridge University Press.
Hai-Son Le, Thomas Lavergne, Alexandre Allauzen,
Marianna Apidianaki, Li Gong, Aurelien Max,
Artem Sokolov, Guillaume Wisniewski, and Fran-
cois Yvon. 2012. Limsi at wmt12. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, pages 330?337.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Tsuyoshi Okita, Alfredo Maldonado Guerra, Yvette
Graham, and Andy Way. 2010a. Multi-Word Ex-
pression sensitive word alignment. In Proceed-
ings of the Fourth International Workshop On Cross
Lingual Information Access (CLIA2010, collocated
with COLING2010), Beijing, China., pages 1?8.
Tsuyoshi Okita, Jie Jiang, Rejwanul Haque, Hala Al-
Maghout, Jinhua Du, Sudip Kumar Naskar, and
Andy Way. 2010b. MaTrEx: the DCU MT System
for NTCIR-8. In Proceedings of the MII Test Col-
lection for IR Systems-8 Meeting (NTCIR-8), pages
377?383.
Tsuyoshi Okita, Raphae?l Rubino, and Josef van Gen-
abith. 2012a. Sentence-level quality estima-
tion for mt system combination. In Proceedings
of ML4HMT Workshop (collocated with COLING
2012), pages 55?64.
Tsuyoshi Okita, Antonio Toral, and Josef van Gen-
abith. 2012b. Topic modeling-based domain adap-
tation for system combination. In Proceedings
of ML4HMT Workshop (collocated with COLING
2012), pages 45?54.
Tsuyoshi Okita. 2012. Neural Probabilistic Language
Model for System Combination. In Proceedings
of ML4HMT Workshop (collocated with COLING
2012), pages 65?76.
Tsuyoshi Okita. 2013a. Joint space neural probabilis-
tic language model for statistical machine transla-
tion. Technical Report at arXiv, 1301(3614).
Tsuyoshi Okita. 2013b. Local graph matching with
active learning for recognizing inference in text at
ntcir-10. NTCIR 10 Conference, pages 499?506.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics, pages 312?319.
183
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space lan-
guage models on a gpu for statistical machine trans-
lation. In NAACL-HLT workshop on the Future of
Language Modeling for HLT, pages 11?19.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Paul Smolensky. 1986. Chapter 6: Information pro-
cessing in dynamical systems: Foundations of har-
mony theory. In Rumelhart, David E.; McLel-
land, James L. Parallel Distributed Processing:
Explorations in the Microstructure of Cognition,
1:194281.
Lucia Specia, D. Raj, and Marco Turchi. 2010. Ma-
chine translation evaluation versus quality estima-
tion. Machine Translation, Springer, 24(1):39?50.
Andreas Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, pages 901?904.
Graham Taylor and Geoffrey Hinton. 2009. Factored
conditional restricted boltzmann machines for mod-
eling motion style. In Proceedings of the 26th Inter-
national Conference on Machine Learning (ICML),
pages 1025?1032.
Yee Whye Teh. 2006. A hierarchical bayesian lan-
guage model based on pitman-yor processes. In
Proceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-06),
Prague, Czech Republic, pages 985?992.
Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189208.
184
