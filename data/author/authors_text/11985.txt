Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 262?270,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Compiling a Massive, Multilingual Dictionary via Probabilistic Inference
Mausam Stephen Soderland Oren Etzioni
Daniel S. Weld Michael Skinner* Jeff Bilmes
University of Washington, Seattle *Google, Seattle
{mausam,soderlan,etzioni,weld,bilmes}@cs.washington.edu mskinner@google.com
Abstract
Can we automatically compose a large set
of Wiktionaries and translation dictionar-
ies to yield a massive, multilingual dic-
tionary whose coverage is substantially
greater than that of any of its constituent
dictionaries?
The composition of multiple translation
dictionaries leads to a transitive inference
problem: if word A translates to word
B which in turn translates to word C,
what is the probability that C is a trans-
lation of A? The paper introduces a
novel algorithm that solves this problem
for 10,000,000 words in more than 1,000
languages. The algorithm yields PANDIC-
TIONARY, a novel multilingual dictionary.
PANDICTIONARY contains more than four
times as many translations than in the
largest Wiktionary at precision 0.90 and
over 200,000,000 pairwise translations in
over 200,000 language pairs at precision
0.8.
1 Introduction and Motivation
In the era of globalization, inter-lingual com-
munication is becoming increasingly important.
Although nearly 7,000 languages are in use to-
day (Gordon, 2005), most language resources are
mono-lingual, or bi-lingual.1 This paper investi-
gates whether Wiktionaries and other translation
dictionaries available over the Web can be auto-
matically composed to yield a massive, multilin-
gual dictionary with superior coverage at compa-
rable precision.
We describe the automatic construction of a
massive multilingual translation dictionary, called
1The English Wiktionary, a lexical resource developed by
volunteers over the Internet is one notable exception that con-
tains translations of English words in about 500 languages.
Figure 1: A fragment of the translation graph for two senses
of the English word ?spring?. Edges labeled ?1? and ?3? are
for spring in the sense of a season, and ?2? and ?4? are for
the flexible coil sense. The graph shows translation entries
from an English dictionary merged with ones from a French
dictionary.
PANDICTIONARY, that could serve as a resource
for translation systems operating over a very
broad set of language pairs. The most immedi-
ate application of PANDICTIONARY is to lexical
translation?the translation of individual words or
simple phrases (e.g., ?sweet potato?). Because
lexical translation does not require aligned cor-
pora as input, it is feasible for a much broader
set of languages than statistical Machine Transla-
tion (SMT). Of course, lexical translation cannot
replace SMT, but it is useful for several applica-
tions including translating search-engine queries,
library classifications, meta-data tags,2 and recent
applications like cross-lingual image search (Et-
zioni et al, 2007), and enhancing multi-lingual
Wikipedias (Adar et al, 2009). Furthermore,
lexical translation is a valuable component in
knowledge-based Machine Translation systems,
e.g., (Bond et al, 2005; Carbonell et al, 2006).
PANDICTIONARY currently contains over 200
million pairwise translations in over 200,000 lan-
guage pairs at precision 0.8. It is constructed from
information harvested from 631 online dictionar-
ies and Wiktionaries. This necessitates match-
2Meta-data tags appear in community Web sites such as
flickr.com and del.icio.us.
262
ing word senses across multiple, independently-
authored dictionaries. Because of the millions of
translations in the dictionaries, a feasible solution
to this sense matching problem has to be scalable;
because sense matches are imperfect and uncer-
tain, the solution has to be probabilistic.
The core contribution of this paper is a princi-
pled method for probabilistic sense matching to in-
fer lexical translations between two languages that
do not share a translation dictionary. For exam-
ple, our algorithm can conclude that Basque word
?udaherri? is a translation of Maori word ?koanga?
in Figure 1. Our contributions are as follows:
1. We describe the design and construction of
PANDICTIONARY?a novel lexical resource
that spans over 200 million pairwise transla-
tions in over 200,000 language pairs at 0.8
precision, a four-fold increase when com-
pared to the union of its input translation dic-
tionaries.
2. We introduce SenseUniformPaths, a scal-
able probabilistic method, based on graph
sampling, for inferring lexical translations,
which finds 3.5 times more inferred transla-
tions at precison 0.9 than the previous best
method.
3. We experimentally contrast PANDIC-
TIONARY with the English Wiktionary and
show that PANDICTIONARY is from 4.5 to
24 times larger depending on the desired
precision.
The remainder of this paper is organized as fol-
lows. Section 2 describes our earlier work on
sense matching (Etzioni et al, 2007). Section 3
describes how the PANDICTIONARY builds on and
improves on their approach. Section 4 reports on
our experimental results. Section 5 considers re-
lated work on lexical translation. The paper con-
cludes in Section 6 with directions for future work.
2 Building a Translation Graph
In previous work (Etzioni et al, 2007) we intro-
duced an approach to sense matching that is based
on translation graphs (see Figure 1 for an exam-
ple). Each vertex v ? V in the graph is an or-
dered pair (w, l) where w is a word in a language
l. Undirected edges in the graph denote transla-
tions between words: an edge e ? E between (w1,
l1) and (w2, l2) represents the belief that w1 and
w2 share at least one word sense.
Construction: The Web hosts a large num-
ber of bilingual dictionaries in different languages
and several Wiktionaries. Bilingual dictionaries
translate words from one language to another, of-
ten without distinguishing the intended sense. For
example, an Indonesian-English dictionary gives
?light? as a translation of the Indonesian word ?en-
teng?, but does not indicate whether this means il-
lumination, light weight, light color, or the action
of lighting fire.
The Wiktionaries (wiktionary.org) are sense-
distinguished, multilingual dictionaries created by
volunteers collaborating over the Web. A transla-
tion graph is constructed by locating these dictio-
naries, parsing them into a common XML format,
and adding the nodes and edges to the graph.
Figure 1 shows a fragment of a translation
graph, which was constructed from two sets of
translations for the word ?spring? from an English
Wiktionary, and two corresponding entries from
a French Wiktionary for ?printemps? (spring sea-
son) and ?ressort? (flexible spring). Translations of
the season ?spring? have edges labeled with sense
ID=1, the flexible coil sense has ID=2, translations
of ?printemps? have ID=3, and so forth.3
For clarity, we show only a few of the actual
vertices and edges; e.g., the figure doesn?t show
the edge (ID=1) between ?udaherri? and ?primav-
era?.
Inference: In our previous system we had
a simple inference procedure over translation
graphs, called TRANSGRAPH, to find translations
beyond those provided by any source dictionary.
TRANSGRAPH searched for paths in the graph be-
tween two vertices and estimated the probability
that the path maintains the same word sense along
all edges in the path, even when the edges come
from different dictionaries. For example, there are
several paths between ?udaherri? and ?koanga? in
Figure 1, but all shift from sense ID 1 to 3. The
probability that the two words are translations is
equivalent to the probability that IDs 1 and 3 rep-
resent the same sense.
TRANSGRAPH used two formulae to estimate
these probabilities. One formula estimates the
probability that two multi-lingual dictionary en-
tries represent the same word sense, based on the
proportion of overlapping translations for the two
entries. For example, most of the translations of
3Sense-distinguished multi-lingual entries give rise to
cliques all of which share a common sense ID.
263
French ?printemps? are also translations of the sea-
son sense of ?spring?. A second formula is based
on triangles in the graph (useful for bilingual dic-
tionaries): a clique of 3 nodes with an edge be-
tween each pair of nodes. In such cases, there is
a high probability that all 3 nodes share a word
sense.
Critique: While TRANSGRAPH was the first
to present a scalable inference method for lexical
translation, it suffers from several drawbacks. Its
formulae operate only on local information: pairs
of senses that are adjacent in the graph or triangles.
It does not incorporate evidence from longer paths
when an explicit triangle is not present. Moreover,
the probabilities from different paths are com-
bined conservatively (either taking the max over
all paths, or using ?noisy or? on paths that are
completely disjoint, except end points), thus lead-
ing to suboptimal precision/recall.
In response to this critique, the next section
presents an inference algorithm, called SenseUni-
formPaths (SP), with substantially improved recall
at equivalent precision.
3 Translation Inference Algorithms
In essence, inference over a translation graph
amounts to transitive sense matching: if word A
translates to word B, which translates in turn to
word C, what is the probability that C is a trans-
lation of A? If B is polysemous then C may not
share a sense with A. For example, in Figure 2(a)
if A is the French word ?ressort? (the flexible-
coil sense of spring) and B is the English word
?spring?, then Slovenian word ?vzmet? may or may
not be a correct translation of ?ressort? depending
on whether the edge (B,C) denotes the flexible-
coil sense of spring, the season sense, or another
sense. Indeed, given only the knowledge of the
path A ? B ? C we cannot claim anything with
certainty regarding A to C.
However, if A, B, and C are on a circuit that
starts at A, passes through B and C and re-
turns to A, there is a high probability that all
nodes on that circuit share a common word sense,
given certain restrictions that we enumerate later.
Where TRANSGRAPH used evidence from circuits
of length 3, we extend this to paths of arbitrary
lengths.
To see how this works, let us begin with the sim-
plest circuit, a triangle of three nodes as shown in
Figure 2(b). We can be quite certain that ?vzmet?
shares the sense of coil with both ?spring? and
?ressort?. Our reasoning is as follows: even
though both ?ressort? and ?spring? are polysemous
they share only one sense. For a triangle to form
we have two choices ? (1) either ?vzmet? means
spring coil, or (2) ?vzmet? means both the spring
season and jurisdiction, but not spring coil. The
latter is possible but such a coincidence is very un-
likely, which is why a triangle is strong evidence
for the three words to share a sense.
As an example of longer paths, our inference
algorithms can conclude that in Figure 2(c), both
?molla? and ?vzmet? have the sense coil, even
though no explicit triangle is present. To show
this, let us define a translation circuit as follows:
Definition 1 A translation circuit from v?1 with
sense s? is a cycle that starts and ends at v?1 with
no repeated vertices (other than v?1 at end points).
Moreover, the path includes an edge between v?1
and another vertex v?2 that also has sense s
?.
All vertices on a translation circuit are mutual
translations with high probability, as in Figure
2(c). The edge from ?spring? indicates that ?vzmet?
means either coil or season, while the edge from
?ressort? indicates that ?molla? means either coil
or jurisdiction. The edge from ?vzmet? to ?molla?
indicates that they share a sense, which will hap-
pen if all nodes share the sense season or if either
?vzmet? has the unlikely combination of coil and
jurisdiction (or ?molla? has coil and season).
We also develop a mathematical model of
sense-assignment to words that lets us formally
prove these insights. For more details on the the-
ory please refer to our extended version. This pa-
per reports on our novel algorithm and experimen-
tal results.
These insights suggest a basic version of our al-
gorithm: ?given two vertices, v?1 and v
?
2 , that share
a sense (say s?) compute all translation circuits
from v?1 in the sense s
?; mark all vertices in the
circuits as translations of the sense s??.
To implement this algorithm we need to decide
whether a vertex lies on a translation circuit, which
is trickier than it seems. Notice that knowing
that v is connected independently to v?1 and v
?
2
doesn?t imply that there exists a translation circuit
through v, because both paths may go through a
common node, thus violating of the definition of
translation circuit. For example, in Figure 2(d) the
Catalan word ?ploma? has paths to both spring and
ressort, but there is no translation circuit through
264
spring
English
ressort
French
vzmet
Slovenian
spring
English
ressort
French
vzmet
Slovenian
spring
English
vzmet
Slovenian
ressort
French
molla
Italian
spring
English
ressort
French
ploma
Catalan
Feder
German
???? 
Russian
spring
English
ressort
French
fj?der
Swedish
penna
Italian
Feder
German
(a)                         (b)                                   (c)                                (d)                     (e)
season
coil
jurisdiction
coil
s* s*
s* s*
s*
? ? ?
? ?
feather
coil
?
?
Figure 2: Snippets of translation graphs illustrating various inference scenarios. The nodes in question mark represent the
nodes in focus for each illustration. For all cases we are trying to infer translations of the flexible coil sense of spring.
it. Hence, it will not be considered a transla-
tion. This example also illustrates potential errors
avoided by our algorithm ? here, German word
?Feder? mean feather and spring coil, but ?ploma?
means feather and not the coil.
An exhaustive search to find translation circuits
would be too slow, so we approximate the solution
by a random walk scheme. We start the random
walk from v?1 (or v
?
2) and choose random edges
without repeating any vertices in the current path.
At each step we check if the current node has an
edge to v?2 (or v
?
1). If it does, then all the ver-
tices in the current path form a translation circuit
and, thus, are valid translations. We repeat this
random walk many times and keep marking the
nodes. In our experiments for each inference task
we performed a total of 2,000 random walks (NR
in pseudo-code) of max circuit length 7. We chose
these parameters based on a development set of 50
inference tasks.
Our first experiments with this basic algorithm
resulted in a much higher recall than TRANS-
GRAPH, albeit, at a significantly lower precision.
A closer examination of the results revealed two
sources of error ? (1) errors in source dictionary
data, and (2) correlated sense shifts in translation
circuits. Below we add two new features to our
algorithm to deal with each of these error sources,
respectively.
3.1 Errors in Source Dictionaries
In practice, source dictionaries contain mistakes
and errors occur in processing the dictionaries to
create the translation graph. Thus, existence of a
single translation circuit is only limited evidence
for a vertex as a translation. We wish to exploit
the insight that more translation circuits constitute
stronger evidence. However, the different circuits
may share some edges, and thus the evidence can-
not be simply the number of translation circuits.
We model the errors in dictionaries by assigning
a probability less than 1.0 to each edge4 (pe in the
4In our experiments we used a flat value of 0.6, chosen by
pseudo-code). We assume that the probability of
an edge being erroneous is independent of the rest
of the graph. Thus, a translation graph with pos-
sible data errors converts into a distribution over
accurate translation graphs.
Under this distribution, we can use the proba-
bility of existence of a translation circuit through a
vertex as the probability that the vertex is a trans-
lation. This value captures our insights, since a
larger number of translation circuits gives a higher
probability value.
We sample different graph topologies from our
given distribution. Some translation circuits will
exist in some of the sampled graphs, but not in
others. This, in turn, means that a given vertex v
will only be on a circuit for a fraction of the sam-
pled graphs. We take the proportion of samples in
which v is on a circuit to be the probability that v
is in the translation set. We refer to this algorithm
as Unpruned SenseUniformPaths (uSP).
3.2 Avoiding Correlated Sense-shifts
The second source of errors are circuits that in-
clude a pair of nodes sharing the same polysemy,
i.e., having the same pair of senses. A circuit
might maintain sense s? until it reaches a node that
has both s? and a distinct si. The next edge may
lead to a node with si, but not s?, causing an ex-
traction error. The path later shifts back to sense
s? at a second node that also has s? and si. An ex-
ample for this is illustrated in Figure 2(e), where
both the German and Swedish words mean feather
and spring coil. Here, Italian ?penna? means only
the feather and not the coil.
Two nodes that share the same two senses oc-
cur frequently in practice. For example, many
languages use the same word for ?heart? (the or-
gan) and center; similarly, it is common for lan-
guages to use the same word for ?silver?, the metal
and the color. These correlations stem from com-
parameter tuning on a development set of 50 inference tasks.
In future we can use different values for different dictionaries
based on our confidence in their accuracy.
265
Figure 3: The set {B, C} has a shared ambiguity - each
node has both sense 1 (from the lower clique) and sense 2
(from the upper clique). A circuit that contains two nodes
from the same ambiguity set with an intervening node not in
that set is likely to create translation errors.
mon metaphor and the shared evolutionary roots
of some languages.
We are able to avoid circuits with this type of
correlated sense-shift by automatically identifying
ambiguity sets, sets of nodes known to share mul-
tiple senses. For instance, in Figure 2(e) ?Feder?
and ?fj?der? form an ambiguity set (shown within
dashed lines), as they both mean feather and coil.
Definition 2 An ambiguity set A is a set of ver-
tices that all share the same two senses. I.e.,
?s1, s2, with s1 6= s2 s.t. ?v ? A, sense(v, s1)?
sense(v, s2), where sense(v, s) denotes that v has
sense s.
To increase the precision of our algorithm we
prune the circuits that contain two nodes in the
same ambiguity set and also have one or more in-
tervening nodes that are not in the ambiguity set.
There is a strong likelihood that the intervening
nodes will represent a translation error.
Ambiguity sets can be detected from the graph
topology as follows. Each clique in the graph rep-
resents a set of vertices that share a common word
sense. When two cliques intersect in two or more
vertices, the intersecting vertices share the word
sense of both cliques. This may either mean that
both cliques represent the same word sense, or that
the intersecting vertices form an ambiguity set. A
large overlap between two cliques makes the for-
mer case more likely; a small overlap makes it
more likely that we have found an ambiguity set.
Figure 3 illustrates one such computation.
All nodes of the clique V1, V2, A,B,C,D share
a word sense, and all nodes of the clique
B,C,E, F,G,H also share a word sense. The set
{B,C} has nodes that have both senses, forming
an ambiguity set. We denote the set of ambiguity
sets by A in the pseudo-code.
Having identified these ambiguity sets, we mod-
ify our random walk scheme by keeping track of
whether we are entering or leaving an ambiguity
set. We prune away all paths that enter the same
ambiguity set twice. We name the resulting algo-
rithm SenseUniformPaths (SP), summarized at a
high level in Algorithm 1.
Comparing Inference Algorithms Our evalua-
tion demonstrated that SP outperforms uSP. Both
these algorithms have significantly higher recall
than TRANSGRAPH algorithm. The detailed re-
sults are presented in Section 4.2. We choose SP
as our inference algorithm for all further research,
in particular to create PANDICTIONARY.
3.3 Compiling PanDictionary
Our goal is to automatically compile PANDIC-
TIONARY, a sense-distinguished lexical transla-
tion resource, where each entry is a distinct word
sense. Associated with each word sense is a list of
translations in multiple languages.
We use Wiktionary senses as the base senses
for PANDICTIONARY. Recall that SP requires two
nodes (v?1 and v
?
2) for inference. We use the Wik-
tionary source word as v?1 and automatically pick
the second word from the set of Wiktionary trans-
lations of that sense by choosing a word that is
well connected, and, which does not appear in
other senses of v?1 (i.e., is expected to share only
one sense with v?1).
We first run SenseUniformPaths to expand the
approximately 50,000 senses in the English Wik-
tionary. We further expand any senses from the
other Wiktionaries that are not yet covered by
PANDICTIONARY, and add these to PANDIC-
TIONARY. This results in the creation of the
world?s largest multilingual, sense-distinguished
translation resource, PANDICTIONARY. It con-
tains a little over 80,000 senses. Its construction
takes about three weeks on a 3.4 GHz processor
with a 2 GB memory.
Algorithm 1 S.P.(G, v?1, v
?
2,A)
1: parameters NG: no. of graph samples, NR: no. of ran-
dom walks, pe: prob. of sampling an edge
2: createNG versions ofG by sampling each edge indepen-
dently with probability pe
3: for all i = 1..NG do
4: for all vertices v : rp[v][i] = 0
5: perform NR random walks starting at v?1 (or v
?
2 ) and
pruning any walk that enters (or exits) an ambiguity
set in A twice. All walks that connect to v?2 (or v
?
1 )
form a translation circuit.
6: for all vertices v do
7: if(v is on a translation circuit) rp[v][i] = 1
8: return
?
i
rp[v][i]
NG
as the prob. that v is a translation
266
4 Empirical Evaluation
In our experiments we investigate three key ques-
tions: (1) which of the three algorithms (TG, uSP
and SP) is superior for translation inference (Sec-
tion 4.2)? (2) how does the coverage of PANDIC-
TIONARY compare with the largest existing mul-
tilingual dictionary, the English Wiktionary (Sec-
tion 4.3)? (3) what is the benefit of inference over
the mere aggregation of 631 dictionaries (Section
4.4)? Additionally, we evaluate the inference algo-
rithm on two other dimensions ? variation with the
degree of polysemy of source word, and variation
with original size of the seed translation set.
4.1 Experimental Methodology
Ideally, we would like to evaluate a random sam-
ple of the more than 1,000 languages represented
in PANDICTIONARY.5 However, a high-quality
evaluation of translation between two languages
requires a person who is fluent in both languages.
Such people are hard to find and may not even
exist for many language pairs (e.g., Basque and
Maori). Thus, our evaluation was guided by our
ability to recruit volunteer evaluators. Since we
are based in an English speaking country we were
able to recruit local volunteers who are fluent in
a range of languages and language families, and
who are also bilingual in English.6
The experiments in Sections 4.2 and 4.3 test
whether translations in a PANDICTIONARY have
accurate word senses. We provided our evalua-
tors with a random sample of translations into their
native language. For each translation we showed
the English source word and gloss of the intended
sense. For example, a Dutch evaluator was shown
the sense ?free (not imprisoned)? together with the
Dutch word ?loslopende?. The instructions were
to mark a word as correct if it could be used to ex-
press the intended sense in a sentence in their na-
tive language. For experiments in Section 4.4 we
tested precision of pairwise translations, by having
informants in several pairs of languages discuss
whether the words in their respective languages
can be used for the same sense.
We use the tags of correct or incorrect to com-
pute the precision: the percentage of correct trans-
5The distribution of words in PANDICTIONARY is highly
non-uniform ranging from 182,988 words in English to 6,154
words in Luxembourgish and 189 words in Tuvalu.
6The languages used was based on the availability of na-
tive speakers. This varied between the different experiments,
which were conducted at different times.
Figure 4: The SenseUniformPaths algorithm (SP) more
than doubles the number of correct translations at precision
0.95, compared to a baseline of translations that can be found
without inference.
lations divided by correct plus incorrect transla-
tions. We then order the translations by probabil-
ity and compute the precision at various probabil-
ity thresholds.
4.2 Comparing Inference Algorithms
Our first evaluation compares our SenseUniform-
Paths (SP) algorithm (before and after pruning)
with TRANSGRAPH on both precision and num-
ber of translations.
To carry out this comparison, we randomly sam-
pled 1,000 senses from English Wiktionary and
ran the three algorithms over them. We evalu-
ated the results on 7 languages ? Chinese, Danish,
German, Hindi, Japanese, Russian, and Turkish.
Each informant tagged 60 random translations in-
ferred by each algorithm, which resulted in 360-
400 tags per algorithm7. The precision over these
was taken as a surrogate for the precision across
all the senses.
We compare the number of translations for each
algorithm at comparable precisions. The baseline
is the set of translations (for these 1000 senses)
found in the source dictionaries without inference,
which has a precision 0.95 (as evaluated by our
informants).8
Our results are shown in Figure 4. At this high
precision, SP more than doubles the number of
baseline translations, finding 5 times as many in-
ferred translations (in black) as TG.
Indeed, both uSP and SP massively outperform
TG. SP is consistently better than uSP, since it
performs better for polysemous words, due to its
pruning based on ambiguity sets. We conclude
7Some translations were marked as ?Don?t know?.
8Our informants tended to underestimate precision, often
marking correct translations in minor senses of a word as in-
correct.
267
0.5
0.6
0.7
0.8
0.9
1
0.0 4.0 8.0 12.0 16.0
Pr
ec
is
io
n
Translations in Millions
PanDictionary
English Wiktionary
Figure 5: Precision vs. coverage curve for PANDIC-
TIONARY. It quadruples the size of the English Wiktionary at
precision 0.90, is more than 8 times larger at precision 0.85
and is almost 24 times the size at precision 0.7.
that SP is the best inference algorithm and employ
it for PANDICTIONARY construction.
4.3 Comparison with English Wiktionary
We now compare the coverage of PANDIC-
TIONARY with the English Wiktionary at varying
levels of precision. The English Wiktionary is the
largest Wiktionary with a total of 403,413 transla-
tions. It is also more reliable than some other Wik-
tionaries in making word sense distinctions. In this
study we use only the subset of PANDICTIONARY
that was computed starting from the English Wik-
tionary senses. Thus, this subsection under-reports
PANDICTIONARY?s coverage.
To evaluate a huge resource such as PANDIC-
TIONARY we recruited native speakers of 14 lan-
guages ? Arabic, Bulgarian, Danish, Dutch, Ger-
man, Hebrew, Hindi, Indonesian, Japanese, Ko-
rean, Spanish, Turkish, Urdu, and Vietnamese. We
randomly sampled 200 translations per language,
which resulted in about 2,500 tags. Figure 5
shows the total number of translations in PANDIC-
TIONARY in senses from the English Wiktionary.
At precision 0.90, PANDICTIONARY has 1.8 mil-
lion translations, 4.5 times as many as the English
Wiktionary.
We also compare the coverage of PANDIC-
TIONARY with that of the English Wiktionary in
terms of languages covered. Table 1 reports, for
each resource, the number of languages that have
a minimum number of distinct words in the re-
source. PANDICTIONARY has 1.4 times as many
languages with at least 1,000 translations at pre-
cision 0.90 and more than twice at precision 0.7.
These observations reaffirm our faith in the pan-
lingual nature of the resource.
PANDICTIONARY?s ability to expand the lists
of translations provided by the EnglishWiktionary
is most pronounced for senses with a small num-
0.75
0.8
0.85
0.9
0.95
1 2 3,4 >4
Pre
cis
ion
Avg precision 0.90
Avg precision 0.85
Polysemy of the English source word
3-4
Figure 6: Variation of precision with the degree of poly-
semy of the source English word. The precision decreases as
polysemy increases, still maintaining reasonably high values.
ber of translations. For example, at precision 0.90,
senses that originally had 3 to 6 translations are in-
creased 5.3 times in size. The increase is 2.2 times
when the original sense size is greater than 20.
For closer analysis we divided the English
source words (v?1) into different bins based on the
number of senses that English Wiktionary lists for
them. Figure 6 plots the variation of precision with
this degree of polysemy. We find that translation
quality decreases as degree of polysemy increases,
but this decline is gradual, which suggests that SP
algorithm is able to hold its ground well in difficult
inference tasks.
4.4 Comparison with All Source Dictionaries
We have shown that PANDICTIONARY has much
broader coverage than the English Wiktionary, but
how much of this increase is due to the inference
algorithm versus the mere aggregation of hundreds
of translation dictionaries in PANDICTIONARY?
Since most bilingual dictionaries are not sense-
distinguished, we ignore the word senses and
count the number of distinct (word1, word2) trans-
lation pairs.
We evaluated the precision of word-word trans-
lations by a collaborative tagging scheme, with
two native speakers of different languages, who
are both bi-lingual in English. For each sug-
gested translation they discussed the various
senses of words in their respective languages
and tag a translation correct if they found some
sense that is shared by both words. For this
study we tagged 7 language pairs: Hindi-Hebrew,
# languages with distinct words
? 1000 ? 100 ? 1
English Wiktionary 49 107 505
PanDictionary (0.90) 67 146 608
PanDictionary (0.85) 75 175 794
PanDictionary (0.70) 107 607 1066
Table 1: PANDICTIONARY covers substantially more lan-
guages than the English Wiktionary.
268
050
100
150
200
250
EW 631D PD(0.9) PD(0.85) PD(0.8)
Inferred transl. Direct transl.
Tra
nsl
ati
on
s(i
n m
illio
ns)
Figure 7: The number of distinct word-word translation
pairs from PANDICTIONARY is several times higher than the
number of translation pairs in the English Wiktionary (EW)
or in all 631 source dictionaries combined (631 D). A major-
ity of PANDICTIONARY translations are inferred by combin-
ing entries from multiple dictionaries.
Japanese-Russian, Chinese-Turkish, Japanese-
German, Chinese-Russian, Bengali-German, and
Hindi-Turkish.
Figure 7 compares the number of word-word
translation pairs in the English Wiktionary (EW),
in all 631 source dictionaries (631 D), and in PAN-
DICTIONARY at precisions 0.90, 0.85, and 0.80.
PANDICTIONARY increases the number of word-
word translations by 73% over the source dictio-
nary translations at precision 0.90 and increases it
by 2.7 times at precision 0.85. PANDICTIONARY
also adds value by identifying the word sense of
the translation, which is not given in most of the
source dictionaries.
5 Related Work
Because we are considering a relatively new prob-
lem (automatically building a panlingual transla-
tion resource) there is little work that is directly re-
lated to our own. The closest research is our previ-
ous work on TRANSGRAPH algorithm (Etzioni et
al., 2007). Our current algorithm outperforms the
previous state of the art by 3.5 times at precision
0.9 (see Figure 4). Moreover, we compile this in a
dictionary format, thus considerably reducing the
response time compared to TRANSGRAPH, which
performed inference at query time.
There has been considerable research on meth-
ods to acquire translation lexicons from either
MRDs (Neff and McCord, 1990; Helmreich et
al., 1993; Copestake et al, 1994) or from par-
allel text (Gale and Church, 1991; Fung, 1995;
Melamed, 1997; Franz et al, 2001), but this has
generally been limited to a small number of lan-
guages. Manually engineered dictionaries such as
EuroWordNet (Vossen, 1998) are also limited to
a relatively small set of languages. There is some
recent work on compiling dictionaries from mono-
lingual corpora, which may scale to several lan-
guage pairs in future (Haghighi et al, 2008).
Little work has been done in combining mul-
tiple dictionaries in a way that maintains word
senses across dictionaries. Gollins and Sanderson
(2001) explored using triangulation between alter-
nate pivot languages in cross-lingual information
retrieval. Their triangulation essentially mixes
together circuits for all word senses, hence, is un-
able to achieve high precision.
Dyvik?s ?semantic mirrors? uses translation
paths to tease apart distinct word senses from
inputs that are not sense-distinguished (Dyvik,
2004). However, its expensive processing and
reliance on parallel corpora would not scale to
large numbers of languages. Earlier (Knight and
Luk, 1994) discovered senses of Spanish words by
matching several English translations to a Word-
Net synset. This approach applies only to specific
kinds of bilingual dictionaries, and also requires a
taxonomy of synsets in the target language.
Random walks, graph sampling and Monte
Carlo simulations are popular in literature, though,
to our knowledge, none have applied these to our
specific problems (Henzinger et al, 1999; Andrieu
et al, 2003; Karger, 1999).
6 Conclusions
We have described the automatic construction of
a unique multilingual translation resource, called
PANDICTIONARY, by performing probabilistic in-
ference over the translation graph. Overall, the
construction process consists of large scale in-
formation extraction over the Web (parsing dic-
tionaries), combining it into a single resource (a
translation graph), and then performing automated
reasoning over the graph (SenseUniformPaths) to
yield a much more extensive and useful knowl-
edge base.
We have shown that PANDICTIONARY has
more coverage than any other existing bilingual
or multilingual dictionary. Even at the high preci-
sion of 0.90, PANDICTIONARY more than quadru-
ples the size of the English Wiktionary, the largest
available multilingual resource today.
We plan to make PANDICTIONARY available
to the research community, and also to the Wik-
tionary community in an effort to bolster their ef-
forts. PANDICTIONARY entries can suggest new
translations for volunteers to add to Wiktionary
entries, particularly if combined with an intelli-
gent editing tool (e.g., (Hoffmann et al, 2009)).
269
Acknowledgments
This research was supported by a gift from the
Utilika Foundation to the Turing Center at Uni-
versity of Washington. We acknowledge Paul
Beame, Nilesh Dalvi, Pedro Domingos, Rohit
Khandekar, Daniel Lowd, Parag, Jonathan Pool,
Hoifung Poon, Vibhor Rastogi, Gyanit Singh for
fruitful discussions and insightful comments on
the research. We thank the language experts who
donated their time and language expertise to eval-
uate our systems. We also thank the anynomous
reviewers of the previous drafts of this paper for
their valuable suggestions in improving the evalu-
ation and presentation.
References
E. Adar, M. Skinner, and D. Weld. 2009. Information
arbitrage in multi-lingual Wikipedia. In Procs. of
Web Search and Data Mining(WSDM 2009).
C. Andrieu, N. De Freitas, A. Doucet, and M. Jor-
dan. 2003. An Introduction to MCMC for Machine
Learning. Machine Learning, 50:5?43.
F. Bond, S. Oepen, M. Siegel, A. Copestake, and
D D. Flickinger. 2005. Open source machine trans-
lation with DELPH-IN. In Open-Source Machine
Translation Workshop at MT Summit X.
J. Carbonell, S. Klein, D. Miller, M. Steinbaum,
T. Grassiany, and J. Frey. 2006. Context-based ma-
chine translation. In AMTA.
A. Copestake, T. Briscoe, P. Vossen, A. Ageno,
I. Castellon, F. Ribas, G. Rigau, H. Rodriquez, and
A. Samiotou. 1994. Acquisition of lexical trans-
lation relations from MRDs. Machine Translation,
3(3?4):183?219.
H. Dyvik. 2004. Translation as semantic mirrors: from
parallel corpus to WordNet. Language and Comput-
ers, 49(1):311?326.
O. Etzioni, K. Reiter, S. Soderland, and M. Sammer.
2007. Lexical translation with application to image
search on the Web. In Machine Translation Summit
XI.
M. Franz, S. McCarly, and W. Zhu. 2001. English-
Chinese information retrieval at IBM. In Proceed-
ings of TREC 2001.
P. Fung. 1995. A pattern matching method for finding
noun and proper noun translations from noisy paral-
lel corpora. In Proceedings of ACL-1995.
W. Gale and K.W. Church. 1991. A Program for
Aligning Sentences in Bilingual Corpora. In Pro-
ceedings of ACL-1991.
T. Gollins and M. Sanderson. 2001. Improving cross
language retrieval with triangulated translation. In
SIGIR.
Raymond G. Gordon, Jr., editor. 2005. Ethnologue:
Languages of the World (Fifteenth Edition). SIL In-
ternational.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and
D. Klein. 2008. Learning bilingual lexicons from
monolingual corpora. In ACL.
S. Helmreich, L. Guthrie, and Y. Wilks. 1993. The
use of machine readable dictionaries in the Pangloss
project. In AAAI Spring Symposium on Building
Lexicons for Machine Translation.
Monika R. Henzinger, Allan Heydon, Michael Mitzen-
macher, and Marc Najork. 1999. Measuring index
quality using random walks on the web. In WWW.
R. Hoffmann, S. Amershi, K. Patel, F. Wu, J. Foga-
rty, and D. S. Weld. 2009. Amplifying commu-
nity content creation with mixed-initiative informa-
tion extraction. In ACM SIGCHI (CHI2009).
D. R. Karger. 1999. A randomized fully polynomial
approximation scheme for the all-terminal network
reliability problem. SIAM Journal of Computation,
29(2):492?514.
K. Knight and S. Luk. 1994. Building a large-scale
knowledge base for machine translation. In AAAI.
I.D. Melamed. 1997. A Word-to-Word Model of
Translational Equivalence. In Proceedings of ACL-
1997 and EACL-1997, pages 490?497.
M. Neff and M. McCord. 1990. Acquiring lexical data
from machine-readable dictionary resources for ma-
chine translation. In 3rd Intl Conference on Theoret-
ical and Methodological Issues in Machine Transla-
tion of Natural Language.
P. Vossen, editor. 1998. EuroWordNet: A multilingual
database with lexical semantic networds. Kluwer
Academic Publishers.
270
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 193?196,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
A Rose is a Roos is a Ruusu: Querying Translations for Web Image Search
Janara Christensen Mausam Oren Etzioni
Turing Center
Dept. of Computer Science and Engineering
University of Washington, Seattle, WA 98105 USA
{janara, mausam, etzioni}@cs.washington.edu
Abstract
We query Web Image search engines with
words (e.g., spring) but need images that
correspond to particular senses of the word
(e.g., flexible coil). Querying with poly-
semous words often yields unsatisfactory
results from engines such as Google Im-
ages. We build an image search engine,
IDIOM, which improves the quality of re-
turned images by focusing search on the
desired sense. Our algorithm, instead of
searching for the original query, searches
for multiple, automatically chosen trans-
lations of the sense in several languages.
Experimental results show that IDIOM out-
performs Google Images and other com-
peting algorithms returning 22% more rel-
evant images.
1 Introduction
One out of five Web searches is an image search
(Basu, 2009). A large subset of these searches
is subjective in nature, where the user is looking
for different images for a single concept (Linsley,
2009). However, it is a common user experience
that the images returned are not relevant to the in-
tended concept. Typical reasons include (1) exis-
tence of homographs (other words that share the
same spelling, possibly in another language), and
(2) polysemy, several meanings of the query word,
which get merged in the results.
For example, the English word ?spring? has sev-
eral senses ? (1) the season, (2) the water body, (3)
spring coil, and (4) to jump. Ten out of the first fif-
teen Google images for spring relate to the season
sense, three to water body, one to coil and none to
the jumping sense. Simple modifications to query
do not always work. Searching for spring water
results in many images of bottles of spring water
and searching for spring jump returns only three
images (out of fifteen) of someone jumping.
Polysemous words are common in English. It
is estimated that average polysemy of English is
more than 2 and average polysemy of common
English words is much higher (around 4). Thus,
it is not surprising that polysemy presents a signif-
icant limitation in the context of Web Search. This
is especially pronounced for image search where
query modification by adding related words may
not help, since, even though the new words might
be present on the page, they may not be all associ-
ated with an image.
Recently Etzioni et al (2007) introduced PAN-
IMAGES, a novel approach to image search, which
presents the user with a set of translations. E.g., it
returns 38 translations for the coil sense of spring.
The user can query one or more translations to get
the relevant images. However, this method puts
the onus of choosing a translation on the user. A
typical user is unaware of most properties of lan-
guages and has no idea whether a translation will
make a good query. This results in an added bur-
den on the user to try different translations before
finding the one that returns the relevant images.
Our novel system, IDIOM, removes this addi-
tional burden. Given a desired sense it automati-
cally picks the good translations, searches for as-
sociated images and presents the final images to
the user. For example, it automatically queries the
French ressort when looking for images of spring
coil. We make the following contributions:
? We automatically learn a predictor for "good"
translations to query given a desired sense. A
good translation is one that is monosemous
and is in a major language, i.e., is expected to
yield a large number of images.
? Given a sense we run our predictor on all its
translations to shortlist a set of three transla-
tions to query.
? We evaluate our predictor by comparing the
images that its shortlists return against the
193
images that several competing methods re-
turn. Our evaluation demonstrates that ID-
IOM returns at least one good image for 35%
more senses (than closest competitor) and
overall returns 22% better images.
2 Background
IDIOM makes heavy use of a sense disambiguated,
vastly multilingual dictionary called PANDIC-
TIONARY (Mausam et al, 2009). PANDIC-
TIONARY is automatically constructed by prob-
abilistic inference over a graph of translations,
which is compiled from a large number of multi-
lingual and bilingual dictionaries. For each sense
PANDICTIONARY provides us with a set of trans-
lations in several languages. Since it is gener-
ated by inference, some of the asserted transla-
tions may be incorrect ? it additionally associates
a probability score with each translation. For
our work we choose a probability threshold such
that the overall precision of the dictionary is 0.9
(evaluated based on a random sample). PANDIC-
TIONARY has about 80,000 senses and about 1.8
million translations at precision 0.9.
We use Google Image Search as our underlying
image search engine, but our methods are indepen-
dent of the underlying search engine used.
3 The IDIOM Algorithm
At the highest level IDIOM operates in three main
steps: (1) Given a new query q it looks up its vari-
ous senses in PANDICTIONARY. It displays these
senses and asks the user to select the intended
sense, s
q
. (2) It runs Algorithm 1 to shortlist three
translations of s
q
that are expected to return high
quality images. (3) It queries Google Images us-
ing the three shortlisted translations and displays
the images. In this fashion IDIOM searches for
images that are relevant to the intended concept
as opposed to using a possibly ambiguous query.
The key technical component is the second step
? shortlisting the translations. We first use PAN-
DICTIONARY to acquire a set of high probability
translations of s
q
. We run each of these transla-
tions through a learned classifier, which predicts
whether it will make a good query, i.e., whether
we can expect images relevant to this sense if
queried using this translation. The classifier ad-
ditionally outputs a confidence score, which we
use to rank the various translations. We pick the
top three translations, as long as they are above a
minimum confidence score, and return those as the
shortlisted queries. Algorithm 1 describes this as
a pseudo-code.
Algorithm 1 findGoodTranslationsToQuery(s
q
)
1: translations = translations of s
q
in PANDICTIONARY
2: for all w ? translations do
3: pd = getPanDictionaryFeatures(w, s
q
)
4: g = getGoogleFeatures(w, s
q
)
5: conf[w] = confidence in Learner.classify(pd, g)
6: sort all words w in decreasing order of conf scores
7: return top three w from the sorted list
3.1 Features for Classifier
What makes a translation w good to query? A
desired translation is one that (1) is in a high-
coverage language, so that the number of images
returned is large, (2) monosemously expresses the
intended sense s
q
, or at least has this sense as
its dominant sense, and (3) does not have homo-
graphs in other languages. Such a translation is
expected to yield images relevant to only the in-
tended sense. We construct several features that
provide us evidence for these desired characteris-
tics. Our features are automatically extracted from
PANDICTIONARY and Google.
For the first criterion we restrict the transla-
tions to a set of high-coverage languages includ-
ing English, French, German, Spanish, Chinese,
Japanese, Arabic, Russian, Korean, Italian, and
Portuguese. Additionally, we include the lan-
guage as well as number of documents returned by
Google search of w as features for the classifier.
To detect if w is monosemous we add a feature
reflecting the degree of polysemy of w: the num-
ber of PANDICTIONARY senses thatw belongs to.
The higher this number the more polysemous w
is expected to be. We also include the number of
languages that have w in their vocabulary, thus,
adding a feature for the degree of homography.
PANDICTIONARY is arranged such that each
sense has an English source word. If the source
word is part of many senses but s
q
is much more
popular than others or s
q
is ordered before the
other senses then we can expect s
q
to be the dom-
inant sense for this word. We include features like
size of the sense and order of the sense.
Part of speech of s
q
is another feature. Finally
we also add the probability score that w is a trans-
lation of s
q
in our feature set.
3.2 Training the Classifier
To train our classifier we used Weka (Witten and
Frank, 2005) on a hand labeled dataset of 767 ran-
194
0 100 200 300 4000.
00
0.10
0.20
Number of Good Images Returned
Prec
ision IDIOMSWSW+GRSW+R
IDIOM SW SW+G SW+R R
Perc
enta
ge C
orrec
t
0
20
40
60
IDIOM SW SW+G SW+R R
Perc
enta
ge C
orrec
t
0
20
40
60
Figure 1: (a): Precision of images vs. the number of relevant images returned. IDIOM covers the maximum area. (b,c) The
percentage of senses for which at least one relevant result was returned, for (b) all senses and (c) for minor senses of the queries.
domly chosen word sense pairs (e.g., pair of ?pri-
mavera,? and ?the season spring?). We labeled a
pair as positive if googling the word returns at least
one good image for the sense in the top three. We
compared performance among a number of ma-
chine learning algorithms and found that Random
Forests (Breiman, 2001) performed the best over-
all with 69% classification accuracy using ten fold
cross validation versus 63% for Naive Bayes and
62% for SVMs. This high performance of Ran-
dom Forests mirrors other past experiments (Caru-
ana and Niculescu-Mizil, 2006).
Because of the ensemble nature of Random
Forests it is difficult to inspect the learned clas-
sifier for analysis. Still, anecdotal evidence sug-
gests that the classifier is able to learn an effective
model of good translations. We observe that it fa-
vors English whenever the English word is part of
one or few senses ? it picks out auction when the
query is ?sale? in the sense of ?act of putting up
for auction to highest bidder". In cases where En-
glish is more ambiguous it chooses a relatively less
ambiguous word in another language. It chooses
the French word ressort for finding ?spring? in the
sense of coil. For the query ?gift? we notice that it
does not choose the original query. This matches
our intuition, since gift has many homographs ?
the German word ?Gift? means poison or venom.
4 Experiments
Can querying translations instead of the original
query improve the quality of image search? If so,
then how much does our classifier help compared
to querying random translations? We also analyze
our results and study the variation of image qual-
ity along various dimensions, like part of speech,
abstractness/concreteness of the sense, and ambi-
guity of the original query.
As a comparison, we are interested in how ID-
IOM performs in relation to other methods for
querying Google Images. We compare IDIOM to
several methods. (1) Source Word (SW): Querying
with only the source word. This comparison func-
tions as our baseline. (2) Source Word + Gloss
(SW+G): Querying with the source word and the
gloss for the sense
1
. This method is one way to fo-
cus the source word towards the desired sense. (3)
Source Word + Random (SW+R): Querying with
three pairs of source word and a random transla-
tion. This is another natural way to extend the
baseline for the intended sense. (4) Random (R):
Querying with three random translations. This
tests the extent to which our classifier improves
our results compared to randomly choosing trans-
lations shown to the user in PANIMAGES.
We randomly select fifty English queries from
PANDICTIONARY and look up all senses contain-
ing these in PANDICTIONARY, resulting in a total
of 134 senses. These queries include short word
sequences (e.g., ?open sea?), mildly polysemous
queries like ?pan? (means Greek God and cooking
vessel) and highly polysemous ones like ?light?.
For each sense of each word, we query Google
Images with the query terms suggested by each
method and evaluate the top fifteen results. For
methods in which we have three queries, we eval-
uate the top five results for each query. We evalu-
ate a total of fifteen results because Google Images
fits fifteen images on each page for our screen size.
Figure 1(a) compares the precision of the five
methods with the number of good images re-
turned. We vary the number of images in con-
sideration from 1 to 15 to generate various points
in the graph. IDIOM outperforms the others by
wide margins overall producing a larger number of
good images and at higher precision. Surprisingly,
the closest competitor is the baseline method as
opposed to other methods that try to focus the
search towards the intended sense. This is prob-
ably because the additional words in the query (ei-
ther from gloss or a random translation) confuse
Google Images rather than focusing the search.
IDIOM covers 41% more area than SW. Overall
1
PANDICTIONARY provides a gloss (short explanation)
for each sense. E.g., a gloss for ?hero? is ?role model.?
195
1 sense 2 or 3 senses >3 senses
Perc
enta
ge C
orrec
t
02
04
06
08
0 IDIOMSWSW+GSW+RR
Noun Verb AdjectiveP
erce
ntag
e Co
rrect
0
20
40
60
80 IDIOMSWSW+GSW+RR
Concrete Abstract
Perc
enta
ge C
orrec
t
0
20
40
60
80 IDIOMSWSW+GSW+RR
Figure 2: The percentage of senses for which at least one relevant result was returned varied along several dimensions: (a)
polysemy of original query, and (b) part of speech of the sense, (c) abstractness/concreteness of the sense.
IDIOM produces 22% better images compared to
SW (389 vs 318).
We also observe that random translations return
much worse images than IDIOM suggesting that a
classifier is essential for high quality images.
Figure 1(b) compares the percentage of senses
for which at least one good result was returned in
the fifteen. Here IDIOM performs the best at 51%.
Each other method performs at about 40%. The re-
sults are statistically highly significant (p < 0.01).
Figure 1(c) compares the performance just on
the subset of the non-dominant senses of the query
words. All methods perform worse than in Figure
1(b) but IDIOM outperforms the others.
We also analyze our results across several di-
mensions. Figure 2(a) compares the performance
as a function of polysemy of the original query. As
expected, the disparity in methods is much more
for high polysemy queries. Most methods perform
well for the easy case of unambiguous queries.
Figure 2(b) compares along the different parts
of speech. For nouns and verbs, IDIOM returns the
best results. For adjectives, IDIOM and SW per-
form the best. Overall, nouns are the easiest for
finding images and we did not find much differ-
ence between verbs and adjectives.
Finally, Figure 2(c) reports how the methods
perform on abstract versus concrete queries. We
define a sense as abstract if it does not have a nat-
ural physical manifestation. For example, we clas-
sify ?nest? (a bird built structure) as concrete, and
?confirm? (to strengthen) as abstract. IDIOM per-
forms better than the other methods, but the results
vary massively between the two categories.
Overall, we find that our new system consis-
tently produces better results across the several di-
mensions and various metrics.
5 Related Work and Conclusions
Related Work: The popular paradigm for image
search is keyword-based, but it suffers due to pol-
ysemy and homography. An alternative paradigm
is content based (Datta et al, 2008), which is very
slow and works on simpler images. The field
of cross-lingual information retrieval (Ballesteros
and Croft, 1996) often performs translation-based
search. Other than PANIMAGES (which we out-
perform), no one to our knowledge has used this
for image search.
Conclusions: The recent development of PAN-
DICTIONARY (Mausam et al, 2009), a sense-
distinguished, massively multilingual dictionary,
enables a novel image search engine called ID-
IOM. We show that querying unambiguous trans-
lations of a sense produces images for 35% more
concepts compared to querying just the English
source word. In the process we learn a classi-
fier that predicts whether a given translation is a
good query for the intended sense or not. We
plan to release an image search website based
on IDIOM. In the future we wish to incorporate
knowledge from WordNet and cross-lingual links
in Wikipedia to increase IDIOM?s coverage beyond
the senses from PANDICTIONARY.
References
L. Ballesteros and B. Croft. 1996. Dictionary methods for
cross-lingual information retrieval. In DEXA Conference
on Database and Expert Systems Applications.
Dev Basu. 2009. How To Leverage Rich Me-
dia SEO for Small Businesses. In Search En-
gine Journal. http://www.searchenginejournal.com/rich -
media-small-business-seo/9580.
L. Breiman. 2001. Random forests. Machine Learning,
45(1):5?32.
R. Caruana and A. Niculescu-Mizil. 2006. An empiri-
cal comparison of supervised learning algorithms. In
ICML?06, pages 161?168.
R. Datta, D. Joshi, J. Li, and J. Wang. 2008. Image retrieval:
Ideas, influences, and trends of the new age. ACM Com-
puting Surveys, 40(2):1?60.
O. Etzioni, K. Reiter, S. Soderland, and M. Sammer. 2007.
Lexical translation with application to image search on the
Web. In Machine Translation Summit XI.
Peter Linsley. 2009. Google Image Search. In SMX West.
Mausam, S. Soderland, O. Etzioni, D. Weld, M. Skinner, and
J. Bilmes. 2009. Compiling a massive, multilingual dic-
tionary via probabilistic inference. In ACL?09.
I. Witten and E. Frank. 2005. Data Mining: Practical Ma-
chine Learning Tools and Techniques. Morgan Kaufmann.
196
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1266?1276,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Identifying Functional Relations in Web Text
Thomas Lin, Mausam, Oren Etzioni
Turing Center
University of Washington
Seattle, WA 98195, USA
{tlin,mausam,etzioni}@cs.washington.edu
Abstract
Determining whether a textual phrase denotes
a functional relation (i.e., a relation that maps
each domain element to a unique range el-
ement) is useful for numerous NLP tasks
such as synonym resolution and contradic-
tion detection. Previous work on this prob-
lem has relied on either counting methods or
lexico-syntactic patterns. However, determin-
ing whether a relation is functional, by ana-
lyzing mentions of the relation in a corpus,
is challenging due to ambiguity, synonymy,
anaphora, and other linguistic phenomena.
We present the LEIBNIZ system that over-
comes these challenges by exploiting the syn-
ergy between the Web corpus and freely-
available knowledge resources such as Free-
base. It first computes multiple typed function-
ality scores, representing functionality of the
relation phrase when its arguments are con-
strained to specific types. It then aggregates
these scores to predict the global functionality
for the phrase. LEIBNIZ outperforms previ-
ous work, increasing area under the precision-
recall curve from 0.61 to 0.88. We utilize
LEIBNIZ to generate the first public reposi-
tory of automatically-identified functional re-
lations.
1 Introduction
The paradigm of Open Information Extraction (IE)
(Banko et al, 2007; Banko and Etzioni, 2008) has
scaled extraction technology to the massive set of
relations expressed in Web text. However, additional
work is needed to better understand these relations,
and to place them in richer semantic structures. A
step in that direction is identifying the properties of
these relations, e.g., symmetry, transitivity and our
focus in this paper ? functionality. We refer to this
problem as functionality identification.
A binary relation is functional if, for a given arg1,
there is exactly one unique value for arg2. Exam-
ples of functional relations are father, death date,
birth city, etc. We define a relation phrase to be
functional if all semantic relations commonly ex-
pressed by that phrase are functional. For exam-
ple, we say that the phrase ?was born in? denotes
a functional relation, because the different seman-
tic relations expressed by the phrase (e.g., birth city,
birth year, etc.) are all functional.
Knowing that a relation is functional is helpful
for numerous NLP inference tasks. Previous work
has used functionality for the tasks of contradiction
detection (Ritter et al, 2008), quantifier scope dis-
ambiguation (Srinivasan and Yates, 2009), and syn-
onym resolution (Yates and Etzioni, 2009). It could
also aid in other tasks such as ontology generation
and information extraction. For example, consider
two sentences from a contradiction detection task:
(1) ?George Washington was born in Virginia.? and
(2) ?George Washington was born in Texas.?
As Ritter et al (2008) points out, we can only de-
termine that the two sentences are contradictory if
we know that the semantic relation referred to by
the phrase ?was born in? is functional, and that both
Virginia and Texas are distinct states.
Automatic functionality identification is essential
when dealing with a large number of relations as in
Open IE, or in complex domains where expert help
1266
Distributional
Difference
Assertions
Web
Corpus IE
Functionality prediction for Web relations
Combination Policy
Freebase
Clean 
Lists
type 
listsinstances per relation
functionality scores (typed)
Figure 1: Our system, LEIBNIZ, uses the Web and Free-
base to determine functionality of Web relations.
is scarce or expensive (e.g., biomedical texts). This
paper tackles automatic functionality identification
using Web text. While functionality identification
has been utilized as a module in various NLP sys-
tems, this is the first paper to focus exclusively on
functionality identification as a bona fide NLP infer-
ence task.
It is natural to identify functions based on triples
extracted from text instead of analyzing sentences
directly. Thus, as our input, we utilize tuples ex-
tracted by TEXTRUNNER (Banko and Etzioni, 2008)
when run over a corpus of 500 million webpages.
TEXTRUNNER maps sentences to tuples of the form
<arg1, relation phrase, arg2> and enables our
LEIBNIZ system to focus on the problem of decid-
ing whether the relation phrase is a function.
The naive approach, which classifies a relation
phrase as non-functional if several arg1s have multi-
ple arg2s in our extraction set, fails due to several
reasons: synonymy ? a unique entity may be re-
ferred by multiple strings, polysemy of both entities
and relations ? a unique string may refer to multiple
entities/relations, metaphorical usage, extraction er-
rors and more. These phenomena conspire to make
the functionality determination task inherently sta-
tistical and surprisingly challenging.
In addition, a functional relation phrase may ap-
pear non-functional until we consider the types of its
arguments. In our ?was born in? example, <George
Washington, was born in, 1732> does not contradict
<George Washington, was born in, Virginia> even
though we see two distinct arg2s for the same arg1.
To solve functionality identification, we need to con-
sider typed relations where the relations analyzed
are constrained to have specific argument types.
We develop several approaches to overcome these
challenges. Our first scheme employs approximate
argument merging to overcome the synonymy and
anaphora problems. Our second approach, DIS-
TRDIFF, takes a statistical view of the problem
and learns a separator for the typical count dis-
tributions of functional versus non-functional rela-
tions. Finally, our third and most successful scheme,
CLEANLISTS, identifies and processes a cleaner
subset of the data by intersecting the corpus with en-
tities in a secondary knowledge-base (in our case,
Freebase (Metaweb Technologies, 2009)). Utiliz-
ing pre-defined types, CLEANLISTS first identifies
typed functionality for suitable types for that rela-
tion phrase, and then combines them to output a final
functionality label. LEIBNIZ, a hybrid of CLEAN-
LISTS and DISTRDIFF, returns state-of-the-art re-
sults for our task.
Our work makes the following contributions:
1. We identify several linguistic phenomena that
make the problem of corpus-based functional-
ity identification surprisingly difficult.
2. We designed and implemented three novel
techniques for identifying functionality based
on instance-based counting, distributional dif-
ferences, and use of external knowledge bases.
3. Our best method, LEIBNIZ, outperforms the
existing approaches by wide margins, increas-
ing area under the precision-recall curve from
0.61 to 0.88. It is also capable of distinguishing
functionality of typed relation phrases, when
the arguments are restricted to specific types.
4. Utilizing LEIBNIZ, we created the first public
repository of functional relations.1
2 Related Work
There is a recent surge in large knowledge bases
constructed by human collaboration such as Free-
base (Metaweb Technologies, 2009) and VerbNet
(Kipper-Schuler, 2005). VerbNet annotates its
verbs with several properties but not functionality.
Freebase does annotate some relations with an ?is
unique? property, which is similar to functionality,
but the number of relations in Freebase is still much
1available at http://www.cs.washington.edu/
research/leibniz
1267
George Washington was born in :
Virginia
Westmoreland County
America
a town
a plantation
1732
February
the British colony of Virginia
Rudy Giuliani visited:
Florida
Boca Raton Synagogue
the Florida EvergladesSouth Carolina
Michigan
Republican headquarters
a famous cheesesteak restaurant
Philadelphia
Colonial Beach, Virginia
Figure 2: Sample arg2 values for a non-functional relation (visited) vs. a functional relation (was born in) illustrate
the challenge in discriminating functionality from Web text.
smaller than the hundreds of thousands of relations
existing on the Web, necessitating automatic ap-
proaches to functionality identification.
Discovering functional dependencies has been
recognized as an important database analysis tech-
nique (Huhtala et al, 1999; Yao and Hamilton,
2008), but the database community does not address
any of the linguistic phenomena which make this
a challenging problem in NLP. Three groups of re-
searchers have studied functionality identification in
the context of natural language.
AuContraire (Ritter et al, 2008) is a contradic-
tion detection system that also learns relation func-
tionality. Their approach combines a probabilis-
tic model based on (Downey et al, 2005) with es-
timates on whether each arg1 is ambiguous. The
estimates are used to weight each arg1?s contri-
bution to an overall functionality score for each
relation. Both argument-ambiguity and relation-
functionality are jointly estimated using an EM-like
method. While elegant, AuContraire requires sub-
stantial hand-engineered knowledge, which limits
the scalability of their approach.
Lexico-syntactic patterns: Srinivasan and Yates
(2009) disambiguate a quantifier?s scope by first
making judgments about relation functionality. For
functionality, they look for numeric phrases follow-
ing the relation. For example, the presence of the nu-
meric term ?four? in the sentence ?the fire destroyed
four shops? suggests that destroyed is not functional,
since the same arg1 can destroy multiple things.
The key problem with this approach is that it often
assigns different functionality labels for the present
tense and past tense phrases of the same semantic re-
lation. For example, it will consider ?lived in? to be
non-functional, but ?lives in? to be functional, since
we rarely say ?someone lives in many cities?. Since
both these phrases refer to the same semantic rela-
tion this approach has low precision. Moreover, it
performs poorly for relation phrases that naturally
expect numbers as the target argument (e.g., ?has an
atomic number of?).
While these lexico-syntactic patterns do not per-
form as well for our task, they are well-suited for
identifying whether a verb phrase can take multiple
objects or not. This can be understood as a function-
ality property of the verb phrase within a sentence,
as opposed to functionality of the semantic relation
the phrase represents.
WIE: In a preliminary study, Popescu (2007) ap-
plies an instance based counting approach, but her
relations require manually annotated type restric-
tions, which makes the approach less scalable.
Finally, functionality is just one property of rela-
tions that can be learned from text. A number of
other studies (Guarino and Welty, 2004; Volker et
al., 2005; Culotta et al, 2006) have examined detect-
ing other relation properties from text and applying
them to tasks such as ontology cleaning.
3 Challenges for Functionality Identification
A functional binary relation r is formally defined as
one such that ?x, y1, y2 : r(x, y1)?r(x, y2)? y1 =
y2. We define a relation string to be functional if all
semantic relations commonly expressed by the rela-
tion string are individually functional. Thus, under
our definition, ?was born in? and ?died in? are func-
tional, even though they can take different arg2s for
the same arg1, e.g., year, city, state, country, etc.
The definition of a functional relation suggests a
naive instance-based counting algorithm for identi-
fying functionality. ?Look for the number of arg2s
for each arg1. If all (or most) arg1s have exactly one
arg2, label the relation phrase functional, else, non-
functional.? Unfortunately, this naive algorithm fails
for our task exposing several linguistic phenomena
1268
that make our problem hard (see Figure 2):
Synonymy: Various arg2s for the same arg1 may
refer to the same entity. This makes many func-
tional relations seem non-functional. For instance,
<George Washington, was born in, Virginia> and
<George Washington, was born in, the British
colony of Virginia> are not in conflict. Other
examples of synonyms include ?Windy City? and
?Chicago?; ?3rd March? and ?03/03?, etc.
Anaphora: An entity can be referred to by using
several phrases. For instance,<George Washington,
was born in, a town> does not conflict with his be-
ing born in ?Colonial Beach, Virginia?, since ?town?
is an anaphora for his city of birth. Other examples
include ?The US President? for ?George W. Bush?,
and ?the superpower? to refer to ?United States?. The
effect is similar to that of synonyms ? many relations
incorrectly appear non-functional.
Argument Ambiguity: <George Washington, was
born in, ?Kortrijk, Belgium?> in addition to his be-
ing born in ?Virginia? suggests that ?was born in?
is non-functional. However, the real cause is that
?George Washington? is ambiguous and refers to dif-
ferent people. This ambiguity gets more pronounced
if the person is referred to just by their first (or last
name), e.g., ?Clinton? is commonly used to refer to
both Hillary and Bill Clinton.
Relation Phrase Ambiguity: A relation phrase can
have several senses. For instance ?weighs 80 kilos?
is a different weighs than ?weighs his options?.
Type Restrictions: A closely related problem
is type-variations in the argument. E.g., <George
Washington, was born in, America> vs. <George
Washington, born in, Virginia> both use the same
sense of ?was born in? but refer to different semantic
relations ? one that takes a country in arg2, and the
other that takes a state. Moreover, different argu-
ment types may result in different functionality la-
bels. For example, ?published in? is functional if the
arg2 is a year, but non-functional if it is a language,
since a book could be published in many languages.
We refer to this finer notion of functionality as typed
functionality.
Data Sparsity: There is limited data for more ob-
scure relations instances and non-functional relation
phrases appear functional due to lack of evidence.
Textually Functional Relations: Last but not least,
some relations that are not functional may appear
functional in text. An example is ?collects?. We col-
lect many things, but rarely mention it in text. Usu-
ally, someone?s collection is mentioned in text only
when it makes the news. We name such relations
textually functional. Even though we could build
techniques to reduce the impact of other phenomena,
no instance based counting scheme could overcome
the challenge posed by textually functional relations.
Finally, we note that our functionality predictor
operates over tuples generated by an Open IE sys-
tem. The extractors are not perfect and their errors
can also complicate our analysis.
4 Algorithms
To overcome these challenges, we design three al-
gorithms. Our first algorithm, IBC, applies several
rules to determine whether two arg2s are equal. Our
second algorithm, DISTRDIFF, takes a statistical ap-
proach, and tries to learn a discriminator between
typical count distributions for functional and non-
functional relations. Our final approach, CLEAN-
LISTS, applies counting over a cleaner subset of the
corpus, which is generated based on entities present
in a secondary KB such as Freebase.
From this section onwards, we gloss over the dis-
tinction between a semantic relation and a relation
phrase, since our algorithms do not have access to
relations and operate only at the phrase level. We
use ?relation? to refer to the phrases.
4.1 Instance Based Counting (IBC)
For each relation, IBC computes a global function-
ality score by aggregating local functionality scores
for each arg1. The local functionality for each arg1
computes the fraction of arg2 pairs that refer to the
same entity. To operationalize this computation we
need to identify which arg2s co-refer. Moreover, we
also need to pick an aggregation strategy to combine
local functionality scores.
Data Cleaning: Common nouns in arg1s are of-
ten anaphoras for other entities. For example, <the
company, was headquartered in, ...> refers to dif-
ferent companies in different extractions. To combat
this, IBC restricts arg1s to proper nouns. Secondly,
to counter extraction errors and data bias, it retains
1269
George Washington was born in :
Colonial
Beach
Colonial 
Beach,
Virginia
Westmoreland County, Virginia
February February 1732 1732
Figure 3: IBC judges that Colonial Beach and Westmore-
land County, Virginia refer to the same entity.
an extraction only once per unique sentence. This
reduces the disproportionately large frequencies of
some assertions that are generated from a single ar-
ticle published at multiple websites. Similarly, it al-
lows an extraction only once per website url. More-
over, it filters out any arg1 that does not appear at
least 10 times with that relation.
Equality Checking: This key component judges
if two arg2s refer to the same entity. It first em-
ploys weak typing by disallowing equality checks
across common nouns, proper nouns, dates and
numbers. This mitigates the relation ambiguity
problem, since we never compare ?born in(1732)?
and ?born in(Virginia)?. Within the same category it
judges two arg2s to co-refer if they share a content
word. It also performs a connected component anal-
ysis (Hopcraft and Tarjan, 1973) to take a transitive
closure of arg2s judged equal (see Figure 3).
For example, for the relation ?was named after?
and arg1=?Bluetooth? our corpus has three arg2s:
?Harald Bluetooth?, ?Harald Bluetooth, the King of
Denmark? and ?the King of Denmark?. Our equal-
ity method judges all three as referring to the same
entity. Note that this is a heuristic approach, which
could make mistakes. But for an error, there needs
to be extractions with the same arg1, relation and
similar arg2s. Such cases exist, but are not com-
mon. Our equality checking mitigates the problems
of anaphora, synonymy as well as some typing.
Aggregation: We try several methods to aggre-
gate local functionality scores for each arg1 into a
global score for the relation. These include, a simple
average, a weighted average weighted by frequency
of each arg1, a weighted average weighted by log
of frequency of each arg1, and a Bayesian approach
that estimates the probability that a relation is func-
tional using statistics over a small development set.
Overall, the log-weighting works the best: it assigns
a higher score for popular arguments, but not so high
that it drowns out all the other evidence.
4.2 DISTRDIFF
Our second algorithm, DISTRDIFF, takes a purely
statistical, discriminative view of the problem. It
recognizes that, due to aforementioned reasons,
whether a relation is functional or not, there are
bound to be several arg1s that look locally functional
and several that look locally non-functional. The
difference is in the number of such arg1s ? a func-
tional relation will have more of the former type.
DISTRDIFF studies the count distributions for a
small development set of functional relations (and
similarly for non-functional) and attempts to build
a separator between the two. As an illustration,
Figure 4(a) plots the arg2 counts for various arg1s
for a functional relation (?is headquartered in?).
Each curve represents a unique arg1. For an arg1,
the x-axis represents the rank (based on frequency)
of arg2s and y-axis represents the normalized fre-
quency of the arg2. For example, if an arg1 is found
with just one arg2, then x=1 will match with y=1
(the first point has all the mass) and x=2 will match
with y=0. If, on the other hand, an arg1 is found
with five arg2s, say, appearing ten times each, then
the first five x-points will map to 0.2 and the sixth
point will map to 0.
We illustrate the same plot for a non-functional
relation (?visited?) in Figure 4(b). It is evident from
the two figures that, as one would expect, curves for
most arg1s die early in case of a functional relation,
whereas the lower ranked arg2s are more densely
populated in case of a non-functional relation.
We aggregate this information using slope of the
best-fit line for each arg1 curve. For functional re-
lations, the best-fit lines have steep slopes, whereas
for non-functional the lines are flatter. We bucket the
slopes in integer bins and count the fraction of arg1s
appearing in each bin. This lets us aggregate the
information into a single slope-distribution for each
relation. Bold lines in Figure 4(c) illustrate the aver-
age slope-distributions, averaged over ten sample re-
lations of each kind ? dashed for non-functional and
solid for functional. Most non-functional relations
have a much higher probability of arg1s with low
magnitude slopes, whereas functional relations are
1270
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
1 2 3 4 5 6 7 8 9 10
Arg
um
ent
 2 M
ass
 
Result Position 
is headquartered in (functional) 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
1 2 3 4 5 6 7 8 9 10
Arg
um
ent
 2 M
ass
 
Result Position 
visited (not functional) 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0 1 2 3 4 10
Ma
ss 
Floor(|Slope|) 
Average Slope Distributions 
functional average
nonfunctional average
was born on (sample func)
visited (sample nonfunc)
Figure 4: DISTRDIFF: Arg2 count distributions fall more sharply for (a) a sample functional relation, than (b) a
sample non-functional relation. (c) The distance of aggregated slope-distributions from average slope-distributions
can be used to predict the functionality.
the opposite. Notice that the aggregated curve for
?visited? in the figure is closer to the average curve
for non-functional than to functional and vice-versa
for ?was born on?.
We plot the aggregated slope-distributions for
each relation and use the distance from average dis-
tributions as a means to predict the functionality. We
use KL divergence (Kullback and Leibler, 1951) to
compute the distance between two distributions. We
score a relation?s functionality in three ways using:
(1) KLFUNC, its distance from average functional
slope-distribution Favg, (2) KLDIFF, its distance
from average functional minus its distance from av-
erage non-functional Navg, and (3) average of these
two scores. For a relation with slope distribution R,
the scores are computed as:
KLFUNC =
?
iR(i)ln
R(i)
Favg(i)
KLDIFF = KLFUNC - (
?
iR(i)ln
R(i)
Navg(i)
)
Section 5.2 compares the three scoring functions.
A purely statistical approach is resilient to noisy
data, and does not need to explicitly account for the
various issues we detailed earlier. A disadvantage
is that it cannot handle relation ambiguity and type
restrictions. Moreover, we may need to relearn the
separator if applying DISTRDIFF to a corpus with
very different count distributions.
4.3 CLEANLISTS
Our third algorithm, CLEANLISTS, is based on the
intuition that for identifying functionality we need
not reason over all the data in our corpus; instead,
a small but cleaner subset of the data may work
best. This clean subset should ideally be free of syn-
onyms, ambiguities and anaphora, and be typed.
Several knowledge-bases such as Wordnet,
Wikipedia, and Freebase (Fellbaum, 1998;
Wikipedia, 2004; Metaweb Technologies, 2009),
are readily and freely available and they all provide
clean typed lists of entities. In our experiments
CLEANLISTS employs Freebase as a source of
clean lists, but we could use any of these or other
domain-specific ontologies such as SNOMED
(Price and Spackman, 2000) as well.
CLEANLISTS takes the intersection of Freebase
entities with our corpus to generate a clean subset for
functionality analysis. Freebase currently has over
12 million entities in over 1,000 typed lists. Thus,
this intersection retains significant portions of the
useful data, and gets rid of most of anaphora and
synonymy issues. Moreover, by matching against
typed lists, many relation ambiguities are separated
as well, since ambiguous relations often take dif-
ferent types in the arguments (e.g., ?ran(Distance)?
vs. ?ran(Company)?). To mitigate the effect of argu-
ment ambiguity, we additionally get rid of instances
in which arg1s match multiple names in the Freebase
list of names.
As an example, consider the ?was born in? rela-
tion. CLEANLISTS will remove instances with only
?Clinton? in arg1, since it matches multiple people
in Freebase. It will treat the different types, e.g.,
cities, states, countries, months separately and ana-
lyze the functionality for each of these individually.
1271
By intersecting the relation data with argument lists
for these types, we will be left with a smaller, but
much cleaner, subset of relation data, one for each
type. CLEANLISTS analyzes each subset using sim-
ple, instance based counting and computes a typed
functionality score for each type. Thus, it first com-
putes typed functionality for each relation.
There are two subtleties in applying this algo-
rithm. First, we need to identify the set of types to
consider for each relation. Our algorithm currently
picks the types that occur most in each relation?s
observed data. In the future, we could also use a
selectional preferences system (Ritter et al, 2010;
Kozareva and Hovy, 2010). Note that we remove
Freebase types such as Written Work from consid-
eration for containing many entities whose primary
senses are not that type. For example, both ?Al Gore?
and ?William Clinton? are also names of books, but
references in text to these are rarely a reference to
the written work sense.
Secondly, an argument could belong to multiple
Freebase lists. For example, ?California? is both a
city and a state. We apply a simple heuristic: if a
string appears in multiple lists under consideration,
we assign it to the smallest of the lists (the list of
cities is much larger than states). This simple heuris-
tic usually assigns an argument to its intended type.
On a development set, the error rate of this heuristic
is<4%, though it varies a bit depending on the types
involved.
CLEANLISTS determines the overall functional-
ity of a relation string by aggregating the scores for
each type. It outputs functional if a majority of typed
senses for the relation are functional. For example,
CLEANLISTS judges ?was born in? to be functional,
since all relevant type restrictions are individually
typed functional ? everyone is born in exactly one
country, city, state, month, etc.
CLEANLISTS has a much higher precision due to
the intersection with clean lists, though at some cost
of recall. The reason for lower recall is that the ap-
proach has a bias towards types that are easy to enu-
merate. It does not have different distances (e.g., 50
kms, 20 miles, etc.) in its lists. Moreover, arguments
that do not correspond to a noun cannot be handled.
For example, in the sentence, ?He weighed eating
a cheeseburger against eating a salad?, the arg2 of
?weighed? can?t be matched to a Freebase list. To
increase the recall we back off to DISTRDIFF in the
cases when CLEANLISTS is unable to make a pre-
diction. This combination gives the best balance of
precision and recall for our task. We name our final
system LEIBNIZ.
One current limitation is that using only those
arg2s that exactly match clean lists leaves out some
good data (e.g., a tuple with an arg2 of ?Univ of
Wash? will not match against a list of universities
that spells it as ?University of Washington?). Be-
cause we have access to entity types, using typed
equality checkers (Prager et al, 2007) with the clean
lists would allow us to recapture much of this useful
data. Moreover, the knowledge of functions could
apply to building new type nanotheories and reduce
considerable manual effort. We wish to study this in
the future.
5 Evaluation
In our evaluation, we wish to answer three ques-
tions: (1) How do our three approaches, Instance
Based Counting (IBC), DISTRDIFF, and CLEAN-
LISTS, compare on the functionality identification
task? (2) How does our final system, LEIBNIZ,
compare against the existing state of the art tech-
niques? (3) How well is LEIBNIZ able to identify
typed functionality for different types in the same
relation phrase?
5.1 Dataset
For our experiments we test on the set of 887 re-
lations used by Ritter et al (2008) in their exper-
iments. We use the Open IE corpus generated by
running TEXTRUNNER on 500 million high quality
Webpages (Banko and Etzioni, 2008) as the source
of instance data for these relations. Extractor and
corpus differences lead to some relations not occur-
ring (or not occurring with sufficient frequency to
properly analyze, i.e.,? 5 arg1 with? 10 evidence),
leaving a dataset of 629 relations on which to test.
Two human experts tagged these relations for
functionality. Tagging the functionality of relation
phrases can be a bit subjective, as it requires the
experts to imagine the various senses of a phrase
and judge functionality over all those senses. The
inter-annotator agreement between the experts was
95.5%. We limit ourselves to the subset of the data
1272
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
Pre
cis
ion
Recall
Distr. Diff. Scoring Functions
KLfunc
KLdiff
Average
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
Pre
cis
ion
Recall
Internal Methods Comparison
IBC
Distr. Diff.
CleanLists
Figure 5: (a) The best scoring method for DISTRDIFF averages KLFUNC and KLDIFF. (b) CLEANLISTS performs
significantly better than DISTRDIFF, which performs significantly better than IBC.
on which the two experts agreed (a subset of 601
relation phrases).
5.2 Internal Comparisons
First, we compare the three scoring functions for
DISTRDIFF. We vary the score thresholds to gener-
ate the different points on the precision-recall curves
for each of the three. Figure 5(a) plots these curves.
It is evident that the hybrid scoring function, i.e.,
one which is an average of KLFUNC (distance from
average functional) and KLDIFF (distance from av-
erage functional minus distance from average non-
functional) performs the best. We use this scoring in
the further experiments involving DISTRDIFF.
Next, we compare our three algorithms on
the dataset. Figure 5(b) reports the results.
CLEANLISTS outperforms DISTRDIFF by vast mar-
gins, covering a 33.5% additional area under the
precision-recall curve. Overall, CLEANLISTS finds
the very high precision points, because of its use of
clean data. However, it is unable to make 23.1% of
the predictions, primarily because the intersection
between the corpus and Freebase entities results in
very little data for those relations. DISTRDIFF per-
forms better than IBC, due to its statistical nature,
but the issues described in Section 3 plague both
these systems much more than CLEANLISTS.
To increase the recall LEIBNIZ uses a combina-
tion of DISTRDIFF and CLEANLISTS, in which the
algorithm backs off to DISTRDIFF if CLEANLISTS
is unable to output a prediction.
5.3 External Comparisons
We next compare LEIBNIZ against the existing state
of the art approaches. Our competitors are AuCon-
traire and NumericTerms (Ritter et al, 2008; Srini-
vasan and Yates, 2009). Because we use the Au-
Contraire dataset, we report the results from their
best performing system. We reimplement a version
of NumericTerms using their list of numeric quanti-
fiers and extraction patterns that best correspond to
our relation format. We run our implementation of
NumericTerms on a dataset of 100 million English
sentences from a crawl of high quality Webpages to
generate the functionality labels.
Figure 6(a) reports the results of this experiment.
We find that LEIBNIZ outperforms AuContraire by
vast margins covering an additional 44% area in the
precision-recall curve. AuContraire?s AUC is 0.61
whereas LEIBNIZ covers 0.88. A Bootstrap Per-
centile Test (Keller et al, 2005) on F1 score found
the improvement of our techniques over AuCon-
traire to be statistically significant at ? = 0.05. Nu-
mericTerms does not perform well, because it makes
decisions based only on the local evidence in a sen-
tence, and does not integrate the knowledge from
different occurrences of the same relation. It returns
many false positives, such as ?lives in?, which ap-
pear functional to the lexico-syntactic pattern, but
are clearly non-functional, e.g., one could live in
many places over a lifetime.
An example of a LEIBNIZ error is the ?repre-
sented? relation. LEIBNIZ classifies this as func-
tional, because it finds several strongly functional
senses (e.g., when a person represents a country),
1273
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
Pre
cis
ion
Recall
Typed Functionality
AuContraire
NumericTerms
Leibniz
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
Pre
cis
ion
Recall
External Comparison
AuContraire
NumericTerms
Leibniz
Figure 6: (a) LEIBNIZ, which is a hybrid of CLEANLISTS and DISTRDIFF, achieves 0.88 AUC and outperforms the
0.61 AUC from AuContraire (Ritter et al, 2008) and the 0.05 AUC from NumericTerms (Srinivasan and Yates, 2009).
(b) LEIBNIZ is able to tease apart different senses of polysemous relations much better than other systems.
but the human experts might have had some non-
functional senses in mind while labeling.
5.4 Typed Functionality
Next, we conduct a study of the typed functional-
ity task. We test on ten common polysemous re-
lations, each having both a functional and a non-
functional sense. An example is the ?was pub-
lished in? relation. If arg2 is a year it is func-
tional, e.g. <Harry Potter 5, was published in,
2003>. However, ?was published in(Language)?
is not functional, e.g. <Harry Potter 5, was pub-
lished in, [French / Spanish / English]>. Simi-
larly, ?will become(Company)? is functional because
when a company is renamed, it transitions away
from the old name exactly once, e.g. <Cingular,
will become, AT&T Wireless>. However, ?will be-
come(government title)? is not functional, because
people can hold different offices in their life, e.g.,
<Obama, will become, [Senator / President]>.
In this experiment, a simple baseline of predict-
ing the same label for the two types of each rela-
tion achieves a precision of 0.5. Figure 6(b) presents
the results of this study. AuContraire achieves a flat
0.5, since it cannot distinguish between types. Nu-
mericTerms can be modified to distinguish between
basic types ? check the word just after the numeric
term to see whether it matches the type name. For
example, the modified NumericTerms will search
the Web for instances of ?was published in [nu-
meric term] years? vs. ?was published in [numeric
term] languages?. This scheme works better when
the type name is simple (e.g., languages) rather than
complex (e.g., government titles).
LEIBNIZ performs the best and is able to tease
apart the functionality of various types very well.
When LEIBNIZ did not work, it was generally be-
cause of textual functionality, which is a larger issue
for typed functionality than general functionality. Of
course, these results are merely suggestive ? we per-
form a larger-scale experiment and generate a repos-
itory of typed functions next.
6 A Repository of Functional Relations
We now report on a repository of typed functional
relations generated automatically by applying LEIB-
NIZ to a large collection of relation phrases. Instead
of starting with the most frequent relations from
TEXTRUNNER, we use OCCAM?s relations (Fader
et al, 2010) because they are more specific. For in-
stance, where TEXTRUNNER outputs an underspec-
ified tuple, <Gold, has, an atomic number of 79>,
OCCAM extracts <Gold, has an atomic number of,
79>. OCCAM enables LEIBNIZ to identify far more
functional relations than TEXTRUNNER.
6.1 Addressing Evidence Sparsity
Scaling up to a large collection of typed relations
requires us to consider the size of our data sets. For
example, consider which relation is more likely to be
functional?a relation with 10 instances all of which
indicate functionality versus a relation with 100 in-
stances where 95 behave functionally.
To address this problem, we adapt the likelihood
ratio approach from Schoenmackers et al (2010).
1274
For a typed relation with n instances, f of which in-
dicate functionality, the G-test (Dunning, 1993), G
= 2*(f*ln(f/k)+(n-f)*ln((n-f)/(n-k))), provides a mea-
sure for the likelihood that the relation is not func-
tional. Here k denotes the evidence indicating func-
tionality for the case where the relation is not func-
tional. Setting k = n*0.25 worked well for us. This
G-score replaces our previous metric for scoring
functional relations.
6.2 Evaluation of the Repository
In CLEANLISTS a factor that affects the quality of
the results is the exact set of lists that is used. If
the lists are not clean, results get noisy. For exam-
ple, Freebase?s list of films contains 73,000 entries,
many of which (e.g., ?Egg?) are not films in their pri-
mary senses. Even with heuristics such as assigning
terms to their smallest lists and disqualifying dictio-
nary words that occur from large type lists, there is
still significant noise left.
Using LEIBNIZ with a set of 35 clean lists on
OCCAM?s extraction corpus, we generated a repos-
itory of 5,520 typed functional relations. To eval-
uate this resource a human expert tagged a random
subset of the top 1,000 relations. Of these relations
22% were either ill-formed or had non-sensical type
constraints. From the well-formed typed relations
the precision was estimated to be 0.8. About half
the errors were due to textual functionality and the
rest were LEIBNIZ errors. Some examples of good
functions found include isTheSequelTo(videogame)
and areTheBirthstoneFor(month). An example of
a textually functional relation found is wasThe-
FounderOf(company).
This is the first public repository of automatically-
identified functional relations. Scaling up our data
set forced us to confront new sources of noise in-
cluding extractor errors, errors due to mismatched
types, and errors due to sparse evidence. Still, our
initial results are encouraging and we hope that our
resource will be valuable as a baseline for future
work.
7 Conclusions
Functionality identification is an important subtask
for Web-scale information extraction and other ma-
chine reading tasks. We study the problem of pre-
dicting the functionality of a relation phrase auto-
matically from Web text. We presented three algo-
rithms for this task: (1) instance-based counting, (2)
DISTRDIFF, which takes a statistical approach and
discriminatively classifies the relations using aver-
age arg-distributions, and (3) CLEANLISTS, which
performs instance based counting on a subset of
clean data generated by intersection of the corpus
with a knowledge-base like Freebase.
Our best approach, LEIBNIZ, is a hybrid of
DISTRDIFF and CLEANLISTS, and outperforms
the existing state-of-the-art approaches by covering
44% more area under the precision-recall curve. We
also observe that an important sub-component of
identifying a functional relation phrase is identifying
typed functionality, i.e., functionality when the ar-
guments of the relation phrase are type-constrained.
Because CLEANLISTS is able to use typed lists, it
can successfully identify typed functionality.
We run our techniques on a large set of relations to
output a first repository of typed functional relations.
We release this list for further use by the research
community.2
Future Work: Functionality is one of the sev-
eral properties a relation can possess. Others in-
clude selectional preferences, transitivity (Schoen-
mackers et al, 2008), mutual exclusion, symme-
try, etc. These properties are very useful in increas-
ing our understanding about these Open IE relation
strings. We believe that the general principles devel-
oped in this work, for example, connecting the Open
IE knowledge with an existing knowledge resource,
will come in very handy in identifying these other
properties.
Acknowledgements
We would like to thank Alan Ritter, Alex Yates and
Anthony Fader for access to their data sets. We
would like to thank Stephen Soderland, Yoav Artzi,
and the anonymous reviewers for helpful comments
on previous drafts. This research was supported
in part by NSF grant IIS-0803481, ONR grant
N00014-08-1-0431, DARPA contract FA8750-09-
C-0179, a NDSEG Fellowship, and carried out at
the University of Washington?s Turing Center.
2available at http://www.cs.washington.edu/
research/leibniz
1275
References
M. Banko and O. Etzioni. 2008. The tradeoffs between
open and traditional relation extraction. In Proceed-
ings of the 46th Annual Meeting of the Association for
Computational Linguistics (ACL).
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence (IJCAI).
A. Culotta, A. McCallum, and J. Betz. 2006. Integrating
probabilistic extraction models and data mining to dis-
cover relations and patterns in text. In Proceedings of
the HLT-NAACL.
D. Downey, O. Etzioni, and S. Soderland. 2005. A prob-
abilistic model of redundancy in information extrac-
tion. In Proceedings of the 19th International Joint
Conference on Artificial Intelligence (IJCAI).
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. In Computational Linguis-
tics, volume 19.
A. Fader, O. Etzioni, and S. Soderland. 2010. Identi-
fying well-specified relations for open information ex-
traction. (In preparation).
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
N. Guarino and C. Welty. 2004. An overview of On-
toClean. In Handbook of Ontologies in Information
Systems, pages 151?172.
J. Hopcraft and R. Tarjan. 1973. Efficient algorithms
for graph manipulation. Communications of the ACM,
16:372?378.
Y. Huhtala, J. Ka?rkka?inen, P. Porkka, and H. Toivonen.
1999. TANE: An efficient algorithm for discover-
ing functional and approximate dependencies. In The
Computer Journal.
M. Keller, S. Bengio, and S.Y. Wong. 2005. Bench-
marking non-parametric statistical tests. In Advances
in Neural Information Processing Systems (NIPS) 18.
K. Kipper-Schuler. 2005. Verbnet: A broad-coverage,
comprehensive verb lexicon. In Ph.D. thesis. Univer-
sity of Pennsylvania.
Z. Kozareva and E. Hovy. 2010. Learning arguments
and supertypes of semantic relations using recursive
patterns. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL).
S. Kullback and R.A. Leibler. 1951. On information
and sufficiency. Annals of Mathematical Statistics,
22(1):79?86.
Metaweb Technologies. 2009. Freebase data dumps. In
http://download.freebase.com/datadumps/.
A-M. Popescu. 2007. Information extraction from un-
structured web text. In Ph.D. thesis. University of
Washington.
J. Prager, S. Luger, and J. Chu-Carroll. 2007. Type nan-
otheories: a framework for term comparison. In Pro-
ceedings of the 16th ACM Conference on Information
and Knowledge Management (CIKM).
C. Price and K. Spackman. 2000. SNOMED clincal
terms. In British Journal of Healthcare Computing &
Information Management, volume 17.
A. Ritter, D. Downey, S. Soderland, and O. Etzioni.
2008. It?s a contradiction - no, it?s not: A case study
using functional relations. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
A. Ritter, Mausam, and O. Etzioni. 2010. A latent dirich-
let alocation method for selectional preferences. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL).
S. Schoenmackers, O. Etzioni, and D. Weld. 2008. Scal-
ing textual inference to the web. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing (EMNLP).
S. Schoenmackers, J. Davis, O. Etzioni, and D. Weld.
2010. Learning first-order horn clauses from web text.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP).
P. Srinivasan and A. Yates. 2009. Quantifier scope
disambiguation using extracted pragmatic knowledge:
Preliminary results. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP).
J. Volker, D. Vrandecic, and Y. Sure. 2005. Auto-
matic evaluation of ontologies (AEON). In Proceed-
ings of the 4th International Semantic Web Conference
(ISWC).
Wikipedia. 2004. Wikipedia: The free encyclopedia. In
http://www.wikipedia.org. Wikimedia Foundation.
H. Yao and H. Hamilton. 2008. Mining functional de-
pendencies from data. In Data Mining and Knowledge
Discovery.
A. Yates and O. Etzioni. 2009. Unsupervised methods
for determining object and relation synonyms on the
web. In Journal of Artificial Intelligence Research,
volume 34, pages 255?296.
1276
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1524?1534,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Named Entity Recognition in Tweets:
An Experimental Study
Alan Ritter, Sam Clark, Mausam and Oren Etzioni
Computer Science and Engineering
University of Washington
Seattle, WA 98125, USA
{aritter,ssclark,mausam,etzioni}@cs.washington.edu
Abstract
People tweet more than 100 Million times
daily, yielding a noisy, informal, but some-
times informative corpus of 140-character
messages that mirrors the zeitgeist in an un-
precedented manner. The performance of
standard NLP tools is severely degraded on
tweets. This paper addresses this issue by
re-building the NLP pipeline beginning with
part-of-speech tagging, through chunking, to
named-entity recognition. Our novel T-NER
system doubles F1 score compared with the
Stanford NER system. T-NER leverages the
redundancy inherent in tweets to achieve this
performance, using LabeledLDA to exploit
Freebase dictionaries as a source of distant
supervision. LabeledLDA outperforms co-
training, increasing F1 by 25% over ten com-
mon entity types.
Our NLP tools are available at: http://
github.com/aritter/twitter_nlp
1 Introduction
Status Messages posted on Social Media websites
such as Facebook and Twitter present a new and
challenging style of text for language technology
due to their noisy and informal nature. Like SMS
(Kobus et al, 2008), tweets are particularly terse
and difficult (See Table 1). Yet tweets provide a
unique compilation of information that is more up-
to-date and inclusive than news articles, due to the
low-barrier to tweeting, and the proliferation of mo-
bile devices.1 The corpus of tweets already exceeds
1See the ?trending topics? displayed on twitter.com
the size of the Library of Congress (Hachman, 2011)
and is growing far more rapidly. Due to the vol-
ume of tweets, it is natural to consider named-entity
recognition, information extraction, and text mining
over tweets. Not surprisingly, the performance of
?off the shelf? NLP tools, which were trained on
news corpora, is weak on tweet corpora.
In response, we report on a re-trained ?NLP
pipeline? that leverages previously-tagged out-of-
domain text, 2 tagged tweets, and unlabeled tweets
to achieve more effective part-of-speech tagging,
chunking, and named-entity recognition.
1 The Hobbit has FINALLY started filming! I
cannot wait!
2 Yess! Yess! Its official Nintendo announced
today that they Will release the Nintendo 3DS
in north America march 27 for $250
3 Government confirms blast n nuclear plants n
japan...don?t knw wht s gona happen nw...
Table 1: Examples of noisy text in tweets.
We find that classifying named entities in tweets is
a difficult task for two reasons. First, tweets contain
a plethora of distinctive named entity types (Compa-
nies, Products, Bands, Movies, and more). Almost
all these types (except for People and Locations) are
relatively infrequent, so even a large sample of man-
ually annotated tweets will contain few training ex-
amples. Secondly, due to Twitter?s 140 character
limit, tweets often lack sufficient context to deter-
mine an entity?s type without the aid of background
2Although tweets can be written on any subject, following
convention we use the term ?domain? to include text styles or
genres such as Twitter, News or IRC Chat.
1524
knowledge.
To address these issues we propose a distantly su-
pervised approach which applies LabeledLDA (Ra-
mage et al, 2009) to leverage large amounts of unla-
beled data in addition to large dictionaries of entities
gathered from Freebase, and combines information
about an entity?s context across its mentions.
We make the following contributions:
1. We experimentally evaluate the performance of
off-the-shelf news trained NLP tools when ap-
plied to Twitter. For example POS tagging
accuracy drops from about 0.97 on news to
0.80 on tweets. By utilizing in-domain, out-
of-domain, and unlabeled data we are able to
substantially boost performance, for example
obtaining a 52% increase in F1 score on seg-
menting named entities.
2. We introduce a novel approach to distant super-
vision (Mintz et al, 2009) using Topic Models.
LabeledLDA is applied, utilizing constraints
based on an open-domain database (Freebase)
as a source of supervision. This approach in-
creases F1 score by 25% relative to co-training
(Blum and Mitchell, 1998; Yarowsky, 1995) on
the task of classifying named entities in Tweets.
The rest of the paper is organized as follows.
We successively build the NLP pipeline for Twitter
feeds in Sections 2 and 3. We first present our ap-
proaches to shallow syntax ? part of speech tagging
(?2.1), and shallow parsing (?2.2). ?2.3 describes a
novel classifier that predicts the informativeness of
capitalization in a tweet. All tools in ?2 are used
as features for named entity segmentation in ?3.1.
Next, we present our algorithms and evaluation for
entity classification (?3.2). We describe related work
in ?4 and conclude in ?5.
2 Shallow Syntax in Tweets
We first study two fundamental NLP tasks ? POS
tagging and noun-phrase chunking. We also discuss
a novel capitalization classifier in ?2.3. The outputs
of all these classifiers are used in feature generation
for named entity recognition in the next section.
For all experiments in this section we use a dataset
of 800 randomly sampled tweets. All results (Tables
Accuracy Error
Reduction
Majority Baseline (NN) 0.189 -
Word?s Most Frequent Tag 0.760 -
Stanford POS Tagger 0.801 -
T-POS(PTB) 0.813 6%
T-POS(Twitter) 0.853 26%
T-POS(IRC + PTB) 0.869 34%
T-POS(IRC + Twitter) 0.870 35%
T-POS(PTB + Twitter) 0.873 36%
T-POS(PTB + IRC + Twitter) 0.883 41%
Table 2: POS tagging performance on tweets. By training
on in-domain labeled data, in addition to annotated IRC
chat data, we obtain a 41% reduction in error over the
Stanford POS tagger.
2, 4 and 5) represent 4-fold cross-validation experi-
ments on the respective tasks.3
2.1 Part of Speech Tagging
Part of speech tagging is applicable to a wide range
of NLP tasks including named entity segmentation
and information extraction.
Prior experiments have suggested that POS tag-
ging has a very strong baseline: assign each word
to its most frequent tag and assign each Out of Vo-
cabulary (OOV) word the most common POS tag.
This baseline obtained a 0.9 accuracy on the Brown
corpus (Charniak et al, 1993). However, the appli-
cation of a similar baseline on tweets (see Table 2)
obtains a much weaker 0.76, exposing the challeng-
ing nature of Twitter data.
A key reason for this drop in accuracy is that Twit-
ter contains far more OOV words than grammatical
text. Many of these OOV words come from spelling
variation, e.g., the use of the word ?n? for ?in? in Ta-
ble 1 example 3. Although NNP is the most frequent
tag for OOV words, only about 1/3 are NNPs.
The performance of off-the-shelf news-trained
POS taggers also suffers on Twitter data. The state-
of-the-art Stanford POS tagger (Toutanova et al,
2003) improves on the baseline, obtaining an accu-
racy of 0.8. This performance is impressive given
that its training data, the Penn Treebank WSJ (PTB),
is so different in style from Twitter, however it is a
huge drop from the 97% accuracy reported on the
3We used Brendan O?Connor?s Twitter tokenizer
1525
Gold Predicted Stanford
Error
T-POS Error Error
Reduction
NN NNP 0.102 0.072 29%
UH NN 0.387 0.047 88%
VB NN 0.071 0.032 55%
NNP NN 0.130 0.125 4%
UH NNP 0.200 0.036 82%
Table 3: Most common errors made by the Stanford POS
Tagger on tweets. For each case we list the fraction of
times the gold tag is misclassified as the predicted for
both our system and the Stanford POS tagger. All verbs
are collapsed into VB for compactness.
PTB. There are several reasons for this drop in per-
formance. Table 3 lists common errors made by
the Stanford tagger. First, due to unreliable capi-
talization, common nouns are often misclassified as
proper nouns, and vice versa. Also, interjections
and verbs are frequently misclassified as nouns. In
addition to differences in vocabulary, the grammar
of tweets is quite different from edited news text.
For instance, tweets often start with a verb (where
the subject ?I? is implied), as in: ?watchng american
dad.?
To overcome these differences in style and vocab-
ulary, we manually annotated a set of 800 tweets
(16K tokens) with tags from the Penn TreeBank tag
set for use as in-domain training data for our POS
tagging system, T-POS.4 We add new tags for the
Twitter specific phenomena: retweets, @usernames,
#hashtags, and urls. Note that words in these cate-
gories can be tagged with 100% accuracy using sim-
ple regular expressions. To ensure fair comparison
in Table 2, we include a postprocessing step which
tags these words appropriately for all systems.
To help address the issue of OOV words and
lexical variations, we perform clustering to group
together words which are distributionally similar
(Brown et al, 1992; Turian et al, 2010). In particu-
lar, we perform hierarchical clustering using Jcluster
(Goodman, 2001) on 52 million tweets; each word
is uniquely represented by a bit string based on the
path from the root of the resulting hierarchy to the
word?s leaf. We use the Brown clusters resulting
from prefixes of 4, 8, and 12 bits. These clusters are
often effective in capturing lexical variations, for ex-
4Using MMAX2 (Mu?ller and Strube, 2006) for annotation.
ample, following are lexical variations on the word
?tomorrow? from one cluster after filtering out other
words (most of which refer to days):
?2m?, ?2ma?, ?2mar?, ?2mara?, ?2maro?,
?2marrow?, ?2mor?, ?2mora?, ?2moro?, ?2mo-
row?, ?2morr?, ?2morro?, ?2morrow?, ?2moz?,
?2mr?, ?2mro?, ?2mrrw?, ?2mrw?, ?2mw?,
?tmmrw?, ?tmo?, ?tmoro?, ?tmorrow?, ?tmoz?,
?tmr?, ?tmro?, ?tmrow?, ?tmrrow?, ?tm-
rrw?, ?tmrw?, ?tmrww?, ?tmw?, ?tomaro?,
?tomarow?, ?tomarro?, ?tomarrow?, ?tomm?,
?tommarow?, ?tommarrow?, ?tommoro?, ?tom-
morow?, ?tommorrow?, ?tommorw?, ?tomm-
row?, ?tomo?, ?tomolo?, ?tomoro?, ?tomorow?,
?tomorro?, ?tomorrw?, ?tomoz?, ?tomrw?,
?tomz?
T-POS uses Conditional Random Fields5 (Laf-
ferty et al, 2001), both because of their ability to
model strong dependencies between adjacent POS
tags, and also to make use of highly correlated fea-
tures (for example a word?s identity in addition to
prefixes and suffixes). Besides employing the Brown
clusters computed above, we use a fairly standard set
of features that include POS dictionaries, spelling
and contextual features.
On a 4-fold cross validation over 800 tweets,
T-POS outperforms the Stanford tagger, obtaining a
26% reduction in error. In addition we include 40K
tokens of annotated IRC chat data (Forsythand and
Martell, 2007), which is similar in style. Like Twit-
ter, IRC data contains many misspelled/abbreviated
words, and also more pronouns, and interjections,
but fewer determiners than news. Finally, we also
leverage 50K POS-labeled tokens from the Penn
Treebank (Marcus et al, 1994).
Overall T-POS trained on 102K tokens (12K from
Twitter, 40K from IRC and 50K from PTB) results
in a 41% error reduction over the Stanford tagger,
obtaining an accuracy of 0.883. Table 3 lists gains
on some of the most common error types, for ex-
ample, T-POS dramatically reduces error on inter-
jections and verbs that are incorrectly classified as
nouns by the Stanford tagger.
2.2 Shallow Parsing
Shallow parsing, or chunking is the task of identi-
fying non-recursive phrases, such as noun phrases,
5We use MALLET (McCallum, 2002).
1526
Accuracy Error
Reduction
Majority Baseline (B-NP) 0.266 -
OpenNLP 0.839 -
T-CHUNK(CoNLL) 0.854 9%
T-CHUNK(Twitter) 0.867 17%
T-CHUNK(CoNLL + Twitter) 0.875 22%
Table 4: Token-Level accuracy at shallow parsing tweets.
We compare against the OpenNLP chunker as a baseline.
verb phrases, and prepositional phrases in text. Ac-
curate shallow parsing of tweets could benefit sev-
eral applications such as Information Extraction and
Named Entity Recognition.
Off the shelf shallow parsers perform noticeably
worse on tweets, motivating us again to annotate in-
domain training data. We annotate the same set of
800 tweets mentioned previously with tags from the
CoNLL shared task (Tjong Kim Sang and Buchholz,
2000). We use the set of shallow parsing features de-
scribed by Sha and Pereira (2003), in addition to the
Brown clusters mentioned above. Part-of-speech tag
features are extracted based on cross-validation out-
put predicted by T-POS. For inference and learning,
again we use Conditional Random Fields. We utilize
16K tokens of in-domain training data (using cross
validation), in addition to 210K tokens of newswire
text from the CoNLL dataset.
Table 4 reports T-CHUNK?s performance at shal-
low parsing of tweets. We compare against the off-
the shelf OpenNLP chunker6, obtaining a 22% re-
duction in error.
2.3 Capitalization
A key orthographic feature for recognizing named
entities is capitalization (Florian, 2002; Downey et
al., 2007). Unfortunately in tweets, capitalization
is much less reliable than in edited texts. In addi-
tion, there is a wide variety in the styles of capital-
ization. In some tweets capitalization is informative,
whereas in other cases, non-entity words are capital-
ized simply for emphasis. Some tweets contain all
lowercase words (8%), whereas others are in ALL
CAPS (0.6%).
To address this issue, it is helpful to incorporate
information based on the entire content of the mes-
6http://incubator.apache.org/opennlp/
P R F1
Majority Baseline 0.70 1.00 0.82
T-CAP 0.77 0.98 0.86
Table 5: Performance at predicting reliable capitalization.
sage to determine whether or not its capitalization
is informative. To this end, we build a capitaliza-
tion classifier, T-CAP, which predicts whether or not
a tweet is informatively capitalized. Its output is
used as a feature for Named Entity Recognition. We
manually labeled our 800 tweet corpus as having
either ?informative? or ?uninformative? capitaliza-
tion. The criteria we use for labeling is as follows:
if a tweet contains any non-entity words which are
capitalized, but do not begin a sentence, or it con-
tains any entities which are not capitalized, then its
capitalization is ?uninformative?, otherwise it is ?in-
formative?.
For learning , we use Support Vector Ma-
chines.7 The features used include: the frac-
tion of words in the tweet which are capitalized,
the fraction which appear in a dictionary of fre-
quently lowercase/capitalized words but are not low-
ercase/capitalized in the tweet, the number of times
the word ?I? appears lowercase and whether or not
the first word in the tweet is capitalized. Results
comparing against the majority baseline, which pre-
dicts capitalization is always informative, are shown
in Table 5. Additionally, in ?3 we show that fea-
tures based on our capitalization classifier improve
performance at named entity segmentation.
3 Named Entity Recognition
We now discuss our approach to named entity recog-
nition on Twitter data. As with POS tagging and
shallow parsing, off the shelf named-entity recog-
nizers perform poorly on tweets. For example, ap-
plying the Stanford Named Entity Recognizer to one
of the examples from Table 1 results in the following
output:
[Yess]ORG! [Yess]ORG! Its official
[Nintendo]LOC announced today that they
Will release the [Nintendo]ORG 3DS in north
[America]LOC march 27 for $250
7http://www.chasen.org/?taku/software/
TinySVM/
1527
The OOV word ?Yess? is mistaken as a named en-
tity. In addition, although the first occurrence of
?Nintendo? is correctly segmented, it is misclassi-
fied, whereas the second occurrence is improperly
segmented ? it should be the product ?Nintendo
3DS?. Finally ?north America? should be segmented
as a LOCATION, rather than just ?America?. In gen-
eral, news-trained Named Entity Recognizers seem
to rely heavily on capitalization, which we know to
be unreliable in tweets.
Following Collins and Singer (1999), Downey et
al. (2007) and Elsner et al (2009), we treat classi-
fication and segmentation of named entities as sepa-
rate tasks. This allows us to more easily apply tech-
niques better suited towards each task. For exam-
ple, we are able to use discriminative methods for
named entity segmentation and distantly supervised
approaches for classification. While it might be ben-
eficial to jointly model segmentation and (distantly
supervised) classification using a joint sequence la-
beling and topic model similar to that proposed by
Sauper et al (2010), we leave this for potential fu-
ture work.
Because most words found in tweets are not part
of an entity, we need a larger annotated dataset to ef-
fectively learn a model of named entities. We there-
fore use a randomly sampled set of 2,400 tweets for
NER. All experiments (Tables 6, 8-10) report results
using 4-fold cross validation.
3.1 Segmenting Named Entities
Because capitalization in Twitter is less informative
than news, in-domain data is needed to train models
which rely less heavily on capitalization, and also
are able to utilize features provided by T-CAP.
We exhaustively annotated our set of 2,400 tweets
(34K tokens) with named entities.8 A convention on
Twitter is to refer to other users using the @ sym-
bol followed by their unique username. We deliber-
ately choose not to annotate @usernames as entities
in our data set because they are both unambiguous,
and trivial to identify with 100% accuracy using a
simple regular expression, and would only serve to
inflate our performance statistics. While there is am-
biguity as to the type of @usernames (for example,
8We found that including out-of-domain training data from
the MUC competitions lowered performance at this task.
P R F1 F1 inc.
Stanford NER 0.62 0.35 0.44 -
T-SEG(None) 0.71 0.57 0.63 43%
T-SEG(T-POS) 0.70 0.60 0.65 48%
T-SEG(T-POS, T-CHUNK) 0.71 0.61 0.66 50%
T-SEG(All Features) 0.73 0.61 0.67 52%
Table 6: Performance at segmenting entities varying the
features used. ?None? removes POS, Chunk, and capital-
ization features. Overall we obtain a 52% improvement
in F1 score over the Stanford Named Entity Recognizer.
they can refer to people or companies), we believe
they could be more easily classified using features
of their associated user?s profile than contextual fea-
tures of the text.
T-SEG models Named Entity Segmentation as a
sequence-labeling task using IOB encoding for rep-
resenting segmentations (each word either begins, is
inside, or is outside of a named entity), and uses
Conditional Random Fields for learning and infer-
ence. Again we include orthographic, contextual
and dictionary features; our dictionaries included a
set of type lists gathered from Freebase. In addition,
we use the Brown clusters and outputs of T-POS,
T-CHUNK and T-CAP in generating features.
We report results at segmenting named entities in
Table 6. Compared with the state-of-the-art news-
trained Stanford Named Entity Recognizer (Finkel
et al, 2005), T-SEG obtains a 52% increase in F1
score.
3.2 Classifying Named Entities
Because Twitter contains many distinctive, and in-
frequent entity types, gathering sufficient training
data for named entity classification is a difficult task.
In any random sample of tweets, many types will
only occur a few times. Moreover, due to their
terse nature, individual tweets often do not contain
enough context to determine the type of the enti-
ties they contain. For example, consider following
tweet:
KKTNY in 45min..........
without any prior knowledge, there is not enough
context to determine what type of entity ?KKTNY?
refers to, however by exploiting redundancy in the
data (Downey et al, 2010), we can determine it is
likely a reference to a television show since it of-
1528
ten co-occurs with words such as watching and pre-
mieres in other contexts.9
In order to handle the problem of many infre-
quent types, we leverage large lists of entities and
their types gathered from an open-domain ontology
(Freebase) as a source of distant supervision, allow-
ing use of large amounts of unlabeled data in learn-
ing.
Freebase Baseline: Although Freebase has very
broad coverage, simply looking up entities and their
types is inadequate for classifying named entities in
context (0.38 F-score, ?3.2.1). For example, accord-
ing to Freebase, the mention ?China? could refer to
a country, a band, a person, or a film. This prob-
lem is very common: 35% of the entities in our data
appear in more than one of our (mutually exclusive)
Freebase dictionaries. Additionally, 30% of entities
mentioned on Twitter do not appear in any Freebase
dictionary, as they are either too new (for example a
newly released videogame), or are misspelled or ab-
breviated (for example ?mbp? is often used to refer
to the ?mac book pro?).
Distant Supervision with Topic Models: To
model unlabeled entities and their possible types, we
apply LabeledLDA (Ramage et al, 2009), constrain-
ing each entity?s distribution over topics based on
its set of possible types according to Freebase. In
contrast to previous weakly supervised approaches
to Named Entity Classification, for example the Co-
Training and Na??ve Bayes (EM) models of Collins
and Singer (1999), LabeledLDA models each entity
string as a mixture of types rather than using a single
hidden variable to represent the type of each men-
tion. This allows information about an entity?s dis-
tribution over types to be shared across mentions,
naturally handling ambiguous entity strings whose
mentions could refer to different types.
Each entity string in our data is associated with a
bag of words found within a context window around
all of its mentions, and also within the entity itself.
As in standard LDA (Blei et al, 2003), each bag of
words is associated with a distribution over topics,
Multinomial(?e), and each topic is associated with a
distribution over words, Multinomial(?t). In addi-
tion, there is a one-to-one mapping between topics
and Freebase type dictionaries. These dictionaries
9Kourtney & Kim Take New York.
constrain ?e, the distribution over topics for each en-
tity string, based on its set of possible types, FB[e].
For example, ?Amazon could correspond to a distribu-
tion over two types: COMPANY, and LOCATION,
whereas ?Apple might represent a distribution over
COMPANY, and FOOD. For entities which aren?t
found in any of the Freebase dictionaries, we leave
their topic distributions ?e unconstrained. Note that
in absence of any constraints LabeledLDA reduces
to standard LDA, and a fully unsupervised setting
similar to that presented by Elsner et. al. (2009).
In detail, the generative process that models our
data for Named Entity Classification is as follows:
for each type: t = 1 . . . T do
Generate ?t according to symmetric Dirichlet
distribution Dir(?).
end for
for each entity string e = 1 . . . |E| do
Generate ?e over FB[e] according to Dirichlet
distribution Dir(?FB[e]).
for each word position i = 1 . . . Ne do
Generate ze,i from Mult(?e).
Generate the word we,i from Mult(?ze,i).
end for
end for
To infer values for the hidden variables, we apply
Collapsed Gibbs sampling (Griffiths and Steyvers,
2004), where parameters are integrated out, and the
ze,is are sampled directly.
In making predictions, we found it beneficial to
consider ?traine as a prior distribution over types forentities which were encountered during training. In
practice this sharing of information across contexts
is very beneficial as there is often insufficient evi-
dence in an isolated tweet to determine an entity?s
type. For entities which weren?t encountered dur-
ing training, we instead use a prior based on the dis-
tribution of types across all entities. One approach
to classifying entities in context is to assume that
?traine is fixed, and that all of the words inside theentity mention and context, w, are drawn based on
a single topic, z, that is they are all drawn from
Multinomial(?z). We can then compute the poste-
rior distribution over types in closed form with a
simple application of Bayes rule:
P (z|w) ?
?
w?w
P (w|z : ?)P (z : ?traine )
During development, however, we found that rather
than making these assumptions, using Gibbs Sam-
1529
Type Top 20 Entities not found in Freebase dictionaries
PRODUCT nintendo ds lite, apple ipod, generation black, ipod nano, apple iphone, gb black, xperia, ipods, verizon
media, mac app store, kde, hd video, nokia n8, ipads, iphone/ipod, galaxy tab, samsung galaxy, playstation
portable, nintendo ds, vpn
TV-SHOW pretty little, american skins, nof, order svu, greys, kktny, rhobh, parks & recreation, parks & rec, dawson
?s creek, big fat gypsy weddings, big fat gypsy wedding, winter wipeout, jersey shores, idiot abroad, royle,
jerseyshore, mr . sunshine, hawaii five-0, new jersey shore
FACILITY voodoo lounge, grand ballroom, crash mansion, sullivan hall, memorial union, rogers arena, rockwood
music hall, amway center, el mocambo, madison square, bridgestone arena, cat club, le poisson rouge,
bryant park, mandalay bay, broadway bar, ritz carlton, mgm grand, olympia theatre, consol energy center
Table 7: Example type lists produced by LabeledLDA. No entities which are shown were found in Freebase; these are
typically either too new to have been added, or are misspelled/abbreviated (for example rhobh=?Real Housewives of
Beverly Hills?). In a few cases there are segmentation errors.
pling to estimate the posterior distribution over types
performs slightly better. In order to make predic-
tions, for each entity we use an informative Dirich-
let prior based on ?traine and perform 100 iterations of
Gibbs Sampling holding the hidden topic variables
in the training data fixed (Yao et al, 2009). Fewer
iterations are needed than in training since the type-
word distributions, ? have already been inferred.
3.2.1 Classification Experiments
To evaluate T-CLASS?s ability to classify entity
mentions in context, we annotated the 2,400 tweets
with 10 types which are both popular on Twitter,
and have good coverage in Freebase: PERSON,
GEO-LOCATION, COMPANY, PRODUCT, FACIL-
ITY, TV-SHOW, MOVIE, SPORTSTEAM, BAND,
and OTHER. Note that these type annotations are
only used for evaluation purposes, and not used dur-
ing training T-CLASS, which relies only on distant
supervision. In some cases, we combine multi-
ple Freebase types to create a dictionary of entities
representing a single type (for example the COM-
PANY dictionary contains Freebase types /busi-
ness/consumer company and /business/brand). Be-
cause our approach does not rely on any manually
labeled examples, it is straightforward to extend it
for a different sets of types based on the needs of
downstream applications.
Training: To gather unlabeled data for inference,
we run T-SEG, our entity segmenter (from ?3.1), on
60M tweets, and keep the entities which appear 100
or more times. This results in a set of 23,651 dis-
tinct entity strings. For each entity string, we col-
lect words occurring in a context window of 3 words
from all mentions in our data, and use a vocabulary
of the 100K most frequent words. We run Gibbs
sampling for 1,000 iterations, using the last sample
to estimate entity-type distributions ?e, in addition
to type-word distributions ?t. Table 7 displays the
20 entities (not found in Freebase) whose posterior
distribution ?e assigns highest probability to selected
types.
Results: Table 8 presents the classification re-
sults of T-CLASS compared against a majority base-
line which simply picks the most frequent class
(PERSON), in addition to the Freebase baseline,
which only makes predictions if an entity appears
in exactly one dictionary (i.e., appears unambigu-
ous). T-CLASS also outperforms a simple super-
vised baseline which applies a MaxEnt classifier us-
ing 4-fold cross validation over the 1,450 entities
which were annotated for testing. Additionally we
compare against the co-training algorithm of Collins
and Singer (1999) which also leverages unlabeled
data and uses our Freebase type lists; for seed rules
we use the ?unambiguous? Freebase entities. Our
results demonstrate that T-CLASS outperforms the
baselines and achieves a 25% increase in F1 score
over co-training.
Tables 9 and 10 present a breakdown of F1 scores
by type, both collapsing types into the standard
classes used in the MUC competitions (PERSON,
LOCATION, ORGANIZATION), and using the 10
popular Twitter types described earlier.
Entity Strings vs. Entity Mentions: DL-Cotrain
and LabeledLDA use two different representations
for the unlabeled data during learning. LabeledLDA
groups together words across all mentions of an en-
1530
System P R F1
Majority Baseline 0.30 0.30 0.30
Freebase Baseline 0.85 0.24 0.38
Supervised Baseline 0.45 0.44 0.45
DL-Cotrain 0.54 0.51 0.53
LabeledLDA 0.72 0.60 0.66
Table 8: Named Entity Classification performance on the
10 types. Assumes segmentation is given as in (Collins
and Singer, 1999), and (Elsner et al, 2009).
Type LL FB CT SP N
PERSON 0.82 0.48 0.65 0.83 436
LOCATION 0.74 0.21 0.55 0.67 372
ORGANIZATION 0.66 0.52 0.55 0.31 319
overall 0.75 0.39 0.59 0.49 1127
Table 9: F1 classification scores for the 3 MUC types
PERSON, LOCATION, ORGANIZATION. Results are
shown using LabeledLDA (LL), Freebase Baseline (FB),
DL-Cotrain (CT) and Supervised Baseline (SP). N is the
number of entities in the test set.
Type LL FB CT SP N
PERSON 0.82 0.48 0.65 0.86 436
GEO-LOC 0.77 0.23 0.60 0.51 269
COMPANY 0.71 0.66 0.50 0.29 162
FACILITY 0.37 0.07 0.14 0.34 103
PRODUCT 0.53 0.34 0.40 0.07 91
BAND 0.44 0.40 0.42 0.01 54
SPORTSTEAM 0.53 0.11 0.27 0.06 51
MOVIE 0.54 0.65 0.54 0.05 34
TV-SHOW 0.59 0.31 0.43 0.01 31
OTHER 0.52 0.14 0.40 0.23 219
overall 0.66 0.38 0.53 0.45 1450
Table 10: F1 scores for classification broken down by
type for LabeledLDA (LL), Freebase Baseline (FB), DL-
Cotrain (CT) and Supervised Baseline (SP). N is the num-
ber of entities in the test set.
P R F1
DL-Cotrain-entity 0.47 0.45 0.46
DL-Cotrain-mention 0.54 0.51 0.53
LabeledLDA-entity 0.73 0.60 0.66
LabeledLDA-mention 0.57 0.52 0.54
Table 11: Comparing LabeledLDA and DL-Cotrain
grouping unlabeled data by entities vs. mentions.
System P R F1
COTRAIN-NER (10 types) 0.55 0.33 0.41
T-NER(10 types) 0.65 0.42 0.51
COTRAIN-NER (PLO) 0.57 0.42 0.49
T-NER(PLO) 0.73 0.49 0.59
Stanford NER (PLO) 0.30 0.27 0.29
Table 12: Performance at predicting both segmentation
and classification. Systems labeled with PLO are evalu-
ated on the 3 MUC types PERSON, LOCATION, ORGA-
NIZATION.
tity string, and infers a distribution over its possi-
ble types, whereas DL-Cotrain considers the entity
mentions separately as unlabeled examples and pre-
dicts a type independently for each. In order to
ensure that the difference in performance between
LabeledLDA and DL-Cotrain is not simply due to
this difference in representation, we compare both
DL-Cotrain and LabeledLDA using both unlabeled
datasets (grouping words by all mentions vs. keep-
ing mentions separate) in Table 11. As expected,
DL-Cotrain performs poorly when the unlabeled ex-
amples group mentions; this makes sense, since Co-
Training uses a discriminative learning algorithm,
so when trained on entities and tested on individual
mentions, the performance decreases. Additionally,
LabeledLDA?s performance is poorer when consid-
ering mentions as ?documents?. This is likely due
to the fact that there isn?t enough context to effec-
tively learn topics when the ?documents? are very
short (typically fewer than 10 words).
End to End System: Finally we present the end
to end performance on segmentation and classifica-
tion (T-NER) in Table 12. We observe that T-NER
again outperforms co-training. Moreover, compar-
ing against the Stanford Named Entity Recognizer
on the 3 MUC types, T-NER doubles F1 score.
4 Related Work
There has been relatively little previous work on
building NLP tools for Twitter or similar text styles.
Locke and Martin (2009) train a classifier to recog-
nize named entities based on annotated Twitter data,
handling the types PERSON, LOCATION, and OR-
GANIZATION. Developed in parallel to our work,
Liu et al (2011) investigate NER on the same 3
types, in addition to PRODUCTs and present a semi-
1531
supervised approach using k-nearest neighbor. Also
developed in parallel, Gimpell et al (2011) build a
POS tagger for tweets using 20 coarse-grained tags.
Benson et. al. (2011) present a system which ex-
tracts artists and venues associated with musical per-
formances. Recent work (Han and Baldwin, 2011;
Gouws et al, 2011) has proposed lexical normaliza-
tion of tweets which may be useful as a preprocess-
ing step for the upstream tasks like POS tagging and
NER. In addition Finin et. al. (2010) investigate
the use of Amazon?s Mechanical Turk for annotat-
ing Named Entities in Twitter, Minkov et. al. (2005)
investigate person name recognizers in email, and
Singh et. al. (2010) apply a minimally supervised
approach to extracting entities from text advertise-
ments.
In contrast to previous work, we have demon-
strated the utility of features based on Twitter-
specific POS taggers and Shallow Parsers in seg-
menting Named Entities. In addition we take a dis-
tantly supervised approach to Named Entity Classi-
fication which exploits large dictionaries of entities
gathered from Freebase, requires no manually anno-
tated data, and as a result is able to handle a larger
number of types than previous work. Although we
found manually annotated data to be very beneficial
for named entity segmentation, we were motivated
to explore approaches that don?t rely on manual la-
bels for classification due to Twitter?s wide range of
named entity types. Additionally, unlike previous
work on NER in informal text, our approach allows
the sharing of information across an entity?s men-
tions which is quite beneficial due to Twitter?s terse
nature.
Previous work on Semantic Bootstrapping has
taken a weakly-supervised approach to classifying
named entities based on large amounts of unla-
beled text (Etzioni et al, 2005; Carlson et al, 2010;
Kozareva and Hovy, 2010; Talukdar and Pereira,
2010; McIntosh, 2010). In contrast, rather than
predicting which classes an entity belongs to (e.g.
a multi-label classification task), LabeledLDA esti-
mates a distribution over its types, which is then use-
ful as a prior when classifying mentions in context.
In addition there has been been work on Skip-
Chain CRFs (Sutton, 2004; Finkel et al, 2005)
which enforce consistency when classifying multi-
ple occurrences of an entity within a document. Us-
ing topic models (e.g. LabeledLDA) for classifying
named entities has a similar effect, in that informa-
tion about an entity?s distribution of possible types
is shared across its mentions.
5 Conclusions
We have demonstrated that existing tools for POS
tagging, Chunking and Named Entity Recognition
perform quite poorly when applied to Tweets. To
address this challenge we have annotated tweets and
built tools trained on unlabeled, in-domain and out-
of-domain data, showing substantial improvement
over their state-of-the art news-trained counterparts,
for example, T-POS outperforms the Stanford POS
Tagger, reducing error by 41%. Additionally we
have shown the benefits of features generated from
T-POS and T-CHUNK in segmenting Named Entities.
We identified named entity classification as a par-
ticularly challenging task on Twitter. Due to their
terse nature, tweets often lack enough context to
identify the types of the entities they contain. In ad-
dition, a plethora of distinctive named entity types
are present, necessitating large amounts of training
data. To address both these issues we have presented
and evaluated a distantly supervised approach based
on LabeledLDA, which obtains a 25% increase in F1
score over the co-training approach to Named En-
tity Classification suggested by Collins and Singer
(1999) when applied to Twitter.
Our POS tagger, Chunker Named Entity Rec-
ognizer are available for use by the research
community: http://github.com/aritter/
twitter_nlp
Acknowledgments
We would like to thank Stephen Soderland, Dan
Weld and Luke Zettlemoyer, in addition to the
anonymous reviewers for helpful comments on a
previous draft. This research was supported in part
by NSF grant IIS-0803481, ONR grant N00014-11-
1-0294, Navy STTR contract N00014-10-M-0304, a
National Defense Science and Engineering Graduate
(NDSEG) Fellowship 32 CFR 168a and carried out
at the University of Washington?s Turing Center.
1532
References
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In The
49th Annual Meeting of the Association for Computa-
tional Linguistics, Portland, Oregon, USA. To appear.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn. Res.
Avrim Blum and Tom M. Mitchell. 1998. Combining
labeled and unlabeled sata with co-training. In COLT,
pages 92?100.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist.
Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-
tevam R. Hruschka, Jr., and Tom M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of the third ACM interna-
tional conference on Web search and data mining,
WSDM ?10.
Eugene Charniak, Curtis Hendrickson, Neil Jacobson,
and Mike Perkowitz. 1993. Equations for part-of-
speech tagging. In AAAI, pages 784?789.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Empirical
Methods in Natural Language Processing.
Doug Downey, Matthew Broadhead, and Oren Etzioni.
2007. Locating complex named entities in web text.
In Proceedings of the 20th international joint confer-
ence on Artifical intelligence.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2010. Analysis of a probabilistic model of redundancy
in unsupervised information extraction. Artif. Intell.,
174(11):726?748.
Micha Elsner, Eugene Charniak, and Mark Johnson.
2009. Structured generative models for unsupervised
named-entity clustering. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, NAACL ?09.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: an ex-
perimental study. Artif. Intell.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in Twitter data with crowd-
sourcing. In Proceedings of the NAACL Workshop on
Creating Speech and Text Language Data With Ama-
zon?s Mechanical Turk. Association for Computational
Linguistics, June.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, ACL ?05.
Radu Florian. 2002. Named entity recognition as a house
of cards: classifier stacking. In Proceedings of the 6th
conference on Natural language learning - Volume 20,
COLING-02.
Eric N. Forsythand and Craig H. Martell. 2007. Lexical
and discourse analysis of online chat dialog. In Pro-
ceedings of the International Conference on Semantic
Computing.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for twit-
ter: Annotation, features, and experiments. In ACL.
Joshua T. Goodman. 2001. A bit of progress in language
modeling. Technical report, Microsoft Research.
Stephan Gouws, Donald Metzler, Congxing Cai, and Ed-
uard Hovy. 2011. Contextual bearing on linguistic
variation in social media. In ACL Workshop on Lan-
guage in Social Media, Portland, Oregon, USA. To
appear.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, April.
Mark Hachman. 2011. Humanity?s tweets: Just 20 ter-
abytes. In PCMAG.COM.
Bo Han and Timothy Baldwin. 2011. Lexical normalisa-
tion of short text messages: Makn sens a #twitter. In
The 49th Annual Meeting of the Association for Com-
putational Linguistics, Portland, Oregon, USA. To ap-
pear.
Catherine Kobus, Franc?ois Yvon, and Ge?raldine
Damnati. 2008. Normalizing sms: are two metaphors
better than one ? In COLING, pages 441?448.
Zornitsa Kozareva and Eduard H. Hovy. 2010. Not all
seeds are equal: Measuring the quality of text mining
seeds. In HLT-NAACL.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011. Recognizing named entities in tweets.
In ACL.
Brian Locke and James Martin. 2009. Named entity
recognition: Adapting to microblogging. In Senior
Thesis, University of Colorado.
1533
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics.
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit. In http://mallet.
cs.umass.edu.
Tara McIntosh. 2010. Unsupervised discovery of nega-
tive categories in lexicon bootstrapping. In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, EMNLP ?10.
Einat Minkov, Richard C. Wang, and William W. Cohen.
2005. Extracting personal names from email: apply-
ing named entity recognition to informal text. In Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, HLT ?05, pages 443?450, Morristown, NJ,
USA. Association for Computational Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of ACL-IJCNLP
2009.
Christoph Mu?ller and Michael Strube. 2006. Multi-level
annotation of linguistic data with MMAX2. In Sabine
Braun, Kurt Kohn, and Joybrato Mukherjee, editors,
Corpus Technology and Language Pedagogy: New Re-
sources, New Tools, New Methods, pages 197?214. Pe-
ter Lang, Frankfurt a.M., Germany.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled lda: a super-
vised topic model for credit attribution in multi-labeled
corpora. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 1 - Volume 1, EMNLP ?09, pages 248?256,
Morristown, NJ, USA. Association for Computational
Linguistics.
Christina Sauper, Aria Haghighi, and Regina Barzilay.
2010. Incorporating content structure into text anal-
ysis applications. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 377?387, Morristown,
NJ, USA. Association for Computational Linguistics.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology - Volume 1, NAACL ?03.
Sameer Singh, Dustin Hillard, and Chris Leggetter. 2010.
Minimally-supervised extraction of entities from text
advertisements. In Human Language Technologies:
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT).
Charles Sutton. 2004. Collective segmentation and la-
beling of distant entities in information extraction.
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learning
methods for class-instance acquisition. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1473?1481. Associ-
ation for Computational Linguistics.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the conll-2000 shared task: chunking.
In Proceedings of the 2nd workshop on Learning lan-
guage in logic and the 4th conference on Computa-
tional natural language learning - Volume 7, ConLL
?00.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ?10.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference on
streaming document collections. In Proceedings of
the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd annual meeting on Association for
Computational Linguistics, ACL ?95.
1534
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 523?534, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Open Language Learning for Information Extraction
Mausam, Michael Schmitz, Robert Bart, Stephen Soderland, and Oren Etzioni
Turing Center
Department of Computer Science and Engineering
University of Washington, Seattle
{mausam,schmmd,rbart,soderlan,etzioni}@cs.washington.edu
Abstract
Open Information Extraction (IE) systems ex-
tract relational tuples from text, without re-
quiring a pre-specified vocabulary, by iden-
tifying relation phrases and associated argu-
ments in arbitrary sentences. However, state-
of-the-art Open IE systems such as REVERB
and WOE share two important weaknesses ?
(1) they extract only relations that are medi-
ated by verbs, and (2) they ignore context,
thus extracting tuples that are not asserted as
factual. This paper presents OLLIE, a sub-
stantially improved Open IE system that ad-
dresses both these limitations. First, OLLIE
achieves high yield by extracting relations me-
diated by nouns, adjectives, and more. Sec-
ond, a context-analysis step increases preci-
sion by including contextual information from
the sentence in the extractions. OLLIE obtains
2.7 times the area under precision-yield curve
(AUC) compared to REVERB and 1.9 times
the AUC of WOEparse.
1 Introduction
While traditional Information Extraction (IE)
(ARPA, 1991; ARPA, 1998) focused on identifying
and extracting specific relations of interest, there
has been great interest in scaling IE to a broader
set of relations and to far larger corpora (Banko et
al., 2007; Hoffmann et al2010; Mintz et al2009;
Carlson et al2010; Fader et al2011). However,
the requirement of having pre-specified relations of
interest is a significant obstacle. Imagine an intel-
ligence analyst who recently acquired a terrorist?s
laptop or a news reader who wishes to keep abreast
of important events. The substantial endeavor in
1. ?After winning the Superbowl, the Saints are now
the top dogs of the NFL.?
O: (the Saints; win; the Superbowl)
2. ?There are plenty of taxis available at Bali airport.?
O: (taxis; be available at; Bali airport)
3. ?Microsoft co-founder Bill Gates spoke at ...?
O: (Bill Gates; be co-founder of; Microsoft)
4. ?Early astronomers believed that the earth is the
center of the universe.?
R: (the earth; be the center of; the universe)
W: (the earth; be; the center of the universe)
O: ((the earth; be the center of; the universe)
AttributedTo believe; Early astronomers)
5. ?If he wins five key states, Romney will be elected
President.?
R,W: (Romney; will be elected; President)
O: ((Romney; will be elected; President)
ClausalModifier if; he wins five key states)
Figure 1: OLLIE (O) has a wider syntactic range and finds
extractions for the first three sentences where REVERB
(R) and WOEparse (W) find none. For sentences #4,5,
REVERB and WOEparse have an incorrect extraction by
ignoring the context that OLLIE explicitly represents.
analyzing their corpus is the discovery of important
relations, which are likely not pre-specified. Open
IE (Banko et al2007) is the state-of-the-art
approach for such scenarios.
However, the state-of-the-art Open IE systems,
REVERB (Fader et al2011; Etzioni et al2011)
and WOEparse (Wu and Weld, 2010) suffer from two
key drawbacks. Firstly, they handle a limited sub-
set of sentence constructions for expressing relation-
ships. Both extract only relations that are mediated
by verbs, and REVERB further restricts this to a sub-
set of verbal patterns. This misses important infor-
mation mediated via other syntactic entities such as
nouns and adjectives, as well as a wider range of
verbal structures (examples #1-3 in Figure 1).
523
Secondly, REVERB and WOEparse perform only
a local analysis of a sentence, so they often extract
relations that are not asserted as factual in the sen-
tence (examples #4,5). This often occurs when the
relation is within a belief, attribution, hypothetical
or other conditional context.
In this paper we present OLLIE (Open Language
Learning for Information Extraction), 1 our novel
Open IE system that overcomes the limitations of
previous Open IE by (1) expanding the syntactic
scope of relation phrases to cover a much larger
number of relation expressions, and (2) expand-
ing the Open IE representation to allow additional
context information such as attribution and clausal
modifiers. OLLIE extractions obtain a dramatically
higher yield at higher or comparable precision rela-
tive to existing systems.
The outline of the paper is as follows. First, we
provide background on Open IE and how it relates
to Semantic Role Labeling (SRL). Section 3 de-
scribes the syntactic scope expansion component,
which is based on a novel approach that learns open
pattern templates. These are relation-independent
dependency parse-tree patterns that are automati-
cally learned using a novel bootstrapped training set.
Section 4 discusses the context analysis component,
which is based on supervised training with linguistic
and lexical features.
Section 5 compares OLLIE with REVERB and
WOEparse on a dataset from three domains: News,
Wikipedia, and a Biology textbook. We find that
OLLIE obtains 2.7 times the area in precision-yield
curves (AUC) as REVERB and 1.9 times the AUC
as WOEparse. Moreover, for specific relations com-
monly mediated by nouns (e.g., ?is the president
of?) OLLIE obtains two order of magnitude higher
yield. We also compare OLLIE to a state-of-the-art
SRL system (Johansson and Nugues, 2008) on an
IE-related end task and find that they both have com-
parable performance at argument identification and
have complimentary strengths in sentence analysis.
In Section 6 we discuss related work on pattern-
based relation extraction.
2 Background
Open IE systems extract tuples consisting of argu-
ment phrases from the input sentence and a phrase
1Available for download at http://openie.cs.washington.edu
from the sentence that expresses a relation between
the arguments, in the format (arg1; rel; arg2). This is
done without a pre-specified set of relations and with
no domain-specific knowledge engineering. We
compare OLLIE to two state-of-the-art Open IE sys-
tems: (1) REVERB (Fader et al2011), which
uses shallow syntactic processing to identify rela-
tion phrases that begin with a verb and occur be-
tween the argument phrases;2 (2) WOEparse (Wu
and Weld, 2010), which uses bootstrapping from en-
tries in Wikipedia info-boxes to learn extraction pat-
terns in dependency parses. Like REVERB, the
relation phrases begin with verbs, but can handle
long-range dependencies and relation phrases that
do not come between the arguments. Unlike RE-
VERB, WOE does not include nouns within the re-
lation phrases (e.g., cannot represent ?is the presi-
dent of? relation phrase). Both systems ignore con-
text around the extracted relations that may indi-
cate whether it is a supposition or conditionally true
rather than asserted as factual (see #4-5 in Figure 1).
The task of Semantic role labeling is to identify
arguments of verbs in a sentence, and then to clas-
sify the arguments by mapping the verb to a se-
mantic frame and mapping the argument phrases to
roles in that frame, such as agent, patient, instru-
ment, or benefactive. SRL systems can also identify
and classify arguments of relations that are mediated
by nouns when trained on NomBank annotations.
Where SRL begins with a verb or noun and then
looks for arguments that play roles with respect to
that verb or noun, Open IE looks for a phrase that ex-
presses a relation between a pair of arguments. That
phrase is often more than simply a single verb, such
as the phrase ?plays a role in?, or ?is the CEO of?.
3 Relational Extraction in OLLIE
Figure 2 illustrates OLLIE?s architecture for learning
and applying binary extraction patterns. First, it uses
a set of high precision seed tuples from REVERB to
bootstrap a large training set. Second, it learns open
pattern templates over this training set. Next, OLLIE
applies these pattern templates at extraction time.
This section describes these three steps in detail. Fi-
nally, OLLIE analyzes the context around the tuple
(Section 4) to add information (attribution, clausal
modifiers) and a confidence function.
2Available for download at http://reverb.cs.washington.edu/
524
ReVerb 
Seed Tuples 
Training Data 
Open Pattern Learning Bootstrapper 
Pattern Templates 
Pattern Matching Context Analysis Sentence Tuples Ext. Tuples 
Extraction 
Learning 
Figure 2: System architecture: OLLIE begins with seed
tuples from REVERB, uses them to build a bootstrap
training set, and learns open pattern templates. These are
applied to individual sentences at extraction time.
3.1 Constructing a Bootstrapping Set
Our goal is to automatically create a large training
set, which encapsulates the multitudes of ways in
which information is expressed in text. The key ob-
servation is that almost every relation can also be ex-
pressed via a REVERB-style verb-based expression.
So, bootstrapping sentences based on REVERB?s tu-
ples will likely capture all relation expressions.
We start with over 110,000 seed tuples ? these are
high confidence REVERB extractions from a large
Web corpus (ClueWeb)3 that are asserted at least
twice and contain only proper nouns in the argu-
ments. These restrictions reduce ambiguity while
still covering a broad range of relations. For ex-
ample, a seed tuple may be (Paul Annacone; is the
coach of; Federer) that REVERB extracts from the
sentence ?Paul Annacone is the coach of Federer.?
For each seed tuple, we retrieve all sentences in a
Web corpus that contains all content words in the
tuple. We obtain a total of 18 million sentences.
For our example, we will retrieve all sentences that
contain ?Federer?, ?Paul?, ?Annacone? and some syn-
tactic variation of ?coach?. We may find sentences
like ?Now coached by Annacone, Federer is win-
ning more titles than ever.?
Our bootstrapping hypothesis assumes that all
these sentences express the information of the orig-
inal seed tuple. This hypothesis is not always true.
As an example, for a seed tuple (Boyle; is born in;
Ireland) we may retrieve a sentence ?Felix G. Whar-
ton was born in Donegal, in the northwest of Ireland,
a county where the Boyles did their schooling.?
3http://lemurproject.org/clueweb09.php/
To reduce bootstrapping errors we enforce addi-
tional dependency restrictions on the sentences. We
only allow sentences where the content words from
arguments and relation can be linked to each other
via a linear path of size four in the dependency parse.
To implement this restriction, we only use the sub-
set of content words that are headwords in the parse
tree. In the above sentence ?Ireland?, ?Boyle? and
?born? connect via a dependency path of length six,
and hence this sentence is rejected from the training
set. This reduces our set to 4 million (seed tuple,
sentence) pairs.
In our implementation, we use Malt Dependency
Parser (Nivre and Nilsson, 2004) for dependency
parsing, since it is fast and hence, easily applica-
ble to a large corpus of sentences. We post-process
the parses using Stanford?s CCprocessed algorithm,
which compacts the parse structure for easier extrac-
tion (de Marneffe et al2006).
We randomly sampled 100 sentences from our
bootstrapping set and found that 90 of them sat-
isfy our bootstrapping hypothesis (64 without de-
pendency constraints). We find this quality to be sat-
isfactory for our needs of learning general patterns.
Bootstrapped data has been previously used to
generate positive training data for IE (Hoffmann et
al., 2010; Mintz et al2009). However, previous
systems retrieved sentences that only matched the
two arguments, which is error-prone, since multiple
relations can hold between a pair of entities (e.g.,
Bill Gates is the CEO of, a co-founder of, and has a
high stake in Microsoft).
Alternatively, researchers have developed sophis-
ticated probabilistic models to alleviate the effect
of noisy data (Riedel et al2010; Hoffmann et al
2011). In our case, by enforcing that a sentence ad-
ditionally contains some syntactic form of the rela-
tion content words, our bootstrapping set is naturally
much cleaner.
Moreover, this form of bootstrapping is better
suited for Open IE?s needs, as we will use this data
to generalize to other unseen relations. Since the
relation words in the sentence and seed match, we
can learn general pattern templates that may apply
to other relations too. We discuss this process next.
3.2 Open Pattern Learning
OLLIE?s next step is to learn general patterns that
encode various ways of expressing relations. OL-
525
Extraction Template Open Pattern
1. (arg1; be {rel} {prep}; arg2) {arg1} ?nsubjpass? {rel:postag=VBN} ?{prep ?}? {arg2}
2. (arg1; {rel}; arg2) {arg1} ?nsubj? {rel:postag=VBD} ?dobj? {arg2}
3. (arg1; be {rel} by; arg2) {arg1} ?nsubjpass? {rel:postag=VBN} ?agent? {arg2}
4. (arg1; be {rel} of; arg2) {rel:postag=NN;type=Person} ?nn? {arg1} ?nn? {arg2}
5. (arg1; be {rel} {prep}; arg2) {arg1} ?nsubjpass? {slot:postag=VBN;lex ?announce|name|choose...}
?dobj? {rel:postag=NN} ?{prep ?}? {arg2}
Figure 3: Sample open pattern templates. Notice that some patterns (1-3) are purely syntactic, and others are seman-
tic/lexically constrained (in bold font). A dependency parse that matches pattern #1 is shown in Figure 4.
LIE learns open pattern templates ? a mapping from
a dependency path to an open extraction, i.e., one
that identifies both the arguments and the exact
(REVERB-style) relation phrase. Figure 3 gives ex-
amples of high-frequency pattern templates learned
by OLLIE. Note that some of the dependency
paths are completely unlexicalized (#1-3), whereas
in other cases some nodes have lexical or semantic
restrictions (#4, 5).
Open pattern templates encode the ways in
which a relation (in the first column) may
be expressed in a sentence (second column).
For example, a relation (Godse; kill; Gandhi)
may be expressed with a dependency path (#2)
{Godse}?nsubj?{kill:postag=VBD}?dobj?{Gandhi}.
To learn the pattern templates, we first extract the
dependency path connecting the arguments and re-
lation words for each seed tuple and the associated
sentence. We annotate the relation node in the path
with the exact relation word (as a lexical constraint)
and the POS (postag constraint). We create a re-
lation template from the seed tuple by normalizing
?is?/?was?/?will be? to ?be?, and replacing the rela-
tion content word with {rel}.4
If the dependency path has a node that is not part
of the seed tuple, we call it a slot node. Intuitively,
if slot words do not negate the tuple they can be
skipped over. As an example, ?hired? is a slot word
for the tuple (Annacone; is the coach of; Federer) in
the sentence ?Federer hired Annacone as a coach?.
We associate postag and lexical constraints with the
slot node as well. (see #5 in Figure 3).
Next, we perform several syntactic checks on
each candidate pattern. These checks are the con-
straints that we found to hold in very general pat-
terns, which we can safely generalize to other un-
seen relations. The checks are: (1) There are no slot
4Our current implementation only allows a single relation
content word; extending to multiple words is straightforward ?
the templates will require rel1, rel2,. . .
nodes in the path. (2) The relation node is in the
middle of arg1 and arg2. (3) The preposition edge
(if any) in the pattern matches the preposition in the
relation. (4) The path has no nn or amod edges.
If the checks hold true we accept it as a purely
syntactic pattern with no lexical constraints. Oth-
ers are semantic/lexical patterns and require further
constraints to be reliable as extraction patterns.
3.2.1 Purely Syntactic Patterns
For syntactic patterns, we aggressively general-
ize to unseen relations and prepositions. We remove
all lexical restrictions from the relation nodes. We
convert all preposition edges to an abstract {prep ?}
edge. We also replace the specific prepositions in
extraction templates with {prep}.
As an example, consider the sentences, ?Michael
Webb appeared on Oprah...? and ?...when Alexan-
der the Great advanced to Babylon.? and associ-
ated seed tuples (Michael Webb; appear on; Oprah)
and (Alexander; advance to; Babylon). Both these
data points return the same open pattern after gen-
eralization: ?{arg1} ?nsubj? {rel:postag=VBD}
?{prep ?}? {arg2}? with the extraction template
(arg1, {rel} {prep}, arg2). Other examples of syn-
tactic pattern templates are #1-3 in Figure 3.
3.2.2 Semantic/Lexical Patterns
Patterns that do not satisfy the checks are not as
general as those that do, but are still important. Con-
structions like ?Microsoft co-founder Bill Gates...?
work for some relation words (e.g., founder, CEO,
director, president, etc.) but would not work for
other nouns; for instance, from ?Chicago Symphony
Orchestra? we should not conclude that (Orchestra;
is the Symphony of; Chicago).
Similarly, we may conclude (Annacone; is the
coach of; Federer) from the sentence ?Federer hired
Annacone as a coach.?, but this depends on the se-
mantics of the slot word, ?hired?. If we replaced
526
?hired? by ?fired? or ?considered? then the extraction
would be false.
To enable such patterns we retain the lexical con-
straints on the relation words and slot words.5 We
collect all patterns together based only on the syn-
tactic restrictions and convert the lexical constraint
into a list of words with which the pattern was seen.
Example #5 in Figure 3 shows one such lexical list.
Can we generalize these lexically-annotated pat-
terns further? Our insight is that we can generalize
a list of lexical items to other similar words. For
example, if we see a list like {CEO, director, presi-
dent, founder}, then we should be able to generalize
to ?chairman? or ?minister?.
Several ways to compute semantically similar
words have been suggested in the literature like
Wordnet-based, distributional similarity, etc. (e.g.,
(Resnik, 1996; Dagan et al1999; Ritter et al
2010)). For our proof of concept, we use a simple
overlap metric with two important Wordnet classes
? Person and Location. We generalize to these types
when our list has a high overlap (> 75%) with hy-
ponyms of these classes. If not, we simply retain the
original lexical list without generalization. Example
#4 in Figure 3 is a type-generalized pattern.
We combine all syntactic and semantic patterns
and sort in descending order based on frequency of
occurrence in the training set. This imposes a natural
ranking on the patterns ? more frequent patterns are
likely to give higher precision extractions.
3.3 Pattern Matching for Extraction
We now describe how these open patterns are used
to extract binary relations from a new sentence. We
first match the open patterns with the dependency
parse of the sentence and identify the base nodes for
arguments and relations. We then expand these to
convey all the information relevant to the extraction.
As an example, consider the sentence: ?I learned
that the 2012 Sasquatch music festival is scheduled
for May 25th until May 28th.? Figure 4 illustrates the
dependency parse. To apply pattern #1 from Figure
3 we first match arg1 to ?festival?, rel to ?scheduled?
and arg2 to ?25th? with prep ?for?. However, (festi-
val, be scheduled for, 25th) is not a very meaningful
extraction. We need to expand this further.
5For highest precision extractions, we may also need seman-
tic constraints on the arguments. In this work, we increase our
yield by ignoring the argument-type constraints.
learned_VBD 
I_PRP scheduled_VBN 
that_IN festival_NN is_VBZ 25th_NNP 28th_NNP 
the_DET Sasquatch_NNP music_NN May_NNP_11 May_NNP_14 2012_CD 
nsubj ccomp 
complm nsubjpass auxpass prep_for prep_until 
det num nn nn nn nn 
Figure 4: A sample dependency parse. The col-
ored/greyed nodes represent all words that are extracted
from the pattern {arg1} ?nsubjpass? {rel:postag=VBN}
?{prep ?}? {arg2}. The extraction is (the 2012
Sasquatch Music Festival; is scheduled for; May 25th).
For the arguments we expand on amod, nn, det,
neg, prep of, num, quantmod edges to build the
noun-phrase. When the base noun is not a proper
noun, we also expand on rcmod, infmod, partmod,
ref, prepc of edges, since these are relative clauses
that convey important information. For relation
phrases, we expand on advmod, mod, aux, auxpass,
cop, prt edges. We also include dobj and iobj in the
case that they are not in an argument. After identi-
fying the words in arg/relation we choose their order
as in the original sentence. For example, these rules
will result in the extraction (the Sasquatch music fes-
tival; be scheduled for; May 25th).
3.4 Comparison with WOEparse
OLLIE?s algorithm is similar to that of WOEparse
? both systems follow the basic structure of boot-
strap learning of patterns based on dependency parse
paths. However, there are three significant differ-
ences. WOE uses Wikipedia-based bootstrapping,
finding a sentence in a Wikipedia article that con-
tains the infobox values. Since WOE does not have
access to a seed relation phrase, it heuristically as-
signs all intervening words between the arguments
in the parse as the relation phrase. This often results
in under-specified or nonsensical relation phrases.
For example, from the sentence ?David Miscavige
learned that after Tom Cruise divorced Mimi Rogers,
he was pursuing Nicole Kidman.? WOE?s heuristics
will extract the relation divorced was pursuing be-
tween ?Tom Cruise? and ?Nicole Kidman?. OLLIE,
in contrast, produces well-formed relation phrases
by basing its templates on REVERB relation phrases.
Secondly, WOE does not assign semantic/lexical
restrictions to its patterns, and thus, has lower preci-
sion due to aggressive syntactic generalization. Fi-
nally, WOE is designed to have verb-mediated rela-
527
tion phrases that do not include nouns, thus missing
important relations such as ?is the president of?. In
our experiments (see Figure 5) we find WOEparse to
have lower precision and yield than OLLIE.
4 Context Analysis in OLLIE
We now turn to the context analysis component,
which handles the problem of extractions that are not
asserted as factual in the text. In some cases, OLLIE
can handle this by extending the tuple representation
with an extra field that turns an otherwise incorrect
tuple into a correct one. In other cases, there is no re-
liable way to salvage the extraction, and OLLIE can
avoid an error by giving the tuple a low confidence.
Cases where OLLIE extends the tuple representa-
tion include conditional truth and attribution. Con-
sider sentence #4 in Figure 1. It is not asserting that
the earth is the center of the universe. OLLIE adds
an AttributedTo field, which makes the final extrac-
tion valid (see OLLIE extraction in Figure 1). This
field indicates who said, suggested, believes, hopes,
or doubts the information in the main extraction.
Another case is when the extraction is only condi-
tionally true. Sentence #5 in Figure 1 does not assert
as factual that (Romney; will be elected; President),
so it is an incorrect extraction. However, adding
a condition (?if he wins five states?) can turn this
into a correct extraction. We extend OLLIE to have
a ClausalModifier field when there is a dependent
clause that modifies the main extraction.
Our approach for extracting these additional fields
makes use of the dependency parse structure. We
find that attributions are marked by a ccomp (clausal
complement) edge. For example, in the parse of sen-
tence #4 there is a ccomp edge between ?believe?
and ?center?. Our algorithm first checks for the pres-
ence of a ccomp edge to the relation node. However,
not all ccomp edges are attributions. We match the
context verb (e.g., ?believe?) with a list of commu-
nication and cognition verbs from VerbNet (Schuler,
2006) to detect attributions. The context verb and its
subject then populate the AttributedTo field.
Similarly, the clausal modifiers are marked by ad-
vcl (adverbial clause) edge. We filter these lexically,
and add a ClausalModifier field when the first word
of the clause matches a list of 16 terms created using
a training set: {if, when, although, because, ...}.
OLLIE has high precision for AttributedTo and
ClausalModifier fields, nearly 98% on a develop-
ment set, however, these two fields do not cover all
the cases where an extraction is not asserted as fac-
tual. To handle others, we train OLLIE?s confidence
function to reduce the confidence of an extraction if
its context indicates it is likely to be non-factual.
We use a supervised logistic regression classifier
for the confidence function. Features include the
frequency of the extraction pattern, the presence of
AttributedTo or ClausalModifier fields, and the po-
sition of certain words in the extraction?s context,
such as function words or the communication and
cognition verbs used for the AttributedTo field. For
example, one highly predictive feature tests whether
or not the word ?if? comes before the extraction
when no ClausalModifier fields are attached. Our
training set was 1000 extractions drawn evenly from
Wikipedia, News, and Biology sentences.
5 Experiments
Our experiments evaluate three main questions. (1)
How does OLLIE?s performance compare with exis-
ting state-of-the-art open extractors? (2) What are
the contributions of the different sub-components
within OLLIE? (3) How do OLLIE?s extractions com-
pare with semantic role labeling argument identifi-
cation?
5.1 Comparison of Open IE Systems
Since Open IE is designed to handle a variety of
domains, we create a dataset of 300 random sen-
tences from three sources: News, Wikipedia and Bi-
ology textbook. The News and Wikipedia test sets
are a random subset of Wu and Weld?s test set for
WOEparse. We ran three systems, OLLIE, REVERB
and WOEparse on this dataset resulting in a total of
1,945 extractions from all three systems. Two an-
notators tagged the extractions as correct if the sen-
tence asserted or implied that the relation was true.
Inter-annotator agreement was 0.96, and we retained
the subset of extractions on which the two annotators
agree for further analysis.
All systems associate a confidence value with an
extraction ? ranking with these confidence values
generates a precision-yield curve for this dataset.
Figure 5 reports the curves for the three systems.
We find that OLLIE has a higher performance, ow-
ing primarily to its higher yield at comparable preci-
528
0.5  
0.6  
0.7  
0.8  
0.9  
1  
0  100  200  300  400  500  600  
OLLIE  
ReVerb 
WOE  
Yield 
Pre
cis
ion
 
parse 
Figure 5: Comparison of different Open IE systems. OL-
LIE achieves substantially larger area under the curve
than other Open IE systems.
sion. OLLIE finds 4.4 times more correct extractions
than REVERB and 4.8 times more than WOEparse at
a precision of about 0.75. Overall, OLLIE has 2.7
times larger area under the curve than REVERB and
1.9 times larger than WOEparse.6 We use the Boot-
strap test (Cohen, 1995) to find that OLLIE?s better
performance compared to the two systems is highly
statistically significant.
We perform further analysis to understand the rea-
sons behind the high yield from OLLIE. We find that
40% of the OLLIE extractions that REVERB misses
are due to OLLIE?s use of parsers ? REVERB misses
those because its shallow syntactic analysis cannot
skip over the intervening clauses or prepositional
phrases between the relation phrase and the argu-
ments. About 30% of the additional yield is those
extractions where the relation is not between its ar-
guments (see instance #1 in Figure 1). The rest are
due to other causes such as OLLIE?s ability to handle
relationships mediated by nouns and adjectives, or
REVERB?s shallow syntactic analysis, etc. In con-
trast, OLLIE misses very few extractions returned by
REVERB, mostly due to parser errors.
We find that WOEparse misses extractions found
by OLLIE for a variety of reasons. The primary
cause is that WOEparse does not include nouns in re-
lation phrases. It also misses some verb-based pat-
terns, probably due to training noise. In other cases,
WOEparse misses extractions due to ill-formed rela-
tion phrases (as in the example of Section 3.4: ?di-
vorced was pursuing? instead of the correct relation
?was pursuing?).
While the bulk of OLLIE?s extractions in our test
6Evaluating recall is difficult at this scale ? however, since
yield is proportional to recall, the area differences also hold for
the equivalent precision-recall curves.
Relation OLLIE REVERB incr.
is capital of 8,566 146 59x
is president of 21,306 1,970 11x
is professor at 8,334 400 21x
is scientist of 730 5 146x
Figure 6: OLLIE finds many more correct extractions for
relations that are typically expressed by noun phrases ?
up to 146 times that of REVERB. WOEparse outputs no
instances of these, because it does not allow nouns in the
relation. These results are at point of maximum yield
(with comparable precisions around 0.66).
set were verb-mediated, our intuition suggests that
there exist many relationships that are most natu-
rally expressed via noun phrases. To demonstrate
this effect, we chose four such relations ? is capi-
tal of, is president of, is professor at, and is scientist
of. We ran our systems on 100 million random sen-
tences from the ClueWeb corpus. Figure 6 reports
the yields of these four relations.7
OLLIE found up to 146 times as many extrac-
tions for these relations than REVERB. Because
WOEparse does not include nouns in relation phrases,
it is unable to extract any instance of these relations.
We examine a sample of the extractions to verify that
noun-mediated extractions are the main reason for
this large yield boost over REVERB (73% of OLLIE
extractions were noun-mediated). High-frequency
noun patterns like ?Obama, the president of the US?,
?Obama, the US president?, ?US President Obama?
far outnumber sentences of the form ?Obama is the
president of the US?. These relations are seldom the
primary information in a sentence, and are typically
mentioned in passing in noun phrases that express
the relation.
For some applications, noun-mediated relations
are important, as they associate people with work
places and job titles. Overall, we think of the results
in Figure 6 as a ?best case analysis? that illustrates
the dramatic increase in yield for certain relations,
due to syntactic scope expansion in Open IE.
5.2 Analysis of OLLIE
We perform two control experiments to understand
the value of semantic/lexical restrictions in pattern
learning and precision boost due to context analysis
component.
7We multiply the total number of extractions with precision
on a sample for that relation to estimate the yield.
529
0  
0.2  
0.4  
0.6  
0.8  
1  
0  10  20  30  40  50  60  
OLLIE  
OLLIE[Lex]  
OLLIE[syn]  
Yield 
Pre
cis
ion
 
Figure 7: Results on the subset of extractions from pat-
terns with semantic/lexical restrictions. Ablation study
on patterns with semantic/lexical restrictions. These pat-
terns without restrictions (OLLIE[syn]) result in low pre-
cision. Type generalization improves yield compared to
patterns with only lexical constraints (OLLIE[lex]).
Are semantic restrictions important for open pat-
tern learning? How much does type generalization
help? To answer these questions we compare three
systems ? OLLIE without semantic or lexical restric-
tions (OLLIE[syn]), OLLIE with lexical restrictions
but no type generalization (OLLIE[lex]) and the full
system (OLLIE). We restrict this experiment to the
patterns where OLLIE adds semantic/lexical restric-
tions, rather than dilute the result with patterns that
would be unchanged by these variants.
Figure 7 shows the results of this experiment on
our dataset from three domains. As the curves
show, OLLIE was correct to add lexical/semantic
constraints to these patterns ? precision is quite low
without the restrictions. This matches our intuition,
since these are not completely general patterns and
generalizing to all unseen relations results in a large
number of errors. OLLIE[lex] performs well though
at lower yield. The type generalization helps the
yield somewhat, without hurting the precision. We
believe that a more data-driven type generalization
that uses distributional similarity (e.g., (Ritter et al
2010)) may help much more. Also, notice that over-
all precision numbers are lower, since these are the
more difficult relations to extract reliably. We con-
clude that lexical/semantic restrictions are valuable
for good performance of OLLIE.
We also compare our full system to a version that
does not use the context analysis of Section 4. Fig-
ure 8 compares OLLIE to a version (OLLIE[pat]) that
does not add the AttributedTo and ClausalModifier
fields, and, instead of context-sensitive confidence
function, uses the pattern frequency in the training
0.5  
0.6  
0.7  
0.8  
0.9  
1  
0  100  200  300  400  500  600  
OLLIE  
OLLIE[pat]  
Yield 
Pre
cis
ion
 
Figure 8: Context analysis increases precision, raising the
area under the curve by 19%.
set as a ranking function. 10% of the sentences have
an OLLIE extraction with ClausalModifier and 6%
have AttributedTo fields. Adding ClausalModifier
corrects errors for 21% of extractions that have a
ClausalModifier and does not introduce any new er-
rors. Adding AttributedTo corrects errors for 55%
of the extractions with AttributedTo and introduces
an error for 3% of the extractions. Overall, we find
that OLLIE gives a significant boost to precision over
OLLIE[pat] and obtains 19% additional AUC.
Finally, we analyze the errors made by OLLIE.
Unsurprisingly, because of OLLIE?s heavy reliance
on the parsers, parser errors account for a large part
of OLLIE?s errors (32%). 18% of the errors are due
to aggressive generalization of a pattern to all un-
seen relations and 12% due to incorrect application
of lexically annotated patterns. About 14% of the er-
rors are due to important context missed by OLLIE.
Another 12% of the errors are because of the limita-
tions of binary representation, which misses impor-
tant information that can only be expressed in n-ary
tuples.
We believe that as parsers become more robust
OLLIE?s performance will improve even further. The
presence of context-related errors suggests that there
is more to investigate in context analysis. Finally, in
the future we wish to extend the representation to
include n-ary extractions.
5.3 Comparison with SRL
Our final evaluation suggests answers to two im-
portant questions. First, how does a state-of-the-art
Open IE system do in terms of absolute recall? Sec-
ond, how do Open IE systems compare against state-
of-the-art SRL systems?
SRL, as discussed in Section 2, has a very dif-
ferent goal ? analyzing verbs and nouns to identify
530
their arguments, then mapping the verb or noun to
a semantic frame and determining the role that each
argument plays in that frame. These verbs and nouns
need not make the full relation phrase, although, re-
cent work has shown that they may be converted
to Open IE style extractions with additional post-
processing (Christensen et al2011).
While a direct comparison between OLLIE and
a full SRL system is problematic, we can compare
performance of OLLIE and the argument identifica-
tion step of an SRL system. We set each system the
following task ? ?based on a sentence, find all noun-
pairs that have an asserted relationship.? This task is
permissive for both systems, as it does not require
finding an exact relation phrase or argument bound-
ary, or determining the argument roles in a relation.
We create a gold standard by tagging a random
50 sentences of our test set to identify all pairs of
NPs that have an asserted relation. We only counted
relation expressed by a verb or noun in the text, and
did not include relations expressed simply with ?of?
or apostrophe-s. Where a verb mediates between an
argument and multiple NPs, we represent this as a
binary relation for all pairs of NPs.
For example the sentence, ?Macromolecules
translocated through the phloem include proteins
and various types of RNA that enter the sieve tubes
through plasmodesmata.? has five binary relations.
arg1: arg2: relation term
Macromolecules phloem translocated
Macromolecules proteins include
Macromolecules types of RNA include
types of RNA sieve tubes enter
types of RNA plasmodesmata enter
We find an average of 4.0 verb-mediated relations
and 0.3 noun-mediated relations per sentence. Eval-
uating OLLIE against this gold standard helps to an-
swer the question of absolute recall: what percent-
age of binary relations expressed in a sentence can
our systems identify.
For comparison, we use a state-of-the-art SRL
system from Lund University (Johansson and
Nugues, 2008), which is trained on PropBank
(Martha and Palmer, 2002) for its verb-frames and
NomBank (Meyers et al2004) for noun-frames.
The PropBank version of the system won the very
competitive 2008 CONLL SRL evaluation.
We conduct this experiment by manually compar-
LUND OLLIE union
Verb relations 0.58 (0.69) 0.49 (0.55) 0.71 (0.83)
Noun relations 0.07 (0.33) 0.13 (0.13) 0.20 (0.33)
All relations 0.54 (0.67) 0.47 (0.52) 0.67 (0.80)
Figure 9: Recall of LUND and OLLIE on binary relations.
In parentheses is recall with oracle co-reference. Both
systems identify approximately half of all argument pairs,
but have lower recall on noun-mediated relations.
ing the outputs of LUND and OLLIE against the gold
standard. For each pair of NPs in the gold standard
we determine whether the systems find a relation
with that pair of NPs as arguments. Recall is based
on the percentage of NP pairs where the head nouns
matches head nouns of two different arguments in an
extraction or semantic frame. If the argument value
is conjunctive, we count a match against the head
noun of each item in the list. We also count cases
where system output would match the gold standard,
given perfect co-reference.
Figure 9 shows the recall for OLLIE and LUND,
with recall based on oracle co-referential matches
in parentheses. Our analysis shows strong recall
for both systems for verb-mediated relations: LUND
finding about two thirds of the argument pairs and
OLLIE finding over half. Both systems have low
recall for noun-mediated relations, with most of
LUND?s recall requiring co-reference. We observe
that a union of the two systems raises recall to
0.71 for verb-mediated relations and 0.83 with co-
reference, demonstrating that each system is identi-
fying argument pairs that the other missed.
It is not surprising that OLLIE has recall of ap-
proximately 0.5, since it is tuned for high precision
extraction, and avoids less reliable extractions from
constructions such as reduced relative clauses and
gerunds, or from noun-mediated relations with long-
range dependencies. In contrast, SRL is tuned to
identify the argument structure for nearly all verbs
and nouns in a sentence. The missing recall from
SRL is primarily where it does not identify both ar-
guments of a binary relation, or where the correct
argument is buried in a long argument phrase, but is
not its head noun.
It is surprising that LUND, trained on Nom-
Bank, identifies so few noun-mediated argument
pairs without co-reference. An example will make
this clear. For the sentence, ?Clarcor, a maker of
packaging and filtration products, said ...?, the tar-
531
get relation is between Clarcor and the products it
makes. LUND identifies a frame maker.01 in which
argument A0 has head noun ?maker? and A1 is a PP
headed by ?products?, missing the actual name of the
maker without co-reference post-processing. OLLIE
finds the extraction (Clarcor; be a maker of; packag-
ing and filtration products) where the heads of both
arguments matched those of the target. In another
example, LUND identifies ?his? and ?brother? as the
arguments of the frame brother.01, rather than the
actual names of the two brothers.
We can draw several conclusions from this exper-
iment. First, nouns, although less frequently mediat-
ing relations, are much harder and both systems are
failing significantly on those ? OLLIE is somewhat
better. Two, neither systems dominates the other;
in fact, recall is increased significantly by a union
of the two. Three, and probably most importantly,
significant information is still being missed by both
systems, and more research is warranted.
6 Related Work
There is a long history of bootstrapping and pat-
tern learning approaches in traditional informa-
tion extraction, e.g., DIPRE (Brin, 1998), Snow-
Ball (Agichtein and Gravano, 2000), Espresso (Pan-
tel and Pennacchiotti, 2006), PORE (Wang et al
2007), SOFIE (Suchanek et al2009), NELL (Carl-
son et al2010), and PROSPERA (Nakashole et
al., 2011). All these approaches first bootstrap data
based on seed instances of a relation (or seed data
from existing resources such as Wikipedia) and then
learn lexical or lexico-POS patterns to create an ex-
tractor. Other approaches have extended these to
learning patterns based on full syntactic analysis of
a sentence (Bunescu and Mooney, 2005; Suchanek
et al2006; Zhao and Grishman, 2005).
OLLIE has significant differences from the previ-
ous work in pattern learning. First, and most impor-
tantly, these previous systems learn an extractor for
each relation of interest, whereas OLLIE is an open
extractor. OLLIE?s strength is its ability to gener-
alize from one relation to many other relations that
are expressed in similar forms. This happens both
via syntactic generalization and type generalization
of relation words (sections 3.2.1 and 3.2.2). This ca-
pability is essential as many relations in the test set
are not even seen in the training set ? in early exper-
iments we found that non-generalized pattern learn-
ing (equivalent to traditional IE) had significantly
less yield at a slightly higher precision.
Secondly, previous systems begin with seeds that
consist of a pair of entities, whereas we also in-
clude the content words from REVERB relations in
our training seeds. This results in a much higher
precision bootstrapping set and high rule preci-
sion while still allowing morphological variants that
cover noun-mediated relations. A third difference is
in the scale of the training ? REVERB yields millions
of training seeds, where previous systems had orders
of magnitude less. This enables OLLIE to learn pat-
terns with greater coverage.
The closest to our work is the pattern learning
based open extractor WOEparse. Section 3.4 de-
tails the differences between the two extractors. An-
other extractor, StatSnowBall (Zhu et al2009), has
an Open IE version, which learns general but shal-
low patterns. Preemptive IE (Shinyama and Sekine,
2006) is a paradigm related to Open IE that first
groups documents based on pairwise vector cluster-
ing, then applies additional clustering to group en-
tities based on document clusters. The clustering
steps make it difficult for it to scale to large corpora.
7 Conclusions
Our work describes OLLIE, a novel Open IE ex-
tractor that makes two significant advances over
the existing Open IE systems. First, it expands
the syntactic scope of Open IE systems by identi-
fying relationships mediated by nouns and adjec-
tives. Our experiments found that for some rela-
tions this increases the number of correct extrac-
tions by two orders of magnitude. Second, by an-
alyzing the context around an extraction, OLLIE is
able to identify cases where the relation is not as-
serted as factual, but is hypothetical or conditionally
true. OLLIE increases precision by reducing con-
fidence in those extractions or by associating addi-
tional context in the extractions, in the form of at-
tribution and clausal modifiers. Overall, OLLIE ob-
tains 1.9 to 2.7 times more area under precision-
yield curves compared to existing state-of-the-art
open extractors. OLLIE is available for download at
http://openie.cs.washington.edu.
532
Acknowledgments
This research was supported in part by NSF grant IIS-0803481,
ONR grant N00014-08-1-0431, DARPA contract FA8750-09-
C-0179 and the Intelligence Advanced Research Projects Ac-
tivity (IARPA) via Air Force Research Laboratory (AFRL) con-
tract number FA8650-10-C-7058. The U.S. Government is au-
thorized to reproduce and distribute reprints for Governmen-
tal purposes notwithstanding any copyright annotation thereon.
The views and conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily representing
the official policies or endorsements, either expressed or im-
plied, of IARPA, AFRL, or the U.S. Government. This research
is carried out at the University of Washington?s Turing Center.
We thank Fei Wu and Dan Weld for providing WOE?s code
and Anthony Fader for releasing REVERB?s code. Peter Clark,
Alan Ritter, and Luke Zettlemoyer provided valuable feedback
on the research and Dipanjan Das helped us with state-of-the-
art SRL systems. We also thank the anonymous reviewers for
their comments on an earlier draft.
References
E. Agichtein and L. Gravano. 2000. Snowball: Ex-
tracting relations from large plain-text collections. In
Procs. of the Fifth ACM International Conference on
Digital Libraries.
ARPA. 1991. Proc. 3rd Message Understanding Conf.
Morgan Kaufmann.
ARPA. 1998. Proc. 7th Message Understanding Conf.
Morgan Kaufmann.
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the Web. In Procs. of IJCAI.
S. Brin. 1998. Extracting Patterns and Relations from the
World Wide Web. In WebDB Workshop at 6th Interna-
tional Conference on Extending Database Technology,
EDBT?98, pages 172?183, Valencia, Spain.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proc. of HLT/EMNLP.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Procs. of AAAI.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2011. An analysis of open informa-
tion extraction based on semantic role labeling. In
Proceedings of the 6th International Conference on
Knowledge Capture (K-CAP ?11).
Paul R. Cohen. 1995. Empirical Methods for Artificial
Intelligence. MIT Press.
Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.
1999. Similarity-based models of word cooccurrence
probabilities. Machine Learning, 34(1-3):43?69.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Lan-
guage Resources and Evaluation (LREC 2006).
Oren Etzioni, Anthony Fader, Janara Christensen,
Stephen Soderland, and Mausam. 2011. Open infor-
mation extraction: the second generation. In Proceed-
ings of the International Joint Conference on Artificial
Intelligence (IJCAI ?11).
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.
2010. Learning 5000 relational extractors. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, ACL ?10, pages 286?
295.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke S.
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In ACL, pages 541?550.
Richard Johansson and Pierre Nugues. 2008. The ef-
fect of syntactic representation on semantic role label-
ing. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (COLING 08),
pages 393?400.
Paul Kingsbury Martha and Martha Palmer. 2002. From
treebank to propbank. In Proceedings of the Third In-
ternational Conference on Language Resources and
Evaluation (LREC 02).
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. Annotating
Noun Argument Structure for NomBank. In Proceed-
ings of LREC-2004, Lisbon, Portugal.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In ACL-IJCNLP ?09: Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
2, pages 1003?1011.
Ndapandula Nakashole, Martin Theobald, and Gerhard
Weikum. 2011. Scalable knowledge harvesting with
high precision and high recall. In Proceedings of the
Fourth International Conference on Web Search and
Web Data Mining (WSDM 2011), pages 227?236.
Joakim Nivre and Jens Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of the Conference
on Natural Language Learning (CoNLL-04), pages
49?56.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of 21st Interna-
tional Conference on Computational Linguistics and
533
44th Annual Meeting of the Association for Computa-
tional Linguistics (ACL?06).
P. Resnik. 1996. Selectional constraints: an information-
theoretic model and its computational realization.
Cognition.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In ECML/PKDD (3), pages 148?163.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet alation method for selectional preferences.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics (ACL ?10).
Karin Kipper Schuler. 2006. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. thesis,
University of Pennsylvania.
Y. Shinyama and S. Sekine. 2006. Preemptive informa-
tion extraction using unrestricted relation discovery.
In Procs. of HLT/NAACL.
Fabian M. Suchanek, Georgiana Ifrim, and Gerhard
Weikum. 2006. Combining linguistic and statistical
analysis to extract relations from web documents. In
Procs. of KDD, pages 712?717.
Fabian M. Suchanek, Mauro Sozio, and Gerhard
Weikum. 2009. Sofie: a self-organizing framework
for information extraction. In Proceedings of WWW,
pages 631?640.
Gang Wang, Yong Yu, and Haiping Zhu. 2007. Pore:
Positive-only relation extraction from wikipedia text.
In Proceedings of 6th International Semantic Web
Conference and 2nd Asian Semantic Web Conference
(ISWC/ASWC?07), pages 580?594.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using Wikipedia. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics (ACL ?10).
Shubin Zhao and Ralph Grishman. 2005. Extracting re-
lations with integrated information using kernel meth-
ods. In Procs. of ACL.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and
Ji-Rong Wen. 2009. StatSnowball: a statistical ap-
proach to extracting entity relationships. In WWW
?09: Proceedings of the 18th international conference
on World Wide Web, pages 101?110, New York, NY,
USA. ACM.
534
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 893?903, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities
Thomas Lin, Mausam, Oren Etzioni
Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
{tlin, mausam, etzioni}@cs.washington.edu
Abstract
Entity linking systems link noun-phrase men-
tions in text to their corresponding Wikipedia
articles. However, NLP applications would
gain from the ability to detect and type all
entities mentioned in text, including the long
tail of entities not prominent enough to have
their own Wikipedia articles. In this paper we
show that once the Wikipedia entities men-
tioned in a corpus of textual assertions are
linked, this can further enable the detection
and fine-grained typing of the unlinkable en-
tities. Our proposed method for detecting un-
linkable entities achieves 24% greater accu-
racy than a Named Entity Recognition base-
line, and our method for fine-grained typing is
able to propagate over 1,000 types from linked
Wikipedia entities to unlinkable entities. De-
tection and typing of unlinkable entities can
increase yield for NLP applications such as
typed question answering.
1 Introduction
A key challenge in machine reading (Etzioni et al
2006) is to identify the entities mentioned in text,
and associate them with appropriate background in-
formation such as their type. Consider the sentence
?Some people think that pineapple juice is good for
vitamin C.? To analyze this sentence, a machine
should know that ?pineapple juice? refers to a bev-
erage, while ?vitamin C? refers to a nutrient.
Entity linking (Bunescu and Pas?ca, 2006;
Cucerzan, 2007) addresses this problem by link-
ing noun phrases within the sentence to entries
in a large, fixed entity catalog (almost always
example noun phrases status
?apple juice? ?orange juice? present
?prune juice? ?wheatgrass juice? absent
?radiation exposure? ?workplace stress? present
?asbestos exposure? ?financial stress? absent
?IJCAI? ?OOPSLA? present
?EMNLP? ?ICAPS? absent
Table 1: Wikipedia has entries for prominent entities,
while missing tail and new entities of the same types.
Wikipedia). Thus, entity linking has a limited and
somewhat arbitrary range. In our example, systems
by (Ferragina and Scaiella, 2010) and (Ratinov et
al., 2011) both link ?vitamin C? correctly, but link
?pineapple juice? to ?pineapple.? ?Pineapple juice?
is not entity linked as a beverage because it is not
prominent enough to have its own Wikipedia entry.
As Table 1 shows, Wikipedia often has prominent
entities, while missing tail and new entities of the
same types.1 (Wang et al2012) notes that there
are more than 900 different active shoe brands, but
only 82 exist in Wikipedia. In scenarios such as in-
telligence analysis and local search, non-Wikipedia
entities are often the most important.
Hence, we introduce the unlinkable noun phrase
problem: Given a noun phrase that does not link
into Wikipedia, return whether it is an entity, as well
its fine-grained semantic types. Deciding if a non-
Wikipedia noun phrase is an entity is challenging
because many of them are not entities (e.g., ?Some
people,? ?an addition? and ?nearly half?). Predict-
1The same problem occurs with Freebase, which is also
missing the same Table 1 entities.
893
ing semantic types is a challenge because of the di-
versity of entity types in general text. In our experi-
ments, we utilized the Freebase type system, which
contains over 1,000 semantic types.
The first part of this paper proposes a novel
method for detecting entities by observing that enti-
ties often have different usage-over-time character-
istics than non-entities. Evaluation shows that our
method achieves 24% relative accuracy gain over
a NER baseline. The second part of this paper
shows how instance-to-instance class propagation
(Kozareva et al2011) can be adapted and scaled to
semantically type general noun-phrase entities using
types from linked entities, by leveraging over one
million different possible textual relations.
Contributions of our research include:
? We motivate and introduce the unlinkable noun
phrase problem, which extends previous work
in entity linking.
? We propose a novel method for discriminating
entities from arbitrary noun phrases, utilizing
features derived from Google Books ngrams.
? We adapt and scale instance-to-instance class
propagation in order to associate types with
non-Wikipedia entities.
? We implement and evaluate our methods, em-
pirically verifying improvement over appropri-
ate baselines.
2 Background
In this section we provide an overview of entity link-
ing, how we entity link our data set, and describe
how our problem and approach differ from related
areas such as NER and Web extraction.
2.1 Entity Linking
Given text, the task of entity linking (Bunescu
and Pas?ca, 2006; Cucerzan, 2007; Milne and Wit-
ten, 2008; Kulkarni et al2009) is to identify the
Wikipedia entities within the text, and mark them
with which Wikipedia entity they correspond to. En-
tity linking elevates us from plain text into mean-
ingful entities that have properties, semantic types,
and relationships with each other. Other entity cata-
logs can be used in place of Wikipedia, especially in
domain-specific contexts, but general purpose link-
ing systems all use Wikipedia because of its broad
general coverage, and to leverage its article texts and
link structure during the linking process.
A problem we observed when using entity link-
ing systems is that despite containing over 3 million
entities, Wikipedia does not cover a significant num-
ber of entities. This occurs with entities that are not
prominent enough to have their own dedicated arti-
cle and with entities that are very new. For exam-
ple, Facebook has over 600 million users, and each
of them could be considered an entity. The REVERB
extractor (Fader et al2011) on the ClueWeb09 Web
corpus found over 1.4 billion noun phrases partic-
ipating in textual relationships, and a sizable por-
tion of these noun phrases are entities. While re-
cent research has used NIL features to determine
whether they are being asked to link an entity not in
Wikipedia (Dredze et al2010; Ploch, 2011), there
has been no research on whether given noun phrases
that are unlinkable (for not being in Wikipedia) are
entities, and how to make them usable if they are.
Our goal is to address this problem of learning
whether non-Wikipedia noun phrases are entities,
and assigning semantic types to them to make them
useful. We begin with a corpus of 15 million ?(noun
phrase subject, textual relation, noun phrase object)?
assertions from the Web that were extracted by RE-
VERB (Fader et al2011).2 REVERB already filters
out relative pronouns, WHO-adverbs, and existential
?there? noun phrases that do not make meaningful
arguments. We then employ standard entity linking
techniques including string matching, prominence
priors (Fader et al2009), and context matching
(Bunescu and Pas?ca, 2006) to link the noun phrase
subjects into Wikipedia.
In this manner, we were able to entity link the
noun phrase subject of 9,699,967 extractions, while
the remaining 5,028,301 extractions had no matches
(mostly due to no close string matches). There were
1,401,713 distinct noun phrase subjects in the 5 mil-
lion extractions that had no matches. These are the
unlinkable noun phrases we will study here.
2.2 Named Entity Recognition
Named Entity Recognition (NER) is the task of
identifying named entities in text. A key difference
between our final goals and NER is that in the con-
2available at http://reverb.cs.washington.edu
894
text of entity linking and Wikipedia, there are many
more entities than just the named entities. For ex-
ample, ?apple juice? and ?television? are Wikipedia
entities (with Wikipedia articles), but are not tradi-
tional named entities. Still, as named entities do
comprise a sizable portion of our unlinkable noun
phrases, we compare against a NER baseline in our
entity detection step.
Fine-grained NER (Sekine and Nobata, 2004; Lee
et al2007) has studied scaling NER to up to 200 se-
mantic types. This differs from our semantic typing
of unlinked entities because our approach assumes
access to corpora-level relationships between a large
set of linked entities (with semantic types) and the
unlinked entities. As a result we are able to propa-
gate 1,339 Freebase semantic types from the linked
entities to the unlinked entities, which is substan-
tially more types than fine-grained NER.
2.3 Extracting Entity Sets
There is a line of research in using Web extraction
(Etzioni et al2005) and entity set expansion (Pantel
et al2009) to extract lists of typed entities from the
Web (e.g., a list of every city). Our problem instead
focuses on determining whether any individual noun
phrase is an entity, and what semantic types it holds.
Given a noun phrase representing a person name, we
return that this is a person entity even if it is not in a
list of people names harvested from the Web.
3 Architecture
Our goal is: given (1) a large set of linked assertions
L and (2) a large set of unlinked assertions U , for
each unlinkable noun phrase subject n ? U , predict:
(1) whether n is an entity, and if so, then (2) the set
of Freebase semantic types for n. For L we use the
9.7 million assertions whose subject argument we
were able to link in Section 2.1, and for U we use
the 5 million assertions that we could not link.
We divide the system into two components. The
first component (described in Section 4) takes any
unlinkable noun phrase and outputs whether it is an
entity. All n ? U classified as entities are placed in
a set E. The second component (described in Sec-
tion 5) uses L and U to predict the semantic types
for each entity e ? E.
Figure 1: Usage over time in Google books for the noun
phrase ?Prices quoted? (e.g., from ?Prices quoted are for
2 adults?) which is not an entity.
Figure 2: Usage over time for the unlinkable noun phrase
?Soluble fibre,? which is an entity. The best fit line has
steeper slope compared to Figure 1.
4 Detecting Unlinkable Entities
This first task takes in any unlinkable noun phrase
and outputs whether it is an entity. There is a long
history of discussion in analytic philosophy litera-
ture on the question of what exists (e.g., (Quine,
1948)). We adopt a more pragmatic view, defin-
ing an ?entity? as a noun phrase that could have a
Wikipedia-style article if there were no notability or
newness considerations, and which would have se-
mantic types. We are interested in entities that could
help populate an entity store. ?EMNLP 2012? is an
example of an entity, while ?The method? and ?12
cats? are examples of noun phrases that are not en-
tities. This is challenging because at a surface level,
many entities and non-entities look similar: ?Sex
and the City? is an entity, while ?John and David?
is not. ?Eminem? is an entity, while ?Youd? (a typo
from ?You?d?) is not.
We address this task by training a classifier with
features primarily derived from a timestamped cor-
pus. An intuition here is that when considering
only unlinkable noun phrases, usage patterns across
895
Figure 3: Plot of R2 vs Slope for the usage over time of a collection of noun phrases selected for illustrative purposes.
Many of the non-entities occur at lower Slope and higher R2, while the entities often have higher slope and/or lower
R2. ?Bluetooth technology? actually has even higher slope, but was adjusted left to fit in this figure.
time often differ for entities and non-entities. Noun
phrase entities that are observed in text going back
hundreds of years (e.g., ?Europe?) almost all have
their own Wikipedia entries, so in unlinkable noun
phrase space, the remaining noun phrases that are
observed in text going back hundreds of years tend
to be all the textual references and expressions that
are not entities. We plan to use this signal to help
separate the entities from the non-entities.
4.1 Classifier Features
We use the Google Books ngram corpus (Michel
et al2010), which contains timestamped usage
of 1-grams through 5-grams in millions of digi-
tized books for each year from 1500 to 2007.3 We
use ngram match count values from case-insensitive
matching. To avoid sparsity anomalies we observed
in years before 1740, we use the data from 1740 on-
ward. While it has not been used for our task before,
this corpus is a rich resource that enables reason-
ing about knowledge (Evans and Foster, 2011) and
3available at http://books.google.com/ngrams/datasets
understanding semantic changes of words over time
(Wijaya and Yeniterzi, 2011). Talukdar et al2012)
recently used it to effectively temporally scope rela-
tional facts.
Our first feature is the slope of the least-squares
best fit line for usage over time. For example, if a
term appears 25 times in books in 1950, 30 times in
1951, ..., 100 times in 2007, then we compute the
straight line that best fits {(1950, 25), (1951, 31), ...,
(2007, 100)}, and examine the slope. We have ob-
served cases of non-entity noun phrases (e.g., Fig-
ure 1) having lower slopes than entity noun phrases
(e.g., Figure 2). Note that we do not normalize
match counts by yearly total frequency, but we do
normalize counts for each term to range from 0 to 1
(setting the max count for each term to 1) to avoid
bias from entity prominence. To capture the current
usage, in cases where there exists a ? 5 year gap in
usage of a term we only use the data after the gap.
Another feature is the R2 fit of the best fit line.
Higher R2 indicates that the data is closer to a
straight line. Figure 3 plots R2 vs Slope values
896
Figure 4: UsageSinceYear of example unlinked terms.
for some sample noun phrases. We observed that
along with their lower Slope, the non-entities often
also had higher R2, indicating that their usage does
not vary wildly from year to year. This contrasts
with certain entities (e.g., ?FY 99? for ?Fiscal Year
1999?) whose usage sometimes varied sharply from
year to year based on their prominence in those spe-
cific years.
A third feature is UsageSinceYear, which finds the
year from when a term last started continually being
used. For example, a UsageSinceYear value of 1920
would indicate that the term was used in books ev-
ery year from 1920 through 2007. Figure 4 shows
examples of where various terms fall along this di-
mension.
From the books ngram corpus, we also calcu-
late features for: PercentProperCaps - the percent-
age of case-insensitive matches for the term where
all words began with a capital letter, PercentExact-
Match - the percentage of case-insensitive matches
for the term that matched the capitalization in the
assertion exactly, and Frequency - the total number
of case-insensitive occurrences of the term in the
book ngrams data, summed across all years, which
reflects prominence. Last, we also include a sim-
ple numeric feature to detect the presence of leading
numeric words (e.g., ?5? in ?5 days? or ?Three? in
?Three choices?).
4.2 Evaluation
From the corpus of 15 million REVERB assertions,
there were 1.4 million unlinked noun phrases includ-
ing 17% unigrams, 51% bigrams, 21% trigrams, and
11% 4-grams or longer. Bigrams comprise over half
the noun phrases and the books bigram data is a self-
contained download that is easier to obtain and store
system correctly classified
Majority class baseline 50.4%
Named Entity Recognition 63.3%
Slope feature only 61.1%
PUF feature combination 69.1%
ALL features 78.4%
Table 2: Our classifier using all features (ALL) outper-
forms majority class and NER baselines.
than the full books ngram corpus, so we focus on
bigrams in our evaluation. In a random sample of
unlinked bigrams, we found that 73% were present
in the books ngram data (65% exact match, 8% case-
insensitive match only), while 27% were not (these
were mostly entities or errors with non-alphabetic
characters). Coverage is a greater issue with longer
ngrams (e.g., there are many more possible 5-grams
than bigrams, so any individual 5-gram is less likely
to reach the minimum threshold to be included in the
books data), but as mentioned earlier, only 11% of
unlinkable noun phrases were 4-grams or longer.
We randomly sampled 250 unlinked bigrams that
had books ngram data, and asked 2 annotators to la-
bel each as ?entity,? ?non-entity,? or ?unclear.? Our
goal is to separate noun phrases that are clearly en-
tities (e.g., ?prune juice?) from those that are clearly
not entities (e.g., ?prices quoted?), rather than to de-
bate phrases that may be in some entity store defi-
nitions but not others, so we asked the annotators to
choose ?unclear? when there was any doubt. There
were 151 bigrams that both annotators believed to
be very clear labels, including 69 that both annota-
tors labeled as entities, 70 that both annotators la-
beled as non-entities, and 12 with label disagree-
ment. Cohen?s kappa was 0.84, indicating excellent
agreement. Our experiment is now to separate the
69 clear entities from the 70 clear non-entities.
For experiment baselines we use the majority
class baseline MAJ, as well as a Named Entity
Recognition baseline NER. For NER we used the
Illinois Named Entity Tagger (Ratinov and Roth,
2009) on the highest setting (that achieved 90.5 F1
score on the CoNLL03 shared task). NER expects a
sentence, so we use the longest assertion in the cor-
pus that the noun phrase was observed in. We eval-
uate several combinations of our features to test dif-
897
ferent aspects of our system: Slope uses only Slope,
PUF uses PercentProperCaps + UsageSinceYear +
Frequency, and ALL uses all features. We evaluate
using the WEKA J48 Decision Tree on default set-
tings, with leave-one-out cross validation.
Table 2 shows the results. MAJ correctly classifies
50.4% of instances, NER correctly classifies 63.3%
and ALL correctly classifies 78.4%.
4.3 Analysis
Overall, 78.4% correctly classified instances is fairly
strong performance on this task. By using the de-
scribed features, our classifier was able to detect and
filter many of the non-entity noun phrases in this
scenario. Compared to the 63.3% of NER, it is an
absolute gain of 15.1%, a relative gain of 24%, and a
reduction in error of 41.1% (from 36.7% to 21.6%).
Student?s t-test at 95% confidence verified that the
difference was significant.
We found that while low Slope (especially with
higher R2) often indicated non-entity, there were nu-
merous cases where higher Slope did not necessarily
indicate entity. For example, the noun phrase ?sev-
eral websites? has fairly sharp slope, but still does
not denote a clear entity. In these cases, the addi-
tion of other features can serve as additional useful
signal. One error from ALL is the term ?Analyst esti-
mates,? which the annotators labeled as a non-entity,
but which occasionally appears in text (especially ti-
tles) as ?Analyst Estimates,? and is a relatively re-
cent phrase. NER misses entities such as ?synthetic
cubism? and ?hunter orange? that occur in our data
but are not traditional named entities. We observed
that while none of our features achieves over 70%
accuracy by themselves, they perform well in con-
junction with each other.
5 Propagating Semantic Types
This second task uses a set of linked assertions L and
set of unlinked assertions U to predict the semantic
types for each entity e ? E. If the previous step
output that ?Sun Microsystems? is likely to be an
entity, then the goal of this step is to further predict
that it has the Freebase types such as organization
and software developer.
From L we use the set of linked entities and the
textual relations they occur with. For example, L
might contain that the entity Microsoft links to a par-
ticular Wikipedia article, and also that it occurs with
textual relations such as ?has already announced?
and ?has released updates for.? For each Wikipedia-
linked entity in L, we further look up its exact set
of Freebase types.4 From U we obtain the set of
textual relations that each e ? E is in the domain
of. We now have a large set of class-labeled in-
stances (all entities in L), a large set of unlabeled
instances (E), and a method to connect the unla-
beled instances with the class-labeled instances (via
any shared textual relations), so we cast this task
as an instance-to-instance class propagation problem
(Kozareva et al2011) for propagating class labels
from labeled to unlabeled instances.
We build on the recent work of Kozareva et al
(2011), and adapt their approach to leverage the
scale and resources of our scenario. While they use
only one type of edge between instances, namely
shared presence in the high precision DAP pattern
(Hovy et al2009), our final system uses 1.3 mil-
lion textual relations from |L ? U |, corresponding
to 1.3 million potential edge types. Their evaluation
involved only 20 semantic classes, while we use all
1,339 Freebase types covered by our entities in L.
There is a rich history of other approaches for
predicting semantic types. (Talukdar et al2008)
and (Talukdar and Pereira, 2010) model relation-
ships between instances and classes, but our un-
linked entities do not come with any class informa-
tion. Pattern-based approaches (Pas?ca, 2004; Pantel
and Ravichandran, 2004) are popular, but (Kozareva
et al2011) notes that they ?are constraint to the in-
formation matched by the pattern and often suffer
from recall,? meaning that they do not cover many
instances. Classifiers have also been trained for fine-
grained semantic typing, but for noticeably fewer
types than we work with. (Rahman and Ng, 2010)
studied hierarchical and collective classification us-
ing 92 types, and FIGER (Ling and Weld, 2012) re-
cently used an adapted perceptron for multi-class
multi-label classification into 112 types.
5.1 Algorithm
Given an entity e, our algorithm involves: (1) find-
ing the textual relations that e is in the domain of, (2)
4data available at http://download.freebase.com/wex
898
Figure 5: This example illustrates the set of Freebase type predictions for the noun phrase ?Sun Microsystems.? We
predict the semantic type of a noun phrase by: (1) finding the textual relations it is in the domain of, (2) finding linked
entities that are also in the domain of those textual relations, and (3) observing their semantic types.
finding linked entities that are also in the domain of
those textual relations, and then (3) predicting types
by observing the types of those linked entities. Fig-
ure 5 illustrates how we would predict the semantic
types of the noun phrase ?Sun Microsystems.?
Find Relations: Obtain the set R of all textual re-
lations in U that e is in the domain of. For example,
if U contains the assertion ?(Sun Microsystems, has
released a minor update to, Java 1.4.2),? then the tex-
tual relation ?has released a minor update to? should
be added to R when typing ?Sun Microsystems.?
Find Similar Entities: Find the linked entities in
L that are in the domain of the most relations in
R. In our example, entities such as ?Microsoft? and
?Apple Inc.? have the greatest overlap in textual re-
lations because they are most often in the domain
of the same textual relations, e.g., (?Microsoft, has
released a minor update to, Windows Live Essen-
tials?). Create a set S of the entities that share the
most textual relations. We found keeping 10 similar
entities (|S| = 10) is generally enough to predict the
original entity?s types in the final step.
Predict Types: Return the most frequent Freebase
types of the entities in S as the prediction. To
avoid penalizing very small types, if there are n in-
stances of semantic class C in S, then we rank C us-
ing a type score T (n,C, S) = max(n/|S|, n/|C|),
which we found to perform better than T (n,C, S) =
avg(n/|S|, n/|C|). For ?Sun Microsystems,? busi-
ness operation was the top predicted type because
all entities in S were observed to include business
operation type.
5.2 Edge Validity
This algorithm will only be effective if entities that
share textual relation strings are more likely to be
of the same semantic types. To verify this, we sam-
pled 30,000 linked entities from L that had at least
30 textual relations each, and associated each with
their 30 most frequent relations. From the 900 mil-
lion possible entity pairs, we then randomly sample
500 entity pairs that shared exactly k out of 30 rela-
tions, for each k from 0 to 15. At each k we then use
our sampled pairs to estimate the probability that any
two entities sharing exactly k relations (out of their
30 possible) will share at least one type.
The results are shown in Figure 6. We found that
entities sharing more textual relations were in fact
more likely to have semantic types in common. Two
entities that shared exactly 0 of 30 textual relations
were only 11% likely to share a semantic type, while
two entities that shared exactly 10 of 30 relations
were 80% likely to share a semantic type. This vali-
dates our use of textual relations as a signal-bearing
edge in instance-to-instance class propagation.
5.3 Weighting Textual Relations
The algorithm as currently described treats all tex-
tual relations equally, when in reality some are
stronger signal to entity type than others. For exam-
ple, two entities in the domain of the ?came with?
relation often will not share semantic types, but two
entities in the domain of the ?autographed? relation
will almost always share a type. To capture this intu-
ition, we define relation weight w(r) as the observed
probability (among the linked entities) that two en-
899
Figure 6: Entities that share more textual relations are
more likely to have semantic types in common.
tities will share a Freebase type if they both occur
in the domain of r. If D(r) = all entities observed
in the domain of relation r and T (e) = all Freebase
types listed for entity e, then weight w(r) of a tex-
tual relation string r is:
w(r) =
?
e1,e2?D(r), e1 6=e2
I(e1, e2)
|D(r)| ? (|D(r)| ? 1)
I(e1, e2) =
{
1, if |T (e1) ? T (e2)| > 0
0, otherwise.
Table 3 shows examples of high weight relations,
and Table 4 shows low weight relations. We now
modify the Find Similar Entities step such that if
a linked entity shares a set of relations Q with the
entity being typed, then it receives a score which
considers all shared relations q ? Q but uses the
high weight relations more. On a development set
we found that a score of
?
q?Q 10
4?w(q) was effec-
tive, as higher weight signifies much stronger signal.
This score then determines which entities to place in
S.
5.4 Evaluation
The goal of the evaluation is to judge how well our
method can predict the Freebase semantic types of
entities in our scenario. Our linked entities cov-
ered 1,339 Freebase types, including many interest-
ing types such as computer operating system, reli-
gious text, airline and baseball team. Human judges
would have trouble manually annotating new enti-
ties with all these types because there are too many
to keep in mind and understand the characteristics
?is a highway in?
?is a university located in?
?became the president of?
?turned down the role of?
?has an embassy in?
Table 3: Example relations found to have high weight.
?comes with?
?is a generic term for?
?works best on?
?can be made from?
?is almost identical to?
Table 4: Example relations found to have low weight.
of. Instead, we automatically generate testing data
by sampling entities from L, and then test on abil-
ity to recover the actual Freebase types (which we
know).
We sample a HEAD set of distinct 500 Freebase
entities (drawn randomly from our set of linked ex-
tractions), and a TAIL set of 500 entities (drawn ran-
domly from our set of linked entities). An entity
that occurs in many extractions is more likely to be
in HEAD than TAIL. Our sampling also picks only
entities that occur with at least 10 relations, which
is appropriate for the Web scenario where more in-
stances can always be queried for.
For baselines we use random baseline BRandom
and a frequency baseline BFrequency which always
returns types in order of their frequency among
all linked entities (e.g., always person, then loca-
tion, etc). We evaluate our system without rela-
tion weighting (SNoWeight) and also with relation
weighting (SWeighted). For SWeighted we leave all
the test set entities out when calculating global re-
lation weights. Our metrics are Precision at 1 and
F1 score. Precision at 1 measures how often the top
returned type is a correct type, and is useful for ap-
plications that want one type per entity. F1 mea-
sures how well the method recovers the full set of
Freebase types (for each test case we graph preci-
sion/recall and take the max F1), and is useful for
applications such as typed question answering.
Table 5 shows the results. BRandom performs
poorly because there are so many semantic types,
and very few of them are correct for each test
case. BFrequency performs slightly better on TAIL
than HEAD because TAIL contains more entities of
the most frequent types. SNoWeight performance
900
HEAD TAIL
Prec@1 F1 Prec@1 F1
BRandom 0.008 0.028 0.004 0.023
BFrequency 0.244 0.302 0.298 0.322
SNoWeight 0.542? 0.465? 0.510? 0.456?
SWeighted 0.610? 0.521? 0.598? 0.522?
Table 5: Evaluation on HEAD and TAIL, 500 elements each. ? indicates statistical significance over BFrequency, and
? over both BFrequency and SNoWeight. Significance is measured using the Student?s t-test at 95% confidence. The
top type predicted by our SWeighted method is correct about 60% of the time, while the top type predicted by the
BFrequency baseline is correct under 30% of the time.
is statistically significant above all baselines, and
SWeighted is statistically significant over SNoWeight
on both test sets and metrics.
5.5 Analysis
SWeighted was successful at recovering the correct
Freebase types of many entities. For example, it
finds that ?Atherosclerosis? is a medical risk fac-
tor by connecting it to ?obesity? and ?diabetes,? that
?Supernatural? is a TV program and a Netflix title
by connecting it to ?House? and ?30 Rock,? and that
?America West? is an aircraft owner and an airline
by connecting it to ?Etihad Airways? and ?China
Eastern Airlines.? While precision at 1 around 60%
may not be high enough yet for certain applications,
it is significantly better than competing approaches,
which are under 30%, and we hope that our values
can serve as a non-trivial baseline on this task for
future systems.
One example where SWeighted made some mis-
takes is fictional characters. Many fictional charac-
ters participate in a textual relations that make them
look like people (e.g., ?was born on?), but predicting
that they belong to people class is incorrect. Some
performance hit was also due to entity linking errors.
From an assertion like ?The Four Seasons is located
in Hamamatsu,? our entity linker (and other entity
linkers we tried) prefer linking ?The Four Seasons?
to Vivaldi?s music composition rather than the hotel
chain. We are then unable to recover music compo-
sition type from relations like ?is located in.? Our
algorithm relies on accurate entity linking in L, but
there is a precision/recall tradeoff to consider here
because it also benefits from higher coverage of en-
tities and relations in L.
As a general reference for performance of
state-of-the-art fine-grained entity classification, the
FIGER system (Ling and Weld, 2012) for classify-
ing into 112 types reported F1 scores ranging from
0.471 to 0.693 in their experiments. It is important to
note that these numbers are not directly comparable
to us because they used different data, different (and
fewer) types, and different evaluation methodology.
6 Discussion
This paper presented an approach for working with
non-Wikipedia entities in text. Consider the follow-
ing possibilities for a noun phrase in a text corpus:
Wikipedia Entity: (e.g., ?Computer Science,?
?South America,? ?apple juice?) - Entity linking
techniques can identify and type these.
Non-Wikipedia, Non-Entity: (e.g., ?strange
things,? ?Early studies,? ?A link?) - Our classifier
from Section 4 is able to filter these.
Non-Wikipedia, Entity: (e.g., ?Safflower oil,?
?prune juice,? ?Amazon UK?) - We identify these
as entities, then propagate semantic types to them.
Our technique finds that ?Safflower oil? occurs with
high weight relations such as ?is sometimes used to
treat? and ?can be substituted for,? making it similar
to linked entities such as ?Phentermine? and ?Dan-
delion,? and then correctly predicts semantic types
including food ingredient and medical treatment.
6.1 Typed Question Answering
From our set of 15 million assertions, we found and
typed many non-Wikipedia entities. In food while
Wikipedia has ?crab meat,? we find it is missing oth-
ers such as ?rabbit meat? and ?goat milk.? In job ti-
tles it has ?scientist? and ?lawyer,? but we find it is
missing ?PhD student,? ?fashion designer,? and oth-
ers. We find many of the people and employers not
901
prominent enough for Wikipedia.
One application of this research is to increase the
yield of applications such as Typed Question An-
swering (Buscaldi and Rosso, 2006). For example,
consider the query ?What computer game is a lot of
fun?? A search for assertions matching ?* is a lot
of fun? in the data yields around 1,000 results such
as ?camping,? ?David Sedaris? and ?Hawaii.? En-
tity linking allows us to identify just the computer
games in Wikipedia that match the query, such as
?Civilization.? However, around 400 query matches
could not be entity linked. Our noun-phrase clas-
sifier filters out non-entities such as ?actual play,?
?Just this? and ?Two kids.? After predicting types
for the matches that did not get filtered, we find ad-
ditional non-Wikipedia computer games that match
the query, including ?Cooking Dash,? ?Delicious
Deluxe? and ?Slingo Supreme.?
7 Future Work
An area we are continuing to improve the system
on is textual ambiguity. For example, an unlinkable
noun phrase might simultaneously be the name of a
film, a car, and a person. Instead of outputting that
the noun phrase holds all of those types, a stronger
output would be to realize that the noun phrase is
ambiguous, determine how many senses it has, and
determine which sense is being referred to in each
instance. We have ideas for how to detect ambiguous
entities using mutual exclusion (Carlson, 2010) and
functional relations. For example, if we predict that
a noun phrase has film and car types but we also
observe in our linked instances that these types are
mutually exclusive, then this is good evidence that
the noun phrase refers to multiple terms.
We also plan to continue improving our tech-
niques, as there is still plenty of room for improve-
ment on both subtasks. For detecting new entities,
we are interested in seeing if timestamped Twitter
data could be analyzed to increase both recall and
precision. For predicting semantic types, (Kozareva
et al2011) proposed additional techniques which
we have not fully explored. Also, we can incor-
porate additional signals such as shared term heads
when they are available, in order to help find terms
that are likely to share types. Last, we would like
to feed back our system output to improve system
performance. For example, non-entity noun phrases
that make it to the typing step might lead to particu-
lar predicted type distributions that indicate an error
occurred earlier in the process.
8 Conclusion
In this paper we showed that while entity linking
cannot link to entities outside of Wikipedia, once a
large text corpus has been entity linked, the presence
and content of the existing links can be leveraged to
help detect and semantically type the non-Wikipedia
entities as well. We designed techniques for de-
tecting whether unlinkable noun phrases are entities,
and if they are, then propagating semantic types to
them from the linked entities. In our evaluations, we
showed that our techniques achieve statistically sig-
nificant improvement over baseline methods.
Our research here takes initial steps toward a fu-
ture where the vast universe of entities that are not
prominent enough to include in manually-authored
knowledge bases is analyzed automatically instead
of being left behind.
Acknowledgements
We thank Stephen Soderland, Xiao Ling, and the
three anonymous reviewers for their helpful feed-
back on earlier drafts. This research was sup-
ported in part by NSF grant IIS-0803481, ONR grant
N00014-08-1-0431, and DARPA contract FA8750-
09-C-0179, and carried out at the University of
Washington?s Turing Center.
References
Razvan Bunescu and Marius Pas?ca. 2006. Using ency-
clopedic knowledge for named entity disambiguation.
In Proceedings of the 11th Conference of the European
Chapter of the Association of Computational Linguis-
tics (EACL).
Davide Buscaldi and Paolo Rosso. 2006. Mining knowl-
edge from wikipedia for the question answering task.
In Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC).
Andrew Carlson. 2010. Coupled Semi-Supervised
Learning. Ph.D. thesis, Carnegie Mellon University.
Silviu Cucerzan. 2007. Large-scale named entity disam-
biguation based on wikipedia data. In Proceedings of
EMNLP.
902
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation for
knowledge base population. In Proceedings of COL-
ING.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the Web: An
experimental study. In Artificial Intelligence.
Oren Etzioni, Michele Banko, and Michael J. Cafarella.
2006. Machine Reading. In Proceedings of the 21st
National Conference on Artificial Intelligence (AAAI).
James A. Evans and Jacob G. Foster. 2011. Metaknowl-
edge. In Science.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2009. Scaling Wikipedia-based named entity disam-
biguation to arbitrary Web text. In IJCAI-09 Workshop
on User-contributed Knowledge and Artificial Intelli-
gence (WikiAI09).
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Paolo Ferragina and Ugo Scaiella. 2010. Tagme:
On-the-fly annotation of short text fragments (by
wikipedia entities). In Proceedings of CIKM.
Eduard Hovy, Zornitsa Kozareva, and Ellen Riloff. 2009.
Toward completeness in concept extraction and classi-
fication. In Proceedings of EMNLP.
Zornitsa Kozareva, Konstantin Voevodski, and Shang-
Hua Teng. 2011. Class label enhancement via related
instances. In Proceedings of EMNLP.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective annotation of
wikipedia entities in text. In Proceedings of KDD.
Changki Lee, Yi-Gyu Hwang, and Myung-Gil Jang.
2007. Fine-grained named entity recognition and rela-
tion extraction for question answering. In Proceedings
of SIGIR.
Xiao Ling and Daniel S. Weld. 2012. Fine-grained entity
recognition. In Proceedings of the 26th Conference on
Artificial Intelligence (AAAI).
Jean-Baptiste Michel, Yuan Kui Shen, Aviva P. Aiden,
Adrian Veres, Matthew K. Gray, The Google Books
Team, Joseph P. Pickett, Dale Hoiberg, Dan Clancy,
Peter Norvig, Jon Orwant, and Steven Pinker. 2010.
Quantitative analysis of culture using millions of digi-
tized books. In Science.
David Milne and Ian H. Witten. 2008. Learning to link
with wikipedia. In Proceedings of the 17h ACM Inter-
national Conference on Information and Knowledge
Management (CIKM).
Marius Pas?ca. 2004. Acquisition of categorized named
entities for web search. In Proceedings of the ACM
International Conference on Information and Knowl-
edge Management (CIKM).
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In Proceedings of
HLT-NAACL.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP.
Danuta Ploch. 2011. Exploring entity relations for
named entity disambiguation. In Proceedings of the
Annual Meeting of the Association of Computational
Linguistics (ACL).
Willard Van Orman Quine. 1948. On what there is. In
Review of Metaphysics.
Altaf Rahman and Vincent Ng. 2010. Inducing fine-
grained semantic classes via hierarchical and collec-
tive classification. In Proceedings of COLING.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning (CoNLL).
Lev Ratinov, Dan Roth, Doug Downey, and Mike Ander-
son. 2011. Local and global algorithms for disam-
biguation to wikipedia. In Proceedings of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL).
Satoshi Sekine and Chikashi Nobata. 2004. Definition,
dictionaries and tagger for extended named entity hier-
archy. In Proceedings of the 4th International Confer-
ence on Language Resources and Evaluation (LREC).
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learning
methods for class-instance acquisition. In Proceedings
of the Annual Meeting of the Association of Computa-
tional Linguistics (ACL).
Partha Pratim Talukdar, Joseph Reisinger, Marius Pas?ca,
Deepak Ravichandran, Rahul Bhagat, and Fernando
Pereira. 2008. Weakly supervised acquisition of la-
beled class instances using graph random walks. In
Proceedings of EMNLP.
Partha Pratim Talukdar, Derry Tanti Wijaya, and Tom
Mitchell. 2012. Coupled temporal scoping of rela-
tional facts. In Proceedings of WSDM.
Chi Wang, Kaushik Chakrabarti, Tao Cheng, and Surajit
Chaudhuri. 2012. Targeted disambiguation of ad-hoc,
homogeneous sets of named entities. In Proceedings
of the 21st International World Wide Web Conference
(WWW).
Derry Tanti Wijaya and Reyyan Yeniterzi. 2011. Un-
derstanding semantic changes of words over centuries.
In Workshop on Detecting and Exploiting Cultural Di-
versity on the Social Web.
903
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 424?434,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Latent Dirichlet Allocation method for Selectional Preferences
Alan Ritter, Mausam and Oren Etzioni
Department of Computer Science and Engineering
Box 352350, University of Washington, Seattle, WA 98195, USA
{aritter,mausam,etzioni}@cs.washington.edu
Abstract
The computation of selectional prefer-
ences, the admissible argument values for
a relation, is a well-known NLP task with
broad applicability. We present LDA-SP,
which utilizes LinkLDA (Erosheva et al,
2004) to model selectional preferences.
By simultaneously inferring latent top-
ics and topic distributions over relations,
LDA-SP combines the benefits of pre-
vious approaches: like traditional class-
based approaches, it produces human-
interpretable classes describing each re-
lation?s preferences, but it is competitive
with non-class-based methods in predic-
tive power.
We compare LDA-SP to several state-of-
the-art methods achieving an 85% increase
in recall at 0.9 precision over mutual in-
formation (Erk, 2007). We also eval-
uate LDA-SP?s effectiveness at filtering
improper applications of inference rules,
where we show substantial improvement
over Pantel et al?s system (Pantel et al,
2007).
1 Introduction
Selectional Preferences encode the set of admissi-
ble argument values for a relation. For example,
locations are likely to appear in the second argu-
ment of the relation X is headquartered in Y and
companies or organizations in the first. A large,
high-quality database of preferences has the po-
tential to improve the performance of a wide range
of NLP tasks including semantic role labeling
(Gildea and Jurafsky, 2002), pronoun resolution
(Bergsma et al, 2008), textual inference (Pantel
et al, 2007), word-sense disambiguation (Resnik,
1997), and many more. Therefore, much atten-
tion has been focused on automatically computing
them based on a corpus of relation instances.
Resnik (1996) presented the earliest work in
this area, describing an information-theoretic ap-
proach that inferred selectional preferences based
on the WordNet hypernym hierarchy. Recent work
(Erk, 2007; Bergsma et al, 2008) has moved away
from generalization to known classes, instead
utilizing distributional similarity between nouns
to generalize beyond observed relation-argument
pairs. This avoids problems like WordNet?s poor
coverage of proper nouns and is shown to improve
performance. These methods, however, no longer
produce the generalized class for an argument.
In this paper we describe a novel approach to
computing selectional preferences by making use
of unsupervised topic models. Our approach is
able to combine benefits of both kinds of meth-
ods: it retains the generalization and human-
interpretability of class-based approaches and is
also competitive with the direct methods on pre-
dictive tasks.
Unsupervised topic models, such as latent
Dirichlet alocation (LDA) (Blei et al, 2003) and
its variants are characterized by a set of hidden
topics, which represent the underlying semantic
structure of a document collection. For our prob-
lem these topics offer an intuitive interpretation ?
they represent the (latent) set of classes that store
the preferences for the different relations. Thus,
topic models are a natural fit for modeling our re-
lation data.
In particular, our system, called LDA-SP, uses
LinkLDA (Erosheva et al, 2004), an extension of
LDA that simultaneously models two sets of dis-
tributions for each topic. These two sets represent
the two arguments for the relations. Thus, LDA-SP
is able to capture information about the pairs of
topics that commonly co-occur. This information
is very helpful in guiding inference.
We run LDA-SP to compute preferences on a
massive dataset of binary relations r(a1, a2) ex-
424
tracted from the Web by TEXTRUNNER (Banko
and Etzioni, 2008). Our experiments demon-
strate that LDA-SP significantly outperforms state
of the art approaches obtaining an 85% increase
in recall at precision 0.9 on the standard pseudo-
disambiguation task.
Additionally, because LDA-SP is based on a for-
mal probabilistic model, it has the advantage that
it can naturally be applied in many scenarios. For
example, we can obtain a better understanding of
similar relations (Table 1), filter out incorrect in-
ferences based on querying our model (Section
4.3), as well as produce a repository of class-based
preferences with a little manual effort as demon-
strated in Section 4.4. In all these cases we obtain
high quality results, for example, massively out-
performing Pantel et al?s approach in the textual
inference task.1
2 Previous Work
Previous work on selectional preferences can
be broken into four categories: class-based ap-
proaches (Resnik, 1996; Li and Abe, 1998; Clark
and Weir, 2002; Pantel et al, 2007), similarity
based approaches (Dagan et al, 1999; Erk, 2007),
discriminative (Bergsma et al, 2008), and genera-
tive probabilistic models (Rooth et al, 1999).
Class-based approaches, first proposed by
Resnik (1996), are the most studied of the four.
They make use of a pre-defined set of classes, ei-
ther manually produced (e.g. WordNet), or auto-
matically generated (Pantel, 2003). For each re-
lation, some measure of the overlap between the
classes and observed arguments is used to iden-
tify those that best describe the arguments. These
techniques produce a human-interpretable output,
but often suffer in quality due to an incoherent tax-
onomy, inability to map arguments to a class (poor
lexical coverage), and word sense ambiguity.
Because of these limitations researchers have
investigated non-class based approaches, which
attempt to directly classify a given noun-phrase
as plausible/implausible for a relation. Of these,
the similarity based approaches make use of a dis-
tributional similarity measure between arguments
and evaluate a heuristic scoring function:
Srel(arg)=
?
arg??Seen(rel)
sim(arg, arg?) ? wtrel(arg)
1Our repository of selectional preferences is available
at http://www.cs.washington.edu/research/
ldasp.
Erk (2007) showed the advantages of this ap-
proach over Resnik?s information-theoretic class-
based method on a pseudo-disambiguation evalu-
ation. These methods obtain better lexical cover-
age, but are unable to obtain any abstract represen-
tation of selectional preferences.
Our solution fits into the general category
of generative probabilistic models, which model
each relation/argument combination as being gen-
erated by a latent class variable. These classes
are automatically learned from the data. This re-
tains the class-based flavor of the problem, with-
out the knowledge limitations of the explicit class-
based approaches. Probably the closest to our
work is a model proposed by Rooth et al (1999),
in which each class corresponds to a multinomial
over relations and arguments and EM is used to
learn the parameters of the model. In contrast,
we use a LinkLDA framework in which each re-
lation is associated with a corresponding multi-
nomial distribution over classes, and each argu-
ment is drawn from a class-specific distribution
over words; LinkLDA captures co-occurrence of
classes in the two arguments. Additionally we
perform full Bayesian inference using collapsed
Gibbs sampling, in which parameters are inte-
grated out (Griffiths and Steyvers, 2004).
Recently, Bergsma et. al. (2008) proposed the
first discriminative approach to selectional prefer-
ences. Their insight that pseudo-negative exam-
ples could be used as training data allows the ap-
plication of an SVM classifier, which makes use of
many features in addition to the relation-argument
co-occurrence frequencies used by other meth-
ods. They automatically generated positive and
negative examples by selecting arguments having
high and low mutual information with the rela-
tion. Since it is a discriminative approach it is
amenable to feature engineering, but needs to be
retrained and tuned for each task. On the other
hand, generative models produce complete prob-
ability distributions of the data, and hence can be
integrated with other systems and tasks in a more
principled manner (see Sections 4.2.2 and 4.3.1).
Additionally, unlike LDA-SP Bergsma et al?s sys-
tem doesn?t produce human-interpretable topics.
Finally, we note that LDA-SP and Bergsma?s sys-
tem are potentially complimentary ? the output of
LDA-SP could be used to generate higher-quality
training data for Bergsma, potentially improving
their results.
425
Topic models such as LDA (Blei et al, 2003)
and its variants have recently begun to see use
in many NLP applications such as summarization
(Daume? III and Marcu, 2006), document align-
ment and segmentation (Chen et al, 2009), and
inferring class-attribute hierarchies (Reisinger and
Pasca, 2009). Our particular model, LinkLDA, has
been applied to a few NLP tasks such as simul-
taneously modeling the words appearing in blog
posts and users who will likely respond to them
(Yano et al, 2009), modeling topic-aligned arti-
cles in different languages (Mimno et al, 2009),
and word sense induction (Brody and Lapata,
2009).
Finally, we highlight two systems, developed
independently of our own, which apply LDA-style
models to similar tasks. O? Se?aghdha (2010) pro-
poses a series of LDA-style models for the task
of computing selectional preferences. This work
learns selectional preferences between the fol-
lowing grammatical relations: verb-object, noun-
noun, and adjective-noun. It also focuses on
jointly modeling the generation of both predicate
and argument, and evaluation is performed on a
set of human-plausibility judgments obtaining im-
pressive results against Keller and Lapata?s (2003)
Web hit-count based system. Van Durme and
Gildea (2009) proposed applying LDA to general
knowledge templates extracted using the KNEXT
system (Schubert and Tong, 2003). In contrast,
our work uses LinkLDA and focuses on modeling
multiple arguments of a relation (e.g., the subject
and direct object of a verb).
3 Topic Models for Selectional Prefs.
We present a series of topic models for the task of
computing selectional preferences. These models
vary in the amount of independence they assume
between a1 and a2. At one extreme is Indepen-
dentLDA, a model which assumes that both a1 and
a2 are generated completely independently. On
the other hand, JointLDA, the model at the other
extreme (Figure 1) assumes both arguments of a
specific extraction are generated based on a single
hidden variable z. LinkLDA (Figure 2) lies be-
tween these two extremes, and as demonstrated in
Section 4, it is the best model for our relation data.
We are given a set R of binary relations and a
corpus D = {r(a1, a2)} of extracted instances for
these relations. 2 Our task is to compute, for each
argument ai of each relation r, a set of usual ar-
gument values (noun phrases) that it takes. For
example, for the relation is headquartered in the
first argument set will include companies like Mi-
crosoft, Intel, General Motors and second argu-
ment will favor locations like New York, Califor-
nia, Seattle.
3.1 IndependentLDA
We first describe the straightforward application
of LDA to modeling our corpus of extracted rela-
tions. In this case two separate LDA models are
used to model a1 and a2 independently.
In the generative model for our data, each rela-
tion r has a corresponding multinomial over topics
?r, drawn from a Dirichlet. For each extraction, a
hidden topic z is first picked according to ?r, and
then the observed argument a is chosen according
to the multinomial ?z .
Readers familiar with topic modeling terminol-
ogy can understand our approach as follows: we
treat each relation as a document whose contents
consist of a bags of words corresponding to all the
noun phrases observed as arguments of the rela-
tion in our corpus. Formally, LDA generates each
argument in the corpus of relations as follows:
for each topic t = 1 . . . T do
Generate ?t according to symmetric Dirich-
let distribution Dir(?).
end for
for each relation r = 1 . . . |R| do
Generate ?r according to Dirichlet distribu-
tion Dir(?).
for each tuple i = 1 . . . Nr do
Generate zr,i from Multinomial(?r).
Generate the argument ar,i from multi-
nomial ?zr,i .
end for
end for
One weakness of IndependentLDA is that it
doesn?t jointly model a1 and a2 together. Clearly
this is undesirable, as information about which
topics one of the arguments favors can help inform
the topics chosen for the other. For example, class
pairs such as (team, game), (politician, political is-
sue) form much more plausible selectional prefer-
ences than, say, (team, political issue), (politician,
game).
2We focus on binary relations, though the techniques pre-
sented in the paper are easily extensible to n-ary relations.
426
3.2 JointLDA
As a more tightly coupled alternative, we first
propose JointLDA, whose graphical model is de-
picted in Figure 1. The key difference in JointLDA
(versus LDA) is that instead of one, it maintains
two sets of topics (latent distributions over words)
denoted by ? and ?, one for classes of each ar-
gument. A topic id k represents a pair of topics,
?k and ?k, that co-occur in the arguments of ex-
tracted relations. Common examples include (Per-
son, Location), (Politician, Political issue), etc.
The hidden variable z = k indicates that the noun
phrase for the first argument was drawn from the
multinomial ?k, and that the second argument was
drawn from ?k. The per-relation distribution ?r is
a multinomial over the topic ids and represents the
selectional preferences, both for arg1s and arg2s
of a relation r.
Although JointLDA has many desirable proper-
ties, it has some drawbacks as well. Most notably,
in JointLDA topics correspond to pairs of multi-
nomials (?k, ?k); this leads to a situation in which
multiple redundant distributions are needed to rep-
resent the same underlying semantic class. For
example consider the case where we we need to
represent the following selectional preferences for
our corpus of relations: (person, location), (per-
son, organization), and (person, crime). Because
JointLDA requires a separate pair of multinomials
for each topic, it is forced to use 3 separate multi-
nomials to represent the class person, rather than
learning a single distribution representing person
and choosing 3 different topics for a2. This results
in poor generalization because the data for a single
class is divided into multiple topics.
In order to address this problem while maintain-
ing the sharing of influence between a1 and a2, we
next present LinkLDA, which represents a com-
promise between IndependentLDA and JointLDA.
LinkLDA is more flexible than JointLDA, allow-
ing different topics to be chosen for a1, and a2,
however still models the generation of topics from
the same distribution for a given relation.
3.3 LinkLDA
Figure 2 illustrates the LinkLDA model in the
plate notation, which is analogous to the model
in (Erosheva et al, 2004). In particular note that
each ai is drawn from a different hidden topic zi,
however the zi?s are drawn from the same distri-
bution ?r for a given relation r. To facilitate learn-
?
a
1
a
2
?
|R|
N
?
?
1
?
T
?
2
z
Figure 1: JointLDA
?
z
1
z
2
a
1
a
2
?
|R|
N
?
?
1
?
T
?
2
Figure 2: LinkLDA
ing related topic pairs between arguments we em-
ploy a sparse prior over the per-relation topic dis-
tributions. Because a few topics are likely to be
assigned most of the probability mass for a given
relation it is more likely (although not necessary)
that the same topic number k will be drawn for
both arguments.
When comparing LinkLDA with JointLDA the
better model may not seem immediately clear. On
the one hand, JointLDA jointly models the gen-
eration of both arguments in an extracted tuple.
This allows one argument to help disambiguate
the other in the case of ambiguous relation strings.
LinkLDA, however, is more flexible; rather than
requiring both arguments to be generated from one
of |Z| possible pairs of multinomials (?z, ?z), Lin-
kLDA allows the arguments of a given extraction
to be generated from |Z|2 possible pairs. Thus,
instead of imposing a hard constraint that z1 =
z2 (as in JointLDA), LinkLDA simply assigns a
higher probability to states in which z1 = z2, be-
cause both hidden variables are drawn from the
same (sparse) distribution ?r. LinkLDA can thus
re-use argument classes, choosing different com-
binations of topics for the arguments if it fits the
data better. In Section 4 we show experimentally
that LinkLDA outperforms JointLDA (and Inde-
pendentLDA) by wide margins. We use LDA-SP
to refer to LinkLDA in all the experiments below.
3.4 Inference
For all the models we use collapsed Gibbs sam-
pling for inference in which each of the hid-
den variables (e.g., zr,i,1 and zr,i,2 in LinkLDA)
are sampled sequentially conditioned on a full-
assignment to all others, integrating out the param-
eters (Griffiths and Steyvers, 2004). This produces
robust parameter estimates, as it allows computa-
tion of expectations over the posterior distribution
427
as opposed to estimating maximum likelihood pa-
rameters. In addition, the integration allows the
use of sparse priors, which are typically more ap-
propriate for natural language data. In all exper-
iments we use hyperparameters ? = ?1 = ?2 =
0.1. We generated initial code for our samplers us-
ing the Hierarchical Bayes Compiler (Daume III,
2007).
3.5 Advantages of Topic Models
There are several advantages to using topic mod-
els for our task. First, they naturally model the
class-based nature of selectional preferences, but
don?t take a pre-defined set of classes as input.
Instead, they compute the classes automatically.
This leads to better lexical coverage since the is-
sue of matching a new argument to a known class
is side-stepped. Second, the models naturally han-
dle ambiguous arguments, as they are able to as-
sign different topics to the same phrase in different
contexts. Inference in these models is also scalable
? linear in both the size of the corpus as well as
the number of topics. In addition, there are several
scalability enhancements such as SparseLDA (Yao
et al, 2009), and an approximation of the Gibbs
Sampling procedure can be efficiently parallelized
(Newman et al, 2009). Finally we note that, once
a topic distribution has been learned over a set of
training relations, one can efficiently apply infer-
ence to unseen relations (Yao et al, 2009).
4 Experiments
We perform three main experiments to assess the
quality of the preferences obtained using topic
models. The first is a task-independent evaluation
using a pseudo-disambiguation experiment (Sec-
tion 4.2), which is a standard way to evaluate the
quality of selectional preferences (Rooth et al,
1999; Erk, 2007; Bergsma et al, 2008). We use
this experiment to compare the various topic mod-
els as well as the best model with the known state
of the art approaches to selectional preferences.
Secondly, we show significant improvements to
performance at an end-task of textual inference in
Section 4.3. Finally, we report on the quality of
a large database of Wordnet-based preferences ob-
tained after manually associating our topics with
Wordnet classes (Section 4.4).
4.1 Generalization Corpus
For all experiments we make use of a corpus
of r(a1, a2) tuples, which was automatically ex-
tracted by TEXTRUNNER (Banko and Etzioni,
2008) from 500 million Web pages.
To create a generalization corpus from this
large dataset. We first selected 3,000 relations
from the middle of the tail (we used the 2,000-
5,000 most frequent ones)3 and collected all in-
stances. To reduce sparsity, we discarded all tu-
ples containing an NP that occurred fewer than 50
times in the data. This resulted in a vocabulary of
about 32,000 noun phrases, and a set of about 2.4
million tuples in our generalization corpus.
We inferred topic-argument and relation-topic
multinomials (?, ?, and ?) on the generalization
corpus by taking 5 samples at a lag of 50 after
a burn in of 750 iterations. Using multiple sam-
ples introduces the risk of topic drift due to lack
of identifiability, however we found this to not be
a problem in practice. During development we
found that the topics tend to remain stable across
multiple samples after sufficient burn in, and mul-
tiple samples improved performance. Table 1 lists
sample topics and high ranked words for each (for
both arguments) as well as relations favoring those
topics.
4.2 Task Independent Evaluation
We first compare the three LDA-based approaches
to each other and two state of the art similarity
based systems (Erk, 2007) (using mutual informa-
tion and Jaccard similarity respectively). These
similarity measures were shown to outperform the
generative model of Rooth et al (1999), as well
as class-based methods such as Resnik?s. In this
pseudo-disambiguation experiment an observed
tuple is paired with a pseudo-negative, which
has both arguments randomly generated from the
whole vocabulary (according to the corpus-wide
distribution over arguments). The task is, for each
relation-argument pair, to determine whether it is
observed, or a random distractor.
4.2.1 Test Set
For this experiment we gathered a primary corpus
by first randomly selecting 100 high-frequency re-
lations not in the generalization corpus. For each
relation we collected all tuples containing argu-
ments in the vocabulary. We held out 500 ran-
domly selected tuples as the test set. For each tu-
3Many of the most frequent relations have very weak se-
lectional preferences, and thus provide little signal for infer-
ring meaningful topics. For example, the relations has and is
can take just about any arguments.
428
Topic t Arg1 Relations which assign
highest probability to t
Arg2
18 The residue - The mixture - The reaction
mixture - The solution - the mixture - the re-
action mixture - the residue - The reaction -
the solution - The filtrate - the reaction - The
product - The crude product - The pellet -
The organic layer - Thereto - This solution
- The resulting solution - Next - The organic
phase - The resulting mixture - C. )
was treated with, is
treated with, was
poured into, was
extracted with, was
purified by, was di-
luted with, was filtered
through, is disolved in,
is washed with
EtOAc - CH2Cl2 - H2O - CH.sub.2Cl.sub.2
- H.sub.2O - water - MeOH - NaHCO3 -
Et2O - NHCl - CHCl.sub.3 - NHCl - drop-
wise - CH2Cl.sub.2 - Celite - Et.sub.2O -
Cl.sub.2 - NaOH - AcOEt - CH2C12 - the
mixture - saturated NaHCO3 - SiO2 - H2O
- N hydrochloric acid - NHCl - preparative
HPLC - to0 C
151 the Court - The Court - the Supreme Court
- The Supreme Court - this Court - Court
- The US Supreme Court - the court - This
Court - the US Supreme Court - The court
- Supreme Court - Judge - the Court of Ap-
peals - A federal judge
will hear, ruled in, de-
cides, upholds, struck
down, overturned,
sided with, affirms
the case - the appeal - arguments - a case -
evidence - this case - the decision - the law
- testimony - the State - an interview - an
appeal - cases - the Court - that decision -
Congress - a decision - the complaint - oral
arguments - a law - the statute
211 President Bush - Bush - The President -
Clinton - the President - President Clinton
- President George W. Bush - Mr. Bush -
The Governor - the Governor - Romney -
McCain - The White House - President -
Schwarzenegger - Obama
hailed, vetoed, pro-
moted, will deliver,
favors, denounced,
defended
the bill - a bill - the decision - the war - the
idea - the plan - the move - the legislation -
legislation - the measure - the proposal - the
deal - this bill - a measure - the program -
the law - the resolution - efforts - the agree-
ment - gay marriage - the report - abortion
224 Google - Software - the CPU - Clicking -
Excel - the user - Firefox - System - The
CPU - Internet Explorer - the ability - Pro-
gram - users - Option - SQL Server - Code
- the OS - the BIOS
will display, to store, to
load, processes, cannot
find, invokes, to search
for, to delete
data - files - the data - the file - the URL -
information - the files - images - a URL - the
information - the IP address - the user - text
- the code - a file - the page - IP addresses -
PDF files - messages - pages - an IP address
Table 1: Example argument lists from the inferred topics. For each topic number t we list the most
probable values according to the multinomial distributions for each argument (?t and ?t). The middle
column reports a few relations whose inferred topic distributions ?r assign highest probability to t.
ple r(a1, a2) in the held-out set, we removed all
tuples in the training set containing either of the
rel-arg pairs, i.e., any tuple matching r(a1, ?) or
r(?, a2). Next we used collapsed Gibbs sampling
to infer a distribution over topics, ?r, for each of
the relations in the primary corpus (based solely
on tuples in the training set) using the topics from
the generalization corpus.
For each of the 500 observed tuples in the test-
set we generated a pseudo-negative tuple by ran-
domly sampling two noun phrases from the distri-
bution of NPs in both corpora.
4.2.2 Prediction
Our prediction system needs to determine whether
a specific relation-argument pair is admissible ac-
cording to the selectional preferences or is a ran-
dom distractor (D). Following previous work, we
perform this experiment independently for the two
relation-argument pairs (r, a1) and (r, a2).
We first compute the probability of observing
a1 for first argument of relation r given that it is
not a distractor, P (a1|r,?D), which we approx-
imate by its probability given an estimate of the
parameters inferred by our model, marginalizing
over hidden topics t. The analysis for the second
argument is similar.
P (a1|r,?D) ? PLDA(a1|r) =
TX
t=0
P (a1|t)P (t|r)
=
TX
t=0
?t(a1)?r(t)
A simple application of Bayes Rule gives the
probability that a particular argument is not a
distractor. Here the distractor-related proba-
bilities are independent of r, i.e., P (D|r) =
P (D), P (a1|D, r) = P (a1|D), etc. We estimate
P (a1|D) according to their frequency in the gen-
eralization corpus.
P (?D|r, a1) =
P (?D|r)P (a1|r,?D)
P (a1|r)
?
P (?D)PLDA(a1|r)
P (D)P (a1|D) + P (?D)PLDA(a1|r)
4.2.3 Results
Figure 3 plots the precision-recall curve for the
pseudo-disambiguation experiment comparing the
three different topic models. LDA-SP, which uses
LinkLDA, substantially outperforms both Inde-
pendentLDA and JointLDA.
Next, in figure 4, we compare LDA-SP with
mutual information and Jaccard similarities us-
ing both the generalization and primary corpus for
429
0.0 0.2 0.4 0.6 0.8 1.00
.4
0.6
0.8
1.0
recall
prec
ision
LDA?SPIndependentLDAJointLDA
Figure 3: Comparison of LDA-based approaches
on the pseudo-disambiguation task. LDA-SP (Lin-
kLDA) substantially outperforms the other mod-
els.
0.0 0.2 0.4 0.6 0.8 1.00
.4
0.6
0.8
1.0
recall
prec
ision
LDA?SPJaccardMutual Information
Figure 4: Comparison to similarity-based selec-
tional preference systems. LDA-SP obtains 85%
higher recall at precision 0.9.
computation of similarities. We find LDA-SP sig-
nificantly outperforms these methods. Its edge is
most noticed at high precisions; it obtains 85%
more recall at 0.9 precision compared to mutual
information. Overall LDA-SP obtains an 15% in-
crease in the area under precision-recall curve over
mutual information. All three systems? AUCs are
shown in Table 2; LDA-SP?s improvements over
both Jaccard and mutual information are highly
significant with a significance level less than 0.01
using a paired t-test.
In addition to a superior performance in se-
lectional preference evaluation LDA-SP also pro-
duces a set of coherent topics, which can be use-
ful in their own right. For instance, one could use
them for tasks such as set-expansion (Carlson et
al., 2010) or automatic thesaurus induction (Et-
LDA-SP MI-Sim Jaccard-Sim
AUC 0.833 0.727 0.711
Table 2: Area under the precision recall curve.
LDA-SP?s AUC is significantly higher than both
similarity-based methods according to a paired t-
test with a significance level below 0.01.
zioni et al, 2005; Kozareva et al, 2008).
4.3 End Task Evaluation
We now evaluate LDA-SP?s ability to improve per-
formance at an end-task. We choose the task of
improving textual entailment by learning selec-
tional preferences for inference rules and filtering
inferences that do not respect these. This applica-
tion of selectional preferences was introduced by
Pantel et. al. (2007). For now we stick to infer-
ence rules of the form r1(a1, a2) ? r2(a1, a2),
though our ideas are more generally applicable to
more complex rules. As an example, the rule (X
defeats Y) ? (X plays Y) holds when X and Y
are both sports teams, however fails to produce a
reasonable inference if X and Y are Britain and
Nazi Germany respectively.
4.3.1 Filtering Inferences
In order for an inference to be plausible, both re-
lations must have similar selectional preferences,
and further, the arguments must obey the selec-
tional preferences of both the antecedent r1 and
the consequent r2.4 Pantel et al (2007) made
use of these intuitions by producing a set of class-
based selectional preferences for each relation,
then filtering out any inferences where the argu-
ments were incompatible with the intersection of
these preferences. In contrast, we take a proba-
bilistic approach, evaluating the quality of a spe-
cific inference by measuring the probability that
the arguments in both the antecedent and the con-
sequent were drawn from the same hidden topic
in our model. Note that this probability captures
both the requirement that the antecedent and con-
sequent have similar selectional preferences, and
that the arguments from a particular instance of the
rule?s application match their overlap.
We use zri,j to denote the topic that generates
the jth argument of relation ri. The probability
that the two arguments a1, a2 were drawn from
the same hidden topic factorizes as follows due to
the conditional independences in our model:5
P (zr1,1 = zr2,1, zr1,2 = zr2,2|a1, a2) =
P (zr1,1 = zr2,1|a1)P (zr1,2 = zr2,2|a2)
4Similarity-based and discriminative methods are not ap-
plicable to this task as they offer no straightforward way
to compare the similarity between selectional preferences of
two relations.
5Note that all probabilities are conditioned on an estimate
of the parameters ?, ?, ? from our model, which are omitted
for compactness.
430
To compute each of these factors we simply
marginalize over the hidden topics:
P (zr1,j = zr2,j |aj) =
TX
t=1
P (zr1,j = t|aj)P (zr2,j = t|aj)
where P (z = t|a) can be computed using
Bayes rule. For example,
P (zr1,1 = t|a1) =
P (a1|zr1,1 = t)P (zr1,1 = t)
P (a1)
=
?t(a1)?r1(t)
P (a1)
4.3.2 Experimental Conditions
In order to evaluate LDA-SP?s ability to filter in-
ferences based on selectional preferences we need
a set of inference rules between the relations in
our corpus. We therefore mapped the DIRT In-
ference rules (Lin and Pantel, 2001), (which con-
sist of pairs of dependency paths) to TEXTRUN-
NER relations as follows. We first gathered all in-
stances in the generalization corpus, and for each
r(a1, a2) created a corresponding simple sentence
by concatenating the arguments with the relation
string between them. Each such simple sentence
was parsed using Minipar (Lin, 1998). From
the parses we extracted all dependency paths be-
tween nouns that contain only words present in
the TEXTRUNNER relation string. These depen-
dency paths were then matched against each pair
in the DIRT database, and all pairs of associated
relations were collected producing about 26,000
inference rules.
Following Pantel et al (2007) we randomly
sampled 100 inference rules. We then automati-
cally filtered out any rules which contained a nega-
tion, or for which the antecedent and consequent
contained a pair of antonyms found in WordNet
(this left us with 85 rules). For each rule we col-
lected 10 random instances of the antecedent, and
generated the consequent. We randomly sampled
300 of these inferences to hand-label.
4.3.3 Results
In figure 5 we compare the precision and recall of
LDA-SP against the top two performing systems
described by Pantel et al (ISP.IIM-? and ISP.JIM,
both using the CBC clusters (Pantel, 2003)). We
find that LDA-SP achieves both higher precision
and recall than ISP.IIM-?. It is also able to achieve
the high-precision point of ISP.JIM and can trade
precision to get a much larger recall.
0.0 0.2 0.4 0.6 0.8 1.00
.4
0.6
0.8
1.0
recall
prec
isio
n
X
O
 
XO
LDA?SPISP.JIMISP.IIM?OR
Figure 5: Precision and recall on the inference fil-
tering task.
Top 10 Inference Rules Ranked by LDA-SP
antecedent consequent KL-div
will begin at will start at 0.014999
shall review shall determine 0.129434
may increase may reduce 0.214841
walk from walk to 0.219471
consume absorb 0.240730
shall keep shall maintain 0.264299
shall pay to will notify 0.290555
may apply for may obtain 0.313916
copy download 0.316502
should pay must pay 0.371544
Bottom 10 Inference Rules Ranked by LDA-SP
antecedent consequent KL-div
lose to shall take 10.011848
should play could do 10.028904
could play get in 10.048857
will start at move to 10.060994
shall keep will spend 10.105493
should play get in 10.131299
shall pay to leave for 10.131364
shall keep return to 10.149797
shall keep could do 10.178032
shall maintain have spent 10.221618
Table 3: Top 10 and Bottom 10 ranked inference
rules ranked by LDA-SPafter automatically filter-
ing out negations and antonyms (using WordNet).
In addition we demonstrate LDA-SP?s abil-
ity to rank inference rules by measuring the
Kullback Leibler Divergence6 between the topic-
distributions of the antecedent and consequent, ?r1
and ?r2 respectively. Table 3 shows the top 10 and
bottom 10 rules out of the 26,000 ranked by KL
Divergence after automatically filtering antonyms
(using WordNet) and negations. For slight varia-
tions in rules (e.g., symmetric pairs) we mention
only one example to show more variety.
6KL-Divergence is an information-theoretic measure of
the similarity between two probability distributions, and de-
fined as follows: KL(P ||Q) =
P
x P (x) log
P (x)
Q(x) .
431
4.4 A Repository of Class-Based Preferences
Finally we explore LDA-SP?s ability to produce a
repository of human interpretable class-based se-
lectional preferences. As an example, for the re-
lation was born in, we would like to infer that
the plausible arguments include (person, location)
and (person, date).
Since we already have a set of topics, our
task reduces to mapping the inferred topics to an
equivalent class in a taxonomy (e.g., WordNet).
We experimented with automatic methods such
as Resnik?s, but found them to have all the same
problems as directly applying these approaches to
the SP task.7 Guided by the fact that we have a
relatively small number of topics (600 total, 300
for each argument) we simply chose to label them
manually. By labeling this small number of topics
we can infer class-based preferences for an arbi-
trary number of relations.
In particular, we applied a semi-automatic
scheme to map topics to WordNet. We first applied
Resnik?s approach to automatically shortlist a few
candidate WordNet classes for each topic. We then
manually picked the best class from the shortlist
that best represented the 20 top arguments for a
topic (similar to Table 1). We marked all incoher-
ent topics with a special symbol ?. This process
took one of the authors about 4 hours to complete.
To evaluate how well our topic-class associa-
tions carry over to unseen relations we used the
same random sample of 100 relations from the
pseudo-disambiguation experiment.8 For each ar-
gument of each relation we picked the top two top-
ics according to frequency in the 5 Gibbs samples.
We then discarded any topics which were labeled
with ?; this resulted in a set of 236 predictions. A
few examples are displayed in table 4.
We evaluated these classes and found the accu-
racy to be around 0.88. We contrast this with Pan-
tel?s repository,9 the only other released database
of selectional preferences to our knowledge. We
evaluated the same 100 relations from his website
and tagged the top 2 classes for each argument and
evaluated the accuracy to be roughly 0.55.
7Perhaps recent work on automatic coherence ranking
(Newman et al, 2010) and labeling (Mei et al, 2007) could
produce better results.
8Recall that these 100 were not part of the original 3,000
in the generalization corpus, and are, therefore, representative
of new ?unseen? relations.
9http://demo.patrickpantel.com/
Content/LexSem/paraphrase.htm
arg1 class relation arg2 class
politician#1 was running for leader#1
people#1 will love show#3
organization#1 has responded to accusation#2
administrative unit#1 has appointed administrator#3
Table 4: Class-based Selectional Preferences.
We emphasize that tagging a pair of class-based
preferences is a highly subjective task, so these re-
sults should be treated as preliminary. Still, these
early results are promising. We wish to undertake
a larger scale study soon.
5 Conclusions and Future Work
We have presented an application of topic mod-
eling to the problem of automatically computing
selectional preferences. Our method, LDA-SP,
learns a distribution over topics for each rela-
tion while simultaneously grouping related words
into these topics. This approach is capable of
producing human interpretable classes, however,
avoids the drawbacks of traditional class-based ap-
proaches (poor lexical coverage and ambiguity).
LDA-SP achieves state-of-the-art performance on
predictive tasks such as pseudo-disambiguation,
and filtering incorrect inferences.
Because LDA-SP generates a complete proba-
bilistic model for our relation data, its results are
easily applicable to many other tasks such as iden-
tifying similar relations, ranking inference rules,
etc. In the future, we wish to apply our model
to automatically discover new inference rules and
paraphrases.
Finally, our repository of selectional pref-
erences for 10,000 relations is available at
http://www.cs.washington.edu/
research/ldasp.
Acknowledgments
We would like to thank Tim Baldwin, Colin
Cherry, Jesse Davis, Elena Erosheva, Stephen
Soderland, Dan Weld, in addition to the anony-
mous reviewers for helpful comments on a previ-
ous draft. This research was supported in part by
NSF grant IIS-0803481, ONR grant N00014-08-
1-0431, DARPA contract FA8750-09-C-0179, a
National Defense Science and Engineering Grad-
uate (NDSEG) Fellowship 32 CFR 168a, and car-
ried out at the University of Washington?s Turing
Center.
432
References
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
ACL-08: HLT.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2008. Discriminative learning of selectional pref-
erence from unlabeled text. In EMNLP.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In EACL, pages 103?111,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Andrew Carlson, Justin Betteridge, Richard C. Wang,
Estevam R. Hruschka Jr., and Tom M. Mitchell.
2010. Coupled semi-supervised learning for infor-
mation extraction. In WSDM 2010.
Harr Chen, S. R. K. Branavan, Regina Barzilay, and
David R. Karger. 2009. Global models of document
structure using latent permutations. In NAACL.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Comput. Linguist.
Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.
1999. Similarity-based models of word cooccur-
rence probabilities. In Machine Learning.
Hal Daume? III and Daniel Marcu. 2006. Bayesian
query-focused summarization. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics.
Hal Daume III. 2007. hbc: Hierarchical bayes com-
piler. http://hal3.name/hbc.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics.
Elena Erosheva, Stephen Fienberg, and John Lafferty.
2004. Mixed-membership models of scientific pub-
lications. Proceedings of the National Academy of
Sciences of the United States of America.
Oren Etzioni, Michael Cafarella, Doug Downey,
Ana maria Popescu, Tal Shaked, Stephen Soderl,
Daniel S. Weld, and Alex Yates. 2005. Unsuper-
vised named-entity extraction from the web: An ex-
perimental study. Artificial Intelligence.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Comput. Linguist.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proc Natl Acad Sci U S A.
Frank Keller and Mirella Lapata. 2003. Using the web
to obtain frequencies for unseen bigrams. Comput.
Linguist.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In ACL-08: HLT.
Hang Li and Naoki Abe. 1998. Generalizing case
frames using a thesaurus and the mdl principle.
Comput. Linguist.
Dekang Lin and Patrick Pantel. 2001. Dirt-discovery
of inference rules from text. In KDD.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proc. Workshop on the Evaluation of
Parsing Systems.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic
models. In KDD.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In EMNLP.
David Newman, Arthur Asuncion, Padhraic Smyth,
and Max Welling. 2009. Distributed algorithms for
topic models. JMLR.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of topic
coherence. In NAACL-HLT.
Diarmuid O? Se?aghdha. 2010. Latent variable mod-
els of selectional preference. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard H. Hovy. 2007.
Isp: Learning inferential selectional preferences. In
HLT-NAACL.
Patrick Andre Pantel. 2003. Clustering by commit-
tee. Ph.D. thesis, University of Alberta, Edmonton,
Alta., Canada.
Joseph Reisinger and Marius Pasca. 2009. Latent vari-
able models of concept-attribute attachment. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP.
P. Resnik. 1996. Selectional constraints: an
information-theoretic model and its computational
realization. Cognition.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proc. of the ACL SIGLEX Work-
shop on Tagging Text with Lexical Semantics: Why,
What, and How?
433
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via em-based clustering. In
Proceedings of the 37th annual meeting of the Asso-
ciation for Computational Linguistics on Computa-
tional Linguistics.
Lenhart Schubert and Matthew Tong. 2003. Extract-
ing and evaluating general world knowledge from
the brown corpus. In In Proc. of the HLT-NAACL
Workshop on Text Meaning, pages 7?13.
Benjamin Van Durme and Daniel Gildea. 2009. Topic
models for corpus-centric knowledge generalization.
In Technical Report TR-946, Department of Com-
puter Science, University of Rochester, Rochester.
Tae Yano, William W. Cohen, and Noah A. Smith.
2009. Predicting response to political blog posts
with topic models. In NAACL.
L. Yao, D. Mimno, and A. Mccallum. 2009. Effi-
cient methods for topic model inference on stream-
ing document collections. In KDD.
434
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 52?60,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Semantic Role Labeling for Open Information Extraction
Janara Christensen, Mausam, Stephen Soderland and Oren Etzioni
University of Washington, Seattle
Abstract
Open Information Extraction is a recent
paradigm for machine reading from arbitrary
text. In contrast to existing techniques, which
have used only shallow syntactic features, we
investigate the use of semantic features (se-
mantic roles) for the task of Open IE. We com-
pare TEXTRUNNER (Banko et al, 2007), a
state of the art open extractor, with our novel
extractor SRL-IE, which is based on UIUC?s
SRL system (Punyakanok et al, 2008). We
find that SRL-IE is robust to noisy heteroge-
neous Web data and outperforms TEXTRUN-
NER on extraction quality. On the other
hand, TEXTRUNNER performs over 2 orders
of magnitude faster and achieves good pre-
cision in high locality and high redundancy
extractions. These observations enable the
construction of hybrid extractors that output
higher quality results than TEXTRUNNER and
similar quality as SRL-IE in much less time.
1 Introduction
The grand challenge of Machine Reading (Etzioni
et al, 2006) requires, as a key step, a scalable
system for extracting information from large, het-
erogeneous, unstructured text. The traditional ap-
proaches to information extraction (e.g., (Soderland,
1999; Agichtein and Gravano, 2000)) do not oper-
ate at these scales, since they focus attention on a
well-defined small set of relations and require large
amounts of training data for each relation. The re-
cent Open Information Extraction paradigm (Banko
et al, 2007) attempts to overcome the knowledge
acquisition bottleneck with its relation-independent
nature and no manually annotated training data.
We are interested in the best possible technique
for Open IE. The TEXTRUNNER Open IE system
(Banko and Etzioni, 2008) employs only shallow
syntactic features in the extraction process. Avoid-
ing the expensive processing of deep syntactic anal-
ysis allowed TEXTRUNNER to process at Web scale.
In this paper, we explore the benefits of semantic
features and in particular, evaluate the application of
semantic role labeling (SRL) to Open IE.
SRL is a popular NLP task that has seen sig-
nificant progress over the last few years. The ad-
vent of hand-constructed semantic resources such as
Propbank and Framenet (Martha and Palmer, 2002;
Baker et al, 1998) have resulted in semantic role la-
belers achieving high in-domain precisions.
Our first observation is that semantically labeled
arguments in a sentence almost always correspond
to the arguments in Open IE extractions. Similarly,
the verbs often match up with Open IE relations.
These observations lead us to construct a new Open
IE extractor based on SRL. We use UIUC?s publicly
available SRL system (Punyakanok et al, 2008) that
is known to be competitive with the state of the art
and construct a novel Open IE extractor based on it
called SRL-IE.
We first need to evaluate SRL-IE?s effectiveness
in the context of large scale and heterogeneous input
data as found on the Web: because SRL uses deeper
analysis we expect SRL-IE to be much slower. Sec-
ond, SRL is trained on news corpora using a re-
source like Propbank, and so may face recall loss
due to out of vocabulary verbs and precision loss due
to different writing styles found on the Web.
In this paper we address several empirical ques-
52
tions. Can SRL-IE, our SRL based extractor,
achieve adequate precision/recall on the heteroge-
neous Web text? What factors influence the relative
performance of SRL-IE vs. that of TEXTRUNNER
(e.g., n-ary vs. binary extractions, redundancy, local-
ity, sentence length, out of vocabulary verbs, etc.)?
In terms of performance, what are the relative trade-
offs between the two? Finally, is it possible to design
a hybrid between the two systems to get the best of
both the worlds? Our results show that:
1. SRL-IE is surprisingly robust to noisy hetero-
geneous data and achieves high precision and
recall on the Open IE task on Web text.
2. SRL-IE outperforms TEXTRUNNER along di-
mensions such as recall and precision on com-
plex extractions (e.g., n-ary relations).
3. TEXTRUNNER is over 2 orders of magnitude
faster, and achieves good precision for extrac-
tions with high system confidence or high lo-
cality or when the same fact is extracted from
multiple sentences.
4. Hybrid extractors that use a combination of
SRL-IE and TEXTRUNNER get the best of
both worlds. Our hybrid extractors make effec-
tive use of available time and achieve a supe-
rior balance of precision-recall, better precision
compared to TEXTRUNNER, and better recall
compared to both TEXTRUNNER and SRL-IE.
2 Background
Open Information Extraction: The recently pop-
ular Open IE (Banko et al, 2007) is an extraction
paradigm where the system makes a single data-
driven pass over its corpus and extracts a large
set of relational tuples without requiring any hu-
man input. These tuples attempt to capture the
salient relationships expressed in each sentence. For
instance, for the sentence, ?McCain fought hard
against Obama, but finally lost the election? an
Open IE system would extract two tuples <McCain,
fought (hard) against, Obama>, and <McCain, lost,
the election>. These tuples can be binary or n-ary,
where the relationship is expressed between more
than 2 entities such as <Gates Foundation, invested
(arg) in, 1 billion dollars, high schools>.
TEXTRUNNER is a state-of-the-art Open IE sys-
tem that performs extraction in three key steps. (1)
A self-supervised learner that outputs a CRF based
classifier (that uses unlexicalized features) for ex-
tracting relationships. The self-supervised nature al-
leviates the need for hand-labeled training data and
unlexicalized features help scale to the multitudes of
relations found on the Web. (2) A single pass extrac-
tor that uses shallow syntactic techniques like part of
speech tagging, noun phrase chunking and then ap-
plies the CRF extractor to extract relationships ex-
pressed in natural language sentences. The use of
shallow features makes TEXTRUNNER highly effi-
cient. (3) A redundancy based assessor that re-ranks
these extractions based on a probabilistic model of
redundancy in text (Downey et al, 2005). This ex-
ploits the redundancy of information in Web text and
assigns higher confidence to extractions occurring
multiple times. All these components enable TEX-
TRUNNER to be a high performance, general, and
high quality extractor for heterogeneous Web text.
Semantic Role Labeling: SRL is a common NLP
task that consists of detecting semantic arguments
associated with a verb in a sentence and their classi-
fication into different roles (such as Agent, Patient,
Instrument, etc.). Given the sentence ?The pearls
I left to my son are fake? an SRL system would
conclude that for the verb ?leave?, ?I? is the agent,
?pearls? is the patient and ?son? is the benefactor.
Because not all roles feature in each verb the roles
are commonly divided into meta-roles (A0-A7) and
additional common classes such as location, time,
etc. Each Ai can represent a different role based
on the verb, though A0 and A1 most often refer to
agents and patients respectively. Availability of lexi-
cal resources such as Propbank (Martha and Palmer,
2002), which annotates text with meta-roles for each
argument, has enabled significant progress in SRL
systems over the last few years.
Recently, there have been many advances in SRL
(Toutanova et al, 2008; Johansson and Nugues,
2008; Coppola et al, 2009; Moschitti et al, 2008).
We use UIUC-SRL as our base SRL system (Pun-
yakanok et al, 2008). Our choice of the system is
guided by the fact that its code is freely available and
it is competitive with state of the art (it achieved the
highest F1 score on the CoNLL-2005 shared task).
UIUC-SRL operates in four key steps: pruning,
argument identification, argument classification and
53
inference. Pruning involves using a full parse tree
and heuristic rules to eliminate constituents that are
unlikely to be arguments. Argument identification
uses a classifier to identify constituents that are po-
tential arguments. In argument classification, an-
other classifier is used, this time to assign role labels
to the candidates identified in the previous stage. Ar-
gument information is not incorporated across argu-
ments until the inference stage, which uses an inte-
ger linear program to make global role predictions.
3 SRL-IE
Our key insight is that semantically labeled argu-
ments in a sentence almost always correspond to the
arguments in Open IE extractions. Thus, we can
convert the output of UIUC-SRL into an Open IE
extraction. We illustrate this conversion process via
an example.
Given the sentence, ?Eli Whitney created the cot-
ton gin in 1793,? TEXTRUNNER extracts two tuples,
one binary and one n-ary, as follows:
binary tuple:
arg0 Eli Whitney
rel created
arg1 the cotton gin
n-ary tuple:
arg0 Eli Whitney
rel created (arg) in
arg1 the cotton gin
arg2 1793
UIUC-SRL labels constituents of a sentence with
the role they play in regards to the verb in the sen-
tence. UIUC-SRL will extract:
A0 Eli Whitney
verb created
A1 the cotton gin
temporal in 1793
To convert UIUC-SRL output to Open IE format,
SRL-IE treats the verb (along with its modifiers and
negation, if present) as the relation. Moreover, it
assumes SRL?s role-labeled arguments as the Open
IE arguments related to the relation. The arguments
here consist of all entities labeled Ai, as well as any
entities that are marked Direction, Location, or Tem-
poral. We order the arguments in the same order as
they are in the sentence and with regard to the re-
lation (except for direction, location and temporal,
which cannot be arg0 of an Open IE extraction and
are placed at the end of argument list). As we are
interested in relations, we consider only extractions
that have at least two arguments.
In doing this conversion, we naturally ignore part
of the semantic information (such as distinctions be-
tween various Ai?s) that UIUC-SRL provides. In
this conversion process an SRL extraction that was
correct in the original format will never be changed
to an incorrect Open IE extraction. However, an in-
correctly labeled SRL extraction could still convert
to a correct Open IE extraction, if the arguments
were correctly identified but incorrectly labeled.
Because of the methodology that TEXTRUNNER
uses to extract relations, for n-ary extractions of the
form <arg0, rel, arg1, ..., argN>, TEXTRUNNER
often extracts sub-parts <arg0, rel, arg1>, <arg0,
rel, arg1, arg2>, ..., <arg0, rel, arg1, ..., argN-1>.
UIUC-SRL, however, extracts at most only one re-
lation for each verb in the sentence. For a fair com-
parison, we create additional subpart extractions for
each UIUC-SRL extraction using a similar policy.
4 Qualitative Comparison of Extractors
In order to understand SRL-IE better, we first com-
pare with TEXTRUNNER in a variety of scenarios,
such as sentences with lists, complex sentences, sen-
tences with out of vocabulary verbs, etc.
Argument boundaries: SRL-IE is lenient in de-
ciding what constitutes an argument and tends to
err on the side of including too much rather than
too little; TEXTRUNNER is much more conservative,
sometimes to the extent of omitting crucial informa-
tion, particularly post-modifying clauses and PPs.
For example, TEXTRUNNER extracts <Bunsen, in-
vented, a device> from the sentence ?Bunsen in-
vented a device called the Spectroscope?. SRL-IE
includes the entire phrase ?a device called the Spec-
troscope? as the second argument. Generally, the
longer arguments in SRL-IE are more informative
than TEXTRUNNER?s succinct ones. On the other
hand, TEXTRUNNER?s arguments normalize better
leading to an effective use of redundancy in ranking.
Lists: In sentences with a comma-separated lists of
nouns, SRL-IE creates one extraction and treats the
entire list as the argument, whereas TEXTRUNNER
separates them into several relations, one for each
item in the list.
Out of vocabulary verbs: While we expected
54
TEXTRUNNER to handle unknown verbs with lit-
tle difficulty due to its unlexicalized nature, SRL-
IE could have had severe trouble leading to a lim-
ited applicability in the context of Web text. How-
ever, contrary to our expectations, UIUC-SRL has
a graceful policy to handle new verbs by attempt-
ing to identify A0 (the agent) and A1 (the patient)
and leaving out the higher numbered ones. In prac-
tice, this is very effective ? SRL-IE recognizes the
verb and its two arguments correctly in ?Larry Page
googled his name and launched a new revolution.?
Part-of-speech ambiguity: Both SRL-IE and
TEXTRUNNER have difficulty when noun phrases
have an identical spelling with a verb. For example,
the word ?write? when used as a noun causes trouble
for both systems. In the sentence, ?Be sure the file
has write permission.? SRL-IE and TEXTRUNNER
both extract <the file, write, permission>.
Complex sentences: Because TEXTRUNNER only
uses shallow syntactic features it has a harder time
on sentences with complex structure. SRL-IE,
because of its deeper processing, can better handle
complex syntax and long-range dependencies, al-
though occasionally complex sentences will create
parsing errors causing difficulties for SRL-IE.
N-ary relations: Both extractors suffer significant
quality loss in n-ary extractions compared to binary.
A key problem is prepositional phrase attachment,
deciding whether the phrase associates with arg1 or
with the verb.
5 Experimental Results
In our quantitative evaluation we attempt to answer
two key questions: (1) what is the relative difference
in performance of SRL-IE and TEXTRUNNER on
precision, recall and computation time? And, (2)
what factors influence the relative performance of
the two systems? We explore the first question in
Section 5.2 and the second in Section 5.3.
5.1 Dataset
Our goal is to explore the behavior of TEXTRUN-
NER and SRL-IE on a large scale dataset containing
redundant information, since redundancy has been
shown to immensely benefit Web-based Open IE ex-
tractors. At the same time, the test set must be a
manageable size, due to SRL-IE?s relatively slow
processing time. We constructed a test set that ap-
proximates Web-scale distribution of extractions for
five target relations ? invent, graduate, study, write,
and develop.
We created our test set as follows. We queried a
corpus of 500M Web documents for a sample of sen-
tences with these verbs (or their inflected forms, e.g.,
invents, invented, etc.). We then ran TEXTRUNNER
and SRL-IE on those sentences to find 200 distinct
values of arg0 for each target relation, 100 from each
system. We searched for at most 100 sentences that
contain both the verb-form and arg0. This resulted
in a test set of an average of 6,000 sentences per re-
lation, for a total of 29,842 sentences. We use this
test set for all experiments in this paper.
In order to compute precision and recall on this
dataset, we tagged extractions by TEXTRUNNER
and by SRL-IE as correct or errors. A tuple is cor-
rect if the arguments have correct boundaries and
the relation accurately expresses the relationship be-
tween all of the arguments. Our definition of cor-
rect boundaries does not favor either system over
the other. For instance, while TEXTRUNNER ex-
tracts <Bunsen, invented, a device> from the sen-
tence ?Bunsen invented a device called the Spectro-
scope?, and SRL-IE includes the entire phrase ?a
device called the Spectroscope? as the second argu-
ment, both extractions would be marked as correct.
Determining the absolute recall in these experi-
ments is precluded by the amount of hand labeling
necessary and the ambiguity of such a task. Instead,
we compute pseudo-recall by taking the union of
correct tuples from both methods as denominator.1
5.2 Relative Performance
Table 1 shows the performance of TEXTRUNNER
and SRL-IE on this dataset. Since TEXTRUNNER
can output different points on the precision-recall
curve based on the confidence of the CRF we choose
the point that maximizes F1.
SRL-IE achieved much higher recall at substan-
tially higher precision. This was, however, at the
cost of a much larger processing time. For our
dataset, TEXTRUNNER took 6.3 minutes and SRL-
1Tuples from the two systems are considered equivalent if
for the relation and each argument, the extracted phrases are
equal or if one phrase is contained within the phrase extracted
by the other system.
55
TEXTRUNNER SRL-IE
P R F1 P R F1
Binary 51.9 27.2 35.7 64.4 85.9 73.7
N-ary 39.3 28.2 32.9 54.4 62.7 58.3
All 47.9 27.5 34.9 62.1 79.9 69.9
Time 6.3 minutes 52.1 hours
Table 1: SRL-IE outperforms TEXTRUNNER in both re-
call and precision, but has over 2.5 orders of magnitude
longer run time.
IE took 52.1 hours ? roughly 2.5 orders of magni-
tude longer. We ran our experiments on quad-core
2.8GHz processors with 4GB of memory.
It is important to note that our results for TEX-
TRUNNER are different from prior results (Banko,
2009). This is primarily due to a few operational
criteria (such as focusing on proper nouns, filtering
relatively infrequent extractions) identified in prior
work that resulted in much higher precision, proba-
bly at significant cost of recall.
5.3 Comparison under Different Conditions
Although SRL-IE has higher overall precision,
there are some conditions under which TEXTRUN-
NER has superior precision. We analyze the perfor-
mance of these two systems along three key dimen-
sions: system confidence, redundancy, and locality.
System Confidence: TEXTRUNNER?s CRF-based
extractor outputs a confidence score which can be
varied to explore different points in the precision-
recall space. Figure 1(a) and Figure 2(a) report the
results from ranking extractions by this confidence
value. For both binary and n-ary extractions the con-
fidence value improves TEXTRUNNER?s precision
and for binary the high precision end has approxi-
mately the same precision as SRL-IE. Because of
its use of an integer linear program, SRL-IE does
not associate confidence values with extractions and
is shown as a point in these figures.
Redundancy: In this experiment we use the re-
dundancy of extractions as a measure of confidence.
Here redundancy is the number of times a relation
has been extracted from unique sentences. We com-
pute redundancy over normalized extractions, ignor-
ing noun modifiers, adverbs, and verb inflection.
Figure 1(b) and Figure 2(b) display the results for
binary and n-ary extractions, ranked by redundancy.
We use a log scale on the x-axis since high redun-
dancy extractions account for less than 1% of the
recall. For binary extractions, redundancy improved
TEXTRUNNER?s precision significantly, but at a dra-
matic loss in recall. TEXTRUNNER achieved 0.8
precision with 0.001 recall at redundancy of 10 and
higher. For highly redundant information (common
concepts, etc.) TEXTRUNNER has higher precision
than SRL-IE and would be the algorithm of choice.
In n-ary relations for TEXTRUNNER and in binary
relations for SRL-IE, redundancy actually hurts
precision. These extractions tend to be so specific
that genuine redundancy is rare, and the highest fre-
quency extractions are often systematic errors. For
example, the most frequent SRL-IE extraction was
<nothing, write, home>.
Locality: Our experiments with TEXTRUNNER led
us to discover a new validation scheme for the ex-
tractions ? locality. We observed that TEXTRUN-
NER?s shallow features can identify relations more
reliably when the arguments are closer to each other
in the sentence. Figure 1(c) and Figure 2(c) report
the results from ranking extractions by the number
of tokens that separate the first and last arguments.
We find a clear correlation between locality and
precision of TEXTRUNNER, with precision 0.77 at
recall 0.18 for TEXTRUNNER where the distance is
4 tokens or less for binary extractions. For n-ary re-
lations, TEXTRUNNER can match SRL-IE?s preci-
sion of 0.54 at recall 0.13. SRL-IE remains largely
unaffected by locality, probably due to the parsing
used in SRL.
6 A TEXTRUNNER SRL-IE Hybrid
We now present two hybrid systems that combine
the strengths of TEXTRUNNER (fast processing time
and high precision on a subset of sentences) with the
strengths of SRL-IE (higher recall and better han-
dling of long-range dependencies). This is set in a
scenario where we have a limited budget on com-
putational time and we need a high performance ex-
tractor that utilizes the available time efficiently.
Our approach is to run TEXTRUNNER on all sen-
tences, and then determine the order in which to pro-
cess sentences with SRL-IE. We can increase preci-
sion by filtering out TEXTRUNNER extractions that
are expected to have low precision.
56
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
1e?04 1e?03 1e?02 1e?01 1e+00
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
Figure 1: Ranking mechanisms for binary relations. (a) The confidence specified by the CRF improves TEXTRUN-
NER?s precision. (b) For extractions with highest redundancy, TEXTRUNNER has higher precision than SRL-IE. Note
the log scale for the x-axis. (c) Ranking by the distance between arguments gives a large boost to TEXTRUNNER?s
precision.
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
1e?04 1e?03 1e?02 1e?01 1e+00
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
Figure 2: Ranking mechanisms for n-ary relations. (a) Ranking by confidence gives a slight boost to TEXTRUNNER?s
precision. (b) Redundancy helps SRL-IE, but not TEXTRUNNER. Note the log scale for the x-axis. (c) Ranking by
distance between arguments raises precision for TEXTRUNNER and SRL-IE.
A naive hybrid will run TEXTRUNNER over all
the sentences and use the remaining time to run
SRL-IE on a random subset of the sentences and
take the union of all extractions. We refer to this
version as RECALLHYBRID, since this does not lose
any extractions, achieving highest possible recall.
A second hybrid, which we call PRECHYBRID,
focuses on increasing the precision and uses the fil-
ter policy and an intelligent order of sentences for
extraction as described below.
Filter Policy for TEXTRUNNER Extractions: The
results from Figure 1 and Figure 2 show that TEX-
TRUNNER?s precision is low when the CRF confi-
dence in the extraction is low, when the redundancy
of the extraction is low, and when the arguments are
far apart. Thus, system confidence, redundancy, and
locality form the key factors for our filter policy: if
the confidence is less than 0.5 and the redundancy
is less than 2 or the distance between the arguments
in the sentence is greater than 5 (if the relation is
binary) or 8 (if the relation is n-ary) discard this tu-
ple. These thresholds were determined by a param-
eter search over a small dataset.
Order of Sentences for Extraction: An optimal
ordering policy would apply SRL-IE first to the sen-
tences where TEXTRUNNER has low precision and
leave the sentences that seem malformed (e.g., in-
complete sentences, two sentences spliced together)
for last. As we have seen, the distance between the
first and last argument is a good indicator for TEX-
TRUNNER precision. Moreover, a confidence value
of 0.0 by TEXTRUNNER?s CRF classifier is good ev-
idence that the sentence may be malformed and is
unlikely to contain a valid relation.
We rank sentences S in the following way, with
SRL-IE processing sentences from highest ranking
to lowest: if CRF.confidence = 0.0 then S.rank = 0,
else S.rank = average distance between pairs of ar-
guments for all tuples extracted by TEXTRUNNER
from S.
While this ranking system orders sentences ac-
cording to which sentence is likely to yield maxi-
mum new information, it misses the cost of compu-
tation. To account for computation time, we also
estimate the amount of time SRL-IE will take to
process each sentence using a linear model trained
on the sentence length. We then choose the sentence
57
that maximizes information gain divided by compu-
tation time.
6.1 Properties of Hybrid Extractors
The choice between the two hybrid systems is a
trade-off between recall and precision: RECALLHY-
BRID guarantees the best recall, since it does not lose
any extractions, while PRECHYBRID is designed to
maximize the early boost in precision. The evalua-
tion in the next section bears out these expectations.
6.2 Evaluation of Hybrid Extractors
Figure 3(a) and Figure 4(a) report the precision of
each system for binary and n-ary extractions mea-
sured against available computation time. PRECHY-
BRID starts at slightly higher precision due to our
filtering of potentially low quality extractions from
TEXTRUNNER. For binary this precision is even
better than SRL-IE?s. It gradually loses precision
until it reaches SRL-IE?s level. RECALLHYBRID
improves on TEXTRUNNER?s precision, albeit at a
much slower rate and remains worse than SRL-IE
and PRECHYBRID throughout.
The recall for binary and n-ary extractions are
shown in Figure 3(b) and Figure 4(b), again mea-
sured against available time. While PRECHYBRID
significantly improves on TEXTRUNNER?s recall, it
does lose recall compared to RECALLHYBRID, es-
pecially for n-ary extractions. PRECHYBRID also
shows a large initial drop in recall due to filtering.
Lastly, the gains in precision from PRECHYBRID
are offset by loss in recall that leaves the F1 mea-
sure essentially identical to that of RECALLHYBRID
(Figures 3(c),4(c)). However, for a fixed time bud-
get both hybrid F-measures are significantly bet-
ter than TEXTRUNNER and SRL-IE F-measures
demonstrating the power of the hybrid extractors.
Both methods reach a much higher F1 than TEX-
TRUNNER: a gain of over 0.15 in half SRL-IE?s
processing time and over 0.3 after the full process-
ing time. Both hybrids perform better than SRL-IE
given equal processing time.
We believe that most often constructing a higher
quality database of facts with a relatively lower
recall is more useful than vice-versa, making
PRECHYBRID to be of wider applicability than RE-
CALLHYBRID. Still the choice of the actual hybrid
extractor could change based on the task.
7 Related Work
Open information extraction is a relatively recent
paradigm and hence, has been studied by only a
small number of researchers. The most salient is
TEXTRUNNER, which also introduced the model
(Banko et al, 2007; Banko and Etzioni, 2008).
A version of KNEXT uses heuristic rules and syn-
tactic parses to convert a sentence into an unscoped
logical form (Van Durme and Schubert, 2008). This
work is more suitable for extracting common sense
knowledge as opposed to factual information.
Another Open IE system, Kylin (Weld et al,
2008), suggests automatically building an extractor
for each relation using self-supervised training, with
training data generated using Wikipedia infoboxes.
This work has the limitation that it can only extract
relations expressed in Wikipedia infoboxes.
A paradigm related to Open IE is Preemptive IE
(Shinyama and Sekine, 2006). While one goal of
Preemptive IE is to avoid relation-specificity, Pre-
emptive IE does not emphasize Web scalability,
which is essential to Open IE.
(Carlson et al, 2009) presents a semi-supervised
approach to information extraction on the Web. It
learns classifiers for different relations and couples
the training of those classifiers with ontology defin-
ing constraints. While we attempt to learn unknown
relations, it learns a pre-defined set of relations.
Another related system is WANDERLUST (Akbik
and Bro?, 2009). The authors of this system anno-
tated 10,000 sentences parsed with LinkGrammar,
resulting in 46 general linkpaths as patterns for rela-
tion extraction. With these patterns WANDERLUST
extracts binary relations from link grammar link-
ages. In contrast to our approaches, this requires a
large set of hand-labeled examples.
USP (Poon and Domingos, 2009) is based on
Markov Logic Networks and attempts to create a
full semantic parse in an unsupervised fashion. They
evaluate their work on biomedical text, so its appli-
cability to general Web text is not yet clear.
8 Discussion and Future Work
The Heavy Tail: It is well accepted that informa-
tion on the Web is distributed according to Zipf?s
58
0 10 20 30 40 50
0.
0
0.
2
0.
4
0.
6
Time (hours)
P
re
ci
si
on
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
0 10 20 30 40 50
0.
0
0.
4
0.
8
Time (hours)
R
ec
al
l
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
0 10 20 30 40 50
0.
0
0.
4
0.
8
Time (hours)
F?
m
ea
su
re
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
Figure 3: (a) Precision for binary extractions for PRECHYBRID starts higher than the precision of SRL-IE. (b) Recall
for binary extractions rises over time for both hybrid systems, with PRECHYBRID starting lower. (c) Hybrid extractors
obtain the best F-measure given a limited budget of computation time.
0 10 20 30 40 50
0.
0
0.
2
0.
4
0.
6
Time (hours)
P
re
ci
si
on
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
0 10 20 30 40 50
0.
0
0.
4
0.
8
Time (hours)
R
ec
al
l
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
0 10 20 30 40 50
0.
0
0.
4
0.
8
Time (hours)
F?
m
ea
su
re
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
Figure 4: (a) PRECHYBRID also gives a strong boost to precision for n-ary extractions. (b) Recall for n-ary extractions
for RECALLHYBRID starts substantially higher than PRECHYBRID and finally reaches much higher recall than SRL-
IE alone. (c) F-measure for n-ary extractions. The hybrid extractors outperform others.
Law (Downey et al, 2005), implying that there is a
heavy tail of facts that are mentioned only once or
twice. The prior work on Open IE ascribes prime
importance to redundancy based validation, which,
as our results show (Figures 1(b), 2(b)), captures a
very tiny fraction of the available information. We
believe that deeper processing of text is essential to
gather information from this heavy tail. Our SRL-
IE extractor is a viable algorithm for this task.
Understanding SRL Components: UIUC-SRL
as well as other SRL algorithms have different sub-
components ? parsing, argument classification, joint
inference, etc. We plan to study the effective con-
tribution of each of these components. Our hope is
to identify the most important subset, which yields
a similar quality at a much reduced computational
cost. Another alternative is to add the best perform-
ing component within TEXTRUNNER.
9 Conclusions
This paper investigates the use of semantic features,
in particular, semantic role labeling for the task of
open information extraction. We describe SRL-IE,
the first SRL based Open IE system. We empirically
compare the performance of SRL-IE with TEX-
TRUNNER, a state-of-the-art Open IE system and
find that on average SRL-IE has much higher re-
call and precision, however, TEXTRUNNER outper-
forms in precision for the case of highly redundant
or high locality extractions. Moreover, TEXTRUN-
NER is over 2 orders of magnitude faster.
These complimentary strengths help us design hy-
brid extractors that achieve better performance than
either system given a limited budget of computation
time. Overall, we provide evidence that, contrary to
belief in the Open IE literature (Banko and Etzioni,
2008), semantic approaches have a lot to offer for
the task of Open IE and the vision of machine read-
ing.
10 Acknowledgements
This research was supported in part by NSF grant
IIS-0803481, ONR grant N00014-08-1-0431, and
DARPA contract FA8750-09-C-0179, and carried
out at the University of Washington?s Turing Cen-
ter.
59
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the Fifth ACM International Con-
ference on Digital Libraries.
Alan Akbik and Ju?gen Bro?. 2009. Wanderlust: Extract-
ing semantic relations from natural language text us-
ing dependency grammar patterns. In Proceedings of
the Workshop on Semantic Search (SemSearch 2009)
at the 18th International World Wide Web Conference
(WWW 2009).
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceedings
of the 17th international conference on Computational
linguistics, pages 86?90.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT, pages 28?36.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the web. In IJCAI?07: Pro-
ceedings of the 20th international joint conference on
Artifical intelligence, pages 2670?2676.
Michele Banko. 2009. Open Information Extraction for
the Web. Ph.D. thesis, University of Washington.
Andrew Carlson, Justin Betteridge, Estevam R. Hruschka
Jr., and Tom M. Mitchell. 2009. Coupling semi-
supervised learning of categories and relations. In
Proceedings of the NAACL HLT 2009 Workskop on
Semi-supervised Learning for Natural Language Pro-
cessing.
Bonaventura Coppola, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Shallow semantic parsing
for spoken language understanding. In NAACL ?09:
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Short Papers, pages 85?88.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2005. A probabilistic model of redundancy in infor-
mation extraction. In IJCAI ?05: Proceedings of the
20th international joint conference on Artifical intelli-
gence, pages 1034?1041.
Oren Etzioni, Michele Banko, and Michael J. Cafarella.
2006. Machine reading. In AAAI?06: proceedings of
the 21st national conference on Artificial intelligence,
pages 1517?1519.
Richard Johansson and Pierre Nugues. 2008. The ef-
fect of syntactic representation on semantic role label-
ing. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 393?400.
Paul Kingsbury Martha and Martha Palmer. 2002. From
treebank to propbank. In In Proceedings of LREC-
2002.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP ?09: Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1?10.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2).
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311.
Stephen Soderland. 1999. Learning information extrac-
tion rules for semi-structured and free text. Machine
Learning, 34(1-3):233?272.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34(2):161?
191.
Benjamin Van Durme and Lenhart Schubert. 2008. Open
knowledge extraction through compositional language
processing. In STEP ?08: Proceedings of the 2008
Conference on Semantics in Text Processing, pages
239?254.
Daniel S. Weld, Raphael Hoffmann, and Fei Wu. 2008.
Using wikipedia to bootstrap open information extrac-
tion. SIGMOD Rec., 37(4):62?68.
60
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 87?95,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Machine Reading at the University of Washington
Hoifung Poon, Janara Christensen, Pedro Domingos, Oren Etzioni, Raphael Hoffmann,
Chloe Kiddon, Thomas Lin, Xiao Ling, Mausam, Alan Ritter, Stefan Schoenmackers,
Stephen Soderland, Dan Weld, Fei Wu, Congle Zhang
Department of Computer Science & Engineering
University of Washington
Seattle, WA 98195
{hoifung,janara,pedrod,etzioni,raphaelh,chloe,tlin,xiaoling,mausam,
aritter,stef,soderland,weld,wufei,clzhang}@cs.washington.edu
Abstract
Machine reading is a long-standing goal of AI
and NLP. In recent years, tremendous progress
has been made in developing machine learning
approaches for many of its subtasks such as
parsing, information extraction, and question
answering. However, existing end-to-end so-
lutions typically require substantial amount of
human efforts (e.g., labeled data and/or man-
ual engineering), and are not well poised for
Web-scale knowledge acquisition. In this pa-
per, we propose a unifying approach for ma-
chine reading by bootstrapping from the easi-
est extractable knowledge and conquering the
long tail via a self-supervised learning pro-
cess. This self-supervision is powered by joint
inference based on Markov logic, and is made
scalable by leveraging hierarchical structures
and coarse-to-fine inference. Researchers at
the University of Washington have taken the
first steps in this direction. Our existing work
explores the wide spectrum of this vision and
shows its promise.
1 Introduction
Machine reading, or learning by reading, aims to
extract knowledge automatically from unstructured
text and apply the extracted knowledge to end tasks
such as decision making and question answering. It
has been a major goal of AI and NLP since their
early days. With the advent of the Web, the billions
of online text documents contain virtually unlimited
amount of knowledge to extract, further increasing
the importance and urgency of machine reading.
In the past, there has been a lot of progress in
automating many subtasks of machine reading by
machine learning approaches (e.g., components in
the traditional NLP pipeline such as POS tagging
and syntactic parsing). However, end-to-end solu-
tions are still rare, and existing systems typically re-
quire substantial amount of human effort in manual
engineering and/or labeling examples. As a result,
they often target restricted domains and only extract
limited types of knowledge (e.g., a pre-specified re-
lation). Moreover, many machine reading systems
train their knowledge extractors once and do not
leverage further learning opportunities such as ad-
ditional text and interaction with end users.
Ideally, a machine reading system should strive to
satisfy the following desiderata:
End-to-end: the system should input raw text, ex-
tract knowledge, and be able to answer ques-
tions and support other end tasks;
High quality: the system should extract knowledge
with high accuracy;
Large-scale: the system should acquire knowledge
at Web-scale and be open to arbitrary domains,
genres, and languages;
Maximally autonomous: the system should incur
minimal human effort;
Continuous learning from experience: the
system should constantly integrate new infor-
mation sources (e.g., new text documents) and
learn from user questions and feedback (e.g.,
via performing end tasks) to continuously
improve its performance.
These desiderata raise many intriguing and chal-
lenging research questions. Machine reading re-
search at the University of Washington has explored
87
a wide spectrum of solutions to these challenges and
has produced a large number of initial systems that
demonstrated promising performance. During this
expedition, an underlying unifying vision starts to
emerge. It becomes apparent that the key to solving
machine reading is to:
1. Conquer the long tail of textual knowledge via
a self-supervised learning process that lever-
ages data redundancy to bootstrap from the
head and propagates information down the long
tail by joint inference;
2. Scale this process to billions of Web documents
by identifying and leveraging ubiquitous struc-
tures that lead to sparsity.
In Section 2, we present this vision in detail, iden-
tify the major dimensions these initial systems have
explored, and propose a unifying approach that sat-
isfies all five desiderata. In Section 3, we reivew
machine reading research at the University of Wash-
ington and show how they form synergistic effort
towards solving the machine reading problem. We
conclude in Section 4.
2 A Unifying Approach for Machine
Reading
The core challenges to machine reading stem from
the massive scale of the Web and the long-tailed dis-
tribution of textual knowledge. The heterogeneous
Web contains texts that vary substantially in subject
matters (e.g., finance vs. biology) and writing styles
(e.g., blog posts vs. scientific papers). In addition,
natural languages are famous for their myraid vari-
ations in expressing the same meaning. A fact may
be stated in a straightforward way such as ?kale con-
tains calcium?. More often though, it may be stated
in a syntactically and/or lexically different way than
as phrased in an end task (e.g., ?calcium is found in
kale?). Finally, many facts are not even stated ex-
plicitly, and must be inferred from other facts (e.g.,
?kale prevents osteoporosis? may not be stated ex-
plicitly but can be inferred by combining facts such
as ?kale contains calcium? and ?calcium helps pre-
vent osteoporosis?). As a result, machine reading
must not rely on explicit supervision such as manual
rules and labeled examples, which will incur pro-
hibitive cost in the Web scale. Instead, it must be
able to learn from indirect supervision.





	






Figure 1: A unifying vision for machine reading: boot-
strap from the head regime of the power-law distribu-
tion of textual knowledge, and conquer the long tail in
a self-supervised learning process that raises certainty on
sparse extractions by propagating information via joint
inference from frequent extractions.
A key source of indirect supervision is meta
knowledge about the domains. For example, the
TextRunner system (Banko et al, 2007) hinges on
the observation that there exist general, relation-
independent patterns for information extraction. An-
other key source of indirect supervision is data re-
dundancy. While a rare extracted fact or inference
pattern may arise by chance of error, it is much less
likely so for the ones with many repetitions (Downey
et al, 2010). Such highly-redundant knowledge can
be extracted easily and with high confidence, and
can be leveraged for bootstrapping. For knowledge
that resides in the long tail, explicit forms of redun-
dancy (e.g., identical expressions) are rare, but this
can be circumvented by joint inference. For exam-
ple, expressions that are composed with or by sim-
ilar expressions probably have the same meaning;
the fact that kale prevents osteoporosis can be de-
rived by combining the facts that kale contains cal-
cium and that calcium helps prevent osteoporosis via
a transitivity-through inference pattern. In general,
joint inference can take various forms, ranging from
simple voting to shrinkage in a probabilistic ontol-
ogy to sophisticated probabilistic reasoning based
on a joint model. Simple ones tend to scale bet-
ter, but their capability in propagating information
is limited. More sophisticated methods can uncover
implicit redundancy and propagate much more in-
88
formation with higher quality, yet the challenge is
how to make them scale as well as simple ones.
To do machine reading, a self-supervised learning
process, informed by meta knowledege, stipulates
what form of joint inference to use and how. Effec-
tively, it increases certainty on sparse extractions by
propagating information from more frequent ones.
Figure 1 illustrates this unifying vision.
In the past, machine reading research at the Uni-
versity of Washington has explored a variety of so-
lutions that span the key dimensions of this uni-
fying vision: knowledge representation, bootstrap-
ping, self-supervised learning, large-scale joint in-
ference, ontology induction, continuous learning.
See Section 3 for more details. Based on this ex-
perience, one direction seems particularly promising
that we would propose here as our unifying approach
for end-to-end machine reading:
Markov logic is used as the unifying framework for
knowledge representation and joint inference;
Self-supervised learning is governed by a joint
probabilistic model that incorporates a small
amount of heuristic knowledge and large-scale
relational structures to maximize the amount
and quality of information to propagate;
Joint inference is made scalable to the Web by
coarse-to-fine inference.
Probabilistic ontologies are induced from text to
guarantee tractability in coarse-to-fine infer-
ence. This ontology induction and popula-
tion are incorporated into the joint probabilistic
model for self-supervision;
Continuous learning is accomplished by combin-
ing bootstrapping and crowdsourced content
creation to synergistically improve the reading
system from user interaction and feedback.
A distinctive feature of this approach is its empha-
sis on using sophisticated joint inference. Recently,
joint inference has received increasing interest in
AI, machine learning, and NLP, with Markov logic
(Domingos and Lowd, 2009) being one of the lead-
ing unifying frameworks. Past work has shown that
it can substantially improve predictive accuracy in
supervised learning (e.g., (Getoor and Taskar, 2007;
Bakir et al, 2007)). We propose to build on these ad-
vances, but apply joint inference beyond supervised
learning, with labeled examples supplanted by indi-
rect supervision.
Another distinctive feature is that we propose
to use coarse-to-fine inference (Felzenszwalb and
McAllester, 2007; Petrov, 2009) as a unifying
framework to scale inference to the Web. Es-
sentially, coarse-to-fine inference leverages the
sparsity imposed by hierarchical structures that
are ubiquitous in human knowledge (e.g., tax-
onomies/ontologies). At coarse levels (top levels in
a hierarchy), ambiguities are rare (there are few ob-
jects and relations), and inference can be conducted
very efficiently. The result is then used to prune un-
promising refinements at the next level. This process
continues down the hierarchy until decision can be
made. In this way, inference can potentially be sped
up exponentially, analogous to binary tree search.
Finally, we propose a novel form of continuous
learning by leveraging the interaction between the
system and end users to constantly improve the per-
formance. This is straightforward to do in our ap-
proach given the self-supervision process and the
availability of powerful joint inference. Essentially,
when the system output is applied to an end task
(e.g., answering questions), the feedback from user
is collected and incorporated back into the system
as a bootstrap source. The feedback can take the
form of explicit supervision (e.g., via community
content creation or active learning) or indirect sig-
nals (e.g., click data and query logs). In this way,
we can bootstrap an online community by an initial
machine reading system that provides imperfect but
valuable service in end tasks, and continuously im-
prove the quality of system output, which attracts
more users with higher degree of participation, thus
creating a positive feedback loop and raising the ma-
chine reading performance to a high level that is dif-
ficult to attain otherwise.
3 Summary of Progress to Date
The University of Washington has been one of the
leading places for machine reading research and has
produced many cutting-edge systems, e.g., WIEN
(first wrapper induction system for information ex-
traction), Mulder (first fully automatic Web-scale
question answering system), KnowItAll/TextRunner
(first systems to do open-domain information extrac-
89
tion from the Web corpus at large scale), Kylin (first
self-supervised system for Wikipedia-based infor-
mation extraction), UCR (first unsupervised corefer-
ence resolution system that rivals the performance of
supervised systems), Holmes (first Web-scale joint
inference system), USP (first unsupervised system
for semantic parsing).
Figure 2 shows the evolution of the major sys-
tems; dashed lines signify influence in key ideas
(e.g., Mulder inspires KnowItAll), and solid lines
signify dataflow (e.g., Holmes inputs TextRunner tu-
ples). These systems span a wide spectrum in scal-
ability (assessed by speed and quantity in extrac-
tion) and comprehension (assessed by unit yield of
knowledge at a fixed precision level). At one ex-
treme, the TextRunner system is highly scalable, ca-
pable of extracting billions of facts, but it focuses on
shallow extractions from simple sentences. At the
other extreme, the USP and LOFT systems achieve
much higher level of comprehension (e.g., in a task
of extracting knowledge from biomedical papers and
answering questions, USP obtains more than three
times as many correct answers as TextRunner, and
LOFT obtains more than six times as many correct
answers as TextRunner), but are much less scalable
than TextRunner.
In the remainder of the section, we review the
progress made to date and identify key directions for
future work.
3.1 Knowledge Representation and Joint
Inference
Knowledge representations used in these systems
vary widely in expressiveness, ranging from sim-
ple ones like relation triples (<subject, relation,
object>; e.g., in KnowItAll and TextRunner), to
clusters of relation triples or triple components (e.g.,
in SNE, RESOLVER), to arbitrary logical formulas
and their clusters (e.g., in USP, LOFT). Similarly,
a variety forms of joint inference have been used,
ranging from simple voting to heuristic rules to so-
phisticated probabilistic models. All these can be
compactly encoded in Markov logic (Domingos and
Lowd, 2009), which provides a unifying framework
for knowledge representation and joint inference.
Past work at Washington has shown that in su-
pervised learning, joint inference can substantially
improve predictive performance on tasks related to
machine reading (e.g., citation information extrac-
tion (Poon and Domingos, 2007), ontology induc-
tion (Wu and Weld, 2008), temporal information
extraction (Ling and Weld, 2010)). In addition, it
has demonstrated that sophisticated joint inference
can enable effective learning without any labeled
information (UCR, USP, LOFT), and that joint in-
ference can scale to millions of Web documents by
leveraging sparsity in naturally occurring relations
(Holmes, Sherlock), showing the promise of our uni-
fying approach.
Simpler representations limit the expressiveness
in representing knowledge and the degree of sophis-
tication in joint inference, but they currently scale
much better than more expressive ones. A key direc-
tion for future work is to evaluate this tradeoff more
thoroughly, e.g., for each class of end tasks, to what
degree do simple representations limit the effective-
ness in performing the end tasks? Can we automate
the choice of representations to strike the best trade-
off for a specific end task? Can we advance joint
inference algorithms to such a degree that sophisti-
cated inference scales as well as simple ones?
3.2 Bootstrapping
Past work at Washington has identified and lever-
aged a wide range of sources for bootstrapping. Ex-
amples include Wikipedia (Kylin, KOG, IIA, WOE,
WPE), Web lists (KnowItAll, WPE), Web tables
(WebTables), Hearst patterns (KnowItAll), heuristic
rules (TextRunner), semantic role labels (SRL-IE),
etc.
In general, potential bootstrap sources can be
broadly divided into domain knowledge (e.g., pat-
terns and rules) and crowdsourced contents (e.g., lin-
guistic resources, Wikipedia, Amazon Mechanical
Turk, the ESP game).
A key direction for future work is to combine
bootstrapping with crowdsourced content creation
for continuous learning. (Also see Subsection 3.6.)
3.3 Self-Supervised Learning
Although the ways past systems conduct self-
supervision vary widely in detail, they can be di-
vided into two broad categories. One uses heuristic
rules that exploit existing semi-structured resources
to generate noisy training examples for use by su-
pervised learning methods and with cotraining (e.g.,
90

	


		

	
 	