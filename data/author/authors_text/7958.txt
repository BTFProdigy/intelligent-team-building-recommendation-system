Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 849?856
Manchester, August 2008
Learning Entailment Rules for Unary Templates
Idan Szpektor
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
szpekti@macs.biu.ac.il
Ido Dagan
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
dagan@macs.biu.ac.il
Abstract
Most work on unsupervised entailment
rule acquisition focused on rules between
templates with two variables, ignoring
unary rules - entailment rules between
templates with a single variable. In this pa-
per we investigate two approaches for un-
supervised learning of such rules and com-
pare the proposed methods with a binary
rule learning method. The results show
that the learned unary rule-sets outperform
the binary rule-set. In addition, a novel
directional similarity measure for learning
entailment, termed Balanced-Inclusion, is
the best performing measure.
1 Introduction
In many NLP applications, such as Question An-
swering (QA) and Information Extraction (IE), it
is crucial to recognize whether a specific target
meaning is inferred from a text. For example, a
QA system has to deduce that ?SCO sued IBM? is
inferred from ?SCO won a lawsuit against IBM?
to answer ?Whom did SCO sue??. This type of
reasoning has been identified as a core semantic
inference paradigm by the generic Textual Entail-
ment framework (Giampiccolo et al, 2007).
An important type of knowledge needed for
such inference is entailment rules. An entailment
rule specifies a directional inference relation be-
tween two templates, text patterns with variables,
such as ?X win lawsuit against Y ? X sue Y ?.
Applying this rule by matching ?X win lawsuit
against Y ? in the above text allows a QA system to
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
infer ?X sue Y ? and identify ?IBM?, Y ?s instantia-
tion, as the answer for the above question. Entail-
ment rules capture linguistic and world-knowledge
inferences and are used as an important building
block within different applications, e.g. (Romano
et al, 2006).
One reason for the limited performance of
generic semantic inference systems is the lack of
broad-scale knowledge-bases of entailment rules
(in analog to lexical resources such as WordNet).
Supervised learning of broad coverage rule-sets is
an arduous task. This sparked intensive research
on unsupervised acquisition of entailment rules
(and similarly paraphrases) e.g. (Lin and Pantel,
2001; Szpektor et al, 2004; Sekine, 2005).
Most unsupervised entailment rule acquisition
methods learn binary rules, rules between tem-
plates with two variables, ignoring unary rules,
rules between unary templates (templates with
only one variable). However, a predicate quite of-
ten appears in the text with just a single variable
(e.g. intransitive verbs or passives), where infer-
ence requires unary rules, e.g. ?X take a nap?X
sleep? (further motivations in Section 3.1).
In this paper we focus on unsupervised learn-
ing of unary entailment rules. Two learning ap-
proaches are proposed. In our main approach,
rules are learned by measuring how similar the
variable instantiations of two templates in a corpus
are. In addition to adapting state-of-the-art similar-
ity measures for unary rule learning, we propose a
new measure, termed Balanced-Inclusion, which
balances the notion of directionality in entailment
with the common notion of symmetric semantic
similarity. In a second approach, unary rules are
derived from binary rules learned by state-of-the-
art binary rule learning methods.
We tested the various unsupervised unary rule
849
learning methods, as well as a binary rule learn-
ing method, on a test set derived from a standard
IE benchmark. This provides the first comparison
between the performance of unary and binary rule-
sets. Several results rise from our evaluation: (a)
while most work on unsupervised learning ignored
unary rules, all tested unary methods outperformed
the binary method; (b) it is better to learn unary
rules directly than to derive them from a binary
rule-base; (c) our proposed Balanced-Inclusion
measure outperformed all other tested methods in
terms of F1 measure. Moreover, only Balanced-
Inclusion improved F1 score over a baseline infer-
ence that does not use entailment rules at all .
2 Background
This section reviews relevant distributional simi-
larity measures, both symmetric and directional,
which were applied for either lexical similarity or
unsupervised entailment rule learning.
Distributional similarity measures follow the
Distributional Hypothesis, which states that words
that occur in the same contexts tend to have similar
meanings (Harris, 1954). Various measures were
proposed in the literature for assessing such simi-
larity between two words, u and v. Given a word q,
its set of features F
q
and feature weights w
q
(f) for
f ? F
q
, a common symmetric similarity measure
is Lin similarity (Lin, 1998a):
Lin(u, v) =
?
f?F
u
?F
v
[w
u
(f) + w
v
(f)]
?
f?F
u
w
u
(f) +
?
f?F
v
w
v
(f)
where the weight of each feature is the pointwise
mutual information (pmi) between the word and
the feature: w
q
(f) = log[
Pr(f |q)
Pr(f)
].
Weeds and Weir (2003) proposed to measure the
symmetric similarity between two words by av-
eraging two directional (asymmetric) scores: the
coverage of each word?s features by the other. The
coverage of u by v is measured by:
Cover(u, v) =
?
f?F
u
?F
v
w
u
(f)
?
f?F
u
w
u
(f)
The average can be arithmetic or harmonic:
WeedsA(u, v) =
1
2
[Cover(u, v) + Cover(v, u)]
WeedsH(u, v) =
2 ? Cover(u, v) ? Cover(v, u)
Cover(u, v) + Cover(v, u)
Weeds et al also used pmi for feature weights.
Binary rule learning algorithms adopted such
lexical similarity approaches for learning rules be-
tween templates, where the features of each tem-
plate are its variable instantiations in a corpus,
such as {X=?SCO?, Y =?IBM?} for the example
in Section 1. Some works focused on learning
rules from comparable corpora, containing com-
parable documents such as different news articles
from the same date on the same topic (Barzilay
and Lee, 2003; Ibrahim et al, 2003). Such corpora
are highly informative for identifying variations of
the same meaning, since, typically, when variable
instantiations are shared across comparable docu-
ments the same predicates are described. However,
it is hard to collect broad-scale comparable cor-
pora, as the majority of texts are non-comparable.
A complementary approach is learning from the
abundant regular, non-comparable, corpora. Yet,
in such corpora it is harder to recognize varia-
tions of the same predicate. The DIRT algorithm
(Lin and Pantel, 2001) learns non-directional bi-
nary rules for templates that are paths in a depen-
dency parse-tree between two noun variables X
and Y . The similarity between two templates t and
t
?
is the geometric average:
DIRT (t, t
?
) =
?
Lin
x
(t, t
?
) ? Lin
y
(t, t
?
)
where Lin
x
is the Lin similarity between X?s in-
stantiations of t and X?s instantiations of t
?
in
a corpus (equivalently for Lin
y
). Some works
take the combination of the two variable instantia-
tions in each template occurrence as a single com-
plex feature, e.g. {X-Y =?SCO-IBM?}, and com-
pare between these complex features of t and t
?
(Ravichandran and Hovy, 2002; Szpektor et al,
2004; Sekine, 2005).
Directional Measures Most rule learning meth-
ods apply a symmetric similarity measure between
two templates, viewing them as paraphrasing each
other. However, entailment is in general a direc-
tional relation. For example, ?X acquire Y ?
X own Y ? and ?countersuit against X ? lawsuit
against X?.
(Weeds and Weir, 2003) propose a directional
measure for learning hyponymy between two
words, ?l? r?, by giving more weight to the cov-
erage of the features of l by r (with ? >
1
2
):
WeedsD(l, r)=?Cover(l, r)+(1??)Cover(r, l)
When ?=1, this measure degenerates into
Cover(l, r), termed Precision(l, r). With
850
Precision(l, r) we obtain a ?soft? version of
the inclusion hypothesis presented in (Geffet and
Dagan, 2005), which expects l to entail r if the
?important? features of l appear also in r.
Similarly, the LEDIR algorithm (Bhagat et al,
2007) identifies the entailment direction between
two binary templates, l and r, which participate
in a relation learned by (the symmetric) DIRT, by
measuring the proportion of instantiations of l that
are covered by the instantiations of r.
As far as we know, only (Shinyama et al, 2002)
and (Pekar, 2006) learn rules between unary tem-
plates. However, (Shinyama et al, 2002) relies
on comparable corpora for identifying paraphrases
and simply takes any two templates from compa-
rable sentences that share a named entity instan-
tiation to be paraphrases. Such approach is not
feasible for non-comparable corpora where statis-
tical measurement is required. (Pekar, 2006) learns
rules only between templates related by local dis-
course (information from different documents is
ignored). In addition, their template structure is
limited to only verbs and their direct syntactic ar-
guments, which may yield incorrect rules, e.g. for
light verbs (see Section 5.2). To overcome this lim-
itation, we use a more expressive template struc-
ture.
3 Learning Unary Entailment Rules
3.1 Motivations
Most unsupervised rule learning algorithms fo-
cused on learning binary entailment rules. How-
ever, using binary rules for inference is not enough.
First, a predicate that can have multiple arguments
may still occur with only one of its arguments.
For example, in ?The acquisition of TCA was suc-
cessful?, ?TCA? is the only argument of ?acqui-
sition?. Second, some predicate expressions are
unary by nature. For example, modifiers, such as
?the elected X?, or intransitive verbs. In addition,
it appears more tractable to learn all variations for
each argument of a predicate separately than to
learn them for combinations of argument pairs.
For these reasons, it seems that unary rule learn-
ing should be addressed in addition to binary rule
learning. We are further motivated by the fact that
some (mostly supervised) works in IE found learn-
ing unary templates useful for recognizing relevant
named entities (Riloff, 1996; Sudo et al, 2003;
Shinyama and Sekine, 2006), though they did not
attempt to learn generic knowledge bases of entail-
ment rules.
This paper investigates acquisition of unary en-
tailment rules from regular non-comparable cor-
pora. We first describe the structure of unary
templates and then explore two conceivable ap-
proaches for learning unary rules. The first ap-
proach directly assesses the relation between two
given templates based on the similarity of their in-
stantiations in the corpus. The second approach,
which was also mentioned in (Iftene and Balahur-
Dobrescu, 2007), derives unary rules from learned
binary rules.
3.2 Unary Template Structure
To learn unary rules we first need to define their
structure. In this paper we work at the syntac-
tic representation level. Texts are represented by
dependency parse trees (using the Minipar parser
(Lin, 1998b)) and templates by parse sub-trees.
Given a dependency parse tree, any sub-tree can
be a candidate template, setting some of its nodes
as variables (Sudo et al, 2003). However, the num-
ber of possible templates is exponential in the size
of the sentence. In the binary rule learning litera-
ture, the main solution for exhaustively learning all
rules between any pair of templates in a given cor-
pus is to restrict the structure of templates. Typi-
cally, a template is restricted to be a path in a parse
tree between two variable nodes (Lin and Pantel,
2001; Ibrahim et al, 2003).
Following this approach, we chose the structure
of unary templates to be paths as well, where one
end of the path is the template?s variable. How-
ever, paths with one variable have more expressive
power than paths between two variables, since the
combination of two unary paths may generate a
binary template that is not a path. For example,
the combination of ?X call indictable? and ?call Y
indictable? is the template ?X call Y indictable?,
which is not a path between X and Y .
For every noun node v in a parsed sentence, we
generate templates with v as a variable as follows:
1. Traverse the path from v towards the root of
the parse tree. Whenever a candidate pred-
icate is encountered (any noun, adjective or
verb) the path from that node to v is taken as
a template. We stop when the first verb or
clause boundary (e.g. a relative clause) is en-
countered, which typically represent the syn-
tactic boundary of a specific predicate.
851
2. To enable templates with control verbs and
light verbs, e.g. ?X help preventing?, ?X
make noise?, whenever a verb is encoun-
tered we generate templates that are paths be-
tween v and the verb?s modifiers, either ob-
jects, prepositional complements or infinite
or gerund verb forms (paths ending at stop
words, e.g. pronouns, are not generated).
3. To capture noun modifiers that act as predi-
cates, e.g. ?the losingX?, we extract template
paths between v and each of its modifiers,
nouns or adjectives, that are derived from a
verb. We use the Catvar database to identify
verb derivations (Habash and Dorr, 2003).
As an example for the procedure, the templates
extracted from the sentence ?The losing party
played it safe? with ?party? as the variable are:
?losing X?, ?X play? and ?X play safe?.
3.3 Direct Learning of Unary Rules
We applied the lexical similarity measures pre-
sented in Section 2 for unary rule learning. Each
argument instantiation of template t in the corpus
is taken as a feature f , and the pmi between t and
f is used for the feature?s weight. We first adapted
DIRT for unary templates (unary-DIRT, apply-
ing Lin-similarity to the single feature vector), as
well as its output filtering by LEDIR. The various
Weeds measures were also applied
1
: symmetric
arithmetic average, symmetric harmonic average,
weighted arithmetic average and Precision.
After initial analysis, we found that given a right
hand side template r, symmetric measures such
as Lin (in DIRT) generally tend to prefer (score
higher) relations ?l, r? in which l and r are related
but do not necessarily participate in an entailment
or equivalence relation, e.g. the wrong rule ?kill X
? injure X?.
On the other hand, directional measures such as
Weeds Precision tend to prefer directional rules in
which the entailing template is infrequent. If an in-
frequent template has common instantiations with
another template, the coverage of its features is
typically high, whether or not an entailment rela-
tion exists between the two templates. This behav-
ior generates high-score incorrect rules.
Based on this analysis, we propose a new
measure that balances the two behaviors, termed
1
We applied the best performing parameter values pre-
sented in (Bhagat et al, 2007) and (Weeds and Weir, 2003).
Balanced-Inclusion (BInc). BInc identifies entail-
ing templates based on a directional measure but
penalizes infrequent templates using a symmetric
measure:
BInc(l, r) =
?
Lin(l, r) ? Precision(l, r)
3.4 Deriving Unary Rules From Binary Rules
An alternative way to learn unary rules is to first
learn binary entailment rules and then derive unary
rules from them. We derive unary rules from a
given binary rule-base in two steps. First, for each
binary rule, we generate all possible unary rules
that are part of that rule (each unary template is
extracted following the same procedure described
in Section 3.2). For example, from ?X find solu-
tion to Y ? X solve Y ? we generate the unary
rules ?X find? X solve?, ?X find solution? X
solve?, ?solution to Y ? solve Y ? and ?find solu-
tion to Y ? solve Y ?. The score of each generated
rule is set to be the score of the original binary rule.
The same unary rule can be derived from dif-
ferent binary rules. For example, ?hire Y ? em-
ploy Y ? is derived both from ?X hire Y ? X em-
ploy Y ? and ?hire Y for Z ? employ Y for Z?,
having a different score from each original binary
rule. The second step of the algorithm aggregates
the different scores yielded for each derived rule
to produce the final rule score. Three aggregation
functions were tested: sum (Derived-Sum), aver-
age (Derived-Avg) and maximum (Derived-Max).
4 Experimental Setup
We want to evaluate learned unary and binary rule
bases by their utility for NLP applications through
assessing the validity of inferences that are per-
formed in practice using the rule base.
To perform such experiments, we need a test-
set of seed templates, which correspond to a set of
target predicates, and a corpus annotated with all
argument mentions of each predicate. The evalu-
ation assesses the correctness of all argument ex-
tractions, which are obtained by matching in the
corpus either the seed templates or templates that
entail them according to the rule-base (the latter
corresponds to rule-application).
Following (Szpektor et al, 2008), we found the
ACE 2005 event training set
2
useful for this pur-
pose. This standard IE dataset includes 33 types of
event predicates such as Injure, Sue and Divorce.
2
http://projects.ldc.upenn.edu/ace/
852
All event mentions are annotated in the corpus, in-
cluding the instantiated arguments of the predicate.
ACE guidelines specify for each event its possible
arguments, each associated with a semantic role.
For instance, some of the Injure event arguments
are Agent, Victim and Time.
To utilize the ACE dataset for evaluating entail-
ment rule applications, we manually represented
each ACE event predicate by unary seed templates.
For example, the seed templates for Injure are ?A
injure?, ?injure V ? and ?injure in T ?. We mapped
each event role annotation to the corresponding
seed template variable, e.g. ?Agent? to A and
?Victim? to V in the above example. Templates
are matched using a syntactic matcher that han-
dles simple morpho-syntactic phenomena, as in
(Szpektor and Dagan, 2007). A rule application
is considered correct if the matched argument is
annotated by the corresponding ACE role.
For testing binary rule-bases, we automatically
generated binary seed templates from any two
unary seeds that share the same predicate. For ex-
ample, for Injure the binary seeds ?A injure V ?, ?A
injure in T ? and ?injure V in T ? were automatically
generated from the above unary seeds.
We performed two adaptations to the ACE
dataset to fit it better to our evaluation needs. First,
our evaluation aims at assessing the correctness of
inferring a specific target semantic meaning, which
is denoted by a specific predicate, using rules.
Thus, four events that correspond ambiguously to
multiple distinct predicates were ignored. For in-
stance, the Transfer-Money event refers to both do-
nating and lending money, and thus annotations of
this event cannot be mapped to a specific seed tem-
plate. We also omitted 3 events with less than 10
mentions, and were left with 26 events (6380 argu-
ment mentions).
Additionally, we regard all entailing mentions
under the textual entailment definition as correct.
However, event mentions are annotated as correct
in ACE only if they explicitly describe the target
event. For instance, a Divorce mention does en-
tail a preceding marriage event but it does not ex-
plicitly describe it, and thus it is not annotated as
a Marry event. To better utilize the ACE dataset,
we considered for a target event the annotations of
other events that entail it as being correct as well.
We note that each argument was considered sep-
arately. For example, we marked a mention of a
divorced person as entailing the marriage of that
person, but did not consider the place and time of
the divorce act to be those of the marriage .
5 Results and Analysis
We implemented the unary rule learning algo-
rithms described in Section 3 and the binary DIRT
algorithm (Lin and Pantel, 2001). We executed
each method over the Reuters RCV1 corpus
3
,
learning for each template r in the corpus the top
100 rules in which r is entailed by another tem-
plate l, ?l? r?. All rules were learned in canonical
form (Szpektor and Dagan, 2007). The rule-base
learned by binary DIRT was taken as the input for
deriving unary rules from binary rules.
The performance of each acquired rule-base was
measured for each ACE event. We measured the
percentage of correct argument mentions extracted
out of all correct argument mentions annotated for
the event (recall) and out of all argument mentions
extracted for the event (precision). We also mea-
sured F1, their harmonic average, and report macro
average Recall, Precision and F1 over the 26 event
types.
No threshold setting mechanism is suggested in
the literature for the scores of the different algo-
rithms, especially since rules for different right
hand side templates have different score ranges.
Thus, we follow common evaluation practice (Lin
and Pantel, 2001; Geffet and Dagan, 2005) and test
each learned rule-set by taking the top K rules for
each seed template, whereK ranges from 0 to 100.
WhenK=0, no rules are used and mentions are ex-
tracted only by direct matching of seed templates.
Our rule application setting provides a rather
simplistic IE system (for example, no named entity
recognition or approximate template matching). It
is thus useful for comparing different rule-bases,
though the absolute extraction figures do not re-
flect the full potential of the rules. In Secion 5.2
we analyze the full-system?s errors to isolate the
rules? contribution to overall system performance.
5.1 Results
In this section we focus on the best performing
variations of each algorithm type: binary DIRT,
unary DIRT, unary Weeds Harmonic, BInc and
Derived-Avg. We omitted the results of methods
that were clearly inferior to others: (a) WeedsA,
WeedsD and Weeds-Precision did not increase
3
http://about.reuters.com/researchandstandards/corpus/
853
Recall over not using rules because rules with in-
frequent templates scored highest and arithmetic
averaging could not balance well these high scores;
(b) out of the methods for deriving unary rules
from binary rule-bases, Derived-Avg performed
best; (c) filtering with (the directional) LEDIR did
not improve the performance of unary DIRT.
Figure 1 presents Recall, Precision and F1 of the
methods for different cutoff points. First, we ob-
serve that even when matching only the seed tem-
plates (K=0), unary seeds outperform the binary
seeds in terms of both Precision and Recall. This
surprising behavior is consistent through all rule
cutoff points: all unary learning algorithms per-
form better than binary DIRT in all parameters.
The inferior behavior of binary DIRT is analyzed
in Section 5.2.
The graphs show that symmetric unary ap-
proaches substantially increase recall, but dramati-
cally decrease precision already at the top 10 rules.
As a result, F1 only decreases for these methods.
Lin similarity (DIRT) and Weeds-Harmonic show
similar behaviors. They consistently outperform
Derived-Avg. One reason for this is that incorrect
unary rules may be derived even from correct bi-
nary rules. For example, from ?X gain seat on
Y ? elect X to Y ? the incorrect unary rule ?X
gain? electX? is also generated. This problem is
less frequent when unary rules are directly scored
based on their corpus statistics.
The directional measure of BInc yields a more
accurate rule-base, as can be seen by the much
slower precision reduction rate compared to the
other algorithms. As a result, it is the only algo-
rithm that improves over the F1 baseline of K=0,
with the best cutoff point at K=20. BInc?s re-
call increases moderately compared to other unary
learning approaches, but it is still substantially bet-
ter than not using rules (a relative recall increase of
50% already at K=10). We found that many of the
correct mentions missed by BInc but identified by
other methods are due to occasional extractions of
incorrect frequent rules, such as partial templates
(see Section 5.2). This is reflected in the very low
precision of the other methods. On the other hand,
some correct rules were only learned by BInc, e.g.
?countersuit againstX?X sue? and ?X take wife
? X marry?.
When only one argument is annotated for a spe-
cific event mention (28% of ACE predicate men-
tions, which account for 15% of all annotated ar-
Figure 1: Average Precision, Recall and F1 at dif-
ferent top K rule cutoff points.
guments), binary rules either miss that mention, or
extract both the correct argument and another in-
correct one. To neutralize this bias, we also tested
the various methods only on event mentions an-
notated with two or more arguments and obtained
similar results to those presented for all mentions.
This further emphasizes the general advantage of
using unary rules over binary rules.
854
5.2 Analysis
Binary-DIRT We analyzed incorrect rules both
for binary-DIRT and BInc by randomly sampling,
for each algorithm, 200 rules that extracted incor-
rect mentions. We manually classified each rule ?l
? r? as either: (a) Correct - the rule is valid in
some contexts of the event but extracted some in-
correct mentions; (b) Partial Template - l is only a
part of a correct template that entails r. For exam-
ple, learning ?X decide? X meet? instead of ?X
decide to meet ? X meet?; (e) Incorrect - other
incorrect rules, e.g. ?charge X ? convict X?.
Table 1 summarizes the analysis and demon-
strates two problems of binary-DIRT. First, rela-
tive to BInc, it tends to learn incorrect rules for
high frequency templates, and therefore extracted
many more incorrect mentions for the same num-
ber of incorrect rules. Second, a large percentage
of incorrect mentions extracted are due to partial
templates at the rule left-hand-side. Such rules are
leaned because many binary templates have a more
complex structure than paths between arguments.
As explained in Section 3.2 the unary template
structure we use is more expressive, enabling to
learn the correct rules. For example, BInc learned
?take Y into custody ? arrest Y ? while binary-
DIRT learned ?X take Y ? X arrest Y ?.
System Level Analysis We manually analyzed
the reasons for false positives (incorrect extrac-
tions) and false negatives (missed extractions) of
BInc, at its best performing cutoff point (K=20),
by sampling 200 extractions of each type.
From the false positives analysis (Table 2) we
see that 39% of the errors are due to incorrect rules.
The main reasons for learning such rules are those
discussed in Section 3.3: (a) related templates that
are not entailing; (b) infrequent templates. All
learning methods suffer from these issues. As was
shown by our results, BInc provides a first step to-
wards reducing these problems. Yet, these issues
require further research.
Apart from incorrectly learned rules, incorrect
template matching (e.g. due to parse errors) and
context mismatch contribute together 46% of the
errors. Context mismatches occur when the entail-
ing template is matched in inappropriate contexts.
For example, ?slam X ? attack X? should not be
applied when X is a ball, only when it is a person.
The rule-set net effect on system precision is better
estimated by removing these errors and fixing the
annotation errors, which yields 72% precision.
Binary DIRT Balanced Inclusion
Correct 16 (70) 38 (91)
Partial Template 27 (2665) 6 (81)
Incorrect 157 (2584) 156 (787)
Total 200 (5319) 200 (959)
Table 1: Rule type distribution of a sample of 200
rules that extracted incorrect mentions. The corre-
sponding numbers of incorrect mentions extracted
by the sampled rules is shown in parentheses.
Reason % mentions
Incorrect Rule learned 39.0
Context mismatch 27.0
Match error 19.0
Annotation problem 15.0
Table 2: Distribution of reasons for false positives
(incorrect argument extractions) by BInc at K=20.
Reason % mentions
Rule not learned 61.5
Match error 25.0
Discourse analysis needed 12.0
Argument is predicative 1.5
Table 3: Distribution of reasons for false negatives
(missed argument mentions) by BInc at K=20.
Table 3 presents the analysis of false negatives.
First, we note that 12% of the arguments cannot
be extracted by rules alone, due to necessary dis-
course analysis. Thus, a recall upper bound for en-
tailment rules is 88%. Many missed extractions are
due to rules that were not learned (61.5%). How-
ever, 25% of the mentions were missed because of
incorrect syntactic matching of correctly learned
rules. By assuming correct matches in these cases
we isolate the recall of the rule-set (along with the
seeds), which yields 39% recall.
6 Conclusions
We presented two approaches for unsupervised ac-
quisition of unary entailment rules from regular
(non-comparable) corpora. In the first approach,
rules are directly learned based on distributional
similarity measures. The second approach de-
rives unary rules from a given rule-base of binary
rules. Under the first approach we proposed a
novel directional measure for scoring entailment
rules, termed Balanced-Inclusion.
We tested the different approaches utilizing a
standard IE test-set and compared them to binary
rule learning. Our results suggest the advantage of
learning unary rules: (a) unary rule-bases perform
855
better than binary rules; (b) it is better to directly
learn unary rules than to derive them from binary
rule-bases. In addition, the Balanced-Inclusion
measure outperformed all other tested methods.
In future work, we plan to explore additional
unary template structures and similarity scores,
and to improve rule application utilizing context
matching methods such as (Szpektor et al, 2008).
Acknowledgements
This work was partially supported by ISF grant
1095/05, the IST Programme of the European
Community under the PASCAL Network of Ex-
cellence IST-2002-506778 and the NEGEV project
(www.negev-initiative.org).
References
Barzilay, Regina and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In Proceedings of
HLT-NAACL.
Bhagat, Rahul, Patrick Pantel, and Eduard Hovy. 2007.
Ledir: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of
EMNLP.
Geffet, Maayan and Ido Dagan. 2005. The distribu-
tional inclusion hypotheses and lexical entailment.
In Proceedings of ACL.
Giampiccolo, Danilo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recogniz-
ing textual entailment challenge. In Proceedings of
WTEP.
Habash, Nizar and Bonnie Dorr. 2003. A categorial
variation database for english. In Proceedings of
NACL.
Harris, Z. 1954. Distributional structure. Word,
10(23):146?162.
Ibrahim, Ali, Boris Katz, and Jimmy Lin. 2003. Ex-
tracting structural paraphrases from aligned mono-
lingual corpora. In Proceedings of IWP.
Iftene, Adrian and Alexandra Balahur-Dobrescu. 2007.
Hypothesis transformation and semantic variability
rules used in recognizing textual entailment. In Pro-
ceedings of WTEP.
Lin, Dekang and Patrick Pantel. 2001. Discovery of
inference rules for question answering. In Natu-
ral Language Engineering, volume 7(4), pages 343?
360.
Lin, Dekang. 1998a. Automatic retrieval and cluster-
ing of similar words. In Proceedings of COLING-
ACL.
Lin, Dekang. 1998b. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalu-
ation of Parsing Systems at LREC.
Pekar, Viktor. 2006. Acquisition of verb entailment
from text. In Proceedings of NAACL.
Ravichandran, Deepak and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In Proceedings of ACL.
Riloff, Ellen. 1996. Automatically generating extrac-
tion patterns from untagged text. In AAAI/IAAI, Vol.
2, pages 1044?1049.
Romano, Lorenza, Milen Kouylekov, Idan Szpektor,
Ido Dagan, and Alberto Lavelli. 2006. Investigat-
ing a generic paraphrase-based approach for relation
extraction. In Proceedings of EACL.
Sekine, Satoshi. 2005. Automatic paraphrase discov-
ery based on context and keywords between ne pairs.
In Proceedings of IWP.
Shinyama, Yusuke and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of HLT-NAACL.
Shinyama, Yusuke, Satoshi Sekine, Sudo Kiyoshi, and
Ralph Grishman. 2002. Automatic paraphrase ac-
quisition from news articles. In Proceedings of HLT.
Sudo, Kiyoshi, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic ie pattern acquisition. In Pro-
ceedings of ACL.
Szpektor, Idan and Ido Dagan. 2007. Learning canon-
ical forms of entailment rules. In Proceedings of
RANLP.
Szpektor, Idan, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. In Proceedings of EMNLP.
Szpektor, Idan, Ido Dagan, Roy Bar Haim, and Jacob
Goldberger. 2008. Contextual preferences. In Pro-
ceedings of ACL.
Weeds, Julie and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
EMNLP.
856
Investigating a Generic Paraphrase-based Approach
for Relation Extraction
Lorenza Romano
ITC-irst
via Sommarive, 18
38050 Povo (TN), Italy
romano@itc.it
Milen Kouylekov
ITC-irst
via Sommarive, 18
38050 Povo (TN), Italy
kouylekov@itc.it
Idan Szpektor
Department of Computer Science
Bar Ilan University
Ramat Gan, 52900, Israel
szpekti@cs.biu.ac.il
Ido Dagan
Department of Computer Science
Bar Ilan University
Ramat Gan, 52900, Israel
dagan@cs.biu.ac.il
Alberto Lavelli
ITC-irst
via Sommarive, 18
38050 Povo (TN), Italy
lavelli@itc.it
Abstract
Unsupervised paraphrase acquisition has
been an active research field in recent
years, but its effective coverage and per-
formance have rarely been evaluated. We
propose a generic paraphrase-based ap-
proach for Relation Extraction (RE), aim-
ing at a dual goal: obtaining an applicative
evaluation scheme for paraphrase acquisi-
tion and obtaining a generic and largely
unsupervised configuration for RE.We an-
alyze the potential of our approach and
evaluate an implemented prototype of it
using an RE dataset. Our findings reveal a
high potential for unsupervised paraphrase
acquisition. We also identify the need for
novel robust models for matching para-
phrases in texts, which should address syn-
tactic complexity and variability.
1 Introduction
A crucial challenge for semantic NLP applica-
tions is recognizing the many different ways for
expressing the same information. This seman-
tic variability phenomenon was addressed within
specific applications, such as question answering,
information extraction and information retrieval.
Recently, the problem was investigated within
generic application-independent paradigms, such
as paraphrasing and textual entailment.
Eventually, it would be most appealing to apply
generic models for semantic variability to concrete
applications. This paper investigates the applica-
bility of a generic ?paraphrase-based? approach to
the Relation Extraction (RE) task, using an avail-
able RE dataset of protein interactions. RE is
highly suitable for such investigation since its goal
is to exactly identify all the different variations in
which a target semantic relation can be expressed.
Taking this route sets up a dual goal: (a) from
the generic paraphrasing perspective - an objective
evaluation of paraphrase acquisition performance
on a concrete application dataset, as well as identi-
fying the additional mechanisms needed to match
paraphrases in texts; (b) from the RE perspective -
investigating the feasibility and performance of a
generic paraphrase-based approach for RE.
Our configuration assumes a set of entailing
templates (non-symmetric ?paraphrases?) for the
target relation. For example, for the target rela-
tion ?X interact with Y? we would assume a set of
entailing templates as in Tables 3 and 7. In addi-
tion, we require a syntactic matching module that
identifies template instances in text.
First, we manually analyzed the protein-
interaction dataset and identified all cases in which
protein interaction is expressed by an entailing
template. This set a very high idealized upper
bound for the recall of the paraphrase-based ap-
proach for this dataset. Yet, obtaining high cover-
age in practice would require effective paraphrase
acquisition and lexical-syntactic template match-
ing. Next, we implemented a prototype that uti-
lizes a state-of-the-art method for learning en-
tailment relations from the web (Szpektor et al,
2004), the Minipar dependency parser (Lin, 1998)
and a syntactic matching module. As expected,
the performance of the implemented system was
much lower than the ideal upper bound, yet ob-
taining quite reasonable practical results given its
unsupervised nature.
The contributions of our investigation follow
409
the dual goal set above. To the best of our knowl-
edge, this is the first comprehensive evaluation
that measures directly the performance of unsuper-
vised paraphrase acquisition relative to a standard
application dataset. It is also the first evaluation of
a generic paraphrase-based approach for the stan-
dard RE setting. Our findings are encouraging for
both goals, particularly relative to their early ma-
turity level, and reveal constructive evidence for
the remaining room for improvement.
2 Background
2.1 Unsupervised Information Extraction
Information Extraction (IE) and its subfield Rela-
tion Extraction (RE) are traditionally performed
in a supervised manner, identifying the different
ways to express a specific information or relation.
Given that annotated data is expensive to produce,
unsupervised or weakly supervised methods have
been proposed for IE and RE.
Yangarber et al (2000) and Stevenson and
Greenwood (2005) define methods for automatic
acquisition of predicate-argument structures that
are similar to a set of seed relations, which rep-
resent a specific scenario. Yangarber et al (2000)
approach was evaluated in two ways: (1) manually
mapping the discovered patterns into an IE system
and running a full MUC-style evaluation; (2) using
the learned patterns to perform document filtering
at the scenario level. Stevenson and Greenwood
(2005) evaluated their method through document
and sentence filtering at the scenario level.
Sudo et al (2003) extract dependency subtrees
within relevant documents as IE patterns. The goal
of the algorithm is event extraction, though perfor-
mance is measured by counting argument entities
rather than counting events directly.
Hasegawa et al (2004) performs unsupervised
hierarchical clustering over a simple set of fea-
tures. The algorithm does not extract entity pairs
for a given relation from a set of documents but
rather classifies all relations in a large corpus. This
approach is more similar to text mining tasks than
to classic IE problems.
To conclude, several unsupervised approaches
learn relevant IE templates for a complete sce-
nario, but without identifying their relevance to
each specific relation within the scenario. Accord-
ingly, the evaluations of these works either did not
address the direct applicability for RE or evaluated
it only after further manual postprocessing.
2.2 Paraphrases and Entailment Rules
A generic model for language variability is us-
ing paraphrases, text expressions that roughly con-
vey the same meaning. Various methods for auto-
matic paraphrase acquisition have been suggested
recently, ranging from finding equivalent lexical
elements to learning rather complex paraphrases
at the sentence level1.
More relevant for RE are ?atomic? paraphrases
between templates, text fragments containing vari-
ables, e.g. ?X buy Y ? X purchase Y?. Under a
syntactic representation, a template is a parsed text
fragment, e.g. ?X subj? interact mod? with pcomp?n? Y?
(based on the syntactic dependency relations of
the Minipar parser). The parses include part-of-
speech tags, which we omit for clarity.
Dagan and Glickman (2004) suggested that a
somewhat more general notion than paraphrasing
is that of entailment relations. These are direc-
tional relations between two templates, where the
meaning of one can be entailed from the meaning
of the other, e.g. ?X bind to Y? X interact with Y?.
For RE, when searching for a target relation, it is
sufficient to identify an entailing template since it
implies that the target relation holds as well. Un-
der this notion, paraphrases are bidirectional en-
tailment relations.
Several methods extract atomic paraphrases by
exhaustively processing local corpora (Lin and
Pantel, 2001; Shinyama et al, 2002). Learn-
ing from a local corpus is bounded by the cor-
pus scope, which is usually domain specific (both
works above processed news domain corpora). To
cover a broader range of domains several works
utilized the Web, while requiring several manu-
ally provided examples for each input relation,
e.g. (Ravichandran and Hovy, 2002). Taking a
step further, the TEASE algorithm (Szpektor et al,
2004) provides a completely unsupervised method
for acquiring entailment relations from the Web
for a given input relation (see Section 5.1).
Most of these works did not evaluate their re-
sults in terms of application coverage. Lin and
Pantel (2001) compared their results to human-
generated paraphrases. Shinyama et al (2002)
measured the coverage of their learning algorithm
relative to the paraphrases present in a given cor-
pus. Szpektor et al (2004) measured ?yield?, the
number of correct rules learned for an input re-
1See the 3rd IWP workshop for a sample of recent works
on paraphrasing (http://nlp.nagaokaut.ac.jp/IWP2005/).
410
lation. Ravichandran and Hovy (2002) evaluated
the performance of a QA system that is based
solely on paraphrases, an approach resembling
ours. However, they measured performance using
Mean Reciprocal Rank, which does not reveal the
actual coverage of the learned paraphrases.
3 Assumed Configuration for RE
Phenomenon Example
Passive form ?Y is activated by X?
Apposition ?X activates its companion, Y?
Conjunction ?X activates prot3 and Y?
Set ?X activates two proteins, Y and Z?
Relative clause ?X, which activates Y?
Coordination ?X binds and activates Y?
Transparent head ?X activates a fragment of Y?
Co-reference ?X is a kinase, though it activates Y?
Table 1: Syntactic variability phenomena, demon-
strated for the normalized template ?X activate Y?.
The general configuration assumed in this pa-
per for RE is based on two main elements: a list
of lexical-syntactic templates which entail the re-
lation of interest and a syntactic matcher which
identifies the template occurrences in sentences.
The set of entailing templates may be collected ei-
ther manually or automatically. We propose this
configuration both as an algorithm for RE and as
an evaluation scheme for paraphrase acquisition.
The role of the syntactic matcher is to iden-
tify the different syntactic variations in which tem-
plates occur in sentences. Table 1 presents a list
of generic syntactic phenomena that are known in
the literature to relate to linguistic variability. A
phenomenon which deserves a few words of ex-
planation is the ?transparent head noun? (Grish-
man et al, 1986; Fillmore et al, 2002). A trans-
parent noun N1 typically occurs in constructs of
the form ?N1 preposition N2? for which the syn-
tactic relation involving N1, which is the head of
the NP, applies to N2, the modifier. In the example
in Table 1, ?fragment? is the transparent head noun
while the relation ?activate? applies to Y as object.
4 Manual Data Analysis
4.1 Protein Interaction Dataset
Bunescu et al (2005) proposed a set of tasks re-
garding protein name and protein interaction ex-
traction, for which they manually tagged about
200 Medline abstracts previously known to con-
tain human protein interactions (a binary symmet-
ric relation). Here we consider their RE task of
extracting interacting protein pairs, given that the
correct protein names have already been identi-
fied. All protein names are annotated in the given
gold standard dataset, which includes 1147 anno-
tated interacting protein pairs. Protein names are
rather complex, and according to the annotation
adopted by Bunescu et al (2005) can be substrings
of other protein names (e.g., <prot> <prot>
GITR </prot> ligand </prot>). In such
cases, we considered only the longest names and
protein pairs involving them. We also ignored all
reflexive pairs, in which one protein is marked
as interacting with itself. Altogether, 1052 inter-
actions remained. All protein names were trans-
formed into symbols of the type ProtN , where N
is a number, which facilitates parsing.
For development purposes, we randomly split
the abstracts into a 60% development set (575 in-
teractions) and a 40% test set (477 interactions).
4.2 Dataset analysis
In order to analyze the potential of our approach,
two of the authors manually annotated the 575 in-
teracting protein pairs in the development set. For
each pair the annotators annotated whether it can
be identified using only template-based matching,
assuming an ideal implementation of the configu-
ration of Section 3. If it can, the normalized form
of the template connecting the two proteins was
annotated as well. The normalized template form
is based on the active form of the verb, stripped
of the syntactic phenomena listed in Table 1. Ad-
ditionally, the relevant syntactic phenomena from
Table 1 were annotated for each template instance.
Table 2 provides several example annotations.
A Kappa value of 0.85 (nearly perfect agree-
ment) was measured for the agreement between
the two annotators, regarding whether a protein
pair can be identified using the template-based
method. Additionally, the annotators agreed on
96% of the normalized templates that should be
used for the matching. Finally, the annotators
agreed on at least 96% of the cases for each syn-
tactic phenomenon except transparent heads, for
which they agreed on 91% of the cases. This high
level of agreement indicates both that template-
based matching is a well defined task and that nor-
malized template form and its syntactic variations
are well defined notions.
Several interesting statistics arise from the an-
411
Sentence Annotation
We have crystallized a complex between human FGF1 and
a two-domain extracellular fragment of human FGFR2.
? template: ?complex between X and Y?
? transparent head: ?fragment of X?
CD30 and its counter-receptor CD30 ligand (CD30L) are
members of the TNF-receptor / TNFalpha superfamily and
function to regulate lymphocyte survival and differentiation.
? template: ?X?s counter-receptor Y?
? apposition
? co-reference
iCdi1, a human G1 and S phase protein phosphatase that
associates with Cdk2.
? template: ?X associate with Y?
? relative clause
Table 2: Examples of annotations of interacting protein pairs. The annotation describes the normalized
template and the different syntactic phenomena identified.
Template f Template f Template f
X interact with Y 28 interaction of X with Y 12 X Y interaction 5
X bind to Y 22 X associate with Y 11 X interaction with Y 4
X Y complex 17 X activate Y 6 association of X with Y 4
interaction between X and Y 16 binding of X to Y 5 X?s association with Y 3
X bind Y 14 X form complex with Y 5 X be agonist for Y 3
Table 3: The 15 most frequent templates and their instance count (f ) in the development set.
notation. First, 93% of the interacting protein pairs
(537/575) can be potentially identified using the
template-based approach, if the relevant templates
are provided. This is a very promising finding,
suggesting that the template-based approach may
provide most of the requested information. We
term these 537 pairs as template-based pairs. The
remaining pairs are usually expressed by complex
inference or at a discourse level.
Phenomenon % Phenomenon %
transparent head 34 relative clause 8
apposition 24 co-reference 7
conjunction 24 coordination 7
set 13 passive form 2
Table 4: Occurrence percentage of each syntactic
phenomenon within template-based pairs (537).
Second, for 66% of the template-based pairs
at least one syntactic phenomenon was annotated.
Table 4 contains the occurrence percentage of each
phenomenon in the development set. These results
show the need for a powerful syntactic matcher on
top of high performance template acquisition, in
order to correctly match a template in a sentence.
Third, 175 different normalized templates were
identified. For each template we counted its tem-
plate instances, the number of times the tem-
plate occurred, counting only occurrences that ex-
press an interaction of a protein pair. In total,
we counted 341 template instances for all 175
templates. Interestingly, 50% of the template in-
stances (184/341) are instances of the 21 most fre-
quent templates. This shows that, though protein
interaction can be expressed in many ways, writ-
ers tend to choose from among just a few common
expressions. Table 3 presents the most frequent
templates. Table 5 presents the minimal number
of templates required to obtain the range of differ-
ent recall levels.
Furthermore, we grouped template variants
that are based on morphological derivations (e.g.
?X interact with Y? and ?X Y interaction?)
and found that 4 groups, ?X interact with Y?,
?X bind to Y?, ?X associate with Y? and ?X com-
plex with Y?, together with their morphological
derivations, cover 45% of the template instances.
This shows the need to handle generic lexical-
syntactic phenomena, and particularly morpholog-
ical based variations, separately from the acquisi-
tion of normalized lexical syntactic templates.
To conclude, this analysis indicates that the
template-based approach provides very high cov-
erage for this RE dataset, and a small number of
normalized templates already provides significant
recall. However, it is important to (a) develop
a model for morphological-based template vari-
ations (e.g. as encoded in Nomlex (Macleod et
al., )), and (b) apply accurate parsing and develop
syntactic matching models to recognize the rather
412
complex variations of template instantiations in
text. Finally, we note that our particular figures
are specific to this dataset and the biological ab-
stracts domain. However, the annotation and anal-
ysis methodologies are general and are suggested
as highly effective tools for further research.
R(%) # templates R(%) # templates
10 2 60 39
20 4 70 73
30 6 80 107
40 11 90 141
50 21 100 175
Table 5: The number of most frequent templates
necessary to reach different recall levels within the
341 template instances.
5 Implemented Prototype
This section describes our initial implementation
of the approach in Section 3.
5.1 TEASE
The TEASE algorithm (Szpektor et al, 2004) is
an unsupervised method for acquiring entailment
relations from the Web for a given input template.
In this paper we use TEASE for entailment rela-
tion acquisition since it processes an input tem-
plate in a completely unsupervised manner and
due to its broad domain coverage obtained from
the Web. The reported percentage of correct out-
put templates for TEASE is 44%.
The TEASE algorithm consists of 3 steps,
demonstrated in Table 6. TEASE first retrieves
from the Web sentences containing the input tem-
plate. From these sentences it extracts variable in-
stantiations, termed anchor-sets, which are identi-
fied as being characteristic for the input template
based on statistical criteria (first column in Ta-
ble 6). Characteristic anchor-sets are assumed to
uniquely identify a specific event or fact. Thus,
any template that appears with such an anchor-set
is assumed to have an entailment relationship with
the input template. Next, TEASE retrieves from
the Web a corpus S of sentences that contain the
characteristic anchor-sets (second column), hop-
ing to find occurrences of these anchor-sets within
templates other than the original input template.
Finally, TEASE parses S and extracts templates
that are assumed to entail or be entailed by the
input template. Such templates are identified as
maximal most general sub-graphs that contain the
anchor sets? positions (third column in Table 6).
Each learned template is ranked by number of oc-
currences in S.
5.2 Transformation-based Graph Matcher
In order to identify instances of entailing templates
in sentences we developed a syntactic matcher that
is based on transformations rules. The matcher
processes a sentence in 3 steps: 1) parsing the sen-
tence with the Minipar parser, obtaining a depen-
dency graph2; 2) matching each template against
the sentence dependency graph; 3) extracting can-
didate term pairs that match the template variables.
A template is considered directly matched in a
sentence if it appears as a sub-graph in the sen-
tence dependency graph, with its variables instan-
tiated. To further address the syntactic phenomena
listed in Table 1 we created a set of hand-crafted
parser-dependent transformation rules, which ac-
count for the different ways in which syntactic
relationships may be realized in a sentence. A
transformation rule maps the left hand side of the
rule, which strictly matches a sub-graph of the
given template, to the right hand side of the rule,
which strictly matches a sub-graph of the sentence
graph. If a rule matches, the template sub-graph is
mapped accordingly into the sentence graph.
For example, to match the syntactic tem-
plate ?X(N) subj? activate(V) obj? Y(N)? (POS
tags are in parentheses) in the sentence ?Prot1
detected and activated Prot2? (see Figure 1) we
should handle the coordination phenomenon.
The matcher uses the transformation rule
?Var1(V) ? and(U)mod? Word(V) conj? Var1(V)?
to overcome the syntactic differences. In this
example Var1 matches the verb ?activate?, Word
matches the verb ?detect? and the syntactic rela-
tions for Word are mapped to the ones for Var1.
Thus, we can infer that the subject and object
relations of ?detect? are also related to ?activate?.
6 Experiments
6.1 Experimental Settings
To acquire a set of entailing templates we first ex-
ecuted TEASE on the input template ?X subj? in-
teract mod? with pcomp?n? Y?, which corresponds to
the ?default? expression of the protein interaction
2We chose a dependency parser as it captures directly the
relations between words; we use Minipar due to its speed.
413
Extracted Anchor-set Sentence containing Anchor-set Learned Template
X=?chemokines?,
Y=?specific receptors?
Chemokines bind to specific receptors on the target
cells
X subj? bind mod?
to
pcomp?n
? Y
X=?Smad3?, Y=?Smad4? Smad3 / Smad4 complexes translocate to the nucleus X Y nn? complex
Table 6: TEASE output at different steps of the algorithm for ?X subj? interact mod? with pcomp?n? Y?.
1. X bind to Y 7. X Y complex 13. X interaction with Y
2. X activate Y 8. X recognize Y 14. X trap Y
3. X stimulate Y 9. X block Y 15. X recruit Y
4. X couple to Y 10. X binding to Y 16. X associate with Y
5. interaction between X and Y 11. X Y interaction 17. X be linked to Y
6. X become trapped in Y 12. X attach to Y 18. X target Y
Table 7: The top 18 correct templates learned by TEASE for ?X interact with Y?.
detect(V )
subjwwppp
pp
pp
pp
pp
conj

mod
''NN
NN
NN
NN
NN
N
obj // Prot2(N)
Prot1(N) activate(V ) and(U)
Figure 1: The dependency parse graph of the sen-
tence ?Prot1 detected and activated Prot2?.
relation. TEASE learned 118 templates for this
relation. Table 7 lists the top 18 learned templates
that we considered as correct (out of the top 30
templates in TEASE output). We then extracted
interacting protein pair candidates by applying the
syntactic matcher to the 119 templates (the 118
learned plus the input template). Candidate pairs
that do not consist of two proteins, as tagged in the
input dataset, were filtered out (see Section 4.1;
recall that our experiments were applied to the
dataset of protein interactions, which isolates the
RE task from the protein name recognition task).
In a subsequent experiment we iteratively ex-
ecuted TEASE on the 5 top-ranked learned tem-
plates to acquire additional relevant templates. In
total, we obtained 1233 templates that were likely
to imply the original input relation. The syntactic
matcher was then reapplied to extract candidate in-
teracting protein pairs using all 1233 templates.
We used the development set to tune a small
set of 10 generic hand-crafted transformation rules
that handle different syntactic variations. To han-
dle transparent head nouns, which is the only phe-
nomenon that demonstrates domain dependence,
we extracted a set of the 5 most frequent trans-
parent head patterns in the development set, e.g.
?fragment of X?.
In order to compare (roughly) our performance
with supervised methods applied to this dataset, as
summarized in (Bunescu et al, 2005), we adopted
their recall and precision measurement. Their
scheme counts over distinct protein pairs per ab-
stract, which yields 283 interacting pairs in our test
set and 418 in the development set.
6.2 Manual Analysis of TEASE Recall
experiment pairs instances
input 39% 37%
input + iterative 49% 48%
input + iterative + morph 63% 62%
Table 8: The potential recall of TEASE in terms of
distinct pairs (out of 418) and coverage of template
instances (out of 341) in the development set.
Before evaluating the system as a whole we
wanted to manually assess in isolation the cover-
age of TEASE output relative to all template in-
stances that were manually annotated in the devel-
opment set. We considered a template as covered
if there is a TEASE output template that is equal
to the manually annotated template or differs from
it only by the syntactic phenomena described in
Section 3 or due to some parsing errors. Count-
ing these matches, we calculated the number of
template instances and distinct interacting protein
pairs that are covered by TEASE output.
Table 8 presents the results of our analysis. The
414
1st line shows the coverage of the 119 templates
learned by TEASE for the input template ?X inter-
act with Y?. It is interesting to note that, though we
aim to learn relevant templates for the specific do-
main, TEASE learned relevant templates also by
finding anchor-sets of different domains that use
the same jargon, such as particle physics.
We next analyzed the contribution of the itera-
tive learning for the additional 5 templates to recall
(2nd line in Table 8). With the additional learned
templates, recall increased by about 25%, showing
the importance of using the iterative steps.
Finally, when allowing matching between a
TEASE template and a manually annotated tem-
plate, even if one is based on a morphologi-
cal derivation of the other (3rd line in Table 8),
TEASE recall increased further by about 30%.
We conclude that the potential recall of the cur-
rent version of TEASE on the protein interaction
dataset is about 60%. This indicates that signif-
icant coverage can be obtained using completely
unsupervised learning from the web, as performed
by TEASE. However, the upper bound for our cur-
rent implemented system is only about 50% be-
cause our syntactic matching does not handle mor-
phological derivations.
6.3 System Results
experiment recall precision F1
input 0.18 0.62 0.28
input + iterative 0.29 0.42 0.34
Table 9: System results on the test set.
Table 9 presents our system results for the test
set, corresponding to the first two experiments in
Table 8. The recall achieved by our current imple-
mentation is notably worse than the upper bound
of the manual analysis because of two general set-
backs of the current syntactic matcher: 1) parsing
errors; 2) limited transformation rule coverage.
First, the texts from the biology domain pre-
sented quite a challenge for the Minipar parser.
For example, in the sentences containing the
phrase ?X bind specifically to Y? the parser consis-
tently attaches the PP ?to? to ?specifically? instead
of to ?bind?. Thus, the template ?X bind to Y? can-
not be directly matched.
Second, we manually created a small number of
transformation rules that handle various syntactic
phenomena, since we aimed at generic domain in-
dependent rules. The most difficult phenomenon
to model with transformation rules is transparent
heads. For example, in ?the dimerization of Prot1
interacts with Prot2?, the transparent head ?dimer-
ization of X? is domain dependent. Transforma-
tion rules that handle such examples are difficult
to acquire, unless a domain specific learning ap-
proach (either supervised or unsupervised) is used.
Finally, we did not handle co-reference resolution
in the current implementation.
Bunescu et al (2005) and Bunescu and Mooney
(2005) approached the protein interaction RE task
using both handcrafted rules and several super-
vised Machine Learning techniques, which uti-
lize about 180 manually annotated abstracts for
training. Our results are not directly comparable
with theirs because they adopted 10-fold cross-
validation, while we had to divide the dataset into
a development and a test set, but a rough compari-
son is possible. For the same 30% recall, the rule-
based method achieved precision of 62% and the
best supervised learning algorithm achieved preci-
sion of 73%. Comparing to these supervised and
domain-specific rule-based approaches our system
is noticeably weaker, yet provides useful results
given that we supply very little domain specific in-
formation and acquire the paraphrasing templates
in a fully unsupervised manner. Still, the match-
ing models need considerable additional research
in order to achieve the potential performance sug-
gested by TEASE.
7 Conclusions and Future Work
We have presented a paraphrase-based approach
for relation extraction (RE), and an implemented
system, that rely solely on unsupervised para-
phrase acquisition and generic syntactic template
matching. Two targets were investigated: (a) a
mostly unsupervised, domain independent, con-
figuration for RE, and (b) an evaluation scheme
for paraphrase acquisition, providing a first evalu-
ation of its realistic coverage. Our approach differs
from previous unsupervised IE methods in that we
identify instances of a specific relation while prior
methods identified template relevance only at the
general scenario level.
We manually analyzed the potential of our ap-
proach on a dataset annotated with protein in-
teractions. The analysis shows that 93% of the
interacting protein pairs can be potentially iden-
tified with the template-based approach. Addi-
415
tionally, we manually assessed the coverage of
the TEASE acquisition algorithm and found that
63% of the distinct pairs can be potentially rec-
ognized with the learned templates, assuming an
ideal matcher, indicating a significant potential re-
call for completely unsupervised paraphrase ac-
quisition. Finally, we evaluated our current system
performance and found it weaker than supervised
RE methods, being far from fulfilling the poten-
tial indicated in our manual analyses due to insuf-
ficient syntactic matching. But, even our current
performance may be considered useful given the
very small amount of domain-specific information
used by the system.
Most importantly, we believe that our analysis
and evaluation methodologies for an RE dataset
provide an excellent benchmark for unsupervised
learning of paraphrases and entailment rules. In
the long run, we plan to develop and improve our
acquisition and matching algorithms, in order to
realize the observed potential of the paraphrase-
based approach. Notably, our findings point to the
need to learn generic morphological and syntactic
variations in template matching, an area which has
rarely been addressed till now.
Acknowledgements
This work was developed under the collaboration
ITC-irst/University of Haifa. Lorenza Romano
has been supported by the ONTOTEXT project,
funded by the Autonomous Province of Trento un-
der the FUP-2004 research program.
References
Razvan Bunescu and Raymond J. Mooney. 2005. Sub-
sequence kernels for relation extraction. In Proceed-
ings of the 19th Conference on Neural Information
Processing Systems, Vancouver, British Columbia.
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun K.
Ramani, and Yuk Wah Wong. 2005. Comparative
experiments on learning information extractors for
proteins and their interactions. Artificial Intelligence
in Medicine, 33(2):139?155. Special Issue on Sum-
marization and Information Extraction from Medi-
cal Documents.
Ido Dagan and Oren Glickman. 2004. Probabilis-
tic textual entailment: Generic applied modeling of
language variability. In Proceedings of the PAS-
CAL Workshop on Learning Methods for Text Un-
derstanding and Mining, Grenoble, France.
Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato.
2002. Seeing arguments through transparent struc-
tures. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC 2002), pages 787?791, Las Palmas, Spain.
Ralph Grishman, Lynette Hirschman, and Ngo Thanh
Nhan. 1986. Discovery procedures for sublanguage
selectional patterns: Initial experiments. Computa-
tional Linguistics, 12(3).
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grish-
man. 2004. Discoverying relations among named
entities from large corpora. In Proceedings of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2004), Barcelona, Spain.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation on
MINIPAR. In Proceedings of LREC-98 Workshop
on Evaluation of Parsing Systems, Granada, Spain.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. Nomlex: A lexi-
con of nominalizations. In Proceedings of the 8th
International Congress of the European Association
for Lexicography, Liege, Belgium.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a Question Answering
system. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), Philadelphia, PA.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase ac-
quisition from news articles. In Proceedings of
the Human Language Technology Conference (HLT
2002), San Diego, CA.
Mark Stevenson and Mark A. Greenwood. 2005. A
semantic approach to IE pattern induction. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2005), Ann
Arbor, Michigan.
K. Sudo, S. Sekine, and R. Grishman. 2003. An im-
proved extraction pattern representation model for
automatic IE pattern acquisition. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL 2003), Sapporo, Japan.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisi-
tion of entailment relations. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2004), Barcelona,
Spain.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisition
of domain knowledge for information extraction. In
Proceedings of the 18th International Conference on
Computational Linguistics, Saarbruecken, Germany.
416
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 456?463,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Instance-based Evaluation of Entailment Rule Acquisition
Idan Szpektor, Eyal Shnarch, Ido Dagan
Dept. of Computer Science
Bar Ilan University
Ramat Gan, Israel
{szpekti,shey,dagan}@cs.biu.ac.il
Abstract
Obtaining large volumes of inference knowl-
edge, such as entailment rules, has become
a major factor in achieving robust seman-
tic processing. While there has been sub-
stantial research on learning algorithms for
such knowledge, their evaluation method-
ology has been problematic, hindering fur-
ther research. We propose a novel evalua-
tion methodology for entailment rules which
explicitly addresses their semantic proper-
ties and yields satisfactory human agreement
levels. The methodology is used to compare
two state of the art learning algorithms, ex-
posing critical issues for future progress.
1 Introduction
In many NLP applications, such as Question An-
swering (QA) and Information Extraction (IE), it is
crucial to recognize that a particular target mean-
ing can be inferred from different text variants. For
example, a QA system needs to identify that ?As-
pirin lowers the risk of heart attacks? can be inferred
from ?Aspirin prevents heart attacks? in order to an-
swer the question ?What lowers the risk of heart at-
tacks??. This type of reasoning has been recognized
as a core semantic inference task by the generic tex-
tual entailment framework (Dagan et al, 2006).
A major obstacle for further progress in seman-
tic inference is the lack of broad-scale knowledge-
bases for semantic variability patterns (Bar-Haim et
al., 2006). One prominent type of inference knowl-
edge representation is inference rules such as para-
phrases and entailment rules. We define an entail-
ment rule to be a directional relation between two
templates, text patterns with variables, e.g. ?X pre-
vent Y ? X lower the risk of Y ?. The left-hand-
side template is assumed to entail the right-hand-
side template in certain contexts, under the same
variable instantiation. Paraphrases can be viewed
as bidirectional entailment rules. Such rules capture
basic inferences and are used as building blocks for
more complex entailment inference. For example,
given the above rule, the answer ?Aspirin? can be
identified in the example above.
The need for large-scale inference knowledge-
bases triggered extensive research on automatic ac-
quisition of paraphrase and entailment rules. Yet the
current precision of acquisition algorithms is typ-
ically still mediocre, as illustrated in Table 1 for
DIRT (Lin and Pantel, 2001) and TEASE (Szpek-
tor et al, 2004), two prominent acquisition algo-
rithms whose outputs are publicly available. The
current performance level only stresses the obvious
need for satisfactory evaluation methodologies that
would drive future research.
The prominent approach in the literature for eval-
uating rules, termed here the rule-based approach, is
to present the rules to human judges asking whether
each rule is correct or not. However, it is difficult to
explicitly define when a learned rule should be con-
sidered correct under this methodology, and this was
mainly left undefined in previous works. As the cri-
terion for evaluating a rule is not well defined, using
this approach often caused low agreement between
human judges. Indeed, the standards for evaluation
in this field are lower than other fields: many papers
456
don?t report on human agreement at all and those
that do report rather low agreement levels. Yet it
is crucial to reliably assess rule correctness in or-
der to measure and compare the performance of dif-
ferent algorithms in a replicable manner. Lacking a
good evaluation methodology has become a barrier
for further advances in the field.
In order to provide a well-defined evaluation
methodology we first explicitly specify when entail-
ment rules should be considered correct, following
the spirit of their usage in applications. We then
propose a new instance-based evaluation approach.
Under this scheme, judges are not presented only
with the rule but rather with a sample of sentences
that match its left hand side. The judges then assess
whether the rule holds under each specific example.
A rule is considered correct only if the percentage of
examples assessed as correct is sufficiently high.
We have experimented with a sample of input
verbs for both DIRT and TEASE. Our results show
significant improvement in human agreement over
the rule-based approach. It is also the first compar-
ison between such two state-of-the-art algorithms,
which showed that they are comparable in precision
but largely complementary in their coverage.
Additionally, the evaluation showed that both al-
gorithms learn mostly one-directional rules rather
than (symmetric) paraphrases. While most NLP ap-
plications need directional inference, previous ac-
quisition works typically expected that the learned
rules would be paraphrases. Under such an expec-
tation, unidirectional rules were assessed as incor-
rect, underestimating the true potential of these algo-
rithms. In addition, we observed that many learned
rules are context sensitive, stressing the need to learn
contextual constraints for rule applications.
2 Background: Entailment Rules and their
Evaluation
2.1 Entailment Rules
An entailment rule ?L ? R? is a directional rela-
tion between two templates, L and R. For exam-
ple, ?X acquire Y ? X own Y ? or ?X beat Y ?
X play against Y ?. Templates correspond to text
fragments with variables, and are typically either lin-
ear phrases or parse sub-trees.
The goal of entailment rules is to help applica-
Input Correct Incorrect
(?) X modify Y X adopt Y
X change Y (?) X amend Y X create Y
(DIRT) (?) X revise Y X stick to Y
(?) X alter Y X maintain Y
X change Y (?) X affect Y X follow Y
(TEASE) (?) X extend Y X use Y
Table 1: Examples of templates suggested by DIRT
and TEASE as having an entailment relation, in
some direction, with the input template ?X change
Y ?. The entailment direction arrows were judged
manually and added for readability.
tions infer one text variant from another. A rule can
be applied to a given text only when L can be in-
ferred from it, with appropriate variable instantia-
tion. Then, using the rule, the application deduces
that R can also be inferred from the text under the
same variable instantiation. For example, the rule
?X lose to Y ?Y beat X? can be used to infer ?Liv-
erpool beat Chelsea? from ?Chelsea lost to Liver-
pool in the semifinals?.
Entailment rules should typically be applied only
in specific contexts, which we term relevant con-
texts. For example, the rule ?X acquire Y ?
X buy Y ? can be used in the context of ?buying?
events. However, it shouldn?t be applied for ?Stu-
dents acquired a new language?. In the same man-
ner, the rule ?X acquire Y ?X learn Y ? should be
applied only when Y corresponds to some sort of
knowledge, as in the latter example.
Some existing entailment acquisition algorithms
can add contextual constraints to the learned rules
(Sekine, 2005), but most don?t. However, NLP ap-
plications usually implicitly incorporate some con-
textual constraints when applying a rule. For ex-
ample, when answering the question ?Which com-
panies did IBM buy?? a QA system would apply
the rule ?X acquire Y ?X buy Y ? correctly, since
the phrase ?IBM acquire X? is likely to be found
mostly in relevant economic contexts. We thus ex-
pect that an evaluation methodology should consider
context relevance for entailment rules. For example,
we would like both ?X acquire Y ?X buy Y ? and
?X acquire Y ?X learn Y ? to be assessed as cor-
rect (the second rule should not be deemed incorrect
457
just because it is not applicable in frequent economic
contexts).
Finally, we highlight that the common notion of
?paraphrase rules? can be viewed as a special case
of entailment rules: a paraphrase ?L? R? holds if
both templates entail each other. Following the tex-
tual entailment formulation, we observe that many
applied inference settings require only directional
entailment, and a requirement for symmetric para-
phrase is usually unnecessary. For example, in or-
der to answer the question ?Who owns Overture??
it suffices to use a directional entailment rule whose
right hand side is ?X own Y ?, such as ?X acquire
Y ?X own Y ?, which is clearly not a paraphrase.
2.2 Evaluation of Acquisition Algorithms
Many methods for automatic acquisition of rules
have been suggested in recent years, ranging from
distributional similarity to finding shared contexts
(Lin and Pantel, 2001; Ravichandran and Hovy,
2002; Shinyama et al, 2002; Barzilay and Lee,
2003; Szpektor et al, 2004; Sekine, 2005). How-
ever, there is still no common accepted framework
for their evaluation. Furthermore, all these methods
learn rules as pairs of templates {L,R} in a sym-
metric manner, without addressing rule directional-
ity. Accordingly, previous works (except (Szpektor
et al, 2004)) evaluated the learned rules under the
paraphrase criterion, which underestimates the prac-
tical utility of the learned rules (see Section 2.1).
One approach which was used for evaluating au-
tomatically acquired rules is to measure their contri-
bution to the performance of specific systems, such
as QA (Ravichandran and Hovy, 2002) or IE (Sudo
et al, 2003; Romano et al, 2006). While measuring
the impact of learned rules on applications is highly
important, it cannot serve as the primary approach
for evaluating acquisition algorithms for several rea-
sons. First, developers of acquisition algorithms of-
ten do not have access to the different applications
that will later use the learned rules as generic mod-
ules. Second, the learned rules may affect individual
systems differently, thus making observations that
are based on different systems incomparable. Third,
within a complex system it is difficult to assess the
exact quality of entailment rules independently of
effects of other system components.
Thus, as in many other NLP learning settings,
a direct evaluation is needed. Indeed, the promi-
nent approach for evaluating the quality of rule ac-
quisition algorithms is by human judgment of the
learned rules (Lin and Pantel, 2001; Shinyama et
al., 2002; Barzilay and Lee, 2003; Pang et al, 2003;
Szpektor et al, 2004; Sekine, 2005). In this evalua-
tion scheme, termed here the rule-based approach, a
sample of the learned rules is presented to the judges
who evaluate whether each rule is correct or not. The
criterion for correctness is not explicitly described in
most previous works. By the common view of con-
text relevance for rules (see Section 2.1), a rule was
considered correct if the judge could think of rea-
sonable contexts under which it holds.
We have replicated the rule-based methodology
but did not manage to reach a 0.6 Kappa agree-
ment level between pairs of judges. This approach
turns out to be problematic because the rule correct-
ness criterion is not sufficiently well defined and is
hard to apply. While some rules might obviously
be judged as correct or incorrect (see Table 1), judg-
ment is often more difficult due to context relevance.
One judge might come up with a certain context
that, to her opinion, justifies the rule, while another
judge might not imagine that context or think that
it doesn?t sufficiently support rule correctness. For
example, in our experiments one of the judges did
not identify the valid ?religious holidays? context
for the correct rule ?X observe Y ?X celebrate Y ?.
Indeed, only few earlier works reported inter-judge
agreement level, and those that did reported rather
low Kappa values, such as 0.54 (Barzilay and Lee,
2003) and 0.55 - 0.63 (Szpektor et al, 2004).
To conclude, the prominent rule-based methodol-
ogy for entailment rule evaluation is not sufficiently
well defined. It results in low inter-judge agreement
which prevents reliable and consistent assessments
of different algorithms.
3 Instance-based Evaluation Methodology
As discussed in Section 2.1, an evaluation methodol-
ogy for entailment rules should reflect the expected
validity of their application within NLP systems.
Following that line, an entailment rule ?L ? R?
should be regarded as correct if in all (or at least
most) relevant contexts in which the instantiated
template L is inferred from the given text, the instan-
458
Rule Sentence Judgment
1 X seek Y ?X disclose Y If he is arrested, he can immediately seek bail. Left not entailed
2 X clarify Y ?X prepare Y He didn?t clarify his position on the subject. Left not entailed
3 X hit Y ?X approach Y Other earthquakes have hit Lebanon since ?82. Irrelevant context
4 X lose Y ?X surrender Y Bread has recently lost its subsidy. Irrelevant context
5 X regulate Y ?X reform Y The SRA regulates the sale of sugar. No entailment
6 X resign Y ?X share Y Lopez resigned his post at VW last week. No entailment
7 X set Y ?X allow Y The committee set the following refunds. Entailment holds
8 X stress Y ?X state Y Ben Yahia also stressed the need for action. Entailment holds
Table 2: Rule evaluation examples and their judgment.
tiated template R is also inferred from the text. This
reasoning corresponds to the common definition of
entailment in semantics, which specifies that a text
L entails another text R if R is true in every circum-
stance (possible world) in which L is true (Chierchia
and McConnell-Ginet, 2000).
It follows that in order to assess if a rule is cor-
rect we should judge whether R is typically en-
tailed from those sentences that entail L (within rel-
evant contexts for the rule). We thus present a new
evaluation scheme for entailment rules, termed the
instance-based approach. At the heart of this ap-
proach, human judges are presented not only with
a rule but rather with a sample of examples of the
rule?s usage. Instead of thinking up valid contexts
for the rule the judges need to assess the rule?s va-
lidity under the given context in each example. The
essence of our proposal is a (apparently non-trivial)
protocol of a sequence of questions, which deter-
mines rule validity in a given sentence.
We shall next describe how we collect a sample of
examples for evaluation and the evaluation process.
3.1 Sampling Examples
Given a rule ?L?R?, our goal is to generate evalua-
tion examples by finding a sample of sentences from
which L is entailed. We do that by automatically re-
trieving, from a given corpus, sentences that match
L and are thus likely to entail it, as explained below.
For each example sentence, we automatically ex-
tract the arguments that instantiate L and generate
two phrases, termed left phrase and right phrase,
which are constructed by instantiating the left tem-
plate L and the right template R with the extracted
arguments. For example, the left and right phrases
generated for example 1 in Table 2 are ?he seek bail?
and ?he disclose bail?, respectively.
Finding sentences that match L can be performed
at different levels. In this paper we match lexical-
syntactic templates by finding a sub-tree of the sen-
tence parse that is identical to the template structure.
Of course, this matching method is not perfect and
will sometimes retrieve sentences that do not entail
the left phrase for various reasons, such as incorrect
sentence analysis or semantic aspects like negation,
modality and conditionals. See examples 1-2 in Ta-
ble 2 for sentences that syntactically match L but
do not entail the instantiated left phrase. Since we
should assess R?s entailment only from sentences
that entail L, such sentences should be ignored by
the evaluation process.
3.2 Judgment Questions
For each example generated for a rule, the judges are
presented with the given sentence and the left and
right phrases. They primarily answer two questions
that assess whether entailment holds in this example,
following the semantics of entailment rule applica-
tion as discussed above:
Qle: Is the left phrase entailed from the sentence?
A positive/negative answer corresponds to a
?Left entailed/not entailed? judgment.
Qre: Is the right phrase entailed from the sentence?
A positive/negative answer corresponds to an
?Entailment holds/No entailment? judgment.
The first question identifies sentences that do not en-
tail the left phrase, and thus should be ignored when
evaluating the rule?s correctness. While inappropri-
ate matches of the rule left-hand-side may happen
459
and harm an overall system precision, such errors
should be accounted for a system?s rule matching
module rather than for the rules? precision. The sec-
ond question assesses whether the rule application is
valid or not for the current example. See examples
5-8 in Table 2 for cases where entailment does or
doesn?t hold.
Thus, the judges focus only on the given sentence
in each example, so the task is actually to evaluate
whether textual entailment holds between the sen-
tence (text) and each of the left and right phrases
(hypotheses). Following past experience in textual
entailment evaluation (Dagan et al, 2006) we expect
a reasonable agreement level between judges.
As discussed in Section 2.1, we may want to ig-
nore examples whose context is irrelevant for the
rule. To optionally capture this distinction, the
judges are asked another question:
Qrc: Is the right phrase a likely phrase in English?
A positive/negative answer corresponds to a
?Relevant/Irrelevant context? evaluation.
If the right phrase is not likely in English then the
given context is probably irrelevant for the rule, be-
cause it seems inherently incorrect to infer an im-
plausible phrase. Examples 3-4 in Table 2 demon-
strate cases of irrelevant contexts, which we may
choose to ignore when assessing rule correctness.
3.3 Evaluation Process
For each example, the judges are presented with the
three questions above in the following order: (1) Qle
(2) Qrc (3) Qre. If the answer to a certain question
is negative then we do not need to present the next
questions to the judge: if the left phrase is not en-
tailed then we ignore the sentence altogether; and if
the context is irrelevant then the right phrase cannot
be entailed from the sentence and so the answer to
Qre is already known as negative.
The above entailment judgments assume that we
can actually ask whether the left or right phrases
are correct given the sentence, that is, we assume
that a truth value can be assigned to both phrases.
This is the case when the left and right templates
correspond, as expected, to semantic relations. Yet
sometimes learned templates are (erroneously) not
relational, e.g. ?X , Y , IBM? (representing a list).
We therefore let the judges initially mark rules that
include such templates as non-relational, in which
case their examples are not evaluated at all.
3.4 Rule Precision
We compute the precision of a rule by the percent-
age of examples for which entailment holds out
of all ?relevant? examples. We can calculate the
precision in two ways, as defined below, depending
on whether we ignore irrelevant contexts or not
(obtaining lower precision if we don?t). When
systems answer an information need, such as a
query or question, irrelevant contexts are sometimes
not encountered thanks to additional context which
is present in the given input (see Section 2.1). Thus,
the following two measures can be viewed as upper
and lower bounds for the expected precision of the
rule applications in actual systems:
upper bound precision: #Entailment holds#Relevant context
lower bound precision: #Entailment holds#Left entailed
where # denotes the number of examples with
the corresponding judgment.
Finally, we consider a rule to be correct only if
its precision is at least 80%, which seems sensible
for typical applied settings. This yields two alterna-
tive sets of correct rules, corresponding to the upper
bound and lower bound precision measures. Even
though judges may disagree on specific examples for
a rule, their judgments may still agree overall on the
rule?s correctness. We therefore expect the agree-
ment level on rule correctness to be higher than the
agreement on individual examples.
4 Experimental Settings
We applied the instance-based methodology to eval-
uate two state-of-the-art unsupervised acquisition al-
gorithms, DIRT (Lin and Pantel, 2001) and TEASE
(Szpektor et al, 2004), whose output is publicly
available. DIRT identifies semantically related tem-
plates in a local corpus using distributional sim-
ilarity over the templates? variable instantiations.
TEASE acquires entailment relations from the Web
for a given input template I by identifying charac-
teristic variable instantiations shared by I and other
templates.
460
For the experiment we used the published DIRT
and TEASE knowledge-bases1. For every given in-
put template I , each knowledge-base provides a list
of learned output templates {Oj}nI1 , where nI is the
number of output templates learned for I . Each out-
put template is suggested as holding an entailment
relation with the input template I , but the algorithms
do not specify the entailment direction(s). Thus,
each pair {I,Oj} induces two candidate directional
entailment rules: ?I?Oj? and ?Oj?I?.
4.1 Test Set Construction
The test set construction consists of three sampling
steps: selecting a set of input templates for the two
algorithms, selecting a sample of output rules to be
evaluated, and selecting a sample of sentences to be
judged for each rule.
First, we randomly selected 30 transitive verbs
out of the 1000 most frequent verbs in the Reuters
RCV1 corpus2. For each verb we manually
constructed a lexical-syntactic input template by
adding subject and object variables. For exam-
ple, for the verb ?seek? we constructed the template
?X subj??? seek obj??? Y ?.
Next, for each input template I we considered
the learned templates {Oj}nI1 from each knowledge-
base. Since DIRT has a long tail of templates with
a low score and very low precision, DIRT templates
whose score is below a threshold of 0.1 were filtered
out3. We then sampled 10% of the templates in each
output list, limiting the sample size to be between
5-20 templates for each list (thus balancing between
sufficient evaluation data and judgment load). For
each sampled template O we evaluated both direc-
tional rules, ?I?O? and ?O?I?. In total, we sam-
pled 380 templates, inducing 760 directional rules
out of which 754 rules were unique.
Last, we randomly extracted a sample of example
sentences for each rule ?L?R? by utilizing a search
engine over the first CD of Reuters RCV1. First, we
retrieved all sentences containing all lexical terms
within L. The retrieved sentences were parsed using
the Minipar dependency parser (Lin, 1998), keep-
ing only sentences that syntactically match L (as
1Available at http://aclweb.org/aclwiki/index.php?title=Te-
xtual Entailment Resource Pool
2http://about.reuters.com/researchandstandards/corpus/
3Following advice by Patrick Pantel, DIRT?s co-author.
explained in Section 3.1). A sample of 15 match-
ing sentences was randomly selected, or all match-
ing sentences if less than 15 were found. Finally,
an example for judgment was generated from each
sampled sentence and its left and right phrases (see
Section 3.1). We did not find sentences for 108
rules, and thus we ended up with 646 unique rules
that could be evaluated (with 8945 examples to be
judged).
4.2 Evaluating the Test-Set
Two human judges evaluated the examples. We
randomly split the examples between the judges.
100 rules (1287 examples) were cross annotated for
agreement measurement. The judges followed the
procedure in Section 3.3 and the correctness of each
rule was assessed based on both its upper and lower
bound precision values (Section 3.4).
5 Methodology Evaluation Results
We assessed the instance-based methodology by
measuring the agreement level between judges. The
judges agreed on 75% of the 1287 shared exam-
ples, corresponding to a reasonable Kappa value of
0.64. A similar kappa value of 0.65 was obtained
for the examples that were judged as either entail-
ment holds/no entailment by both judges. Yet, our
evaluation target is to assess rules, and the Kappa
values for the final correctness judgments of the
shared rules were 0.74 and 0.68 for the lower and
upper bound evaluations. These Kappa scores are
regarded as ?substantial agreement? and are substan-
tially higher than published agreement scores and
those we managed to obtain using the standard rule-
based approach. As expected, the agreement on
rules is higher than on examples, since judges may
disagree on a certain example but their judgements
would still yield the same rule assessment.
Table 3 illustrates some disagreements that were
still exhibited within the instance-based evaluation.
The primary reason for disagreements was the dif-
ficulty to decide whether a context is relevant for
a rule or not, resulting in some confusion between
?Irrelevant context? and ?No entailment?. This may
explain the lower agreement for the upper bound
precision, for which examples judged as ?Irrelevant
context? are ignored, while for the lower bound both
461
Rule Sentence Judge 1 Judge 2
X sign Y ?X set Y Iraq and Turkey sign agreement
to increase trade cooperation
Entailment holds Irrelevant context
X worsen Y ?X slow Y News of the strike worsened the
situation
Irrelevant context No entailment
X get Y ?X want Y He will get his parade on Tuesday Entailment holds No entailment
Table 3: Examples for disagreement between the two judges.
judgments are conflated and represent no entailment.
Our findings suggest that better ways for distin-
guishing relevant contexts may be sought in future
research for further refinement of the instance-based
evaluation methodology.
About 43% of all examples were judged as ?Left
not entailed?. The relatively low matching precision
(57%) made us collect more examples than needed,
since ?Left not entailed? examples are ignored. Bet-
ter matching capabilities will allow collecting and
judging fewer examples, thus improving the effi-
ciency of the evaluation process.
6 DIRT and TEASE Evaluation Results
DIRT TEASE
P Y P Y
Rules:
Upper Bound 30.5% 33.5 28.4% 40.3
Lower Bound 18.6% 20.4 17% 24.1
Templates:
Upper Bound 44% 22.6 38% 26.9
Lower Bound 27.3% 14.1 23.6% 16.8
Table 4: Average Precision (P) and Yield (Y) at the
rule and template levels.
We evaluated the quality of the entailment rules
produced by each algorithm using two scores: (1)
micro average Precision, the percentage of correct
rules out of all learned rules, and (2) average Yield,
the average number of correct rules learned for each
input template I , as extrapolated based on the sam-
ple4. Since DIRT and TEASE do not identify rule
directionality, we also measured these scores at the
4Since the rules are matched against the full corpus (as in IR
evaluations), it is difficult to evaluate their true recall.
template level, where an output template O is con-
sidered correct if at least one of the rules ?I?O? or
?O? I? is correct. The results are presented in Ta-
ble 4. The major finding is that the overall quality of
DIRT and TEASE is very similar. Under the specific
DIRT cutoff threshold chosen, DIRT exhibits some-
what higher Precision while TEASE has somewhat
higher Yield (recall that there is no particular natural
cutoff point for DIRT?s output).
Since applications typically apply rules in a spe-
cific direction, the Precision for rules reflects their
expected performance better than the Precision for
templates. Obviously, future improvement in pre-
cision is needed for rule learning algorithms. Mean-
while, manual filtering of the learned rules can prove
effective within limited domains, where our evalua-
tion approach can be utilized for reliable filtering as
well. The substantial yield obtained by these algo-
rithms suggest that they are indeed likely to be valu-
able for recall increase in semantic applications.
In addition, we found that only about 15% of the
correct templates were learned by both algorithms,
which implies that the two algorithms largely com-
plement each other in terms of coverage. One ex-
planation may be that DIRT is focused on the do-
main of the local corpus used (news articles for the
published DIRT knowledge-base), whereas TEASE
learns from the Web, extracting rules from multiple
domains. Since Precision is comparable it may be
best to use both algorithms in tandem.
We also measured whether O is a paraphrase of
I , i.e. whether both ?I ?O? and ?O? I? are cor-
rect. Only 20-25% of all correct templates were as-
sessed as paraphrases. This stresses the significance
of evaluating directional rules rather than only para-
phrases. Furthermore, it shows that in order to im-
prove precision, acquisition algorithms must iden-
tify rule directionality.
462
About 28% of all ?Left entailed? examples were
evaluated as ?Irrelevant context?, yielding the large
difference in precision between the upper and lower
precision bounds. This result shows that in order
to get closer to the upper bound precision, learning
algorithms and applications need to identify the rel-
evant contexts in which a rule should be applied.
Last, we note that the instance-based quality as-
sessment corresponds to the corpus from which the
example sentences were taken. It is therefore best to
evaluate the rules using a corpus of the same domain
from which they were learned, or the target applica-
tion domain for which the rules will be applied.
7 Conclusions
Accurate learning of inference knowledge, such as
entailment rules, has become critical for further
progress of applied semantic systems. However,
evaluation of such knowledge has been problematic,
hindering further developments. The instance-based
evaluation approach proposed in this paper obtained
acceptable agreement levels, which are substantially
higher than those obtained for the common rule-
based approach.
We also conducted the first comparison between
two state-of-the-art acquisition algorithms, DIRT
and TEASE, using the new methodology. We found
that their quality is comparable but they effectively
complement each other in terms of rule coverage.
Also, we found that most learned rules are not para-
phrases but rather one-directional entailment rules,
and that many of the rules are context sensitive.
These findings suggest interesting directions for fu-
ture research, in particular learning rule direction-
ality and relevant contexts, issues that were hardly
explored till now. Such developments can be then
evaluated by the instance-based methodology, which
was designed to capture these two important aspects
of entailment rules.
Acknowledgements
The authors would like to thank Ephi Sachs and
Iddo Greental for their evaluation. This work was
partially supported by ISF grant 1095/05, the IST
Programme of the European Community under the
PASCAL Network of Excellence IST-2002-506778,
and the ITC-irst/University of Haifa collaboration.
References
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second pascal recognising textual entail-
ment challenge. In Second PASCAL Challenge Work-
shop for Recognizing Textual Entailment.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of NAACL-HLT.
Gennaro Chierchia and Sally McConnell-Ginet. 2000.
Meaning and Grammar (2nd ed.): an introduction to
semantics. MIT Press, Cambridge, MA.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment chal-
lenge. Lecture Notes in Computer Science, 3944:177?
190.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalu-
ation of Parsing Systems at LREC.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of HLT-NAACL.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Lorenza Romano, Milen Kouylekov, Idan Szpektor, Ido
Dagan, and Alberto Lavelli. 2006. Investigating a
generic paraphrase-based approach for relation extrac-
tion. In Proceedings of EACL.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of HLT.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic IE pattern acquisition. In Pro-
ceedings of ACL.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP.
463
Proceedings of ACL-08: HLT, pages 683?691,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Contextual Preferences
Idan Szpektor, Ido Dagan, Roy Bar-Haim
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
{szpekti,dagan,barhair}@cs.biu.ac.il
Jacob Goldberger
School of Engineering
Bar-Ilan University
Ramat Gan, Israel
goldbej@eng.biu.ac.il
Abstract
The validity of semantic inferences depends
on the contexts in which they are applied.
We propose a generic framework for handling
contextual considerations within applied in-
ference, termed Contextual Preferences. This
framework defines the various context-aware
components needed for inference and their
relationships. Contextual preferences extend
and generalize previous notions, such as se-
lectional preferences, while experiments show
that the extended framework allows improving
inference quality on real application data.
1 Introduction
Applied semantic inference is typically concerned
with inferring a target meaning from a given text.
For example, to answer ?Who wrote Idomeneo??,
Question Answering (QA) systems need to infer the
target meaning ?Mozart wrote Idomeneo? from a
given text ?Mozart composed Idomeneo?. Following
common Textual Entailment terminology (Giampic-
colo et al, 2007), we denote the target meaning by h
(for hypothesis) and the given text by t.
A typical applied inference operation is matching.
Sometimes, h can be directly matched in t (in the
example above, if the given sentence would be liter-
ally ?Mozart wrote Idomeneo?). Generally, the tar-
get meaning can be expressed in t in many differ-
ent ways. Indirect matching is then needed, using
inference knowledge that may be captured through
rules, termed here entailment rules. In our exam-
ple, ?Mozart wrote Idomeneo? can be inferred using
the rule ?X compose Y ? X write Y ?. Recently,
several algorithms were proposed for automatically
learning entailment rules and paraphrases (viewed
as bi-directional entailment rules) (Lin and Pantel,
2001; Ravichandran and Hovy, 2002; Shinyama et
al., 2002; Szpektor et al, 2004; Sekine, 2005).
A common practice is to try matching the struc-
ture of h, or of the left-hand-side of a rule r, within
t. However, context should be considered to allow
valid matching. For example, suppose that to find
acquisitions of companies we specify the target tem-
plate hypothesis (a hypothesis with variables) ?X ac-
quire Y ?. This h should not be matched in ?children
acquire language quickly?, because in this context
Y is not a company. Similarly, the rule ?X charge
Y ? X accuse Y ? should not be applied to ?This
store charged my account?, since the assumed sense
of ?charge? in the rule is different than its sense in
the text. Thus, the intended contexts for h and r
and the context within the given t should be prop-
erly matched to verify valid inference.
Context matching at inference time was of-
ten approached in an application-specific manner
(Harabagiu et al, 2003; Patwardhan and Riloff,
2007). Recently, some generic methods were pro-
posed to handle context-sensitive inference (Dagan
et al, 2006; Pantel et al, 2007; Downey et al, 2007;
Connor and Roth, 2007), but these usually treat
only a single aspect of context matching (see Sec-
tion 6). We propose a comprehensive framework for
handling various contextual considerations, termed
Contextual Preferences. It extends and generalizes
previous work, defining the needed contextual com-
ponents and their relationships. We also present and
implement concrete representation models and un-
683
supervised matching methods for these components.
While our presentation focuses on semantic infer-
ence using lexical-syntactic structures, the proposed
framework and models seem suitable for other com-
mon types of representations as well.
We applied our models to a test set derived from
the ACE 2005 event detection task, a standard In-
formation Extraction (IE) benchmark. We show the
benefits of our extended framework for textual in-
ference and present component-wise analysis of the
results. To the best of our knowledge, these are also
the first unsupervised results for event argument ex-
traction in the ACE 2005 dataset.
2 Contextual Preferences
2.1 Notation
As mentioned above, we follow the generic Tex-
tual Entailment (TE) setting, testing whether a target
meaning hypothesis h can be inferred from a given
text t. We allow h to be either a text or a template,
a text fragment with variables. For example, ?The
stock rose 8%? entails an instantiation of the tem-
plate hypothesis ?X gain Y ?. Typically, h represents
an information need requested in some application,
such as a target predicate in IE.
In this paper, we focus on parse-based lexical-
syntactic representation of texts and hypotheses, and
on the basic inference operation of matching. Fol-
lowing common practice (de Salvo Braz et al, 2005;
Romano et al, 2006; Bar-Haim et al, 2007), h is
syntactically matched in t if it can be embedded in
t?s parse tree. For template hypotheses, the matching
induces a mapping between h?s variables and their
instantiation in t.
Matching h in t can be performed either directly
or indirectly using entailment rules. An entailment
rule r: ?LHS ? RHS? is a directional entailment
relation between two templates. h is matched in t us-
ing r if LHS is matched in t and h matches RHS.
In the example above, r: ?X rise Y ?X gain Y ? al-
lows us to entail ?X gain Y ?, with ?stock? and ?8%?
instantiating h?s variables. We denote vars(z) the
set of variables of z, where z is a template or a rule.
2.2 Motivation
When matching considers only the structure of hy-
potheses, texts and rules it may result in incorrect
inference due to contextual mismatches. For exam-
ple, an IE system may identify mentions of public
demonstrations using the hypothesis h: ?X demon-
strate?. However, h should not be matched in ?Engi-
neers demonstrated the new system?, due to a mis-
match between the intended sense of ?demonstrate?
in h and its sense in t. Similarly, when looking for
physical attack mentions using the hypothesis ?X at-
tack Y ?, we should not utilize the rule r: ?X accuse
Y ?X attack Y ?, due to a mismatch between a ver-
bal attack in r and an intended physical attack in h.
Finally, r: ?X produce Y ? X lay Y ? (applicable
when X refers to poultry and Y to eggs) should not
be matched in t: ?Bugatti produce the fastest cars?,
due to a mismatch between the meanings of ?pro-
duce? in r and t. Overall, such incorrect inferences
may be avoided by considering contextual informa-
tion for t, h and r during their matching process.
2.3 The Contextual Preferences Framework
We propose the Contextual Preferences (CP) frame-
work for addressing context at inference time. In this
framework, the representation of an object z, where
z may be a text, a template or an entailment rule, is
enriched with contextual information denoted cp(z).
This information helps constraining or disambiguat-
ing the meaning of z, and is used to validate proper
matching between pairs of objects.
We consider two components within cp(z): (a)
a representation for the global (?topical?) context
in which z typically occurs, denoted cpg(z); (b)
a representation for the preferences and constraints
(?hard? preferences) on the possible terms that can
instantiate variables within z, denoted cpv(z). For
example, cpv(?X produce Y ? X lay Y ?) may
specify that X?s instantiations should be similar to
?chicken? or ?duck?.
Contextual Preferences are used when entailment
is assessed between a text t and a hypothesis h, ei-
ther directly or by utilizing an entailment-rule r. On
top of structural matching, we now require that the
Contextual Preferences of the participants in the in-
ference will also match. When h is directly matched
in t, we require that each component in cp(h) will
be matched with its counterpart in cp(t). When r is
utilized, we additionally require that cp(r) will be
matched with both cp(t) and cp(h). Figure 1 sum-
marizes the matching relationships between the CP
684
Figure 1: The directional matching relationships between
a hypothesis (h), an entailment rule (r) and a text (t) in the
Contextual Preferences framework.
components of h, t and r.
Like Textual Entailment inference, Contextual
Preferences matching is directional. When matching
h with t we require that the global context prefer-
ences specified by cpg(h) would subsume those in-
duced by cpg(t), and that the instantiations of h?s
variables in t would adhere to the preferences in
cpv(h) (since t should entail h, but not necessarily
vice versa). For example, if the preferred global con-
text of a hypothesis is sports, it would match a text
that discusses the more specific topic of basketball.
To implement the CP framework, concrete models
are needed for each component, specifying its repre-
sentation, how it is constructed, and an appropriate
matching procedure. Section 3 describes the specific
CP models that were implemented in this paper.
The CP framework provides a generic view of
contextual modeling in applied semantic inference.
Mapping from a specific application to the generic
framework follows the mappings assumed in the
Textual Entailment paradigm. For example, in QA
the hypothesis to be proved corresponds to the affir-
mative template derived from the question (e.g. h:
?X invented the PC? for ?Who invented the PC??).
Thus, cpg(h) can be constructed with respect to
the question?s focus while cpv(h) may be gener-
ated from the expected answer type (Moldovan et
al., 2000; Harabagiu et al, 2003). Construction of
hypotheses? CP for IE is demonstrated in Section 4.
3 Contextual Preferences Models
This section presents the current models that we im-
plemented for the various components of the CP
framework. For each component type we describe
its representation, how it is constructed, and a cor-
responding unsupervised match score. Finally, the
different component scores are combined to yield
an overall match score, which is used in our exper-
iments to rank inference instances by the likelihood
of their validity. Our goal in this paper is to cover the
entire scope of the CP framework by including spe-
cific models that were proposed in previous work,
where available, and elsewhere propose initial mod-
els to complete the CP scope.
3.1 Contextual Preferences for Global Context
To represent the global context of an object z we
utilize Latent Semantic Analysis (LSA) (Deerwester
et al, 1990), a well-known method for representing
the contextual-usage of words based on corpus sta-
tistics. We use LSA analysis of the BNC corpus1,
in which every term is represented by a normalized
vector of the top 100 SVD dimensions, as described
in (Gliozzo, 2005).
To construct cpg(z) we first collect a set of terms
that are representative for the preferred general con-
text of z. Then, the (single) vector which is the sum
of the LSA vectors of the representative terms be-
comes the representation of cpg(z). This LSA vec-
tor captures the ?average? typical contexts in which
the representative terms occur.
The set of representative terms for a text t con-
sists of all the nouns and verbs in it, represented
by their lemma and part of speech. For a rule r:
?LHS ? RHS?, the representative terms are the
words appearing in LHS and in RHS. For exam-
ple, the representative terms for ?X divorce Y ? X
marry Y ? are {divorce:v, marry:v}. As mentioned
earlier, construction of hypotheses and their contex-
tual preferences depends on the application at hand.
In our experiments these are defined manually, as
described in Section 4, derived from the manual de-
finitions of target meanings in the IE data.
The score of matching the cpg components of two
objects, denoted by mg(?, ?), is the Cosine similarity
of their LSA vectors. Negative values are set to 0.
3.2 Contextual Preferences for Variables
3.2.1 Representation
For comparison with prior work, we follow (Pan-
tel et al, 2007) and represent preferences for vari-
1http://www.natcorp.ox.ac.uk/
685
able instantiations using a distributional approach,
and in addition incorporate a standard specification
of named-entity types. Thus, cpv is represented by
two lists. The first list, denoted cpv:e, contains ex-
amples for valid instantiations of that variable. For
example, cpv:e(X kill Y ? Y die of X) may be
[X: {snakebite, disease}, Y : {man, patient}]. The
second list, denoted cpv:n, contains the variable?s
preferred named-entity types (if any). For exam-
ple, cpv:n(X born in Y ) may be [X: {Person}, Y :
{Location}]. We denote cpv:e(z)[j] and cpv:n(z)[j]
as the lists for a specific variable j of the object z.
For a text t, in which a template p is matched, the
preference cpv:e(t) for each template variable is sim-
ply its instantiation in t. For example, when ?X eat
Y ? is matched in t: ?Many Americans eat fish reg-
ularly?, we construct cpv:e(t) = [X: {Many Ameri-
cans}, Y : {fish}]. Similarly, cpv:n(t) for each vari-
able is the named-entity type of its instantiation in
t (if it is a named entity). We identify entity types
using the default Lingpipe2 Named-Entity Recog-
nizer (NER), which recognizes the types Location,
Person and Organization. In the above example,
cpv:n(t)[X] would be {Person}.
For a rule r: LHS ? RHS, we automatically
add to cpv:e(r) all the variable instantiations that
were found common for both LHS and RHS in a
corpus (see Section 4), as in (Pantel et al, 2007; Pen-
nacchiotti et al, 2007). To construct cpv:n(r), we
currently use a simple approach where each individ-
ual term in cpv:e(r) is analyzed by the NER system,
and its type (if any) is added to cpv:n(r).
For a template hypothesis, we currently repre-
sent cpv(h) only by its list of preferred named-entity
types, cpv:n. Similarly to cpg(h), the preferred types
for each template variable were adapted from those
defined in our IE data (see Section 4).
To allow compatible comparisons with previous
work (see Sections 5 and 6), we utilize in this
paper only cpv:e when matching between cpv(r)
and cpv(t), as only this representation was exam-
ined in prior work on context-sensitive rule applica-
tions. cpv:n is utilized for context matches involving
cpv(h). We denote the score of matching two cpv
components by mv(?, ?).
2http://www.alias-i.com/lingpipe/
3.2.2 Matching cpv:e
Our primary matching method is based on repli-
cating the best-performing method reported in (Pan-
tel et al, 2007), which utilizes the CBC distribu-
tional word clustering algorithm (Pantel, 2003). In
short, this method extends each cpv:e list with CBC
clusters that contain at least one term in the list, scor-
ing them according to their ?relevancy?. The score
of matching two cpv:e lists, denoted here SCBC(?, ?),
is the score of the highest scoring member that ap-
pears in both lists.
We applied the final binary match score presented
in (Pantel et al, 2007), denoted here binaryCBC:
mv:e(r, t) is 1 if SCBC(r, t) is above a threshold and
0 otherwise. As a more natural ranking method, we
also utilize SCBC directly, denoted rankedCBC,
having mv:e(r, t) = SCBC(r, t).
In addition, we tried a simpler method that di-
rectly compares the terms in two cpv:e lists, uti-
lizing the commonly-used term similarity metric of
(Lin, 1998a). This method, denoted LIN , uses the
same raw distributional data as CBC but computes
only pair-wise similarities, without any clustering
phase. We calculated the scores of the 1000 most
similar terms for every term in the Reuters RVC1
corpus3. Then, a directional similarity of term a
to term b, s(a, b), is set to be their similarity score
if a is in b?s 1000 most similar terms and 0 other-
wise. The final score of matching r with t is deter-
mined by a nearest-neighbor approach, as the score
of the most similar pair of terms in the correspond-
ing two lists of the same variable: mv:e(r, t) =
maxj?vars(r)[maxa?cpv:e(t)[j],b?cpv:e(r)[j][s(a, b)]].
3.2.3 Matching cpv:n
We use a simple scoring mechanism for compar-
ing between two named-entity types a and b, s(a, b):
1 for identical types and 0.8 otherwise.
A variable j has a single preferred entity type
in cpv:n(t)[j], the type of its instantiation in t.
However, it can have several preferred types for h.
When matching h with t, j?s match score is that
of its highest scoring type, and the final score is
the product of all variable scores: mv:n(h, t) =?
j?vars(h)(maxa?cpv:n(h)[j][s(a, cpv:n(t)[j])]).
Variable j may also have several types in r, the
3http://about.reuters.com/researchandstandards/corpus/
686
types of the common arguments in cpv:e(r). When
matching h with r, s(a, cpv:n(t)[j]) is replaced with
the average score for a and each type in cpv:n(r)[j].
3.3 Overall Score for a Match
A final score for a given match, denoted allCP, is
obtained by the product of all six matching scores
of the various CP components (multiplying by 1
if a component score is missing). The six scores
are the results of matching any of the two compo-
nents of h, t and r: mg(h, t), mv(h, t), mg(h, r),
mv(h, r), mg(r, t) and mv(r, t) (as specified above,
mv(r, t) is based on matching cpv:e while mv(h, r)
and mv(h, t) are based on matching cpv:n). We use
rankedCBC for calculating mv(r, t).
Unlike previous work (e.g. (Pantel et al, 2007)),
we also utilize the prior score of a rule r, which
is provided by the rule-learning algorithm (see next
section). We denote by allCP+pr the final match
score obtained by the product of the allCP score
with the prior score of the matched rule.
4 Experimental Settings
Evaluating the contribution of Contextual Prefer-
ences models requires: (a) a sample of test hypothe-
ses, and (b) a corresponding corpus that contains
sentences which entail these hypotheses, where all
hypothesis matches (either direct or via rules) are an-
notated. We found that the available event mention
annotations in the ACE 2005 training set4 provide a
useful test set that meets these generic criteria, with
the added value of a standard real-world dataset.
The ACE annotation includes 33 types of events,
for which all event mentions are annotated in the
corpus. The annotation of each mention includes the
instantiated arguments for the predicates, which rep-
resent the participants in the event, as well as general
attributes such as time and place. ACE guidelines
specify for each event type its possible arguments,
where all arguments are optional. Each argument is
associated with a semantic role and a list of possible
named-entity types. For instance, an Injure event
may have the arguments {Agent, Victim, Instrument,
Time, Place}, where Victim should be a person.
For each event type we manually created a small
set of template hypotheses that correspond to the
4http://projects.ldc.upenn.edu/ace/
given event predicate, and specified the appropri-
ate semantic roles for each variable. We consid-
ered only binary hypotheses, due to the type of
available entailment rules (see below). For In-
jure, the set of hypotheses included ?A injure V?
and ?injure V in T? where role(A)={Agent, In-
strument}, role(V)={Victim}, and role(T)={Time,
Place}. Thus, correct match of an argument corre-
sponds to correct role identification. The templates
were represented as Minipar (Lin, 1998b) depen-
dency parse-trees.
The Contextual Preferences for h were con-
structed manually: the named-entity types for
cpv:n(h) were set by adapting the entity types given
in the guidelines to the types supported by the Ling-
pipe NER (described in Section 3.2). cpg(h) was
generated from a short list of nouns and verbs that
were extracted from the verbal event definition in
the ACE guidelines. For Injure, this list included
{injure:v, injury:n, wound:v}. This assumes that
when writing down an event definition the user
would also specify such representative keywords.
Entailment-rules for a given h (rules in which
RHS is equal to h) were learned automatically by
the DIRT algorithm (Lin and Pantel, 2001), which
also produces a quality score for each rule. We im-
plemented a canonized version of DIRT (Szpektor
and Dagan, 2007) on the Reuters corpus parsed by
Minipar. Each rule?s arguments for cpv(r) were also
collected from this corpus.
We assessed the CP framework by its ability to
correctly rank, for each predicate (event), all the
candidate entailing mentions that are found for it
in the test corpus. Such ranking evaluation is suit-
able for unsupervised settings, with a perfect rank-
ing placing all correct mentions before any incor-
rect ones. The candidate mentions are found in the
parsed test corpus by matching the specified event
hypotheses, either directly or via the given set of en-
tailment rules, using a syntactic matcher similar to
the one in (Szpektor and Dagan, 2007). Finally, the
mentions are ranked by their match scores, as de-
scribed in Section 3.3. As detailed in the next sec-
tion, those candidate mentions which are also an-
notated as mentions of the same event in ACE are
considered correct.
The evaluation aims to assess the correctness of
inferring a target semantic meaning, which is de-
687
noted by a specific predicate. Therefore, we elim-
inated four ACE event types that correspond to mul-
tiple distinct predicates. For instance, the Transfer-
Money event refers to both donating and lending
money, which are not distinguished by the ACE an-
notation. We also omitted three events with less than
10 mentions and two events for which the given set
of learned rules could not match any mention. We
were left with 24 event types for evaluation, which
amount to 4085 event mentions in the dataset. Out of
these, our binary templates can correctly match only
mentions with at least two arguments, which appear
2076 times in the dataset.
Comparing with previous evaluation methodolo-
gies, in (Szpektor et al, 2007; Pantel et al, 2007)
proper context matching was evaluated by post-hoc
judgment of a sample of rule applications for a sam-
ple of rules. Such annotation needs to be repeated
each time the set of rules is changed. In addition,
since the corpus annotation is not exhaustive, re-
call could not be computed. By contrast, we use a
standard real-world dataset, in which all mentions
are annotated. This allows immediate comparison
of different rule sets and matching methods, without
requiring any additional (post-hoc) annotation.
5 Results and Analysis
We experimented with three rule setups over the
ACE dataset, in order to measure the contribution
of the CP framework. In the first setup no rules are
used, applying only direct matches of template hy-
potheses to identify event mentions. In the other two
setups we also utilized DIRT?s top 50 or 100 rules
for each hypothesis.
A match is considered correct when all matched
arguments are extracted correctly according to their
annotated event roles. This main measurement is de-
noted All. As an additional measurement, denoted
Any, we consider a match as correct if at least one
argument is extracted correctly.
Once event matches are extracted, we first mea-
sure for each event its Recall, the number of correct
mentions identified out of all annotated event men-
tions5 and Precision, the number of correct matches
out of all extracted candidate matches. These figures
5For Recall, we ignored mentions with less than two argu-
ments, as they cannot be correctly matched by binary templates.
quantify the baseline performance of the DIRT rule
set used. To assess our ranking quality, we measure
for each event the commonly used Average Preci-
sion (AP) measure (Voorhees and Harmann, 1998),
which is the area under the non-interpolated recall-
precision curve, while considering for each setup all
correct extracted matches as 100% Recall. Overall,
we report Mean Average Precision (MAP), macro
average Precision and macro average Recall over the
ACE events. Tables 1 and 2 summarize the main re-
sults of our experiments. As far as we know, these
are the first published unsupervised results for iden-
tifying event arguments in the ACE 2005 dataset.
Examining Recall, we see that it increases sub-
stantially when rules are applied: by more than
100% for the top 50 rules, and by about 150% for
the top 100, showing the benefit of entailment-rules
to covering language variability. The difference be-
tween All and Any results shows that about 65%
of the rules that correctly match one argument also
match correctly both arguments.
We use two baselines for measuring the CP rank-
ing contribution: Precision, which corresponds to
the expected MAP of random ranking, and MAP
of ranking using the prior rule score provided by
DIRT. Without rules, the baseline All Precision is
34.1%, showing that even the manually constructed
hypotheses, which correspond directly to the event
predicate, extract event mentions with limited accu-
racy when context is ignored. When rules are ap-
plied, Precision is very low. But ranking is consider-
ably improved using only the prior score (from 1.4%
to 22.7% for 50 rules), showing that the prior is an
informative indicator for valid matches.
Our main result is that the allCP and allCP+pr
methods rank matches statistically significantly bet-
ter than the baselines in all setups (according to the
Wilcoxon double-sided signed-ranks test at the level
of 0.01 (Wilcoxon, 1945)). In the All setup, ranking
is improved by 70% for direct matching (Table 1).
When entailment-rules are also utilized, prior-only
ranking is improved by about 35% and 50% when
using allCP and allCP+pr, respectively (Table 2).
Figure 2 presents the average Recall-Precision curve
of the ?50 Rules, All? setup for applying allCP or
allCP+pr, compared to prior-only ranking baseline
(other setups behave similarly). The improvement
in ranking is evident: the drop in precision is signif-
688
R P MAP (%)
(%) (%) cpv cpg allCP
All 14.0 34.1 46.5 52.2 60.2
Any 21.8 66.0 72.2 80.5 84.1
Table 1: Recall (R), Precision (P) and Mean Average Pre-
cision (MAP) when only matching template hypotheses
directly.
# R P MAP (%)
Rules (%) (%) prior allCP allCP+pr
All 50 29.6 1.4 22.7 30.6 34.1100 34.9 0.7 20.5 26.3 30.2
Any 50 46.5 3.5 41.2 43.7 48.6100 52.9 1.8 35.5 35.1 40.8
Table 2: Recall (R), Precision (P) and Mean Average Pre-
cision (MAP) when also using rules for matching.
icantly slower when CP is used. The behavior of CP
with and without the prior is largely the same up to
50% Recall, but later on our implemented CP mod-
els are noisier and should be combined with the prior
rule score.
Templates are incorrectly matched for several rea-
sons. First, there are context mismatches which are
not scored sufficiently low by our models. Another
main cause is incorrect learned rules in which LHS
and RHS are topically related, e.g. ?X convict Y ?
X arrest Y ?, or rules that are used in the wrong en-
tailment direction, e.g. ?X marry Y ?X divorce Y ?
(DIRT does not learn rule direction). As such rules
do correspond to plausible contexts of the hypothe-
sis, their matches obtain relatively high CP scores.
In addition, some incorrect matches are caused by
our syntactic matcher, which currently does not han-
dle certain phenomena such as co-reference, modal-
ity or negation, and due to Minipar parse errors.
5.1 Component Analysis
Table 3 displays the contribution of different CP
components to ranking, when adding only that com-
ponent?s match score to the baselines, and under ab-
lation tests, when using all CP component scores ex-
cept the tested component, with or without the prior.
As it turns out, matching h with t (i.e. cp(h, t),
which combines cpg(h, t) and cpv(h, t)) is most use-
ful. With our current models, using only cp(h, t)
along with the prior, while ignoring cp(r), achieves
50 Rules  -  All
0
10
20
30
40
50
60
70
80
90
100
0 10 20 30 40 50 60 70 80 90 100Relative Recall
Prec
ision
baseline CP CP + prior
Figure 2: Recall-Precision curves for ranking using: (a)
only the prior (baseline); (b) allCP; (c) allCP+pr.
the highest score in the table. The strong impact of
matching h and t?s preferences is also evident in Ta-
ble 1, where ranking based on either cpg or cpv sub-
stantially improves precision, while their combina-
tion provides the best ranking. These results indicate
that the two CP components capture complementary
information and both are needed to assess the cor-
rectness of a match.
When ignoring the prior rule score, cp(r, t) is the
major contributor over the baseline Precision. For
cpv(r, t), this is in synch with the result in (Pantel
et al, 2007), which is based on this single model
without utilizing prior rule scores. On the other
hand, cpv(r, t) does not improve the ranking when
the prior is used, suggesting that this contextual
model for the rule?s variables is not stronger than the
context-insensitive prior rule score. Furthermore,
relative to this cpv(r, t) model from (Pantel et al,
2007), our combined allCP model, with or without
the prior (first row of Table 2), obtains statistically
significantly better ranking (at the level of 0.01).
Comparing between the algorithms for match-
ing cpv:e (Section 3.2.2) we found that while
rankedCBC is statistically significantly better than
binaryCBC, rankedCBC and LIN generally
achieve the same results. When considering the
tradeoffs between the two, LIN is based on a much
simpler learning algorithm while CBC?s output is
more compact and allows faster CP matches.
689
Addition To Ablation From
P prior allCP allCP+pr
Baseline 1.4 22.7 30.6 34.1
cpg(h, t) ?10.4 ?35.4 32.4 33.7
cpv(h, t) ?11.0 29.9 27.6 32.9
cp(h, t) ?8.9 ?37.5 28.6 30.0
cpg(r, t) ?4.2 ?30.6 32.5 35.4
cpv(r, t) ?21.7 21.9 ?12.9 33.6
cp(r, t) ?26.0 ?29.6 ?17.9 36.8
cpg(h, r) ?8.1 22.4 31.9 34.3
cpv(h, r) ?10.7 22.7 ?27.9 34.4
cp(h, r) ?16.5 22.4 ?29.2 34.4
cpg(h, r, t) ?7.7 ?30.2 ?27.5 ?29.2
cpv(h, r, t) ?27.5 29.2 ?7.7 30.2
? Indicates statistically significant changes compared to the baseline,
according to the Wilcoxon test at the level of 0.01.
Table 3: MAP(%), under the ?50 rules, All? setup, when
adding component match scores to Precision (P) or prior-
only MAP baselines, and when ranking with allCP or
allCP+pr methods but ignoring that component scores.
Currently, some models do not improve the re-
sults when the prior is used. Yet, we would like to
further weaken the dependency on the prior score,
since it is biased towards frequent contexts. We
aim to properly identify also infrequent contexts (or
meanings) at inference time, which may be achieved
by better CP models. More generally, when used
on top of all other components, some of the mod-
els slightly degrade performance, as can be seen by
those figures in the ablation tests which are higher
than the corresponding baseline. However, due to
their different roles, each of the matching compo-
nents might capture some unique preferences. For
example, cp(h, r) should be useful to filter out rules
that don?t match the intended meaning of the given
h. Overall, this suggests that future research for bet-
ter models should aim to obtain a marginal improve-
ment by each component.
6 Related Work
Context sensitive inference was mainly investigated
in an application-dependent manner. For exam-
ple, (Harabagiu et al, 2003) describe techniques for
identifying the question focus and the answer type in
QA. (Patwardhan and Riloff, 2007) propose a super-
vised approach for IE, in which relevant text regions
for a target relation are identified prior to applying
extraction rules.
Recently, the need for context-aware inference
was raised (Szpektor et al, 2007). (Pantel et al,
2007) propose to learn the preferred instantiations of
rule variables, termed Inferential Selectional Prefer-
ences (ISP). Their clustering-based model is the one
we implemented for mv(r, t). A similar approach
is taken in (Pennacchiotti et al, 2007), where LSA
similarity is used to compare between the preferred
variable instantiations for a rule and their instanti-
ations in the matched text. (Downey et al, 2007)
use HMM-based similarity for the same purpose.
All these methods are analogous to matching cpv(r)
with cpv(t) in the CP framework.
(Dagan et al, 2006; Connor and Roth, 2007) pro-
posed generic approaches for identifying valid appli-
cations of lexical rules by classifying the surround-
ing global context of a word as valid or not for that
rule. These approaches are analogous to matching
cpg(r) with cpg(t) in our framework.
7 Conclusions
We presented the Contextual Preferences (CP)
framework for assessing the validity of inferences
in context. CP enriches the representation of tex-
tual objects with typical contextual information that
constrains or disambiguates their meaning, and pro-
vides matching functions that compare the prefer-
ences of objects involved in the inference. Experi-
ments with our implemented CP models, over real-
world IE data, show significant improvements rela-
tive to baselines and some previous work.
In future research we plan to investigate improved
models for representing and matching CP, and to ex-
tend the experiments to additional applied datasets.
We also plan to apply the framework to lexical infer-
ence rules, for which it seems directly applicable.
Acknowledgements
The authors would like to thank Alfio Massimiliano
Gliozzo for valuable discussions. This work was
partially supported by ISF grant 1095/05, the IST
Programme of the European Community under the
PASCAL Network of Excellence IST-2002-506778,
the NEGEV project (www.negev-initiative.org) and
the FBK-irst/Bar-Ilan University collaboration.
690
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI.
Michael Connor and Dan Roth. 2007. Context sensitive
paraphrasing with a global unsupervised classifier. In
Proceedings of the European Conference on Machine
Learning (ECML).
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein, and Carlo Strapparava. 2006. Direct word
sense matching for lexical substitution. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of ACL.
Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-
yakanok, Dan Roth, and Mark Sammons. 2005. An
inference model for semantic entailment in natural lan-
guage. In Proceedings of the National Conference on
Artificial Intelligence (AAAI).
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Doug Downey, Stefan Schoenmackers, and Oren Etzioni.
2007. Sparse information extraction: Unsupervised
language models to the rescue. In Proceedings of the
45th Annual Meeting of ACL.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
Bill Dolan. 2007. The third pascal recognizing tex-
tual entailment challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing.
Alfio Massimiliano Gliozzo. 2005. Semantic Domains
in Computational Linguistics. Ph.D. thesis. Advisor-
Carlo Strapparava.
Sanda M. Harabagiu, Steven J. Maiorano, and Marius A.
Pas?ca. 2003. Open-domain textual question answer-
ing techniques. Nat. Lang. Eng., 9(3):231?267.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. In Natural Lan-
guage Engineering, volume 7(4), pages 343?360.
Dekang Lin. 1998a. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Dekang Lin. 1998b. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalua-
tion of Parsing Systems at LREC.
Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada
Mihalcea, Roxana Girju, Richard Goodrum, and
Vasile Rus. 2000. The structure and performance of
an open-domain question answering system. In Pro-
ceedings of the 38th Annual Meeting of ACL.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Hu-
man Language Technologies 2007: The Conference of
NAACL; Proceedings of the Main Conference.
Patrick Andre Pantel. 2003. Clustering by committee.
Ph.D. thesis. Advisor-Dekang Lin.
Siddharth Patwardhan and Ellen Riloff. 2007. Effec-
tive information extraction with semantic affinity pat-
terns and relevant regions. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
Marco Pennacchiotti, Roberto Basili, Diego De Cao, and
Paolo Marocco. 2007. Learning selectional prefer-
ences for entailment or paraphrasing rules. In Pro-
ceedings of RANLP.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the 40th Annual Meeting of ACL.
Lorenza Romano, Milen Kouylekov, Idan Szpektor, Ido
Dagan, and Alberto Lavelli. 2006. Investigating a
generic paraphrase-based approach for relation extrac-
tion. In Proceedings of the 11th Conference of the
EACL.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Yusuke Shinyama, Satoshi Sekine, Sudo Kiyoshi, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of Human
Language Technology Conference.
Idan Szpektor and Ido Dagan. 2007. Learning canonical
forms of entailment rules. In Proceedings of RANLP.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP 2004,
pages 41?48, Barcelona, Spain.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acquisi-
tion. In Proceedings of the 45th Annual Meeting of
ACL.
Ellen M. Voorhees and Donna Harmann. 1998.
Overview of the seventh text retrieval conference
(trec?7). In The Seventh Text Retrieval Conference.
Frank Wilcoxon. 1945. Individual comparisons by rank-
ing methods. Biometrics Bulletin, 1(6):80?83.
691
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 791?799,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Source-Language Entailment Modeling for Translating Unknown Terms
Shachar Mirkin?, Lucia Specia?, Nicola Cancedda?, Ido Dagan?, Marc Dymetman?, Idan Szpektor?
? Computer Science Department, Bar-Ilan University
? Xerox Research Centre Europe
{mirkins,dagan,szpekti}@cs.biu.ac.il
{lucia.specia,nicola.cancedda,marc.dymetman}@xrce.xerox.com
Abstract
This paper addresses the task of handling
unknown terms in SMT. We propose us-
ing source-language monolingual models
and resources to paraphrase the source text
prior to translation. We further present a
conceptual extension to prior work by al-
lowing translations of entailed texts rather
than paraphrases only. A method for
performing this process efficiently is pre-
sented and applied to some 2500 sentences
with unknown terms. Our experiments
show that the proposed approach substan-
tially increases the number of properly
translated texts.
1 Introduction
Machine Translation systems frequently encounter
terms they are not able to translate due to some
missing knowledge. For instance, a Statistical Ma-
chine Translation (SMT) system translating the
sentence ?Cisco filed a lawsuit against Apple for
patent violation? may lack words like filed and
lawsuit in its phrase table. The problem is espe-
cially severe for languages for which parallel cor-
pora are scarce, or in the common scenario when
the SMT system is used to translate texts of a do-
main different from the one it was trained on.
A previously suggested solution (Callison-
Burch et al, 2006) is to learn paraphrases of
source terms from multilingual (parallel) corpora,
and expand the phrase table with these para-
phrases1. Such solutions could potentially yield a
paraphrased sentence like ?Cisco sued Apple for
patent violation?, although their dependence on
bilingual resources limits their utility.
In this paper we propose an approach that con-
sists in directly replacing unknown source terms,
1As common in the literature, we use the term para-
phrases to refer to texts of equivalent meaning, of any length
from single words (synonyms) up to complete sentences.
using source-language resources and models in or-
der to achieve two goals.
The first goal is coverage increase. The avail-
ability of bilingual corpora, from which para-
phrases can be learnt, is in many cases limited.
On the other hand, monolingual resources and
methods for extracting paraphrases from monolin-
gual corpora are more readily available. These
include manually constructed resources, such as
WordNet (Fellbaum, 1998), and automatic meth-
ods for paraphrases acquisition, such as DIRT (Lin
and Pantel, 2001). However, such resources have
not been applied yet to the problem of substitut-
ing unknown terms in SMT. We suggest that by
using such monolingual resources we could pro-
vide paraphrases for a larger number of texts with
unknown terms, thus increasing the overall cover-
age of the SMT system, i.e. the number of texts it
properly translates.
Even with larger paraphrase resources, we may
encounter texts in which not all unknown terms are
successfully handled through paraphrasing, which
often results in poor translations (see Section 2.1).
To further increase coverage, we therefore pro-
pose to generate and translate texts that convey a
somewhat more general meaning than the original
source text. For example, using such approach,
the following text could be generated: ?Cisco ac-
cused Apple of patent violation?. Although less in-
formative than the original, a translation for such
texts may be useful. Such non-symmetric relation-
ships (as between filed a lawsuit and accused) are
difficult to learn from parallel corpora and there-
fore monolingual resources are more appropriate
for this purpose.
The second goal we wish to accomplish by
employing source-language resources is to rank
the alternative generated texts. This goal can be
achieved by using context-models on the source
language prior to translation. This has two advan-
tages. First, the ranking allows us to prune some
791
candidates before supplying them to the transla-
tion engine, thus improving translation efficiency.
Second, the ranking may be combined with target
language information in order to choose the best
translation, thus improving translation quality.
We position the problem of generating alterna-
tive texts for translation within the Textual Entail-
ment (TE) framework (Giampiccolo et al, 2007).
TE provides a generic way for handling language
variability, identifying when the meaning of one
text is entailed by the other (i.e. the meaning of
the entailed text can be inferred from the mean-
ing of the entailing one). When the meanings of
two texts are equivalent (paraphrase), entailment
is mutual. Typically, a more general version of
a certain text is entailed by it. Hence, through TE
we can formalize the generation of both equivalent
and more general texts for the source text. When
possible, a paraphrase is used. Otherwise, an alter-
native text whose meaning is entailed by the orig-
inal source is generated and translated.
We assess our approach by applying an SMT
system to a text domain that is different from the
one used to train the system. We use WordNet
as a source language resource for entailment rela-
tionships and several common statistical context-
models for selecting the best generated texts to be
sent to translation. We show that the use of source
language resources, and in particular the extension
to non-symmetric textual entailment relationships,
is useful for substantially increasing the amount of
texts that are properly translated. This increase is
observed relative to both using paraphrases pro-
duced by the same resource (WordNet) and us-
ing paraphrases produced from multilingual paral-
lel corpora. We demonstrate that by using simple
context-models on the source, efficiency can be
improved, while translation quality is maintained.
We believe that with the use of more sophisticated
context-models further quality improvement can
be achieved.
2 Background
2.1 Unknown Terms
A very common problem faced by machine trans-
lation systems is the need to translate terms (words
or multi-word expressions) that are not found in
the system?s lexicon or phrase table. The reasons
for such unknown terms in SMT systems include
scarcity of training material and the application
of the system to text domains that differ from the
ones used for training.
In SMT, when unknown terms are found in the
source text, the systems usually omit or copy them
literally into the target. Though copying the source
words can be of some help to the reader if the
unknown word has a cognate in the target lan-
guage, this will not happen in the most general
scenario where, for instance, languages use dif-
ferent scripts. In addition, the presence of a sin-
gle unknown term often affects the translation of
wider portions of text, inducing errors in both lex-
ical selection and ordering. This phenomenon is
demonstrated in the following sentences, where
the translation of the English sentence (1) is ac-
ceptable only when the unknown word (in bold) is
replaced with a translatable paraphrase (3):
1. ?. . . , despite bearing the heavy burden of the
unemployed 10% or more of the labor force.?
2. ?. . . , malgre? la lourde charge de compte le
10% ou plus de cho?meurs labor la force .?
3. ?. . . , malgre? la lourde charge des cho?meurs
de 10% ou plus de la force du travail.?
Several approaches have been proposed to deal
with unknown terms in SMT systems, rather than
omitting or copying the terms. For example, (Eck
et al, 2008) replace the unknown terms in the
source text by their definition in a monolingual
dictionary, which can be useful for gisting. To
translate across languages with different alpha-
bets approaches such as (Knight and Graehl, 1997;
Habash, 2008) use transliteration techniques to
tackle proper nouns and technical terms. For trans-
lation from highly inflected languages, certain ap-
proaches rely on some form of lexical approx-
imation or morphological analysis (Koehn and
Knight, 2003; Yang and Kirchhoff, 2006; Langlais
and Patry, 2007; Arora et al, 2008). Although
these strategies yield gain in coverage and transla-
tion quality, they only account for unknown terms
that should be transliterated or are variations of
known ones.
2.2 Paraphrasing in MT
A recent strategy to broadly deal with the prob-
lem of unknown terms is to paraphrase the source
text with terms whose translation is known to
the system, using paraphrases learnt from multi-
lingual corpora, typically involving at least one
?pivot? language different from the target lan-
guage of immediate interest (Callison-Burch et
792
al., 2006; Cohn and Lapata, 2007; Zhao et al,
2008; Callison-Burch, 2008; Guzma?n and Gar-
rido, 2008). The procedure to extract paraphrases
in these approaches is similar to standard phrase
extraction in SMT systems, and therefore a large
amount of additional parallel corpus is required.
Moreover, as discussed in Section 5, when un-
known texts are not from the same domain as the
SMT training corpus, it is likely that paraphrases
found through such methods will yield misleading
translations.
Bond et al (2008) use grammars to paraphrase
the whole source sentence, covering aspects like
word order and minor lexical variations (tenses
etc.), but not content words. The paraphrases are
added to the source side of the corpus and the cor-
responding target sentences are duplicated. This,
however, may yield distorted probability estimates
in the phrase table, since these were not computed
from parallel data.
The main use of monolingual paraphrases in
MT to date has been for evaluation. For exam-
ple, (Kauchak and Barzilay, 2006) paraphrase ref-
erences to make them closer to the system transla-
tion in order to obtain more reliable results when
using automatic evaluation metrics like BLEU
(Papineni et al, 2002).
2.3 Textual Entailment and Entailment Rules
Textual Entailment (TE) has recently become a
prominent paradigm for modeling semantic infer-
ence, capturing the needs of a broad range of
text understanding applications (Giampiccolo et
al., 2007). Yet, its application to SMT has been so
far limited to MT evaluation (Pado et al, 2009).
TE defines a directional relation between two
texts, where the meaning of the entailed text (hy-
pothesis, h) can be inferred from the meaning of
the entailing text, t. Under this paradigm, para-
phrases are a special case of the entailment rela-
tion, when the relation is symmetric (the texts en-
tail each other). Otherwise, we say that one text
directionally entails the other.
A common practice for proving (or generating)
h from t is to apply entailment rules to t. An
entailment rule, denoted LHS ? RHS, specifies
an entailment relation between two text fragments
(the Left- and Right- Hand Sides), possibly with
variables (e.g. build X in Y ? X is completed
in Y ). A paraphrasing rule is denoted with ?.
When a rule is applied to a text, a new text is in-
ferred, where the matched LHS is replaced with the
RHS. For example, the rule skyscraper? building
is applied to ?The world?s tallest skyscraper was
completed in Taiwan? to infer ?The world?s tallest
building was completed in Taiwan?. In this work,
we employ lexical entailment rules, i.e. rules with-
out variables. Various resources for lexical rules
are available, and the prominent one is WordNet
(Fellbaum, 1998), which has been used in virtu-
ally all TE systems (Giampiccolo et al, 2007).
Typically, a rule application is valid only under
specific contexts. For example, mouse ? rodent
should not be applied to ?Use the mouse to mark
your answers?. Context-models can be exploited
to validate the application of a rule to a text. In
such models, an explicit Word Sense Disambigua-
tion (WSD) is not necessarily required; rather, an
implicit sense-match is sought after (Dagan et al,
2006). Within the scope of our paper, rule ap-
plication is handled similarly to Lexical Substitu-
tion (McCarthy and Navigli, 2007), considering
the contextual relationship between the text and
the rule. However, in general, entailment rule ap-
plication addresses other aspects of context match-
ing as well (Szpektor et al, 2008).
3 Textual Entailment for Statistical
Machine Translation
Previous solutions for handling unknown terms in
a source text s augment the SMT system?s phrase
table based on multilingual corpora. This allows
indirectly paraphrasing s, when the SMT system
chooses to use a paraphrase included in the table
and produces a translation with the corresponding
target phrase for the unknown term.
We propose using monolingual paraphrasing
methods and resources for this task to obtain a
more extensive set of rules for paraphrasing the
source. These rules are then applied to s directly
to produce alternative versions of the source text
prior to the translation step. Moreover, further
coverage increase can be achieved by employing
directional entailment rules, when paraphrasing is
not possible, to generate more general texts for
translation.
Our approach, based on the textual entailment
framework, considers the newly generated texts as
entailed from the original one. Monolingual se-
mantic resources such as WordNet can provide en-
tailment rules required for both these symmetric
and asymmetric entailment relations.
793
Input: A text t with one or more unknown terms;
a monolingual resource of entailment rules;
k - maximal number of source alternatives to produce
Output: A translation of either (in order of preference):
a paraphrase of t OR a text entailed by t OR t itself
1. For each unknown term - fetch entailment rules:
(a) Fetch rules for paraphrasing; disregard rules
whose RHS is not in the phrase table
(b) If the set of rules is empty: fetch directional en-
tailment rules; disregard rules whose RHS is not
in the phrase table
2. Apply a context-model to compute a score for each rule
application
3. Compute total source score for each entailed text as a
combination of individual rule scores
4. Generate and translate the top-k entailed texts
5. If k > 1
(a) Apply target model to score the translation
(b) Compute final source-target score
6. Pick highest scoring translation
Figure 1: Scheme for handling unknown terms by using
monolingual resources through textual entailment
Through the process of applying entailment
rules to the source text, multiple alternatives of
entailed texts are generated. To rank the candi-
date texts we employ monolingual context-models
to provide scores for rule applications over the
source sentence. This can be used to (a) directly
select the text with the highest score, which can
then be translated, or (b) to select a subset of top
candidates to be translated, which will then be
ranked using the target language information as
well. This pruning reduces the load of the SMT
system, and allows for potential improvements in
translation quality by considering both source- and
target-language information.
The general scheme through which we achieve
these goals, which can be implemented using dif-
ferent context-models and scoring techniques, is
detailed in Figure 1. Details of our concrete im-
plementation are given in Section 4.
Preliminary analysis confirmed (as expected)
that readers prefer translations of paraphrases,
when available, over translations of directional en-
tailments. This consideration is therefore taken
into account in the proposed method.
The input is a text unit to be translated, such as a
sentence or paragraph, with one or more unknown
terms. For each unknown term we first fetch a
list of candidate rules for paraphrasing (e.g. syn-
onyms), where the unknown term is the LHS. For
example, if our unknown term is dodge, a possi-
ble candidate might be dodge ? circumvent. We
inflect the RHS to keep the original morphologi-
cal information of the unknown term and filter out
rules where the inflected RHS does not appear in
the phrase table (step 1a in Figure 1).
When no applicable rules for paraphrasing are
available (1b), we fetch directional entailment
rules (e.g. hypernymy rules such as dodge ?
avoid), and filter them in the same way as for para-
phrasing rules. To each set of rules for a given un-
known term we add the ?identity-rule?, to allow
leaving the unknown term unchanged, the correct
choice in cases of proper names, for example.
Next, we apply a context-model to compute an
applicability score of each rule to the source text
(step 2). An entailed text?s total score is the com-
bination (e.g. product, see Section 4) of the scores
of the rules used to produce it (3). A set of the
top-k entailed texts is then generated and sent for
translation (4).
If more than one alternative is produced by the
source model (and k > 1), a target model is ap-
plied on the selected set of translated texts (5a).
The combined source-target model score is a com-
bination of the scores of the source and target
models (5b). The final translation is selected to be
the one that yields the highest combined source-
target score (6). Note that setting k = 1 is equiva-
lent to using the source-language model alone.
Our algorithm validates the application of the
entailment rules at two stages ? before and af-
ter translation, through context-models applied at
each end. As the experiments will show in Sec-
tion 4, a large number of possible combinations of
entailment rules is a common scenario, and there-
fore using the source context models to reduce this
number plays an important role.
4 Experimental Setting
To assess our approach, we conducted a series of
experiments; in each experiment we applied the
scheme described in 3, changing only the mod-
els being used for scoring the generated and trans-
lated texts. The setting of these experiments is de-
scribed in what follows.
SMT data To produce sentences for our experi-
ments, we use Matrax (Simard et al, 2005), a stan-
dard phrase-based SMT system, with the excep-
tion that it allows gaps in phrases. We use approxi-
mately 1M sentence pairs from the English-French
794
Europarl corpus for training, and then translate a
test set of 5,859 English sentences from the News
corpus into French. Both resources are taken
from the shared translation task in WMT-2008
(Callison-Burch et al, 2008). Hence, we compare
our method in a setting where the training and test
data are from different domains, a common sce-
nario in the practical use of MT systems.
Of the 5,859 translated sentences, 2,494 contain
unknown terms (considering only sequences with
alphabetic symbols), summing up to 4,255 occur-
rences of unknown terms. 39% of the 2,494 sen-
tences contain more than a single unknown term.
Entailment resource We use WordNet 3.0 as
a resource for entailment rules. Paraphrases are
generated using synonyms. Directionally entailed
texts are created using hypernyms, which typically
conform with entailment. We do not rely on sense
information in WordNet. Hence, any other seman-
tic resource for entailment rules can be utilized.
Each sentence is tagged using the OpenNLP
POS tagger2. Entailment rules are applied for un-
known terms tagged as nouns, verbs, adjectives
and adverbs. The use of relations from WordNet
results in 1,071 sentences with applicable rules
(with phrase table entries) for the unknown terms
when using synonyms, and 1,643 when using both
synonyms and hypernyms, accounting for 43%
and 66% of the test sentences, respectively.
The number of alternative sentences generated
for each source text varies from 1 to 960 when
paraphrasing rules were applied, and reaches very
large numbers, up to 89,700 at the ?worst case?,
when all TE rules are employed, an average of 456
alternatives per sentence.
Scoring source texts We test our proposed
method using several context-models shown to
perform reasonably well in previous work:
? FREQ: The first model we use is a context-
independent baseline. A common useful
heuristic to pick an entailment rule is to se-
lect the candidate with the highest frequency
in the corpus (Mccarthy et al, 2004). In this
model, a rule?s score is the normalized num-
ber of occurrences of its RHS in the training
corpus, ignoring the context of the LHS.
? LSA: Latent Semantic Analysis (Deerwester
et al, 1990) is a well-known method for rep-
2http://opennlp.sourceforge.net
resenting the contextual usage of words based
on corpus statistics. We represented each
term by a normalized vector of the top 100
SVD dimensions, as described in (Gliozzo,
2005). This model measures the similarity
between the sentence words and the RHS in
the LSA space.
? NB: We implemented the unsupervised
Na??ve Bayes model described in (Glickman
et al, 2006) to estimate the probability that
the unknown term entails the RHS in the
given context. The estimation is based on
corpus co-occurrence statistics of the context
words with the RHS.
? LMS: This model generates the Language
Model probability of the RHS in the source.
We use 3-grams probabilities as produced by
the SRILM toolkit (Stolcke, 2002).
Finally, as a simple baseline, we generated a ran-
dom score for each rule application, RAND.
The score of each rule application by any of
the above models is normalized to the range (0,1].
To combine individual rule applications in a given
sentence, we use the product of their scores. The
monolingual data used for the models above is the
source side of the training parallel corpus.
Target-language scores On the target side we
used either a standard 3-gram language-model, de-
noted LMT, or the score assigned by the com-
plete SMT log-linear model, which includes the
language model as one of its components (SMT).
A pair of a source:target models comprises a
complete model for selecting the best translated
sentence, where the overall score is the product of
the scores of the two models.
We also applied several combinations of source
models, such as LSA combined with LMS, to take
advantage of their complementary strengths. Ad-
ditionally, we assessed our method with source-
only models, by setting the number of sentences to
be selected by the source model to one (k = 1).
5 Results
5.1 Manual Evaluation
To evaluate the translations produced using the
various source and target models and the different
rule-sets, we rely mostly on manual assessment,
since automatic MT evaluation metrics like BLEU
do not capture well the type of semantic variations
795
Model
Precision (%) Coverage (%)
PARAPH. TE PARAPH. TE
1 ?:SMT 75.8 73.1 32.5 48.1
2 NB:SMT 75.2 71.5 32.3 47.1
3 LSA:SMT 74.9 72.4 32.1 47.7
4 NB:? 74.7 71.1 32.1 46.8
5 LMS:LMT 73.8 70.2 31.7 46.3
6 FREQ:? 72.5 68.0 31.2 44.8
7 RAND 57.2 63.4 24.6 41.8
Table 1: Translation acceptance when using only para-
phrases and when using all entailment rules. ?:? indicates
which model is applied to the source (left side) and which to
the target language (right side).
generated in our experiments, particularly at the
sentence level.
In the manual evaluation, two native speakers
of the target language judged whether each trans-
lation preserves the meaning of its reference sen-
tence, marking it as acceptable or unacceptable.
From the sentences for which rules were applica-
ble, we randomly selected a sample of sentences
for each annotator, allowing for some overlap-
ping for agreement analysis. In total, the transla-
tions of 1,014 unique source sentences were man-
ually annotated, of which 453 were produced us-
ing only hypernyms (no paraphrases were appli-
cable). When a sentence was annotated by both
annotators, one annotation was picked randomly.
Inter-annotator agreement was measured by the
percentage of sentences the annotators agreed on,
as well as via the Kappa measure (Cohen, 1960).
For different models, the agreement rate varied
from 67% to 78% (72% overall), and the Kappa
value ranged from 0.34 to 0.55, which is compa-
rable to figures reported for other standard SMT
evaluation metrics (Callison-Burch et al, 2008).
Translation with TE For each model m, we
measured Precisionm, the percentage of accept-
able translations out of all sampled translations.
Precisionm was measured both when using only
paraphrases (PARAPH.) and when using all entail-
ment rules (TE). We also measured Coveragem,
the percentage of sentences with acceptable trans-
lations, Am, out of all sentences (2,494). As
our annotators evaluated only a sample of sen-
tences, Am is estimated as the model?s total num-
ber of sentences with applicable rules, Sm, mul-
tiplied by the model?s Precision (Sm was 1,071
for paraphrases and 1,643 for entailment rules):
Coveragem = Sm?Precisionm2,494 .
Table 1 presents the results of several source-
target combinations when using only paraphrases
and when also using directional entailment rules.
When all rules are used, a substantial improve-
ment in coverage is consistently obtained across
all models, reaching a relative increase of 50%
over paraphrases only, while just a slight decrease
in precision is observed (see Section 5.3 for some
error analysis). This confirms our hypothesis that
directional entailment rules can be very useful for
replacing unknown terms.
For the combination of source-target models,
the value of k is set depending on which rule-set
is used. Preliminary analysis showed that k = 5
is sufficient when only paraphrases are used and
k = 20 when directional entailment rules are also
considered.
We measured statistical significance between
different models for precision of the TE re-
sults according to the Wilcoxon signed ranks test
(Wilcoxon, 1945). Models 1-6 in Table 1 are sig-
nificantly better than the RAND baseline (p <
0.03), and models 1-3 are significantly better than
model 6 (p < 0.05). The difference between
?:SMT and NB:SMT or LSA:SMT is not statisti-
cally significant.
The results in Table 1 therefore suggest that
taking a source model into account preserves the
quality of translation. Furthermore, the quality is
maintained even when source models? selections
are restricted to a rather small top-k ranks, at a
lower computational cost (for the models combin-
ing source and target, like NB:SMT or LSA:SMT).
This is particularly relevant for on-demand MT
systems, where time is an issue. For such systems,
using this source-language based pruning method-
ology will yield significant performance gains as
compared to target-only models.
We also evaluated the baseline strategy where
unknown terms are omitted from the translation,
resulting in 25% precision. Leaving unknown
words untranslated also yielded very poor transla-
tion quality in an analysis performed on a similar
dataset.
Comparison to related work We compared our
algorithm with an implementation of the algo-
rithm proposed by (Callison-Burch et al, 2006)
(see Section 2.2), henceforth CB, using the Span-
ish side of Europarl as the pivot language.
Out of the tested 2,494 sentences with unknown
terms, CB found paraphrases for 706 sentences
(28.3%), while with any of our models, including
796
Model Precision (%) Coverage (%) Better (%)
NB:SMT (TE) 85.3 56.2 72.7
CB 85.3 24.2 12.7
Table 2: Comparison between our top model and the
method by Callison-Burch et al (2006), showing the per-
centage of times translations were considered acceptable, the
model?s coverage and the percentage of times each model
scored better than the other (in the 14% remaining cases, both
models produced unacceptable translations).
NB:SMT , our algorithm found applicable entail-
ment rules for 1,643 sentences (66%).
The quality of the CB translations was manually
assessed for a sample of 150 sentences. Table 2
presents the precision and coverage on this sample
for both CB and NB:SMT , as well as the number
of times each model?s translation was preferred by
the annotators. While both models achieve equally
high precision scores on this sample, the NB:SMT
model?s translations were undoubtedly preferred
by the annotators, with a considerably higher cov-
erage.
With the CB method, given that many of the
phrases added to the phrase table are noisy, the
global quality of the sentences seem to have been
affected, explaining why the judges preferred the
NB:SMT translations. One reason for the lower
coverage of CB is the fact that paraphrases were
acquired from a corpus whose domain is differ-
ent from that of the test sentences. The entail-
ment rules in our models are not limited to para-
phrases and are derived from WordNet, which has
broader applicability. Hence, utilizing monolin-
gual resources has proven beneficial for the task.
5.2 Automatic MT Evaluation
Although automatic MT evaluation metrics are
less appropriate for capturing the variations gen-
erated by our method, to ensure that there was no
degradation in the system-level scores according
to such metrics we also measured the models? per-
formance using BLEU and METEOR (Agarwal
and Lavie, 2007). The version of METEOR we
used on the target language (French) considers the
stems of the words, instead of surface forms only,
but does not make use of WordNet synonyms.
We evaluated the performance of the top mod-
els of Table 1, as well as of a baseline SMT sys-
tem that left unknown terms untranslated, on the
sample of 1,014 manually annotated sentences. As
shown in Table 3, all models resulted in improve-
ment with respect to the original sentences (base-
Model BLEU (TE) METEOR (TE)
?:SMT 15.50 0.1325
NB:SMT 15.37 0.1316
LSA:SMT 15.51 0.1318
NB:? 15.37 0.1311
CB 15.33 0.1299
Baseline SMT 15.29 0.1294
Table 3: Performance of the best models according to auto-
matic MT evaluation metrics at the corpus level. The baseline
refers to translation of the text without applying any entail-
ment rules.
line). The difference in METEOR scores is statis-
tically significant (p < 0.05) for the three top mod-
els against the baseline. The generally low scores
may be attributed to the fact that training and test
sentences are from different domains.
5.3 Discussion
The use of entailed texts produced using our ap-
proach clearly improves the quality of translations,
as compared to leaving unknown terms untrans-
lated or omitting them altogether. While it is clear
that textual entailment is useful for increasing cov-
erage in translation, further research is required to
identify the amount of information loss incurred
when non-symmetric entailment relations are be-
ing used, and thus to identify the cases where such
relations are detrimental to translation.
Consider, for example, the sentence: ?Conven-
tional military models are geared to decapitate
something that, in this case, has no head.?. In this
sentence, the unknown term was replaced by kill,
which results in missing the point originally con-
veyed in the text. Accordingly, the produced trans-
lation does not preserve the meaning of the source,
and was considered unacceptable: ?Les mode`les
militaires visent a` faire quelque chose que, dans
ce cas, n?est pas responsable.?.
In other cases, the selected hypernyms were too
generic words, such as entity or attribute, which
also fail to preserve the sentence?s meaning. On
the other hand, when the unknown term was a
very specific word, hypernyms played an impor-
tant role. For example, ?Bulgaria is the most
sought-after east European real estate target, with
its low-cost ski chalets and oceanfront homes?.
Here, chalets are replaced by houses or units (de-
pending on the model), providing a translation that
would be acceptable by most readers.
Other incorrect translations occurred when the
unknown term was part of a phrase, for exam-
ple, troughs replaced with depressions in peaks
797
and troughs, a problem that also strongly affects
paraphrasing. In another case, movement was the
hypernym chosen to replace labor in labor move-
ment, yielding an awkward text for translation.
Many of the cases which involved ambiguity
were resolved by the applied context-models, and
can be further addressed, together with the above
mentioned problems, with better source-language
context models.
We suggest that other types of entailment rules
could be useful for the task beyond the straight-
forward generalization using hypernyms, which
was demonstrated in this work. This includes
other types of lexical entailment relations, such as
holonymy (e.g. Singapore ? Southeast Asia) as
well as lexical syntactic rules (X cure Y ? treat
Y with X). Even syntactic rules, such as clause re-
moval, can be recruited for the task: ?Obama, the
44th president, declared Monday . . . ?? ?Obama
declared Monday . . . ?. When the system is un-
able to translate a term found in the embedded
clause, the translation of the less informative sen-
tence may still be acceptable by readers.
6 Conclusions and Future Work
In this paper we propose a new entailment-based
approach for addressing the problem of unknown
terms in machine translation. Applying this ap-
proach with lexical entailment rules from Word-
Net, we show that using monolingual resources
and textual entailment relationships allows sub-
stantially increasing the quality of translations
produced by an SMT system. Our experiments
also show that it is possible to perform the process
efficiently by relying on source language context-
models as a filter prior to translation. This pipeline
maintains translation quality, as assessed by both
human annotators and standard automatic mea-
sures.
For future work we suggest generating entailed
texts with a more extensive set of rules, in particu-
lar lexical-syntactic ones. Combining rules from
monolingual and bilingual resources seems ap-
pealing as well. Developing better context-models
to be applied on the source is expected to further
improve our method?s performance. Specifically,
we suggest taking into account the prior likelihood
that a rule is correct as part of the model score.
Finally, some researchers have advocated re-
cently the use of shared structures such as parse
forests (Mi and Huang, 2008) or word lattices
(Dyer et al, 2008) in order to allow a compact rep-
resentation of alternative inputs to an SMT system.
This is an approach that we intend to explore in
future work, as a way to efficiently handle the dif-
ferent source language alternatives generated by
entailment rules. However, since most current MT
systems do not accept such type of inputs, we con-
sider the results on pruning by source-side context
models as broadly relevant.
Acknowledgments
This work was supported in part by the ICT Pro-
gramme of the European Community, under the
PASCAL 2 Network of Excellence, ICT-216886
and The Israel Science Foundation (grant No.
1112/08). We wish to thank Roy Bar-Haim and
the anonymous reviewers of this paper for their
useful feedback. This publication only reflects the
authors? views.
References
Abhaya Agarwal and Alon Lavie. 2007. METEOR:
An Automatic Metric for MT Evaluation with High
Levels of Correlation with Human Judgments. In
Proceedings of WMT-08.
Karunesh Arora, Michael Paul, and Eiichiro Sumita.
2008. Translation of Unknown Words in Phrase-
Based Statistical Machine Translation for Lan-
guages of Rich Morphology. In Proceedings of
SLTU.
Francis Bond, Eric Nichols, Darren Scott Appling, and
Michael Paul. 2008. Improving Statistical Machine
Translation by Paraphrasing the Training Data. In
Proceedings of IWSLT.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Trans-
lation Using Paraphrases. In Proceedings of HLT-
NAACL.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further Meta-Evaluation of Machine Translation. In
Proceedings of WMT.
Chris Callison-Burch. 2008. Syntactic Constraints
on Paraphrases Extracted from Parallel Corpora. In
Proceedings of EMNLP.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological
Measurement, 20(1):37?46.
Trevor Cohn and Mirella Lapata. 2007. Machine
Translation by Triangulation: Making Effective Use
of Multi-Parallel Corpora. In Proceedings of ACL.
798
Ido Dagan, Oren Glickman, Alfio Massimiliano
Gliozzo, Efrat Marmorshtein, and Carlo Strappar-
ava. 2006. Direct Word Sense Matching for Lexical
Substitution. In Proceedings of ACL.
Scott Deerwester, S.T. Dumais, G.W. Furnas, T.K. Lan-
dauer, and R.A. Harshman. 1990. Indexing by La-
tent Semantic Analysis. Journal of the American So-
ciety for Information Science, 41.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing Word Lattice Trans-
lation. In Proceedings of ACL-HLT.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2008.
Communicating Unknown Words in Machine Trans-
lation. In Proceedings of LREC.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The Third PASCAL Recog-
nising Textual Entailment Challenge. In Proceed-
ings of ACL-WTEP Workshop.
Oren Glickman, Ido Dagan, Mikaela Keller, Samy
Bengio, and Walter Daelemans. 2006. Investigat-
ing Lexical Substitution Scoring for Subtitle Gener-
ation. In Proceedings of CoNLL.
Alfio Massimiliano Gliozzo. 2005. Semantic Domains
in Computational Linguistics. Ph.D. thesis, Univer-
sity of Trento.
Francisco Guzma?n and Leonardo Garrido. 2008.
Translation Paraphrases in Phrase-Based Machine
Translation. In Proceedings of CICLing.
Nizar Habash. 2008. Four Techniques for Online
Handling of Out-of-Vocabulary Words in Arabic-
English Statistical Machine Translation. In Pro-
ceedings of ACL-HLT.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of
HLT-NAACL.
Kevin Knight and Jonathan Graehl. 1997. Machine
Transliteration. In Proceedings of ACL.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings
of EACL.
Philippe Langlais and Alexandre Patry. 2007. Trans-
lating Unknown Words by Analogical Learning. In
Proceedings of EMNLP-CoNLL.
Dekang Lin and Patrick Pantel. 2001. DIRT ? Discov-
ery of Inference Rules from Text. In Proceedings of
SIGKDD.
Diana McCarthy and Roberto Navigli. 2007.
SemEval-2007 Task 10: English Lexical Substitu-
tion Task. In Proceedings of SemEval.
Diana Mccarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding Predominant Word Senses
in Untagged Text. In Proceedings of ACL.
Haitao Mi and Liang Huang. 2008. Forest-based
Translation Rule Extraction. In Proceedings of
EMNLP.
Sebastian Pado, Michel Galley, Daniel Jurafsky, and
Christopher D. Manning. 2009. Textual Entail-
ment Features for Machine Translation Evaluation.
In Proceedings of WMT.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of ACL.
M. Simard, N. Cancedda, B. Cavestro, M. Dymet-
man, E. Gaussier, C. Goutte, and K. Yamada. 2005.
Translating with Non-contiguous Phrases. In Pro-
ceedings of HLT-EMNLP.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proceedings of ICSLP.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual Preferences. In Pro-
ceedings of ACL-HLT.
Frank Wilcoxon. 1945. Individual Comparisons by
Ranking Methods. Biometrics Bulletin, 1(6):80?83.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-Based
Backoff Models for Machine Translation of Highly
Inflected Languages. In Proceedings of EACL.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot Approach for Extracting Paraphrase
Patterns from Bilingual Corpora. In Proceedings of
ACL-HLT.
799
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 69?72,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Directional Distributional Similarity for Lexical Expansion
Lili Kotlerman, Ido Dagan, Idan Szpektor
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
lili.dav@gmail.com
{dagan,szpekti}@cs.biu.ac.il
Maayan Zhitomirsky-Geffet
Department of Information Science
Bar-Ilan University
Ramat Gan, Israel
maayan.geffet@gmail.com
Abstract
Distributional word similarity is most
commonly perceived as a symmetric re-
lation. Yet, one of its major applications
is lexical expansion, which is generally
asymmetric. This paper investigates the
nature of directional (asymmetric) similar-
ity measures, which aim to quantify distri-
butional feature inclusion. We identify de-
sired properties of such measures, specify
a particular one based on averaged preci-
sion, and demonstrate the empirical bene-
fit of directional measures for expansion.
1 Introduction
Much work on automatic identification of seman-
tically similar terms exploits Distributional Simi-
larity, assuming that such terms appear in similar
contexts. This has been now an active research
area for a couple of decades (Hindle, 1990; Lin,
1998; Weeds and Weir, 2003).
This paper is motivated by one of the prominent
applications of distributional similarity, namely
identifying lexical expansions. Lexical expansion
looks for terms whose meaning implies that of a
given target term, such as a query. It is widely
employed to overcome lexical variability in ap-
plications like Information Retrieval (IR), Infor-
mation Extraction (IE) and Question Answering
(QA). Often, distributional similarity measures are
used to identify expanding terms (e.g. (Xu and
Croft, 1996; Mandala et al, 1999)). Here we de-
note the relation between an expanding term u and
an expanded term v as ?u ? v?.
While distributional similarity is most promi-
nently modeled by symmetric measures, lexical
expansion is in general a directional relation. In
IR, for instance, a user looking for ?baby food?
will be satisfied with documents about ?baby pap?
or ?baby juice? (?pap ? food?, ?juice ? food?);
but when looking for ?frozen juice? she will not
be satisfied by ?frozen food?. More generally, di-
rectional relations are abundant in NLP settings,
making symmetric similarity measures less suit-
able for their identification.
Despite the need for directional similarity mea-
sures, their investigation counts, to the best of
our knowledge, only few works (Weeds and Weir,
2003; Geffet and Dagan, 2005; Bhagat et al,
2007; Szpektor and Dagan, 2008; Michelbacher et
al., 2007) and is utterly lacking. From an expan-
sion perspective, the common expectation is that
the context features characterizing an expanding
word should be largely included in those of the ex-
panded word.
This paper investigates the nature of directional
similarity measures. We identify their desired
properties, design a novel measure based on these
properties, and demonstrate its empirical advan-
tage in expansion settings over state-of-the-art
measures
1
. In broader prospect, we suggest that
asymmetric measures might be more suitable than
symmetric ones for many other settings as well.
2 Background
The distributional word similarity scheme follows
two steps. First, a feature vector is constructed
for each word by collecting context words as fea-
tures. Each feature is assigned a weight indicating
its ?relevance? (or association) to the given word.
Then, word vectors are compared by some vector
similarity measure.
1
Our directional term-similarity resource will be available
at http://aclweb.org/aclwiki/index.php?
title=Textual_Entailment_Resource_Pool
69
To date, most distributional similarity research
concentrated on symmetric measures, such as the
widely cited and competitive (as shown in (Weeds
and Weir, 2003)) LIN measure (Lin, 1998):
LIN(u, v) =
?
f?FV
u
?FV
v
[w
u
(f) + w
v
(f)]
?
f?FV
u
w
u
(f) +
?
f?FV
v
w
v
(f)
where FV
x
is the feature vector of a word x and
w
x
(f) is the weight of the feature f in that word?s
vector, set to their pointwise mutual information.
Few works investigated a directional similarity
approach. Weeds and Weir (2003) and Weeds et
al. (2004) proposed a precision measure, denoted
here WeedsPrec, for identifying the hyponymy re-
lation and other generalization/specification cases.
It quantifies the weighted coverage (or inclusion)
of the candidate hyponym?s features (u) by the hy-
pernym?s (v) features:
WeedsPrec(u ? v) =
?
f?FV
u
?FV
v
w
u
(f)
?
f?FV
u
w
u
(f)
The assumption behind WeedsPrec is that if one
word is indeed a generalization of the other then
the features of the more specific word are likely to
be included in those of the more general one (but
not necessarily vice versa).
Extending this rationale to the textual entail-
ment setting, Geffet and Dagan (2005) expected
that if the meaning of a word u entails that of
v then all its prominent context features (under
a certain notion of ?prominence?) would be in-
cluded in the feature vector of v as well. Their
experiments indeed revealed a strong empirical
correlation between such complete inclusion of
prominent features and lexical entailment, based
on web data. Yet, such complete inclusion cannot
be feasibly assessed using an off-line corpus, due
to the huge amount of required data.
Recently, (Szpektor and Dagan, 2008) tried
identifying the entailment relation between
lexical-syntactic templates using WeedsPrec, but
observed that it tends to promote unreliable rela-
tions involving infrequent templates. To remedy
this, they proposed to balance the directional
WeedsPrec measure by multiplying it with the
symmetric LIN measure, denoted here balPrec:
balPrec(u?v)=
?
LIN(u, v)?WeedsPrec(u?v)
Effectively, this measure penalizes infrequent tem-
plates having short feature vectors, as those usu-
ally yield low symmetric similarity with the longer
vectors of more common templates.
3 A Statistical Inclusion Measure
Our research goal was to develop a directional
similarity measure suitable for learning asymmet-
ric relations, focusing empirically on lexical ex-
pansion. Thus, we aimed to quantify most effec-
tively the above notion of feature inclusion.
For a candidate pair ?u ? v?, we will refer to
the set of u?s features, which are those tested for
inclusion, as tested features. Amongst these fea-
tures, those found in v?s feature vector are termed
included features.
In preliminary data analysis of pairs of feature
vectors, which correspond to a known set of valid
and invalid expansions, we identified the follow-
ing desired properties for a distributional inclusion
measure. Such measure should reflect:
1. the proportion of included features amongst
the tested ones (the core inclusion idea).
2. the relevance of included features to the ex-
panding word.
3. the relevance of included features to the ex-
panded word.
4. that inclusion detection is less reliable if the
number of features of either expanding or ex-
panded word is small.
3.1 Average Precision as the Basis for an
Inclusion Measure
As our starting point we adapted the Average
Precision (AP) metric, commonly used to score
ranked lists such as query search results. This
measure combines precision, relevance ranking
and overall recall (Voorhees and Harman, 1999):
AP =
?
N
r=1
[P (r) ? rel(r)]
total number of relevant documents
where r is the rank of a retrieved document
amongst the N retrieved, rel(r) is an indicator
function for the relevance of that document, and
P (r) is precision at the given cut-off rank r.
In our case the feature vector of the expanded
word is analogous to the set of all relevant docu-
ments while tested features correspond to retrieved
documents. Included features thus correspond to
relevant retrieved documents, yielding the follow-
70
ing analogous measure in our terminology:
AP (u ? v) =
?
|FV
u
|
r=1
[P (r) ? rel(f
r
)]
|FV
v
|
rel(f) =
{
1, if f ? FV
v
0, if f /? FV
v
P (r) =
|included features in ranks 1 to r|
r
where f
r
is the feature at rank r in FV
u
.
This analogy yields a feature inclusion measure
that partly addresses the above desired properties.
Its score increases with a larger number of in-
cluded features (correlating with the 1
st
property),
while giving higher weight to highly ranked fea-
tures of the expanding word (2
nd
property).
To better meet the desired properties we in-
troduce two modifications to the above measure.
First, we use the number of tested features |FV
u
|
for normalization instead of |FV
v
|. This captures
better the notion of feature inclusion (1
st
property),
which targets the proportion of included features
relative to the tested ones.
Second, in the classical AP formula all relevant
documents are considered relevant to the same ex-
tent. However, features of the expanded word dif-
fer in their relevance within its vector (3
rd
prop-
erty). We thus reformulate rel(f) to give higher
relevance to highly ranked features in |FV
v
|:
rel
?
(f) =
{
1 ?
rank(f,FV
v
)
|FV
v
|+1
, if f ? FV
v
0 , if f /? FV
v
where rank(f, FV
v
) is the rank of f in FV
v
.
Incorporating these twomodifications yields the
APinc measure:
APinc(u?v)=
?
|FV
u
|
r=1
[P (r) ? rel
?
(f
r
)]
|FV
u
|
Finally, we adopt the balancing approach in
(Szpektor and Dagan, 2008), which, as explained
in Section 2, penalizes similarity for infrequent
words having fewer features (4
th
property) (in our
version, we truncated LIN similarity lists after top
1000 words). This yields our proposed directional
measure balAPinc:
balAPinc(u?v) =
?
LIN(u, v) ? APinc(u?v)
4 Evaluation and Results
4.1 Evaluation Setting
We tested our similarity measure by evaluating its
utility for lexical expansion, compared with base-
lines of the LIN, WeedsPrec and balPrec measures
(Section 2) and a balanced version of AP (Sec-
tion 3), denoted balAP. Feature vectors were cre-
ated by parsing the Reuters RCV1 corpus and tak-
ing the words related to each term through a de-
pendency relation as its features (coupled with the
relation name and direction, as in (Lin, 1998)). We
considered for expansion only terms that occur at
least 10 times in the corpus, and as features only
terms that occur at least twice.
As a typical lexical expansion task we used
the ACE 2005 events dataset
2
. This standard IE
dataset specifies 33 event types, such as Attack,
Divorce, and Law Suit, with all event mentions
annotated in the corpus. For our lexical expan-
sion evaluation we considered the first IE subtask
of finding sentences that mention the event.
For each event we specified a set of representa-
tive words (seeds), by selecting typical terms for
the event (4 on average) from its ACE definition.
Next, for each similarity measure, the terms found
similar to any of the event?s seeds (?u ? seed?)
were taken as expansion terms. Finally, to mea-
sure the sole contribution of expansion, we re-
moved from the corpus all sentences that contain
a seed word and then extracted all sentences that
contain expansion terms as mentioning the event.
Each of these sentences was scored by the sum of
similarity scores of its expansion terms.
To evaluate expansion quality we compared the
ranked list of sentences for each event to the gold-
standard annotation of event mentions, using the
standard Average Precision (AP) evaluation mea-
sure. We report Mean Average Precision (MAP)
for all events whose AP value is at least 0.1 for at
least one of the tested measures
3
.
4.1.1 Results
Table 1 presents the results for the different tested
measures over the ACE experiment. It shows that
the symmetric LIN measure performs significantly
worse than the directional measures, assessing that
a directional approach is more suitable for the ex-
pansion task. In addition, balanced measures con-
sistently perform better than unbalanced ones.
According to the results, balAPinc is the best-
performing measure. Its improvement over all
other measures is statistically significant accord-
ing to the two-sided Wilcoxon signed-rank test
2
http://projects.ldc.upenn.edu/ace/, training part.
3
The remaining events seemed useless for our compar-
ative evaluation, since suitable expansion lists could not be
found for them by any of the distributional methods.
71
LIN WeedsPrec balPrec AP balAP balAPinc
0.068 0.044 0.237 0.089 0.202 0.312
Table 1: MAP scores of the tested measures on the
ACE experiment.
seed LIN balAPinc
death murder, killing, inci-
dent, arrest, violence
suicide, killing, fatal-
ity, murder, mortality
marry divorce, murder, love, divorce, remarry,
dress, abduct father, kiss, care for
arrest detain, sentence,
charge, jail, convict
detain, extradite,
round up, apprehend,
imprison
birth abortion, pregnancy, wedding day,
resumption, seizure, dilation, birthdate,
passage circumcision, triplet
injure wound, kill, shoot, wound, maim, beat
detain, burn up, stab, gun down
Table 2: Top 5 expansion terms learned by LIN
and balAPinc for a sample of ACE seed words.
(Wilcoxon, 1945) at the 0.01 level. Table 2
presents a sample of the top expansion terms
learned for some ACE seeds with either LIN or
balAPinc, demonstrating the more accurate ex-
pansions generated by balAPinc. These results
support the design of our measure, based on the
desired properties that emerged from preliminary
data analysis for lexical expansion.
Finally, we note that in related experiments we
observed statistically significant advantages of the
balAPincmeasure for an unsupervised text catego-
rization task (on the 10 most frequent categories in
the Reuters-21578 collection). In this setting, cat-
egory names were taken as seeds and expanded by
distributional similarity, further measuring cosine
similarity with categorized documents similarly to
IR query expansion. These experiments fall be-
yond the scope of this paper and will be included
in a later and broader description of our work.
5 Conclusions and Future work
This paper advocates the use of directional similar-
ity measures for lexical expansion, and potentially
for other tasks, based on distributional inclusion of
feature vectors. We first identified desired proper-
ties for an inclusion measure and accordingly de-
signed a novel directional measure based on av-
eraged precision. This measure yielded the best
performance in our evaluations. More generally,
the evaluations supported the advantage of multi-
ple directional measures over the typical symmet-
ric LIN measure.
Error analysis showed that many false sentence
extractions were caused by ambiguous expanding
and expanded words. In future work we plan to
apply disambiguation techniques to address this
problem. We also plan to evaluate the performance
of directional measures in additional tasks, and
compare it with additional symmetric measures.
Acknowledgements
This work was partially supported by the NEGEV
project (www.negev-initiative.org), the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886 and by the Israel
Science Foundation grant 1112/08.
References
R. Bhagat, P. Pantel, and E. Hovy. 2007. LEDIR: An
unsupervised algorithm for learning directionality of
inference rules. In Proceedings of EMNLP-CoNLL.
M. Geffet and I. Dagan. 2005. The distributional in-
clusion hypotheses and lexical entailment. In Pro-
ceedings of ACL.
D. Hindle. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL.
R. Mandala, T. Tokunaga, and H. Tanaka. 1999. Com-
bining multiple evidence from different types of the-
saurus for query expansion. In Proceedings of SI-
GIR.
L. Michelbacher, S. Evert, and H. Schutze. 2007.
Asymmetric association measures. In Proceedings
of RANLP.
I. Szpektor and I. Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of COL-
ING.
E. M. Voorhees and D. K. Harman, editors. 1999. The
Seventh Text REtrieval Conference (TREC-7), vol-
ume 7. NIST.
J. Weeds and D. Weir. 2003. A general framework for
distributional similarity. In Proceedings of EMNLP.
J. Weeds, D. Weir, and D. McCarthy. 2004. Character-
ising measures of lexical distributional similarity. In
Proceedings of COLING.
F. Wilcoxon. 1945. Individual comparisons by ranking
methods. Biometrics Bulletin, 1:80?83.
J. Xu and W. B. Croft. 1996. Query expansion using
local and global document analysis. In Proceedings
of SIGIR.
72
Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 55?60,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Definition and Analysis of Intermediate Entailment Levels
Roy Bar-Haim, Idan Szpektor, Oren Glickman
Computer Science Department
Bar Ilan University
Ramat-Gan 52900, Israel
{barhair,szpekti,glikmao}@cs.biu.ac.il
Abstract
In this paper we define two intermediate
models of textual entailment, which corre-
spond to lexical and lexical-syntactic lev-
els of representation. We manually anno-
tated a sample from the RTE dataset ac-
cording to each model, compared the out-
come for the two models, and explored
how well they approximate the notion of
entailment. We show that the lexical-
syntactic model outperforms the lexical
model, mainly due to a much lower rate
of false-positives, but both models fail to
achieve high recall. Our analysis also
shows that paraphrases stand out as a
dominant contributor to the entailment
task. We suggest that our models and an-
notation methods can serve as an evalua-
tion scheme for entailment at these levels.
1 Introduction
Textual entailment has been proposed recently as
a generic framework for modeling semantic vari-
ability in many Natural Language Processing ap-
plications, such as Question Answering, Informa-
tion Extraction, Information Retrieval and Docu-
ment Summarization. The textual entailment rela-
tionship holds between two text fragments, termed
text and hypothesis, if the truth of the hypothesis can
be inferred from the text.
Identifying entailment is a complex task that in-
corporates many levels of linguistic knowledge and
inference. The complexity of modeling entail-
ment was demonstrated in the first PASCAL Chal-
lenge Workshop on Recognizing Textual Entailment
(RTE) (Dagan et al, 2005). Systems that partici-
pated in the challenge used various combinations of
NLP components in order to perform entailment in-
ferences. These components can largely be classi-
fied as operating at the lexical, syntactic and seman-
tic levels (see Table 1 in (Dagan et al, 2005)). How-
ever, only little research was done to analyze the
contribution of each inference level, and on the con-
tribution of individual inference mechanisms within
each level.
This paper suggests that decomposing the com-
plex task of entailment into subtasks, and analyz-
ing the contribution of individual NLP components
for these subtasks would make a step towards bet-
ter understanding of the problem, and for pursuing
better entailment engines. We set three goals in this
paper. First, we consider two modeling levels that
employ only part of the inference mechanisms, but
perform perfectly at each level. We explore how
well these models approximate the notion of entail-
ment, and analyze the differences between the out-
come of the different levels. Second, for each of the
presented levels, we evaluate the distribution (and
contribution) of each of the inference mechanisms
typically associated with that level. Finally, we sug-
gest that the definitions of entailment at different
levels of inference, as proposed in this paper, can
serve as guidelines for manual annotation of a ?gold
standard? for evaluating systems that operate at a
particular level. Altogether, we set forth a possible
methodology for annotation and analysis of entail-
55
ment datasets.
We introduce two levels of entailment: Lexical
and Lexical-Syntactic. We propose these levels as
intermediate stages towards a complete entailment
model. We define an entailment model for each
level and manually evaluate its performance over a
sample from the RTE test-set. We focus on these
two levels as they correspond to well-studied NLP
tasks, for which robust tools and resources exist,
e.g. parsers, part of speech taggers and lexicons. At
each level we included inference types that represent
common practice in the field. More advanced pro-
cessing levels which involve logical/semantic infer-
ence are less mature and were left beyond the scope
of this paper.
We found that the main difference between the
lexical and lexical-syntactic levels is that the lexical-
syntactic level corrects many false-positive infer-
ences done at the lexical level, while introducing
only a few false-positives of its own. As for iden-
tifying positive cases (recall), both systems exhibit
similar performance, and were found to be comple-
mentary. Neither of the levels was able to iden-
tify more than half of the positive cases, which
emphasizes the need for deeper levels of analysis.
Among the different inference components, para-
phrases stand out as a dominant contributor to the
entailment task, while synonyms and derivational
transformations were found to be the most frequent
at the lexical level.
Using our definitions of entailment models as
guidelines for manual annotation resulted in a high
level of agreement between two annotators, suggest-
ing that the proposed models are well-defined.
Our study follows on previous work (Vander-
wende et al, 2005), which analyzed the RTE Chal-
lenge test-set to find the percentage of cases in
which syntactic analysis alone (with optional use
of thesaurus for the lexical level) suffices to decide
whether or not entailment holds. Our study extends
this work by considering a broader range of infer-
ence levels and inference mechanisms and providing
a more detailed view. A fundamental difference be-
tween the two works is that while Vanderwende et al
did not make judgements on cases where additional
knowledge was required beyond syntax, our entail-
ment models were evaluated over all of the cases,
including those that require higher levels of infer-
ence. This allows us to view the entailment model at
each level as an idealized system approximating full
entailment, and to evaluate its overall success.
The rest of the paper is organized as follows: sec-
tion 2 provides definitions for the two entailment
levels; section 3 describes the annotation experiment
we performed, its results and analysis; section 4 con-
cludes and presents planned future work.
2 Definition of Entailment Levels
In this section we present definitions for two en-
tailment models that correspond to the Lexical and
Lexical-Syntactic levels. For each level we de-
scribe the available inference mechanisms. Table 1
presents several examples from the RTE test-set to-
gether with annotation of entailment at the different
levels.
2.1 The Lexical entailment level
At the lexical level we assume that the text T and
hypothesis H are represented by a bag of (possibly
multi-word) terms, ignoring function words. At this
level we define that entailment holds between T and
H if every term h in H can be matched by a corre-
sponding entailing term t in T . t is considered as en-
tailing h if either h and t share the same lemma and
part of speech, or t can be matched with h through a
sequence of lexical transformations of the types de-
scribed below.
Morphological derivations This inference mech-
anism considers two terms as equivalent if one can
be obtained from the other by some morphologi-
cal derivation. Examples include nominalizations
(e.g. ?acquisition ? acquire?), pertainyms (e.g.
?Afghanistan ? Afghan?), or nominal derivations
like ?terrorist ? terror?.
Ontological relations This inference mechanism
refers to ontological relations between terms. A
term is inferred from another term if a chain of valid
ontological relations between the two terms exists
(Andreevskaia et al, 2005). In our experiment we
regarded the following three ontological relations
as providing entailment inferences: (1) ?synonyms?
(e.g. ?free ? release? in example 1361, Table 1);
(2) ?hypernym? (e.g. ?produce ? make?) and (3)
?meronym-holonym? (e.g. ?executive ? company?).
56
No. Text Hypothesis Task Ent. Lex.
Ent.
Syn.
Ent.
322 Turnout for the historic vote for the first
time since the EU took in 10 new mem-
bers in May has hit a record low of
45.3%.
New members joined the
EU.
IR true false true
1361 A Filipino hostage in Iraq was released. A Filipino hostage was
freed in Iraq.
CD true true true
1584 Although a Roscommon man by birth,
born in Rooskey in 1932, Albert ?The
Slasher? Reynolds will forever be a
Longford man by association.
Albert Reynolds was born
in Co. Roscommon.
QA true true true
1911 The SPD got just 21.5% of the vote
in the European Parliament elections,
while the conservative opposition par-
ties polled 44.5%.
The SPD is defeated by
the opposition parties.
IE true false false
2127 Coyote shot after biting girl in Vanier
Park.
Girl shot in park. IR false true false
Table 1: Examples of text-hypothesis pairs, taken from the PASCAL RTE test-set. Each line includes the
example number at the RTE test-set, the text and hypothesis, the task within the test-set, whether entailment
holds between the text and hypothesis (Ent.), whether Lexical entailment holds (Lex. Ent.) and whether
Lexical-Syntactic entailment holds (Syn. Ent.).
Lexical World knowledge This inference mech-
anism refers to world knowledge reflected at the
lexical level, by which the meaning of one term
can be inferred from the other. It includes both
knowledge about named entities, such as ?Tal-
iban ? organization? and ?Roscommon ? Co.
Roscommon? (example 1584 in Table 1), and other
lexical relations between words, such as WordNet?s
relations ?cause? (e.g. ?kill ? die?) and ?entail? (e.g.
?snore ? sleep?).
2.2 The Lexical-syntactic entailment level
At the lexical-syntactic level we assume that the
text and the hypothesis are represented by the set of
syntactic dependency relations of their dependency
parse. At this level we ignore determiners and aux-
iliary verbs, but do include relations involving other
function words. We define that entailment holds be-
tween T and H if the relations within H can be
?covered? by the relations in T . In the trivial case,
lexical-syntactic entailment holds if all the relations
composing H appear verbatim in T (while addi-
tional relations within T are allowed). Otherwise,
such coverage can be obtained by a sequence of
transformations applied to the relations in T , which
should yield all the relations in H .
One type of such transformations are the lexical
transformations, which replace corresponding lexi-
cal items, as described in sub-section 2.1. When ap-
plying morphological derivations it is assumed that
the syntactic structure is appropriately adjusted. For
example, ?Mexico produces oil? can be mapped to
?oil production by Mexico? (the NOMLEX resource
(Macleod et al, 1998) provides a good example for
systematic specification of such transformations).
Additional types of transformations at this level
are specified below.
Syntactic transformations This inference mech-
anism refers to transformations between syntactic
structures that involve the same lexical elements and
preserve the meaning of the relationships between
them (as analyzed in (Vanderwende et al, 2005)).
Typical transformations include passive-active and
apposition (e.g. ?An Wang, a native of Shanghai ?
An Wang is a native of Shanghai?).
57
Entailment paraphrases This inference mecha-
nism refers to transformations that modify the syn-
tactic structure of a text fragment as well as some
of its lexical elements, while holding an entailment
relationship between the original text and the trans-
formed one. Such transformations are typically de-
noted as ?paraphrases? in the literature, where a
wealth of methods for their automatic acquisition
were proposed (Lin and Pantel, 2001; Shinyama et
al., 2002; Barzilay and Lee, 2003; Szpektor et al,
2004). Following the same spirit, we focus here on
transformations that are local in nature, which, ac-
cording to the literature, may be amenable for large
scale acquisition. Examples include: ?X is Y man
by birth ? X was born in Y? (example 1584 in Ta-
ble 1), ?X take in Y ? Y join X?1 and ?X is holy
book of Y ? Y follow X?2.
Co-reference Co-references provide equivalence
relations between different terms in the text and
thus induce transformations that replace one term
in a text with any of its co-referenced terms. For
example, the sentence ?Italy and Germany have
each played twice, and they haven?t beaten anybody
yet.?3 entails ?Neither Italy nor Germany have
won yet?, involving the co-reference transformation
?they ? Italy and Germany?.
Example 1584 in Table 1 demonstrates the
need to combine different inference mechanisms
to achieve lexical-syntactic entailment, requiring
world-knowledge, paraphrases and syntactic trans-
formations.
3 Empirical Analysis
In this section we present the experiment that we
conducted in order to analyze the two entailment
levels, which are presented in section 2, in terms of
relative performance and correlation with the notion
of textual entailment.
3.1 Data and annotation procedure
The RTE test-set4 contains 800 Text-Hypothesis
pairs (usually single sentences), which are typical
1Example no 322 in the PASCAL RTE test-set.
2Example no 1575 in the PASCAL RTE test-set.
3Example no 298 in the PASCAL RTE test-set.
4The complete RTE dataset can be obtained at
http://www.pascal-network.org/Challenges/RTE/Datasets/
to various NLP applications. Each pair is annotated
with a boolean value, indicating whether the hypoth-
esis is entailed by the text or not, and the test-set
is balanced in terms of positive and negative cases.
We shall henceforth refer to this annotation as the
gold standard. We constructed a sample of 240 pairs
from four different tasks in the test-set, which corre-
spond to the main applications that may benefit from
entailment: information extraction (IE), information
retrieval (IR), question answering (QA), and compa-
rable documents (CD). We randomly picked 60 pairs
from each task, and in total 118 of the cases were
positive and 122 were negative.
In our experiment, two of the authors annotated,
for each of the two levels, whether or not entailment
can be established in each of the 240 pairs. The an-
notators agreed on 89.6% of the cases at the lexical
level, and 88.8% of the cases at the lexical-syntactic
level, with Kappa statistics of 0.78 and 0.73, re-
spectively, corresponding to ?substantial agreement?
(Landis and Koch, 1977). This relatively high level
of agreement suggests that the notion of lexical and
lexical-syntactic entailment we propose are indeed
well-defined.
Finally, in order to establish statistics from the an-
notations, the annotators discussed all the examples
they disagreed on and produced a final joint deci-
sion.
3.2 Evaluating the different levels of entailment
L LS
True positive (118) 52 59
False positive (122) 36 10
Recall 44% 50%
Precision 59% 86%
F1 0.5 0.63
Accuracy 58% 71%
Table 2: Results per level of entailment.
Table 2 summarizes the results obtained from our
annotated dataset for both lexical (L) and lexical-
syntactic (LS) levels. Taking a ?system?-oriented
perspective, the annotations at each level can be
viewed as the classifications made by an idealized
system that includes a perfect implementation of the
inference mechanisms in that level. The first two
58
rows show for each level how the cases, which were
recognized as positive by this level (i.e. the entail-
ment holds), are distributed between ?true positive?
(i.e. positive according to the gold standard) and
?false positive? (negative according to the gold stan-
dard). The total number of positive and negative
pairs in the dataset is reported in parentheses. The
rest of the table details recall, precision, F1 and ac-
curacy.
The distribution of the examples in the RTE test-
set cannot be considered representative of a real-
world distribution (especially because of the con-
trolled balance between positive and negative exam-
ples). Thus, our statistics are not appropriate for
accurate prediction of application performance. In-
stead, we analyze how well these simplified models
of entailment succeed in approximating ?real? en-
tailment, and how they compare with each other.
The proportion between true and false positive
cases at the lexical level indicates that the correla-
tion between lexical match and entailment is quite
low, reflected in the low precision achieved by this
level (only 59%). This result can be partly attributed
to the idiosyncracies of the RTE test-set: as reported
in (Dagan et al, 2005), samples with high lexical
match were found to be biased towards the negative
side. Interestingly, our measured accuracy correlates
well with the performance of systems at the PAS-
CAL RTE Workshop, where the highest reported ac-
curacy of a lexical system is 0.586 (Dagan et al,
2005).
As one can expect, adding syntax considerably re-
duces the number of false positives - from 36 to only
10. Surprisingly, at the same time the number of true
positive cases grows from 52 to 59, and correspond-
ingly, precision rise to 86%. Interestingly, neither
the lexical nor the lexical-syntactic level are able to
cover more than half of the positive cases (e.g. ex-
ample 1911 in Table 1).
In order to better understand the differences be-
tween the two levels, we next analyze the overlap
between them, presented in Table 3. Looking at
Table 3(a), which contains only the positive cases,
we see that many examples were recognized only by
one of the levels. This interesting phenomenon can
be explained on the one hand by lexical matches that
could not be validated in the syntactic level, and on
the other hand by the use of paraphrases, which are
Lexical-Syntactic
H ? T H; T
Lexical H ? T 38 14H; T 21 45
(a) positive examples
Lexical-Syntactic
H ? T H; T
Lexical H ? T 7 29H; T 3 83
(b) negative examples
Table 3: Correlation between the entailment lev-
els. (a) includes only the positive examples from
the RTE dataset sample, and (b) includes only the
negative examples.
introduced only in the lexical-syntactic level. (e.g.
example 322 in Table 1).
This relatively symmetric situation changes as we
move to the negative cases, as shown in Table 3(b).
By adding syntactic constraints, the lexical-syntactic
level was able to fix 29 false positive errors, misclas-
sified at the lexical level (as demonstrated in exam-
ple 2127, Table 1), while introducing only 3 new
false-positive errors. This exemplifies the impor-
tance of syntactic matching for precision.
3.3 The contribution of various inference
mechanisms
Inference Mechanism f 4R %
Synonym 19 14.4% 16.1%
Morphological 16 10.1% 13.5%
Lexical World knowledge 12 8.4% 10.1%
Hypernym 7 4.2% 5.9%
Mernoym 1 0.8% 0.8%
Entailment Paraphrases 37 26.2% 31.3%
Syntactic transformations 22 16.9% 18.6%
Coreference 10 5.0% 8.4%
Table 4: The frequency (f ), contribution to recall
(4R) and percentage (%), within the gold standard
positive examples, of the various inference mecha-
nisms at each level, ordered by their significance.
59
In order to get a sense of the contribution of the
various components at each level, statistics on the in-
ference mechanisms that contributed to the coverage
of the hypothesis by the text (either full or partial)
were recorded by one annotator. Only the positive
cases in the gold standard were considered.
For each inference mechanism we measured its
frequency, its contribution to the recall of the related
level and the percentage of cases in which it is re-
quired for establishing entailment. The latter also
takes into account cases where only partial cover-
age could be achieved, and thus indicates the signif-
icance of each inference mechanism for any entail-
ment system, regardless of the models presented in
this paper. The results are summarized in Table 4.
From Table 4 it stands that paraphrases are the
most notable contributors to recall. This result in-
dicates the importance of paraphrases to the en-
tailment task and the need for large-scale para-
phrase collections. Syntactic transformations are
also shown to contribute considerably, indicating the
need for collections of syntactic transformations as
well. In that perspective, we propose our annota-
tion framework as means for evaluating collections
of paraphrases or syntactic transformations in terms
of recall.
Finally, we note that the co-reference moderate
contribution can be partly attributed to the idiosyn-
cracies of the RTE test-set: the annotators were
guided to replace anaphors with the appropriate ref-
erence, as reported in (Dagan et al, 2005).
4 Conclusions
In this paper we presented the definition of two en-
tailment models, Lexical and Lexical-Syntactic, and
analyzed their performance manually. Our experi-
ment shows that the lexical-syntactic level outper-
forms the lexical level in all measured aspects. Fur-
thermore, paraphrases and syntactic transformations
emerged as the main contributors to recall. These
results suggest that a lexical-syntactic framework
is a promising step towards a complete entailment
model.
Beyond these empirical findings we suggest that
the presented methodology can be used generically
to annotate and analyze entailment datasets.
In future work, it would be interesting to analyze
higher levels of entailment, such as logical inference
and deep semantic understanding of the text.
Acknowledgements
We would like to thank Ido Dagan for helpful discus-
sions and for his scientific supervision. This work
was supported in part by the IST Programme of the
European Community, under the PASCAL Network
of Excellence, IST-2002-506778. This publication
only reflects the authors? views.
References
Alina Andreevskaia, Zhuoyan Li and Sabine Bergler.
2005. Can Shallow Predicate Argument Structures
Determine Entailment?. In Proceedings of Pascal
Challenge Workshop on Recognizing Textual Entail-
ment, 2005.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL
2003. pages 16-23, Edmonton, Canada.
Ido Dagan, Bernardo Magnini and Oren Glickman. 2005.
The PASCAL Recognising Textual Entailment Chal-
lenge. In Proceedings of Pascal Challenge Workshop
on Recognizing Textual Entailment, 2005.
J.R. Landis and G.G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33:159-174.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for Question Answering. Natural Language
Engineering, 7(4):343-360.
C. Macleod, R. Grishman, A. Meyers, L. Barrett and R.
Reeves. 1998. Nomlex: A lexicon of nominalizations.
In Proceedings of the 8th International Congress of the
European Association for Lexicography, 1998. Lie`ge,
Belgium: EURALEX.
Yusuke Shinyama and Satoshi Sekine, Kiyoshi Sudo and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of Human
Language Technology Conference (HLT 2002). San
Diego, USA.
Idan Szpektor, Hristo Tanev, Ido Dagan and Bonnaven-
tura Coppola. 2004. Scaling Web-based Acquistion
of Entailment Relations. In Proceedings of EMNLP
2004.
Lucy Vanderwende, Deborah Coughlin and Bill Dolan.
2005. What Syntax Contribute in Entailment Task. In
Proceedings of Pascal Challenge Workshop on Recog-
nizing Textual Entailment, 2005.
60
Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 65?72,
Prague, 28 June 2007. c?2007 Association for Computational Linguistics
Cross Lingual and Semantic Retrieval for Cultural Heritage Appreciation
Idan Szpektor, Ido Dagan
Dept. of Computer Science
Bar Ilan University
szpekti@cs.biu.ac.il
Alon Lavie
Language Technologies Inst.
Carnegie Mellon University
alavie+@cs.cmu.edu
Danny Shacham, Shuly Wintner
Dept. of Computer Science
University of Haifa
shuly@cs.haifa.ac.il
Abstract
We describe a system which enhances the
experience of museum visits by providing
users with language-technology-based in-
formation retrieval capabilities. The sys-
tem consists of a cross-lingual search en-
gine, augmented by state of the art semantic
expansion technology, specifically designed
for the domain of the museum (history and
archaeology of Israel). We discuss the tech-
nology incorporated in the system, its adap-
tation to the specific domain and its contri-
bution to cultural heritage appreciation.
1 Introduction
Museum visits are enriching experiences: they pro-
vide stimulation to the senses, and through them to
the mind. But the experience does not have to end
when the visit ends: further exploration of the ar-
tifacts and their influence on the visitor is possible
after the visit, either on location or elsewhere. One
common means of exploration is Information Re-
trieval (IR) via a Search Engine. For example, a mu-
seum could implement a search engine over a col-
lection of documents relating to the topics exhibited
in the museum.
However, such document collections are usually
much smaller than general collections, in particular
the World Wide Web. Consequently, phenomena in-
herent to natural languages may severely hamper the
performance of human language technology when
applied to small collections. One such phenomenon
is the semantic variability of natural languages, the
ability to express a specific meaning in many dif-
ferent ways. For example, the expression ?Archae-
ologists found a new tomb? can be expressed also
by ?Archaeologists discovered a tomb? or ?A sar-
cophagus was dug up by Egyptian Researchers?. On
top of monolingual variability, the same information
can also be expressed in different languages. Ignor-
ing natural language variability may result in lower
recall of relevant documents for a given query, espe-
cially in small document collections.
This paper describes a system that attempts to
cope with semantic variability through the use of
state of the art human language technology. The
system provides both semantic expansion and cross
lingual IR (and presentation of information) in the
domain of archaeology and history of Israel. It
was specifically developed for the Hecht Museum
in Haifa, Israel, which contains a small but unique
collection of artifacts in this domain. The system
provides different users with different capabilities,
bridging over language divides; it addresses seman-
tic variation in novel ways; and it thereby comple-
ments the visit to the museum with long-lasting in-
stillation of information.
The main component of the system is a domain-
specific search engine that enables users to specify
queries and retrieve information pertaining to the do-
main of the museum. The engine is enriched by lin-
guistic capabilities which embody an array of means
for addressing semantic variation. Queries are ex-
panded using two main techniques: semantic expan-
sion based on textual entailment; and cross-lingual
expansion based on translation of Hebrew queries
to English and vice versa. Retrieved documents are
presented as links with associated snippets; the sys-
tem also translates snippets from Hebrew to English.
The main contribution of this work is, of course,
the system itself, which was recently demonstrated
65
successfully at the museum and which we believe
could be useful to a variety of museum visitor types,
from children to experts. For example, the system
provides Hebrew speakers access to English doc-
uments pertaining to the domain of the museum,
and vice versa, thereby expanding the availability
of multilingual material to museum visitors. More
generally, it is an instance of adaptation of state of
the art human language technology to the domain
of cultural heritage appreciation, demonstrating how
general resources and tools are adapted to a specific
domain, thereby improving their accuracy and us-
ability. Finally, it provides a test-bed for evaluating
the contribution of language technology in general,
as well as specific components and resources, to a
large-scale natural language processing system.
2 Background and Motivation
Internet search is hampered by the complexity of
natural languages. The two main characteristics of
this complexity are ambiguity and variability: the
former refers to the fact that a given text can be
interpreted in more than one way; the latter indi-
cates that the same meaning can be linguistically ex-
pressed in several ways. The two phenomena make
simple search techniques too weak for unsophisti-
cated users, as existing search engines perform only
direct keyword matching, with very limited linguis-
tic processing of the texts they retrieve.
Specifically, IR systems that do not address the
variability in languages may suffer from lower re-
call, especially in restricted domains and small doc-
ument locations. We next describe two prominent
types of variability that we think should be ad-
dressed in IR systems.
2.1 Textual Entailment and Entailment Rules
In many NLP applications, such as Question An-
swering (QA), Information Extraction (IE) and In-
formation Retrieval (IR), it is crucial to recognize
that a specific target meaning can be inferred from
different text variants. For example, a QA system
needs to induce that ?Mendelssohn wrote inciden-
tal music? can be inferred from ?Mendelssohn com-
posed incidental music? in order to answer the ques-
tion ?Who wrote incidental music??. This type of
reasoning has been identified as a core semantic in-
ference task by the generic textual entailment frame-
work (Dagan et al, 2006; Bar-Haim et al, 2006).
The typical way to address variability in IR is to
use lexical query expansion (Lytinen et al, 2000;
Zukerman and Raskutti, 2002). However, there are
variability patterns that cannot be described using
just constant phrase to phrase entailment. Another
important type of knowledge representation is en-
tailment rules and paraphrases. An entailment rule
is a directional relation between two templates, text
patterns with variables, e.g., ?X compose Y ?
X write Y ?. The left hand side is assumed to en-
tail the right hand side in certain contexts, under
the same variable instantiation. Paraphrases can be
viewed as bidirectional entailment rules. Such rules
capture basic inferences in the language, and are
used as building blocks for more complex entail-
ment inference. For example, given the above en-
tailment rule, a QA system can identify the answer
?Mendelssohn? in the above example. This need
sparked intensive research on automatic acquisition
of paraphrase and entailment rules.
Although knowledge-bases of entailment-rules
and paraphrases learned by acquisition algorithms
were used in other NLP applications, such as QA
(Lin and Pantel, 2001; Ravichandran and Hovy,
2002) and IE (Sudo et al, 2003; Romano et al,
2006), to the best of our knowledge the output of
such algorithms was never applied to IR before.
2.2 Cross Lingual Information Retrieval
The difficulties caused by variability are amplified
when the user is not a native speaker of the language
in which the retrieved texts are written. For exam-
ple, while most Israelis can read English documents,
fewer are comfortable with the specification of Eng-
lish queries. In a museum setting, some visitors may
be able to read Hebrew documents but still be rel-
atively poor at searching for them. Other visitors
may be unable to read Hebrew texts, but still benefit
from non-textual information that are contained in
Hebrew documents (e.g., pictures, maps, audio and
video files, external links, etc.)
This problem is addressed by the paradigm of
Cross-Lingual Information Retrieval (CLIR). This
paradigm has become a very active research area
in recent years, addressing the needs of multilingual
and non-English speaking communities, such as the
66
European Union, East-Asian nations and Spanish
speaking communities in the US (Hull and Grefen-
stette, 1996; Ballesteros and Croft, 1997; Carbonell
et al, 1997). The common approach for CLIR is
to translate a query in a source language to another
target language and then issue the translated query
to retrieve target language documents. As explained
above, CLIR research has to address various generic
problems caused by the variability and ambiguity of
natural languages, as well as specific problems re-
lated to the particular languages being addressed.
3 Coping with Semantic Variability in IR
We describe a search engine that is capable of per-
forming: (a) semantic English information retrieval;
and (b) cross-lingual (Hebrew-English and English-
Hebrew) information retrieval, allowing users to
pose queries in either of the two languages and re-
trieve documents in both. This is achieved by two
sub-processes of the search engine: first, the en-
gine performs shallow semantic linguistic inference
and supports the retrieval of documents which con-
tain phrases that imply the meaning of the translated
query, even when no exact match of the translated
keywords is found. This is enabled by automatic ac-
quisition of semantic variability patterns that are fre-
quent in the language, which extend traditional lexi-
cal query expansion techniques. Second, the engine
translates the original or expanded query to the tar-
get language, based on several linguistic processes
and a machine readable bilingual dictionary. The re-
sult is a semantic expansion of a given query to a va-
riety of alternative wordings in which an answer to
this query may be expressed in the target language
of the retrieved documents.
These enhancements are facilitated via a speci-
fication of the domain. As our system is specifi-
cally designed to work in the domain of the history
and archaeology, we could focus our attention on re-
sources and tools that are dedicated to this domain.
Thus, for example, lexicons and dictionaries, whose
preparation is always costly and time consuming,
were developed with the specific domain in mind;
and textual entailment and paraphrase patterns were
extracted for the specific domain. While the result-
ing system is focused on visiting the Hecht Museum,
the methodology which we used and discuss here
can be adapted to other areas of cultural heritage, as
well as to other narrow domains, in the same way.
3.1 Setting Up a Basic Retrieval Application
We created a basic retrieval system in two steps:
first, we collected relevant documents; then, we im-
plemented a search engine over the collected docu-
ments.
In order to construct a local corpus, an archae-
ology expert searched the Web for relevant sites
and pages. We then downloaded all the documents
linked from those pages using a crawler. The expert
looked for documents in both English and Hebrew.
In total, we collected a non-comparable bilingual
corpus for Archaeology containing several thousand
documents in English and Hebrew.
We implemented our enhanced retrieval modules
on top of the basic Jakarta Lucene indexing and
search engine1. All documents were indexed using
Lucene, but instead of inflected words, we indexed
the lemma of each word (see detailed description of
our Hebrew lemmatization in Section 3.3). In order
to match the indexed terms, query terms (either He-
brew or English) were also lemmatized before the
index was searched, in a manner similar to lemma-
tizing the documents.
3.2 Query Expansion Using Entailment Rules
As described in Section 2.1, entailment rules had not
been used as a knowledge resource for expanding IR
queries, prior to our work. In this paper we use this
resource instead of the typical lexical expansion in
order to test its benefit. Most entailment rules cap-
ture relations between different predicates. We thus
focus on documents retrieved for queries that con-
tain a predicate over one or two entities, which we
term here Relational IR. We would like to retrieve
only documents that describe an occurrence of that
predicate, but possibly in words different than the
ones used in the query. In this section we describe
in detail how we learn entailment rules and how we
apply them in query expansion.
Automatically Learning Entailment Rules from
the Web Many algorithms for automatically learn-
ing paraphrases and entailment rules have been
explored in recent years (Lin and Pantel, 2001;
1http://jakarta.apache.org/lucene/docs/index.html
67
Ravichandran and Hovy, 2002; Shinyama et al,
2002; Barzilay and Lee, 2003; Sudo et al, 2003;
Szpektor et al, 2004; Satoshi, 2005). In this pa-
per we use TEASE (Szpektor et al, 2004), a state-
of-the-art unsupervised acquisition algorithm for
lexical-syntactic entailment rules.
TEASE acquires entailment relations for a given
input template from the Web. It first retrieves from
the Web sentences that match the input template.
From these sentences it extracts the variable instan-
tiations, termed anchor-sets, which are identified as
being characteristic for the input template based on
statistical criteria.
Next, TEASE retrieves from the Web sentences
that contain the extracted anchor-sets. The retrieved
sentences are parsed and the anchors found in each
sentence are replaced with their corresponding vari-
ables. Finally, from this retrieved corpus of parsed
sentences, templates that are assumed to entail or
be entailed by the input template are learned. The
learned templates are ranked by the number of oc-
currences they were learned from.
Entailment Rules for Domain Specific Query Ex-
pansion Our goal is to use the knowledge-base of
entailment rules learned by TEASE in order to per-
form query expansion. The two subtasks that arise
are: (a) acquiring an appropriate knowledge-base
of rules; and (b) expanding a query given such a
knowledge-base.
TEASE learns entailment rules for a given input
template. As our document collection is domain
specific, a list of such relevant input templates can
be prepared. In our case, we used an archaeology
expert to generate a list of verbs and verb phrases
that relate to archaeology, such as: ?excavate?, ?in-
vade?, ?build?, ?reconstruct?, ?grow? and ?be located
in?. We then executed TEASE on each of the tem-
plates representing these verbs in order to learn from
the Web rules in which the input templates partici-
pate. An example for such rules is presented in Ta-
ble 1. We learned approximately 3900 rules for 80
input templates.
Since TEASE learns lexical-syntactic rules, we
need a syntactic representation of the query. We
parse each query using the Minipar dependency
parser (Lin, 1998). We next try to match the left
hand side template of every rule in the learned
knowledge-base. Since TEASE does not identify
the direction of the relation learned between two
templates, we try both directional rules that are in-
duced from a learned relation. Whenever a match
is found, a new query is generated, in which the
constant terms of the matched left hand side tem-
plate are replaced with the constant terms of the right
hand side template. For example, given the query
?excavations of Jerusalem by archaeologists? and a
learned rule ?excavation of Y by X ? X dig in Y ?,
a new query is generated, containing the terms ?ar-
chaeologists dig in Jerusalem?. Finally, we retrieve
all the documents that contain all the terms of at least
one of the expanded queries (including the original
query). The basic search engine provides a score for
each document. We re-score each document as the
sum of scores it obtained from the different queries
that it matched. Figure 1 shows an example of our
query expansion, where the first retrieved documents
do not contain the words used to describe the predi-
cate in the query, but other ways to describe it.
All the templates learned by TEASE contain two
variables, and thus the rules that are learned can only
be applied to queries that contain predicates over
two terms. In order to broaden the coverage of the
learned rules, we automatically generate also all the
partial templates of a learned template. These are
templates that contain just one of variables in the
original template. We then generate rules between
these partial templates that correspond to the origi-
nal rules. With partial templates/rules, expansion for
the query in Figure 1 becomes possible.
3.3 Cross-lingual IR
Until very recently, linguistic resources for Hebrew
were few and far between (Wintner, 2004). The last
few years, however, have seen a proliferation of re-
sources and tools for this language. In this work we
utilize a relatively large-scale lexicon of over 22,000
entries (Itai et al, 2006); a finite-state based mor-
phological analyzer of Hebrew that is directly linked
to the lexicon (Yona and Wintner, 2007); a medium-
size bilingual dictionary of some 24,000 word pairs;
and a rudimentary Hebrew to English machine trans-
lation system (Lavie et al, 2004). All these re-
sources had to be adapted to the domain of the Hecht
museum.
Cross-lingual language technology is utilized in
68
Figure 1: Semantic expansion example. Note that the expanded queries that were generated in the first two
retrieved texts (listed under ?matched query?) do not contain the original query.
three different components of the system: Hebrew
documents are morphologically processed to pro-
vide better indexing; query terms in English are
translated to Hebrew and vice versa; and Hebrew
snippets are translated to English. We discuss each
of these components in this section.
Linguistically-aware indexing The correct level
of indexing for morphologically-rich language has
been a matter of some debate in the information re-
trieval literature. When Arabic is concerned, Dar-
wish and Oard (2002) conclude that ?Character n-
grams or lightly stemmed words were found to
typically yield near-optimal retrieval effectiveness?.
Since Hebrew is even more morphologically (and
orthographically) ambiguous than Arabic, and espe-
cially in light of the various prefix particles which
can be attached to Hebrew words, we opted for full
morphological analysis of Hebrew documents be-
fore they are indexed, followed by indexing on the
lexeme.
We use the HAMSAH morphological analyzer
(Yona and Wintner, 2007), which was recently re-
written in Java and is therefore more portable and
efficient (Wintner, 2007). We processed the entire
domain specific corpus described above and used
the resulting lexemes to index documents. This pre-
processing brought to the foreground several omis-
sions of the analyzer, mostly due to domain-specific
terms missing in the lexicon. We selected the one
thousand most frequent words with no morphologi-
cal analysis and added their lexemes to the lexicon.
While we do not have quantitative evaluation met-
rics, the coverage of the system improved in a very
evident way.
Query translation When users submit a query in
one language they are provided with the option to re-
quest a translation of the query to the other language,
thereby retrieving documents in the other language.
The motivation behind this capability is that users
who may be able to read documents in a language
may find the specification of queries in that language
too challenging; also, retrieving documents in a for-
eign language may be useful due to the non-textual
information in the retrieved documents, especially in
a museum environment.
In order to support cross-lingual query specifica-
tion we capitalized on a medium-size bilingual dic-
tionary that was already used for Hebrew to Eng-
lish machine translation. Since the coverage of the
dictionary was rather limited, and many domain-
specific items were missing, we chose the one thou-
sand most frequent lexemes which had no transla-
69
Input Template Learned Template
X excavate Y X discover Y , X find Y ,
X uncover Y , X examine Y ,
X unearth Y , X explore Y
X construct Y X build Y , X develop Y ,
X create Y , X establish Y
X contribute to Y X cause Y , X linked to Y ,
X involve in Y
date X to Y X built in Y , X began in Y ,
X go back to Y
X cover Y X bury Y ,
X provide coverage for Y
X invade Y X occupy Y , X attack Y ,
X raid Y , X move into Y
X restore Y X protect Y , X preserve Y ,
X save Y , X conserve Y
Table 1: Examples for correct templates that were
learned by TEASE for input templates.
tions and translated them manually, augmenting the
lexicon with missing Hebrew lexemes where neces-
sary and expanding the bilingual dictionary to cover
this domain.
In order to translate query terms we use the He-
brew English dictionary also as an English-Hebrew
dictionary. While this is known to be sub-optimal,
our current results support such an adaptation in lieu
of dedicated directional bilingual dictionaries.
Translating a query from one language to another
may introduce ambiguity where none exists. For
example, the query term spinh ?vessel? is unam-
biguous in Hebrew, but once translated into English
will result in retrieving documents on both senses
of the English word. Usually, this problem is over-
come since users tend to specify multi-term queries,
and the terms disambiguate each other. However,
a more systematic solution can be offered since we
have access to semantic expansion capabilities (in a
single language). That is, expanding the query in
the source language will result in more query terms
which, when translated, are more likely to disam-
biguate the context. We leave such an extension for
future work.
Snippet translation When Hebrew documents are
retrieved, we augment the (Hebrew) snippet which
the system produces by an English translation. We
use an extended, improved version of a rudimentary
Hebrew to English MT system developed by Lavie
et al (2004). Extensions include an improved mor-
phological analysis of the input, an extended bilin-
gual dictionary and a revised set of transfer rules,
as well as a more modern transfer engine and a
much larger language model for generating the tar-
get (English) sentences.
The MT system is transfer based: it performs lin-
guistic pre-processing of the source language (in our
case, morphological analysis) and post-processing
of the target (generation of English word forms), and
uses a small set of transfer rules to translate local
structures from the source to the target and create
translation hypotheses, which are stored in a lattice.
A statistical language model is used to decode the
lattice and select the best hypotheses.
The benefit of this architecture is that domain spe-
cific adaptation of the system is relatively easy, and
does not require a domain specific parallel corpus
(which we do not have). The system has access
to our domain-specific lexicon and bilingual dictio-
nary, and we even refined some transfer rules due to
peculiarities of the domain. One advantage of the
transfer-based approach is that it enables us to treat
out-of-lexicon items in a unique way. We consider
such items proper names, and transfer rules process
them as such. As an example, Figure 2 depicts the
translation of a Hebrew snippet meaning A jar from
the early bronze period with seashells from the Nile.
The word nilws ?Nile? is missing from the lexicon,
but this does not prevent the system from producing
a legible translation, using the transliterated form
where an English equivalent is unavailable.
4 Conclusions
We described a system for cross-lingual and
semantically-enhanced retrieval of information in
the cultural heritage domain, obtained by adapting
existing state-of-the-art tools and resources to the
domain. The system enhances the experience of mu-
seum visits, using language technology as a vehi-
cle for long-lasting instillation of information. Due
to the novelty of this application and the dearth of
available multilingual annotated resources in this
domain, we are unable to provide a robust, quan-
70
Figure 2: Translation example
Query Without Expansion With Expansion
Relevant Total Relevant Total
in Top 10 Retrieved in Top 10 Retrieved
discovering boats 2 2 5 86
growing vineyards 0 0 6 8
Persian invasions 5 5 8 22
excavations of the Byzantine period 10 37 10 100
restoring mosaics 0 0 3 69
Table 2: Analysis of the number of relevant documents out of the top 10 and the total number of retrieved
documents (up to 100) for a sample of queries.
titative evaluation of the approach. A preliminary
analysis of a sample of queries is presented in Ta-
ble 2. It illustrates the potential of expansion for
document collections of narrow domain. In what
follows we provide some qualitative impressions.
We observed that the system was able to learn
many expansion rules that cannot be induced from
manually constructed lexical resources, such as the-
sauri or WordNet (Fellbaum, 1998). This is espe-
cially true for rules that are specific for a narrow do-
main, e.g. ?X restore Y ? X preserve Y ?. Fur-
thermore, the system learned lexical syntactic rules
that cannot be expressed by a mere lexical substitu-
tion, but include also a syntactic transformation. For
example, ?date X to Y ? X go back to Y ?.
In addition, since rules are acquired by searching
the Web, they are not necessarily restricted to learn-
ing from the target domain, but can be learned from
similar terminology in other domains. For example,
the rule ?X discover Y ? X find Y ? was learned
from contexts such as {X=?astronomers? ;Y =?new
planets?} and {X=?zoologists? ;Y =?new species?}.
The quality of the rules that were automatically
acquired is mediocre. We found that although many
rules were useful for expansion, they had to be
manually filtered in order to retain only rules that
achieved high precision.
Finally, we note that applying semantic query ex-
pansion (using entailment rules), followed by Eng-
lish to Hebrew query translation, results in query ex-
pansion for Hebrew using techniques that were so
far applicable only to resource-rich languages, such
as English.
Acknowledgements
This research was supported by the Israel Internet
Association; by THE ISRAEL SCIENCE FOUN-
DATION (grant No. 137/06 and grant No. 1095/05);
by the Caesarea Rothschild Institute for Interdisci-
plinary Application of Computer Science at the Uni-
versity of Haifa; by the ITC-irst/University of Haifa
collaboration; and by the US National Science Foun-
dation (grants IIS-0121631, IIS-0534217, and the
Office of International Science and Engineering).
71
We wish to thank the Hebrew Knowledge Center
at the Technion for providing resources for Hebrew.
We are grateful to Oliviero Stock, Martin Golumbic,
Alon Itai, Dalia Bojan, Erik Peterson, Nurit Mel-
nik, Yaniv Eytani and Noam Ordan for their help
and support.
References
Lisa Ballesteros and W. Bruce Croft. 1997. Phrasal
translation and query expansion techniques for cross-
language information retrieval. In ACM SIGIR Con-
ference on Research and Development in Information
Retrieval, pages 84?91.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second pascal recognising textual entail-
ment challenge. In Second PASCAL Challenge Work-
shop for Recognizing Textual Entailment.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL.
Jaime G. Carbonell, Yiming Yang, Robert E. Frederk-
ing, Ralf D. Brown, Yibing Geng, and Danny Lee.
1997. Translingual information retrieval: A compar-
ative evaluation. In IJCAI (1), pages 708?715.
Ido Dagan, Oren Glickman, and Bernardo. Magnini.
2006. The pascal recognising textual entailment chal-
lenge. In Lecture Notes in Computer Science, Volume
3944, volume 3944, pages 177?190.
Kareem Darwish and Douglas W. Oard. 2002. Term se-
lection for searching printed Arabic. In SIGIR ?02:
Proceedings of the 25th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, pages 261?268, New York, NY,
USA. ACM Press.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Language, Speech and Com-
munication. MIT Press.
D. A. Hull and G. Grefenstette. 1996. Querying across
languages. a dictionary-based approach to multilingual
information retrieval. In Proceedings of the 19th ACM
SIGIR Conference, pages 49?57.
Alon Itai, Shuly Wintner, and Shlomo Yona. 2006. A
computational lexicon of contemporary Hebrew. In
Proceedings of The fifth international conference on
Language Resources and Evaluation (LREC-2006).
Alon Lavie, Shuly Wintner, Yaniv Eytani, Erik Peterson,
and Katharina Probst. 2004. Rapid prototyping of a
transfer-based Hebrew-to-English machine translation
system. In Proceedings of TMI-2004: The 10th Inter-
national Conference on Theoretical and Methodolog-
ical Issues in Machine Translation, Baltimore, MD,
October.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. In Natural Lan-
guage Engineering, volume 7(4), pages 343?360.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalu-
ation of Parsing Systems at LREC.
S. Lytinen, N. Tomuro, and T. Repede. 2000. The use of
wordnet sense tagging in faqfinder. In Proceedings of
the AAAI00 Workshop on AI and Web Search.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Lorenza Romano, Milen Kouylekov, Idan Szpektor, Ido
Dagan, and Alberto Lavelli. 2006. Investigating a
generic paraphrase-based approach for relation extrac-
tion. In Proceedings of EACL.
Sekine Satoshi. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Yusuke Shinyama, Satoshi Sekine, Sudo Kiyoshi, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of HLT.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic ie pattern acquisition. In Pro-
ceedings of ACL.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP.
Shuly Wintner. 2004. Hebrew computational linguis-
tics: Past and future. Artificial Intelligence Review,
21(2):113?138.
Shuly Wintner. 2007. Finite-state technology as a pro-
gramming environment. In Alexander Gelbukh, edi-
tor, Proceedings of the Conference on Computational
Linguistics and Intelligent Text Processing (CICLing-
2007), volume 4394 of Lecture Notes in Computer Sci-
ence, pages 97?106, Berlin and Heidelberg, February.
Springer.
Shlomo Yona and Shuly Wintner. 2007. A finite-state
morphological grammar of Hebrew. Natural Lan-
guage Engineering. To appear.
Ingrid Zukerman and Bhavani Raskutti. 2002. Lexical
query paraphrasing for document retrieval. In Pro-
ceedings of ACL.
72
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 131?136,
Prague, June 2007. c?2007 Association for Computational Linguistics
Semantic Inference at the Lexical-Syntactic Level for Textual Entailment
Recognition
Roy Bar-Haim?,Ido Dagan?, Iddo Greental?, Idan Szpektor? and Moshe Friedman?
?Computer Science Department, Bar-Ilan University, Ramat-Gan 52900, Israel
?Linguistics Department, Tel Aviv University, Ramat Aviv 69978, Israel
{barhair,dagan}@cs.biu.ac.il,greenta@post.tau.ac.il,
{szpekti,friedmm}@cs.biu.ac.il
Abstract
We present a new framework for textual en-
tailment, which provides a modular integra-
tion between knowledge-based exact infer-
ence and cost-based approximate matching.
Diverse types of knowledge are uniformly
represented as entailment rules, which were
acquired both manually and automatically.
Our proof system operates directly on parse
trees, and infers new trees by applying en-
tailment rules, aiming to strictly generate the
target hypothesis from the source text. In or-
der to cope with inevitable knowledge gaps,
a cost function is used to measure the re-
maining ?distance? from the hypothesis.
1 Introduction
According to the traditional formal semantics ap-
proach, inference is conducted at the logical level.
However, practical text understanding systems usu-
ally employ shallower lexical and lexical-syntactic
representations, augmented with partial semantic
annotations. Such practices are typically partial and
quite ad-hoc, and lack a clear formalism that speci-
fies how inference knowledge should be represented
and applied. The current paper proposes a step to-
wards filling this gap, by defining a principled se-
mantic inference mechanism over parse-based rep-
resentations.
Within the textual entailment setting a system is
required to recognize whether a hypothesized state-
ment h can be inferred from an asserted text t.
Some inferences can be based on available knowl-
edge, such as information about synonyms and para-
phrases. However, some gaps usually arise and it
is often not possible to derive a complete ?proof?
based on available inference knowledge. Such sit-
uations are typically handled through approximate
matching methods.
This paper focuses on knowledge-based infer-
ence, while employing rather basic methods for ap-
proximate matching. We define a proof system
that operates over syntactic parse trees. New trees
are derived using entailment rules, which provide a
principled and uniform mechanism for incorporat-
ing a wide variety of manually and automatically-
acquired inference knowledge. Interpretation into
stipulated semantic representations, which is often
difficult to obtain, is circumvented altogether. Our
research goal is to explore how far we can get with
such an inference approach, and identify the scope
in which semantic interpretation may not be needed.
For a detailed discussion of our approach and related
work, see (Bar-Haim et al, 2007).
2 Inference Framework
The main contribution of the current work is a prin-
cipled semantic inference mechanism, that aims to
generate a target text from a source text using en-
tailment rules, analogously to logic-based proof sys-
tems. Given two parsed text fragments, termed
text (t) and hypothesis (h), the inference system (or
prover) determines whether t entails h. The prover
applies entailment rules that aim to transform t into
h through a sequence of intermediate parse trees.
For each generated tree p, a heuristic cost function is
employed to measure the likelihood of p entailing h.
131
ROOT
i 
rain VERB
expletive
ssfff
ff
ff
ff
f wha
++XX
XX
XX
XX
XX
it OTHER when PREP
i 
see VERB
obj
qqcccccc
cccc
cccc
cccc
cc
bessff
ff
ff
ff
ff
by

mod
++XX
XX
XX
XX
XX
Mary NOUN
mod 
be VERB by PREP
pcomp?n

yesterday NOUN
beautiful ADJ John NOUN
ROOT
i 
rain VERB
expletive
rreeee
eee
eee
e wha
,,YYY
YYY
YYY
YY
it OTHER when PREP
i 
see VERB
subj
rreeee
eee
eee
e
obj

mod
,,YYY
YYY
YYY
YY
John NOUN Mary NOUN
mod 
yesterday NOUN
beautiful ADJ
Source: it rained when beautiful Mary was seen by John yester-
day
Derived: it rained when John saw beautiful Mary yesterday
(a) Application of passive to active transformation
L
V VERB
obj
ssfff
ff
ff
ff
f
be 
by
++XX
XX
XX
XX
XX
R
V VERB
subj
ssfff
ff
ff
ff
f obj
++XX
XX
XX
XX
XX
N1 NOUN be VERB by PREP
pcomp?n

N2 NOUN N1 NOUN
N2 NOUN
(b) Passive to active transformation (substitution rule). The dotted arc represents alignment.
Figure 1: Application of inference rules. POS and relation labels are based on Minipar (Lin, 1998b)
If a complete proof is found (h was generated), the
prover concludes that entailment holds. Otherwise,
entailment is determined by comparing the minimal
cost found during the proof search to some threshold
?.
3 Proof System
Like logic-based systems, our proof system consists
of propositions (t, h, and intermediate premises),
and inference (entailment) rules, which derive new
propositions from previously established ones.
3.1 Propositions
Propositions are represented as dependency trees,
where nodes represent words, and hold a set of fea-
tures and their values. In our representation these
features include the word lemma and part-of-speech,
and additional features that may be added during the
proof process. Edges are annotated with dependency
relations.
3.2 Inference Rules
At each step of the proof an inference rule gener-
ates a derived tree d from a source tree s. A rule
is primarily composed of two templates, termed left-
hand-side (L), and right-hand-side (R). Templates
are dependency subtrees which may contain vari-
ables. Figure 1(b) shows an inference rule, where
V , N1 and N2 are common variables. L specifies
the subtree of s to be modified, and R specifies the
new generated subtree. Rule application consists of
the following steps:
L matching The prover first tries to match L in s.
L is matched in s if there exists a one-to-one node
mapping function f from L to s, such that: (i) For
each node u, f(u) has the same features and feature
values as u. Variables match any lemma value in
f(u). (ii) For each edge u ? v in L, there is an
edge f(u) ? f(v) in s, with the same dependency
relation. If matching fails, the rule is not applicable
to s. Otherwise, successful matching induces vari-
132
LROOT
i 
R
ROOT
i 
V1 VERB
wha 
V2 VERB
when ADJ
i 
V2 VERB
Figure 2: Temporal clausal modifier extraction (in-
troduction rule)
able binding b(X), for each variableX in L, defined
as the full subtree rooted in f(X) if X is a leaf, or
f(X) alone otherwise. We denote by l the subtree
in s to which L was mapped (as illustrated in bold
in Figure 1(a), left tree).
R instantiation An instantiation of R, which we
denote r, is generated in two steps: (i) creating a
copy of R; (ii) replacing each variable X with a
copy of its binding b(X) (as set during L matching).
In our example this results in the subtree John saw
beautiful Mary.
Alignment copying the alignment relation be-
tween pairs of nodes in L and R specifies which
modifiers in l that are not part of the rule structure
need to be copied to the generated tree r. Formally,
for any two nodes u in l and v in r whose matching
nodes in L and R are aligned, we copy the daugh-
ter subtrees of u in s, which are not already part of
l, to become daughter subtrees of v in r. The bold
nodes in the right part of Figure 1(b) correspond to
r after alignment. yesterday was copied to r due to
the alignment of its parent verb node.
Derived tree generation by rule type Our for-
malism has two methods for generating the derived
tree: substitution and introduction, as specified by
the rule type. With substitution rules, the derived
tree d is obtained by making a local modification to
the source tree s. Except for this modification s and
d are identical (a typical example is a lexical rule,
such as buy ? purchase). For this type, d is formed
by copying s while replacing l (and the descendants
of l?s nodes) with r. This is the case for the passive
rule. The right part of Figure 1(a) shows the derived
tree for the passive rule application. By contrast, in-
troduction rules are used to make inferences from a
subtree of s, while the other parts of s are ignored
and do not affect d. A typical example is inference
of a proposition embedded as a relative clause in s.
In this case the derived tree d is simply taken to be
r. Figure 2 presents such a rule that derives propo-
sitions embedded within temporal modifiers. Note
that the derived tree does not depend on the main
clause. Applying this rule to the right part of Figure
1(b) yields the proposition John saw beautiful Mary
yesterday.
3.3 Annotation Rules
Annotation rules add features to parse tree nodes,
and are used in our system to annotate negation and
modality. Annotation rules do not have an R. In-
stead, nodes of L may contain annotation features.
If L is matched in a tree then the annotations are
copied to the matched nodes. Annotation rules are
applied to t and to each inferred premise prior to
any entailment rule application and these features
may block inappropriate subsequent rule applica-
tions, such as for negated predicates.
4 Rules for Generic Linguistic Structures
Based on the above framework we have manually
created a rule base for generic linguistic phenomena.
4.1 Syntactic-Based Rules
These rules capture entailment inferences associ-
ated with common syntactic structures. They have
three major functions: (i) simplification and canon-
ization of the source tree (categories 6 and 7 in Ta-
ble 1); (ii) extracting embedded propositions (cate-
gories 1, 2, 3); (iii) inferring propositions from non-
propositional subtrees (category 4).
4.2 Polarity-Based Rules
Consider the following two examples:
John knows that Mary is here?Mary is here.
John believes that Mary is here;Mary is here.
Valid inference of propositions embedded as verb
complements depends on the verb properties, and
the polarity of the context in which the verb appears
(positive, negative, or unknown) (Nairn et al, 2006).
We extracted from the polarity lexicon of Nairn et
al. a list of verbs for which inference is allowed in
positive polarity context, and generated entailment
133
# Category Example: source Example: derived
1 Conjunctions Helena?s very experienced and has played a long
time on the tour.
? Helena has played a long time on the tour.
2 Clausal modi-
fiers
But celebrations were muted as many Iranians ob-
served a Shi?ite mourning month.
? Many Iranians observed a Shi?ite mourning
month.
3 Relative
clauses
The assailants fired six bullets at the car, which car-
ried Vladimir Skobtsov.
? The car carried Vladimir Skobtsov.
4 Appositives Frank Robinson, a one-time manager of the Indians,
has the distinction for the NL.
? Frank Robinson is a one-time manager of the
Indians.
5 Determiners The plaintiffs filed their lawsuit last year in U.S.
District Court in Miami.
? The plaintiffs filed a lawsuit last year in U.S.
District Court in Miami.
6 Passive We have been approached by the investment banker. ? The investment banker approached us.
7 Genitive
modifier
Malaysia?s crude palm oil output is estimated to
have risen by up to six percent.
? The crude palm oil output of Malasia is esti-
mated to have risen by up to six percent.
8 Polarity Yadav was forced to resign. ? Yadav resigned.
9 Negation,
modality
What we?ve never seen is actual costs come
down.
What we?ve never seen is actual costs come down.
(;What we?ve seen is actual costs come down.)
Table 1: Summary of rule base for generic linguistic structures.
rules for these verbs (category 8). The list was com-
plemented with a few reporting verbs, such as say
and announce, assuming that in the news domain the
speaker is usually considered reliable.
4.3 Negation and Modality Annotation Rules
We use annotation rules to mark negation and
modality of predicates (mainly verbs), based on their
descendent modifiers. Category 9 in Table 1 illus-
trates a negation rule, annotating the verb seen for
negation due to the presence of never.
4.4 Generic Default Rules
Generic default rules are used to define default be-
havior in situations where no case-by-case rules are
available. We used one default rule that allows re-
moval of any modifiers from nodes.
5 Lexical-based Rules
These rules have open class lexical components, and
consequently are numerous compared to the generic
rules described in section 4. Such rules are acquired
either lexicographically or automatically.
The rules described in the section 4 are applied
whenever their L template is matched in the source
premise. For high fan-out rules such as lexical-based
rules (e.g. words with many possible synonyms),
this may drastically increase the size of the search
space. Therefore, the rules described below are ap-
plied only if L is matched in the source premise p
and R is matched in h.
5.1 Lexical Rules
Lexical entailment rules, such as ?steal ? take? and
?Britain ? UK? were created based on WordNet
(Fellbaum, 1998). Given p and h, a lexical rule
lemmap ? lemmah may be applied if lemmap
and lemmah are lemmas of open-class words ap-
pearing in p and h respectively, and there is a path
from lemmah to lemmap in the WordNet ontology,
through synonym and hyponym relations.
5.2 Lexical-Syntactic Rules
In order to find lexical-syntactic paraphrases and en-
tailment rules, such as ?X strike Y ? X hit Y ? and
?X buy Y ?X own Y ? that would bridge between p
and h, we applied the DIRT algorithm (Lin and Pan-
tel, 2001) to the first CD of the Reuters RCV1 cor-
pus1. DIRT does not identify the entailment direc-
tion, hence we assumed bi-directional entailment.
We calculate off-line only the feature vector of ev-
ery template found in the corpus, where each path
between head nouns is considered a template in-
stance. Then, given a premise p, we first mark all
lexical noun alignments between p and h. Next, for
every pair of alignments we extract the path between
the two nouns in p, labeled pathp, and the corre-
sponding path between the aligned nouns in h, la-
beled pathh. We then on-the-fly test whether there
is a rule ?pathp ? pathh? by extracting the stored
feature vectors of pathp and pathh and measuring
1http://about.reuters.com/researchandstandards/corpus/
134
their similarity. If the score exceeds a given thresh-
old2, we apply the rule to p.
Another enhancement that we added to DIRT is
template canonization. At learning time, we trans-
form every template identified in the corpus into
its canonized form3 using a set of morpho-syntactic
rules, similar to the ones described in Section 4. In
addition, we apply nominalization rules such as ?ac-
quisition of Y by X ? X acquire Y ?, which trans-
form a nominal template into its related verbal form.
We automatically generate these rules (Ron, 2006),
based on Nomlex (Macleod et al, 1998).
At inference time, before retrieving feature vec-
tors, we canonize pathp into pathcp and pathh into
pathch. We then assess the rule ?path
c
p ? path
c
h?,
and if valid, we apply the rule ?pathp ? pathh? to
p. In order to ensure the validity of the implicature
?pathp ? pathcp ? path
c
h ? pathh?, we canonize
pathp using the same rule set used at learning time,
but we apply only bi-directional rules to pathh (e.g.
conjunct heads are not removed from pathh).
6 Approximate Matching
As mentioned in section 2, approximate matching
is incorporated into our system via a cost function,
which estimates the likelihood of h being entailed
from a given premise p. Our cost function C(p, h) is
a linear combination of two measures: lexical cost,
Clex(p, h) and lexical-syntactic cost ClexSyn(p, h):
C(p, h) = ?ClexSyn(p, h)+ (1??)Clex(p, h) (1)
Let m?() be a (possibly partial) 1-1 mapping of the
nodes of h to the nodes of p, where each node
is mapped to a node with the same lemma, such
that the number of matched edges is maximized.
An edge u ? v in h is matched in p if m?(u)
and m?(v) are both defined, and there is an edge
m?(u) ? m?(v) in p, with the same dependency rela-
tion. ClexSyn(p, h) is then defined as the percentage
of unmatched edges in h.
Similarly, Clex(p, h) is the percentage of un-
matched lemmas in h, considering only open-class
words, defined as:
Clex(p, h) = 1?
?
l?h Score(l)
#OpenClassWords(h)
(2)
2We set the threshold to 0.01
3The active verbal form with direct modifiers
where Score(l) is 1 if it appears in p, or if it is
a derivation of a word in p (according to Word-
Net). Otherwise, Score(l) is the maximal Lin
dependency-based similarity score between l and the
lemmas of p (Lin, 1998a) (synonyms and hyper-
nyms/hyponyms are handled by the lexical rules).
7 System Implementation
Deriving the initial propositions t and h from the in-
put text fragments consists of the following steps:
(i) Anaphora resolution, using the MARS system
(Mitkov et al, 2002). Each anaphor was replaced by
its antecedent. (ii) Sentence splitting, using mxter-
minator (Reynar and Ratnaparkhi, 1997). (iii) De-
pendency parsing, using Minipar (Lin, 1998b).
The proof search is implemented as a depth-first
search, with maximal depth (i.e. proof length) of
4. If the text contains more than one sentence, the
prover aims to prove h from each of the parsed sen-
tences, and entailment is determined based on the
minimal cost. Thus, the only cross-sentence infor-
mation that is considered is via anaphora resolution.
8 Evaluation
Full (run1) Lexical (run2)
Dataset Task Acc. Avg.P Acc. Avg.P
Test IE 0.4950 0.5021 0.5000 0.5379
Official IR 0.6600 0.6174 0.6450 0.6539
Results QA 0.7050 0.8085 0.6600 0.8075
SUM 0.5850 0.6200 0.5300 0.5927
All 0.6112 0.6118 0.5837 0.6093
Dev. All 0.6443 0.6699 0.6143 0.6559
Table 2: Empirical evaluation - results.
The results for our submitted runs are listed in Ta-
ble 2, including per-task scores. run1 is our full sys-
tem, denoted F . It was tuned on a random sample
of 100 sentences from the development set, result-
ing in ? = 0.6 and ? = 0.6242 (entailment thresh-
old). run2 is a lexical configuration, denoted L, in
which ? = 0 (lexical cost only), ? = 0.2375 and
the only inference rules used were WordNet Lexical
rules. We found that the higher accuracy achieved
by F as compared to L might have been merely due
to a lucky choice of threshold. Setting the threshold
to its optimal value with respect to the test set re-
sulted in an accuracy of 62.4% for F , and 62.9% for
135
L. This is also hinted by the very close average pre-
cision scores for both systems, which do not depend
on the threshold. The last row in the table shows
the results obtained for 7/8 of the development set
that was not used for tuning, denoted Dev, using the
same parameter settings. Again, F performs bet-
ter than L. F is still better when using an optimal
threshold (which increases accuracy up to 65.3% for
F and 63.9% for L. Overall, F does not show yet a
consistent significant improvement over L.
Initial analysis of the results (based on Dev) sug-
gests that the coverage of the current rules is still
rather low. Without approximate matching (h must
be fully proved using the entailment rules) the re-
call is only 4.3%, although the precision (92%) is
encouraging. Lexical-syntactic rules were applied
in about 3% of the attempted proofs, and in most
cases involved only morpho-syntactic canonization,
with no lexical variation. As a result, entailment was
determined mainly by the cost function. Entailment
rules managed to reduce the cost in about 30% of the
attempted proofs.
We have qualitatively analyzed a subset of false
negative cases, to determine whether failure to com-
plete the proof is due to deficient components of
the system or due to higher linguistic and knowl-
edge levels. For each pair, we assessed the reasoning
steps a successful derivation of h from t would take.
We classified each pair according to the most de-
manding type of reasoning step it would require. We
allowed rules that are presently unavailable in our
system, as long as they are similar in power to those
that are currently available. We found that while
the single dominant cause for proof failure is lack
of world knowledge, e.g. the king?s son is a mem-
ber of the royal family, the combination of miss-
ing lexical-syntactic rules and parser failures equally
contributed to proof failure.
9 Conclusion
We defined a novel framework for semantic infer-
ence at the lexical-syntactic level, which allows a
unified representation of a wide variety of inference
knowledge. In order to reach reasonable recall on
RTE data, we found that we must scale our rule ac-
quisition, mainly by improving methods for auto-
matic rule learning.
Acknowledgments
We are grateful to Cleo Condoravdi for making the
polarity lexicon developed at PARC available for
this research. We also wish to thank Ruslan Mitkov,
Richard Evans, and Viktor Pekar from University of
Wolverhampton for running the MARS system for
us. This work was partially supported by ISF grant
1095/05, the IST Programme of the European Com-
munity under the PASCAL Network of Excellence
IST-2002-506778, the Israel Internet Association
(ISOC-IL) grant 9022 and the ITC-irst/University of
Haifa collaboration.
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In AAAI (to appear).
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Language, Speech and Com-
munication. MIT Press.
Dekang Lin and Patrik Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 4(7):343?360.
Dekang Lin. 1998a. Automatic retrieval and clustering
of similar words. In Proceedings of COLING/ACL.
Dekang Lin. 1998b. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalua-
tion of Parsing Systems at LREC.
C. Macleod, R. Grishman, A. Meyers, L. Barrett, and
R. Reeves. 1998. Nomlex: A lexicon of nominal-
izations. In EURALEX.
Ruslan Mitkov, Richard Evans, and Constantin Orasan.
2002. A new, fully automatic version of Mitkov?s
knowledge-poor pronoun resolution method. In Pro-
ceedings of CICLing.
Rowan Nairn, Cleo Condoravdi, and Lauri Karttunen.
2006. Computing relative polarity for textual infer-
ence. In Proceedings of ICoS-5.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proceedings of ANLP.
Tal Ron. 2006. Generating entailment rules based on
online lexical resources. Master?s thesis, Computer
Science Department, Bar-Ilan University, Ramat-Gan,
Israel.
136
Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 27?35,
Suntec, Singapore, 6 August 2009.
c
?2009 ACL and AFNLP
Augmenting WordNet-based Inference with Argument Mapping
Idan Szpektor
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
szpekti@cs.biu.ac.il
Ido Dagan
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
dagan@cs.biu.ac.il
Abstract
WordNet is a useful resource for lexi-
cal inference in applications. Inference
over predicates, however, often requires
a change in argument positions, which
is not specified in WordNet. We pro-
pose a novel framework for augmenting
WordNet-based inferences over predicates
with corresponding argument mappings.
We further present a concrete implementa-
tion of this framework, which yields sub-
stantial improvement to WordNet-based
inference.
1 Introduction
WordNet (Miller, 1995), a manually constructed
lexical database, is probably the mostly used re-
source for lexical inference in NLP tasks, such
as Question Answering (QA), Information Extrac-
tion (IE), Information Retrieval and Textual En-
tailment (RTE) (Moldovan and Mihalcea, 2000;
Pasca and Harabagiu, 2001; Bar-Haim et al, 2006;
Giampiccolo et al, 2007).
Inference using WordNet typically involves lex-
ical substitutions for words in text based on
WordNet relations, a process known as lexical
chains (Barzilay and Elhadad, 1997; Moldovan
and Novischi, 2002). For example, the an-
swer to ?From which country was Louisiana ac-
quired?? can be inferred from ?The United
States bought up Louisiana from France? using the
chains ?France ? European country ? country?
and ?buy up? buy? acquire?.
When performing inference between predicates
there is an additional complexity on top of lex-
ical substitution: the syntactic relationship be-
tween the predicate and its arguments may change
as well. For example, ?X buy Y for Z ?
X pay Z for Y ?.
Currently, argument mappings are not specified
for WordNet?s relations. Therefore, correct Word-
Net inference chains over predicates can be per-
formed only for substitution relations (mainly syn-
onyms and hypernyms, e.g. ?buy? acquire?), for
which argument positions do not change. Other
relation types that may be used for inference can-
not be utilized when the predicate arguments need
to be traced as well. Examples include the Word-
Net ?entailment? relation (e.g. ?buy ? pay?) and
relations between morphologically derived words
(e.g. ?acquire? acquisition?).
Our goal is to obtain argument mappings for
WordNet relations that are often used for infer-
ence. In this paper we address several prominent
WordNet relations, including verb-noun deriva-
tions and the verb-verb ?entailment? and ?cause?
relations, referred henceforth as inferential rela-
tions. Under the Textual Entailment paradigm,
all these relations can be viewed as express-
ing entailment. Accordingly, we propose a
novel framework, called Argument-mapped Word-
Net (AmWN), that represents argument map-
pings for inferential relations as entailment rules.
These rules are augmented with subcategorization
frames and functional roles, which are proposed
as a generally-needed extension for predicative en-
tailment rules.
Following our new representation scheme, we
present a concrete implementation of AmWN for
a large number of WordNet?s relations. The map-
pings for these relations are populated by com-
bining information from manual and corpus-based
resources, which provides broader coverage com-
pared to prior work and more accurate mappings.
Table 1 shows typical inference chains obtained
27
Rule Chains
shopping:n ofX
obj
? buying:n ofX
obj
? buy:vX
obj
? pay:v forX
mod
vote:v onX
mod
? decide:v onX
mod
? debate:vX
obj
X
obj
?s sentence:n ? condemn:vX
obj
? convict:vX
obj
?X
obj
?s conviction:n
X
ind?obj
?s teacher:n ? teach:v toX
ind?obj
?X
subj
learn:v
Table 1: Examples for inference chains obtained using AmWN. Arguments are subscripted with func-
tional roles, e.g. subject (subj) and indirect-object (ind-obj). For brevity, predicate frames are omitted.
using our implementation.
To further improve WordNet-based inference
for NLP applications, we address the phenom-
ena of rare WordNet senses. Rules generated for
such senses might hurt inference accuracy since
they are often applied incorrectly to texts when
matched against inappropriate, but more frequent
senses of the rule words. Since word sense disam-
biguation (WSD) solutions are typically not suf-
ficiently robust yet, most applications do not cur-
rently apply WSD methods. Hence, we propose
to optionally filter out such rules using a novel
corpus-based validation algorithm.
We tested both WordNet and AmWN on a test
set derived from a standard IE benchmark. The
results show that AmWN substantially improves
WordNet-based inference in terms of both recall
and precision
1
.
2 Argument-Mapping Entailment Rules
In our framework we represent argument map-
pings for inferential relations between predicates
through an extension of entailment rules over syn-
tactic representations. As defined in earlier works,
an entailment rule specifies an inference rela-
tion between an entailing template and an en-
tailed template, where templates are parse sub-
trees with argument variables (Szpektor and Da-
gan, 2008). For example, ?X
subj
??? buy
obj
??? Y
? ?X
subj
??? pay
prep?for
??????? Y ?.
When a rule is applied to a text, a new conse-
quent is inferred by instantiating the entailed tem-
plate variables with the argument instantiations of
the entailing template in the text. In our example,
?IBM paid for Cognos? can be inferred from ?IBM
bought Cognos?. This way, the syntactic structure
of the rule templates specifies the required argu-
ment positions for correct argument mapping.
However, representing entailment rule structure
only by syntactic argument positions is insufficient
for predicative rules. Correct argument mapping
1
We plan to make our AmWN publicly available.
depends also on the specific syntactic functional
roles of the arguments (subject, object etc.) and on
the suitable subcategorization frame (frame) for
the predicate mention - a set of functional roles
that a predicate may occur with. For example,
?X?s buyout ? buy X? is incorrectly applied to
?IBM?s buyout of Cognos? if roles are ignored,
since ?IBM? plays the subject role while X needs
to be an object.
Seeking to address this issue, we were inspired
by the Nomlex database (Macleod et al, 1998)
(see Section 3.2.1) and explicitly specify argu-
ment mapping for each frame and functional role.
As in Nomlex, we avoid the use of semantic
roles and stick to the syntactic level, augment-
ing the representation of templates with: (a) a
syntactic functional role for each argument; (b)
the valid predicate frame for this template men-
tions. We note that such functional roles typically
coincide with dependency relations of the verbal
form. A rule example is ?X
subj
break
{intrans}
?
damage
{trans}
X
obj
?
2
. More examples are shown
in Table 1.
Unlike Nomlex records, our templates can be
partial: they may contain only some of the possi-
ble predicate arguments, e.g. ?buy
{trans}
X
obj
?,
where the subject, included in the frame, is omit-
ted. Partial templates are necessary for matching
predicate occurrences that include only some of
the possible arguments, as in ?Cognos was bought
yesterday?. Additionally, some resources, such as
automatic rule learning methods (Lin and Pantel,
2001; Sekine, 2005), can provide only partial ar-
gument information, and we would want to repre-
sent such knowledge as well.
In our framework we follow (Szpektor and Da-
gan, 2008) and use only rules between unary tem-
plates, containing a single argument. Such tem-
plates can describe any argument mapping by de-
2
Functional roles are denoted by subscripts of the argu-
ments and frames by subscripts of the predicate. We short-
hand trans for transitive frame {subject, object} and intrans
for intransitive {subject}. For brevity, we will not show all
template information when examples are self explanatory.
28
composing templates with several arguments into
unary ones, while preserving the specification of
the subcategorization frame.
To apply a rule, the entailing template must be
first matched in the text, which includes match-
ing the template?s syntactic dependency structure,
functional roles, and frame. Such procedure re-
quires texts to be annotated with these types of in-
formation. This can be reasonably performed with
existing tools and resources, as described for our
own text processing in Section 4.
Explicitly matching frames and functional roles
in rules avoids incorrect rule applications. For ex-
ample, ?X
obj
?s buyout? buy X
obj
? would be ap-
plied only to ?Cognos?s buyout by IBM? follow-
ing proper role annotation of the text, but not to
?IBM?s buyout of Cognos?. As another example,
?X
subj
break
{intrans}
? damage
{trans}
X
obj
?
would be applied only to the intransitive occur-
rence of ?break?, e.g. ?The vase broke?, but not
to ?John broke the vase?.
Ambiguous cases may occur during annotation.
For example, the role of ?John? in ?John?s invita-
tion was well intended? could be either subject or
object. Such recognized ambiguities should be left
unannotated, blocking incorrect rule application.
3 Argument Mapping for WordNet
Following our extension of entailment rules, we
present Argument-mapped WordNet (AmWN), a
framework for extendingWordNet?s inferential re-
lations with argument mapping at the syntactic
representation level.
3.1 Argument Mapping Representation
The AmWN structure follows that of WordNet: a
directed graph whose nodes are WordNet synsets
and edges are relations between synsets. Since
we focus on entailment between predicates, we
include only predicative synsets: all verb synsets
and noun synsets identified as predicates (see Sec-
tion 3.2). In addition, only WordNet relations that
correspond to some type of entailment are consid-
ered, as detailed in Section 3.2.
In our framework, different subcategorization
frames are treated as having different ?meanings?,
since different frames may correspond to differ-
ent entailment rules. Each WordNet synset is split
into several nodes, one for each of its frames. We
take frame descriptions for verbs from WordNet
3
.
3
We also tried using VerbNet (Kipper et al, 2000), with-
Figure 1: A description of ?buy/purchase X ?
pay for X? as a mapping edge in AmWN.
Since WordNet does not provide frames for noun
predicates, these are taken from Nomlex-plus (see
Section 3.2).
There are two types of graph edges that rep-
resent entailment rules between nodes: mapping
edges and substitution edges. Mapping edges
specify entailment rules that require argument
mapping, where the entailing and entailed tem-
plate predicates are replaced by synsets. Thus, an
edge represents all rules between entailing and en-
tailed synset members, as in Figure 1.
Substitution edges connect pairs of predicates,
of the same part-of-speech, which preserve argu-
ment positions in inference. This is analogous to
how WordNet may be currently used for inference
via the synonym and hypernym relations. Unlike
WordNet, substitution edges in AmWN may con-
nect only nodes that have the same subcategoriza-
tion frame.
AmWN is utilized by generating rule chains for
a given input unary template. First, starting nodes
that match the input predicate are selected. Then,
rules are generated by traversing either incoming
or outgoing graph edges transitively, depending
on the entailment direction requested. Specific
synset-ids, if known, may also be added to the in-
put to constrain the relevant starting nodes for the
input predicate. Table 1 shows examples of rule
chains from AmWN.
3.2 Argument Mapping Population
After defining the AmWN representation, we next
describe our implementation of AmWN. We first
populate the AmWN graph with substitution edges
for WordNet?s hypernyms and synonyms (as self
edges), e.g. ?buy ? purchase? and ?buy ? ac-
quire?. The following subsections describe how
mapping edges are created based on various man-
ual and corpus-based information resources.
3.2.1 Nominalization Relations
The relation between a verb and its nominaliza-
tions, e.g. between ?employ? and ?employment?,
out any current performance improvement.
29
:ORTH "employment"
:VERB "employ"
:VERB-SUBC ((NOM-NP
:SUBJECT ((DET-POSS)
(N-N-MOD)
(PP :PVAL ("by")))
:OBJECT ((DET-POSS)
(PP :PVAL ("of")))
Figure 2: Part of the employment Nomlex entry,
describing the possible syntactic dependency posi-
tions for each role of the transitive frame. It states,
for example, that the verbal ?object? role can be
mapped to employment either as a possessive or as
the complement of the preposition ?of?.
is described in WordNet by the derivationally re-
lated relation. To add argument mappings for
these relations we utilize Nomlex-plus (Meyers
et al, 2004), a database of around 5000 English
nominalizations. Nomlex specifies for each ver-
bal subcategorization frame of each nominaliza-
tion how its argument positions are mapped to
functional roles of related verbs.
For each Nomlex entry, we extract all possible
argument mappings between the verbal and nom-
inal forms, as well as between different argument
realizations of the noun. For example, the map-
pings ?X
obj
?s employment ? employ X
obj
? and
?X
obj
?s employment? employment of X
obj
? are
derived from the entry in Figure 2.
The major challenge in integrating Nomlex and
WordNet is to identify for each Nomlex noun
which WordNet synsets describe its predicative
meanings. For example, one synset of ?acquisi-
tion? that is derivationally related to ?acquire? is
not predicative: ?an ability that has been acquired
by training?. We mark noun synsets as predicative
if they are (transitive) hyponyms of the act high-
level synset.
Once predicative synsets are identified, we cre-
ate, for each synset, a node for each subcate-
gorization frame of its noun members, as found
in Nomlex-plus. In some nodes not all original
synset members are retained, since not all mem-
bers share all their frames. Mapping edges are
then added between nodes that have the same
frame. We add both noun-verb edges and noun
self-edges that map different realizations of the
same functional role (e.g. ?X
obj
?s employment?
employment of X
obj
?).
As rich as Nomlex-plus is, it still does not in-
clude all nominalizations. For example, the nouns
Lexical Relation Extracted Mappings
buy ? pay
buy forX ? payX
X buy ?X pay
divorce ? marry
divorce fromX ? marryX
divorce fromX ?X marry
kill ? die
killX ?X die
kill amongX ?X die
breathe ? inhale
breatheX ? inhaleX
breathe inX ? inhaleX
remind ? remember
remindX ?X remember
remind ofX ? rememberX
teach ? learn
teachX ? learnX
teach toX ?X learn
give ? have
giveX ? haveX
give toX ?X have
Table 2: Some argument mappings for WordNet
verb-verb relations discovered by unary-DIRT.
?divorce? (related to the verb ?divorce?) and ?strik-
ing? are missing. WordNet has a much richer set
of nominalizations that we would like to use. To
do so, we inherit associated frames and argument
realizations for each nominalization synset from
its closest hypernym that does appear in Nomlex.
Thus, ?divorce? inherits its information from ?sep-
aration? and ?striking? inherits from ?hit?. A by-
product of this process is the automatic extension
of Nomlex-plus with 5100 new nominalization en-
tries, based on the inherited information
4
.
3.2.2 Verb-Verb Relations
There are two inferential relations between verbs
in WordNet that do not preserve argument posi-
tions: cause and entailment. Unlike for nomi-
nalizations, there is no broad-coverage manual re-
source of argument mapping for these relations.
Hence, we turn to unsupervised approaches that
learn entailment rules from corpus statistics.
Many algorithms were proposed for learning
entailment rules between templates from corpora
(Lin and Pantel, 2001; Szpektor et al, 2004;
Sekine, 2005), but typically with mediocre accu-
racy. However, we only search for rules between
verbs for which WordNet aleady indicates the ex-
istence of an entailment relation and are thus not
affected by rules that wrongly relate non-entailing
verbs. We acquired a rule-set containing the top
300 rules for every unary template in the Reuters
RCV1 corpus
5
by implementing the unary-DIRT
algorithm (Szpektor and Dagan, 2008), which was
shown to have relatively high recall compared to
other algorithms.
4
We plan making this extension publicly available as well.
5
http://about.reuters.com/researchandstandards/corpus/
30
To extract argument mappings, we identify all
AmWN node pairs whose synsets are related in
WordNet by a cause or an entailment relation.
For each pair, we look for unary-DIRT rules be-
tween any pair of members in the entailing and
entailed synsets. For example, the synset {buy,
purchase} entails {pay}, so we look for rules map-
ping either ?buy? pay? or ?purchase? pay?. Ta-
ble 2 presents examples for discovered mappings.
While unary-DIRT rules are not annotated with
functional roles, they can be derived straightfor-
wardly from the verbal dependency relations avail-
able in the rule?s templates. The obtained rules are
then added to AmWN as mapping edges.
We only search for rules that map a functional
role in the frame of one verb to any role for the
other verb. Focusing on frame elements avoids ex-
tracting mapping rules learned for adjuncts, which
tend to be of low precision.
3.3 Rule Filtering
In preliminary analysis we found two phenomena,
sense drifting and rare senses, which may reduce
the effectiveness of AmWN-based inference even
if each graph edge by itself, taken out of context, is
correct. To address these phenomena within prac-
tical inference we propose the following optional
methods for rule filtering.
Sense Drifting WordNet verbs typically have
a more fine-grained set of synsets than their re-
lated nominalizations. There are cases where sev-
eral verb synsets are related to the same nomi-
nal synset. Since entailment between a verb and
its nominalization is bidirectional, all such verb
synsets would end up entailing each other via the
nominal node.
Alas, some of these connected verb synsets rep-
resent quite different meanings, which results in
incorrect inferences. This problem, which we call
sense drifting, is demonstrated in Figure 3. To ad-
dress it, we constrain each rule generation chain
to include at most one verb-noun edge, which still
connects the noun and verb hierarchies.
Rare Senses Some word senses in WordNet are
rare. Thus, applying rules that correspond to such
senses yields many incorrect inferences, since
they are typically matched against other frequent
senses of the word. Such a rule is ?have X ? X
is born?, corresponding to a rare sense of ?have?.
WSD is a possible solution for this problem. How-
ever, most state-of-the-art IE, QA and RTE sys-
tems do not rely on WSD methods, which are cur-
rently not sufficiently robust.
To circumvent the rare sense problem, we in-
stead filter out such rules. Each AmWN rule is
validated against our unary-DIRT rule-set, which,
being corpus-based, contains mostly rules for fre-
quent senses. A rule is directly-validated if it is
in the corpus-based rule-set, or if it is a nominal-
verb rule which describes a reliable morpholog-
ical change for a predicate. The AmWN graph-
path that generated each rule is automatically ex-
amined. A rule is considered valid if there is a
sequence of directly-validated intermediate rules
along the path whose transitive chaining generates
the rule. Invalid rules are filtered out.
To illustrate, suppose the rule ?a? d? was gen-
erated by the chain ?a ? b ? c ? d?. It is valid
if there is a rule chain along the path that yields ?a
? d?, e.g. {?a? b?,?b? c?,?c? d?} or {?a? b?,
?b? d?}, whose rules are all directly-validated.
4 Experimental Setup
We follow here the experimental setup presented
in (Szpektor and Dagan, 2008), testing the gener-
ated rules on the ACE 2005 event dataset
6
. This
standard IE benchmark includes 33 types of event
predicates such as Injure, Sue and Divorce
7
. The
ACE guidelines specify for each event its possi-
ble arguments. For example, some of the Injure
event arguments are Agent and Victim. All event
mentions, including their instantiated arguments,
are annotated in a corpus collected from various
sources (newswire articles, blogs, etc.).
To utilize the ACE dataset for evaluating rule
applications, each ACE event predicate was rep-
resented by a set of unary seed templates, one for
each event argument. Example seed templates for
Injure are ?A injure? and ?injure V ?. Each event ar-
gument is mapped to the corresponding seed tem-
plate variable, e.g. ?Agent? to A and ?Victim? to V
in the above example.
We manually annotated each seed template with
a subcategorization frame and an argument func-
tional role, e.g. ?injure
{trans}
V
obj
?. We also in-
cluded relevant WordNet synset-ids, so only rules
fitting the target meaning of the event will be ex-
tracted. In this experiment, we focused only on
the core semantic arguments. Adjuncts (time and
6
http://projects.ldc.upenn.edu/ace/
7
Only 26 frequent event types that correspond to a unique
predicate were tested, following (Szpektor and Dagan, 2008).
31
Synset Members WordNet Gloss
(verb) collar, nail, apprehend, arrest, pick up, nab, cop take into custody
m
(noun) apprehension, arrest, catch, collar, pinch, the act of apprehending (especially apprehending
taking into custody a criminal)
m
(verb) get, catch, capture succeed in catching or seizing, especially after a chase
m
(noun) capture, seizure the act of taking of a person by force
m
(verb) seize take or capture by force
? (hypernym)
(verb) kidnap, nobble, abduct, snatch take away to an undisclosed location against their will
and usually in order to extract a ransom
Figure 3: A WordNet sense-drifting traversal, generating the incorrect inference ?kidnap? arrest?.
place) were ignored since they typically don?t re-
quire argument mapping, the main target for our
assessment.
The ACE corpus was dependency-parsed with
Minipar (Lin, 1998) and annotated with functional
roles and frames for each predicate mention. The
functional roles for a verb mention were taken di-
rectly from the corresponding dependency tree re-
lations. Its frame was chosen to be the largest
WordNet frame of that verb that matched the men-
tion?s roles.
Nominalization frames and functional roles
in the text were annotated using our extended
Nomlex-plus database. For each nominal mention,
we found the largest Nomlex frame whose syntac-
tic argument positions matched those of the men-
tion?s arguments. The arguments were then anno-
tated with the specified roles of the chosen frame.
Ambiguous cases, where the same argument posi-
tion could match multiple roles, were left unanno-
tated, as discussed in Section 2.
Argument mentions for events were found in
the annotated corpus by matching either the seed
templates or the templates entailing them in some
rules. The matching procedure follows the one de-
scribed in Section 2. Templates are matched us-
ing a syntactic matcher that handles simple syn-
tactic variations such as passive-form and con-
junctions. For example, ?wound
{trans}
V
obj
? injure
{trans}
V
obj
? was matched in the text
?Hagel
obj
was wounded
trans
in Vietnam?. A rule
application is considered correct if the matched ar-
gument is annotated in the corpus with the corre-
sponding ACE role.
We note that our system performance on the
ACE task as such is limited. First, WordNet does
not provide all types of needed rules. Second, the
system of our experimental setting is rather basic,
with limited matching capabilities and without a
WSD module. However, this test-set is still very
useful for relative comparison of WordNet and our
proposed AmWN.
5 Results and Analysis
We tested four different rule-set configurations:
a) only the seed templates, without any rules; b)
rules generated based on WordNet 3.0 without ar-
gument mapping, using only synonym and hyper-
nym relations; c) WordNet rules from (b), filtered
using our corpus-based validation method for rare
senses; d) rules generated from our AmWN.
Out of the 8953 non-substitutable inferential re-
lations that we identified in WordNet, our AmWN
implementation created mapping edges for 75% of
8325 Noun-Verb relations and 70% of 628 Verb-
Verb relations. Altogether 41549 mapping edges
between synset nodes were added. A manual er-
ror analysis of these mappings is provided in Sec-
tion 5.2.
Each configuration was evaluated for each ACE
event. We measured the percentage of correct ar-
gument mentions extracted out of all correct argu-
ment mentions annotated for the event (recall) and
out of all argument mentions extracted (precision),
and F1, their harmonic average. We report macro
averages over the 26 event types.
5.1 Results
Table 3 summarizes the results for the different
configurations. As expected, matching only the
seed templates yields the highest precision but
lowest recall. Using the standard WordNet config-
uration actually decreases overall F1 performance.
Though recall increases relatively by 30%, thanks
to WordNet expansions, F1 is penalized by a sharp
32
Configuration R (%) P (%) F1
No Rules 13.5 63.0 20.7
WordNet 17.5 35.3 18.5
WordNet with rule validation 16.5 46.9 20.4
AmWN 20.8 43.9 24.2
Table 3: Recall (R), Precision (P) and F1 results
for the different tested configurations.
relative drop in precision (by 56%). The main rea-
son for this decline is the application of rules in-
volving infrequent word senses, as elaborated in
Section 3.3.
When our rule validation approach is applied
to standard WordNet expansions, a much higher
precision is achieved with only a small decline in
recall. This shows that our corpus-based filtering
method manages to avoid many of the noisy rules
for rare senses, while maintaining those that are
frequently involved in inference.
Finally, our main result shows that adding ar-
gument mapping improves performance substan-
tially. AmWN achieves a much higher recall
than WordNet. Recall increases relatively by 26%
over validated WordNet, and by 54% over the
no-rules baseline. Furthermore, precision drops
only slightly, by 6%, compared to validated Word-
Net. This shows that argument mapping increases
WordNet?s graph connectivity, while our rule-
validation method maintains almost the same pre-
cision for many more generated rules. The im-
provement in overall F1 performance is statisti-
cally significant compared to all other configura-
tions, according to the two-sided Wilcoxon signed
rank test at the level of 0.01 (Wilcoxon, 1945).
5.2 Error Analysis
We manually analyzed the reasons for false pos-
itives (incorrect extractions) and false negatives
(missed extractions) of AmWN by sampling 300
extractions of each type.
From the false positives analysis (Table 4) we
see that practically all generated rules are correct
(99.4%), that is, they would be valid in some con-
texts. Almost all errors come frommatching errors
(including parse errors) and context mismatches,
due to our limited IE implementation. The only
two incorrect rules sampled were due to an in-
correct Nomlex entry and a WordNet synset that
should have been split into two separate senses.
Considering that correct extractions resulted, per
our analysis, from correct rules, the analysis of this
Reason % mentions
Context mismatch 57.2
Match error 33.6
Errors in gold-standard annotation 8.6
Incorrect Rule learned 0.6
Table 4: Distribution of reasons for false positives
(incorrect argument extractions).
Reason % mentions
Rule not learned 67.7
Match error 18.0
Discourse analysis needed 12.0
Argument is predicative 1.3
Errors in gold-standard annotation 1.0
Table 5: Distribution of reasons for false negatives
(missed argument mentions).
sample indicates that virtually all AmWN edges
that get utilized in practice are correct.
Context mismatches, which constitute the ma-
jority of errors (57.2%), occur when the entail-
ing template of a rule is matched in inappropriate
contexts. This occurs typically when the match is
against another sense of the predicate, or when an
argument is not of the requested type (e.g. ?The
Enron sentence? vs. ?A one month sentence?). In
future work, we plan to address this problem by
utilizing context-sensitive application of rules in
the spirit of (Szpektor et al, 2008).
Table 5 presents the false negatives analysis.
Most missed extractions are due to rules that were
not learned (67.7%). These mainly involve com-
plex templates (?file a lawsuit ? sue?) and infer-
ence rules that are not synonyms/hypernyms (?ex-
ecute ? sentence?), which are not widely anno-
tated inWordNet. From further analysis, we found
that 10% of these misses are due to rules that are
generated from AmWN but filtered out by one of
our filtering methods (Section 3.3).
12% of the arguments cannot be extracted by
rules alone, due to required discourse analysis,
while 18% of the mentions were missed due to in-
correct syntactic matching. By assuming correct
matches in these cases and avoiding rule filtering,
we can estimate the upper bound recall of the rule-
set for the ACE dataset to be 40%.
In conclusion, for better performance the sys-
tem should be augmented with context modeling
and better template matching. Additionally, other
rule-bases, e.g. DIRT (Lin and Pantel, 2001),
should be added to increase rule coverage.
33
Configuration R (%) P (%) F1
AmWN 20.8 43.9 24.2
No nominalization mappings 18.1 45.5 21.8
No verb-verb mappings 19.3 43.8 22.8
No rule validation 22.0 30.4 20.9
No sense drift blocking 22.5 37.4 21.7
Table 6: The Recall (R), Precision (P) and F1 re-
sults for ablation tests.
5.3 Component Analysis
Table 6 presents ablations tests that assess the
marginal contribution of each AmWN component.
Nominal-verb and verb-verb mappings contribute
to the graph connectivity, hence the recall reduc-
tion when they are removed.
Complementary to recall components, rule fil-
tering improves precision. When removing the
corpus-based rule-validation, recall increases rel-
atively by 6% but precision drops relatively by
30%, showing the benefit of noisy-rule filtering.
Allowing sense drifting hurts precision, a rela-
tive drop of 22%. Yet, recall increases relatively
by 8%, indicating that some verb synsets, con-
nected via a shared nominal, entail each other even
though they are not connected directly. For exam-
ple, ?foundX? createX? was generated only via
the shared nominal ?founding?. In future work, we
plan to apply AmWN to a coarse-grained set of
WordNet synsets (Palmer et al, 2007) as a possi-
ble solution to sense drifting.
6 Related Work
Several works attempt to extend WordNet with ad-
ditional lexical semantic information (Moldovan
and Rus, 2001; Snow et al, 2006; Suchanek et al,
2007; Clark et al, 2008). However, the only pre-
vious work we are aware of that enriches Word-
Net with argument mappings is (Novischi and
Moldovan, 2006). This work utilizes VerbNet?s
subcategorization frames to identify possible verb
arguments. Argument mapping is provided only
between verbs, ignoring relations between verbs
and nouns. Arguments are mapped based on the-
matic role names shared between frames of dif-
ferent verbs. However, the semantic interpretation
of thematic roles is generally inconsistent across
verbs (Lowe et al, 1997; Kaisser and Webber,
2007). Instead, we discover these mappings from
corpus statistics, offering an accurate approach (as
analyzed in Section 5.2).
A frame semantics approach for argument
mapping between predicates is proposed by the
FrameNet project (Baker et al, 1998). Currently,
FrameNet is the only resource for frame-semantic
argument mappings. However, it is manually con-
structed and currently covers much less predi-
cates and relations than WordNet. Furthermore,
frame-semantic parsers are less robust than syntac-
tic parsers, presently hindering the utilization of
this approach in applications (Burchardt and Pen-
nacchiotti, 2008).
Nomlex argument mapping patterns similar to
ours were derived for IE in (Meyers et al, 1998),
but they were not integrated with any additional
information, such as WordNet.
7 Conclusions
We presented Argument-mapped WordNet
(AmWN), a novel framework for augment-
ing WordNet with argument mappings at the
syntactic representation level. With AmWN,
non-substitutable WordNet relations can also
be utilized correctly, increasing the coverage of
WordNet-based inference. The standard entail-
ment rule representation is augmented in our
work with functional roles and subcategorization
frames, shown to be a feasible extension needed
for correct rule application in general.
Our implementation of AmWN populates
WordNet with mappings based on combining
manual and corpus-based resources. It covers a
broader range of relations compared to prior work
and yields more accurate mappings. We also in-
troduced a novel corpus-based validation mecha-
nism, avoiding rules for infrequent senses. Our
experiments show that AmWN substantially im-
proves standard WordNet-based inference.
In future work we plan to add mappings be-
tween verbs and adjectives and between different
frames of a verb. We also want to incorporate
resources for additional subcategorization frames,
such as VerbNet. Finally, we plan to enhance our
text annotation based on noun-compound disam-
biguation (Lapata and Lascarides, 2003).
Acknowledgements
This work was partially supported by the NEGEV
project (www.negev-initiative.org), the PASCAL-
2 Network of Excellence of the European Commu-
nity FP7-ICT-2007-1-216886, the FBK-irst/Bar-
Ilan University collaboration and the Israel Sci-
ence Foundation grant 1112/08.
34
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of ACL.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second pascal recognising tex-
tual entailment challenge. In Second PASCAL Chal-
lenge Workshop for Recognizing Textual Entailment.
Regina Barzilay and Michael Elhadad. 1997. Using
lexical chains for text summarization. In Proceed-
ings of ACL.
Aljoscha Burchardt and Marco Pennacchiotti. 2008.
Fate: a framenet-annotated corpus for textual entail-
ment. In Proceedings of LREC.
Peter Clark, Christiane Fellbaum, Jerry R. Hobbs, Phil
Harrison, William R. Murray, and John Thompson.
2008. Augmenting WordNet for Deep Understand-
ing of Text. In Proceedings of STEP 2008.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recogniz-
ing textual entailment challenge. In Proceedings of
the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing.
Michael Kaisser and Bonnie Webber. 2007. Question
answering based on semantic roles. In ACL 2007
Workshop on Deep Linguistic Processing.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon.
In Proceedings of AAAI.
Mirella Lapata and Alex Lascarides. 2003. Detect-
ing novel compounds: The role of distributional ev-
idence. In Proceedings of EACL.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Eval-
uation of Parsing Systems at LREC 1998, Granada,
Spain.
John B. Lowe, Collin F. Baker, and Charles J. Fillmore.
1997. A frame-semantic approach to semantic an-
notation. In Proceedings of the SIGLEX Workshop
on Tagging Text with Lexical Semantics: Why, What,
and How?
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. NOMLEX:
A Lexicon of Nominalizations. In Proceedings of
EURALEX.
AdamsMeyers, CatherineMacleod, Roman Yangarber,
Ralph Grishman, Leslie Barrett, and Ruth Reeves.
1998. Using nomlex to produce nominalization pat-
terns for information extraction. In Proceedings of
COLING.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekeley, Veronkia Zielinska, and Brian
Young. 2004. The Cross-Breeding of Dictionaries.
In Proceedings of LREC.
George A. Miller. 1995. Wordnet: A lexical database
for english. In Communications of the ACM.
Dan Moldovan and Rada Mihalcea. 2000. Using
wordnet and lexical operators to improve internet
searches. IEEE Internet Computing, 4(1):34?43.
Dan Moldovan and Adrian Novischi. 2002. Lexical
chains for question answering. In Proceedings of
COLING.
Dan Moldovan and Vasile Rus. 2001. Logic form
transformation of wordnet and its applicability to
question answering. In Proceedings of ACL.
Adrian Novischi and Dan Moldovan. 2006. Question
answering with lexical chains propagating verb ar-
guments. In Proceedings of ACL.
Martha Palmer, Hoa Trang Dang, and Christiane
Fellbaum. 2007. Making fine-grained and
coarse-grained sense distinctions, both manually
and automatically. Natural Language Engineering,
13(2):137?163.
Marius Pasca and Sanda Harabagiu. 2001. The in-
formative role of wordnet in open-domain question
answering. In Proceedings of Workshop on WordNet
and Other Lexical Resources: Applications, Exten-
sions and Customizations.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of ACL.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A core of semantic knowl-
edge - unifying wordnet and wikipedia. In Proceed-
ings of WWW2007.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
ture Coppola. 2004. Scaling web based acquisition
of entailment patterns. In Proceedings of EMNLP
2004.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In Pro-
ceedings of ACL.
Frank Wilcoxon. 1945. Individual comparisons by
ranking methods. Biometrics Bulletin, 1:80?83.
35
Scaling Web-based Acquisition of Entailment Relations
Idan Szpektor
idan@szpektor.net
?ITC-Irst, Via Sommarive, 18 (Povo) - 38050 Trento, Italy
?DIT - University of Trento, Via Sommarive, 14 (Povo) - 38050 Trento, Italy
?Department of Computer Science, Bar Ilan University - Ramat Gan 52900, Israel
Department of Computer Science, Tel Aviv University - Tel Aviv 69978, Israel
Hristo Tanev?
tanev@itc.it
Ido Dagan?
dagan@cs.biu.ac.il
Bonaventura Coppola??
coppolab@itc.it
Abstract
Paraphrase recognition is a critical step for nat-
ural language interpretation. Accordingly, many
NLP applications would benefit from high coverage
knowledge bases of paraphrases. However, the scal-
ability of state-of-the-art paraphrase acquisition ap-
proaches is still limited. We present a fully unsuper-
vised learning algorithm for Web-based extraction
of entailment relations, an extended model of para-
phrases. We focus on increased scalability and gen-
erality with respect to prior work, eventually aiming
at a full scale knowledge base. Our current imple-
mentation of the algorithm takes as its input a verb
lexicon and for each verb searches the Web for re-
lated syntactic entailment templates. Experiments
show promising results with respect to the ultimate
goal, achieving much better scalability than prior
Web-based methods.
1 Introduction
Modeling semantic variability in language has
drawn a lot of attention in recent years. Many ap-
plications like QA, IR, IE and Machine Translation
(Moldovan and Rus, 2001; Hermjakob et al, 2003;
Jacquemin, 1999) have to recognize that the same
meaning can be expressed in the text in a huge vari-
ety of surface forms. Substantial research has been
dedicated to acquiring paraphrase patterns, which
represent various forms in which a certain meaning
can be expressed.
Following (Dagan and Glickman, 2004) we ob-
serve that a somewhat more general notion needed
for applications is that of entailment relations (e.g.
(Moldovan and Rus, 2001)). These are directional
relations between two expressions, where the mean-
ing of one can be entailed from the meaning of the
other. For example ?X acquired Y? entails ?X owns
Y?. These relations provide a broad framework for
representing and recognizing semantic variability,
as proposed in (Dagan and Glickman, 2004). For
example, if a QA system has to answer the question
?Who owns Overture?? and the corpus includes the
phrase ?Yahoo acquired Overture?, the system can
use the known entailment relation to conclude that
this phrase really indicates the desired answer. More
examples of entailment relations, acquired by our
method, can be found in Table 1 (section 4).
To perform such inferences at a broad scale, ap-
plications need to possess a large knowledge base
(KB) of entailment patterns. We estimate such a
KB should contain from between a handful to a few
dozens of relations per meaning, which may sum
to a few hundred thousands of relations for a broad
domain, given that a typical lexicon includes tens of
thousands of words.
Our research goal is to approach unsupervised ac-
quisition of such a full scale KB. We focus on de-
veloping methods that acquire entailment relations
from the Web, the largest available resource. To
this end substantial improvements are needed in or-
der to promote scalability relative to current Web-
based approaches. In particular, we address two
major goals: reducing dramatically the complexity
of required auxiliary inputs, thus enabling to apply
the methods at larger scales, and generalizing the
types of structures that can be acquired. The algo-
rithms described in this paper were applied for ac-
quiring entailment relations for verb-based expres-
sions. They successfully discovered several rela-
tions on average per each randomly selected expres-
sion.
2 Background and Motivations
This section provides a qualitative view of prior
work, emphasizing the perspective of aiming at a
full-scale paraphrase resource. As there are still
no standard benchmarks, current quantitative results
are not comparable in a consistent way.
The major idea in paraphrase acquisition is often
to find linguistic structures, here termed templates,
that share the same anchors. Anchors are lexical
elements describing the context of a sentence. Tem-
plates that are extracted from different sentences
and connect the same anchors in these sentences,
are assumed to paraphrase each other. For example,
the sentences ?Yahoo bought Overture? and ?Yahoo
acquired Overture? share the anchors {X=Yahoo,
Y =Overture}, suggesting that the templates ?X buy
Y? and ?X acquire Y? paraphrase each other. Algo-
rithms for paraphrase acquisition address two prob-
lems: (a) finding matching anchors and (b) identify-
ing template structure, as reviewed in the next two
subsections.
2.1 Finding Matching Anchors
The prominent approach for paraphrase learning
searches sentences that share common sets of mul-
tiple anchors, assuming they describe roughly the
same fact or event. To facilitate finding many
matching sentences, highly redundant comparable
corpora have been used. These include multiple
translations of the same text (Barzilay and McKe-
own, 2001) and corresponding articles from multi-
ple news sources (Shinyama et al, 2002; Pang et
al., 2003; Barzilay and Lee, 2003). While facilitat-
ing accuracy, we assume that comparable corpora
cannot be a sole resource due to their limited avail-
ability.
Avoiding a comparable corpus, (Glickman and
Dagan, 2003) developed statistical methods that
match verb paraphrases within a regular corpus.
Their limited scale results, obtaining several hun-
dred verb paraphrases from a 15 million word cor-
pus, suggest that much larger corpora are required.
Naturally, the largest available corpus is the Web.
Since exhaustive processing of the Web is not feasi-
ble, (Duclaye et al, 2002) and (Ravichandran and
Hovy, 2002) attempted bootstrapping approaches,
which resemble the mutual bootstrapping method
for Information Extraction of (Riloff and Jones,
1999). These methods start with a provided known
set of anchors for a target meaning. For example,
the known anchor set {Mozart, 1756} is given as in-
put in order to find paraphrases for the template ?X
born in Y?. Web searching is then used to find occur-
rences of the input anchor set, resulting in new tem-
plates that are supposed to specify the same relation
as the original one (?born in?). These new templates
are then exploited to get new anchor sets, which
are subsequently processed as the initial {Mozart,
1756}. Eventually, the overall procedure results in
an iterative process able to induce templates from
anchor sets and vice versa.
The limitation of this approach is the requirement
for one input anchor set per target meaning. Prepar-
ing such input for all possible meanings in broad
domains would be a huge task. As will be explained
below, our method avoids this limitation by find-
ing all anchor sets automatically in an unsupervised
manner.
Finally, (Lin and Pantel, 2001) present a notably
different approach that relies on matching sepa-
rately single anchors. They limit the allowed struc-
ture of templates only to paths in dependency parses
connecting two anchors. The algorithm constructs
for each possible template two feature vectors, rep-
resenting its co-occurrence statistics with the two
anchors. Two templates with similar vectors are
suggested as paraphrases (termed inference rule).
Matching of single anchors relies on the gen-
eral distributional similarity principle and unlike the
other methods does not require redundancy of sets
of multiple anchors. Consequently, a much larger
number of paraphrases can be found in a regular
corpus. Lin and Pantel report experiments for 9
templates, in which their system extracted 10 cor-
rect inference rules on average per input template,
from 1GB of news data. Yet, this method also suf-
fers from certain limitations: (a) it identifies only
templates with pre-specified structures; (b) accuracy
seems more limited, due to the weaker notion of
similarity; and (c) coverage is limited to the scope
of an available corpus.
To conclude, several approaches exhaustively
process different types of corpora, obtaining vary-
ing scales of output. On the other hand, the Web is
a huge promising resource, but current Web-based
methods suffer serious scalability constraints.
2.2 Identifying Template Structure
Paraphrasing approaches learn different kinds of
template structures. Interesting algorithms are pre-
sented in (Pang et al, 2003; Barzilay and Lee,
2003). They learn linear patterns within similar con-
texts represented as finite state automata. Three
classes of syntactic template learning approaches
are presented in the literature: learning of predicate
argument templates (Yangarber et al, 2000), learn-
ing of syntactic chains (Lin and Pantel, 2001) and
learning of sub-trees (Sudo et al, 2003). The last
approach is the most general with respect to the tem-
plate form. However, its processing time increases
exponentially with the size of the templates.
As a conclusion, state of the art approaches still
learn templates of limited form and size, thus re-
stricting generality of the learning process.
3 The TE/ASE Acquisition Method
Motivated by prior experience, we identify two ma-
jor goals for scaling Web-based acquisition of en-
tailment relations: (a) Covering the broadest pos-
sible range of meanings, while requiring minimal
input and (b) Keeping template structures as gen-
eral as possible. To address the first goal we re-
quire as input only a phrasal lexicon of the rel-
evant domain (including single words and multi-
word expressions). Broad coverage lexicons are
widely available or may be constructed using known
term acquisition techniques, making it a feasible
and scalable input requirement. We then aim to
acquire entailment relations that include any of the
lexicon?s entries. The second goal is addressed by a
novel algorithm for extracting the most general tem-
plates being justified by the data.
For each lexicon entry, denoted a pivot, our
extraction method performs two phases: (a) ex-
tract promising anchor sets for that pivot (ASE,
Section 3.1), and (b) from sentences contain-
ing the anchor sets, extract templates for which
an entailment relation holds with the pivot (TE,
Section 3.2). Examples for verb pivots are:
?acquire?, ?fall to?, ?prevent? . We will use the pivot
?prevent? for examples through this section.
Before presenting the acquisition method we first
define its output. A template is a dependency parse-
tree fragment, with variable slots at some tree nodes
(e.g. ?X subj? prevent obj? Y? ). An entailment rela-
tion between two templates T1 and T2 holds if
the meaning of T2 can be inferred from the mean-
ing of T1 (or vice versa) in some contexts, but
not necessarily all, under the same variable instan-
tiation. For example, ?X subj? prevent obj? Y? entails
?X
subj
? reduce
obj
? Y risk? because the sentence ?as-
pirin reduces heart attack risk? can be inferred from
?aspirin prevents a first heart attack?. Our output
consists of pairs of templates for which an entail-
ment relation holds.
3.1 Anchor Set Extraction (ASE)
The goal of this phase is to find a substantial num-
ber of promising anchor sets for each pivot. A good
anchor-set should satisfy a proper balance between
specificity and generality. On one hand, an anchor
set should correspond to a sufficiently specific set-
ting, so that entailment would hold between its dif-
ferent occurrences. On the other hand, it should be
sufficiently frequent to appear with different entail-
ing templates.
Finding good anchor sets based on just the input
pivot is a hard task. Most methods identify good re-
peated anchors ?in retrospect?, that is after process-
ing a full corpus, while previous Web-based meth-
ods require at least one good anchor set as input.
Given our minimal input, we needed refined crite-
ria that identify a priori the relatively few promising
anchor sets within a sample of pivot occurrences.
ASE ALGORITHM STEPS:
For each pivot (a lexicon entry)
1. Create a pivot template, Tp
2. Construct a parsed sample corpus S for Tp:
(a) Retrieve an initial sample from the Web
(b) Identify associated phrases for the pivot
(c) Extend S using the associated phrases
3. Extract candidate anchor sets from S:
(a) Extract slot anchors
(b) Extract context anchors
4. Filter the candidate anchor sets:
(a) by absolute frequency
(b) by conditional pivot probability
Figure 1: Outline of the ASE algorithm.
The ASE algorithm (presented in Figure 1) per-
forms 4 main steps.
STEP (1) creates a complete template, called the
pivot template and denoted Tp, for the input pivot,
denoted P . Variable slots are added for the ma-
jor types of syntactic relations that interact with P ,
based on its syntactic type. These slots enable us to
later match Tp with other templates. For verbs, we
add slots for a subject and for an object or a modifier
(e.g. ?X subj? prevent obj? Y? ).
STEP (2) constructs a sample corpus, denoted S,
for the pivot template. STEP (2.A) utilizes a Web
search engine to initialize S by retrieving sentences
containing P . The sentences are parsed by the
MINIPAR dependency parser (Lin, 1998), keeping
only sentences that contain the complete syntactic
template Tp (with all the variables instantiated).
STEP (2.B) identifies phrases that are statistically
associated with Tp in S. We test all noun-phrases
in S , discarding phrases that are too common on
the Web (absolute frequency higher than a thresh-
old MAXPHRASEF), such as ?desire?. Then we se-
lect the N phrases with highest tf ?idf score1. These
phrases have a strong collocation relationship with
the pivot P and are likely to indicate topical (rather
than anecdotal) occurrences of P . For example, the
phrases ?patient? and ?American Dental Associa-
tion?, which indicate contexts of preventing health
problems, were selected for the pivot ?prevent?. Fi-
1Here, tf ?idf = freqS(X) ? log
(
N
freqW (X)
)
where freqS(X) is the number of occurrences in S containing
X , N is the total number of Web documents, and freqW (X)
is the number of Web documents containing X .
nally, STEP (2.C) expands S by querying the Web
with the both P and each of the associated phrases,
adding the retrieved sentences to S as in step (2.a).
STEP (3) extracts candidate anchor sets for Tp.
From each sentence in S we try to generate one can-
didate set, containing noun phrases whose Web fre-
quency is lower than MAXPHRASEF. STEP (3.A)
extracts slot anchors ? phrases that instantiate the
slot variables of Tp. Each anchor is marked
with the corresponding slot. For example, the
anchors {antibioticssubj? , miscarriage obj?} were ex-
tracted from the sentence ?antibiotics in pregnancy
prevent miscarriage?.
STEP (3.B) tries to extend each candidate set with
one additional context anchor, in order to improve
its specificity. This anchor is chosen as the highest
tf ?idf scoring phrase in the sentence, if it exists. In
the previous example, ?pregnancy? is selected.
STEP (4) filters out bad candidate anchor sets by
two different criteria. STEP (4.A) maintains only
candidates with absolute Web frequency within a
threshold range [MINSETF, MAXSETF], to guaran-
tee an appropriate specificity-generality level. STEP
(4.B) guarantees sufficient (directional) association
between the candidate anchor set c and Tp, by esti-
mating
Prob(Tp|c) ?
freqW (P ? c)
freqW (c)
where freqW is Web frequency and P is the pivot.
We maintain only candidates for which this prob-
ability falls within a threshold range [SETMINP,
SETMAXP]. Higher probability often corresponds
to a strong linguistic collocation between the
candidate and Tp, without any semantic entail-
ment. Lower probability indicates coincidental co-
occurrence, without a consistent semantic relation.
The remaining candidates in S become the in-
put anchor-sets for the template extraction phase,
for example, {Aspirinsubj? , heart attackobj?} for ?pre-
vent?.
3.2 Template Extraction (TE)
The Template Extraction algorithm accepts as its in-
put a list of anchor sets extracted from ASE for each
pivot template. Then, TE generates a set of syntactic
templates which are supposed to maintain an entail-
ment relationship with the initial pivot template. TE
performs three main steps, described in the follow-
ing subsections:
1. Acquisition of a sample corpus from the Web.
2. Extraction of maximal most general templates
from that corpus.
3. Post-processing and final ranking of extracted
templates.
3.2.1 Acquisition of a sample corpus from the
Web
For each input anchor set, TE acquires from the
Web a sample corpus of sentences containing it.
For example, a sentence from the sample corpus
for {aspirin, heart attack} is: ?Aspirin stops heart
attack??. All of the sample sentences are then
parsed with MINIPAR (Lin, 1998), which gener-
ates from each sentence a syntactic directed acyclic
graph (DAG) representing the dependency structure
of the sentence. Each vertex in this graph is labeled
with a word and some morphological information;
each graph edge is labeled with the syntactic rela-
tion between the words it connects.
TE then substitutes each slot anchor (see section
3.1) in the parse graphs with its corresponding slot
variable. Therefore, ?Aspirin stops heart attack??
will be transformed into ?X stop Y?. This way all
the anchors for a certain slot are unified under the
same variable name in all sentences. The parsed
sentences related to all of the anchor sets are sub-
sequently merged into a single set of parse graphs
S = {P1, P2, . . . , Pn} (see P1 and P2 in Figure 2).
3.2.2 Extraction of maximal most general
templates
The core of TE is a General Structure Learning al-
gorithm (GSL ) that is applied to the set of parse
graphs S resulting from the previous step. GSL
extracts single-rooted syntactic DAGs, which are
named spanning templates since they must span at
least over Na slot variables, and should also ap-
pear in at least Nr sentences from S (In our exper-
iments we set Na=2 and Nr=2). GSL learns maxi-
mal most general templates: they are spanning tem-
plates which, at the same time, (a) cannot be gener-
alized by further reduction and (b) cannot be further
extended keeping the same generality level.
In order to properly define the notion of maximal
most general templates, we introduce some formal
definitions and notations.
DEFINITION: For a spanning template t we define
a sentence set, denoted with ?(t), as the set of all
parsed sentences in S containing t.
For each pair of templates t1 and t2, we use the no-
tation t1  t2 to denote that t1 is included as a sub-
graph or is equal to t2. We use the notation t1 ? t2
when such inclusion holds strictly. We define T (S)
as the set of all spanning templates in the sample S.
DEFINITION: A spanning template t ? T (S) is
maximal most general if and only if both of the fol-
lowing conditions hold:
CONDITION A: For ?t? ? T (S), t?  t, it holds that
?(t) = ?(t?).
CONDITION B: For ?t? ? T (S), t ? t?, it holds that
?(t) ? ?(t?).
Condition A ensures that the extracted templates do
not contain spanning sub-structures that are more
?general? (i.e. having a larger sentence set); con-
dition B ensures that the template cannot be further
enlarged without reducing its sentence set.
GSL performs template extraction in two main
steps: (1) build a compact graph representation of
all the parse graphs from S; (2) extract templates
from the compact representation.
A compact graph representation is an aggregate
graph which joins all the sentence graphs from S
ensuring that all identical spanning sub-structures
from different sentences are merged into a single
one. Therefore, each vertex v (respectively, edge
e) in the aggregate graph is either a copy of a cor-
responding vertex (edge) from a sentence graph Pi
or it represents the merging of several identically
labeled vertices (edges) from different sentences in
S. The set of such sentences is defined as the sen-
tence set of v (e), and is represented through the set
of index numbers of related sentences (e.g. ?(1,2)?
in the third tree of Figure 2). We will denote with
Gi the compact graph representation of the first i
sentences in S. The parse trees P1 and P2 of two
sentences and their related compact representation
G2 are shown in Figure 2.
Building the compact graph representation
The compact graph representation is built incremen-
tally. The algorithm starts with an empty aggregate
graph G0 and then merges the sentence graphs from
S one at a time into the aggregate structure.
Let?s denote the current aggregate graph with
Gi?1(Vg, Eg) and let Pi(Vp, Ep) be the parse graph
which will be merged next. Note that the sentence
set of Pi is a single element set {i}.
During each iteration a new graph is created as
the union of both input graphs: Gi = Gi?1 ? Pi.
Then, the following merging procedure is per-
formed on the elements of Gi
1. ADDING GENERALIZED VERTICES TO Gi.
For every two vertices vg ? Vg, vp ? Vp having
equal labels, a new generalized vertex vnewg is cre-
ated and added to Gi. The new vertex takes the same
label and holds a sentence set which is formed from
the sentence set of vg by adding i to it. Still with
reference to Figure 2, the generalized vertices in G2
are ?X?, ?Y? and ?stop?. The algorithm connects the
generalized vertex vnewg with all the vertices which
are connected with vg and vp.
2. MERGING EDGES. If two edges eg ? Eg and
ep ? Ep have equal labels and their corresponding
adjacent vertices have been merged, then ea and ep
are also merged into a new edge. In Figure 2 the
edges (?stop?, ?X? ) and (?stop?, ?Y? ) from P1 and
P2 are eventually merged into G2.
3. DELETING MERGED VERTICES. Every vertex
v from Vp or Vg for which at least one generalized
vertex vnewg exists is deleted from Gi.
As an optimization step, we merge only vertices
and edges that are included in equal spanning tem-
plates.
Extracting the templates
GSL extracts all maximal most general templates
from the final compact representation Gn using the
following sub-algorithm:
1. BUILDING MINIMAL SPANNING TREES. For
every Na different slot variables in Gn having a
common ancestor, a minimal spanning tree st is
built. Its sentence set is computed as the intersec-
tion of the sentence sets of its edges and vertices.
2. EXPANDING THE SPANNING TREES. Every
minimal spanning tree st is expanded to the maxi-
mal sub-graph maxst whose sentence set is equal to
?(st). All maximal single-rooted DAGs in maxst
are extracted as candidate templates. Maximality
ensures that the extracted templates cannot be ex-
panded further while keeping the same sentence set,
satisfying condition B.
3. FILTERING. Candidates which contain an-
other candidate with a larger sentence set are filtered
out. This step guarantees condition A.
In Figure 2 the maximal most general template in
G2 is ?X
subj
? stop
obj
? Y? .
3.2.3 Post-processing and ranking of extracted
templates
As a last step, names and numbers are filtered out
from the templates. Moreover, TE removes those
templates which are very long or which appear with
just one anchor set and in less than four sentences.
Finally, the templates are sorted first by the number
of anchor sets with which each template appeared,
and then by the number of sentences in which they
appeared.
4 Evaluation
We evaluated the results of the TE/ASE algorithm
on a random lexicon of verbal forms and then as-
sessed its performance on the extracted data through
human-based judgments.
P1 : stop
subj
z
z
z
||zz
z
z
obj
A
A
A
  A
AA
A
P2 : stop
subj
z
z
z
||zz
z
z
obj

by
J
J
J
J
%%J
J
J
J
G2 : stop(1, 2)
subj(1,2)
rr
rr
xxrr
rr
obj(1,2)

by(2)
OO
OO
''O
OO
O
X Y X Y absorbing X(1, 2) Y (1, 2) absorbing(2)
Figure 2: Two parse trees and their compact representation (sentence sets are shown in parentheses).
4.1 Experimental Setting
The test set for human evaluation was generated by
picking out 53 random verbs from the 1000 most
frequent ones found in a subset of the Reuters cor-
pus2. For each verb entry in the lexicon, we pro-
vided the judges with the corresponding pivot tem-
plate and the list of related candidate entailment
templates found by the system. The judges were
asked to evaluate entailment for a total of 752 tem-
plates, extracted for 53 pivot lexicon entries; Table
1 shows a sample of the evaluated templates; all of
them are clearly good and were judged as correct
ones.
Pivot Template Entailment Templates
X prevent Y X provides protection against Y
X reduces Y
X decreases the risk of Y
X be cure for Y
X a day keeps Y away
X to combat Y
X accuse Y X call Y indictable
X testifies against Y
Y defense before X
X acquire Y X snap up Y
Y shareholders approve X
buyout
Y shareholders receive shares
of X stock
X go back to Y Y allowed X to return
Table 1: Sample of templates found by TE/ASE and
included in the evaluation test set.
Concerning the ASE algorithm, threshold pa-
rameters3 were set as PHRASEMAXF=107, SET-
MINF=102, SETMAXF=105, SETMINP=0.066,
and SETMAXP=0.666. An upper limit of 30 was
imposed on the number of possible anchor sets used
for each pivot. Since this last value turned out to
be very conservative with respect to system cover-
2Known as Reuters Corpus, Volume 1, English Language,
1996-08-20 to 1997-08-19.
3All parameters were tuned on a disjoint development lexi-
con before the actual experiment.
age, we subsequently attempted to relax it to 50 (see
Discussion in Section 4.3).
Further post-processing was necessary over ex-
tracted data in order to remove syntactic variations
referring to the same candidate template (typically
passive/active variations).
Three possible judgment categories have been
considered: Correct if an entailment relationship
in at least one direction holds between the judged
template and the pivot template in some non-bizarre
context; Incorrect if there is no reasonable context
and variable instantiation in which entailment holds;
No Evaluation if the judge cannot come to a definite
conclusion.
4.2 Results
Each of the three assessors (referred to as J#1, J#2,
and J#3) issued judgments for the 752 different
templates. Correct templates resulted to be 283,
313, and 295 with respect to the three judges. No
evaluation?s were 2, 0, and 16, while the remaining
templates were judged Incorrect.
For each verb, we calculate Yield as the absolute
number of Correct templates found and Precision as
the percentage of good templates out of all extracted
templates. Obtained Precision is 44.15%, averaged
over the 53 verbs and the 3 judges. Considering Low
Majority on judges, the precision value is 42.39%.
Average Yield was 5.5 templates per verb.
These figures may be compared (informally, as
data is incomparable) with average yield of 10.1
and average precision of 50.3% for the 9 ?pivot?
templates of (Lin and Pantel, 2001). The compar-
ison suggests that it is possible to obtain from the
(very noisy) web a similar range of precision as was
obtained from a clean news corpus. It also indi-
cates that there is potential for acquiring additional
templates per pivot, which would require further re-
search on broadening efficiently the search for addi-
tional web data per pivot.
Agreement among judges is measured by the
Kappa value, which is 0.55 between J#1 and J#2,
0.57 between J#2 and J#3, and 0.63 between J#1
and J#3. Such Kappa values correspond to moder-
ate agreement for the first two pairs and substantial
agreement for the third one. In general, unanimous
agreement among all of the three judges has been
reported on 519 out of 752 templates, which corre-
sponds to 69%.
4.3 Discussion
Our algorithm obtained encouraging results, ex-
tracting a considerable amount of interesting tem-
plates and showing inherent capability of discover-
ing complex semantic relations.
Concerning overall coverage, we managed to find
correct templates for 86% of the verbs (46 out of
53). Nonetheless, presented results show a substan-
tial margin of possible improvement. In fact yield
values (5.5 Low Majority, up to 24 in best cases),
which are our first concern, are inherently depen-
dent on the breadth of Web search performed by
the ASE algorithm. Due to computational time, the
maximal number of anchor sets processed for each
verb was held back to 30, significantly reducing the
amount of retrieved data.
In order to further investigate ASE potential, we
subsequently performed some extended experiment
trials raising the number of anchor sets per pivot
to 50. This time we randomly chose a subset of
10 verbs out of the less frequent ones in the origi-
nal main experiment. Results for these verbs in the
main experiment were an average Yield of 3 and an
average Precision of 45.19%. In contrast, the ex-
tended experiments on these verbs achieved a 6.5
Yield and 59.95% Precision (average values). These
results are indeed promising, and the substantial
growth in Yield clearly indicates that the TE/ASE
algorithms can be further improved. We thus sug-
gest that the feasibility of our approach displays the
inherent scalability of the TE/ASE process, and its
potential to acquire a large entailment relation KB
using a full scale lexicon.
A further improvement direction relates to tem-
plate ranking and filtering. While in this paper
we considered anchor sets to have equal weights,
we are also carrying out experiments with weights
based on cross-correlation between anchor sets.
5 Conclusions
We have described a scalable Web-based approach
for entailment relation acquisition which requires
only a standard phrasal lexicon as input. This min-
imal level of input is much simpler than required
by earlier web-based approaches, while succeeding
to maintain good performance. This result shows
that it is possible to identify useful anchor sets in
a fully unsupervised manner. The acquired tem-
plates demonstrate a broad range of semantic rela-
tions varying from synonymy to more complicated
entailment. These templates go beyond trivial para-
phrases, demonstrating the generality and viability
of the presented approach.
From our current experiments we can expect to
learn about 5 relations per lexicon entry, at least for
the more frequent entries. Moreover, looking at the
extended test, we can extrapolate a notably larger
yield by broadening the search space. Together with
the fact that we expect to find entailment relations
for about 85% of a lexicon, it is a significant step
towards scalability, indicating that we will be able
to extract a large scale KB for a large scale lexicon.
In future work we aim to improve the yield by in-
creasing the size of the sample-corpus in a qualita-
tive way, as well as precision, using statistical meth-
ods such as supervised learning for better anchor set
identification and cross-correlation between differ-
ent pivots. We also plan to support noun phrases
as input, in addition to verb phrases. Finally, we
would like to extend the learning task to discover the
correct entailment direction between acquired tem-
plates, completing the knowledge required by prac-
tical applications.
Like (Lin and Pantel, 2001), learning the context
for which entailment relations are valid is beyond
the scope of this paper. As stated, we learn entail-
ment relations holding for some, but not necessarily
all, contexts. In future work we also plan to find the
valid contexts for entailment relations.
Acknowledgements
The authors would like to thank Oren Glickman
(Bar Ilan University) for helpful discussions and as-
sistance in the evaluation, Bernardo Magnini for his
scientific supervision at ITC-irst, Alessandro Vallin
and Danilo Giampiccolo (ITC-irst) for their help in
developing the human based evaluation, and Prof.
Yossi Matias (Tel-Aviv University) for supervising
the first author. This work was partially supported
by the MOREWEB project, financed by Provincia
Autonoma di Trento. It was also partly carried out
within the framework of the ITC-IRST (TRENTO,
ITALY) ? UNIVERSITY OF HAIFA (ISRAEL) col-
laboration project. For data visualization and analy-
sis the authors intensively used the CLARK system
(www.bultreebank.org) developed at the Bulgarian
Academy of Sciences .
References
Regina Barzilay and Lillian Lee. 2003. Learning
to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In Proceedings
of HLT-NAACL 2003, pages 16?23, Edmonton,
Canada.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting paraphrases from a parallel corpus. In
Proceedings of ACL 2001, pages 50?57, Toulose,
France.
Ido Dagan and Oren Glickman. 2004. Probabilis-
tic textual entailment: Generic applied modeling
of language variability. In PASCAL Workshop on
Learning Methods for Text Understanding and
Mining, Grenoble.
Florence Duclaye, Franc?ois Yvon, and Olivier
Collin. 2002. Using the Web as a linguistic re-
source for learning reformulations automatically.
In Proceedings of LREC 2002, pages 390?396,
Las Palmas, Spain.
Oren Glickman and Ido Dagan. 2003. Identifying
lexical paraphrases from a single corpus: a case
study for verbs. In Proceedings of RANLP 2003.
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2003. Natural language based reformula-
tion resource and Web Exploitation. In Ellen M.
Voorhees and Lori P. Buckland, editors, Proceed-
ings of the 11th Text Retrieval Conference (TREC
2002), Gaithersburg, MD. NIST.
Christian Jacquemin. 1999. Syntagmatic and
paradigmatic representations of term variation.
In Proceedings of ACL 1999, pages 341?348.
Dekang Lin and Patrick Pantel. 2001. Discovery of
inference rules for Question Answering. Natural
Language Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation
of MINIPAR. In Proceedings of the Workshop
on Evaluation of Parsing Systems at LREC 1998,
Granada, Spain.
Dan Moldovan and Vasile Rus. 2001. Logic form
transformation of WordNet and its applicability
to Question Answering. In Proceedings of ACL
2001, pages 394?401, Toulose, France.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations:
Extracting paraphrases and generating new sen-
tences. In Proceedings of HLT-NAACL 2003, Ed-
monton, Canada.
Deepak Ravichandran and Eduard Hovy. 2002.
Learning surface text patterns for a Question An-
swering system. In Proceedings of ACL 2002,
Philadelphia, PA.
Ellen Riloff and Rosie Jones. 1999. Learning dic-
tionaries for Information Extraction by multi-
level bootstrapping. In Proceedings of the Six-
teenth National Conference on Artificial Intelli-
gence (AAAI-99), pages 474?479.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo,
and Ralph Grishman. 2002. Automatic para-
phrase acquisition from news articles. In Pro-
ceedings of Human Language Technology Con-
ference (HLT 2002), San Diego, USA.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern represen-
tation model for automatic IE pattern acquisition.
In Proceedings of ACL 2003.
Roman Yangarber, Ralph Grishman, Pasi
Tapanainen, and Silja Huttunen. 2000. Un-
supervised discovery of scenario-level patterns
for Information Extraction. In Proceedings of
COLING 2000.
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 194?204, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Learning Verb Inference Rules from Linguistically-Motivated Evidence
Hila Weisman?, Jonathan Berant?, Idan Szpektor?, Ido Dagan?
? Computer Science Department, Bar-Ilan University
? The Blavatnik School of Computer Science, Tel Aviv University
? Yahoo! Research Israel
{weismah1,dagan}@cs.biu.ac.il
{jonatha6}@post.tau.ac.il
{idan}@yahoo-inc.com
Abstract
Learning inference relations between verbs is
at the heart of many semantic applications.
However, most prior work on learning such
rules focused on a rather narrow set of in-
formation sources: mainly distributional sim-
ilarity, and to a lesser extent manually con-
structed verb co-occurrence patterns. In this
paper, we claim that it is imperative to uti-
lize information from various textual scopes:
verb co-occurrence within a sentence, verb co-
occurrence within a document, as well as over-
all corpus statistics. To this end, we propose
a much richer novel set of linguistically mo-
tivated cues for detecting entailment between
verbs and combine them as features in a su-
pervised classification framework. We empir-
ically demonstrate that our model significantly
outperforms previous methods and that infor-
mation from each textual scope contributes to
the verb entailment learning task.
1 Introduction
Inference rules are an important building block of
many semantic applications, such as Question An-
swering (Ravichandran and Hovy, 2002) and In-
formation Extraction (Shinyama and Sekine, 2006).
For example, given the sentence ?Churros are
coated with sugar?, one can use the rule ?coat ?
cover? to answer the question ?What are Churros
covered with??. Inference rules specify a directional
inference relation between two text fragments, and
we follow the Textual Entailment modeling of infer-
ence (Dagan et al2006), which refers to such rules
as entailment rules. In this work we focus on one
of the most important rule types, namely, lexical en-
tailment rules between verbs (verb entailment), e.g.,
?whisper ? talk?, ?win ? play? and ?buy ? own?.
The significance of such rules has led to active re-
search in automatic learning of entailment rules be-
tween verbs or verb-like structures (Zanzotto et al
2006; Abe et al2008; Schoenmackers et al2010).
Most prior efforts to learn verb entailment rules
from large corpora employed distributional similar-
ity methods, assuming that verbs are semantically
similar if they occur in similar contexts (Lin, 1998;
Berant et al2012). This led to the automatic ac-
quisition of large scale knowledge bases, but with
limited precision. Fewer works, such as VerbOcean
(Chklovski and Pantel, 2004), focused on identi-
fying verb entailment through verb instantiation of
manually constructed patterns. For example, the
sentence ?he scared and even startled me? implies
that ?startle? scare?. This led to more precise rule
extraction, but with poor coverage since contrary
to nouns, in which patterns are common (Hearst,
1992), verbs do not co-occur often within rigid pat-
terns. However, verbs do tend to co-occur in the
same document, and also in different clauses of the
same sentence.
In this paper, we claim that on top of standard
pattern-based and distributional similarity methods,
corpus-based learning of verb entailment can greatly
benefit from exploiting additional linguistically-
motivated cues that are specific to verbs. For in-
stance, when verbs co-occur in different clauses of
the same sentence, the syntactic relation between the
clauses can be viewed as a proxy for the semantic re-
lation between the verbs. Moreover, we claim that to
194
improve performance it is crucial to combine infor-
mation sources from different textual scopes: verb
co-occurrence within a sentence and within a docu-
ment, distributional similarity over the entire corpus,
etc.
Our contribution in this paper is two-fold. First,
we suggest a novel set of entailment indicators that
help to detect the likelihood of verb entailment.
Our novel indicators are specific to verbs and are
linguistically-motivated. Second, we encode our
novel indicators as features within a supervised clas-
sification framework and integrate them with other
standard features adapted from prior work. This re-
sults in a supervised corpus-based learning method
that combines verb entailment information at the
sentence, document and corpus levels.
We test our model on a manually labeled data
set, and show that it outperforms the best perform-
ing previous work by 24%. In addition, we ex-
amine the effectiveness of indicators that operate at
the sentence-level, document-level and corpus-level.
This analysis reveals that using a rich and diverse
set of indicators that capture sentence-level interac-
tions between verbs substantially improves verb en-
tailment detection.
2 Background
The main approach for learning entailment rules be-
tween verbs and verb-like structures has employed
the distributional hypothesis, which assumes that
words with similar meanings appear in similar con-
texts. For example, we expect the words ?buy? and
?purchase? to occur with similar subjects and objects
in a large corpus. This observation has led to ample
work on developing both symmetric and directional
similarity measures that attempt to capture semantic
relations between lexical items by comparing their
neighborhood context (Lin, 1998; Weeds and Weir,
2003; Geffet and Dagan, 2005; Szpektor and Dagan,
2008; Kotlerman et al2010).
A far less explored direction for learning verb en-
tailment involves exploiting verb co-occurrence in
a sentence or a document. One prominent work
is Chklovsky and Pantel?s VerbOcean (2004). In
VerbOcean, the authors manually constructed 33
patterns and divided them into five pattern groups,
where each group signals one of the following five
semantic relations: similarity, strength, antonymy,
enablement and happens-before. For example, the
pattern ?Xed and later Yed? signals the happens-
before relation between the verbs ?X? and ?Y?. Start-
ing with candidate verb pairs based on a distribu-
tional similarity measure, the patterns are used to
choose a semantic relation per verb pair based on
the different patterns this pair instantiates. This
method is more precise than distributional similarity
approaches, but it is highly susceptible to sparseness
issues, since verbs do not typically co-occur within
rigid patterns. Utilizing verb co-occurrence at the
document level, Chambers and Jurafsky (2008) es-
timate whether a pair of verbs is narratively related
by counting the number of times the verbs share an
argument in the same document. In a similar man-
ner, Pekar (2008) detects entailment rules between
templates from shared arguments within discourse-
related clauses in the same document.
Recently, supervised classification has become
standard in performing various semantic tasks.
Mirkin et al2006) introduced a system for learn-
ing entailment rules between nouns (e.g., ?novel?
book?) that combines distributional similarity and
Hearst patterns as features in a supervised clas-
sifier. Pennacchiotti and Pantel (2009) augment
Mirkin et alfeatures with web-based features for
the task of entity extraction. Hagiwara et al2009)
perform synonym identification based on both dis-
tributional and contextual features. Tremper (2010)
extract ?loose? sentence-level features in order to
identify the presupposition relation (e.g., , the verb
?win? presupposes the verb ?play?). Last, Be-
rant et al2012) utilized various distributional
similarity features to identify entailment between
lexical-syntactic predicates.
In this paper, we follow the supervised approach
for semantic relation detection in order to identify
verb entailment. While we utilize and adapt useful
features from prior work, we introduce a diverse set
of novel features for the task, effectively combining
verb co-occurrence information at the sentence, doc-
ument, and corpus levels.
3 Linguistically-Motivated Indicators
As mentioned in Section 1, verbs behave quite dif-
ferently from nouns in corpora. In this section, we
195
introduce linguistically motivated indicators that are
specific to verbs and may signal the semantic re-
lation between verb pairs. Then, in Section 4 we
describe how these indicators are exactly encoded
as features within a supervised classification frame-
work.
Verb co-occurrence When (non-auxiliary) verbs
co-occur in a sentence, they are often the main verbs
of different clauses. We thus aim to use information
about the relation between clauses to learn about
the relation between the clauses? main verbs. Dis-
course markers (Hobbs, 1979; Schiffrin, 1988) are
lexical terms such as ?because? and ?however? that
indicate a semantic relation between discourse frag-
ments (i.e., propositions or speech acts). We suggest
that these markers can indicate semantic relations
between the main verbs of the connected clauses.
For example, in the sentence ?He always snores
while he sleeps?, the marker ?while? indicates a tem-
poral relation between the clauses, indicating that
?snoring? occurs while ?sleeping? (and so ?snore?
sleep?).
Often the relation between clauses is not ex-
pressed explicitly with an overt discourse marker,
but is still implied by the syntactic structure of
the sentence. For example, in dependency parsing
the relation can be captured by labeled dependency
edges expressing that one clause is an adverbial ad-
junct of the other, or that two clauses are coordi-
nated. This can indicate the existence (or lack) of
entailment between verbs. For instance, in the sen-
tence ?When I walked into the room, he was working
out?, the verb ?walk? is an adverbial adjunct of the
verb ?work out?. Such co-occurrence structure does
not indicate a deep semantic relation, such as entail-
ment, between the two verbs.
Verb classes Verb classes are sets of semantically-
related verbs sharing some linguistic properties
(Levin, 1993). One of the most general verb classes
are stative vs. event verbs (Jackendoff, 1983). Sta-
tive verb, such as ?love? and ?think?, usually describe
a state that lasts some time. On the other hand, event
verbs, such as ?run? and ?kiss?, describe an action.
We hypothesize that verb classes are relevant for de-
termining entailment, for example, that stative verbs
are not likely to entail event verbs.
Verb generality Verb-particle constructions are
multi-word expressions consisting of a head verb
and a particle, e.g., switch off (Baldwin and Villav-
icencio, 2002). We conjecture that the more gen-
eral a verb is, the more likely it is to appear with
many different particles. Detecting verb generality
can help us tackle an infamous property of distribu-
tional similarity methods, namely, the difficulty in
detecting the direction of entailment (Berant et al
2012). For example, the verb ?cover? appears with
many different particles such as ?up? and ?for?, while
the verb ?coat? does not. Thus, assuming we have
evidence for an entailment relation between the two
verbs, this indicator can help us discern the direction
of entailment and determine that ?coat? cover?.
Typed Distributional Similarity As discussed in
section 2, distributional similarity is the most com-
mon source of information for learning semantic re-
lations between verbs. Yet, we suggest that on top
of standard distributional similarity measures, which
take several verbal arguments into account (such as
subject, object, etc.) simultaneously, we should also
focus on each type of argument independently. In
particular, we apply this approach to compute simi-
larity between verbs based on the set of adverbs that
modify them. Our hypothesis is that adverbs may
contain relevant information for capturing the direc-
tion of entailment. If a verb appears with a small set
of adverbs, it is more likely to be a specific verb that
already conveys a specific action or state, making an
additional adverb redundant. For example, the verb
?whisper? conveys a specific manner of talking and
will probably not appear with the adverb ?loudly?,
while the verb ?talk? is more likely to appear with
such an adverb. Thus, measuring similarity based
solely on adverb modifiers could reveal this phe-
nomenon.
4 Supervised Entailment Detection
In the previous section, we discussed linguistic ob-
servations regarding novel indicators that may help
in detecting entailment relations between verbs. We
next describe how to incorporate these indicators as
features within a supervised framework for learning
lexical entailment rules between verbs. We follow
prior work on supervised lexical semantics (Mirkin
et al2006; Hagiwara et al2009; Tremper, 2010)
196
and address the rule learning task as a classification
task. Specifically, given an ordered verb pair (v1, v2)
as input, we learn a classifier that detects whether the
entailment relation ?v1? v2? holds for this pair.
We next detail how our novel indicators, as well
as other diverse sources of information found useful
in prior work, are encoded as features. Then, we
describe the learning model and our feature analysis
procedure.
4.1 Entailment features
Most of our features are based on information ex-
tracted from the target verb pair co-occurring within
varying textual scopes (sentence, document, cor-
pus). Hence, we group the features according to
their related scope. Naturally, when the scope is
small, i.e., at a sentence level, the semantic rela-
tion between the verbs is easier to discern but the
information may be sparse. Conversely, when co-
occurrence is loose the relation is harder to discern
but coverage is increased.
4.1.1 Sentence-level co-occurrence
We next detail features that address co-occurrence
of the target verb pair within a sentence. These in-
clude our novel linguistically-motivated indicators,
as well as features that were adapted from prior
work.
Discourse markers As discussed in Section 3,
discourse markers may signal relations between the
main verbs of adjacent clauses. The literature is
abundant with taxonomies that classify markers to
various discourse relations (Mann and Thompson,
1988; Hovy and Maier, 1993; Knott and Sanders,
1998). Inspired by Marcu and Echihabi (2002), we
employ markers that are mapped to four discourse
relations ?Contrast?, ?Cause?, ?Condition? and ?Tem-
poral?, as specified in Table 1. This definition
can be viewed as a relaxed version of VerbOcean?s
(Chklovski and Pantel, 2004) patterns, although the
underlying intuition is different (see Section 3).
For a target verb pair (v1, v2) and each discourse
relation r, we count the number of times that v1 is
the main verb in the main clause, v2 is the main verb
in the subordinate clause, and the clauses are con-
nected via a marker mapped to r. For example, given
the sentence ?You must enroll in the competition be-
fore you can participate in it?, the verb pair (?en-
roll?,?participate?) appears in the ?Temporal? rela-
tion, indicated by the marker ?before?, where ?enroll?
is in the main clause. Each count is then normalized
by the total number of times (v1, v2) appear with any
marker. The same procedure is done when v1 is in
the subordinate clause and v2 in the main clause. We
term the features by the relevant discourse relation,
e.g., ?v1-contrast-v2? refers to v1 being in the main
clause and connected to the subordinate clause via a
contrast marker.
Dependency relations between clauses As noted
in Section 3, the syntactic structure of verb co-
occurrence can indicate the existence or lack of en-
tailment. In dependency parsing this may be ex-
pressed via the label of the dependency relation con-
necting the main and subordinate clauses. In our ex-
periments we used the ukWaC corpus1 (Baroni et al
2009) which was parsed by the MALT parser (Nivre
et al2006). Hence, we identified three MALT de-
pendency relations that connect a main clause with
its subordinate clause. The first relation is the object
complement relation ?obj?. In this case the subor-
dinate clause is an object complement of the main
clause. For example, in ?it surprised me that the
lizard could talk? the verb pair (?surprise?,?talk?) is
connected by the ?obj? relation. The second rela-
tion is the adverbial adjunct relation ?adv?, in which
the subordinate clause is adverbial and describes the
time, place, manner, etc. of the main clause, e.g., ?he
gave his consent without thinking about the reper-
cussions?. The last relation is the coordination rela-
tion ?coord?, e.g., ?every night my dog Lucky sleeps
on the bed and my cat Flippers naps in the bathtub?.
Similar to discourse markers, we compute for
each verb pair (v1,v2) and each dependency label d
the proportion of times that v1 is the main verb of the
main clause, v2 is the main verb of the subordinate
clause, and the clauses are connected by dependency
relation d, out of all the times they are connected by
any dependency relation. We term the features by
the dependency label, e.g., ?v1-adv-v2? refers to v1
being in the main clause and connected to the subor-
dinate clause via an adverbial adjunct.
1http://wacky.sslmit.unibo.it/doku.php?
id=corpora
197
Discourse Rel. Discourse Markers
Contrast although , despite , but , whereas , notwithstanding , though
Cause because , therefore , thus
Condition if , unless
Temporal whenever , after , before , until , when , finally , during , afterwards , meanwhile
Table 1: Discourse relations and their mapped markers.
Pattern-based We follow Chklovski and Pan-
tel (2004) and extract occurrences of VerbOcean pat-
terns that are instantiated by the target verb pair. As
mentioned in Section 2, VerbOcean patterns were
originally grouped into five semantic classes. Based
on a preliminary study we conducted, we decided
to utilize only four strength-class patterns as posi-
tive indicators for entailment, e.g., ?he scared and
even startled me?, and three antonym-class patterns
as negative indicators for entailment, e.g., ?you can
either open or close the door?. We note that these
patterns are also commonly used by RTE systems2.
Since the corpus pattern counts were very sparse,
we defined for a target verb pair (v1, v2) two bi-
nary features: the first denotes whether the verb
pair instantiates at least one positive pattern, and
the second denotes whether the verb pair instanti-
ates at least one negative pattern. For example, given
the aforementioned sentences, the value of the pos-
itive feature for the verb pair (?startle?,?scare?) is
?1?. Patterns are directional, and so the value of
(?scare?,?startle?) is ?0?.
Polarity We compute the proportion of times that
the two verbs appear in different polarity. For exam-
ple, in ?he didn?t say why he left?, the verb ?say? ap-
pears in negative polarity and the verb ?leave? in pos-
itive polarity. Such change in polarity is usually an
indicator of non-entailment between the two verbs.
Tense ordering The temporal relation between
verbs may provide information about their seman-
tic relation. For each verb pair co-occurrence, we
extract the verbs? tenses and order them as follows:
past < present < future. We then add the fea-
tures ?tense-v1<tense-v2?, ?tense-v1=tense-v2?, and
?tense-v1>tense-v2?, corresponding to the propor-
2http://aclweb.org/aclwiki/index.php?
title=RTE_Knowledge_Resources#Ablation_
Tests
tion of times the tense of v1 is smaller, equal to,
or bigger than the tense of v2. This indicates the
prevalent temporal relation between the verbs in the
corpus and may assist in detecting the direction of
entailment. e.g., if tense-v1>tense-v2, the verb pair
is less likely to entail.
Co-reference Following Tremper (2010), in every
co-occurrence of (v1,v2) we extract for each verb
the set of arguments at either the subject or object
positions, denoted A1 and A2 (for v1 and v2, re-
spectively). We then compute the proportion of co-
occurrences in which v1 and v2 share an argument,
i.e., A1 ? A2 6= ?, out of all the co-occurrences in
which bothA1 andA2 are non-empty. The intuition,
which is similar to distributional similarity, is that
semantically related verbs tend to share arguments.
Syntactic and lexical distance Following Trem-
per (2010) again, we compute the average distance
d in dependency edges between the co-occurring
verbs. We compute three features corresponding to
three bins indicating if d < 3, 3 ? d ? 7, or
d > 7. Similar features are computed for the dis-
tance in words (bins are 0 < d < 5, 5 ? d ? 10,
d > 10). This feature provides insight into the syn-
tactic relatedness of the verbs.
Sentence-level pmi Pointwise mutual information
(pmi) between v1 and v2 is computed, where the co-
occurrence scope is a sentence. Higher pmi should
hint at semantically related verbs.
4.1.2 Document-level co-occurrence
This group of features addresses co-occurrence of
a target verb pair within the same document. These
features are less sparse, but tend to capture coarser
semantic relations between the target verbs.
Narrative score Chambers and Jurafsky (2008)
suggested a method for learning sequences of ac-
tions or events (expressed by verbs) in which a sin-
198
gle entity is involved. They proposed a pmi-like nar-
rative score (see Eq. (1) in their paper) that esti-
mates whether a pair consisting of a verb and one
of its dependency relations (v1, r1) is narratively-
related to another such pair (v2, r2). Their estima-
tion is based on quantifying the likelihood that two
verbs will share an argument that instantiates both
the dependency position (v1, r1) and (v2, r2) within
documents in which the two verbs co-occur. For ex-
ample, given the document ?Lindsay was prosecuted
for DUI. Lindsay was convicted of DUI.? the pairs
(?prosecute?,?subj?) and (?convict?,?subj?) share the
argument ?Lindsay? and are part of a narrative chain.
Such narrative relations may provide cues to the se-
mantic relatedness of the verb pair.
We compute for every target verb pair nine fea-
tures using their narrative score. In four features,
r1 = r2 and the common dependency is either a sub-
ject, an object, a preposition complement (e.g., ?we
meet at the station.), or an adverb (termed chamb-
subj, chamb-obj, and so on). In the next three fea-
tures, r1 6= r2 and r1, r2 denote either a subject,
object, or preposition complement3 (termed chamb-
subj-obj and so on). Last, we add as features the
average of the four features where r1 = r2 (termed
chamb-same), and the average of the three features
where r1 6= r2 (termed chamb-diff ).
Document-level pmi Similar to sentence-level
pmi, we compute the pmi between v1 and v2, but
this time the co-occurrence scope is a document.
4.1.3 Corpus-level statistics
The final group of features ignores sentence or
document boundaries and is based on overall corpus
statistics.
Distributional similarity Following our hypoth-
esis regarding typed distributional similarity (Sec-
tion 3), we first compute for each verb and each
argument (subject, object, preposition complement
and adverb) a separate vector that counts the num-
ber of times each word in the corpus instantiates
the argument of that verb. In addition, we also
compute a vector that is the concatenation of the
previous separate vectors, which captures the stan-
dard distributional similarity statistics. We then
3adverbs never instantiate the subject, object or preposition
complement positions.
apply three state-of-the-art distributional similarity
measures, Lin (Lin, 1998), Weeds precision (Weeds
and Weir, 2003) and BInc (Szpektor and Dagan,
2008), to compute for every verb pair a similarity
score between each of the five count vectors4. We
term each feature by the method and argument, e.g.,
weeds-prep and lin-all represent the Weeds measure
over prepositional complements and the Lin mea-
sure over all arguments.
Verb classes Following our discussion in Sec-
tion 3, we first measure for each target verb v a ?sta-
tive? feature f by computing the proportion of times
it appears in progressive tense, since stative verbs
usually do not appear in the progressive tense (e.g.,
?knowing?). Then, given a verb pair (v1,v2) and their
corresponding stative features f1 and f2, we add two
features f1 ? f2 and
f1
f2
, which capture the interaction
between the verb classes of the two verbs.
Verb generality For each verb, we add as a feature
the number of different particles it appears with in
the corpus, following the hypothesis that this is a
cue to its generality. Then, given a verb pair (v1,v2)
and their corresponding features f1 and f2, we add
the feature f1f2 . We expect that when
f1
f2
is high, v1 is
more general than v2, which is a negative entailment
indicator.
4.2 Learning model and feature analysis
The total number of features in our model as de-
scribed above is 63. We combine the features in
a supervised classification framework with a linear
SVM. Since our model contains many novel fea-
tures, it is important to investigate their utility for
detecting verb entailment. To that end, we employ
feature ranking methods as suggested by Guyon et
al. (2003). In feature ranking methods, features are
ranked by some score computed for each feature in-
dependently. In this paper we use Pearson correla-
tion between the feature values and the correspond-
ing labels as the ranking criterion.
4We employ the common practice of using the pmi between
a verb and an argument rather than the argument count as the
argument?s weight.
199
5 Evaluation and Analysis
5.1 Experimental Setting
To evaluate our proposed supervised model, we con-
structed a dataset containing labeled verb pairs. We
started by randomly sampling 50 verbs out of the
common verbs in the RCV1 corpus5, which we de-
note here as seed verbs. Next, we extracted the 20
most similar verbs to each seed verb according to
the Lin similarity measure (Lin, 1998), which was
computed on the RCV1 corpus. Then, for each seed
verb vs and one of its extracted similar verbs vis we
generated the two directed pairs (vs, vis) and (v
i
s, vs),
which represent the candidate rules ?vs ? vis? and
?vis ? vs? respectively. To reduce noise, we filtered
out verb pairs where one of the verbs is an auxiliary
or a light verb such as ?do?, ?get? and ?have?. This
step resulted in 812 verb pairs as our dataset6, which
were manually annotated by the authors as repre-
senting a valid entailment rule or not. To annotate
these pairs, we generally followed the rule-based ap-
proach for entailment rule annotation, where a rule
?v1 ? v2? is considered as correct if the annotator
could think of reasonable contexts under which the
rule holds (Dekang and Pantel, 2001; Szpektor et
al., 2004). In total 225 verb pairs were labeled as
entailing (the rule ?v1 ? v2? was judged as correct)
and 587 verb pairs were labeled as non-entailing (the
rule ?v1 ? v2? was judged as incorrect). The Inter-
Annotator Agreement (IAA) for a random sample of
100 pairs was moderate (0.47), as expected from the
rule-based approach (Szpektor et al2007).
For each verb pair, all 63 features within our
model (Section 4) were computed using the ukWaC
corpus (Baroni et al2009), which contains 2 billion
words. For classification, we utilized SVM-perf?s
(Joachims, 2005) linear SVM implementation with
default parameters, and evaluated our model by per-
forming 10-fold cross validation (CV) over the la-
beled dataset.
5http://trec.nist.gov/data/reuters/
reuters.html
6The data set is available at http://www.cs.biu.ac.
il/?nlp/downloads/verb-pair-annotation.
html
5.2 Feature selection and analysis
As discussed in Section 4.2, we followed the feature
ranking method proposed by Guyon et al2003) to
investigate the utility of our proposed features. Ta-
ble 2 depicts the 10 most positively and negatively
correlated features with entailment according to the
Pearson correlation measure
From Table 2, it is clear that distributional simi-
larity features are amongst the most positively cor-
related with entailment, which is in line with prior
work (Geffet and Dagan, 2005; Kotlerman et al
2010). Looking more closely, our suggestion for
typed distributional similarity proved to be useful,
and indeed most of the highly correlated distribu-
tional similarity features are typed measures. Stand-
ing out are the adverb-typed measures, with two fea-
tures in the top 10, including the highest, ?Weeds-
adverb?, and ?BInc-adverb?. We also note that the
highly correlated distributional similarity measures
are directional, Weeds and BInc.
The table also indicates that document-level co-
occurrence contributes positively to entailment de-
tection. This includes both the Chambers narrative
measure, with the typed feature Chambers-obj, and
document-level PMI, which captures a more loose
co-occurrence relationship between verbs. Again,
we point at the significant correlation of our novel
typed measures with verb entailment, in this case the
typed narrative measure.
Last, our feature analysis shows that many of our
novel co-occurrence features at the sentence level
contribute useful negative information. For exam-
ple, verbs connected via an adverbial adjunct (?v2-
adverb-v1?) or an object complement (?v1-obj-v2?)
are negatively correlated with entailment. In addi-
tion, the novel ?verb generality? feature as well as
the tense difference feature (?tense-v1 > tense-v2?)
are also strong negative indicators. On the other
hand, ?v2-coord-v1? is positively correlated with en-
tailment. This shows that encoding various aspects
of verb co-occurrence at the sentence level can lead
to better prediction of verb entailment. Finally, we
note that PMI at the sentence level is highly corre-
lated with entailment even more than at the docu-
ment level, since the local textual scope is more in-
dicative, though sparser.
To conclude, our feature analysis shows that fea-
200
Rank Top Positive Top Negative
1 Weeds-adverb tense-v1 > tense-v2
2 Sentence-level PMI v2-adverb-v1 co-occurrence
3 Weeds-subj v2-obj-v1 co-occurrence
4 Weeds-prep v1-obj-v2 co-occurrence
5 Weeds-all v1-adverb-v2 co-occurrence
6 Chambers-obj verb generality f1f2
7 v2-coord-v1 co-occurrence v1-contrast-v2
8 BInc-adverb tense-v1 < tense-v2
9 Document-level PMI lexical-distance 0-5
10 Chambers-same Lin-subj
Table 2: Top 10 positive and negative features according to the Pearson correlation score.
tures at all levels: sentence, document and corpus,
contain useful information for entailment detection,
both positive and negative, and should be combined
together. Moreover, many of our novel features are
among the highly correlated features, showing that
devising a rich set of verb-specific and linguistically-
motivated features provides better discriminative ev-
idence for entailment detection.
5.3 Results and Analysis
We compared our method to the following baselines
which were mostly taken from or inspired by prior
work:
Random: A simple decision rule: for any
pair (v1, v2), randomly classify as ?yes? with a
probability equal to the number of entailing verb
pairs out of all verb pairs in the labeled dataset (i.e.,
225
812 = 0.277).
VO-KB: A simple unsupervised rule: for any
pair (v1, v2), classify as ?yes? if the pair appears in
the strength relation (corresponding to entailment)
in the VerbOcean knowledge-base, which was com-
puted over Web counts.
VO-ukWaC: A simple unsupervised rule: for any
pair (v1, v2), classify as ?yes? if the value of the
positive VerbOcean feature is ?1? (Section 4.1, com-
puted over ukWaC).
TDS: Include only the 15 distributional similarity
features in our supervised model. This baseline ex-
tends Berant et al2012), who trained an entailment
Method P% R% AUC F1
All 40.2 71.0 0.65 0.51
TDS+VO 36.8 53.2 0.58 0.41
TDS 34.6 44.8 0.56 0.37
Random 27.9 28.8 0.51 0.28
VO-KB 33.1 14.8 0.53 0.2
VO-ukWaC 23.3 4.7 0.29 0.08
Table 3: Average precision, recall, AUC and F1 for our
method and the baselines.
classifier over several distributional similarity fea-
tures, and provides an evaluation of the discrimina-
tive power of distributional similarity alone, without
co-occurrence features.
TDS+VO: Include only the 15 typed distribu-
tional similarity features and the two VerbOcean
features in our supervised model. This baseline
is inspired by Mirkin et al2006), who combined
distributional similarity features and Hearst pat-
terns (Hearst, 1992) for learning entailment between
nouns.
All: Our full-blown model, including all features
described in Section 4.1.
For all tested methods, we performed 10-fold
cross validation and averaged Precision, Recall,
Area under the ROC curve (AUC) and F1 over the 10
folds. Table 3 presents the results of our full-blown
model as well as the baselines.
First, we note that, as expected, the VerbOcean
baselines VO-KB and VO-ukWaC provide low recall,
201
Method P% R% AUC F1
All 40.2 71.0 0.65 0.51
Sent+Corpus-level 39.7 70.4 0.64 0.50
Sent+Doc-level 39.0 70.0 0.63 0.50
Doc+Corpus-level 37.7 64.0 0.62 0.47
Sent-level 35.8 63.8 0.59 0.46
Doc-level 30.0 45.4 0.52 0.35
Corpus-level 35.4 58.1 0.58 0.44
Table 4: Average precision, recall, AUC and F1 for each
subset of the feature groups.
due to the sparseness of rigid pattern instantiation
for verbs both in the ukWaC corpus and on the web.
Yet, VerbOcean positive and negative patterns do
add some discriminative power over only distribu-
tional similarity measures, as seen by the improve-
ment of TDS+VO over TDS in all criteria. But, it is
the combination of all types of information sources
that yields the best performance. Our complete
model, employing the full set of features, outper-
forms all other models in terms of both precision and
recall. Its improvement in terms of F1 over the sec-
ond best model (TDS+VO), which includes all distri-
butional similarity features as well as pattern-based
features, is by 24%. This result shows the benefits
of integrating linguistically motivated co-occurrence
features with traditional pattern-based and distribu-
tional similarity information.
To further investigate the contribution of fea-
tures at various co-occurrence levels, we trained
and tested our model with all possible combina-
tions of feature groups corresponding to a certain
co-occurrence scope (sentence, document and cor-
pus). Table 4 presents the results of these tests.
The most notable result of this analysis is that
sentence-level features play an important role within
our model. Indeed, removing either the document-
level features (Sent+Corpus-level) or the corpus-
level features (Sent+Doc-level) results in only a
slight decline in performance. Yet, removing the
sentence-level features (Doc+Corpus-level), ends in
a more substantial decline of 8.5% in F1. In addi-
tion, sentence-level features alone (Sent-level) pro-
vide the best discriminative power for verb entail-
ment, compared to document and corpus levels,
which include distributional similarity features. Yet,
we note that sentence-level features alone do not
capture all the information within our model, and
they should be combined with one of the other fea-
ture groups to reach performance close to the com-
plete model. This shows again the importance of
combining co-occurrence indicators at different lev-
els.
As an additional insight from Table 4, we point
out that document-level features are not good en-
tailment indicators by themselves (Doc-level in Ta-
ble 4), and they perform worse than the distribu-
tional similarity baseline (TDS at Table 3). Still, they
do complement each of the other feature groups. In
particular, since the Sent+Doc-level model performs
almost as good as the full model, this subset may
be a good substitute to the full model, since its fea-
tures are easier to extract from large corpora, as they
may be extracted in an on-line fashion, processing
one document at a time (contrary to corpus-level fea-
tures).
As a final analysis, we randomly sampled cor-
rect entailment rules learned by our model but
missed by the typed distributional similarity classi-
fier (TDS). Our overall impression is that employ-
ing co-occurrence information helps to better cap-
ture entailment relations other than synonymy and
troponymy. For example, our model learns that ac-
quire? own, corresponding to the cause-effect en-
tailment relation, and that patent ? invent, corre-
sponding to the presupposition entailment relation.
6 Conclusions and Future Work
We presented a supervised classification model for
detecting lexical entailment between verbs. At the
heart of our model stand novel linguistically moti-
vated indicators that capture positive and negative
entailment information. These indicators encom-
pass co-occurrence relationships between verbs at
the sentence, document and corpus level, as well
as more fine-grained typed distributional similarity
measures. Our model incorporates these novel indi-
cators together with useful features from prior work,
combining co-occurrence and distributional similar-
ity information about verb pairs.
Our experiment over a manually labeled dataset
showed that our model significantly outperforms
several state-of-the-art models both in terms of Pre-
202
cision and Recall. Further feature analysis indicated
that our novel indicators contribute greatly to the
performance of the model, and that co-occurrence
at multiple levels, combined with distributional sim-
ilarity features, is necessary to achieve the model?s
best performance.
In future work we?d like to investigate which in-
dicators may contribute to learning different fine-
grained types of entailment, such as presupposition
and cause-effect, and attempt to perform a more
fine-grained classification to subtypes of entailment.
Acknowledgments
This work was partially supported by the Israel
Science Foundation grant 1112/08, the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Community?s Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT).
References
Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008.
Acquiring event relation knowledge by learning cooc-
currence patterns and fertilizing cooccurrence samples
with verbal nouns. In Proceedings of IJCNLP.
Timothy Baldwin and Aline Villavicencio. 2002. Ex-
tracting the unextractable: a case study on verb-
particles. In proceedings of COLING.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2012. Learning entailment relations by global graph
structure optimization. Computational Linguistics,
38(1):73?111.
Nathanael Chambers and Dan Jurafsky. 2008. Unsuper-
vised learning of narrative event chains. In Proceed-
ings of ACL.
Timothy Chklovski and Patrick Pantel. 2004. Verb
ocean: Mining the web for fine-grained semantic verb
relations. In Proceedings of EMNLP.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment chal-
lenge. In Machine Learning Challenges, volume 3944
of Lecture Notes in Computer Science, pages 177?190.
Springer.
Lin Dekang and Patrick Pantel. 2001. Dirt - discovery
of inference rules from text. In In Proceedings of the
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining.
Maayan Geffet and Ido Dagan. 2005. The distributional
inclusion hypotheses and lexical entailment. In Pro-
ceedings of ACL.
Isabelle Guyon and Andre Elisseeff. 2003. An intro-
duction to variable and feature selection. Journal of
Machine Learning Research, 3:1157?1182.
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko
Toyama. 2009. Supervised synonym acquisition us-
ing distributional features and syntactic patterns. In
Journal of Natural Language Processing.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING.
Jerry Hobbs. 1979. Coherence and coreference. Cogni-
tive Science, 3:67?90.
Eduard Hovy and Elisabeth Maier. 1993. Organizing
Discourse Structure Relations using Metafunctions.
Pinter Publishing.
Ray Jackendoff. 1983. Semantics and Cognition. The
MIT Press.
T. Joachims. 2005. A support vector method for mul-
tivariate performance measures. In Proceedings of
ICML.
Alistair Knott and Ted Sanders. 1998. The classification
of coherence relations and their linguistic markers: An
exploration of two languages. In Journal of Pragmat-
ics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language En-
gineering, 16(4):359?389.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University Of
Chicago Press.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of ICML.
William Mann and Sandra Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse rela-
tions. In Proceedings of ACL.
Shachar Mirkin, Ido Dagan, and Maayan Geffet. 2006.
Integrating pattern-based and distributional similarity
methods for lexical entailment acquisition. In Pro-
ceedings of the COLING/ACL.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of LREC.
203
Viktor Pekar. 2008. Discovery of event entailment
knowledge from text corpora. Comput. Speech Lang.,
22(1):1?16.
Marco Pennacchiotti and Patrick Pantel. 2009. Entity
extraction via ensemble semantics. In Proceedings of
EMNLP.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of ACL.
Deborah Schiffrin. 1988. Discourse Markers. Cam-
bridge University Press.
Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and
Daniel S. Weld. 2010. Learning first-order horn
clauses from web text. In Proceedings of EMNLP.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In Proceedings of NAACL-HLT.
Idan Szpektor and Ido Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of COLING.
Idan Szpektor, Hristo Tanev, and Ido Dagan. 2004. Scal-
ing web-based acquisition of entailment relations. In
In Proceedings of EMNLP.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007. In-
stance based evaluation of entailment rule acquisition.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics.
Galina Tremper. 2010. Weakly supervised learning of
presupposition relations between verbs. In Proceed-
ings of ACL student workshop.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
EMNLP.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using se-
lectional preferences. In Proceedings of the COL-
ING/ACL.
204
Proceedings of the NAACL HLT 2010: Tutorial Abstracts, pages 21?24,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Textual Entailment 
Mark Sammons, University of Illinois
Idan Szpektor, Yahoo!
V.G. Vinod Vydiswaran, University of Illinois
The NLP and ML communities are rising to grander, larger-scale
challenges such as Machine Reading, Learning by Reading, and Learning
to Read, challenges requiring deeper and more integrated natural
language understanding capabilities.
The task of Recognizing Textual Entailment (RTE) requires automated
systems to identify when two spans of text share a common meaning --
for example, that ``Alphaville Inc.'s attempted acquisition of Bauhaus
led to a jump in both companies' stock prices'' entails ``Bauahaus'
stock rose'', but not ``Alphaville acquired Bauhaus''.  This general
capability would be a solid proxy for Natural Language Understanding,
and has direct relevance to the grand challenges named above.
Moreover, it could be used to improve performance in a large range of
Natural Language Processing tasks such as Information Extraction,
Question Answering, Exhaustive Search, Machine Translation and many
others.  The operational definition of Textual Entailment used by
researchers in the field avoids commitment to any specific knowledge
representation, inference method, or learning approach, thus
encouraging application of a wide range of techniques to the problem.
Techniques developed for RTE have now been successfully applied in the
domains of Question Answering, Relation Extraction, and Machine
translation, and RTE systems continue to improve their performance
even as the corpora on which they are evaluated (provided first by
PASCAL, and now by NIST TAC) have become progressively more
challenging.  Over the sequence of RTE challenges from PASCAL and NIST
TAC, the more successful systems seem to have converged in their
overall approach.
The goal of this tutorial is to introduce the task of Recognizing
Textual Entailment to researchers from other areas of NLP.  We will
identify and analyze common inference and learning approaches from a
range of the more successful RTE systems, and investigate the role of
knowledge resources.  We will examine successful applications of RTE
techniques to Question Answering and Machine Translation, and identify
key research challenges that must be overcome to continue improving
RTE systems.
21
Tutorial Outline
1. Introduction (35 minutes)
Define and motivate the Recognizing Textual Entailment (RTE)
task. Introduce the RTE evaluation framework. Define the relationship
between RTE and other major NLP tasks.  Identify (some of) the
semantic challenges inherent in the RTE task, including the
introduction of 'contradiction' as an entailment category.  Describe
the use of RTE components/techniques in Question Answering, Machine
Translation, and Relation Extraction.
2. The State of the Art (35 minutes)
Outline the basic structure underlying RTE systems.  With reference to
recent publications on RTE: cover the range of preprocessing/analysis
that may be used; define representations/data structures typically
used; outline inference procedures and machine learning techniques.
Identify challenging aspects of the RTE problem in the context
of system successes and failures. 
3. Machine Learning for Recognizing Textual Entailment (35 minutes)
Describe the challenges involved in applying machine learning techniques
to the Textual Entailment problem.  Describe in more detail the main
approaches to inference, which explicitly or implicitly use the concept
of alignment.  Show how alignment fits into assumptions of semantic
compositionality, how it facilitates machine learning approaches, and
how it can accommodate phenomena-specific resources.  Show how it
can be used for contradiction detection.  
4. Knowledge Acquisition and Application in Textual Entailment (35 minutes)
Establish the role of knowledge resources in Textual Entailment,
and the consequent importance of Knowledge Acquisition.
Identify knowledge resources currently used in RTE systems, and their
limitations.  Describe existing knowledge acquisition approaches,
emphasizing the need for learning directional semantic relations.
Define suitable representations and algorithms for using knowledge,
including context-sensitive knowledge application.  Discuss the
problem of noisy data, and the prospects for new knowledge
resources/new acquisition approaches.
22
5. Key Challenges for Recognizing Textual Entailment (15 minutes)
Identify the key challenges in improving textual entailment systems:
more reliable inputs (when is a solved problem not solved), domain
adaptation, missing knowledge, scaling up.  The need for a common
entailment infrastructure to promote resource sharing and development.
Biographical Information of the Presenters
Mark Sammons
University of Illinois
201 N. Goodwin Ave.
Urbana, IL 61801 USA
Phone: 1-217-265-6759
Email: mssammon@illinois.edu
Mark Sammons is a Principal Research Scientist working with the Cognitive 
Computation Group at the University of Illinois.  His primary interests are in Natural 
Language Processing and Machine Learning, with a focus on integrating diverse 
information sources in the context of Textual Entailment. His work has focused on 
developing a Textual Entailment framework that can easily incorporate new resources; 
designing appropriate inference procedures for recognizing entailment; and identifying 
and developing automated approaches to recognize and represent implicit content in 
natural language text. Mark received his MSC in Computer Science from the University 
of Illinois in 2004, and his PhD in Mechanical Engineering from the University of Leeds, 
England, in 2000. 
Idan Szpektor
Yahoo! Research, Building 30 Matam Park, Haifa 31905, ISRAEL.
Phone: + 972-74-7924666; Email: idan@yahoo-inc.com 
Idan Szpektor is a Research Scientist at Yahoo! Research. His primary research 
interests are  in natural language processing, machine learning and information 
retrieval. Idan recently submitted his PhD thesis at Bar-Ilan University where he worked 
on unsupervised acquisition and application of broad-coverage knowledge-bases for 
Textual Entailment. He has been a main organizer of the second PASCAL Recognizing 
Textual Entailment Challenge and an advisor for the third RTE Challenge. He served on 
the program committees of EMNLP and TextInfer and reviewed papers for ACL, 
COLING and EMNLP. Idan Szpektor received his M.Sc. from Tel-Aviv University in 
2005, where he worked on unsupervised knowledge acquisition for Textual Entailment. 
V.G.Vinod Vydiswaran
University of Illinois
201 N. Goodwin Ave.
Urbana, IL 61801 USA
23
Phone: 1-217-333-2584
Email: vgvinodv@illinois.edu
V.G.Vinod Vydiswaran is a 3rd year Ph.D. student in the Department of Computer 
Science at the University of Illinois at Urbana-Champaign. His research interests include 
text informatics, natural language processing, machine learning, and information 
extraction. His work has included developing a Textual Entailment system, and applying 
Textual Entailment to relation extraction and information retrieval. He received his 
Masters degree from Indian Institute of Technology Bombay, India in 2004, where he 
worked on Conditional models for Information Extraction. Later, he worked at Yahoo! 
Research & Development Center at Bangalore, India, on scaling Information Extraction 
technologies over the Web.
24
Proceedings of the ACL 2010 Conference Short Papers, pages 241?246,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Generating Entailment Rules from FrameNet
Roni Ben Aharon
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
r.ben.aharon@gmail.com
Idan Szpektor
Yahoo! Research
Haifa, Israel
idan@yahoo-inc.com
Ido Dagan
Department of Computer Science
Bar-Ilan University
Ramat Gan, Israel
dagan@cs.biu.ac.il
Abstract
Many NLP tasks need accurate knowl-
edge for semantic inference. To this end,
mostly WordNet is utilized. Yet Word-
Net is limited, especially for inference be-
tween predicates. To help filling this gap,
we present an algorithm that generates
inference rules between predicates from
FrameNet. Our experiment shows that the
novel resource is effective and comple-
ments WordNet in terms of rule coverage.
1 Introduction
Many text understanding applications, such as
Question Answering (QA) and Information Ex-
traction (IE), need to infer a target textual mean-
ing from other texts. This need was proposed as a
generic semantic inference task under the Textual
Entailment (TE) paradigm (Dagan et al, 2006).
A fundamental component in semantic infer-
ence is the utilization of knowledge resources.
However, a major obstacle to improving semantic
inference performance is the lack of such knowl-
edge (Bar-Haim et al, 2006; Giampiccolo et al,
2007). We address one prominent type of infer-
ence knowledge known as entailment rules, focus-
ing specifically on rules between predicates, such
as ?cure X ? X recover?.
We aim at highly accurate rule acquisition,
for which utilizing manually constructed sources
seem appropriate. The most widely used manual
resource is WordNet (Fellbaum, 1998). Yet it is in-
complete for generating entailment rules between
predicates (Section 2.1). Hence, other manual re-
sources should also be targeted.
In this work1, we explore how FrameNet
(Baker et al, 1998) could be effectively used for
generating entailment rules between predicates.
1The detailed description of our work can be found in
(Ben Aharon, 2010).
FrameNet is a manually constructed database
based on Frame Semantics. It models the semantic
argument structure of predicates in terms of proto-
typical situations called frames.
Prior work utilized FrameNet?s argument map-
ping capabilities but took entailment relations
from other resources, namely WordNet. We
propose a novel method for generating entail-
ment rules from FrameNet by detecting the entail-
ment relations implied in FrameNet. We utilize
FrameNet?s annotated sentences and relations be-
tween frames to extract both the entailment rela-
tions and their argument mappings.
Our analysis shows that the rules generated by
our algorithm have a reasonable ?per-rule? accu-
racy of about 70%2. We tested the generated rule-
set on an entailment testbed derived from an IE
benchmark and compared it both to WordNet and
to state-of-the-art rule generation from FrameNet.
Our experiment shows that our method outper-
forms prior work. In addition, our rule-set?s per-
formance is comparable to WordNet and it is com-
plementary to WordNet when uniting the two re-
sources. Finally, additional analysis shows that
our rule-set accuracy is 90% in practical use.
2 Background
2.1 Entailment Rules and their Acquisition
To generate entailment rules, two issues should
be addressed: a) identifying the lexical entailment
relations between predicates, e.g. ?cure ? re-
cover?; b) mapping argument positions, e.g. ?cure
X ? X recover?. The main approach for gener-
ating highly accurate rule-sets is to use manually
constructed resources. To this end, most systems
mainly utilize WordNet (Fellbaum, 1998), being
the most prominent lexical resource with broad
coverage of predicates. Furthermore, some of its
2The rule-set is available at: http://www.cs.biu.
ac.il/?nlp/downloads
241
relations capture types of entailment relations, in-
cluding synonymy, hypernymy, morphologically-
derived, entailment and cause.
Yet, WordNet is limited for entailment rule gen-
eration. First, many entailment relations, no-
tably for the WordNet entailment and cause re-
lation types, are missing, e.g. ?elect ? vote?.
Furthermore, WordNet does not include argument
mapping between related predicates. Thus, only
substitutable WordNet relations (synonymy and
hypernymy), for which argument positions are
preserved, could be used to generate entailment
rules. The other non-substitutable relations, e.g.
cause (?kill ? die?) and morphologically-derived
(?meet.v? meeting.n?), cannot be used.
2.2 FrameNet
FrameNet (Baker et al, 1998) is a knowledge-
base of frames, describing prototypical situations.
Frames can be related to each other by inter-frame
relations, e.g. Inheritance, Precedence, Usage and
Perspective.
For each frame, several semantic roles are spec-
ified, called frame elements (FEs), denoting the
participants in the situation described. Each FE
may be labeled as core if it is central to the frame.
For example, some core FEs of the Commerce pay
frame are Buyer and Goods, while a non-core FE
is Place. Each FE may also be labeled with a se-
mantic type, e.g. Sentient, Event, and Time.
A frame includes a list of predicates that can
evoke the described situation, called lexical units
(LUs). LUs are mainly verbs but may also be
nouns or adjectives. For example, the frame Com-
merce pay lists the LUs pay.v and payment.n.
Finally, FrameNet contains annotated sentences
that represent typical LU occurrences in texts.
Each annotation refers to one LU in a specific
frame and the FEs of the frame that occur in the
sentence. An example sentence is ?IBuyer have to
pay the billsMoney?. Each sentence is accompa-
nied by a valence pattern, which provides, among
other info, grammatical functions of the core FEs
with respect to the LU. The valence pattern of the
above sentence is [(Buyer Subj), (Money Obj)].
2.3 Using FrameNet for Semantic Inference
To the best of our knowledge, the only work that
utilized FrameNet for entailment rule generation
is LexPar (Coyne and Rambow, 2009). LexPar
first identifies lexical entailment relations by go-
ing over all LU pairs which are either in the
same frame or whose frames are related by one of
FrameNet?s inter-frame relations. Each candidate
pair is considered entailing if the two LUs are ei-
ther synonyms or in a direct hypernymy relation in
WordNet (providing the vast majority of LexPar?s
relations), or if their related frames are connected
via the Perspective relation in FrameNet.
Then, argument mappings between each entail-
ing LU pair are extracted based on the core FEs
that are shared between the two LUs. The syntac-
tic positions of the shared FEs are taken from the
valence patterns of the LUs. A LexPar rule exam-
ple is presented in Figure 3 (top part).
Since most of LexPar?s entailment relations
are based on WordNet?s relations, LexPar?s rules
could be viewed as an intersection of WordNet and
FrameNet lexical relations, accompanied with ar-
gument mappings taken from FrameNet.
3 Rule Extraction from FrameNet
The above prior work identified lexical entailment
relations mainly from WordNet, which limits the
use of FrameNet in two ways. First, some rela-
tions that appear in FrameNet are missed because
they do not appear in WordNet. Second, unlike
FrameNet, WordNet does not include argument
mappings for its relations. Thus, prior work for
rule generation considered only substitutable rela-
tions from WordNet (synonyms and hypernyms),
not utilizing FrameNet?s capability to map argu-
ments of non-substitutable relations.
Our goal in this paper is to generate entail-
ment rules solely from the information within
FrameNet. We present a novel algorithm for gen-
erating entailment rules from FrameNet, called
FRED (FrameNet Entailment-rule Derivation),
which operates in three steps: a) extracting tem-
plates for each LU; b) detecting lexical entailment
relations between pairs of LUs; c) generating en-
tailment rules by mapping the arguments between
two LUs in each entailing pair.
3.1 Template Extraction
Many LUs in FrameNet are accompanied by an-
notated sentences (Section 2.2). From each sen-
tence of a given LU, we extract one template for
each annotated FE in the sentence. Each tem-
plate includes the LU, one argument correspond-
ing to the target FE and their syntactic relation
in the sentence parse-tree. We focus on extract-
ing unary templates, as they can describe any ar-
242
Figure 1: Template extraction for a sentence con-
taining the LU ?arrest?.
gument mapping by decomposing templates with
several arguments into unary ones (Szpektor and
Dagan, 2008). Figure 1 exemplifies this process.
As a pre-parsing step, all FE phrases in a given
sentence are replaced by their related FE names,
excluding syntactic information such as preposi-
tions or possessives (step (b) in Figure 1). Then,
the sentence is parsed using the Minipar depen-
dency parser (Lin, 1998) (step (c)). Finally, a
path in the parse-tree is extracted between each FE
node and the node of the LU (step (d)). Each ex-
tracted path is converted into a template by replac-
ing the FE node with an argument variable.
We simplify each extracted path by removing
nodes along the path that are not part of the syn-
tactic relation between the LU and the FE, such
as conjunctions and other FE nodes. For example,
?Authorities
subj
?? enter
conj
?? arrest? is simplified
into ?Authorities
subj
?? arrest?.
Some templates originated from different anno-
tated sentences share the same LU and syntactic
structure, but differ in their FEs. Usually, one of
these templates is incorrect, due to erroneous parse
(e.g. ?Suspect
obj
?? arrest? is a correct template, in
contrast to ?Charges
obj
?? arrest?). We thus keep
only the most frequently annotated template out of
the identical templates, assuming it is the correct
one.
3.2 Identifying Lexical Entailment Relations
FrameNet groups LUs in frames and describes re-
lations between frames. However, relations be-
tween LUs are not explicitly defined. We next de-
scribe how we automatically extract several types
of lexical entailment relations between LUs using
two approaches.
In the first approach, LUs in the same frame
that are morphological derivations of each other,
e.g. ?negotiation.n? and ?negotiate.v?, are marked
as paraphrases. We take morphological derivation
information from the CATVAR database (Habash
and Dorr, 2003).
The second approach is based on our observa-
tion that some LUs express the prototypical situ-
ation that their frame describes, which we denote
dominant LUs. For example, the LU ?recover? is
dominant for the Recovery frame. We mark LUs
as dominant if they are morphologically derived
from the frame?s name.
Our assumption is that since dominant LUs ex-
press the frame?s generic meaning, their meaning
is likely to be entailed by the other LUs in this
frame. Consequently, we generate such lexical
rules between any dominant LU and any other LU
in a given frame, e.g. ?heal? recover? and ?con-
valescence? recover? for the Recovery frame.
In addition, we assume that if two frames are
related by some type of entailment relation, their
dominant LUs are also related by the same rela-
tion. Accordingly, we extract entailment relations
between dominant LUs of frames that are con-
nected via the Inheritance, Cause and Perspective
relations, where Inheritance and Cause generate
directional entailment relations (e.g. ?choose ?
decide? and ?cure? recover?, respectively) while
Perspective generates bidirectional paraphrase re-
lations (e.g. ?transfer? receive?).
Finally, we generate the transitive closure of
the set of lexical relations identified by the above
methods. For example, the combination of ?sell?
buy? and ?buy? get? generates ?sell? get?.
3.3 Generating Entailment Rules
The final step in the FRED algorithm generates
lexical syntactic entailment rules from the ex-
tracted templates and lexical entailment relations.
For each identified lexical relation ?left? right?
between two LUs, the set of FEs that are shared by
both LUs is collected. Then, for each shared FE,
we take the list of templates that connect this FE
243
Lexical Relation:
cure? recovery
Templates:
Patient
obj
?? cure (cure Patient)
Affliction
of
?? cure (cure of Affliction)
Patient
gen
?? recovery (Patient?s recovery)
Patient
of
?? recovery (recovery of Patient)
Affliction
from
?? recovery (recovery from Affliction)
Intra-LU Entailment Rules:
Patient
gen
?? recovery?? Patient
of
?? recovery
Inter-LU Entailment Rules:
Patient
obj
?? cure =? Patient
gen
?? recovery
Patient
obj
?? cure =? Patient
of
?? recovery
Affliction
of
?? cure =? Affliction
from
?? recovery
Figure 2: Some entailment rules generated for the
lexical relation ?cure.v? recovery.n?.
Configuration R (%) P (%) F1
No-Rules 13.8 57.7 20.9
LexPar 14.1 42.9 17.4
WordNet 18.3 32.2 17.8
FRED 17.6 55.1 24.6
FRED ?WordNet 21.8 33.3 20.9
Table 1: Macro average Recall (R), Precision (P)
and F1 results for the tested configurations.
to each of the LUs, denoted by T feleft and T
fe
right.
Finally, for each template pair, l ? T feleft and r ?
T feright, the rule ?l ? r? is generated. In addition,
we generate paraphrase rules between the various
templates including the same FE and the same LU.
Figure 2 illustrates this process.
To improve rule quality, we filter out rules that
map FEs of adjunct-like semantic types, such as
Time and Location, since different templates of
such FEs may have different semantic meanings
(e.g. ?Time
before
?? arrive? ?Time
after
?? arrive?).
Thus, it is hard to identify those template pairs that
correctly map these FEs for entailment.
We manually evaluated a random sample of 250
rules from the resulting rule-set, out of which we
judged 69% as correct.
4 Application-based Evaluation
4.1 Experimental Setup
We would like to evaluate the overall utility of our
resource for NLP applications, assessing the cor-
rectness of the actual rule applications performed
in practice, as well as to compare its performance
to related resources. To this end, we follow the ex-
perimental setup presented in (Szpektor and Da-
gan, 2009), which utilized the ACE 2005 event
dataset3 as a testbed for entailment rule-sets. We
briefly describe this setup here.
The task is to extract argument mentions for
26 events, such as Sue and Attack, from the ACE
annotated corpus, using a given tested entailment
rule-set. Each event is represented by a set of
unary seed templates, one for each event argu-
ment. Some seed templates for Attack are ?At-
tacker
subj
??attack? and ?attack
obj
??Target?.
Argument mentions are found in the ACE cor-
pus by matching either the seed templates or tem-
plates entailing them found in the tested rule-set.
We manually added for each event its relevant
WordNet synset-ids and FrameNet frame-ids, so
only rules fitting the event target meaning will be
extracted from the tested rule-sets.
4.2 Tested Configurations
We evaluated several rule-set configurations:
No-Rules The system matches only the seed
templates directly, without any additional rules.
WordNet Rules are generated from WordNet
3.0, using only the synonymy and hypernymy rela-
tions (see Section 2.1). Transitive chaining of re-
lations is allowed (Moldovan and Novischi, 2002).
LexPar Rules are generated from the publicly
available LexPar database. We generated unary
rules from each LexPar rule based on a manually
constructed mapping from FrameNet grammatical
functions to Minipar dependency relations. Fig-
ure 3 presents an example of this procedure.
FRED Rules are generated by our algorithm.
FRED ?WordNet The union of the rule-sets of
FRED and WordNet.
4.3 Results
Each configuration was tested on each ACE event.
We measured recall, precision and F1. Table 1
reports macro averages of the three measures over
the 26 ACE events.
As expected, using No-Rules achieves the high-
est precision and the lowest recall compared to all
other configurations. When adding LexPar rules,
3http://projects.ldc.upenn.edu/ace/
244
LexPar rule:
Lexemes: arrest ?? apprehend
Valencies: [(Authorities Subj), (Suspect Obj), (Offense (for))] =? [(Authorities Subj), (Suspect Obj), (Offense (in))]
Generated unary rules:
X
subj
?? arrest =? X
subj
?? apprehend , arrest
obj
?? Y =? apprehend
obj
?? Y , arrest
for
?? Z =? apprehend
in
?? Z
Figure 3: An example for generation of unary entailment rules from a LexPar rule.
only a slight increase in recall is gained. This
shows that the subset of WordNet rules captured
by LexPar (Section 2.3) might be too small for the
ACE application setting.
When using all WordNet?s substitutable rela-
tions, a substantial relative increase in recall is
achieved (32%). Yet, precision decreases dramat-
ically (relative decrease of 44%), causing an over-
all decrease in F1. Most errors are due to correct
WordNet rules whose LHS is ambiguous. Since
we do not apply a WSD module, these rules are
also incorrectly applied to other senses of the LHS.
While this phenomenon is common to all rule-sets,
WordNet suffers from it the most since it contains
many infrequent word senses.
Our main result is that using FRED?s rule-set,
recall increases significantly, a relative increase
of 27% compared to No-Rules, while precision
hardly decreases. Hence, overall F1 is the high-
est compared to all other configurations (a rela-
tive increase of 17% compared to No-Rules). The
improvement in F1 is statistically significant com-
pared to all other configurations, according to the
two-sided Wilcoxon signed rank test at the level of
0.01 (Wilcoxon, 1945).
FRED preforms significantly better than LexPar
in both recall, precision and F1 (a relative increase
of 25%, 28% and 41% respectively). For example,
LexPar hardly utilizes FrameNet?s argument map-
ping capabilities since most of its rules are based
on a sub-set of WordNet?s substitutable relations.
FRED?s precision is substantially higher than
WordNet. This mostly results from the fact
that FrameNet mainly contains common senses
of predicates while WordNet includes many rare
word senses; which, as said above, harms preci-
sion when WSD is not applied. Error analysis
showed that only 7.5% of incorrect extractions are
due to erronous rules in FRED, while the majority
of errors are due to sense mismatch or syntactic
matching errors of the seed templates ot entailing
templates in texts.
FRED?s Recall is somewhat lower than Word-
Net, since FrameNet is a much smaller resource.
Yet, its rules are mostly complementary to those
from WordNet. This added value is demon-
strated by the 19% recall increase for the union of
FRED and WordNet rule-sets compared to Word-
Net alne. FRED provides mainly argument map-
pings for non-substitutable WordNet relations, e.g.
?attack.n on X ? attack.v X?, but also lexical re-
lations that are missing from WordNet, e.g. ?am-
bush.v? attack.v?.
Overall, our experiment shows that the rule-
base generated by FRED seems an appropri-
ate complementary resource to the widely used
WordNet-based rules in semantic inference and
expansion over predicates. This suggestion is es-
pecially appealing since our rule-set performs well
even when a WSD module is not applied.
5 Conclusions
We presented FRED, a novel algorithm for gener-
ating entailment rules solely from the information
contained in FrameNet. Our experiment showed
that FRED?s rules perform substantially better
than LexPar, the only prior rule-set derived from
FrameNet. In addition, FRED?s rule-set largely
complements the rules generated from WordNet
because it contains argument mappings between
non-substitutable predicates, which are missing
from WordNet, as well as lexical relations that are
not included in WordNet.
In future work we plan to investigate combin-
ing FrameNet and WordNet rule-sets in a transitive
manner, instead of their simple union.
Acknowledgments
This work was partially supported by the Rec-
tor?s research grant of Bar-Ilan University, the
PASCAL-2 Network of Excellence of the Eu-
ropean Community FP7-ICT-2007-1-216886 and
the Israel Science Foundation grant 1112/08.
245
References
Collin Baker, Charles Fillmore, and John Lowe. 1998.
The berkeley framenet project. In Proceedings of
COLING-ACL, Montreal, Canada.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second pascal recognising tex-
tual entailment challenge. In Second PASCAL Chal-
lenge Workshop for Recognizing Textual Entailment.
Roni Ben Aharon. 2010. Generating entailment rules
from framenet. Master?s thesis, Bar-Ilan University.
Robert Coyne and Owen Rambow. 2009. Lexpar: A
freely available english paraphrase lexicon automat-
ically extracted from framenet. In Proceedings of
the Third IEEE International Conference on Seman-
tic Computing.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Lecture Notes in Computer Science,
volume 3944, pages 177?190.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Massachusetts.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recogniz-
ing textual entailment challenge. In Proceedings of
the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing.
Nizar Habash and Bonnie Dorr. 2003. A categorial
variation database for english. In Proceedings of
the North American Association for Computational
Linguistics (NAACL ?03), pages 96?102, Edmonton,
Canada. Association for Computational Linguistics.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalu-
ation of Parsing Systems at LREC.
Dan Moldovan and Adrian Novischi. 2002. Lexical
chains for question answering. In Proceedings of
COLING.
Idan Szpektor and Ido Dagan. 2008. Learning en-
tailment rules for unary templates. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 849?856,
Manchester, UK, August.
Idan Szpektor and Ido Dagan. 2009. Augmenting
wordnet-based inference with argument mapping.
In Proceedings of the 2009 Workshop on Applied
Textual Inference, pages 27?35, Suntec, Singapore,
August.
Frank Wilcoxon. 1945. Individual comparisons by
ranking methods. Biometrics Bulletin, 1(6):80?83.
246
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 742?751,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Generating Synthetic Comparable Questions
for News Articles
Oleg Rokhlenko
Yahoo! Research
Haifa 31905, Israel
olegro@yahoo-inc.com
Idan Szpektor
Yahoo! Research
Haifa 31905, Israel
idan@yahoo-inc.com
Abstract
We introduce the novel task of automati-
cally generating questions that are relevant
to a text but do not appear in it. One mo-
tivating example of its application is for
increasing user engagement around news
articles by suggesting relevant compara-
ble questions, such as ?is Beyonce a bet-
ter singer than Madonna??, for the user
to answer. We present the first algorithm
for the task, which consists of: (a) of-
fline construction of a comparable ques-
tion template database; (b) ranking of rel-
evant templates to a given article; and (c)
instantiation of templates only with enti-
ties in the article whose comparison un-
der the template?s relation makes sense.
We tested the suggestions generated by
our algorithm via a Mechanical Turk ex-
periment, which showed a significant im-
provement over the strongest baseline of
more than 45% in all metrics.
1 Introduction
For companies whose revenues are mainly ad-
based, e.g. Facebook, Google and Yahoo, increas-
ing user engagement is an important goal, leading
to more time spent on site and consequently to in-
creased exposure to ads. Examples for typical en-
gaging content include other articles for the user to
read, updates from the user?s social neighborhood
and votes or comments on videos, blogs etc.
In this paper we propose a new way to increase
user engagement around news articles, namely
suggesting questions for the user to answer, which
are related to the viewed article. Our motivation
is that there are questions that are ?irresistible?
because they are fun, involve emotional reaction
and expect simple answers. These are comparative
questions, such as ?is Beyonce a better singer than
Madonna??, ?who is better looking, Brad Pitt or
George Clooney??, ?who is faster: Superman or
Flash?? and ?which camera brand do you prefer:
Canon or Nikon?? Furthermore, such questions
are social in nature since users would be inter-
ested in reading the opinions of other users, similar
to viewing other comments (Schuth et al, 2007).
Hence, a user that provided an answer may return
to view other answers, further increasing her en-
gagement with the site.
One approach for generating comparable ques-
tions would be to employ traditional question gen-
eration, which syntactically transform assertions
in a given text into questions (Mitkov et al, 2006;
Heilman and Smith, 2010; Rus et al, 2010).
Sadly, fun and engaging comparative questions
are typically not found within the text of news
articles. A different approach would be to find
concrete relevant questions within external col-
lections of manually generated comparable ques-
tions. Such collections include Community-based
Question Answering (CQA) sites such as Yahoo!
Answers and Baidu Zhidao and sites that are spe-
cialized in polls, such as Toluna. However, it
is highly unlikely that such sources will contain
enough relevant questions for any news article due
to typical sparseness issues as well as differences
in interests between askers in CQA sites and news
reporters. To better address the motivating appli-
cation above, we propose the novel task of au-
tomatically suggesting comparative questions that
are relevant to a given input news article but do not
appear in it.
To achieve broad coverage for our task, we
present an algorithm that generates synthetic con-
crete questions from question templates, such as
?Who is a better actor: #1 or #2??. Our algorithm
consists of two parts. An offline part constructs
a database of comparative question templates that
appear in a large question corpus. For a given
news article, an online part chooses relevant tem-
742
Figure 1: An example news article from OMG!
plates for the article by matching between the ar-
ticle content and typical template contexts. The
algorithm then instantiates each relevant template
with two entities that appear in the article. Yet,
for a given template, only some of the entities are
plausible slot fillers. For example, ?Madonna? is
not a reasonable filler for ?Who is a better dad, #1
or #2??. Thus, our algorithm employs entity filter-
ing to exclude candidate instantiations that do not
make sense.
To test the performance of our algorithm, we
conducted a Mechanical Turk experiment that as-
sessed the quality of suggested questions for news
articles on celebrities. We compared our algo-
rithm to a random baseline and to a partial ver-
sion of our algorithm that includes a template rel-
evance component but lacks filtering of candidate
instantiations. The results show that the full al-
gorithm provided 45% more correct instantiations,
but surprisingly also 46% more relevant sugges-
tions compared to the stronger baseline. These re-
sults point at the importance of both picking rel-
evant templates and smart instantiation selection
to the quality of generated questions. In addition,
they indicate that user perception of relevance is
affected by the correctness of the question.
2 Motivation and Algorithmic Overview
Before we detail our algorithm, we provide some
motivations and insights to the design choices we
took in our algorithm, which also indicate the dif-
ficulties inherent in the task.
2.1 Motivation
Given a news article, our algorithm generates a set
of comparable questions for the article from ques-
tion templates, e.g. ?who is faster #1 or #2??.
Though the template words typically do not ap-
pear in the article, they need to be relevant to it?s
content, that is they should correspond to one of
the main themes in the article or to one of the pub-
Figure 2: A high-level overview of the comparable
question generation algorithm. The offline part is
colored dark grey and the online part is colored
light blue.
lic interests of the compared entities. For example,
?who is a better dad #1 or #2?? is relevant to the
article in Figure 1, while ?who is faster #1 or #2??
is not relevant. Therefore, we need to model the
typical contents to which each template is relevant.
Looking at the structure of comparable ques-
tions, we observed that a specific comparable re-
lation, such as ?better dad? and ?faster?, can usu-
ally be combined with named entities in several
syntactic ways to construct a concrete question.
We encode this information in generic compara-
ble templates, e.g. ?who is a RE: #1 or #2?? and
?is #1 a RE than #2??, where RE is a slot for a
comparable relation and #1 and #2 are slots for en-
tities. Using the above generic templates, ?Jet Li?
and ?Jackie Chan? can be combined with the com-
parable relation ?better fighter? to generate ?who
is a better fighter: Jackie Chan or Jet Li?? and ?is
Jackie Chan a better fighter than Jet Li?? respec-
tively. Following, our algorithm separately main-
tains comparable relations and generic templates.
In this paper we constrain ourselves to generate
comparable questions between entities that appear
in the article. Yet, not all entities can be compared
to each other under a specific template, adding
substantial complexity to the generation of ques-
tions. Looking at Figure 1, the generated question
?who is faster, Angelina Jolie or David Beckham??
makes sense with respect to David Beckham, but
not with respect to Angelina Jolie, since the typi-
cal reader is rarely interested in her running skills.
Our algorithm thus needs to assess whether an in-
stantiation is correct, that is whether the compar-
ison between the two entities makes sense under
the specific template.
743
Further delving into question correctness, the
above example shows the need to assess each en-
tity by itself. However, even if both entities are
independently valid for the template, their com-
parison may not make sense. For example, ?who
is better looking: Will Smith or Angelina Jolie??
doesn?t feel right, even though each entity by itself
fits the template. This is because when comparing
looks, we expect a same sex comparison.
2.2 Algorithmic Overview
The above observations led us to the design of the
automatic generation algorithm depicted in Fig-
ure 2. The algorithm?s offline part constructs, from
a large collection of questions, a database of com-
parable relations, together with their typical con-
texts. It also extracts generic templates and the
mapping to the relations that may instantiate them.
From this database, we learn: (a) a context profile
per template for relevance matching; (b) a single
entity model per template slot that identify valid
instantiations; and (c) an entity pair model that
detects pairs of entities that can be compared to-
gether under the template. In the online part, these
three models are applied to rank relevant templates
for a given article and to generate only correct
questions with respect to template instantiation.
The next two sections detail the template extrac-
tion component and the model training and appli-
cation component in our algorithm.
3 Comparable Question Mining
To suggest comparable questions our algorithm
needs a database of question templates. As dis-
cussed previously, a good source for mining such
templates are CQA sites. Specifically, in this study
we utilize all questions submitted to Yahoo! An-
swers in 2011 as our corpus. We next describe
how comparable relations and generic comparable
templates are extracted from this corpus.
3.1 Comparable Relation Extraction
An important observation for the task of compa-
rable relation extraction is that many relations are
complex multiword expressions, and thus their au-
tomatic detection is not trivial. Examples for such
relations are marked in the questions ?Who is the
best rapper alive, Eminem or Jay-z?? and ?Who
is the most beautiful woman in the world, Adriana
Lima or Jessica Alba??. Therefore, we decided to
employ a Conditional Random Fields (CRF) tag-
ger (Lafferty et al, 2001) to the task, since CRF
was shown to be state-of-the-art for sequential re-
lation extraction (Mooney and Bunescu, 2005; Cu-
lotta et al, 2006; Jindal and Liu, 2006).
As a pre-processing step for detecting compara-
ble relations, our extraction algorithm identifies all
the named entities of interest in our corpus, keep-
ing only questions that contain at least two entities.
In each of remaining questions, we then substitute
the entity names with the variable slots #i in the
order of their appearance. For example, ?Nnamdi
Asomugha vs. Darrelle Revis? Who is the better
cornerback?? turned into ?#1 vs. #2? Who is the
better cornerback??. This transformation helps us
to design a simpler CRF than that of (Jindal and
Liu, 2006), since our CRF utilizes the known po-
sitions of the target entities in the text.
To train the CRF model, the authors manually
tagged all comparable relation words in approx-
imately 300 transformed questions in the filtered
corpus. The local and global features for the CRF,
which we induce from each question word, are
specified in Figures 3 and 4 respectively. Though
there are many questions in Yahoo! Answers con-
taining two named entities, e.g. ?Is #1 dating
#2??, our CRF tagger is trained to detect only
comparable relations like ?Who is prettier #1 or
#2??. This is due to the labeled training set, which
contains only this kind of relations, and to our fea-
tures, which capture aspects of this specific lin-
guistic structure.
The trained model was then applied to all other
questions in the filtered corpus. This tagging pro-
cess resulted in 60,000 identified question relation
occurrences. From this output we constructed a
database consisting of all occurring relations; each
relation is accompanied by its supporting ques-
tions, those questions in which the relation occur-
rences were found. To achieve a highly accurate
database, we filtered out relations with less than
50 supporting questions, ending with 295 relations
in our database1. The authors conducted a manual
evaluation of the CRF tagger performance, which
showed 80% precision per occurrence. Yet, our
filtering above of relations with low support left
us with virtually 100% precision per relation and
per occurrence.
1We intend to make this database publicly avail-
able under Yahoo! WebscopeTM (http://webscope.
sandbox.yahoo.com).
2http://nlp.stanford.edu/software/
744
(a) The word itself
(b) Whether the word is capitalized
(c) The word?s suffixes of length 1,2, and 3, which helps
in detecting comparative adjectives that ends ?est? or ?er?
(d) The word?s position in the sentence
(e) The word?s Part of speech (POS) tag, based on the
Stanford POS tagger2
(f) The words in a window of ?3 around the current one
(g) The adjective before the word, if exists, which helps
detecting comparative noun phrases, e.g. ?better driver?
and ?best singer?
(h) The shortest word distance between the word and one
of the #i variables.
(i) The shortest word distance of the word to one of the
following connectives: ?between?, ?out?, ?:?, ?,?, ???
Figure 3: CRF local features for each word
(a) WH question type of the question, e.g. what, which,
who, where
(b) The average word distance between all #i variables in
the question
(c) The conjunction tokens appearing between the #i vari-
ables, such as or, vs, and
Figure 4: CRF global features for each word
3.2 Comparable Template Extraction
Our second mining task is to extract generic com-
parable templates that appear in our corpus, as
well as identifying which comparable relation can
instantiate which generic template.
To this end, we replace each recognized rela-
tion sequence with a variable RE in the support
questions annotated with #i variables. For exam-
ple, ?who is the best rapper alive, #1 or #2?? is
transformed to ?who is RE, #1 or #2??. We next
count the occurrences of each templatized ques-
tion. While some questions contain many de-
tails besides the comparable generic template, oth-
ers are simpler and contain only the generic tem-
plate. Through this counting, frequently occurring
generic templates are revealed, such as ?is #1 a
RE than #2??. We retain only generic templates
which appeared more than 50 times.
Finally, for each comparable relation we mark
as applicable only generic templates that occur at
least once in the supporting questions of this rela-
tion. For example, the template ?who is RE: #1 or
#2?? was found applicable for ?funnier?, and thus
could be used to generate the concrete question
?who is funnier: Jennifer Aniston or Courteney
Cox??. On average, each relation was associated
with 3 generic templates.
Algorithm 1 A high level overview of the online
part of the question generation algorithm
Input: A news article
Output: A sorted list of comparable questions
1: Identify all target named entities (NEs) in the article
2: Infer the distribution of LDA topics for the article
3: For each comparable relation R in the database, compute
its relevance score to be the similarity between the topic
distributions of R and the article
4: Rank all the relations according to their relevance score
and pick the top M as relevant
5: for each relevant relation R in the order of relevance
ranking do
6: Filter out all the target NEs that do not pass the single
entity classifier for R
7: Generate all possible NE pairs from the those that
passed the single classifier
8: Filter out all the generated NE pairs that do not pass
the entity pair classifier for R
9: Pick up the top N pairs with positive classification
score to be qualified for generation
10: Instantiate R with each chosen NE pair via a ran-
domly selected generic template
11: end for
4 Online Question Generation
The online part of our automatic generation algo-
rithm takes as input a news article and generates
concrete comparable questions for it. Its high level
description is presented in Algorithm 1. The algo-
rithm starts with identifying the comparable rela-
tions in our database that are relevant to the arti-
cle. For each relevant relation, we then generate
concrete questions by picking generic templates
that are applicable for this relation and instantiat-
ing them with pairs of named entities appearing in
the article. Yet, as discussed before, only for some
entity pairs the comparison under the specific re-
lation makes sense, a quality which we refer to as
instantiation correctness (see Section 2). To this
end, we utilize two supervised models to filter in-
correct instantiations. We next detail the two as-
pects of the online part: ranking relevant relations
and correctly instantiating relations.
4.1 Ranking relevant relations
To assess how relevant a given comparable rela-
tion is to an article, we model the relation?s typ-
ical context as a distribution over latent semantic
topics. Specifically, we utilize Latent Dirichlet Al-
location (LDA) (Blei et al, 2003) to infer latent
topics in texts.
To train an LDA model, we constructed for each
comparable relation a pseudo-document consist-
ing of all questions that contain this relation in
our corpus (the supporting questions). We then
745
trained a model of 200 topics over these pseudo-
documents, resulting in a model over a lexicon of
107,835 words. An additional product of the LDA
training process is a topic distribution for each re-
lation?s pseudo-document, which we consider as
the relation?s context profile. We note that, un-
less otherwise specified, different model param-
eters were chosen based on a small held out col-
lection of articles and questions, manually anno-
tated by the authors. This collection was used to
validate that the chosen parameter values indeed
?make sense? for the task.
Given a news article, a distribution over LDA
topics is inferred from the article?s text using the
trained model. Then, a cosine similarity between
this distribution and the context profile of each
comparable relation in our database is computed
and taken as the relevance score for this relation.
Finally, we rank all relations according to their rel-
evance score and pick the top M as candidates for
instantiation (M=3 in our experiment).
4.2 Correctly instantiating relations
To generate useful questions from relevant com-
parable relations, we need to retain only correct
instantiations of these relations. To this end, we
utilize two complementing types of filters, one
for each entity by itself, and one for pairs, since
each filter considers different attributes of the en-
tities at hand. For example, for the relation ?is
faster?, the single entity filter looks for athletes of
all kinds, for whom this comparison is of interest
to the reader. The pair filter, on the other hand, at-
tempts to pass only same sex and same profession
comparisons, e.g. male football players or female
baseball players for this relation.
We next describe the various features we ex-
tract for every entity and the supervised models
that given this feature vector representation assess
the correctness of an instantiation.
4.2.1 Entity Features
We want to represent each entity as a vector of fea-
tures that capture different aspects of entity char-
acterization. To this end, we utilize two different
broad-scale sources of information about named
entities. The first is DBPedia3, which contains
structured information on entries in Wikipedia,
many of them are named entities that appear in
news articles. The second source is the corpus of
3http://wiki.dbpedia.org/About
CQA questions, which in our study was harvested
from Yahoo! Answers (see Section 3).
For named entities with a DBPedia en-
try, we extract all the DBPedia properties of
classes subject and type as indicator features.
Some example features for Brad Pitt include
Actors from Oklahoma, AmericanAtheists, Artist
and American film producers.
One property that is currently missing from DB-
Pedia is gender, a feature that was found to be very
useful in our experiments. We automatically in-
duce this feature from the Wikipedia abstract in
each DBPedia entry. Specifically, we construct a
histogram of male and female pronouns: he and
his vs. she and her. The majority pronoun sex is
then chosen to be the gender of the named entity,
or none if the histogram is empty.
One way to utilize the CQA question corpus
could be to extract co-occurring words with each
target entity as relevant contexts. Yet, since
our questions come from Yahoo! Answers, we
decided to use another attribute of the ques-
tions, the category to which the question is as-
signed, within a hierarchy of 1,669 categories
(e.g. ?Sports>Baseball? and ?Pets>Dogs?). For
each named entity, we construct a histogram of
the number of questions containing it that are as-
signed to each category. This histogram is normal-
ized into a probability distribution with Laplace
smoothing of 0.03, to incorporate the uncertainty
that lies in named entities that appear only very
few times. The categories and their probabilities
are added as features, providing a high level rep-
resentation of relevant contexts for the entity.
4.2.2 Single entity filtering
We view the task of single entity filtering as a clas-
sification task. To this end, we trained a classifier
per relation, constructing a different labeled train-
ing set for each relation. Positive examples are the
entities that instantiate this relation in our CQA
corpus. As negative examples, we take named en-
tities that were never seen instantiating the relation
in the corpus, but still occurred in some questions.
We note that our named entity tagger could recog-
nize more than 200,000 named entities, and most
of them are negative for a given relation.
For each relation we select negative examples
by sampling uniformly from its negative entity list,
assuming that the probability of hitting false neg-
atives is low for such a long list. It is known
that better classification performance is typically
746
achieved for a balanced training set (Provost,
2000). In our case, we over sample to help the
classifier explore the large space of negative ex-
amples. Specifically, we sample 2,000 negative
examples and duplicate the positive set to reach
a similar number.
We utilize the Support Vector Machines (SVM)
implementation of LIBSVM (Chang and Lin,
2011) with a linear kernel as our classifier. The
feature vector of each named entity was induced
as described in Section 4.2.1. We split the labeled
dataset into 70% training set and 30% validation
set. Feature selection using information gain was
performed on the training set to throw out non-
significant features (Mitchell, 1997). The average
accuracy of the single classifiers, measured over
the validation sets, was 91%.
4.2.3 Entity pair filtering
Similar to single entity filtering, we view the task
of filtering entity pairs as a classification task,
training a separate classifier for each relation. En-
tity pairs that instantiate the given relation in the
question corpus are considered positive examples.
Yet, the space of all the pairs that never instanti-
ated the relation is huge, and the set of positive
examples is relatively much smaller compared to
the situation in the single entity classifier. In our
study, uniform negative example sampling turned
the training into a trivial task, preventing from
the classifier to choose an useful discriminative
boundary. Therefore, we generate negative exam-
ples by sampling only from pairs of named enti-
ties that both pass the single entity filter for this
relation. The risk here is that we may sample false
negative examples. Still, this sampling scheme en-
abled the classifier to identify better discriminative
features.
To generate features for a candidate pair, we
take the two feature vectors of the two entities
and induce families of pair features by compar-
ing between the two vectors. Figure 5 describes
the various features we generate. We utilize LIB-
SVM with an RBF kernel for this task, splitting
the examples into 70% training set and 30% vali-
dation set. We over sampled the positive examples
to reach up to 100 examples.
The average accuracy of the pair classifiers on
the validation set was 83%. For example, named
entities that pass the single entity filtering for
?be funny?, include Jay Leno, David Letterman
(American TV hosts), Jim Carrey, and Steve Mar-
(a) All shared DBPedia indicator features in the two vec-
tors: fDBPediaa ? fDBPediab , indicating them as shared,e.g. ?FilmMaker s?
(b) All DBPedia features that appear only in one of the
vectors, termed one-side features: fDBPediaa \ fDBPediab
and fDBPediab \ fDBPediaa , indicating them as such, e.g.?FilmMaker o?
(c) Wikipedia categories that are ancestors of at least two
one-side features that appear in the training set. For ex-
ample, a common ancestor of ?Spanish actors? and ?Rus-
sian actors? is ?European actors?. These features provide
a high level perspective on one-side features
(d) The Yahoo! Answers categories in which both named
entities appear
(e) Hellinger distance (Pollard, 2001) between the proba-
bility distributions over categories of the two entities
(f) Three indicator gender features: whether both named
entities are males, both are females or are different
Figure 5: The entity pair features generated from
two single entity feature vectors fa and fb
tin (actors). The pair classifier assigned positive
scores only to {Jay Leno, David Letterman} (TV
hosts) and {Jim Carrey, Steve Martin} (actors) but
not to other pairings of these entities.
5 Evaluation
5.1 Experimental Settings
To evaluate our algorithm?s performance, we de-
signed a Mechanical Turk (MTurk) experiment in
which human annotators assess the quality of the
questions that our algorithm generates for a sam-
ple of news articles. As the source of test arti-
cles, we chose the OMG! website4, which contains
news articles on celebrities.
Test articles were selected by first randomly
sampling 5,000 news article from those that were
posted on OMG! in 2011. We then filtered out ar-
ticles that are longer than 4,000 characters, which
were found to be tiresome for annotators to read,
and those that are shorter than 300 characters,
which consist mainly of video and photos. We
were left with a pool of 1,016 articles from which
we randomly sampled 100 as the test set.
For each test article our algorithm obtained the
top three relevant comparable relations, and for
each relation selected the best instantiation (if ex-
ists). We used two baselines for performance com-
parison. The first random baseline chooses a rela-
tion randomly out of all possible relations in the
database and then instantiates it with a random
pair of entities that appear in the article. The sec-
ond relevance baseline chooses the most relevant
4http://www.omg.com/
747
Relevance Correctness
Random baseline 29% 43%
Relevance baseline 37% 53%
Full algorithm 54% 77%
Table 1: Relevance and correctness percentage by
tested algorithm
relation to the article based on our algorithm, but
still instantiates it with a random pair. For each test
article, we presented to the evaluators the ques-
tions generated by the three tested algorithms in
a random order to avoid any bias. We note that
our second baseline enabled us to measure the
stand-alone contribution of the LDA-based rele-
vance model. In addition, it enabled us to measure
the relative contribution of the instantiation mod-
els on top of relevance model.
Each article was evaluated by 10 MTurk work-
ers, which were asked to mark for each displayed
question whether it is relevant and whether it is
correct (see Section 2 for relevance and correct-
ness definitions). The workers were given pre-
cise instructions along with examples before they
started the test. A control story was used to filter
out dishonest or incapable workers5.
5.2 Results
For each tested algorithm, we separately counted
the percentage of annotations that marked each
question as relevant and the percentage of anno-
tations that marked each question as instantiated
correctly, denoted relevance score and correctness
score. We then averaged these scores over all
questions that were displayed for the test articles.
The results are presented in Table 1. The differ-
ences between the full algorithm and the baselines
are statistically significant at p < 0.01 and between
baselines the differences are statistically signifi-
cant at p < 0.05 using the Wilcoxon double-sided
signed-ranks test (Wilcoxon, 1945).
Our main result is that our full algorithm sub-
stantially outperforms the stronger relevance base-
line. It improves the correctness score by 45%,
which points at the effectiveness of our two step
filtering of incorrect instantiations. It?s perfor-
mance is just under 80%, showing high quality
entity pair selection for relations. Yet, we did not
expect to see an increase of 46% in the relevance
5We intend to make the tested articles, the instructions
to annotators and their annotations publicly available under
Yahoo! WebscopeTM (http://webscope.sandbox.
yahoo.com).
metric, since both the full algorithm and the rele-
vance baseline use the same relevance component
to rank relations by. One explanation for this is
that sometimes the instantiation filter eliminates
all possible entity pairs for some relation that is
incorrectly considered relevant by the algorithm.
Thus, the filtering of entities provides also an ad-
ditional filtering perspective on relevance. In ad-
dition, it may be that humans tend to be more
permissive when assessing the relevance of a cor-
rectly instantiated question.
To illustrate the differences between baselines
and the full algorithm, Table 2 presents an exam-
ple article together with the suggested generated
questions by each algorithm. The random baseline
picked an irrelevant relation, and while the rele-
vance baseline selected a relevant relation, ?a bet-
ter president?, it was instantiated incorrectly. The
full algorithm, on the other hand, both chose rel-
evant relations for all three questions and instan-
tiated them correctly. Especially, the incorrectly
instantiated relation in the relevance baseline is
now correctly instantiated with plausible presiden-
tial candidates.
Comparing between baselines, the relevance
baseline beats the random baseline by 28% in
terms of relevance. This is not surprising, since
this was the focus of this baseline. Yet, it also im-
proved correctness by 23% over the random base-
line. This is an unexpected result that indicates
that when users view relevant relations, they may
be more forgiving in their perception of unreason-
able instantiations.
For each article, our full algorithm attempts to
generate three questions, one for each of the top
three relevant questions. It is possible that for
some articles not all three questions will be gen-
erated, due to instantiation filtering. We found
that for 85% of the articles all three questions
were generated. For the remaining 15% at least
one question was always generated, and for 13 ofthem two questions were composed. Furthermore,
we found that the relevance and correctness scores
were not affected by the position of the question.
In the case of instantiation correctness, since the
best pair was picked for each relation and this
component is quite accurate, this is somewhat ex-
pected. In the case of relevance, this indicates that
there are usually several relations in our database
that are relevant to the article.
748
Ron Livingston is teaming up with Tom Hanks and HBO again after their successful 2001 collaboration on Band of Broth-
ers. The actor has been cast in HBO?s upcoming film Game Change that centers on the 2008 presidential campaign,
Deadline reports. He joins Ed Harris, Julianne Moore and Woody Harrelson. The Jay Roach-directed movie follows John
McCain (Harris) as he selects Alaska Gov. Sarah Palin (Moore) as his running mate, throughout the campaign and to their
ultimate defeat to Barack Obama. Livingston will play Mark Wallace, one of the campaign?s senior advisors and the man
who prepped Palin for her debate. Harrelson will play campaign strategist Steve Schmidt. . .
Algorithm Question
Random baseline Who is a better singer, Sarah Palin or Barack Obama ?
Relevance baseline Would Ron Livingston be a better president than Julianne Moore ?
Full algorithm Who has the best movies Tom Hanks or Julianne Moore ?
Full algorithm Is John Mccain a better leader than Barack Obama ?
Full algorithm Would Sarah Palin be a better president than John Mccain ?
Table 2: Automatically generated questions by the baselines and the full algorithm to an example article
5.3 Error Analysis
To better understand the performance of our algo-
rithm, we looked at some low quality questions
that were generated, either due to incorrect instan-
tiation or due to irrelevance to the article.
Starting with relevance, one of the repeating
mistakes was promoting relations that are related
to a list of named entities in the article, but not to
its main theme. For example, the relation ?who is
a better actor? was incorrectly ranked high for an
article about Ricky Gervais claiming that he has
been asked to host Globes again after he offended
Angelina Jolie, Johnny Depp, Robert Downey
Jr. and Charlie Sheen, among others during last
Globes ceremony. The reason for this mistake is
that many named entities appear as frequent terms
in LDA topics, and thus mentioning many names
that belong to a single topic drives LDA to as-
sign this topic a high probability. Yet, unlike other
cases, here entity filtering does not help ignoring
such errors, since the same entities that triggered
the ranking of the relation are also valid instantia-
tions for it.
Analyzing incorrect instantiations, many mis-
takes are due to mismatches between the two com-
pared entities that were too fine grained for our al-
gorithm to catch. For example, ?who?s the better
guitarist: Paul McCartney or Ringo Starr?? was
generated since our algorithm failed to identify
that Ringo Starr is a drummer rather than a gui-
tarist, though both participants in the relation are
musicians. In other cases, strong co-occurrence of
the two celebs in our question corpus convinced
the classifiers that they can be matched. For ex-
ample, ?who is a better dancer Michael Jackson
or Debbie Rowe?? was incorrectly generated,
since Debbie Rowe is not a dancer. Yet, she was
Michael Jackson?s wife and they appear together
in a lot of questions in our corpus.
6 Related Work
Traditionally, question generation focuses on con-
verting assertions in a text into question forms
(Brown et al, 2005; Mitkov et al, 2006; Myller,
2007; Heilman and Smith, 2010; Rus et al, 2010;
Agarwal et al, 2011; Olney et al, 2012). To
the best of our knowledge, there is no prior work
on our task, which is to generate relevant syn-
thetic questions whose content, except for the ar-
guments, might not appear in the text.
Our extraction of comparable relations falls
within the field of Relation Extraction, in which
CRF is a state-of-the-art method (Mooney and
Bunescu, 2005; Culotta et al, 2006). We note
that in the works of Jindal and Liu (2006) and
Li et. al. (2010) comparative questions are iden-
tified as an intermediate step for the task of ex-
tracting compared entities, which are unknown in
their setting. We, on the other hand, detect the
compared entities in a pre-processing step, and our
target is the extraction of the comparable relations
given known candidate entities.
Our algorithm ranks relevant templates based
on the similarity between an article?s content and
the typical context of each relation. Prior work
rank relevant concrete questions to a given in-
put question, focusing on strong lexical similari-
ties (Jeon et al, 2005; Cai et al, 2011; Hao and
Agichtein, 2012). We, however, do not expect to
find direct lexical similarities between candidate
relations and the article. Instead, we are interested
in a higher level topical similarity to the input ar-
ticle, for which LDA topics were shown to help
(Celikyilmaz et al, 2010).
Finally, several works present unsupervised
methods for ranking proper template instantia-
749
tions, mainly as selectional preferences (Light and
Greiff, 2002; Erk, 2007; Ritter et al, 2010). How-
ever, we eventually choose instantiation candi-
dates, and thus preferred supervised methods that
enable filtering and not just ranking. Furthermore,
we target a more subtle discrimination between
entities than prior work, e.g. between quarter-
backs, singers and actors. Machine learning nat-
urally incorporates the many features that capture
different aspects of entity characterization.
7 Conclusions
We introduced the novel task of automatically gen-
erating synthetic comparable questions that are
relevant to a given news article but do not neces-
sarily appear in it. To this end, we proposed an
algorithm that consists of two parts. The offline
part identifies comparable relations in a large col-
lection of questions. Its output is a database of
comparable relations together with a context pro-
file for each relation and models that detect cor-
rect instantiations of this relation, all learned from
the question corpus. In the online part, given a
news article, the algorithm identifies relevant com-
parable relations based on the similarity between
the article content and each relation?s context pro-
file. Then, relevant relations are instantiated only
with pairs of named entities from the article whose
comparison makes sense by applying the instanti-
ation correctness models to candidate pairs.
We assessed the performance of our algorithm
via a Mechanical Turk experiment. A partial ver-
sion of our algorithm, without instantiation filter-
ing, was our strongest baseline. The full algorithm
outperformed this baseline by 45% on question
correctness, but surprisingly also by 46% on ques-
tion relevance. These results show that our super-
vised filtering methods are successful in keeping
only correct pairs, but they also serve as an ad-
ditional filtering for relevant relations, on top of
context matching.
In future work, we want to generate more di-
verse and intriguing questions by selecting rele-
vant named entities for template instantiation that
do not appear in the article. Another direction
would be take a supervised approach, training
classifiers over a labeled dataset for filtering irrel-
evant templates and incorrect instantiations. Fi-
nally, it would be interesting to see how our algo-
rithm performs on other news domains.
References
Manish Agarwal, Rakshit Shah, and Prashanth Man-
nem. 2011. Automatic question generation using
discourse cues. In Proceedings of the 6th Workshop
on Innovative Use of NLP for Building Educational
Applications, IUNLPBEA ?11, pages 1?9, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Jonathan C. Brown, Gwen A. Frishkoff, and Maxine
Eskenazi. 2005. Automatic question generation for
vocabulary assessment. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ?05, pages 819?826, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Li Cai, Guangyou Zhou, Kang Liu, and Jun Zhao.
2011. Learning the latent topics for question re-
trieval in community qa. In Proceedings of 5th In-
ternational Joint Conference on Natural Language
Processing, pages 273?281, Chiang Mai, Thailand,
November. Asian Federation of Natural Language
Processing.
Asli Celikyilmaz, Dilek Hakkani-Tur, and Gokhan Tur.
2010. Lda based similarity modeling for question
answering. In Proceedings of the NAACL HLT 2010
Workshop on Semantic Search, SS ?10, pages 1?9,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM TIST,
2(3):27.
Aron Culotta, Andrew McCallum, and Jonathan Betz.
2006. Integrating probabilistic extraction models
and data mining to discover relations and patterns in
text. In Proceedings of the main conference on Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, HLT-NAACL ?06, pages 296?
303, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 216?223, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Tianyong Hao and Eugene Agichtein. 2012. Finding
similar questions in collaborative question answer-
ing archives: toward bootstrapping-based equivalent
pattern learning. Inf. Retr., 15(3-4):332?353, June.
Michael Heilman and Noah A. Smith. 2010. Good
question! statistical ranking for question generation.
750
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
?10, pages 609?617, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of the 14th ACM in-
ternational conference on Information and knowl-
edge management, CIKM ?05, pages 84?90, New
York, NY, USA. ACM.
Nitin Jindal and Bing Liu. 2006. Mining comparative
sentences and relations. In proceedings of the 21st
national conference on Artificial intelligence - Vol-
ume 2, AAAI?06, pages 1331?1336. AAAI Press.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML.
Shasha Li, Chin-Yew Lin, Young-In Song, and Zhou-
jun Li. 2010. Comparable entity mining from com-
parative questions. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 650?658. Association
for Computational Linguistics.
Marc Light and Warren R. Greiff. 2002. Statistical
models for the induction and use of selectional pref-
erences. Cognitive Science, 26(3):269?281.
Tom M. Mitchell. 1997. Machine learning. McGraw
Hill series in computer science. McGraw-Hill.
Ruslan Mitkov, Le An Ha, and Nikiforos Karamanis.
2006. A computer-aided environment for gener-
ating multiple-choice test items. Nat. Lang. Eng.,
12(2):177?194, June.
Raymond J. Mooney and Razvan Bunescu. 2005. Min-
ing knowledge from text using information extrac-
tion. SIGKDD Explor. Newsl., 7(1):3?10, June.
Niko Myller. 2007. Automatic generation of predic-
tion questions during program visualization. Elec-
tron. Notes Theor. Comput. Sci., 178:43?49, July.
A.M. Olney, A.C. Graesser, and N.K. Person. 2012.
Question generation from concept maps. Dialogue
& Discourse, 3(2):75?99.
D. Pollard. 2001. A User?s Guide to Measure Theo-
retic Probability. Cambridge University Press.
F. Provost. 2000. Machine learning from imbalanced
data sets 101. Proceedings of the AAAI-2000 Work-
shop on Imbalanced Data Sets.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent dirichlet alocation method for selectional pref-
erences. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics,
ACL ?10, pages 424?434, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Vasile Rus, Brendan Wyse, Paul Piwek, Mihai C. Lin-
tean, Svetlana Stoyanchev, and Cristian Moldovan.
2010. The first question generation shared task eval-
uation challenge. In John D. Kelleher, Brian Mac
Namee, Ielka van der Sluis, Anja Belz, Albert Gatt,
and Alexander Koller, editors, INLG 2010 - Pro-
ceedings of the Sixth International Natural Lan-
guage Generation Conference, July 7-9, 2010, Trim,
Co. Meath, Ireland. The Association for Computer
Linguistics.
Anne Schuth, Maarten Marx, and Maarten de Rijke.
2007. Extracting the discussion structure in com-
ments on news-articles. In Proceedings of the 9th
annual ACM international workshop on Web infor-
mation and data management, WIDM ?07, pages
97?104, New York, NY, USA. ACM.
Frank Wilcoxon. 1945. Individual comparisons by
ranking methods. Biometrics Bulletin, 1:80?83.
751
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1331?1340,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Two Level Model for Context Sensitive Inference Rules
Oren Melamud?, Jonathan Berant?, Ido Dagan?, Jacob Goldberger?, Idan Szpektor?
? Computer Science Department, Bar-Ilan University
? Computer Science Department, Stanford University
? Faculty of Engineering, Bar-Ilan University
? Yahoo! Research Israel
{melamuo,dagan,goldbej}@{cs,cs,eng}.biu.ac.il
joberant@stanford.edu
idan@yahoo-inc.com
Abstract
Automatic acquisition of inference rules
for predicates has been commonly ad-
dressed by computing distributional simi-
larity between vectors of argument words,
operating at the word space level. A re-
cent line of work, which addresses context
sensitivity of rules, represented contexts in
a latent topic space and computed similar-
ity over topic vectors. We propose a novel
two-level model, which computes simi-
larities between word-level vectors that
are biased by topic-level context repre-
sentations. Evaluations on a naturally-
distributed dataset show that our model
significantly outperforms prior word-level
and topic-level models. We also release a
first context-sensitive inference rule set.
1 Introduction
Inference rules for predicates have been identi-
fied as an important component in semantic ap-
plications, such as Question Answering (QA)
(Ravichandran and Hovy, 2002) and Information
Extraction (IE) (Shinyama and Sekine, 2006). For
example, the inference rule ?X treat Y ? X relieve
Y? can be useful to extract pairs of drugs and the
illnesses which they relieve, or to answer a ques-
tion like ?Which drugs relieve headache??. Along
this vein, such inference rules constitute a crucial
component in generic modeling of textual infer-
ence, under the Textual Entailment paradigm (Da-
gan et al, 2006; Dinu and Wang, 2009).
Motivated by these needs, substantial research
was devoted to automatic learning of inference
rules from corpora, mostly in an unsupervised dis-
tributional setting. This research line was mainly
initiated by the highly-cited DIRT algorithm (Lin
and Pantel, 2001), which learns inference for bi-
nary predicates with two argument slots (like the
rule in the example above). DIRT represents a
predicate by two vectors, one for each of the ar-
gument slots, where the vector entries correspond
to the argument words that occurred with the pred-
icate in the corpus. Inference rules between pairs
of predicates are then identified by measuring the
similarity between their corresponding argument
vectors. This general scheme was further en-
hanced in several directions, e.g. directional sim-
ilarity (Bhagat et al, 2007; Szpektor and Dagan,
2008) and meta-classification over similarity val-
ues (Berant et al, 2011). Consequently, several
knowledge resources of inference rules were re-
leased, containing the top scoring rules for each
predicate (Schoenmackers et al, 2010; Berant et
al., 2011; Nakashole et al, 2012).
The above mentioned methods provide a sin-
gle confidence score for each rule, which is based
on the obtained degree of argument-vector sim-
ilarities. Thus, a system that applies an infer-
ence rule to a text may estimate the validity of
the rule application based on the pre-specified rule
score. However, the validity of an inference rule
may depend on the context in which it is applied,
such as the context specified by the given predi-
cate?s arguments. For example, ?AT&T acquire T-
Mobile ? AT&T purchase T-Mobile?, is a valid
application of the rule ?X acquire Y ? X pur-
chase Y?, while ?Children acquire skills ? Chil-
dren purchase skills? is not. To address this issue, a
line of works emerged which computes a context-
sensitive reliability score for each rule application,
based on the given context.
The major trend in context-sensitive inference
models utilizes latent or class-based methods for
context modeling (Pantel et al, 2007; Szpektor et
al., 2008; Ritter et al, 2010; Dinu and Lapata,
2010b). In particular, the more recent methods
(Ritter et al, 2010; Dinu and Lapata, 2010b) mod-
eled predicates in context as a probability distribu-
tion over topics learned by a Latent Dirichlet Allo-
1331
cation (LDA) model. Then, similarity is measured
between the two topic distribution vectors corre-
sponding to the two sides of the rule in the given
context, yielding a context-sensitive score for each
particular rule application.
We notice at this point that while context-
insensitive methods represent predicates by ar-
gument vectors in the original fine-grained word
space, context-sensitive methods represent them
as vectors at the level of latent topics. This raises
the question of whether such coarse-grained topic
vectors might be less informative in determining
the semantic similarity between the two predi-
cates.
To address this hypothesized caveat of prior
context-sensitive rule scoring methods, we pro-
pose a novel generic scheme that integrates word-
level and topic-level representations. Our scheme
can be applied on top of any context-insensitive
?base? similarity measure for rule learning, which
operates at the word level, such as Cosine or
Lin (Lin, 1998). Rather than computing a single
context-insensitive rule score, we compute a dis-
tinct word-level similarity score for each topic in
an LDA model. Then, when applying a rule in a
given context, these different scores are weighed
together based on the specific topic distribution
under the given context. This way, we calculate
similarity over vectors in the original word space,
while biasing them towards the given context via
a topic model.
In order to promote replicability and equal-term
comparison with our results, we based our experi-
ments on publicly available datasets, both for un-
supervised learning of the evaluated models and
for testing them over a random sample of rule ap-
plications. We apply our two-level scheme over
three state-of-the-art context-insensitive similar-
ity measures. The evaluation compares perfor-
mances both with the original context-insensitive
measures and with recent LDA-based context-
sensitive methods, showing consistent and robust
advantages of our scheme. Finally, we release
a context-sensitive rule resource comprising over
2,000 frequent verbs and one million rules.
2 Background and Model Setting
This section presents components of prior work
which are included in our model and experiments,
setting the technical preliminaries for the rest of
the paper. We first present context-insensitive rule
learning, based on distributional similarity at the
word level, and then context-sensitive scoring for
rule applications, based on topic-level similarity.
Some further discussion of related work appears
in Section 6.
2.1 Context-insensitive Rule Learning
A predicate inference rule ?LHS ? RHS?, such
as ?X acquire Y ? X purchase Y?, specifies a
directional inference relation between two predi-
cates. Each rule side consists of a lexical pred-
icate and (two) variable slots for its arguments.1
Different representations have been used to spec-
ify predicates and their argument slots, such as
word lemma sequences, regular expressions and
dependency parse fragments. A rule can be ap-
plied when its LHS matches a predicate with a
pair of arguments in a text, allowing us to infer its
RHS, with the corresponding instantiations for the
argument variables. For example, given the text
?AT&T acquires T-Mobile?, the above rule infers
?AT&T purchases T-Mobile?.
The DIRT algorithm (Lin and Pantel, 2001)
follows the distributional similarity paradigm to
learn predicate inference rules. For each predi-
cate, DIRT represents each of its argument slots
by an argument vector. We denote the two vectors
of the X and Y slots of a predicate pred by vxpred
and vypred, respectively. Each entry of a vector vcorresponds to a particular word (or term) w that
instantiated the argument slot in a learning corpus,
with a value v(w) = PMI(pred, w) (with PMI
standing for point-wise mutual information).
To learn inference rules, DIRT considers (in
principle) each pair of binary predicates that
occurred in the corpus for a candidate rule,
?LHS ? RHS?. Then, DIRT computes a reliabil-
ity score for the rule by combining the measured
similarities between the corresponding argument
vectors of the two rule sides. Concretely, denot-
ing by l and r the predicates appearing in the two
rule sides, DIRT?s reliability score is defined as
follows:
(1)scoreDIRT(LHS ? RHS)
=
?
sim(vxl , vxr ) ? sim(v
y
l , v
y
r )
where sim(v, v?) is a vector similarity measure.
Specifically, DIRT employs the Lin similarity
1We follow most of the inference-rule learning literature,
which focused on binary predicates. However, our context-
sensitive scheme can be applied to any arity.
1332
measure from (Lin, 1998), defined as follows:
(2)Lin(v, v?) =
?
w?v?v? [v(w) + v?(w)]?
w?v?v? [v(w) + v?(w)]
We note that the general DIRT scheme may be
used while employing other ?base? vector similar-
ity measures. For example, the Lin measure is
symmetric, and thus using it would yield the same
reliability score when swapping the two sides of
a rule. This issue has been addressed in a sepa-
rate line of research which introduced directional
similarity measures suitable for inference rela-
tions (Bhagat et al, 2007; Szpektor and Dagan,
2008; Kotlerman et al, 2010). In our experiments
we apply our proposed context-sensitive similarity
scheme over three different base similarity mea-
sures.
DIRT and similar context-insensitive inference
methods provide a single reliability score for a
learned inference rule, which aims to predict the
validity of the rule?s applications. However, as
exemplified in the Introduction, an inference rule
may be valid in some contexts but invalid in oth-
ers (e.g. acquiring entails purchasing for goods,
but not for skills). Since vector similarity in DIRT
is computed over the single aggregate argument
vector, the obtained reliability score tends to be
biased towards the dominant contexts of the in-
volved predicates. For example, we may expect
a higher score for ?acquire ? purchase? than for
?acquire ? learn?, since the former matches a
more frequent sense of acquire in a typical corpus.
Following this observation, it is desired to obtain
a context-sensitive reliability score for each rule
application in a given context, as described next.
2.2 Context-sensitive Rule Applications
To assess the reliability of applying an inference
rule in a given context we need some model for
context representation, that should affect the rule
reliability score. A major trend in past work is
to represent contexts in a reduced-dimensionality
latent or class-based model. A couple of earlier
works utilized a cluster-based model (Pantel et al,
2007) and an LSA-based model (Szpektor et al,
2008), in a selectional-preferences style approach.
Several more recent works utilize a Latent Dirich-
let Allocation (LDA) (Blei et al, 2003) frame-
work. We now present an underlying unified view
of the topic-level models in (Ritter et al, 2010;
Dinu and Lapata, 2010b), which we follow in our
own model and in comparative model evaluations.
We note that a similar LDA model construction
was employed also in (Se?aghdha, 2010), for esti-
mating predicate-argument likelihood.
First, an LDA model is constructed, as follows.
Similar to the construction of argument vectors
in the distributional model (described above in
subsection 2.1), all arguments instantiating each
predicate slot are extracted from a large learning
corpus. Then, for each slot of each predicate, a
pseudo-document is constructed containing the set
of all argument words that instantiated this slot in
the corpus. We denote the two documents con-
structed for the X and Y slots of a predicate pred
by dxpred and dypred, respectively. In comparison tothe distributional model, these two documents cor-
respond to the analogous argument vectors vxpred
and vypred, both containing exactly the same set ofwords.
Next, an LDA model is learned from the set
of all pseudo-documents, extracted for all predi-
cates.2 The learning process results in the con-
struction of K latent topics, where each topic t
specifies a distribution over all words, denoted by
p(w|t), and a topic distribution for each pseudo-
document d, denoted by p(t|d).
Within the LDA model we can derive the
a-posteriori topic distribution conditioned on a
particular word within a document, denoted by
p(t|d,w) ? p(w|t) ? p(t|d). In the topic-level
model, d corresponds to a predicate slot and w to
a particular argument word instantiating this slot.
Hence, p(t|d,w) is viewed as specifying the rele-
vance (or likelihood) of the topic t for the predi-
cate slot in the context of the given argument in-
stantiation. For example, for the predicate slot ?ac-
quire Y? in the context of the argument ?IBM?, we
expect high relevance for a topic about companies,
while in the context of the argument ?knowledge?
we expect high relevance for a topic about abstract
concepts. Accordingly, the distribution p(t|d,w)
over all topics provides a topic-level representa-
tion for a predicate slot in the context of a particu-
lar argument w. This representation is used by the
topic-level model to compute a context-sensitive
score for inference rule applications, as follows.
2We note that there are variants in the type of LDA model
and the way the pseudo-documents are constructed in the
referenced prior work. In order to focus on the inference
methods rather than on the underlying LDA model, we use
the LDA framework described in this paper for all compared
methods.
1333
Consider the application of an inference rule
?LHS ? RHS? in the context of a particular pair
of arguments for the X and Y slots, denoted by
wx and wy, respectively. Denoting by l and r the
predicates appearing in the two rule sides, the reli-
ability score of the topic-level model is defined as
follows (we present a geometric mean formulation
for consistency with DIRT):
(3)scoreTopic(LHS ? RHS, wx, wy)
=
?
sim(dxl , dxr , wx) ? sim(d
y
l , d
y
r , wy)
where sim(d, d?, w) is a topic-distribution similar-
ity measure conditioned on a given context word.
Specifically, Ritter et al (2010) utilized the dot
product form for their similarity measure:
(4)simDC(d, d?, w) = ?t[p(t|d,w) ? p(t|d?, w)]
(the subscript DC stands for double-conditioning,
as both distributions are conditioned on the argu-
ment word, unlike the measure below).
Dinu and Lapata (2010b) presented a slightly
different similarity measure for topic distributions
that performed better in their setting as well as in a
related later paper on context-sensitive scoring of
lexical similarity (Dinu and Lapata, 2010a). In this
measure, the topic distribution for the right hand
side of the rule is not conditioned on w:
(5)simSC(d, d?, w) = ?t[p(t|d,w) ? p(t|d?)]
(the subscript SC stands for single-conditioning,
as only the left distribution is conditioned on the
argument word). They also experimented with a
few variants for the structure of the similarity mea-
sure and assessed that best results are obtained
with the dot product form. In our experiments,
we employ these two similarity measures for topic
distributions as baselines representing topic-level
models.
Comparing the context-insensitive and context-
sensitive models, we see that both of them mea-
sure similarity between vector representations of
corresponding predicate slots. However, while
DIRT computes sim(v, v?) over vectors in the
original word-level space, topic-level models com-
pute sim(d, d?, w) by measuring similarity of vec-
tors in a reduced-dimensionality latent space. As
conjectured in the introduction, such coarse-grain
representation might lead to loss of information.
Hence, in the next section we propose a com-
bined two-level model, which represents predicate
slots in the original word-level space while biasing
the similarity measure through topic-level context
models.
3 Two-level Context-sensitive Inference
Our model follows the general DIRT scheme
while extending it to handle context-sensitive scor-
ing of rule applications, addressing the scenario
dealt by the context-sensitive topic models. In
particular, we define the context-sensitive score
scoreWT, where WT stands for the combination
of the Word/Topic levels:
(6)scoreWT(LHS ? RHS, wx, wy)
=
?
sim(vxl , vxr , wx) ? sim(v
y
l , v
y
r , wy)
Thus, our model computes similarity over word-
level (rather than topic-level) argument vectors,
while biasing it according to the specific argu-
ment words in the given rule application con-
text. The core of our contribution is thus defining
the context-sensitive word-level vector similarity
measure sim(v, v?, w), as described in the remain-
der of this section.
Following the methods in Section 2, for each
predicate pred we construct, from the learning
corpus, its argument vectors vxpred and vypred aswell as its argument pseudo-documents dxpred and
dypred. For convenience, when referring to an ar-gument vector v, we will denote the correspond-
ing pseudo-document by dv. Based on all pseudo-
documents we learn an LDA model and obtain its
associated probability distributions.
The calculation of sim(v, v?, w) is composed of
two steps. At learning time, we compute for each
candidate rule a separate, topic-biased, similarity
score per each of the topics in the LDA model.
Then, at rule application time, we compute an
overall reliability score for the rule by combining
the per-topic similarity scores, while biasing the
score combination according to the given context
of w. These two steps are described in the follow-
ing two subsections.
3.1 Topic-biased Word-vector Similarities
Given a pair of word vectors v and v?, and
any desired ?base? vector similarity measure sim
(e.g. simLin), we compute a topic-biased sim-
ilarity score for each LDA topic t, denoted by
simt(v, v?). simt(v, v?) is computed by applying
1334
the original similarity measure over topic-biased
versions of v and v?, denoted by vt and v?t:
simt(v, v?) = sim(vt, v?t)
where
vt(w) = v(w) ? p(t|dv, w)
That is, each value in the biased vector, vt(w),
is obtained by weighing the original value v(w)
by the relevance of the topic t to the argument
word w within dv. This way, rather than replac-
ing altogether the word-level values v(w) by the
topic probabilities p(t|dv, w), as done in the topic-
level models, we use the latter to only bias the for-
mer while preserving fine-grained word-level rep-
resentations. The notation Lint denotes the simt
measure when applied using Lin as the base simi-
larity measure sim.
This learning process results in K different
topic-biased similarity scores for each candidate
rule, where K is the number of LDA topics. Ta-
ble 1 illustrates topic-biased similarities for the Y
slot of two rules involving the predicate ?acquire?.
As can be seen, the topic-biased score Lint for ?ac-
quire? learn? for t2 is higher than the Lin score,
since this topic is characterized by arguments that
commonly appear with both predicates of the rule.
Consequently, the two predicates are found to be
distributionally similar when biased for this topic.
On the other hand, the topic-biased similarity for
t1 is substantially lower, since prominent words
in this topic are likely to occur with ?acquire? but
not with ?learn?, yielding low distributional simi-
larity. Opposite behavior is exhibited for the rule
?acquire? purchase?.
3.2 Context-sensitive Similarity
When applying an inference rule, we compute
for each slot its context-sensitive similarity score
simWT(v, v?, w), where v and v? are the slot?s ar-
gument vectors for the two rule sides and w is the
word instantiating the slot in the given rule appli-
cation. This score is computed as a weighted aver-
age of the rule?s K topic-biased similarity scores
simt. In this average, each topic is weighed by
its ?relevance? for the context in which the rule is
applied, which consists of the left-hand-side pred-
icate v and the argument w. This relevance is cap-
Topic t1 t2
Top 5
words
calbiochem rights
corel syndrome
networks majority
viacom knowledge
financially skill
acquire ? learn
Lint(v, v?) 0.040 0.334
Lin(v, v?) 0.165
acquire ? purchase
Lint(v, v?) 0.427 0.241
Lin(v, v?) 0.267
Table 1: Two characteristic topics for the Y slot of
?acquire?, along with their topic-biased Lin sim-
ilarities scores Lint, compared with the original
Lin similarity, for two rules. The relevance of each
topic to different arguments of ?acquire? is illus-
trated by showing the top 5 words in the argument
vector vyacquire for which the illustrated topic is the
most likely one.
tured by p(t|dv, w):
simWT(v, v?, w) =
?
t
[p(t|dv, w) ? simt(v, v?)]
(7)
This way, a rule application would obtain a high
score only if the current context fits those topics
for which the rule is indeed likely to be valid, as
captured by a high topic-biased similarity. The no-
tation LinWT denotes the simWT measure, when
using Lint as the topic-biased similarity measure.
Table 2 illustrates the calculation of context-
sensitive similarity scores in four rule applica-
tions, involving the Y slot of the predicate ?ac-
quire?. We observe that relative to the fixed
context-insensitive Lin score, the score of ?ac-
quire ? learn? is substantially promoted for
the argument ?skill? while being demoted for
?Skype?. The opposite behavior is observed for
?acquire ? purchase?, altogether demonstrating
how our model successfully biases the similarity
score according to rule validity in context.
4 Experimental Settings
To evaluate our model, we compare it both to
context-insensitive similarity measures as well as
to prior context-sensitive methods. Furthermore,
to better understand its applicability in typical
NLP tasks, we focus on an evaluation setting that
corresponds to a natural distribution of examples
from a large corpus.
1335
Topic t1 t2
Top 5
words
calbiochem rights
corel syndrome
networks majority
viacom knowledge
financially skill
?acquire Skype ? learn Skype?
p(t|dv, w) 0.974 0.000
Lint(v, v?) 0.040 0.334
LinWT(v, v?, w) 0.039
Lin(v, v?) 0.165
?acquire Skype ? purchase Skype?
p(t|dv, w) 0.974 0.000
Lint(v, v?) 0.427 0.241
LinWT(v, v?, w) 0.417
Lin(v, v?) 0.267
?acquire skill ? learn skill?
p(t|dv, w) 0.000 0.380
Lint(v, v?) 0.040 0.334
LinWT(v, v?, w) 0.251
Lin(v, v?) 0.165
?acquire skill ? purchase skill?
p(t|dv, w) 0.000 0.380
Lint(v, v?) 0.427 0.241
LinWT(v, v?, w) 0.181
Lin(v, v?) 0.267
Table 2: Context-sensitive similarity scores (in
bold) for the Y slots of four rule applications. The
components of the score calculation are shown for
the topics of Table 1. For each rule application,
the table shows a couple of the topic-biased scores
Lint of the rule (as in Table 1), along with the topic
relevance for the given context p(t|dv, w), which
weighs the topic-biased scores in the LinWT cal-
culation. The context-insensitive Lin score is
shown for comparison.
4.1 Evaluated Rule Application Methods
We evaluated the following rule application meth-
ods: the original context-insensitive word model,
following DIRT (Lin and Pantel, 2001), as de-
scribed in Equation 1, denoted by CI; our own
topic-word context-sensitive model, as described
in Equation 6, denoted by WT. In addition, we
evaluated two variants of the topic-level context-
sensitive model, denoted DC and SC. DC follows
the double conditioned contextualized similarity
measure according to Equation 4, as implemented
by (Ritter et al, 2010), while SC follows the sin-
gle conditioned one at Equation 5, as implemented
by (Dinu and Lapata, 2010b; Dinu and Lapata,
2010a).
Since our model can contextualize various dis-
tributional similarity measures, we evaluated the
performance of all the above methods on several
base similarity measures and their learned rule-
sets, namely Lin (Lin, 1998), BInc (Szpektor and
Dagan, 2008) and vector Cosine similarity. The
Lin similarity measure is described in Equation 2.
Binc (Szpektor and Dagan, 2008) is a directional
similarity measure between word vectors, which
outperformed Lin for predicate inference (Szpek-
tor and Dagan, 2008).
To build the rule-sets and models for the tested
approaches we utilized the ReVerb corpus (Fader
et al, 2011), a large scale publicly available web-
based open extractions data set, containing about
15 million unique template extractions.3 ReVerb
template extractions/instantiations are in the form
of a tuple (x, pred, y), containing pred, a verb
predicate, x, the argument instantiation of the tem-
plate?s slot X , and y, the instantiation of the tem-
plate?s slot Y .
ReVerb includes over 600,000 different tem-
plates that comprise a verb but may also include
other words, for example ?X can accommodate up
to Y?. Yet, many of these templates share a similar
meaning, e.g. ?X accommodate up to Y?, ?X can
accommodate up to Y?, ?X will accommodate up
to Y?, etc. Following Sekine (2005), we clustered
templates that share their main verb predicate in
order to scale down the number of different pred-
icates in the corpus and collect richer word co-
occurrence statistics per predicate.
Next, we applied some clean-up preprocessing
to the ReVerb extractions. This includes discard-
ing stop words, rare words and non-alphabetical
words instantiating either the X or the Y argu-
ments. In addition, we discarded all predicates
that co-occur with less than 100 unique argument
words in each slot. The remaining corpus consists
of 7 million unique extractions and 2,155 verb
predicates.
Finally, we trained an LDA model, as described
in Section 2, using Mallet (McCallum, 2002).
Then, for each original context-insensitive simi-
larity measure, we learned from ReVerb a rule-set
comprised of the top 500 rules for every identi-
fied predicate. To complete the learning, we cal-
culated the topic-biased similarity score for each
learned rule under each LDA topic, as specified
in our context-sensitive model. We release a rule
set comprising the top 500 context-sensitive rules
that we learned for each of the verb predicates in
our learning corpus, along with our trained LDA
3ReVerb is available at http://reverb.cs.
washington.edu/
1336
Method Lin BInc Cosine
Valid 266 254 272
Invalid 545 523 539
Total 811 777 811
Table 3: Sizes of rule application test set for each
learned rule-set.
model.4
4.2 Evaluation Task
To evaluate the performance of the different meth-
ods we chose the dataset constructed by Zeich-
ner et al (2012). 5 This publicly available dataset
contains about 6,500 manually annotated predi-
cate template rule applications, each one labeled
as correct or incorrect. For example, ?Jack agree
with Jill 9 Jack feel sorry for Jill? is a rule ap-
plication in this dataset, labeled as incorrect, and
?Registration open this month? Registration be-
gin this month? is another rule application, labeled
as correct. Rule applications were generated by
randomly sampling extractions from ReVerb, such
as (?Jack?,?agree with?,?Jill?) and then sampling
possible rules for each, such as ?agree with? feel
sorry for?. Hence, this dataset provides naturally
distributed rule inferences with respect to ReVerb.
Whenever we evaluated a distributional similar-
ity measure (namely Lin, BInc, or Cosine), we
discarded instances from Zeichner et al?s dataset
in which the assessed rule is not in the context-
insensitive rule-set learned for this measure or the
argument instantiation of the rule is not in the LDA
lexicon. We refer to the remaining instances as the
test set per measure, e.g. Lin?s test set. Table 3
details the size of each such test set in our experi-
ment.
Finally, the task under which we assessed the
tested models is to rank all rule applications in
each test set, aiming to rank the valid rule appli-
cations above the invalid ones.
5 Results
We evaluated the performance of each tested
method by measuring Mean Average Precision
(MAP) (Manning et al, 2008) of the rule appli-
cation ranking computed by this method. In order
4Our resource is available at: http://www.cs.biu.
ac.il/? nlp/downloads/wt-rules.html
5The dataset is available at: http://
www.cs.biu.ac.il/?nlp/downloads/
annotation-rule-application.htm
Method Lin BInc Cosine
CI 0.503 0.513 0.513
DC 0.451 (1200) 0.455 (1200) 0.455 (1200)
SC 0.443 (1200) 0.458 (1200) 0.452 (1200)
WT 0.562 (100) 0.584 (50) 0.565 (25)
Table 4: MAP values on corresponding test set ob-
tained by each method. Figures in parentheses in-
dicate optimal number of LDA topics.
to compute MAP values and corresponding statis-
tical significance, we randomly split each test set
into 30 subsets. For each method we computed
Average Precision on every subset and then took
the average over all subsets as the MAP value.
Since all tested context-sensitive approaches are
based on LDA topics, we varied for each method
the number of LDA topics K that optimizes its
performance, ranging from 25 to 1600 topics. We
used LDA hyperparameters ? = 0.01 and ? = 0.1
for K < 600 and ? = 50K for K >= 600.
Table 4 presents the optimal MAP performance
of each tested measure. Our main result is that
our model outperforms all other methods, both
context-insensitive and context-sensitive, by a rel-
ative increase of more than 10% for all three sim-
ilarity measures that we tested. This improvement
is statistically significant at p < 0.01 for BInc and
Lin, and p < 0.015 for Cosine, using paired t-
test. This shows that our model indeed success-
fully leverages contextual information beyond the
basic context-agnostic rule scores and is robust
across measures.
Surprisingly, both baseline topic-level context-
sensitive methods, namely DC and SC, underper-
formed compared to their context-insensitive base-
lines. While Dinu and Lapata (Dinu and Lap-
ata, 2010b) did show improvement over context-
insensitive DIRT, this result was obtained on the
verbs of the Lexical Substitution Task in SemEval
(McCarthy and Navigli, 2007), which was manu-
ally created with a bias for context-sensitive sub-
stitutions. However, our result suggests that topic-
level models might not be robust enough when ap-
plied to a random sample of inferences.
An interesting indication of the differences be-
tween our word-topic model, WT, and topic-only
models, DC and SC, lies in the optimal number of
LDA topics required for each method. The num-
ber of topics in the range 25-100 performed almost
equally well under the WT model for all base mea-
sures, with a moderate decline for higher numbers.
1337
The need for this rather small number of topics is
due to the nature of utilization of topics in WT.
Specifically, topics are leveraged for high-level
domain disambiguation, while fine grained word-
level distributional similarity is computed for each
rule under each such domain. This works best for
a relatively low number of topics. However, in
higher numbers, topics relate to narrower domains
and then topic biased word level similarity may
become less effective due to potential sparseness.
On the other hand, DC and SC rely on topics as
a surrogate to predicate-argument co-occurrence
features, and thus require a relatively large num-
ber of them to be effective.
Delving deeper into our test-set, Zeichner et al
provided a more detailed annotation for each in-
valid rule application. Specifically, they annotated
whether the context under which the rule is ap-
plied is valid. For example, in ?John bought my
car 9 John sold my car? the inference is invalid
due to an inherently incorrect rule, but the con-
text is valid. On the other hand in ?my boss raised
my salary 9 my boss constructed my salary? the
context {?my boss?, ?my salary?} for applying
?raise? construct? is invalid. Following, we split
the test-set for the base Lin measure into two test-
sets: (a) test-setvc, which includes all correct rule
applications and incorrect ones only under valid
contexts, and (b) test-setivc, which includes again
all correct rule applications but incorrect ones only
under invalid contexts.
Table 5 presents the performance of each com-
pared method on the two test sets. On test-
setivc, where context mismatches are abundant,
our model outperformed all other baselines (sta-
tistically significant at p < 0.01). In addition,
this time DC slightly outperformed CI. This re-
sult more explicitly shows the advantages of in-
tegrating word-level and context-sensitive topic-
level similarities for differentiating valid and in-
valid contexts for rule applications. Yet, many in-
valid rule applications occur under valid contexts
due to inherently incorrect rules, and we want to
make sure that also in this scenario our model
does not fall behind the context-insensitive mea-
sure. Indeed, on test-setvc, in which context mis-
matches are rare, our algorithm is still better than
the original measure, indicating that WT can be
safely applied to distributional similarity measures
without concerns of reduced performance in dif-
ferent context scenarios.
test-setivc test-setvc
Size
(valid:invalid)
432
(266:166)
645
(266:379)
CI 0.780 0.587
DC 0.796 0.498
SC 0.779 0.512
WT 0.854 0.621
Table 5: MAP results for the two split Lin test-
sets.
6 Discussion and Future Work
This paper addressed the problem of computing
context-sensitive reliability scores for predicate in-
ference rules. In particular, we proposed a novel
scheme that applies over any base distributional
similarity measure which operates at the word
level, and computes a single context-insensitive
score for a rule. Based on such a measure, our
scheme constructs a context-sensitive similarity
measure that computes a reliability score for pred-
icate inference rules applications in the context of
given arguments.
The contextualization of the base similarity
score was obtained using a topic-level LDA
model, which was used in a novel way. First,
it provides a topic bias for learning separate per-
topic word-level similarity scores between predi-
cates. Then, given a specific candidate rule ap-
plication, the LDA model is used to infer the
topic distribution relevant to the context speci-
fied by the given arguments. Finally, the context-
sensitive rule application score is computed as a
weighted average of the per-topic word-level sim-
ilarity scores, which are weighed according to the
inferred topic distribution.
While most works on context-insensitive pred-
icate inference rules, such as DIRT (Lin and Pan-
tel, 2001), are based on word-level similarity mea-
sures, almost all prior models addressing context-
sensitive predicate inference rules are based on
topic models (except for (Pantel et al, 2007),
which was outperformed by later models). We
therefore focused on comparing the performance
of our two-level scheme with state-of-the-art prior
topic-level and word-level models of distributional
similarity, over a random sample of inference rule
applications. Under this natural setting, the two-
level scheme consistently outperformed both types
of models when tested with three different base
similarity measures. Notably, our model shows
stable performance over a large subset of the data
1338
where context sensitivity is rare, while topic-level
models tend to underperform in such cases com-
pared to the base context-insensitive methods.
Our work is closely related to another research
line that addresses lexical similarity and substi-
tution scenarios in context. While we focus on
lexical-syntactic predicate templates and instanti-
ations of their argument slots as context, lexical
similarity methods consider various lexical units
that are not necessarily predicates, with their con-
text typically being the collection of words in a
window around them.
Various approaches have been proposed to ad-
dress lexical similarity. A number of works are
based on a compositional semantics approach,
where a prior representation of a target lexical unit
is composed with the representations of words in
its given context (Mitchell and Lapata, 2008; Erk
and Pado?, 2008; Thater et al, 2010). Other works
(Erk and Pado?, 2010; Reisinger and Mooney,
2010) use a rather large word window around tar-
get words and compute similarities between clus-
ters comprising instances of word windows. In ad-
dition, (Dinu and Lapata, 2010a) adapted the pred-
icate inference topic model from (Dinu and Lap-
ata, 2010b) to compute lexical similarity in con-
text.
A natural extension of our work would be to ex-
tend our two level model to accommodate context-
sensitive lexical similarity. For this purpose we
will need to redefine the scope of context in our
model, and adapt our method to compute context-
biased lexical similarities accordingly. Then we
will also be able to evaluate our model on the
Lexical Substitution Task (McCarthy and Navigli,
2007), which has been commonly used in recent
years as a benchmark for context-sensitive lexical
similarity models.
In a different NLP task, Eidelman et al (2012)
utilize a similar approach to ours for improving
the performance of statistical machine translation
(SMT). They learn an LDA model on the source
language side of the training corpus with the pur-
pose of identifying implicit sub-domains. Then
they utilize the distribution over topics inferred for
each document in their corpus to compute sepa-
rate per-topic translation probability tables. Fi-
nally, they train a classifier to translate a given
target word based on these tables and the inferred
topic distribution of the given document in which
the target word appears. A notable difference be-
tween our approach and theirs is that we use predi-
cate pseudo-documents consisting of argument in-
stantiations to learn our LDA model, while Eidel-
man et al use the real documents in a corpus.
We believe that combining these two approaches
may improve performance for both textual infer-
ence and SMT and plan to experiment with this
direction in future work.
Acknowledgments
This work was partially supported by the Israeli
Ministry of Science and Technology grant 3-8705,
the Israel Science Foundation grant 880/12, and
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
References
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
ACL.
Rahul Bhagat, Patrick Pantel, Eduard Hovy, and Ma-
rina Rey. 2007. Ledir: An unsupervised algorithm
for learning directionality of inference rules. In Pro-
ceedings of EMNLP-CoNLL.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. the Journal of ma-
chine Learning research, 3:993?1022.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Lecture Notes in Computer Science,
volume 3944, pages 177?190.
Georgiana Dinu and Mirella Lapata. 2010a. Measur-
ing distributional similarity in context. In Proceed-
ings of EMNLP.
Georgiana Dinu and Mirella Lapata. 2010b. Topic
models for meaning similarity in context. In Pro-
ceedings of COLING: Posters.
Georgiana Dinu and Rui Wang. 2009. Inference rules
and their application to recognizing textual entail-
ment. In Proceedings EACL.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic transla-
tion model adaptation. In Proceedings of the ACL
conference short papers.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP.
Katrin Erk and Sebastian Pado?. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of the ACL conference short papers.
1339
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Dekang Lin and Patrick Pantel. 2001. DIRT ? discov-
ery of inference rules from text. In Proceedings of
ACM SIGKDDConference on Knowledge Discovery
and Data Mining 2001.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In
Proceedings of SemEval.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: A taxonomy of relational
patterns with semantic types. EMNLP12.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Hu-
man Language Technologies 2007: The Conference
of the North American Chapter of the Association
for Computational Linguistics.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In Proceedings of ACL.
Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter
of the Association for Computational Linguistics.
Alan Ritter, Oren Etzioni, et al 2010. A latent dirich-
let alocation method for selectional preferences. In
Proceedings of ACL.
Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and
Daniel Weld. 2010. Learning first-order horn
clauses from web text. In Proceedings of EMNLP.
Diarmuid O Se?aghdha. 2010. Latent variable models
of selectional preference. In Proceedings of ACL.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of the Third International Workshop on
Paraphrasing (IWP2005).
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main
Conference.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In Pro-
ceedings of ACL-08: HLT.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of ACL.
Naomi Zeichner, Jonathan Berant, and Ido Dagan.
2012. Crowdsourcing inference-rule evaluation. In
Proceedings of ACL (short papers).
1340
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 283?288,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Using Lexical Expansion to Learn Inference Rules from Sparse Data
Oren Melamud?, Ido Dagan?, Jacob Goldberger?, Idan Szpektor?
? Computer Science Department, Bar-Ilan University
? Faculty of Engineering, Bar-Ilan University
? Yahoo! Research Israel
{melamuo,dagan,goldbej}@{cs,cs,eng}.biu.ac.il
idan@yahoo-inc.com
Abstract
Automatic acquisition of inference rules
for predicates is widely addressed by com-
puting distributional similarity scores be-
tween vectors of argument words. In
this scheme, prior work typically refrained
from learning rules for low frequency
predicates associated with very sparse ar-
gument vectors due to expected low reli-
ability. To improve the learning of such
rules in an unsupervised way, we propose
to lexically expand sparse argument word
vectors with semantically similar words.
Our evaluation shows that lexical expan-
sion significantly improves performance
in comparison to state-of-the-art baselines.
1 Introduction
The benefit of utilizing template-based inference
rules between predicates was demonstrated in
NLP tasks such as Question Answering (QA)
(Ravichandran and Hovy, 2002) and Information
Extraction (IE) (Shinyama and Sekine, 2006). For
example, the inference rule ?X treat Y ? X relieve
Y?, between the templates ?X treat Y? and ?X re-
lieve Y? may be useful to identify the answer to
?Which drugs relieve stomach ache??.
The predominant unsupervised approach for
learning inference rules between templates is via
distributional similarity (Lin and Pantel, 2001;
Ravichandran and Hovy, 2002; Szpektor and Da-
gan, 2008). Specifically, each argument slot in
a template is represented by an argument vector,
containing the words (or terms) that instantiate this
slot in all of the occurrences of the template in a
learning corpus. Two templates are then deemed
semantically similar if the argument vectors of
their corresponding slots are similar.
Ideally, inference rules should be learned for
all templates that occur in the learning corpus.
However, many templates are rare and occur only
few times in the corpus. This is a typical NLP
phenomenon that can be associated with either a
small learning corpus, as in the cases of domain
specific corpora and resource-scarce languages, or
with templates with rare terms or long multi-word
expressions such as ?X be also a risk factor to Y?
or ?X finish second in Y?, which capture very spe-
cific meanings. Due to few occurrences, the slots
of rare templates are represented with very sparse
argument vectors, which in turn lead to low relia-
bility in distributional similarity scores.
A common practice in prior work for learn-
ing predicate inference rules is to simply disre-
gard templates below a minimal frequency thresh-
old (Lin and Pantel, 2001; Kotlerman et al, 2010;
Dinu and Lapata, 2010; Ritter et al, 2010). Yet,
acquiring rules for rare templates may be benefi-
cial both in terms of coverage, but also in terms
of more accurate rule application, since rare tem-
plates are less ambiguous than frequent ones.
We propose to improve the learning of rules be-
tween infrequent templates by expanding their ar-
gument vectors. This is done via a ?dual? distribu-
tional similarity approach, in which we consider
two words to be similar if they instantiate similar
sets of templates. We then use these similarities
to expand the argument vector of each slot with
words that were identified as similar to the original
arguments in the vector. Finally, similarities be-
tween templates are computed using the expanded
vectors, resulting in a ?smoothed? version of the
original similarity measure.
Evaluations on a rule application task show
that our lexical expansion approach significantly
improves the performance of the state-of-the-art
DIRT algorithm (Lin and Pantel, 2001). In addi-
tion, our approach outperforms a similarity mea-
sure based on vectors of latent topics instead of
word vectors, a common way to avoid sparseness
issues by means of dimensionality reduction.
283
2 Technical Background
The distributional similarity score for an inference
rule between two predicate templates, e.g. ?X re-
sign Y? X quit Y?, is typically computed by mea-
suring the similarity between the argument vec-
tors of the corresponding X slots and Y slots of
the two templates. To this end, first the argument
vectors should be constructed and then a similarity
measure between two vectors should be provided.
We note that we focus here on binary templates
with two slots each, but this approach can be ap-
plied to any template.
A common starting point is to compute a
co-occurrence matrix M from a learning cor-
pus. M ?s rows correspond to the template slots
and the columns correspond to the various terms
that instantiate the slots. Each entry Mi,j , e.g.
Mx quit,John, contains a count of the number of
times the term j instantiated the template slot i in
the corpus. Thus, each row Mi,? corresponds to
an argument vector for slot i. Next, some func-
tion of the counts is used to assign weights to all
Mi,j entries. In this paper we use pointwise mu-
tual information (PMI), which is common in prior
work (Lin and Pantel, 2001; Szpektor and Dagan,
2008).
Finally, rules are assessed using some similar-
ity measure between corresponding argument vec-
tors. The state-of-the-art DIRT algorithm (Lin and
Pantel, 2001) uses the highly cited Lin similarity
measures (Lin, 1998) to score rules between bi-
nary templates as follows:
(1)Lin(v, v?) =
?
w?v?v? [v(w) + v?(w)]?
w?v?v? [v(w) + v?(w)]
(2)DIRT (l ? r)
=
?
Lin(vl:x, vr:x) ? Lin(vl:y, vr:y)
where v and v? are two argument vectors, l and
r are the templates participating in the inference
rule and vl:x corresponds to the argument vector
of slot X of template l, etc. While the original
DIRT algorithm utilizes the Lin measure, one can
replace it with any other vector similarity measure.
A separate line of research for word simi-
larity introduced directional similarity measures
that have a bias for identifying generaliza-
tion/specification relations, i.e. relations be-
tween predicates with narrow (or specific) seman-
tic meanings to predicates with broader meanings
inferred by them (unlike the symmetric Lin). One
such example is the Cover measure (Weeds and
Weir, 2003):
(3)Cover(v, v?) =
?
w?v?v? [v(w)]?
w?v?v? [v(w)]
As can be seen, in the core of the Lin and Cover
measures, as well as in many other well known
distributional similarity measures such as Jaccard,
Dice and Cosine, stand the number of shared ar-
guments vs. the total number of arguments in the
two vectors. Therefore, when the argument vec-
tors are sparse, containing very few non-zero fea-
tures, these scores become unreliable and volatile,
changing greatly with every inclusion or exclusion
of a single shared argument.
3 Lexical Expansion Scheme
We wish to overcome the sparseness issues in rare
feature vectors, especially in cases where argu-
ment vectors of semantically similar predicates
comprise similar but not exactly identical argu-
ments. To this end, we propose a three step
scheme. First, we learn lexical expansion sets for
argument words, such as the set {euros, money}
for the word dollars. Then we use these sets to ex-
pand the argument word vectors of predicate tem-
plates. For example, given the template ?X can
be exchanged for Y?, with the following argument
words instantiating slot X {dollars, gold}, and
the expansion set above, we would expand the ar-
gument word vector to include all the following
words {dollars, euros, money, gold}. Finally, we
use the expanded argument word vectors to com-
pute the scores for predicate inference rules with a
given similarity measure.
When a template is instantiated with an ob-
served word, we expect it to also be instantiated
with semantically similar words such as the ones
in the expansion set of the observed word. We
?blame? the lack of such template occurrences
only on the size of the corpus and the sparseness
phenomenon in natural languages. Thus, we uti-
lize our lexical expansion scheme to synthetically
add these expected but missing occurrences, ef-
fectively smoothing or generalizing over the ex-
plicitly observed argument occurrences. Our ap-
proach is inspired by query expansion (Voorhees,
1994) in Information Retrieval (IR), as well as by
the recent lexical expansion framework proposed
in (Biemann and Riedl, 2013), and the work by
284
Miller et al (2012) on word sense disambigua-
tion. Yet, to the best of our knowledge, this is the
first work that applies lexical expansion to distri-
butional similarity feature vectors. We next de-
scribe our scheme in detail.
3.1 Learning Lexical Expansions
We start by constructing the co-occurrence matrix
M (Section 2), where each entry Mt:s,w indicates
the number of times that word w instantiates slot
s of template t in the learning corpus, denoted by
?t:s?, where s can be either X or Y.
In traditional distributional similarity, the rows
Mt:s,? serve as argument vectors of template slots.
However, to learn expansion sets we take a ?dual?
view and consider each matrix column M?:?,w (de-
noted vw) as a feature vector for the argument
word w. Under this view, templates (or more
specifically, template slots) are the features. For
instance, for the word dollars the respective fea-
ture vector may include entries such as ?X can be
exchanged for?, ?can be exchanged for Y?, ?pur-
chase Y? and ?sell Y?.
We next learn an expansion set per each word
w by computing the distributional similarity be-
tween the vectors of w and any other argument
word w?, sim(vw, vw?). Then we take the N most
similar words as w?s expansion set with degree
N , denoted by LNw = {w?1, ..., w?N}. Any simi-
larity measure could be used, but as our experi-
ments show, different measures generate sets with
different properties, and some may be fitter for ar-
gument vector expansion than others.
3.2 Expanding Argument Vectors
Given a row count vector Mt:s,? for slot s of tem-
plate t, we enrich it with expansion sets as fol-
lows. For each w in Mt:s,?, the original count in
vt:s(w) is redistributed equally between itself and
all words in w?s expansion set, i.e. all w? ? LNw ,
(possibly yielding fractional counts) where N is a
global parameter of the model. Specifically, the
new count that is assigned to each word w is its
remaining original count after it has been redis-
tributed (or zero if no original count), plus all the
counts that were distributed to it from other words.
Next, PMI weights are recomputed according to
the new counts, and the resulting expanded vector
is denoted by v+t:s. Similarity between template
slots is now computed over the expanded vectors
instead of the original ones, e.g. Lin(v+l:x, v+r:x).
4 Experimental Settings
We constructed a relatively small learning corpus
for investigating the sparseness issues of such cor-
pora. To this end, we used a random sample from
the large scale web-based ReVerb corpus1 (Fader
et al, 2011), comprising tuple extractions of pred-
icate templates with their argument instantiations.
We applied some clean-up preprocessing to these
extractions, discarding stop words, rare words and
non-alphabetical words that instantiated either the
X or the Y argument slots. In addition, we dis-
carded templates that co-occur with less than 5
unique argument words in either of their slots, as-
suming that such few arguments cannot convey re-
liable semantic information, even with expansion.
Our final corpus consists of around 350,000 ex-
tractions and 14,000 unique templates. In this cor-
pus around one third of the extractions refer to
templates that co-occur with at most 35 unique ar-
guments in both their slots.
We evaluated the quality of inference
rules using the dataset constructed by Zeich-
ner et al (2012)2, which contains about 6,500
manually annotated template rule applications,
each labeled as correct or not. For example,
?The game develop eye-hand coordination9 The
game launch eye-hand coordination? is a rule
application in this dataset of the rule ?X develop
Y ? X launch Y?, labeled as incorrect, and
?Captain Cook sail to Australia? Captain Cook
depart for Australia? is a rule application of the
rule ?X sail to Y ? X depart for Y?, labeled as
correct. Specifically, we induced two datasets
from Zeichner et al?s dataset, denoted DS-5-35
and DS-5-50, which consist of all rule applica-
tions whose templates are present in our learning
corpus and co-occurred with at least 5 and at
most 35 and 50 unique argument words in both
their slots, respectively. DS-5-35 includes 311
rule applications (104 correct and 207 incorrect)
and DS-5-50 includes 502 rule applications (190
correct and 312 incorrect).
Our evaluation task is to rank all rule applica-
tions in each test set based on the similarity scores
of the applied rules. Optimal performance would
rank all correct rule applications above the in-
correct ones. As a baseline for rule scoring we
1http://reverb.cs.washington.edu/
2http://www.cs.biu.ac.il/nlp/
downloads/annotation-rule-application.
htm
285
used the DIRT algorithm scheme, denoted DIRT-
LE-None. We then compared between the perfor-
mance of this baseline and its expanded versions,
testing two similarity measures for generating the
expansion sets of arguments: Lin and Cover. We
denote these expanded methods DIRT-LE-SIM-N,
where SIM is the similarity measure used to gen-
erate the expansion sets and N is the lexical expan-
sion degree, e.g. DIRT-LE-Lin-2.
We remind the reader that our scheme utilizes
two similarity measures. The first measure as-
sesses the similarity between the argument vectors
of the two templates in the rule. This measure
is kept constant in our experiments and is iden-
tical to DIRT?s similarity measure (Lin). 3 The
second measure assesses the similarity between
words and is used for the lexical expansion of ar-
gument vectors. Since this is the research goal
of this paper, we experimented with two different
measures for lexical expansion: a symmetric mea-
sure (Lin) and an asymmetric measure (Cover).
To this end we evaluated their effect on DIRT?s
rule ranking performance and compared them to a
vanilla version of DIRT without lexical expansion.
As another baseline, we follow Dinu and La-
pata (2010) inducing LDA topic vectors for tem-
plate slots and computing predicate template infer-
ence rule scores based on similarity between these
vectors. We use standard hyperparameters for
learning the LDA model (Griffiths and Steyvers,
2004). This method is denoted LDA-K, where K is
the number of topics in the model.
5 Results
We evaluated the performance of each tested
method by measuring Mean Average Precision
(MAP) (Manning et al, 2008) of the rule applica-
tion ranking computed by this method. In order
to compute MAP values and corresponding sta-
tistical significance, we randomly split each test
set into 30 subsets. For each method we com-
puted Average Precision on every subset and then
took the average as the MAP value. We varied
the degree of the lexical expansion in our model
and the number of topics in the topic model base-
line to analyze their effect on the performance of
these methods on our datasets. We note that in our
model a greater degree of lexical expansion cor-
3Experiments with Cosine as the template similarity mea-
sure instead of Lin for both DIRT and its expanded versions
yielded similar results. We omit those for brevity.
responds to more aggressive smoothing (or gen-
eralization) of the explicitly observed data, while
the same goes for a lower number of topics in the
topic model. The results on DS-5-35 and DS-5-50
are illustrated in Figure 1.
The most dramatic improvement over the base-
lines is evident in DS-5-35, where DIRT-LE-
Cover-2 achieves a MAP score of 0.577 in com-
parison to 0.459 achieved by its DIRT-LE-None
baseline. This is indeed the dataset where we ex-
pected expansion to affect most due the extreme
sparseness of argument vectors. Both DIRT-LE-
Cover-N and DIRT-LE-Lin-N outperform DIRT-
LE-None for all tested values of N , with statisti-
cal significance via a paired t-test at p < 0.05 for
DIRT-LE-Cover-N where 1 ? N ? 5, and p <
0.01 for DIRT-LE-Cover-2. On DS-5-50, improve-
ment over the DIRT-LE-None baseline is still sig-
nificant with both DIRT-LE-Cover-N and DIRT-
LE-Lin-N outperforming DIRT-LE-None. DIRT-
LE-Cover-N again performs best and achieves a
relative improvement of over 10% with statistical
significance at p < 0.05 for 2 ? N ? 3.
The above shows that expansion is effective for
improving rule learning between infrequent tem-
plates. Furthermore, the fact that DIRT-LE-Cover-
N outperforms DIRT-LE-Lin-N suggests that us-
ing directional expansions, which are biased to
generalizations of the observed argument words,
e.g. vehicle as an expansion for car, is more ef-
fective than using symmetrically related words,
such as bicycle or automobile. This conclusion
appears also to be valid from a semantic reason-
ing perspective, as given an observed predicate-
argument occurrence, such as ?drive car? we can
more likely infer that a presumed occurrence of
the same predicate with a generalization of the ar-
gument, such as ?drive vehicle?, is valid, i.e. ?drive
car ? drive vehicle?. On the other hand while
?drive car ? drive automobile? is likely to be
valid, ?drive car ? drive bicycle? and ?drive ve-
hicle? drive bicycle? are not.
Figure 1 also depicts the performance of LDA
as a vector smoothing approach. LDA-K out-
performs the DIRT-LE-None baseline under DS-
5-35 but with no statistical significance. Under
DS-5-50 LDA-K performs worst, slightly outper-
forming DIRT-LE-None only for K=450. Further-
more, under both datasets, LDA-K is outperformed
by DIRT-LE-Cover-N. These results indicate that
LDA is less effective than our expansion approach.
286
Figure 1: MAP scores on DS-5-35 and DS-5-50 for the original DIRT scheme, denoted DIRT-LE-None,
and for the compared smoothing methods as follows. DIRT with varied degrees of lexical expansion
is denoted as DIRT-LE-Lin-N and DIRT-LE-Cover-N. The topic model with varied number of topics is
denoted as LDA-K. Data labels indicate the expansion degree (N) or the number of LDA topics (K),
depending on the tested method.
One reason may be that in our model, every expan-
sion set may be viewed as a cluster around a spe-
cific word, an outstanding difference in compari-
son to topics, which provide a global partition over
all words. We note that performance improve-
ment of singleton document clusters over global
partitions was also shown in IR (Kurland and Lee,
2009).
In order to further illustrate our lexical expan-
sion scheme we focus on the rule application
?Captain Cook sail to Australia? Captain Cook
depart for Australia?, which is labeled as correct
in our test set and corresponds to the rule ?X sail
to Y ? X depart for Y?. There are 30 words in-
stantiating the X slot of the predicate ?sail to?
in our learning corpus including {Columbus, em-
peror, James, John, trader}. On the other hand,
there are 18 words instantiating the X slot of the
predicate ?depart for? including {Amanda, Jerry,
Michael, mother, queen}. While semantic simi-
larity between these two sets of words is evident,
they share no words in common, and therefore the
original DIRT algorithm, DIRT-LE-None, wrongly
assigns a zero score to the rule.
The following are descriptions of some of the
argument word expansions performed by DIRT-
LE-Cover-2 (using the notation LNw defined in Sec-
tion 3.1) for the X slot of ?sail to? L2John = {mr.,
dr.}, L2trader = {people, man}, and for the X slot
of ?depart for?, L2Michael = {John, mr.}, L2mother =
{people, woman}. Given these expansions the two
slots now share the following words {mr. ,people,
John} and the rule score becomes positive.
It is also interesting to compare the expansions
performed by DIRT-LE-Lin-2 to the above. For
instance in this case L2mother = {father, sarah},
which does not identify people as a shared argu-
ment for the rule.
6 Conclusions
We propose to improve the learning of infer-
ence rules between infrequent predicate templates
with sparse argument vectors by utilizing a novel
scheme that lexically expands argument vectors
with semantically similar words. Similarities be-
tween argument words are discovered using a dual
distributional representation, in which templates
are the features.
We tested the performance of our expansion
approach on rule application datasets that were
biased towards rare templates. Our evaluation
showed that rule learning with expanded vectors
outperformed the baseline learning with original
vectors. It also outperformed an LDA-based simi-
larity model that overcomes sparseness via dimen-
sionality reduction.
In future work we plan to investigate how our
scheme performs when integrated with manually
constructed resources for lexical expansion, such
as WordNet (Fellbaum, 1998).
Acknowledgments
This work was partially supported by the Israeli
Ministry of Science and Technology grant 3-8705,
the Israel Science Foundation grant 880/12, and
the European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
287
References
Chris Biemann and Martin Riedl. 2013. Text: Now
in 2d! a framework for lexical expansion with con-
textual similarity. Journal of Language Modeling,
1(1).
Georgiana Dinu and Mirella Lapata. 2010. Topic mod-
els for meaning similarity in context. In Proceedings
of COLING: Posters.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228?5235.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Oren Kurland and Lillian Lee. 2009. Clusters, lan-
guage models, and ad hoc information retrieval.
ACM Transactions on Information Systems (TOIS),
27(3):13.
Dekang Lin and Patrick Pantel. 2001. DIRT ? discov-
ery of inference rules from text. In Proceedings of
KDD.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Tristan Miller, Chris Biemann, Torsten Zesch, and
Iryna Gurevych. 2012. Using distributional similar-
ity for lexical expansion in knowledge-based word
sense disambiguation. Proceedings of COLING,
Mumbai, India.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In Proceedings of ACL.
Alan Ritter, Oren Etzioni, et al 2010. A latent dirich-
let alocation method for selectional preferences. In
Proceedings of ACL.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of NAACL.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Ellen M Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of SIGIR.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
EMNLP.
Naomi Zeichner, Jonathan Berant, and Ido Dagan.
2012. Crowdsourcing inference-rule evaluation. In
Proceedings of ACL (short papers).
288
Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 20?29,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Classification-based Contextual Preferences
Shachar Mirkin, Ido Dagan, Lili Kotlerman
Bar-Ilan University
Ramat Gan, Israel
{mirkins,dagan,davidol}@cs.biu.ac.il
Idan Szpektor
Yahoo! Research
Haifa, Israel
idan@yahoo-inc.com
Abstract
This paper addresses context matching in tex-
tual inference. We formulate the task under
the Contextual Preferences framework which
broadly captures contextual aspects of infer-
ence. We propose a generic classification-
based scheme under this framework which co-
herently attends to context matching in infer-
ence and may be employed in any inference-
based task. As a test bed for our scheme we use
the Name-based Text Categorization (TC) task.
We define an integration of Contextual Prefer-
ences into the TC setting and present a concrete
self-supervised model which instantiates the
generic scheme and is applied to address con-
text matching in the TC task. Experiments on
standard TC datasets show that our approach
outperforms the state of the art in context mod-
eling for Name-based TC.
1 Introduction
Textual inference is prevalent in text understanding
applications. For example, in Question Answering
(QA) the expected answer should be inferred from
retrieved passages, and in Information Extraction (IE)
the meaning of the target event is inferred from its
mention in the text.
Lexical inferences make a substantial part of the
inference process. In such cases, a target term is
inferred from text expressions based on either one of
two types of lexical matches: (i) a direct match of
the target term in the text. For instance, the IE event
injure may be detected by finding the word injure in
the text; (ii) an indirect match, through a term that
implies the meaning of the target term, e.g. inferring
injure from hurt.
In either case, due to word ambiguity, it is nec-
essary to validate that the context of the match con-
forms with the intended meaning of the target term
before carrying out an inference operation based on
this match. For example, ?You hurt my feelings? con-
stitutes an invalid context for the injure event as hurt
in this text does not refer to a physical injury. Simi-
larly, inferring the protest-related event demonstrate
based on demo is deemed invalid although demo im-
plies the meaning of the word demonstrate in other
contexts, e.g., concerning software demonstration.
Although seemingly equivalent, a closer look re-
veals that the above two examples correspond to two
distinct contextual mismatch situations. While the
match of hurt is invalid for injure in the particular
given context, an inference based on demo is invalid
for the protest demonstrate event in any context.
Thus, several types of context matching are in-
volved in textual inference. While most prior work
addressed only specific context matching scenarios,
Szpektor et al (2008) presented a broader view,
proposing a generic framework for context match-
ing in inference, termed Contextual Preferences (CP).
CP specifies the types of context matching that need
to be considered in inference, allowing a model of
choice to be applied for validating each type of match.
Szpektor et al applied CP to an IE task using differ-
ent models to validate each type of context match.
In this work we adopt CP as our context matching
framework and propose a novel classification-based
scheme which provides unified modeling for CP. We
represent typical contexts of the textual objects that
participate in inference using classifiers; at inference
time, each match is assessed by the respective classi-
fiers which determine its contextual validity.
As a test bed we applied our scheme to the task
20
of Name-based Text Categorization. This is an unsu-
pervised setting of TC where the only input given is
the category name, and in which context validation
is of high importance. We instantiate the scheme
with a novel self-supervised model and apply it to
the TC task. We suggest a method for integrating any
CP-based context matching model into TC and use it
to combine the context matching scores generated by
our model. Results on two standard TC datasets show
that our approach outperforms the state of the art con-
text model for this task and suggest applying this
scheme to additional inference-based applications.
2 Background
2.1 Context matching in inference
Word ambiguity has been traditionally addressed
through Word Sense Disambiguation (WSD) (Nav-
igli, 2009). The WSD task requires selecting the
meaning of a target term from amongst a predefined
set of senses, based on sense-inventories such as
WordNet (Fellbaum, 1998).
An alternative approach eliminates the reliance on
such inventories. Instead of explicit sense identifi-
cation, a direct sense-match between terms is pur-
sued (Dagan et al, 2006). Lexical substitution (Mc-
Carthy and Navigli, 2009) is probably the most com-
monly known task that follows this approach. Con-
text matching is a generalization of lexical substitu-
tion, which seeks a match between terms in context,
not necessarily for the purpose of substitution. For in-
stance, the word played in ?U2 played their first-ever
concert in Russia? contextually matches music, al-
though music cannot substitute played in this context.
The context matching task, therefore, is to determine
(by quantifying or giving a binary decision) the va-
lidity of a match between two terms in context.
In Section 1 we informally presented two cases of
contextual mismatches. A comprehensive view of
context matching types is provided by the Contextual
Preferences framework (Szpektor et al, 2008). CP
is phrased in terms of the Textual Entailment (TE)
paradigm (Dagan et al, 2009). In TE, a text t entails
a textual hypothesis h if the meaning of h can be
inferred from t. Formulating the IE example from
Section 1 within TE, h may be the name of the target
event, injure, and t is a text segment from which h
can be inferred. A direct match occurs when a term
in h is identical to a term in t. An inference based
on an indirect match is viewed as the application of
a lexical entailment rule, r, such as ?hurt? injure?,
where the entailing left-hand side (LHS) of the rule
(hurt) is matched in the text, while the entailed right-
hand side (RHS), injure, is matched in the hypothesis.
Hence, three inference objects take part in infer-
ence operations: t, h and r. Most prior work ad-
dressed only specific contextual matches between
these objects. For example, Harabagiu et al (2003)
matched the contexts of t and h for QA (answer and
question, respectively); Barak et al (2009) matched
t and h (document and category) in TC, while other
works, including those applying lexical substitution,
typically validated the context match between t and r
(Kauchak and Barzilay, 2006; Dagan et al, 2006;
Pantel et al, 2007; Connor and Roth, 2007).
In comparison, in the CP framework, all possible
contextual matches among t, h and r are considered:
t?h, t? r and r?h. The three context matches are
depicted in Figure 1 (left). In CP, the representation
of each inference object is enriched with contextual
information which is used to characterize its valid
contexts. Such information may be the words of the
event description in IE, corpus instances based on
which a rule was learned, or an annotation of relevant
WordNet senses in Name-based TC. For example,
a category name hockey may be assigned with the
sense number corresponding to ice hockey, but not
to field hockey, in order to designate information that
limits the valid contexts of the category to the former
among the two meanings of the name.
Before an inference operation is performed, the
context representations of each pair among the partic-
ipating objects should be matched by a context model
in order to assess the contextual validity of the opera-
tion. Along with the context representation and the
specific context matching models, the way context
model decisions are combined needs to be specified
in a concrete implementation of the CP framework.
2.2 Context matching models
Several approaches were taken in prior work to
model context matching, mostly within the scope
of learning selectional preferences of templatized
lexical-syntactic rules (e.g. ?X
subj
???? hit
obj
??? Y ??
?X
subj
???? attack
obj
??? Y ?).
21
Pantel et al (2007) and Szpektor et al (2008) rep-
resented the context of such rules as the intersection
of preferences of the rule?s LHS and RHS, namely the
observed argument instantiations or their semantic
classes. A rule is deemed applicable to a given text if
the argument instantiations in the text are similar to
the selectional preferences of the rule. To overcome
sparseness, other works represented context in latent
space. Pennacchiotti et al (2007) and Szpektor et al
(2008) measured the similarity between the Latent
Semantic Analysis (LSA) (Deerwester et al, 1990)
representations of matched contexts. Dinu and La-
pata (2010) used Latent Dirichlet Allocation (LDA)
(Blei et al, 2003) to model templates? latent senses,
determining rule applicability based on the similarity
between the two sides of the rule when instantiated
by the context, while Ritter et al (2010) used LDA
to model argument classes, considering a rule valid
for a given argument instantiation if its instantiated
templates are drawn from the same hidden topic.
A different approach is provided by classification-
based models which learn classifiers for inference
objects. A classifier is trained based on positive and
negative examples which represent valid or invalid
contexts of the object; from those, features charac-
terizing the context are extracted, e.g. words in a
window around the target term or syntactic links with
it. Given a new context, the classifier assesses its va-
lidity with respect to the learned classification model.
Classifiers in prior work were applied to determine
rule applicability in a given context (t ? r). Train-
ing a classifier for word paraphrasing, Kauchak and
Barzilay (2006) used occurrences of the rule?s RHS as
positive context examples, and randomly picked neg-
ative examples. A similar approach was applied by
Dagan et al (2006), which used a single-class SVM
to avoid selecting negative examples. In both works,
a resulting classifier represents a word with all its
senses intermixed. Clearly, this poses no problem for
monosemous words, but is biased towards the more
common senses of polysemous words. Indeed, Dagan
et al (2006) report a negative correlation between the
degree of polysemy of a word and the performance of
its classifier. Connor and Roth (2007) used per-rule
classifiers to produce a noisy training set for learning
a global classifier for verb substitution.
In this work we follow the classification-based ap-
proach which seems appealing for several reasons.
t
r
t
C h(
r)
h
C r(
t)
C h(
t)r
h
Figure 1: Left: An illustration of the CP relationships as in
(Szpektor et al, 2008), with arrows indicting the context
matching direction; Right: The application of classifiers
to the tested contexts under our scheme.
First, it allows seamlessly integrating various types
of information via classifiers? features; unlike some
of the above models, it is not inherently dependent on
the type of rules that are utilized and easily accom-
modates to both lexical and lexical-syntactic rules
through the choice of features. In addition, it does
not rely on a predefined similarity measure and pro-
vides flexibility in terms of model?s parameters. Fi-
nally, this approach captures the notion of direction-
ality which is fundamental in textual inference, and
is therefore better suited to applied inference than
previously proposed symmetric context models.
In comparison to prior classification-based models,
our approach addresses all three context matches
specified by CP, rather than only the rule-text match.
It is not limited to substitutable terms or even to
terms with the same part of speech. In addition, we
avoid learning a classifier for all senses combined,
but rather learn it for the specific intended meaning.
2.3 Name-based Text Categorization
Name-based TC (Gliozzo et al, 2009) is an unsu-
pervised setting of Text Categorization in which the
only input provided is the category name, e.g. trade,
?mergers and acquisitions? or guns. When category
names are ambiguous, e.g. space, categories are not
well defined; thus, auxiliary information is expected
to accompany the name for disambiguation, such as
a list of relevant senses or a category description.
Typically, unsupervised TC consists of two steps.
First, an unsupervised method is applied to an unla-
beled corpus, automatically labeling some of the doc-
uments to categories. Then, the labeled documents
from the first step are used to train a supervised TC
classifier which is used to label any document in the
test set (Gliozzo et al, 2009; Downey and Etzioni,
2009; Barak et al, 2009).
22
In this work we focus on the above unsupervised
step. Gliozzo et al (2009) addressed this task by rep-
resenting both documents and categories by LSA vec-
tors which implicitly capture contextual similarities
between terms. Each document was then assigned
to the most similar category based on cosine simi-
larity between the LSA vectors. Barak et al (2009)
required an occurrence of a term entailing the cat-
egory name (or the category name itself) in order
to regard the category as a candidate for the docu-
ment. To assess the contextual validity of the match,
they used LSA document-category similarity as in
(Gliozzo et al, 2009). For example, to classify a doc-
ument into the category medicine, at least one lexical
entailment rule, e.g. ?drug? medicine?, should be
matched in the document. Then, the validity of drug
for medicine in the matched document is assessed by
the LSA context model. In this work we adopt Barak
et al?s requirement for a match for the category in
the document, but address context matching in an
entirely different way.
Name-based TC provides a convenient setting for
evaluating context matching approaches for two main
reasons. First, all types of context matchings are real-
ized in this application (see Section 3); second, as the
hypothesis consists of a single term or a few terms,
the TC gold standard annotation corresponds quite
directly to the context matching task for lexical infer-
ences; in other applications where longer hypotheses
are involved, context matching performance may be
masked by other factors.
3 Contextual Matches in TC
Within Name-based TC, the Textual Entailment ter-
minology is mapped as follows: h is a term denoting
the category name (e.g. merger or acquisition); t is
a matched term in the document to be categorized
from which h may be inferred; and a match refers
to an occurrence in the document of either h (direct
match) or the LHS of an entailment rule r whose RHS
is a category name (indirect match).1
Under the CP view, a context model needs to ad-
dress the following three context matching cases
within a TC setting.
t?h: Assessing the validity of a match in the docu-
ment with respect to the category?s intended meaning.
1Note that t and h both refer here to individual terms.
For example, the occurrence of the category name
space (in the sense of outer space) in ?the server ran
out of disk space? does not indicate a space-related
text, and should be dismissed by the context model.
t? r: This case refers to a rule match in the docu-
ment. A context model should ensure that the mean-
ing of a match is compatible with that of the rule.
For example, ?alien? space? is a valid rule for the
space category. Yet, it should not be applied to ?The
US welcomes a large number of aliens every year?,
since alien in this sentence has a different meaning
than the intended meaning of the rule.
r ? h: The match between the intended meanings
of the category name and the RHS of the rule. For
instance, the rule ?room? space? is not suitable at
all for the (outer) space category.
4 A Classification-based Scheme for CP
Szpektor et al (2008) introduced a vector-space
model to implement CP, in which the text t, the rule
r and the hypothesis h share the same contextual
representation. However, in CP, r, h and t have non-
symmetric roles: the context of t should be tested as
valid for r and h and not vice versa, and the context
of r should be validated for h and not the other way
around. This stems from the need to consider direc-
tionality in context matching. For instance, a text
about football typically constitutes a valid context
for the more general sports context, but not vice
versa. Indeed, directionality may be captured in
vector-space models by using a directional similarity
measure (Kotlerman et al, 2010), but only symmetric
measures were used in context matching work so far.
Based on this distinction between the inference
objects? roles, we present a novel scheme that uses
two types of classifiers to represent context:
Ch: A classifier that identifies valid contexts for h. It
tests contexts of t (for t? h matching) or r (for
r ? h matching), assigning them scores Ch(t)
and Ch(r), respectively.
Cr: A classifier that identifies valid contexts for ap-
plying the rule r. It tests the context of t, assign-
ing it a score Cr(t).
Figure 1 (right) shows the classifiers scores which
are assigned to each of the matching types.
23
Hence, h always acts as the classifying object, t is
always the classified object, while r acts as both. Con-
text matching is quantified by the degree by which
the classified object represents a valid context for the
classifying object in a given inference scenario.
In comparison to the CP implementation in (Szpek-
tor et al, 2008), our approach uses a unified model
which captures directionality in context matching.
To instantiate the scheme, one needs to define the
way training examples are obtained and processed.
This may be done within supervised classification,
where labeled examples are provided, or ? as we do
in this work ? using self-supervised classifiers which
obtain training examples automatically. We present
such an instantiation in Section 5, where a classifier is
trained for each category and each rule. When more
complex hypotheses are involved, Ch classifiers can
be trained separately for each relevant part of the
hypothesis, using the rest for disambiguation.
A combination of the three model scores provides
a final context matching score. In Section 6 we sug-
gest a way to combine the actual classification scores
as part of the integration of CP into TC, but other
combinations are plausible. In particular, binary clas-
sifications (valid vs. invalid) may be used as filters.
That is, the context is classified as valid only if all
relevant models classify it as such.
5 A Self-supervised Context Model
We now turn to demonstrate how our classification-
based scheme may be implemented. The model be-
low is exemplified on Name-based TC, but may be
applicable to other tasks, with few changes.
5.1 Training-set generation
Our implementation is self-supervised as we want
to integrate it within the unsupervised TC setting.
That is, the classifiers automatically obtain training
examples for the classifying object (a category or a
rule) without relying on labeled documents.
We obtain examples by querying the TC training
corpus with automatically-generated search queries.
The difficulty lies in correctly constructing queries
that will retrieve documents representing either valid
or invalid contexts for the classifying object. To this
end, we retrieve examples through a gradual process
in which the most accurate (least ambiguous) query
is used first and less accurate queries follow, until the
designated number of examples is acquired.
5.1.1 Obtaining positive examples
To acquire positive training examples, we con-
struct queries which are comprised of two main
clauses. The first contains the seeds, terms which
characterize the classifying object. Primarily, these
are the category name or the LHS of the rule. The sec-
ond consists of context words which are used when
the seeds are polysemous, and are intended to assist
disambiguation. When context words are used, at
least one seed and at least one context word must
be matched to retrieve a document. For example,
given the highly ambiguous category name space,
we first construct the query using only the monose-
mous term outer space; if the number of retrieved
documents does not meet the requirement, a second
query may be constructed: (?outer space? OR space)
AND (infinite OR science OR . . . ).
To generate a rule classifier Cr, we retrieve posi-
tive examples as follows. If the LHS term is monose-
mous according to WordNet2, we first query using
this term alone (e.g. decrypt), and add its monose-
mous synonyms and hyponyms if more examples are
required (e.g. decrypt OR decode). If the LHS is
polysemous, we carry out Procedure 1. Intuitively,
this procedure tries to minimize ambiguity by using
monosemous terms as much as possible; when poly-
semous terms must be used, it tries to ensure there are
monosemous terms to disambiguate them. Note that
entailment directionality is maintained throughout
the process, as seeds are only expanded with more
specific (entailing) terms, while context words are
only expanded with more general (entailed) terms.
Procedure 1 : Retrieval of Cr positive examples
Apply sequentially until sufficient examples are obtained:
1: Set the LHS as seed and the RHS?s monosemous syn-
onyms, hypernyms and derivations as context words.
2: Add monosemous synonyms and hyponyms of the
LHS to the seeds.
3: As in 2, but use polysemous terms as well.
4: Add polysemous context words.
Positive examples for category classifiers (Ch) are
obtained through a similar procedure as for rule clas-
2Terms not in WordNet are assumed monosemous.
24
sifiers. If the category is part of a hierarchy, we also
use the name of the parent category (e.g. sport for
rec.sport.hockey) as a context word.
5.1.2 Obtaining negative examples
Negative examples are even more challenging to
acquire. In prior work negative examples were se-
lected randomly (Kauchak and Barzilay, 2006; Con-
nor and Roth, 2007). We follow this method, but
also attempt to identify negative examples that are
semantically similar to the positive ones in order to
improve the discriminative power of the classifier
(Smith and Eisner, 2005). We do that by applying
a similar procedure which uses cohyponyms of the
seeds, e.g. baseball for hockey or islam for christian-
ity. Cohyponymy is a non-entailing relation; hence,
by using it we expect to obtain semantically-related,
yet invalid contexts. If not enough negative exam-
ples are retrieved using cohyponyms, we select the
remaining required examples randomly.
As the distribution of positive and negative ex-
amples in the data is unknown, we set the ratio of
negative to positive examples as a parameter of the
model, as in (Bergsma et al, 2008).
5.1.3 Insufficient examples
When the number of training examples for a rule
or a category is below a certain minimum, the re-
sulting classifier is expected to be of poor quality.
This usually happens for positive examples in any of
the following two cases: (i) the seed is rare in the
training set; (ii) the desired sense of the seed is rarely
found in the training set, and unwanted senses were
filtered by our retrieval query. For instance, nazarene
does not occur at all in the training set, and the classi-
fier corresponding to the rule ?nazarene? christian?
cannot be generated. On the other hand, cone does
appear in the corpus but not in the astrophysical sense
the rule ?cone? space? refers to. In such cases we
refrain from generating the classifier and use instead
a default score of 0 for each classified object. The
idea is that rare terms will also occur infrequently in
the test set, while cases where the term is found in
the corpus, but in a different sense than the desired
one, will be blocked.
5.1.4 Feature extraction
We extract global and local lexical features that are
standard in WSD work. Global features include all
the terms in the document or in the sentence in which
a match was found. Local features are extracted
around matches of seeds which comprised the query
that retrieved the document. These features include
the terms in a window around the match, and the
noun, verb, adjective and adverb nearest to the match
in either direction. For randomly sampled negative
examples, where no matched query terms exist, we
randomly select terms in the document as ?matches?
for local feature extraction. If more than one match of
the same term is found in a document, we assume one-
sense-per-discourse (Gale et al, 1992) and jointly
extract features for all matches of the term.
5.2 Applying the classifiers
During inference, for each direct match in a docu-
ment, the corresponding Ch is applied. For an indi-
rect match, the respective Cr is also applied.
In addition, Ch is applied to the matched rules.
Unlike t, a rule is not represented by a single text.
Therefore, to test a rule?s match with the category,
we randomly sample from the training set documents
containing the rule?s LHS. We apply Ch to each sam-
pled example and compute the ratio of positive classi-
fications. The result is a score indicating the domain-
specific probability of the rule to be applicable to
the category, and may be interpreted as an in-domain
prior. For instance, the rule ?check ? hockey? is
assigned a score of 0.05, since the sense of check
as a hockey defense technique is rare in the corpus.
On the other hand, non ambiguous rules, e.g. ?war-
ship ? ship? are assigned a high probability (1.0),
and so are rules whose LHS is ambiguous but its dom-
inant sense in the training corpus is the same one the
rule refers to, e.g. ?margin? earnings?(0.85).
We do not assign negative classifier scores to in-
valid matches but rather set them to zero instead. The
reason is that an invalid context only indicates that
the term cannot be used for entailing the category
name, but not that the document itself is irrelevant.
6 CP for Text Categorization
CP may be employed in any inference-based task,
but the integration with each task is somewhat dif-
ferent and needs to be specified. Below we present
a methodology for integrating CP into Name-based
Text Categorization.
25
As in (Barak et al, 2009) (Barak09 below), we
represent documents and categories by term-vectors
in the following way: a document vector contains
the document terms; a category vector contains two
sets of terms: C, the terms denoting the category
name, and E , their entailing terms. For example, oil
is added to the vector of the category crude by the
rule ?oil? crude? (i.e. crude ? C and oil ? E).
Barak09 assigned equal values of 1 to all vector
entries. We suggest integrating a CP-based context
model into TC by re-weighting the terms in the vec-
tors, prior to determining the final document-category
categorization score through vector similarity. Given
a category c, with term vector C, and a document d
with term vector D, the model re-weights vector en-
tries of matching terms (i.e., terms in C ?D), based
on the validity of the context match. Valid matches
should be assigned with higher scores than invalid
ones, leading to higher overall vector similarity for
documents with valid matches for the given category.
Non-matching terms are ignored as their weights are
canceled out in the subsequent vector product.
Specifically, the model assigns a new weight
wD(u) to a matching term u in the document vec-
tor D based on the model?s assessment of: (a) t? h,
the context match between the (match in the) doc-
ument and the category; and (if an indirect match)
(b) t ? r, the context match between the document
and the rule ?u? ci?, where ci ? C. The model also
sets a new weight wC(v) to a term v in the category
vector C based on the context match for r ? h, be-
tween the rule ?v ? cj? (cj ? C) and the category.
For instance, using our context matching scheme in
TC, wD(u) is set to Ch(u) or
Ch(u)+Cr(u)
2 for direct
and indirect matches, respectively; wC(v) is left as 1
if v ? C and set to Ch(v) when v ? E .
Barak09 assigned a single global context score to
a document-category pair using the LSA representa-
tions of their vectors. In our approach, however, we
consider the actual matches from the three different
views, hence the re-weighting of the vector entries
using three model scores.
7 Experimental Setting
7.1 Datasets and knowledge resources
Following (Gliozzo et al, 2009) and (Barak et al,
2009), we evaluated our method on two standard TC
datasets: Reuters-10 and 20-Newsgroups.
The Reuters-10 (R10, for short) is a sub-corpus
of the Reuters-21578 collection3, constructed from
the ten most frequent categories in the Reuters tax-
onomy. We used the Apte split of the Reuters-21578
collection, often used in TC tasks. The top 10 cate-
gories include about 9,000 documents, split into train-
ing (70%) and test (30%) sets. The 20-Newsgroups
(20NG) corpus is a collection of newsgroup postings
gathered from twenty different categories from the
Usenet Newsgroups hierarchy4. We used the ?by-
date? version of the corpus, which contains approxi-
mately 20,000 documents partitioned (nearly) evenly
across the categories and divided in advance to train-
ing (60%) and test (40%) sets.
As in (Gliozzo et al, 2009; Barak et al, 2009), we
adjusted non-standard category names (e.g. forsale
was renamed to sale) and manually specified for each
category its relevant WordNet senses. The sense tag-
ging properly defines the categories, and is expected
to accompany such hypotheses. Other types of in-
formation may be used for this purpose, e.g. words
from category descriptions, if such exist.
We applied standard preprocessing (sentence split-
ting, tokenization, lemmatization and part of speech
tagging) to all documents in the datasets. All terms,
including those denoting category names and rules,
are represented by their lemma and part of speech.
As sources for lexical entailment rules we used
WordNet 3.0 (synonyms, hyponyms, derivations
and meronyms) and a Wikipedia-derived rule-base
(Shnarch et al, 2009). Unlike Barak09 we did not
limit the rules extracted from WordNet to the most
frequent senses and used all rule types from the
Wikipedia-based resource.
7.2 Self-supervised model tuning
Tuning of the self-supervised context model?s pa-
rameters (number of training examples, negative to
positive ratio, feature set and the way negative exam-
ples are obtained) was performed over development
sets sampled from the training sets. Based on this tun-
ing, some parameters varied between the datasets and
between classifier types (Ch vs. Cr). For example,
3http://kdd.ics.uci.edu/databases/
reuters21578/reuters21578.html
4http://people.csail.mit.edu/jrennie/
20Newsgroups/
26
selection of negative examples based on cohyponyms
was found useful for Cr classifiers in R10, while ran-
dom examples were used in the rest of the cases.
We used SVMperf (Joachims, 2006) with a linear
kernel and binary feature weighting.
For querying the corpus we used the Lucene search
engine5 in its default setting. Up to 150 positive
examples were retrieved for each classifier, with 5
examples set as the required minimum. This resulted
in generating 100% of the hypothesis classifiers for
both datasets and 95% and 70% of the rule classifiers
for R10 and 20NG, respectively.
We computed Ch(r) scores based on up to 20 sam-
pled instances. If less than 2 examples were found in
the training set, we assigned an ?unknown? context
match probability of 0.5, since a rare LHS occurrence
does not indicate anything about its meaning in the
corpus. Such cases constituted 2% (R10) and 11%
(20NG) of the utilized rules.
7.3 Baseline models
To provide a more meaningful comparison with prior
work, we focus on the first unsupervised step in the
typical Name-based TC flow, without the subsequent
supervised training. Our goal is to improve the accu-
racy of this first step, and we therefore compare our
context model?s performance to two unsupervised
methods used by Barak09.
The first baseline, denoted Barakno-cxt, is the co-
sine similarity score between the document and cate-
gory vectors where all terms are equally weighted to
a score of 1.6 This baseline shows the performance
when no context model is employed.
The second baseline, denoted Barakfull, is a repli-
cation of the state of the art context model for Name-
based TC. In this method, LSA vectors are con-
structed for a document by averaging the LSA vectors
of its individual terms, and for a category by averag-
ing the LSA vectors of the terms denoting its name.
The categorization score of a document-category pair
is set to be the product between the cosine similarity
score of the LSA vectors and the score given by the
above Barakno-cxt method. We note that LSA-based
context models performed best also in (Gliozzo et al,
2009) and (Szpektor et al, 2008).
5http://lucene.apache.org
6Other attempted weighting schemes, such as tf-idf, did not
yield better performance.
Model
Reuters-10
Accuracy P R F1
Barakno-cxt 73.2 63.6 77.0 69.7
Barakfull 76.3 68.0 79.2 73.2
Class.-based 79.3 71.8 83.6 77.2
Model
20-Newsgroups
Accuracy P R F1
Barakno-cxt 63.7 44.5 74.6 55.8
Barakfull 69.4 50.1 82.8 62.4
Class.-based 73.4 54.7 76.4 63.7
Table 1: Evaluation results.
All models were constructed based on the TC train-
ing sets, using no external corpora. The vocabulary
consists of terms that appear more than once in the
training set. The terms we consider include nouns,
verbs, adjectives and adverbs, as well as nominal
multi-word expressions.
8 Results and Analysis
Given a document, all categories for which a lexical
match was found in the document are considered,
and the document is classified to the highest scoring
category. If all categories are assigned non-positive
scores, the document is not assigned to any of them.
Based on this requirement that a document con-
tains at least one match for the category, 4862
document-category pairs were considered for clas-
sification in R10 and 9955 pairs in 20NG. We eval-
uated our context model, as well as the baselines,
based on the accuracy of these classifications, i.e.
the percentage of correct decisions among the candi-
date document-category pairs. We also measured the
models? performance in terms of micro-averaged pre-
cision (P ), relative recall (R) and F1. Like Barak09,
recall is computed relative to the potential recall of
the rule-set which provides the entailing terms.
Table 1 presents the evaluation results. As in
Barak09, the LSA-based model outperforms the first
baseline, supporting its usefulness as a context model.
In both datasets our model outperformed the base-
lines in terms of accuracy. This result is statistically
significant with p < 0.01 according to McNemar?s
test (McNemar, 1947). Recall is lower for our model
in 20NG but F1 scores are higher for both datasets.
These results indicate that the classification-based
context model provides a favorable alternative to the
27
Removed
Reuters-10 20-Newsgroups
Accuracy F1 Accuracy F1
- 79.3 77.2 73.4 63.7
Ch(t) 76.2 72.3 71.9 61.0
Cr(t) 80.5 77.6 74.3 64.5
Ch(r) 78.4 75.7 73.1 63.4
Table 2: Ablation tests results.
state of the art LSA-based method.
Table 2 presents ablation tests of our model. In
each test we measured the classification performance
when one of the three classification scores is ignored.
Clearly, Ch(t) is the most beneficial component, and
in general the category classifiers help improving
overall performance. The limited performance of Cr
may be related to higher ambiguity in rules relative to
category names, resulting in noisier training data. In
addition, the small size of the training set limits the
number of training examples for rule classifiers. This
problem affects Cr more than Ch since, by nature,
the corpus includes more occurrences of category
names. Still, Cr contributes to improved recall (this
fact is not visible in Table 2).
The coverage of the utilized rule-set determines
the maximal (absolute) recall that can be achieved
by any model. With the rule-set we used in this ex-
periment, the recall upper bound was 59.1% for R10
and 40.6% for 20NG. However, rule coverage af-
fects precision as well: In many cases documents are
assigned to incorrect categories because the correct
category is not even a candidate as no entailing term
was matched for it in the document. For instance,
a document with the sentence ?For sale or trade!!!
BMW R60US. . . ? was classified by our method to
the category forsale, while its gold-standard category
is motorcycles. Yet, none of the rules in our rule-set
triggered motorcycles as a candidate category for this
document. Ideally, a context model would rule out
all incorrect candidate categories; in practice even a
single low score for one of the competing categories
results in a false positive error in such cases (in addi-
tion to the recall loss). To reduce these problems we
intend to employ additional knowledge resources in
future work.
Our algorithm for retrieving training examples
turned out to be not sufficiently accurate, particularly
for negative examples. This is a challenging task that
requires further research. Although useful for some
classifier types, the use of cohyponyms may retrieve
potentially positive examples as negative ones, since
terms that are considered cohyponyms in WordNet
are often perceived as near synonyms in common
usage, e.g. buyout and purchase in the context of
acquisitions. Likewise, using WordNet senses to de-
termine ambiguity is also inaccurate. Rare or too
fine-grained senses, common in WordNet, cause a
term to be considered ambiguous, which in turn trig-
gers the use of less accurate retrieval methods. For
example, auction has a bridge-related WordNet sense
which is irrelevant for our dataset, but made the term
be considered ambiguous. This calls for develop-
ment of other methods for determining word ambigu-
ity, which consider the actual usage of terms in the
domain rather than relying solely on WordNet.
9 Conclusions
In this paper we presented a generic classification-
based scheme for comprehensively addressing con-
text matching in textual inference scenarios. We
presented a concrete implementation of the proposed
scheme for Name-based TC, and showed how CP
decisions can be integrated within the TC setting.
Utilizing classifiers for context matching offers
several advantages. They naturally incorporate di-
rectionality and allow integrating various types of
information, including ones not used in this work
such as syntactic features. Our results indeed support
this approach. Still, further research is required re-
garding issues raised by the use of multiple classifiers,
scalability in particular.
Hypotheses in TC are available in advance. While
also the case in other applications, it constitutes a
practical challenge when hypotheses are given ?on-
line?, like Information Retrieval queries, since classi-
fiers will have to be generated on the fly. We intend
to address this issue in future work.
Lastly, we plan to apply the generic classification-
based approach to address context matching in other
inference-based applications.
Acknowledgments
This work was partially supported by the Israel Sci-
ence Foundation grant 1112/08 and the NEGEV
project (www.negev-initiative.org).
28
References
Libby Barak, Ido Dagan, and Eyal Shnarch. 2009. Text
Categorization from Category Name via Lexical Refer-
ence. In HLT-NAACL (Short Papers).
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative Learning of Selectional Preference from
Unlabeled Text. In In Proceedings of EMNLP.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Machine
Learning Research, 3:993?1022, March.
Michael Connor and Dan Roth. 2007. Context Sensitive
Paraphrasing with a Global Unsupervised Classifier. In
Proceedings of ECML.
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein, and Carlo Strapparava. 2006. Direct Word
Sense Matching for Lexical Substitution. In Proceed-
ings of COLING-ACL.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing Textual Entailment: Rational, Eval-
uation and Approaches. Natural Language Engineer-
ing, pages 15(4):1?17.
Scott Deerwester, Scott Deerwester, Susan T. Dumais,
George W. Furnas, Thomas K. Landauer, and Richard
Harshman. 1990. Indexing by Latent Semantic Anal-
ysis. Journal of the American Society for Information
Science, 41:391?407.
Georgiana Dinu and Mirella Lapata. 2010. Topic Models
for Meaning Similarity in Context. In Proceedings of
Coling 2010: Posters.
Doug Downey and Oren Etzioni. 2009. Look Ma, No
Hands: Analyzing the Monotonic Feature Abstraction
for Text Classification. In Proceedings of NIPS.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and Com-
munication). The MIT Press.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One Sense per Discourse. In Proceed-
ings of the workshop on Speech and Natural Language.
Alfio Gliozzo, Carlo Strapparava, and Ido Dagan. 2009.
Improving Text Categorization Bootstrapping via Unsu-
pervised Learning. ACM Trans. Speech Lang. Process.,
6:1:1?1:24, October.
Sanda M. Harabagiu, Steven J. Maiorano, and Marius A.
Pas?ca. 2003. Open-domain Textual Question Answer-
ing Techniques. Natural Language Engineering, 9:231?
267, September.
Thorsten Joachims. 2006. Training Linear SVMs in
Linear Time. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD).
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of HLT-
NAACL.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional Distributional
Similarity for Lexical Inference. Natural Language
Engineering, 16(4):359?389.
Diana McCarthy and Roberto Navigli. 2009. The English
Lexical Substitution Task. Language Resources and
Evaluation, 43(2):139?159.
Quinn McNemar. 1947. Note on the Sampling Error
of the Difference between Correlated Proportions or
Percentages. Psychometrika, 12(2):153?157, June.
Roberto Navigli. 2009. Word Sense Disambiguation: A
Survey. ACM Computing Surveys, 41(2):1?69.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timo-
thy Chklovski, and Eduard Hovy. 2007. ISP: Learning
Inferential Selectional Preferences. In Proceedings of
NAACL-HLT.
Marco Pennacchiotti, Roberto Basili, Diego De Cao, and
Paolo Marocco. 2007. Learning Selectional Prefer-
ences for Entailment or Paraphrasing Rules. In Pro-
ceedings of RANLP.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A La-
tent Dirichlet Allocation Method for Selectional Prefer-
ences. In Proceedings of ACL.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting Lexical Reference Rules from Wikipedia. In
Proceedings of IJCNLP-ACL.
Noah A. Smith and Jason Eisner. 2005. Contrastive
Estimation: Training Log-linear Models on Unlabeled
Data. In Proceedings of ACL.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob Gold-
berger. 2008. Contextual Preferences. In Proceedings
of ACL-08: HLT.
29
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 181?190,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Probabilistic Modeling of Joint-context in Distributional Similarity
Oren Melamud
?
, Ido Dagan
?
, Jacob Goldberger
?
, Idan Szpektor
?
, Deniz Yuret
?
? Computer Science Department, Bar-Ilan University
? Faculty of Engineering, Bar-Ilan University
? Yahoo! Research Israel
? Koc? University
{melamuo,dagan,goldbej}@{cs,cs,eng}.biu.ac.il
idan@yahoo-inc.com, dyuret@ku.edu.tr
Abstract
Most traditional distributional similarity
models fail to capture syntagmatic patterns
that group together multiple word features
within the same joint context. In this work
we introduce a novel generic distributional
similarity scheme under which the power
of probabilistic models can be leveraged
to effectively model joint contexts. Based
on this scheme, we implement a concrete
model which utilizes probabilistic n-gram
language models. Our evaluations sug-
gest that this model is particularly well-
suited for measuring similarity for verbs,
which are known to exhibit richer syntag-
matic patterns, while maintaining compa-
rable or better performance with respect
to competitive baselines for nouns. Fol-
lowing this, we propose our scheme as a
framework for future semantic similarity
models leveraging the substantial body of
work that exists in probabilistic language
modeling.
1 Introduction
The Distributional Hypothesis is commonly
phrased as ?words which are similar in meaning
occur in similar contexts? (Rubenstein and Good-
enough, 1965). Distributional similarity models
following this hypothesis vary in two major as-
pects, namely the representation of the context and
the respective computational model. Probably the
most prominent class of distributional similarity
models represents context as a vector of word fea-
tures and computes similarity using feature vector
arithmetics (Lund and Burgess, 1996; Turney et
al., 2010). To construct the feature vectors, the
context of each target word token
1
, which is com-
monly a word window around it, is first broken
1
We use word type to denote an entry in the vocabulary,
and word token for a particular occurrence of a word type.
into a set of individual independent words. Then
the weights of the entries in the word feature vec-
tor capture the degree of association between the
target word type and each of the individual word
features, independently of one another.
Despite its popularity, it was suggested that
the word feature vector approach misses valu-
able information, which is embedded in the co-
location and inter-relations of words (e.g. word
order) within the same context (Ruiz-Casado et al.,
2005). Following this motivation, Ruiz-Casado
et al. (2005) proposed an alternative composite-
feature model, later adopted in (Agirre et al.,
2009). This model adopts a richer context repre-
sentation by considering entire word window con-
texts as features, while keeping the same compu-
tational vector-based model. Although showing
interesting potential, this approach suffers from a
very high-dimensional feature space resulting in
data sparseness problems. Therefore, it requires
exceptionally large learning corpora to consider
large windows effectively.
A parallel line of work adopted richer context
representations as well, with a different compu-
tational model. These works utilized neural net-
works to learn low dimensional continuous vector
representations for word types, which were found
useful for measuring semantic similarity (Col-
lobert and Weston, 2008; Mikolov et al., 2013).
These vectors are trained by optimizing the pre-
diction of target words given their observed con-
texts (or variants of this objective). Most of these
models consider each observed context as a joint
set of context words within a word window.
In this work we follow the motivation in the pre-
vious works above to exploit richer joint-context
representations for modeling distributional simi-
larity. Under this approach the set of features in
the context of each target word token is consid-
ered to jointly reflect on the meaning of the target
word type. To further facilitate this type of mod-
181
eling we propose a novel probabilistic computa-
tional scheme for distributional similarity, which
leverages the power of probabilistic models and
addresses the data sparseness challenge associated
with large joint-contexts. Our scheme is based on
the following probabilistic corollary to the distri-
butional hypothesis:
(1)?words are similar in meaning if
they are likely to occur in the same contexts?
To realize this corollary, our distributional sim-
ilarity scheme assigns high similarity scores to
word pairs a and b, for which a is likely in the con-
texts that are observed for b and vice versa. The
scheme is generic in the sense that various under-
lying probabilistic models can be used to provide
the estimates for the likelihood of a target word
given a context. This allows concrete semantic
similarity models based on this scheme to lever-
age the capabilities of probabilistic models, such
as established language models, which typically
address the modeling of joint-contexts.
We hypothesize that an underlying model that
could capture syntagmatic patterns in large word
contexts, yet is flexible enough to deal with data
sparseness, is desired. It is generally accepted
that the semantics of verbs in particular are cor-
related with their syntagmatic properties (Levin,
1993; Hanks, 2013). This provides grounds to ex-
pect that such model has the potential to excel for
verbs. To capture syntagmatic patterns, we choose
in this work standard n-gram language models as
the basis for a concrete model implementing our
scheme. This choice is inspired by recent work on
learning syntactic categories (Yatbaz et al., 2012),
which successfully utilized such language mod-
els to represent word window contexts of target
words. However, we note that other richer types
of language models, such as class-based (Brown
et al., 1992) or hybrid (Tan et al., 2012), can be
seamlessly integrated into our scheme.
Our evaluations suggest that our model is in-
deed particularly advantageous for measuring se-
mantic similarity for verbs, while maintaining
comparable or better performance with respect to
competitive baselines for nouns.
2 Background
In this section we provide additional details re-
garding previous works that we later use as base-
lines in our evaluations.
To implement the composite-feature approach,
Ruiz-Casado et al. (2005) used a Web search en-
gine to compare entire window contexts of target
word types. For example, a single feature that
could be retrieved this way for the target word like
is ?Children cookies and milk?. They showed
good results on detecting synonyms in the 80
multiple-choice questions TOEFL test. Agirre et
al. (2009) constructed composite-feature vectors
using an exceptionally large 1.6 Teraword learn-
ing corpus. They found that this approach out-
performs the traditional independent feature vec-
tor approach on a subset of the WordSim353 test-
set (Finkelstein et al., 2001), which is designed to
test the more restricted relation of semantic simi-
larity (to be distinguished from looser semantic re-
latedness). We are not aware of additional works
following this approach, of using entire word win-
dows as features.
Neural networks have been used to train lan-
guage models that are based on low dimensional
continuous vector representations for word types,
also called word embeddings (Bengio et al., 2003;
Mikolov et al., 2010). Although originally de-
signed to improve language models, later works
have shown that such word embeddings are useful
in various other NLP tasks, including measuring
semantic similarity with vector arithmetics (Col-
lobert and Weston, 2008; Mikolov et al., 2013).
Specifically, the recent work by Mikolov et al.
(2013) introduced the CBOW and Skip-gram mod-
els, achieving state-of-the-art results in detecting
semantic analogies. The CBOW model is trained
to predict a target word given the set of context
words in a word window around it, where this
context is considered jointly as a bag-of-words.
The Skip-gram model is trained to predict each of
the context words independently given the target
word.
3 Probabilistic Distributional Similarity
3.1 Motivation
In this section we briefly demonstrate the bene-
fits of considering joint-contexts of words. As an
illustrative example, we note that the target words
like and surround may share many individual word
features such as ?school? and ?campus? in the sen-
tences ?Mary?s son likes the school campus? and
?The forest surrounds the school campus?. This
potentially implies that individual features may
not be sufficient to accurately reflect the difference
182
between such words. Alternatively, we could use
the following composite features to model the con-
text of these words, ?Mary?s son the school
campus? and ?The forest the school campus?.
This would discriminate better between like and
surround. However, in this case sentences such as
?Mary?s son likes the school campus? and ?John?s
son loves the school campus? will not provide any
evidence to the similarity between like and love,
since ?Mary?s son the school campus? is a dif-
ferent feature than ?John?s son the school cam-
pus?.
In the remainder of this section we propose
a modeling scheme and then a concrete model,
which can predict that like and love are likely to
occur in each other?s joint-contexts, whereas like
and surround are not, and then assign similarity
scores accordingly.
3.2 The probabilistic similarity scheme
We now present a computational scheme that re-
alizes our proposed corollary (1) to the distribu-
tional hypothesis and facilitates robust probabilis-
tic modeling of joint contexts. First, we slightly
rephrase this corollary as follows: ?words a and
b are similar in meaning if word b is likely in
the contexts of a and vice versa?. We denote the
probability of an occurrence of a target word b
given a joint-context c by p(b|c). For example,
p(love|?Mary?s son the school campus?) is the
probability of the word love to be the filler of the
?place-holder? in the given joint-context ?Mary?s
son the school campus?. Similarly, we denote
p(c|a) as the probability of a joint-context c given
a word a, which fills its place-holder. We now
propose p
sim
(b|a) to reflect how likely b is in the
joint-contexts of a. We define this measure as:
(2)p
sim
(b|a) =
?
c
p(c|a) ? p(b|c)
where c goes over all possible joint-contexts in the
language.
To implement this measure we need to find
an efficient estimate for p
sim
(b|a). The most
straight forward strategy is to compute sim-
ple corpus count ratio estimates for p(b|c) and
p(c|a), denoted p
#
(b|c) =
count(b,c)
count(?,c)
and
p
#
(c|a) =
count(a,c)
count(a,?)
. However, when consid-
ering large joint-contexts for c, this approach be-
comes similar to the composite-feature approach
since it is based on co-occurrence counts of tar-
get words with large joint-contexts. Therefore, we
expect in this case to encounter the data sparse-
ness problems mentioned in Section 1, where se-
mantically similar word type pairs that share only
few or no identical joint-contexts yield very low
p
sim
(b|a) estimates.
To address the data sparseness challenge and
adopt more advanced context modeling, we aim to
use a more robust underlying probabilistic model
? for our scheme and denote the probabilities es-
timated by this model by p
?
(b|c) and p
?
(c|a). We
note that contrary to the count ratio model, given a
robust model ?, such as a language model, p
?
(b|c)
and p
?
(c|a) can be positive even if the target words
b and a were not observed with the joint-context c
in the learning corpus.
While using p
?
(b|c) and p
?
(c|a) to estimate the
value of p
sim
(b|a) addresses the sparseness chal-
lenge, it introduces a computational challenge.
This is because estimating p
sim
(b|a) would re-
quire computing the sum over all of the joint-
contexts in the learning corpus regardless of
whether they were actually observed with either
word type a or b. For that reason we choose a
middle ground approach, estimating p(b|c) with
?, while using a count ratio estimate for p(c|a),
as follows. We denote the collection of all joint-
contexts observed for the target word a in the
learning corpus by C
a
, where |C
a
|= count(a, ?).
For example, C
like
= {c
1
=?Mary?s son the
school campus?, c
2
=?John?s daughter to read
poetry?,...}. We note that this collection is a multi-
set, where the same joint-context can appear more
than once.
We now approximate p
sim
(b|a) from Equation
(2) as follows:
(3)
p?
sim
?
(b|a) =
?
c
p
#
(c|a) ? p
?
(b|c) =
1
|C
a
|
?
?
c?C
a
p
?
(b|c)
We note that this formulation still addresses
sparseness of data by using a robust model, such as
a language model, to estimate p
?
(b|c). At the same
time it requires our model to sum only over the
joint-contexts in the collection C
a
, since contexts
not observed for a yield p
#
(c|a) = 0. Even so,
since the size of these context collections grows
linearly with the corpus size, considering all ob-
served contexts may still present a scalability chal-
lenge. Nevertheless, we expect our approximation
p?
sim
?
(b|a) to converge with a reasonable sample
183
size from a?s joint-contexts. Therefore, in order
to bound computational complexity, we limit the
size of the context collections used to train our
model to a maximum of N by randomly sampling
N entries from larger collections. In all our ex-
periments we use N = 10, 000. Higher values
of N yielded negligible performances differences.
Overall we see that our model estimates p?
sim
?
(b|a)
as the average probability predicted for b in (a
large sample of) the contexts observed for a.
Finally, we define our similarity measure for tar-
get word types a and b:
(4)sim
?
(a, b) =
?
p?
sim
?
(b|a) ? p?
sim
?
(a|b)
As intended, this similarity measure promotes
word pairs in which both b is likely in the con-
texts of a and vice versa. Next, we describe a
model which implements this scheme with an n-
gram language model as a concrete choice for ?.
3.3 Probabilistic similarity using language
models
In this work we focus on the word window context
representation, which is the most common. We
define a word window of order k around a target
word as a window with up to k words to each side
of the target word, not crossing sentence bound-
aries. The word window does not include the tar-
get word itself, but rather a ?place-holder? for it.
Since word windows are sequences of words,
probabilistic language models are a natural choice
of a model ? for estimating p
?
(b|c). Language
models assign likelihood estimates to sequences
of words using approximation strategies. In
this work we choose n-gram language models,
aiming to capture syntagmatic properties of the
word contexts, which are sensitive to word or-
der. To approximate the probability of long se-
quences of words, n-gram language models com-
pute the product of the estimated probability of
each word in the sequence conditioned on at most
the n ? 1 words preceding it. Furthermore, they
use ?discounting? methods to improve the esti-
mates of conditional probabilities when learning
data is sparse. Specifically, in this work we use
the Kneser-Ney n-gram model (Kneser and Ney,
1995).
We compute p
?
(b|c) as follows:
(5)p
?
(b|c) =
p
?
(b, c)
p
?
(c)
where p
?
(b, c) is the probability of the word se-
quence comprising the word window c, in which
the word b fills the place-holder. For instance, for
c = ?I drive my to work every? and b = car,
p
?
(b, c) is the estimated language model probabil-
ity of ?I drive my car to work every?. p
?
(c) is the
marginal probability of p
?
(?, c) over all possible
words in the vocabulary.
2
4 Experimental Settings
Although sometimes used interchangeably, it is
common to distinguish between semantic simi-
larity and semantic relatedness (Budanitsky and
Hirst, 2001; Agirre et al., 2009). Semantic simi-
larity is used to describe ?likeness? relations, such
as the relations between synonyms, hypernym-
hyponyms, and co-hyponyms. Semantic relat-
edness refers to a broader range of relations in-
cluding also meronymy and various other asso-
ciative relations as in ?pencil-paper? or ?penguin-
Antarctica?. In this work we focus on semantic
similarity and evaluate all compared methods on
several semantic similarity tasks.
Following previous works (Lin, 1998; Riedl and
Biemann, 2013) we use Wordnet to construct large
scale gold standards for semantic similarity evalu-
ations. We perform the evaluations separately for
nouns and verbs to test our hypothesis that our
model is particularly well-suited for verbs. To fur-
ther evaluate our results on verbs we use the verb
similarity test-set released by (Yang and Powers,
2006), which contains pairs of verbs associated
with semantic similarity scores based on human
judgements.
4.1 Compared methods
We compare our model with a traditional fea-
ture vector model, the composite-feature model
(Agirre et al., 2009), and the recent state-of-the-art
word embedding models, CBOW and Skip-gram
(Mikolov et al., 2013), all trained on the same
learning corpus and evaluated on equal grounds.
We denote the traditional feature vector baseline
by IFV
W?k
, where IFV stands for ?Independent-
Feature Vector? and k is the order of the con-
text word window considered. Similarly, we
2
Computing p
?
(c) by summing over all possible place-
holder filler words, as we did in this work, is computation-
ally intensive. However, this can be done more efficiently
by implementing customized versions of (at least some) n-
gram language models with little computational overhead,
e.g. by counting the learning corpus occurrences of n-gram
templates, in which one of the elements matches any word.
184
denote the composite-feature vector baseline by
CFV
W?k
, where CFV stands for ?Composite-
Feature Vector?. This baseline constructs
traditional-like feature vectors, but considers en-
tire word windows around target word tokens as
single features. In both of these baselines we use
Cosine as the vector similarity measure, and posi-
tive pointwise mutual information (PPMI) for the
feature vector weights. PPMI is a well-known
variant of pointwise mutual information (Church
and Hanks, 1990), and the combination of Cosine
with PPMI was shown to perform particularly well
in (Bullinaria and Levy, 2007).
We denote Mikolov?s CBOW and Skip-gram
baseline models by CBOW
W?k
and SKIP
W?k
respectively, where k denotes again the order of
the window used to train these models. We used
Mikolov?s word2vec utility
3
with standard param-
eters (600 dimensions, negative sampling 15) to
learn the word embeddings, and Cosine as the vec-
tor similarity measure between them.
As the underlying probabilistic language model
for our method we use the Berkeley implementa-
tion
4
(Pauls and Klein, 2011) of the Kneser-Ney
n-gram model with the default discount parame-
ters. We denote our model PDS
W?k
, where PDS
stands for ?Probabilistic Distributional Similar-
ity?, and k is the order of the context word win-
dow. In order to avoid giving our model an un-
fair advantage of tuning the order of the language
model n as an additional parameter, we use a fixed
n = k + 1. This means that the conditional prob-
abilities that our n-gram model learns consider a
scope of up to half the size of the window, which
is the distance in words between the target word
and either end of the window. We note that this
is the smallest reasonable value for n, as smaller
values effectively mean that there will be context
words within the window that are more than n
words away from the target word, and therefore
will not be considered by our model.
As learning corpus we used the first CD of
the freely available Reuters RCV1 dataset (Rose
et al., 2002). This learning corpus contains ap-
proximately 100M words, which is comparable in
size to the British National Corpus (BNC) (As-
ton, 1997). We first applied part-of-speech tag-
ging and lemmatization to all words. Then we
represented each word w in the corpus as the pair
3
http://code.google.com/p/word2vec
4
http://code.google.com/p/berkeleylm/
[pos(w), lemma(w)], where pos(w) is a coarse-
grained part-of-speech category and lemma(w) is
the lemmatized form of w. Finally, we converted
every pair [pos(w), lemma(w)] that occurs less
than 100 times in the learning corpus to the pair
[pos(w), ? ], which represents all rare words of the
same part-of-speech tag. Ignoring rare words is a
common practice used in order to clean up the cor-
pus and reduce the vocabulary size (Gorman and
Curran, 2006; Collobert and Weston, 2008).
The above procedure resulted in a word vocabu-
lary of 27K words. From this vocabulary we con-
structed a target verb set with over 2.5K verbs by
selecting all verbs that exist in Wordnet (Fellbaum,
2010). We repeated this procedure to create a tar-
get noun set with over 9K nouns. We used our
learning corpus for all compared methods and had
them assign a semantic similarity score for every
pair of verbs and every pair of nouns in these tar-
get sets. These scores were later used in all of our
evaluations.
4.2 Wordnet evaluation
There is a shortage of large scale test-sets for se-
mantic similarity. Popular test-sets such as Word-
Sim353 and the TOEFL synonyms test contain
only 353 and 80 test items respectively, and there-
fore make it difficult to obtain statistically signif-
icant results. To automatically construct larger-
scale test-sets for semantic similarity, we sampled
large target word subsets from our corpus and used
Wordnet as a gold standard for their semantically
similar words, following related previous evalua-
tions (Lin, 1998; Riedl and Biemann, 2013). We
constructed two test-sets for our primary evalua-
tion, one for verb similarity and another for noun
similarity.
To perform the verb similarity evaluation, we
randomly sampled 1,000 verbs from the target
verb set, where the probability of each verb to be
sampled is set to be proportional to its frequency in
the learning corpus. Next, for each sampled verb
a we constructed a Wordnet-based gold standard
set of semantically similar words. In this set each
verb a
?
is annotated as a ?synonym? of a if at least
one of the senses of a
?
is a synonym of any of the
senses of a. In addition, each verb a
?
is annotated
as a ?semantic neighbor? of a if at least one of the
senses of a
?
is a synonym, co-hyponym, or a di-
rect hypernym/hyponym of any of the senses of a.
We note that by definition all verbs annotated as
185
synonyms of a are annotated as semantic neigh-
bors as well. Next, per each verb a and an evalu-
ated method, we generated a ranked list of all other
verbs, which was induced according to the similar-
ity scores of this method.
Finally, we evaluated the compared methods
on two tasks, ?synonym detection? and ?seman-
tic neighbor detection?. In the synonym detection
task we evaluated the methods? ability to retrieve
as much verbs annotated in our gold standard as
?synonyms?, in the top-n entries of their ranked
lists. Similarly, we evaluated all methods on the
?semantic neighbors? task. The synonym detec-
tion task is designed to evaluate the ability of the
compared methods to identify a more restrictive
interpretation of semantic similarity, while the se-
mantic neighbor detection task does the same for
a somewhat broader interpretation.
We repeated the above procedure for sam-
pling 1,000 target nouns, constructing the noun
Wordnet-based gold standards and evaluating on
the two semantic similarity tasks.
4.3 VerbSim evaluation
The publicly available VerbSim test-set contains
130 verb pairs, each annotated with an average of
6 human judgements of semantic similarity (Yang
and Powers, 2006). We extracted a 107 pairs sub-
set of this dataset for which all verbs are in our
learning corpus. We followed works such as (Yang
and Powers, 2007; Agirre et al., 2009) and com-
pared the Spearman correlations between the verb-
pair similarity scores assigned by the compared
methods and the manually annotated scores in this
dataset.
5 Results
For each method and verb a in our 1,000 tested
verbs, we used the Wordnet gold standard to com-
pute the precision at top-1, top-5 and top-10 of the
ranked list generated by this method for a. We
then computed mean precision values averaged
over all verbs for each of the compared methods,
denoted as P@1, P@5 and P@10. The detailed
report of P@10 results is omitted for brevity, as
they behave very similarly to P@5. We varied the
context window order used by all methods to test
its effect on the results. We measured the same
metrics for nouns.
The results of our Wordnet-based 1,000 verbs
evaluation are presented in the upper part of Fig-
ure 1. The results show significant improvement
of our method over all baselines, with a margin
between 2 to 3 points on the synonyms detection
task and 5 to 7 points on the semantic neighbors
detection task. Our best performing configura-
tions are PDS
W?3
and PDS
W?4
, outperform-
ing all other baselines on both tasks and in all pre-
cision categories. This difference is statistically
significant at p < 0.001 using a paired t-test in all
cases except for the P@1 in the synonyms detec-
tion task. Within the baselines, the composite fea-
ture vector (CFV) performs somewhat better than
the independent feature vector (IFV) baseline, and
both methods perform best around window order
of two, with gradual decline for larger windows.
The word embedding baselines, CBOW and SKIP,
perform comparably to the feature vector base-
lines and to one another, with best performance
achieved around window order of four.
When gradually increasing the context window
order within the range of up to 4 words, our PDS
model shows improvement. This is in contrast to
the feature vector baselines, whose performance
declines for context window orders larger than 2.
This suggests that our approach is able to take ad-
vantage of larger contexts in comparison to stan-
dard feature vector models. The decline in perfor-
mance for the independent feature vector baseline
(IFV) may be related to the fact that independent
features farther away from the target word are gen-
erally more loosely related to it. This seems con-
sistent with previous works, where narrow win-
dows of the order of two words performed well
(Bullinaria and Levy, 2007; Agirre et al., 2009;
Bruni et al., 2012) and in particular so when eval-
uating semantic similarity rather than relatedness.
On the other hand, the decline in performance for
the composite feature vector baseline (CFV) may
be attributed to the data sparseness phenomenon
associated with larger windows. The performance
of the word embedding baselines (CBOW and
SKIP) starts declining very mildly only for win-
dow orders larger than 4. This might be attributed
to the fact that these models assign lower weights
to context words the farther away they are from the
center of the window.
The results of our Wordnet-based 1,000 nouns
evaluation are presented in the lower part of Fig-
ure 1. These results are partly consistent with the
results achieved for verbs, but with a couple of
notable differences. First, though our model still
186
Figure 1: Mean precision scores as a function of window order, obtained against the Wordnet-based gold
standard, on both the verb and noun test-sets with both the synonyms and semantic neighbor detection
tasks. ?P@n? stands for precision in the top-n words of the ranked lists. Note that the Y-axis scale varies
between graphs.
outperforms or performs comparably to all other
baselines, in this case the advantage of our model
over the feature vector baselines is much more
moderate and not statistically significant. Second,
the word embedding baselines generally perform
worst (with CBOW performing a little better than
SKIP), and our model outperforms them in both
P@5 and P@10 with a margin of around 2 points
for the synonyms detection task and 3-4 points for
the neighbor detection task, with statistical signif-
icance at p < 0.001.
Next, to reconfirm the particular applicability
of our model to verb similarity as apparent from
the Wordnet evaluation, we performed the Verb-
Sim evaluation and present the results in Table 1.
We compared the Spearman correlation obtained
for the top-performing window order of each of
the evaluated methods in the Wordnet verbs eval-
uation. We present two sets of results. The ?all
scores? results follow the standard evaluation pro-
cedure, considering all similarity scores produced
by each method. In the ?top-100 scores? results,
for each method we converted to zero the scores
that it assigned to word pairs, where neither of
the words is in the top-100 most similar words
of the other. Then we performed the evaluation
with these revised scores. This procedure focuses
on evaluating the quality of the methods? top-
100 ranked word lists. The results show that our
method outperforms all baselines by a nice mar-
187
Method All scores top-100 scores
PDS W-4 0.616 0.625
CFV W-2 0.477 0.497
IFV W-2 0.467 0.546
SKIP W-4 0.469 0.512
CBOW W-5 0.528 0.469
Table 1: Spearman correlation values obtained for
the VerbSim evaluation. Each method was evalu-
ated with the optimal window order found in the
Wordnet verbs evaluation.
gin of more than 8 points with the score of 0.616
and 0.625 for the ?all scores? and ?top-100 scores?
evaluations respectively. Though not statistically
significant, due to the small test-set size, these re-
sults support the ones from the Wordnet evalu-
ation, suggesting that our model performs better
than the baselines on measuring verb similarity.
In summary, our results suggest that in lack of a
robust context modeling scheme it is hard for dis-
tributional similarity models to effectively lever-
age larger word window contexts for measuring
semantic similarity. It appears that this is some-
what less of a concern when it comes to noun sim-
ilarity, as the simple feature vector models reach
near-optimal performance with small word win-
dows of order 2, but it is an important factor for
verb similarity. In his recent book, Hanks (2013)
claims that contrary to nouns, computational mod-
els that are to capture the meanings of verbs must
consider their syntagmatic patterns in text. Our
particularly good results on verb similarity sug-
gest that our modeling approach is able to cap-
ture such information in larger context windows.
We further conjecture that the reason the word em-
bedding baselines did not do as well as our model
on verb similarity might be due to their particular
choice of joint-context formulation, which is not
sensitive to word order. However, these conjec-
tures should be further validated with additional
evaluations in future work.
6 Future Directions
In this paper we investigated the potential for im-
proving distributional similarity models by model-
ing jointly the occurrence of several features under
the same context. We evaluated several previous
works with different context modeling approaches
and suggest that the type of the underlying con-
text modeling may have significant effect on the
performance of the semantic model. Further-
more, we introduced a generic probabilistic distri-
butional similarity approach, which can leverage
the power of established probabilistic language
models to effectively model joint-contexts for the
purpose of measuring semantic similarity. Our
concrete model utilizing n-gram language models
outperforms several competitive baselines on se-
mantic similarity tasks, and appears to be partic-
ularly well-suited for verbs. In the remainder of
this section we describe some potential future di-
rections that can be pursued.
First, the performance of our generic scheme
is largely inherited from the nature of its under-
lying language model. Therefore, we see much
potential in exploring the use of other types of
language models, such as class-based (Brown et
al., 1992), syntax-based (Pauls and Klein, 2012)
or hybrid (Tan et al., 2012). Furthermore, a sim-
ilar approach to ours could be attempted in word
embedding models. For instance, our syntagmatic
joint-context modeling approach could be investi-
gated by word embedding models to generate bet-
ter embeddings for verbs.
Another direction relates to the well known ten-
dency of many words, and particularly verbs, to
assume different meanings (or senses) under dif-
ferent contexts. To address this phenomenon con-
text sensitive similarity and inference models have
been proposed (Dinu and Lapata, 2010; Melamud
et al., 2013). Similarly to many semantic similar-
ity models, our current model aggregates informa-
tion from all observed contexts of a target word
type regardless of its different senses. However,
we believe that our approach is well suited to ad-
dress context sensitive similarity with proper en-
hancements, as it considers joint-contexts that can
more accurately disambiguate the meaning of tar-
get words. As an example, it is possible to con-
sider the likelihood of word b to occur in a subset
of the contexts observed for word a, which is bi-
ased towards a particular sense of a.
Finally, we note that our model is not a classic
vector space model and therefore common vec-
tor composition approaches (Mitchell and Lap-
ata, 2008) cannot be directly applied to it. In-
stead, other methods, such as similarity of com-
positions (Turney, 2012), should be investigated to
extend our approach for measuring similarity be-
tween phrases.
188
Acknowledgments
This work was partially supported by the Israeli
Ministry of Science and Technology grant 3-8705,
the Israel Science Foundation grant 880/12, the
European Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
no. 287923 (EXCITEMENT) and the Scien-
tific and Technical Research Council of Turkey
(T
?
UB
?
ITAK, Grant Number 112E277).
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of NAACL. Association for Computational Lin-
guistics.
Guy Aston. 1997. The BNC Handbook Exploring the
British National Corpus with SARA Guy Aston and
Lou Burnard.
Yoshua Bengio, Rjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
Peter F. Brown, Peter V. Desouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4):467?479.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of ACL.
Alexander Budanitsky and Graeme Hirst. 2001.
Semantic distance in wordnet: An experimental,
application-oriented evaluation of five measures. In
Workshop on WordNet and Other Lexical Resources.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39(3):510?526.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22?29.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160?167. ACM.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of EMNLP.
Christiane Fellbaum. 2010. WordNet. Springer.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of the 10th inter-
national conference on World Wide Web. ACM.
James Gorman and James R. Curran. 2006. Scaling
distributional similarity to large corpora. In Pro-
ceedings of ACL.
Patrick Hanks. 2013. Lexical Analysis: Norms and
Exploitations. Mit Press.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In In-
ternational Conference on Acoustics, Speech, and
Signal Processing. IEEE.
Beth Levin. 1993. English verb classes and alter-
nations: A preliminary investigation. University of
Chicago press.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, & Computers.
Oren Melamud, Jonathan Berant, Ido Dagan, Jacob
Goldberger, and Idan Szpektor. 2013. A two level
model for context sensitive inference rules. In Pro-
ceedings of ACL.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL.
Adam Pauls and Dan Klein. 2011. Faster and Smaller
N -Gram Language Models. In Proceedings of ACL.
Adam Pauls and Dan Klein. 2012. Large-scale syntac-
tic language modeling with treelets. In Proceedings
of ACL.
Martin Riedl and Chris Biemann. 2013. Scaling to
large?3 data: An efficient and effective method to
compute distributional thesauri. In Proceedings of
EMNLP.
Tony Rose, Mark Stevenson, and Miles Whitehead.
2002. The Reuters Corpus Volume 1-from Yester-
day?s News to Tomorrow?s Language Resources. In
LREC.
189
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Using context-window overlapping
in synonym discovery and ontology extension. Pro-
ceedings of RANLP.
Ming Tan, Wenli Zhou, Lei Zheng, and Shaojun Wang.
2012. A scalable distributed syntactic, semantic,
and lexical language model. Computational Lin-
guistics, 38(3):631?671.
Peter D. Turney, Patrick Pantel, et al. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. Journal of artificial intelligence research.
Peter D. Turney. 2012. Domain and function: A
dual-space model of semantic relations and compo-
sitions. Journal of Artificial Intelligence Researc,
44(1):533?585, May.
Dongqiang Yang and David M. W. Powers. 2006. Verb
similarity on the taxonomy of wordnet. In the 3rd
International WordNet Conference (GWC-06).
Dongqiang Yang and David M. W. Powers. 2007.
An empirical investigation into grammatically con-
strained contexts in predicting distributional similar-
ity. In Australasian Language Technology Workshop
2007, pages 117?124.
Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret. 2012.
Learning syntactic categories using paradigmatic
representations of word context. In Proceedings of
EMNLP.
190
